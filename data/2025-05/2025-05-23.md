## [UPDATED!] **2025-05-23** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image Super-Resolution in Earth System|ViFOR：一种用于地球系统多图像超分辨率的傅里叶增强视觉Transformer|Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić|<http://arxiv.org/pdf/2502.12427v2>|ViFOR结合ViT和傅里叶隐神经表示网络，有效提升多图像超分辨率性能。|
|🆕 发布|Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation|Clip4Retrofit：通过跨架构CLIP蒸馏实现边缘设备上的实时图像标注|Li Zhong, Ahmed Ghazal, Jun-Jun Wan, Frederik Zilly, Patrick Mackens, Joachim E. Vollrath, Bogdan Sorin Coseriu|<http://arxiv.org/pdf/2505.18039v1>|Clip4Retrofit通过跨架构CLIP蒸馏，在资源受限的边缘设备上实现实时图像标注。|
|🆕 发布|RemoteSAM: Towards Segment Anything for Earth Observation|远程SAM：迈向地球观测的任何分割|Liang Yao, Fan Liu, Delong Chen, Chuanyi Zhang, Yijun Wang, Ziyun Chen, Wei Xu, Shimin Di .etc.|<http://arxiv.org/pdf/2505.18022v1>|[代码](https://github.com/1e12Leon/RemoteSAM.); 开发了一种适用于地球观测的Segment Anything模型，通过数据与模型创新，实现了高效的多任...|
|🆕 发布|Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment|可解释的基于解剖引导的AI在前列腺MRI中的应用：基于虚拟活检的风险评估的基座模型和虚拟临床试验|Danial Khan, Zohaib Salahuddin, Yumeng Zhang, Sheng Kuang, Shruti Atul Mali, Henry C. Woodruff, Sina Amirrajab, Rachel Cavill .etc.|<http://arxiv.org/pdf/2505.17971v1>|开发了一种基于深度学习的自动化前列腺癌风险评估系统，通过解剖引导和可解释性增强，显著提升了诊断准确性...|
|📝 更新|Simpler Fast Vision Transformers with a Jumbo CLS Token|更简单快速的视觉Transformer与巨型CLS标记|Anthony Fuller, Yousef Yassin, Daniel G. Kyrollos, Evan Shelhamer, James R. Green|<http://arxiv.org/pdf/2502.15021v2>|[代码](https://github.com/antofuller/jumbo); 提出Jumbo CLS Token增强Vision Transformers，在保持速度的同时显著提...|
|📝 更新|MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining|MAP：利用掩码自回归预训练释放混合Mamba-Transformer视觉骨干的潜力|Yunze Liu, Li Yi|<http://arxiv.org/pdf/2410.00871v3>|[代码](https://github.com/yunzeliu/MAP); 提出MAP预训练策略，有效提升混合Mamba-Transformer视觉骨干网络性能。|
|🆕 发布|Wildfire Detection Using Vision Transformer with the Wildfire Dataset|基于Wildfire Dataset的视觉Transformer wildfire检测|Gowtham Raj Vuppari, Navarun Gupta, Ahmed El-Sayed, Xingguo Xiong|<http://arxiv.org/pdf/2505.17395v1>|利用Vision Transformer和Wildfire数据集，提高了火灾检测的准确性和实时性。|


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer|多视图MRI直肠癌外周血管侵犯和浆膜下筋膜侵犯分类的基础模型框架|Yumeng Zhang, Zohaib Salahuddin, Danial Khan, Shruti Atul Mali, Henry C. Woodruff, Sina Amirrajab, Eduardo Ibor-Crespo, Ana Jimenez-Pastor .etc.|<http://arxiv.org/pdf/2505.18058v1>|开发了一种基于基础模型和多视角融合的MRI分类框架，显著提升了直肠癌侵犯诊断的准确性。|
|📝 更新|Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis|通过注意力分析探索多模态大型语言模型中的隐式视觉误解|Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue|<http://arxiv.org/pdf/2505.10541v2>|通过注意力分析揭示多模态大语言模型中的隐式视觉误解，并提出注意力准确率指标和新型基准来量化这种误解。|
|🆕 发布|Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data|像素到预后：多中心NSCLC数据中多区域CT-放射组学和基础模型特征的协调|Shruti Atul Mali, Zohaib Salahuddin, Danial Khan, Yumeng Zhang, Henry C. Woodruff, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati .etc.|<http://arxiv.org/pdf/2505.17893v1>|通过整合多区域CT图像特征和基础模型特征，提高了非小细胞肺癌患者生存预测的准确性。|
|🆕 发布|TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis|TextFlux：一种无OCR的DiT模型，用于高保真多语言场景文本合成|Yu Xie, Jielei Zhang, Pengyu Chen, Ziyue Wang, Weihang Wang, Longwen Gao, Peiyi Li, Huyang Sun .etc.|<http://arxiv.org/pdf/2505.17778v1>|提出TextFlux，一种无需OCR的DiT模型，实现高保真多语言场景文本合成。|
|📝 更新|Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property|通过具有一致性属性的二级嵌套特征归因解释黑盒模型预测|Yuya Yoshikawa, Masanari Kimura, Ryotaro Shimizu, Yuki Saito|<http://arxiv.org/pdf/2405.14522v2>|提出了一种基于一致性属性的模型无关局部解释方法，有效利用输入的嵌套结构同时估计两级特征归因。|
|📝 更新|LaViDa: A Large Diffusion Language Model for Multimodal Understanding|LaViDa：一种用于多模态理解的超大扩散语言模型|Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin .etc.|<http://arxiv.org/pdf/2505.16839v2>|LaViDa通过结合扩散模型和视觉编码器，实现了快速、可控的多模态理解。|
|📝 更新|Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models|重新思考视觉语言模型在安全微调中的瓶颈问题|Yi Ding, Lijun Li, Bing Cao, Jing Shao|<http://arxiv.org/pdf/2501.18533v2>|提出MIS数据集，通过多图像输入与安全思维链标签，提升视觉语言模型在安全场景下的推理能力。|
|📝 更新|HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation|人本基准：大型多模态模型评估的人中心框架|Shaina Raza, Aravind Narayanan, Vahid Reza Khazaie, Ashmal Vayani, Mukund S. Chettiar, Amandeep Singh, Mubarak Shah, Deval Pandya|<http://arxiv.org/pdf/2505.11454v2>|[代码](https://vectorinstitute.github.io/HumaniBench); HumaniBench构建首个以人为中心的AI基准，评估大型多模态模型在公平、伦理等原则上的表现。|
|🆕 发布|Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies|利用人类方法诊断视觉语言模型对色彩视觉缺陷的感知|Kazuki Hayashi, Shintaro Ozaki, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe|<http://arxiv.org/pdf/2505.17461v1>|利用色觉缺陷检测方法评估视觉语言模型对个体色觉差异的感知能力，揭示其模拟色觉感知的局限性。|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models|AutoMiSeg：通过基础模型测试时自适应的自动医学图像分割|Xingjian Li, Qifeng Wu, Colleen Que, Yiran Ding, Adithya S. Ubaradka, Jianhua Xing, Tianyang Wang, Min Xu|<http://arxiv.org/pdf/2505.17931v1>|提出了一种自动医疗图像分割方法，通过测试时自适应基础模型实现零样本分割。|
|🆕 发布|FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks|快速CAV：解释深度神经网络的概念激活向量的高效计算|Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling|<http://arxiv.org/pdf/2505.17883v1>|FastCAV通过加速概念激活向量的提取，有效降低了深度神经网络可解释性计算的成本。|
|🆕 发布|HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning|全息语言模型：基于语言的跨感官人类感知与推理的基础模型|Chuhao Zhou, Jianfei Yang|<http://arxiv.org/pdf/2505.17645v1>|HoloLLM通过融合多种传感器数据，显著提升了语言基础的人感推理准确性。|
|🆕 发布|RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition|鲁棒混合扩散恢复用于不完整多模态情感识别|Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, Kaixiang Yang|<http://arxiv.org/pdf/2505.17501v1>|提出RoHyDR框架，通过混合扩散和对抗学习，有效恢复缺失模态信息，提升多模态情感识别鲁棒性。|
|🆕 发布|Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads|去偏CLIP：解释和纠正注意力头中的偏差|Wei Jie Yeo, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy|<http://arxiv.org/pdf/2505.17425v1>|[代码](https://github.com/wj210/CLIP_LTC.); 提出LTC框架，识别并纠正CLIP模型中的偏见，提升分类性能。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Synergistic Bleeding Region and Point Detection in Laparoscopic Surgical Videos|腹腔镜手术视频中的协同出血区域和点检测|Jialun Pei, Zhangjun Zhou, Diandian Guo, Zhixi Li, Jing Qin, Bo Du, Pheng-Ann Heng|<http://arxiv.org/pdf/2503.22174v2>|开发了一种同时检测腹腔镜手术中出血区域和出血点的智能检测器，显著提升了手术成功率。|
|🆕 发布|ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection|ViP$^2$-CLIP：基于统一对齐的视觉感知提示在零样本异常检测中的应用|Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li|<http://arxiv.org/pdf/2505.17692v1>|ViP$^2$-CLIP通过融合视觉上下文生成细粒度文本提示，实现零样本异常检测，提升跨域泛化能力。|
|📝 更新|Enhanced 3D Object Detection via Diverse Feature Representations of 4D Radar Tensor|基于4D雷达张量多样化特征表示的增强3D目标检测|Seung-Hyun Song, Dong-Hee Paek, Minh-Quan Dao, Ezio Malis, Seung-Hyun Kong|<http://arxiv.org/pdf/2502.06114v3>|提出了一种利用多教师知识蒸馏的4D雷达三维目标检测框架，显著提升检测性能并降低数据需求。|
|🆕 发布|OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics|奥里恩基准：信息图表中图表和可识别物体检测的基准|Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu|<http://arxiv.org/pdf/2505.17473v1>|构建OrionBench基准，提升图表和可识别物体检测在信息图表中的准确性。|
|🆕 发布|Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture|优化YOLOv8用于停车位检测：定制YOLOv8架构的对比分析|Apar Pokhrel, Gia Dao|<http://arxiv.org/pdf/2505.17364v1>|针对停车空间检测难题，本文提出通过比较分析定制YOLOv8架构优化，提升检测准确性和效率。|


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery Detection|法医适配器：释放CLIP以实现通用的面部伪造检测|Xinjie Cui, Yuezun Li, Delong Zhu, Jiaran Zhou, Junyu Dong, Siwei Lyu|<http://arxiv.org/pdf/2411.19715v3>|[代码](https://github.com/OUC-VAS/ForensicsAdapter.); 设计Forensics Adapter，将CLIP转化为高效且通用的面部伪造检测器。|
|🆕 发布|Mahalanobis++: Improving OOD Detection via Feature Normalization|马哈拉诺比斯++：通过特征归一化改进异常检测|Maximilian Mueller, Matthias Hein|<http://arxiv.org/pdf/2505.18032v1>|通过特征$\ell_2$归一化，显著提升了基于马氏距离的异常检测性能。|
|📝 更新|CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering|CostFilter-AD：通过匹配成本过滤增强异常检测|Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu|<http://arxiv.org/pdf/2505.01476v2>|[代码](https://github.com/ZHE-SAPI/CostFilter-AD.); 将成本滤波技术应用于无监督异常检测，有效抑制匹配噪声，提升检测精度。|
|🆕 发布|MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings|多分辨率EEGWaveNet：用于长时程脑电图记录中癫痫检测的多分辨率EEGWaveNet|Kazi Mahmudul Hassan, Xuyang Zhao, Hidenori Sugano, Toshihisa Tanaka|<http://arxiv.org/pdf/2505.17972v1>|提出了一种多分辨率EEGWaveNet模型，有效区分癫痫事件与背景EEG和伪迹，显著提升检测准确率。|
|🆕 发布|Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection|双注意力残差U-Net在脑超声图像中准确检测脑室内出血的分割|Dan Yuan, Yi Feng, Ziyun Tang|<http://arxiv.org/pdf/2505.17683v1>|[代码](https://github.com/DanYuan001/BrainImgSegment.); 提出了一种结合双注意力机制的Residual U-Net，显著提升了脑超声图像中脑室区域的分割精度。|
|📝 更新|Boosting Edge Detection with Pixel-wise Feature Selection: The Extractor-Selector Paradigm|基于像素级特征选择的边缘检测增强：提取-选择范式|Hao Shu|<http://arxiv.org/pdf/2501.02534v2>|提出了一种基于像素级特征选择的边缘检测新框架，显著提升了边缘检测精度。|
|🆕 发布|MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery|Morton-Order退化估计机制用于恶劣天气图像恢复的MODEM|Hainuo Wang, Qiming Hu, Xiaojie Guo|<http://arxiv.org/pdf/2505.17581v1>|[代码](https://github.com/hainuo-wang/MODEM.git.); 提出了一种基于Morton编码和选择性状态空间模型的机制，用于准确估计恶劣天气图像的退化，实现自适应...|
|📝 更新|ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection|自适应感受野卷积和波纹注意力分层网络用于红外小目标检测|Xingye Cui, Junhai Luo, Jiakun Deng, Kexuan Li, Xiangyu Qiu, Zhenming Peng|<http://arxiv.org/pdf/2505.10595v2>|[代码](https://github.com/Leaf2001/ARFC-WAHNet.); 提出了一种自适应感受野卷积和波纹注意力分层网络，有效提升了红外小目标检测的准确性和鲁棒性。|
|🆕 发布|PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation|姿态BH：超越人体姿态估计的多数据集原型训练|Uyoung Jeong, Jonathan Freer, Seungryul Baek, Hyung Jin Chang, Kwang In Kim|<http://arxiv.org/pdf/2505.17475v1>|[代码](https://github.com/uyoung-jeong/PoseBH.); PoseBH通过引入非参数关键点原型和跨类型自监督机制，有效解决了多数据集姿态估计中的关键点异质性和...|


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching|双向最优标记匹配的超声心动图分割：BOTM：超声心动图分割|Zhihua Liu, Lei Tong, Xilin He, Che Liu, Rossella Arcucci, Chen Jin, Huiyu Zhou|<http://arxiv.org/pdf/2505.18052v1>|提出了一种基于双向最优标记匹配的超声心动图分割框架，有效解决了形状变化和区域模糊问题。|
|🆕 发布|Semantic segmentation with reward|语义分割与奖励|Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan|<http://arxiv.org/pdf/2505.17905v1>|提出RSS方法，利用奖励机制提升语义分割网络在像素级和图像级上的收敛性。|
|🆕 发布|SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data|SynRES：通过合成数据迈向真实场景中的指称表达式分割|Dong-Hee Kim, Hyunjee Song, Donghyun Kim|<http://arxiv.org/pdf/2505.17695v1>|[代码](https://github.com/UTLLab/SynRES.); 提出SynRES，通过合成数据解决复杂场景下指代表达式分割模型性能下降问题。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification|标题翻译结果：  SemSegBench & DetecBench：超越分类的可靠性和泛化性基准测试|Shashank Agnihotri, David Schader, Jonas Jakubassa, Nico Sharei, Simon Kral, Mehmet Ege Kaçar, Ruben Weber, Margret Keuper|<http://arxiv.org/pdf/2505.18015v1>|[代码](https://github.com/shashankskagnihotri/benchmarking_reliability_generalization); 提出SEMSEGBENCH和DetecBench工具，评估语义分割和目标检测模型的可靠性和泛化能力。|
|🆕 发布|Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring|赫菲斯托斯迷你立方体：用于火山活动监测的全局、多模态数据集|Nikolas Papadopoulos, Nikolaos Ioannis Bountos, Maria Sdraka, Andreas Karavias, Ioannis Papoutsis|<http://arxiv.org/pdf/2505.17782v1>|构建了全球火山活动监测的多模态数据集，推动机器学习在火山监测中的应用。|
|📝 更新|A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep Learning Image Classifiers|深度学习图像分类器严格评估的综合评估基准|Michael W. Spratling|<http://arxiv.org/pdf/2308.04137v3>|提出了一种全面评估基准，以评估深度学习图像分类器的鲁棒性和可靠性。|


## 生成式视觉模型 (Generative Visual Modeling)


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|TokBench: Evaluating Your Visual Tokenizer before Visual Generation|TokBench：在视觉生成之前评估您的视觉分词器|Junfeng Wu, Dongliang Luo, Weizhi Zhao, Zhihao Xie, Yuanhao Wang, Junyi Li, Xudong Xie, Yuliang Liu .etc.|<http://arxiv.org/pdf/2505.18142v1>|提出TokBench基准，评估视觉分词器在文本和面部重建中的性能，揭示其保留细节特征的局限性。|
|📝 更新|RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs|RBench-V：一种针对多模态输出视觉推理模型的初步评估|Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang .etc.|<http://arxiv.org/pdf/2505.16770v2>|[代码](https://evalmodels.github.io/rbenchv); 提出RBench-V基准，评估模型通过多模态输出进行视觉推理的能力。|
|🆕 发布|Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations|看见还是没看见？用于减轻物体幻觉的可解释视觉感知潜在引导|Boxu Chen, Ziwei Zheng, Le Yang, Zeyu Geng, Zhengyu Zhao, Chenhao Lin, Chao Shen|<http://arxiv.org/pdf/2505.17812v1>|[代码](https://github.com/Ziwei-Zheng/VaLSe.); 提出VaLSe框架，通过解释和引导潜在空间来减轻大型视觉语言模型中的物体幻觉问题。|
|🆕 发布|Generative Data Augmentation for Object Point Cloud Segmentation|生成数据增强用于点云分割|Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic|<http://arxiv.org/pdf/2505.17783v1>|提出了一种基于3D扩散模型的生成数据增强方法，有效提升了点云分割性能。|
|🆕 发布|Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM|槽-MLLM：多模态LLM的对象中心视觉标记化|Donghwan Chi, Hyomin Kim, Yoonjin Oh, Yongjin Kim, Donghoon Lee, Daejin Jo, Jongmin Kim, Junyeob Baek .etc.|<http://arxiv.org/pdf/2505.17726v1>|提出一种以对象为中心的视觉分词器，提升多模态LLM对视觉内容的理解和生成能力。|
|🆕 发布|Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports|利用布局模态增强大型视觉-语言模型，以实现针对日本年度证券报告的表格问答|Hayato Aida, Kosuke Takahashi, Takahiro Omi|<http://arxiv.org/pdf/2505.17625v1>|提出了一种结合表格文本和布局特征增强大型视觉语言模型，以提升对日证券年报表格问答的准确度。|
|📝 更新|Q-Insight: Understanding Image Quality via Visual Reinforcement Learning|Q-Insight：通过视觉强化学习理解图像质量|Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang|<http://arxiv.org/pdf/2503.22679v2>|[代码](https://github.com/lwq20020127/Q-Insight.); Q-Insight通过视觉强化学习，有效理解图像质量，实现零样本泛化。|
|🆕 发布|RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning|RePrompt：基于强化学习的文本到图像生成中的推理增强重提示|Mingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao Han .etc.|<http://arxiv.org/pdf/2505.17540v1>|通过引入推理增强，RePrompt显著提升了基于短提示的文本到图像生成质量。|
|📝 更新|AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era|AnimeDL-2M：扩散时代百万规模AI生成动漫图像检测与定位|Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen|<http://arxiv.org/pdf/2504.11015v2>|[代码](https://flytweety.github.io/AnimeDL2M); 提出AnimeDL-2M基准和AniXplore模型，有效检测和定位动漫图像伪造。|


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders|区域编码：基于补丁图像编码器的快速高效区域编码|Savya Khosla, Sethuraman TV, Barnett Lee, Alexander Schwing, Derek Hoiem|<http://arxiv.org/pdf/2505.18153v1>|[代码](https://github.com/savya08/REN.); 提出了一种快速高效的区域编码网络，通过点提示直接生成区域标记，显著提升性能和速度。|
|🆕 发布|WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions|奇趣玩：从单张图像和动作生成动态3D场景|Zizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, Jiajun Wu|<http://arxiv.org/pdf/2505.18151v1>|[代码](https://kyleleey.github.io/WonderPlay); WonderPlay通过结合物理模拟和视频生成，从单张图像和动作生成动态3D场景。|
|🆕 发布|F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles|F-ANcGAN：一种用于纳米粒子合成图像生成的注意力增强循环一致生成对抗网络架构|Varun Ajith, Anindya Pal, Saumik Bhattacharya, Sayantari Ghosh|<http://arxiv.org/pdf/2505.18106v1>|提出了一种基于注意力增强的循环一致性生成对抗网络，有效生成纳米粒子合成图像，缓解数据短缺问题。|
|🆕 发布|DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation|舞动共享！身份保留的多人物交互视频生成|Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li .etc.|<http://arxiv.org/pdf/2505.18078v1>|[代码](https://DanceTog.github.io/.); 提出了一种新框架，通过融合姿态和跟踪信息，实现了多人物身份保留的视频生成。|
|📝 更新|Multi-Faceted Multimodal Monosemanticity|多角度多模态单义性|Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, Yulan He, Yifei Wang|<http://arxiv.org/pdf/2502.14888v3>|开发多模态可解释性工具，揭示CLIP模型中不同模态特征，提升下游任务性能。|
|🆕 发布|SpikeGen: Generative Framework for Visual Spike Stream Processing|SpikeGen：视觉脉冲流处理的生成框架|Gaole Dai, Menghang Dong, Rongyu Zhang, Ruichuan An, Shanghang Zhang, Tiejun Huang|<http://arxiv.org/pdf/2505.18049v1>|SpikeGen通过生成模型有效解决视觉脉冲流处理中空间信息稀疏问题，提升不同视觉模态协同增强。|
|🆕 发布|RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration|RestoreVAR：全图像修复的视觉自回归生成|Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel|<http://arxiv.org/pdf/2505.18047v1>|RestoreVAR通过引入视觉自回归模型，显著提升了图像修复速度和性能。|
|🆕 发布|ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback|舒适心智：基于树状规划和反应性反馈的通用生成方法|Litao Guo, Xinli Xu, Luozhou Wang, Jiantao Lin, Jinsong Zhou, Zixin Zhang, Bolan Su, Ying-Cong Chen|<http://arxiv.org/pdf/2505.17908v1>|[代码](https://github.com/LitaoGuo/ComfyMind); 定位问题：现有通用生成模型脆弱，难以支持复杂应用。|
|🆕 发布|Track Anything Annotate: Video annotation and dataset generation of computer vision models|《任意目标跟踪标注：计算机视觉模型的视频标注与数据集生成》|Nikita Ivanov, Mark Klimov, Dmitry Glukhikh, Tatiana Chernysheva, Igor Glukhikh|<http://arxiv.org/pdf/2505.17884v1>|[代码](https://github.com/lnikioffic/track-anything-annotate); 提出了一种基于视频跟踪和分割的自动化标注工具，大幅提升数据集生成效率。|
|🆕 发布|Multi-Person Interaction Generation from Two-Person Motion Priors|多人交互生成基于双人运动先验|Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho|<http://arxiv.org/pdf/2505.17860v1>|提出一种利用两人运动先验和图驱动的交互采样方法，有效生成多样化多人交互动作。|
|🆕 发布|R-Genie: Reasoning-Guided Generative Image Editing|R-Genie：推理引导的生成图像编辑|Dong Zhang, Lingfeng He, Rui Yan, Fei Shen, Jinhui Tang|<http://arxiv.org/pdf/2505.17768v1>|R-Genie通过结合推理和生成模型，实现了基于复杂文本指令的智能图像编辑。|
|🆕 发布|Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek|Seek-CAD：基于深度Seek的局部推理的自优化3D参数化CAD生成模型|Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou|<http://arxiv.org/pdf/2505.17702v1>|Seek-CAD利用本地开源LLM DeepSeek-R1，结合视觉和思维链反馈，实现了无监督的3D...|
|📝 更新|ReactDiff: Latent Diffusion for Facial Reaction Generation|ReactDiff：面部反应生成的潜在扩散|Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang|<http://arxiv.org/pdf/2505.14151v2>|[代码](https://github.com/Hunan-Tiger/ReactDiff); ReactDiff通过融合多模态Transformer和条件扩散，有效提升了面部反应生成的相关性和多...|
|📝 更新|Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints|Ctrl-Room：具有布局约束的可控文本到3D房间网格生成|Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, Ping Tan|<http://arxiv.org/pdf/2310.03602v4>|Ctrl-Room通过分离布局和外观建模，实现了从文本到3D室内场景的生成与灵活编辑。|
|🆕 发布|CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis|CGS-GAN：用于高分辨率人头合成的3D一致高斯分层生成对抗网络|Florian Barthel, Wieland Morgenstern, Paul Hinzer, Anna Hilsmann, Peter Eisert|<http://arxiv.org/pdf/2505.17590v1>|[代码](https://fraunhoferhhi.github.io/cgs-gan); 提出CGS-GAN，通过多视角正则化和改进的生成器架构，实现无视角条件约束下的人头3D高分辨率合成。|
|🆕 发布|MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation|MMMG：多任务多模态生成全面可靠的评估套件|Jihan Yao, Yushi Hu, Yujie Yi, Bin Han, Shangbin Feng, Guang Yang, Bingbing Wen, Ranjay Krishna .etc.|<http://arxiv.org/pdf/2505.17613v1>|提出MMMG，为多模态生成提供全面可靠的人机一致评估基准。|
|🆕 发布|Scaling Image and Video Generation via Test-Time Evolutionary Search|通过测试时进化搜索扩展图像和视频生成|Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan|<http://arxiv.org/pdf/2505.17618v1>|[代码](https://tinnerhrhe.github.io/evosearch.); 提出EvoSearch方法，通过测试时进化搜索提升图像和视频生成模型的可扩展性和多样性。|
|🆕 发布|Co-Reinforcement Learning for Unified Multimodal Understanding and Generation|协同强化学习实现统一的多模态理解和生成|Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma|<http://arxiv.org/pdf/2505.17534v1>|提出CoRL框架，通过联合优化提升ULMs的多模态理解和生成能力。|
|🆕 发布|Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision|从全语义监督中学习通用和灵活的轨迹模型|Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Xiao Han, Qidong Liu, Xuetao Wei, Yuxuan Liang|<http://arxiv.org/pdf/2505.17437v1>|提出OmniTraj框架，通过融合多种语义信息，实现高效灵活的轨迹检索。|
|📝 更新|Challenger: Affordable Adversarial Driving Video Generation|挑战者：经济实惠的对抗性驾驶视频生成|Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao .etc.|<http://arxiv.org/pdf/2505.15880v2>|Challenger通过联合优化交通交互和传感器观察，生成低成本、逼真的对抗性驾驶视频。|
|🆕 发布|Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention|直接3D-S2：借助空间稀疏注意力实现大规模3D生成|Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu .etc.|<http://arxiv.org/pdf/2505.17412v1>|[代码](https://nju3dv.github.io/projects); Direct3D-S2通过空间稀疏注意力机制，大幅降低3D生成训练成本，实现高效的大规模3D形状生成...|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations|双言：3D说话头像对话中的双发言人交互|Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, Zhaoxin Fan|<http://arxiv.org/pdf/2505.18096v1>|[代码](https://ziqiaopeng.github.io/dualtalk.); DualTalk通过模拟说话者和听者的动态行为，显著提升了3D对话头像的自然性和表现力。|
|🆕 发布|CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention|自适应多模态跨注意力深度伪造图像检测：CAMME|Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil|<http://arxiv.org/pdf/2505.18035v1>|提出CAMME框架，通过多模态跨注意力机制实现自适应深度伪造图像检测，显著提升检测准确率。|
|📝 更新|RCR: Robust Crowd Reconstruction with Upright Space from a Single Large-scene Image|RCR：基于单张大场景图像的鲁棒直立空间人群重建|Jing Huang, Hao Wen, Tianyi Zhou, Haozhe Lin, Yu-kun Lai, Kun Li|<http://arxiv.org/pdf/2411.06232v2>|提出RCR方法，从单张大场景图像中实现鲁棒的群体重建，解决深度模糊和透视失真问题。|
|📝 更新|D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing|D3C2-Net：用于压缩感知的双域深度卷积编码网络|Weiqi Li, Bin Chen, Shuai Liu, Shijie Zhao, Bowen Du, Yongbing Zhang, Jian Zhang|<http://arxiv.org/pdf/2207.13560v2>|[代码](https://github.com/lwq20020127/D3C2-Net.); 提出了一种结合图像域和卷积编码域先验的深度神经网络，有效提升了压缩感知任务的性能和灵活性。|
|🆕 发布|InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO|InfLVG：基于GRPO的强化推理时一致长视频生成|Xueji Fang, Liyuan Ma, Zhiyang Chen, Mingyuan Zhou, Guo-jun Qi|<http://arxiv.org/pdf/2505.17574v1>|[代码](https://github.com/MAPLE-AIGC/InfLVG.); InfLVG通过GRPO优化，实现长视频生成，保持场景一致性，提升视频长度和语义准确性。|
|🆕 发布|The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts|《连贯性陷阱：当多模态语言模型创作的叙事利用操纵的视觉上下文时》|Yuchen Zhang, Yaxiong Wang, Yujiao Wu, Lianwei Wu, Li Zhu|<http://arxiv.org/pdf/2505.17476v1>|提出了一种利用MLLM生成高风险虚假信息并检测其的方法，以应对多媒体操纵检测的挑战。|


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Towards more transferable adversarial attack in black-box manner|向更可迁移的黑盒对抗攻击迈进|Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, Chun Pong Lau|<http://arxiv.org/pdf/2505.18097v1>|提出了一种新型损失函数和替代模型，显著提升了黑盒对抗攻击的可迁移性，同时降低计算成本。|
|🆕 发布|FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation|FDBPL：基于蒸馏的快速提示学习，用于区域感知视觉-语言模型适应|Zherui Zhang, Jiaxin Wu, Changwei Wang, Rongtao Xu, Longzhao Huang, Wenhao Xu, Wenbo Xu, Li Guo .etc.|<http://arxiv.org/pdf/2505.18053v1>|提出FDBPL，通过共享软监督上下文和区域感知提示学习，实现高效参数蒸馏式提示学习，提升视觉语言模型...|
|🆕 发布|Knot So Simple: A Minimalistic Environment for Spatial Reasoning|结不再简单：一个用于空间推理的简约环境|Zizhao Chen, Yoav Artzi|<http://arxiv.org/pdf/2505.18028v1>|[代码](https://github.com/lil-lab/knotgym.); KnotGym构建了一个基于图像观察的绳结操作环境，用于测试空间推理和操作能力。|
|🆕 发布|Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation|任意词分割：开放集基于实例分割的掩码提示逆算|Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe .etc.|<http://arxiv.org/pdf/2505.17994v1>|Segment Anyword通过利用冻结扩散模型的token级跨注意力图，实现了无需训练的开放集语...|
|🆕 发布|Diffusion Classifiers Understand Compositionality, but Conditions Apply|扩散分类器理解组合性，但条件适用|Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach|<http://arxiv.org/pdf/2505.17955v1>|[代码](https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.); 该论文研究了扩散分类器在理解视觉场景组合性方面的能力，并揭示了模型成功的关键条件。|
|🆕 发布|DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning|扩散奖励：通过奖励反馈学习增强盲脸修复|Bin Wu, Wei Wang, Yahui Liu, Zixiang Li, Yao Zhao|<http://arxiv.org/pdf/2505.17910v1>|[代码](https://github.com/01NeuralNinja/DiffusionReward.); DiffusionReward通过奖励反馈学习，有效提升了盲脸修复的逼真度和身份一致性。|
|📝 更新|Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation|消除扩散模型中的不良概念：对抗性保留|Anh Bui, Long Vuong, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung|<http://arxiv.org/pdf/2410.15618v4>|[代码](https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.); 提出一种通过识别和保留对抗性概念来有效消除扩散模型中不希望内容的方法。|
|📝 更新|Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them|扩散模型中用于概念擦除的神奇目标及其寻找方法|Anh Bui, Trang Vu, Long Vuong, Trung Le, Paul Montague, Tamas Abraham, Junae Kim, Dinh Phung|<http://arxiv.org/pdf/2501.18950v3>|[代码](https://github.com/tuananhbui89/Adaptive-Guided-Erasure); 提出自适应引导消除法，有效消除扩散模型中的有害内容，同时保留无关概念。|
|📝 更新|QVGen: Pushing the Limit of Quantized Video Generative Models|QVGen：推动量化视频生成模型的极限|Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang|<http://arxiv.org/pdf/2505.11497v2>|QVGen通过量化感知训练和降秩策略，首次在4位量化下实现与全精度视频生成模型相当的质量。|
|🆕 发布|SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation|海狮：语义部分感知潜在点扩散模型用于3D生成|Dekai Zhu, Yan Di, Stefan Gavranovic, Slobodan Ilic|<http://arxiv.org/pdf/2505.17721v1>|SeaLion通过语义部分感知的潜在点扩散模型，实现了带分割标签的高质量点云生成。|
|🆕 发布|CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment|CAS-IQA：训练视觉-语言模型进行合成血管造影质量评估|Bo Wang, De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Nu-Fang Xiao, Jian-Long Hao, Ming-Yuan Liu, Zeng-Guang Hou|<http://arxiv.org/pdf/2505.17619v1>|提出一种基于视觉-语言模型的合成血管造影图像质量评估框架，显著提升评估准确性。|
|🆕 发布|Enhancing Fourier-based Doppler Resolution with Diffusion Models|基于扩散模型的傅里叶多普勒分辨率增强|Denisa Qosja, Kilian Barth, Simon Wagner|<http://arxiv.org/pdf/2505.17567v1>|利用扩散模型提升傅里叶多普勒分辨率，有效分离密集目标。|
|🆕 发布|Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model|模型已知道最佳噪声：视频扩散模型中的注意力贝叶斯主动噪声选择|Kwanyoung Kim, Sanghyun Kim|<http://arxiv.org/pdf/2505.17561v1>|[代码](https://anse-project.github.io/anse-project); 提出了一种基于注意力机制的主动噪声选择方法，显著提升了视频扩散模型的质量和时序一致性。|
|🆕 发布|T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models|T2VUnlearning：文本到视频扩散模型的概念擦除方法|Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li|<http://arxiv.org/pdf/2505.17550v1>|[代码](https://github.com/VDIGPKU/T2VUnlearning.git); 提出T2VUnlearning方法，有效消除T2V模型中特定有害概念，同时保持其他概念生成能力。|
|🆕 发布|Deeper Diffusion Models Amplify Bias|深度扩散模型放大偏差|Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian|<http://arxiv.org/pdf/2505.17560v1>|揭示了深度扩散模型放大训练数据偏差的风险，并提出了一种无需训练即可提升生成图像质量的方法。|
|🆕 发布|Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning|通过对抗混合提示调整增强视觉语言模型的对抗鲁棒性|Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, Xingxing Wei|<http://arxiv.org/pdf/2505.17509v1>|提出Adversarial Mixture Prompt Tuning方法，增强视觉语言模型对对抗样...|
|📝 更新|Panoptic Captioning: Seeking An Equivalency Bridge for Image and Text|全景描述：寻求图像与文本的等价桥梁|Kun-Yu Lin, Hongjun Wang, Weining Ren, Kai Han|<http://arxiv.org/pdf/2505.16334v2>|[代码](https://visual-ai.github.io/pancap); 提出了一种名为PancapChain的图像描述生成方法，显著提升了多模态大语言模型在图像描述任务上的...|
|🆕 发布|Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling|变分自编码离散扩散：增强维度相关性建模|Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang|<http://arxiv.org/pdf/2505.17384v1>|提出VADD框架，通过潜在变量建模增强离散扩散模型，显著提升样本质量。|
|🆕 发布|Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues|利用散光模糊线索重用金盏花进行零样本度量深度估计|Chinmay Talegaonkar, Nikhil Gandudi Suresh, Zachary Novack, Yash Belhe, Priyanka Nagasamudra, Nicholas Antipa|<http://arxiv.org/pdf/2505.17358v1>|通过引入散焦模糊线索，将预训练的Marigold模型转化为无监督的深度预测器，有效提升了零样本深度估...|
|🆕 发布|Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey|通过强化学习和奖励建模实现扩散模型的对齐与安全性：综述|Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar|<http://arxiv.org/pdf/2505.17352v1>|通过强化学习和奖励建模，该论文提出了一种方法来提高扩散模型与人类偏好和安全性约束的一致性。|
|📝 更新|When Are Concepts Erased From Diffusion Models?|当概念从扩散模型中被擦除时？|Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen|<http://arxiv.org/pdf/2505.17013v2>|提出评估方法，揭示扩散模型中概念消除的全面性及其与鲁棒性的权衡。|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation|3D人脸重建误差分解：一个模块化基准，用于公平且快速的方法评估|Evangelos Sariyanidi, Claudio Ferrari, Federico Nocentini, Stefano Berretti, Andrea Cavallaro, Birkan Tunc|<http://arxiv.org/pdf/2505.18025v1>|提出模块化3D人脸重建基准，揭示现有误差度量方法的不足，并提升重建方法评估的准确性。|
|🆕 发布|Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets|标题翻译：基于单张深度图像的规范姿态重建，用于有限数据集上的3D非刚性姿态恢复|Fahd Alhamazani, Yu-Kun Lai, Paul L. Rosin|<http://arxiv.org/pdf/2505.17992v1>|提出了一种从单张深度图重建标准姿态模型的方法，有效解决非刚性物体3D姿态恢复问题。|
|🆕 发布|SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes|SplatCo：结构-视图协同高斯分层渲染，用于大规模无界场景的细节保留渲染|Haihong Xiao, Jianan Zou, Yuxin Zhou, Ying He, Wenxiong Kang|<http://arxiv.org/pdf/2505.17951v1>|[代码](https://github.com/SCUT-BIP-Lab/SplatCo.); SplatCo通过结构-视图协同高斯分层渲染，实现了大规模场景的高保真细节还原。|
|🆕 发布|Is Single-View Mesh Reconstruction Ready for Robotics?|单视图网格重建是否已准备好应用于机器人学？|Frederik Nolte, Bernhard Schölkopf, Ingmar Posner|<http://arxiv.org/pdf/2505.17966v1>|评估单视图网格重建模型在机器人操作中的适用性，揭示其在实际应用中的局限性。|
|📝 更新|VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold|VGGT-SLAM：基于SL(4)流形优化的密集RGB SLAM|Dominic Maggio, Hyungtae Lim, Luca Carlone|<http://arxiv.org/pdf/2505.12549v2>|VGGT-SLAM通过优化SL(4)流形，解决了无标定相机下RGB SLAM的重建模糊性问题，显著提...|
|🆕 发布|Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery|朝向眼科手术中手-器械交互的动态3D重建|Ming Hu, Zhendi Yu, Feilong Tang, Kaiwen Chen, Yulong Li, Imran Razzak, Junjun He, Tolga Birdal .etc.|<http://arxiv.org/pdf/2505.17677v1>|构建了首个大规模眼外科手术动态3D重建数据集，并提出新模型显著提升手和器械重建精度。|
|🆕 发布|From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation|从飞行到洞察：通过高斯喷溅和语言引导分割的语义3D重建用于空中检查|Mahmoud Chick Zaouali, Todd Charter, Homayoun Najjaran|<http://arxiv.org/pdf/2505.17402v1>|提出了一种结合语言引导和语义分割的无人机航拍三维重建方法，提升空中检测和场景理解能力。|
|📝 更新|Decoupled Geometric Parameterization and its Application in Deep Homography Estimation|解耦几何参数化及其在深度单应性估计中的应用|Yao Huang, Si-Yuan Cao, Yaqing Ding, Hao Yin, Shibin Xie, Shuting Wang, Zhijun Fang, Jiachun Wang .etc.|<http://arxiv.org/pdf/2505.16599v2>|提出了一种基于SKS分解的几何参数化方法，简化了深度单应性估计过程。|


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using Multi-scale Implicit Neural Representation|快速全脑运动鲁棒性中尺度活体MR成像：基于多尺度隐式神经网络表示|Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi|<http://arxiv.org/pdf/2502.08634v2>|提出ROVER-MRI，通过多尺度隐式神经网络快速恢复脑部精细结构，显著缩短扫描时间并提高图像质量。|
|📝 更新|Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets|探索广义步态识别：降低室内和室外数据集中的冗余和噪声|Qian Zhou, Xianda Guo, Jilong Wang, Chuanfu Shen, Zhongyuan Wang, Hua Zou, Qin Zou, Chao Liang .etc.|<http://arxiv.org/pdf/2505.15176v2>|[代码](https://github.com/li1er3/Generalized_Gait.); 提出了一种统一框架，通过解耦损失和筛选样本，有效提升跨域步态识别性能。|
|🆕 发布|UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions|超骨UDF：基于神经无符号距离函数的超声自监督骨骼表面重建|Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti, Giuseppe Loggia, Lisa Reissner, Bastian Sigrist, Jonas Hein, Lilian Calvet .etc.|<http://arxiv.org/pdf/2505.17912v1>|提出了一种基于神经无符号距离函数的自监督超声骨表面重建方法，有效提升了重建精度。|
|🆕 发布|Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification|原型FG3D：基于原型的可解释细粒度3D形状分类|Shuxian Ma, Zihao Dong, Runmin Cong, Sam Kwong, Xiuli Shao|<http://arxiv.org/pdf/2505.17666v1>|提出Proto-FG3D，通过原型学习方法提升3D形状细粒度分类的准确性和可解释性。|
|🆕 发布|DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer|基于DECT的空间挤压方法在乳腺癌转移淋巴结多分类中的应用|Hai Jiang, Chushan Zheng, Jiawei Pan, Yuanpin Zhou, Qiongting Liu, Xiang Zhang, Jun Shen, Yao Lu|<http://arxiv.org/pdf/2505.17528v1>|提出了一种基于DECT的空间挤压方法，有效提高了乳腺癌淋巴结转移负担的多类分类准确性。|
|📝 更新|TDFormer: A Top-Down Attention-Controlled Spiking Transformer|TDFormer：一种自顶向下的注意力控制脉冲神经网络变换器|Zizheng Zhu, Yingchao Yu, Zeqi Zheng, Zhaofei Yu, Yaochu Jin|<http://arxiv.org/pdf/2505.15840v2>|TDFormer通过引入自上而下的反馈结构，显著提升了基于脉冲神经网络的计算机视觉模型性能。|
|🆕 发布|PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints|动物足迹识别：这些脚印是谁的？|Inpyo Song, Hyemin Hwang, Jangwon Lee|<http://arxiv.org/pdf/2505.17445v1>|PawPrint提出了一种通过脚印识别动物个体的方法，为宠物管理和野生动物保护提供了一种非侵入式解决...|


## 时序视觉分析 (Temporal Visual Analysis)


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding|深度视频发现：用于长视频理解的代理搜索与工具使用|Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu|<http://arxiv.org/pdf/2505.18079v1>|提出了一种基于代理搜索和工具使用的深度视频发现方法，显著提升了长视频理解能力。|
|🆕 发布|VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR|VIBE：用于TL;DR的视频到文本信息瓶颈评估|Shenghui Chen, Po-han Li, Sandeep Chichali, Ufuk Topcu|<http://arxiv.org/pdf/2505.17423v1>|VIBE通过评估视频摘要的准确性和实用性，提高了视觉语言模型在决策任务中的效率。|
|📝 更新|Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection|闪回：基于记忆的零样本、实时视频异常检测|Hyogun Lee, Haksub Kim, Ig-Jae Kim, Yonghun Choi|<http://arxiv.org/pdf/2505.15205v2>|Flashback通过构建伪场景记忆库，实现零样本、实时视频异常检测。|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios|SHARDeg：退化场景下骨骼人体动作识别基准|Simon Malzard, Nitish Mital, Richard Walters, Victoria Nockles, Raghuveer Rao, Celso M. De Melo|<http://arxiv.org/pdf/2505.18048v1>|构建了首个针对骨骼人体动作识别在退化场景下的基准，并提升了模型对退化数据的鲁棒性。|
|🆕 发布|Multi-task Learning For Joint Action and Gesture Recognition|多任务学习用于联合动作和手势识别|Konstantinos Spathis, Nikolaos Kardaris, Petros Maragos|<http://arxiv.org/pdf/2505.17867v1>|通过多任务学习联合动作和手势识别，实现更高效、鲁棒和通用的视觉表征。|
|🆕 发布|Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition|时间一致性约束的可迁移对抗攻击与背景混合用于动作识别|Ping Li, Jianan Ni, Bo Pang|<http://arxiv.org/pdf/2505.17807v1>|[代码](https://github.com/mlvccn/BMTC_TransferAttackVid.); 提出了一种背景混合增强的时序一致性攻击方法，显著提升了动作识别模型的对抗样本迁移能力。|
|🆕 发布|SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding|SVL：基于脉冲的视觉-语言预训练，用于高效的3D开放世界理解|Xuerui Qiu, Peixi Wu, Yaozhi Wen, Shaowei Gu, Yuqi Pan, Xinhao Luo, Bo XU, Guoqi Li|<http://arxiv.org/pdf/2505.17674v1>|[代码](https://github.com/bollossom/SVL.); 提出SVL预训练框架，提升SNN在3D开放世界理解中的效率和准确性。|


### 视频目标跟踪 (Video Object Tracking)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization|ProTAL：一种用于时间动作定位的拖拽式视频编程框架|Yuchen He, Jianbing Lv, Liqi Cheng, Lingyu Meng, Dazhen Deng, Yingcai Wu|<http://arxiv.org/pdf/2505.17555v1>|提出ProTAL框架，通过拖拽和连接定义关键事件，实现视频动作定位的半监督学习。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Boosting Open Set Recognition Performance through Modulated Representation Learning|通过调制表示学习提升开放集识别性能|Amit Kumar Kundu, Vaishnavi Patil, Joseph Jaja|<http://arxiv.org/pdf/2505.18137v1>|通过引入温度调节的表示学习方法，提升了开放集识别性能。|
|🆕 发布|Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning|局部敏感哈希在对比学习中的高效难负样本采样|Fabian Deuser, Philipp Hausenblas, Hannah Schieber, Daniel Roth, Martin Werner, Norbert Oswald|<http://arxiv.org/pdf/2505.17844v1>|提出了一种基于LSH的GPU高效方法，以高效地挖掘对比学习中的硬负样本。|
|🆕 发布|5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization|5G-DIL：基于相似性感知采样的动态5G室内定位领域增量学习|Nisha Lakshmana Raichur, Lucas Heublein, Christopher Mutschler, Felix Ott|<http://arxiv.org/pdf/2505.17684v1>|定位论文核心贡献：|
|🆕 发布|PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings|病理视域：通过自监督对比学习和病理信息合成嵌入的少样本病理检测|Sinchee Chin, Yinuo Ma, Xiaochen Yang, Jing-Hao Xue, Wenming Yang|<http://arxiv.org/pdf/2505.17614v1>|PathoSCOPE通过自监督对比学习和病理信息合成嵌入，实现少量样本的无监督病理检测，显著提升数据...|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Accelerating Learned Image Compression Through Modeling Neural Training Dynamics|通过建模神经网络训练动态加速学习图像压缩|Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu|<http://arxiv.org/pdf/2505.18107v1>|通过建模神经网络训练动态，提出STDET机制和SMA技术，加速了学习图像压缩方法的训练过程。|
|🆕 发布|U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding|U2-BENCH：在超声理解上对大型视觉-语言模型的基准测试|Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang .etc.|<http://arxiv.org/pdf/2505.17779v1>|构建了首个超声图像理解基准U2-BENCH，评估大型视觉语言模型在医学超声领域的性能。|
|📝 更新|Camera Movement Estimation and Path Correction using the Combination of Modified A-SIFT and Stereo System for 3D Modelling|基于改进A-SIFT与立体系统的相机运动估计及路径校正用于三维建模|Usha Kumari, Shuvendu Rana|<http://arxiv.org/pdf/2503.17668v2>|提出改进ASIFT与双摄像头SFM结合的方法，实现高精度三维相机运动轨迹估计。|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Semantic Correspondence: Unified Benchmarking and a Strong Baseline|语义对应：统一基准测试和强大基线|Kaiyan Zhang, Xinghui Li, Jingyi Lu, Kai Han|<http://arxiv.org/pdf/2505.18060v1>|[代码](https://github.com/Visual-AI/Semantic-Correspondence.); 首次全面综述语义对应方法，提出统一基准和高效基线。|
|🆕 发布|DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval|细节融合：用于合成图像检索的双分支细节增强框架|Yuxin Yang, Yinan Zhou, Yuxin Chen, Ziqi Zhang, Zongyang Ma, Chunfeng Yuan, Bing Li, Lin Song .etc.|<http://arxiv.org/pdf/2505.17796v1>|提出DetailFusion框架，通过细节增强提升合成图像检索性能。|
|📝 更新|MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention|MMInference：通过模态感知排列稀疏注意力加速长上下文VLMs的预填充|Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li .etc.|<http://arxiv.org/pdf/2504.16083v2>|提出MMInference，通过模态感知排列稀疏注意力加速长上下文视觉语言模型预填充。|
|📝 更新|Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)|追踪飞行：探索分析平原斑马（Equus quagga）逃逸反应的计算框架|Isla Duporge, Sofia Minano, Nikoloz Sirmpilatze, Igor Tatarnikov, Scott Wolf, Adam L. Tyson, Daniel Rubenstein|<http://arxiv.org/pdf/2505.16882v2>|开发了一种计算框架，通过图像注册和SfM技术，有效分离动物和无人机运动，分析草原斑马逃逸行为。|


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds|开放集步态识别：稀疏毫米波雷达点云|Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi|<http://arxiv.org/pdf/2503.07435v3>|提出了一种从稀疏毫米波雷达点云中进行开放集步态识别的新方法，显著提升了识别准确率。|
|🆕 发布|A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances|核心集选择文献综述：引言与近期进展|Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel|<http://arxiv.org/pdf/2505.17799v1>|该论文综合了多种Coreset选择方法，构建了统一分类体系，并分析了其影响和挑战。|
|🆕 发布|Soft-CAM: Making black box models self-explainable for high-stakes decisions|软-CAM：使高风险决策的黑盒模型可解释|Kerol Djoumessi, Philipp Berens|<http://arxiv.org/pdf/2505.17748v1>|Soft-CAM通过改进CNN架构，使模型在保持性能的同时，生成直观的类激活图，提升黑盒模型的解释性...|
|🆕 发布|MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity|MinkUNeXt-SI：基于点云的场所识别改进，包括球坐标和激光雷达强度|Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente|<http://arxiv.org/pdf/2505.17591v1>|提出了一种结合Minkowski卷积和U-net架构的深度学习方法，显著提升了基于点云的场所识别准确...|
|🆕 发布|Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes|基于MRI的胎盘植入症谱及其亚型分类的解剖学引导的多任务学习|Hai Jiang, Qiongting Liu, Yuanpin Zhou, Jiawei Pan, Ting Song, Yao Lu|<http://arxiv.org/pdf/2505.17484v1>|提出了一种基于解剖引导的多任务学习CNN模型，实现MRI胎盘侵袭谱及其亚型的准确分类。|
|🆕 发布|Dual-sensing driving detection model|双传感器驾驶检测模型|Leon C. C. K, Zeng Hui|<http://arxiv.org/pdf/2505.17392v1>|提出了一种结合计算机视觉和生理信号分析的驾驶疲劳检测模型，有效提升了检测准确性和可靠性。|
|🆕 发布|Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches|物联网僵尸网络检测中GNN是否值得投入？VAE-GNN与ViT-MLP和VAE-MLP方法的比较研究|Hassan Wasswa, Hussein Abbass, Timothy Lynar|<http://arxiv.org/pdf/2505.17363v1>|比较了VAE-GNN、ViT-MLP和VAE-MLP在物联网僵尸网络检测中的性能，发现VAE和ViT...|
|🆕 发布|Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction|图注意力神经网络在僵尸网络检测中的应用：评估自动编码器、变分自编码器和基于PCA的降维|Hassan Wasswa, Hussein Abbass, Timothy Lynar|<http://arxiv.org/pdf/2505.17357v1>|提出了一种基于图注意力神经网络和降维技术的网络钓鱼检测框架，有效提升了检测精度。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development|注意领域差距：测量真实世界与合成点云之间的领域差距以促进自动驾驶开发|Nguyen Duc, Yan-Ling Lai, Patrick Madlindl, Xinyuan Zhu, Benedikt Schwab, Olaf Wysocki, Ludwig Hoegner, Thomas H. Kolbe|<http://arxiv.org/pdf/2505.17959v1>|提出了一种测量真实世界与合成点云域间差距的新方法，以提升自动驾驶数据模拟的可靠性。|
|📝 更新|SceneTracker: Long-term Scene Flow Estimation Network|场景追踪器：长期场景光流估计网络|Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, Dewen Hu|<http://arxiv.org/pdf/2403.19924v4>|[代码](https://github.com/wwsource/SceneTracker.); 提出SceneTracker，一种迭代式LSFE网络，有效解决3D空间遮挡和深度噪声问题。|


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|DiffBreak: Is Diffusion-Based Purification Robust?|标题翻译：DiffBreak：基于扩散的净化是否稳健？|Andre Kassis, Urs Hengartner, Yaoliang Yu|<http://arxiv.org/pdf/2411.16598v3>|揭示了扩散净化方法在对抗样本防御中的脆弱性，并提出改进方案以增强其鲁棒性。|
|📝 更新|X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP|X-Transfer攻击：迈向对CLIP的超可迁移对抗攻击|Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey|<http://arxiv.org/pdf/2505.05528v2>|[代码](https://github.com/HanxunH/XTransferBench); 提出X-Transfer攻击方法，实现CLIP模型间超可迁移的对抗攻击。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models|思考与否？通过强化学习为视觉-语言模型进行选择性推理|Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou|<http://arxiv.org/pdf/2505.16854v2>|[代码](https://github.com/kokolerk/TON.); 提出TON方法，通过选择性推理提升视觉语言模型效率。|
|🆕 发布|Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling|基于层次视觉-语言对齐与建模的吉像素图像小样本学习|Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi|<http://arxiv.org/pdf/2505.17982v1>|[代码](https://github.com/bryanwong17/HiVE-MIL); HiVE-MIL通过构建统一图和动态过滤机制，有效解决多尺度视觉-语言对齐问题，提升少量样本学习性能...|
|🆕 发布|Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging|临床验证：基于光谱成像的实时组织氧饱和度深度学习估计|Jens De Winne, Siri Willems, Siri Luthman, Danilo Babin, Hiep Luong, Wim Ceelen|<http://arxiv.org/pdf/2505.18010v1>|提出深度学习方法实时估算组织氧合，有效降低模拟与临床数据差距。|
|🆕 发布|Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy|肾结石类型在输尿管镜检查中少样本学习方法的评估|Carlos Salazar-Ruiz, Francisco Lopez-Tiro, Ivan Reyes-Amezcua, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul|<http://arxiv.org/pdf/2505.17921v1>|提出了一种基于少样本学习的深度学习方法，有效识别肾结石类型，即使训练数据有限。|
|🆕 发布|Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention|基于位置增强和多头交叉注意力的跨视角物体级地理定位|Zheyang Huang, Jagannath Aryal, Saeid Nahavandi, Xuequan Lu, Chee Peng Lim, Lei Wei, Hailing Zhou|<http://arxiv.org/pdf/2505.17911v1>|[代码](https://github.com/ZheyangH/OCGNet); 提出OCGNet，通过位置增强和多头交叉注意力实现物体级跨视图地理定位，提升定位精度。|
|🆕 发布|ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification|ICPL-ReID：多光谱目标重识别的基于身份条件的提示学习|Shihao Li, Chenglong Li, Aihua Zheng, Jin Tang, Bin Luo|<http://arxiv.org/pdf/2505.17821v1>|提出ICPL框架，通过文本提示学习统一多光谱特征，提升多光谱目标重识别性能。|
|🆕 发布|An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma|注意力增强的深度学习系统，结合Grad-CAM可视化技术用于青光眼的早期筛查|Ramanathan Swaminathan|<http://arxiv.org/pdf/2505.17808v1>|提出结合卷积神经网络、Vision Transformer和交叉注意力模块的混合深度学习模型，用于早...|
|📝 更新|Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities|超图Tversky感知域增量学习在缺失模态下进行脑肿瘤分割|Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong|<http://arxiv.org/pdf/2505.16809v2>|提出了一种针对脑肿瘤分割的域增量学习方法，有效应对缺失模态问题并提升分割性能。|
|🆕 发布|Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling|Plan-R1：作为语言模型的 安全且可行的轨迹规划|Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen|<http://arxiv.org/pdf/2505.17659v1>|提出Plan-R1，将轨迹规划作为语言建模，显著提升自动驾驶安全性。|
|🆕 发布|Instruct2See: Learning to Remove Any Obstructions Across Distributions|Instruct2See：学习跨分布去除任何障碍|Junhang Li, Yu Guo, Chuhua Xian, Shengfeng He|<http://arxiv.org/pdf/2505.17649v1>|[代码](https://jhscut.github.io/Instruct2See.); Instruct2See提出零样本框架，通过多模态提示处理，实现跨分布的遮挡去除。|
|🆕 发布|Wildfire spread forecasting with Deep Learning|基于深度学习的森林火灾蔓延预测|Nikolaos Anastasiou, Spyros Kondylatos, Ioannis Papoutsis|<http://arxiv.org/pdf/2505.17556v1>|提出了一种基于深度学习的野火蔓延预测框架，显著提升了预测准确性。|
|🆕 发布|SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction|SUFFICIENT：一种针对扫描特定的高分辨率3D各向同性胎儿脑MRI重建的无监督深度学习框架|Jiangjie Wu, Lixuan Chen, Zhenghao Li, Xin Li, Saban Ozturk, Lihui Wang, Rongpin Wang, Hongjiang Wei .etc.|<http://arxiv.org/pdf/2505.17472v1>|提出了一种无监督深度学习框架，有效重建高分辨率三维胎儿脑部MRI图像。|
|🆕 发布|Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning|诱雷达：一种基于深度学习的多模型点击诱饵检测算法|Bhanuka Gamage, Adnan Labib, Aisha Joomun, Chern Hong Lim, KokSheik Wong|<http://arxiv.org/pdf/2505.17448v1>|提出BaitRadar算法，利用多模型深度学习技术有效检测YouTube视频标题党内容。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|One RL to See Them All: Visual Triple Unified Reinforcement Learning|一视同仁：视觉三元统一强化学习|Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai .etc.|<http://arxiv.org/pdf/2505.18129v1>|提出V-Triune系统，通过统一视觉推理和感知任务，显著提升视觉语言模型在多任务上的性能。|
|🆕 发布|Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion|指令化：揭秘元数据至视觉指令微调数据转换|Jacob Hansen, Wei Lin, Junmo Kang, Muhammad Jehanzeb Mirza, Hongyin Luo, Rogerio Feris, Alan Ritter, James Glass .etc.|<http://arxiv.org/pdf/2505.18115v1>|[代码](https://github.com/jacob-hansen/Instructify.); 提出了一种将图像元数据转换为视觉指令的开放方法，有效提升了数据质量和可扩展性。|
|🆕 发布|Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking|自适应SAM 2进行视觉目标跟踪：MMVPR挑战赛多模态跟踪第一名的解决方案|Cheng-Yen Yang, Hsiang-Wei Huang, Pyong-Kun Kim, Chien-Kai Kuo, Jui-Wei Chang, Kwang-Ju Kim, Chung-I Huang, Jenq-Neng Hwang|<http://arxiv.org/pdf/2505.18111v1>|将SAM2应用于视觉目标跟踪，通过优化提升性能，在MMVPR挑战赛中获得第一。|
|🆕 发布|CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays|CXReasonBench：胸部X光片结构化诊断推理评估基准|Hyungyung Lee, Geon Choi, Jung-Oh Lee, Hangyul Yoon, Hyuk Gi Hong, Edward Choi|<http://arxiv.org/pdf/2505.18087v1>|[代码](https://github.com/ttumyche/CXReasonBench); 提出CXReasonBench，评估胸部X光片中的结构化诊断推理，揭示大型视觉语言模型在临床推理上的...|
|🆕 发布|LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision|寻找何方？通过自监督学习高效视觉识别|Anthony Fuller, Yousef Yassin, Junfeng Wen, Daniel G. Kyrollos, Tarek Ibrahim, James R. Green, Evan Shelhamer|<http://arxiv.org/pdf/2505.18051v1>|LookWhere通过自适应计算和自监督学习，高效选择和提取图像特征，实现高分辨率图像的稀疏识别和标...|
|📝 更新|WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs|野直播：无人机上的近实时视觉野生动物追踪|Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine .etc.|<http://arxiv.org/pdf/2504.10165v3>|[代码](https://dat-nguyenvn.github.io/WildLive); 提出了一种在无人机上实现近实时高分辨率野生动物跟踪的方法，显著提升无人机处理速度。|
|📝 更新|Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering|超越目的地：探索感知具身问答的新基准|Kaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin|<http://arxiv.org/pdf/2503.11117v3>|构建了首个评估探索和推理能力的EQA基准，并提出混合探索模型和新的评估指标以提升EQA效率。|
|🆕 发布|Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding|你注意到了我提出的问题吗？通过注意力引导的集成解码缓解多模态幻觉|Yeongjae Cho, Keonwoo Kim, Taebaek Hwang, Sungzoon Cho|<http://arxiv.org/pdf/2505.17529v1>|提出了一种通过注意力引导的集成解码策略，有效缓解了多模态幻觉问题。|
|🆕 发布|VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models|VEAttack：针对大型视觉语言模型的下游无关视觉编码器攻击|Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu|<http://arxiv.org/pdf/2505.17440v1>|[代码](https://github.com/hfmei/VEAttack-LVLM); VEAttack通过攻击视觉编码器，有效降低大型视觉语言模型的鲁棒性，无需依赖特定任务信息。|
|📝 更新|Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning|防御多模态后门模型：通过排斥性视觉提示调整|Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng|<http://arxiv.org/pdf/2412.20392v3>|提出RVPT方法，通过特征排斥损失增强CLIP模型对输入扰动的视觉特征抵抗性，有效防御多模态后门攻击...|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|VideoGameBench: Can Vision-Language Models complete popular video games?|视频游戏基准：视觉-语言模型能否完成流行视频游戏？|Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press|<http://arxiv.org/pdf/2505.18134v1>|提出VideoGameBench基准，挑战VLM在真实游戏环境中完成任务，揭示其感知和导航能力局限。|


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency|基于小波变换的立体匹配框架解决频率收敛不一致问题|Xiaobao Wei, Jiawei Liu, Dongbo Yang, Junda Cheng, Changyong Shu, Wei Wang|<http://arxiv.org/pdf/2505.18024v1>|[代码](https://github.com/SIA-IDE/Wavelet-Stereo); 提出了一种基于小波变换的立体匹配框架，有效解决频率收敛不一致问题，提升边缘和纹理细节的匹配精度。|
|🆕 发布|To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models|是否粘合？经典图像匹配与学习图像匹配在移动地图相机与纹理语义3D建筑模型中的应用对比|Simone Gaisbauer, Prabin Gyawali, Qilin Zhang, Olaf Wysocki, Boris Jutzi|<http://arxiv.org/pdf/2505.17973v1>|[代码](https://github.com/simBauer/To); 比较了经典和深度学习图像匹配方法在语义3D建筑模型匹配中的性能，发现深度学习方法更优。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression|MMXU：一种用于疾病进展的多模态和多X射线理解数据集|Linjie Mu, Zhongzhen Huang, Shengqian Qin, Yakun Zhu, Shaoting Zhang, Xiaofan Zhang|<http://arxiv.org/pdf/2502.11651v2>|[代码](https://github.com/linjiemu/MMXU.); 构建了MMXU数据集，通过整合历史记录，显著提升了视觉语言模型在医学诊断中的准确性。|
|📝 更新|A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers|眼科图像分析的无技术障碍的医生友好平台|Meng Wang, Tian Lin, Qingshan Hou, Aidi Lin, Jingcheng Wang, Qingsheng Peng, Truong X. Nguyen, Danqi Fang .etc.|<http://arxiv.org/pdf/2504.15928v2>|开发了一款无需技术背景即可使用的眼科图像分析平台，实现跨不同临床场景的疾病诊断。|
|📝 更新|On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?|关于医学视觉-语言模型的鲁棒性：它们真的具有泛化能力吗？|Raza Imam, Rufael Marew, Mohammad Yaqub|<http://arxiv.org/pdf/2505.15425v2>|该论文提出了一种针对医疗视觉语言模型的鲁棒性评估框架，并通过低秩自适应和少样本调整增强了模型对噪声的...|
|🆕 发布|VLM Models and Automated Grading of Atopic Dermatitis|VLM模型与特应性皮炎的自动评分|Marc Lalonde, Hamed Ghodrati|<http://arxiv.org/pdf/2505.17835v1>|该论文提出利用七种视觉语言模型自动评估特应性皮炎严重程度，为医学图像分析提供新途径。|
|📝 更新|ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis|ViCTr：病理感知图像合成的关键一致性迁移|Onkar Susladkar, Gayatri Deshmukh, Yalcin Tur, Gorkhem Durak, Ulas Bagci|<http://arxiv.org/pdf/2505.04963v2>|ViCTr通过结合纠正流轨迹和Tweedie校正的扩散过程，实现了高保真、病理感知的医学图像合成。|
|📝 更新|Autoregressive Sequence Modeling for 3D Medical Image Representation|自回归序列建模用于3D医学图像表示|Siwen Wang, Churan Wang, Fei Gao, Lixian Su, Fandong Zhang, Yizhou Wang, Yizhou Yu|<http://arxiv.org/pdf/2409.08691v2>|提出了一种基于自回归序列建模的3D医学图像表示方法，有效提取了图像的上下文信息。|
|🆕 发布|Semi-Supervised Medical Image Segmentation via Dual Networks|半监督医学图像分割通过双网络|Yunyao Lu, Yihang Wu, Reem Kateb, Ahmad Chaddad|<http://arxiv.org/pdf/2505.17690v1>|[代码](https://github.com/AIPMLab/Semi-supervised-Segmentation.); 提出了一种基于双网络架构的半监督医学图像分割方法，有效解决了数据依赖和伪标签问题。|
|🆕 发布|Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport|基于知识信息的动态最优传输实现前瞻性医学图像重建|Taoran Zheng, Xing Li, Yan Yang, Xiang Gu, Zongben Xu, Jian Sun|<http://arxiv.org/pdf/2505.17644v1>|提出了一种基于知识引导的动态最优传输方法，有效解决了医学图像重建中模拟与实际数据之间的性能差距问题。|
|🆕 发布|FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation|频域感知U-Net：用于不平衡医学图像分割的频域U-Net|Ruiqi Xing|<http://arxiv.org/pdf/2505.17544v1>|FreqU-FNet通过在频域内操作，有效利用频谱特征和自适应解码策略，提升了医学图像分割中少数类别...|
|🆕 发布|Research on Defect Detection Method of Motor Control Board Based on Image Processing|基于图像处理的电机控制板缺陷检测方法研究|Jingde Huang, Zhangyu Huang, Chenyu Li, Jiantong Liu|<http://arxiv.org/pdf/2505.17493v1>|提出了一种基于图像处理的电机控制板缺陷检测方法，大幅提升检测准确率和效率。|
|🆕 发布|Graph Mamba for Efficient Whole Slide Image Understanding|图蟒：高效的全切片图像理解|Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, Xiaosong Wang|<http://arxiv.org/pdf/2505.17457v1>|提出WSI-GMamba框架，结合GNN和Mamba优势，实现高效大规模病理图像分析。|
|🆕 发布|Dual Ascent Diffusion for Inverse Problems|双上升扩散逆问题|Minseo Kim, Axel Levy, Gordon Wetzstein|<http://arxiv.org/pdf/2505.17353v1>|提出了一种基于扩散模型的优化框架，有效解决逆问题并提升图像质量。|
|🆕 发布|EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion|EVM-Fusion：一种具有神经算法融合的可解释视觉Mamba架构|Zichuan Yang|<http://arxiv.org/pdf/2505.17367v1>|提出EVM-Fusion架构，通过神经算法融合和多路径设计，实现可解释的多器官医学图像分类。|


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method|从众包街景图像中构建楼层编号估计：慕尼黑数据集和基线方法|Yao Sun, Sining Chen, Yifan Tian, Xiao Xiang Zhu|<http://arxiv.org/pdf/2505.18021v1>|[代码](https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.); 提出了一种从街景图像直接估算建筑层数的深度学习方法，并发布了慕尼黑建筑层数数据集。|
|📝 更新|Selective Structured State Space for Multispectral-fused Small Target Detection|选择性结构化状态空间用于多光谱融合小目标检测|Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang|<http://arxiv.org/pdf/2505.14043v3>|提出了一种融合多光谱信息并增强局部细节的轻量级目标检测方法，有效提升了小目标检测精度。|
|🆕 发布|Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization|高光谱异常检测融合统一非凸张量环因子正则化|Wenjin Qin, Hailin Wang, Hao Shu, Feng Zhang, Jianjun Wang, Xiangyong Cao, Xi-Le Zhao, Gemine Vivone|<http://arxiv.org/pdf/2505.17881v1>|提出了一种融合统一非凸张量环因子正则化的高光谱异常检测方法，显著提升了检测精度。|
|🆕 发布|RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection|RQR3D：基于BEV的3D目标检测中回归目标的重新参数化|Ozsel Kilinc, Cem Tarhan|<http://arxiv.org/pdf/2505.17732v1>|提出RQR3D方法，通过关键点回归提升BEV-based 3D物体检测精度和鲁棒性。|
|📝 更新|DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation Networks for Remote Sensing Imagery|多样网：针对遥感图像的决策多样化半监督语义分割网络|Wanli Ma, Oktay Karakus, Paul L. Rosin|<http://arxiv.org/pdf/2311.13716v3>|[代码](https://github.com/WANLIMA-CARDIFF/DiverseNet.); 提出DiverseNet，通过多决策头和多样化模型，有效提升遥感图像半监督语义分割性能。|
|🆕 发布|EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy|EMRA-proxy：利用注意力代理增强遥感图像的多类区域语义分割|Yichun Yu, Yuqing Lan, Zhihuan Xing, Xiaoyi Yang, Tingyue Tang, Dan Yu|<http://arxiv.org/pdf/2505.17665v1>|提出RAPNet，通过区域级注意力和全局类别细化，显著提升遥感图像多类区域语义分割精度。|


### 工业视觉检测 (Industrial Visual Inspection)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain|安全MVDrive：真实世界域中的多视角安全关键驾驶视频合成|Jiawei Zhou, Linye Lyu, Zhuotao Tian, Cheng Zhuo, Yu Li|<http://arxiv.org/pdf/2505.17727v1>|[代码](https://zhoujiawei3.github.io/SafeMVDrive); SafeMVDrive首次提出在真实世界领域生成高质量、安全关键的多视角驾驶视频，以评估和增强自动驾...|
|🆕 发布|FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving|未来视界驾驶：基于时空CoT的视觉思考用于自动驾驶|Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei|<http://arxiv.org/pdf/2505.17685v1>|提出了一种基于时空CoT的视觉推理方法，提升自动驾驶的视觉理解能力。|
|📝 更新|EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving|EMT：自动驾驶的视觉多任务基准数据集|Nadya Abdel Madjid, Murad Mebrahtu, Abdulrahman Ahmad, Abdelmoamen Nasser, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji|<http://arxiv.org/pdf/2502.19260v4>|[代码](https://github.com/AV-Lab/emt-dataset.); 构建了适用于自动驾驶的多任务基准数据集EMT，支持跟踪、轨迹预测和意图预测。|
|🆕 发布|Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection|中心感知残差异常合成用于多类工业异常检测|Qiyu Chen, Huiyuan Luo, Haiming Yao, Wei Luo, Zhen Qu, Chengkan Lv, Zhengtao Zhang|<http://arxiv.org/pdf/2505.17551v1>|[代码](https://github.com/cqylunlun/CRAS.); 提出CRAS方法，通过中心感知残差学习和距离引导异常合成，有效解决多类工业异常检测中的类别干扰和样本...|
|📝 更新|ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents|ChatStitch：通过环绕视图无监督深度图像拼接的结构化可视化与协作LLM代理|Hao Liang, Zhipeng Dong, Kaixin Chen, Jiyuan Guo, Yufeng Yue, Yi Yang, Mengyin Fu|<http://arxiv.org/pdf/2503.14948v2>|ChatStitch通过协同LLM代理实现环绕视角无监督深度图像拼接，有效解决交互效率和重叠区域扭曲...|
|📝 更新|MedCFVQA: A Causal Approach to Mitigate Modality Preference Bias in Medical Visual Question Answering|医视问答中的因果方法减轻模态偏好偏差|Shuchang Ye, Usman Naseem, Mingyuan Meng, Dagan Feng, Jinman Kim|<http://arxiv.org/pdf/2505.16209v2>|提出MedCFVQA模型，通过因果图消除医学视觉问答中的模态偏好偏差。|


### 智能交通视觉 (Intelligent Transportation Vision)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras|基于事件相机的相位相关法在户外驾驶环境中的距离估计|Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato|<http://arxiv.org/pdf/2505.17582v1>|利用事件相机和LED条，通过相位相关技术实现户外驾驶环境中的距离估计。|
|🆕 发布|Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds|基于反射预测的知识蒸馏，用于压缩点云中的鲁棒3D目标检测|Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, Junhui Hou|<http://arxiv.org/pdf/2505.17442v1>|提出了一种基于反射预测的知识蒸馏方法，有效提升了压缩点云的3D物体检测准确率。|


### 创意媒体生成 (Creative Media Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Real-time Traffic Accident Anticipation with Feature Reuse|实时交通事故预测与特征复用|Inpyo Song, Jangwon Lee|<http://arxiv.org/pdf/2505.17449v1>|提出RARE框架，通过复用预训练目标检测器的中间特征，实现实时交通事故预测，大幅提升速度并保持高精度...|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 神经-符号视觉 (Neuro-symbolic Vision)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models|更大步态：利用大型视觉模型的层状表示解锁步态识别|Dingqing Ye, Chao Fan, Zhanbo Huang, Chengwen Luo, Jianqiang Li, Shiqi Yu, Xiaoming Liu|<http://arxiv.org/pdf/2505.18132v1>|提出BiggerGait，通过整合大型视觉模型的多层表示，显著提升步态识别性能。|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving|TopoPoint：通过自动驾驶中的端点检测增强拓扑推理|Yanping Fu, Xinyuan Liu, Tianyu Li, Yike Ma, Yucheng Zhang, Feng Dai|<http://arxiv.org/pdf/2505.17771v1>|[代码](https://github.com/Franpin/TopoPoint.); TopoPoint通过端点检测增强拓扑推理，显著提升自动驾驶中路口理解准确度。|
|🆕 发布|Promptable cancer segmentation using minimal expert-curated data|可提示的癌症分割：使用最少专家精选数据|Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed|<http://arxiv.org/pdf/2505.17915v1>|提出了一种仅需少量标注数据即可实现可提示癌症分割的新方法，显著降低训练成本。|

