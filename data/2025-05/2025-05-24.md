## [UPDATED!] **2025-05-24** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes|SD-OVON：动态场景中开放词汇物体导航的语义感知数据集和基准生成管道|Dicong Qiu, Jiadi You, Zeying Gong, Ronghe Qiu, Hui Xiong, Junwei Liang|<http://arxiv.org/pdf/2505.18881v1>|提出SD-OVON，为动态场景中的开放词汇物体导航构建语义感知数据集和基准生成流程。|
|🆕 发布|Inference Compute-Optimal Video Vision Language Models|推理计算最优视频视觉语言模型|Peiqi Wang, ShengYun Peng, Xuewen Zhang, Hanchao Yu, Yibo Yang, Lifu Huang, Fujun Liu, Qifan Wang|<http://arxiv.org/pdf/2505.18855v1>|该论文通过优化计算资源分配，揭示了视频视觉语言模型中语言模型大小、帧数和视觉标记数量的最佳配置。|
|📝 更新|Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation|像素到点：融合二维和三维特征进行面部皮肤分割|Victoria Yue Chen, Daoye Wang, Stephan Garbin, Jan Bednarik, Sebastian Winberg, Timo Bolkart, Thabo Beeler|<http://arxiv.org/pdf/2504.19718v3>|融合2D和3D特征，实现3D人脸扫描皮肤区域准确分割。|
|🆕 发布|Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models|强化微调提升多模态大型语言模型的推理能力|Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang .etc.|<http://arxiv.org/pdf/2505.18536v1>|[代码](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.); 通过强化微调提升多模态大型语言模型的推理能力。|
|🆕 发布|BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks|生物力学GPT：迈向临床相关运动任务的生物力学流畅多模态基础模型|Ruize Yang, Ann Kennedy, R. James Cotton|<http://arxiv.org/pdf/2505.18465v1>|开发了一种基于语言模型的生物力学数据分析工具，有效提升临床运动任务分析能力。|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|SCHEME: Scalable Channel Mixer for Vision Transformers|方案：可扩展的视觉Transformer通道混合器|Deepak Sridhar, Yunsheng Li, Nuno Vasconcelos|<http://arxiv.org/pdf/2312.00412v4>|提出SCHEME，一种高效可扩展的通道混合器，显著提升视觉Transformer模型性能。|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis|OpenOmni：通过渐进式多模态对齐和实时自我感知情感语音合成推进开源全模态大型语言模型|Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen .etc.|<http://arxiv.org/pdf/2501.04561v5>|提出OpenOmni，通过渐进式多模态对齐和实时情感语音合成，突破开源多模态大语言模型的技术瓶颈。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Unsupervised cell segmentation by fast Gaussian Processes|无监督细胞分割：快速高斯过程|Laura Baracaldo, Blythe King, Haoran Yan, Yizi Lin, Nina Miolane, Mengyang Gu|<http://arxiv.org/pdf/2505.18902v1>|开发了一种基于快速高斯过程的无需参数调整的无监督细胞分割算法，有效提高了细胞分割的准确性和可扩展性。|
|🆕 发布|Reasoning Segmentation for Images and Videos: A Survey|图像与视频的推理分割：综述|Yiqing Shen, Chenjia Li, Fei Xiong, Jeong-O Jeong, Tianpeng Wang, Michael Latman, Mathias Unberath|<http://arxiv.org/pdf/2505.18816v1>|首次全面综述了基于推理的图像和视频分割方法，推动自然语言与视觉感知融合。|


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Deep Learning for Breast Cancer Detection: Comparative Analysis of ConvNeXT and EfficientNet|深度学习在乳腺癌检测中的应用：ConvNeXT与EfficientNet的对比分析|Mahmudul Hasan|<http://arxiv.org/pdf/2505.18725v1>|比较ConvNeXT和EfficientNet在乳腺钼靶图像中预测癌症风险，ConvNeXT模型表现...|
|📝 更新|DitHub: A Modular Framework for Incremental Open-Vocabulary Object Detection|DitHub：一个用于增量开放词汇目标检测的模块化框架|Chiara Cappellino, Gianluca Mancusi, Matteo Mosconi, Angelo Porrello, Simone Calderara, Rita Cucchiara|<http://arxiv.org/pdf/2503.09271v2>|[代码](https://aimagelab.github.io/DitHub); 提出DitHub框架，通过模块化方法实现开放词汇目标检测的增量适应，显著提升模型性能。|
|📝 更新|ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera|ASGrasp：通用透明物体重建与6自由度抓取检测的RGB-D主动立体相机|Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang|<http://arxiv.org/pdf/2405.05648v2>|[代码](https://pku-epic.github.io/ASGrasp); 提出ASGrasp，通过RGB-D相机实现透明物体重建和6自由度抓取检测，突破深度相机在透明物体几何...|
|🆕 发布|Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling|缓解域适应中目标检测的上下文偏差：使用掩码池化|Hojun Son, Asma Almutairi, Arpan Kusari|<http://arxiv.org/pdf/2505.18446v1>|通过引入Mask Pooling，该论文有效缓解了域适应目标检测中的上下文偏差问题。|
|📝 更新|IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme|IGL-DT：在有限标注方案下，基于双重教师语义分割框架的迭代全局-局部特征学习方法|Dinh Dai Quan Tran, Hoang-Thien Nguyen, Thanh-Huy Nguyen, Gia-Van To, Tien-Huy Nguyen, Quan Nguyen|<http://arxiv.org/pdf/2504.09797v2>|IGL-DT通过结合双教师策略和迭代全局-局部特征学习，有效提升了半监督语义分割的准确性。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SerendibCoins: Exploring The Sri Lankan Coins Dataset|斯里兰卡硬币数据集探索：SerendibCoins|NH Wanigasingha, ES Sithpahan, MKA Ariyaratne, PRS De Silva|<http://arxiv.org/pdf/2505.18634v1>|构建了斯里兰卡硬币数据集，并验证了CNN在硬币分类中的优越性。|
|🆕 发布|Guiding the Experts: Semantic Priors for Efficient and Focused MoE Routing|引导专家：高效且专注的MoE路由的语义先验|Chengxi Min, Wei Wang, Yahui Liu, Weixin Ye, Enver Sangineto, Qi Wang, Yao Zhao|<http://arxiv.org/pdf/2505.18586v1>|提出了一种基于语义先验的MoE路由增强策略，显著提升了专家路由效率和模型性能。|


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra|基于热带几何的边缘检测：使用最小-加和最大-加代数|Shivam Kumar Jha S, Jaya NN Iyer|<http://arxiv.org/pdf/2505.18625v1>|提出了一种基于热带几何的边缘检测方法，利用最小-加和最大-加代数改进边缘清晰度和连续性。|


## 生成式视觉模型 (Generative Visual Modeling)


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity|LinGen：迈向具有线性计算复杂度的高分辨率分钟级文本到视频生成|Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo .etc.|<http://arxiv.org/pdf/2412.09856v2>|[代码](https://lineargen.github.io/.); LinGen通过线性复杂度计算实现高分辨率短时视频生成，显著降低计算成本并提升视频质量。|
|🆕 发布|Localizing Knowledge in Diffusion Transformers|在扩散Transformer中定位知识|Arman Zarei, Samyadeep Basu, Keivan Rezaei, Zihao Lin, Sayan Nag, Soheil Feizi|<http://arxiv.org/pdf/2505.18832v1>|提出了一种定位扩散Transformer中知识的方法，以提升模型的可解释性和可控性。|
|🆕 发布|StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations|风格守护：通过风格扰动预防基于文本到图像模型的风貌模仿攻击|Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao|<http://arxiv.org/pdf/2505.18766v1>|StyleGuard通过风格扰动和升级损失，有效防御了基于文本到图像模型的风格模仿攻击。|


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation|稀疏VideoGen2：通过语义感知排列的稀疏注意力加速视频生成|Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li .etc.|<http://arxiv.org/pdf/2505.18875v1>|SVG2通过语义感知排列和稀疏注意力，有效提升了视频生成速度和质量。|
|🆕 发布|Eye-See-You: Reverse Pass-Through VR and Head Avatars|眼见为实：反向传递VR与头部虚拟形象|Ankan Dash, Jingyi Gu, Guiling Wang, Chen Chen|<http://arxiv.org/pdf/2505.18869v1>|提出RevAvatar框架，利用AI技术实现VR反向透视，解决VR头显遮挡问题，提升虚拟环境交互体验...|
|🆕 发布|REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing|REGen：长视频到短视频的多模态检索嵌入生成|Weihan Xu, Yimeng Ma, Jingyue Huang, Yang Li, Wenye Ma, Taylor Berg-Kirkpatrick, Julian McAuley, Paul Pu Liang .etc.|<http://arxiv.org/pdf/2505.18880v1>|提出REGen模型，通过检索嵌入生成技术，实现长视频到短视频的连贯编辑，有效插入视频片段。|
|🆕 发布|OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks|全能力多模态生成基准：50+任务评测|Jiayu Wang, Yang Jiao, Yue Yu, Tianwen Qian, Shaoxiang Chen, Jingjing Chen, Yu-Gang Jiang|<http://arxiv.org/pdf/2505.18775v1>|[代码](https://github.com/emilia113/OmniGenBench.); OmniGenBench构建了一个全面基准，评估多模态模型在图像生成中的指令遵循能力。|
|🆕 发布|VORTA: Efficient Video Diffusion via Routing Sparse Attention|VORTA：通过路由稀疏注意力实现高效视频扩散|Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Shunyu Liu, Dacheng Tao|<http://arxiv.org/pdf/2505.18809v1>|VORTA通过稀疏注意力和路由策略，有效加速视频扩散模型，实现高效视频生成。|
|🆕 发布|Dual-Path Stable Soft Prompt Generation for Domain Generalization|双路径稳定软提示生成以实现领域泛化|Yuedi Zhang, Shuanghao Bai, Wanqi Zhou, Zhirong Luan, Badong Chen|<http://arxiv.org/pdf/2505.18770v1>|提出DPSPG方法，通过引入负学习解决领域泛化中提示生成的不稳定性和泛化能力不足问题。|
|🆕 发布|C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging|C3R：用于显微镜成像统一评估的通道条件细胞表示|Umar Marikkar, Syed Sameed Husain, Muhammad Awais, Sara Atito|<http://arxiv.org/pdf/2505.18745v1>|提出C3R框架，通过通道条件化细胞表示实现显微镜图像的统一评估和跨数据集泛化。|
|🆕 发布|Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation|超越提示的校准：评估文本到图像生成中的世界知识对齐|Wenchao Zhang, Jiahe Tian, Runze He, Jizhong Han, Jiao Dai, Miaomiao Feng, Wei Mi, Xiaodan Zhang|<http://arxiv.org/pdf/2505.18730v1>|[代码](https://github.com/smile365317/ABP.); 提出ABP基准和ABPScore指标，评估T2I模型在生成图像与超提示世界知识对齐方面的能力。|
|📝 更新|Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization|探索跨任务泛化中视觉-语言-动作操作的极限|Jiaming Zhou, Ke Ye, Jiayi Liu, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao .etc.|<http://arxiv.org/pdf/2505.15660v2>|设计AGNOSTOS基准和X-ICM方法，提升VLA模型在未知任务上的跨任务泛化能力。|
|🆕 发布|Affective Image Editing: Shaping Emotional Factors via Text Descriptions|情感图像编辑：通过文本描述塑造情感因素|Peixuan Zhang, Shuchen Weng, Chengxuan Zhu, Binghao Tang, Zijian Jia, Si Li, Boxin Shi|<http://arxiv.org/pdf/2505.18699v1>|提出AIEdiT，通过文本描述塑造图像情感，实现用户情感需求的图像编辑。|
|🆕 发布|Manifold-aware Representation Learning for Degradation-agnostic Image Restoration|基于流形的退化无关图像恢复表示学习|Bin Ren, Yawei Li, Xu Zheng, Yuqian Fu, Danda Pani Paudel, Ming-Hsuan Yang, Luc Van Gool, Nicu Sebe|<http://arxiv.org/pdf/2505.18679v1>|[代码](https://amazingren.github.io/MIRAGE); MIRAGE通过分解特征空间和跨层对比学习，实现了对多种退化图像的鲁棒修复。|
|🆕 发布|ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation|图表银河：信息图表理解和生成数据集|Zhen Li, Yukai Guo, Duan Li, Xinyuan Guo, Bowen Li, Lanxi Xiao, Shenyu Qiao, Jiashu Chen .etc.|<http://arxiv.org/pdf/2505.18668v1>|构建ChartGalaxy数据集，提升大型视觉语言模型对信息图表的理解和生成能力。|
|🆕 发布|Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter|模适配器：无需调优且通用的多概念个性化通过调制适配器|Weizhi Zhong, Huan Yang, Zheng Liu, Huiguo He, Zijian He, Xuesong Niu, Di Zhang, Guanbin Li|<http://arxiv.org/pdf/2505.18612v1>|提出了一种无需微调的多概念个性化生成方法，通过调制适配器实现抽象和物体概念的定制。|
|🆕 发布|Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility|改进不相容扩散：通过降低其相容性来加速扩散训练|Yiheng Li, Feng Liang, Dan Kondratyuk, Masayoshi Tomizuka, Kurt Keutzer, Chenfeng Xu|<http://arxiv.org/pdf/2505.18521v1>|[代码](https://github.com/yhli123/Immiscible-Diffusion.); 通过降低扩散模型中噪声空间的混合度，该论文提出了一种加速扩散训练的新方法，显著提升了训练效率。|
|📝 更新|LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes|LiDAR-EDIT：通过编辑真实场景中的物体布局生成激光雷达数据|Shing-Hei Ho, Bao Thach, Minghan Zhu|<http://arxiv.org/pdf/2412.00592v3>|LiDAR-EDIT通过编辑真实场景中的物体布局生成合成激光雷达数据，实现场景可控且保持背景真实。|
|📝 更新|OneDiff: A Generalist Model for Image Difference Captioning|OneDiff：一种用于图像差异描述的通用模型|Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue, Jing Liu|<http://arxiv.org/pdf/2407.05645v4>|OneDiff模型通过结合视觉Delta模块和Siamese图像编码器，实现了对图像差异的精确描述，...|
|📝 更新|EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation|EvAnimate：基于事件条件的图像到视频生成的人体动画|Qiang Qu, Ming Li, Xiaoming Chen, Tongliang Liu|<http://arxiv.org/pdf/2503.18552v2>|提出EvAnimate，利用事件相机数据生成高质量、时间一致的人体动画，解决传统方法在复杂条件下的不...|
|🆕 发布|Syn3DTxt: Embedding 3D Cues for Scene Text Generation|Syn3DTxt：为场景文本生成嵌入3D线索|Li-Syun Hsiung, Jun-Kai Tu, Kuan-Wu Chu, Yu-Hsuan Chiu, Yan-Tsung Peng, Sheng-Luen Chung, Gee-Sern Jison Hsu|<http://arxiv.org/pdf/2505.18479v1>|提出了一种结合表面法线构建合成数据集的新标准，以增强场景文本生成中的三维空间关系表示。|
|🆕 发布|TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP|TNG-CLIP：为CLIP否定意识训练时否定数据生成|Yuliang Cai, Jesse Thomason, Mohammad Rostami|<http://arxiv.org/pdf/2505.18434v1>|TNG-CLIP通过训练时生成否定数据，提升CLIP对否定语义的理解能力。|


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Robust multi-coil MRI reconstruction via self-supervised denoising|鲁棒的多线圈MRI重建通过自监督去噪|Asad Aali, Marius Arvinte, Sidharth Kumar, Yamin I. Arefeen, Jonathan I. Tamir|<http://arxiv.org/pdf/2411.12919v4>|通过自监督去噪预处理，提升了基于深度学习的多线圈MRI重建效果。|
|🆕 发布|How to build a consistency model: Learning flow maps via self-distillation|如何构建一致性模型：通过自蒸馏学习流图|Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden|<http://arxiv.org/pdf/2505.18825v1>|通过自蒸馏将现有蒸馏方案转化为直接训练算法，提出了一种学习流图的新方法，提高了生成模型的效率。|
|🆕 发布|SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models|SAMA：迈向多轮参考式基于大语言模型的视频聊天|Ye Sun, Hao Zhang, Henghui Ding, Tiehua Zhang, Xingjun Ma, Yu-Gang Jiang|<http://arxiv.org/pdf/2505.18812v1>|提出SAMA模型，通过联合学习视频理解和定位，实现多轮视频对话。|
|📝 更新|AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration|标题翻译：AsymRnR：基于非对称缩减和恢复的视频扩散Transformer加速|Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, Zhao Jin, Dacheng Tao|<http://arxiv.org/pdf/2412.11706v3>|提出AsymRnR方法，通过不对称降维和恢复加速视频扩散Transformer，显著提升处理速度。|
|🆕 发布|Rethinking Direct Preference Optimization in Diffusion Models|重新思考扩散模型中的直接偏好优化|Junyong Kang, Seohyun Lim, Kyungjune Baek, Hyunjung Shim|<http://arxiv.org/pdf/2505.18736v1>|提出了一种新策略，通过稳定参考模型更新和时步感知训练，提升扩散模型中直接偏好优化的性能。|
|🆕 发布|Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model|使用内部细节增强扩散模型恢复真实世界图像|Peng Xiao, Hongbo Zhao, Yijun Wang, Jianxin Lin|<http://arxiv.org/pdf/2505.18674v1>|提出了一种内部细节增强扩散模型，有效恢复现实世界退化图像细节。|
|📝 更新|OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates|OSCAR：多比特率一步扩散编解码器|Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo .etc.|<http://arxiv.org/pdf/2505.16091v2>|[代码](https://github.com/jp-guo/OSCAR.); OSCAR提出了一种一步扩散编解码器，实现多比特率图像压缩，显著提升效率。|
|🆕 发布|Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps|《能否指引我回家？基于公交地图的细粒度视觉推理基准研究》|Sicheng Feng, Song Wang, Shuyi Ouyang, Lingdong Kong, Zikai Song, Jianke Zhu, Huan Wang, Xinchao Wang|<http://arxiv.org/pdf/2505.18675v1>|构建ReasonMap基准，评估MLLMs在细粒度视觉推理上的能力，揭示开源与闭源模型性能差异。|
|🆕 发布|Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy|基于八叉树GAN的3D微CT图像内存高效超分辨率：提升分辨率和分割精度|Evgeny Ugolkov, Xupeng He, Hyung Kwak, Hussein Hoteit|<http://arxiv.org/pdf/2505.18664v1>|提出了一种基于八叉树GAN的内存高效3D微CT图像超分辨率算法，显著提升分辨率和分割精度。|
|🆕 发布|So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection|So-Fake：社交媒体图像伪造检测的基准与解释|Zhenglin Huang, Tianxiao Li, Xiangtai Li, Haiquan Wen, Yiwei He, Jiangning Zhang, Hao Fei, Xi Yang .etc.|<http://arxiv.org/pdf/2505.18660v1>|构建了大规模社交媒体图像伪造检测基准和解释性检测框架，显著提升了伪造检测准确性和可解释性。|
|🆕 发布|DVD-Quant: Data-free Video Diffusion Transformers Quantization|DVD-Quant：无数据视频扩散变换器量化|Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang|<http://arxiv.org/pdf/2505.18663v1>|[代码](https://github.com/lhxcs/DVD-Quant.); 提出DVD-Quant，一种无需数据校准的视频扩散Transformer量化框架，显著提升视频生成速...|
|🆕 发布|EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models|EvdCLIP：利用大型语言模型中的实体视觉描述改进视觉-语言检索|GuangHao Meng, Sunan He, Jinpeng Wang, Tao Dai, Letian Zhang, Jieming Zhu, Qing Li, Gang Wang .etc.|<http://arxiv.org/pdf/2505.18594v1>|EvdCLIP通过利用实体视觉描述增强查询，有效提升了视觉语言检索的准确性。|
|🆕 发布|Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment|链式缩放：通过尺度自回归和偏好对齐实现极端超分辨率|Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye|<http://arxiv.org/pdf/2505.18600v1>|提出Chain-of-Zoom框架，通过多尺度提示和自回归链实现超分辨率图像的极端放大。|
|🆕 发布|Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations|释放扩散变换器通过调节大量激活进行视觉对应|Chaofan Gan, Yuanpeng Tu, Xi Chen, Tieyuan Chen, Yuxi Li, Mehrtash Harandi, Weiyao Lin|<http://arxiv.org/pdf/2505.18584v1>|提出了一种通过调节扩散模型中的大规模激活来提升视觉对应精度的方法。|
|🆕 发布|On Denoising Walking Videos for Gait Recognition|关于步行视频去噪以实现步态识别|Dongyang Jin, Chao Fan, Jingzhe Ma, Jingkai Zhou, Weihua Chen, Shiqi Yu|<http://arxiv.org/pdf/2505.18582v1>|[代码](https://github.com/ShiqiYu/OpenGait.); 提出DenoisingGait，通过去噪和特征匹配提升步行视频姿态识别准确度。|
|🆕 发布|ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts|ThinkVideo：基于思维链的高质量推理视频分割|Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang|<http://arxiv.org/pdf/2505.18561v1>|ThinkVideo通过利用MLLM的零样本思维链能力，实现了高质量的视频对象分割。|
|🆕 发布|Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models|扩散融合：扩散模型推理时的多偏好对齐|Min Cheng, Fatemeh Doudi, Dileep Kalathil, Mohammad Ghavamzadeh, Panganamala R. Kumar|<http://arxiv.org/pdf/2505.18547v1>|[代码](https://github.com/bluewoods127/DB-2025); 提出Diffusion Blend方法，实现推理时多偏好对齐，优化扩散模型。|
|📝 更新|Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel Images|上下文很重要：针对吉像素图像的查询感知动态长序列建模|Zhengrui Guo, Qichen Sun, Jiabo Ma, Lishuang Feng, Jinzhuo Wang, Hao Chen|<http://arxiv.org/pdf/2501.18984v2>|[代码](https://github.com/dddavid4real/Querent.); 提出了一种高效动态建模框架，通过查询感知优化长序列图像分析，显著提升计算效率和预测性能。|
|🆕 发布|HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model|诚实脸：迈向一步扩散模型下的诚实人脸修复|Jingkai Wang, Wu Miao, Jue Gong, Zheng Chen, Xing Liu, Hong Gu, Yutong Liu, Yulun Zhang|<http://arxiv.org/pdf/2505.18469v1>|[代码](https://github.com/jkwang28/HonestFace); HonestFace通过一步扩散模型，实现高保真、真实感的面部图像修复。|
|📝 更新|Role Bias in Text-to-Image Diffusion Models: Diagnosing and Mitigating Compositional Failures through Intermediate Decomposition|文本到图像扩散模型中的角色偏差：通过中间分解诊断和缓解组合失败|Sina Malakouti, Adriana Kovashka|<http://arxiv.org/pdf/2503.10037v2>|通过中间分解诊断和缓解文本到图像扩散模型中的角色偏差，提出了一种减轻角色崩溃的轻量级框架。|
|🆕 发布|OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data|全一致性：从配对风格化数据中学习风格无关的一致性|Yiren Song, Cheng Liu, Mike Zheng Shou|<http://arxiv.org/pdf/2505.18445v1>|OmniConsistency通过大规模Diffusion Transformers，提出了一种跨风...|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding|基于CLIP的3D理解的自监督和泛化性分词|Guofeng Mei, Bin Ren, Juan Liu, Luigi Riz, Xiaoshui Huang, Xu Zheng, Yongshun Gong, Ming-Hsuan Yang .etc.|<http://arxiv.org/pdf/2505.18819v1>|提出S4Token，通过结合超点分组和坐标缩放归一化，实现跨域通用的3D场景理解。|
|🆕 发布|Generative RLHF-V: Learning Principles from Multi-modal Human Preference|生成式RLHF-V：从多模态人类偏好中学习原则|Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo .etc.|<http://arxiv.org/pdf/2505.18531v1>|Generative RLHF-V通过结合生成式奖励模型与多模态RLHF，有效提升了多模态大语言模型...|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 视觉定位与映射 (Visual Localization & Mapping)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos|超越领域随机化：基于事件的视觉鲁棒对抗模仿感知|Andrea Ramazzina, Vittorio Giammarino, Matteo El-Hariry, Mario Bijelic|<http://arxiv.org/pdf/2505.18899v1>|提出了一种基于事件感知的视觉鲁棒模仿方法，通过转换视频为事件表示，实现对抗性模仿的视觉不变性。|


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting|超级GS：通过高斯分层实现一致且详细的3D超分辨率场景重建|Shiyun Xie, Zhiru Wang, Yinghao Zhu, Xu Wang, Chengwei Pan, Xiwang Dong|<http://arxiv.org/pdf/2505.18649v1>|SuperGS通过两阶段训练框架和不确定性建模，实现了高分辨率场景重建，解决了低分辨率输入导致的细节...|
|🆕 发布|Spiking Transformers Need High Frequency Information|脉冲神经网络需要高频信息|Yuetong Fang, Deming Zhou, Ziqing Wang, Hongwei Ren, ZeCui Zeng, Lusong Li, Shibo Zhou, Renjing Xu|<http://arxiv.org/pdf/2505.18608v1>|揭示脉冲神经网络偏好传播低频信息，提出Max-Former通过增强高频信号提升性能。|


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|PromptHMR: Promptable Human Mesh Recovery|PromptHMR：可提示的人体网格恢复|Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael J. Black, Muhammed Kocabas|<http://arxiv.org/pdf/2504.06397v2>|PromptHMR通过空间和语义提示，实现了在复杂场景中准确估计人体姿态和形状。|


## 时序视觉分析 (Temporal Visual Analysis)


### 视频目标跟踪 (Video Object Tracking)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment|融合跟踪：任意多视图环境中的端到端多目标跟踪|Xiaohe Li, Pengfei Li, Zide Fan, Ying Geng, Fangli Mou, Haohua Wu, Yunping Ge|<http://arxiv.org/pdf/2505.18727v1>|提出FusionTrack框架，实现任意多视角环境下的多目标跟踪，显著提升跟踪精度。|


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement|PMQ-VE：视频增强的渐进式多帧量化|ZhanFeng Feng, Long Peng, Xin Di, Yong Guo, Wenbo Li, Yulun Zhang, Renjing Pei, Yang Wang .etc.|<http://arxiv.org/pdf/2505.12266v2>|[代码](https://github.com/xiaoBIGfeng/PMQ-VE.); 提出PMQ-VE，通过渐进式多帧量化提升视频增强性能，解决现有方法计算和内存需求高的问题。|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations|海洋观测中的地理感知浮游动物识别与分类数据集：ZooplanktonBench|Fukun Liu, Adam T. Greer, Gengchen Mai, Jin Sun|<http://arxiv.org/pdf/2505.18477v1>|构建了包含地理信息的浮游动物数据集，以解决复杂环境下计算机视觉识别和分类的挑战。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning|真实：基于表示增强的示例无关类增量学习分析学习|Run He, Di Fang, Yizhu Chen, Kai Tong, Cen Chen, Yi Wang, Lap-pui Chau, Huiping Zhuang|<http://arxiv.org/pdf/2403.13522v2>|提出了一种增强表征的解析学习方法，有效缓解了无样本类增量学习中的遗忘问题。|


### 跨模态一致性学习 (Cross-modal Consistency Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Multi-Level Embedding and Alignment Network with Consistency and Invariance Learning for Cross-View Geo-Localization|多级嵌入与对齐网络：基于一致性和不变性学习的跨视图地理定位|Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong|<http://arxiv.org/pdf/2412.14819v4>|[代码](https://github.com/ISChenawei/MEAN.); 提出了一种轻量级网络，通过多级嵌入和一致性学习，有效解决跨视图地理定位中的特征关联和计算问题。|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations|那是在谈论什么？科学演示的视频到文本摘要数据集|Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata .etc.|<http://arxiv.org/pdf/2502.08279v4>|构建了针对科学演讲的视频到文本摘要数据集VISTA，并应用基于计划的框架提升摘要质量。|
|📝 更新|GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model|GAPrompt：面向3D视觉模型的几何感知点云提示|Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou|<http://arxiv.org/pdf/2505.04119v2>|[代码](https://github.com/zhoujiahuan1991/ICML2025-VGP.); 提出GAPrompt，通过几何线索增强3D视觉模型适应性，实现高效参数高效微调。|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Rethinking Causal Mask Attention for Vision-Language Inference|重新思考视觉-语言推理中的因果掩码注意力|Xiaohuan Pei, Tao Huang, YanXiang Ma, Chang Xu|<http://arxiv.org/pdf/2505.18605v1>|提出未来感知注意力机制，解决因果掩码在视觉语言推理中的不足，提升模型推理能力。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 不确定性量化 (Uncertainty Quantification)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models|大语言模型中同质性偏差不能由标记采样不确定性解释|Messi H. J. Lee, Soyeon Jeon|<http://arxiv.org/pdf/2501.19337v2>|研究发现，大语言模型中的同质性偏差并非由token采样不确定性引起，建议优先干预表示学习机制和训练语...|


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation|深思熟虑后再进行适应：通过在线测试时适应提高DeepFake检测的适应性|Hong-Hanh Nguyen-Le, Van-Tuan Tran, Dinh-Thuc Nguyen, Nhien-An Le-Khac|<http://arxiv.org/pdf/2505.18787v1>|[代码](https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation); 提出了一种在线测试时自适应方法，显著提升了Deepfake检测器的适应性和鲁棒性。|
|📝 更新|The Double-Ellipsoid Geometry of CLIP|CLIP的双椭球几何形状|Meir Yossef Levi, Guy Gilboa|<http://arxiv.org/pdf/2411.14517v3>|揭示了CLIP嵌入的双椭球几何结构，优化了对比训练中的实例嵌入和匹配。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CageNet: A Meta-Framework for Learning on Wild Meshes|笼网：野生网格学习元框架|Michal Edelstein, Hsueh-Ti Derek Liu, Mirela Ben-Chen|<http://arxiv.org/pdf/2505.18772v1>|提出CageNet元框架，通过笼形几何概念扩展通用框架在复杂网格上的应用。|
|🆕 发布|WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation|弱MCN：弱监督指代表达理解与分割的多任务协作网络|Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo|<http://arxiv.org/pdf/2505.18686v1>|[代码](https://github.com/MRUIL/WeakMCN.); 提出WeakMCN，通过多任务协作网络提升弱监督指代表达式理解和分割的性能。|
|🆕 发布|Learning without Isolation: Pathway Protection for Continual Learning|无隔离学习：持续学习的路径保护|Zhikang Chen, Abudukelimu Wuerkaixi, Sen Cui, Haoxuan Li, Ding Li, Jingfeng Zhang, Bo Han, Gang Niu .etc.|<http://arxiv.org/pdf/2505.18568v1>|提出了一种基于路径保护的持续学习方法，有效防止灾难性遗忘。|
|📝 更新|Paper Copilot Position: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process|论文合作者职位：人工智能和机器学习社区应采用更透明和规范的同行评审流程|Jing Yang|<http://arxiv.org/pdf/2502.00874v2>|倡导建立更透明、开放的同行评审流程，以促进社区参与和推动人工智能与机器学习领域发展。|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 视觉操作与控制 (Visual Manipulation & Control)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models|ManipLVM-R1：基于大型视觉-语言模型的具身操作推理强化学习|Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang .etc.|<http://arxiv.org/pdf/2505.16517v2>|提出ManipLVM-R1，通过强化学习与可验证奖励，提升机器人操作中的物理推理和泛化能力。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding|视觉语言模型为何在视觉算术中挣扎？迈向增强的图表和几何理解|Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu|<http://arxiv.org/pdf/2502.11492v3>|提出CogAlign方法，通过认知发展理论增强视觉语言模型在视觉算术理解上的能力。|
|🆕 发布|LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders|洛伦兹优化鲁棒嵌入：用于视觉编码器的拉格朗日优化嵌入|Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, Seyed-Mohsen Moosavi-Dezfooli|<http://arxiv.org/pdf/2505.18884v1>|提出LORE框架，通过约束优化提升视觉编码器对对抗攻击的鲁棒性，同时保持清洁数据准确性。|
|🆕 发布|Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation|一次不只需看：迈向具有选择性视觉重访的多模态交互推理|Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu|<http://arxiv.org/pdf/2505.18842v1>|提出了一种轻量级模型v1，通过选择性视觉重访增强多模态交互推理能力。|
|🆕 发布|ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models|ToDRE：基于多样性和任务感知的视觉标记剪枝，以实现高效的大规模视觉-语言模型|Duo Li, Zuhao Yang, Shijian Lu|<http://arxiv.org/pdf/2505.18757v1>|设计ToDRE框架，通过多样性和任务相关性剪枝视觉token，提升大视觉语言模型效率。|
|🆕 发布|GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains|GRE Suite：通过微调视觉-语言模型和增强推理链进行地理定位推断|Chun Wang, Xiaoran Pan, Zihao Pan, Haofan Wang, Yiren Song|<http://arxiv.org/pdf/2505.18700v1>|[代码](https://github.com/Thorin215/GRE.); 提出GRE Suite，通过增强推理链提高视觉语言模型在地理定位任务中的准确性和可解释性。|
|🆕 发布|Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU|为何不替换？通过CPU上的手工制作-学习特征协作实现长期视觉定位的维持|Yicheng Lin, Yunlong Jiang, Xujia Jiao, Bin Han|<http://arxiv.org/pdf/2505.18652v1>|[代码](https://github.com/linyicheng1/ORB_SLAM3_localization.); 提出一种结合手工和深度学习特征的视觉定位框架，显著提升长期定位精度和效率。|
|🆕 发布|Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning|Doc-CoB：基于视觉链式框推理的多模态文档理解增强|Ye Mo, Zirui Shao, Kai Ye, Xianwei Mao, Bo Zhang, Hangdi Xing, Peng Ye, Gang Huang .etc.|<http://arxiv.org/pdf/2505.18603v1>|引入视觉链式框推理，提升多模态文档理解准确性。|
|📝 更新|3D Visual Illusion Depth Estimation|三维视觉错觉深度估计|Chengtang Yao, Zhidan Liu, Jiaxi Zeng, Lidong Yu, Yuwei Wu, Yunde Jia|<http://arxiv.org/pdf/2505.13061v3>|提出了一种利用视觉语言模型常识的鲁棒深度估计框架，有效应对3D视觉错觉挑战。|
|🆕 发布|Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning|聚焦于关键信息：通过自动注意力对齐调整增强医学视觉-语言模型|Aofei Chang, Le Huang, Alex James Boyd, Parminder Bhatia, Taha Kass-Hout, Cao Xiao, Fenglong Ma|<http://arxiv.org/pdf/2505.18503v1>|提出A$^3$Tune框架，通过自动注意力对齐调整提升医学视觉语言模型性能。|
|🆕 发布|Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning|在视觉表示中锚定身体意识以实现高效策略学习|Junlin Wang, Zhiyun Lin|<http://arxiv.org/pdf/2505.18487v1>|[代码](https://github.com/HenryWJL/icon); 提出ICon方法，通过对比学习增强视觉Transformer的表示，提升机器人操作策略学习效率。|
|📝 更新|Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans|人机对齐基准：多模态大型语言模型与人类推理能力的细粒度评估|Yansheng Qiu, Li Xiao, Zhaopan Xu, Pengfei Zhou, Zheng Wang, Kaipeng Zhang|<http://arxiv.org/pdf/2505.11141v2>|提出Human-Aligned Bench，评估MLLM在多模态推理能力上与人类表现的细微差异。|
|📝 更新|LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification|LLaVA-ReID：交互式行人重识别的选优多图像提问者|Yiding Lu, Mouxing Yang, Dezhong Peng, Peng Hu, Yijie Lin, Xi Peng|<http://arxiv.org/pdf/2504.10174v3>|分类交互式人物重识别，LLaVA-ReID通过生成针对性问题提升识别准确率。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Little Data, Big Impact: Privacy-Aware Visual Language Models via Minimal Tuning|少量数据，巨大影响：通过最小调整实现隐私感知视觉语言模型|Laurens Samson, Nimrod Barazani, Sennay Ghebreab, Yuki M. Asano|<http://arxiv.org/pdf/2405.17423v3>|通过最小数据调整，该论文提出了一种隐私感知视觉语言模型，显著提升了隐私敏感内容的识别能力。|


### 多模态对话系统 (Multimodal Dialogue Systems)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|VISTANet: VIsual Spoken Textual Additive Net for Interpretable Multimodal Emotion Recognition|VISTANet：用于可解释多模态情感识别的视觉语音文本加性网络|Puneet Kumar, Sarthak Malik, Balasubramanian Raman, Xiaobai Li|<http://arxiv.org/pdf/2208.11450v4>|[代码](https://github.com/MIntelligence-Group/MMEmoRec.); 提出VISTANet，融合视觉、语音和文本信息，实现可解释的多模态情感识别。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|FairREAD: Re-fusing Demographic Attributes after Disentanglement for Fair Medical Image Classification|公平READ：解耦后重新融合人口统计属性以实现公平医学图像分类|Yicheng Gao, Jinkui Hao, Bo Zhou|<http://arxiv.org/pdf/2412.16373v2>|FairREAD通过重新融合解耦后的敏感属性，显著降低医疗图像分类中的不公平性，同时保持诊断准确性。|
|🆕 发布|MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image Segmentation|MSLAU-Net：一种用于医学图像分割的混合CNN-Transformer网络|Libin Lan, Yanxin Li, Xiaojuan Liu, Juan Zhou, Jianxun Zhang, Nannan Huang, Yudong Zhang|<http://arxiv.org/pdf/2505.18823v1>|[代码](https://github.com/Monsoon49/MSLAU-Net.); 提出MSLAU-Net，融合CNN与Transformer优势，有效提升医学图像分割性能。|
|📝 更新|Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic Analysis|探索QUIC动态：用于加密流量分析的大规模数据集|Barak Gahtan, Robert J. Shahla, Alex M. Bronstein, Reuven Cohen|<http://arxiv.org/pdf/2410.03728v6>|构建了大规模QUIC流量数据集VisQUIC，支持可控解密和机器学习分析，推动加密流量研究。|
|🆕 发布|MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images|MoMBS：混合阶数小批量采样提升从不同质量图像中的模型训练|Han Li, Hu Han, S. Kevin Zhou|<http://arxiv.org/pdf/2505.18741v1>|提出MoMBS方法，通过混合阶次小批量采样优化不同质量训练样本的使用。|
|🆕 发布|ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery|ReflectGAN：从卫星图像中建模植被效应以估算土壤碳含量的方法|Dristi Datta, Manoranjan Paul, Manzur Murshed, Shyh Wei Teng, Leigh M. Schmidtke|<http://arxiv.org/pdf/2505.18546v1>|ReflectGAN通过重建裸土反射率，有效提升了植被覆盖区土壤有机碳估算的准确性。|
|🆕 发布|TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image Segmentation|TK-Mamba：结合KAN与Mamba进行文本驱动3D医学图像分割|Haoyu Yang, Yuxiang Cai, Jintao Chen, Xuhong Zhang, Wenhui Lei, Xiaoming Shi, Jianwei Yin, Yankai Jiang|<http://arxiv.org/pdf/2505.18525v1>|[代码](https://github.com/yhy-whu/TK-Mamba.); 结合Mamba和KAN，提出了一种高效的多模态框架，显著提升了3D医学图像分割的准确性和效率。|


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban Scenes via Remote Sensing Imagery|L2RSI：基于跨视图激光雷达和遥感图像的大规模城市场景定位识别|Ziwei Shi, Xiaoran Zhang, Yan Xia, Yu Zang, Siqi Shen, Cheng Wang|<http://arxiv.org/pdf/2503.11245v2>|[代码](https://shizw695.github.io/L2RSI); 提出了一种利用遥感图像进行大规模城市场景激光雷达地点识别的新方法，有效降低了成本并提高了定位精度。|
|📝 更新|RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models|RSTeller：利用公开可用数据和大型语言模型，在遥感领域扩展视觉语言模型的规模，并丰富语言语义|Junyao Ge, Xu Zhang, Yang Zheng, Kaitai Guo, Jimin Liang|<http://arxiv.org/pdf/2408.14744v4>|[代码](https://github.com/SlytherinGe/RSTeller.); 利用大型语言模型自动生成语义丰富的遥感图像描述，降低标注成本，推动视觉语言模型发展。|


### 创意媒体生成 (Creative Media Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos|ProphetDWM：一种用于预测未来动作和视频的驾驶世界模型|Xiaodong Wang, Peixi Peng|<http://arxiv.org/pdf/2505.18650v1>|提出ProphetDWM，一种预测未来动作和视频的驾驶世界模型，解决动作控制和预测问题。|
|🆕 发布|HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection|超假：高级深度伪造检测的超光谱重建和注意力引导分析|Pavan C Shekar, Pawan Soni, Vivek Kanhangad|<http://arxiv.org/pdf/2505.18587v1>|提出HyperFake，通过重建高光谱数据并利用注意力机制，实现更精准的深度伪造检测。|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 量子视觉算法 (Quantum Visual Algorithms)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Vision Graph Prompting via Semantic Low-Rank Decomposition|基于语义低秩分解的视觉图提示|Zixiang Ai, Zichen Liu, Jiahuan Zhou|<http://arxiv.org/pdf/2505.04121v2>|[代码](https://github.com/zhoujiahuan1991/ICML2025-VGP.); 提出一种基于语义低秩分解的视觉图提示方法，显著提升视觉图神经网络在下游任务上的迁移性能。|

