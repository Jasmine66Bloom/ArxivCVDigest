## [UPDATED!] **2025-05-29** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence|空间-MLLM：提升基于视觉的空间智能中的MLLM能力|Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan|<http://arxiv.org/pdf/2505.23747v1>|[代码](https://diankun-wu.github.io/Spatial-MLLM); 提出Spatial-MLLM，从纯2D观察中实现视觉空间推理，显著提升MLLM在空间理解与推理任务上...|
|🆕 发布|FMG-Det: Foundation Model Guided Robust Object Detection|FMG-Det：基于基础模型的鲁棒目标检测|Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins|<http://arxiv.org/pdf/2505.23726v1>|提出FMG-Det，结合MIL框架和预训练模型，有效提升噪声标注下的目标检测性能。|
|🆕 发布|Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model|Muddit：统一离散扩散模型解放超越文本到图像的生成|Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong .etc.|<http://arxiv.org/pdf/2505.23606v1>|Muddit通过结合预训练视觉模型和轻量文本解码器，实现了快速并行多模态生成。|
|🆕 发布|Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition|统一多任务视觉-语言模型在手写数学表达式识别中的微调|Yu Li, Jin Jiang, Jianhua Zhu, Shuai Peng, Baole Wei, Yuxuan Zhou, Liangcai Gao|<http://arxiv.org/pdf/2505.23566v1>|[代码](https://github.com/BFlameSwift/Uni-MuMER); 提出Uni-MuMER，通过微调预训练视觉语言模型实现手写数学表达式识别，显著提升识别准确率。|
|🆕 发布|Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation|跨越几何和语义基础模型以实现广义单目深度估计|Sanggyun Ma, Wonjoon Choi, Jihun Park, Jaeyeul Kim, Seunghun Lee, Jiwan Seo, Sunghoon Im|<http://arxiv.org/pdf/2505.23400v1>|提出BriGeS方法，融合几何和语义信息，有效提升复杂场景单目深度估计性能。|
|🆕 发布|Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis|维度缩减攻击！视频生成模型在可控图像合成方面是专家|Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, Bin Wang|<http://arxiv.org/pdf/2505.23325v1>|[代码](https://dra-ctrl-2025.github.io/DRA-Ctrl); 提出了一种利用视频生成模型进行可控图像合成的“降维攻击”方法，显著提升了图像生成效果。|
|📝 更新|Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision|Nexus：一种全感知和交互的语言、音频和视觉模型|Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Haohan Li, Yu Lu, Shilin Zhou .etc.|<http://arxiv.org/pdf/2503.01879v3>|提出Nexus模型，融合语言、音频和视觉，解决多模态数据集限制和计算成本问题。|
|🆕 发布|Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion|通过对抗性物体融合破坏视觉-语言模型驱动的导航服务|Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, Tao Xiang|<http://arxiv.org/pdf/2505.23266v1>|提出Adversarial Object Fusion攻击框架，有效破坏基于视觉语言模型的导航服务。|
|📝 更新|RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting|河蟒：全球河流径流和洪水预报的状态空间模型|Mohamad Hakam Shams Eddin, Yikui Zhang, Stefan Kollet, Juergen Gall|<http://arxiv.org/pdf/2505.22535v2>|RiverMamba通过结合空间建模和气象预测，实现了对全球河流径流和洪水的高精度预测。|
|🆕 发布|Navigating the Accuracy-Size Trade-Off with Flexible Model Merging|在灵活模型融合中导航精度-大小权衡|Akash Dhasade, Divyansh Jhunjhunwala, Milos Vujasinovic, Gauri Joshi, Anne-Marie Kermarrec|<http://arxiv.org/pdf/2505.23209v1>|FlexMerge提出了一种灵活的模型合并框架，有效平衡了模型大小与准确率。|
|📝 更新|UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation|UniBiomed：一种通用的基于理解的生物医学图像解释基础模型|Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Luyang Luo, Neeraj Mahboobani, Varut Vardhanabhuti, Ronald Cheong Kin Chan .etc.|<http://arxiv.org/pdf/2504.21336v2>|UniBiomed提出了一种集成多模态模型，实现准确诊断与同时分割生物医学图像目标，优化AI辅助图像...|
|📝 更新|CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation|CAST：对比自适应与蒸馏的半监督实例分割|Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu|<http://arxiv.org/pdf/2505.21904v2>|CAST通过半监督知识蒸馏，将预训练模型压缩成紧凑专家，实现半监督实例分割。|
|📝 更新|Interpreting the linear structure of vision-language model embedding spaces|解读视觉-语言模型嵌入空间的线性结构|Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil|<http://arxiv.org/pdf/2504.11695v2>|通过训练稀疏自编码器揭示视觉-语言模型嵌入空间中跨模态语义的线性结构。|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis|视声通：一种用于语音合成的多模态仅解码器模型|Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly|<http://arxiv.org/pdf/2411.17690v2>|[代码](https://apple.github.io/visatronic-demo); Visatronic提出了一种多模态解码器模型，通过统一处理文本、视频和语音，实现了语音生成与视频和...|
|🆕 发布|DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers|DA-VPT：视觉Transformer的语义引导视觉提示微调|Li Ren, Chen Chen, Liqiang Wang, Kien Hua|<http://arxiv.org/pdf/2505.23694v1>|提出DA-VPT框架，通过语义信息引导视觉提示微调，提升ViT模型性能。|
|📝 更新|UniViTAR: Unified Vision Transformer with Native Resolution|统一分辨率原生视觉Transformer：UniViTAR|Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma|<http://arxiv.org/pdf/2504.01792v2>|UniViTAR通过引入分辨率课程学习和视觉模态自适应，实现了对原始分辨率视觉数据的统一建模。|
|📝 更新|RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning|环视-空中：基于仿射变换对比学习的空中遥感基础模型|Wenhui Diao, Haichen Yu, Kaiyue Kang, Tong Ling, Di Liu, Yingchao Feng, Hanbo Bi, Libo Ren .etc.|<http://arxiv.org/pdf/2409.13366v3>|提出RingMo-Aerial模型，通过引入FE-MSA机制和对比学习，提升空中遥感视觉任务检测能力...|
|📝 更新|QMamba: On First Exploration of Vision Mamba for Image Quality Assessment|QMamba：对视觉Mamba在图像质量评估中的首次探索|Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen|<http://arxiv.org/pdf/2406.09546v2>|[代码](https://github.com/bingo-G/QMamba.git); 探索Mamba模型在图像质量评估中的应用，提出QMamba和StylePrompt，显著提升感知能力...|
|🆕 发布|HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers|超点前体：基于双分支交叉注意力变换器的3D空间多模态融合|Aldino Rizaldy, Richard Gloaguen, Fabian Ewald Fassnacht, Pedram Ghamisi|<http://arxiv.org/pdf/2505.23206v1>|[代码](https://github.com/aldinorizaldy/hyperpointformer.); 提出了一种基于3D点云的多模态融合方法，通过双分支Transformer模型和交叉注意力机制，实现了...|
|📝 更新|Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation|DCLIP（Distill CLIP）：通过跨模态Transformer蒸馏增强图像-文本检索|Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma|<http://arxiv.org/pdf/2505.21549v2>|DCLIP通过跨模态Transformer蒸馏，提升了图像-文本检索能力，同时保持零样本分类性能。|
|📝 更新|Minimal Sufficient Views: A DNN model making predictions with more evidence has higher accuracy|最小充分视图：具有更多证据的深度神经网络模型预测精度更高|Keisuke Kawano, Takuro Kutsuna, Keisuke Sano|<http://arxiv.org/pdf/2402.01095v2>|提出MSVs概念，证明更多证据的DNN模型预测更准确。|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models|视频REPA：通过与基础模型的关系对齐学习物理以实现视频生成|Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, Yu Cheng|<http://arxiv.org/pdf/2505.23656v1>|[代码](https://videorepa.github.io/.); VideoREPA通过关系对齐将视频理解基础模型中的物理理解能力注入T2V模型，显著提升视频生成中的...|
|📝 更新|It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data|《盲配！迈向无需并行数据的视觉-语言对应》|Dominik Schnaus, Nikita Araslanov, Daniel Cremers|<http://arxiv.org/pdf/2503.24129v2>|首次提出无需平行数据实现视觉-语言对应匹配的方法，通过无监督学习将语义知识嵌入其他模态。|
|🆕 发布|OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data|全地球六层与跨层交互的多模态地球观测数据综合评估基准：OmniEarth-Bench|Fengxiang Wang, Mingshuo Chen, Xuming He, YiFan Zhang, Feng Liu, Zijie Guo, Zhenghao Hu, Jiong Wang .etc.|<http://arxiv.org/pdf/2505.23522v1>|构建了首个涵盖地球六大圈层及其交互的综合性多模态基准OmniEarth-Bench，以促进地球系统感...|
|🆕 发布|MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification|多模态协同融合网络用于细粒度语义分类|Yang Qiao, Xiaoyu Zhong, Xiaofeng Gu, Zhiguo Yu|<http://arxiv.org/pdf/2505.23365v1>|提出MCFNet，通过多模态协作融合网络提升细粒度语义分类精度。|
|🆕 发布|UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning|UniRL：通过监督学习和强化学习实现的自改进统一多模态模型|Weijia Mao, Zhenheng Yang, Mike Zheng Shou|<http://arxiv.org/pdf/2505.23380v1>|[代码](https://github.com/showlab/UniRL.); 提出UniRL，一种无需外部图像数据，通过自监督和强化学习提升多模态模型生成与理解能力的后训练方法。|
|📝 更新|mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus|mOSCAR：一个大规模多语言和多模态文档级语料库|Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, Rémi Lacroix, Cordelia Schmid, Rachel Bawden, Benoît Sagot|<http://arxiv.org/pdf/2406.08707v2>|构建了首个大规模多语言多模态文档语料库mOSCAR，提升多语言图像-文本模型的学习能力。|
|📝 更新|Audio Visual Segmentation Through Text Embeddings|基于文本嵌入的视听分割|Kyungbok Lee, You Zhang, Zhiyao Duan|<http://arxiv.org/pdf/2502.16359v2>|[代码](https://github.com/bok-bok/AV2T-SAM.); 提出AV2T-SAM框架，通过文本嵌入空间增强音频-视觉分割，有效利用预训练模型和跨模态语义对齐。|
|📝 更新|Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection|多模态逆注意力网络结合内在判别特征利用的假新闻检测|Tianlin Zhang, En Yu, Yi Shao, Jiande Sun|<http://arxiv.org/pdf/2502.01699v2>|提出了一种基于多模态逆注意力网络的方法，有效提升了假新闻检测的准确性。|
|📝 更新|Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection|超越面部交换：一种基于扩散的多模态深度伪造检测的数字人基准|Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Zhaofeng He|<http://arxiv.org/pdf/2505.16512v2>|提出DigiShield，有效检测基于扩散模型的数字人伪造，提升多模态深度伪造检测性能。|
|📝 更新|YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction|YH-MINER：自然生态珊瑚指标提取的多模态智能系统|Mingzhuang Wang, Yvyang Li, Xiyang Zhang, Fei Tan, Qi Shi, Guotao Zhang, Siqi Chen, Yufei Liu .etc.|<http://arxiv.org/pdf/2505.22250v2>|开发YH-MINER系统，利用多模态大模型实现珊瑚礁生态监测，提高自动识别和分类准确率。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping|从运动中提取房间：无姿态室内3D物体检测作为定位与建图|Justin Lazarow, Kai Kang, Afshin Dehghan|<http://arxiv.org/pdf/2505.23756v1>|提出了一种基于运动的无姿态室内3D物体检测方法，通过定位和映射生成高精度语义3D地图。|
|🆕 发布|Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need|提升领域增量学习：选择最优参数就是你所需要的一切|Qiang Wang, Xiang Song, Yuhang He, Jizhou Han, Chenhao Ding, Xinyuan Gao, Yihong Gong|<http://arxiv.org/pdf/2505.23744v1>|[代码](https://github.com/qwangcv/SOYO.); 提出SOYO框架，通过优化参数选择提升领域增量学习性能。|
|📝 更新|An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data|基于自监督异常检测的有限数据连续膝关节骨关节炎严重程度分级的人工智能系统|Niamh Belton, Aonghus Lawlor, Kathleen M. Curran|<http://arxiv.org/pdf/2407.11500v2>|[代码](https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.); 提出了一种基于自监督异常检测的连续膝关节骨关节炎分级方法，显著提高诊断准确性和减少数据需求。|
|🆕 发布|Network Inversion for Uncertainty-Aware Out-of-Distribution Detection|网络反演用于不确定感知的异常值检测|Pirzada Suhail, Rehna Afroz, Amit Sethi|<http://arxiv.org/pdf/2505.23448v1>|提出一种结合网络反演和分类器训练的框架，同时解决异常值检测和不确定性估计问题。|
|🆕 发布|VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration|VITON-DRR：通过非刚性配准实现细节保留的虚拟试穿|Ben Li, Minqi Li, Jie Ren, Kaibing Zhang|<http://arxiv.org/pdf/2505.23439v1>|提出VITON-DRR方法，通过非刚性配准保留服装细节，实现更准确的虚拟试穿。|
|📝 更新|All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning|所有补丁都重要，更多补丁更佳：通过全景补丁学习增强人工智能图像检测|Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao .etc.|<http://arxiv.org/pdf/2504.01396v2>|通过引入泛化性更强的“全图块学习”框架，有效解决了AI生成图像检测中的“少数图块偏差”问题。|
|📝 更新|X2-DFD: A framework for eXplainable and eXtendable Deepfake Detection|X2-DFD：一种可解释和可扩展的深度伪造检测框架|Yize Chen, Zhiyuan Yan, Guangliang Cheng, Kangran Zhao, Siwei Lyu, Baoyuan Wu|<http://arxiv.org/pdf/2410.06126v4>|提出X2-DFD框架，通过多模态大语言模型增强深度伪造检测的准确性和可解释性。|
|📝 更新|Benchmarking YOLOv8 for Optimal Crack Detection in Civil Infrastructure|基准测试YOLOv8在民用基础设施裂缝检测中的最优性能|Woubishet Zewdu Taffese, Ritesh Sharma, Mohammad Hossein Afsharmovahed, Gunasekaran Manogaran, Genda Chen|<http://arxiv.org/pdf/2501.06922v2>|通过优化YOLOv8模型，实现了桥梁裂缝检测的高效与准确，为基础设施监测提供了新基准。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Color Image Set Recognition Based on Quaternionic Grassmannians|基于四元数 Grassmannian 的彩色图像集识别|Xiang Xiang Wang, Tin-Yau Tam|<http://arxiv.org/pdf/2505.23629v1>|提出了一种基于四元数 Grassmannian 的颜色图像集识别方法，有效捕捉颜色信息并实现高效分类...|
|🆕 发布|VModA: An Effective Framework for Adaptive NSFW Image Moderation|VModA：一种有效的自适应NSFW图像审查框架|Han Bao, Qinying Wang, Zhi Chen, Qingming Li, Xuhong Zhang, Changjiang Li, Zonghui Wang, Shouling Ji .etc.|<http://arxiv.org/pdf/2505.23386v1>|VModA提出了一种适应性强、准确率高的NSFW图像检测框架，有效应对复杂语义和平台规则差异。|
|🆕 发布|DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification|DSAGL：双流注意力引导的弱监督全切片图像分类学习|Daoxi Cao, Hangbei Cheng, Yijin Li, Ruolin Zhou, Xinyi Li, Xuehan Zhang, Binwei Li, Xuancheng Gu .etc.|<http://arxiv.org/pdf/2505.23341v1>|DSAGL通过结合教师-学生架构和双流设计，有效解决弱监督全切片图像分类中的实例模糊性和语义一致性难...|


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|RefCut: Interactive Segmentation with Reference Guidance|RefCut：参考引导的交互式分割|Zheng Lin, Nan Zhou, Chen-Xi Du, Deng-Ping Fan, Shi-Min Hu|<http://arxiv.org/pdf/2503.17820v2>|RefCut通过引入参考图像和掩码，有效解决交互式分割中的模糊性问题，提升用户标注效率和模型性能。|
|🆕 发布|Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation|跨越经典与现代计算机视觉：感知网络在树冠语义分割中的应用|Georgios Voulgaris|<http://arxiv.org/pdf/2505.23597v1>|提出PerceptiveNet模型，结合Log-Gabor卷积层和CNN-Transformer，显...|
|🆕 发布|Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation|自适应空间增强用于半监督语义分割|Lingyan Ran, Yali Li, Tao Zhuo, Shizhou Zhang, Yanning Zhang|<http://arxiv.org/pdf/2505.23438v1>|提出自适应空间增强方法，有效提升半监督语义分割性能。|
|📝 更新|Calibrating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation|弱监督语义分割中Transformer的未规范过度平滑校准|Lechao Cheng, Zerun Liu, Jingxuan He, Chaowei Fang, Dingwen Zhang, Meng Wang|<http://arxiv.org/pdf/2305.03112v2>|提出AReAM机制，通过浅层亲和力引导深层注意力，缓解Transformer在弱监督语义分割中的过度...|
|🆕 发布|Federated Unsupervised Semantic Segmentation|联邦无监督语义分割|Evangelos Charalampakis, Vasileios Mygdalis, Ioannis Pitas|<http://arxiv.org/pdf/2505.23292v1>|首次提出基于联邦学习的无监督语义分割框架，实现完全去中心化、无标签的语义分割训练。|
|🆕 发布|LeMoRe: Learn More Details for Lightweight Semantic Segmentation|LeMoRe：为轻量级语义分割学习更多细节|Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte|<http://arxiv.org/pdf/2505.23093v1>|LeMoRe通过结合显式和隐式建模，实现轻量级语义分割，平衡性能与效率。|


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features|弱监督下利用多分辨率学习特征进行图像操作区域定位|Ziyong Wang, Charith Abhayaratne|<http://arxiv.org/pdf/2505.23586v1>|提出了一种基于多分辨率特征融合的弱监督图像篡改定位方法，有效实现无像素级标注下的篡改区域定位。|
|📝 更新|FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies|FlexEvent：面向不同操作频率的灵活事件帧目标检测|Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, Wei Tsang Ooi|<http://arxiv.org/pdf/2412.06708v2>|FlexEvent提出了一种自适应频率的物体检测框架，有效提升了动态环境下的实时感知能力。|
|🆕 发布|CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection|粗到细Transformer用于实时目标检测：CF-DETR|Woojin Shin, Donghwa Kang, Byeongyun Park, Brent Byunghoon Kang, Jinkyu Lee, Hyeongboo Baek|<http://arxiv.org/pdf/2505.23317v1>|提出CF-DETR，通过粗细粒度动态调整和实时调度框架，实现实时且高精度的物体检测。|
|📝 更新|BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes|生物VL-QR：基于微二维码的以自我为中心的生物化学视觉-语言数据集|Tomohiro Nishimoto, Taichi Nishimura, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura .etc.|<http://arxiv.org/pdf/2404.03161v3>|[代码](https://nishi10mo.github.io/BioVL-QR); 提出结合Micro QR码和手部物体检测的方法，有效提升了生化视频理解能力。|
|🆕 发布|Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images|基于语言的物体检测学习：解决航拍图像中的多种变化|Sungjune Park, Hyunjun Kim, Beomchan Park, Yong Man Ro|<http://arxiv.org/pdf/2505.23193v1>|提出了一种名为LANGO的空中图像检测框架，通过语言引导学习缓解场景和实例级变化带来的影响。|
|🆕 发布|WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems|WTEFNet：高级驾驶辅助系统实时低光物体检测|Hao Wu, Junzhou Chen, Ronghui Zhang, Nengchao Lyu, Hongyu Hu, Yanyong Guo, Tony Z. Qiu|<http://arxiv.org/pdf/2505.23201v1>|WTEFNet通过低光增强、特征提取和自适应融合检测，实现实时低光场景下的物体检测。|
|📝 更新|Demystifying Catastrophic Forgetting in Two-Stage Incremental Object Detector|揭秘两阶段增量目标检测中的灾难性遗忘|Qirui Wu, Shizhou Zhang, De Cheng, Yinghui Xing, Di Xu, Peng Wang, Yanning Zhang|<http://arxiv.org/pdf/2502.05540v3>|[代码](https://github.com/fanrena/NSGP-RePRE); 提出NSGP-RePRE框架，通过区域原型重放和梯度投影解决增量目标检测中的灾难性遗忘问题。|
|🆕 发布|DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes|DIP-R1：基于强化学习的深度检测与感知，透视并理解复杂场景|Sungjune Park, Hyunjun Kim, Junho Kim, Seongho Kim, Yong Man Ro|<http://arxiv.org/pdf/2505.23179v1>|开发了一种基于强化学习的框架DIP-R1，显著提升了多模态大型语言模型在复杂场景中的视觉感知能力。|
|🆕 发布|Identification of Patterns of Cognitive Impairment for Early Detection of Dementia|认知障碍模式识别以早期检测痴呆|Anusha A. S., Uma Ranjan, Medha Sharma, Siddharth Dutt|<http://arxiv.org/pdf/2505.23109v1>|该论文提出了一种通过识别个体认知损伤模式来早期检测痴呆的新方法。|
|📝 更新|OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics|奥里恩基准：信息图表中图表和可识别物体检测的基准|Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu|<http://arxiv.org/pdf/2505.17473v3>|提出OrionBench基准，解决图表和可识别物体检测问题，提升视觉语言模型对图表理解能力。|


## 生成式视觉模型 (Generative Visual Modeling)


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models|文本区域：从冻结的图像-文本模型中提取的文本对齐区域标记|Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem|<http://arxiv.org/pdf/2505.23769v1>|[代码](https://github.com/avaxiao/TextRegion.); 提出TextRegion，结合图像-文本模型和SAM2优势，生成文本对齐区域标记，提升视觉理解能力。|
|📝 更新|Diffusion Classifiers Understand Compositionality, but Conditions Apply|扩散分类器理解组合性，但条件适用|Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach|<http://arxiv.org/pdf/2505.17955v2>|[代码](https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.); 该论文研究了扩散分类器在理解视觉场景组合性方面的能力，并揭示了模型成功的关键条件。|
|🆕 发布|DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP|暗光增强：通过重新任务扩散模型推进相机ISP的暗光原始图像增强|Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen|<http://arxiv.org/pdf/2505.23743v1>|通过重新任务预训练的生成扩散模型，DarkDiff显著提升了低光条件下相机ISP的图像增强效果。|
|🆕 发布|D-AR: Diffusion via Autoregressive Models|D-AR：自回归模型驱动的扩散|Ziteng Gao, Mike Zheng Shou|<http://arxiv.org/pdf/2505.23660v1>|[代码](https://github.com/showlab/D-AR); 提出了一种将图像扩散过程转化为自回归模型的新方法，实现了图像的自动生成和降噪。|
|📝 更新|BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis|脑MRDiff：一种用于解剖一致脑MRI合成的扩散模型|Moinak Bhattacharya, Saumya Gupta, Annie Singh, Chao Chen, Gagandeep Singh, Prateek Prasanna|<http://arxiv.org/pdf/2504.04532v2>|提出BrainMRDiff，一种基于扩散模型和拓扑保持的脑MRI合成方法，显著提升诊断图像质量。|
|🆕 发布|ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer|免疫差异：一种用于预测肺癌免疫治疗反应的扩散模型|Moinak Bhattacharya, Judy Huang, Amna F. Sher, Gagandeep Singh, Chao Chen, Prateek Prasanna|<http://arxiv.org/pdf/2505.23675v1>|ImmunoDiff通过扩散模型结合解剖先验和临床数据，提高了肺癌免疫治疗反应预测的准确性。|
|🆕 发布|ZeroSep: Separate Anything in Audio with Zero Training|ZeroSep：无需训练即可在音频中分离任何内容|Chao Huang, Yuesheng Ma, Junxuan Huang, Susan Liang, Yunlong Tang, Jing Bi, Wenqiang Liu, Nima Mesgarani .etc.|<http://arxiv.org/pdf/2505.23625v1>|ZeroSep通过预训练的文本引导音频扩散模型实现零样本音频源分离，无需特定训练。|
|📝 更新|Diffusion Sampling Correction via Approximately 10 Parameters|通过约10个参数进行扩散采样校正|Guangyi Wang, Wei Peng, Lijiang Li, Wenyu Chen, Yuren Cai, Songzhi Su|<http://arxiv.org/pdf/2411.06503v3>|[代码](https://github.com/onefly123/PAS.); 提出PCA自适应搜索方法，以约10个参数显著提升扩散概率模型采样效率。|
|📝 更新|ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction|重组网络：可学习的关键点和扩散用于二维壁画重建|Adeela Islam, Stefano Fiorini, Stuart James, Pietro Morerio, Alessio Del Bue|<http://arxiv.org/pdf/2505.21117v2>|ReassembleNet通过学习关键点和扩散技术，有效解决复杂二维壁画重构的难题。|
|📝 更新|Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage|迈向鲁棒的超高清图像描述：一种多智能体方法和用于事实性和覆盖率的双重评估指标|Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, Sungroh Yoon|<http://arxiv.org/pdf/2412.15484v3>|提出多智能体协作纠正图像描述，提升事实准确性和覆盖度。|
|🆕 发布|LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter|基于潜在代码簿对齐适配器的有效扩散盲人脸修复|Runyi Li, Bin Chen, Jian Zhang, Radu Timofte|<http://arxiv.org/pdf/2505.23462v1>|提出LAFR，通过潜代码本对齐适配器高效实现低质量图像的无监督人脸修复。|
|🆕 发布|Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\textsuperscript{\textregistered} Assessment|鲁棒且无需标注的噪声真实世界压疮图像伤口分割：迈向自动化的DESIGN®评估|Yun-Cheng Tsai|<http://arxiv.org/pdf/2505.23392v1>|提出了一种无需标注的伤口分割方法，通过结合YOLOv11n检测器和FUSegNet模型，实现了跨部位...|
|🆕 发布|TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models|轨迹约束的扩散模型中概念擦除|Finn Carter|<http://arxiv.org/pdf/2505.23312v1>|TRACE通过轨迹约束和注意力机制，有效消除扩散模型中的特定概念，同时保持生成质量。|
|📝 更新|Aligning Text to Image in Diffusion Models is Easier Than You Think|《在扩散模型中将文本对齐到图像比您想象的要简单》|Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, Jong Chul Ye|<http://arxiv.org/pdf/2503.08250v4>|提出SoftREPA方法，通过对比学习优化文本与图像表示的语义一致性。|
|🆕 发布|Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging|近端算法展开：用于单像素成像的灵活且高效的重建网络|Ping Wang, Lishun Wang, Gang Qu, Xiaodong Wang, Yulun Zhang, Xin Yuan|<http://arxiv.org/pdf/2505.23180v1>|[代码](https://github.com/pwangcs/ProxUnroll.); 设计了一种灵活高效的重建网络，解决单像素成像问题，兼顾了准确性和速度。|
|🆕 发布|HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image|HiGarment：基于跨模态和谐度的平面草图到逼真服装图像的扩散模型|Junyi Guo, Jingxuan Zhang, Fangyu Wu, Huanda Lu, Qiufeng Wang, Wenmian Yang, Eng Gee Lim, Dongming Lu|<http://arxiv.org/pdf/2505.23186v1>|HiGarment通过融合平面草图和文本指导，提出了一种新的服装生成模型，有效解决了服装生产过程中的...|
|📝 更新|Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models|非刚性运动校正的MRI重建：通过粗到细的扩散模型|Frederic Wang, Jonathan I. Tamir|<http://arxiv.org/pdf/2505.15057v2>|提出了一种利用定制扩散模型进行MRI重建和非刚性运动校正的新框架，有效减少运动伪影。|
|🆕 发布|FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing|FlowAlign：基于轨迹正则化、无需反演的流图像编辑|Jeongsol Kim, Yeobin Hong, Jong Chul Ye|<http://arxiv.org/pdf/2505.23145v1>|FlowAlign通过引入流匹配损失，实现了稳定且可逆的图像编辑。|
|📝 更新|Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging|通过细粒度文本桥接改进脑到图像重建|Runze Xia, Shuo Feng, Renzhi Wang, Congchi Yin, Xuyun Wen, Piji Li|<http://arxiv.org/pdf/2505.22150v2>|提出FgB2I方法，通过细粒度文本桥接提升脑到图像重建的细节和语义一致性。|
|🆕 发布|TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance|文本SR：多语言OCR引导的扩散超分辨率|Keren Ye, Ignacio Garcia Dorado, Michalis Raptis, Mauricio Delbracio, Irene Zhu, Peyman Milanfar, Hossein Talebi|<http://arxiv.org/pdf/2505.23119v1>|TextSR通过结合文本检测和OCR，有效指导多语言场景文本图像超分辨率，提升文本清晰度和整体质量。|
|📝 更新|FreSca: Scaling in Frequency Space Enhances Diffusion Models|FreSca：频域空间中的缩放增强扩散模型|Chao Huang, Susan Liang, Yunlong Tang, Jing Bi, Li Ma, Yapeng Tian, Chenliang Xu|<http://arxiv.org/pdf/2504.02154v3>|FreSca通过频率空间缩放，提升扩散模型对全局结构与细节的精细控制。|
|🆕 发布|SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images|空间喷溅：从稀疏未定位图像中高效生成语义3D|Yu Sheng, Jiajun Deng, Xinran Zhang, Yu Zhang, Bei Hua, Yanyong Zhang, Jianmin Ji|<http://arxiv.org/pdf/2505.23044v1>|SpatialSplat通过引入冗余感知高斯和双场语义表示，有效降低语义3D重建参数量，提升性能。|
|📝 更新|Textured Gaussians for Enhanced 3D Scene Appearance Modeling|纹理高斯增强三维场景外观建模|Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang .etc.|<http://arxiv.org/pdf/2411.18625v2>|提出纹理高斯增强3D场景建模，提升单个高斯表达力，优化图像质量。|
|📝 更新|One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks|一个提示验证您的模型：通过不可迁移的对抗攻击进行黑盒文本到图像模型的验证|Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, Hongwei Li|<http://arxiv.org/pdf/2410.22725v4>|提出VerifyPrompt，通过不可迁移的对抗攻击验证T2I模型，确保第三方服务提供真实模型。|
|📝 更新|Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface|Surf2CT：基于皮肤表面的胸廓3D CT合成的级联3D流匹配模型|Siyeop Yoon, Yujin Oh, Pengfei Jin, Sifan Song, Matthew Tivnan, Dufan Wu, Xiang Li, Quanzheng Li|<http://arxiv.org/pdf/2505.22511v2>|Surf2CT通过外部表面扫描和简单人口数据生成真实人体胸腔3D CT图像，开创了非侵入性内部解剖成...|
|📝 更新|CellFlux: Simulating Cellular Morphology Changes via Flow Matching|细胞流：通过流动匹配模拟细胞形态变化|Yuhui Zhang, Yuchang Su, Chenyu Wang, Tianhong Li, Zoe Wefers, Jeffrey Nirschl, James Burgess, Daisy Ding .etc.|<http://arxiv.org/pdf/2502.09775v2>|[代码](https://yuhui-zh15.github.io/CellFlux); CellFlux通过流匹配模拟细胞形态变化，有效区分实验干扰，提升细胞行为模拟准确性。|


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers|LoRAShop：基于校正流变换器的免训练多概念图像生成与编辑|Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag|<http://arxiv.org/pdf/2505.23758v1>|LoRAShop通过利用LoRA模型和特征交互模式，实现了无需训练的多概念图像编辑，提升了个性化图像...|
|🆕 发布|MAGREF: Masked Guidance for Any-Reference Video Generation|MAGREF：任意参考视频生成的掩码引导|Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang .etc.|<http://arxiv.org/pdf/2505.23742v1>|[代码](https://github.com/MAGREF-Video/MAGREF); 提出MAGREF框架，通过掩码引导实现多参考视频生成，解决多主体一致性和高质量生成问题。|
|🆕 发布|How Animals Dance (When You're Not Looking)|动物舞蹈之谜（当你不在场时）|Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz|<http://arxiv.org/pdf/2505.23738v1>|提出了一种基于关键帧和图优化的动物舞蹈视频生成方法，实现音乐同步和舞蹈动作捕捉。|
|📝 更新|SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D Hand-Object Trajectories|SIGHT：合成图像-文本条件化和几何引导的3D手-物体轨迹|Alexey Gavryushin, Alexandros Delitzas, Luc Van Gool, Marc Pollefeys, Kaichun Mo, Xi Wang|<http://arxiv.org/pdf/2503.22869v3>|SIGHT通过融合图像和文本信息，生成逼真的3D手-物体交互轨迹，解决了现有方法依赖3D模型或缺乏具...|
|🆕 发布|OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation|开放大学：统一多模态理解和生成的简单基线|Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy|<http://arxiv.org/pdf/2505.23661v1>|[代码](https://github.com/wusize/OpenUni.); 提出了一种简单轻量级的方法，通过连接预训练的多模态LLMs和扩散模型，实现统一的多模态理解和生成。|
|📝 更新|SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes|SynTable：用于杂乱桌面场景未见物体无模态实例分割的合成数据生成管道|Zhili Ng, Haozhe Wang, Zhengshen Zhang, Francis Tay Eng Hock, Marcelo H. Ang Jr|<http://arxiv.org/pdf/2307.07333v3>|SynTable提出了一种基于NVIDIA Isaac Sim的自动化合成数据生成工具，用于未见物体...|
|🆕 发布|Autoregressive Meta-Actions for Unified Controllable Trajectory Generation|自回归元动作实现统一可控轨迹生成|Jianbo Zhao, Taiyu Ban, Xiyang Wang, Qibin Zhou, Hangning Zhou, Zhihao Liu, Mu Yang, Lei Liu .etc.|<http://arxiv.org/pdf/2505.23612v1>|[代码](https://arma-traj.github.io/.); 提出了一种基于自回归元动作的统一可控轨迹生成方法，有效解决了现有框架中元动作与实际轨迹时间不匹配的问...|
|📝 更新|ReDDiT: Rehashing Noise for Discrete Visual Generation|ReDDiT：为离散视觉生成重哈希噪声|Tianren Ma, Xiaosong Zhang, Boyu Yang, Junlan Feng, Qixiang Ye|<http://arxiv.org/pdf/2505.19656v2>|提出ReDDiT框架，通过改进噪声设计和采样策略，显著提升离散扩散模型生成质量。|
|🆕 发布|R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation|R2I-Bench：基于推理驱动的文本到图像生成基准测试|Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, Lifu Huang|<http://arxiv.org/pdf/2505.23493v1>|构建R2I-Bench基准，评估推理驱动的文本到图像生成，揭示现有模型推理能力不足。|
|🆕 发布|VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation|VCapsBench：用于视频字幕质量评估的大规模细粒度基准|Shi-Xue Zhang, Hongfa Wang, Duojun Huang, Xin Li, Xiaobin Zhu, Xu-Cheng Yin|<http://arxiv.org/pdf/2505.23484v1>|[代码](https://github.com/GXYM/VCapsBench.); 构建了首个大规模细粒度视频字幕质量评估基准VCapsBench，以提升视频字幕生成质量。|
|🆕 发布|Semantics-Aware Human Motion Generation from Audio Instructions|语义感知音频指令生成的人体运动|Zi-An Wang, Shihao Zou, Shiyao Yu, Mingyuan Zhang, Chao Dong|<http://arxiv.org/pdf/2505.23465v1>|提出了一种从音频指令生成语义一致的人体动作的新方法，利用记忆检索注意力模块提升生成效果。|
|🆕 发布|A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation|逆向因果框架用于缓解场景图生成中的虚假相关性以实现去偏|Shuzhou Sun, Li Liu, Tianpeng Liu, Shuaifeng Zhi, Ming-Ming Cheng, Janne Heikkilä, Yongxiang Liu|<http://arxiv.org/pdf/2505.23451v1>|提出反向因果框架，通过干预混淆变量来缓解场景图生成中的虚假相关性，从而消除预测偏差。|
|🆕 发布|CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis|冷冻CCD：用于冷冻电镜合成的条件循环一致性扩散与生物物理建模|Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao .etc.|<http://arxiv.org/pdf/2505.23444v1>|CryoCCD通过结合生物物理建模和生成技术，生成结构准确的冷冻电镜图像，提升下游分析任务性能。|
|📝 更新|ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025|ReferDINO-Plus：2025年CVPR第4届PVUW MeViS挑战赛的第二个解决方案|Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Jian-Fang Hu|<http://arxiv.org/pdf/2503.23509v2>|[代码](https://github.com/iSEE-Laboratory/ReferDINO-Plus.); 提出ReferDINO-Plus，通过融合ReferDINO和SAM2的优势，显著提升视频目标分割性...|
|🆕 发布|Video Editing for Audio-Visual Dubbing|视频配音的音频-视频剪辑|Binyamin Manela, Sharon Gannot, Ethan Fetyaya|<http://arxiv.org/pdf/2505.23406v1>|提出EdiDub框架，通过内容感知编辑技术显著提升视频配音中的面部动作同步和视觉自然度。|
|📝 更新|SafeCFG: Controlling Harmful Features with Dynamic Safe Guidance for Safe Generation|安全CFG：通过动态安全引导控制有害特征以实现安全生成|Jiadong Pan, Liang Li, Hongcheng Gao, Zheng-Jun Zha, Qingming Huang, Jiebo Luo|<http://arxiv.org/pdf/2412.16039v2>|SafeCFG通过动态安全引导，有效控制有害特征，实现高质量且安全的图像生成。|
|🆕 发布|Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis|合成生成和潜投影去噪在多发性硬化症边缘病变中的应用|Alexandra G. Roberts, Ha M. Luu, Mert Şişman, Alexey V. Dimov, Ceren Tozlu, Ilhami Kovanlikaya, Susan A. Gauthier, Thanh D. Nguyen .etc.|<http://arxiv.org/pdf/2505.23353v1>|[代码](https://github.com/agr78/PRLx-GAN); 开发了一种生成和降噪多发性硬化症中磁性边缘病变的合成图像方法，以提升检测性能。|
|📝 更新|ChatHuman: Chatting about 3D Humans with Tools|ChatHuman：使用工具进行3D人类聊天的对话|Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black|<http://arxiv.org/pdf/2405.04533v2>|ChatHuman通过整合多种3D人体分析工具，实现语言驱动的交互式分析，提升3D人体任务处理能力。|
|🆕 发布|Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering|扩散采样路径讲述更多：一种高效的即插即用样本筛选策略|Sixian Wang, Zhiwei Tang, Tsung-Hui Chang|<http://arxiv.org/pdf/2505.23343v1>|提出了一种无需外部奖励信号或模型重训练的CFG-Rejection策略，有效过滤低质量样本，提升扩散...|
|📝 更新|DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes|梦工坊：多视角驾驶场景的感知运动自回归视频生成|Jianbiao Mei, Tao Hu, Xuemeng Yang, Licheng Wen, Yu Yang, Tiantian Wei, Yukai Ma, Min Dou .etc.|<http://arxiv.org/pdf/2409.04003v4>|[代码](https://pjlab-adg.github.io/DriveArena); DreamForge通过引入视角指导和运动感知注意力，实现了多视图驾驶场景的长期可控视频生成。|
|🆕 发布|Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching|超越最优传输：模型对齐耦合的流匹配|Yexiong Lin, Yu Yao, Tongliang Liu|<http://arxiv.org/pdf/2505.23346v1>|[代码](https://yexionglin.github.io/mac); 提出了一种基于模型偏好的耦合方法，有效提升Flow Matching的生成质量和效率。|
|📝 更新|RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation|RefVNLI：迈向可扩展的主语驱动文本到图像生成评估|Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin .etc.|<http://arxiv.org/pdf/2504.17502v2>|提出RefVNLI，一种评估文本到图像生成中文本对齐和主题保留的指标，有效解决现有评估方法的局限性。|
|🆕 发布|Quality assessment of 3D human animation: Subjective and objective evaluation|3D人体动画质量评估：主观与客观评价|Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-Hélène Olivier|<http://arxiv.org/pdf/2505.23301v1>|首次提出针对非参数人体动画的质量评估方法，通过数据驱动框架实现高精度主观评价预测。|
|🆕 发布|Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation|Wav2Sem：3D语音驱动面部动画的即插即用音频语义解耦|Hao Li, Ju Dai, Xin Zhao, Feng Zhou, Junjun Pan, Lei Li|<http://arxiv.org/pdf/2505.23290v1>|[代码](https://github.com/wslh852/Wav2Sem.git.); 提出Wav2Sem模块，有效解耦音频特征，提升3D面部动画的精确度和自然度。|
|🆕 发布|GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation|GenCAD-Self-Repairing：3D CAD生成的可行性提升|Chikaha Tsuji, Enrique Flores Medina, Harshit Gupta, Md Ferdous Alam|<http://arxiv.org/pdf/2505.23287v1>|提出GenCAD-Self-Repairing框架，通过扩散指导和自修复流程显著提升3D CAD生成...|
|📝 更新|CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner|CraftsMan3D：基于3D原生生成和交互式几何精修的高保真网格生成|Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long|<http://arxiv.org/pdf/2405.14979v3>|[代码](https://github.com/wyysf-98/CraftsMan); CraftsMan3D通过3D原生生成和交互式几何细化，实现了高保真3D模型的高效生成。|
|📝 更新|Cross-Modal Causal Intervention for Medical Report Generation|跨模态因果干预在医疗报告生成中的应用|Weixing Chen, Yang Liu, Ce Wang, Jiarui Zhu, Guanbin Li, Cheng-Lin Liu, Liang Lin|<http://arxiv.org/pdf/2303.09117v5>|[代码](https://github.com/WissingChen/CMCRL.); 提出了一种跨模态因果干预框架，有效解决医学报告生成中的视觉-语言偏差和图像质量问题。|
|📝 更新|Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage|探索解耦和可控的人脸图像合成：从端到端到分阶段|Zhengwentai Sun, Chenghong Li, Hongjie Liao, Xihe Yang, Keru Zheng, Heyuan Li, Yihao Zhi, Shuliang Ning .etc.|<http://arxiv.org/pdf/2503.19486v3>|[代码](https://taited.github.io/discohuman-project); 提出了一种分阶段的人像合成方法，有效提升了细粒度可控性和泛化能力。|
|🆕 发布|UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes|UniTEX：适用于3D形状的通用高保真生成纹理|Yixun Liang, Kunming Luo, Xiao Chen, Rui Chen, Hongyu Yan, Weiyu Li, Jiarui Liu, Ping Tan|<http://arxiv.org/pdf/2505.23253v1>|[代码](https://github.com/YixunLiang/UniTEX.); UniTEX通过直接在3D空间中生成纹理函数，实现了高质量、一致性的3D形状纹理生成。|
|🆕 发布|Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks|愚弄观察者：通过语义提示攻击破解AIGC检测器|Run Hao, Peng Ying|<http://arxiv.org/pdf/2505.23192v1>|提出一种利用语法树和蒙特卡洛树搜索的对抗性提示生成框架，有效绕过AIGC检测器。|
|📝 更新|Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek|Seek-CAD：基于深度Seek的局部推理的自优化3D参数化CAD生成模型|Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou|<http://arxiv.org/pdf/2505.17702v2>|Seek-CAD利用本地开源LLM DeepSeek-R1，结合视觉和思维链反馈，实现了无监督的3D...|
|🆕 发布|RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer|机器人视觉策略迁移：几何一致的视频扩散|Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang .etc.|<http://arxiv.org/pdf/2505.23171v1>|[代码](https://horizonrobotics.github.io/robot_lab); RoboTransfer通过融合多视图几何和场景组件控制，生成几何一致的视频，有效缩小模拟与现实差距...|
|🆕 发布|Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization|伪多源域泛化：弥合单源与多源域泛化之间的差距|Shohei Enomoto|<http://arxiv.org/pdf/2505.23173v1>|[代码](https://github.com/s-enmt/PseudoDomainBed.); 提出Pseudo Multi-source Domain Generalization方法，通过风格...|
|🆕 发布|Implicit Inversion turns CLIP into a Decoder|隐式逆变换将CLIP转化为解码器|Antonio D'Orazio, Maria Rosaria Briglia, Donato Crisostomi, Dario Loi, Emanuele Rodolà, Iacopo Masi|<http://arxiv.org/pdf/2505.23161v1>|利用CLIP模型直接进行图像合成，无需解码器或训练，实现文本到图像生成等功能。|
|🆕 发布|HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring|HMAD：通过锚定偏移提议和模拟监督多目标评分推进端到端驾驶|Bin Wang, Pingjun Li, Jinkun Liu, Jun Cheng, Hailong Lei, Yinze Rong, Huan-ang Gao, Kangliang Chen .etc.|<http://arxiv.org/pdf/2505.23129v1>|HMAD通过结合轨迹提议机制和模拟监督多目标评分，提升了端到端自动驾驶的轨迹生成和路径选择能力。|
|🆕 发布|Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing|从零到英雄：基于参考的零样本初始化赋能视频外观编辑|Tongtong Su, Chengyu Wang, Jun Huang, Dongming Lu|<http://arxiv.org/pdf/2505.23134v1>|[代码](https://github.com/Tonniia/Zero2Hero.); 提出Zero-to-Hero方法，通过参考图像和注意力机制实现视频外观编辑，提高编辑精度和一致性。|
|🆕 发布|MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation|MMGT：运动掩码引导的两阶段网络用于协同语音手势视频生成|Siyuan Wang, Jiawei Liu, Wei Wang, Yeying Jin, Jinsong Du, Zhi Han|<http://arxiv.org/pdf/2505.23120v1>|[代码](https://github.com/SIA-IDE/MMGT.); 提出MMGT网络，通过音频和运动掩码生成同步的语音手势视频，解决传统方法在细节控制和运动生成上的局限...|
|📝 更新|M3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes|M3Bench：3D场景中移动操作全身运动生成的基准测试|Zeyu Zhang, Sixu Yan, Muzhi Han, Zaijin Wang, Xinggang Wang, Song-Chun Zhu, Hangxin Liu|<http://arxiv.org/pdf/2410.06678v3>|M3Bench提出新基准，通过M3BenchMaker自动生成数据，评估移动操作中全身运动生成。|
|🆕 发布|GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion|GeoMan：基于图像到视频扩散的人体几何估计的时序一致性|Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, Umar Iqbal|<http://arxiv.org/pdf/2505.23085v1>|GeoMan通过图像到视频扩散模型，实现了从单目视频中准确估计3D人体几何形状，提高了时间一致性和泛...|
|🆕 发布|Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation|统一视觉-语言模型是否必要：理解和生成中的泛化|Jihai Zhang, Tianle Li, Linjie Li, Zhengyuan Yang, Yu Cheng|<http://arxiv.org/pdf/2505.23043v1>|探究了统一视觉语言模型中理解和生成任务的泛化能力，提出混合训练数据可提升模型性能。|
|📝 更新|Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation|跨模态RAG：子维度检索增强的文本到图像生成|Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao|<http://arxiv.org/pdf/2505.21956v2>|提出了一种基于子维度检索增强的文本到图像生成方法，有效解决复杂查询的元素检索问题。|
|📝 更新|Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling|生成但验证：通过回顾性重采样减少视觉-语言模型中的幻觉|Tsung-Han Wu, Heekyung Lee, Jiaxin Ge, Joseph E. Gonzalez, Trevor Darrell, David M. Chan|<http://arxiv.org/pdf/2504.13169v2>|提出REVERSE框架，通过结合幻觉感知训练和动态自我验证，有效减少视觉语言模型中的幻觉问题。|
|📝 更新|Minute-Long Videos with Dual Parallelisms|极短视频的双并行性|Zeqing Wang, Bowen Zheng, Xingyi Yang, Zhenxiong Tan, Yuecong Xu, Xinchao Wang|<http://arxiv.org/pdf/2505.21070v2>|提出了一种名为DualParal的分布式推理策略，通过并行处理视频帧和模型层，显著降低长视频生成的时...|
|📝 更新|Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning|定理验证的逆向思维问题生成用于几何推理|Linger Deng, Linghao Zhu, Yuliang Liu, Yu Wang, Qunyi Xie, Jingjing Wu, Gang Zhang, Yingying Zhu .etc.|<http://arxiv.org/pdf/2410.17885v3>|提出TR-CoT框架，通过定理验证反向推理，提升几何推理能力。|
|🆕 发布|Toward Memory-Aided World Models: Benchmarking via Spatial Consistency|迈向辅助记忆的世界模型：通过空间一致性进行基准测试|Kewei Lian, Shaofei Cai, Yilun Du, Yitao Liang|<http://arxiv.org/pdf/2505.22976v1>|构建了促进空间一致性记忆模块发展的数据集和基准，以提升世界模型在模拟和规划等下游任务中的可靠性。|
|🆕 发布|HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions|超动：基于DiT的复杂动作姿态引导的人体图像动画|Shuolin Xu, Siming Zheng, Ziyi Wang, HC Yu, Jinwei Chen, Huaqi Zhang, Bo Li, Peng-Tao Jiang|<http://arxiv.org/pdf/2505.22977v1>|提出Open-HyperMotionX数据集和HyperMotionX基准，通过DiT模型和RoPE...|
|🆕 发布|MOVi: Training-free Text-conditioned Multi-Object Video Generation|MOVi：无需训练的文本条件多目标视频生成|Aimon Rahman, Jiang Liu, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Yusheng Su, Vishal M. Patel .etc.|<http://arxiv.org/pdf/2505.22980v1>|提出一种无需训练的文本条件多物体视频生成方法，显著提升现有模型的多物体生成能力。|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization|层剥皮：自回归剥皮进行层状图像向量化|Ronghuan Wu, Wanchao Su, Jing Liao|<http://arxiv.org/pdf/2505.23740v1>|LayerPeeler通过自回归剥离策略，有效解决图像向量化中遮挡问题，提升路径语义和视觉保真度。|
|🆕 发布|Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation|Hallo4：通过直接偏好优化和时序运动调制实现高保真动态肖像动画|Jiahao Cui, Yan Chen, Mingwang Xu, Hanlin Shang, Yuxuan Chen, Yun Zhan, Zilong Dong, Yao Yao .etc.|<http://arxiv.org/pdf/2505.23525v1>|[代码](https://github.com/xyz123xyz456/hallo4.); 提出Hallo4，通过直接偏好优化和时序运动调制，实现高保真动态人像动画。|
|🆕 发布|Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings|点还是线？基于线表示的CAD图纸全景符号检测|Xingguang Wei, Haomin Wang, Shenglong Ye, Ruifeng Luo, Yanting Zhang, Lixin Gu, Jifeng Dai, Yu Qiao .etc.|<http://arxiv.org/pdf/2505.23395v1>|提出VecFormer，通过线条表示法提高CAD图纸中符号识别的准确性和效率。|
|📝 更新|Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion|Zero4D：基于单视频的无监督4D视频生成|Jangho Park, Taesung Kwon, Jong Chul Ye|<http://arxiv.org/pdf/2503.22622v3>|提出了一种无需训练的4D视频生成方法，从单视频生成多视角视频。|


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos|VF-Eval：评估用于生成AIGC视频反馈的多模态LLMs|Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao|<http://arxiv.org/pdf/2505.23693v1>|提出VF-Eval基准，全面评估MLLM在AIGC视频上的反馈生成能力。|
|📝 更新|Position: Interactive Generative Video as Next-Generation Game Engine|交互式生成视频：下一代游戏引擎的位置|Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu|<http://arxiv.org/pdf/2503.17359v2>|提出交互式生成视频技术，构建下一代游戏引擎，实现无限创意内容生成。|
|📝 更新|Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression|擦除概念，引导生成：概念抑制的全面综述|Yiwei Xie, Ping Liu, Zheng Zhang|<http://arxiv.org/pdf/2505.19398v2>|提出了一种概念擦除方法，从T2I模型中移除特定语义概念，以促进更安全的生成模型发展。|
|📝 更新|Compositional Scene Understanding through Inverse Generative Modeling|通过逆生成建模进行组合场景理解|Yanbo Wang, Justin Dauwels, Yilun Du|<http://arxiv.org/pdf/2505.21780v2>|[代码](https://energy-based-model.github.io/compositional-inference.); 通过组合生成模型，实现了从自然图像中推断场景结构和全局因素，并应用于零样本多对象感知。|
|📝 更新|Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable|双数据对齐使AI生成图像检测器更容易泛化|Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu .etc.|<http://arxiv.org/pdf/2505.14359v3>|提出Dual Data Alignment方法，通过像素和频率域对齐，提升AI图像检测器泛化能力。|
|🆕 发布|Unsupervised Transcript-assisted Video Summarization and Highlight Detection|无监督转录辅助视频摘要与关键帧检测|Spyros Barbakos, Charalampos Antoniadis, Gerasimos Potamianos, Gianluca Setti|<http://arxiv.org/pdf/2505.23268v1>|提出一种结合视频帧和字幕的RL框架，实现无监督视频摘要和关键帧检测。|
|🆕 发布|PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling|预FM：基于预测未来建模的在线视听事件解析|Xiao Yu, Yan Fang, Xiaojie Jin, Yao Zhao, Yunchao Wei|<http://arxiv.org/pdf/2505.23155v1>|[代码](https://github.com/XiaoYu-1123/PreFM.); 提出PreFM框架，通过预测未来建模实现实时音频-视觉事件解析，显著提升性能并降低参数量。|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Weight Space Representation Learning on Diverse NeRF Architectures|基于多样化NeRF架构的权重空间表示学习|Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano|<http://arxiv.org/pdf/2502.09623v2>|提出了一种处理多样化NeRF架构的框架，通过元网络学习实现架构无关的表示学习。|
|📝 更新|PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes|全景NeRF-360：城市场景中的全景3D到2D标签迁移|Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao|<http://arxiv.org/pdf/2309.10815v3>|[代码](https://github.com/fuxiao0719/PanopticNeRF); PanopticNeRF-360通过结合粗略3D标注和噪声2D语义线索，实现城市场景中全景3D到2D...|
|🆕 发布|PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views|物理NeRF：基于物理的稀疏视图3D重建|Mohamed Rayan Barhdadi, Hasan Kurban, Hussein Alnuweiri|<http://arxiv.org/pdf/2505.23481v1>|PhysicsNeRF通过引入物理约束，有效提升了稀疏视角下的3D重建性能。|
|🆕 发布|LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering|LODGE：基于细节级别的大规模高斯分层渲染与高效渲染|Jonas Kulhanek, Marie-Julie Rakotosaona, Fabian Manhardt, Christina Tsalicoglou, Michael Niemeyer, Torsten Sattler, Songyou Peng, Federico Tombari|<http://arxiv.org/pdf/2505.23158v1>|提出了一种高效的大规模场景LOD高斯分层渲染方法，显著降低渲染时间和内存需求。|
|📝 更新|NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID|下一代：通过文本调制实现的多模态物体重识别的多粒度专家混合|Shihao Li, Chenglong Li, Aihua Zheng, Andong Lu, Jin Tang, Jixin Ma|<http://arxiv.org/pdf/2505.20001v2>|提出一种基于文本调控的多模态对象重识别框架，有效提升识别准确率。|


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis|辐射三角形汤与软连接力用于3D重建和新视角合成|Nathaniel Burgdorfer, Philippos Mordohai|<http://arxiv.org/pdf/2505.23642v1>|提出了一种基于三角形和软连接力的3D重建与新型视图合成方法，提高了重建精度和视图质量。|
|🆕 发布|UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors|城市工艺：通过分层语义几何先验进行城市视图外推|Tianhang Wang, Fan Lu, Sanqing Qu, Guo Yu, Shihang Du, Ya Wu, Yuan Huang, Guang Chen|<http://arxiv.org/pdf/2505.23434v1>|设计UrbanCraft，利用分层语义几何先验解决城市场景重建中新型视角合成问题。|
|📝 更新|3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling|3D-UIR：基于物理外观-介质解耦的3D水下场景重建的3D高斯|Jieyu Yuan, Yujun Li, Yuanlin Zhang, Chunle Guo, Xiongxin Tang, Ruixing Wang, Chongyi Li|<http://arxiv.org/pdf/2505.21238v2>|[代码](https://bilityniu.github.io/3D-UIR.); 提出了一种基于物理建模的3D水下场景重建方法，有效解决了散射介质带来的渲染失真问题。|
|🆕 发布|Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting|整体大规模场景重建通过混合高斯分层渲染|Chuandong Liu, Huijiao Wang, Lei Yu, Gui-Song Xia|<http://arxiv.org/pdf/2505.23280v1>|[代码](https://github.com/azhuantou/MixGS.); 提出MixGS，通过混合高斯分层重建大规模场景，实现全局优化与局部细节兼顾。|
|📝 更新|Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud|稀疏2D高斯分层：基于密集点云的稀疏视图表面重建|Natsuki Takama, Shintaro Ito, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki|<http://arxiv.org/pdf/2505.19854v2>|[代码](https://gsisaoki.github.io/SPARSE2DGS); 提出Sparse2DGS方法，通过结合DUSt3R和COLMAP MVS生成密集点云，提高仅用三张图...|
|🆕 发布|Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object|零到三维：零样本部分视图图像到三维物体|Yuxuan Lin, Ruihang Chu, Zhenyu Chen, Xiao Tang, Lei Ke, Haoling Li, Yingji Zhong, Zhihao Li .etc.|<http://arxiv.org/pdf/2505.23054v1>|提出了一种无需训练的方法，通过融合局部密集观测和多源先验，实现零视角到三维物体的重建。|
|🆕 发布|Pose-free 3D Gaussian splatting via shape-ray estimation|无姿态3D高斯分层通过形状射线估计|Youngju Na, Taeyeon Kim, Jumin Lee, Kyu Beom Han, Woo Jae Kim, Sung-eui Yoon|<http://arxiv.org/pdf/2505.22978v1>|提出了一种无需姿态估计的3D高斯分层渲染方法，通过形状和相机射线联合估计克服了姿态不精确的问题。|


## 时序视觉分析 (Temporal Visual Analysis)


### 视频目标跟踪 (Video Object Tracking)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory|一个轨迹，一个标记：通过全景子对象轨迹实现的有根视频标记|Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna|<http://arxiv.org/pdf/2505.23617v1>|提出了一种基于轨迹的视觉视频编码方法，显著提升了视频理解性能和效率。|


### 时序建模与预测 (Temporal Modeling & Prediction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization|CLIP-AE：基于CLIP的跨视图音频-视觉增强用于无监督时间动作定位|Rui Xia, Dan Jiang, Quan Zhang, Ke Zhang, Chun Yuan|<http://arxiv.org/pdf/2505.23524v1>|提出CLIP辅助跨视图音频视觉增强方法，解决无监督动作定位中特征聚焦和边界确定难题。|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Robustness-enhanced Myoelectric Control with GAN-based Open-set Recognition|基于GAN的开放集识别增强的鲁棒肌电控制|Cheng Wang, Ziyang Feng, Pin Zhang, Manjiang Cao, Yiming Yuan, Tengfei Chang|<http://arxiv.org/pdf/2412.15819v2>|提出基于GAN的开放集识别方法，增强肌电图控制系统的鲁棒性和实用性。|
|📝 更新|BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change|BAH视频数据集：用于识别视频中行为改变的矛盾/犹豫|Manuela González-González, Soufiane Belharbi, Muhammad Osama Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich, Simon L Bacon .etc.|<http://arxiv.org/pdf/2505.19328v2>|构建了首个用于视频情感识别的BAH数据集，为行为改变干预提供个性化支持。|
|🆕 发布|Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition|基于骨架的动作识别时空联合密度驱动学习|Shanaka Ramesh Gunasekara, Wanqing Li, Philip Ogunbona, Jack Yang|<http://arxiv.org/pdf/2505.23012v1>|提出了一种基于时空联合密度的学习方法，有效提升了骨骼动作识别性能。|
|📝 更新|ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations|海洋观测中的地理感知浮游动物识别与分类数据集：ZooplanktonBench|Fukun Liu, Adam T. Greer, Gengchen Mai, Jin Sun|<http://arxiv.org/pdf/2505.18477v3>|[代码](https://lfk118.github.io/ZooplanktonBench_Webpage.); 构建了包含地理信息的浮游动物数据集，以解决复杂环境中计算机视觉识别和分类的挑战。|


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning|VAU-R1：通过强化微调推进视频异常理解|Liyun Zhu, Qixiang Chen, Xi Shen, Xiaodong Cun|<http://arxiv.org/pdf/2505.23504v1>|[代码](https://github.com/GVCLab/VAU-R1.); VAU-R1通过强化微调提升视频异常理解，并构建首个视频异常推理基准VAU-Bench。|
|📝 更新|ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications|ProDisc-VAD：一种用于视频监控应用中弱监督异常检测的高效系统|Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu|<http://arxiv.org/pdf/2505.02179v2>|[代码](https://github.com/modadundun/ProDisc-VAD.); 提出ProDisc-VAD，通过原型交互层和伪实例判别增强，有效解决弱监督视频异常检测中的标签模糊问...|
|🆕 发布|VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?|视频推理基准：多模态语言模型能否执行以视觉为中心的复杂视频推理？|Yuanxin Liu, Kun Ouyang, Haoning Wu, Yi Liu, Lin Sui, Xinhao Li, Yan Zhong, Y. Charles .etc.|<http://arxiv.org/pdf/2505.23359v1>|VideoReasonBench评估了视觉中心复杂视频推理，揭示了多模态LLMs在复杂视频理解任务上...|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Rethinking Positive Pairs in Contrastive Learning|重新思考对比学习中的正样本对|Jiantao Wu, Sara Atito, Zhenhua Feng, Shentong Mo, Josef Kitler, Muhammad Awais|<http://arxiv.org/pdf/2410.18200v2>|提出SimLAP框架，从语义不同样本对中学习相似性，优化任意样本对的相似度。|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch|绘制FLOPs：迈向高效的人体草图网络|Aneeshan Sain, Subhajit Maity, Pinaki Nath Chowdhury, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song|<http://arxiv.org/pdf/2505.23763v1>|提出针对草图数据的轻量级网络，通过知识蒸馏和抽象选择器降低FLOPs，实现高效草图识别。|
|🆕 发布|REOrdering Patches Improves Vision Models|重新排列补丁可提升视觉模型|Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta|<http://arxiv.org/pdf/2505.23751v1>|提出REOrder框架，通过优化图像块顺序显著提升视觉模型性能。|
|🆕 发布|PixelThink: Towards Efficient Chain-of-Pixel Reasoning|像素思考：迈向高效的像素链推理|Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu .etc.|<http://arxiv.org/pdf/2505.23727v1>|PixelThink通过结合任务难度和模型不确定性，有效提升了推理效率和分割性能。|
|🆕 发布|ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS|ZPressor：面向可扩展前馈3DGS的瓶颈感知压缩|Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang|<http://arxiv.org/pdf/2505.23734v1>|ZPressor通过信息瓶颈原理，实现轻量级压缩，提升3DGS模型处理多视角输入的效率和鲁棒性。|
|🆕 发布|Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation|多目标域适应的兼容合并后训练量化|Juncheol Shin, Minsang Seok, Seonggon Kim, Eunhyeok Park|<http://arxiv.org/pdf/2505.23651v1>|提出HDRQ方法，解决量化模型在多目标域自适应中的模型合并问题，实现高效迁移学习。|
|📝 更新|A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data|基于全连接隐马尔可夫随机场的神经影像数据假发现率控制方法|Taehyo Kim, Qiran Jia, Mony J. de Leon, Hai Shu|<http://arxiv.org/pdf/2505.20688v2>|提出fcHMRF-LIS方法，有效控制神经影像数据中假发现率，提高检测准确性。|
|🆕 发布|Buffer-free Class-Incremental Learning with Out-of-Distribution Detection|无缓冲类增量学习与分布外检测|Srishti Gupta, Daniele Angioni, Maura Pintor, Ambra Demontis, Lea Schönherr, Battista Biggio, Fabio Roli|<http://arxiv.org/pdf/2505.23412v1>|提出了一种无需缓冲区且结合分布外检测的类增量学习方法，有效提升了开放世界场景下的模型性能。|
|📝 更新|Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?|多模态大型语言模型中的Token剪枝：我们是否解决了正确的问题？|Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang|<http://arxiv.org/pdf/2502.11501v2>|探究并改进了多模态大语言模型中token pruning的设计与评估，以提升效率。|
|📝 更新|Can We Predict Performance of Large Models across Vision-Language Tasks?|能否预测大型模型在视觉-语言任务中的性能？|Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould|<http://arxiv.org/pdf/2410.10112v2>|提出了一种基于矩阵补全的预测框架，可预测大型视觉语言模型在不同任务上的性能。|
|📝 更新|LEAVS: An LLM-based Labeler for Abdominal CT Supervision|LEAVS：基于LLM的腹部CT监督标签器|Ricardo Bigolin Lanfredi, Yan Zhuang, Mark Finkelstein, Praveen Thoppey Srinivasan Balamuralikrishna, Luke Krembs, Brandon Khoury, Arthi Reddy, Pritam Mukherjee .etc.|<http://arxiv.org/pdf/2503.13330v2>|提出LEAVS，一种基于大型语言模型的标注器，用于从腹部CT报告中提取多种异常的确定性和紧急性标签。|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis|多模态大型语言模型在内窥镜分析中的全面评估|Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan|<http://arxiv.org/pdf/2505.23601v1>|构建了首个全面评估多模态大语言模型在内镜分析中应用能力的基准EndoBench。|
|🆕 发布|Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications|文化遗产应用中的元数据丰富模型：融合神经网络和语义知识图谱的立场论文|Jan Ignatowicz, Krzysztof Kutt, Grzegorz J. Nalepa|<http://arxiv.org/pdf/2505.23543v1>|提出了一种结合神经网络和语义知识图谱的元数据丰富模型，以提升文化遗产数字化过程中的可访问性和互操作性...|
|🆕 发布|URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration|统一多状态视角的RWKV模型用于低光图像恢复|Rui Xu, Yuzhen Niu, Yuezhou Li, Huangbiao Xu, Wenxi Liu, Yuzhong Chen|<http://arxiv.org/pdf/2505.23068v1>|提出URWKV模型，通过多状态视角实现灵活有效的低光图像退化恢复。|


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks|NACHOS：受硬件约束的早期退出神经网络的神经架构搜索|Matteo Gambella, Jary Pomponi, Simone Scardapane, Manuel Roveri|<http://arxiv.org/pdf/2401.13330v2>|NACHOS提出首个针对硬件约束的早期退出神经网络架构搜索框架，实现高效准确的设计。|
|🆕 发布|A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization|基于0-1整数优化的非水密场景级点云全局姿态划分与征服方法|Zhuodong Li, Fei Hou, Wencheng Wang, Xuequan Lu, Ying He|<http://arxiv.org/pdf/2505.23469v1>|[代码](https://github.com/zd-lee/DACPO.); 提出DACPO方法，通过分割和合并策略优化非封闭场景点云的全局方向。|
|🆕 发布|Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss|重新审视用于校准的重加权风险：AURC、焦点损失和逆焦点损失|Han Zhou, Sebastian G. Gruber, Teodora Popordanoska, Matthew B. Blaschko|<http://arxiv.org/pdf/2505.23463v1>|该论文通过分析不同加权风险函数，提出了一种基于AURC的优化方法，有效提升了模型的校准性能。|
|📝 更新|Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement|《Transformer推理是否需要注意力机制？探索函数保持的注意力替换》|Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang|<http://arxiv.org/pdf/2505.21535v2>|探索用LSTM替换Transformer中的注意力机制，实现高效且准确的无注意力推理。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Mobi-$π$: Mobilizing Your Robot Learning Policy|Mobi-$π$：移动您的机器人学习策略|Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika Antonova, Jeannette Bohg|<http://arxiv.org/pdf/2505.23692v1>|提出了一种通过优化机器人姿态实现策略迁移的方法，有效提升了移动机器人执行复杂任务的能力。|
|📝 更新|A Benchmark and Evaluation for Real-World Out-of-Distribution Detection Using Vision-Language Models|真实世界分布外检测的基准与评估：基于视觉-语言模型|Shiho Noda, Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa|<http://arxiv.org/pdf/2501.18463v3>|[代码](https://github.com/hoshi23/OOD-X-Benchmarks.); 构建了三个新的基准，评估了视觉语言模型在现实世界分布外检测中的性能。|
|📝 更新|Differential Coding for Training-Free ANN-to-SNN Conversion|无监督训练的ANN到SNN转换的差分编码|Zihan Huang, Wei Fang, Tong Bu, Peng Xue, Zecheng Hao, Wenxuan Liu, Yuanhong Tang, Zhaofei Yu .etc.|<http://arxiv.org/pdf/2503.00301v2>|[代码](https://github.com/h-z-h-cell/ANN-to-SNN-DCGS.); 提出差分编码方法，降低ANN到SNN转换中的能耗和延迟。|
|🆕 发布|Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift|多样化的原型集成提高对亚群体变化的鲁棒性|Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi .etc.|<http://arxiv.org/pdf/2505.23027v1>|[代码](https://github.com/minhto2802/dpe4subpop); 提出Diverse Prototypical Ensembles方法，通过多样分类器集合增强模型对子...|
|📝 更新|Dataset Distillation of 3D Point Clouds via Distribution Matching|3D点云的分布匹配数据蒸馏|Jae-Young Yim, Dongwook Kim, Jae-Young Sim|<http://arxiv.org/pdf/2503.22154v2>|提出了一种基于分布匹配的3D点云数据蒸馏方法，有效降低训练复杂度并提升模型泛化能力。|
|🆕 发布|Number of Clusters in a Dataset: A Regularized K-means Approach|数据集中聚类数量的优化K-means方法|Behzad Kamgar-Parsi, Behrooz Kamgar-Parsi|<http://arxiv.org/pdf/2505.22991v1>|提出了一种基于理想簇假设的K-means正则化方法，为超参数λ提供严格界限，减少聚类结果的不确定性。|


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition|对抗性语义和标签扰动攻击用于行人属性识别|Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, Jin Tang|<http://arxiv.org/pdf/2505.23313v1>|[代码](https://github.com/Event-AHU/OpenPAR.); 首次提出针对行人属性识别的对抗攻击与防御框架，有效应对语义和标签扰动攻击。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ZeroGUI: Automating Online GUI Learning at Zero Human Cost|零GUI：零人工成本自动化在线GUI学习|Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu .etc.|<http://arxiv.org/pdf/2505.23762v1>|[代码](https://github.com/OpenGVLab/ZeroGUI.); ZeroGUI通过自动化在线GUI学习，实现零成本训练GUI智能体，显著提升性能。|
|🆕 发布|To Trust Or Not To Trust Your Vision-Language Model's Prediction|信任还是不信任你的视觉-语言模型预测|Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink|<http://arxiv.org/pdf/2505.23745v1>|[代码](https://github.com/EPFL-IMOS/TrustVLM.); 提出TrustVLM框架，通过图像嵌入空间改进置信度评分，提升视觉语言模型预测可靠性。|
|🆕 发布|Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning|皮肤病变表型学通过嵌套多模态对比学习|Dionysis Christopoulos, Sotiris Spanos, Eirini Baltzi, Valsamis Ntouskos, Konstantinos Karantzalos|<http://arxiv.org/pdf/2505.23709v1>|提出SLIMP，通过嵌套多模态对比学习，有效提升皮肤病变分类性能。|
|📝 更新|CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information Injection and Restricted Pseudo Labeling based Improved Semi-Supervised Few-Shot Learning|CVOCSemRPL：基于类方差优化聚类、语义信息注入和限制性伪标签的改进半监督小样本学习|Souvik Maji, Rhythm Baghel, Pratik Mazumder|<http://arxiv.org/pdf/2501.14401v2>|提出了一种基于类方差优化和语义信息注入的半监督少样本学习方法，显著提升了模型性能。|
|📝 更新|Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning|绕过音频-视觉深度伪造检测数据集的捷径：无监督学习|Stefan Smeu, Dragos-Alexandru Boldisor, Dan Oneata, Elisabeta Oneata|<http://arxiv.org/pdf/2412.00175v3>|通过无监督学习绕过音频-视频深度伪造检测数据集的捷径，提升检测鲁棒性。|
|🆕 发布|TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning|时间点：通过自监督关键点和描述符学习加速时间序列对齐|Ron Shapira Weber, Shahar Ben Ishay, Andrey Lavrinenko, Shahaf E. Finder, Oren Freifeld|<http://arxiv.org/pdf/2505.23475v1>|[代码](https://github.com/BGU-CS-VIL/TimePoint); TimePoint通过自监督学习加速时间序列对齐，显著提升准确性和效率。|
|🆕 发布|PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening|PAN-Crafter：学习模态一致性对齐以实现PAN锐化|Jeonghyeok Do, Sungpyo Kim, Geunhyuk Youk, Jaehyup Lee, Munchurl Kim|<http://arxiv.org/pdf/2505.23367v1>|PAN-Crafter通过模态一致性对齐框架，有效解决跨模态对齐问题，显著提升PAN锐化效果。|
|📝 更新|Stereo Radargrammetry Using Deep Learning from Airborne SAR Images|基于机载SAR图像的深度学习立体雷达测图|Tatsuya Sasayama, Shintaro Ito, Koichi Ito, Takafumi Aoki|<http://arxiv.org/pdf/2505.20876v3>|[代码](https://gsisaoki.github.io/IGARSS2025_sasayama); 提出了一种基于深度学习的立体雷达测图法，显著提升了空中合成孔径雷达图像的精度和范围。|
|🆕 发布|LADA: Scalable Label-Specific CLIP Adapter for Continual Learning|LADA：可扩展的标签特定CLIP适配器用于持续学习|Mao-Lin Luo, Zi-Hao Zhou, Tong Wei, Min-Ling Zhang|<http://arxiv.org/pdf/2505.23271v1>|[代码](https://github.com/MaolinLuo/LADA.); LADA通过添加轻量级标签特定记忆单元，有效解决了CLIP模型在持续学习中的参数选择和遗忘问题。|
|🆕 发布|EAD: An EEG Adapter for Automated Classification|EAD：一种用于自动分类的脑电图适配器|Pushapdeep Singh, Jyoti Nigam, Medicherla Vamsi Krishna, Arnav Bhavsar, Aditya Nigam|<http://arxiv.org/pdf/2505.23107v1>|提出EAD框架，实现跨设备EEG数据分类，提高不同通道数数据一致性。|
|📝 更新|BECAME: BayEsian Continual Learning with Adaptive Model MErging|贝叶斯自适应模型融合的持续学习：BECAME|Mei Li, Yuxiang Lu, Qinyan Dai, Suizhi Huang, Yue Ding, Hongtao Lu|<http://arxiv.org/pdf/2504.02666v2>|提出了一种基于贝叶斯原理的模型自适应融合方法，有效平衡了持续学习中的稳定性和可塑性。|
|📝 更新|WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation|弱MCN：弱监督指代表达理解与分割的多任务协作网络|Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo|<http://arxiv.org/pdf/2505.18686v2>|[代码](https://github.com/MRUIL/WeakMCN.); 提出WeakMCN，通过多任务协作网络提升弱监督指代表达式理解和分割性能。|


### 小样本学习 (Few-shot Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views|任意Splat：从非约束视角的前馈3D高斯Splatting|Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu .etc.|<http://arxiv.org/pdf/2505.23716v1>|[代码](https://city-super.github.io/anysplat); AnySplat通过单次前向预测，实现了无约束视角下的实时3D场景重建。|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 视觉操作与控制 (Visual Manipulation & Control)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks|《正立吗？通过细粒度多轴感知任务在多模态语言模型中解耦方向理解》|Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer|<http://arxiv.org/pdf/2505.21649v2>|提出DORI基准，揭示多模态系统在物体方向理解上的局限性。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence|MMSI-Bench：多图像空间智能基准|Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen .etc.|<http://arxiv.org/pdf/2505.23764v1>|构建了MMSI-Bench基准，评估多图像空间智能，揭示现有模型与人类差距。|
|🆕 发布|Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint|困惑于谜题：当视觉-语言模型无法接受提示时|Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan|<http://arxiv.org/pdf/2505.23759v1>|构建基准测试，揭示视觉语言模型在解谜题时抽象推理和隐喻理解上的局限性。|
|🆕 发布|Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought|Argus：基于地面思维链的视觉中心推理|Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang .etc.|<http://arxiv.org/pdf/2505.23766v1>|[代码](https://yunzeman.github.io/argus); Argus通过引入视觉注意力定位机制，有效提升了多模态推理任务中的视觉中心化推理能力。|
|🆕 发布|Grounded Reinforcement Learning for Visual Reasoning|基于视觉推理的 grounded 强化学习|Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, Katerina Fragkiadaki|<http://arxiv.org/pdf/2505.23678v1>|提出ViGoRL，通过视觉坐标锚定推理步骤，显著提升视觉推理能力。|
|🆕 发布|Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles|拼图-R1：基于规则的视觉强化学习在拼图中的应用研究|Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko|<http://arxiv.org/pdf/2505.23590v1>|[代码](https://github.com/zifuwanggg/Jigsaw-R1); 通过使用拼图游戏作为实验框架，研究了基于规则的视觉强化学习，揭示了其在多模态学习中的潜力和挑战。|
|🆕 发布|Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information|Qwen再次审视：引导视觉-语言推理模型重新关注视觉信息|Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li|<http://arxiv.org/pdf/2505.23558v1>|[代码](https://github.com/Liar406/Look_Again.); Qwen-LookAgain通过视觉-文本反思过程引导模型重新关注视觉信息，有效减少视觉语言推理模型...|
|🆕 发布|Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization|精细调整下一尺度视觉自回归模型与组相对策略优化|Matteo Gallici, Haitz Sáez de Ocáriz Borde|<http://arxiv.org/pdf/2505.23331v1>|利用强化学习优化视觉自回归模型，显著提升图像质量和生成风格控制。|
|🆕 发布|TrackVLA: Embodied Visual Tracking in the Wild|TrackVLA：野生环境中的具身视觉跟踪|Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu .etc.|<http://arxiv.org/pdf/2505.23189v1>|[代码](https://pku-epic.github.io/TrackVLA-web.); 提出TrackVLA模型，通过视觉-语言-动作协同，实现动态环境中的目标跟踪。|
|🆕 发布|Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning|像放射科医生一样解读胸部X光片：一个包含临床推理的基准|Jinquan Guan, Qi Chen, Lizhou Liang, Yuhang Liu, Vu Minh Hieu Phan, Minh-Son To, Jian Chen, Yutong Xie|<http://arxiv.org/pdf/2505.23143v1>|[代码](https://github.com/guanjinquan/CXRTrek); 构建了模拟放射科医生诊断推理过程的CXRTrek数据集，并提出CXRTrekNet模型以实现更准确的...|
|🆕 发布|PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents|PhotoArtAgent：基于语言模型的艺术代理智能照片修图|Haoyu Chen, Keda Tao, Yizao Wang, Xinlei Wang, Lei Zhu, Jinjin Gu|<http://arxiv.org/pdf/2505.23130v1>|PhotoArtAgent通过结合VLM和自然语言推理，实现了类似专业艺术家的智能照片修图。|
|🆕 发布|CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing|CURVE：基于CLIP的简单图像处理视觉图像增强的强化学习|Yuka Ogino, Takahiro Toizumi, Atsushi Ito|<http://arxiv.org/pdf/2505.23102v1>|CURVE通过CLIP模型和简单图像处理，实现了低光图像增强，兼顾了视觉效果和计算效率。|
|📝 更新|From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration|从头到尾：通过自适应数据校准实现大型视觉-语言模型中平衡表示的研究|Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng|<http://arxiv.org/pdf/2503.12821v4>|提出ADR框架，通过数据重平衡和合成缓解LVLM训练数据长尾问题，提升模型性能。|
|🆕 发布|Multi-Sourced Compositional Generalization in Visual Question Answering|多源组合泛化在视觉问答中的应用|Chuanhao Li, Wenbo Ye, Zhen Li, Yuwei Wu, Yunde Jia|<http://arxiv.org/pdf/2505.23045v1>|[代码](https://github.com/NeverMoreLCH/MSCG.); 提出了一种基于检索增强的框架，提升视觉问答模型在多源组合泛化方面的能力。|
|🆕 发布|Synthetic Document Question Answering in Hungarian|合成匈牙利语文档问答|Jonathan Li, Zoltan Csaki, Nidhi Hiremath, Etash Guha, Fenglu Hong, Edward Ma, Urmish Thakker|<http://arxiv.org/pdf/2505.23008v1>|构建了针对匈牙利语的文档问答数据集，显著提升了现代VLM在多语言文档问答任务上的性能。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CLDTracker: A Comprehensive Language Description for Visual Tracking|CLDTracker：一种全面的视觉跟踪语言描述|Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer|<http://arxiv.org/pdf/2505.23704v1>|[代码](https://github.com/HamadYA/CLDTracker); CLDTracker通过结合视觉和语言信息，实现了鲁棒的视觉跟踪。|
|📝 更新|SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding|SeeGround：零样本开放词汇3D视觉定位的视觉与地面|Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang|<http://arxiv.org/pdf/2412.04383v2>|SeeGround通过融合2D图像和3D文本描述，实现了零样本3D视觉定位，显著提升了性能。|
|🆕 发布|Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model|基于视觉-语言模型的富知识图像描述中的光束引导知识重放|Reem AlJunaid, Muzammil Behzad|<http://arxiv.org/pdf/2505.23358v1>|提出KRCapVLM框架，通过知识重放和beam search解码，提升图像描述的丰富性和准确性。|
|🆕 发布|Are MLMs Trapped in the Visual Room?|MLMs是否被困在视觉房间内？|Yazhou Zhang, Chunwang Zou, Qimeng Liu, Lu Rong, Ben Yao, Zheng Lian, Qiuchi Li, Peng Zhang .etc.|<http://arxiv.org/pdf/2505.23272v1>|提出“视觉房间”假说，通过感知和认知双层评估框架，揭示多模态大模型在视觉理解上的局限性。|


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models|检索视觉对比解码以减轻大型视觉-语言模型中的对象幻觉|Jihoon Lee, Min Song|<http://arxiv.org/pdf/2505.20569v2>|提出RVCD方法，通过检索视觉对比解码有效抑制大型视觉语言模型中的物体幻觉问题。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 智能交通视觉 (Intelligent Transportation Vision)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models|即兴VLA：开放权重和开放数据用于驾驶视觉-语言-动作模型|Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu .etc.|<http://arxiv.org/pdf/2505.23757v1>|[代码](https://github.com/ahydchh/Impromptu-VLA.); 提出Impromptu VLA数据集，解决自动驾驶VLA模型在非结构化场景下的性能瓶颈。|


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks|ThinkGeo：评估用于遥感任务的工具增强智能体|Akashah Shabbir, Muhammad Akhtar Munir, Akshay Dudhane, Muhammad Umer Sheikh, Muhammad Haris Khan, Paolo Fraccaro, Juan Bernabe Moreno, Fahad Shahbaz Khan .etc.|<http://arxiv.org/pdf/2505.23752v1>|构建了首个评估工具辅助智能体在遥感任务中空间推理能力的测试平台ThinkGeo。|
|📝 更新|GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control|GeoDrive：基于3D几何信息的精确动作控制的驾驶世界模型|Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shanghang Zhang|<http://arxiv.org/pdf/2505.22421v2>|GeoDrive通过整合3D几何信息，提升了自动驾驶场景建模的准确性和可靠性。|
|📝 更新|Information Entropy Guided Height-aware Histogram for Quantization-friendly Pillar Feature Encoder|信息熵引导的高度感知直方图用于量化友好型柱状特征编码器|Sifan Zhou, Zhihang Yuan, Dawei Yang, Ziyu Zhao, Xing Hu, Yuguang Shi, Xiaobo Lu, Qiang Wu|<http://arxiv.org/pdf/2405.18734v5>|提出了一种基于高度感知的柱状特征编码器，有效提升3D检测性能并降低量化复杂度。|
|📝 更新|Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery|利用卫星影像追踪实现可持续发展目标6的进展|Othmane Echchabi, Aya Lahlou, Nizar Talty, Josh Malcolm Manto, Ka Leung Lam|<http://arxiv.org/pdf/2411.19093v2>|利用卫星图像和深度学习技术，该研究构建了评估非洲地区清洁水和卫生设施覆盖情况的模型框架，为可持续发展...|
|🆕 发布|RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries|RSFAKE-1M：用于检测扩散生成遥感伪造的大规模数据集|Zhihong Tan, Jiayi Wang, Huiying Shi, Binyuan Huang, Hongchen Wei, Zhenzhong Chen|<http://arxiv.org/pdf/2505.23283v1>|构建了RSFAKE-1M数据集，用于检测扩散模型生成的遥感图像伪造，提升了伪造检测模型的泛化能力和鲁...|
|🆕 发布|Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey|遥感图像超分辨率技术进展：全面综述|Yunliang Qi, Meng Lou, Yimin Liu, Lu Li, Zhen Yang, Wen Nie|<http://arxiv.org/pdf/2505.23248v1>|该论文全面综述了遥感图像超分辨率技术，揭示了现有方法的局限性，并提出了未来研究方向。|
|🆕 发布|SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model|SeG-SR：通过视觉-语言模型将语义知识集成到遥感图像超分辨率中|Bowen Chen, Keyan Chen, Mohan Yang, Zhengxia Zou, Zhenwei Shi|<http://arxiv.org/pdf/2505.23010v1>|[代码](https://github.com/Mr-Bamboo/SeG-SR.); 提出SeG-SR框架，通过视觉语言模型提取语义知识，指导遥感图像超分辨率，提升重建效果。|


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact (Version. 2.0)|卫星影像与人工智能：海洋保护新时代，从研究到部署与影响（版本2.0）|Patrick Beukema, Favyen Bastani, Yawen Zheng, Piper Wolters, Henry Herzog, Joe Ferdinando|<http://arxiv.org/pdf/2312.03207v2>|开发四款针对不同卫星数据的计算机视觉模型，助力全球海洋保护实时监测。|
|🆕 发布|Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging|比较持久条码聚合与特征拼接对医学影像的影响|Dashti A. Ali, Richard K. G. Do, William R. Jarnagin, Aras T. Asaad, Amber L. Simpson|<http://arxiv.org/pdf/2505.23637v1>|比较了持久性条码聚合和特征拼接对医学图像分类性能的影响，提出特征拼接方法更优。|
|🆕 发布|DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification|深度胸腔：用于有效多任务学习的动态无梯度任务加权|Youssef Mohamed, Noran Mohamed, Khaled Abouhashad, Feilong Tang, Sara Atito, Shoaib Jameel, Imran Razzak, Ahmed B. Zaky|<http://arxiv.org/pdf/2505.23595v1>|[代码](https://github.com/youssefkhalil320/DeepChest-MTL); DeepChest通过动态任务权重优化，有效提升了胸部X光片多任务分类的性能。|
|🆕 发布|PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation|主成分分析在乳腺超声肿瘤分割中提升跨数据集泛化能力|Christian Schmidt, Heinrich Martin Overhoff|<http://arxiv.org/pdf/2505.23587v1>|利用PCA预处理数据，提高乳腺癌超声肿瘤分割模型的泛化能力。|
|🆕 发布|Can Large Language Models Challenge CNNS in Medical Image Analysis?|大型语言模型能否在医学图像分析中挑战卷积神经网络？|Shibbir Ahmed, Shahnewaz Karim Sakib, Anindya Bijoy Das|<http://arxiv.org/pdf/2505.23503v1>|提出了一种比较CNN和LLM在医学图像分析中性能的框架，发现LLM结合额外过滤可显著提升诊断性能。|
|🆕 发布|Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs|图像美学推理：基于多语言大模型的医学图像筛查新基准|Zheng Sun, Yi Wei, Long Yu|<http://arxiv.org/pdf/2505.23265v1>|提出医疗图像筛查新基准，利用强化学习提升MLLMs图像审美推理能力。|
|📝 更新|Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation|基于SAM和CLIP的零样本伪标签生成用于半监督语义分割|Nagito Saito, Shintaro Ito, Koichi Ito, Takafumi Aoki|<http://arxiv.org/pdf/2505.19846v2>|[代码](https://gsisaoki.github.io/ZERO-SHOT-PLG); 利用SAM和CLIP生成高质量伪标签，提升半监督语义分割模型准确率。|
|📝 更新|GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning|GETReason：通过分层多智能体推理增强图像上下文提取|Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta|<http://arxiv.org/pdf/2505.21863v2>|GETReason通过分层多智能体推理，提升图像上下文提取，有效关联图像与事件背景。|
|🆕 发布|Deep Modeling and Optimization of Medical Image Classification|深度建模与医学图像分类优化|Yihang Wu, Muhammad Owais, Reem Kateb, Ahmad Chaddad|<http://arxiv.org/pdf/2505.23040v1>|[代码](https://github.com/AIPMLab/SkinCancerSimulation.); 提出了一种结合CLIP、联邦学习和传统机器学习的方法，有效提升了医学图像分类的准确性和数据隐私保护。|


### 生物特征识别 (Biometric Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels|SHTOcc：基于稀疏头部和尾部体素的有效的3D占用预测|Qiucheng Yu, Yuan Xie, Xin Tan|<http://arxiv.org/pdf/2505.22461v2>|[代码](https://github.com/ge95net/SHTOcc); 提出SHTOcc方法，通过稀疏头尾体素构建和去耦学习，有效解决3D占用预测中的长尾问题和模型偏差。|


### 工业视觉检测 (Industrial Visual Inspection)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector|《寻求网格：用于BEV检测器的空间一致3D对抗性对象》|Aixuan Li, Mochu Xiang, Jing Zhang, Yuchao Dai|<http://arxiv.org/pdf/2505.22499v2>|提出了一种生成空间一致3D对抗物体的方法，以评估BEV检测器的鲁棒性。|
|🆕 发布|Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving|基于扩散的生成模型在自动驾驶中的3D占用预测|Yunshen Wang, Yicheng Liu, Tianyuan Yuan, Yucheng Mao, Yingshi Liang, Xiuyu Yang, Honggang Zhang, Hang Zhao|<http://arxiv.org/pdf/2505.23115v1>|利用扩散模型将3D占用预测转化为生成建模任务，有效提升自动驾驶场景预测的准确性和鲁棒性。|
|🆕 发布|Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions|面向基于标签比例的层次学习，实现隐私保护细粒度视觉分类|Jinyi Chang, Dongliang Chang, Lei Chen, Bingyao Yu, Zhanyu Ma|<http://arxiv.org/pdf/2505.23031v1>|提出了一种基于标签比例的层次化学习方法，实现了隐私保护下的细粒度视觉分类。|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 神经-符号视觉 (Neuro-symbolic Vision)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection|SAMamba：基于分层视觉的自适应状态空间建模红外小目标检测|Wenhao Xu, Shuchen Zheng, Changwei Wang, Zherui Zhang, Chuan Ren, Rongtao Xu, Shibiao Xu|<http://arxiv.org/pdf/2505.23214v1>|[代码](https://github.com/zhengshuchen/SAMamba.); SAMamba通过融合特征选择和上下文建模，有效提升了红外小目标检测性能。|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Agentic Knowledgeable Self-awareness|代理知识自我意识|Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang .etc.|<http://arxiv.org/pdf/2504.03553v2>|[代码](https://github.com/zjunlp/KnowSelf.); 提出KnowSelf方法，使LLM基于的智能体具备自主调节知识利用的能力，提升决策效果。|
|🆕 发布|iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures|iHDR：任意曝光次数的迭代HDR成像|Yu Yuan, Yiheng Chi, Xingguang Zhang, Stanley Chan|<http://arxiv.org/pdf/2505.22971v1>|提出iHDR，一种迭代融合任意数量曝光图像的HDR成像新框架，有效解决传统方法固定输入限制问题。|
|📝 更新|Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion|自监督增强前视声纳图像：通过特征空间转换和多帧融合弥合跨模态退化差距|Zhisheng Zhang, Peng Zhang, Fengxiang Wang, Liangli Ma, Fuchun Sun|<http://arxiv.org/pdf/2504.10974v3>|提出了一种通过特征空间转换和多帧融合增强前视声纳图像的方法，有效缩小跨模态退化差距。|

