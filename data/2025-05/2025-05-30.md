## [UPDATED!] **2025-05-30** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM|MIRAGE：评估多模态推理链中MLLM的幻觉|Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang|<http://arxiv.org/pdf/2505.24238v2>|提出MIRAGE基准和改进方法，有效识别和减少多模态大语言模型中的推理幻觉。|
|🆕 发布|Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts|混合专家：通过高效混合视觉专家缓解多模态学习冲突|Xin He, Xumeng Han, Longhui Wei, Lingxi Xie, Qi Tian|<http://arxiv.org/pdf/2505.24541v1>|Mixpert通过混合视觉专家架构和动态路由机制，有效缓解了多模态学习中的冲突，提高了多任务学习效率...|
|📝 更新|VITA: Towards Open-Source Interactive Omni Multimodal LLM|VITA：迈向开源交互式全模态大型语言模型|Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, Meng Zhao, Yi-Fan Zhang, Shaoqi Dong .etc.|<http://arxiv.org/pdf/2408.05211v3>|提出VITA，首个开源的多模态LLM，实现视频、图像、文本和音频的交互式处理与分析。|
|📝 更新|Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding|情感-Qwen：训练混合专家以实现统一的情感和通用视觉-语言理解|Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang .etc.|<http://arxiv.org/pdf/2505.06685v2>|[代码](https://github.com/24DavidHuang/Emotion-Qwen.); 提出Emotion-Qwen，通过混合专家模型和大规模数据集，提升视频情感理解和通用视觉语言理解能力...|
|🆕 发布|un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP|un$^2$CLIP：通过逆unCLIP提升CLIP的视觉细节捕捉能力|Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen|<http://arxiv.org/pdf/2505.24517v1>|[代码](https://github.com/LiYinqi/un2CLIP.); 通过逆unCLIP模型，提升CLIP捕捉图像细节的能力。|
|📝 更新|Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner|病理-R1：一种基于多模态强化学习的病理专家推理器|Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi .etc.|<http://arxiv.org/pdf/2505.11404v2>|[代码](https://github.com/Wenchuan-Zhang/Patho-R1.); 构建高质量病理推理数据集，提出Patho-R1模型，提升病理诊断准确性和推理合理性。|
|🆕 发布|Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models|混合-R1：多模态大型语言模型推理能力的统一奖励视角|Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song .etc.|<http://arxiv.org/pdf/2505.24164v1>|[代码](https://github.com/xushilin1/mixed-r1.); 提出Mixed-R1框架，通过混合奖励和训练数据提升多模态大语言模型的推理能力。|
|🆕 发布|Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction|基于流式视频预测的双手通用基础策略|Chenyou Fan, Fangzheng Yan, Chenjia Bai, Jiepeng Wang, Chi Zhang, Zhen Wang, Xuelong Li|<http://arxiv.org/pdf/2505.24156v1>|提出了一种基于文本预测的轻量级双臂操作策略，有效解决了数据稀缺和泛化困难问题。|
|🆕 发布|S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation|S4-Driver：可扩展的自监督驾驶多模态大型语言模型，具有时空视觉表示|Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen .etc.|<http://arxiv.org/pdf/2505.24139v1>|提出S4-Driver，通过时空视觉表示实现无需标注的自动驾驶运动规划。|
|🆕 发布|Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting|代理-FDA：基于代理的特征分布对齐，用于微调视觉基础模型而不忘却|Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald .etc.|<http://arxiv.org/pdf/2505.24088v1>|提出Proxy-FDA方法，通过特征分布对齐减少视觉基础模型微调时的概念遗忘问题。|


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Enhancing Large Vision Model in Street Scene Semantic Understanding through Leveraging Posterior Optimization Trajectory|通过利用后验优化轨迹增强街景场景语义理解的大型视觉模型|Wei-Bin Kou, Qingfeng Lin, Ming Tang, Jingreng Lei, Shuai Wang, Rongguang Ye, Guangxu Zhu, Yik-Chung Wu|<http://arxiv.org/pdf/2501.01710v2>|通过结合预训练的大视觉模型和后验优化轨迹引导优化方案，有效提升了自动驾驶场景语义理解模型的泛化能力和...|
|📝 更新|Using Knowledge Graphs to harvest datasets for efficient CLIP model training|利用知识图谱高效采集数据集以训练CLIP模型|Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox|<http://arxiv.org/pdf/2505.02746v2>|利用知识图谱优化网络搜索策略，大幅减少数据量高效训练CLIP模型。|
|📝 更新|Efficient Adaptation For Remote Sensing Visual Grounding|高效遥感视觉定位自适应|Hasan Moughnieh, Mohamad Chalhoub, Hasan Nasrallah, Cristiano Nattero, Paolo Campanella, Giovanni Nico, Ali J. Ghandour|<http://arxiv.org/pdf/2503.23083v3>|利用参数高效微调技术，有效降低遥感视觉定位的计算成本，实现高效跨模态理解。|
|🆕 发布|Geospatial Foundation Models to Enable Progress on Sustainable Development Goals|地理空间基础模型以促进可持续发展目标进展|Pedram Ghamisi, Weikang Yu, Xiaokang Zhang, Aldino Rizaldy, Jian Wang, Chufeng Zhou, Richard Gloaguen, Gustau Camps-Valls|<http://arxiv.org/pdf/2505.24528v1>|构建SustainFM框架，评估地理空间基础模型在可持续发展目标中的应用潜力。|
|🆕 发布|Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model|周期-LLM：扩展多模态大型语言模型的周期能力|Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu|<http://arxiv.org/pdf/2505.24476v1>|[代码](https://github.com/keke-nice/Period-LLM.); 提出Period-LLM模型，增强多模态大语言模型在周期性任务上的表现。|
|📝 更新|Unpaired Deblurring via Decoupled Diffusion Model|无配对去模糊通过解耦扩散模型|Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang|<http://arxiv.org/pdf/2502.01522v2>|UID-Diff通过解耦结构特征和模糊模式，有效提升了未知领域图像去模糊性能。|
|📝 更新|Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models|珊瑚结：多模态大型语言模型中关系幻觉评估、分析和缓解的全面基准|Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu|<http://arxiv.org/pdf/2408.09429v3>|构建了针对关系幻觉的综合基准Reefknot，并提出基于置信度的缓解策略，有效降低多模态大语言模型中...|
|🆕 发布|Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization|利用基础模型实现鲁棒且通用的6自由度支气管镜定位|Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Hongbin Liu|<http://arxiv.org/pdf/2505.24249v1>|提出PANSv2框架，利用基础模型增强深度估计和地标检测，实现鲁棒且通用的支气管镜定位。|
|🆕 发布|From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models|从幻觉到越狱：重新思考大型基础模型的脆弱性|Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang|<http://arxiv.org/pdf/2505.24232v1>|提出统一框架揭示大模型幻觉与越狱攻击的关联，提出联合防御策略提升模型鲁棒性。|
|🆕 发布|Benchmarking Foundation Models for Zero-Shot Biometric Tasks|基准测试用于零样本生物识别任务的基座模型|Redwan Sony, Parisa Farmanifard, Hamzeh Alzwairy, Nitish Shukla, Arun Ross|<http://arxiv.org/pdf/2505.24214v1>|构建基准评估预训练模型在零样本和少样本生物识别任务中的表现，实现跨模态生物识别。|
|🆕 发布|Pretraining Deformable Image Registration Networks with Random Images|基于随机图像预训练可变形图像配准网络|Junyu Chen, Shuwen Wei, Yihao Liu, Aaron Carass, Yong Du|<http://arxiv.org/pdf/2505.24167v1>|提出以随机图像注册作为代理任务，预训练图像注册网络，提升精度和效率。|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks|Agent-X：评估以视觉为中心的智能体任务中的深度多模态推理|Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud .etc.|<http://arxiv.org/pdf/2505.24876v1>|[代码](https://github.com/mbzuai-oryx/Agent-X); 构建了大规模基准Agent-X，评估视觉中心型智能体在多步和深度推理能力。|
|📝 更新|A Decade of Wheat Mapping for Lebanon|黎巴嫩小麦制图十年回顾|Hasan Wehbi, Hasan Nasrallah, Mohamad Hasan Zahweh, Zeinab Takach, Veera Ganesh Yalla, Ali J. Ghandour|<http://arxiv.org/pdf/2504.11366v2>|提出了一种基于TSViT和FTW框架的改进方法，提高了小麦田边界提取的精度和准确性。|
|📝 更新|Structured 3D Latents for Scalable and Versatile 3D Generation|结构化3D潜在表示：可扩展且通用的3D生成|Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong .etc.|<http://arxiv.org/pdf/2412.01506v3>|提出了一种基于结构化3D潜变量的3D生成方法，实现灵活的输出格式和高质量3D资产创建。|
|🆕 发布|PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge|PCIE_Pose：用于EgoExo4D姿态和熟练度估计挑战的解决方案|Feng Chen, Kanokphan Lertniphonphan, Qiancheng Yan, Xiaohui Fan, Jun Xie, Tao Zhang, Zhepeng Wang|<http://arxiv.org/pdf/2505.24411v1>|开发HP-ViT+和融合多模态特征，实现高精度手部和全身姿态估计及技能评估。|
|🆕 发布|Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing|利用视觉Transformer的中间特征进行人脸防伪|Mika Feng, Koichi Ito, Takafumi Aoki, Tetsushi Ohki, Masakatsu Nishigaki|<http://arxiv.org/pdf/2505.24402v1>|提出一种基于ViT的中间特征，结合数据增强的对抗攻击检测方法，有效识别伪造人脸。|
|📝 更新|Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications|大型语言模型驱动的分布式集成多模态感知与语义通信|Yubo Peng, Luping Xiang, Bingxin Zhang, Kun Yang|<http://arxiv.org/pdf/2505.18194v2>|提出了一种基于LLM的分布式多模态感知与语义通信框架，有效提升了复杂环境下的感知精度和通信效率。|
|📝 更新|ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts|ExPLoRA：参数高效的扩展预训练以适应域偏移下的视觉Transformer|Samar Khanna, Medhanie Irgau, David B. Lobell, Stefano Ermon|<http://arxiv.org/pdf/2406.10973v4>|[代码](https://samar-khanna.github.io/ExPLoRA); ExPLoRA通过高效的自监督预训练，有效提升视觉Transformer在领域迁移下的迁移学习性能。|
|📝 更新|Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner|增强表格识别的视觉LLMs：一个基准和邻域引导的工具链推理器|Yitong Zhou, Mingyue Cheng, Qingyang Mao, Feiyang Xu, Xin Li|<http://arxiv.org/pdf/2412.20662v3>|提出基准和工具链推理器，显著提升VLLMs在无监督表格识别中的性能。|
|📝 更新|Mamba-R: Vision Mamba ALSO Needs Registers|Mamba-R：视觉Mamba同样需要寄存器|Feng Wang, Jiahao Wang, Sucheng Ren, Guoyizhe Wei, Jieru Mei, Wei Shao, Yuyin Zhou, Alan Yuille .etc.|<http://arxiv.org/pdf/2405.14858v2>|[代码](https://github.com/wangf3014/Mamba-Reg.); 通过引入寄存器并优化其使用，Mamba-R显著提升了Vision Mamba在图像识别任务中的性能和...|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation|非均匀圆柱分割网络：高效激光雷达语义分割|Xuzhi Wang, Wei Feng, Lingdong Kong, Liang Wan|<http://arxiv.org/pdf/2505.24634v2>|[代码](https://github.com/alanWXZ/NUC-Net); 提出NUC-Net，通过非均匀圆柱分割网络高效解决LiDAR语义分割中的计算成本和点分布不均问题。|


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection|MSVCOD：大规模多场景视频伪装目标检测数据集|Shuyong Gao, Yu'ang Feng, Qishan Wang, Lingyi Hong, Xinyu Zhou, Liu Fei, Yan Wang, Wenqiang Zhang|<http://arxiv.org/pdf/2502.13859v2>|构建了大规模多场景视频伪装物体检测数据集MSVCOD，并提出了一种无需额外运动特征融合模块的一体化检...|
|📝 更新|ENACT: Entropy-based Clustering of Attention Input for Reducing the Computational Needs of Object Detection Transformers|ENACT：基于熵的注意力输入聚类以降低目标检测Transformer的计算需求|Giorgos Savathrakis, Antonis Argyros|<http://arxiv.org/pdf/2409.07541v2>|[代码](https://github.com/GSavathrakis/ENACT.); 提出基于熵的注意力输入聚类方法，降低目标检测Transformer的计算需求。|
|📝 更新|WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver Assistance Systems|WTEFNet：高级驾驶辅助系统实时低光物体检测|Hao Wu, Junzhou Chen, Ronghui Zhang, Nengchao Lyu, Hongyu Hu, Yanyong Guo, Tony Z. Qiu|<http://arxiv.org/pdf/2505.23201v2>|WTEFNet通过低光增强、特征提取和自适应融合，实现实时低光场景下的物体检测。|


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Bi-Manual Joint Camera Calibration and Scene Representation|双目联合相机标定与场景表示|Haozhan Tang, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi|<http://arxiv.org/pdf/2505.24819v1>|提出了一种无需标记的联合相机标定和场景表示框架，实现多机器人协同操作。|
|📝 更新|Good Keypoints for the Two-View Geometry Estimation Problem|优质关键点用于双视图几何估计问题|Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer|<http://arxiv.org/pdf/2503.18767v2>|提出BoNeSS-ST关键点检测器，优化两视图几何估计中的关键点选择，提升单应性估计精度。|
|🆕 发布|RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement|RT-X Net：用于低光图像增强的RGB-热成像跨注意力网络|Raman Jha, Adithya Lenka, Mani Ramanagopal, Aswin Sankaranarayanan, Kaushik Mitra|<http://arxiv.org/pdf/2505.24705v1>|[代码](https://github.com/jhakrraman/rt-xnet.); 提出RT-X Net，融合RGB和热图像，有效提升夜间图像增强效果。|
|🆕 发布|6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly|基于先验知识集成的点云数据6D姿态估计：自主拆解案例研究|Chengzhi Wu, Hao Fu, Jan-Philipp Kaiser, Erik Tabuchi Barczak, Julius Pfrommer, Gisela Lanza, Michael Heizmann, Jürgen Beyerer|<http://arxiv.org/pdf/2505.24669v1>|通过整合先验知识，提出了一种用于点云数据上6D姿态估计的全面管道，有效解决了螺栓检测和自动拆解问题。|
|🆕 发布|Leadership Assessment in Pediatric Intensive Care Unit Team Training|儿科重症监护室团队培训中的领导力评估|Liangyang Ouyang, Yuki Sakai, Ryosuke Furuta, Hisataka Nozawa, Hikoro Matsui, Yoichi Sato|<http://arxiv.org/pdf/2505.24389v1>|开发基于自视角视觉的自动化分析框架，以评估儿科重症监护室团队领导技能。|
|📝 更新|MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection|MonoCoP：单目3D目标检测的预测链|Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu|<http://arxiv.org/pdf/2505.04594v4>|MonoCoP通过链式预测策略，有效提升了单目3D物体检测的深度估计精度。|
|📝 更新|Refinement Module based on Parse Graph of Feature Map for Human Pose Estimation|基于特征图解析图的精炼模块用于人体姿态估计|Shibang Liu, Xuemei Xie, Guangming Shi|<http://arxiv.org/pdf/2501.11069v6>|设计了一种基于特征图解析图的优化模块，提升了人体姿态估计的准确性和适应性。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification|SASP：基于条带感知的细粒度鸟类图像分类|Zheng Wang|<http://arxiv.org/pdf/2505.24380v1>|提出了一种基于条带感知的空间感知框架，有效提升了鸟类图像分类的准确性和鲁棒性。|


## 生成式视觉模型 (Generative Visual Modeling)


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction|任意文本引导的3D细节化构建|Qimin Chen, Yuezhi Yang, Yifang Wang, Vladimir G. Kim, Siddhartha Chaudhuri, Hao Zhang, Zhiqin Chen|<http://arxiv.org/pdf/2505.20431v2>|提出了一种基于文本指导的3D细节化模型，可快速将粗略3D形状转换为高质量资产。|
|📝 更新|Absolute Coordinates Make Motion Generation Easy|绝对坐标使运动生成变得简单|Zichong Meng, Zeyu Han, Xiaogang Peng, Yiming Xie, Huaizu Jiang|<http://arxiv.org/pdf/2505.19377v2>|提出使用绝对坐标简化运动生成，显著提升运动精度和下游任务应用。|
|📝 更新|HandCraft: Anatomically Correct Restoration of Malformed Hands in Diffusion Generated Images|《HandCraft：基于扩散生成图像的畸形手解剖正确修复》|Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell|<http://arxiv.org/pdf/2411.04332v3>|HandCraft通过构建手部掩码和深度图，实现了对生成图像中畸形手的解剖修复。|
|📝 更新|LaWa: Using Latent Space for In-Generation Image Watermarking|LaWa：利用潜在空间进行生成图像水印|Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi, Yong Zhang|<http://arxiv.org/pdf/2408.05868v3>|LaWa通过在生成模型中嵌入水印，实现了对AI生成图像的不可见水印，提高了图像的鲁棒性和感知质量。|
|📝 更新|GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers|GenHancer：不完美的生成模型实际上是秘密强大的视觉增强器|Shijie Ma, Yuying Ge, Teng Wang, Yuxin Guo, Yixiao Ge, Ying Shan|<http://arxiv.org/pdf/2503.19480v2>|GenHancer通过优化生成模型条件和使用轻量级降噪器，有效提升了视觉表征增强能力。|
|🆕 发布|AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion|AdaHuman：基于组合多视图扩散的可动详细3D人体生成|Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal|<http://arxiv.org/pdf/2505.24877v1>|AdaHuman通过组合多视角扩散模型和3DGS细化模块，实现了从单张图片生成高质量、可动画的3D人...|
|🆕 发布|GenSpace: Benchmarking Spatially-Aware Image Generation|空间感知图像生成基准：GenSpace|Zehan Wang, Jiayang Xu, Ziang Zhang, Tianyu Pan, Chao Du, Hengshuang Zhao, Zhou Zhao|<http://arxiv.org/pdf/2505.24870v1>|提出GenSpace评估框架，揭示AI图像生成在空间感知上的局限性。|
|🆕 发布|ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL|ReasonGen-R1：通过SFT和RL实现自回归图像生成模型的CoT|Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen .etc.|<http://arxiv.org/pdf/2505.24875v1>|通过结合思维链推理和强化学习，ReasonGen-R1显著提升了自回归图像生成模型的生成质量。|
|🆕 发布|MiniMax-Remover: Taming Bad Noise Helps Video Object Removal|最小-最大移除器：驯服不良噪声有助于视频物体去除|Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, Kam-Fai Wong|<http://arxiv.org/pdf/2505.24873v1>|MiniMax-Remover通过简化模型和优化策略，实现了高效且高质量的视频物体去除。|
|🆕 发布|Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation|绘制所有你的想象：复杂指令图像生成的全面基准和代理框架|Yucheng Zhou, Jiahao Yuan, Qianning Wang|<http://arxiv.org/pdf/2505.24787v1>|[代码](https://github.com/yczhou001/LongBench-T2I.); 构建了LongBench-T2I基准和Plan2Gen框架，以评估和促进复杂指令驱动的图像生成。|
|🆕 发布|DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds|梦舞：通过修复稳定高斯世界动画角色艺术|Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, Guosheng Lin, Zhigang Tu|<http://arxiv.org/pdf/2505.24733v1>|DreamDance通过结合场景和姿态感知的修复技术，实现了稳定、连贯的角色动画。|
|📝 更新|HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator|HEIE：基于MLLM的分层可解释AIGC图像可信度评估器|Fan Yang, Ru Zhen, Jianing Wang, Yanhao Zhang, Haoxiang Chen, Haonan Lu, Sicheng Zhao, Guiguang Ding|<http://arxiv.org/pdf/2411.17261v2>|[代码](https://yfthu.github.io/HEIE); 提出了一种基于多模态大语言模型的层次化可解释AIGC图像可信度评估方法，有效解决图像质量问题和可解释...|
|🆕 发布|TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching|肿瘤基因：基于边界感知的肿瘤掩码合成与校正流匹配|Shengyuan Liu, Wenting Chen, Boyun Zheng, Wentao Pan, Xiang Li, Yixuan Yuan|<http://arxiv.org/pdf/2505.24687v1>|TumorGen通过边界感知和高效匹配技术，实现了高效且逼真的肿瘤数据合成。|
|📝 更新|DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space|DCTdiff：DCT空间中图像生成建模的有趣特性|Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Wenshuo Chen, Albert Ali Salah .etc.|<http://arxiv.org/pdf/2412.15032v2>|[代码](https://github.com/forever208/DCTdiff.); DCTdiff通过在DCT空间建模图像，显著提升了生成质量和训练效率，同时揭示了图像建模的有趣性质。|
|📝 更新|AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction|《AnimeGamer：基于下一游戏状态预测的无尽动漫生活模拟》|Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan|<http://arxiv.org/pdf/2504.01014v2>|[代码](https://github.com/TencentARC/AnimeGamer.); AnimeGamer通过预测下一游戏状态，实现了具有动态动画和语境一致性的无限动漫生活模拟。|
|📝 更新|TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation|剧院生成：使用大型语言模型进行一致的多轮图像生成的人物管理|Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li .etc.|<http://arxiv.org/pdf/2404.18919v2>|TheaterGen通过整合LLM和T2I模型，实现多轮图像生成，显著提升语义和上下文一致性。|
|📝 更新|DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers|DiffDecompose：通过扩散变换进行逐层分解的Alpha合成图像|Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song|<http://arxiv.org/pdf/2505.21541v2>|[代码](https://github.com/Wangzt1121/DiffDecompose.); 提出了一种基于扩散模型的图像分解方法，有效解决了半透明层分解难题。|
|📝 更新|GPT4Point: A Unified Framework for Point-Language Understanding and Generation|GPT4Point：点云-语言理解和生成统一框架|Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao|<http://arxiv.org/pdf/2312.02980v2>|提出GPT4Point，一种统一3D点云语言理解和生成框架，突破MLLM在3D理解上的局限。|
|🆕 发布|UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation|UniGeo：驯服视频扩散以实现统一一致几何估计|Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, Xiaojuan Qi|<http://arxiv.org/pdf/2505.24521v1>|利用视频生成模型内在一致性，实现跨帧几何属性一致估计。|
|🆕 发布|Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation|原因-SVG：混合奖励强化学习在矢量图形生成中的“啊哈”时刻|Ximing Xing, Yandong Guan, Jing Zhang, Dong Xu, Qian Yu|<http://arxiv.org/pdf/2505.24499v1>|Reason-SVG通过“绘图思维”和混合奖励强化学习，显著提升了LLM生成高质量SVG的能力。|
|📝 更新|CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features|CAE-Net：基于卷积和注意力机制，结合空间和频域特征的通用深度伪造图像检测|Kafi Anan, Anindya Bhattacharjee, Ashir Intesher, Kaidul Islam, Abrar Assaeem Fuad, Utsab Saha, Hafiz Imtiaz|<http://arxiv.org/pdf/2502.10682v2>|提出了一种基于卷积和注意力机制的深度伪造图像检测方法，有效解决了类别不平衡问题并提高了检测准确率。|
|🆕 发布|SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds|SPPSFormer：基于Superpoint的高质量Transformer，用于点云中的屋顶平面实例分割|Cheng Zeng, Xiatian Qi, Chi Chen, Kai Sun, Wangle Zhang, Yuxuan Liu, Yan Meng, Bisheng Yang|<http://arxiv.org/pdf/2505.24475v1>|提出了一种基于高质量Superpoint和Transformer的屋顶平面实例分割方法，显著提升了分...|
|🆕 发布|Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields|图流匹配：通过邻域感知流场增强图像生成|Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber|<http://arxiv.org/pdf/2505.24434v1>|提出Graph Flow Matching，通过聚合邻域信息提升图像生成质量。|
|📝 更新|CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner|CraftsMan3D：基于3D原生生成和交互式几何精修的高保真网格生成|Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long|<http://arxiv.org/pdf/2405.14979v4>|[代码](https://github.com/wyysf-98/CraftsMan); CraftsMan3D通过3D原生生成和交互式几何细化，实现了高保真3D模型的高效生成和细节优化。|
|🆕 发布|InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing|InteractAnything：通过LLM反馈和物体功能解析实现零样本人机交互合成|Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang|<http://arxiv.org/pdf/2505.24315v1>|提出了一种无需特定数据集训练的零样本3D人-物交互生成框架，通过LLM反馈和物体属性解析实现自然交互...|
|🆕 发布|Category-aware EEG image generation based on wavelet transform and contrast semantic loss|基于小波变换和对比语义损失的类别感知脑电图图像生成|Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa|<http://arxiv.org/pdf/2505.24301v1>|[代码](https://github.com/zes0v0inn/DWT_EEG_Reconstruction); 提出了一种基于波变换和对比语义损失的Transformer编码器，显著提升了EEG信号到图像的生成和...|
|🆕 发布|Interactive Video Generation via Domain Adaptation|通过领域自适应的交互式视频生成|Ishaan Rawal, Suryansh Kumar|<http://arxiv.org/pdf/2505.24253v1>|通过引入域适应和时序先验，该论文提升了交互式视频生成中运动轨迹控制和视觉质量。|
|🆕 发布|LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework|LTM3D：利用自回归扩散框架连接标记空间以实现条件3D生成|Xin Kang, Zihan Zheng, Lei Chu, Yue Gao, Jiahao Li, Hao Pan, Xuejin Chen, Yan Lu|<http://arxiv.org/pdf/2505.24245v1>|LTM3D通过结合扩散和自回归模型，实现了条件3D形状生成，提高了生成形状的准确性和灵活性。|
|🆕 发布|Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin|利用二阶Levenberg-Marquardt-Langevin释放扩散采样中的高质量图像生成|Fangyikang Wang, Hubery Yin, Lei Qian, Yinan Li, Shaobin Zhuang, Huminhao Zhu, Yilin Zhang, Yanlong Tang .etc.|<http://arxiv.org/pdf/2505.24222v1>|引入Levenberg-Marquardt-Langevin方法，显著提升扩散模型图像生成质量。|
|📝 更新|dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph|双条件生成对抗网络：从单一形态进行人脸去形变|Nitish Shukla, Arun Ross|<http://arxiv.org/pdf/2411.14494v4>|提出dc-GAN，一种基于形态图像和嵌入的生成对抗网络，有效解决人脸去变形问题。|
|🆕 发布|Reasoning Can Hurt the Inductive Abilities of Large Language Models|推理可能损害大型语言模型的归纳能力|Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang|<http://arxiv.org/pdf/2505.24225v1>|研究发现，过度推理会损害大型语言模型的归纳能力，并提出结构化干预来优化推理步骤。|
|📝 更新|Diagram-Driven Course Questions Generation|基于图表的课程问题生成|Xinyu Zhang, Lingling Zhang, Yanrui Wu, Muye Huang, Wenjun Wu, Bo Li, Shaowei Wang, Basura Fernando .etc.|<http://arxiv.org/pdf/2411.17771v4>|提出DDCQG任务，构建DiagramQG数据集，并设计HKI-DDCQG框架生成课程相关的问题。|
|📝 更新|Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation|展示-1：将像素和潜在扩散模型结合用于文本到视频生成|David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou|<http://arxiv.org/pdf/2309.15818v3>|[代码](https://github.com/showlab/Show-1.); 提出Show-1混合模型，结合像素和潜在扩散模型，高效生成精确文本-视频匹配的高清视频。|
|📝 更新|MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation|MTVCrafter：面向开放世界人类图像动画的4D运动标记化|Yanbo Ding, Xirui Hu, Zhizhi Guo, Chi Zhang, Yali Wang|<http://arxiv.org/pdf/2505.10238v4>|[代码](https://github.com/DINGYANB/MTVCrafter.); MTVCrafter通过直接建模4D运动序列，为开放世界人类图像动画提供更灵活和可控的解决方案。|
|📝 更新|Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning|定理验证的逆向思维问题生成用于几何推理|Linger Deng, Linghao Zhu, Yuliang Liu, Yu Wang, Qunyi Xie, Jingjing Wu, Gang Zhang, Yingying Zhu .etc.|<http://arxiv.org/pdf/2410.17885v4>|提出TR-CoT框架，通过定理验证反向推理生成几何推理问题，提升模型几何推理能力。|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|The Structural Safety Generalization Problem|结构安全泛化问题|Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri .etc.|<http://arxiv.org/pdf/2504.09712v2>|提出结构安全泛化问题解决方案，通过结构重写防护栏提升AI安全。|
|🆕 发布|ViStoryBench: Comprehensive Benchmark Suite for Story Visualization|ViStoryBench：故事可视化全面基准套件|Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang .etc.|<http://arxiv.org/pdf/2505.24862v1>|构建了ViStoryBench基准，全面评估故事可视化模型的性能。|
|📝 更新|V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow|V2SFlow：基于语音分解和校正流的视频到语音生成|Jeongsoo Choi, Ji-Hoon Kim, Jinyu Li, Joon Son Chung, Shujie Liu|<http://arxiv.org/pdf/2411.19486v2>|[代码](https://github.com/kaistmm/V2SFlow); V2SFlow通过分解语音信号并使用Transformer架构的流匹配解码器，从无声视频中直接生成自...|
|🆕 发布|LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text|LegalEval-Q：用于评估LLM生成法律文本质量的新基准|Li yunhan, Wu gengshen|<http://arxiv.org/pdf/2505.24826v1>|[代码](https://github.com/lyxx3rd/LegalEval-Q.); 提出LegalEval-Q基准，评估LLM生成法律文本质量，揭示训练数据优化局限。|
|🆕 发布|Reinforcing Video Reasoning with Focused Thinking|强化视频推理的专注思考|Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang .etc.|<http://arxiv.org/pdf/2505.24718v1>|[代码](https://github.com/longmalongma/TW-GRPO); 提出TW-GRPO框架，通过聚焦思维和密集奖励粒度提升视频推理的准确性和效率。|
|📝 更新|Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization|高效语义引导的提示组织通用目标劫持|Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu|<http://arxiv.org/pdf/2405.14189v2>|提出一种高效语义引导的提示组织方法，实现便捷的通用目标劫持攻击。|
|📝 更新|AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation|AutoStudio：构建多轮交互式图像生成中的统一主题|Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang|<http://arxiv.org/pdf/2406.01388v3>|AutoStudio通过多智能体框架实现多轮交互式图像生成，保持主题一致性并提升生成质量。|
|🆕 发布|VUDG: A Dataset for Video Understanding Domain Generalization|视频理解领域泛化数据集：VUDG|Ziyi Wang, Zhi Gao, Boxuan Yu, Zirui Dai, Yuxiang Song, Qingyuan Lu, Jin Chen, Xinxiao Wu|<http://arxiv.org/pdf/2505.24346v1>|构建了VUDG数据集，评估视频理解中的领域泛化能力，揭示模型对数据分布变化的鲁棒性差异。|
|🆕 发布|Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders|线程关键帧与叙事：作为强大长视频理解者的MLLMs|Bo Fang, Wenhao Wu, Qiangqiang Wu, Yuxin Song, Antoni B. Chan|<http://arxiv.org/pdf/2505.24158v1>|提出Nar-KFC模块，通过关键帧与叙事结合，有效提升长视频理解能力。|
|🆕 发布|ComposeAnything: Composite Object Priors for Text-to-Image Generation|《ComposeAnything：用于文本到图像生成的复合对象先验》|Zeeshan Khan, Shizhe Chen, Cordelia Schmid|<http://arxiv.org/pdf/2505.24086v1>|ComposeAnything通过引入2.5D语义布局，显著提升了文本到图像生成中复杂物体组合的生成...|


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|$\textit{Revelio}$: Interpreting and leveraging semantic information in diffusion models|Revelio：在扩散模型中解释和利用语义信息|Dahye Kim, Xavier Thomas, Deepti Ghadiyaram|<http://arxiv.org/pdf/2411.16725v2>|[代码](https://github.com/revelio-diffusion/revelio); 揭示并利用扩散模型中的语义信息，通过k稀疏自动编码器提升模型可解释性。|
|📝 更新|Autoregression-free video prediction using diffusion model for mitigating error propagation|无自回归扩散模型用于缓解误差传播的视频预测|Woonho Ko, Jin Bok Park, Il Yong Chun|<http://arxiv.org/pdf/2505.22111v2>|提出ARFree视频预测框架，利用扩散模型避免误差传播，提升长期视频预测性能。|
|📝 更新|Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models|渐进式提示细化以提高文本到图像生成模型的对齐|Ketan Suhaas Saichandran, Xavier Thomas, Prakhar Kaushik, Deepti Ghadiyaram|<http://arxiv.org/pdf/2503.17794v4>|提出SCoPE方法，通过逐步细化输入提示，提升文本到图像生成模型的对齐精度。|
|📝 更新|DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On|扩散趋势：虚拟试衣的极简方法|Wengyi Zhan, Mingbao Lin, Shuicheng Yan, Rongrong Ji|<http://arxiv.org/pdf/2412.14465v2>|DiffusionTrend通过无需重新训练扩散模型，实现了虚拟试衣的细节捕捉与高效生成。|
|📝 更新|On the Design Fundamentals of Diffusion Models: A Survey|关于扩散模型设计原理的综述|Ziyi Chang, George Alex Koulieris, Hyung Jin Chang, Hubert P. H. Shum|<http://arxiv.org/pdf/2306.04542v4>|该论文全面综述了扩散模型设计要素，为组件分析和模型实现提供细致视角。|
|📝 更新|One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models|一步到位：用于文本到图像扩散模型的稀疏自编码器|Viacheslav Surkov, Chris Wendler, Antonio Mari, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre, David Bau|<http://arxiv.org/pdf/2410.22366v3>|利用稀疏自编码器学习文本到图像模型的可解释特征，提升理解和操控模型内部机制。|
|📝 更新|Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives|多模态大型语言模型时代的图像标题评估：挑战与未来展望|Sara Sarto, Marcella Cornia, Rita Cucchiara|<http://arxiv.org/pdf/2503.14604v2>|分析了多模态LLMs时代图像描述评估的挑战，提出了改进现有评估指标和未来研究方向的建议。|
|🆕 发布|Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data|基于晶格变形框架和扩散增强合成数据的农业场景中类别级6D目标姿态估计|Marios Glytsos, Panagiotis P. Filntisis, George Retsinas, Petros Maragos|<http://arxiv.org/pdf/2505.24636v1>|提出PLANTPose，一种基于RGB输入的农业场景中类别级6D姿态估计框架，有效处理形状、大小和纹...|
|📝 更新|MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications|MedDiff-FM：一种适用于多种医学图像应用的扩散基础模型|Yongrui Yu, Yannian Gu, Shaoting Zhang, Xiaofan Zhang|<http://arxiv.org/pdf/2410.15432v2>|提出MedDiff-FM，一种基于扩散模型的通用医学图像应用基础模型，解决多任务需求。|
|🆕 发布|SORCE: Small Object Retrieval in Complex Environments|复杂环境中的小目标检索：SORCE|Chunxu Liu, Chi Xie, Xiaxu Chen, Wei Li, Feng Zhu, Rui Zhao, Limin Wang|<http://arxiv.org/pdf/2505.24441v1>|提出SORCE，通过多模态大语言模型和区域提示，有效检索复杂环境中的小物体。|
|🆕 发布|EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering|EasyText：可控扩散Transformer的多语言文本渲染|Runnan Lu, Yuxuan Zhang, Jiaming Liu, Haofan Wang, Yiren Song|<http://arxiv.org/pdf/2505.24417v1>|EasyText通过Diffusion Transformer实现可控多语言文本渲染，显著提升视觉质...|
|📝 更新|A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis|多模态文本-图像分析中自监督对比学习的综述|Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif|<http://arxiv.org/pdf/2503.11101v3>|该论文综述了自监督对比学习在文本-图像分析中的应用，提高了图像理解和文本分析能力。|
|🆕 发布|IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models|IRBridge：利用预训练生成扩散模型解决图像恢复桥问题|Hanting Wang, Tao Jin, Wang Lin, Shulei Wang, Hai Huang, Shengpeng Ji, Zhou Zhao|<http://arxiv.org/pdf/2505.24406v1>|IRBridge通过利用预训练生成模型，有效解决了图像修复中计算成本高和性能有限的问题。|
|🆕 发布|Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering|网格-LOGAT：基于网格的局部和全局区域转录用于视频问答|Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi|<http://arxiv.org/pdf/2505.24371v1>|Grid-LOGAT通过结合视觉语言模型和大型语言模型，在视频问答中实现基于网格的局部和全局区域转录...|
|🆕 发布|DisTime: Distribution-based Time Representation for Video Large Language Models|DisTime：基于分布的视频大型语言模型的时间表示|Yingsen Zeng, Zepeng Huang, Yujie Zhong, Chengjian Feng, Jie Hu, Lin Ma, Yang Liu|<http://arxiv.org/pdf/2505.24329v1>|[代码](https://github.com/josephzpng/DisTime.); DisTime通过创建连续时间嵌入空间和分布式时间解码器，有效提升视频大语言模型的时间理解能力。|
|🆕 发布|Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames|《视域之外，仍处语境之中？VLMs跨断帧的以自我为中心的空间推理》|Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel|<http://arxiv.org/pdf/2505.24257v1>|提出Disjoint-3DQA基准，评估VLMs在多帧场景中理解物体空间关系的能力。|
|📝 更新|When Are Concepts Erased From Diffusion Models?|当概念从扩散模型中被抹去时？|Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen|<http://arxiv.org/pdf/2505.17013v4>|提出评估方法，揭示扩散模型中概念消除的全面性及其与鲁棒性的权衡。|
|📝 更新|T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models|T2VUnlearning：文本到视频扩散模型的概念擦除方法|Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li|<http://arxiv.org/pdf/2505.17550v2>|[代码](https://github.com/VDIGPKU/T2VUnlearning.git); 提出T2VUnlearning方法，有效消除T2V模型中特定有害概念，同时保持其他内容生成能力。|
|📝 更新|Masked Autoencoders Are Effective Tokenizers for Diffusion Models|掩码自编码器是扩散模型有效的标记化器|Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu .etc.|<http://arxiv.org/pdf/2502.03444v2>|提出MAETok，通过掩码建模的自动编码器学习语义丰富的潜在空间，显著提升扩散模型生成质量。|
|🆕 发布|Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation|基于置信度边距加权伪标签的Shuffle PatchMix增强用于增强无源域适应|Prasanna Reddy Pulakurthi, Majid Rabbani, Jamison Heard, Sohail Dianat, Celso M. de Melo, Raghuveer Rao|<http://arxiv.org/pdf/2505.24216v1>|[代码](https://github.com/PrasannaPulakurthi/SPM); 提出了一种结合Shuffle PatchMix增强和置信度加权伪标签的源免费域自适应新方法，显著提升...|
|🆕 发布|STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models|STORK：提升扩散和流匹配模型的中间NFE采样的保真度|Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu|<http://arxiv.org/pdf/2505.24210v1>|[代码](https://github.com/ZT220501/STORK.); 提出STORK方法，有效提升中NFE采样在扩散和流匹配模型中的图像生成质量。|
|📝 更新|ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization|ErpGS：基于3D高斯正则化的等角图像渲染增强|Shintaro Ito, Natsuki Takama, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki|<http://arxiv.org/pdf/2505.19883v2>|提出ErpGS，通过3D高斯正则化提升等角图像渲染精度，解决360度相机投影模型导致的畸变问题。|
|📝 更新|Graph-guided Cross-composition Feature Disentanglement for Compositional Zero-shot Learning|图引导的跨组合特征解耦用于组合零样本学习|Yuxia Geng, Runkai Zhu, Jiaoyan Chen, Jintai Chen, Xiang Chen, Zhuo Chen, Shuofei Qiao, Yuxiang Wang .etc.|<http://arxiv.org/pdf/2408.09786v2>|[代码](https://github.com/zhurunkai/DCDA.); 提出了一种基于图引导的跨组合特征解耦方法，显著提升了组合零样本学习性能。|


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape|重保留：通过注意力统计重塑的超稀疏视觉生成|Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu|<http://arxiv.org/pdf/2505.22918v2>|[代码](https://github.com/cccrrrccc/Re-ttention); 提出Re-ttention方法，通过利用扩散模型的时间冗余，实现超稀疏视觉生成，大幅降低计算复杂度。|
|📝 更新|Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis|视频-MME：视频分析中多模态LLMs的首个全面评估基准|Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou .etc.|<http://arxiv.org/pdf/2405.21075v3>|构建了首个全面评估多模态LLMs在视频分析中性能的基准，推动了对长序列和多模态数据的处理研究。|
|📝 更新|Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation|增强与跳过：一种简单无指导的少数族裔生成扩散方法|Soobin Um, Beomsu Kim, Jong Chul Ye|<http://arxiv.org/pdf/2502.06516v2>|[代码](https://github.com/soobin-um/BnS.); 提出了一种简单高效的Boost-and-Skip方法，无需指导生成少数族裔样本。|
|📝 更新|A Survey on Text-Driven 360-Degree Panorama Generation|文本驱动360度全景图生成综述|Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue|<http://arxiv.org/pdf/2502.14799v2>|[代码](https://littlewhitesea.github.io/Text-Driven-Pano-Gen); 提出了一种从文本描述直接生成360度全景图的方法，简化了传统内容制作流程。|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Tackling View-Dependent Semantics in 3D Language Gaussian Splatting|解决3D语言高斯分层中的视角依赖语义问题|Jiazhong Cen, Xudong Zhou, Jiemin Fang, Changsong Wen, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian|<http://arxiv.org/pdf/2505.24746v1>|[代码](https://github.com/SJTU-DeepVisionLab/LaGa.); 提出LaGa方法，通过跨视角语义连接解决3D场景中视角依赖语义问题，显著提升语义理解精度。|
|🆕 发布|SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping|SR3D：释放单视图3D重建以实现透明和镜面物体抓取|Mingxu Zhang, Xiaoqi Li, Jiahui Xu, Kaichen Zhou, Hojin Bae, Yan Shen, Chuyan Xiong, Jiaming Liu .etc.|<http://arxiv.org/pdf/2505.24305v1>|SR3D通过单视图3D重建技术，实现了透明和镜面物体的高精度抓取。|
|📝 更新|Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery|朝向眼科手术中手-器械交互的动态3D重建|Ming Hu, Zhengdi Yu, Feilong Tang, Kaiwen Chen, Yulong Li, Imran Razzak, Junjun He, Tolga Birdal .etc.|<http://arxiv.org/pdf/2505.17677v2>|构建了OphNet-3D数据集，提出H-Net和OH-Net模型，显著提升了眼科手术中手和器械的3D...|


### 视觉定位与映射 (Visual Localization & Mapping)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Black-box Adversarial Attacks on CNN-based SLAM Algorithms|基于CNN的SLAM算法的黑盒对抗攻击|Maria Rafaela Gkeka, Bowen Sun, Evgenia Smirni, Christos D. Antonopoulos, Spyros Lalis, Nikolaos Bellas|<http://arxiv.org/pdf/2505.24654v1>|提出黑盒对抗攻击方法，揭示CNN-SLAM算法对深度攻击的脆弱性。|


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|GARLIC: GAussian Representation LearnIng for spaCe partitioning|GARLIC：用于空间划分的Gaussian表示学习|Panagiotis Rigas, Panagiotis Drivas, Charalambos Tzamos, Ioannis Chamodrakas, George Ioannakis, Leonidas J. Guibas, Ioannis Z. Emiris|<http://arxiv.org/pdf/2505.24608v1>|GARLIC通过高斯表示学习，高效处理高维空间，实现快速检索和分类。|
|🆕 发布|Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation|通过高质量连续几何表示连接3D异常定位与修复|Bozhong Zheng, Jinye Gan, Xiaohao Xu, Wenqiao Li, Xiaonan Huang, Na Ni, Yingna Wu|<http://arxiv.org/pdf/2505.24431v1>|[代码](https://github.com/ZZZBBBZZZ/PASDF); 提出PASDF框架，通过连续几何表示实现3D异常检测与修复，提升定位精度和修复质量。|


## 时序视觉分析 (Temporal Visual Analysis)


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution|FlashDepth：2K分辨率下的实时流媒体视频深度估计|Gene Chou, Wenqi Xian, Guandao Yang, Mohamed Abdelfattah, Bharath Hariharan, Noah Snavely, Ning Yu, Paul Debevec|<http://arxiv.org/pdf/2504.07093v2>|[代码](https://github.com/Eyeline-Research/FlashDepth); FlashDepth提出了一种实时高分辨率视频深度估计方法，显著提升边界清晰度和速度。|
|🆕 发布|Time Blindness: Why Video-Language Models Can't See What Humans Can?|时间盲视：为何视频-语言模型无法看到人类所能看到的？|Ujjwal Upadhyay, Mukul Ranjan, Zhiqiang Shen, Mohamed Elhoseiny|<http://arxiv.org/pdf/2505.24867v1>|[代码](https://timeblindness.github.io/.); 揭示了视频语言模型在时间感知上的局限性，并提出了SpookyBench基准以促进时间模式识别研究。|
|🆕 发布|SiLVR: A Simple Language-based Video Reasoning Framework|SiLVR：一种基于简单语言的视频推理框架|Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius|<http://arxiv.org/pdf/2505.24869v1>|[代码](https://github.com/CeeZh/SILVR.); 提出SiLVR框架，通过语言描述和推理模型解决复杂视频理解问题。|
|🆕 发布|Learning reusable concepts across different egocentric video understanding tasks|跨不同自视角视频理解任务学习可重用概念|Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Tatiana Tommasi, Giuseppe Averta|<http://arxiv.org/pdf/2505.24690v1>|提出Hier-EgoPack框架，通过跨任务学习，使机器人具备携带和运用通用技能的能力。|
|🆕 发布|Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification|基于3D视频分类的森林机械作业时空分析|Maciej Wielgosz, Simon Berg, Heikki Korpunen, Stephan Hoffmann|<http://arxiv.org/pdf/2505.24375v1>|提出了一种基于3D视频分类的框架，有效识别森林机械操作，减轻传统时间研究的工作负担。|
|📝 更新|VideoRoPE: What Makes for Good Video Rotary Position Embedding?|视频RoPE：什么因素造就了优秀的视频旋转位置嵌入？|Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan .etc.|<http://arxiv.org/pdf/2502.05173v3>|[代码](https://github.com/Wiselnn570/VideoRoPE); 提出VideoRoPE，通过3D结构优化RoPE在视频中的时空关系，提升视频检索和理解的性能。|


### 视频目标跟踪 (Video Object Tracking)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance|EasyREG：利用增强现实设备实现简易基于深度标记的无标记配准和跟踪用于手术引导|Yue Yang, Christoph Leuze, Brian Hargreaves, Bruce Daniel, Fred Baik|<http://arxiv.org/pdf/2504.09498v2>|提出了一种基于AR设备深度传感器的无标记手术导航系统，实现高精度定位和实时跟踪。|
|📝 更新|Camouflaged Object Tracking: A Benchmark|伪装目标跟踪：一个基准|Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Defeng Huang, Huikai Shao, Qijun Zhao, Shuiwang Li|<http://arxiv.org/pdf/2408.13877v4>|[代码](https://github.com/openat25/HIPTrack-MLS.); 构建了首个伪装物体跟踪数据集，并提出HiPTrack-MLS框架提升跟踪性能。|
|📝 更新|SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness for Hyperspectral Object Tracking|SSF-Net：具有光谱角度感知的空谱融合网络用于高光谱目标跟踪|Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du, Jing Tian|<http://arxiv.org/pdf/2403.05852v2>|提出了一种融合空间光谱信息和光谱角度感知的跟踪网络，有效提升了超光谱目标跟踪的准确性。|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Reading Recognition in the Wild|野外场景中的阅读识别|Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort .etc.|<http://arxiv.org/pdf/2505.24848v1>|构建首个大规模多模态阅读识别数据集，提出灵活的Transformer模型实现阅读行为识别。|
|🆕 发布|EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding|自我-外中心手术室活动理解数据集：EgoExOR|Ege Özsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab|<http://arxiv.org/pdf/2505.24287v1>|构建首个融合第一人称和第三人称视角的手术室数据集，以提升手术活动理解。|


### 时序建模与预测 (Temporal Modeling & Prediction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization|基于LLM的查询扩展以增强语言驱动动作定位中的边界预测|Zirui Shang, Xinxiao Wu, Shuo Yang|<http://arxiv.org/pdf/2505.24282v1>|利用LLM扩展查询，结合语义相似度和时间距离建模，提升语言驱动动作定位边界预测的稳定性。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI|对比不变的自监督胎盘MRI分割|Xinliu Zhong, Ruiying Liu, Emily S. Nichols, Xuzhe Zhang, Andrew F. Laine, Emma G. Duerden, Yun Wang|<http://arxiv.org/pdf/2505.24739v1>|提出了一种利用多回波T2*-加权MRI进行胎盘分割的对比不变自监督分割框架，有效提高了分割准确性。|
|🆕 发布|A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning|基于跨分支融合的对比学习框架用于点云自监督学习|Chengzhi Wu, Qianliang Huang, Kun Jin, Julius Pfrommer, Jürgen Beyerer|<http://arxiv.org/pdf/2505.24641v1>|提出了一种基于跨分支融合的对比学习框架，有效提升了点云自监督学习的性能。|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images|超越美丽图片：结合单图像和多图像超分辨率处理Sentinel-2图像|Aditya Retnanto, Son Le, Sebastian Mueller, Armin Leitner, Michael Riffler, Konrad Schindler, Yohan Iddawela|<http://arxiv.org/pdf/2505.24799v2>|提出SEN4X混合超分辨率架构，结合单图和多图技术提升Sentinel-2图像分辨率，显著改善城市土...|
|🆕 发布|CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning|CL-LoRA：无重放类增量学习的持续低秩自适应|Jiangpeng He, Zhihao Duan, Fengqing Zhu|<http://arxiv.org/pdf/2505.24816v1>|提出CL-LoRA，通过双重适配器架构实现无重放类增量学习，有效降低参数冗余并提升模型性能。|
|📝 更新|Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness|对抗剪枝：对抗鲁棒性剪枝方法综述与基准|Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio, Giorgio Giacinto, Fabio Roli|<http://arxiv.org/pdf/2409.01249v2>|[代码](https://github.com/pralab/AdversarialPruningBenchmark); 构建了对抗鲁棒性剪枝方法的分类框架和公平评估基准，以解决现有方法的比较难题。|
|🆕 发布|Optimal Weighted Convolution for Classification and Denosing|最优加权卷积用于分类和去噪|Simone Cammarasana, Giuseppe Patanè|<http://arxiv.org/pdf/2505.24558v1>|[代码](https://github.com/cammarasana123/weightedConvolution2.0.); 提出了一种加权卷积方法，通过空间密度函数增强CNN，提升图像分类和去噪性能。|
|📝 更新|IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method|IDEA：基于逆领域专家适应的主动深度神经网络知识产权保护方法|Chaohui Xu, Qi Cui, Jinxin Dong, Weiyang He, Chip-Hong Chang|<http://arxiv.org/pdf/2410.00059v2>|IDEA通过逆向领域自适应和专家模型混合，实现了主动授权和源追踪，有效保护DNN知识产权。|
|🆕 发布|Optimal Density Functions for Weighted Convolution in Learning Models|最优密度函数在加权卷积学习模型中的应用|Simone Cammarasana, Giuseppe Patanè|<http://arxiv.org/pdf/2505.24527v1>|提出加权卷积方法，通过优化密度函数提升图像处理模型精度。|
|🆕 发布|A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization|基于超像素粒子群优化的新型冠状动脉配准方法|Peng Qi, Wenxi Qu, Tianliang Yao, Haonan Ma, Dylan Wintle, Yinyi Lai, Giorgos Papanastasiou, Chengjia Wang|<http://arxiv.org/pdf/2505.24351v1>|提出了一种基于超级像素粒子群优化的冠状动脉图像配准方法，有效解决了多模态图像配准的挑战。|
|🆕 发布|50 Years of Automated Face Recognition|五十年自动化人脸识别发展历程|Minchul Kim, Anil Jain, Xiaoming Liu|<http://arxiv.org/pdf/2505.24247v1>|概述了50年人脸识别技术发展历程，分析了关键创新，并展望了未来研究方向。|
|📝 更新|Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency|探险家：优化视觉Mamba架构设计以提高效率|Feng Wang, Timing Yang, Yaodong Yu, Sucheng Ren, Guoyizhe Wei, Angtian Wang, Wei Shao, Yuyin Zhou .etc.|<http://arxiv.org/pdf/2410.07599v2>|[代码](https://github.com/wangf3014/Adventurer.); Adventurer通过将图像视为序列并使用单向语言模型学习视觉表示，有效解决了高分辨率图像的内存和...|
|🆕 发布|Boosting All-in-One Image Restoration via Self-Improved Privilege Learning|通过自我改进的特权学习提升一体化图像恢复|Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu|<http://arxiv.org/pdf/2505.24207v1>|[代码](https://github.com/Aitical/SIPL.); 通过自改进特权学习，该论文提出了一种在图像修复中利用模型自身输出进行迭代优化的新方法，显著提升了修复...|


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding|U2-BENCH：在超声理解上对大型视觉-语言模型的基准测试|Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang .etc.|<http://arxiv.org/pdf/2505.17779v2>|构建了首个超声图像理解基准U2-BENCH，评估大型视觉语言模型在医学超声领域的性能。|
|🆕 发布|DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics|DiG-Net：通过超范围动态手势识别提升辅助机器人生活质量|Eran Bamani Beeri, Eden Nissinman, Avishai Sintov|<http://arxiv.org/pdf/2505.24786v1>|DiG-Net通过超远距离动态手势识别，显著提升了辅助机器人的可用性和生活质量。|
|📝 更新|Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data|解耦无源个性化中性目标数据面部表情识别|Masoumeh Sharafi, Emma Ollivier, Muhammad Osama Zeeshan, Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger|<http://arxiv.org/pdf/2503.20771v4>|提出一种利用中性控制视频数据，通过解耦特征生成缺失表情数据，提升面部表情识别模型适应性的方法。|
|🆕 发布|Hyperbolic Dataset Distillation|双曲数据蒸馏|Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama|<http://arxiv.org/pdf/2505.24623v1>|提出了一种基于双曲空间的超参数数据蒸馏方法，有效提升了模型性能和训练稳定性。|
|🆕 发布|SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition|SARD：用于书籍风格文本识别的大规模合成阿拉伯语OCR数据集|Omer Nacar, Yasser Al-Habashi, Serry Sibaee, Adel Ammar, Wadii Boulila|<http://arxiv.org/pdf/2505.24600v1>|构建大规模合成阿拉伯OCR数据集，以解决现有数据集规模和多样性不足的问题。|
|🆕 发布|Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution|基于聚类算子的模型引导网络用于空间光谱超分辨率|Ivan Pereira-Sánchez, Julia Navarro, Ana Belén Petro, Joan Duran|<http://arxiv.org/pdf/2505.24605v1>|[代码](https://github.com/TAMI-UIB/JSSUNet); 提出了一种模型驱动的联合空间光谱超分辨率框架，有效提升了低分辨率高光谱图像重建质量。|
|📝 更新|Scaling Large Motion Models with Million-Level Human Motions|大规模运动模型与百万级人类动作的扩展|Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu|<http://arxiv.org/pdf/2410.03311v3>|[代码](https://beingbeyond.github.io/Being-M0); 构建百万级动作数据集，提出Motionbook方法，提升动作生成模型的性能和灵活性。|
|🆕 发布|Digital twins enable full-reference quality assessment of photoacoustic image reconstructions|数字孪生实现全参考质量评估光声图像重建|Janek Gröhl, Leonid Kunyansky, Jenni Poimala, Thomas R. Else, Francesca Di Cecio, Sarah E. Bohndiek, Ben T. Cox, Andreas Hauptmann|<http://arxiv.org/pdf/2505.24514v1>|利用数字孪生技术，实现了基于全参考的超声光声图像重建算法质量评估。|
|🆕 发布|AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders|AMIA：自动遮挡和联合意图分析使LVLMs成为鲁棒的越狱防御者|Yuqi Zhang, Yuchun Miao, Zuchao Li, Liang Ding|<http://arxiv.org/pdf/2505.24519v1>|AMIA通过自动遮挡无关图像区域和联合意图分析，显著提升了大型视觉语言模型的鲁棒性。|
|🆕 发布|S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification|S3CE-Net：用于长序列事件重识别的脉冲引导时空语义耦合与扩展网络|Xianheng Ma, Hongchen Tan, Xiuping Liu, Yi Zhang, Huasheng Wang, Jiang Liu, Ying Chen, Hantao Liu|<http://arxiv.org/pdf/2505.24401v1>|[代码](https://github.com/Mhsunshine/SC3E_Net.); 提出了一种基于脉冲神经网络的低参数长序列事件重识别模型，显著提升了识别准确率。|
|🆕 发布|Progressive Class-level Distillation|渐进式类级别蒸馏|Jiayan Li, Jun Li, Zhourui Zhang, Jianhua Xu|<http://arxiv.org/pdf/2505.24310v1>|提出渐进式类别级蒸馏方法，有效提升低概率类别知识在知识蒸馏中的传递。|
|📝 更新|ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS|ZPressor：面向可扩展前馈3DGS的瓶颈感知压缩|Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang|<http://arxiv.org/pdf/2505.23734v2>|ZPressor通过信息瓶颈原理，实现轻量级压缩，提升3DGS模型处理多视角输入的效率和鲁棒性。|
|🆕 发布|Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling|迈向通过子空间解耦的联邦多任务学习统一建模|Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du|<http://arxiv.org/pdf/2505.24185v1>|提出FedDEA方法，通过子空间解耦实现联邦多任务学习，有效解决异构任务联合训练难题。|
|📝 更新|Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework|扩展量化网络：一种高效的微表情标注与检测框架|Jingyi Zhou, Senlin Luo, Haofan Chen|<http://arxiv.org/pdf/2411.06160v3>|提出EQN框架，有效解决微表情标注与检测难题，实现自动微表情标注与能量级评分。|
|🆕 发布|Federated Foundation Model for GI Endoscopy Images|联邦基础模型用于胃镜图像|Alina Devkota, Annahita Amireskandari, Joel Palko, Shyam Thakkar, Donald Adjeroh, Xiajun Jiang, Binod Bhattarai, Prashnna K. Gyawali|<http://arxiv.org/pdf/2505.24108v1>|提出了一种联邦学习框架，在保护隐私的前提下，训练用于胃肠镜图像的通用基础模型，显著提升了下游任务性能...|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Efficient RAW Image Deblurring with Adaptive Frequency Modulation|高效自适应频率调制下的RAW图像去模糊|Wenlong Jiao, Binglong Li, Wei Shang, Ping Wang, Dongwei Ren|<http://arxiv.org/pdf/2505.24407v1>|[代码](https://github.com/WenlongJiao/FrENet); 提出了一种针对RAW图像去模糊的FrENet框架，通过自适应频率调制和频率域跳过连接，实现了高效且高...|
|🆕 发布|PCIE_Interaction Solution for Ego4D Social Interaction Challenge|PCIE_针对Ego4D社交交互挑战的交互解决方案|Kanokphan Lertniphonphan, Feng Chen, Junda Xu, Fengbu Lan, Jun Xie, Tao Zhang, Zhepeng Wang|<http://arxiv.org/pdf/2505.24404v1>|[代码](https://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction); 针对Ego4D社交交互挑战，提出融合视觉和音频信息的PCIE_Interaction解决方案，显著提...|
|🆕 发布|Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction|基于稀疏性的并行成像一致性提升自监督MRI重建|Yaşar Utku Alçalar, Mehmet Akçakaya|<http://arxiv.org/pdf/2505.24136v1>|提出了一种通过稀疏域一致性增强的并行成像一致性训练方法，显著提升了自监督MRI重建的图像质量。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering|渲染弯曲：基于可微渲染的对抗攻击综述|Matthew Hull, Haoran Wang, Matthew Lau, Alec Helbling, Mansi Phute, Chao Zhang, Zsolt Kira, Willian Lunardi .etc.|<http://arxiv.org/pdf/2411.09749v2>|首次提出统一框架，整合不同目标与任务，推动可微分渲染在对抗攻击中的应用研究。|
|📝 更新|GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack|GSBA$^K$：基于几何分数的$top$-$K$黑盒攻击|Md Farhamdur Reza, Richeng Jin, Tianfu Wu, Huaiyu Dai|<http://arxiv.org/pdf/2503.12827v3>|GSBA$^K$提出了一种针对多标签分类器的$top$-$K$黑盒攻击方法，有效提升攻击成功率与查询...|
|🆕 发布|PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches|PatchDEMUX：针对对抗性补丁的多标签分类器的可认证鲁棒框架|Dennis Jacob, Chong Xiang, Prateek Mittal|<http://arxiv.org/pdf/2505.24703v1>|PatchDEMUX提出了一种针对多标签分类器对抗性补丁的认证鲁棒框架，显著提升了防御能力。|
|📝 更新|Improving Adversarial Robustness via Phase and Amplitude-aware Prompting|通过相位和幅度感知提示提高对抗鲁棒性|Yibo Xu, Dawei Zhou, Decheng Liu, Nannan Wang|<http://arxiv.org/pdf/2502.03758v2>|提出了一种基于相位和幅度提示的防御方法，有效提升了对抗样本的鲁棒性。|


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning|MoDoMoDo：多域数据混合用于多模态LLM强化学习|Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu .etc.|<http://arxiv.org/pdf/2505.24871v1>|提出了一种多领域数据混合策略，显著提升了多模态LLM在视觉语言任务上的泛化能力和推理能力。|
|🆕 发布|Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV|高效利用近似留一法估计正则化泰勒M估计量|Karim Abou-Moustafa|<http://arxiv.org/pdf/2505.24781v1>|提出了一种高效估计RTME正则化参数的方法，显著提升了LOOCV估计的准确性和效率。|
|📝 更新|SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity|SAMBLE：针对局部细节与全局均匀性之间最佳权衡的形状特定点云采样|Chengzhi Wu, Yuxin Wan, Hao Fu, Julius Pfrommer, Zeyun Zhong, Junwei Zheng, Jiaming Zhang, Jürgen Beyerer|<http://arxiv.org/pdf/2504.19581v2>|提出SAMBLE方法，通过学习形状特定采样策略，优化点云局部细节与全局均匀性的平衡。|
|📝 更新|Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation|伪标签在在线无源通用域自适应中的应用分析|Pascal Schlachter, Jonathan Fuss, Bin Yang|<http://arxiv.org/pdf/2504.11992v2>|[代码](https://github.com/pascalschlachter/PLAnalysis.); 分析了伪标签在在线无源通用域自适应中的作用，揭示了其关键性和优化策略。|
|🆕 发布|Provably Improving Generalization of Few-Shot Models with Synthetic Data|使用合成数据证明提高少样本模型泛化能力的论文标题翻译为中文为：  利用合成数据证明提升少样本模型泛化性能|Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than|<http://arxiv.org/pdf/2505.24190v1>|提出了一种理论框架，通过原型学习优化数据分区和模型训练，有效提升小样本图像分类模型的泛化能力。|


### 不确定性量化 (Uncertainty Quantification)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Post-hoc Probabilistic Vision-Language Models|后处理概率视觉-语言模型|Anton Baumann, Rui Li, Marcus Klasson, Santeri Mentu, Shyamgopal Karthik, Zeynep Akata, Arno Solin, Martin Trapp|<http://arxiv.org/pdf/2412.06014v3>|提出了一种无需额外训练的VLM后处理不确定性估计方法，有效提升预测不确定性和主动学习效率。|


### 视觉安全与隐私 (Visual Security & Privacy)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models|病理学中的蝴蝶效应：探索病理基础模型中的安全性|Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao .etc.|<http://arxiv.org/pdf/2505.24141v1>|[代码](https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.); 首次系统性地研究了病理基础模型在对抗攻击下的安全性，并提出了一种无需标签的攻击框架。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MorphoSeg: An Uncertainty-Aware Deep Learning Method for Biomedical Segmentation of Complex Cellular Morphologies|形态分割：一种用于复杂细胞形态生物医学分割的不确定性感知深度学习方法|Tianhao Zhang, Heather J. McCourty, Berardo M. Sanchez-Tafolla, Anton Nikolaev, Lyudmila S. Mihaylova|<http://arxiv.org/pdf/2409.17110v2>|[代码](https://github.com/RanchoGoose/MorphoSeg.); 提出MorphoSeg，一种利用不确定性感知的深度学习方法，显著提升复杂细胞形态分割的准确性。|
|🆕 发布|Segmenting France Across Four Centuries|法国四百年间的分割|Marta López-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu|<http://arxiv.org/pdf/2505.24824v1>|[代码](https://github.com/Archiel19/FRAx4.git.); 构建了包含多世纪法国地图的数据库，用于分析土地利用变化，并评估了不同深度学习模型在历史地图分割中的表...|
|📝 更新|Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning|深度增强：Dropout作为自监督学习的增强方法|Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon|<http://arxiv.org/pdf/2303.14537v5>|提出Deep Augmentation，通过在神经网络深层应用dropout或PCA，有效缓解自监督...|
|🆕 发布|Conformal Prediction for Zero-Shot Models|零样本模型中的保形预测|Julio Silva-Rodríguez, Ismail Ben Ayed, Jose Dolz|<http://arxiv.org/pdf/2505.24693v1>|提出Conf-OT方法，通过迁移学习提高零样本模型在视觉任务中的可靠性和不确定性。|
|🆕 发布|BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models|BIMA：用于大型视觉-语言模型幻觉预测和缓解的双射最大似然学习方法|Huu-Thien Tran, Thanh-Dat Truong, Khoa Luu|<http://arxiv.org/pdf/2505.24649v1>|提出BIMA方法，利用归一化流理论有效缓解大型视觉语言模型中的幻觉问题。|
|🆕 发布|Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models|基于角度不变注意力深度学习模型的云光学厚度反演|Zahid Hassan Tushar, Adeleke Ademakinwa, Jianwu Wang, Zhibo Zhang, Sanjay Purushotham|<http://arxiv.org/pdf/2505.24638v1>|提出了一种基于角度编码的注意力深度学习模型，显著提高了云光学厚度估计的准确性。|
|🆕 发布|Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers|多样化与征服：含异常值的开放集不一致性在鲁棒半监督学习中的应用|Heejo Kong, Sung-Jin Kim, Gunho Jung, Seong-Whan Lee|<http://arxiv.org/pdf/2505.24443v1>|[代码](https://github.com/heejokong/DivCon.); 提出了一种通过构建不同偏好的模型集合来增强开放集半监督学习鲁棒性的方法，以应对异常值影响。|
|🆕 发布|Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning|在CLIP中通过高效微调提升组合意识|Amit Peleg, Naman Deep Singh, Matthias Hein|<http://arxiv.org/pdf/2505.24424v1>|CLIC通过结合多图像和描述，有效提升CLIP的组成理解，改善检索性能。|
|📝 更新|Nested Hash Layer: A Plug-and-play Module for Multiple-length Hash Code Learning|嵌套哈希层：多长度哈希码学习的即插即用模块|Liyang He, Yuren Zhang, Rui Li, Zhenya Huang, Runze Wu, Enhong Chen|<http://arxiv.org/pdf/2412.08922v2>|提出Nested Hash Layer，解决多长度哈希码学习效率与效果平衡问题，提升模型性能。|
|🆕 发布|A Mathematical Perspective On Contrastive Learning|对比学习的数学视角|Ricardo Baptista, Andrew M. Stuart, Son Tran|<http://arxiv.org/pdf/2505.24134v1>|从数学角度解析对比学习，提出概率框架和低秩矩阵近似，拓展了多模态学习算法。|
|🆕 发布|Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors|弱监督下基于部分级语义先验的适应性定位引导|Peiran Xu, Yadong Mu|<http://arxiv.org/pdf/2505.24103v1>|[代码](https://github.com/woyut/WSAG-PLSP); 利用现有模型语义知识，提出弱监督方法，显著提升物体功能定位性能。|


### 小样本学习 (Few-shot Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning|零样本汉字识别：基于分层多粒度图像-文本对齐|Yinglian Zhu, Haiyang Yu, Qizao Wang, Wei Lu, Xiangyang Xue, Bin Li|<http://arxiv.org/pdf/2505.24837v1>|提出了一种基于层次多粒度图像-文本对齐的零样本汉字识别框架，显著提升了识别准确率。|
|🆕 发布|Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification|轻量级关系嵌入在任务插值少样本网络中用于增强胃肠道疾病分类|Xinliu Zhong, Leo Hwa Liang, Angela S. Koh, Yeo Si Yong|<http://arxiv.org/pdf/2505.24792v1>|提出了一种轻量级关系嵌入方法，有效提升了胃肠道疾病分类的准确性。|


### 零/少样本泛化 (Zero/Few-shot Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models|魔法的背后，MERLIM：大型图像-语言模型的多模态评估基准|Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, Bernard Ghanem|<http://arxiv.org/pdf/2312.02219v3>|构建MERLIM多模态评估基准，评估IT-LVLMs在基础计算机视觉任务上的能力，揭示其局限性。|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 视觉操作与控制 (Visual Manipulation & Control)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks|《正立吗？通过细粒度多轴感知任务在多模态语言模型中解耦方向理解》|Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer|<http://arxiv.org/pdf/2505.21649v3>|提出DORI基准，揭示多模态系统在物体方向理解上的局限性。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 多模态对话系统 (Multimodal Dialogue Systems)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents|开放CaptchaWorld：一个用于测试和基准测试多模态LLM代理的全面在线平台|Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen|<http://arxiv.org/pdf/2505.24878v1>|构建了Open CaptchaWorld平台，通过多模态LLM在动态CAPTCHA测试中评估其视觉推...|


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ProxyThinker: Test-Time Guidance through Small Visual Reasoners|ProxyThinker：通过小型视觉推理器进行测试时引导|Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, Vicente Ordonez|<http://arxiv.org/pdf/2505.24872v1>|[代码](https://github.com/MrZilinXiao/ProxyThinker.); ProxyThinker通过小型视觉推理器在推理时引导大型模型，实现无需训练的视觉推理能力提升。|
|🆕 发布|VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software|视频CAD：一个用于从CAD软件中学习UI交互和3D推理的大规模视频数据集|Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed|<http://arxiv.org/pdf/2505.24838v1>|构建大规模视频数据集VideoCAD，用于从CAD软件学习UI交互和3D推理。|
|🆕 发布|Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck|视觉LLMs在层次视觉理解方面表现不佳，LLMs是瓶颈|Yuwen Tan, Yuan Qing, Boqing Gong|<http://arxiv.org/pdf/2505.24840v1>|揭示视觉LLMs缺乏层次视觉理解能力，并提出通过VQA任务提升LLMs层次一致性的方法。|
|📝 更新|SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement|SoTA with Less：基于MCTS的样本选择，实现数据高效视觉推理自我改进|Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang .etc.|<http://arxiv.org/pdf/2504.07934v3>|通过MCTS指导的样本选择，ThinkLite-VL模型在更少训练样本下实现视觉推理性能的显著提升。|
|📝 更新|Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation|跨域序列推荐的视觉和文本表示的层次注意力融合|Wangyu Wu, Zhenhong Chen, Siqi Song, Xianglin Qiu, Xiaowei Huang, Fei Ma, Jimin Xiao|<http://arxiv.org/pdf/2504.15085v2>|提出HAF-VT模型，融合视觉和文本表示，提升跨域推荐认知建模。|
|📝 更新|RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs|冗余镜头：揭示和利用视觉标记处理冗余以实现高效仅解码器多语言语言模型|Hongliang Li, Jiaxin Zhang, Wenhui Liao, Dezhi Peng, Kai Ding, Lianwen Jin|<http://arxiv.org/pdf/2501.19036v3>|[代码](https://github.com/L-Hugh/RedundancyLens.); RedundancyLens揭示并利用视觉token处理冗余，实现高效解码器仅MLLM。|
|📝 更新|"See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models|《见世界，识知识：大型视觉语言模型的中国事实性评估》|Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan .etc.|<http://arxiv.org/pdf/2502.11718v4>|构建首个中文视觉问答基准，评估大型视觉语言模型的事实准确性。|
|📝 更新|Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models|拓展多模态大型语言模型中视觉先验知识的边界|Qiao Liang, Yanjiang Liu, Weixiang Zhou, Ben He, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han .etc.|<http://arxiv.org/pdf/2503.18034v2>|提出VisPRE框架，通过增强视觉编码器先验知识，显著提升多模态大语言模型对视觉内容的理解能力。|
|📝 更新|Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information|Qwen再次审视：引导视觉-语言推理模型重新关注视觉信息|Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li|<http://arxiv.org/pdf/2505.23558v2>|[代码](https://github.com/Liar406/Look_Again); Qwen-LookAgain通过引导模型重新关注视觉信息，有效减轻了视觉语言推理模型中的幻觉问题。|
|📝 更新|ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom|ProReason：解耦视觉与智慧的跨模态主动推理|Jingqi Zhou, Sheng Wang, Jingwei Dong, Kai Liu, Lei Li, Jiahui Gao, Jiyue Jiang, Lingpeng Kong .etc.|<http://arxiv.org/pdf/2410.14138v3>|提出ProReason框架，通过解耦视觉感知和文本推理，提升视觉语言模型在推理任务上的表现。|
|📝 更新|VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought|VLM-R$^3$：区域识别、推理和优化以增强多模态思维链|Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang .etc.|<http://arxiv.org/pdf/2505.16192v2>|VLM-R$^3$通过区域识别、推理和细化，提升了多模态思维链的视觉证据精确性。|
|📝 更新|Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration|评估视觉与文化解读：K-Viscuit基准与人类-VLM协作|ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo|<http://arxiv.org/pdf/2406.16469v3>|提出K-Viscuit基准，通过人机协作构建文化VLM基准，评估VLM在文化相关问题上的理解能力。|
|📝 更新|Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task|超越感知：通过多阶段任务评估抽象视觉推理|Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A. Ehinger|<http://arxiv.org/pdf/2505.21850v2>|构建了多阶段抽象视觉推理基准和多阶段评估指标，揭示了现有多模态大语言模型在复杂推理阶段的挑战。|
|🆕 发布|Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models|轻如欺骗：GPT驱动的自然重光照对抗视觉-语言预训练模型|Ying Yang, Jie Zhang, Xiao Lv, Di Lin, Tao Xiang, Qing Guo|<http://arxiv.org/pdf/2505.24227v1>|提出LightD框架，通过语义引导的重新照明生成自然对抗样本，有效对抗视觉语言预训练模型。|
|🆕 发布|Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT|视觉推理并非推理：基于图的多元路径视觉物理CoT评估的MVPBench|Zhuobai Dong, Junchao Yi, Ziyuan Zheng, Haochen Han, Xiangxi Zheng, Alex Jinpeng Wang, Fangming Liu, Linjie Li|<http://arxiv.org/pdf/2505.24182v1>|构建MVPBench基准，评估多路径视觉物理推理能力，揭示MLLMs在物理场景中推理能力不足。|
|🆕 发布|Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry|无需训练的基于视觉特征的几何投影零样本3D对称性检测|Isaac Aguirre, Ivan Sipiran|<http://arxiv.org/pdf/2505.24162v1>|提出了一种无需训练的零样本3D对称性检测方法，利用视觉特征在几何上回射识别反射对称面。|
|📝 更新|IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction|IMTS值得时间×通道补丁：用于不规则多元时间序列预测的视觉掩码自编码器|Zhangyi Hu, Jiemin Wu, Hua Xu, Mingqian Liao, Ninghui Feng, Bo Gao, Songning Lai, Yutao Yue|<http://arxiv.org/pdf/2505.22815v2>|[代码](https://github.com/WHU-HZY/VIMTS.); 提出VIMTS框架，利用视觉掩码自编码器处理不规则多变量时间序列预测，有效解决缺失值问题。|
|🆕 发布|CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs|CSVQA：用于评估VLMs STEM推理能力的中文多模态基准|Ai Jian, Weijie Qiu, Xiaokun Wang, Peiyu Wang, Yunzhuo Hao, Jiangbo Pei, Yichen Wei, Yi Peng .etc.|<http://arxiv.org/pdf/2505.24120v1>|构建CSVQA基准，评估VLMs在科学推理方面的能力，揭示其局限性。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|VideoGameBench: Can Vision-Language Models complete popular video games?|视频游戏基准：视觉-语言模型能否完成流行视频游戏？|Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press|<http://arxiv.org/pdf/2505.18134v2>|VideoGameBench挑战VLM在无辅助信息下完成经典游戏，揭示实时交互限制。|
|📝 更新|Are MLMs Trapped in the Visual Room?|MLMs是否被困在视觉房间内？|Yazhou Zhang, Chunwang Zou, Qimeng Liu, Lu Rong, Ben Yao, Zheng Lian, Qiuchi Li, Peng Zhang .etc.|<http://arxiv.org/pdf/2505.23272v2>|提出“视觉房间”假说，通过感知与认知双层评估框架揭示多模态大模型在视觉理解上的局限性。|
|🆕 发布|KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval|KEVER^2：知识增强的视觉情感推理与检索|Fanhang Man, Xiaoyue Chen, Huandong Wang, Baining Zhao, Han Li, Xinlei Chen, Yong Li|<http://arxiv.org/pdf/2505.24342v1>|提出了一种结合知识增强的视觉情感推理与检索框架，显著提升了跨领域情感识别的准确率。|
|🆕 发布|D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding|D2AF：一种用于视觉定位的驱动式标注与过滤框架|Yichi Zhang, Gongwei Chen, Jun Zhu, Jia Wan|<http://arxiv.org/pdf/2505.24372v1>|提出D2AF框架，通过图像自动标注和筛选，有效提升视觉定位任务性能。|


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking|基于场景感知重排序的文本式人物检索：SA-Person|Yingjia Xu, Jinlin Wu, Zhen Chen, Daming Gao, Yang Yang, Zhen Lei, Min Cao|<http://arxiv.org/pdf/2505.24466v1>|提出一种结合场景感知重排序的文本基人物检索方法，有效提升场景级检索效果。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 创意媒体生成 (Creative Media Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection|人声头像基准与深度伪造检测的多模态分析与评估|Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu .etc.|<http://arxiv.org/pdf/2505.24866v1>|构建了TalkingHeadBench基准，评估了先进生成模型下深度伪造检测模型的鲁棒性和泛化能力。|


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|CLIP-IT: CLIP-based Pairing for Histology Images Classification|CLIP-IT：基于CLIP的病理图像配对分类|Banafsheh Karimian, Giulia Avanzato, Soufian Belharbi, Luke McCaffrey, Mohammadhadi Shateri, Eric Granger|<http://arxiv.org/pdf/2504.16181v2>|CLIP-IT通过结合外部文本信息，无需手动配对样本，有效利用特权文本信息，提升病理图像分类性能。|
|🆕 发布|Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation|解耦竞争框架用于半监督医学图像分割|Jiahe Chen, Jiahe Ying, Shen Wang, Jianwei Zheng|<http://arxiv.org/pdf/2505.24667v1>|[代码](https://github.com/JiaheChen2002/DCF.); 提出了一种解耦竞争框架，有效缓解了半监督医学图像分割中的过耦合和认知偏差问题。|
|📝 更新|LesionDiffusion: Towards Text-controlled General Lesion Synthesis|损伤扩散：迈向文本控制的通用损伤合成|Henrui Tian, Wenhui Lei, Linrui Dai, Hanyu Chen, Xiaofan Zhang|<http://arxiv.org/pdf/2503.00741v4>|[代码](https://github.com/HengruiTianSJTU/LesionDiffusion.); 提出LesionDiffusion，通过文本控制实现医学图像中病变的合成，提升病变识别性能。|
|🆕 发布|Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation|释放中间域力量以实现混合域半监督医学图像分割|Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao|<http://arxiv.org/pdf/2505.24567v1>|[代码](https://github.com/MQinghe/UST-RUN); 提出UST-RUN框架，利用中间域信息解决混合域半监督医学图像分割问题。|
|🆕 发布|ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation|ACM-UNet：CNN与Mamba自适应集成以实现高效的医学图像分割|Jing Huang, Yongkang Zhao, Yuhan Li, Zhitao Dai, Cheng Chen, Qiying Lai|<http://arxiv.org/pdf/2505.24481v1>|[代码](https://github.com/zyklcode/ACM-UNet.); ACM-UNet通过轻量适配器结合CNN和Mamba，有效提升医疗图像分割性能。|
|📝 更新|Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective|对抗鲁棒的免费AI生成图像检测：信息论视角|Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang|<http://arxiv.org/pdf/2505.22604v2>|提出了一种无需训练的基于信息理论的图像检测方法，有效防御对抗攻击。|
|🆕 发布|pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation|pyMEAL：一种针对鲁棒和泛化医学图像翻译的多编码器增强感知学习|Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, Kannie W. Y. Chan|<http://arxiv.org/pdf/2505.24421v1>|提出MEAL框架，通过多编码器融合不同增强特征，提升医学图像转换的鲁棒性和泛化能力。|
|🆕 发布|KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices|凯罗斯AD：基于SAM的嵌入式设备工业异常检测模型|Uzair Khan, Franco Fummi, Luigi Capogrosso|<http://arxiv.org/pdf/2505.24334v1>|[代码](https://github.com/intelligolabs/KairosAD.); 提出了一种基于MobileSAM的轻量级工业异常检测模型，有效降低计算需求并提升部署效率。|
|🆕 发布|DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?|DrVD-Bench：在医学图像诊断中，视觉-语言模型推理是否像人类医生一样？|Tianhong Zhou, Yin Xu, Yingtao Zhu, Chuxi Xiao, Haiyang Bian, Lei Wei, Xuegong Zhang|<http://arxiv.org/pdf/2505.24173v1>|构建了首个临床视觉推理多模态基准DrVD-Bench，评估视觉语言模型在医学图像诊断中的推理能力。|
|🆕 发布|Beyond the LUMIR challenge: The pathway to foundational registration models|超越LUMIR挑战：基础配准模型的路径|Junyu Chen, Shuwen Wei, Joel Honkamaa, Pekka Marttinen, Hang Zhang, Min Liu, Yichao Zhou, Zuopeng Tan .etc.|<http://arxiv.org/pdf/2505.24160v1>|提出LUMIR挑战，推动无监督脑MRI图像注册，实现跨模态、疾病和物种的零样本泛化。|


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval|REJEPA：一种用于高效遥感图像检索的新型联合嵌入预测架构|Shabnam Choudhury, Yash Salunkhe, Sarthak Mehrotra, Biplab Banerjee|<http://arxiv.org/pdf/2504.03169v2>|提出REJEPA，一种高效、精确的遥感图像检索框架，通过联合嵌入预测架构提升检索准确率。|
|🆕 发布|Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing|可变形注意力机制在目标检测中的应用：遥感案例|Anasse Boutayeb, Iyad Lahsen-cherif, Ahmed El Khadimi|<http://arxiv.org/pdf/2505.24489v1>|提出Deformable-DETR模型，有效提升遥感图像目标检测准确率。|
|🆕 发布|GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models|地理视觉标注器：基于视觉和语言模型的零样本地理空间分类|Gilles Quentin Hacheme, Girmaw Abebe Tadesse, Caleb Robinson, Akram Zaytar, Rahul Dodhia, Juan M. Lavista Ferres|<http://arxiv.org/pdf/2505.24340v1>|[代码](https://github.com/microsoft/geo-vision-labeler); GeoVision Labeler通过结合视觉和语言模型实现零样本地理空间图像分类，提高灾害响应和土...|
|🆕 发布|Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation|重新审视跨模态知识蒸馏：一种用于RGBD语义分割的解耦方法|Roger Ferrod, Cássio F. Dantas, Luigi Di Caro, Dino Ienco|<http://arxiv.org/pdf/2505.24361v1>|提出了一种基于解耦表示和对比学习的跨模态知识蒸馏方法，有效解决了RGBD语义分割中模态不匹配问题。|
|🆕 发布|STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising|STAR-Net：一种用于遥感图像去噪的可解释模型辅助网络|Jingjing Liu, Jiashun Jin, Xianchao Xiu, Jianhua Zhang, Wanquan Liu|<http://arxiv.org/pdf/2505.24327v1>|STAR-Net提出了一种基于低秩先验的遥感图像去噪方法，有效捕捉非局部自相似性并自动学习正则化参数...|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 量子视觉算法 (Quantum Visual Algorithms)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors|从视频中学习3D世界：利用3D视觉几何先验增强多模态语言模型|Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang|<http://arxiv.org/pdf/2505.24625v1>|提出一种从视频数据中直接学习3D空间理解的MLLM增强方法，显著提升3D场景理解和空间推理能力。|
|📝 更新|GUICourse: From General Vision Language Models to Versatile GUI Agents|GUI课程：从通用视觉语言模型到多才多艺的GUI智能体|Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu .etc.|<http://arxiv.org/pdf/2406.11317v2>|[代码](https://github.com/yiye3/GUICourse.); 提出GUICourse，通过数据集增强，将通用视觉语言模型训练成多才多艺的GUI智能体。|
|📝 更新|JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models|《JailBound：突破视觉-语言模型内部安全边界》|Jiaxin Song, Yixu Wang, Jie Li, Rui Yu, Yan Teng, Xingjun Ma, Yingchun Wang|<http://arxiv.org/pdf/2505.19610v2>|提出JailBound框架，通过探测和跨越视觉语言模型内部安全边界，有效提升其对抗攻击成功率。|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores|TC-GS：利用张量核心的更快高斯分层模块|Zimu Liao, Jifeng Ding, Rong Fu, Siwei Cui, Ruixuan Gong, Li Wang, Boni Hu, Yi Wang .etc.|<http://arxiv.org/pdf/2505.24796v1>|[代码](https://github.com/TensorCore3DGS/3DGSTensorCore); 提出TC-GS模块，利用张量核心加速3D高斯分层渲染，实现2.18倍速度提升。|
|🆕 发布|Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism|超越FACS：数据驱动的面部表情词典及其在预测自闭症中的应用|Evangelos Sariyanidi, Lisa Yankowitz, Robert T. Schultz, John D. Herrington, Birkan Tunc, Jeffrey Cohn|<http://arxiv.org/pdf/2505.24679v1>|[代码](https://github.com/sariyanidi/FacialBasis.); 提出了一种数据驱动的面部表情字典，用于更全面地预测自闭症诊断。|
|📝 更新|FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks|FieldWorkArena：真实野外工作任务的代理人工智能基准|Atsunori Moteki, Shoichi Masui, Fan Yang, Yueqi Song, Yonatan Bisk, Graham Neubig, Ikuo Kusajima, Yasuto Watanabe .etc.|<http://arxiv.org/pdf/2505.19662v2>|FieldWorkArena提出首个针对真实世界现场工作任务的Agentic AI基准，评估其在多模...|

