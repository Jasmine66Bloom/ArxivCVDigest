## [UPDATED!] **2025-06-02** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation|QARI-OCR：通过多模态大型语言模型自适应实现高保真阿拉伯文识别|Ahmed Wasfy, Omer Nacar, Abdelakreem Elkhateb, Mahmoud Reda, Omar Elshehy, Adel Ammar, Wadii Boulila|<http://arxiv.org/pdf/2506.02295v1>|QARI-OCR通过多模态大语言模型优化，显著提升了阿拉伯文OCR准确率和效率。|
|🆕 发布|Motion aware video generative model|运动感知视频生成模型|Bowen Xue, Giuseppe Claudio Guarnera, Shuang Zhao, Zahra Montazeri|<http://arxiv.org/pdf/2506.02244v1>|引入物理信息频率域方法，显著提升生成视频的运动质量和物理合理性。|
|📝 更新|Ola: Pushing the Frontiers of Omni-Modal Language Model|《Ola：推动全模态语言模型前沿》|Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao|<http://arxiv.org/pdf/2502.04328v3>|[代码](https://github.com/Ola-Omni/Ola.); 提出Ola模型，显著提升跨图像、视频和音频的多模态理解能力。|
|🆕 发布|TIIF-Bench: How Does Your T2I Model Follow Your Instructions?|TIIF-Bench：你的T2I模型是如何遵循你的指令的？|Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, Lei Zhang|<http://arxiv.org/pdf/2506.02161v1>|[代码](https://a113n-w3i.github.io/TIIF_Bench); 构建了TIIF-Bench基准，全面评估T2I模型对复杂指令的遵循能力。|
|📝 更新|Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model|《Articulate-Anything：通过视觉-语言基础模型自动建模关节对象》|Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna .etc.|<http://arxiv.org/pdf/2410.13882v4>|Articulate-Anything通过视觉语言模型自动生成可交互的3D数字孪生，简化了复杂物体的...|
|🆕 发布|Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks|许多对许多：统一多个视频和图像生成与操纵任务的训练|Tao Yang, Ruibin Li, Yangming Shi, Yuqi Zhang, Qide Dong, Haoran Cheng, Weiguo Feng, Shilei Wen .etc.|<http://arxiv.org/pdf/2506.01758v1>|[代码](https://github.com/leeruibin/MfM.git.); 提出一种多任务统一训练框架，通过联合学习提升视频和图像生成与操纵性能。|
|📝 更新|Segment Anything for Histopathology|病理切片中的任意分割|Titus Griebel, Anwai Archit, Constantin Pape|<http://arxiv.org/pdf/2502.00408v2>|[代码](https://github.com/computational-cell-analytics/patho-sam.); 提出PathoSAM，一种基于Segment Anything Model的核分割模型，显著提升病理...|
|🆕 发布|EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models|地球之心：迈向多粒度和多传感器地球观测的大型多模态模型|Yan Shu, Bin Ren, Zhitong Xiong, Danda Pani Paudel, Luc Van Gool, Begum Demir, Nicu Sebe, Paolo Rota|<http://arxiv.org/pdf/2506.01667v1>|EarthMind通过引入空间注意力提示和跨模态融合，有效提升了多粒度和多传感器地球观测数据理解能力...|
|📝 更新|SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models|空间LLM：一种基于3D信息的多模态大型智能空间模型复合设计|Wufei Ma, Luoxin Ye, Nessa McWeeney, Celso M de Melo, Jieneng Chen, Alan Yuille|<http://arxiv.org/pdf/2505.00788v2>|检测并解决大型多模态模型缺乏3D空间推理能力的问题，通过引入3D信息训练数据与架构设计，实现更强大的...|
|📝 更新|Safety at Scale: A Comprehensive Survey of Large Model Safety|大规模模型安全：全面综述|Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu .etc.|<http://arxiv.org/pdf/2502.05206v4>|系统综述大型模型安全风险，提出防御策略和挑战，推动构建安全AI模型。|
|📝 更新|Keypoint-Integrated Instruction-Following Data Generation for Enhanced Human Pose and Action Understanding in Multimodal Models|基于关键点集成的指令遵循数据生成，以增强多模态模型中的人体姿态和动作理解|Dewen Zhang, Wangpeng An, Hayaru Shouno|<http://arxiv.org/pdf/2409.09306v2>|[代码](https://github.com/Ody-trek/Keypoint-Instruction-Tuning.); 通过融合人体关键点与视觉特征，提出了一种生成视觉语言指令跟随数据的方法，显著提升了多模态模型对人类姿...|
|📝 更新|PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?|像素基础模型：我们在像素级视觉基础模型的道路上是否走在正确的方向上？|Mennatullah Siam|<http://arxiv.org/pdf/2502.04192v3>|提出PixFoundation，挑战像素级视觉基础模型在视觉问答和视觉基础上的局限性。|
|🆕 发布|SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost|SAM-I2V：将SAM升级以支持低于0.2%训练成本的提示式视频分割|Haiyang Mei, Pengyu Zhang, Mike Zheng Shou|<http://arxiv.org/pdf/2506.01304v1>|[代码](https://github.com/showlab/SAM-I2V.); 检测SAM模型，降低视频分割训练成本，实现高效视频分割。|
|🆕 发布|Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors|视觉稀疏转向：通过稀疏引导转向向量提升零样本图像分类|Gerasimos Chatzoudis, Zhuowei Li, Gemma E. Moran, Hao Wang, Dimitris N. Metaxas|<http://arxiv.org/pdf/2506.01247v1>|提出VS2和VS2++，通过稀疏特征引导视觉模型，显著提升零样本图像分类准确率。|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications|无监督时间序列信号分析：自动编码器和视觉Transformer架构与应用综述|Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari|<http://arxiv.org/pdf/2504.16972v2>|该论文综述了利用自编码器和视觉Transformer进行无监督时间序列信号分析，提出了一种融合创新方...|
|📝 更新|Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications|唤醒视觉：针对TinyML计算机视觉应用的定制数据集和基准套件|Colby Banbury, Emil Njor, Andrea Mattia Garavagno, Mark Mazumder, Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries .etc.|<http://arxiv.org/pdf/2405.00892v5>|开发了一套自动化流程，生成针对TinyML应用的定制数据集和基准，显著提升模型性能。|
|📝 更新|Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems|Erwin：一种用于大规模物理系统的基于树的分层Transformer|Maksim Zhdanov, Max Welling, Jan-Willem van de Meent|<http://arxiv.org/pdf/2502.17019v2>|Erwin通过树状层次化Transformer，高效处理大规模物理系统，解决长距离交互和多尺度耦合问...|
|🆕 发布|Speed-up of Vision Transformer Models by Attention-aware Token Filtering|视觉Transformer模型通过注意力感知令牌过滤加速|Takahiro Naruko, Hiroaki Akutsu|<http://arxiv.org/pdf/2506.01519v1>|提出ATF方法，通过注意力感知的token过滤，加速ViT模型，同时保持检索准确率。|
|📝 更新|RemoteSAM: Towards Segment Anything for Earth Observation|远程SAM：迈向地球观测的任何分割|Liang Yao, Fan Liu, Delong Chen, Chuanyi Zhang, Yijun Wang, Ziyun Chen, Wei Xu, Shimin Di .etc.|<http://arxiv.org/pdf/2505.18022v3>|[代码](https://github.com/1e12Leon/RemoteSAM.); 开发了一种适用于地球观测的Segment Anything模型，通过数据与模型创新，实现了高效的多任...|
|📝 更新|Parameter Efficient Fine-Tuning of Segment Anything Model for Biomedical Imaging|参数高效的Segment Anything模型在生物医学图像中的微调|Carolin Teuber, Anwai Archit, Constantin Pape|<http://arxiv.org/pdf/2502.00418v2>|[代码](https://github.com/computational-cell-analytics/peft-sam.); 首次提出针对生物医学图像的Segment Anything Model参数高效微调方法，实现资源节约...|
|📝 更新|RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers|RePaViT：通过前馈网络层的结构重参数化实现可扩展视觉Transformer加速|Xuwei Xu, Yang Li, Yudong Chen, Jiajun Liu, Sen Wang|<http://arxiv.org/pdf/2505.21847v2>|[代码](https://github.com/Ackesnal/RePaViT.); 提出了一种通过结构重参数化FFN层加速大规模ViT的方法，显著降低延迟并提升准确率。|
|📝 更新|Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation|Flex3D：具有灵活重建模型和输入视图优化的前馈3D生成|Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, Filippos Kokkinos|<http://arxiv.org/pdf/2410.00890v3>|Flex3D通过灵活的重建模型和输入视图优化，实现了从任意数量高质量视图生成高质量3D内容。|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|A minimalistic representation model for head direction system|头部方向系统的一种简约表示模型|Minglu Zhao, Dehong Xu, Deqian Kong, Wen-Hao Zhang, Ying Nian Wu|<http://arxiv.org/pdf/2411.10596v2>|提出了一种简化模型学习头部方向系统，实现高维表示并准确路径积分。|
|🆕 发布|E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models|E3D-Bench：端到端3D几何基础模型基准测试|Wenyan Cong, Yiqing Liang, Yancheng Zhang, Ziyi Yang, Yan Wang, Boris Ivanovic, Marco Pavone, Chen Chen .etc.|<http://arxiv.org/pdf/2506.01933v1>|构建了首个3D几何基础模型基准，全面评估了该领域模型性能。|
|🆕 发布|MLLMs Need 3D-Aware Representation Supervision for Scene Understanding|机器学习模型需要3D感知表示监督以实现场景理解|Xiaohu Huang, Jingjing Wu, Qunyi Xie, Kai Han|<http://arxiv.org/pdf/2506.01946v1>|[代码](https://visual-ai.github.io/3drs); 提出3DRS框架，通过引入3D模型知识提升MLLM的3D表示能力，增强场景理解。|
|🆕 发布|Elucidating the representation of images within an unconditional diffusion model denoiser|揭示无条件扩散模型去噪器中的图像表示|Zahra Kadkhodaie, Stéphane Mallat, Eero Simoncelli|<http://arxiv.org/pdf/2506.01912v1>|揭示了无监督扩散模型去噪器内部图像表示及其在图像空间中的语义相似性。|
|🆕 发布|ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding|ShapeLLM-Omni：一种用于3D生成与理解的本地多模态LLM|Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, Jun Zhu|<http://arxiv.org/pdf/2506.01853v1>|[代码](https://github.com/JAMESYJL/ShapeLLM-Omni); 提出ShapeLLM-Omni，一种能理解和生成3D内容的原生多模态大语言模型。|
|📝 更新|I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue|我看懂了你的意思：多模态对话中的共语音手势用于指称消解|Esam Ghaleb, Bulat Khaertdinov, Aslı Özyürek, Raquel Fernández|<http://arxiv.org/pdf/2503.00071v2>|提出了一种基于自监督预训练的多模态手势表示学习方法，提高了参考解析的准确性。|
|🆕 发布|MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs|运动视觉：提升多模态大型语言模型中的细粒度运动理解|Yipeng Du, Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Xiang Li, Jian Yang, Zhenheng Yang .etc.|<http://arxiv.org/pdf/2506.01674v1>|MotionSight通过引入视觉提示和构建大规模数据集，显著提升了多模态LLMs对视频细微动作的理...|
|🆕 发布|Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement|Zoom-Refine：通过局部放大和自优化提升高分辨率多模态理解|Xuan Yu, Dayan Guan, Michael Ying Yang, Yanfeng Gu|<http://arxiv.org/pdf/2506.01663v1>|[代码](https://github.com/xavier-yu114/Zoom-Refine); Zoom-Refine通过局部放大和自我优化，提升MLLM对高分辨率图像的精细理解能力。|
|🆕 发布|SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes|SAM2-LOVE：基于语言辅助的视听场景中的Segment Anything Model 2|Yuji Wang, Haoran Xu, Yong Liu, Jiaze Li, Yansong Tang|<http://arxiv.org/pdf/2506.01558v1>|检测视频中的场景，通过融合文本、音频和视觉信息，实现更准确的物体分割。|
|📝 更新|Urban Safety Perception Assessments via Integrating Multimodal Large Language Models with Street View Images|通过整合多模态大型语言模型与街景图像进行城市安全感知评估|Jiaxin Zhang, Yunqin Li, Tomohiro Fukuda, Bowen Wang|<http://arxiv.org/pdf/2407.19719v3>|利用多模态大型语言模型和街景图像，实现了城市安全感知评估的自动化。|
|📝 更新|MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM|MIRAGE：评估多模态推理链中MLLM的幻觉|Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang|<http://arxiv.org/pdf/2505.24238v2>|提出MIRAGE基准和改进方法，有效识别和减少多模态大语言模型中的推理幻觉。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution|NTIRE 2025 挑战赛：原始图像恢复与超分辨率|Marcos V. Conde, Radu Timofte, Zihao Lu, Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand|<http://arxiv.org/pdf/2506.02197v1>|提出NTIRE 2025挑战，推动RAW图像修复与超分辨率技术发展。|
|📝 更新|MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection|MTevent：用于6D姿态估计和移动目标检测的多任务事件相机数据集|Shrutarv Awasthi, Anas Gouda, Sven Franke, Jérôme Rutinowski, Frank Hoffmann, Moritz Roidl|<http://arxiv.org/pdf/2505.11282v2>|构建了MTevent数据集，用于解决高速移动机器人中基于事件相机的6D姿态估计和移动目标检测问题。|
|📝 更新|Benchmarking 3D Human Pose Estimation Models under Occlusions|遮挡下3D人体姿态估计模型的基准测试|Filipa Lino, Carlos Santiago, Manuel Marques|<http://arxiv.org/pdf/2504.10350v2>|评估了3D人体姿态估计模型在遮挡条件下的鲁棒性，揭示了当前模型在处理遮挡时的局限性。|
|📝 更新|DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training|DNTextSpotter：通过改进去噪训练实现任意形状场景文本检测|Yu Xie, Qian Qiao, Jun Gao, Tianxiang Wu, Jiaqing Fan, Yue Zhang, Jielei Zhang, Huyang Sun|<http://arxiv.org/pdf/2408.00355v4>|提出DNTextSpotter，通过改进去噪训练解决任意形状场景文本检测的不稳定性问题。|
|🆕 发布|SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation|SteerPose：基于关节的同步外参数相机标定与匹配|Sang-Eun Lee, Ko Nishino, Shohei Nobuhara|<http://arxiv.org/pdf/2506.01691v1>|SteerPose通过神经网络同时实现多相机系统中的相机标定和匹配，无需额外标定目标。|
|🆕 发布|VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding|VRD-IU：视觉丰富文档智能与理解的经验教训|Yihao Ding, Soyeon Caren Han, Yan Li, Josiah Poon|<http://arxiv.org/pdf/2506.01388v1>|通过VRD-IU竞赛，提出多种方法解决复杂文档智能理解问题，提升文档信息提取与定位性能。|
|🆕 发布|Target Driven Adaptive Loss For Infrared Small Target Detection|目标驱动自适应损失函数用于红外小目标检测|Yuho Shoji, Takahiro Toizumi, Atsushi Ito|<http://arxiv.org/pdf/2506.01349v1>|提出TDA损失函数，提升红外小目标检测性能，解决局部区域检测和低对比度目标鲁棒性问题。|


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|OD3: Optimization-free Dataset Distillation for Object Detection|OD3：无优化数据集蒸馏用于目标检测|Salwa K. Al Khatib, Ahmed ElHagry, Shitong Shao, Zhiqiang Shen|<http://arxiv.org/pdf/2506.01942v1>|[代码](https://github.com/VILA-Lab/OD3.); OD3提出了一种无优化数据蒸馏框架，显著提升了目标检测的压缩率和准确率。|
|🆕 发布|unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning|无监督多目标分割：基于中心边界推理|Yafei Yang, Zihui Zhang, Bo Yang|<http://arxiv.org/pdf/2506.01778v1>|unMORE通过中心边界推理实现无监督多对象分割，显著超越现有方法。|
|🆕 发布|Dirty and Clean-Label attack detection using GAN discriminators|基于GAN判别器的污点和干净标签攻击检测|John Smutny|<http://arxiv.org/pdf/2506.01224v1>|利用GAN判别器检测并防御计算机视觉模型中的污点和干净标签攻击。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Concept Based Explanations and Class Contrasting|基于概念的解释与类别对比|Rudolf Herdt, Daniel Otero Baguer|<http://arxiv.org/pdf/2502.03422v2>|[代码](https://github.com/rherdt185/concept-based-explanations-and-class-contrasting); 提出了一种基于概念的方法，用于解释深度神经网络预测并对比不同类别。|
|🆕 发布|A Novel Context-Adaptive Fusion of Shadow and Highlight Regions for Efficient Sonar Image Classification|一种用于高效声纳图像分类的阴影和亮区新颖的上下文自适应融合方法|Kamal Basha S, Anukul Kiran B, Athira Nambiar, Suresh Rajendran|<http://arxiv.org/pdf/2506.01445v1>|提出了一种融合阴影和亮部区域特征的新方法，有效提升了水下声纳图像分类的准确性和鲁棒性。|
|🆕 发布|Rethinking Image Histogram Matching for Image Classification|重新思考图像直方图匹配在图像分类中的应用|Rikuto Otsuka, Yuho Shoji, Yuka Ogino, Takahiro Toizumi, Atsushi Ito|<http://arxiv.org/pdf/2506.01346v1>|提出了一种针对低对比度图像的改进图像直方图匹配方法，提升下游分类器性能。|


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping|MSDNet：基于Transformer引导的原型设计的多尺度解码器用于小样本语义分割|Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh|<http://arxiv.org/pdf/2409.11316v3>|提出了一种基于Transformer的多尺度解码器，通过原型引导和多层次特征融合，有效提升了少样本语...|
|📝 更新|NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation|非均匀圆柱分割网络：高效激光雷达语义分割|Xuzhi Wang, Wei Feng, Lingdong Kong, Liang Wan|<http://arxiv.org/pdf/2505.24634v2>|[代码](https://github.com/alanWXZ/NUC-Net); 提出NUC-Net，通过非均匀圆柱分割网络高效解决LiDAR语义分割中的计算成本和点分布不均问题。|
|📝 更新|RAFT: Robust Augmentation of FeaTures for Image Segmentation|RAFT：用于图像分割的鲁棒特征增强|Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin|<http://arxiv.org/pdf/2505.04529v2>|提出RAFT框架，通过数据增强和特征调整，有效缓解了图像分割中的Syn2Real问题。|


## 生成式视觉模型 (Generative Visual Modeling)


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability|数据说明书不够：用于自动质量指标和责任的数据评分标准|Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang .etc.|<http://arxiv.org/pdf/2506.01789v2>|[代码](https://github.com/datarubrics/datarubrics.); 提出DataRubrics框架，通过系统化评估指标提升数据集质量评估和透明度。|
|🆕 发布|Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning|医学世界模型：肿瘤演化的生成模拟以辅助治疗规划|Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille .etc.|<http://arxiv.org/pdf/2506.02327v1>|开发了一种医学世界模型，通过模拟肿瘤演化预测治疗效果，辅助临床决策。|
|📝 更新|The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects|《男CEO与女助理：双主体文本到图像生成中性别偏见的评估与缓解》|Yixin Wan, Kai-Wei Chang|<http://arxiv.org/pdf/2402.11089v4>|提出Paired Stereotype Test框架评估性别偏见，并设计FairCritic框架提升...|
|🆕 发布|Dual-Process Image Generation|双过程图像生成|Grace Luo, Jonathan Granskog, Aleksander Holynski, Trevor Darrell|<http://arxiv.org/pdf/2506.01955v1>|提出了一种从视觉语言模型中学习新任务的图像生成方法，通过文本和图像界面实现多种控制任务。|
|🆕 发布|IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout|IMAGHarmony：具有一致对象数量和布局的可控图像编辑|Fei Shen, Xiaoyu Du, Yutong Gao, Jian Yu, Yushe Cao, Xing Lei, Jinhui Tang|<http://arxiv.org/pdf/2506.01949v1>|[代码](https://github.com/muzishen/IMAGHarmony.); IMAGHarmony通过结构感知和偏好引导噪声选择，实现了多对象场景下图像编辑的精细控制。|
|🆕 发布|Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control|基于协同轨迹控制的机器人操作视频生成学习|Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, Dahua Lin|<http://arxiv.org/pdf/2506.01943v1>|提出RoboMaster框架，通过协同轨迹控制实现多物体交互的视频生成，提升机器人操作决策数据质量。|
|🆕 发布|Low-Rank Head Avatar Personalization with Registers|低秩头像个性化与寄存器|Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Md Moniruzzaman, Chen-Ping Yu, Yi-Hsuan Tsai, Dimitris Samaras|<http://arxiv.org/pdf/2506.01935v1>|提出了一种结合低秩自适应和注册模块的头像个性化方法，有效捕捉面部细节。|
|🆕 发布|TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation|物种扩散：用于细粒度物种生成的渐进式训练扩散模型|Amin Karimi Monsefi, Mridul Khurana, Rajiv Ramnath, Anuj Karpatne, Wei-Lun Chao, Cheng Zhang|<http://arxiv.org/pdf/2506.01923v1>|[代码](https://amink8.github.io/TaxaDiffusion); TaxaDiffusion通过层次化训练，有效提升了细粒度动物图像生成的准确性和多样性。|
|🆕 发布|Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences|循环一致性作为奖励：无需人类偏好的图像-文本对齐学习|Hyojin Bahng, Caroline Chan, Fredo Durand, Phillip Isola|<http://arxiv.org/pdf/2506.02095v1>|提出利用循环一致性作为奖励，实现无需人工偏好的图像-文本对齐学习。|
|🆕 发布|WorldExplorer: Towards Generating Fully Navigable 3D Scenes|世界探索者：迈向生成完全可导航的3D场景|Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner|<http://arxiv.org/pdf/2506.01799v1>|WorldExplorer通过自回归视频轨迹生成，构建了全可导航且视觉质量一致的3D场景，实现了真实...|
|📝 更新|Enhancing Sample Generation of Diffusion Models using Noise Level Correction|基于噪声水平校正增强扩散模型样本生成|Abulikemu Abuduweili, Chenyang Yuan, Changliu Liu, Frank Permenter|<http://arxiv.org/pdf/2412.05488v3>|提出一种噪声水平校正网络，显著提升扩散模型样本生成质量。|
|🆕 发布|VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking|视频标题生成-R1：通过结构化思维增强多语言语言模型在视频标题生成中的应用|Desen Meng, Rui Huang, Zhilin Dai, Xinhao Li, Yifan Xu, Jun Zhang, Zhenpeng Huang, Meng Zhang .etc.|<http://arxiv.org/pdf/2506.01725v1>|通过结构化思考增强视频多模态LLMs的描述能力，显著提升视频字幕生成准确度。|
|📝 更新|OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation|开放大学：统一多模态理解和生成的简单基线|Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy|<http://arxiv.org/pdf/2505.23661v3>|[代码](https://github.com/wusize/OpenUni.); 提出OpenUni，一种简单轻量级方法，通过连接预训练的多模态LLMs和扩散模型实现统一的多模态理解...|
|🆕 发布|Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation|沉默是金：利用对抗样本消除基于LDM的人脸生成中的音频控制|Yuan Gan, Jiaxu Miao, Yunze Wang, Yi Yang|<http://arxiv.org/pdf/2506.01591v1>|[代码](https://github.com/yuangan/Silencer.); 提出Silencer方法，利用对抗样本消除LDM生成的人脸视频中的音频控制，保护肖像隐私。|
|🆕 发布|LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model|长时驾驶世界模型构建的跨粒度蒸馏|Xiaodong Wang, Zhirong Wu, Peixi Peng|<http://arxiv.org/pdf/2506.01546v1>|[代码](https://Wang-Xiaodong1899.github.io/longdwm); 提出LongDWM，通过跨粒度蒸馏构建长期驾驶世界模型，有效提升视频生成质量和效率。|
|🆕 发布|HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception|HOSIG：基于分层场景感知的全身人-物-场景交互生成|Wei Yao, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang|<http://arxiv.org/pdf/2506.01579v1>|[代码](http://yw0208.github.io/hosig); HOSIG通过分层场景感知，实现高保真全身体验与动态物体和静态场景的交互。|
|🆕 发布|G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models|G4Seg：基于扩散模型的近似分割细化生成方法|Tianjiao Zhang, Fei Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang|<http://arxiv.org/pdf/2506.01539v1>|利用扩散模型生成差异促进不精确分割细化，实现语义对应对齐和前景概率更新。|
|📝 更新|ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM Conversation|ReelWave：通过多模态LLM对话的多代理电影声音生成|Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai|<http://arxiv.org/pdf/2503.07217v3>|提出多智能体框架，通过多模态LLM对话实现电影音效的自主生成。|
|🆕 发布|FDSG: Forecasting Dynamic Scene Graphs|FDSG：动态场景图的预测|Yi Yang, Yuren Cong, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang|<http://arxiv.org/pdf/2506.01487v1>|FDSG提出了一种预测动态场景图的方法，有效预测未来帧中的实体和关系动态。|
|📝 更新|OmniCaptioner: One Captioner to Rule Them All|全场景描述器：一统众描述者|Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen .etc.|<http://arxiv.org/pdf/2504.07089v3>|OmniCaptioner提出了一种跨视觉域的统一文本描述框架，有效连接视觉与文本模态。|
|🆕 发布|Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity|无妥协之高效：辅助CLIP的文本到图像GANs，提升多样性|Yuya Kobayashi, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji|<http://arxiv.org/pdf/2506.01493v1>|提出SCAD模型，通过双特化判别器和PPD指标，在降低训练成本的同时显著提升文本到图像生成多样性。|
|🆕 发布|Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation|通过强化学习解锁“啊哈”时刻：推进协作视觉理解和生成|Kaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, Yunfei Li, Siliang Tang .etc.|<http://arxiv.org/pdf/2506.01480v1>|通过强化学习实现视觉理解和生成的协同进化，推动图像生成进入迭代自省过程。|
|🆕 发布|DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing|DNAEdit：基于文本引导的校正流编辑的直接噪声对齐|Chenxi Xie, Minghan Li, Shuai Li, Yuhui Wu, Qiaosi Yi, Lei Zhang|<http://arxiv.org/pdf/2506.01430v1>|[代码](https://xiechenxi99.github.io/DNAEdit); 提出DNAEdit，通过直接噪声对齐和移动速度引导，显著提升了文本指导下的图像编辑效果。|
|🆕 发布|Playing with Transformer at 30+ FPS via Next-Frame Diffusion|通过下一帧扩散在30+ FPS下玩转Transformer|Xinle Cheng, Tianyu He, Jiayi Xu, Junliang Guo, Di He, Jiang Bian|<http://arxiv.org/pdf/2506.01380v1>|提出Next-Frame Diffusion，通过并行生成和高效推理实现超过30FPS的实时视频生成...|
|📝 更新|RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection|雷达：通过补充知识注入增强放射学报告生成|Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu|<http://arxiv.org/pdf/2505.14318v2>|RADAR通过结合LLM内部知识和外部补充知识，提升了放射学报告生成的准确性和信息量。|
|🆕 发布|Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation|超高清图像生成：数据、方法和评估|Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang|<http://arxiv.org/pdf/2506.01331v1>|[代码](https://github.com/zhang0jhon/diffusion-4k.); 建立了Aesthetic-4K数据集，提出Diffusion-4K框架，显著提升超高清图像合成质量。|
|🆕 发布|A 2-Stage Model for Vehicle Class and Orientation Detection with Photo-Realistic Image Generation|两阶段车辆类别和姿态检测及照片级图像生成模型|Youngmin Kim, Donghwa Kang, Hyeongboo Baek|<http://arxiv.org/pdf/2506.01338v1>|提出一种两阶段模型，结合照片级图像生成，有效解决车辆类别和方向检测中的数据不平衡和预测难题。|
|🆕 发布|$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models|Ψ-Sampler：基于SMC的得分模型推理时奖励对齐的初始粒子采样|Taehoon Yoon, Yunhong Min, Kyeongmin Yeo, Minhyuk Sung|<http://arxiv.org/pdf/2506.01320v1>|提出$\Psi$-Sampler，通过从奖励感知后验初始化粒子，有效提升基于分数模型的推理时奖励对齐...|
|📝 更新|FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models|FactCheXcker：缓解胸部X光报告生成模型中的测量幻觉|Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar|<http://arxiv.org/pdf/2411.18672v3>|[代码](https://github.com/rajpurkarlab/FactCheXcker.); FactCheXcker通过改进查询-代码-更新范式，有效减少医学报告生成模型中的测量幻觉。|
|📝 更新|Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings|通过理解生成：具有逻辑符号归一化的神经视觉生成|Yifei Peng, Zijie Zha, Yu Jin, Zhexu Luo, Wang-Zhou Dai, Zhong Ren, Yao-Xiang Ding, Kun Zhou|<http://arxiv.org/pdf/2310.17451v3>|[代码](https://github.com/future-item/AbdGen.); 提出AbdGen方法，通过逻辑符号强化，使神经视觉生成模型可控，提升模型透明度和泛化能力。|
|📝 更新|Fact-Checking of AI-Generated Reports|人工智能生成报告的事实核查|Razi Mahmood, Diego Machado Reyes, Ge Wang, Mannudeep Kalra, Pingkun Yan|<http://arxiv.org/pdf/2307.14634v2>|提出了一种利用图像验证AI生成报告真实性的方法，以检测并移除虚假句子。|
|📝 更新|Probing Equivariance and Symmetry Breaking in Convolutional Networks|探究卷积网络中的等变性探测和对称性破缺|Sharvaree Vadgama, Mohammad Mohaiminul Islam, Domas Buracas, Christian Shewmake, Artem Moskalev, Erik Bekkers|<http://arxiv.org/pdf/2501.01999v3>|通过理论分析和实证研究，提出了一种统一的群卷积架构，揭示了在特定任务几何下，约束更强的等变模型优于非...|


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment|Diff2Flow：通过扩散模型对齐训练流匹配模型|Johannes Schusterbauer, Ming Gui, Frank Fundel, Björn Ommer|<http://arxiv.org/pdf/2506.02221v1>|[代码](https://github.com/CompVis/diff2flow.); 提出Diff2Flow框架，通过扩散模型与流匹配模型结合，实现高效迁移知识并提升性能。|
|🆕 发布|RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge Report|从智能手机RGB图像中重建原始图像。NTIRE 2025挑战赛报告|Marcos V. Conde, Radu Timofte, Radu Berdan, Beril Besbinar, Daisuke Iso, Pengzhou Ji, Xiong Dun, Zeying Fan .etc.|<http://arxiv.org/pdf/2506.01947v1>|该论文提出了一种从sRGB图像恢复RAW图像的方法，为生成逼真RAW数据建立了新标准。|
|📝 更新|VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL|VARD：基于价值强化学习的扩散模型高效且密集微调|Fengyuan Dai, Zifeng Zhuang, Yufei Huang, Siteng Huang, Bangyan Liao, Donglin Wang, Fajie Yuan|<http://arxiv.org/pdf/2505.15791v2>|提出VARD方法，通过价值函数和KL正则化实现扩散模型的高效和密集微调。|
|🆕 发布|Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?|像素级指标对稀疏视图计算机断层扫描重建是否可靠？|Tianyu Lin, Xinran Li, Chuntung Zhuang, Qi Chen, Yuanhao Cai, Kai Ding, Alan L. Yuille, Zongwei Zhou|<http://arxiv.org/pdf/2506.02093v1>|提出新方法评估CT重建结构完整性，显著提升关键结构重建质量。|
|🆕 发布|Is Extending Modality The Right Path Towards Omni-Modality?|迈向全模态的正确路径是扩展模态吗？|Tinghui Zhu, Kai Zhang, Muhao Chen, Yu Su|<http://arxiv.org/pdf/2506.01872v1>|探究了扩展模态对多模态模型的影响，并提出了一种新的方法以实现更全面的跨模态理解。|
|🆕 发布|UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment|UMA：基于多级表面对齐的超详细人像|Heming Zhu, Guoxing Sun, Christian Theobalt, Marc Habermann|<http://arxiv.org/pdf/2506.01802v1>|提出了一种通过多级表面对齐学习超详细人偶模型的方法，显著提升了渲染质量和几何精度。|
|🆕 发布|R2SM: Referring and Reasoning for Selective Masks|R2SM：选择性掩码的指称和推理|Yu-Lin Shih, Wei-En Tai, Cheng Sun, Yu-Chiang Frank Wang, Hwann-Tzong Chen|<http://arxiv.org/pdf/2506.01795v1>|提出R2SM任务，通过用户意图驱动选择掩码类型，挑战视觉语言模型进行意图感知的图像分割。|
|🆕 发布|WoMAP: World Models For Embodied Open-Vocabulary Object Localization|WoMAP：用于具身开放词汇物体定位的世界模型|Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa .etc.|<http://arxiv.org/pdf/2506.01600v1>|检测机器人通过训练世界模型实现开放词汇物体定位，显著提升定位准确性和泛化能力。|
|🆕 发布|Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment|通过对抗者偏好对齐增强基于扩散的无限制对抗攻击|Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li .etc.|<http://arxiv.org/pdf/2506.01511v1>|[代码](https://github.com/deep-kaixun/APA.); 提出APA框架，通过对抗偏好对齐提升扩散模型生成无限制对抗样本的攻击效果。|
|📝 更新|SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion|SwiftEdit：通过一步扩散实现闪电般的文本引导图像编辑|Trong-Tung Nguyen, Quang Nguyen, Khoi Nguyen, Anh Tran, Cuong Pham|<http://arxiv.org/pdf/2412.04301v4>|SwiftEdit通过一步扩散实现即时文本引导图像编辑，大幅提升速度并保持编辑效果。|
|📝 更新|CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image|CAP-Net：一种从单张RGB-D图像中统一估计分类可动部件的6D姿态和尺寸的网络|Jingshun Huang, Haitao Lin, Tianyu Wang, Yanwei Fu, Xiangyang Xue, Yi Zhu|<http://arxiv.org/pdf/2504.11230v3>|提出CAP-Net，一种结合RGB-D特征，实现单阶段6D姿态和尺寸估计的新方法，显著提升精度并增强...|
|🆕 发布|Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark|面向可扩展的视频异常检索：一个合成视频-文本基准|Shuyu Yang, Yilun Wang, Yaxiong Wang, Li Zhu, Zhedong Zheng|<http://arxiv.org/pdf/2506.01466v1>|[代码](https://svta-mm.github.io/SVTA.github.io); 构建了首个大规模跨模态异常检索基准SVTA，利用生成模型解决数据稀缺和隐私问题。|
|📝 更新|ARFlow: Human Action-Reaction Flow Matching with Physical Guidance|ARFlow：基于物理引导的人体动作-反应流匹配|Wentao Jiang, Jingya Wang, Kaiyang Ji, Baoxiong Jia, Siyuan Huang, Ye Shi|<http://arxiv.org/pdf/2503.16973v3>|提出ARFlow，通过直接映射和物理引导机制，有效解决动作-反应合成中的复杂性和物理违规问题。|
|🆕 发布|AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning|AgentCPM-GUI：基于强化微调构建移动使用代理|Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong .etc.|<http://arxiv.org/pdf/2506.01391v1>|提出AgentCPM-GUI，通过预训练和强化学习，构建了适用于移动设备的鲁棒GUI智能交互代理。|
|🆕 发布|Synthetic Data Augmentation using Pre-trained Diffusion Models for Long-tailed Food Image Classification|基于预训练扩散模型的合成数据增强在长尾食品图像分类中的应用|GaYeon Koh, Hyun-Jic Oh, Jeonghyun Noh, Won-Ki Jeong|<http://arxiv.org/pdf/2506.01368v1>|提出利用预训练扩散模型进行两阶段合成数据增强，有效解决长尾食品图像分类中的类别不平衡问题。|
|📝 更新|VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models|VL-RewardBench：视觉-语言生成奖励模型的挑战性基准|Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu .etc.|<http://arxiv.org/pdf/2411.17451v2>|VL-RewardBench构建挑战性基准，揭示视觉语言生成奖励模型局限。|
|📝 更新|The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning|双管齐下的可解释令牌嵌入：扩散模型反学习中的越狱攻击与防御|Siyi Chen, Yimeng Zhang, Sijia Liu, Qing Qu|<http://arxiv.org/pdf/2504.21307v2>|提出了一种可解释的攻击方法，揭示了扩散模型未学习模型仍保留有害概念的原因，并开发了相应的防御策略。|
|📝 更新|Mixed-View Panorama Synthesis using Geospatially Guided Diffusion|基于地理空间引导扩散的混合视角全景合成|Zhexiao Xiong, Xin Xing, Scott Workman, Subash Khanal, Nathan Jacobs|<http://arxiv.org/pdf/2407.09672v2>|提出了一种利用卫星图像和少量输入全景图合成新全景图的方法，有效应对稀疏或远离目标区域的全景图。|


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images|对称视觉对比优化：用最小对比图像对齐视觉-语言模型|Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber|<http://arxiv.org/pdf/2502.13928v2>|提出S-VCO方法，通过最小对比图像优化视觉语言模型，显著提升其视觉依赖任务性能。|
|🆕 发布|Image Generation from Contextually-Contradictory Prompts|从情境矛盾提示中生成图像|Saar Huberman, Or Patashnik, Omer Dahary, Ron Mokady, Daniel Cohen-Or|<http://arxiv.org/pdf/2506.01929v1>|提出了一种解决文本提示中语义冲突的图像生成方法，通过阶段感知提示分解框架实现准确图像生成。|
|🆕 发布|OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation|全视V2V：通过动态内容操作实现的多功能视频生成与编辑|Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou .etc.|<http://arxiv.org/pdf/2506.01801v1>|OmniV2V通过动态内容操作，实现跨场景的视频生成与编辑，提升视频处理能力。|
|📝 更新|Survey on Vision-Language-Action Models|视觉-语言-动作模型综述|Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Dinmukhammed Mukashev .etc.|<http://arxiv.org/pdf/2502.06851v3>|该论文利用大型语言模型自动生成视觉-语言-动作模型综述，为AI辅助文献综述提供初步框架。|
|📝 更新|TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images|文本破坏者：一种无需训练和标注的图像异常文本破坏扩散方法|Mengcheng Li, Fei Chao, Chia-Wen Lin, Rongrong Ji|<http://arxiv.org/pdf/2411.00355v2>|提出TextDestroyer，一种无需训练和标注的图像文字消除方法，彻底消除文字信息，保护隐私。|
|📝 更新|DIS-CO: Discovering Copyrighted Content in VLMs Training Data|DIS-CO：在VLMs训练数据中发现受版权保护的内容|André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li|<http://arxiv.org/pdf/2502.17358v3>|[代码](https://github.com/avduarte333/DIS-CO); DIS-CO通过查询VLM识别特定帧，推断训练数据中是否包含版权内容。|
|🆕 发布|DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion|DiffuseSlide：无需训练的高帧率视频生成扩散|Geunmin Hwang, Hyun-kyu Ko, Younghyun Kim, Seungryong Lee, Eunbyung Park|<http://arxiv.org/pdf/2506.01454v1>|DiffuseSlide通过利用低帧率视频的关键帧和独特技术，实现了无需训练的高帧率视频生成，显著提...|
|🆕 发布|PointT2I: LLM-based text-to-image generation via keypoints|基于关键点的LLM文本到图像生成|Taekyung Lee, Donggyu Lee, Myungjoo Kang|<http://arxiv.org/pdf/2506.01370v1>|提出PointT2I，利用LLM生成与文本描述的人类姿态准确对应的图像。|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?|从简单视觉推理泛化到困难视觉推理：我们能否减轻视觉语言模型中的模态不平衡问题？|Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora|<http://arxiv.org/pdf/2501.02669v2>|提出了一种通过简单到复杂任务训练策略，缓解视觉语言模型模态不平衡问题的方法。|
|🆕 发布|MS-RAFT-3D: A Multi-Scale Architecture for Recurrent Image-Based Scene Flow|MS-RAFT-3D：一种基于循环图像的多尺度场景流架构|Jakob Schmid, Azin Jahedi, Noah Berenguel Senn, Andrés Bruhn|<http://arxiv.org/pdf/2506.01443v1>|[代码](https://github.com/cv-stuttgart/MS-RAFT-3D.); 开发了一种多尺度架构，显著提升了基于图像的场景流预测精度。|
|🆕 发布|No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond|尚未训练已获收益：迈向通用的体育及多领域多目标跟踪|Tomasz Stanczyk, Seongro Yoon, Francois Bremond|<http://arxiv.org/pdf/2506.01373v1>|[代码](https://github.com/tstanczyk95/McByte.); 提出McByte，一种无需训练的跟踪框架，通过时间传播分割掩码提升多目标跟踪鲁棒性。|
|📝 更新|Distractor-free Generalizable 3D Gaussian Splatting|无干扰泛化3D高斯分层|Yanqi Bao, Jing Liao, Jing Huo, Yang Gao|<http://arxiv.org/pdf/2411.17605v2>|[代码](https://github.com/bbbbby-99/DGGS.); 提出DGGS，一种解决3D场景中干扰数据影响，实现无干扰通用3D高斯分层渲染的方法。|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction|Rig3R：基于Rig感知的3D重建学习条件|Samuel Li, Pujith Kachana, Prajwal Chidananda, Saurabh Nair, Yasutaka Furukawa, Matthew Brown|<http://arxiv.org/pdf/2506.02265v1>|Rig3R通过结合结构信息，显著提升了从多相机机位估计3D场景结构和相机位姿的性能。|
|🆕 发布|SAB3R: Semantic-Augmented Backbone in 3D Reconstruction|SAB3R：3D重建中的语义增强骨干网络|Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng|<http://arxiv.org/pdf/2506.02112v1>|提出了一种结合语义增强和轻量级蒸馏的3D重建方法，实现了语义分割和3D重建的统一。|
|📝 更新|MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors|MASt3R-SLAM：基于3D重建先验的实时稠密SLAM|Riku Murai, Eric Dexheimer, Andrew J. Davison|<http://arxiv.org/pdf/2412.12392v2>|MASt3R-SLAM通过引入3D重建先验，实现了实时单目SLAM，在多种基准测试中达到最先进性能。|


### 视觉定位与映射 (Visual Localization & Mapping)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss|PAIR-Net：通过预训练音频-视觉融合和对齐损失增强自视角说话人检测|Yu Wang, Juhyung Ha, David J. Crandall|<http://arxiv.org/pdf/2506.02247v1>|PAIR-Net通过预训练音频-视觉融合和同步损失，有效提升了自拍摄像头视频中的主动说话者检测性能。|


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Quantifying task-relevant representational similarity using decision variable correlation|使用决策变量相关性量化任务相关表征相似度|Yu, Qian, Wilson S. Geisler, Xue-Xin Wei|<http://arxiv.org/pdf/2506.02164v1>|提出决策变量相关系数方法，量化模型间任务相关表征相似度，揭示模型与生物视觉系统差异。|
|📝 更新|SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model|海溅：利用3D高斯溅射和物理基础图像形成模型表示水下场景|Daniel Yang, John J. Leonard, Yogesh Girdhar|<http://arxiv.org/pdf/2409.17345v2>|SeaSplat通过结合3D高斯分层和物理图像形成模型，实现了水下场景的实时渲染，同时恢复真实色彩并...|
|📝 更新|MaxSup: Overcoming Representation Collapse in Label Smoothing|MaxSup：克服标签平滑中的表示崩溃|Yuxuan Zhou, Heng Li, Zhi-Qi Cheng, Xudong Yan, Yifei Dong, Mario Fritz, Margret Keuper|<http://arxiv.org/pdf/2502.15798v2>|[代码](https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization); MaxSup通过惩罚预测而非真实标签，克服了标签平滑导致的过自信和特征表示压缩问题，提升了模型泛化能...|
|📝 更新|A Survey of 3D Reconstruction with Event Cameras|3D重建与事件相机综述|Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu, Weidong Cai|<http://arxiv.org/pdf/2505.08438v2>|首次全面综述基于事件相机的3D重建，分类并组织现有方法，并讨论未来挑战。|
|📝 更新|Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)|基于低秩正则化流形的运动补偿心脏磁共振成像（DMoCo）|Joseph Kettelkamp, Ludovica Romanin, Sarv Priya, Mathews Jacob|<http://arxiv.org/pdf/2505.03149v3>|提出了一种基于低秩模型的运动补偿算法，有效提升了自由呼吸心脏MRI图像重建质量。|
|🆕 发布|FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens|FreqPolicy：基于频率自回归的连续标记视觉运动策略|Yiming Zhong, Yumeng Liu, Chuyang Xiao, Zemin Yang, Youzhuo Wang, Yufei Zhu, Ye Shi, Yujing Sun .etc.|<http://arxiv.org/pdf/2506.01583v1>|提出了一种基于频率域和连续隐表示的视觉运动策略学习新范式，有效提升了机器人操作精度和效率。|
|📝 更新|A Survey on Event-driven 3D Reconstruction: Development under Different Categories|事件驱动3D重建综述：不同类别下的进展|Chuanzhi Xu, Haoxian Zhou, Haodong Chen, Vera Chung, Qiang Qu|<http://arxiv.org/pdf/2503.19753v3>|综述了基于事件驱动的3D重建方法，分类并分析了几何、学习和混合方法，并指出了未来研究方向。|
|🆕 发布|Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression|傅里叶调制隐式神经网络在多光谱卫星图像压缩中的应用|Woojin Cho, Steve Andreas Immanuel, Junhyuk Heo, Darongsae Kwon|<http://arxiv.org/pdf/2506.01234v1>|提出了一种基于隐式神经表示和傅里叶调制的多光谱卫星图像压缩框架，有效解决高维度和分辨率问题。|


## 时序视觉分析 (Temporal Visual Analysis)


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency|视频LLMs的强化学习调优：奖励设计和数据效率|Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, Si Liu|<http://arxiv.org/pdf/2506.01908v1>|[代码](https://github.com/appletea233/Temporal-R1); 提出了一种基于强化学习的视频理解方法，通过优化奖励设计和数据选择，显著提升了多模态大语言模型在视频理...|
|🆕 发布|EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models|EPFL-Smart-Kitchen-30：密集标注的烹饪数据集，包含3D运动学，以挑战视频和语言模型|Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera .etc.|<http://arxiv.org/pdf/2506.01608v1>|[代码](https://github.com/amathislab/EPFL-Smart-Kitchen); 构建了密集标注的烹饪数据集，提出多模态基准挑战视频和语言模型，以促进行为理解和建模。|
|🆕 发布|ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding|ReAgent-V：一个基于奖励的多智能体视频理解框架|Yiyang Zhou, Yangfan He, Yaofeng Su, Siwei Han, Joel Jang, Gedas Bertasius, Mohit Bansal, Huaxiu Yao|<http://arxiv.org/pdf/2506.01300v1>|提出ReAgent-V框架，通过实时奖励和高效帧选择提升视频理解能力。|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Efficient Egocentric Action Recognition with Multimodal Data|高效的多模态数据自回归动作识别|Marco Calzavara, Ard Kastrati, Matteo Macchini, Dushan Vasilevski, Roger Wattenhofer|<http://arxiv.org/pdf/2506.01757v1>|通过降低RGB帧采样率并提高3D手姿输入频率，实现了高效、实时的人体动作识别。|
|🆕 发布|SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition|半监督视频Transformer在手术阶段识别中的应用|Yiping Li, Ronald de Jong, Sahar Nasirihaghighi, Tim Jaspers, Romy van Jaarsveld, Gino Kuiper, Richard van Hillegersberg, Fons van der Sommen .etc.|<http://arxiv.org/pdf/2506.01471v1>|检测手术阶段识别，提出半监督视频Transformer模型，利用未标记数据提升性能。|
|🆕 发布|EgoBrain: Synergizing Minds and Eyes For Human Action Understanding|自我脑：协同心灵与视觉以理解人类行为|Nie Lin, Yansen Wang, Dongqi Han, Weibang Jiang, Jingyuan Li, Ryosuke Furuta, Yoichi Sato, Dongsheng Li|<http://arxiv.org/pdf/2506.01353v1>|EgoBrain通过融合脑电图和视觉数据，实现了对人类行为理解的突破性进展。|


### 时序建模与预测 (Temporal Modeling & Prediction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|AdaWorld: Learning Adaptable World Models with Latent Actions|AdaWorld：通过潜在动作学习可适应的世界模型|Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan|<http://arxiv.org/pdf/2503.18938v4>|AdaWorld通过在预训练中融入动作信息，实现了高效适应新环境的可适应世界模型学习。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination|增强生物医学多模态表示学习：基于多尺度预训练和扰动报告判别|Xinliu Zhong, Kayhan Batmanghelich, Li Sun|<http://arxiv.org/pdf/2506.01902v1>|提出一种基于扰动报告判别和多尺度预训练的方法，有效提升生物医学多模态表示学习。|
|📝 更新|Robust Multimodal Learning via Cross-Modal Proxy Tokens|鲁棒的多模态学习通过跨模态代理标记|Md Kaykobad Reza, Ameya Patil, Mashhour Solh, M. Salman Asif|<http://arxiv.org/pdf/2501.17823v3>|提出了一种通过跨模态代理标记增强多模态模型鲁棒性的方法，有效应对缺失模态问题。|
|🆕 发布|Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition|脊波former：多阶段对比训练用于细粒度跨域指纹识别|Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur|<http://arxiv.org/pdf/2506.01806v1>|提出了一种多阶段对比训练的指纹识别方法，有效提升了无接触指纹匹配的准确性和可靠性。|
|📝 更新|Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video Retrieval|对比对齐与语义差距感知校正的文本-视频检索|Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong|<http://arxiv.org/pdf/2505.12499v4>|提出GARE框架，通过语义差距感知校正缓解文本-视频检索中的模式差距，提升检索准确性和鲁棒性。|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Scalable Multi-Robot Informative Path Planning for Target Mapping via Deep Reinforcement Learning|可扩展的多机器人信息路径规划：通过深度强化学习进行目标映射|Apoorva Vashisth, Manav Kulshrestha, Damon Conover, Aniket Bera|<http://arxiv.org/pdf/2409.16967v3>|[代码](https://github.com/AccGen99/marl_ipp); 提出了一种基于深度强化学习的多机器人信息路径规划方法，显著提升了目标映射效率。|
|🆕 发布|GSCodec Studio: A Modular Framework for Gaussian Splat Compression|GSCodec Studio：一个用于高斯喷溅压缩的模块化框架|Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu|<http://arxiv.org/pdf/2506.01822v1>|[代码](https://github.com/JasonLSC/GSCodec_Studio); GSCodec Studio提出模块化框架，统一压缩和重建Gaussian Splat，提升静态和动...|
|🆕 发布|Data Pruning by Information Maximization|数据剪枝通过信息最大化|Haoru Tan, Sitong Wu, Wei Huang, Shizhen Zhao, Xiaojuan Qi|<http://arxiv.org/pdf/2506.01701v1>|InfoMax通过最大化信息量和最小化冗余，实现了高效的数据剪枝和核心集选择。|
|🆕 发布|Active Learning via Vision-Language Model Adaptation with Open Data|通过开放数据实现视觉-语言模型自适应的主动学习|Tong Wang, Jiaqi Wang, Shu Kong|<http://arxiv.org/pdf/2506.01724v1>|提出一种利用开放资源进行主动学习的视觉语言模型自适应方法，显著提升模型性能。|
|📝 更新|S2A: A Unified Framework for Parameter and Memory Efficient Transfer Learning|S2A：一种参数和内存高效迁移学习的统一框架|Tian Jin, Enjun Du, Changwei Wang, Wenhao Xu, Ding Luo|<http://arxiv.org/pdf/2503.08154v3>|提出S2A框架，通过优化模型结构和激活量化，显著降低参数和内存占用，提升PETL方法在内存受限设备上...|
|🆕 发布|Multi-Modal Dataset Distillation in the Wild|野外多模态数据集蒸馏|Zhuohang Dang, Minnan Luo, Chengyou Jia, Hangwei Qian, Xiaojun Chang, Ivor W. Tsang|<http://arxiv.org/pdf/2506.01586v1>|提出MDW框架，有效压缩并净化多模态数据集，提升模型训练效率和性能。|
|🆕 发布|NTIRE 2025 the 2nd Restore Any Image Model (RAIM) in the Wild Challenge|NTIRE 2025 第二届“野外任意图像恢复模型”（RAIM）挑战赛|Jie Liang, Radu Timofte, Qiaosi Yi, Zhengqiang Zhang, Shuaizheng Liu, Lingchen Sun, Rongyuan Wu, Xindong Zhang .etc.|<http://arxiv.org/pdf/2506.01394v1>|建立了NTIRE 2025 RAIM挑战，推动真实图像修复技术发展。|


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images|超越美丽图片：结合单图像和多图像超分辨率处理Sentinel-2图像|Aditya Retnanto, Son Le, Sebastian Mueller, Armin Leitner, Michael Riffler, Konrad Schindler, Yohan Iddawela|<http://arxiv.org/pdf/2505.24799v2>|提出SEN4X混合超分辨率架构，结合单图和多图技术提升Sentinel-2图像分辨率，显著提高城市土...|
|🆕 发布|Robust Federated Learning against Noisy Clients via Masked Optimization|基于掩码优化的鲁棒联邦学习以对抗噪声客户端|Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, Min Liu|<http://arxiv.org/pdf/2506.02079v1>|[代码](https://github.com/Sprinter1999/MaskedOptim); 提出了一种通过掩码优化和标签校正机制，增强联邦学习鲁棒性的方法，有效应对噪声客户端问题。|
|🆕 发布|Sheep Facial Pain Assessment Under Weighted Graph Neural Networks|基于加权图神经网络的羊面部疼痛评估|Alam Noor, Luis Almeida, Mohamed Daoudi, Kai Li, Eduardo Tovar|<http://arxiv.org/pdf/2506.01468v1>|提出一种加权图神经网络模型，通过羊面部特征检测和疼痛等级预测，提高羊面部疼痛评估的准确性。|
|📝 更新|GD doesn't make the cut: Three ways that non-differentiability affects neural network training|GD无法过关：非可微性对神经网络训练的三个影响|Siddharth Krishna Kumar|<http://arxiv.org/pdf/2401.08426v8>|揭示了非光滑函数梯度方法与经典梯度下降在神经网络训练中的差异，挑战了现有优化理论。|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Semantic Palette-Guided Color Propagation|语义调色板引导的颜色传播|Zi-Yu Zhang, Bing-Feng Seng, Ya-Feng Du, Kang Li, Zhe-Cheng Wang, Zheng-Jun Du|<http://arxiv.org/pdf/2506.01441v1>|提出了一种基于语义调色板的色彩传播方法，实现内容感知的局部色彩编辑。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation|双编码特征过滤广义注意力UNET用于视网膜血管分割|Md Tauhidul Islam, Wu Da-Wen, Tang Qing-Qing, Zhao Kai-Yang, Yin Teng, Li Yan-Fei, Shang Wen-Yi, Liu Jing-Yu .etc.|<http://arxiv.org/pdf/2506.02312v1>|提出了一种基于双重编码和注意力引导的UNET模型，有效提升了视网膜血管分割的准确性和泛化能力。|
|📝 更新|GASP: Gaussian Splatting for Physic-Based Simulations|GASP：基于物理模拟的高斯喷溅|Piotr Borycki, Weronika Smolak, Joanna Waczyńska, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek|<http://arxiv.org/pdf/2409.05819v2>|提出了一种基于Gaussian Splatting的物理模拟方法，有效提升了3D场景渲染性能。|
|🆕 发布|Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models|激励大型语言模型进行高级指令遵循的推理|Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li .etc.|<http://arxiv.org/pdf/2506.01413v1>|[代码](https://github.com/yuleiqin/RAIF.); 提出了一种通过激励推理提升大型语言模型复杂指令遵循能力的方法。|


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent|快速且鲁棒的各向异性坐标下降旋转平均|Yaroslava Lochman, Carl Olsson, Christopher Zach|<http://arxiv.org/pdf/2506.01940v1>|[代码](https://ylochman.github.io/acd); 提出了一种快速且鲁棒的旋转平均方法，有效解决了传统方法的效率和鲁棒性问题。|
|📝 更新|Certified Robustness to Clean-Label Poisoning Using Diffusion Denoising|基于扩散去噪的针对干净标签投毒的认证鲁棒性|Sanghyun Hong, Nicholas Carlini, Alexey Kurakin|<http://arxiv.org/pdf/2403.11981v2>|提出了一种利用扩散去噪模型防御清洁标签投毒攻击的方法，显著降低了攻击成功率。|


### 不确定性量化 (Uncertainty Quantification)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates|基于CLIPScore质量估计的粒度词语评估与不确定性校准的共形风险控制框架|Gonçalo Gomes, Bruno Martins, Chrysoula Zerva|<http://arxiv.org/pdf/2504.01225v2>|检测CLIPScore评估的误差和不确定性，提出了一种基于模型无关的符合性风险控制框架来改进质量估计...|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 零/少样本泛化 (Zero/Few-shot Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation|通过置信度引导的数据增强改善未知协变量偏移下的知识蒸馏|Niclas Popp, Kevin Alexander Laube, Matthias Hein, Lukas Schott|<http://arxiv.org/pdf/2506.02294v1>|通过引入基于差异最大化的数据增强策略，有效提升了知识蒸馏在未知协变量偏移情况下的鲁棒性。|


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Hierarchical Material Recognition from Local Appearance|从局部外观进行分层材料识别|Matthew Beveridge, Shree K. Nayar|<http://arxiv.org/pdf/2505.22911v2>|提出了一种基于图注意力网络和材料分类层次结构的材料识别方法，显著提升了识别准确性和泛化能力。|
|📝 更新|Monge-Ampere Regularization for Learning Arbitrary Shapes from Point Clouds|从点云学习任意形状的Monge-Ampère正则化|Chuanxiang Yang, Yuanfeng Zhou, Guangshun Wei, Long Ma, Junhui Hou, Yuan Liu, Wenping Wang|<http://arxiv.org/pdf/2410.18477v3>|[代码](https://github.com/chuanxiang-yang/S2DF.); 提出了一种基于Monge-Ampere正则化的方法，从点云中学习任意形状，无需地面真实值监督。|
|📝 更新|MultiFlow: A unified deep learning framework for multi-vessel classification, segmentation and clustering of phase-contrast MRI validated on a multi-site single ventricle patient cohort|多流：一种用于多血管分类、分割和聚类相位对比MRI的统一深度学习框架，在多中心单心室患者队列中得到验证|Tina Yao, Nicole St. Clair, Madeline Gong, Gabriel F. Miller, Jennifer A. Steeden, Rahul H. Rathod, Vivek Muthurangu, FORCE Investigators|<http://arxiv.org/pdf/2502.11993v2>|MultiFlow提出了一种深度学习框架，自动分类和分割单心室患者的心血管，并识别出与死亡/移植和肝...|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 视觉导航与路径规划 (Visual Navigation & Path Planning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation|进化导航：基于LLM的视觉语言导航的自适应实体推理|Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han .etc.|<http://arxiv.org/pdf/2506.01551v1>|[代码](https://github.com/expectorlin/EvolveNav.); 提出EvolveNav框架，通过形式化思维链训练和自我反思后训练，提升基于LLM的视觉语言导航的准确...|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Entity Image and Mixed-Modal Image Retrieval Datasets|实体图像和混合模态图像检索数据集|Cristian-Ioan Blaga, Paul Suganthan, Sahil Dua, Krishna Srinivasan, Enrique Alfonseca, Peter Dornbach, Tom Duerig, Imed Zitouni .etc.|<http://arxiv.org/pdf/2506.02291v1>|[代码](https://github.com/google-research-datasets/wit-retrieval.); 构建了两个新数据集，为混合模态图像检索提供了基准和评估工具。|
|🆕 发布|CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention|CLIP驱动雨感知：基于模式感知网络路由和掩码引导交叉注意力的自适应去雨|Cong Guan, Osamu Yoshie|<http://arxiv.org/pdf/2506.01366v1>|提出CLIP-RPN，通过自适应路由和跨模态注意力机制，有效处理不同雨型，实现雨景去雨。|


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos|Fire360：退化360度消防视频中的鲁棒感知和情景记忆基准|Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, Klara Nahrstedt|<http://arxiv.org/pdf/2506.02167v1>|Fire360构建了一个评估消防场景中视觉感知和推理的基准，以提升模型在恶劣环境下的表现。|
|🆕 发布|SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis|SynthRL：通过可验证数据合成扩展视觉推理|Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh|<http://arxiv.org/pdf/2506.02096v1>|SynthRL通过可验证数据合成，有效提升视觉推理模型的泛化能力。|
|📝 更新|Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward|揭示LVLM对基本视觉变化的鲁棒性不足：原因与前进之路|Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R. Fung|<http://arxiv.org/pdf/2504.16727v3>|揭示了大型视觉语言模型对视觉变化的脆弱性，并提出了一种评估和改进其鲁棒性的框架。|
|🆕 发布|MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs|MoDA：用于指令式多模态语言模型中细粒度视觉定位的调制适配器|Wayner Barrios, Andrés Villa, Juan León Alcázar, SouYoung Jin, Bernard Ghanem|<http://arxiv.org/pdf/2506.01850v1>|MoDA通过指令引导的调制，有效提升了视觉概念在复杂场景中的定位能力。|
|🆕 发布|FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning|FaceCoT：基于思维链推理的人脸反欺骗基准数据集|Honglu Zhang, Zhiqin Fang, Ningning Zhao, Saihui Hou, Long Ma, Renwang Pei, Zhaofeng He|<http://arxiv.org/pdf/2506.01783v1>|构建首个针对人脸反欺骗的视觉问答数据集，并引入思维链增强渐进学习策略，显著提升模型性能。|
|🆕 发布|Visual Explanation via Similar Feature Activation for Metric Learning|基于相似特征激活的度量学习视觉解释|Yi Liao, Ugochukwu Ejike Akpudo, Jue Zhang, Yongsheng Gao, Jun Zhou, Wenyi Zeng, Weichuan Zhang|<http://arxiv.org/pdf/2506.01636v1>|提出了一种针对度量学习模型的视觉解释方法，通过相似特征激活图（SFAM）提供可解释的视觉解释。|
|📝 更新|ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla|ChitroJera：面向孟加拉语的区域相关视觉问答数据集|Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Fabiha Haider, Fariha Tanjim Shifat, Md Tasmim Rahman Adib, Anam Borhan Uddin, Md Farhan Ishmam .etc.|<http://arxiv.org/pdf/2410.14991v2>|构建了大规模孟加拉语视觉问答数据集ChitroJera，提升了孟加拉语视觉语言模型性能。|
|🆕 发布|Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation|汉服-Bench：跨时序文化理解和再创造的多模态基准|Li Zhou, Lutong Yu, Dongchu Xie, Shaohuan Cheng, Wenyan Li, Haizhou Li|<http://arxiv.org/pdf/2506.01565v1>|构建了Hanfu-Bench多模态数据集，以解决跨时文化理解和再创造中的挑战。|
|📝 更新|Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles|拼图-R1：基于规则的视觉强化学习在拼图中的应用研究|Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko|<http://arxiv.org/pdf/2505.23590v2>|[代码](https://github.com/zifuwanggg/Jigsaw-R1.); 利用拼图游戏研究规则式视觉强化学习，揭示其泛化能力与学习模式。|
|🆕 发布|Sparse Imagination for Efficient Visual World Model Planning|稀疏想象的高效视觉世界模型规划|Junha Chun, Youngjoon Jeong, Taesup Kim|<http://arxiv.org/pdf/2506.01392v1>|提出了一种通过稀疏想象降低视觉世界模型计算负担的方法，显著提升实时决策效率。|
|🆕 发布|ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition|ViTA-PAR：基于属性提示的视觉和文本属性对齐用于行人属性识别|Minjeong Park, Hongbeen Park, Jinkyu Kim|<http://arxiv.org/pdf/2506.01411v1>|[代码](https://github.com/mlnjeongpark/ViTA-PAR.); ViTA-PAR通过多模态提示和视觉-语言对齐，提升了行人属性识别的准确性和鲁棒性。|
|🆕 发布|SEMNAV: A Semantic Segmentation-Driven Approach to Visual Semantic Navigation|语义分割驱动的视觉语义导航方法：SEMNAV|Rafael Flor-Rodríguez, Carlos Gutiérrez-Álvarez, Francisco Javier Acevedo-Rodríguez, Sergio Lafuente-Arroyo, Roberto J. López-Sastre|<http://arxiv.org/pdf/2506.01418v1>|[代码](https://github.com/gramuah/semnav); SEMNAV通过语义分割增强感知，提升机器人视觉导航能力，缩小模拟与现实差距。|
|🆕 发布|SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization|SVQA-R1：通过视图一致奖励优化强化MLLMs中的空间推理|Peiyao Wang, Haibin Ling|<http://arxiv.org/pdf/2506.01371v1>|提出SVQA-R1框架，通过视角一致奖励优化强化视觉语言模型的空间推理能力。|
|🆕 发布|Learning Sparsity for Effective and Efficient Music Performance Question Answering|学习稀疏性以实现有效且高效的音乐表演问答|Xingjian Diao, Tianzhen Yang, Chunhui Zhang, Weiyi Wu, Ming Cheng, Jiang Gui|<http://arxiv.org/pdf/2506.01319v1>|提出Sparsify框架，通过稀疏学习提高音乐表演问答的效率和准确性。|
|🆕 发布|Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation|标题翻译结果：  多模态结构化知识的抽象视觉理解：MLLM评估的新视角|Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Min Zhang, Wen Zhang, Huajun Chen|<http://arxiv.org/pdf/2506.01293v1>|[代码](https://github.com/zjukg/M3STR); 提出M3STR基准，评估MLLM在抽象视觉理解中的结构化知识能力。|


### 多模态对话系统 (Multimodal Dialogue Systems)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning|电影拼图：通过多模态顺序学习进行视觉叙事推理|Jianghui Wang, Yuxuan Wang, Dongyan Zhao, Zilong Zheng|<http://arxiv.org/pdf/2306.02252v3>|MoviePuzzle通过重新排列电影片段，提升视频模型对视觉叙事结构的理解能力。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding|ReFoCUS：基于强化学习的帧优化以实现上下文理解|Hosu Lee, Junho Kim, Hyunjun Kim, Yong Man Ro|<http://arxiv.org/pdf/2506.01274v1>|ReFoCUS通过强化学习优化视频帧选择，提升视频问答模型的上下文理解能力。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MedEBench: Revisiting Text-instructed Image Editing on Medical Domain|MedEBench：重新审视医学领域的文本指令图像编辑|Minghao Liu, Zhitao He, Zhiyuan Fan, Qingyun Wang, Yi R. Fung|<http://arxiv.org/pdf/2506.01921v2>|构建了MedEBench基准，评估医疗图像编辑，提升临床应用可靠性。|
|📝 更新|GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning|GETReason：通过分层多智能体推理增强图像上下文提取|Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta|<http://arxiv.org/pdf/2505.21863v3>|GETReason通过分层多智能体推理，提升图像上下文提取，有效关联图像与事件背景。|
|🆕 发布|VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis|视觉-语言对比蒸馏用于准确高效的自动胎盘分析|Manas Mehta, Yimu Pan, Kelly Gallagher, Alison D. Gernand, Jeffery A. Goldstein, Delia Mwinyelle, Leena Mithal, James Z. Wang|<http://arxiv.org/pdf/2506.02229v1>|提出VLCD方法，通过知识蒸馏和预蒸馏技术，提高胎盘图像分析的准确性和效率。|
|📝 更新|MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders|MedVAE：基于大规模可泛化自动编码器的医疗图像高效自动解释|Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince .etc.|<http://arxiv.org/pdf/2502.14753v2>|[代码](https://github.com/StanfordMIMI/MedVAE.); MedVAE通过大规模自编码器高效压缩医学图像，同时保留关键特征，提升下游模型训练效率。|
|🆕 发布|Implicit Deformable Medical Image Registration with Learnable Kernels|隐式可变形医学图像配准与可学习核|Stefano Fogarollo, Gregor Laimer, Reto Bale, Matthias Harders|<http://arxiv.org/pdf/2506.02150v1>|提出了一种基于可学习核的隐式医学图像配准方法，显著提升了配准精度和可靠性。|
|📝 更新|In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review|图像中的医学影像数据集、伪影及其生活综述|Amelia Jiménez-Sánchez, Natalia-Rozalia Avlona, Sarah de Boer, Víctor M. Campello, Aasa Feragen, Enzo Ferrante, Melanie Ganz, Judy Wawira Gichoya .etc.|<http://arxiv.org/pdf/2501.10727v2>|提出了一种动态跟踪医疗影像数据集及其研究瑕疵的持续审查方法，以提升算法泛化能力和患者预后。|
|📝 更新|Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback|提升医学大视觉-语言模型通过异常感知反馈|Yucheng Zhou, Lingran Song, Jianbing Shen|<http://arxiv.org/pdf/2501.01377v2>|提出UMed-LVLM，通过异常感知反馈显著提升医学图像异常检测与理解能力。|
|🆕 发布|Neural shape reconstruction from multiple views with static pattern projection|基于静态图案投影的多视角神经形状重建|Ryo Furukawa, Kota Nishihara, Hiroshi Kawasaki|<http://arxiv.org/pdf/2506.01389v1>|提出了一种通过动态投影和神经SDF实现多视角无纹理物体3D形状重建的方法。|
|🆕 发布|RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes|雷达Splat：用于自动驾驶场景高保真数据合成和3D重建的雷达高斯Splatting|Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner|<http://arxiv.org/pdf/2506.01379v1>|[代码](https://umautobots.github.io/radarsplat.); 提出RadarSplat，通过结合高斯散布和雷达噪声建模，实现高保真雷达数据合成和3D场景重建。|


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote Sensing Image Object Detection|轻量级边缘高斯驱动网络用于低质量遥感图像目标检测|Wei Lu, Si-Bao Chen, Hui-Dong Li, Qing-Ling Shu, Chris H. Q. Ding, Jin Tang, Bin Luo|<http://arxiv.org/pdf/2503.14012v2>|[代码](https://github.com/lwCVer/LEGNet.); 提出了一种轻量级网络，通过边缘高斯聚合模块增强低质量遥感图像特征，有效提升低质量目标检测性能。|


### 工业视觉检测 (Industrial Visual Inspection)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset|STORM：使用综合有序回归数据集对MLLM视觉评分进行基准测试|Jinhong Wang, Shuo Tong, Jian liu, Dongqi Tang, Jintai Chen, Haochao Ying, Hongxia Xu, Danny Chen .etc.|<http://arxiv.org/pdf/2506.01738v1>|[代码](https://storm-bench.github.io/.); 构建了STORM数据集和基准，提升MLLM在视觉评级任务中的表现。|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 视觉认知计算 (Visual Cognitive Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Shape and Texture Recognition in Large Vision-Language Models|形状与纹理在大视觉-语言模型中的识别|Sagi Eppel, Mor Bismut, Alona Faktor-Strugatski|<http://arxiv.org/pdf/2503.23062v2>|构建大型形状与纹理数据集，评估并揭示了大型视觉语言模型在形状和纹理识别上的不足。|


### 量子视觉算法 (Quantum Visual Algorithms)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training|随机层级洗牌以提升视觉Mamba训练|Zizheng Huang, Haoxing Chen, Jiaqi Li, Jun Lan, Huijia Zhu, Weiqiang Wang, Limin Wang|<http://arxiv.org/pdf/2408.17081v2>|提出了一种名为SLWS的随机层间洗牌方法，有效提升了Vision Mamba模型的训练效果。|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Beyond black and white: A more nuanced approach to facial recognition with continuous ethnicity labels|超越黑白：使用连续种族标签的更细致的面部识别方法|Pedro C. Neto, Naser Damer, Jaime S. Cardoso, Ana F. Sequeira|<http://arxiv.org/pdf/2506.01532v1>|提出连续种族标签，以更细致的方法解决面部识别模型中的数据偏差问题。|
|🆕 发布|Variance-Based Defense Against Blended Backdoor Attacks|基于方差防御混合后门攻击|Sujeevan Aseervatham, Achraf Kerzazi, Younès Bennani|<http://arxiv.org/pdf/2506.01444v1>|提出一种基于方差检测的防御策略，有效识别并防御混合后门攻击。|
|🆕 发布|Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects|PMBOK指南适合人工智能吗？面对人工智能项目重新评估项目管理|Alexey Burdakov, Max Jaihyun Ahn|<http://arxiv.org/pdf/2506.02214v1>|针对AI项目，提出改进PMBOK框架，融入数据生命周期管理、迭代开发框架和伦理考量，以提升项目管理实...|
|🆕 发布|DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes|双图映射：动态变化场景中自然语言导航的在线开放词汇语义映射|Jiajun Jiang, Yiming Zhu, Zirui Wu, Jie Song|<http://arxiv.org/pdf/2506.01950v1>|DualMap通过结合全局和局部地图，实现了动态场景中的在线开放词汇语义映射和导航。|

