## [UPDATED!] **2025-06-10** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Foundation Models in Medical Imaging -- A Review and Outlook|医学成像中的基础模型——综述与展望|Vivien van Veldhuizen, Vanessa Botha, Chunyao Lu, Melis Erdal Cesur, Kevin Groot Lipman, Edwin D. de Jong, Hugo Horlings, Clárisa I. Sanchez .etc.|<http://arxiv.org/pdf/2506.09095v2>|概述了基础模型在医学成像领域的应用，通过无监督学习提高临床任务适应性。|
|🆕 发布|Do Multiple Instance Learning Models Transfer?|多实例学习模型是否具有迁移性？|Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood|<http://arxiv.org/pdf/2506.09022v2>|[代码](https://github.com/mahmoodlab/MIL-Lab); 揭示了多实例学习模型在计算病理学中的迁移性，证实预训练模型能显著提升小数据集性能。|
|🆕 发布|MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis|MIRAGE：多模态基础模型与全面视网膜OCT图像分析基准|José Morano, Botond Fazekas, Emese Sükei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie .etc.|<http://arxiv.org/pdf/2506.08900v2>|[代码](https://github.com/j-morano/MIRAGE.); 提出MIRAGE模型，一种针对视网膜OCT图像分析的多模态基础模型，并创建了新的评估基准，提升了分类...|
|🆕 发布|UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation|无监督功效蒸馏：机器人操作泛化的新方法（UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation）|Yihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, Li Fei-Fei|<http://arxiv.org/pdf/2506.09284v1>|提出无监督学习方法UAD，从基础模型中提取知识以预测物体操作能力，无需手动标注，实现机器人操作在多样...|
|📝 更新|Multimodal Pragmatic Jailbreak on Text-to-image Models|多模态语用破解在文本到图像模型中的应用|Tong Liu, Zhixin Lai, Jiawen Wang, Gengyuan Zhang, Shuo Chen, Philip Torr, Vera Demberg, Volker Tresp .etc.|<http://arxiv.org/pdf/2409.19149v2>|[代码](https://multimodalpragmatic.github.io/.); 提出了一种新型“多模态语用破解”方法，揭示了文本到图像模型生成不安全内容的风险，并提出了相应的评估体...|
|📝 更新|FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts|FC-Attack：通过自动生成的流程图破解多模态大型语言模型|Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He|<http://arxiv.org/pdf/2502.21059v2>|提出了一种利用自动生成流程图的攻击方法FC-Attack，有效突破多模态大语言模型的安全限制。|
|📝 更新|SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models|空间LLM：一种面向空间智能大型多模态模型的复合3D信息增强设计|Wufei Ma, Luoxin Ye, Celso M de Melo, Jieneng Chen, Alan Yuille|<http://arxiv.org/pdf/2505.00788v3>|[代码](https://3d-spatial-reasoning.github.io/spatial-llm); 提出了一种增强大型多模态模型3D空间推理能力的方法，通过3D数据集和模型设计优化，实现了超越GPT-...|
|🆕 发布|Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models|通过适应遮挡视觉模型实现高效的医学视觉-语言对齐|Chenyu Lian, Hong-Yu Zhou, Dongyun Liang, Jing Qin, Liansheng Wang|<http://arxiv.org/pdf/2506.08990v1>|[代码](https://github.com/DopamineLcy/ALTA.); 提出了一种高效的医学视觉语言对齐方法ALTA，通过适配掩码视觉模型，大幅减少参数和计算量，提升了对齐...|
|🆕 发布|Segment Concealed Objects with Incomplete Supervision|用不完全监督分割隐藏对象|Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang .etc.|<http://arxiv.org/pdf/2506.08955v1>|提出了一种统一框架SEE，利用半监督学习和特征聚类解决不完全监督下隐蔽对象分割问题，实现了一流性能。|
|📝 更新|TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group Resampler|TinyLLaVA-Video：面向视频理解的更小语言模型组重采样技术|Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang|<http://arxiv.org/pdf/2501.15513v2>|[代码](https://github.com/ZhangXJ199/TinyLLaVA-Video.); 提出了一种创新的视频级分组重采样机制，显著减小了轻量级视频理解模型的参数量，同时提升了性能和训练效率...|
|📝 更新|CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation|CAD-Llama：利用大型语言模型进行计算机辅助设计参数化3D模型生成|Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou|<http://arxiv.org/pdf/2505.04481v2>|提出了一种增强大型语言模型生成能力的方法，通过自适应预训练和指令微调，实现了参数化3D CAD模型的...|
|🆕 发布|Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models|Landsat-Bench：Landsat基础模型的数据集和基准|Isaac Corley, Lakshay Sharma, Ruth Crasto|<http://arxiv.org/pdf/2506.08780v1>|提出了Landsat-Bench，通过定制化基准数据集促进了Landsat地理空间基础模型的性能提升...|
|📝 更新|Enhancing Safety of Foundation Models for Visual Navigation through Collision Avoidance via Repulsive Estimation|通过排斥估计实现避障以提高视觉导航基础模型的安全性|Joonkyung Kim, Joonyeol Sim, Woojun Kim, Katia Sycara, Changjoo Nam|<http://arxiv.org/pdf/2506.03834v2>|提出CARE模块，通过排斥力估计增强视觉导航的安全性，无需额外传感器或模型微调。|
|📝 更新|EVA: An Embodied World Model for Future Video Anticipation|EVA：一种用于未来视频预测的具身世界模型|Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue .etc.|<http://arxiv.org/pdf/2410.15461v2>|提出RoG策略和EVA模型，通过融合预训练模型优势，提升机器人场景中未来视频预测的准确性和泛化能力。|
|🆕 发布|AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models|原子视觉能力基准测试：面向视觉基础模型的AVA-Bench|Zheda Mai, Arpita Chowdhury, Zihe Wang, Sooyoung Jeon, Lemeng Wang, Jiacheng Hou, Jihyung Kil, Wei-Lun Chao|<http://arxiv.org/pdf/2506.09082v1>|提出AVA-Bench，通过分离14种基础视觉能力，精确评估视觉基础模型的性能。|
|🆕 发布|FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation|FlagEvalMM：一种用于全面多模态模型评估的灵活框架|Zheqi He, Yesheng Liu, Jing-shu Zheng, Xuejing Li, Richeng Xuan, Jin-Ge Yao, Xi Yang|<http://arxiv.org/pdf/2506.09081v1>|[代码](https://github.com/flageval-baai/FlagEvalMM.); 提出了一种全面的多模态模型评估框架FlagEvalMM，通过独立评估服务提高了评估效率和灵活性。|
|🆕 发布|VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks|VersaVid-R1：从问题回答到字幕任务的多功能视频理解和推理模型|Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu .etc.|<http://arxiv.org/pdf/2506.09079v1>|提出两个新数据集并训练了VersaVid-R1模型，实现了视频理解与推理的多种任务。|
|📝 更新|From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models|从像素到谓词：通过预训练的视觉语言模型学习符号世界模型|Ashay Athalye, Nishanth Kumar, Tom Silver, Yichao Liang, Jiuguang Wang, Tomás Lozano-Pérez, Leslie Pack Kaelbling|<http://arxiv.org/pdf/2501.00296v3>|利用预训练视觉语言模型学习抽象符号世界模型，实现零样本泛化解决复杂决策问题。|
|📝 更新|MedVersa: A Generalist Foundation Model for Medical Image Interpretation|MedVersa：一种用于医学图像解读的通用基础模型|Hong-Yu Zhou, Julián Nicolás Acosta, Subathra Adithan, Suvrankar Datta, Eric J. Topol, Pranav Rajpurkar|<http://arxiv.org/pdf/2405.07988v2>|MedVersa通过训练大规模医疗数据，实现了一款多模态通用医学图像解析模型，达到或超越专业解决方案...|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation|RS-MTDF：遥感多教师蒸馏与融合用于半监督语义分割|Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng|<http://arxiv.org/pdf/2506.08772v2>|提出RS-MTDF框架，利用预训练视觉基础模型作为多教师指导半监督遥感图像语义分割，提升泛化能力和语...|
|📝 更新|EMMA: Efficient Visual Alignment in Multi-Modal LLMs|EMMA：多模态大型语言模型中的高效视觉对齐|Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami|<http://arxiv.org/pdf/2410.02080v2>|[代码](https://github.com/SaraGhazanfari/EMMA); 提出EMMA方法，通过高效融合视觉与文本编码提升多模态大语言模型性能与鲁棒性。|
|📝 更新|SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning|空间推理器：迈向明确且可泛化的三维空间推理|Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, Alan Yuille|<http://arxiv.org/pdf/2504.20024v2>|提出了一种显式的3D空间推理模型SpatialReasoner，通过引入明确的3D表示，提升了空间推...|
|🆕 发布|Seedance 1.0: Exploring the Boundaries of Video Generation Models|《Seedance 1.0：探索视频生成模型边界》|Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li .etc.|<http://arxiv.org/pdf/2506.09113v1>|提出了Seedance 1.0模型，通过多源数据融合、高效架构设计、精细优化训练策略，实现了高质量且...|
|🆕 发布|Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment|通过多模态融合增强从CBCT生成的合成CT：CBCT质量和对齐对影响的探究|Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr|<http://arxiv.org/pdf/2506.08716v1>|通过多模态融合提高合成CT质量，有效缓解CBCT图像 artifacts 问题。|
|📝 更新|From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models|从生成到泛化：视频扩散模型中的涌现式少样本学习|Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro|<http://arxiv.org/pdf/2506.07280v2>|提出利用视频扩散模型进行少量样本学习，实现了在多种视觉任务中的强泛化能力。|
|📝 更新|RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language|RAVEN：基于查询引导的表示对齐用于音频、视频、嵌入式传感器和自然语言的问题回答|Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam|<http://arxiv.org/pdf/2505.17114v2>|[代码](https://github.com/BASHLab/RAVEN.); RAVEN通过查询引导的跨模态关模块QuART，优化了多模态问答中模态不匹配问题，显著提升了准确性和...|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Vision Transformers Don't Need Trained Registers|视觉变换器无需训练寄存器|Nick Jiang, Amil Dravid, Alexei Efros, Yossi Gandelsman|<http://arxiv.org/pdf/2506.08010v2>|提出了一种无需重新训练的解决视觉变换器噪声问题的方法，通过转移高范数激活至未训练的额外令牌，改善模型...|
|📝 更新|LieRE: Lie Rotational Positional Encodings|《LieRE：李旋转位置编码》|Sophie Ostmeier, Brian Axelrod, Maya Varma, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz|<http://arxiv.org/pdf/2406.10322v4>|[代码](https://github.com/StanfordMIMI/LieRE.); 将RoPE扩展到高维旋转矩阵的LieRE方法，提升了图像分类任务的准确率和分辨率适应性。|
|📝 更新|CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference|CHOSEN：面向高效视觉Transformer推理的硬件优化堆栈编译方法|Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram|<http://arxiv.org/pdf/2407.12736v4>|提出软件硬件协同设计框架CHOSEN，优化Vision Transformers在FPGA上的部署，...|
|🆕 发布|Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF|跨谱人体识别与侧面信息嵌入：基于LLCM的基准测试及对IJB-MDF中距离引起的遮挡分析|Anirudh Nanduri, Siyuan Huang, Rama Chellappa|<http://arxiv.org/pdf/2506.08953v1>|提出侧信息嵌入方法，提升跨谱域人体识别性能，并分析可见-红外遮挡影响。|
|🆕 发布|Inherently Faithful Attention Maps for Vision Transformers|视觉变换器中的本质忠实注意力图|Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos|<http://arxiv.org/pdf/2506.08915v1>|引入了一种二值注意力掩码方法，通过聚焦相关区域提高视觉变换器对异常背景的鲁棒性。|
|🆕 发布|Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers|时间序列表示法用于分类：预训练视觉变换器中隐藏的奥秘|Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata|<http://arxiv.org/pdf/2506.08641v1>|提出将时间序列转化为图像，利用预训练视觉变换器提升分类性能的新框架。|
|🆕 发布|Diversity-Guided MLP Reduction for Efficient Large Vision Transformers|多样性引导的MLP缩减策略用于高效的大规模视觉变换器|Chengchao Shen, Hourun Zhu, Gongfan Fang, Jianxin Wang, Xinchao Wang|<http://arxiv.org/pdf/2506.08591v1>|[代码](https://github.com/visresearch/DGMR.); 提出了一种多样性引导的MLP缩减方法，大幅减少大型视觉Transformer参数，实现近无损性能。|
|🆕 发布|Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models|"即插即用线性注意力机制用于预训练图像与视频修复模型"|Srinivasan Kidambi, Pravin Nair|<http://arxiv.org/pdf/2506.08520v1>|提出了一种线性近似自注意力机制，无需重新训练即可高效加速预训练图像和视频修复模型。|
|📝 更新|A Survey of the Self Supervised Learning Mechanisms for Vision Transformers|《视觉变换器中自监督学习机制的综述》|Asifullah Khan, Anabia Sohail, Mustansar Fiaz, Mehdi Hassan, Tariq Habib Afridi, Sibghat Ullah Marwat, Farzeen Munir, Safdar Ali .etc.|<http://arxiv.org/pdf/2408.17059v5>|系统综述了视觉变换器中自监督学习机制，提出了分类框架并分析了优劣。|
|📝 更新|An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification|基于迁移学习的可解释视觉变换器与支持向量机相结合的高效干旱胁迫识别方法|Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo|<http://arxiv.org/pdf/2407.21666v2>|提出了一种结合视觉变换器和支持向量机的可解释干旱压力检测方法，提高了作物干旱早期识别的准确性和可解释...|
|🆕 发布|SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging|SEMA：一种通过标记定位和平均实现的可扩展且高效的水蛇类似注意力机制|Nhat Thanh Tran, Fanghui Xue, Shuai Zhang, Jiancheng Lyu, Yunling Zheng, Yingyong Qi, Jack Xin|<http://arxiv.org/pdf/2506.08297v1>|提出SEMA方法，通过token定位与平均化，实现高效且可扩展的视觉注意力机制。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating|基于Transformer的光谱-空间注意力解耦与自适应门控的超光谱图像分类|Guandong Li, Mengxia Ye|<http://arxiv.org/pdf/2506.08324v2>|提出STNet网络，通过解耦空间与光谱注意力和自适应门控机制，有效提升高光谱图像分类性能。|
|🆕 发布|Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification|归一化Radon累积分布变换用于最优传输基础上的图像分类中的不变性和鲁棒性|Matthias Beckmann, Robert Beinert, Jonas Bresch|<http://arxiv.org/pdf/2506.08761v1>|提出了一种归一化Radon累积分布变换方法，增强了图像分类中的不变性和鲁棒性。|
|🆕 发布|Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification|基于生物启发的深度学习在胎儿超声图像分类中的应用|Rinat Prochii, Elizaveta Dakhova, Pavel Birulin, Maxim Sharaev|<http://arxiv.org/pdf/2506.08623v1>|提出了一种生物启发深度学习框架，能同时识别16种胎儿结构，准确率高且模型轻量。|
|🆕 发布|An Adaptive Method Stabilizing Activations for Enhanced Generalization|一种自适应方法稳定激活以增强泛化能力|Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko|<http://arxiv.org/pdf/2506.08353v1>|[代码](https://github.com/hseung88/adaact.); 引入AdaAct算法，通过调整学习率以适应激活值方差，提高神经元输出稳定性，从而增强模型泛化能力。|


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5|"树莓派5上用于航空应急目标检测的量化YOLOv4-Tiny高效边缘部署"|Sindhu Boddu, Arindam Mukherjee|<http://arxiv.org/pdf/2506.09300v1>|实现了量化YOLOv4-Tiny模型在Raspberry Pi 5上的高效部署，大幅降低能耗同时保持...|
|🆕 发布|Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery|使用量化YOLOv4-Tiny进行紧急响应的轻量级目标检测在航空影像中的应用|Sindhu Boddu, Arindam Mukherjee|<http://arxiv.org/pdf/2506.09299v1>|提出了一种针对紧急情况下空域影像的轻量级物体检测方法，通过量化YOLOv4-Tiny模型，实现了模型...|
|🆕 发布|Gender Fairness of Machine Learning Algorithms for Pain Detection|机器学习算法在疼痛检测中的性别公平性|Dylan Green, Yuting Shang, Jiaee Cheong, Yang Liu, Hatice Gunes|<http://arxiv.org/pdf/2506.11132v1>|探究机器学习算法在疼痛检测中的性别公平性，发现高准确率与公平性之间存在权衡。|
|🆕 发布|Data Augmentation For Small Object using Fast AutoAugment|使用快速自动增强的小目标数据增强|DaeEun Yoon, Semin Kim, SangWook Yoo, Jongha Lee|<http://arxiv.org/pdf/2506.08956v1>|提出一种数据增强方法，通过Fast AutoAugment快速找到最优策略，显著提升小物体检测性能。|
|🆕 发布|WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos|《WetCat：湿实验室白内障手术视频技能评估自动化》|Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann, Jens Kowal|<http://arxiv.org/pdf/2506.08896v1>|介绍了WetCat，首个专为自动化评估白内障手术技能的湿实验室视频数据集，旨在提高手术训练的效率和客...|
|🆕 发布|WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras|基于小波去噪增强的实时目标检测Transformer：面向事件相机机器人感知的WD-DETR|Yangjie Cui, Boyang Gao, Yiwei Zhang, Xin Dong, Jinwu Xiang, Daochun Li, Zhan Tu|<http://arxiv.org/pdf/2506.09098v1>|提出了一种利用小波去噪增强实时物体检测的Transformer网络，有效提升了事件相机感知质量与检测...|
|🆕 发布|ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction|ATAS：任意到任意自蒸馏以提高开放词汇密集预测性能|Juan Yeo, Soonwoo Cha, Jiwoo Song, Hyunbin Jin, Taesup Kim|<http://arxiv.org/pdf/2506.08678v1>|提出ATAS方法，通过内部自蒸馏同时增强语义连贯性和细粒度视觉语言对齐，提升开放词汇密集预测任务性能...|
|🆕 发布|Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection|层次化神经崩溃检测变换器用于类别增量目标检测|Duc Thanh Pham, Hong Dang Nguyen, Nhat Minh Nguyen Quoc, Linh Ngo Van, Sang Dinh Viet, Duc Anh Nguyen|<http://arxiv.org/pdf/2506.08562v1>|提出了一种高效且性能卓越的Hier-DETR框架，通过利用神经崩溃和类标签的层级关系解决增量对象检测...|
|📝 更新|Adaptive path planning for efficient object search by UAVs in agricultural fields|自适应路径规划以实现无人机在农田中的高效物体搜索|Rick van Essen, Eldert van Henten, Lammert Kooistra, Gert Kootstra|<http://arxiv.org/pdf/2504.02473v2>|[代码](https://github.com/wur-abe/uav_adaptive_planner.); 提出自适应路径规划方法，助力无人机高效搜索农田中目标物体。|
|🆕 发布|Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN|自适应目标检测：基于ESRGAN增强分辨率与Faster R-CNN|Divya Swetha K, Ziaul Haque Choudhury, Hemanta Kumar Bhuyan, Biswajit Brahma, Nilayam Kumar Kamila|<http://arxiv.org/pdf/2506.11122v1>|提出了一种结合ESRGAN图像增强与Faster R-CNN检测的算法，有效提升了低分辨率图像的物体...|


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos|多阶段边界感知变换器网络用于未剪辑手术视频中的动作分割|Rezowan Shuvo, M S Mekala, Eyad Elyan|<http://arxiv.org/pdf/2504.18756v2>|提出了一种多阶段边界感知的Transformer网络，通过层级滑动窗口注意力优化手术视频中的动作分割...|
|🆕 发布|Monocular 3D Hand Pose Estimation with Implicit Camera Alignment|单目3D手部姿态估计与隐式相机对齐|Christos Pantazopoulos, Spyridon Thermos, Gerasimos Potamianos|<http://arxiv.org/pdf/2506.11133v1>|[代码](https://github.com/cpantazop/HandRepo); 提出了一种优化管道，通过2D关键点输入估计3D手部姿态，无需相机参数知识。|
|🆕 发布|ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds|箭姿：无色点云的分割、检测与五自由度位姿估计网络|Frederik Hagelskjaer|<http://arxiv.org/pdf/2506.08699v1>|提出了一种快速检测与5自由度位姿估计网络，用于无色点云，实现业界领先性能且处理速度快。|
|📝 更新|Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection|双注意力残差U-Net用于脑室内出血检测中的精确脑超声分割|Dan Yuan, Yi Feng, Ziyun Tang|<http://arxiv.org/pdf/2505.17683v2>|[代码](https://github.com/DanYuan001/BrainImgSegment.); 提出了一种融合双注意力机制的改进Residual U-Net，实现了脑室区域的高精度分割。|


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ContextLoss: Context Information for Topology-Preserving Segmentation|上下文损失：用于保持拓扑结构的分割中的上下文信息|Benedict Schacht, Imke Greving, Simone Frintrop, Berit Zeller-Plumhoff, Christian Wilms|<http://arxiv.org/pdf/2506.11134v1>|提出ContextLoss损失函数，通过考虑拓扑错误的整体上下文，有效提升图像分割中的拓扑正确性。|
|🆕 发布|Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation|“分段这个玩意儿：基于注视点的有效点提示分割的注视区标记化”|Tanner Schmidt, Richard Newcombe|<http://arxiv.org/pdf/2506.11131v1>|提出了一种基于焦点区域的图像分割模型，通过非均匀降采样减少计算量，实现了高效的单点提示分割。|
|🆕 发布|Rethinking Range-View LiDAR Segmentation in Adverse Weather|在恶劣天气下重新思考范围视图激光雷达分割|Longyu Yang, Ping Hu, Lu Zhang, Jun Liu, Yap-Peng Tan, Heng Tao Shen, Xiaofeng Zhu|<http://arxiv.org/pdf/2506.08979v1>|提出了一种模块化轻量级框架，通过分离处理几何属性和反射强度，有效增强了恶劣天气下LiDAR段落的鲁棒...|
|🆕 发布|ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network|ECMNet：基于高效CNN-Mamba网络的轻量级语义分割|Feixiang Du, Shengkun Wu|<http://arxiv.org/pdf/2506.08629v1>|提出了一种融合CNN与Mamba的轻量级语义分割网络ECMNet，通过增强特征表示和融合多尺度信息，...|
|🆕 发布|DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View|DCD：一种用于胎儿超声四腔视图的语义分割模型|Donglian Li, Hui Guo, Minglang Chen, Huizhen Chen, Jialing Chen, Bocheng Liang, Pengchen Liang, Ying Tan|<http://arxiv.org/pdf/2506.08534v1>|提出DCD模型，通过深度学习技术自动精确分割胎儿心脏A4C视图的关键结构，提升 prenatal c...|


## 生成式视觉模型 (Generative Visual Modeling)


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models|“Cosmos-Drive-Dreams：基于世界基础模型的可扩展合成驾驶数据生成”|Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff .etc.|<http://arxiv.org/pdf/2506.09042v2>|提出了一种生成高保真合成驾驶数据的Cosmos-Drive-Dreams方法，解决了自动驾驶系统训练...|
|🆕 发布|SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping|跳变频率感知自回归建模加速方法：SkipVAR|Jiajun Li, Yue Ma, Xinyu Zhang, Qingyan Wei, Songhua Liu, Linfeng Zhang|<http://arxiv.org/pdf/2506.08908v2>|[代码](https://github.com/fakerone-li/SkipVAR); 提出了一种自适应频率感知的跳步策略，有效减少了视觉自回归模型中的计算冗余，实现了图像生成的加速。|
|📝 更新|ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment|“ReSpace：基于文本驱动的三维场景合成与编辑及偏好对齐”|Martin JJ. Bucher, Iro Armeni|<http://arxiv.org/pdf/2506.02459v2>|提出了一种基于自然语言驱动的3D室内场景生成与编辑框架，通过双阶段训练实现用户指导下的精细场景构建与...|
|📝 更新|Is Perturbation-Based Image Protection Disruptive to Image Editing?|基于扰动的图像保护是否会破坏图像编辑？|Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati|<http://arxiv.org/pdf/2506.04394v2>|发现基于扰动保护的图像在扩散模型编辑下仍能生成满意结果，质疑了其作为图像保护手段的有效性。|
|🆕 发布|MagCache: Fast Video Generation with Magnitude-Aware Cache|《MagCache：基于幅度感知缓存的快速视频生成》|Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian|<http://arxiv.org/pdf/2506.09045v1>|提出了一种自适应跳过不重要时间步的缓存策略，显著提高了视频生成速度并保持了视觉质量。|
|🆕 发布|Bias Analysis in Unconditional Image Generative Models|无条件图像生成模型中的偏差分析|Xiaofeng Zhang, Michelle Lin, Simon Lacoste-Julien, Aaron Courville, Yash Goyal|<http://arxiv.org/pdf/2506.09106v1>|分析了无条件图像生成模型中的偏见产生机制，强调了评价框架和属性分类器选择的重要性。|
|🆕 发布|Diffuse and Disperse: Image Generation with Representation Regularization|扩散与分散：带有表征正则化的图像生成|Runqian Wang, Kaiming He|<http://arxiv.org/pdf/2506.09027v1>|提出了一种新的正则化方法Dispersive Loss，通过鼓励内部表示在隐空间中散布，有效提升了扩...|
|🆕 发布|Product of Experts for Visual Generation|专家乘积模型在视觉生成中的应用|Yunzhi Zhang, Carson Murtuza-Lanier, Zizhang Li, Yilun Du, Jiajun Wu|<http://arxiv.org/pdf/2506.08894v1>|提出了一种无需训练的异质模型知识融合框架Product of Experts，用于提升图像和视频合成...|
|🆕 发布|HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation|《HunyuanVideo-HOMA：多模态驱动人动画中的通用人物-物体交互》|Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao, Zhiyong Xu, Hongmei Wang .etc.|<http://arxiv.org/pdf/2506.08797v1>|提出弱监督条件下的多模态驱动框架HunyuanVideo-HOMA，通过稀疏运动指导提升人-物交互动...|
|📝 更新|Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO|深入探究基于CoT的图像生成中的强化学习：DPO与GRPO对比研究|Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng|<http://arxiv.org/pdf/2505.17017v2>|[代码](https://github.com/ZiyuGuo99/Image-Generation-CoT); 首次全面比较了DPO和GRPO在自回归图像生成中的性能，揭示了奖励模型对算法泛化能力的影响。|
|🆕 发布|Orientation Matters: Making 3D Generative Models Orientation-Aligned|《方向至关重要：使三维生成模型方向对齐》|Yichong Lu, Yuzhuo Tian, Zijin Jiang, Yikun Zhao, Yuanbo Yang, Hao Ouyang, Haoji Hu, Huimin Yu .etc.|<http://arxiv.org/pdf/2506.08640v1>|提出任务驱动的3D对象生成方法，通过构建对齐的3D模型数据集，提高了生成模型在多种任务中的表现。|
|🆕 发布|MAMBO: High-Resolution Generative Approach for Mammography Images|MAMBO：用于乳腺X线照片图像的高分辨率生成方法|Milica Škipina, Nikola Jovišić, Nicola Dall'Asen, Vanja Švenda, Anil Osman Tur, Slobodan Ilić, Elisa Ricci, Dubravko Ćulibrk|<http://arxiv.org/pdf/2506.08677v1>|提出了一种生成高清哺乳图像的MAMBO模型，通过集成扩散模型捕捉细节，助力乳腺癌早期诊断。|
|🆕 发布|BakuFlow: A Streamlining Semi-Automatic Label Generation Tool|BakuFlow：一种简化的半自动标注生成工具|Jerry Lin, Partick P. W. Chen|<http://arxiv.org/pdf/2506.09083v1>|BakuFlow通过集成自动和半自动标注工具，大幅提升了大规模计算机视觉任务的数据标注效率和准确性。|
|📝 更新|ATI: Any Trajectory Instruction for Controllable Video Generation|任意轨迹指令用于可控视频生成的ATI方法|Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma|<http://arxiv.org/pdf/2505.22944v3>|[代码](https://anytraj.github.io/.); 提出统一框架，通过轨迹输入实现视频生成中运动控制的精准化和一体化。|
|🆕 发布|Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models|重新思考文本到图像模型中图像-文本对齐的自动评估|Huixuan Zhang, Xiaojun Wan|<http://arxiv.org/pdf/2506.08480v1>|指出现有图像文本对齐评估方法的不足，并提出改进建议。|
|🆕 发布|Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance|通过自适应低通引导增强图像到视频模型中的运动动态|June Suk Choi, Kyungmin Lee, Sihyun Yu, Yisol Choi, Jinwoo Shin, Kimin Lee|<http://arxiv.org/pdf/2506.08456v1>|提出自适应低通引导策略，有效增强图像转视频模型的动态效果，同时保持图像质量。|
|📝 更新|How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation|图像如何与LiDAR对齐与互补？迈向协调的多模态全景三维分割|Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao|<http://arxiv.org/pdf/2505.18956v2>|[代码](https://github.com/IMPL-Lab/IAL.git); 提出Image-Assists-LiDAR框架，通过同步数据增强和特征融合提升3D全景分割准确度。|
|🆕 发布|SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills|《SakugaFlow：一种模拟人类绘画过程并提供新手绘画技能互动辅导的分阶段插画框架》|Kazuki Kawamura, Jun Rekimoto|<http://arxiv.org/pdf/2506.08443v1>|提出了一种分阶段插画框架SakugaFlow，模拟人类绘画过程并辅助新手绘画技能学习。|


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Fine-Grained Spatially Varying Material Selection in Images|图像中精细粒度的空间变化材料选择|Julia Guerrero-Viu, Michael Fischer, Iliyan Georgiev, Elena Garces, Diego Gutierrez, Belen Masia, Valentin Deschaintre|<http://arxiv.org/pdf/2506.09023v2>|提出了一种基于视觉变换器的图像材质选择方法，能在不同光照和反射条件下精确选择材质，并支持纹理和子纹理...|
|🆕 发布|Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models|跨帧表示对齐用于微调视频扩散模型|Sungwon Hwang, Hyojin Jang, Kinam Kim, Minho Park, Jaegul choo|<http://arxiv.org/pdf/2506.09229v1>|提出了一种跨帧表示对齐技术，有效提升了视频扩散模型微调时的视觉保真度和帧间一致性。|
|📝 更新|Spatial Reasoning with Denoising Models|"基于去噪模型的空间推理"|Christopher Wewer, Bart Pogodzinski, Bernt Schiele, Jan Eric Lenssen|<http://arxiv.org/pdf/2502.21075v2>|提出了一种通过去噪生成模型进行空间推理的框架，显著提升了生成模型在复杂推理任务中的准确性。|
|📝 更新|ICONS: Influence Consensus for Vision-Language Data Selection|视觉语言数据选择的影响共识|Xindi Wu, Mengzhou Xia, Rulin Shao, Zhiwei Deng, Pang Wei Koh, Olga Russakovsky|<http://arxiv.org/pdf/2501.00654v3>|提出了一种基于梯度影响共识的视觉语言数据选择方法，有效提高了多任务学习的数据利用率和模型性能。|
|🆕 发布|CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation|通过检索增强评估的图像文化归因：CAIRe|Arnav Yayavaram, Siddharth Yayavaram, Simran Khanuja, Michael Saxon, Graham Neubig|<http://arxiv.org/pdf/2506.09109v1>|提出了一种评估图像文化相关性的新指标CAIRe，有效衡量文本到图像模型在不同文化背景下的公平性表现。|
|🆕 发布|ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations|ADAM：使用大规模语言模型进行上下文感知注释的自主发现与标注模型|Amirreza Rouhi, Solmaz Arezoomandan, Knut Peterson, Joseph T. Woods, David K. Han|<http://arxiv.org/pdf/2506.08968v1>|提出了一种无需训练、能自我完善的框架ADAM，利用大型语言模型和视觉嵌入为开放世界中的未知对象生成和...|
|🆕 发布|ORIDa: Object-centric Real-world Image Composition Dataset|ORIDa：以对象为中心的现实世界图像组合数据集|Jinwoo Kim, Sangmin Han, Jinho Jeong, Jiwoo Choi, Dongyoung Kim, Seon Joo Kim|<http://arxiv.org/pdf/2506.08964v1>|介绍了ORIDa，一个大规模真实捕获的图像组合数据集，为研究物体组合提供了多样性和规模。|
|🆕 发布|What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities|虚拟代理应用的限制因素是什么？OmniBench：一个用于基本虚拟代理能力的可扩展多维基准测试|Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li .etc.|<http://arxiv.org/pdf/2506.08933v1>|[代码](https://omni-bench.github.io/.); 提出了OmniBench，一种自动生成、跨平台的虚拟智能体能力评估基准，通过多维评价框架提升智能体性...|
|🆕 发布|DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval|"DiscoVLA：视觉、语言与对齐中的差异减少以实现参数高效的视频-文本检索"|Leqi Shen, Guoqiang Gong, Tianxiang Hao, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Jungong Han .etc.|<http://arxiv.org/pdf/2506.08887v1>|[代码](https://github.com/LunarShen/DsicoVLA.); 提出DiscoVLA方法，通过融合图像与视频特征及优化语言和对齐差异，提升视频文本检索性能。|
|🆕 发布|Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling|“流多样性且高效：通过随机速度场采样学习动量流匹配”|Zhiyuan Ma, Ruixun Liu, Sixian Liu, Jianjun Li, Bowen Zhou|<http://arxiv.org/pdf/2506.08796v1>|[代码](https://github.com/liuruixun/momentum-fm.); 提出了一种通过随机速度场采样学习动量流匹配的方法，有效扩展了采样空间并提高了生成图像的多样性和质量。|
|🆕 发布|A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory|基于大气散射理论的偏微分方程图像去雾方法|Zhuoran Zheng|<http://arxiv.org/pdf/2506.08793v1>|提出了一种基于偏微分方程和大气散射理论的图像去雾方法，有效提升了去雾质量和效率。|
|📝 更新|A Culturally-Aware Benchmark for Person Re-Identification in Modest Attire|《针对朴素着装人物重识别的文化意识基准》|Alireza Sedighi Moghaddam, Fatemeh Anvari, Mohammadjavad Mirshekari Haghighi, Mohammadali Fakhari, Mohammad Reza Mohammadi|<http://arxiv.org/pdf/2412.18874v2>|提出了针对保守服饰文化背景的ReID基准数据集，提升了跨文化场景下的人体识别性能。|
|📝 更新|EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution|EAM：利用扩散变换器增强任意内容实现盲目超分辨率|Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu|<http://arxiv.org/pdf/2505.05209v2>|提出Enhancing Anything Model，利用扩散变换器提升盲超分辨率性能，实现图像修复...|
|🆕 发布|RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping|RoboSwap：一种基于生成对抗网络的半监督机器人臂替换视频扩散框架|Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Dong Chen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok|<http://arxiv.org/pdf/2506.08632v1>|提出RoboSwap框架，利用GAN和扩散模型实现无监督机器人臂互换，提升跨平台学习数据质量。|
|🆕 发布|LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s|LiftVSR：通过混合时间建模将图像扩散提升至视频超分辨率，仅需4$\times$RTX 4090s|Xijun Wang, Xin Li, Bingchen Li, Zhibo Chen|<http://arxiv.org/pdf/2506.08529v1>|提出 LiftVSR 框架，通过混合时间建模机制在较低计算成本下实现视频超分辨率。|
|📝 更新|Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models|混合解码：一种受注意力启发自适应解码策略，以减轻大规模视觉-语言模型中的幻觉现象|Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan|<http://arxiv.org/pdf/2505.17061v3>|[代码](https://github.com/xlchen0205/MoD.); 提出Mixture of Decoding方法，通过动态调整解码策略减少大型视觉语言模型中的幻觉现象...|
|🆕 发布|How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models|“多少指导才算足够：重新审视无分类器指导文本到视觉扩散模型中的自适应指导”|Huixuan Zhang, Junzhe Zhang, Xiaojun Wan|<http://arxiv.org/pdf/2506.08351v1>|提出了一种简单通用的自适应引导策略Step AG，通过限制引导步骤，提高了生成高质量图像的速度20%...|
|🆕 发布|Learning-based density-equalizing map|基于学习的密度均衡映射|Yanwen Huang, Lok Ming Lui, Gary P. T. Choi|<http://arxiv.org/pdf/2506.10027v1>|提出基于深度学习的密度均衡映射框架，提高了密度均衡质量和通用性，并简化了2D至3D的扩展。|


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|UFM: A Simple Path towards Unified Dense Correspondence with Flow|统一流形匹配：通往统一密集对应关系的简单路径|Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha .etc.|<http://arxiv.org/pdf/2506.09278v1>|提出Unified Flow & Matching模型，统一处理图像匹配与光流估计，实现更准确、快速...|
|🆕 发布|Do Concept Replacement Techniques Really Erase Unacceptable Concepts?|概念替换技术真的能抹除不可接受的概念吗？|Anudeep Das, Gurjot Singh, Prach Chantasantitam, N. Asokan|<http://arxiv.org/pdf/2506.08991v1>|指出现有概念替换技术无法真正删除不可接受的概念，并提出了一个保持输入概念完整性的图像编辑方法 Ant...|
|🆕 发布|CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics|文化框架：评估文本到图像模型及其评价指标中的文化期望一致性|Shravan Nayak, Mehar Bhatia, Xiaofeng Zhang, Verena Rieser, Lisa Anne Hendricks, Sjoerd van Steenkiste, Yash Goyal, Karolina Stańczak .etc.|<http://arxiv.org/pdf/2506.08835v1>|提出CulturalFrames基准，首次量化文本到图像模型在显性与隐性文化期待上的偏差，并指出现有...|
|📝 更新|OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation|OneIG-Bench：图像生成的全维度细微评价基准|Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu .etc.|<http://arxiv.org/pdf/2506.07977v2>|提出全面的OneIG-Bench评估框架，细致评价文本到图像生成模型在多维度性能。|
|📝 更新|ARGUS: Hallucination and Omission Evaluation in Video-LLMs|ARGUS：视频大型语言模型中的虚构与遗漏评估|Ruchit Rawal, Reza Shirkavand, Heng Huang, Gowthami Somepalli, Tom Goldstein|<http://arxiv.org/pdf/2506.07371v2>|提出ARGUS基准，评估视频大语言模型在自由文本生成任务中的虚构和遗漏错误。|
|📝 更新|Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence|破解视觉感知头分歧下轻量级视觉模型中幻觉编码之谜|Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng Jia, Ming Tang, Tat-Seng Chua .etc.|<http://arxiv.org/pdf/2412.13949v3>|提出了一种量化视觉注意力敏感性的方法VHD，通过强化视觉敏感注意力头有效减少了生成文本的幻觉现象。|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|JAFAR: Jack up Any Feature at Any Resolution|JAFAR: 在任意分辨率提升任意特征|Paul Couairon, Loick Chambon, Louis Serrano, Jean-Emmanuel Haugeard, Matthieu Cord, Nicolas Thome|<http://arxiv.org/pdf/2506.11136v1>|提出了一种灵活的特征上采样方法JAFAR，有效提升低分辨率视觉特征至任意目标分辨率，无需高分辨率监督...|
|📝 更新|SMCD: High Realism Motion Style Transfer via Mamba-based Diffusion|SMCD：基于Mamba扩散的高逼真度运动风格迁移|Ziyun Qian, Zeyu Xiao, Xingliang Jin, Dingkang Yang, Mingcheng Li, Zhenyi Wu, Dongliang Kou, Peng Zhai .etc.|<http://arxiv.org/pdf/2405.02844v2>|提出UMSD框架和MSM去噪器，通过信息交互和序列建模提升运动风格迁移的自然度和连贯性。|
|📝 更新|Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap|跨模态检索的多模态无监督域泛化|Christopher Liao, Christian So, Theodoros Tsiligkaridis, Brian Kulis|<http://arxiv.org/pdf/2402.04416v3>|[代码](https://github.com/Chris210634/mudg); 提出了一种跨模态检索方法，通过在无需显式关联源数据与目标任务的条件下，有效提升无监督多模态领域泛化性...|
|📝 更新|StereoVAE: A lightweight stereo-matching system using embedded GPUs|立体VAE：一种使用嵌入式GPU的轻量级立体匹配系统|Qiong Chang, Xiang Li, Xin Xu, Xin Liu, Yun Li, Miyazaki Jun|<http://arxiv.org/pdf/2305.11566v3>|提出了一种结合传统匹配与VAE神经网络的轻量级立体匹配系统，实现了高精度与实时性的平衡。|
|🆕 发布|TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering|《TraGraph-GS：基于轨迹图的高斯散点绘制法用于任意大规模场景渲染》|Xiaohan Zhang, Sitong Wang, Yushen Yan, Yi Yang, Mingda Xu, Qi Liu|<http://arxiv.org/pdf/2506.08704v1>|提出了一种基于轨迹图的Gaussian Splatting方法，有效解决了大规模场景渲染中的精度和效...|
|🆕 发布|MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback|“MOSAIC-F：一种通过个性化反馈提升学生口语表达能力的框架”|Alvaro Becerra, Daniel Andres, Pablo Villegas, Roberto Daza, Ruth Cobos|<http://arxiv.org/pdf/2506.08634v1>|提出了一种集成多模态学习分析、观察、传感器和人工智能的MOSAIC-F框架，为学生的口头表达能力提供...|
|🆕 发布|Image Demoiréing Using Dual Camera Fusion on Mobile Phones|使用手机双摄像头融合的图像去摩尔纹处理|Yanting Mei, Zhilu Zhang, Xiaohe Wu, Wangmeng Zuo|<http://arxiv.org/pdf/2506.08361v1>|[代码](https://github.com/Mrduckk/DCID.); 利用手机双摄像头融合技术，有效去除拍摄电子屏幕时产生的摩尔纹。|
|🆕 发布|Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos|从随意捕捉的RGBD视频中重建通用关节对象|Weikun Peng, Jun Lv, Cewu Lu, Manolis Savva|<http://arxiv.org/pdf/2506.08334v1>|提出了一种从随手拍摄的RGBD视频重建关节对象的方法，实现了在动态场景下的高精度重建。|
|📝 更新|STeP: A Framework for Solving Scientific Video Inverse Problems with Spatiotemporal Diffusion Priors|STeP：一种基于时空扩散先验解决科学视频逆问题的框架|Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman|<http://arxiv.org/pdf/2504.07549v2>|提出了一种利用学习到的时空扩散先验的框架，有效解决了科学视频重建中的时空一致性难题。|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Grids Often Outperform Implicit Neural Representations|网格常常优于隐式神经表示|Namhoon Kim, Sara Fridovich-Keil|<http://arxiv.org/pdf/2506.11139v1>|[代码](https://github.com/voilalab/INR-benchmark.); 该研究揭示了在多数任务中，简单的正则化网格比同等参数量的隐式神经表征训练更快且质量更高。|
|🆕 发布|StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams|“StreamSplat：面向未校准视频流在线动态三维重建”|Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, Renjie Liao|<http://arxiv.org/pdf/2506.08862v1>|[代码](https://github.com/nickwzk/StreamSplat.); 提出了StreamSplat框架，实现了从未校准视频流中实时动态三维重建，通过创新的概率采样和双向形...|
|📝 更新|GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats|《GigaSLAM：基于分层高斯散斑的大规模单目SLAM》|Kai Deng, Yigong Zhang, Jian Yang, Jin Xie|<http://arxiv.org/pdf/2503.08071v2>|[代码](https://github.com/DengKaiCQ/GigaSLAM.); 提出了GigaSLAM，首个适用于千米级室外环境的RGB NeRF/3DGS SLAM框架，通过多级...|
|🆕 发布|A Probability-guided Sampler for Neural Implicit Surface Rendering|概率引导的神经隐式表面渲染采样器|Gonçalo Dias Pais, Valter Piedade, Moitreya Chatterjee, Marcus Greiff, Pedro Miraldo|<http://arxiv.org/pdf/2506.08619v1>|提出了一种概率引导的采样策略，通过优化3D投影空间采样，提高了神经隐式表面渲染的准确性和细节表现。|
|🆕 发布|Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs|《谱主路径的收敛性：深度网络如何从噪声输入中提炼线性表示》|Bowei Tian, Xuntao Lyu, Meng Liu, Hongyi Wang, Ang Li|<http://arxiv.org/pdf/2506.08543v1>|提出输入空间线性假设，通过光谱主路径框架揭示深度网络如何从噪声输入中提炼线性表征。|
|🆕 发布|Sparse Autoencoders Bridge The Deep Learning Model and The Brain|稀疏自编码器连接深度学习模型与大脑|Ziming Mao, Jia Xu, Zeqi Zheng, Haofang Zheng, Dabing Sheng, Yaochu Jin, Guoyuan Yang|<http://arxiv.org/pdf/2506.11123v1>|提出SAE-BrainMap框架，利用稀疏自编码器直接关联深度学习模型与大脑fMRI响应，揭示了模型...|
|🆕 发布|Complex-Valued Holographic Radiance Fields|复值全息辐射场|Yicheng Zhan, Dong-Ha Shin, Seung-Hwan Baek, Kaan Akşit|<http://arxiv.org/pdf/2506.08350v1>|提出了一种优化3D场景的新型复值高斯表示法，大幅提升 holographic rendering 的...|


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Dense Geometry Supervision for Underwater Depth Estimation|水下深度估计的密集几何监督方法|Wenxiang Gua, Lin Qia|<http://arxiv.org/pdf/2504.18233v2>|提出了一种融合纹理和深度的方法，有效提升了水下单目深度估计的准确性和适应性。|
|🆕 发布|SurfR: Surface Reconstruction with Multi-scale Attention|多尺度注意力表面重建：SurfR|Siddhant Ranade, Gonçalo Dias Pais, Ross Tyler Whitaker, Jacinto C. Nascimento, Pedro Miraldo, Srikumar Ramalingam|<http://arxiv.org/pdf/2506.08635v1>|提出了一种快速准确的表面重建算法，通过多尺度注意力机制优化隐式表示，实现了速度与精度的平衡。|
|🆕 发布|Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings|增强梯度泄露攻击：现实联邦学习环境下的数据重建|Mingyuan Fan, Fuyi Wang, Cen Chen, Jianying Zhou|<http://arxiv.org/pdf/2506.08435v1>|提出FedLeak方法，通过部分梯度匹配和梯度正则化，在现实联邦学习环境中有效重构客户端数据。|


### 视觉定位与映射 (Visual Localization & Mapping)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MegaLoc: One Retrieval to Place Them All|MegaLoc：一检索以定位全体|Gabriele Berton, Carlo Masone|<http://arxiv.org/pdf/2502.17237v3>|[代码](https://github.com/gmberton/MegaLoc); 提出了一种多任务通用图像检索模型MegaLoc，实现了多个视觉任务上的性能提升。|


## 时序视觉分析 (Temporal Visual Analysis)


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought|视频-CoT：基于思维链的视频时空理解综合数据集|Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang|<http://arxiv.org/pdf/2506.08817v3>|提出Video-CoT数据集，利用Chain-of-Thought提升视频时空理解能力。|
|🆕 发布|Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization|利用文本-运动跨模态对比损失增强视频记忆性预测及其在视频摘要中的应用|Zhiyi Zhu, Xiaoyu Wu, Youwei Lu|<http://arxiv.org/pdf/2506.08649v1>|提出了一种利用文本-运动跨模态对比损失增强运动特征表示的视频记忆性预测模型，实现了视频摘要中的主观性...|
|🆕 发布|MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding|MLVTG：基于Mamba的特征对齐与LLM驱动的多模态视频时间定位净化|Zhiyi Zhu, Xiaoyu Wu, Zihao Liu, Linlin Yang|<http://arxiv.org/pdf/2506.08512v1>|提出MLVTG框架，通过MambaAligner和LLMRefiner优化多模态视频时间定位，实现精...|


### 时序建模与预测 (Temporal Modeling & Prediction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals|LMPOcc：利用历史遍历中的长期记忆先验进行三维语义占据预测|Shanshuai Yuan, Julong Wei, Muer Tie, Xiangyun Ren, Zhongxue Gan, Wenchao Ding|<http://arxiv.org/pdf/2504.13596v2>|提出了一种利用历史轨迹中的长期记忆先验进行3D语义占据预测的方法，显著提升了静态场景的预测准确性。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning|基于跨模态掩码重建和对比学习的病理学图像空间转录组表达预测|Junzhuo Liu, Markus Eckstein, Zhixiang Wang, Friedrich Feuerhake, Dorit Merhof|<http://arxiv.org/pdf/2506.08854v1>|提出了一种基于对比学习的深度学习方法，从病理切片图像预测空间解析的基因表达，显著提升了预测准确性。|
|📝 更新|k-NN as a Simple and Effective Estimator of Transferability|k-近邻作为一种简单有效的迁移性估计器|Moein Sorkhei, Christos Matsoukas, Johan Fredin Haslum, Emir Konuk, Kevin Smith|<http://arxiv.org/pdf/2503.18528v2>|提出了一种基于k-NN的迁移性评估方法，在多数据集上优于现有指标且易于实施。|
|📝 更新|Mitigating Prior Shape Bias in Point Clouds via Differentiable Center Learning|通过可微分中心学习减轻点云中的先验形状偏差|Zhe Li, Xiying Wang, Jinglin Zhao, Zheng Wang, Debin Liu, Laurence T. Yang|<http://arxiv.org/pdf/2402.02088v4>|提出了一种 Differentiable Center Sampling Network，通过结合全...|
|📝 更新|CASE: Contrastive Activation for Saliency Estimation|CASE: 对比激活用于显著性估计|Dane Williamson, Yangfeng Ji, Matthew Dwyer|<http://arxiv.org/pdf/2506.07327v2>|提出了一种新的对比性激活方法CASE，用于生成更忠实且针对特定类别的显著性解释。|
|🆕 发布|SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding|“SECOND：通过选择性对比解码减轻视觉-语言模型中的感知幻觉”|Woohyeon Park, Woojin Kim, Jaeik Kim, Jaeyoung Do|<http://arxiv.org/pdf/2506.08391v1>|提出Selective and Contrastive Decoding方法，减少视觉模型中的对象幻...|


### 掩码自编码 (Masked Autoencoding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MARMOT: Masked Autoencoder for Modeling Transient Imaging|MARMOT：用于建模瞬时成像的遮蔽自编码器|Siyuan Shen, Ziheng Wang, Xingyue Peng, Suan Xia, Ruiqian Li, Shiying Li, Jingyi Yu|<http://arxiv.org/pdf/2506.08470v1>|提出了一种自监督预训练的masked autoencoder，MARMOT，通过在大量NLOS tr...|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba|“_inceptionMamba：一种具有大带宽卷积和瓶颈Mamba的高效混合网络_”|Yuhang Wang, Jun Li, Zhijian Wu, Jianhua Xu|<http://arxiv.org/pdf/2506.08735v2>|[代码](https://github.com/Wake1021/InceptionMamba.); 提出InceptionMamba架构，通过正交带卷积和瓶颈Mamba模块提升空间依赖捕捉和全局上下文...|
|📝 更新|Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss|重新审视校准的重新加权风险：AURC、焦点损失与逆焦点损失|Han Zhou, Sebastian G. Gruber, Teodora Popordanoska, Matthew B. Blaschko|<http://arxiv.org/pdf/2505.23463v2>|本文建立了加权风险函数与校准误差间的联系，提出优化正则化AURC以提高模型校准性能。|
|🆕 发布|CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities|加拿大火卫星：面向多模态高分辨率森林火灾预测|Hugo Porta, Emanuele Dalsasso, Jessica L. McCarty, Devis Tuia|<http://arxiv.org/pdf/2506.08690v1>|提出CanadaFireSat数据集和基线方法，利用多模态数据实现100米分辨率的高精度野火预测。|
|🆕 发布|Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems|变换器遇见高光谱成像：模型、挑战与开放问题的全面研究|Guyang Zhang, Waleed Abdulla|<http://arxiv.org/pdf/2506.08596v1>|系统梳理了基于Transformer的遥感图像分类方法，针对数据稀缺和模型解释性等问题提出了研究方向...|


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation|UKAN-EP：通过高效注意力机制与金字塔聚合增强U-KAN用于三维多模态MRI脑肿瘤分割|Yanbing Chen, Tianze Tang, Taehyo Kim, Hai Shu|<http://arxiv.org/pdf/2408.00273v2>|[代码](https://github.com/TianzeTang0504/UKAN-EP.); 提出UKAN-EP模型，通过引入注意力机制和特征融合技术，优化多模态MRI脑肿瘤分割效果并降低计算复...|
|📝 更新|ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding|“之字形点魔霸：用于点云理解的空间语义魔霸”|Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian|<http://arxiv.org/pdf/2505.21381v2>|提出 zigzag 扫描路径和语义相似遮蔽策略，提升点云理解中的空间连续性和局部语义建模。|
|🆕 发布|Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting|《Argoverse2场景挖掘挑战技术报告：迭代错误校正与空间感知提示》|Yifei Chen, Ross Greer|<http://arxiv.org/pdf/2506.11124v1>|引入迭代错误纠正和空间感知提示技术，提升自然语言查询驱动的场景挖掘准确性和鲁棒性。|
|🆕 发布|TrajFlow: Multi-modal Motion Prediction via Flow Matching|轨迹流：通过流匹配实现多模态运动预测|Qi Yan, Brian Zhang, Yutong Zhang, Daniel Yang, Joshua White, Di Chen, Jiachao Liu, Langechuan Liu .etc.|<http://arxiv.org/pdf/2506.08541v1>|[代码](https://traj-flow.github.io/.); 提出了一种基于流匹配的多模态运动预测框架TrajFlow，提高了运动预测的效率和准确性。|


### 推理优化 (Inference Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference|HiSin：基于分辨率引导的渐进推理的高效高分辨率正弦图修复|Jiaze E, Srutarshi Banerjee, Tekin Bicer, Guannan Wang, Yanfu Zhang, Bin Ren|<http://arxiv.org/pdf/2506.08809v1>|提出HiSin框架，通过逐级提取低分辨率全局结构和延迟高分辨率处理实现高效的正弦图修复。|
|🆕 发布|SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting|场景散点++：用于语言高斯散点的大型数据集和全面基准测试|Mengjiao Ma, Qi Ma, Yue Li, Jiahuan Cheng, Runyi Yang, Bin Ren, Nikola Popovic, Mingqiang Wei .etc.|<http://arxiv.org/pdf/2506.08710v1>|提出首个大规模3D空间评估基准，推动通用3D场景理解的Gaussian Splatting方法发展。|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera|使用事件相机实时定位网球拍击球点的计算机视觉方法|Yuto Kase, Kai Ishibe, Ryoma Yasuda, Yudai Washida, Sakiko Hashimoto|<http://arxiv.org/pdf/2506.08327v1>|提出了一种使用事件相机实时定位网球拍击球点的技术，解决了传统方法高内存消耗和手动检测误差的问题。|
|🆕 发布|OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal|开放RR-1k：一个用于现实世界反射移除的可扩展数据集|Kangning Yang, Ling Ouyang, Huiming Sun, Jie Cai, Lan Fu, Jiaming Ding, Chiu Man Ho, Zibo Meng|<http://arxiv.org/pdf/2506.08299v1>|[代码](https://github.com/caijie0620/OpenRR-1k.); 提出了一种高效便捷的采集范式，构建了首个高质量、自然多样的OpenRR-1k反射去除数据集。|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Hyperbolic Dual Feature Augmentation for Open-Environment|双曲双特征增强用于开放环境|Peilin Yu, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Shuo Yang, Yunde Jia|<http://arxiv.org/pdf/2506.08906v1>|提出了一种适用于开放环境下的双特征增强方法，通过在双曲空间中同时增强已知和未知类别的特征，提高了学习...|
|🆕 发布|FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching|联邦长尾领域泛化：基于锐度引导的梯度匹配方法|Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi|<http://arxiv.org/pdf/2506.08518v1>|[代码](https://github.com/sunnyinAI/FedTail); FedTAIL通过sharpness引导的梯度匹配优化，有效解决长尾分布和优化目标冲突问题，提升域泛...|
|🆕 发布|RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation|RadioDUN：一种基于物理启发的深度展开网络用于无线电地图估计|Taiqin Chen, Zikun Zhou, Zheng Fang, Wenzhen Zou, Kanjun Liu, Ke Chen, Yongbing Zhang, Yaowei Wang|<http://arxiv.org/pdf/2506.08418v1>|提出RadioDUN网络，通过物理模型和动态重加权模块从稀疏样本估计密集无线电图。|


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation|面向类别的公平对抗训练：通过抗偏软标签蒸馏|Shiji Zhao, Chi Chen, Ranjie Duan, Xizhe Wang, Xingxing Wei|<http://arxiv.org/pdf/2506.08611v1>|提出了一种自适应调整软标签平滑度的方法ABSLD，以提升模型在不同类别间的对抗性公平性。|
|🆕 发布|Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement|面向跨个体肌电信号模式识别的双分支对抗特征解耦方法|Xinyue Niu, Akira Furui|<http://arxiv.org/pdf/2506.08555v1>|提出了一种无需模型校准的跨个体肌电信号识别方法，通过双分支对抗性特征分离实现鲁棒性能。|


### 视觉安全与隐私 (Visual Security & Privacy)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Secure Data Access in Cloud Environments Using Quantum Cryptography|基于量子密码学的云环境中安全数据访问方法|S. Vasavi Venkata Lakshmi, Ziaul Haque Choudhury|<http://arxiv.org/pdf/2506.10028v1>|利用量子密钥分发和量子一次性密码本，确保云环境中数据安全的创新方案。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces|基于几何深度学习的腹主动脉瘤表面局部生长预测|Dieuwertje Alblas, Patryk Rygiel, Julian Suk, Kaj O. Kappe, Marieke Hofman, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink|<http://arxiv.org/pdf/2506.08729v2>|提出了一种基于几何深度学习的模型，用于预测腹主动脉瘤表面局部增长，实现个性化监测策略。|
|🆕 发布|Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting|高斯到场景：通过三维高斯散点自监督学习进行三维场景表征学习|Keyi Liu, Weidong Yang, Ben Fei, Ying He|<http://arxiv.org/pdf/2506.08777v2>|提出了一种利用3D高斯散点投影的自监督学习方法，有效缓解了计算负担并增强了对三维场景的几何理解。|
|🆕 发布|Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping|超越校准：基于物理信息的原始到原始映射学习|Peter Grönquist, Stepan Tulyakov, Dengxin Dai|<http://arxiv.org/pdf/2506.08650v2>|提出了一种物理信息驱动的轻量级模型，有效解决了多相机间色彩一致性难题。|
|📝 更新|Effective Data Augmentation With Diffusion Models|有效的数据增强方法：基于扩散模型|Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov|<http://arxiv.org/pdf/2302.07944v3>|利用预训练的文本到图像扩散模型进行图像编辑，增强数据多样性，提升小样本图像分类准确率。|
|🆕 发布|An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation|基于MRI解读的脑卒中与肿瘤进展可解释深度学习框架|Rajan Das Gupta, Md Imrul Hasan Showmick, Mushfiqur Rahman Abir, Shanjida Akter, Md. Yeasin Rahat, Md. Jakir Hossen|<http://arxiv.org/pdf/2506.09161v1>|提出了一种基于深度学习的脑部病变早期诊断系统，通过MRI图像准确识别脑肿瘤和中风及其阶段。|
|🆕 发布|HomographyAD: Deep Anomaly Detection Using Self Homography Learning|同构AD：使用自同构学习的深度异常检测|Jongyub Seok, Chanjin Kang|<http://arxiv.org/pdf/2506.08784v1>|提出了一种针对实际工业环境设计的基于自同态学习的深度异常检测方法，提高了非完全对齐数据集的性能。|
|🆕 发布|MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning|MoSiC: 用于密集自监督学习的最优传输运动轨迹|Mohammadreza Salehi, Shashanka Venkataramanan, Ioana Simion, Efstratios Gavves, Cees G. M. Snoek, Yuki M Asano|<http://arxiv.org/pdf/2506.08694v1>|[代码](https://github.com/SMSD75/MoSiC); 提出了一种基于运动轨迹的密集自监督学习方法，通过优化特征聚类提高了动态场景下的学习一致性和鲁棒性。|
|🆕 发布|Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization|上下文感知TFL：一种用于时间伪造定位的通用上下文感知对比学习框架|Qilin Yin, Wei Lu, Xiangyang Luo, Xiaochun Cao|<http://arxiv.org/pdf/2506.08493v1>|提出了一种面向时空伪造定位的通用上下文感知对比学习框架，通过增强特征区分度有效提升了伪造片段的定位精...|
|📝 更新|The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning|民粹主义的面孔：利用机器学习分析政治领导人面部情感表达的差异|Sara Major, Aleksandar Tomašević|<http://arxiv.org/pdf/2304.09914v5>|利用深度学习分析政治领袖面部表情，发现不同民粹主义程度领导者的负面情绪得分存在显著差异。|


### 小样本学习 (Few-shot Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts|气候可视化：科学图表上的统计推理与事实验证基准|Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert|<http://arxiv.org/pdf/2506.08700v2>|提出首个大规模科学图表事实核查基准ClimateViz，结合知识图解释提升模型表现。|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 目标导向视觉决策 (Goal-oriented Visual Decision Making)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Visualization of a multidimensional point cloud as a 3D swarm of avatars|多维点云的3D虚拟人群体可视化|Leszek Luchowski, Dariusz Pojda|<http://arxiv.org/pdf/2504.06751v3>|提出用面部特征编码的多维数据可视化技术，通过3D头像群提高数据集的可解释性。|
|🆕 发布|SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything|SAMSelect：一种基于光谱指数搜索的海洋垃圾可视化分段任意模型选择方法|Joost van Dalen, Yuki M. Asano, Marc Russwurm|<http://arxiv.org/pdf/2506.08613v1>|提出SAMSelect算法，通过 Segment Anything 模型自动选择最佳波段组合，提升海...|
|🆕 发布|Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations|生成融合细粒度对齐注释的视觉-语言导航指令|Yibo Cui, Liang Xie, Yu Zhao, Jiawei Sun, Erwei Yin|<http://arxiv.org/pdf/2506.08566v1>|提出细粒度跨模态对齐注释的生成框架，提升视觉语言导航智能体的决策准确性和导航性能。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|LLaVA-c: Continual Improved Visual Instruction Tuning|LLaVA-c: 持续优化的视觉指令微调|Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu|<http://arxiv.org/pdf/2506.08666v2>|提出一种改进视觉指令微调的持续学习方法，平衡任务学习并防止基础模型退化。|
|🆕 发布|MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models|多模态动作模型评估与自适应的开源软件工具包与基准测试套件：MultiNet|Pranav Guruprasad, Yangyue Wang, Harshvardhan Sikka|<http://arxiv.org/pdf/2506.09172v1>|介绍了MultiNet，一个开源的评测和适应多模态动作模型的工具包和基准套件，以促进视觉、语言和动作...|
|🆕 发布|VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning|VIKI-R：通过强化学习协调具身多智能体合作|Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai .etc.|<http://arxiv.org/pdf/2506.09049v1>|提出VIKI-Bench和VIKI-R，通过结合视觉语言模型和强化学习提升多机器人视觉驱动的协作效率...|
|🆕 发布|Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions|苏格拉底-蒙特卡洛树搜索：通过提问正确的问题实现测试时的视觉推理|David Acuna, Ximing Lu, Jaehun Jung, Hyunwoo Kim, Amlan Kar, Sanja Fidler, Yejin Choi|<http://arxiv.org/pdf/2506.08927v1>|提出了一种基于蒙特卡洛树搜索的算法，通过向非推理模型注入子问题-子答案对，实现了测试时的视觉推理能力...|
|📝 更新|Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs|用火灭火（F3）：一种无需训练且高效的可视化对抗样本净化方法在LVLMs中|Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang|<http://arxiv.org/pdf/2506.01064v2>|提出了一种无需训练、高效净化视觉对抗样本的方法，通过向对抗样本注入噪声来提升模型输出的清洁度和可靠性...|
|📝 更新|VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?|VIST-GPT：开启大型语言模型视觉讲述新时代？|Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte|<http://arxiv.org/pdf/2504.19267v3>|提出了一种基于大型多模态模型的视觉讲述方法，使用新型无参考评价指标来更精准地评估故事叙述质量。|
|🆕 发布|PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly|PhyBlock：一种通过三维积木组装进行物理理解和规划的渐进式基准|Liang Ma, Jiajun Wen, Min Lin, Rongtao Xu, Xiwen Liang, Bingqian Lin, Jun Ma, Yongxin Wang .etc.|<http://arxiv.org/pdf/2506.08708v1>|分类|
|🆕 发布|VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism|VReST：通过树搜索和自我激励机制增强大型视觉-语言模型中的推理能力|Congzhi Zhang, Jiawei Peng, Zhenglin Wang, Yilong Lai, Haowen Sun, Heng Chang, Fei Ma, Weijiang Yu|<http://arxiv.org/pdf/2506.08691v1>|提出VReST方法，通过树搜索和自我奖励机制增强大型视觉语言模型在复杂视觉推理上的效果。|
|📝 更新|NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples|自然Bench：在自然对抗样本上评估视觉-语言模型|Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna .etc.|<http://arxiv.org/pdf/2410.14669v4>|提出了一种新的视觉问答基准NaturalBench，通过自然对抗样本揭示了现有视觉语言模型在真实场景...|
|🆕 发布|Data-Efficient Challenges in Visual Inductive Priors: A Retrospective|《视觉归纳先验中的数据高效挑战：回顾》|Robert-Jan Bruintjes, Attila Lengyel, Osman Semih Kayhan, Davide Zambrano, Nergis Tömen, Hadi Jamali-Rad, Jan van Gemert|<http://arxiv.org/pdf/2506.08612v1>|探究了在数据匮乏环境下提升深度学习模型训练效率的方法，推动了融合先验知识的新策略发展。|
|📝 更新|Multimodal Rationales for Explainable Visual Question Answering|多模态解释性视觉问答的推理依据|Kun Li, George Vosselman, Michael Ying Yang|<http://arxiv.org/pdf/2402.03896v3>|[代码](https://github.com/lik1996/MRVQA2025.); 提出MRVQA模型，通过生成视觉和文本解释来提升视觉问答的可解释性和可信度。|
|🆕 发布|From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge|从像素到图：使用场景和知识图应对高清-EPIC视觉问答挑战|Agnese Taluzzi, Davide Gesualdi, Riccardo Santambrogio, Chiara Plizzari, Francesca Palermo, Simone Mentasti, Matteo Matteucci|<http://arxiv.org/pdf/2506.08553v1>|提出SceneNet和KnowledgeNet两种方法，通过场景图和常识知识图提升 egocentr...|
|🆕 发布|Robust Visual Localization via Semantic-Guided Multi-Scale Transformer|通过语义引导的多尺度变换器实现的鲁棒视觉定位|Zhongtao Tian, Wenhao Huang, Zhidong Chen, Xiao Wei Sun|<http://arxiv.org/pdf/2506.08526v1>|提出了一种结合多尺度特征学习和语义理解的视觉定位框架，有效应对动态环境下的定位挑战。|
|📝 更新|Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering|元自适应提示蒸馏用于少量样本视觉问答|Akash Gupta, Amos Storkey, Mirella Lapata|<http://arxiv.org/pdf/2506.06905v2>|提出了一种元学习框架，通过任务相关的软提示蒸馏和适应，增强了小型多模态模型在少量样本下的视觉问答能力...|
|📝 更新|Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment|《一切皆可用言语描述：一个简单统一的多模态框架，具有语义与时间对齐》|Xiaowei Bi, Zheyuan Xu|<http://arxiv.org/pdf/2503.09081v2>|提出UMaT框架，将视觉和听觉输入统一为结构化文本，提升多模态推理效率和长视频问答准确性。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better|自回归语义视觉重建助力视觉语言模型更好地理解|Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, Jiaqi Wang|<http://arxiv.org/pdf/2506.09040v1>|[代码](https://github.com/AlenjandroWang/ASVR.); 引入了Autoregressive Semantic Visual Reconstruction，通...|
|🆕 发布|Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring|更少数据下的更好推理：通过统一模态评分增强视觉语言模型|Mingjie Xu, Andrew Estornell, Hongzheng Yang, Yuzhi Zhao, Zhaowei Zhu, Qi Xuan, Jiaheng Wei|<http://arxiv.org/pdf/2506.08429v1>|提出了一种质量驱动的数据选择方法SCALE，通过跨模态评估提升视觉语言模型的数据质量和任务适应性。|


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Human-Aligned Image Models Improve Visual Decoding from the Brain|《与人类一致性的图像模型提高了大脑视觉解码能力》|Nona Rajabi, Antônio H. Ribeiro, Miguel Vasco, Farzaneh Taleb, Mårten Björkman, Danica Kragic|<http://arxiv.org/pdf/2502.03081v3>|使用人类对齐的图像编码器映射脑信号，提高了图像检索准确度达21%。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding|MedMoE：面向医学视觉语言理解的模态专用混合专家模型|Shivang Chopra, Gabriela Sanchez-Rodriguez, Lingchao Mao, Andrew J Feola, Jing Li, Zsolt Kira|<http://arxiv.org/pdf/2506.08356v2>|提出了一种针对不同医学成像模态的专用视觉表征框架MedMoE，通过动态调整视觉表示以适应诊断上下文，...|
|🆕 发布|Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis|适配视觉-语言基础模型以应对下一代医学超声图像分析|Jingguo Qu, Xinyang Han, Tonghuan Xiao, Jia Ai, Juan Wu, Tong Zhao, Jing Qin, Ann Dorothy King .etc.|<http://arxiv.org/pdf/2506.08849v2>|[代码](https://github.com/jinggqu/NextGen-UIA.); 提出域自适应方法改进视觉语言基础模型，有效提升超声图像分析和分割性能。|
|🆕 发布|PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies|PatchGuard：通过视觉变换器和伪异常实现对抗性稳健的异常检测与定位|Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, Mohammad Hossein Rohban|<http://arxiv.org/pdf/2506.09237v1>|[代码](https://github.com/rohban-lab/PatchGuard); PatchGuard通过在Vision Transformer架构中结合伪异常样本和对抗训练，提高了...|
|📝 更新|The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound|《在医学超声自监督学习中保持语义的转换的有效性》|Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield|<http://arxiv.org/pdf/2504.07904v2>|本研究针对医学超声图像提出语义保持的数据增强方法，显著提升COVID-19分类等任务的性能。|
|🆕 发布|Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule|感知特性距离：在特定决策规则下测量动态条件下感知系统的稳定性和鲁棒性|Boyu Jiang, Liang Shi, Zhengzhi Lin, Loren Stowe, Feng Guo|<http://arxiv.org/pdf/2506.09217v1>|[代码](https://github.com/datadrivenwheels/PCD_Python.); 提出了一种衡量自动驾驶感知系统在动态条件下稳定性和鲁棒性的新指标PCD，并通过SensorRainF...|
|🆕 发布|The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset|RSNA腰椎退行性成像脊柱分类（LumbarDISC）数据集|Tyler J. Richards, Adam E. Flanders, Errol Colak, Luciano M. Prevedello, Robyn L. Ball, Felipe Kitamura, John Mongan, Maryam Vazirabad .etc.|<http://arxiv.org/pdf/2506.09162v1>|介绍了RSNA LumbarDISC数据集，助力于腰椎退行性病变的深度学习模型研发，以提升患者护理和...|
|🆕 发布|DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging|DIsoN：用于医学影像中的非分布外检测的去中心化隔离网络|Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas|<http://arxiv.org/pdf/2506.09024v1>|提出了一种无需共享训练数据，通过模型参数交换进行异常检测的 decentralized OOD 检测...|
|🆕 发布|SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation|SSS：用于医学影像分割的半监督SAM-2与高效提示方法|Hongjie Zhu, Xiwei Liu, Rundong Xue, Zeyu Zhang, Yong Xu, Daji Ergu, Ying Cai, Yang Zhao|<http://arxiv.org/pdf/2506.08949v1>|[代码](https://github.com/AIGeeksGroup/SSS.); 提出了一种半监督学习方法SSS，利用SAM-2模型和增强机制提升医疗图像分割性能。|
|🆕 发布|Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction|低秩增强隐式神经表示用于无监督高维定量磁共振成像重建|Haonan Zhang, Guoyan Lao, Yuyao Zhang, Hongjiang Wei|<http://arxiv.org/pdf/2506.09100v1>|提出了一种结合低秩和连续性先验的LoREIN框架，用于加速三维多参数定量磁共振成像重建。|
|🆕 发布|Image-Based Method For Measuring And Classification Of Iron Ore Pellets Using Star-Convex Polygons|基于星形凸多边形的图像方法用于测量和分类铁矿石球团|Artem Solomko, Oleg Kartashev, Andrey Golov, Mikhail Deulin, Vadim Valynkin, Vasily Kharin|<http://arxiv.org/pdf/2506.11126v1>|提出了一种基于StarDist算法的图像测量方法，提高了铁矿石颗粒尺寸测量的准确性和分类效率。|
|📝 更新|Zero-Shot Gaze-based Volumetric Medical Image Segmentation|零样本注视驱动的体素级医学图像分割|Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin|<http://arxiv.org/pdf/2505.15256v2>|提出使用眼动追踪作为新型交互方式，实现三维医学图像的快速交互式分割。|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 视觉认知计算 (Visual Cognitive Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language|通过低维属性对齐实现视觉与语言的灵活工具选择|Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu|<http://arxiv.org/pdf/2505.22146v2>|提出了一种低维属性对齐框架，有效提升了计算机模型在视觉工具感知与语言任务理解中的工具选择能力。|
|📝 更新|ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models|《ArchiLense：基于视觉大型语言模型的建筑风格定量分析框架》|Jing Zhong, Jun Yin, Peilin Li, Pengyu Zeng, Miao Zang, Ran Luo, Shuai Lu|<http://arxiv.org/pdf/2506.07739v2>|提出ArchDiffBench数据集和ArchiLense框架，利用视觉语言模型自动识别和分析建筑风...|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes|基于事件的运动鲁棒精确形状估计：适用于混合反射率场景|Aniket Dashpute, Jiazhang Wang, James Taylor, Oliver Cossairt, Ashok Veeraraghavan, Florian Willomitzer|<http://arxiv.org/pdf/2311.09652v2>|提出了一种基于事件相机的高精度、运动鲁棒的混合反射场景三维成像方法。|
|🆕 发布|SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction|SDTagNet：利用文本注释导航地图进行在线高精度地图构建|Fabian Immel, Jan-Hendrik Pauls, Richard Fehler, Frank Bieder, Jonas Merkert, Christoph Stiller|<http://arxiv.org/pdf/2506.08997v1>|[代码](https://github.com/immel-f/SDTagNet); 提出利用带有文本注释的标准定义地图来增强远程检测精度，实现在线高精度地图构建的新方法SDTagNet...|
|🆕 发布|HSG-12M: A Large-Scale Spatial Multigraph Dataset|HSG-12M：大规模空间多图数据集|Xianquan Yan, Hakan Akgün, Kenji Kawaguchi, N. Duane Loh, Ching Hua Lee|<http://arxiv.org/pdf/2506.08618v1>|首次构建大规模空间多重图数据集HSG-12M，为几何感知图学习提供新基准，推动科学发现。|
|📝 更新|Just Project! Multi-Channel Despeckling, the Easy Way|“仅投影！多通道去噪，轻松之道”|Loïc Denis, Emanuele Dalsasso, Florence Tupin|<http://arxiv.org/pdf/2408.11531v2>|提出了一种利用现有单通道去噪方法的通用框架MuChaPro，通过生成多个单通道投影并重新组合，有效去...|
|🆕 发布|Princeton365: A Diverse Dataset with Accurate Camera Pose|普林斯顿365：一个具有精确相机姿态的多样化数据集|Karhan Kayan, Stamatis Alexandropoulos, Rishabh Jain, Yiming Zuo, Erich Liang, Jia Deng|<http://arxiv.org/pdf/2506.09035v1>|提出Princeton365数据集，通过创新地面实况收集框架，提高了SLAM准确性与数据多样性，并引...|

