## [UPDATED!] **2025-09-26** (Update Time)


## 视觉表征与基础模型 (Visual Representation & Foundation Models)


### 大规模预训练模型 (Large-scale Pretrained Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|WoW: Towards a World omniscient World model Through Embodied Interaction|《WoW：通过具身交互构建全知世界模型》|Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian .etc.|<http://arxiv.org/pdf/2509.22642v1>|提出了一种通过机器人互动学习物理直觉的14亿参数生成世界模型，实现了物理因果性和动态交互的突破性进展...|
|📝 更新|Large Pre-Training Datasets Don't Always Guarantee Robustness after Fine-Tuning|大规模预训练数据集在微调后并不总能保证鲁棒性|Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete|<http://arxiv.org/pdf/2410.21582v3>|[代码](https://jd730.github.io/projects); 发现大规模预训练模型在微调后易失去鲁棒性，提出Robustness Inheritance Benc...|
|📝 更新|TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting|时间感知多模态模型TAMMs：用于卫星图像变化理解与预测|Zhongbin Guo, Yuhao Wang, Ping Jian, Chengzhi Li, Xinyue Chen, Zhen Yang, Ertai E|<http://arxiv.org/pdf/2506.18862v2>|提出了TAMMs框架，通过增强长时序理解能力，首次统一了卫星图像时间序列的时序变化描述与未来图像预测...|
|📝 更新|Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)|多模态循环集成模型用于预测大脑对自然场景电影反应的研究（Algonauts 2025）|Semih Eren, Deniz Kucukahmetler, Nico Scherf|<http://arxiv.org/pdf/2507.17897v3>|提出了一种融合视觉、听觉和语义信息的多模态循环集成模型，有效预测大脑对自然场景的反应。|
|🆕 发布|Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results|大型多模态模型在检测虚假信息中的有效性：实验结果|Yasmina Kheddache, Marc Lalonde|<http://arxiv.org/pdf/2509.22377v1>|探究大型多模态模型在检测虚假信息中的有效性，提出了一套优化的评估框架和方法。|
|📝 更新|SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models|SCAM：面向多模态基础模型的现实世界印刷稳健性评估|Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Jonas Loos, Leo Pinetzki, Erik Rodner, Lorenz Hufe|<http://arxiv.org/pdf/2504.04893v6>|提出SCAM数据集，揭示了文字攻击对多模态模型的影响，并验证了大型语言模型 backbone 的抗攻...|
|📝 更新|Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling|开源多模态模型性能边界的拓展：模型、数据与测试时缩放|Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye .etc.|<http://arxiv.org/pdf/2412.05271v5>|扩展开源多模态模型性能边界，通过模型、数据和测试时扩展实现显著性能提升。|
|🆕 发布|MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing|MinerU2.5：一种用于高效高分辨率文档解析的解耦视觉-语言模型|Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He .etc.|<http://arxiv.org/pdf/2509.22186v1>|提出了一种高效的文档解析模型MinerU2.5，通过解耦全局布局分析和局部内容识别，实现了高分辨率文...|
|📝 更新|Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders|探究多模态大型语言模型中多视觉编码器的冗余性|Yizhou Wang, Song Mao, Yang Chen, Yufan Shen, Yinqiao Yan, Pinlong Cai, Ding Wang, Guohang Yan .etc.|<http://arxiv.org/pdf/2507.03262v2>|揭示了多视觉编码器在大型多模态语言模型中的冗余问题，并提出衡量编码器贡献度的指标。|
|🆕 发布|FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration|FailureAtlas：通过主动探索映射T2I模型的失败景观|Muxi Chen, Zhaohua Zhang, Chenchen Zhao, Mingyang Chen, Wenyu Jiang, Tianwen Jiang, Jianhuan Zhuo, Yu Tang .etc.|<http://arxiv.org/pdf/2509.21995v1>|[代码](https://github.com/cure-lab/FailureAtlas); 提出主动探索框架 FailureAtlas，系统性映射 T2I 模型失败场景，发现训练数据不足问题。|
|📝 更新|Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal Models Benefits Image Editing|《Draw-In-Mind：在统一多模态模型中重平衡设计师-画师角色以优化图像编辑》|Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou|<http://arxiv.org/pdf/2509.01986v2>|[代码](https://github.com/showlab/DIM.); 提出了一种角色平衡的统一多模态模型，通过明确分工提升图像编辑精度。|
|🆕 发布|Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding|基础模型中的空间推理：以对象为中心的空间理解基准测试|Vahid Mirjalili, Ramin Giahi, Sriram Kollipara, Akshay Kekuda, Kehui Yao, Kai Zhao, Jianpeng Xu, Kaushiki Nag .etc.|<http://arxiv.org/pdf/2509.21922v1>|提出空间推理基准，揭示视觉模型在物体位置与关系理解上的不足。|
|📝 更新|DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models|驱动行为：用于探索VLA模型中类人驾驶决策的基准|Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou .etc.|<http://arxiv.org/pdf/2506.05667v2>|提出了DriveAction基准，为VLA模型提供多样化场景、可靠动作标注和符合人类偏好的评估协议。|
|📝 更新|DOTA: Distributional Test-Time Adaptation of Vision-Language Models|DOTA：视觉语言模型分布式测试时适应|Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike Zheng Shou, Changqing Zhang|<http://arxiv.org/pdf/2409.19375v3>|提出了一种分布性测试时适应方法DOTA，通过动态估计测试数据分布来减少灾难性遗忘，提升视觉语言模型适...|
|📝 更新|Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models|利用模型指导从个性化扩散模型中提取训练数据|Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu|<http://arxiv.org/pdf/2410.03039v3>|[代码](https://github.com/Nicholas0228/FineXtract.); 提出了一种 FineXtract 框架，通过模型引导从微调后的扩散模型中提取训练数据，验证了数据泄露...|
|🆕 发布|On the Status of Foundation Models for SAR Imagery|合成孔径雷达图像基础模型的现状|Nathan Inkawhich|<http://arxiv.org/pdf/2509.21722v1>|探究了基础AI/ML模型在合成孔径雷达图像识别中的应用，通过自监督学习微调实现了性能提升。|


### 视觉Transformer架构 (Vision Transformer Architectures)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach|CCNeXt：一种有效的自监督立体深度估计方法|Alexandre Lopes, Roberto Souza, Helio Pedrini|<http://arxiv.org/pdf/2509.22627v1>|[代码](https://github.com/alelopes/CCNext); 提出了一种高效的自监督立体深度估计方法CCNeXt，通过创新的网络架构在保持高性能的同时大幅提升计算...|
|🆕 发布|LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision|《标注协驾驶员：一种用于计算机视觉中自动化数据整理的深度研究代理》|Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru .etc.|<http://arxiv.org/pdf/2509.22631v1>|提出了一种深度研究代理Labeling Copilot，自动化优化计算机视觉数据集的质量、多样性和成...|
|🆕 发布|EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model|高效深度：一种快速且细节保持的单目深度估计模型|Andrii Litvynchuk, Ivan Livinsky, Anand Ravi, Nima Kalantari, Andrii Tsarov|<http://arxiv.org/pdf/2509.22527v1>|提出EfficientDepth模型，结合transformer与轻量卷积解码器，高效生成细节丰富的...|
|📝 更新|Surgical Vision World Model|手术视觉世界模型|Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai|<http://arxiv.org/pdf/2503.02904v2>|[代码](https://github.com/bhattarailab/Surgical-Vision-World-Model); 提出首个手术视觉世界模型，无需动作标注即可生成可控的手术数据。|
|🆕 发布|Large Material Gaussian Model for Relightable 3D Generation|用于可重光照三维生成的广义材料高斯模型|Jingrui Ye, Lingting Zhu, Runze Zhang, Zeyu Hu, Yingda Yin, Lanjiong Li, Lequan Yu, Qingmin Liao|<http://arxiv.org/pdf/2509.22112v1>|提出了Large Material Gaussian Model，通过生成具有物理渲染材料属性的3D...|
|📝 更新|Octic Vision Transformers: Quicker ViTs Through Equivariance|八次视变换器：通过等方差性实现更快的视觉变换器|David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman|<http://arxiv.org/pdf/2505.15441v3>|引入了利用八次群等方差性的Octic Vision Transformers，实现了比普通ViT更高...|
|📝 更新|Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration|生动-VR：从文本到视频扩散变换器中提炼概念以实现逼真视频修复|Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen|<http://arxiv.org/pdf/2508.14483v3>|[代码](https://github.com/csbhr/Vivid-VR.); Vivid-VR通过概念蒸馏训练策略和改进的控制架构，解决了视频修复中的分布偏移问题，实现了高质量的...|
|📝 更新|Image Recognition with Online Lightweight Vision Transformer: A Survey|在线轻量级视觉Transformer的图像识别：综述|Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo .etc.|<http://arxiv.org/pdf/2505.03113v3>|[代码](https://github.com/ajxklo/Lightweight-VIT); 概述了轻量级视觉Transformer在图像识别中的应用策略，提升了效率并探讨了未来研究方向。|
|🆕 发布|UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models|统一视觉任务与预训练视频生成模型|Lan Chen, Yuchao Gu, Qi Mao|<http://arxiv.org/pdf/2509.21760v1>|[代码](https://github.com/CUC-MIPG/UniVid.); 提出UniVid框架，利用预训练视频生成模型统一处理多种视觉任务，无需任务特定修改。|


### 多模态表征学习 (Multimodal Representation Learning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting|使用二维高斯散点绘制从压缩图像表示中实现视觉-语言对齐|Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe|<http://arxiv.org/pdf/2509.22615v1>|提出了一种基于2D Gaussian Splatting的紧凑视觉表示方法，有效压缩图像数据并提高视...|
|🆕 发布|Explaining multimodal LLMs via intra-modal token interactions|通过模内标记交互解释多模态大型语言模型|Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu, Qunli Zhang, Zheng Hu, Xiaochun Cao|<http://arxiv.org/pdf/2509.22415v1>|提出了一种通过增强模内交互来提升多模态大型语言模型解释性的方法，提高了视觉和文本解释的完整性和准确性...|
|🆕 发布|Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning|通过分层跨模态超图学习进行行人属性识别|Xiao Wang, Shujuan Wu, Xiaoxia Cheng, Changwei Bi, Jin Tang, Bin Luo|<http://arxiv.org/pdf/2509.22331v1>|[代码](https://github.com/Event-AHU/OpenPAR); 提出构建多模态知识图以增强行人的视觉特征与属性关联，引入了知识图引导的跨模态超图学习框架提升行人属性...|
|🆕 发布|MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models|多模态程序合成：使用大型多模态模型进行程序性材质的生成|Jonas Belouadi, Tamy Boubekeur, Adrien Kaiser|<http://arxiv.org/pdf/2509.22151v1>|提出了一种结合视觉和文本的的多模态程序合成框架，有效生成高质量过程材料图，优于纯文本方法。|
|🆕 发布|Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics|轻量级结构化多模态推理在机器人临床场景理解中的应用|Saurav Jha, Stefan K. Ehrlich|<http://arxiv.org/pdf/2509.22014v1>|提出了一种轻量级多模态框架，通过融合视觉与语言信息，增强了医疗机器人场景理解的时序推理和不确定性估计...|
|🆕 发布|WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM|WAVE：使用多模态大型语言模型学习统一且多功能的音频视觉嵌入|Changli Tang, Qinfan Xiao, Ke Mei, Tianyi Wang, Fengyun Rao, Chao Zhang|<http://arxiv.org/pdf/2509.21990v1>|首次提出基于大型语言模型的统一音频-视觉嵌入方法，实现了任意模态间的检索和生成用户指令感知的嵌入。|
|🆕 发布|PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data|"PartSAM：一种基于原生三维数据训练的可扩展提示性部件分割模型"|Zhe Zhu, Le Wan, Rui Xu, Yiheng Zhang, Honghua Chen, Zhiyang Dou, Cheng Lin, Yuan Liu .etc.|<http://arxiv.org/pdf/2509.21965v1>|首次提出原生训练在大规模3D数据上的可提示部分分割模型PartSAM，实现精准识别和自动分解3D物体...|
|🆕 发布|Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization|通过标题中的关键词和整体结构，将该论文标题翻译为中文如下：  “通过标题正则化策略优化的感知一致性多模态大型语言模型推理”|Songjun Tu, Qichao Zhang, Jingbo Sun, Yuqian Fu, Linjing Li, Xiangyuan Lan, Dongmei Jiang, Yaowei Wang .etc.|<http://arxiv.org/pdf/2509.21854v1>|提出了一种名为CapPO的强化学习框架，通过结合图像描述的一致性正则化，有效解决了多模态大语言模型在...|
|📝 更新|On the Value of Cross-Modal Misalignment in Multimodal Representation Learning|《在多模态表征学习中跨模态不匹配的价值》|Yichao Cai, Yuhang Liu, Erdun Gao, Tianjiao Jiang, Zhen Zhang, Anton van den Hengel, Javen Qinfeng Shi|<http://arxiv.org/pdf/2504.10143v7>|探讨了跨模态不对齐对多模态表征学习的影响，并提出利用选择偏差和扰动偏差机制理解并优化学习过程。|
|📝 更新|Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning|通过强化微调统一的跨模态思维链奖励模型|Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang|<http://arxiv.org/pdf/2505.03318v2>|提出了一种统一的多模态长链推理奖励模型，通过强化微调激发模型复杂推理能力，提升奖励信号准确性。|


## 视觉识别与理解 (Visual Recognition & Understanding)


### 目标检测与定位 (Object Detection & Localization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection|基于瓦片分割的语义门控策略在密集目标检测中的证据重要性|Yilun Xiao|<http://arxiv.org/pdf/2509.10779v2>|提出了一种基于分块和聚类验证的后期处理框架，通过利用重叠区域的信息显著提升了无人机影像中密集小目标的...|
|🆕 发布|HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection|《HyCoVAD：一种用于复杂视频异常检测的混合自监督学习-语言模型》|Mohammad Mahdi Hemmatyar, Mahdi Jafari, Mohammad Amin Yousefi, Mohammad Reza Nemati, Mobin Azadani, Hamid Reza Rastad, Amirmohammad Akbari|<http://arxiv.org/pdf/2509.22544v1>|提出HyCoVAD模型，融合自监督学习和大型语言模型，有效识别复杂视频异常。|
|🆕 发布|HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network for UAV Photography|HierLight-YOLO：一种面向无人机摄影的层次化轻量级目标检测网络|Defan Chen, Yaohua Hu, Luchan Zhang|<http://arxiv.org/pdf/2509.22365v1>|提出HierLight-YOLO模型，通过多尺度特征融合和轻量级模块，提升无人机摄影中小目标实时检测...|
|📝 更新|Small Dents, Big Impact: A Dataset and Deep Learning Approach for Vehicle Dent Detection|小凹陷，大影响：车辆凹陷检测的数据集与深度学习方法|Danish Zia Baig, Mohsin Kamal, Zahid Ullah|<http://arxiv.org/pdf/2508.15431v2>|提出了一种基于YOLOv8的深度学习车辆微凹痕检测方法，提高了检测精度和实时性。|
|📝 更新|Real-Time Object Detection Meets DINOv3|实时目标检测遇见DINOv3|Shihua Huang, Yongjie Hou, Longfei Liu, Xuanlong Yu, Xi Shen|<http://arxiv.org/pdf/2509.20787v2>|[代码](https://github.com/Intellindust-AI-Lab/DEIMv2); 集成DINOv3特征的DEIMv2模型在保持实时性的同时，提升了性能并覆盖了从高性能到轻量级部署的多...|
|🆕 发布|Motion-Aware Transformer for Multi-Object Tracking|运动感知Transformer用于多目标跟踪|Xu Yang, Gady Agam|<http://arxiv.org/pdf/2509.21715v1>|提出了一种预测物体运动以优化跟踪的Motion-Aware Transformer，显著提升了多目标...|


### 语义/实例分割 (Semantic/Instance Segmentation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation|野外挖掘：用于语义分割的GOOSE-Ex数据集|Raphael Hagmanns, Peter Mortimer, Miguel Granero, Thorsten Luettel, Janko Petereit|<http://arxiv.org/pdf/2409.18788v2>|扩展GOOSE数据集，通过新增5000帧不同环境下的多模态数据，提升自主系统在复杂环境中的泛化能力。|


### 关键点定位与姿态估计 (Keypoint Detection & Pose Estimation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|$γ$-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition|面向低比特模式识别的可学习量化方法$γ$-Quant|Mishal Fatima, Shashank Agnihotri, Marius Bock, Kanchana Vaishnavi Gandikota, Kristof Van Laerhoven, Michael Moeller, Margret Keuper|<http://arxiv.org/pdf/2509.22448v1>|[代码](https://github.com/Mishalfatima/Gamma-Quant); 提出了一种针对低比特度传感器数据的任务特定非线性量化方法，实现了4比特量化数据与12比特原始数据相当...|
|🆕 发布|SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion|SSVIF：自监督分割导向的可见光与红外图像融合|Zixian Zhao, Xingchen Zhang|<http://arxiv.org/pdf/2509.22450v1>|提出了一种无需标注数据、基于自监督学习的可见光与红外图像融合方法，实现了与传统方法相比更优的分割性能...|
|📝 更新|Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation|基于Mamba驱动的拓扑融合的单目3D人体姿态估计|Zenghao Zheng, Lianping Yang, Jinshan Pan, Hegui Zhu|<http://arxiv.org/pdf/2505.20611v2>|提出了一种融合骨骼拓扑结构的Mamba-Driven Topology Fusion框架，有效降低计...|
|📝 更新|Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models|使用门检测和大语言模型进行建筑合规性检查的自动化设施枚举|Licheng Zhang, Bach Le, Naveed Akhtar, Tuan Ngo|<http://arxiv.org/pdf/2509.17283v2>|提出了一种结合门检测和大型语言模型推理的自动化设施统计方法，提高了建筑合规性检查的效率和准确性。|
|📝 更新|Pose Prior Learner: Unsupervised Categorical Prior Learning for Pose Estimation|姿态先验学习器：用于姿态估计的无监督类别先验学习|Ziyu Wang, Shuangpeng Han, Mengmi Zhang|<http://arxiv.org/pdf/2410.03858v3>|提出了一种无监督学习姿势先验的方法，通过自我监督学习提升姿势估计准确性。|
|🆕 发布|Rate-Distortion Optimized Communication for Collaborative Perception|协作感知中的率失真优化通信|Genjia Liu, Anning Hu, Yue Hu, Wenjun Zhang, Siheng Chen|<http://arxiv.org/pdf/2509.21994v1>|提出了一种基于信息理论的通信优化框架RDcomm，通过任务相关编码和互信息驱动的消息选择，显著提升了...|
|🆕 发布|SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference|单目新型对象位姿估计：基于单张RGB参考图像的SingRef6D方法|Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee|<http://arxiv.org/pdf/2509.21927v1>|提出了一种仅使用单张RGB参考图像的6D姿态估计方法，无需深度传感器或多视角图像，提高了在资源受限条...|
|🆕 发布|Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition|融合场景上下文与语义标签以提升群体级情感识别效果|Qing Zhu, Wangdong Guo, Qirong Mao, Xiaohua Huang, Xiuyan Shao, Wenming Zheng|<http://arxiv.org/pdf/2509.21747v1>|整合场景上下文和情感标签语义信息，提出新框架提升多人物情感识别性能。|


### 图像分类与识别 (Image Classification & Recognition)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models|基于规则的强化学习在文档图像分类中结合视觉语言模型的应用|Michael Jungo, Andreas Fischer|<http://arxiv.org/pdf/2509.22283v1>|[代码](https://github.com/jungomi/vision-finetune.); 探究了基于规则的强化学习在文档图像分类中的应用，提升了模型对非分布内数据的泛化能力。|


## 生成式视觉模型 (Generative Visual Modeling)


### 扩散概率模型 (Diffusion Probabilistic Models)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning|CapRL：通过强化学习激发密集图像字幕能力|Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu .etc.|<http://arxiv.org/pdf/2509.22647v1>|[代码](https://github.com/InternLM/CapRL.); 提出了一种基于强化学习的图像描述生成框架CapRL，通过验证性奖励提升描述的多样性和准确性。|
|🆕 发布|Pixel Motion Diffusion is What We Need for Robot Control|像素运动扩散是机器人控制所必需的|E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo|<http://arxiv.org/pdf/2509.22652v1>|[代码](https://nero1342.github.io/DAWN); 提出DAWN框架，通过像素运动扩散实现语言驱动的机器人控制，实现端到端训练并提高多任务性能。|
|🆕 发布|RefAM: Attention Magnets for Zero-Shot Referral Segmentation|“RefAM：零样本指引用例分割的注意力磁铁”|Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele|<http://arxiv.org/pdf/2509.22650v1>|提出了一种无需额外训练的RefAM框架，利用扩散变换器的注意力得分进行零样本指引用户图像分割，实现了...|
|🆕 发布|Scale-Wise VAR is Secretly Discrete Diffusion|尺度感知的VAR实际上是一种隐式的离散扩散模型|Amandeep Kumar, Nithin Gopalakrishnan Nair, Vishal M. Patel|<http://arxiv.org/pdf/2509.22636v1>|揭示了VAR模型在特定条件下等同于离散扩散过程，实现了更高效的视觉生成。|
|🆕 发布|UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning|统一建模语言辅助的机器人房间清洁：结构化推理与规划|Hongyu Chen, Guangrun Wang|<http://arxiv.org/pdf/2509.22628v1>|提出UML-CoT框架，利用统一建模语言提升机器人清扫任务的推理和规划能力。|
|📝 更新|Metric-Guided Conformal Bounds for Probabilistic Image Reconstruction|基于度量引导的保形界限用于概率图像重建|Matt Y Cheung, Tucker J Netherton, Laurence E Court, Ashok Veeraraghavan, Guha Balakrishnan|<http://arxiv.org/pdf/2404.15274v4>|提出了一种基于度量引导的符合预测框架，为概率性图像重建算法提供了可证明有效的预测边界。|
|📝 更新|Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided Diffusion|扩散课程：通过图像引导扩散的合成数据到真实数据课程|Yijun Liang, Shweta Bhardwaj, Tianyi Zhou|<http://arxiv.org/pdf/2410.13674v4>|提出了一种通过图像引导的扩散模型生成高质量合成数据的方法，构建了Diffusion Curricul...|
|🆕 发布|LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer|LucidFlux：无需字幕的通用图像恢复——通过大规模扩散变换器|Song Fei, Tian Ye, Lujia Wang, Lei Zhu|<http://arxiv.org/pdf/2509.22414v1>|提出了一种无需文本描述的大规模扩散变换器框架LucidFlux，实现了在未知混合退化条件下的图像恢复...|
|🆕 发布|Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models|缩小安全性差距：在视觉自回归模型中实现手术概念擦除|Xinhao Zhong, Yimin Zhou, Zhiqi Zhang, Junhao Li, Yi Sun, Bin Chen, Shu-Tao Xia, Ke Xu|<http://arxiv.org/pdf/2509.22400v1>|提出了一种针对视觉自回归模型的概念擦除框架VARE和S-VARE方法，有效解决了文本到图像生成中的安...|
|🆕 发布|Gradient-based multi-focus image fusion with focus-aware saliency enhancement|基于梯度多聚焦图像融合及聚焦感知显著性增强|Haoyu Li, XiaoSong Li|<http://arxiv.org/pdf/2509.22392v1>|[代码](https://github.com/Lihyua/GICI); 提出了一种基于梯度的多聚焦图像融合方法，通过增强边界细节和聚焦区域，有效解决了现有方法中模糊过渡和细...|
|🆕 发布|RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer|RAPID^3：三层强化加速策略的扩散变换器|Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang .etc.|<http://arxiv.org/pdf/2509.22323v1>|RAPID3通过三级强化学习策略为扩散变换器实现图像自适应加速，无需更新基础生成器即可提升采样速度近...|
|🆕 发布|HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models|HiGS：基于历史引导的采样方法用于扩散模型的即插即用增强|Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber|<http://arxiv.org/pdf/2509.22300v1>|提出了一种历史引导采样技术HiGS，通过整合近期模型预测提升扩散模型图像生成质量和效率。|
|🆕 发布|Jailbreaking on Text-to-Video Models via Scene Splitting Strategy|通过场景分割策略对文本到视频模型的破解|Wonjun Lee, Haon Park, Doehyeon Lee, Bumsub Ham, Suhyun Kim|<http://arxiv.org/pdf/2509.22292v1>|提出了一种名为SceneSplit的黑盒攻击方法，通过场景拆分策略绕过文本到视频模型的安全过滤，实现...|
|🆕 发布|NIFTY: a Non-Local Image Flow Matching for Texture Synthesis|NIFTY：一种用于纹理合成的非局部图像流匹配方法|Pierrick Chatillon, Julien Rabin, David Tschumperlé|<http://arxiv.org/pdf/2509.22318v1>|[代码](https://github.com/PierrickCh/Nifty.git); 提出了一种结合扩散模型和传统纹理优化技术的非参数流匹配方法，有效解决了纹理合成中的初始化问题和视觉伪...|
|🆕 发布|FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing|《FlashEdit：解耦速度、结构与语义以实现精确图像编辑》|Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang|<http://arxiv.org/pdf/2509.22244v1>|[代码](https://github.com/JunyiWuCode/FlashEdit.); FlashEdit通过创新的编辑流程和注意力机制，实现了高效且精确的实时图像编辑。|
|🆕 发布|Polysemous Language Gaussian Splatting via Matching-based Mask Lifting|通过基于匹配的掩码提升实现多义性语言高斯散点绘制|Jiayu Ding, Xinpeng Liu, Zhiyi Pan, Shiqiang Long, Ge Li|<http://arxiv.org/pdf/2509.22225v1>|提出无训练框架MUSplat，将2D理解提升至3D，解决多概念语义表示和视角不一致问题。|
|🆕 发布|Guidance Watermarking for Diffusion Models|扩散模型引导的水印技术|Enoal Gesny, Eva Giboulot, Teddy Furon, Vivien Chappelier|<http://arxiv.org/pdf/2509.22126v1>|引入了一种新的扩散模型水印方法，通过引导扩散过程增强水印的鲁棒性，无需重新训练或微调。|
|🆕 发布|High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling|通过视觉引导生成模型实现跨多样类别的高质量声音分离|Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu|<http://arxiv.org/pdf/2509.22063v1>|提出了一种基于生成模型的音频视觉分离框架，通过结合音频和视觉信息，实现了高质量的多类别声音分离。|
|🆕 发布|Comparative Analysis of GAN and Diffusion for MRI-to-CT translation|《GAN与扩散模型在MRI到CT转换中的比较分析》|Emily Honey, Anders Helbo, Jens Petersen|<http://arxiv.org/pdf/2509.22049v1>|比较了GAN和扩散模型在MRI到CT图像转换中的效果，发现多通道条件和cDDPM架构显著提升性能。|
|📝 更新|DVD-Quant: Data-free Video Diffusion Transformers Quantization|DVD-Quant：无需数据视频扩散变换器量化|Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Haotong Qin, Linghe Kong, Guihai Chen, Yulun Zhang .etc.|<http://arxiv.org/pdf/2505.18663v2>|[代码](https://github.com/lhxcs/DVD-Quant.); 提出DVD-Quant方法，实现了无需数据校准的视频生成模型量化，提升速度近2倍同时保持视频质量。|
|📝 更新|Deeper Diffusion Models Amplify Bias|深度扩散模型放大偏差|Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian|<http://arxiv.org/pdf/2505.17560v2>|揭示了深度扩散模型可能放大训练数据中的固有偏差并影响隐私。|
|📝 更新|video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models|视频-SALMONN 2：增强字幕的音频-视觉大型语言模型|Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang|<http://arxiv.org/pdf/2506.15220v3>|[代码](https://github.com/bytedance/video-SALMONN-2); 提出MrDPO方法，通过多轮优化和轻量级适配器提升视频描述和问答的详尽度和准确性，实现多个基准测试的...|
|🆕 发布|No-Reference Image Contrast Assessment with Customized EfficientNet-B0|无参考图像对比度评估与定制化EfficientNet-B0网络|Javad Hassannataj Joloudari, Bita Mesbahzadeh, Omid Zare, Emrah Arslan, Roohallah Alizadehsani, Hossein Moosaei|<http://arxiv.org/pdf/2509.21967v1>|提出了一种定制化EfficientNet-B0框架，实现了无参考图像对比度质量评估的突破性性能。|
|📝 更新|Calibrated Multi-Preference Optimization for Aligning Diffusion Models|校准多偏好优化以对齐扩散模型|Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin .etc.|<http://arxiv.org/pdf/2502.02588v3>|提出了一种无需人工标注数据的多偏好校准优化方法，有效提升文本到图像模型的性能。|
|📝 更新|Frequency-Domain Refinement with Multiscale Diffusion for Super Resolution|频率域细化结合多尺度扩散的超分辨率重建|Xingjian Wang, Li Chai, Jiming Chen|<http://arxiv.org/pdf/2405.10014v2>|提出了一种频率域引导的多尺度扩散模型，逐步补充高频细节，实现了更高质量的图像超分辨率。|
|🆕 发布|PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning|PANICL：缓解视觉情境学习中过度依赖单一提示的问题|Jiahao Zhang, Bowen Wang, Hong Liu, Yuta Nakashima, Hajime Nagahara|<http://arxiv.org/pdf/2509.21926v1>|提出了一种缓解视觉在位学习中单一提示过度依赖问题的方法PANICL，通过利用多个提示对模型进行平滑训...|
|🆕 发布|StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing|稳定Dub：驯化扩散先验以实现通用和高效视觉配音|Liyang Chen, Tianze Zhou, Xu He, Boshi Tang, Zhiyong Wu, Yang Huang, Yang Wu, Zhongqian Sun .etc.|<http://arxiv.org/pdf/2509.21887v1>|提出了一种结合唇习惯感知建模和遮挡鲁棒合成的视觉配音框架，有效解决了特定说话人唇部动作生成和遮挡问题...|
|📝 更新|SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion Models|SPEED：可扩展、精确、高效的概念擦除方法用于扩散模型|Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin Hao, Guojun Ma, Fuli Feng|<http://arxiv.org/pdf/2503.07392v2>|[代码](https://github.com/Ouxiang-Li/SPEED.); 提出了一种高效的概念擦除方法SPEED，通过直接编辑模型参数精确擦除目标概念，同时保留非目标概念的质...|
|📝 更新|Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models|不稳定的遗忘：扩散模型中概念复兴的潜在风险|Vinith M. Suriyakumar, Rohan Alur, Ayush Sekhari, Manish Raghavan, Ashia C. Wilson|<http://arxiv.org/pdf/2410.08074v3>|揭示了文本到图像扩散模型在增量更新过程中“遗忘”的概念可能重新出现的风险，提出了相关实验方法。|
|🆕 发布|DiTraj: training-free trajectory control for video diffusion transformer|DiTraj：无需训练的轨迹控制视频扩散变换器|Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su .etc.|<http://arxiv.org/pdf/2509.21839v1>|提出了一种无需训练的DiTraj框架，通过前景-背景分离指导和空间-时间解耦位置编码，实现了文本到视...|
|📝 更新|Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling|计划-R1：作为语言建模的安全可行轨迹规划|Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen|<http://arxiv.org/pdf/2505.17659v3>|[代码](https://github.com/XiaolongTang23/Plan-R1.); 提出Plan-R1框架，通过预训练和细调两阶段方法，提高自动驾驶轨迹规划的安全性和可行性。|
|🆕 发布|MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning|多图像推理与定位的强化学习方法（MIRG-RL）|Lihao Zheng, Jiawei Chen, Xintian Shen, Hao Ma, Tao Wei|<http://arxiv.org/pdf/2509.21788v1>|[代码](https://github.com/ZEUS2035/MIRG-RL.); 提出了一种结合监督微调和图像感知强化学习的两阶段训练框架，有效提升了多图像推理和定位能力。|
|📝 更新|DanceText: A Training-Free Layered Framework for Controllable Multilingual Text Transformation in Images|舞文：一种无需训练的可控多语种图像文本转换分层框架|Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Rizwan Qureshi, Shaina Raza, Aman Chadha, Yong Xiang .etc.|<http://arxiv.org/pdf/2504.14108v2>|[代码](https://github.com/YuZhenyuLindy/DanceText.git.); 提出了一种无需训练的分层框架DanceText，实现图像中多语言文本的可控编辑和复杂几何变换。|
|📝 更新|Can Diffusion Models Disentangle? A Theoretical Perspective|扩散模型能否解耦？一种理论视角|Liming Wang, Muhammad Jehanzeb Mirza, Yishu Gong, Yuan Gong, Jiaqi Zhang, Brian H. Tracey, Katerina Placek, Marco Vilela .etc.|<http://arxiv.org/pdf/2504.00220v2>|提出理解扩散模型学习解耦表征的理论框架，并通过实验验证了其有效性。|


### 三维内容生成 (3D Content Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing|“VoiceAssistant-Eval：跨听、说、观评估人工智能助手的基准测试”|Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li|<http://arxiv.org/pdf/2509.22651v1>|[代码](https://mathllm.github.io/VoiceAssistantEval); 提出了VoiceAssistant-Eval基准，全面评估AI助手在听、说、看方面的能力，揭示了性能...|
|🆕 发布|LongLive: Real-time Interactive Long Video Generation|《LongLive：实时交互式长视频生成》|Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie .etc.|<http://arxiv.org/pdf/2509.22622v1>|提出了一种高效的帧级自回归框架LongLive，实现实时交互式长视频生成，同时保持视频质量和效率。|
|📝 更新|NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation|NarrLV：面向长视频生成的全面叙事中心评估方法|X. Feng, H. Yu, M. Wu, S. Hu, J. Chen, C. Zhu, J. Wu, X. Chu .etc.|<http://arxiv.org/pdf/2507.11245v3>|提出NarrLV基准，首次全面评估长视频生成模型在叙事表达方面的能力。|
|🆕 发布|SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection|SpecXNet：一种用于鲁棒深度伪造检测的双域卷积网络|Inzamamul Alam, Md Tanvir Islam, Simon S. Woo|<http://arxiv.org/pdf/2509.22070v1>|[代码](https://github.com/inzamamulDU/SpecXNet); 提出了一种双域卷积网络SpecXNet，通过结合空间和频域特征，有效提高了深度伪造检测的准确性和泛化...|
|🆕 发布|Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors|揭示幻觉以抑制它们：基于生成锚点的视觉语言模型表征编辑|Youxu Shi, Suorong Yang, Dong Liu|<http://arxiv.org/pdf/2509.21997v1>|提出了一种无需训练的自我监督方法，通过生成锚点编辑模型状态以减少大型多模态语言模型中的幻觉现象。|
|📝 更新|VidBridge-R1: Bridging QA and Captioning for RL-based Video Understanding Models with Intermediate Proxy Tasks|VidBridge-R1：通过中间代理任务连接问答和字幕生成，以增强基于强化学习的视频理解模型|Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Weihong Lin, Zekun Wang, Bohan Zeng, Yang Shi, Sihan Yang .etc.|<http://arxiv.org/pdf/2506.09079v2>|提出 VidBridge-R1 模型，通过两个中间代理任务解决视频理解和生成任务间的矛盾，提升模型在...|
|🆕 发布|DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images|《DeHate：一种基于稳定扩散的多模态方法减轻图像中的仇恨言论》|Dwip Dalal, Gautam Vashishtha, Anku Ranui, Aishwarya Reganti, Parth Patwa, Mohd Sarique, Chandan Gupta, Keshav Nath .etc.|<http://arxiv.org/pdf/2509.21787v1>|提出了一种结合稳定扩散技术和数字注意力分析模块的多模态方法，有效识别并消除图像中的仇恨内容。|


### 条件式生成与编辑 (Conditional Generation & Editing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|UIP2P: Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint|无监督指令驱动的图像编辑：基于编辑可逆性约束的方法|Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari|<http://arxiv.org/pdf/2412.15216v2>|提出了一种无需真实编辑图像的监督学习方法，通过编辑可逆性约束实现了更广泛、高保真的图像编辑。|
|🆕 发布|Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs|通过多模态大型语言模型学习人类感知的AI生成视频伪造性|Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen .etc.|<http://arxiv.org/pdf/2509.22646v1>|提出了一种标注人类感知的伪造痕迹的基准，训练语言模型以识别和解释AI生成视频中的深伪痕迹。|
|🆕 发布|Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance|无需训练的合成数据生成：基于双重IP-适配器引导|Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour, Vicky Kalogeiton|<http://arxiv.org/pdf/2509.22635v1>|提出了一种无需训练的合成数据生成方法DIPSY，通过双图像提示和正负引导生成具有高度区分性的合成图像...|
|🆕 发布|SPARK: Synergistic Policy And Reward Co-Evolving Framework|SPARK：协同策略与奖励共演化框架|Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang|<http://arxiv.org/pdf/2509.22624v1>|提出了一种高效的协同策略与奖励共演化框架SPARK，通过回收利用数据训练模型自我评估和改进，无需额外...|
|🆕 发布|JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation|联合差异：在多智能体轨迹生成中桥接连续与离散|Guillem Capellera, Luis Ferraz, Antonio Rubio, Alexandre Alahi, Antonio Agudo|<http://arxiv.org/pdf/2509.22522v1>|提出了一种统一连续和离散事件生成的新框架，实现了多智能体轨迹和关键事件的同时建模。|
|🆕 发布|Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation|《探究多模态大型语言模型在自回归标记生成中的关注点与依赖性：解释自回归标记生成》|Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao|<http://arxiv.org/pdf/2509.22496v1>|[代码](https://github.com/RuoyuChen10/EAGLE.); 提出EAGLE框架，解释多模态大语言模型中自回归生成标记的依赖关系，提升了解释性和可靠性。|
|🆕 发布|Group Critical-token Policy Optimization for Autoregressive Image Generation|用于自回归图像生成的群组关键令牌策略优化|Guohui Zhang, Hu Yu, Xiaoxiao Ma, JingHao Zhang, Yaning Pan, Mingde Yao, Jie Xiao, Linjiang Huang .etc.|<http://arxiv.org/pdf/2509.22485v1>|提出了一种针对关键图像token进行优化的生成策略，通过识别并重点优化关键token，提升了自回归图...|
|🆕 发布|MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning|面向任务驱动的桌面场景生成：基于三维空间推理的方法|Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu .etc.|<http://arxiv.org/pdf/2509.22281v1>|提出任务驱动的桌面场景生成方法，通过3D空间推理生成符合任务需求的现实布局场景。|
|📝 更新|ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation|图表星系：一个用于信息图表理解和生成的数据集|Zhen Li, Duan Li, Yukai Guo, Xinyuan Guo, Bowen Li, Lanxi Xiao, Shenyu Qiao, Jiashu Chen .etc.|<http://arxiv.org/pdf/2505.18668v4>|提出了ChartGalaxy数据集，通过合成 infographic 图表，提升了大型视觉语言模型对...|
|🆕 发布|DragGANSpace: Latent Space Exploration and Control for GANs|DragGANSpace：生成对抗网络的潜在空间探索与控制|Kirsten Odendaal, Neela Kaushik, Spencer Halverson|<http://arxiv.org/pdf/2509.22169v1>|整合StyleGAN、DragGAN与PCA，提升生成图像的潜在空间效率和操控性。|
|🆕 发布|REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation|REFINE-CONTROL：一种用于条件图像生成的半监督蒸馏方法|Yicheng Jiang, Jin Yuan, Hua Yuan, Yao Zhang, Yong Rui|<http://arxiv.org/pdf/2509.22139v1>|提出了一种半监督蒸馏框架REFINE-CONTROL，降低计算成本和延迟，同时保持图像生成的高保真度...|
|📝 更新|Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning|推理以编辑：基于假设性指令的视觉推理图像编辑|Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang .etc.|<http://arxiv.org/pdf/2507.01908v2>|提出了一种结合大规模数据集和深度学习框架的方法，用于理解和执行复杂的假设性指令图像编辑。|
|📝 更新|DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation|动态控制：自适应条件选择以改进文本到图像生成|Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Yong Liu, Yabiao Wang .etc.|<http://arxiv.org/pdf/2412.03255v3>|提出动态控制框架DynamicControl，通过自适应选择条件组合，提升文本到图像生成的可控性、质...|
|📝 更新|Astraea: A Token-wise Acceleration Framework for Video Diffusion Transformers|阿斯特赖亚：一种针对视频扩散变换器的逐标记加速框架|Haosong Liu, Yuge Cheng, Wenxuan Miao, Zihan Liu, Aiyue Chen, Jing Lin, Yiwu Yao, Chen Chen .etc.|<http://arxiv.org/pdf/2506.05096v4>|提出 Astraea 框架，通过智能配置搜索和优化策略，显著加速视频生成速度并保持高质量。|
|📝 更新|FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis|FAST：面向分割的目标异常合成中的前景感知扩散与加速采样轨迹|Xichen Xu, Yanshu Wang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu|<http://arxiv.org/pdf/2509.20295v2>|[代码](https://github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.); 提出了一种前景感知的扩散框架FAST，通过加速采样轨迹和自适应调整噪声，高效生成面向分割的工业异常图...|
|🆕 发布|Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation|关注异常：用于检测主观驱动生成中不一致性的视觉对应方法|Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka|<http://arxiv.org/pdf/2509.21989v1>|[代码](https://abdo-eldesokey.github.io/mind-the-glitch); 提出了一种新方法，通过分离预训练扩散模型中的视觉和语义特征，有效检测并定位生成图像中的不一致性。|
|🆕 发布|MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning|多创作者：通过空间解耦注意力和身份感知强化学习实现高保真多主体生成|Tao Wu, Yibo Jiang, Yehao Lu, Zhizhong Wang, Zeyi Huang, Zequn Qin, Xi Li|<http://arxiv.org/pdf/2509.21953v1>|提出了一种通过空间解耦注意力和身份感知强化学习的高保真多主体生成框架，有效解决了属性泄露问题并提升了...|
|📝 更新|Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy|形状驱动的运动编辑：基于三维代理的精确与一致性视频编辑|Yuhao Liu, Tengfei Wang, Fang Liu, Zhenwei Wang, Rynson W. H. Lau|<http://arxiv.org/pdf/2506.22432v2>|提出Shape-for-Motion框架，通过3D代理实现精确且一致的视频编辑。|
|🆕 发布|SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet|语义控制：一种无需训练的处理ControlNet中松散对齐视觉条件的方法|Woosung Joung, Daewon Chae, Jinkyu Kim|<http://arxiv.org/pdf/2509.21938v1>|[代码](https://mung3477.github.io/semantic-control.); 提出了一种无需训练的SemanticControl方法，有效利用与文本提示不完全对齐的视觉条件生成图...|
|📝 更新|Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance|罕见到常见：在大型语言模型指导下解锁扩散模型在罕见概念上的组合生成能力|Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho|<http://arxiv.org/pdf/2410.22376v3>|[代码](https://github.com/krafton-ai/Rare-to-Frequent.); 提出了一种利用大型语言模型指导的扩散模型方法，有效提升了罕见概念组合的生成准确性。|
|🆕 发布|Taming Flow-based I2V Models for Creative Video Editing|驯服基于流的图像到视频模型以实现创意视频编辑|Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon Wetzstein, Maneesh Agrawala, Anyi Rao|<http://arxiv.org/pdf/2509.21917v1>|提出了一种无需逆过程的无损视频编辑方法，通过向量场修正和结构保持初始化，有效利用现成的图像到视频模型...|
|📝 更新|Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations|控制随机之字形探索的扩散采样：Ctrl-Z采样|Shunqi Mao, Wei Guo, Chaoyi Zhang, Jieting Long, Ke Xie, Weidong Cai|<http://arxiv.org/pdf/2506.20294v3>|提出了一种自适应的Ctrl-Z Sampling方法，通过控制随机曲折探索有效逃离局部最优解，显著提...|
|🆕 发布|Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers|同步phony：基于扩散变换器的同步音频到视频生成|Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh|<http://arxiv.org/pdf/2509.21893v1>|[代码](https://jibin86.github.io/syncphony_project_page); 提出了一种音频同步的视频生成方法，通过引入运动感知损失和音频同步引导，实现了高精度的时间同步和高质量...|
|🆕 发布|Drag4D: Align Your Motion with Text-Driven 3D Scene Generation|Drag4D：文本驱动的三维场景生成与动作对齐|Minjun Kang, Inkyu Shin, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon|<http://arxiv.org/pdf/2509.21888v1>|Drag4D通过结合文本驱动的3D场景生成与物体运动控制，实现了物体在高质量3D背景中的精确动态定位...|
|📝 更新|TempFlow-GRPO: When Timing Matters for GRPO in Flow Models|时间敏感的GRPO：在流模型中时间因素对GRPO的重要性|Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, Bo Zhang|<http://arxiv.org/pdf/2508.04324v3>|提出了一种针对生成模型的新型时序优化框架TempFlow-GRPO，通过精确的奖励分配和时间感知的探...|
|🆕 发布|TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation|TDEdit：一种用于文本拖动引导图像操作的统一扩散框架|Qihang Wang, Yaxiong Wang, Lechao Cheng, Zhun Zhong|<http://arxiv.org/pdf/2509.21905v1>|提出了统一扩散框架TDEdit，结合文本和拖拽交互进行图像编辑，实现高保真度和精确控制。|
|📝 更新|Self-Guidance: Boosting Flow and Diffusion Generation on Their Own|自引导：自主增强流和扩散生成|Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, Guo-Jun Qi|<http://arxiv.org/pdf/2412.05827v5>|[代码](https://github.com/maple-research-lab/Self-Guidance.); 提出Self-Guidance方法，无需额外训练即可提升文本到图像生成模型的质量。|
|📝 更新|RAAG: Ratio Aware Adaptive Guidance|比例感知自适应引导|Shangwen Zhu, Qianyu Peng, Yuting Hu, Zhantao Yang, Han Zhang, Zhao Pu, Andy Zheng, Zhilei Shu .etc.|<http://arxiv.org/pdf/2508.03442v2>|提出自适应引导策略，根据采样过程动态调整引导强度，提升生成模型的速度和质量。|
|🆕 发布|Deepfakes: we need to re-think the concept of "real" images|深度伪造：我们需要重新思考“真实”图像的概念|Janis Keuper, Margret Keuper|<http://arxiv.org/pdf/2509.21864v1>|重新定义“真实”图像的概念，强调构建新数据集以改进伪造图像检测方法。|
|📝 更新|Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection|海王星X：主动X到航海生成用于通用航海目标检测|Yu Guo, Shengfeng He, Yuxu Lu, Haonan An, Yihang Tao, Huilin Zhu, Jingxian Liu, Yuguang Fang|<http://arxiv.org/pdf/2509.20745v2>|[代码](https://github.com/gy65896/Neptune-X.); 提出Neptune-X框架，通过合成数据和任务感知样本选择提升海上目标检测性能。|
|📝 更新|SeamCrafter: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning|《SeamCrafter：通过强化学习增强艺术家UV展开的网格接缝生成》|Duoteng Xu, Yuguang Chen, Jing Li, Xinhai Liu, Xueqi Ma, Zhuo Chen, Dongyu Zhang, Chunchao Guo|<http://arxiv.org/pdf/2509.20725v2>|SeamCrafter通过强化学习优化网格缝合线生成，显著减少UV扭曲和碎片化，提升艺术家贴图效率。|
|📝 更新|SPATIALGEN: Layout-guided 3D Indoor Scene Generation|空间生成：基于布局引导的室内三维场景生成|Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou .etc.|<http://arxiv.org/pdf/2509.14981v3>|提出了SpatialGen模型，利用大规模数据集生成高质量、语义一致的3D室内场景。|
|📝 更新|LiT: Delving into a Simple Linear Diffusion Transformer for Image Generation|LiT：深入研究用于图像生成的简单线性扩散变换器|Jiahao Wang, Ning Kang, Lewei Yao, Mengzhao Chen, Chengyue Wu, Songyang Zhang, Shuchen Xue, Yong Liu .etc.|<http://arxiv.org/pdf/2501.12976v2>|提出了一种简化的线性扩散变换器LiT，通过优化注意力和训练策略，实现了高效图像生成。|
|🆕 发布|MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation|MoWM：基于潜在至像素特征调节的具身规划混合世界模型|Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li|<http://arxiv.org/pdf/2509.21797v1>|[代码](https://github.com/tsinghua-fib-lab/MoWM.); 提出混合世界模型框架MoWM，融合运动感知和像素级细节，提升机器人行动规划精度和泛化能力。|
|📝 更新|Intentional Gesture: Deliver Your Intentions with Gestures for Speech|有意手势：用手势传达您的意图以辅助言语|Pinxin Liu, Haiyang Liu, Luchuan Song, Jason J. Corso, Chenliang Xu|<http://arxiv.org/pdf/2505.15197v2>|[代码](https://andypinxinliu.github.io/Intentional-Gesture); 提出了一种基于意图理解的生成方法Intentional-Gesture，使手势与言语意图更匹配，提升...|


### 时空一致性生成 (Spatiotemporal Coherent Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding|通过潜在先验编码实现的退化感知一体化图像复原|S M A Sharif, Abdur Rehman, Fayaz Ali Dharejo, Radu Timofte, Rizwan Ali Naqvi|<http://arxiv.org/pdf/2509.17792v2>|提出了一种无需外部提示或预设架构的图像复原方法，通过自动推断退化感知表征，实现了更高效的图像复原效果...|
|🆕 发布|UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data|UniMapGen：一种从多模态数据生成大规模地图的框架|Yujian Yuan, Changjie Wu, Xinyuan Chang, Sijin Wang, Hang Zhang, Shiyi Liang, Shuang Zeng, Mu Xu|<http://arxiv.org/pdf/2509.22262v1>|提出了一种生成框架UniMapGen，通过多模态数据高效构建大规模地图，实现道路表示的连续性和平滑性...|
|🆕 发布|Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm|解决视线辅助视觉助手交互范式中的模糊性问题|Zeyu Wang, Baiyu Chen, Kun Yan, Hongjing Piao, Hao Xue, Flora D. Salim, Yuanchun Shi, Yuntao Wang|<http://arxiv.org/pdf/2509.21980v1>|提出方法GLARIFY利用时空注视信息解决智能眼镜多模态查询中的歧义问题，提升视觉语言模型效果。|
|📝 更新|VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation|VidCRAFT3：用于图像到视频生成的相机、物体和光照控制|Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu|<http://arxiv.org/pdf/2502.07531v4>|[代码](https://sixiaozheng.github.io/VidCRAFT3); VidCRAFT3通过集成3D重建、多尺度光流和并行注意力机制，实现了对图像转视频过程中相机、物体和...|
|📝 更新|MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning|MOSS-ChatV：基于过程推理奖励的强化学习在视频时间推理中的应用|Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan .etc.|<http://arxiv.org/pdf/2509.21113v2>|提出了一种基于动态时间扭曲的强化学习框架，通过过程奖励提升视频时序推理的一致性和稳定性。|
|🆕 发布|SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes|SRHand：通过视角/姿态感知的神经图像表示和显式3D网格超分辨率重建手部图像和3D形状|Minje Kim, Tae-Kyun Kim|<http://arxiv.org/pdf/2509.21859v1>|[代码](https://yunminjin2.github.io/projects); 提出了一种用于从低分辨率图像重建高细节3D手部几何和纹理的方法SRHand，结合隐式图像表示和显式手...|
|🆕 发布|LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE|《LongScape：通过上下文感知的MoE方法推进长时距具身世界模型的进展》|Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li|<http://arxiv.org/pdf/2509.21790v1>|[代码](https://github.com/tsinghua-fib-lab/Longscape.); 提出了一种结合扩散去噪和自回归生成的混合框架LongScape，通过动作引导的语义分块和上下文感知的...|
|📝 更新|PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval|PDV: 用于零样本组合图像检索的提示方向向量|Osman Tursun, Sinan Kalkan, Simon Denman, Clinton Fookes|<http://arxiv.org/pdf/2502.07215v3>|提出Prompt Directional Vector方法，动态优化文本和图像嵌入，提升零样本组合图...|
|🆕 发布|UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments|UISim：面向动态移动环境的交互式基于图像的用户界面模拟器|Jiannan Xiang, Yun Zhu, Lei Shu, Maria Wang, Lijun Yu, Gabriel Barcik, James Lyon, Srinivas Sunkara .etc.|<http://arxiv.org/pdf/2509.21733v1>|提出 UISim，一种基于图像的动态移动环境 UI 模拟器，通过预测和合成屏幕图像实现 UI 测试和...|


## 三维视觉与几何推理 (3D Vision & Geometric Reasoning)


### 神经辐射场表示 (Neural Radiance Field Representation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild|MS-GS：野外多外观稀疏视图三维高斯散点绘制|Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa, Cheng Peng|<http://arxiv.org/pdf/2509.15548v3>|提出MS-GS框架，利用3D Gaussian Splatting处理多外观稀疏视图场景，提高场景重...|
|🆕 发布|Category Discovery: An Open-World Perspective|《类别发现：开放世界视角》|Zhenqi He, Yuanpei Liu, Kai Han|<http://arxiv.org/pdf/2509.22542v1>|[代码](https://github.com/Visual-AI/Category-Discovery); 系统综述了类别发现领域的文献，提出了分类方法的分析框架，并指出了未来研究方向。|
|📝 更新|Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations|使用等变神经场表征进行几何感知的稳态偏微分方程推断|Giovanni Catalani, Michael Bauerheim, Frédéric Tost, Xavier Bertrand, Joseph Morlier|<http://arxiv.org/pdf/2504.18591v2>|提出了一种几何感知的神经场方法 enf2enf，用于预测具有几何变化的稳态PDE，实现了高效建模复杂...|
|📝 更新|LiteGS: A High-performance Framework to Train 3DGS in Subminutes via System and Algorithm Codesign|LiteGS：一种通过系统与算法共设计在数分钟内训练3DGS的高性能框架|Kaimin Liao, Hua Wang, Zhi Chen, Luchao Wang, Yaohua Tang|<http://arxiv.org/pdf/2503.01199v3>|提出了LiteGS框架，通过系统与算法协同优化，大幅缩短了3DGS训练时间并提升了模型质量。|


### 多视图几何重建 (Multi-view Geometric Reconstruction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition|GS-2M: 高斯散布用于联合网格重建与材质分解|Dinh Minh Nguyen, Malte Avenhaus, Thomas Lindemeier|<http://arxiv.org/pdf/2509.22276v1>|提出了一种基于3D高斯散点的统一网格重建与材质分解方法，有效处理高反射表面并无需依赖外部模型。|
|🆕 发布|Rigidity-Aware 3D Gaussian Deformation from a Single Image|从单张图像中考虑刚性的三维高斯形变|Jinhyeok Kim, Jaehun Bang, Seunghyun Seo, Kyungdon Joo|<http://arxiv.org/pdf/2509.22222v1>|提出了一种单张图像引导的3D高斯形变框架，通过稀疏视觉线索实现稳健的形变重建。|
|📝 更新|Pose-free 3D Gaussian splatting via shape-ray estimation|基于形状射线估计的无姿态三维高斯散点绘制|Youngju Na, Taeyeon Kim, Jumin Lee, Kyu Beom Han, Woo Jae Kim, Sung-eui Yoon|<http://arxiv.org/pdf/2505.22978v2>|提出了一种无需精确相机位姿的3D高斯渲染方法，通过联合估计形状和光线来减少位置误差导致的失真。|
|🆕 发布|Multi-View Crowd Counting With Self-Supervised Learning|多视角人群计数与自监督学习|Hong Mo, Xiong Zhang, Tengfei Shi, Zhongbo Wu|<http://arxiv.org/pdf/2509.21918v1>|提出了一种自监督学习框架SSLCounter，通过神经体积渲染减少对大规模标注数据的依赖，实现多视角...|
|📝 更新|SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment|SIU3R：超越特征对齐的同步场景理解与三维重建|Qi Xu, Dongxu Wei, Lingzhe Zhao, Wenpu Li, Zhangchi Huang, Shunping Ji, Peidong Liu|<http://arxiv.org/pdf/2507.02705v2>|[代码](https://insomniaaac.github.io/siu3r); 提出了首个无需特征对齐的通用场景理解和3D重建框架，通过像素对齐的3D表示和统一的学习查询集，实现了...|


## 时序视觉分析 (Temporal Visual Analysis)


### 时序建模与预测 (Temporal Modeling & Prediction)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data|基于仅RGB摄像头数据的人体姿态与情感信息的多线索人机交互意图预测：MINT-RVAE方法|Farida Mohsen, Ali Safa|<http://arxiv.org/pdf/2509.22573v1>|提出了一种仅使用RGB摄像头数据的框架级人类交互意图预测方法，通过合成序列生成和新型损失函数提升了模...|
|🆕 发布|UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective|《UrbanFeel：一种通过人类视角对城市场景进行时间和感知理解的全面基准》|Jun He, Yi Lin, Zilong Huang, Jiacong Yin, Junyan Ye, Yuchuan Zhou, Weijia Li, Xiang Zhang|<http://arxiv.org/pdf/2509.22228v1>|提出了UrbanFeel基准，评估多模态大语言模型在理解城市发展及主观环境感知方面的性能。|
|🆕 发布|DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining|《DeLiVR：用于高效视频去雨的差异时空李代数偏差》|Shuning Sun, Jialang Lu, Xiang Chen, Jichao Wang, Dianjie Lu, Guijuan Zhang, Guangwei Gao, Zhuoran Zheng|<http://arxiv.org/pdf/2509.21719v1>|提出了一种利用李群微分偏差的视频去雨方法，有效提升了视频去雨的效率和准确性。|


### 长时序视频理解 (Long-term Video Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Think With Videos For Agentic Long-Video Understanding|为代理型长视频理解思考的视频处理方法|Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, Zhicheng Dou|<http://arxiv.org/pdf/2506.10821v3>|[代码](https://github.com/yhy-2000/VideoDeepResearch); 提出了一种视频探索框架VideoExplorer，通过迭代提问和定位相关时刻，实现了高效、可解释的长...|
|📝 更新|TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video|TAPTRv3：空间与时间上下文促进长视频中任意点位的稳健跟踪|Jinyuan Qu, Hongyang Li, Shilong Liu, Tianhe Ren, Zhaoyang Zeng, Lei Zhang|<http://arxiv.org/pdf/2411.18671v2>|利用空间和时间上下文增强特征查询，TAPTRv3在长视频中实现了更稳健的点追踪性能。|
|🆕 发布|EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking|自我指导：一种面向面对面教学互动的第一视角视频数据集及多模态大型语言模型基准测试|Yuki Sakai, Ryosuke Furuta, Juichun Yen, Yoichi Sato|<http://arxiv.org/pdf/2509.22019v1>|提出首个面对面教学互动的egocentric视频数据集，并使用多模态大语言模型进行基准测试，显著提升...|
|📝 更新|4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming|4DGCPro：高效分层4D高斯压缩用于渐进式体视频流传输|Zihan Zheng, Zhenlong Wu, Houqiang Zhong, Yuan Tian, Ning Cao, Lan Xu, Jiangchao Yao, Xiaoyun Zhang .etc.|<http://arxiv.org/pdf/2509.17513v2>|[代码](https://mediax-sjtu.github.io/4DGCPro); 提出了4DGCPro，一种高效的分层4D高斯压缩框架，实现实时移动解码和高质量渲染的渐进式体视频流传...|


### 视频目标跟踪 (Video Object Tracking)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird's-Eye View|BEVTrack：面向鸟瞰图三维单目标跟踪的简洁而强大的基线方法|Yuxiang Yang, Yingqi Deng, Mian Pan, Jing Zhang, Zheng-Jun Zha|<http://arxiv.org/pdf/2309.02185v7>|[代码](https://github.com/xmm-prio/BEVTrack.); 提出了一种基于单回归损失的简单有效的BEV视角3D单目标跟踪方法，通过学习自适应概率函数提升了跟踪精...|


### 动作识别与理解 (Action Recognition & Understanding)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Prompt-guided Representation Disentanglement for Action Recognition|动作识别中的提示引导表示解耦|Tianci Wu, Guangming Zhu, Jiang Lu, Siyuan Wang, Ning Wang, Nuoye Xiong, Zhang Liang|<http://arxiv.org/pdf/2509.21783v1>|[代码](https://github.com/iamsnaping/ProDA.git); 提出ProDA框架，通过动态提示模块和图解析神经网络分离多动作场景中的特定动作。|


## 自监督与表征学习 (Self-supervised & Representation Learning)


### 对比学习方法 (Contrastive Learning Methods)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning|PSTTS：一种即插即用的标记选择器，用于高效的事件驱动时空表征学习|Xiangmo Zhao, Nan Yang, Yang Wang, Zhanwen Liu|<http://arxiv.org/pdf/2509.22481v1>|提出了一种无需额外参数的Progressive Spatio-Temporal Token Sele...|
|📝 更新|LEO-VL: Efficient Scene Representation for Scalable 3D Vision-Language Learning|LEO-VL：用于可扩展三维视觉-语言学习的有效场景表示|Jiangyong Huang, Xiaojian Ma, Xiongkun Linghu, Yue Fan, Junchao He, Wenxin Tan, Qing Li, Song-Chun Zhu .etc.|<http://arxiv.org/pdf/2506.09935v2>|提出高效场景表示方法，提升3D视觉语言模型的性能和可扩展性。|
|🆕 发布|Enriching Knowledge Distillation with Intra-Class Contrastive Learning|《利用类内对比学习丰富知识蒸馏》|Hua Yuan, Ning Xu, Xin Geng, Yong Rui|<http://arxiv.org/pdf/2509.22053v1>|引入类内对比损失以丰富软标签中的类内信息，增强学生模型的泛化能力。|


## 计算效率与模型优化 (Computational Efficiency & Model Optimization)


### 模型压缩与加速 (Model Compression & Acceleration)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation|“看见、指向、飞行：一种无需学习的通用无人机导航视觉语言模型框架”|Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen .etc.|<http://arxiv.org/pdf/2509.22653v1>|提出了一种无需训练的视觉语言模型框架，实现了通用无人机导航，通过将语言指令转化为图像上的二维路径点进...|
|📝 更新|Content-Aware Mamba for Learned Image Compression|内容感知曼巴：用于学习图像压缩的方法|Yunuo Chen, Zezheng Lyu, Bing He, Hongwei Hu, Qi Wang, Yuan Tian, Li Song, Wenjun Zhang .etc.|<http://arxiv.org/pdf/2508.02192v4>|提出内容感知Mamba模型，通过自适应图像内容优化处理，显著提升图像压缩效率。|
|🆕 发布|Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions|联合图熵知识蒸馏用于点云分类及对抗噪声的鲁棒性|Zhiqiang Tian, Weigang Li, Junwei Hu, Chunhua Deng|<http://arxiv.org/pdf/2509.22150v1>|提出了一种适用于非独立同分布3D点云数据的分类策略JGEKD，通过构建基于联合图熵的损失函数实现类间...|
|🆕 发布|DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints|双焦：基于空间-焦双变分约束的深度估计|Sungmin Woo, Sangyoun Lee|<http://arxiv.org/pdf/2509.21992v1>|提出了一种利用空间和焦距双变量约束的深度估计方法，有效应对复杂场景中的深度估计挑战。|
|📝 更新|Single-weight Model Editing for Post-hoc Spurious Correlation Neutralization|单权重模型编辑用于事后消除伪相关中性化|Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian|<http://arxiv.org/pdf/2501.14182v2>|提出了一种后处理方法，通过修改单个权重消除模型对错误特征的依赖，实现了对误导性关联的精确控制。|
|📝 更新|Re-Densification Meets Cross-Scale Propagation: Real-Time Neural Compression of LiDAR Point Clouds|激光雷达点云的实时神经压缩：重密集化与跨尺度传播相结合|Pengpeng Yu, Haoran Li, Runqing Jiang, Jing Wang, Liang Lin, Yulan Guo|<http://arxiv.org/pdf/2508.20466v2>|[代码](https://github.com/pengpeng-yu/FastPCC.); 提出了一种结合几何重密化和跨尺度特征传播的LiDAR点云实时压缩方法，实现了高效压缩比和实时性能。|
|📝 更新|STQE: Spatial-Temporal Attribute Quality Enhancement for G-PCC Compressed Dynamic Point Clouds|STQE：用于G-PCC压缩动态点云的空间时间属性质量增强|Tian Guo, Hui Yuan, Xiaolong Mao, Shiqi Jiang, Raouf Hamzaoui, Sam Kwong|<http://arxiv.org/pdf/2507.17522v2>|定位动态点云压缩质量增强难题，提出STQE网络，融合时空相关性提升视觉效果。|


### 神经架构优化 (Neural Architecture Optimization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks|SpikeMatch：基于尖峰神经网络时间动态的半监督学习|Jini Yang, Beomseok Oh, Seungryong Kim, Sunok Kim|<http://arxiv.org/pdf/2509.22581v1>|提出首个针对脉冲神经网络(SNN)的半监督学习框架SpikeMatch，利用SNN的时间动态特性生成...|
|🆕 发布|Color Names in Vision-Language Models|计算机视觉模型中的颜色名称|Alexandra Gomez-Villa, Pablo Hernández-Cámara, Muhammad Atif Butt, Valero Laparra, Jesus Malo, Javier Vazquez-Corral|<http://arxiv.org/pdf/2509.22524v1>|系统评估了视觉语言模型对颜色命名的准确性，揭示了模型在不同颜色样本上的性能差异及语言模型架构的影响。|
|🆕 发布|Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks|超越分类准确性：Neural-MedBench与深度推理基准的需求|Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang, Yuanyuan Peng, Huan Gao, Mingkun Xu .etc.|<http://arxiv.org/pdf/2509.22258v1>|提出了一种深度推理基准Neural-MedBench，用于评估多模态临床推理能力，并揭示了现有模型在...|
|📝 更新|Differential-Integral Neural Operator for Long-Term Turbulence Forecasting|差分-积分神经网络算子用于长期湍流预测|Hao Wu, Yuan Gao, Fan Xu, Fan Zhang, Qingsong Wen, Kun Wang, Xiaomeng Huang, Xian Wu|<http://arxiv.org/pdf/2509.21196v2>|提出了一种差分-积分神经操作符，有效解决了长期湍流预测中的误差累积和物理保真度问题。|
|📝 更新|A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data|一种全自动的颅内压分级框架：关键帧识别、视神经鞘直径测量与临床数据融合|Pengxu Wen, Tingting Yu, Ziwei Nie, Cheng Jiang, Zhenyu Yin, Mingyang He, Bo Liao, Xiaoping Yang|<http://arxiv.org/pdf/2509.09368v2>|提出了一种自动化的框架，通过结合关键帧识别、视神经鞘直径测量和临床数据，准确分级颅内压。|
|📝 更新|APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation|APTx神经元：一种集激活与计算于一体的统一可训练神经元架构|Ravin Kumar|<http://arxiv.org/pdf/2507.14270v4>|[代码](https://github.com/mr-ravin/aptx_neuron.); 提出了一种统一可训练的神经元架构APTx Neuron，整合了激活函数和线性变换，提升了表达能力和计...|
|🆕 发布|Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization|揭开美的本质：基于相对-绝对策略优化的高级审美推理|Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao, Tao Gui, Xuanjing Huang|<http://arxiv.org/pdf/2509.21871v1>|提出了一种结合强化学习的综合审美推理框架Aes-R1，通过生成结构化解释和优化评分排序，显著提升了图...|
|🆕 发布|Dynamic Novel View Synthesis in High Dynamic Range|高动态范围下的动态新视角合成|Kaixuan Zhang, Zhipeng Xiong, Minxian Li, Mingwu Ren, Jiankang Deng, Xiatian Zhu|<http://arxiv.org/pdf/2509.21853v1>|提出了一种处理动态高动态范围场景的新型方法HDR-4DGS，通过动态调整色调映射实现时空一致性的高保...|


### 资源受限视觉计算 (Resource-constrained Visual Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs|从偏差到平衡：探索和减轻LVLMs中的空间偏差|Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Weili Guan, Jun Yu, Min Zhang|<http://arxiv.org/pdf/2509.21984v1>|揭示了大型视觉语言模型中的空间偏见问题，并提出平衡位置分配方法以增强模型的空间鲁棒性。|
|🆕 发布|LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation|LFA-Net：一种用于视网膜血管分割的轻量级网络，具有LiteFusion注意力机制|Mehwish Mehmood, Ivor Spence, Muhammad Fahim|<http://arxiv.org/pdf/2509.21738v1>|[代码](https://github.com/Mehwish4593/LFA-Net.); 提出了一种轻量级网络LFA-Net，通过LiteFusion Attention模块有效提升视网膜血...|


## 鲁棒性与可靠性 (Robustness & Reliability)


### 分布外泛化 (Out-of-distribution Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Activation Function Design Sustains Plasticity in Continual Learning|"激活函数设计在持续学习中的塑性维持"|Lute Lillo, Nick Cheney|<http://arxiv.org/pdf/2509.22562v1>|通过精心设计的激活函数，有效维持了持续学习中的模型适应性。|
|📝 更新|Mobi-$π$: Mobilizing Your Robot Learning Policy|移动-$π$：使您的机器人学习策略动起来|Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika Antonova, Jeannette Bohg|<http://arxiv.org/pdf/2505.23692v2>|提出了一种策略动员方法，通过优化机器人基座姿态，使学习到的操纵策略适应新环境中的不同视角。|
|🆕 发布|FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing|FreqDebias：通过一致性驱动的频率去偏实现通用深度伪造检测|Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah|<http://arxiv.org/pdf/2509.22412v1>|提出FreqDebias框架，通过频率域去偏策略提升深伪检测模型跨域泛化能力。|


### 对抗鲁棒性 (Adversarial Robustness)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Text Adversarial Attacks with Dynamic Outputs|动态输出文本对抗攻击|Wenqiang Wang, Siyuan Liang, Xiao Yan, Xiaochun Cao|<http://arxiv.org/pdf/2509.22393v1>|提出了一种针对动态输出场景的文本对抗攻击方法，通过聚类技术将动态输出转化为静态输出，实现了高效攻击大...|
|📝 更新|FERD: Fairness-Enhanced Data-Free Robustness Distillation|FERD：公平性增强的无数据稳健性蒸馏|Zhengxiao Li, Liming Lu, Xu Zheng, Siyuan Liang, Zhenghan Chen, Yongbin Zhou, Shuchao Pang|<http://arxiv.org/pdf/2509.20793v2>|提出了一种公平性增强的无数据稳健性蒸馏框架，通过调整对抗样本的比例和分布，提高了模型的稳健性和公平性...|


### 不确定性量化 (Uncertainty Quantification)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Clinical Uncertainty Impacts Machine Learning Evaluations|《临床不确定性对机器学习评估的影响》|Simone Lionetti, Fabian Gröger, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Ludovic Amruthalingam, Alexander A. Navarini, Marc Pouly|<http://arxiv.org/pdf/2509.22242v1>|提出考虑标注不确定性的机器学习评估方法，以更准确反映临床数据性能。|


## 低资源与高效学习 (Low-resource & Efficient Learning)


### 主动学习策略 (Active Learning Strategies)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Multi-View Hypercomplex Learning for Breast Cancer Screening|多视角超复数学习在乳腺癌筛查中的应用|Eleonora Lopez, Eleonora Grassucci, Danilo Comminiello|<http://arxiv.org/pdf/2204.05798v4>|[代码](https://github.com/ispamm/PHBreast.); 提出了一种基于超复数神经网络的乳腺癌筛查方法，有效解决了多视角图像融合中的问题，提升了诊断准确性。|
|🆕 发布|Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning|学习绳索技巧，然后信任胜利：基于渐进探索的代理强化学习中的自我模仿|Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi .etc.|<http://arxiv.org/pdf/2509.22601v1>|提出了一种基于自我模仿和渐进探索的强化学习方法，平衡了探索与利用，提高了长时任务的学习稳定性。|
|📝 更新|Learning Personalized Driving Styles via Reinforcement Learning from Human Feedback|通过人类反馈的强化学习来学习个性化驾驶风格|Derun Li, Changye Li, Yue Wang, Jianwei Ren, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan .etc.|<http://arxiv.org/pdf/2503.10434v2>|提出了一种结合人类反馈的强化学习框架，实现个性化驾驶风格的自动调整和轨迹生成。|
|🆕 发布|Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data|自适应双模蒸馏与激励策略用于非独立同分布数据上的可扩展、异构联邦学习|Zahid Iqbal|<http://arxiv.org/pdf/2509.22507v1>|提出三阶段方法应对联邦学习中统计异质性和模型异质性挑战，显著提升全局模型准确性和通信效率。|
|🆕 发布|A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation|两位专家的故事：无需源域数据的无监督域自适应合作学习|Jiaping Yu, Muli Yang, Jiapeng Ji, Jiexi Yan, Cheng Deng|<http://arxiv.org/pdf/2509.22229v1>|提出了一种无源域自适应新框架，通过双专家协作学习，有效利用目标数据隐含结构，实现与现有方法相匹敌的性...|
|📝 更新|pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models|个性化联邦微调结合多模态适配器用于视觉-语言模型：pFedMMA|Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani|<http://arxiv.org/pdf/2507.05394v2>|提出了一种利用多模态适配器的个性化联邦学习框架pFedMMA，有效平衡了视觉语言模型的个性化和泛化能...|
|📝 更新|APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning|APTx：优于MISH、SWISH和ReLU变体的深度学习中使用的激活函数|Ravin Kumar|<http://arxiv.org/pdf/2209.06119v6>|[代码](https://github.com/mr-ravin/aptx_activation); 提出了一种新型激活函数APTx，比MISH、SWISH和ReLU变体更高效，降低计算需求以加速模型训...|
|🆕 发布|Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud|基于单视角部分点云的多视角增强的自监督点云补全|Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li|<http://arxiv.org/pdf/2509.22132v1>|提出了一种基于单视角部分点云多视角增强的自监督点云补全方法，有效克服了现有方法依赖完整数据或真实场景...|
|📝 更新|Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey|深度学习在显微镜图像增强中的近期进展：综述|Debasish Dutta, Neeharika Sonowal, Risheraj Barauh, Deepjyoti Chetia, Sanjib Kr Kalita|<http://arxiv.org/pdf/2509.15363v2>|概述了深度学习在显微镜图像增强领域的最新进展，聚焦超分辨率、重建和去噪技术的应用与挑战。|
|🆕 发布|Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning|填补先知差距：增量向量转换用于类增量学习|Zihuan Qiu, Yi Xu, Fanman Meng, Runtong Zhang, Linfeng Xu, Qingbo Wu, Hongliang Li|<http://arxiv.org/pdf/2509.21898v1>|提出了一种增量向量变换方法IVT，通过保持线性连通性减少灾难性遗忘，提升类增量学习性能。|


### 零/少样本泛化 (Zero/Few-shot Generalization)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|GPT-4 for Occlusion Order Recovery|用于遮挡顺序恢复的GPT-4|Kaziwa Saleh, Zhyar Rzgar K Rostam, Sándor Szénási, Zoltán Vámossy|<http://arxiv.org/pdf/2509.22383v1>|利用GPT-4模型预测物体遮挡顺序，无需标注数据即可提升图像理解能力。|
|🆕 发布|A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design|基于Transformer的问答模型全面评估与RAG增强设计|Zichen Zhang, Kunlong Zhang, Hongwei Ruan, Yiming Luo|<http://arxiv.org/pdf/2509.21845v1>|提出了一种混合检索策略，显著提升了多跳问答的准确性和效率。|


## 具身智能与交互视觉 (Embodied Intelligence & Interactive Vision)


### 视觉导航与路径规划 (Visual Navigation & Path Planning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Aerial Path Planning for Urban Geometry and Texture Co-Capture|城市几何与纹理协同捕捉的空中路径规划|Weidan Xiong, Bochuan Zeng, Ziyu Hu, Jianwei Guo, Ke Xie, Hui Huang|<http://arxiv.org/pdf/2509.22227v1>|提出了一种针对城市建筑纹理和几何共捕捉的创新空中路径规划框架，有效提升模型视觉质量。|


## 视觉-语言协同理解 (Vision-Language Joint Understanding)


### 跨模态检索与匹配 (Cross-modal Retrieval & Matching)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Hierarchical Representation Matching for CLIP-based Class-Incremental Learning|基于CLIP的类增量学习中的层次表示匹配|Zhen-Hao Wen, Yan Wang, Ji Feng, Han-Jia Ye, De-Chuan Zhan, Da-Wei Zhou|<http://arxiv.org/pdf/2509.22645v1>|提出了一种利用层次化表征匹配的CLIP基础上的类增量学习方法，通过递归生成文本描述符增强语义空间，有...|
|🆕 发布|Training-Free Multimodal Deepfake Detection via Graph Reasoning|无训练的多模态深度伪造检测方法：基于图推理|Yuxin Liu, Fei Wang, Kun Li, Yiqi Nie, Junjie Chen, Yanyan Wei, Zhangling Duan, Zhaohong Jia|<http://arxiv.org/pdf/2509.21774v1>|提出了一种无需训练的多模态深度伪造检测框架GASP-ICL，通过图推理增强大型视觉语言模型在捕捉伪造...|
|📝 更新|GLEAM: Learning to Match and Explain in Cross-View Geo-Localization|GLEAM：在跨视图地理定位中学习匹配与解释|Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang, Panwang Xia, Qiong Wu .etc.|<http://arxiv.org/pdf/2509.07450v2>|[代码](https://github.com/Lucky-Lance/GLEAM.); 提出GLEAM模型，整合多视角和模态数据，实现准确的跨视角地理定位并引入解释性推理。|


### 视觉问答与推理 (Visual Question Answering & Reasoning)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation|JanusVLN：使用双隐式记忆解耦语义与空间性的视觉-语言导航|Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu .etc.|<http://arxiv.org/pdf/2509.22548v1>|[代码](https://miv-xjtu.github.io/JanusVLN.github.io); 提出双隐式记忆框架JanusVLN，分离处理视觉空间和语义信息，提升视觉语言导航效率。|
|🆕 发布|CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process|电路感知：一种在工程设计过程中连接视觉理解和符号推理的层级电路系统基准|Arman Akbari, Jian Gao, Yifei Zou, Mei Yang, Jinru Duan, Dmitrii Torbunov, Yanzhi Wang, Yihui Ren .etc.|<http://arxiv.org/pdf/2509.22339v1>|提出CircuitSense基准，评估多模态大语言模型在工程设计中的视觉理解和符号推理能力。|
|🆕 发布|RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation|RoboView-Bias：机器人操作中基于载体的视觉偏见基准测试|Enguang Liu, Siyuan Liang, Liming Lu, Xiyu Zeng, Xiaochun Cao, Aishan Liu, Shuchao Pang|<http://arxiv.org/pdf/2509.22356v1>|提出RoboView-Bias，首个用于系统量化机器人操作中视觉偏见的基准，揭示了视觉因素对决策稳定...|
|📝 更新|Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities|重新思考视觉与语言导航中的具身差距：物理与视觉差异的全面研究|Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen .etc.|<http://arxiv.org/pdf/2507.13019v2>|[代码](https://crystalsixone.github.io/vln_pe.github.io); 引入物理真实的VLN-PE平台，揭示了当前方法在实体部署中的性能下降，为提高适应性和实用性提供了新途...|
|🆕 发布|CoFFT: Chain of Foresight-Focus Thought for Visual Language Models|“CoFFT：视觉语言模型的前瞻-聚焦思维链”|Xinyu Zhang, Yuxuan Dong, Lingling Zhang, Chengyou Jia, Zhuohang Dang, Basura Fernando, Jun Liu, Mike Zheng Shou|<http://arxiv.org/pdf/2509.22010v1>|提出了一种模拟人类视觉认知的Chain of Foresight-Focus Thought方法，有...|
|🆕 发布|ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models|ERGO：面向视觉语言模型的高效高分辨率视觉理解|Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim|<http://arxiv.org/pdf/2509.21991v1>|[代码](https://github.com/nota-github/ERGO.); 提出了一种高效的“粗到细”视觉理解框架ERGO，通过先分析降采样图像确定相关区域，再在原图上对这些区...|
|🆕 发布|Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models|医学视觉语言模型中的基准测试与心理谄媚缓解|Zikun Guo, Xinyue Xu, Pei Xiang, Shu Yang, Xin Han, Di Wang, Lijie Hu|<http://arxiv.org/pdf/2509.21979v1>|提出了一种评估和减轻医学视觉问答模型中奉承行为的基准和数据集，以及一种提高基于证据回答的轻量级策略V...|
|🆕 发布|Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach|为多模态语言模型定制视觉情感评估：一种开放词汇、多方面、可扩展的方法|Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou|<http://arxiv.org/pdf/2509.21950v1>|[代码](https://github.com/wdqqdw/MVEI.); 提出了一种面向多模态大语言模型的视觉情感评估方法，通过构建情感陈述判断任务和自动化管道，提高了评估的...|
|🆕 发布|DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation|动态导航：动态特征与层选择以实现高效视觉导航|Jiahui Wang, Changhao Chen|<http://arxiv.org/pdf/2509.21930v1>|提出DynaNav框架，通过动态选择特征和层级适应场景复杂度，有效降低计算负担并提升视觉导航性能。|
|🆕 发布|Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow|视觉多智能体系统：通过视觉流减轻幻觉雪崩效应|Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen, Zhucun Xue, Jiangning Zhang, Yue Liao .etc.|<http://arxiv.org/pdf/2509.21789v1>|[代码](https://github.com/YU-deep/ViF.git.); 提出方法ViF减轻多智能体系统中视觉幻觉累积问题，通过视觉流传递信息并优化注意力分配。|


### 视觉内容描述 (Visual Content Description)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition|GLip：一种全局-局部集成渐进框架，用于鲁棒视觉语音识别|Tianyue Wang, Shuang Yang, Shiguang Shan, Xilin Chen|<http://arxiv.org/pdf/2509.16031v2>|提出了一种全局-局部融合的渐进式框架GLip，有效应对视觉挑战，提升唇语识别的鲁棒性。|


## 领域特定视觉应用 (Domain-specific Visual Applications)


### 遥感与地理信息 (Remote Sensing & Geospatial Information)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|OS-W2S: An Automatic Labeling Engine for Language-Guided Open-Set Aerial Object Detection|OS-W2S：一种面向语言指导的开集航空目标检测自动标注引擎|Guoting Wei, Yu Liu, Xia Yuan, Xizhe Xue, Linlin Guo, Yifan Yang, Chunxia Zhao, Zongwen Bai .etc.|<http://arxiv.org/pdf/2505.03334v2>|[代码](https://github.com/GT-Wei/MI-OAD.); 提出了一种自动标注引擎OS-W2S，通过大规模数据集MI-OAD提升了语言指导的开集航空目标检测性能...|
|🆕 发布|Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models|面向遥感中的忠实推理：为视觉语言模型构建基于感知的地理空间思维链|Jiaqi Liu, Lang Sun, Ronghao Fu, Bo Yang|<http://arxiv.org/pdf/2509.22221v1>|提出了一种感知接地地理空间链式思维框架，通过分阶段训练显著提升了遥感任务中的推理准确性。|
|🆕 发布|Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning|Geo-R1：利用强化微调提升少量样本地理空间指代表达式理解|Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li, Yuxiang Cai, Zhonggen Su, Zhaojun Liu .etc.|<http://arxiv.org/pdf/2509.21976v1>|提出Geo-R1方法，通过强化学习微调提升少量样本地理参照表达理解能力，增强泛化性和可解释性。|
|🆕 发布|Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning|在恶劣天气条件下利用对比学习增强车辆检测|Boying Li, Chang Liu, Petter Kyösti, Mattias Öhman, Devashish Singha Roy, Sofia Plazzi, Hamam Mokayed, Olle Hagner|<http://arxiv.org/pdf/2509.21916v1>|提出了一种利用无标注数据通过对比学习增强车辆检测的框架，显著提升了恶劣天气下的车辆检测性能。|
|🆕 发布|LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation|LG-CD：通过SAM2适应增强语言引导的变化检测|Yixiao Liu, Yizhou Yang, Jinwen Li, Jun Tao, Ruoyu Li, Xiangkun Wang, Min Zhu, Junlong Cheng|<http://arxiv.org/pdf/2509.21894v1>|提出了一种融合文本和视觉信息的语言引导变化检测模型LG-CD，显著提升了遥感变化检测的准确性和鲁棒性...|
|📝 更新|Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy|深度学习在甲烷卫星和机载成像光谱学中用于云和云影分割|Manuel Perez-Carrasco, Maya Nasr, Sebastien Roche, Chris Chan Miller, Zhan Zhang, Core Francisco Park, Eleanor Walker, Cecilia Garraffo .etc.|<http://arxiv.org/pdf/2509.19665v2>|利用深度学习模型显著提升了甲烷遥感图像中云和云影的检测精度与细节捕捉能力。|


### 医学影像分析 (Medical Image Analysis)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation|贝塞尔遇见扩散：跨域稳健生成用于医学图像分割的方法|Chen Li, Meilong Xu, Xiaoling Hu, Weimin Lyu, Chao Chen|<http://arxiv.org/pdf/2509.22476v1>|提出了一种结合贝塞尔曲线和条件扩散模型的方法，有效减少医学图像领域差异，提升跨领域图像分割性能。|
|🆕 发布|U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation|U-MAN：基于多尺度自适应KAN网络的U-Net用于医学图像分割|Bohan Huang, Qianyun Bao, Haoyuan Ma|<http://arxiv.org/pdf/2509.22444v1>|提出U-MAN网络，通过多尺度特征提取和注意力引导融合，提升医学图像分割的精细度和准确性。|
|🆕 发布|Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss|基于深度学习的跨解剖结构CT合成：使用解剖特征优先损失的自适应nnResU-Net方法|Javier Sequeiro González, Arthur Longuefosse, Miguel Díaz Benito, Álvaro García Martín, Fabien Baldacci|<http://arxiv.org/pdf/2509.22394v1>|提出了一种基于深度学习的跨解剖结构CT图像合成方法，通过引入解剖特征优先损失函数，提高了临床相关结构...|
|📝 更新|CARE: Confidence-aware Ratio Estimation for Medical Biomarkers|CARE: 医学生物标志物的置信度感知比例估计|Jiameng Li, Teodora Popordanoska, Aleksei Tiulpin, Sebastian G. Gruber, Frederik Maes, Matthew B. Blaschko|<http://arxiv.org/pdf/2505.19585v2>|提出了一种用于估计医学生物标志物的置信度感知比率估计框架，为临床决策提供了不确定性度量。|
|🆕 发布|RAU: Reference-based Anatomical Understanding with Vision Language Models|基于视觉语言模型的参照式解剖理解RAU|Yiwei Li, Yikang Liu, Jiaqi Guo, Lin Zhao, Zheyuan Zhang, Xiao Chen, Boris Mailhe, Ankush Mukherjee .etc.|<http://arxiv.org/pdf/2509.22404v1>|提出RAU框架，利用视觉语言模型通过参考图像实现医学影像中解剖结构的识别、定位和分割。|
|🆕 发布|Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks|将背景知识融入医学语义分割的逻辑张量网络中|Luca Bergamin, Giovanna Maria Dimitri, Fabio Aiolli|<http://arxiv.org/pdf/2509.22399v1>|将医学常识融入损失函数，使用逻辑张量网络提升医学图像分割性能。|
|🆕 发布|Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation|"基于Johnson-Lindenstrauss引理的指导网络用于高效三维医学分割"|Jinpeng Lu, Linghan Cai, Yinda Chen, Guo Tang, Songhan Jiang, Haoyuan Shi, Zhiwei Xiong|<http://arxiv.org/pdf/2509.22307v1>|[代码](https://github.com/JinPLu/VeloxSeg.); 提出VeloxSeg方法，利用双流CNN-Transformer架构和Johnson-Lindens...|
|🆕 发布|COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics|compass:用于医学分割指标的鲁棒特征符合预测|Matt Y. Cheung, Ashok Veeraraghavan, Guha Balakrishnan|<http://arxiv.org/pdf/2509.22240v1>|提出了一种名为COMPASS的框架，通过利用深度学习模型内部特征，为医学图像分割的度量指标提供高效、...|
|📝 更新|CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis|CARL：面向光谱图像分析的无相机依赖性表征学习|Alexander Baumann, Leonardo Ayala, Silvia Seidlitz, Jan Sellner, Alexander Studier-Fischer, Berkin Özdemir, Lena Maier-Hein, Slobodan Ilic|<http://arxiv.org/pdf/2504.19223v3>|提出了CARL模型，通过自我监督策略学习相机无关的表示，增强了跨相机光谱异质性的鲁棒性。|
|📝 更新|Diverse Subset Selection via Norm-Based Sampling and Orthogonality|基于范数采样和正交性的多样化子集选择|Noga Bar, Raja Giryes|<http://arxiv.org/pdf/2406.01086v2>|提出了一种结合特征范数、随机化和正交化的子集选择方法，有效提高了标注数据的多样性和信息量。|
|📝 更新|HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy|"HiPerformer：一种具有模块化层次融合策略的高性能全局-局部分割模型"|Dayu Tan, Zhenpeng Xu, Yansen Su, Xin Peng, Chunhou Zheng, Weimin Zhong|<http://arxiv.org/pdf/2509.20280v2>|[代码](https://github.com/xzphappy/HiPerformer.); HiPerformer通过模块化层级架构和局部-全局特征融合策略，有效整合局部细节和全局上下文信息，...|
|📝 更新|STHN: Deep Homography Estimation for UAV Thermal Geo-localization with Satellite Imagery|Imagery 卫星影像下的无人机热成像地理定位深度单应性估计：STHN|Jiuhong Xiao, Ning Zhang, Daniel Tortei, Giuseppe Loianno|<http://arxiv.org/pdf/2405.20470v3>|提出了一种 coarse-to-fine 深度同构估计方法，显著提升了无人机热成像在低能见度条件下的...|
|🆕 发布|KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields|KG-SAM：通过条件随机场将解剖知识注入Segment Anything模型中|Yu Li, Da Chang, Xi Xiao|<http://arxiv.org/pdf/2509.21750v1>|KG-SAM通过融合医学知识图谱和条件随机场，提升了医学图像分割的准确性和可靠性。|


### 工业视觉检测 (Industrial Visual Inspection)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|DriveAgent-R1: Advancing VLM-based Autonomous Driving with Active Perception and Hybrid Thinking|驱动代理-R1：通过主动感知与混合思维提升基于视觉语言模型的自驾技术|Weicheng Zheng, Xiaofei Mao, Nanfei Ye, Pengxiang Li, Kun Zhan, Xianpeng Lang, Hang Zhao|<http://arxiv.org/pdf/2507.20879v2>|提出DriveAgent-R1，首个具备主动感知能力的自动驾驶代理，结合文本推理与视觉推理，提升决策...|


### 创意媒体生成 (Creative Media Generation)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors|《RelMap：利用类感知空间关系和语义先验增强在线地图构建》|Tianhui Cai, Yun Zhang, Zewei Zhou, Zhiyu Huang, Jiaqi Ma|<http://arxiv.org/pdf/2507.21567v2>|提出了一种融合位置关系和语义先验的在线高精度地图构建框架，提升了自动驾驶系统的地图构建准确性和泛化能...|


## 新兴理论与跨学科方向 (Emerging Theory & Interdisciplinary Directions)


### 视觉认知计算 (Visual Cognitive Computing)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|Unforgettable Lessons from Forgettable Images: Intra-Class Memorability Matters in Computer Vision|难忘的经验来自易忘的图像：计算机视觉中类内记忆性至关重要|Jie Jing, Yongjian Huang, Serena J. -W. Wang, Shuangpeng Han, Lucia Schiatti, Yen-Ling Kuo, Qing Lin, Mengmi Zhang|<http://arxiv.org/pdf/2412.20761v5>|提出 intra-class memorability 概念及量化方法，揭示了图像内部记忆差异并应用...|


### 量子视觉算法 (Quantum Visual Algorithms)

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|📝 更新|GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder|GeoDANO：基于域无关视觉编码器的几何大模型|Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim|<http://arxiv.org/pdf/2502.11360v2>|[代码](https://github.com/ml-postech/GeoDANO.); 提出GeoDANO模型，通过领域无关视觉编码器识别几何特征，解决平面几何问题。|


## 其他 (Others)


### 未分类

|状态|英文标题|中文标题|作者|PDF链接|代码/贡献|
|---|---|---|---|---|---|
|🆕 发布|CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones|《CubistMerge：用于多样化Vision Transformer骨架的空间保持性标记合并方法》|Wenyi Gong, Mieszko Lis|<http://arxiv.org/pdf/2509.21764v1>|提出了一种保持空间结构的简单有效的token合并方法，兼容空间架构并提升视觉任务性能。|
|📝 更新|Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying|用于点级开放词汇查询的语义一致性语言高斯散点绘制|Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh|<http://arxiv.org/pdf/2503.21767v2>|提出了一种语义一致的语言高斯散点追踪框架，有效提升了三维场景理解的点级查询性能。|

