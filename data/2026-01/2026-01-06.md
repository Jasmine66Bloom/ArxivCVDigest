## [UPDATED!] **2026-01-06** (Update Time)


## è§†è§‰è¡¨å¾ä¸åŸºç¡€æ¨¡å‹ (Visual Representation & Foundation Models)


### è§†è§‰Transformeræ¶æ„ (Vision Transformer Architectures)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|LTX-2: Efficient Joint Audio-Visual Foundation Model|LTX-2ï¼šé«˜æ•ˆçš„è”åˆè§†å¬åŸºç¡€æ¨¡å‹|Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz .etc.|<https://arxiv.org/pdf/2601.03233v1>|æ— |
|ğŸ†• å‘å¸ƒ|On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning|è®º Transformer å›¾åƒåµŒå…¥åœ¨ä¸å¯è§£ç©ºé—´æ¨ç†ä¸­çš„å†…åœ¨æé™|Siyi Lyu, Quan Liu, Feng Yan|<https://arxiv.org/pdf/2601.03048v1>|æ— |
|ğŸ†• å‘å¸ƒ|Lesion Segmentation in FDG-PET/CT Using Swin Transformer U-Net 3D: A Robust Deep Learning Framework|åŸºäº Swin Transformer U-Net 3D çš„ FDG-PET/CT ç—…ç¶åˆ†å‰²ï¼šä¸€ç§é²æ£’çš„æ·±åº¦å­¦ä¹ æ¡†æ¶|Shovini Guha, Dwaipayan Nandi|<https://arxiv.org/pdf/2601.02864v1>|æ— |
|ğŸ†• å‘å¸ƒ|StableDPT: Temporal Stable Monocular Video Depth Estimation|StableDPT: æ—¶é—´ç¨³å®šçš„å•ç›®è§†é¢‘æ·±åº¦ä¼°è®¡|Ivan Sobko, Hayko Riemenschneider, Markus Gross, Christopher Schroers|<https://arxiv.org/pdf/2601.02793v1>|æ— |


### å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ (Large-scale Pretrained Models)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays|åŸºäº Synthetic Chest X-Rays çš„å† çŠ¶åŠ¨è„‰é’™åŒ–æœºå™¨å­¦ä¹ æ£€æµ‹|Dylan Saeed, Ramtin Gharleghi, Susann Beier, Sonit Singh|<https://arxiv.org/pdf/2511.11093v2>|æ— |
|ğŸ†• å‘å¸ƒ|AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation|AnatomiXï¼Œä¸€ä¸ªç”¨äºèƒ¸éƒ¨Xå…‰è§£è¯»çš„è§£å‰–æ„ŸçŸ¥åŸºç¡€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹|Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert|<https://arxiv.org/pdf/2601.03191v1>|[ä»£ç ](https://github.com/aneesurhashmi/anatomix)|
|ğŸ†• å‘å¸ƒ|UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision|UniCorn: é€šè¿‡è‡ªç”Ÿæˆç›‘ç£è¿ˆå‘è‡ªæˆ‘æ”¹è¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹|Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen .etc.|<https://arxiv.org/pdf/2601.03193v1>|æ— |
|ğŸ†• å‘å¸ƒ|Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey|é¢å‘æ— çº¿ç½‘ç»œé¢„æµ‹ä¸æ§åˆ¶çš„å¤šæ¨¡æ€æ•°æ®å¢å¼ºåŸºç¡€æ¨¡å‹ï¼šç»¼è¿°|Han Zhang, Mohammad Farzanullah, Mohammad Ghassemi, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci|<https://arxiv.org/pdf/2601.03181v1>|æ— |
|ğŸ†• å‘å¸ƒ|LesionTABE: Equitable AI for Skin Lesion Detection|LesionTABE: ç”¨äºçš®è‚¤ç—…å˜æ£€æµ‹çš„å…¬å¹³ AI|Rocio Mexia Diaz, Yasmin Greenway, Petru Manescu|<https://arxiv.org/pdf/2601.03090v1>|æ— |
|ğŸ†• å‘å¸ƒ|ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation|ULS+: æ•°æ®é©±åŠ¨çš„æ¨¡å‹è‡ªé€‚åº”å¢å¼ºç—…ç¶åˆ†å‰²|Rianne Weber, Niels Rocholl, Max de Grauw, Mathias Prokop, Ewoud Smit, Alessa Hering|<https://arxiv.org/pdf/2601.02988v1>|æ— |
|ğŸ“ æ›´æ–°|MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection|MemeMind: ä¸€ä¸ªç”¨äºæœ‰å®³Memeæ£€æµ‹çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«Chain-of-Thoughtæ¨ç†|Hexiang Gu, Qifan Yu, Yuan Liu, Zikang Li, Saihui Hou, Jian Zhao, Zhaofeng He|<https://arxiv.org/pdf/2506.18919v3>|æ— |
|ğŸ“ æ›´æ–°|HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion|HAPNet: é€šè¿‡æ··åˆã€éå¯¹ç§°å’Œæ¸è¿›å¼å¼‚æ„ç‰¹å¾èåˆå®ç°å“è¶Šçš„RGB-Thermalåœºæ™¯è§£æ|Jiahang Li, Peng Yun, Yang Xu, Ye Zhang, Mingjian Sun, Qijun Chen, Ilin Alexander, Rui Fan|<https://arxiv.org/pdf/2404.03527v3>|æ— |


### å¤šæ¨¡æ€è¡¨å¾å­¦ä¹  (Multimodal Representation Learning)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs|Text-Guided å±‚èåˆç¼“è§£å¤šæ¨¡æ€ LLM ä¸­çš„å¹»è§‰|Chenchen Lin, Sanbao Su, Rachel Luo, Yuxiao Chen, Yan Wang, Marco Pavone, Fei Miao|<https://arxiv.org/pdf/2601.03100v1>|æ— |
|ğŸ†• å‘å¸ƒ|SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models|SketchThinker-R1ï¼šè¿ˆå‘ Large Multimodal Models ä¸­çš„é«˜æ•ˆ Sketch-Style Reasoning|Ruiyang Zhang, Dongzhan Zhou, Zhedong Zheng|<https://arxiv.org/pdf/2601.02825v1>|æ— |
|ğŸ“ æ›´æ–°|FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications|FCMBench: é¢å‘çœŸå®ä¸–ç•Œåº”ç”¨çš„é‡‘èä¿¡ç”¨å¤šæ¨¡æ€ç»¼åˆåŸºå‡†|Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang .etc.|<https://arxiv.org/pdf/2601.00150v2>|æ— |
|ğŸ“ æ›´æ–°|MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations|MIRAGEï¼šå†œä¸šä¸“å®¶å¼•å¯¼å¯¹è¯ä¸­å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ä¸æ¨ç†çš„åŸºå‡†|Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-TÃ¼r, Vikram S. Adve|<https://arxiv.org/pdf/2506.20100v2>|æ— |


## è§†è§‰è¯†åˆ«ä¸ç†è§£ (Visual Recognition & Understanding)


### ç›®æ ‡æ£€æµ‹ä¸å®šä½ (Object Detection & Localization)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|D^3ETOR: Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing for Weakly-Supervised Camouflaged Object Detection with Scribble Annotations|D^3ETOR: åŸºäºScribble Annotationsçš„å¼±ç›‘ç£ä¼ªè£…ç›®æ ‡æ£€æµ‹çš„è¾©è®ºå¢å¼ºä¼ªæ ‡ç­¾ä¸é¢‘ç‡æ„ŸçŸ¥æ¸è¿›å»å|Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras|<https://arxiv.org/pdf/2512.20260v2>|æ— |
|ğŸ†• å‘å¸ƒ|Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection|æ‰“ç ´Self-Attentionå¤±æ•ˆï¼šé‡æ–°æ€è€ƒInfrared Small Target Detectionçš„Query Initialization|Yuteng Liu, Duanni Meng, Maoxun Yuan, Xingxing Wei|<https://arxiv.org/pdf/2601.02837v1>|æ— |
|ğŸ†• å‘å¸ƒ|DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection|DGA-Netï¼šåˆ©ç”¨æ·±åº¦æç¤ºå’Œå›¾é”šå¼•å¯¼å¢å¼ºSAMä»¥å®ç°ä¼ªè£…ç›®æ ‡æ£€æµ‹|Yuetong Li, Qing Zhang, Yilin Zhao, Gongyang Li, Zeming Liu|<https://arxiv.org/pdf/2601.02831v1>|æ— |
|ğŸ“ æ›´æ–°|Benchmarking CNN and Transformer-Based Object Detectors for UAV Solar Panel Inspection|åŸºäºCNNå’ŒTransformerçš„ç›®æ ‡æ£€æµ‹å™¨åœ¨UAVå¤ªé˜³èƒ½ç”µæ± æ¿æ£€æµ‹ä¸­çš„åŸºå‡†æµ‹è¯•|Ashen Rodrigo, Isuru Munasinghe, Pubudu Sanjeewani, Asanka Perera|<https://arxiv.org/pdf/2509.05348v2>|æ— |
|ğŸ“ æ›´æ–°|Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks|ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè‚ºç™Œæ£€æµ‹çš„å¯è§£é‡ŠAIæŠ€æœ¯|Nishan Rai, Sujan Khatri, Devendra Risal|<https://arxiv.org/pdf/2508.10196v3>|æ— |


### å…³é”®ç‚¹å®šä½ä¸å§¿æ€ä¼°è®¡ (Keypoint Detection & Pose Estimation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images|[ç¿»è¯‘å¤±è´¥] LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images|MatÄ›j PekÃ¡r, VÃ­t Musil, Rudolf Nenutil, Petr Holub, TomÃ¡Å¡ BrÃ¡zdil|<https://arxiv.org/pdf/2601.03163v1>|[ä»£ç ](https://github.com/RationAI/lsp-detr.)|
|ğŸ†• å‘å¸ƒ|Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion|å…·æœ‰åŠ¨æ€æ¨¡ç³Šç‰¹å¾èåˆçš„è¿åŠ¨æ¨¡ç³Šé²æ£’å°éº¦è™«å®³æ£€æµ‹|Han Zhang, Yanwei Wang, Fang Li, Hongjun Wang|<https://arxiv.org/pdf/2601.03046v1>|æ— |
|ğŸ“ æ›´æ–°|Robust Egoistic Rigid Body Localization|é²æ£’çš„è‡ªåˆšä½“å®šä½|Niclas FÃ¼hrling, Giuseppe Thadeu Freitas de Abreu, David GonzÃ¡lez G., Osvaldo Gonsa|<https://arxiv.org/pdf/2501.10219v2>|æ— |
|ğŸ†• å‘å¸ƒ|HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection|HybridSolarNet: ä¸€ç§ç”¨äºå®æ—¶å¤ªé˜³èƒ½ç”µæ± æ¿æ•…éšœæ£€æµ‹çš„è½»é‡çº§ä¸”å¯è§£é‡Šçš„ EfficientNet-CBAM æ¶æ„|Md. Asif Hossain, G M Mota-Tahrin Tayef, Nabil Subhan|<https://arxiv.org/pdf/2601.02928v1>|æ— |
|ğŸ“ æ›´æ–°|RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images|RSwinV2-MD: ä¸€ç§ç”¨äºä»çš®è‚¤å›¾åƒä¸­æ£€æµ‹ Monkeypox çš„å¢å¼ºå‹ Residual SwinV2 Transformer|Rashid Iqbal, Saddam Hussain Khan|<https://arxiv.org/pdf/2601.01835v2>|æ— |


### è¯­ä¹‰/å®ä¾‹åˆ†å‰² (Semantic/Instance Segmentation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types|TEyeDï¼šè¶…è¿‡2000ä¸‡å¼ çœŸå®ä¸–ç•Œçœ¼éƒ¨å›¾åƒï¼ŒåŒ…å«Pupilã€Eyelidå’ŒIrisçš„2Dä¸3Dåˆ†å‰²ã€2Dä¸3Då…³é”®ç‚¹ã€3Dçœ¼çƒã€Gaze Vectorå’ŒEye Movement Types|Wolfgang Fuhl, Gjergji Kasneci, Enkelejda Kasneci|<https://arxiv.org/pdf/2102.02115v5>|æ— |


## ç”Ÿæˆå¼è§†è§‰æ¨¡å‹ (Generative Visual Modeling)


### æ¡ä»¶å¼ç”Ÿæˆä¸ç¼–è¾‘ (Conditional Generation & Editing)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training|Muses: æ— éœ€è®­ç»ƒè®¾è®¡ã€ç»„åˆã€ç”Ÿæˆä¸å­˜åœ¨çš„å¥‡å¹»3Dç”Ÿç‰©|Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai, Jian Yang, Zhenyu Zhang|<https://arxiv.org/pdf/2601.03256v1>|[ä»£ç ](https://luhexiao.github.io/Muses.github.io)|
|ğŸ“ æ›´æ–°|VisRet: Visualization Improves Knowledge-Intensive Text-to-Image Retrieval|VisRet: å¯è§†åŒ–æå‡çŸ¥è¯†å¯†é›†å‹æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢|Di Wu, Yixin Wan, Kai-Wei Chang|<https://arxiv.org/pdf/2505.20291v3>|[ä»£ç ](https://github.com/xiaowu0162/Visualize-then-Retrieve.)|
|ğŸ†• å‘å¸ƒ|DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation|DiffBench é‡è§ DiffAgentï¼šç«¯åˆ°ç«¯ LLM é©±åŠ¨çš„ Diffusion åŠ é€Ÿä»£ç ç”Ÿæˆ|Jiajun jiao, Haowei Zhu, Puyuan Yang, Jianghui Wang, Ji Liu, Ziqiong Liu, Dong Li, Yuejian Fang .etc.|<https://arxiv.org/pdf/2601.03178v1>|æ— |
|ğŸ†• å‘å¸ƒ|Unified Thinker: A General Reasoning Modular Core for Image Generation|Unified Thinkerï¼šç”¨äºå›¾åƒç”Ÿæˆçš„é€šç”¨æ¨ç†æ¨¡å—åŒ–æ ¸å¿ƒ|Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song .etc.|<https://arxiv.org/pdf/2601.03127v1>|æ— |
|ğŸ“ æ›´æ–°|Evaluating Gemini Robotics Policies in a Veo World Simulator|åœ¨ Veo World Simulator ä¸­è¯„ä¼° Gemini Robotics Policies|Gemini Robotics Team, Krzysztof Choromanski, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf .etc.|<https://arxiv.org/pdf/2512.10675v2>|æ— |
|ğŸ“ æ›´æ–°|FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing|FFP-300K: æ‰©å±•é¦–å¸§ä¼ æ’­ä»¥å®ç°å¯æ³›åŒ–çš„è§†é¢‘ç¼–è¾‘|Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang, Xu Peng, Jiangning Zhang, Chengjie Wang .etc.|<https://arxiv.org/pdf/2601.01720v2>|æ— |
|ğŸ“ æ›´æ–°|UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities|UniversalRAG: å¤šæ¨¡æ€ä¸å¤šç²’åº¦è¯­æ–™åº“ä¸Šçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ|Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang|<https://arxiv.org/pdf/2504.20734v3>|æ— |
|ğŸ“ æ›´æ–°|Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face|é’ˆå¯¹3Dåœºä¸ªæ€§åŒ–è¯´è¯äººè„¸çš„é«˜æ•ˆé²æ£’è§†é¢‘é˜²å¾¡æ¡†æ¶|Rui-qing Sun, Xingshan Yao, Tian Lan, Jia-Ling Shi, Chen-Hao Cui, Hui-Yang Zhao, Zhijing Wu, Chen Yang .etc.|<https://arxiv.org/pdf/2512.21019v3>|[ä»£ç ](https://github.com/Richen7418/VDF.)|
|ğŸ†• å‘å¸ƒ|Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning|Zoom-IQA: åŸºäºå¯é åŒºåŸŸæ„ŸçŸ¥æ¨ç†çš„å›¾åƒè´¨é‡è¯„ä¼°|Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou|<https://arxiv.org/pdf/2601.02918v1>|æ— |
|ğŸ†• å‘å¸ƒ|DreamStyle: A Unified Framework for Video Stylization|DreamStyle: ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘é£æ ¼åŒ–æ¡†æ¶|Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He|<https://arxiv.org/pdf/2601.02785v1>|æ— |
|ğŸ“ æ›´æ–°|Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints|é€šè¿‡å…¨å±€æ—‹è½¬æ‰©æ•£å’Œå¤šçº§çº¦æŸç¼“è§£ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯|Xiangyue Zhang, Jianfang Li, Jianqiang Ren, Jiaxu Zhang|<https://arxiv.org/pdf/2511.10076v2>|æ— |
|ğŸ“ æ›´æ–°|Go with Your Gut: Scaling Confidence for Autoregressive Image Generation|[ç¿»è¯‘å¤±è´¥] Go with Your Gut: Scaling Confidence for Autoregressive Image Generation|Harold Haodong Chen, Xianfeng Wu, Wen-Jie Shu, Rongjin Guo, Disen Lan, Harry Yang, Ying-Cong Chen|<https://arxiv.org/pdf/2509.26376v2>|æ— |
|ğŸ†• å‘å¸ƒ|EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework|[ç¿»è¯‘å¤±è´¥] EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework|Junjue Wang, Yanfei Zhong, Zihang Chen, Zhuo Zheng, Ailong Ma, Liangpei Zhang|<https://arxiv.org/pdf/2601.02783v1>|æ— |
|ğŸ†• å‘å¸ƒ|Omni2Sound: Towards Unified Video-Text-to-Audio Generation|[ç¿»è¯‘å¤±è´¥] Omni2Sound: Towards Unified Video-Text-to-Audio Generation|Yusheng Dai, Zehua Chen, Yuxuan Jiang, Baolong Gao, Qiuhong Ke, Jun Zhu, Jianfei Cai|<https://arxiv.org/pdf/2601.02731v1>|[ä»£ç ](https://swapforward.github.io/Omni2Sound.)|
|ğŸ“ æ›´æ–°|RoboTransfer: Controllable Geometry-Consistent Video Diffusion for Manipulation Policy Transfer|RoboTransferï¼šç”¨äºæ“ä½œç­–ç•¥è¿ç§»çš„å¯æ§å‡ ä½•ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£|Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiagang Zhu, Jiaxiong Qiu, Zheng Zhu .etc.|<https://arxiv.org/pdf/2505.23171v2>|[ä»£ç ](https://horizonrobotics.github.io/robot_lab)|
|ğŸ†• å‘å¸ƒ|DreamLoop: Controllable Cinemagraph Generation from a Single Photograph|DreamLoop: ä»å•å¼ ç…§ç‰‡ç”Ÿæˆå¯æ§çš„é™åƒç”µå½±|Aniruddha Mahapatra, Long Mai, Cusuh Ham, Feng Liu|<https://arxiv.org/pdf/2601.02646v1>|æ— |


### ä¸‰ç»´å†…å®¹ç”Ÿæˆ (3D Content Generation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|A Versatile Multimodal Agent for Multimedia Content Generation|ä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ç”¨äºå¤šåª’ä½“å†…å®¹ç”Ÿæˆ|Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu|<https://arxiv.org/pdf/2601.03250v1>|æ— |
|ğŸ“ æ›´æ–°|BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM|BusterX++ï¼šåˆ©ç”¨MLLMè¿ˆå‘ç»Ÿä¸€çš„è·¨æ¨¡æ€AIç”Ÿæˆå†…å®¹æ£€æµ‹ä¸è§£é‡Š|Haiquan Wen, Tianxiao Li, Zhenglin Huang, Yiwei He, Guangliang Cheng|<https://arxiv.org/pdf/2507.14632v3>|æ— |
|ğŸ†• å‘å¸ƒ|LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing|LAMS-Edit: åŸºäºè°ƒåº¦å™¨çš„æ½œåœ¨ä¸æ³¨æ„åŠ›æ··åˆä»¥æ”¹è¿›åŸºäº Diffusion çš„å›¾åƒå’Œé£æ ¼ç¼–è¾‘ä¸­çš„å†…å®¹ä¿æŒ|Wingwa Fu, Takayuki Okatani|<https://arxiv.org/pdf/2601.02987v1>|æ— |
|ğŸ“ æ›´æ–°|How Many Images Does It Take? Estimating Imitation Thresholds in Text-to-Image Models|éœ€è¦å¤šå°‘å¼ å›¾åƒï¼Ÿä¼°ç®— Text-to-Image Models ä¸­çš„æ¨¡ä»¿é˜ˆå€¼|Sahil Verma, Royi Rassin, Arnav Das, Gantavya Bhatt, Preethi Seshadri, Chirag Shah, Jeff Bilmes, Hannaneh Hajishirzi .etc.|<https://arxiv.org/pdf/2410.15002v2>|[ä»£ç ](https://github.com/vsahil/MIMETIC-2.)|


### æ—¶ç©ºä¸€è‡´æ€§ç”Ÿæˆ (Spatiotemporal Coherent Generation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders|PartHOI: åŸºäºéƒ¨ä»¶çš„æ‰‹-ç‰©ä½“äº¤äº’è¿ç§»é€šè¿‡å¹¿ä¹‰åœ†æŸ±|Qiaochu Wang, Chufeng Xiao, Manfred Lau, Hongbo Fu|<https://arxiv.org/pdf/2504.20599v2>|æ— |
|ğŸ†• å‘å¸ƒ|TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors|TA-Prompting: é€šè¿‡æ—¶é—´é”šç‚¹å¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¯†é›†è§†é¢‘æè¿°èƒ½åŠ›|Wei-Yuan Cheng, Kai-Po Chang, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang|<https://arxiv.org/pdf/2601.02908v1>|æ— |
|ğŸ†• å‘å¸ƒ|AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs|AbductiveMLLM: æå‡ MLLMs å†…çš„è§†è§‰æº¯å› æ¨ç†èƒ½åŠ›|Boyu Chang, Qi Wang, Xi Guo, Zhixiong Nan, Yazhou Yao, Tianfei Zhou|<https://arxiv.org/pdf/2601.02771v1>|æ— |
|ğŸ“ æ›´æ–°|SoulX-FlashTalk: Real-Time Infinite Streaming of Audio-Driven Avatars via Self-Correcting Bidirectional Distillation|SoulX-FlashTalkï¼šé€šè¿‡è‡ªæ ¡æ­£åŒå‘è’¸é¦å®ç°éŸ³é¢‘é©±åŠ¨Avatarçš„å®æ—¶æ— é™æµå¼ä¼ è¾“|Le Shen, Qian Qiao, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao .etc.|<https://arxiv.org/pdf/2512.23379v3>|æ— |
|ğŸ†• å‘å¸ƒ|GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images|GRREï¼šåˆ©ç”¨å»é™¤Gé€šé“é‡å»ºè¯¯å·®å®ç°AIç”Ÿæˆå›¾åƒçš„é²æ£’æ£€æµ‹|Shuman He, Xiehua Li, Xioaju Yang, Yang Xiong, Keqin Li|<https://arxiv.org/pdf/2601.02709v1>|æ— |


### æ‰©æ•£æ¦‚ç‡æ¨¡å‹ (Diffusion Probabilistic Models)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|DiT-JSCC: Rethinking Deep JSCC with Diffusion Transformers and Semantic Representations|DiT-JSCC: åˆ©ç”¨Diffusion Transformerså’Œè¯­ä¹‰è¡¨ç¤ºé‡æ–°æ€è€ƒDeep JSCC|Kailin Tan, Jincheng Dai, Sixian Wang, Guo Lu, Shuo Shao, Kai Niu, Wenjun Zhang, Ping Zhang|<https://arxiv.org/pdf/2601.03112v1>|æ— |
|ğŸ†• å‘å¸ƒ|Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries|åŸºäº PointNet çš„ Flow Matching å’Œ Diffusion æ¨¡å‹ç”¨äºåœ¨ä¸è§„åˆ™å‡ ä½•ä¸Šç”Ÿæˆæµåœº|Ali Kashefi|<https://arxiv.org/pdf/2601.03030v1>|æ— |
|ğŸ†• å‘å¸ƒ|VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on|VTONQA: ä¸€ä¸ªç”¨äºVirtual Try-onçš„å¤šç»´è´¨é‡è¯„ä¼°æ•°æ®é›†|Xinyi Wei, Sijing Wu, Zitong Xu, Yunhao Li, Huiyu Duan, Xiongkuo Min, Guangtao Zhai|<https://arxiv.org/pdf/2601.02945v1>|æ— |
|ğŸ†• å‘å¸ƒ|Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion|[ç¿»è¯‘å¤±è´¥] Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion|Jakob LÃ¸nborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl|<https://arxiv.org/pdf/2601.02881v1>|æ— |
|ğŸ“ æ›´æ–°|E$^2$AT: Multimodal Jailbreak Defense via Dynamic Joint Optimization for Multimodal Large Language Models|E$^2$AT: é€šè¿‡åŠ¨æ€è”åˆä¼˜åŒ–çš„å¤šæ¨¡æ€è¶Šç‹±é˜²å¾¡ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹|Liming Lu, Xiang Gu, Shuchao Pang, Siyuan Liang, Haotian Zhu, Xiyu Zeng, Xu Zheng, Yongbin Zhou|<https://arxiv.org/pdf/2503.04833v3>|æ— |
|ğŸ“ æ›´æ–°|Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats|Intervene-All-Pathsï¼šè·¨å¯¹é½æ ¼å¼çš„ LVLM å¹»è§‰ç»Ÿä¸€ç¼“è§£|Jiaye Qian, Ge Zheng, Yuchen Zhu, Sibei Yang|<https://arxiv.org/pdf/2511.17254v2>|æ— |
|ğŸ“ æ›´æ–°|FLUID: Training-Free Face De-identification via Latent Identity Substitution|FLUID: é€šè¿‡Latent Identity Substitutionå®ç°å…è®­ç»ƒäººè„¸å»æ ‡è¯†åŒ–|Jinhyeong Park, Shaheryar Muhammad, Seangmin Lee, Jong Taek Lee, Soon Ki Jung|<https://arxiv.org/pdf/2511.17005v2>|æ— |
|ğŸ†• å‘å¸ƒ|Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing|VRä¸­é€šè¿‡è§†é”¥é‡‡æ ·å’Œå‡ ä½•å¹³æ»‘å®ç°é²æ£’çš„ç½‘æ ¼æ˜¾è‘—æ€§GTè·å–|Guoquan Zheng, Jie Hao, Huiyu Duan, Yongming Han, Liang Yuan, Dong Zhang, Guangtao Zhai|<https://arxiv.org/pdf/2601.02721v1>|æ— |


## ä¸‰ç»´è§†è§‰ä¸å‡ ä½•æ¨ç† (3D Vision & Geometric Reasoning)


### ç¥ç»è¾å°„åœºè¡¨ç¤º (Neural Radiance Field Representation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Aligning Text, Images, and 3D Structure Token-by-Token|é€Tokenå¯¹é½Textã€Imageså’Œ3D Structure|Aadarsh Sahoo, Vansh Tibrewal, Georgia Gkioxari|<https://arxiv.org/pdf/2506.08002v2>|[ä»£ç ](https://glab-caltech.github.io/kyvo)|
|ğŸ†• å‘å¸ƒ|InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields|InfiniDepth: åŸºäºNeural Implicit Fieldsçš„ä»»æ„åˆ†è¾¨ç‡å’Œç»†ç²’åº¦æ·±åº¦ä¼°è®¡|Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou .etc.|<https://arxiv.org/pdf/2601.03252v1>|æ— |
|ğŸ“ æ›´æ–°|Quantifying task-relevant representational similarity using decision variable correlation|ä½¿ç”¨å†³ç­–å˜é‡ç›¸å…³æ€§é‡åŒ–ä»»åŠ¡ç›¸å…³çš„è¡¨å¾ç›¸ä¼¼æ€§|Yu, Qian, Wilson S. Geisler, Xue-Xin Wei|<https://arxiv.org/pdf/2506.02164v3>|æ— |
|ğŸ†• å‘å¸ƒ|DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification|DCG ReID: è§£è€¦åä½œä¸å¼•å¯¼èåˆè¡¨ç¤ºç”¨äºå¤šæ¨¡æ€è½¦è¾† Re-Identification|Aihua Zheng, Ya Gao, Shihao Li, Chenglong Li, Jin Tang|<https://arxiv.org/pdf/2601.02924v1>|æ— |
|ğŸ†• å‘å¸ƒ|Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD|Textile IR: ä¸€ç§é¢å‘ç‰©ç†æ„ŸçŸ¥çš„æ—¶å°š CAD åŒå‘ä¸­é—´è¡¨ç¤º|Petteri Teikari, Neliana Fuenmayor|<https://arxiv.org/pdf/2601.02792v1>|æ— |
|ğŸ“ æ›´æ–°|DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior|DenseSplat: åˆ©ç”¨Neural Radiance Priorç¨ å¯†åŒ–Gaussian Splatting SLAM|Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang|<https://arxiv.org/pdf/2502.09111v2>|æ— |


### è§†è§‰å®šä½ä¸æ˜ å°„ (Visual Localization & Mapping)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|AnyDepth: Depth Estimation Made Easy|AnyDepth: æ·±åº¦ä¼°è®¡å˜å¾—ç®€å•|Zeyu Ren, Zeyu Zhang, Wukai Li, Qingxiang Liu, Hao Tang|<https://arxiv.org/pdf/2601.02760v1>|[ä»£ç ](https://github.com/AIGeeksGroup/AnyDepth.)|
|ğŸ†• å‘å¸ƒ|CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos|CAMO: æ¥è‡ªå•ç›®2Dè§†é¢‘çš„ç±»åˆ«æ— å…³3Dè¿åŠ¨è¿ç§»|Taeyeon Kim, Youngju Na, Jumin Lee, Minhyuk Sung, Sung-Eui Yoon|<https://arxiv.org/pdf/2601.02716v1>|æ— |


### å¤šè§†å›¾å‡ ä½•é‡å»º (Multi-view Geometric Reconstruction)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|SlingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging with arbitrary array geometries|SlingBAG Proï¼šåŠ é€ŸåŸºäºç‚¹äº‘çš„è¿­ä»£é‡å»ºï¼Œç”¨äºä»»æ„é˜µåˆ—å‡ ä½•å½¢çŠ¶çš„3Då…‰å£°æˆåƒ|Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao .etc.|<https://arxiv.org/pdf/2601.00551v2>|[ä»£ç ](https://github.com/JaegerCQ/SlingBAG_Pro.)|


## æ—¶åºè§†è§‰åˆ†æ (Temporal Visual Analysis)


### é•¿æ—¶åºè§†é¢‘ç†è§£ (Long-term Video Understanding)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding|PrismVAU: ç”¨äºå¤šæ¨¡æ€è§†é¢‘å¼‚å¸¸ç†è§£çš„æç¤ºç²¾ç‚¼æ¨ç†ç³»ç»Ÿ|IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera|<https://arxiv.org/pdf/2601.02927v1>|æ— |


### åŠ¨ä½œè¯†åˆ«ä¸ç†è§£ (Action Recognition & Understanding)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|SignX: Continuous Sign Recognition in Compact Pose-Rich Latent Space|SignX: ç´§å‡‘å§¿æ€ä¸°å¯Œæ½œåœ¨ç©ºé—´ä¸­çš„è¿ç»­æ‰‹è¯­è¯†åˆ«|Sen Fang, Yalin Feng, Chunyu Sui, Hongbin Zhong, Hongwei Yi, Dimitris N. Metaxas|<https://arxiv.org/pdf/2504.16315v2>|æ— |


## è‡ªç›‘ç£ä¸è¡¨å¾å­¦ä¹  (Self-supervised & Representation Learning)


### è·¨æ¨¡æ€ä¸€è‡´æ€§å­¦ä¹  (Cross-modal Consistency Learning)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining|é¢å‘æ‹“æ‰‘çš„ç—…ç†ä¸€è‡´æ€§åŒ¹é…ç”¨äºå¼±é…å¯¹ IHC è™šæ‹ŸæŸ“è‰²|Mingzhou Jiang, Jiaying Zhou, Nan Zeng, Mickael Li, Qijie Tang, Chao He, Huazhu Fu, Honghui He|<https://arxiv.org/pdf/2601.02806v1>|æ— |


### å¯¹æ¯”å­¦ä¹ æ–¹æ³• (Contrastive Learning Methods)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval|[ç¿»è¯‘å¤±è´¥] Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval|Qing Wang, Chong-Wah Ngo, Ee-Peng Lim|<https://arxiv.org/pdf/2511.15201v2>|æ— |


## è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ– (Computational Efficiency & Model Optimization)


### èµ„æºå—é™è§†è§‰è®¡ç®— (Resource-constrained Visual Computing)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai|ä¸€ç§ç”¨äºåˆ†æå†å²åŸåŒºæ¸¸å®¢æ„ŸçŸ¥çš„å¤šç»´AIæ¡†æ¶ï¼šä»¥ä¸Šæµ·ä¸ºä¾‹|Kaizhen Tan, Yufan Wu, Yuxuan Liu, Haoran Zeng|<https://arxiv.org/pdf/2509.03830v2>|æ— |
|ğŸ†• å‘å¸ƒ|ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios|ReCCur: ä¸€ç§é¢å‘å¼€æ”¾å’Œè¾¹ç¼˜åœºæ™¯é²æ£’ Vision-Language ç†è§£çš„é€’å½’ Corner-Case ç­›é€‰æ¡†æ¶|Yihan Wei, Shenghai Yuan, Tianchen Deng, Boyang Lou, Enwen Hu|<https://arxiv.org/pdf/2601.03011v1>|æ— |
|ğŸ“ æ›´æ–°|ISCS: Parameter-Guided Feature Pruning for Resource-Constrained Embodied Perception|ISCS: é¢å‘èµ„æºå—é™å…·èº«æ„ŸçŸ¥çš„å‚æ•°å¼•å¯¼ç‰¹å¾å‰ªæ|Jinhao Wang, Nam Ling, Wei Wang, Wei Jiang|<https://arxiv.org/pdf/2509.16853v2>|æ— |


### æ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿ (Model Compression & Acceleration)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework|åŸºäºçœŸå®äººç±»æ¸¸æˆæ•°æ®çš„åœ°ç†å®šä½ï¼šå¤§è§„æ¨¡æ•°æ®é›†ä¸ç±»äººæ¨ç†æ¡†æ¶|Zirui Song, Jingpu Yang, Yuan Huang, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych .etc.|<https://arxiv.org/pdf/2502.13759v3>|æ— |
|ğŸ“ æ›´æ–°|Teeth3DS+: An Extended Benchmark for Intraoral 3D Scans Analysis|Teeth3DS+: ä¸€ä¸ªç”¨äºå£å†…3Dæ‰«æåˆ†æçš„æ‰©å±•åŸºå‡†|Achraf Ben-Hamadou, Nour Neifar, Ahmed Rekik, Oussama Smaoui, Firas Bouzguenda, Sergi Pujades, Edmond Boyer, Edouard Ladroit|<https://arxiv.org/pdf/2210.06094v3>|[ä»£ç ](https://crns-smartvision.github.io/teeth3ds)|
|ğŸ“ æ›´æ–°|Point-Supervised Facial Expression Spotting with Gaussian-Based Instance-Adaptive Intensity Modeling|åŸºäºé«˜æ–¯å®ä¾‹è‡ªé€‚åº”å¼ºåº¦å»ºæ¨¡çš„ç‚¹ç›‘ç£é¢éƒ¨è¡¨æƒ…å®šä½|Yicheng Deng, Hideaki Hayashi, Hajime Nagahara|<https://arxiv.org/pdf/2511.16952v3>|æ— |


### ç¥ç»æ¶æ„ä¼˜åŒ– (Neural Architecture Optimization)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures|ä»è®°å¿†åˆ°åˆ›é€ ï¼šLLM ä½œä¸ºæ–°å‹ Neural-Architectures çš„è®¾è®¡è€…|Waleed Khalid, Dmitry Ignatov, Radu Timofte|<https://arxiv.org/pdf/2601.02997v1>|æ— |
|ğŸ†• å‘å¸ƒ|Foreground-Aware Dataset Distillation via Dynamic Patch Selection|[ç¿»è¯‘å¤±è´¥] Foreground-Aware Dataset Distillation via Dynamic Patch Selection|Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama|<https://arxiv.org/pdf/2601.02727v1>|æ— |


## é²æ£’æ€§ä¸å¯é æ€§ (Robustness & Reliability)


### åˆ†å¸ƒå¤–æ³›åŒ– (Out-of-distribution Generalization)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding|é€šè¿‡å°†æ¦‚å¿µå’Œç‰¹å¾ç©ºé—´ç»“æ„åŒ–ä¸ºå…±æ€§ã€ç‰¹å¼‚æ€§å’Œæ··æ·†æ€§å®ç°ç»†ç²’åº¦æ³›åŒ–|Zhen Wang, Jiaojiao Zhao, Qilong Wang, Yongfeng Dong, Wenlong Yu|<https://arxiv.org/pdf/2601.03056v1>|æ— |
|ğŸ†• å‘å¸ƒ|SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection|SA-ResGS: ç”¨äºNext Best View Selectionçš„è‡ªå¢å¼ºæ®‹å·®3D Gaussian Splatting|Kim Jun-Seong, Tae-Hyun Oh, Eduardo PÃ©rez-Pellitero, Youngkyoon Jang|<https://arxiv.org/pdf/2601.03024v1>|æ— |


## ä½èµ„æºä¸é«˜æ•ˆå­¦ä¹  (Low-resource & Efficient Learning)


### ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ (Active Learning Strategies)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition|LeafLifeï¼šä¸€ç§å…·æœ‰é²æ£’æ€§çš„å¯è§£é‡Šæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè‘¡è„å¶ç—…å®³è¯†åˆ«|B. M. Shahria Alam, Md. Nasim Ahmed|<https://arxiv.org/pdf/2601.03124v1>|æ— |
|ğŸ“ æ›´æ–°|SAGOnline: Segment Any Gaussians Online|SAGOnline: åœ¨çº¿åˆ†å‰²ä»»æ„é«˜æ–¯|Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma .etc.|<https://arxiv.org/pdf/2508.08219v2>|æ— |
|ğŸ“ æ›´æ–°|PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction|PhysSFI-Net: ç”¨äºæ­£é¢Œæ‰‹æœ¯ç»“æœé¢„æµ‹çš„éª¨éª¼ä¸é¢éƒ¨äº¤äº’çš„Physics-informedå‡ ä½•å­¦ä¹ |Jiahao Bao, Huazhen Liu, Yu Zhuang, Leran Tao, Xinyu Xu, Yongtao Shi, Mengjia Cheng, Yiming Wang .etc.|<https://arxiv.org/pdf/2601.02088v2>|æ— |
|ğŸ†• å‘å¸ƒ|Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups|[ç¿»è¯‘å¤±è´¥] Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups|Hyungtae Lim, Minkyun Seo, Luca Carlone, Jaesik Park|<https://arxiv.org/pdf/2601.02759v1>|[ä»£ç ](https://github.com/MIT-SPARK/BUFFER-X.)|
|ğŸ“ æ›´æ–°|MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation|MCD-Net: ä¸€ç§ä»…å…‰å­¦å†°ç¢›åˆ†å‰²çš„è½»é‡çº§æ·±åº¦å­¦ä¹ åŸºçº¿|Zhehuan Cao, Fiseha Berhanu Tesema, Ping Fu, Jianfeng Ren, Ahmed Nasr|<https://arxiv.org/pdf/2601.02091v2>|[ä»£ç ](https://github.com/Lyra-alpha/MCD-Net)|
|ğŸ“ æ›´æ–°|SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents|SmartSnapï¼šè‡ªéªŒè¯æ™ºèƒ½ä½“çš„ä¸»åŠ¨è¯æ®å¯»æ±‚|Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao .etc.|<https://arxiv.org/pdf/2512.22322v2>|[ä»£ç ](https://github.com/TencentYoutuResearch/SmartSnap)|


### å°æ ·æœ¬å­¦ä¹  (Few-shot Learning)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Spatial Polarization Multiplexing: Single-Shot Invisible Shape and Reflectance Recovery|Spatial Polarization Multiplexing: å•æ¬¡æ‹æ‘„ä¸å¯è§å½¢çŠ¶ä¸åå°„ç‡æ¢å¤|Tomoki Ichikawa, Ryo Kawahara, Ko Nishino|<https://arxiv.org/pdf/2504.13177v3>|æ— |
|ğŸ“ æ›´æ–°|FCC: Fully Connected Correlation for One-Shot Segmentation|FCC: ç”¨äº One-Shot Segmentation çš„ Fully Connected Correlation|Seonghyeon Moon, Haein Kong, Muhammad Haris Khan, Mubbasir Kapadia, Yuewei Lin|<https://arxiv.org/pdf/2411.11917v2>|æ— |


## å…·èº«æ™ºèƒ½ä¸äº¤äº’è§†è§‰ (Embodied Intelligence & Interactive Vision)


### è§†è§‰æ“ä½œä¸æ§åˆ¶ (Visual Manipulation & Control)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation|Chain-of-Actionï¼šç”¨äºæœºå™¨äººæ“ä½œçš„è½¨è¿¹è‡ªå›å½’å»ºæ¨¡|Wenbo Zhang, Tianrun Hu, Hanbo Zhang, Yanyuan Qiao, Yuchu Qin, Yang Li, Jiajun Liu, Tao Kong .etc.|<https://arxiv.org/pdf/2506.09990v2>|æ— |


## è§†è§‰-è¯­è¨€ååŒç†è§£ (Vision-Language Joint Understanding)


### è§†è§‰é—®ç­”ä¸æ¨ç† (Visual Question Answering & Reasoning)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Transformers self-organize like newborn visual systems when trained in prenatal worlds|Transformers åœ¨äº§å‰ä¸–ç•Œä¸­è®­ç»ƒæ—¶åƒæ–°ç”Ÿå„¿è§†è§‰ç³»ç»Ÿä¸€æ ·è‡ªç»„ç»‡|Lalit Pandey, Samantha M. W. Wood, Justin N. Wood|<https://arxiv.org/pdf/2601.03117v1>|æ— |
|ğŸ†• å‘å¸ƒ|Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA|åˆ©ç”¨ Large Language Models ç†è§£ Multi-Agent Reasoning ä»¥è¿›è¡Œ Cartoon VQA|Tong Wu, Thanet Markchom|<https://arxiv.org/pdf/2601.03073v1>|æ— |
|ğŸ†• å‘å¸ƒ|IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation|IBISAgent: åœ¨MLLMsä¸­å¢å¼ºåƒç´ çº§è§†è§‰æ¨ç†ä»¥å®ç°é€šç”¨ç”Ÿç‰©åŒ»å­¦å¯¹è±¡æŒ‡ä»£ä¸åˆ†å‰²|Yankai Jiang, Qiaoru Li, Binlu Xu, Haoran Sun, Chao Ding, Junting Dong, Yuxiang Cai, Xuhong Zhang .etc.|<https://arxiv.org/pdf/2601.03054v1>|æ— |
|ğŸ“ æ›´æ–°|CVBench: Benchmarking Cross-Video Synergies for Complex Multimodal Reasoning|CVBenchï¼šé¢å‘å¤æ‚å¤šæ¨¡æ€æ¨ç†çš„è·¨è§†é¢‘ååŒåŸºå‡†æµ‹è¯•|Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng .etc.|<https://arxiv.org/pdf/2508.19542v4>|[ä»£ç ](https://github.com/Hokhim2/CVBench.)|
|ğŸ†• å‘å¸ƒ|Towards Faithful Reasoning in Comics for Small MLLMs|é¢å‘æ¼«ç”»ä¸­ Small MLLMs çš„å¿ å®æ¨ç†|Chengcheng Feng, Haojie Yin, Yucheng Jin, Kaizhu Huang|<https://arxiv.org/pdf/2601.02991v1>|æ— |
|ğŸ“ æ›´æ–°|ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association|ViSTA-SLAM: åŸºäºå¯¹ç§°åŒè§†å›¾å…³è”çš„ Visual SLAM|Ganlin Zhang, Shenhan Qian, Xi Wang, Daniel Cremers|<https://arxiv.org/pdf/2509.01584v2>|[ä»£ç ](https://github.com/zhangganlin/vista-slam)|
|ğŸ“ æ›´æ–°|CaTS-Bench: Can Language Models Describe Time Series?|CaTS-Bench: Language Models èƒ½å¦æè¿° Time Seriesï¼Ÿ|Luca Zhou, Pratham Yashwante, Marshall Fisher, Alessio Sampieri, Zihao Zhou, Fabio Galasso, Rose Yu|<https://arxiv.org/pdf/2509.20823v2>|æ— |
|ğŸ“ æ›´æ–°|Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance|é¢å‘è§†éšœè¾…åŠ©çš„å…·æœ‰è·¨æ¨¡æ€å·®å¼‚åŒ–é‡åŒ–VLMsçš„åœºæ™¯æ„ŸçŸ¥å‘é‡åŒ–è®°å¿†å¤šæ™ºèƒ½ä½“æ¡†æ¶|Xiangxiang Wang, Xuanyu Wang, YiJia Luo, Yongbin Yu, Manping Fan, Jingtao Zhang, Liyong Ren|<https://arxiv.org/pdf/2508.18177v2>|æ— |
|ğŸ†• å‘å¸ƒ|ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration|ClearAIR: ä¸€ç§å—äººç±»è§†è§‰æ„ŸçŸ¥å¯å‘çš„å…¨èƒ½å›¾åƒå¤åŸ|Xu Zhang, Huan Zhang, Guoli Wang, Qian Zhang, Lefei Zhang|<https://arxiv.org/pdf/2601.02763v1>|æ— |
|ğŸ“ æ›´æ–°|RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics|RoboTracerï¼šåœ¨æœºå™¨äººè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­é€šè¿‡æ¨ç†æŒæ¡ç©ºé—´è½¨è¿¹|Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji .etc.|<https://arxiv.org/pdf/2512.13660v2>|[ä»£ç ](https://zhoues.github.io/RoboTracer.)|
|ğŸ†• å‘å¸ƒ|HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps|HOLOï¼šç”¨äºSDåœ°å›¾ç»†ç²’åº¦è§†è§‰å®šä½çš„å•åº”æ€§å¼•å¯¼ä½å§¿ä¼°è®¡ç½‘ç»œ|Xuchang Zhong, Xu Cao, Jinke Feng, Hao Fang|<https://arxiv.org/pdf/2601.02730v1>|æ— |
|ğŸ†• å‘å¸ƒ|Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench|æ­ç¤ºå¹¶å¼¥åˆ MLLMs ä¸­çš„åŠŸèƒ½æ„ŸçŸ¥å·®è·ï¼šé€šè¿‡ PET-Bench å®ç°åŸå­è§†è§‰å¯¹é½ä¸åˆ†å±‚è¯„ä¼°|Zanting Ye, Xiaolong Niu, Xuanbin Wu, Xu Han, Shengyuan Liu, Jing Hao, Zhihao Peng, Hao Sun .etc.|<https://arxiv.org/pdf/2601.02737v1>|[ä»£ç ](https://github.com/yezanting/PET-Bench.)|
|ğŸ“ æ›´æ–°|DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments|DarkEQAï¼šä½å…‰å®¤å†…ç¯å¢ƒä¸­å…·èº«é—®ç­”çš„Vision-Language ModelsåŸºå‡†æµ‹è¯•|Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh|<https://arxiv.org/pdf/2512.24985v2>|æ— |
|ğŸ“ æ›´æ–°|AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans|AdaVLN: é¢å‘åŒ…å«ç§»åŠ¨è¡Œäººçš„è¿ç»­å®¤å†…ç¯å¢ƒä¸­çš„Visual Language Navigation|Dillon Loh, Tomasz Bednarz, Xinxing Xia, Frank Guan|<https://arxiv.org/pdf/2411.18539v3>|æ— |


### å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿ (Multimodal Dialogue Systems)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents|VLN-MME: è¯Šæ–­ä½œä¸ºè¯­è¨€å¼•å¯¼è§†è§‰å¯¼èˆªæ™ºèƒ½ä½“çš„MLLMs|Xunyi Zhao, Gengze Zhou, Qi Wu|<https://arxiv.org/pdf/2512.24851v2>|æ— |


### è·¨æ¨¡æ€æ£€ç´¢ä¸åŒ¹é… (Cross-modal Retrieval & Matching)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval|åœ¨ Hyperbolic Space ä¸­å­¦ä¹  Visual Hierarchies ç”¨äº Image Retrieval|Ziwei Wang, Sameera Ramasinghe, Chenchen Xu, Julien Monteil, Loris Bazzani, Thalaiyasingam Ajanthan|<https://arxiv.org/pdf/2411.17490v4>|æ— |
|ğŸ†• å‘å¸ƒ|Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM|åœ¨ DPV-SLAM ä¸­ä½¿ç”¨ AnyLoc Visual Place Recognition è¿›è¡Œé—­ç¯æ£€æµ‹|Wenzheng Zhang, Kazuki Adachi, Yoshitaka Hara, Sousuke Nakamura|<https://arxiv.org/pdf/2601.02723v1>|æ— |


### è§†è§‰å†…å®¹æè¿° (Visual Content Description)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning|RxnCaption: å°†ååº”å›¾è§£æé‡æ–°è¡¨è¿°ä¸ºè§†è§‰æç¤ºå¼•å¯¼çš„æè¿°ç”Ÿæˆ|Jiahe Song, Chuang Wang, Bowen Jiang, Yinfan Wang, Hao Zheng, Xingjian Wei, Chengjin Liu, Rui Nie .etc.|<https://arxiv.org/pdf/2511.02384v2>|æ— |


## é¢†åŸŸç‰¹å®šè§†è§‰åº”ç”¨ (Domain-specific Visual Applications)


### åŒ»å­¦å½±åƒåˆ†æ (Medical Image Analysis)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ“ æ›´æ–°|ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression|ImageNetè®­ç»ƒçš„CNNå¹¶ä¸åå‘çº¹ç†ï¼šé€šè¿‡å—æ§æŠ‘åˆ¶é‡æ–°å®¡è§†ç‰¹å¾ä¾èµ–|Tom Burgert, Oliver Stoll, Paolo Rota, BegÃ¼m Demir|<https://arxiv.org/pdf/2509.20234v4>|[ä»£ç ](https://github.com/tomburgert/feature-reliance.)|
|ğŸ“ æ›´æ–°|LVLM-Aware Multimodal Retrieval for RAG-Based Medical Diagnosis with General-Purpose Models|é¢å‘åŸºäºRAGçš„é€šç”¨æ¨¡å‹åŒ»å­¦è¯Šæ–­çš„LVLMæ„ŸçŸ¥å¤šæ¨¡æ€æ£€ç´¢|Nir Mazor, Tom Hope|<https://arxiv.org/pdf/2508.17394v4>|[ä»£ç ](https://github.com/Nirmaz/JOMED.)|


### åˆ›æ„åª’ä½“ç”Ÿæˆ (Creative Media Generation)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection|é¢å‘è½¦è·¯ååŒçš„é«˜æ•ˆ3Dç›®æ ‡æ£€æµ‹ï¼šåŸºäºé£é™©æ„å›¾é€‰æ‹©|Li Wang, Boqi Li, Hang Chen, Xingjian Wu, Yichen Wang, Jiewen Tan, Xinyu Zhang, Huaping Liu|<https://arxiv.org/pdf/2601.03001v1>|æ— |


### é¥æ„Ÿä¸åœ°ç†ä¿¡æ¯ (Remote Sensing & Geospatial Information)

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images|D$^3$R-DETR: ç”¨äºèˆªç©ºå›¾åƒå¾®å°ç›®æ ‡æ£€æµ‹çš„DETRï¼Œå…·æœ‰åŒåŸŸå¯†åº¦ç»†åŒ–|Zixiao Wen, Zhen Yang, Xianjie Bao, Lei Zhang, Xiantai Xiang, Wenshuai Li, Yuhan Liu|<https://arxiv.org/pdf/2601.02747v1>|æ— |


## å…¶ä»– (Others)


### æœªåˆ†ç±»

|çŠ¶æ€|è‹±æ–‡æ ‡é¢˜|ä¸­æ–‡æ ‡é¢˜|ä½œè€…|PDFé“¾æ¥|ä»£ç /è´¡çŒ®|
|---|---|---|---|---|---|
|ğŸ†• å‘å¸ƒ|Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement|ä½èµ„æºå¯å‘å¼æ–¹æ³•ç”¨äº Bahnaric å…‰å­¦å­—ç¬¦è¯†åˆ«æ”¹è¿›|Phat Tran, Phuoc Pham, Hung Trinh, Tho Quan|<https://arxiv.org/pdf/2601.02965v1>|æ— |
|ğŸ“ æ›´æ–°|The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology|[ç¿»è¯‘å¤±è´¥] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology|Sungwoo Kang|<https://arxiv.org/pdf/2512.21988v2>|æ— |

