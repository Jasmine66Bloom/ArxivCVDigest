# !UPDATED  -- 2024-01-06

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Exploiting Data Hierarchy as a New Modality for Contrastive Learning**<br />
**Title_cn:** 利用数据层次结构作为对比学习的新模式<br />
**Authors:** Arjun Bhalla, Daniel Levenson, Jan Bernhard, Anton Abilov<br />
**Abstract:** <details><summary>原文: </summary>This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作研究了层次结构数据如何帮助神经网络学习大教堂的概念表示。基础 WikiScenes 数据集提供了大教堂组件的空间组织层次结构。我们提出了一种新颖的分层对比训练方法，该方法利用三元组边缘损失来表示编码器潜在空间中数据的空间层次结构。因此，所提出的方法研究数据集结构是否为自监督学习提供了有价值的信息。我们应用 t-SNE 来可视化所产生的潜在空间，并通过使用常见的下游分类任务将其与其他特定于数据集的对比学习方法进行比较来评估所提出的方法。所提出的方法优于可比较的弱监督方法和基线方法。我们的研究结果表明，数据集结构是弱监督学习的一种有价值的模式。</details>
**PDF:** <http://arxiv.org/pdf/2401.03312v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Large Language Models as Visual Cross-Domain Learners**<br />
**Title_cn:** 作为视觉跨领域学习者的大型语言模型<br />
**Authors:** Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, Yu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型最近取得的进展依赖于独立同分布的假设，阻碍了它们在具有领域转移的现实场景中的应用。为了解决上述问题，跨领域学习旨在提取领域不变的知识，以减少训练和测试数据之间的领域转移。然而，在视觉跨域学习中，传统方法仅关注图像模态，忽略了使用文本模态来缓解域转移。在这项工作中，我们提出大型语言模型作为视觉跨域学习器（LLaVO）。 LLaVO 使用视觉语言模型将图像转换为详细的文本描述。然后，根据设计的指令模板生成的源/目标域的文本描述对大型语言模型进行微调。在域泛化和无监督域适应设置下的各种跨域任务的广泛实验结果证明了该方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03253v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond**<br />
**Title_cn:** MirrorDiffusion：通过提示重新描述及其他方式稳定零样本图像翻译中的扩散过程<br />
**Authors:** Yupei Lin, Xiaoyu Xian, Yukai Shi, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>Recently, text-to-image diffusion models become a new paradigm in image processing fields, including content generation, image restoration and image-to-image translation. Given a target prompt, Denoising Diffusion Probabilistic Models (DDPM) are able to generate realistic yet eligible images. With this appealing property, the image translation task has the potential to be free from target image samples for supervision. By using a target text prompt for domain adaption, the diffusion model is able to implement zero-shot image-to-image translation advantageously. However, the sampling and inversion processes of DDPM are stochastic, and thus the inversion process often fail to reconstruct the input content. Specifically, the displacement effect will gradually accumulated during the diffusion and inversion processes, which led to the reconstructed results deviating from the source domain. To make reconstruction explicit, we propose a prompt redescription strategy to realize a mirror effect between the source and reconstructed image in the diffusion model (MirrorDiffusion). More specifically, a prompt redescription mechanism is investigated to align the text prompts with latent code at each time step of the Denoising Diffusion Implicit Models (DDIM) inversion to pursue a structure-preserving reconstruction. With the revised DDIM inversion, MirrorDiffusion is able to realize accurate zero-shot image translation by editing optimized text prompts and latent code. Extensive experiments demonstrate that MirrorDiffusion achieves superior performance over the state-of-the-art methods on zero-shot image translation benchmarks by clear margins and practical model stability.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，文本到图像的扩散模型成为图像处理领域的新范式，包括内容生成、图像恢复和图像到图像的翻译。给定目标提示，去噪扩散概率模型 (DDPM) 能够生成真实且合格的图像。凭借这一吸引人的特性，图像翻译任务有可能摆脱目标图像样本的监督。通过使用目标文本提示进行域适应，扩散模型能够有利地实现零样本图像到图像的转换。然而，DDPM的采样和反演过程是随机的，因此反演过程常常无法重建输入内容。具体而言，在扩散和反演过程中，位移效应会逐渐累积，导致重建结果偏离源域。为了使重建更明确，我们提出了一种即时重新描述策略，以在扩散模型（MirrorDiffusion）中实现源图像和重建图像之间的镜像效果。更具体地说，研究了提示重新描述机制，以在去噪扩散隐式模型（DDIM）反演的每个时间步骤将文本提示与潜在代码对齐，以实现结构保留重建。通过修改后的 DDIM 反演，MirrorDiffusion 能够通过编辑优化的文本提示和潜在代码来实现精确的零样本图像翻译。大量实验表明，MirrorDiffusion 通过清晰的边缘和实际模型稳定性，在零样本图像转换基准上实现了优于最先进方法的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03221v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **DistFormer: Enhancing Local and Global Features for Monocular Per-Object Distance Estimation**<br />
**Title_cn:** DistFormer：增强单目每个对象距离估计的局部和全局特征<br />
**Authors:** Aniello Panariello, Gianluca Mancusi, Fedy Haj Ali, Angelo Porrello, Simone Calderara, Rita Cucchiara<br />
**Abstract:** <details><summary>原文: </summary>Accurate per-object distance estimation is crucial in safety-critical applications such as autonomous driving, surveillance, and robotics. Existing approaches rely on two scales: local information (i.e., the bounding box proportions) or global information, which encodes the semantics of the scene as well as the spatial relations with neighboring objects. However, these approaches may struggle with long-range objects and in the presence of strong occlusions or unusual visual patterns. In this respect, our work aims to strengthen both local and global cues. Our architecture -- named DistFormer -- builds upon three major components acting jointly: i) a robust context encoder extracting fine-grained per-object representations; ii) a masked encoder-decoder module exploiting self-supervision to promote the learning of useful per-object features; iii) a global refinement module that aggregates object representations and computes a joint, spatially-consistent estimation. To evaluate the effectiveness of DistFormer, we conduct experiments on the standard KITTI dataset and the large-scale NuScenes and MOTSynth datasets. Such datasets cover various indoor/outdoor environments, changing weather conditions, appearances, and camera viewpoints. Our comprehensive analysis shows that DistFormer outperforms existing methods. Moreover, we further delve into its generalization capabilities, showing its regularization benefits in zero-shot synth-to-real transfer.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的每个物体距离估计对于自动驾驶、监控和机器人等安全关键型应用至关重要。现有的方法依赖于两个尺度：局部信息（即边界框比例）或全局信息，它对场景的语义以及与相邻对象的空间关系进行编码。然而，这些方法可能难以处理远距离物体以及存在强烈遮挡或不寻常视觉模式的情况。在这方面，我们的工作旨在加强本地和全球线索。我们的架构——名为 DistFormer——建立在三个共同作用的主要组件之上： i) 一个强大的上下文编码器，提取细粒度的每个对象表示； ii) 一个掩码编码器-解码器模块，利用自我监督来促进每个对象有用特征的学习； iii) 全局细化模块，用于聚合对象表示并计算联合的、空间一致的估计。为了评估 DistFormer 的有效性，我们在标准 KITTI 数据集以及大规模 NuScenes 和 MOTSynth 数据集上进行了实验。此类数据集涵盖各种室内/室外环境、不断变化的天气条件、外观和相机视角。我们的综合分析表明 DistFormer 优于现有方法。此外，我们进一步深入研究了它的泛化能力，展示了它在零样本合成到真实传输中的正则化优势。</details>
**PDF:** <http://arxiv.org/pdf/2401.03191v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Preserving Silent Features for Domain Generalization**<br />
**Title_cn:** 保留静默特征以进行领域泛化<br />
**Authors:** Chujie Zhao, Tianren Zhang, Feng Chen<br />
**Abstract:** <details><summary>原文: </summary>Domain generalization (DG) aims to improve the generalization ability of the model trained on several known training domains over unseen test domains. Previous work has shown that self-supervised contrastive pre-training improves the robustness of the model on downstream tasks. However, in this paper, we find that self-supervised models do not exhibit better generalization performance than supervised models pre-trained on the same dataset in the DG setting. We argue that this is owing to the fact that the richer intra-class discriminative features extracted by self-supervised contrastive learning, which we term silent features, are suppressed during supervised fine-tuning. These silent features are likely to contain features that are more generalizable on the test domain. In this work, we model and analyze this feature suppression phenomenon and theoretically prove that preserving silent features can achieve lower expected test domain risk under certain conditions. In light of this, we propose a simple yet effective method termed STEP (Silent Feature Preservation) to improve the generalization performance of the self-supervised contrastive learning pre-trained model by alleviating the suppression of silent features during the supervised fine-tuning process. Experimental results show that STEP exhibits state-of-the-art performance on standard DG benchmarks with significant distribution shifts.</details>
**Abstract_cn:** <details><summary>译文: </summary>域泛化（DG）旨在提高在多个已知训练域上训练的模型相对于未见过的测试域的泛化能力。先前的工作表明，自监督对比预训练提高了模型在下游任务上的稳健性。然而，在本文中，我们发现自监督模型并没有表现出比在 DG 设置中的相同数据集上预训练的监督模型更好的泛化性能。我们认为这是由于自监督对比学习提取的更丰富的类内判别特征（我们称之为“沉默特征”）在监督微调过程中被抑制。这些静默功能可能包含在测试域上更通用的功能。在这项工作中，我们对这种特征抑制现象进行建模和分析，并从理论上证明，在某些条件下保留沉默特征可以实现较低的预期测试域风险。鉴于此，我们提出了一种简单而有效的方法，称为STEP（沉默特征保留），通过减轻监督微调过程中沉默特征的抑制来提高自监督对比学习预训练模型的泛化性能。实验结果表明，STEP 在标准 DG 基准上表现出最先进的性能，并具有显着的分布变化。</details>
**PDF:** <http://arxiv.org/pdf/2401.03170v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection**<br />
**Title_cn:** 用于 3D 工业异常检测的自监督特征适应<br />
**Authors:** Yuanpeng Tu, Boshen Zhang, Liang Liu, Yuxi Li, Chenhai Xu, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Cai Rong Zhao<br />
**Abstract:** <details><summary>原文: </summary>Industrial anomaly detection is generally addressed as an unsupervised task that aims at locating defects with only normal training samples. Recently, numerous 2D anomaly detection methods have been proposed and have achieved promising results, however, using only the 2D RGB data as input is not sufficient to identify imperceptible geometric surface anomalies. Hence, in this work, we focus on multi-modal anomaly detection. Specifically, we investigate early multi-modal approaches that attempted to utilize models pre-trained on large-scale visual datasets, i.e., ImageNet, to construct feature databases. And we empirically find that directly using these pre-trained models is not optimal, it can either fail to detect subtle defects or mistake abnormal features as normal ones. This may be attributed to the domain gap between target industrial data and source data.Towards this problem, we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method to finetune the adaptors and learn task-oriented representation toward anomaly detection.Both intra-modal adaptation and cross-modal alignment are optimized from a local-to-global perspective in LSFA to ensure the representation quality and consistency in the inference stage.Extensive experiments demonstrate that our method not only brings a significant performance boost to feature embedding based approaches, but also outperforms previous State-of-The-Art (SoTA) methods prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves 97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%.</details>
**Abstract_cn:** <details><summary>译文: </summary>工业异常检测通常被视为一项无监督任务，旨在仅使用正常训练样本来定位缺陷。最近，许多二维异常检测方法被提出并取得了可喜的结果，然而，仅使用二维 RGB 数据作为输入不足以识别难以察觉的几何表面异常。因此，在这项工作中，我们专注于多模式异常检测。具体来说，我们研究了早期的多模态方法，这些方法试图利用在大规模视觉数据集（即 ImageNet）上预先训练的模型来构建特征数据库。我们根据经验发现，直接使用这些预先训练的模型并不是最佳选择，它要么无法检测到细微的缺陷，要么将异常特征误认为是正常特征。这可能归因于目标工业数据和源数据之间的领域差距。针对这个问题，我们提出了一种局部到全局的自监督特征适应（LSFA）方法来微调适配器并学习面向任务的表示以进行异常检测LSFA中从局部到全局的角度对模内适应和跨模态对齐进行优化，以确保推理阶段的表示质量和一致性。大量实验表明，我们的方法不仅为特征带来了显着的性能提升基于嵌入的方法，而且在 MVTec-3D AD 和 Eyecandies 数据集上也显着优于以前的最先进 (SoTA) 方法，例如，LSFA 在 MVTec-3D 上实现了 97.1% I-AUROC，比以前的 SoTA 提高了 +3.4 %。</details>
**PDF:** <http://arxiv.org/pdf/2401.03145v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Spatiotemporally adaptive compression for scientific dataset with feature preservation -- a case study on simulation data with extreme climate events analysis**<br />
**Title_cn:** 具有特征保留的科学数据集时空自适应压缩——极端气候事件分析模拟数据案例研究<br />
**Authors:** Qian Gong, Chengzhu Zhang, Xin Liang, Viktor Reshniak, Jieyang Chen, Anand Rangarajan, Sanjay Ranka, Nicolas Vidal, Lipeng Wan, Paul Ullrich, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Scientific discoveries are increasingly constrained by limited storage space and I/O capacities. For time-series simulations and experiments, their data often need to be decimated over timesteps to accommodate storage and I/O limitations. In this paper, we propose a technique that addresses storage costs while improving post-analysis accuracy through spatiotemporal adaptive, error-controlled lossy compression. We investigate the trade-off between data precision and temporal output rates, revealing that reducing data precision and increasing timestep frequency lead to more accurate analysis outcomes. Additionally, we integrate spatiotemporal feature detection with data compression and demonstrate that performing adaptive error-bounded compression in higher dimensional space enables greater compression ratios, leveraging the error propagation theory of a transformation-based compressor.   To evaluate our approach, we conduct experiments using the well-known E3SM climate simulation code and apply our method to compress variables used for cyclone tracking. Our results show a significant reduction in storage size while enhancing the quality of cyclone tracking analysis, both quantitatively and qualitatively, in comparison to the prevalent timestep decimation approach. Compared to three state-of-the-art lossy compressors lacking feature preservation capabilities, our adaptive compression framework improves perfectly matched cases in TC tracking by 26.4-51.3% at medium compression ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11% computational overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>科学发现越来越受到有限的存储空间和 I/O 容量的限制。对于时间序列模拟和实验，它们的数据通常需要按时间步进行抽取，以适应存储和 I/O 限制。在本文中，我们提出了一种技术，该技术可以解决存储成本问题，同时通过时空自适应、错误控制的有损压缩来提高分析后的准确性。我们研究了数据精度和时间输出率之间的权衡，发现降低数据精度和增加时间步频率可以带来更准确的分析结果。此外，我们将时空特征检测与数据压缩相结合，并证明利用基于变换的压缩器的误差传播理论，在更高维空间中执行自适应误差有界压缩可以实现更大的压缩比。为了评估我们的方法，我们使用著名的 E3SM 气候模拟代码进行实验，并将我们的方法应用于压缩用于气旋跟踪的变量。我们的结果表明，与流行的时间步抽取方法相比，存储大小显着减小，同时定量和定性地提高了旋风跟踪分析的质量。与缺乏特征保留功能的三种最先进的有损压缩器相比，我们的自适应压缩框架在中等压缩比下将TC跟踪中的完美匹配情况提高了26.4-51.3％，在大压缩比下提高了77.3-571.1％，仅 5-11% 的计算开销。</details>
**PDF:** <http://arxiv.org/pdf/2401.03317v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT**<br />
**Title_cn:** 现实主义行动：使用 YOLOv8 和 DeiT 根据医学图像对脑肿瘤进行异常感知诊断<br />
**Authors:** Seyed Mohammad Hossein Hashemi, Leila Safari, Amirhossein Dadashzade Taromi<br />
**Abstract:** <details><summary>原文: </summary>In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we employed a novel performance evaluation method called Patient to Patient (PTP), focusing on the realistic evaluation of the model. In the detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor region. Subsequent testing and evaluation yielded competitive performance both in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer (ViT) model from a fine-tuned ResNet152 as a teacher in the classification phase. This approach demonstrates promising strides in reliable tumor detection and classification, offering potential advancements in tumor diagnosis for real-world medical imaging scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学领域，由于患者群体中肿瘤的罕见性，从图像中可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者预后至关重要。这项研究通过利用深度学习 (DL) 技术来检测和分类具有挑战性的情况下的脑肿瘤，从而解决了这个问题。来自国家脑图谱实验室 (NBML) 的精选数据集包含 81 名患者，其中 30 名肿瘤病例和 51 名正常病例。检测和分类管道分为两个连续的任务。检测阶段涉及全面的数据分析和预处理，以将图像样本的数量和每类患者的数量修改为异常分布（每 1 个肿瘤 9 个正常），以符合现实世界的场景。接下来，除了测试的通用评估指标之外，我们还采用了一种称为患者对患者（PTP）的新颖性能评估方法，重点关注模型的实际评估。在检测阶段，我们微调了YOLOv8n检测模型来检测肿瘤区域。随后的测试和评估在通用评估指标和 PTP 指标方面都取得了有竞争力的表现。此外，使用 Data Efficient Image Transformer (DeiT) 模块，我们从微调的 ResNet152 中提炼出 Vision Transformer (ViT) 模型，作为分类阶段的教师。这种方法在可靠的肿瘤检测和分类方面展示了有希望的进步，为现实世界的医学成像场景的肿瘤诊断提供了潜在的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.03302v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced Structural Inspection of Concrete Bridges**<br />
**Title_cn:** 结构异常的多视图 3D 实例分割，用于增强混凝土桥梁的结构检查<br />
**Authors:** Christian Benz, Volker Rodehorst<br />
**Abstract:** <details><summary>原文: </summary>For effective structural damage assessment, the instances of damages need to be localized in the world of a 3D model. Due to a lack of data, the detection of structural anomalies can currently not be directly learned and performed in 3D space. In this work, a three-stage approach is presented, which uses the good performance of detection models on image level to segment instances of anomalies in the 3D space. In the detection stage, semantic segmentation predictions are produced on image level. The mapping stage transfers the image-level prediction onto the respective point cloud. In the extraction stage, 3D anomaly instances are extracted from the segmented point cloud. Cloud contraction is used to transform cracks into their medial axis representation. For areal anomalies the bounding polygon is extracted by means of alpha shapes. The approach covers the classes crack, spalling, and corrosion and the three image-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are compared. Granted a localization tolerance of 4cm, IoUs of over 90% can be achieved for crack and corrosion and 41% for spalling, which appears to be a specifically challenging class. Detection on instance-level measured in AP is about 45% for crack and spalling and 73% for corrosion.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了进行有效的结构损伤评估，需要将损伤实例定位在 3D 模型的世界中。由于缺乏数据，目前无法在 3D 空间中直接学习和执行结构异常的检测。在这项工作中，提出了一种三阶段方法，该方法利用图像级别检测模型的良好性能来分割 3D 空间中的异常实例。在检测阶段，在图像级别产生语义分割预测。映射阶段将图像级预测传输到相应的点云上。在提取阶段，从分割的点云中提取 3D 异常实例。云收缩用于将裂纹转换为其中轴表示。对于区域异常，通过 alpha 形状提取边界多边形。该方法涵盖了裂纹、剥落和腐蚀等类别，并对三种图像级分割模型 TopoCrack、nnU-Net 和DetectionHMA 进行了比较。假设定位公差为 4 厘米，裂纹和腐蚀的 IoU 可以达到 90% 以上，剥落的 IoU 可以达到 41%，这似乎是一个特别具有挑战性的类别。在 AP 中测得的实例级检测中，裂纹和剥落的检测率约为 45%，腐蚀的检测率为 73%。</details>
**PDF:** <http://arxiv.org/pdf/2401.03298v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Real Time Human Detection by Unmanned Aerial Vehicles**<br />
**Title_cn:** 无人机实时人体检测<br />
**Authors:** Walid Guettala, Ali Sayah, Laid Kahloul, Ahmed Tibermacine<br />
**Abstract:** <details><summary>原文: </summary>One of the most important problems in computer vision and remote sensing is object detection, which identifies particular categories of diverse things in pictures. Two crucial data sources for public security are the thermal infrared (TIR) remote sensing multi-scenario photos and videos produced by unmanned aerial vehicles (UAVs). Due to the small scale of the target, complex scene information, low resolution relative to the viewable videos, and dearth of publicly available labeled datasets and training models, their object detection procedure is still difficult. A UAV TIR object detection framework for pictures and videos is suggested in this study. The Forward-looking Infrared (FLIR) cameras used to gather ground-based TIR photos and videos are used to create the ``You Only Look Once'' (YOLO) model, which is based on CNN architecture. Results indicated that in the validating task, detecting human object had an average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%, using YOLOv7 (YOLO version 7) state of the art model \cite{1}, while the detection speed around 161 frames per second (FPS/second). The usefulness of the YOLO architecture is demonstrated in the application, which evaluates the cross-detection performance of people in UAV TIR videos under a YOLOv7 model in terms of the various UAVs' observation angles. The qualitative and quantitative evaluation of object detection from TIR pictures and videos using deep-learning models is supported favorably by this work.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉和遥感中最重要的问题之一是目标检测，它可以识别图片中不同事物的特定类别。公共安全的两个重要数据源是无人机（UAV）产生的热红外（TIR）遥感多场景照片和视频。由于目标规模小、场景信息复杂、相对于可观看视频的分辨率较低，并且缺乏公开可用的标记数据集和训练模型，其目标检测过程仍然很困难。本研究提出了一种用于图片和视频的无人机 TIR 目标检测框架。用于收集地面 TIR 照片和视频的前视红外 (FLIR) 相机用于创建基于 CNN 架构的“You Only Look Once”(YOLO) 模型。结果表明，在验证任务中，使用 YOLOv7（YOLO 版本 7）最先进的模型\cite{1}，检测人体对象的平均精度为 IOU（Intersection over Union）= 0.5，即 72.5\%，而检测速度约为每秒 161 帧 (FPS/秒)。 YOLO 架构的实用性在该应用中得到了证明，该应用根据各种无人机的观察角度评估了 YOLOv7 模型下无人机 TIR 视频中人员的交叉检测性能。这项工作很好地支持了使用深度学习模型对 TIR 图片和视频进行目标检测的定性和定量评估。</details>
**PDF:** <http://arxiv.org/pdf/2401.03275v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Group Activity Recognition using Unreliable Tracked Pose**<br />
**Title_cn:** 使用不可靠的跟踪姿势进行群体活动识别<br />
**Authors:** Haritha Thilakarathne, Aiden Nibali, Zhen He, Stuart Morgan<br />
**Abstract:** <details><summary>原文: </summary>Group activity recognition in video is a complex task due to the need for a model to recognise the actions of all individuals in the video and their complex interactions. Recent studies propose that optimal performance is achieved by individually tracking each person and subsequently inputting the sequence of poses or cropped images/optical flow into a model. This helps the model to recognise what actions each person is performing before they are merged to arrive at the group action class. However, all previous models are highly reliant on high quality tracking and have only been evaluated using ground truth tracking information. In practice it is almost impossible to achieve highly reliable tracking information for all individuals in a group activity video. We introduce an innovative deep learning-based group activity recognition approach called Rendered Pose based Group Activity Recognition System (RePGARS) which is designed to be tolerant of unreliable tracking and pose information. Experimental results confirm that RePGARS outperforms all existing group activity recognition algorithms tested which do not use ground truth detection and tracking information.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频中的群体活动识别是一项复杂的任务，因为需要一个模型来识别视频中所有个体的行为及其复杂的交互。最近的研究提出，通过单独跟踪每个人并随后将姿势序列或裁剪图像/光流输入模型中来实现最佳性能。这有助于模型在合并到达群体动作类之前识别每个人正在执行什么动作。然而，之前的所有模型都高度依赖于高质量跟踪，并且仅使用地面实况跟踪信息进行评估。在实践中，几乎不可能获得群体活动视频中所有个体的高度可靠的跟踪信息。我们引入了一种基于深度学习的创新群体活动识别方法，称为基于渲染姿势的群体活动识别系统（RePGARS），该方法旨在容忍不可靠的跟踪和姿势信息。实验结果证实，RePGARS 优于所有已测试的现有群体活动识别算法，这些算法不使用地面实况检测和跟踪信息。</details>
**PDF:** <http://arxiv.org/pdf/2401.03262v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding**<br />
**Title_cn:** 3DMIT：用于场景理解的 3D 多模态指令调整<br />
**Authors:** Zeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yifan Xu, Ruifei Ma, Xiangde Liu<br />
**Abstract:** <details><summary>原文: </summary>The remarkable potential of multi-modal large language models (MLLMs) in comprehending both vision and language information has been widely acknowledged. However, the scarcity of 3D scenes-language pairs in comparison to their 2D counterparts, coupled with the inadequacy of existing approaches in understanding of 3D scenes by LLMs, poses a significant challenge. In response, we collect and construct an extensive dataset comprising 75K instruction-response pairs tailored for 3D scenes. This dataset addresses tasks related to 3D VQA, 3D grounding, and 3D conversation. To further enhance the integration of 3D spatial information into LLMs, we introduce a novel and efficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment stage between 3D scenes and language and extends the instruction prompt with the 3D modality information including the entire scene and segmented objects. We evaluate the effectiveness of our method across diverse tasks in the 3D scene domain and find that our approach serves as a strategic means to enrich LLMs' comprehension of the 3D world. Our code is available at https://github.com/staymylove/3DMIT.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在理解视觉和语言信息方面的巨大潜力已得到广泛认可。然而，与 2D 场景相比，3D 场景-语言对的稀缺性，加上法学硕士理解 3D 场景的现有方法的不足，构成了重大挑战。作为回应，我们收集并构建了一个广泛的数据集，其中包含专为 3D 场景定制的 75K 指令响应对。该数据集解决与 3D VQA、3D 基础和 3D 对话相关的任务。为了进一步增强 3D 空间信息与 LLM 的集成，我们引入了一种新颖且高效的提示调整范例 3DMIT。该范例消除了 3D 场景和语言之间的对齐阶段，并使用包括整个场景和分段对象的 3D 模态信息扩展了指令提示。我们评估了我们的方法在 3D 场景领域的不同任务中的有效性，发现我们的方法可以作为丰富法学硕士对 3D 世界的理解的战略手段。我们的代码可在 https://github.com/staymylove/3DMIT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03201v1><br />
**Code:** <https://github.com/staymylove/3DMIT>**<br />
>>**index:** 7<br />
**Title:** **Distribution-aware Interactive Attention Network and Large-scale Cloud Recognition Benchmark on FY-4A Satellite Image**<br />
**Title_cn:** FY-4A卫星图像上的分布感知交互式注意力网络和大规模云识别基准<br />
**Authors:** Jiaqing Zhang, Jie Lei, Weiying Xie, Kai Jiang, Mingxiang Cao, Yunsong Li<br />
**Abstract:** <details><summary>原文: </summary>Accurate cloud recognition and warning are crucial for various applications, including in-flight support, weather forecasting, and climate research. However, recent deep learning algorithms have predominantly focused on detecting cloud regions in satellite imagery, with insufficient attention to the specificity required for accurate cloud recognition. This limitation inspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes nine distinct cloud categories and uses precise domain adaptation methods to align 70,419 image-label pairs in terms of projection, temporal resolution, and spatial resolution, thereby facilitating the training of supervised deep learning networks. Given the complexity and diversity of cloud formations, we have thoroughly analyzed the challenges inherent to cloud recognition tasks, examining the intricate characteristics and distribution of the data. To effectively address these challenges, we designed a Distribution-aware Interactive-Attention Network (DIAnet), which preserves pixel-level details through a high-resolution branch and a parallel multi-resolution cross-branch. We also integrated a distribution-aware loss (DAL) to mitigate the imbalance across cloud categories. An Interactive Attention Module (IAM) further enhances the robustness of feature extraction combined with spatial and channel information. Empirical evaluations on the FYH dataset demonstrate that our method outperforms other cloud recognition networks, achieving superior performance in terms of mean Intersection over Union (mIoU). The code for implementing DIAnet is available at https://github.com/icey-zhang/DIAnet.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的云识别和预警对于各种应用至关重要，包括飞行支持、天气预报和气候研究。然而，最近的深度学习算法主要集中在检测卫星图像中的云区域，而对准确云识别所需的特异性关注不够。这一限制启发我们开发了新颖的 FY-4A-Himawari-8 (FYH) 数据集，其中包括九个不同的云类别，并使用精确的域适应方法在投影、时间分辨率和空间分辨率方面对齐 70,419 个图像标签对，从而促进监督深度学习网络的训练。鉴于云形成的复杂性和多样性，我们彻底分析了云识别任务固有的挑战，检查了数据的复杂特征和分布。为了有效应对这些挑战，我们设计了一个分布式感知交互式注意网络（DIAnet），它通过高分辨率分支和并行多分辨率交叉分支保留像素级细节。我们还集成了分布感知损失 (DAL)，以减轻云类别之间的不平衡。交互式注意力模块（IAM）进一步增强了特征提取与空间和通道信息相结合的鲁棒性。对 FYH 数据集的实证评估表明，我们的方法优于其他云识别网络，在平均交并集（mIoU）方面实现了卓越的性能。实现 DIAnet 的代码可在 https://github.com/icey-zhang/DIAnet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03182v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification**<br />
**Title_cn:** 多模态信息 ViT：高光谱和 LiDAR 分类的信息聚合和分发<br />
**Authors:** Jiaqing Zhang, Jie Lei, Weiying Xie, Geng Yang, Daixun Li, Yunsong Li, Karim Seghouane<br />
**Abstract:** <details><summary>原文: </summary>In multimodal land cover classification (MLCC), a common challenge is the redundancy in data distribution, where irrelevant information from multiple modalities can hinder the effective integration of their unique features. To tackle this, we introduce the Multimodal Informative Vit (MIVit), a system with an innovative information aggregate-distributing mechanism. This approach redefines redundancy levels and integrates performance-aware elements into the fused representation, facilitating the learning of semantics in both forward and backward directions. MIVit stands out by significantly reducing redundancy in the empirical distribution of each modality's separate and fused features. It employs oriented attention fusion (OAF) for extracting shallow local features across modalities in horizontal and vertical dimensions, and a Transformer feature extractor for extracting deep global features through long-range attention. We also propose an information aggregation constraint (IAC) based on mutual information, designed to remove redundant information and preserve complementary information within embedded features. Additionally, the information distribution flow (IDF) in MIVit enhances performance-awareness by distributing global classification information across different modalities' feature maps. This architecture also addresses missing modality challenges with lightweight independent modality classifiers, reducing the computational load typically associated with Transformers. Our results show that MIVit's bidirectional aggregate-distributing mechanism between modalities is highly effective, achieving an average overall accuracy of 95.56% across three multimodal datasets. This performance surpasses current state-of-the-art methods in MLCC. The code for MIVit is accessible at https://github.com/icey-zhang/MIViT.</details>
**Abstract_cn:** <details><summary>译文: </summary>在多模态土地覆盖分类（MLCC）中，一个常见的挑战是数据分布的冗余，来自多种模态的不相关信息可能会阻碍其独特特征的有效整合。为了解决这个问题，我们引入了多模态信息 Vit (MIVit)，这是一个具有创新信息聚合分发机制的系统。这种方法重新定义了冗余级别，并将性能感知元素集成到融合表示中，从而促进向前和向后方向的语义学习。 MIVit 的突出之处在于显着减少了每种模态的单独和融合特征的经验分布中的冗余。它采用定向注意力融合（OAF）来跨水平和垂直维度的模态提取浅层局部特征，并采用 Transformer 特征提取器通过远程注意力提取深层全局特征。我们还提出了一种基于互信息的信息聚合约束（IAC），旨在去除冗余信息并保留嵌入特征内的补充信息。此外，MIVit 中的信息分发流 (IDF) 通过跨不同模态的特征图分发全局分类信息来增强性能意识。该架构还通过轻量级独立模态分类器解决了缺失的模态挑战，减少了通常与 Transformer 相关的计算负载。我们的结果表明，MIVit 模态之间的双向聚合分配机制非常有效，在三个多模态数据集上实现了 95.56% 的平均总体准确率。该性能超越了 MLCC 中当前最先进的方法。 MIVit 的代码可在 https://github.com/icey-zhang/MIViT 访问。</details>
**PDF:** <http://arxiv.org/pdf/2401.03179v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks**<br />
**Title_cn:** 通过变分多模态超图网络进行文本视频检索<br />
**Authors:** Qian Li, Lixin Su, Jiashu Zhao, Long Xia, Hengyi Cai, Suqi Cheng, Hengzhu Tang, Junfeng Wang, Dawei Yin<br />
**Abstract:** <details><summary>原文: </summary>Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content. Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals. Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents. In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos. We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation modeling. By representing textual units and video frames as nodes and using hyperedges to depict their relationships, a multi-modal hypergraph is constructed. In this way, the query and the video can be aligned in a high-order semantic space. In addition, to enhance the model's generalization ability, the extracted features are fed into a variational inference component for computation, obtaining the variational representation under the Gaussian distribution. The incorporation of hypergraphs and variational inference allows our model to capture complex, n-ary interactions among textual and visual contents. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on the text-video retrieval task.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本视频检索是一项具有挑战性的任务，旨在根据文本查询识别相关视频。与传统的文本检索相比，文本视频检索的主要障碍是查询的文本性质与视频内容的视觉丰富性之间的语义差距。以前的工作主要集中在通过精细聚合字帧匹配信号来对齐查询和视频。受人类模块化判断文本与视频相关性的认知过程的启发，由于视频内容的连续性和复杂性，该判断需要高阶匹配信号。在本文中，我们提出了块级文本视频匹配，其中提取查询块来描述特定的检索单元，并将视频块分割成视频中的不同片段。我们将块级匹配制定为查询词和视频帧之间的 n 元相关性建模，并引入用于 n 元相关性建模的多模态超图。通过将文本单元和视频帧表示为节点并使用超边来描述它们的关系，构建了多模态超图。这样，查询和视频可以在高阶语义空间中对齐。此外，为了增强模型的泛化能力，将提取的特征输入变分推理组件进行计算，得到高斯分布下的变分表示。超图和变分推理的结合使我们的模型能够捕获文本和视觉内容之间复杂的、n 元的交互。实验结果表明，我们提出的方法在文本视频检索任务上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03177v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis**<br />
**Title_cn:** UGGNet：桥接 U-Net 和 VGG 进行高级乳腺癌诊断<br />
**Authors:** Tran Cao Minh, Nguyen Kim Quoc, Phan Cong Vinh, Dang Nhu Phu, Vuong Xuan Chi, Ha Minh Tan<br />
**Abstract:** <details><summary>原文: </summary>In the field of medical imaging, breast ultrasound has emerged as a crucial diagnostic tool for early detection of breast cancer. However, the accuracy of diagnosing the location of the affected area and the extent of the disease depends on the experience of the physician. In this paper, we propose a novel model called UGGNet, combining the power of the U-Net and VGG architectures to enhance the performance of breast ultrasound image analysis. The U-Net component of the model helps accurately segment the lesions, while the VGG component utilizes deep convolutional layers to extract features. The fusion of these two architectures in UGGNet aims to optimize both segmentation and feature representation, providing a comprehensive solution for accurate diagnosis in breast ultrasound images. Experimental results have demonstrated that the UGGNet model achieves a notable accuracy of 78.2% on the "Breast Ultrasound Images Dataset."</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学成像领域，乳腺超声已成为早期发现乳腺癌的重要诊断工具。然而，诊断患处位置和病变程度的准确性取决于医生的经验。在本文中，我们提出了一种名为 UGGNet 的新颖模型，结合了 U-Net 和 VGG 架构的强大功能来增强乳腺超声图像分析的性能。该模型的 U-Net 组件有助于准确分割病变，而 VGG 组件则利用深度卷积层来提取特征。 UGGNet 中这两种架构的融合旨在优化分割和特征表示，为乳腺超声图像的准确诊断提供全面的解决方案。实验结果表明，UGGNet 模型在“乳房超声图像数据集”上达到了 78.2% 的显着准确率。</details>
**PDF:** <http://arxiv.org/pdf/2401.03173v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **An Event-Oriented Diffusion-Refinement Method for Sparse Events Completion**<br />
**Title_cn:** 一种面向事件的稀疏事件完成的扩散细化方法<br />
**Authors:** Bo Zhang, Yuqi Han, Jinli Suo, Qionghai Dai<br />
**Abstract:** <details><summary>原文: </summary>Event cameras or dynamic vision sensors (DVS) record asynchronous response to brightness changes instead of conventional intensity frames, and feature ultra-high sensitivity at low bandwidth. The new mechanism demonstrates great advantages in challenging scenarios with fast motion and large dynamic range. However, the recorded events might be highly sparse due to either limited hardware bandwidth or extreme photon starvation in harsh environments. To unlock the full potential of event cameras, we propose an inventive event sequence completion approach conforming to the unique characteristics of event data in both the processing stage and the output form. Specifically, we treat event streams as 3D event clouds in the spatiotemporal domain, develop a diffusion-based generative model to generate dense clouds in a coarse-to-fine manner, and recover exact timestamps to maintain the temporal resolution of raw data successfully. To validate the effectiveness of our method comprehensively, we perform extensive experiments on three widely used public datasets with different spatial resolutions, and additionally collect a novel event dataset covering diverse scenarios with highly dynamic motions and under harsh illumination. Besides generating high-quality dense events, our method can benefit downstream applications such as object classification and intensity frame reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>事件摄像机或动态视觉传感器 (DVS) 记录对亮度变化的异步响应，而不是传统的强度帧，并在低带宽下具有超高灵敏度。新机制在快速运动和大动态范围的挑战性场景中表现出巨大的优势。然而，由于硬件带宽有限或恶劣环境中的极端光子饥饿，记录的事件可能非常稀疏。为了释放事件相机的全部潜力，我们提出了一种创造性的事件序列完成方法，该方法符合事件数据在处理阶段和输出形式的独特特征。具体来说，我们将事件流视为时空域中的 3D 事件云，开发基于扩散的生成模型以从粗到细的方式生成密集云，并恢复精确的时间戳以成功保持原始数据的时间分辨率。为了全面验证我们方法的有效性，我们对三个广泛使用的具有不同空间分辨率的公共数据集进行了广泛的实验，并另外收集了一个新颖的事件数据集，涵盖具有高度动态运动和严酷照明下的多种场景。除了生成高质量的密集事件之外，我们的方法还可以使下游应用受益，例如对象分类和强度帧重建。</details>
**PDF:** <http://arxiv.org/pdf/2401.03153v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Controllable Image Synthesis of Industrial Data Using Stable Diffusion**<br />
**Title_cn:** 使用稳定扩散的工业数据的可控图像合成<br />
**Authors:** Gabriele Valvano, Antonino Agostino, Giovanni De Magistris, Antonino Graziano, Giacomo Veneri<br />
**Abstract:** <details><summary>原文: </summary>Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach, we use the synthetic dataset to optimise a crack segmentor for a real industrial use case. When the available data is small, we observe considerable performance increase under several metrics, showing the method's potential in production environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>训练执行缺陷检测和分割的监督深度神经网络需要大规模的完全注释的数据集，这在工业环境中很难甚至不可能获得。生成式人工智能提供了人为扩大小型工业数据集的机会，从而能够在行业中使用最先进的监督方法。不幸的是，好的生成模型也需要大量数据来训练，而工业数据集通常很小。在这里，我们提出了一种新方法，可以在工业数据上重用通用的预训练生成模型，最终允许生成自标记的缺陷图像。首先，我们让模型学习新概念，从而带来新颖的数据分布。然后，我们迫使它学习调节生成过程，生成满足明确定义的拓扑特征并显示给定几何形状和位置的缺陷的工业图像。为了突出我们方法的优势，我们使用合成数据集来优化实际工业用例的裂纹分割器。当可用数据较小时，我们观察到多个指标下的性能显着提高，显示了该方法在生产环境中的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.03152v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Explicit Visual Prompts for Visual Object Tracking**<br />
**Title_cn:** 视觉对象跟踪的显式视觉提示<br />
**Authors:** Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Shengping Zhang, Xianxian Li<br />
**Abstract:** <details><summary>原文: </summary>How to effectively exploit spatio-temporal information is crucial to capture target appearance changes in visual tracking. However, most deep learning-based trackers mainly focus on designing a complicated appearance model or template updating strategy, while lacking the exploitation of context between consecutive frames and thus entailing the \textit{when-and-how-to-update} dilemma. To address these issues, we propose a novel explicit visual prompts framework for visual tracking, dubbed \textbf{EVPTrack}. Specifically, we utilize spatio-temporal tokens to propagate information between consecutive frames without focusing on updating templates. As a result, we cannot only alleviate the challenge of \textit{when-to-update}, but also avoid the hyper-parameters associated with updating strategies. Then, we utilize the spatio-temporal tokens to generate explicit visual prompts that facilitate inference in the current frame. The prompts are fed into a transformer encoder together with the image tokens without additional processing. Consequently, the efficiency of our model is improved by avoiding \textit{how-to-update}. In addition, we consider multi-scale information as explicit visual prompts, providing multiscale template features to enhance the EVPTrack's ability to handle target scale changes. Extensive experimental results on six benchmarks (i.e., LaSOT, LaSOT\rm $_{ext}$, GOT-10k, UAV123, TrackingNet, and TNL2K.) validate that our EVPTrack can achieve competitive performance at a real-time speed by effectively exploiting both spatio-temporal and multi-scale information. Code and models are available at https://github.com/GXNU-ZhongLab/EVPTrack.</details>
**Abstract_cn:** <details><summary>译文: </summary>如何有效利用时空信息对于视觉跟踪中捕获目标外观变化至关重要。然而，大多数基于深度学习的跟踪器主要专注于设计复杂的外观模型或模板更新策略，而缺乏对连续帧之间上下文的利用，从而导致 \textit{何时以及如何更新} 困境。为了解决这些问题，我们提出了一种用于视觉跟踪的新颖的显式视觉提示框架，称为 \textbf{EVPTrack}。具体来说，我们利用时空标记在连续帧之间传播信息，而不关注更新模板。因此，我们不仅可以缓解 \textit{when-to-update} 的挑战，而且还可以避免与更新策略相关的超参数。然后，我们利用时空标记生成明确的视觉提示，以促进当前帧中的推理。提示与图像标记一起输入转换器编码器，无需额外处理。因此，通过避免 \textit{how-to-update} 提高了我们模型的效率。此外，我们将多尺度信息视为显式视觉提示，提供多尺度模板功能以增强EVPTrack处理目标尺度变化的能力。六个基准测试（即 LaSOT、LaSOT\rm $_{ext}$、GOT-10k、UAV123、TrackingNet 和 TNL2K。）的广泛实验结果验证了我们的 EVPTrack 可以通过有效地利用实时速度实现有竞争力的性能时空和多尺度信息。代码和模型可在 https://github.com/GXNU-ZhongLab/EVPTrack 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03142v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from 3D MRI**<br />
**Title_cn:** 视觉 Transformers 和 Bi-LSTM 用于通过 3D MRI 诊断阿尔茨海默病<br />
**Authors:** Taymaz Akan, Sait Alp, Mohammad A. N Bhuiyanb<br />
**Abstract:** <details><summary>原文: </summary>Alzheimer's is a brain disease that gets worse over time and affects memory, thinking, and behavior. Alzheimer's disease (AD) can be treated and managed if it is diagnosed early, which can slow the progression of symptoms and improve quality of life. In this study, we suggested using the Visual Transformer (ViT) and bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used ViT to extract features from the MRI and then map them to a feature sequence. Then, we used Bi-LSTM sequence modeling to keep the interdependencies between related features. In addition, we evaluated the performance of the proposed model for the binary classification of AD patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our method against other deep learning models in the literature. The proposed method performs well in terms of accuracy, precision, F-score, and recall for the diagnosis of AD.</details>
**Abstract_cn:** <details><summary>译文: </summary>阿尔茨海默氏症是一种脑部疾病，随着时间的推移会变得更严重，并影响记忆、思维和行为。如果及早诊断，阿尔茨海默病 (AD) 可以得到治疗和控制，从而减缓症状的进展并提高生活质量。在这项研究中，我们建议使用 Visual Transformer (ViT) 和 bi-LSTM 处理 MRI 图像来诊断阿尔茨海默病。我们使用 ViT 从 MRI 中提取特征，然后将它们映射到特征序列。然后，我们使用 Bi-LSTM 序列建模来保持相关特征之间的相互依赖关系。此外，我们使用来自阿尔茨海默病神经影像计划 (ADNI) 的数据评估了所提出的 AD 患者二元分类模型的性能。最后，我们根据文献中的其他深度学习模型评估了我们的方法。所提出的方法在 AD 诊断的准确度、精确度、F 分数和召回率方面表现良好。</details>
**PDF:** <http://arxiv.org/pdf/2401.03132v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Transferable Learned Image Compression-Resistant Adversarial Perturbations**<br />
**Title_cn:** 可迁移的学习图像抗压缩对抗性扰动<br />
**Authors:** Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen<br />
**Abstract:** <details><summary>原文: </summary>Adversarial attacks can readily disrupt the image classification system, revealing the vulnerability of DNN-based recognition tasks. While existing adversarial perturbations are primarily applied to uncompressed images or compressed images by the traditional image compression method, i.e., JPEG, limited studies have investigated the robustness of models for image classification in the context of DNN-based image compression. With the rapid evolution of advanced image compression, DNN-based learned image compression has emerged as the promising approach for transmitting images in many security-critical applications, such as cloud-based face recognition and autonomous driving, due to its superior performance over traditional compression. Therefore, there is a pressing need to fully investigate the robustness of a classification system post-processed by learned image compression. To bridge this research gap, we explore the adversarial attack on a new pipeline that targets image classification models that utilize learned image compressors as pre-processing modules. Furthermore, to enhance the transferability of perturbations across various quality levels and architectures of learned image compression models, we introduce a saliency score-based sampling method to enable the fast generation of transferable perturbation. Extensive experiments with popular attack methods demonstrate the enhanced transferability of our proposed method when attacking images that have been post-processed with different learned image compression models.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性攻击很容易破坏图像分类系统，揭示基于 DNN 的识别任务的脆弱性。虽然现有的对抗性扰动主要应用于未压缩图像或通过传统图像压缩方法（即 JPEG）压缩的图像，但有限的研究调查了基于 DNN 的图像压缩背景下图像分类模型的鲁棒性。随着高级图像压缩的快速发展，基于 DNN 的学习图像压缩由于其优于传统压缩的性能，已成为许多安全关键应用（例如基于云的人脸识别和自动驾驶）中传输图像的有前途的方法。因此，迫切需要充分研究通过学习图像压缩进行后处理的分类系统的鲁棒性。为了弥补这一研究差距，我们探索了对新管道的对抗性攻击，该管道针对利用学习图像压缩器作为预处理模块的图像分类模型。此外，为了增强扰动在不同质量水平和学习图像压缩模型架构之间的可传递性，我们引入了一种基于显着性分数的采样方法，以实现可传递扰动的快速生成。对流行攻击方法的大量实验证明，在攻击使用不同学习的图像压缩模型进行后处理的图像时，我们提出的方法具有增强的可转移性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03115v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **ImageLab: Simplifying Image Processing Exploration for Novices and Experts Alike**<br />
**Title_cn:** ImageLab：为新手和专家简化图像处理探索<br />
**Authors:** Sahan Dissanayaka, Oshan Mudanayaka, Thilina Halloluwa, Chameera De Silva<br />
**Abstract:** <details><summary>原文: </summary>Image processing holds immense potential for societal benefit, yet its full potential is often accessible only to tech-savvy experts. Bridging this knowledge gap and providing accessible tools for users of all backgrounds remains an unexplored frontier. This paper introduces "ImageLab," a novel tool designed to democratize image processing, catering to both novices and experts by prioritizing interactive learning over theoretical complexity. ImageLab not only serves as a valuable educational resource but also offers a practical testing environment for seasoned practitioners. Through a comprehensive evaluation of ImageLab's features, we demonstrate its effectiveness through a user study done for a focused group of school children and university students which enables us to get positive feedback on the tool. Our work represents a significant stride toward enhancing image processing education and practice, making it more inclusive and approachable for all.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像处理具有巨大的社会效益潜力，但通常只有精通技术的专家才能充分发挥其潜力。弥合这一知识差距并为所有背景的用户提供易于使用的工具仍然是一个尚未探索的领域。本文介绍了“ImageLab”，这是一种旨在实现图像处理民主化的新颖工具，通过优先考虑交互式学习而不是理论复杂性来满足新手和专家的需求。 ImageLab 不仅是宝贵的教育资源，还为经验丰富的从业者提供了实用的测试环境。通过对 ImageLab 功能的全面评估，我们通过针对小学生和大学生的用户研究展示了其有效性，这使我们能够获得对该工具的积极反馈。我们的工作代表了在加强图像处理教育和实践方面迈出的重大一步，使其对所有人更具包容性和平易近人。</details>
**PDF:** <http://arxiv.org/pdf/2401.03157v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **A Physics-guided Generative AI Toolkit for Geophysical Monitoring**<br />
**Title_cn:** 用于地球物理监测的物理引导生成人工智能工具包<br />
**Authors:** Junhuan Yang, Hanchen Wang, Yi Sheng, Youzuo Lin, Lei Yang<br />
**Abstract:** <details><summary>原文: </summary>Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even worse during model pruning, a step essential in geoscience due to environmental complexities. To tackle this, we introduce the EdGeo toolkit, which employs a diffusion-based model guided by physics principles to generate high-fidelity velocity maps. The toolkit uses the acoustic wave equation to generate corresponding seismic waveform data, facilitating the fine-tuning of pruned ML models. Our results demonstrate significant improvements in SSIM scores and reduction in both MAE and MSE across various pruning ratios. Notably, the ML model fine-tuned using data generated by EdGeo yields superior quality of velocity maps, especially in representing unprivileged features, outperforming other existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>全波形反演（FWI）在地球科学探索地下方面发挥着至关重要的作用。它利用地震波对地下速度图进行成像。随着机器学习 (ML) 技术的发展，使用 ML 执行 FWI 任务的数据驱动方法已经出现，与传统的基于物理的方法相比，它提供了更高的准确性并降低了计算成本。然而，地球科学中的一个常见挑战，即非特权数据，严重限制了机器学习的有效性。由于环境的复杂性，模型修剪是地球科学中必不可少的一步，在模型修剪过程中，这个问题变得更加严重。为了解决这个问题，我们引入了 EdGeo 工具包，它采用物理原理指导的基于扩散的模型来生成高保真速度图。该工具包使用声波方程生成相应的地震波形数据，有助于对剪枝后的机器学习模型进行微调。我们的结果表明，在不同的剪枝率下，SSIM 分数显着提高，MAE 和 MSE 均降低。值得注意的是，使用 EdGeo 生成的数据进行微调的 ML 模型产生了卓越的速度图质量，特别是在表示非特权特征方面，优于其他现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.03131v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **RustNeRF: Robust Neural Radiance Field with Low-Quality Images**<br />
**Title_cn:** RustNeRF：具有低质量图像的鲁棒神经辐射场<br />
**Authors:** Mengfei Li, Ming Lu, Xiaofang Li, Shanghang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D consistency, achieving impressive results in 3D scene modeling and high-fidelity novel-view synthesis. However, there are limitations. First, existing methods assume enough high-quality images are available for training the NeRF model, ignoring real-world image degradation. Second, previous methods struggle with ambiguity in the training set due to unmodeled inconsistencies among different views. In this work, we present RustNeRF for real-world high-quality NeRF. To improve NeRF's robustness under real-world inputs, we train a 3D-aware preprocessing network that incorporates real-world degradation modeling. We propose a novel implicit multi-view guidance to address information loss during image degradation and restoration. Extensive experiments demonstrate RustNeRF's advantages over existing approaches under real-world degradation. The code will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近关于神经辐射场 (NeRF) 的工作利用了多视图 3D 一致性，在 3D 场景建模和高保真新颖视图合成方面取得了令人印象深刻的结果。然而，也有局限性。首先，现有方法假设有足够的高质量图像可用于训练 NeRF 模型，而忽略了现实世界的图像退化。其次，由于不同视图之间未建模的不一致，以前的方法在训练集中遇到了模糊性。在这项工作中，我们提出了用于现实世界高质量 NeRF 的 RustNeRF。为了提高 NeRF 在现实世界输入下的鲁棒性，我们训练了一个包含现实世界退化建模的 3D 感知预处理网络。我们提出了一种新颖的隐式多视图引导来解决图像退化和恢复过程中的信息丢失问题。大量实验证明了 RustNeRF 在实际退化情况下相对于现有方法的优势。代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.03257v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping**<br />
**Title_cn:** Hi-Map：用于高保真单目密集映射的分层分解辐射场<br />
**Authors:** Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce Hi-Map, a novel monocular dense mapping approach based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to achieve efficient and high-fidelity mapping using only posed RGB inputs. Our method eliminates the need for external depth priors derived from e.g., a depth estimation model. Our key idea is to represent the scene as a hierarchical feature grid that encodes the radiance and then factorizes it into feature planes and vectors. As such, the scene representation becomes simpler and more generalizable for fast and smooth convergence on new observations. This allows for efficient computation while alleviating noise patterns by reducing the complexity of the scene representation. Buttressed by the hierarchical factorized representation, we leverage the Sign Distance Field (SDF) as a proxy of rendering for inferring the volume density, demonstrating high mapping fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen the photometric cues and further boost the mapping quality, especially for the distant and textureless regions. Extensive experiments demonstrate our method's superiority in geometric and textural accuracy over the state-of-the-art NeRF-based monocular mapping methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 Hi-Map，一种基于神经辐射场（NeRF）的新型单目密集映射方法。 Hi-Map 具有出色的能力，仅使用摆好的 RGB 输入即可实现高效、高保真的映射。我们的方法消除了对来自深度估计模型等外部深度先验的需要。我们的关键思想是将场景表示为分层特征网格，对辐射进行编码，然后将其分解为特征平面和向量。因此，场景表示变得更简单、更通用，可以快速、平滑地收敛新的观察结果。这样可以实现高效计算，同时通过降低场景表示的复杂性来减轻噪声模式。在分层因式分解表示的支持下，我们利用符号距离场（SDF）作为渲染代理来推断体积密度，展示了高映射保真度。此外，我们引入了双路径编码策略来增强光度线索并进一步提高映射质量，特别是对于远处和无纹理的区域。大量实验证明我们的方法在几何和纹理精度方面优于最先进的基于 NeRF 的单目映射方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.03203v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Short-Time Fourier Transform for deblurring Variational Autoencoders**<br />
**Title_cn:** 用于去模糊变分自动编码器的短时傅立叶变换<br />
**Authors:** Vibhu Dalal<br />
**Abstract:** <details><summary>原文: </summary>Variational Autoencoders (VAEs) are powerful generative models, however their generated samples are known to suffer from a characteristic blurriness, as compared to the outputs of alternative generating techniques. Extensive research efforts have been made to tackle this problem, and several works have focused on modifying the reconstruction term of the evidence lower bound (ELBO). In particular, many have experimented with augmenting the reconstruction loss with losses in the frequency domain. Such loss functions usually employ the Fourier transform to explicitly penalise the lack of higher frequency components in the generated samples, which are responsible for sharp visual features. In this paper, we explore the aspects of previous such approaches which aren't well understood, and we propose an augmentation to the reconstruction term in response to them. Our reasoning leads us to use the short-time Fourier transform and to emphasise on local phase coherence between the input and output samples. We illustrate the potential of our proposed loss on the MNIST dataset by providing both qualitative and quantitative results.</details>
**Abstract_cn:** <details><summary>译文: </summary>变分自动编码器（VAE）是强大的生成模型，但与替代生成技术的输出相比，它们生成的样本存在特征模糊性。为了解决这个问题，人们进行了大量的研究工作，并且一些工作集中于修改证据下界的重建项（ELBO）。特别是，许多人已经尝试用频域中的损失来增加重建损失。此类损失函数通常采用傅立叶变换来明确惩罚生成样本中缺乏高频成分，这些成分导致了清晰的视觉特征。在本文中，我们探讨了先前此类方法尚未被充分理解的方面，并针对这些问题提出了对重建项的增强。我们的推理引导我们使用短时傅立叶变换并强调输入和输出样本之间的局部相位相干性。我们通过提供定性和定量结果来说明我们在 MNIST 数据集上提出的损失的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.03166v1><br />
**Code:** <https://github.com/Vibhu04/Deblurring-Variational-Autoencoders-with-STFT>**<br />
>>**index:** 2<br />
**Title:** **SAR Despeckling via Regional Denoising Diffusion Probabilistic Model**<br />
**Title_cn:** 通过区域去噪扩散概率模型进行 SAR 去斑<br />
**Authors:** Xuran Hu, Ziqiang Xu, Zhihan Chen, Zhengpeng Feng, Mingzhe Zhu, LJubisa Stankovic<br />
**Abstract:** <details><summary>原文: </summary>Speckle noise poses a significant challenge in maintaining the quality of synthetic aperture radar (SAR) images, so SAR despeckling techniques have drawn increasing attention. Despite the tremendous advancements of deep learning in fixed-scale SAR image despeckling, these methods still struggle to deal with large-scale SAR images. To address this problem, this paper introduces a novel despeckling approach termed Region Denoising Diffusion Probabilistic Model (R-DDPM) based on generative models. R-DDPM enables versatile despeckling of SAR images across various scales, accomplished within a single training session. Moreover, The artifacts in the fused SAR images can be avoided effectively with the utilization of region-guided inverse sampling. Experiments of our proposed R-DDPM on Sentinel-1 data demonstrates superior performance to existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>散斑噪声对维持合成孔径雷达（SAR）图像的质量提出了重大挑战，因此SAR去散斑技术引起了越来越多的关注。尽管深度学习在固定尺度 SAR 图像去斑方面取得了巨大进步，但这些方法仍然难以处理大规模 SAR 图像。为了解决这个问题，本文提出了一种新的去斑方法，称为基于生成模型的区域去噪扩散概率模型（R-DDPM）。 R-DDPM 能够在单个训练课程中完成各种尺度的 SAR 图像的多功能去斑。此外，利用区域引导逆采样可以有效避免融合SAR图像中的伪影。我们提出的 R-DDPM 在 Sentinel-1 数据上的实验证明了优于现有方法的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03122v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **CaMML: Context-Aware Multimodal Learner for Large Models**<br />
**Title_cn:** CaMML：大型模型的上下文感知多模态学习器<br />
**Authors:** Yixin Chen, Shuai Zhang, Boran Han, Tong He, Bo Li<br />
**Abstract:** <details><summary>原文: </summary>In this work, we introduce Context-Aware MultiModal Learner (CaMML), for tuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted to seamlessly integrate multimodal contextual samples into large models, thereby empowering the model to derive knowledge from analogous, domain-specific, up-to-date information and make grounded inferences. Importantly, CaMML is highly scalable and can efficiently handle lengthy multimodal context examples owing to its hierarchical design. Based on CaMML, we have developed two multimodal models, CaMML-7B and CaMML-13B, that have shown exceptional performance across an array of benchmark datasets for multimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art performance on over ten widely recognized multimodal benchmark datasets, surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any external resources. Moreover, we have conducted extensive ablative studies to inspect the inner workings of CaMML and performed qualitative analyses to showcase its effectiveness in handling real-world challenging cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们引入了上下文感知多模态学习器（CaMML），用于调整大型多模态模型（LMM）。 CaMML 是一个轻量级模块，旨在将多模态上下文样本无缝集成到大型模型中，从而使模型能够从类似的、特定领域的、最新的信息中获取知识，并做出有根据的推论。重要的是，CaMML 具有高度可扩展性，并且由于其分层设计，可以有效地处理冗长的多模式上下文示例。基于 CaMML，我们开发了两种多模态模型 CaMML-7B 和 CaMML-13B，它们在多模态任务的一系列基准数据集上表现出了卓越的性能。值得注意的是，CaMML-13B 在十多个广泛认可的多模态基准数据集上实现了最先进的性能，以显着的优势超越了 LLaVA-1.5 (13B)，而无需集成任何外部资源。此外，我们还进行了广泛的烧蚀研究来检查 CaMML 的内部运作，并进行定性分析以展示其在处理现实世界中具有挑战性的案例中的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03149v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models**<br />
**Title_cn:** 结合视觉专家解决多模态大语言模型中的信息丢失问题<br />
**Authors:** Xin He, Longhui Wei, Lingxi Xie, Qi Tian<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) are experiencing rapid growth, yielding a plethora of noteworthy contributions in recent months. The prevailing trend involves adopting data-driven methodologies, wherein diverse instruction-following datasets are collected. However, a prevailing challenge persists in these approaches, specifically in relation to the limited visual perception ability, as CLIP-like encoders employed for extracting visual information from inputs. Though these encoders are pre-trained on billions of image-text pairs, they still grapple with the information loss dilemma, given that textual captions only partially capture the contents depicted in images. To address this limitation, this paper proposes to improve the visual perception ability of MLLMs through a mixture-of-experts knowledge enhancement mechanism. Specifically, we introduce a novel method that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs. Extensive experiments have evaluated its effectiveness of advancing MLLMs, showcasing improved visual perception achieved through the integration of visual experts.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型 (MLLM) 正在经历快速增长，近几个月产生了大量值得注意的贡献。流行的趋势涉及采用数据驱动的方法，其中收集不同的指令跟踪数据集。然而，这些方法仍然存在一个普遍的挑战，特别是与有限的视觉感知能力有关，因为类 CLIP 编码器用于从输入中提取视觉信息。尽管这些编码器经过了数十亿图像-文本对的预训练，但由于文本字幕仅部分捕获了图像中描绘的内容，因此它们仍然面临着信息丢失的困境。为了解决这一限制，本文提出通过专家混合知识增强机制来提高 MLLM 的视觉感知能力。具体来说，我们引入了一种新颖的方法，将多任务编码器和视觉工具合并到现有的 MLLM 训练和推理管道中，旨在提供更全面、更准确的视觉输入摘要。大量实验评估了其推进 MLLM 的有效性，展示了通过视觉专家的整合实现的视觉感知的改善。</details>
**PDF:** <http://arxiv.org/pdf/2401.03105v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **MetaISP -- Exploiting Global Scene Structure for Accurate Multi-Device Color Rendition**<br />
**Title_cn:** MetaISP——利用全局场景结构实现准确的多设备色彩再现<br />
**Authors:** Matheus Souza, Wolfgang Heidrich<br />
**Abstract:** <details><summary>原文: </summary>Image signal processors (ISPs) are historically grown legacy software systems for reconstructing color images from noisy raw sensor measurements. Each smartphone manufacturer has developed its ISPs with its own characteristic heuristics for improving the color rendition, for example, skin tones and other visually essential colors. The recent interest in replacing the historically grown ISP systems with deep-learned pipelines to match DSLR's image quality improves structural features in the image. However, these works ignore the superior color processing based on semantic scene analysis that distinguishes mobile phone ISPs from DSLRs. Here, we present MetaISP, a single model designed to learn how to translate between the color and local contrast characteristics of different devices. MetaISP takes the RAW image from device A as input and translates it to RGB images that inherit the appearance characteristics of devices A, B, and C. We achieve this result by employing a lightweight deep learning technique that conditions its output appearance based on the device of interest. In this approach, we leverage novel attention mechanisms inspired by cross-covariance to learn global scene semantics. Additionally, we use the metadata that typically accompanies RAW images and estimate scene illuminants when they are unavailable.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像信号处理器 (ISP) 是历史上发展起来的传统软件系统，用于从嘈杂的原始传感器测量中重建彩色图像。每个智能手机制造商都开发了具有自己特色启发式的 ISP，以改善色彩再现，例如肤色和其他视觉上重要的颜色。最近人们热衷于用深度学习的管道取代历史上发展的 ISP 系统，以匹配 DSLR 的图像质量，从而改善图像的结构特征。然而，这些工作忽略了基于语义场景分析的卓越色彩处理，这是手机 ISP 与 DSLR 的区别。在这里，我们介绍 MetaISP，这是一个单一模型，旨在学习如何在不同设备的颜色和局部对比度特性之间进行转换。 MetaISP 将设备 A 的 RAW 图像作为输入，并将其转换为继承设备 A、B 和 C 的外观特征的 RGB 图像。我们通过采用轻量级深度学习技术来实现此结果，该技术根据设备调节其输出外观出于兴趣。在这种方法中，我们利用受交叉协方差启发的新颖注意机制来学习全局场景语义。此外，我们使用通常伴随 RAW 图像的元数据，并在场景光源不可用时估计场景光源。</details>
**PDF:** <http://arxiv.org/pdf/2401.03220v1><br />
**Code:** <https://github.com/vccimaging/MetaISP>**<br />
>>**index:** 2<br />
**Title:** **PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations**<br />
**Title_cn:** PosDiffNet：带扰动的大视场点云配准的位置神经扩散<br />
**Authors:** Rui She, Sijie Wang, Qiyu Kang, Kai Zhao, Yang Song, Wee Peng Tay, Tianyu Geng, Xingchao Jian<br />
**Abstract:** <details><summary>原文: </summary>Point cloud registration is a crucial technique in 3D computer vision with a wide range of applications. However, this task can be challenging, particularly in large fields of view with dynamic objects, environmental noise, or other perturbations. To address this challenge, we propose a model called PosDiffNet. Our approach performs hierarchical registration based on window-level, patch-level, and point-level correspondence. We leverage a graph neural partial differential equation (PDE) based on Beltrami flow to obtain high-dimensional features and position embeddings for point clouds. We incorporate position embeddings into a Transformer module based on a neural ordinary differential equation (ODE) to efficiently represent patches within points. We employ the multi-level correspondence derived from the high feature similarity scores to facilitate alignment between point clouds. Subsequently, we use registration methods such as SVD-based algorithms to predict the transformation using corresponding point pairs. We evaluate PosDiffNet on several 3D point cloud datasets, verifying that it achieves state-of-the-art (SOTA) performance for point cloud registration in large fields of view with perturbations. The implementation code of experiments is available at https://github.com/AI-IT-AVs/PosDiffNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云配准是3D计算机视觉中的一项关键技术，具有广泛的应用前景。然而，这项任务可能具有挑战性，特别是在具有动态物体、环境噪声或其他扰动的大视野中。为了应对这一挑战，我们提出了一个名为 PosDiffNet 的模型。我们的方法基于窗口级、补丁级和点级对应执行分层注册。我们利用基于 Beltrami 流的图神经偏微分方程（PDE）来获取点云的高维特征和位置嵌入。我们将位置嵌入合并到基于神经常微分方程（ODE）的 Transformer 模块中，以有效地表示点内的补丁。我们采用从高特征相似性得分得出的多级对应关系来促进点云之间的对齐。随后，我们使用基于 SVD 的算法等配准方法来使用相应的点对来预测变换。我们在多个 3D 点云数据集上评估 PosDiffNet，验证其在具有扰动的大视场中实现点云配准的最先进 (SOTA) 性能。实验的实现代码可以在https://github.com/AI-IT-AVs/PosDiffNet获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03167v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment Retargeting**<br />
**Title_cn:** Dress-Me-Up：用于自监督 3D 服装重定向的数据集和方法<br />
**Authors:** Shanthika Naik, Kunwar Singh, Astitva Srivastava, Dhawal Sirikonda, Amit Raj, Varun Jampani, Avinash Sharma<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel self-supervised framework for retargeting non-parameterized 3D garments onto 3D human avatars of arbitrary shapes and poses, enabling 3D virtual try-on (VTON). Existing self-supervised 3D retargeting methods only support parametric and canonical garments, which can only be draped over parametric body, e.g. SMPL. To facilitate the non-parametric garments and body, we propose a novel method that introduces Isomap Embedding based correspondences matching between the garment and the human body to get a coarse alignment between the two meshes. We perform neural refinement of the coarse alignment in a self-supervised setting. Further, we leverage a Laplacian detail integration method for preserving the inherent details of the input garment. For evaluating our 3D non-parametric garment retargeting framework, we propose a dataset of 255 real-world garments with realistic noise and topological deformations. The dataset contains $44$ unique garments worn by 15 different subjects in 5 distinctive poses, captured using a multi-view RGBD capture setup. We show superior retargeting quality on non-parametric garments and human avatars over existing state-of-the-art methods, acting as the first-ever baseline on the proposed dataset for non-parametric 3D garment retargeting.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的自我监督框架，用于将非参数化 3D 服装重新定位到任意形状和姿势的 3D 人体化身上，从而实现 3D 虚拟试穿 (VTON)。现有的自监督 3D 重定向方法仅支持参数化和规范的服装，这些服装只能覆盖在参数化的身体上，例如SMPL。为了促进非参数化服装和身体，我们提出了一种新方法，引入基于 Isomap Embedding 的服装和人体之间的对应匹配，以获得两个网格之间的粗略对齐。我们在自我监督的环境中对粗略对齐进行神经细化。此外，我们利用拉普拉斯细节集成方法来保留输入服装的固有细节。为了评估我们的 3D 非参数服装重定向框架，我们提出了一个包含 255 件真实世界服装的数据集，这些服装具有真实的噪声和拓扑变形。该数据集包含价值 44 美元的独特服装，由 15 名不同受试者以 5 种独特姿势穿着，使用多视图 RGBD 捕获设置捕获。我们在非参数服装和人体头像上展示了优于现有最先进方法的重定向质量，作为非参数 3D 服装重定向拟议数据集的第一个基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.03108v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Analysis and Validation of Image Search Engines in Histopathology**<br />
**Title_cn:** 组织病理学图像搜索引擎的分析和验证<br />
**Authors:** Isaiah Lahr, Saghir Alfasly, Peyman Nejat, Jibran Khan, Luke Kottom, Vaishnavi Kumbhar, Areej Alsaafin, Abubakr Shafique, Sobhan Hemati, Ghazal Alabtah, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Searching for similar images in archives of histology and histopathology images is a crucial task that may aid in patient matching for various purposes, ranging from triaging and diagnosis to prognosis and prediction. Whole slide images (WSIs) are highly detailed digital representations of tissue specimens mounted on glass slides. Matching WSI to WSI can serve as the critical method for patient matching. In this paper, we report extensive analysis and validation of four search methods bag of visual words (BoVW), Yottixel, SISH, RetCCL, and some of their potential variants. We analyze their algorithms and structures and assess their performance. For this evaluation, we utilized four internal datasets ($1269$ patients) and three public datasets ($1207$ patients), totaling more than $200,000$ patches from $38$ different classes/subtypes across five primary sites. Certain search engines, for example, BoVW, exhibit notable efficiency and speed but suffer from low accuracy. Conversely, search engines like Yottixel demonstrate efficiency and speed, providing moderately accurate results. Recent proposals, including SISH, display inefficiency and yield inconsistent outcomes, while alternatives like RetCCL prove inadequate in both accuracy and efficiency. Further research is imperative to address the dual aspects of accuracy and minimal storage requirements in histopathological image search.</details>
**Abstract_cn:** <details><summary>译文: </summary>在组织学和组织病理学图像档案中搜索相似图像是一项至关重要的任务，它可以帮助出于各种目的（从分类和诊断到预后和预测）进行患者匹配。全玻片图像 (WSI) 是载玻片上组织标本的高度详细的数字表示。将 WSI 与 WSI 进行匹配可以作为患者匹配的关键方法。在本文中，我们报告了对四种搜索方法 bag of Visual Words (BoVW)、Yottixel、SISH、RetCCL 及其一些潜在变体的广泛分析和验证。我们分析他们的算法和结构并评估他们的性能。在本次评估中，我们使用了四个内部数据集（1269 美元的患者）和三个公共数据集（1207 美元的患者），五个主要站点的 38 美元不同类别/亚型总计超过 200,000 美元的补丁。某些搜索引擎（例如 BoVW）表现出显着的效率和速度，但准确性较低。相反，Yottixel 等搜索引擎展示了效率和速度，提供了适度准确的结果。最近的提案（包括 SISH）显示出效率低下且产生不一致的结果，而 RetCCL 等替代方案在准确性和效率方面都被证明不足。进一步的研究必须解决组织病理学图像搜索中准确性和最小存储要求的双重问题。</details>
**PDF:** <http://arxiv.org/pdf/2401.03271v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Autonomous Navigation in Complex Environments**<br />
**Title_cn:** 复杂环境下的自主导航<br />
**Authors:** Andrew Gerstenslager, Jomol Lewis, Liam McKenna, Poorva Patel<br />
**Abstract:** <details><summary>原文: </summary>This paper explores the application of CNN-DNN network fusion to construct a robot navigation controller within a simulated environment. The simulated environment is constructed to model a subterranean rescue situation, such that an autonomous agent is tasked with finding a goal within an unknown cavernous system. Imitation learning is used to train the control algorithm to use LiDAR and camera data to navigate the space and find the goal. The trained model is then tested for robustness using Monte-Carlo.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文探讨了 CNN-DNN 网络融合在模拟环境中构建机器人导航控制器的应用。模拟环境的构建是为了模拟地下救援情况，以便自主代理的任务是在未知的洞穴系统中寻找目标。模仿学习用于训练控制算法，以使用激光雷达和摄像头数据来导航空间并找到目标。然后使用蒙特卡罗测试训练后的模型的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03267v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Interpersonal Relationship Analysis with Dyadic EEG Signals via Learning Spatial-Temporal Patterns**<br />
**Title_cn:** 通过学习时空模式进行二元脑电信号的人际关系分析<br />
**Authors:** Wenqi Ji, Fang liu, Xinxin Du, Niqi Liu, Chao Zhou, Mingjin Yu, Guozhen Zhao, Yong-Jin Liu<br />
**Abstract:** <details><summary>原文: </summary>Interpersonal relationship quality is pivotal in social and occupational contexts. Existing analysis of interpersonal relationships mostly rely on subjective self-reports, whereas objective quantification remains challenging. In this paper, we propose a novel social relationship analysis framework using spatio-temporal patterns derived from dyadic EEG signals, which can be applied to quantitatively measure team cooperation in corporate team building, and evaluate interpersonal dynamics between therapists and patients in psychiatric therapy. First, we constructed a dyadic-EEG dataset from 72 pairs of participants with two relationships (stranger or friend) when watching emotional videos simultaneously. Then we proposed a deep neural network on dyadic-subject EEG signals, in which we combine the dynamic graph convolutional neural network for characterizing the interpersonal relationships among the EEG channels and 1-dimension convolution for extracting the information from the time sequence. To obtain the feature vectors from two EEG recordings that well represent the relationship of two subjects, we integrate deep canonical correlation analysis and triplet loss for training the network. Experimental results show that the social relationship type (stranger or friend) between two individuals can be effectively identified through their EEG data.</details>
**Abstract_cn:** <details><summary>译文: </summary>人际关系质量在社会和职业环境中至关重要。现有的人际关系分析大多依赖于主观的自我报告，而客观的量化仍然具有挑战性。在本文中，我们提出了一种利用二元脑电图信号衍生的时空模式的新型社会关系分析框架，可用于定量测量企业团队建设中的团队合作，并评估精神治疗中治疗师和患者之间的人际动态。首先，我们从 72 对具有两种关系（陌生人或朋友）的参与者同时观看情感视频时构建了一个二元脑电图数据集。然后，我们提出了一种针对二元受试者脑电图信号的深度神经网络，其中我们结合了用于表征脑电图通道之间的人际关系的动态图卷积神经网络和用于从时间序列中提取信息的一维卷积。为了从两个脑电图记录中获得能够很好地代表两个受试者关系的特征向量，我们集成了深度典型相关分析和三元组损失来训练网络。实验结果表明，通过脑电图数据可以有效识别两个人之间的社会关系类型（陌生人或朋友）。</details>
**PDF:** <http://arxiv.org/pdf/2401.03250v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features**<br />
**Title_cn:** 使用迁移学习和时空特征构建高效的比特率阶梯<br />
**Authors:** Ali Falahati, Mohammad Karim Safavi, Ardavan Elahi, Farhad Pakdaman, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.</details>
**Abstract_cn:** <details><summary>译文: </summary>提供具有高效比特率的高质量视频是视频行业的主要挑战。传统的一刀切的比特率阶梯方案效率低下，并且由于需要大量编码，在计算上达到最佳内容感知决策是不切实际的。为了缓解这个问题，我们提出了一种使用迁移学习和时空特征的比特率和复杂性有效的比特率阶梯预测方法。我们建议：（1）使用来自著名的预训练 DNN 的特征图来预测有限训练数据的速率质量行为； (2) 通过预测最高质量的最小比特率并将其用于顶级梯级来提高最高质量梯级效率。在 102 个视频场景上测试的方法表明，与暴力破解相比，复杂性降低了 94.1%，BD 速率费用为 1.71%。此外，通过四个网络和消融研究对迁移学习进行了深入研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.03195v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing**<br />
**Title_cn:** MPN：利用多语言补丁神经元进行跨语言模型编辑<br />
**Authors:** Nianwen Si, Hao Zhang, Weiqiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring excessive modifications to the original methodology, thereby showcasing its user-friendly characteristics. Codes will be released soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型以编码大量事实知识而闻名，但由于外部信息不断变化的性质，它们常常变得过时。应对这一挑战的一个有希望的解决方案是利用模型编辑方法以有效的方式更新知识。然而，现有的模型编辑技术大多数仅限于单语言框架，无法解决多语言模型跨语言知识同步的关键问题。为了解决这个问题，我们提出了一种简单而有效的方法来训练多语言补丁神经元来存储跨语言知识。它可以轻松适应现有方法，以增强跨语言编辑能力。为了评估我们的方法，我们使用 XNLI 数据集和自行构建的 XFEVER 数据集进行实验。实验结果表明，我们提出的方法在跨语言编辑任务中实现了性能的提高，而不需要对原始方法进行过多修改，从而展示了其用户友好的特性。代码即将发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.03190v1><br />
**Code:** null<br />

