# !UPDATED  -- 2024-01-07

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems**<br />
**Title_cn:** 智慧城市中的大数据和深度学习：人工智能驱动的交通事故检测和计算机视觉系统的综合数据集<br />
**Authors:** Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Constantinos Zekios, Ahmed Abdelgawad, Magdy Bayoumi<br />
**Abstract:** <details><summary>原文: </summary>In the dynamic urban landscape, where the interplay of vehicles and pedestrians defines the rhythm of life, integrating advanced technology for safety and efficiency is increasingly crucial. This study delves into the application of cutting-edge technological methods in smart cities, focusing on enhancing public safety through improved traffic accident detection. Action recognition plays a pivotal role in interpreting visual data and tracking object motion such as human pose estimation in video sequences. The challenges of action recognition include variability in rapid actions, limited dataset, and environmental factors such as (Weather, Illumination, and Occlusions). In this paper, we present a novel comprehensive dataset for traffic accident detection. This datasets is specifically designed to bolster computer vision and action recognition systems in predicting and detecting road traffic accidents. We integrated datasets from wide variety of data sources, road networks, weather conditions, and regions across the globe. This approach is underpinned by empirical studies, aiming to contribute to the discourse on how technology can enhance the quality of life in densely populated areas. This research aims to bridge existing research gaps by introducing benchmark datasets that leverage state-of-the-art algorithms tailored for traffic accident detection in smart cities. These dataset is expected to advance academic research and also enhance real-time accident detection applications, contributing significantly to the evolution of smart urban environments. Our study marks a pivotal step towards safer, more efficient smart cities, harnessing the power of AI and machine learning to transform urban living.</details>
**Abstract_cn:** <details><summary>译文: </summary>在动态的城市景观中，车辆和行人的相互作用决定了生活节奏，集成先进技术以确保安全和效率变得越来越重要。这项研究深入探讨了尖端技术方法在智慧城市中的应用，重点是通过改进交通事故检测来增强公共安全。动作识别在解释视觉数据和跟踪对象运动（例如视频序列中的人体姿势估计）方面发挥着关键作用。动作识别的挑战包括快速动作的可变性、有限的数据集以及环境因素，例如（天气、照明和遮挡）。在本文中，我们提出了一个用于交通事故检测的新颖的综合数据集。该数据集专门设计用于支持计算机视觉和动作识别系统预测和检测道路交通事故。我们集成了来自全球各种数据源、道路网络、天气状况和地区的数据集。这种方法以实证研究为基础，旨在促进关于技术如何提高人口稠密地区的生活质量的讨论。这项研究旨在通过引入基准数据集来弥补现有的研究差距，这些基准数据集利用专为智慧城市交通事故检测量身定制的最先进算法。这些数据集预计将推进学术研究，并增强实时事故检测应用，为智能城市环境的发展做出重大贡献。我们的研究标志着迈向更安全、更高效的智慧城市的关键一步，利用人工智能和机器学习的力量来改变城市生活。</details>
**PDF:** <http://arxiv.org/pdf/2401.03587v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception**<br />
**Title_cn:** 看不见的反射：利用红外激光反射来实现交通标志感知<br />
**Authors:** Takami Sato, Sri Hrushikesh Varma Bhupathiraju, Michael Clifford, Takeshi Sugawara, Qi Alfred Chen, Sara Rampazzi<br />
**Abstract:** <details><summary>原文: </summary>All vehicles must follow the rules that govern traffic behavior, regardless of whether the vehicles are human-driven or Connected Autonomous Vehicles (CAVs). Road signs indicate locally active rules, such as speed limits and requirements to yield or stop. Recent research has demonstrated attacks, such as adding stickers or projected colored patches to signs, that cause CAV misinterpretation, resulting in potential safety issues. Humans can see and potentially defend against these attacks. But humans can not detect what they can not observe. We have developed an effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is designed to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification. In this work, we formulate the threat model and requirements for an ILR-based traffic sign perception attack to succeed. We evaluate the effectiveness of the ILR attack with real-world experiments against two major traffic sign recognition architectures on four IR-sensitive cameras. Our black-box optimization methodology allows the attack to achieve up to a 100% attack success rate in indoor, static scenarios and a >80.5% attack success rate in our outdoor, moving vehicle scenarios. We find the latest state-of-the-art certifiable defense is ineffective against ILR attacks as it mis-certifies >33.5% of cases. To address this, we propose a detection strategy based on the physical properties of IR laser reflections which can detect 96% of ILR attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>所有车辆都必须遵守交通行为规则，无论车辆是人力驾驶还是联网自动驾驶车辆 (CAV)。路标指示当地现行规则，例如速度限制和让行或停车要求。最近的研究表明，诸如在标志上添加贴纸或投影彩色补丁等攻击会导致 CAV 误解，从而导致潜在的安全问题。人类可以看到并可能防御这些攻击。但人类无法检测到他们无法观察到的东西。我们开发了一种有效的物理世界攻击，利用无滤镜图像传感器的灵敏度和人类不可见的红外激光反射（ILR）的特性。该攻击旨在影响 CAV 摄像头和感知，通过诱导错误分类来破坏交通标志识别。在这项工作中，我们制定了基于 ILR 的交通标志感知攻击成功的威胁模型和要求。我们通过针对四个红外敏感摄像机上的两种主要交通标志识别架构的真实实验来评估 ILR 攻击的有效性。我们的黑盒优化方法允许攻击在室内、静态场景中实现高达 100% 的攻击成功率，在室外、移动车辆场景中实现 >80.5% 的攻击成功率。我们发现最新的、最先进的可认证防御对于 ILR 攻击无效，因为它对超过 33.5% 的案例进行了错误认证。为了解决这个问题，我们提出了一种基于红外激光反射物理特性的检测策略，可以检测 96% 的 ILR 攻击。</details>
**PDF:** <http://arxiv.org/pdf/2401.03582v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **SeTformer is What You Need for Vision and Language**<br />
**Title_cn:** SeTformer 是您视觉和语言所需的工具<br />
**Authors:** Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Michael Felsberg<br />
**Abstract:** <details><summary>原文: </summary>The dot product self-attention (DPSA) is a fundamental component of transformers. However, scaling them to long sequences, like documents or high-resolution images, becomes prohibitively expensive due to quadratic time and memory complexities arising from the softmax operation. Kernel methods are employed to simplify computations by approximating softmax but often lead to performance drops compared to softmax attention. We propose SeTformer, a novel transformer, where DPSA is purely replaced by Self-optimal Transport (SeT) for achieving better performance and computational efficiency. SeT is based on two essential softmax properties: maintaining a non-negative attention matrix and using a nonlinear reweighting mechanism to emphasize important tokens in input sequences. By introducing a kernel cost function for optimal transport, SeTformer effectively satisfies these properties. In particular, with small and basesized models, SeTformer achieves impressive top-1 accuracies of 84.7% and 86.2% on ImageNet-1K. In object detection, SeTformer-base outperforms the FocalNet counterpart by +2.2 mAP, using 38% fewer parameters and 29% fewer FLOPs. In semantic segmentation, our base-size model surpasses NAT by +3.5 mIoU with 33% fewer parameters. SeTformer also achieves state-of-the-art results in language modeling on the GLUE benchmark. These findings highlight SeTformer's applicability in vision and language tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>点积自注意力（DPSA）是 Transformer 的基本组成部分。然而，由于 softmax 操作产生的二次时间和内存复杂性，将它们缩放到长序列（如文档或高分辨率图像）变得非常昂贵。核方法用于通过近似 softmax 来简化计算，但与 softmax 注意力相比，通常会导致性能下降。我们提出了 SeTformer，一种新颖的转换器，其中 DPSA 完全被自优化传输（SeT）取代，以实现更好的性能和计算效率。 SeT 基于两个基本的 softmax 属性：维护非负注意力矩阵并使用非线性重新加权机制来强调输入序列中的重要标记。通过引入用于最佳传输的核成本函数，SeTformer 有效地满足了这些属性。特别是，对于小型和基本尺寸的模型，SeTformer 在 ImageNet-1K 上实现了令人印象深刻的 top-1 准确率，分别为 84.7% 和 86.2%。在目标检测中，SeTformer-base 的性能比 FocalNet 的对应版本高出 +2.2 mAP，使用的参数减少了 38%，FLOP 减少了 29%。在语义分割中，我们的基本大小模型比 NAT 多出 +3.5 mIoU，参数减少了 33%。 SeTformer 还在 GLUE 基准上的语言建模方面取得了最先进的结果。这些发现强调了 SeTformer 在视觉和语言任务中的适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03540v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Text-Driven Traffic Anomaly Detection with Temporal High-Frequency Modeling in Driving Videos**<br />
**Title_cn:** 在驾驶视频中使用时间高频建模进行文本驱动的交通异常检测<br />
**Authors:** Rongqin Liang, Yuanman Li, Jiantao Zhou, Xia Li<br />
**Abstract:** <details><summary>原文: </summary>Traffic anomaly detection (TAD) in driving videos is critical for ensuring the safety of autonomous driving and advanced driver assistance systems. Previous single-stage TAD methods primarily rely on frame prediction, making them vulnerable to interference from dynamic backgrounds induced by the rapid movement of the dashboard camera. While two-stage TAD methods appear to be a natural solution to mitigate such interference by pre-extracting background-independent features (such as bounding boxes and optical flow) using perceptual algorithms, they are susceptible to the performance of first-stage perceptual algorithms and may result in error propagation. In this paper, we introduce TTHF, a novel single-stage method aligning video clips with text prompts, offering a new perspective on traffic anomaly detection. Unlike previous approaches, the supervised signal of our method is derived from languages rather than orthogonal one-hot vectors, providing a more comprehensive representation. Further, concerning visual representation, we propose to model the high frequency of driving videos in the temporal domain. This modeling captures the dynamic changes of driving scenes, enhances the perception of driving behavior, and significantly improves the detection of traffic anomalies. In addition, to better perceive various types of traffic anomalies, we carefully design an attentive anomaly focusing mechanism that visually and linguistically guides the model to adaptively focus on the visual context of interest, thereby facilitating the detection of traffic anomalies. It is shown that our proposed TTHF achieves promising performance, outperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and achieving high generalization on the DADA dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>驾驶视频中的交通异常检测（TAD）对于确保自动驾驶和高级驾驶辅助系统的安全至关重要。以前的单阶段 TAD 方法主要依赖于帧预测，这使得它们容易受到行车记录仪快速移动引起的动态背景的干扰。虽然两阶段 TAD 方法似乎是通过使用感知算法预先提取与背景无关的特征（例如边界框和光流）来减轻此类干扰的自然解决方案，但它们容易受到第一阶段感知算法性能的影响，并且可能会导致错误传播。在本文中，我们介绍了 TTHF，一种新颖的单阶段方法，将视频剪辑与文本提示对齐，为交通异常检测提供了新的视角。与以前的方法不同，我们方法的监督信号是从语言而不是正交的单热向量中导出的，提供了更全面的表示。此外，关于视觉表示，我们建议在时域中对驾驶视频的高频进行建模。该模型捕捉驾驶场景的动态变化，增强驾驶行为的感知，显着提高交通异常的检测能力。此外，为了更好地感知各种类型的交通异常，我们精心设计了一种细心的异常聚焦机制，从视觉和语言上引导模型自适应地聚焦于感兴趣的视觉上下文，从而促进交通异常的检测。结果表明，我们提出的 TTHF 取得了令人鼓舞的性能，在 DoTA 数据集上比最先进的竞争对手高出 +5.4% AUC，并在 DADA 数据集上实现了高度泛化。</details>
**PDF:** <http://arxiv.org/pdf/2401.03522v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production**<br />
**Title_cn:** Re:Draw——语境感知翻译作为艺术生产的可控方法<br />
**Authors:** Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer<br />
**Abstract:** <details><summary>原文: </summary>We introduce context-aware translation, a novel method that combines the benefits of inpainting and image-to-image translation, respecting simultaneously the original input and contextual relevance -- where existing methods fall short. By doing so, our method opens new avenues for the controllable use of AI within artistic creation, from animation to digital art.   As an use case, we apply our method to redraw any hand-drawn animated character eyes based on any design specifications - eyes serve as a focal point that captures viewer attention and conveys a range of emotions, however, the labor-intensive nature of traditional animation often leads to compromises in the complexity and consistency of eye design. Furthermore, we remove the need for production data for training and introduce a new character recognition method that surpasses existing work by not requiring fine-tuning to specific productions. This proposed use case could help maintain consistency throughout production and unlock bolder and more detailed design choices without the production cost drawbacks. A user study shows context-aware translation is preferred over existing work 95.16% of the time.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了上下文感知翻译，这是一种结合了修复和图像到图像翻译的优点的新颖方法，同时尊重原始输入和上下文相关性——这是现有方法的不足之处。通过这样做，我们的方法为从动画到数字艺术的艺术创作中可控地使用人工智能开辟了新的途径。作为一个用例，我们应用我们的方法根据任何设计规范重新绘制任何手绘动画角色的眼睛 - 眼睛作为一个焦点，吸引观众的注意力并传达一系列情感，然而，传统的劳动密集型性质动画通常会导致眼睛设计的复杂性和一致性受到损害。此外，我们不再需要用于训练的生产数据，并引入了一种新的字符识别方法，该方法不需要对特定生产进行微调，从而超越了现有的工作。这个提议的用例可以帮助在整个生产过程中保持一致性，并解锁更大胆、更详细的设计选择，而不会产生生产成本缺陷。一项用户研究表明，95.16% 的情况下，上下文感知翻译优于现有工作。</details>
**PDF:** <http://arxiv.org/pdf/2401.03499v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions**<br />
**Title_cn:** 医学图像分割的 Segment Anything 模型：当前应用和未来方向<br />
**Authors:** Yichi Zhang, Zhenrong Shen, Rushi Jiao<br />
**Abstract:** <details><summary>原文: </summary>Due to the inherent flexibility of prompting, foundation models have emerged as the predominant force in the fields of natural language processing and computer vision. The recent introduction of the Segment Anything Model (SAM) signifies a noteworthy expansion of the prompt-driven paradigm into the domain of image segmentation, thereby introducing a plethora of previously unexplored capabilities. However, the viability of its application to medical image segmentation remains uncertain, given the substantial distinctions between natural and medical images. In this work, we provide a comprehensive overview of recent endeavors aimed at extending the efficacy of SAM to medical image segmentation tasks, encompassing both empirical benchmarking and methodological adaptations. Additionally, we explore potential avenues for future research directions in SAM's role within medical image segmentation. While direct application of SAM to medical image segmentation does not yield satisfactory performance on multi-modal and multi-target medical datasets so far, numerous insights gleaned from these efforts serve as valuable guidance for shaping the trajectory of foundational models in the realm of medical image analysis. To support ongoing research endeavors, we maintain an active repository that contains an up-to-date paper list and a succinct summary of open-source projects at https://github.com/YichiZhang98/SAM4MIS.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于提示固有的灵活性，基础模型已成为自然语言处理和计算机视觉领域的主导力量。最近推出的分段任意模型 (SAM) 标志着提示驱动范式在图像分割领域的显着扩展，从而引入了大量以前未开发的功能。然而，考虑到自然图像和医学图像之间的巨大区别，其应用于医学图像分割的可行性仍然不确定。在这项工作中，我们全面概述了近期旨在将 SAM 的功效扩展到医学图像分割任务的努力，包括经验基准测试和方法适应。此外，我们还探讨了 SAM 在医学图像分割中的作用的未来研究方向的潜在途径。虽然迄今为止，将 SAM 直接应用于医学图像分割并没有在多模态和多目标医学数据集上产生令人满意的性能，但从这些工作中收集到的大量见解可以为塑造医学图像领域基础模型的轨迹提供宝贵的指导。分析。为了支持正在进行的研究工作，我们维护一个活跃的存储库，其中包含最新的论文列表和开源项目的简洁摘要，网址为 https://github.com/YichiZhang98/SAM4MIS。</details>
**PDF:** <http://arxiv.org/pdf/2401.03495v1><br />
**Code:** <https://github.com/yichizhang98/sam4mis>**<br />
>>**index:** 7<br />
**Title:** **A Classification of Critical Configurations for any Number of Projective Views**<br />
**Title_cn:** 任意数量的投影视图的关键配置的分类<br />
**Authors:** Martin Bråtelund<br />
**Abstract:** <details><summary>原文: </summary>Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.</details>
**Abstract_cn:** <details><summary>译文: </summary>运动结构是从一组图像中恢复有关摄像机和 3D 场景的信息的过程。一般来说，在无噪声的情况下，如果提供足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，唯一的恢复也是不可能的；这些称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明它们形成了众所周知的代数簇，例如二次曲面和次数最多为 4 的曲线。本文还通过发现以前未知的关键构型并表明一些以前认为是关键的构型在早期结果的基础上进行了改进。事实上不是。</details>
**PDF:** <http://arxiv.org/pdf/2401.03450v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Bilateral Reference for High-Resolution Dichotomous Image Segmentation**<br />
**Title_cn:** 高分辨率二分图像分割的双边参考<br />
**Authors:** Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, Nicu Sebe<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel bilateral reference framework (***BiRefNet***) for high-resolution dichotomous image segmentation (DIS). It comprises two essential components: the localization module (LM) and the reconstruction module (RM) with our proposed bilateral reference (BiRef). The LM aids in object localization using global semantic information. Within the RM, we utilize BiRef for the reconstruction process, where hierarchical patches of images provide the source reference and gradient maps serve as the target reference. These components collaborate to generate the final predicted maps. We also introduce auxiliary gradient supervision to enhance focus on regions with finer details. Furthermore, we outline practical training strategies tailored for DIS to improve map quality and training process. To validate the general applicability of our approach, we conduct extensive experiments on four tasks to evince that *BiRefNet* exhibits remarkable performance, outperforming task-specific cutting-edge methods across all benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种用于高分辨率二分图像分割（DIS）的新颖双边参考框架（***BiRefNet***）。它包括两个基本组件：定位模块（LM）和带有我们提出的双边参考（BiRef）的重建模块（RM）。 LM 使用全局语义信息帮助对象定位。在 RM 中，我们利用 BiRef 进行重建过程，其中分层图像块提供源参考，梯度图作为目标参考。这些组件协作生成最终的预测图。我们还引入了辅助梯度监督来增强对细节更精细的区域的关注。此外，我们概述了为 DIS 量身定制的实用培训策略，以提高地图质量和培训过程。为了验证我们方法的普遍适用性，我们对四项任务进行了广泛的实验，以证明 *BiRefNet* 表现出卓越的性能，在所有基准测试中都优于特定于任务的尖端方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.03407v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **conv_einsum: A Framework for Representation and Fast Evaluation of Multilinear Operations in Convolutional Tensorial Neural Networks**<br />
**Title_cn:** conv_einsum：卷积张量神经网络中多线性运算的表示和快速评估框架<br />
**Authors:** Tahseen Rabbani, Jiahao Su, Xiaoyu Liu, David Chan, Geoffrey Sangston, Furong Huang<br />
**Abstract:** <details><summary>原文: </summary>Modern ConvNets continue to achieve state-of-the-art results over a vast array of vision and image classification tasks, but at the cost of increasing parameters. One strategy for compactifying a network without sacrificing much expressive power is to reshape it into a tensorial neural network (TNN), which is a higher-order tensorization of its layers, followed by a factorization, such as a CP-decomposition, which strips a weight down to its critical basis components. Passes through TNNs can be represented as sequences of multilinear operations (MLOs), where the evaluation path can greatly affect the number of floating point operations (FLOPs) incurred. While functions such as the popular einsum can evaluate simple MLOs such as contractions, existing implementations cannot process multi-way convolutions, resulting in scant assessments of how optimal evaluation paths through tensorized convolutional layers can improve training speed. In this paper, we develop a unifying framework for representing tensorial convolution layers as einsum-like strings and a meta-algorithm conv_einsum which is able to evaluate these strings in a FLOPs-minimizing manner. Comprehensive experiments, using our open-source implementation, over a wide range of models, tensor decompositions, and diverse tasks, demonstrate that conv_einsum significantly increases both computational and memory-efficiency of convolutional TNNs.</details>
**Abstract_cn:** <details><summary>译文: </summary>现代卷积网络继续在大量视觉和图像分类任务中取得最先进的结果，但代价是增加参数。在不牺牲太多表达能力的情况下压缩网络的一种策略是将其重塑为张量神经网络 (TNN)，这是其各层的高阶张量化，然后进行分解，例如 CP 分解，它剥离了重量降至其关键的基础组件。通过 TNN 的传递可以表示为多线性运算 (MLO) 序列，其中评估路径可以极大地影响所产生的浮点运算 (FLOP) 数量。虽然流行的 einsum 等函数可以评估简单的 MLO（例如收缩），但现有的实现无法处理多路卷积，从而导致很少评估通过张量化卷积层的最佳评估路径如何提高训练速度。在本文中，我们开发了一个统一框架，用于将张量卷积层表示为类似 einsum 的字符串，以及一个元算法 conv_einsum，它能够以 FLOPs 最小化的方式评估这些字符串。使用我们的开源实现，对各种模型、张量分解和不同的任务进行的综合实验表明，conv_einsum 显着提高了卷积 TNN 的计算和内存效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.03384v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning**<br />
**Title_cn:** BCLNet：双视图对应剪枝的双边共识学习<br />
**Authors:** Xiangyang Miao, Guobao Xiao, Shiping Wang, Jun Yu<br />
**Abstract:** <details><summary>原文: </summary>Correspondence pruning aims to establish reliable correspondences between two related images and recover relative camera motion. Existing approaches often employ a progressive strategy to handle the local and global contexts, with a prominent emphasis on transitioning from local to global, resulting in the neglect of interactions between different contexts. To tackle this issue, we propose a parallel context learning strategy that involves acquiring bilateral consensus for the two-view correspondence pruning task. In our approach, we design a distinctive self-attention block to capture global context and parallel process it with the established local context learning module, which enables us to simultaneously capture both local and global consensuses. By combining these local and global consensuses, we derive the required bilateral consensus. We also design a recalibration block, reducing the influence of erroneous consensus information and enhancing the robustness of the model. The culmination of our efforts is the Bilateral Consensus Learning Network (BCLNet), which efficiently estimates camera pose and identifies inliers (true correspondences). Extensive experiments results demonstrate that our network not only surpasses state-of-the-art methods on benchmark datasets but also showcases robust generalization abilities across various feature extraction techniques. Noteworthily, BCLNet obtains 3.98\% mAP5$^{\circ}$ gains over the second best method on unknown outdoor dataset, and obviously accelerates model training speed. The source code will be available at: https://github.com/guobaoxiao/BCLNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>对应修剪旨在在两个相关图像之间建立可靠的对应关系并恢复相对相机运动。现有的方法往往采用渐进策略来处理局部和全球背景，突出强调从局部到全球的过渡，导致忽视不同背景之间的相互作用。为了解决这个问题，我们提出了一种并行上下文学习策略，其中涉及为双视图对应修剪任务获取双边共识。在我们的方法中，我们设计了一个独特的自注意力块来捕获全局上下文，并与已建立的本地上下文学习模块并行处理它，这使我们能够同时捕获本地和全球共识。通过结合这些本地和全球共识，我们得出所需的双边共识。我们还设计了一个重新校准块，减少错误共识信息的影响并增强模型的鲁棒性。我们努力的巅峰是双边共识学习网络（BCLNet），它可以有效地估计相机姿势并识别内部点（真实对应）。大量的实验结果表明，我们的网络不仅超越了基准数据集上最先进的方法，而且还展示了跨各种特征提取技术的强大泛化能力。值得注意的是，BCLNet 在未知的室外数据集上比第二好的方法获得了 3.98\% mAP5$^{\circ}$ 的增益，并且明显加快了模型训练速度。源代码位于：https://github.com/guobaoxiao/BCLNet。</details>
**PDF:** <http://arxiv.org/pdf/2401.03459v1><br />
**Code:** <https://github.com/guobaoxiao/BCLNet>**<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **SpecRef: A Fast Training-free Baseline of Specific Reference-Condition Real Image Editing**<br />
**Title_cn:** SpecRef：特定参考条件真实图像编辑的快速免训练基线<br />
**Authors:** Songyan Chen, Jiancheng Huang<br />
**Abstract:** <details><summary>原文: </summary>Text-conditional image editing based on large diffusion generative model has attracted the attention of both the industry and the research community. Most existing methods are non-reference editing, with the user only able to provide a source image and text prompt. However, it restricts user's control over the characteristics of editing outcome. To increase user freedom, we propose a new task called Specific Reference Condition Real Image Editing, which allows user to provide a reference image to further control the outcome, such as replacing an object with a particular one. To accomplish this, we propose a fast baseline method named SpecRef. Specifically, we design a Specific Reference Attention Controller to incorporate features from the reference image, and adopt a mask mechanism to prevent interference between editing and non-editing regions. We evaluate SpecRef on typical editing tasks and show that it can achieve satisfactory performance. The source code is available on https://github.com/jingjiqinggong/specp2p.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于大扩散生成模型的文本条件图像编辑引起了工业界和研究界的关注。大多数现有方法都是非参考编辑，用户只能提供源图像和文本提示。然而，它限制了用户对编辑结果特征的控制。为了增加用户的自由度，我们提出了一项名为“特定参考条件真实图像编辑”的新任务，它允许用户提供参考图像来进一步控制结果，例如用特定的对象替换对象。为了实现这一目标，我们提出了一种名为 SpecRef 的快速基线方法。具体来说，我们设计了一个特定参考注意控制器来合并参考图像的特征，并采用掩模机制来防止编辑和非编辑区域之间的干扰。我们在典型的编辑任务上对 SpecRef 进行了评估，并表明它可以达到令人满意的性能。源代码可在 https://github.com/jingjiqinggong/specp2p 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03433v1><br />
**Code:** <https://github.com/jingjiqinggong/specp2p>**<br />
>>**index:** 2<br />
**Title:** **Deep Learning-based Image and Video Inpainting: A Survey**<br />
**Title_cn:** 基于深度学习的图像和视频修复：一项调查<br />
**Authors:** Weize Quan, Jiaxi Chen, Yanli Liu, Dong-Ming Yan, Peter Wonka<br />
**Abstract:** <details><summary>原文: </summary>Image and video inpainting is a classic problem in computer vision and computer graphics, aiming to fill in the plausible and realistic content in the missing areas of images and videos. With the advance of deep learning, this problem has achieved significant progress recently. The goal of this paper is to comprehensively review the deep learning-based methods for image and video inpainting. Specifically, we sort existing methods into different categories from the perspective of their high-level inpainting pipeline, present different deep learning architectures, including CNN, VAE, GAN, diffusion models, etc., and summarize techniques for module design. We review the training objectives and the common benchmark datasets. We present evaluation metrics for low-level pixel and high-level perceptional similarity, conduct a performance evaluation, and discuss the strengths and weaknesses of representative inpainting methods. We also discuss related real-world applications. Finally, we discuss open challenges and suggest potential future research directions.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像和视频修复是计算机视觉和计算机图形学中的一个经典问题，旨在填充图像和视频缺失区域中合理且真实的内容。随着深度学习的进步，这个问题最近取得了重大进展。本文的目标是全面回顾基于深度学习的图像和视频修复方法。具体来说，我们从高级修复流程的角度将现有方法分为不同类别，提出不同的深度学习架构，包括CNN、VAE、GAN、扩散模型等，并总结模块设计技术。我们审查培训目标和通用基准数据集。我们提出了低级像素和高级感知相似性的评估指标，进行性能评估，并讨论代表性修复方法的优点和缺点。我们还讨论相关的实际应用。最后，我们讨论了开放的挑战并提出了未来潜在的研究方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.03395v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **GRAM: Global Reasoning for Multi-Page VQA**<br />
**Title_cn:** GRAM：多页面 VQA 的全局推理<br />
**Authors:** Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman<br />
**Abstract:** <details><summary>原文: </summary>The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document-level tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our C-Former model, which reduces the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM's state-of-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 Transformer 的大型语言模型的使用越来越多，带来了处理长序列的挑战。在文档视觉问答（DocVQA）中，领先的方法侧重于单页设置，而文档可以跨越数百页。我们提出了 GRAM，一种将预训练的单页模型无缝扩展到多页设置的方法，无需进行大量计算的预训练。为此，我们利用单页面编码器进行本地页面级理解，并通过文档级指定层和可学习标记对其进行增强，从而促进跨页面的信息流以进行全局推理。为了强制我们的模型利用新引入的文档级标记，我们提出了一种量身定制的偏差适应方法。为了在解码过程中节省额外的计算量，我们使用 C-Former 模型引入了可选的压缩阶段，这减少了编码序列的长度，从而允许在质量和延迟之间进行权衡。大量实验展示了 GRAM 在多页 DocVQA 基准上最先进的性能，证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03411v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Involution Fused ConvNet for Classifying Eye-Tracking Patterns of Children with Autism Spectrum Disorder**<br />
**Title_cn:** 用于对自闭症谱系障碍儿童的眼动追踪模式进行分类的对合融合卷积网络<br />
**Authors:** Md. Farhadul Islam, Meem Arafat Manab, Joyanta Jyoti Mondal, Sarah Zabeen, Fardin Bin Rahman, Md. Zahidul Hasan, Farig Sadeque, Jannatun Noor<br />
**Abstract:** <details><summary>原文: </summary>Autism Spectrum Disorder (ASD) is a complicated neurological condition which is challenging to diagnose. Numerous studies demonstrate that children diagnosed with autism struggle with maintaining attention spans and have less focused vision. The eye-tracking technology has drawn special attention in the context of ASD since anomalies in gaze have long been acknowledged as a defining feature of autism in general. Deep Learning (DL) approaches coupled with eye-tracking sensors are exploiting additional capabilities to advance the diagnostic and its applications. By learning intricate nonlinear input-output relations, DL can accurately recognize the various gaze and eye-tracking patterns and adjust to the data. Convolutions alone are insufficient to capture the important spatial information in gaze patterns or eye tracking. The dynamic kernel-based process known as involutions can improve the efficiency of classifying gaze patterns or eye tracking data. In this paper, we utilise two different image-processing operations to see how these processes learn eye-tracking patterns. Since these patterns are primarily based on spatial information, we use involution with convolution making it a hybrid, which adds location-specific capability to a deep learning model. Our proposed model is implemented in a simple yet effective approach, which makes it easier for applying in real life. We investigate the reasons why our approach works well for classifying eye-tracking patterns. For comparative analysis, we experiment with two separate datasets as well as a combined version of both. The results show that IC with three involution layers outperforms the previous approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>自闭症谱系障碍 (ASD) 是一种复杂的神经系统疾病，诊断起来很困难。大量研究表明，被诊断患有自闭症的儿童很难保持注意力集中，视力也不太集中。眼球追踪技术在自闭症谱系障碍（ASD）背景下引起了特别关注，因为凝视异常长期以来一直被认为是自闭症的一个决定性特征。深度学习 (DL) 方法与眼动追踪传感器相结合，正在利用额外的功能来推进诊断及其应用。通过学习复杂的非线性输入输出关系，深度学习可以准确识别各种注视和眼球跟踪模式并根据数据进行调整。仅卷积不足以捕获注视模式或眼球追踪中的重要空间信息。基于动态内核的过程（称为“内卷”）可以提高对注视模式或眼睛跟踪数据进行分类的效率。在本文中，我们利用两种不同的图像处理操作来了解这些过程如何学习眼球追踪模式。由于这些模式主要基于空间信息，因此我们使用卷积与卷积使其成为混合模式，从而为深度学习模型添加了特定于位置的功能。我们提出的模型以简单而有效的方法实现，这使得它更容易在现实生活中应用。我们研究了我们的方法在眼球追踪模式分类方面效果良好的原因。为了进行比较分析，我们使用两个单独的数据集以及两者的组合版本进行实验。结果表明，具有三个对合层的 IC 优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.03575v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FurniScene: A Large-scale 3D Room Dataset with Intricate Furnishing Scenes**<br />
**Title_cn:** FurniScene：具有复杂家具场景的大型 3D 房间数据集<br />
**Authors:** Genghao Zhang, Yuxi Wang, Chuanchen Luo, Shibiao Xu, Junran Peng, Zhaoxiang Zhang, Man Zhang<br />
**Abstract:** <details><summary>原文: </summary>Indoor scene generation has attracted significant attention recently as it is crucial for applications of gaming, virtual reality, and interior design. Current indoor scene generation methods can produce reasonable room layouts but often lack diversity and realism. This is primarily due to the limited coverage of existing datasets, including only large furniture without tiny furnishings in daily life. To address these challenges, we propose FurniScene, a large-scale 3D room dataset with intricate furnishing scenes from interior design professionals. Specifically, the FurniScene consists of 11,698 rooms and 39,691 unique furniture CAD models with 89 different types, covering things from large beds to small teacups on the coffee table. To better suit fine-grained indoor scene layout generation, we introduce a novel Two-Stage Diffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various indoor scene generation based on FurniScene. Quantitative and qualitative evaluations demonstrate the capability of our method to generate highly realistic indoor scenes. Our dataset and code will be publicly available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>室内场景生成最近引起了人们的广泛关注，因为它对于游戏、虚拟现实和室内设计的应用至关重要。目前的室内场景生成方法可以产生合理的房间布局，但往往缺乏多样性和真实感。这主要是由于现有数据集的覆盖范围有限，仅包括大型家具，而没有日常生活中的小型家具。为了应对这些挑战，我们提出了 FurniScene，这是一个大型 3D 房间数据集，其中包含来自室内设计专业人士的复杂家具场景。具体来说，FurniScene由11,698个房间和39,691个独特的家具CAD模型组成，共有89种不同类型，涵盖了从大床到茶几上的小茶杯。为了更好地适应细粒度的室内场景布局生成，我们引入了一种新颖的两阶段扩散场景模型（TSDSM），并基于 FurniScene 为各种室内场景生成进行了评估基准。定量和定性评估证明了我们的方法生成高度逼真的室内场景的能力。我们的数据集和代码将很快公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.03470v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **See360: Novel Panoramic View Interpolation**<br />
**Title_cn:** See360：新颖的全景插值<br />
**Authors:** Zhi-Song Liu, Marie-Paule Cani, Wan-Chi Siu<br />
**Abstract:** <details><summary>原文: </summary>We present See360, which is a versatile and efficient framework for 360 panoramic view interpolation using latent space viewpoint estimation. Most of the existing view rendering approaches only focus on indoor or synthetic 3D environments and render new views of small objects. In contrast, we suggest to tackle camera-centered view synthesis as a 2D affine transformation without using point clouds or depth maps, which enables an effective 360? panoramic scene exploration. Given a pair of reference images, the See360 model learns to render novel views by a proposed novel Multi-Scale Affine Transformer (MSAT), enabling the coarse-to-fine feature rendering. We also propose a Conditional Latent space AutoEncoder (C-LAE) to achieve view interpolation at any arbitrary angle. To show the versatility of our method, we introduce four training datasets, namely UrbanCity360, Archinterior360, HungHom360 and Lab360, which are collected from indoor and outdoor environments for both real and synthetic rendering. Experimental results show that the proposed method is generic enough to achieve real-time rendering of arbitrary views for all four datasets. In addition, our See360 model can be applied to view synthesis in the wild: with only a short extra training time (approximately 10 mins), and is able to render unknown real-world scenes. The superior performance of See360 opens up a promising direction for camera-centered view rendering and 360 panoramic view interpolation.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 See360，它是一个使用潜在空间视点估计进行 360 度全景视图插值的多功能且高效的框架。大多数现有的视图渲染方法仅关注室内或合成 3D 环境并渲染小物体的新视图。相比之下，我们建议将以相机为中心的视图合成作为 2D 仿射变换来处理，而不使用点云或深度图，这可以实现有效的 360°？全景场景探索。给定一对参考图像，See360 模型学习通过提出的新颖的多尺度仿射变换器 (MSAT) 渲染新颖的视图，从而实现从粗到细的特征渲染。我们还提出了条件潜在空间自动编码器（C-LAE）来实现任意角度的视图插值。为了展示我们方法的多功能性，我们引入了四个训练数据集，即 UrbanCity360、Archinterior360、HungHom360 和 Lab360，它们是从室内和室外环境收集的，用于真实和合成渲染。实验结果表明，所提出的方法足够通用，可以实现所有四个数据集的任意视图的实时渲染。此外，我们的See360模型可以应用于野外视图合成：只需很短的额外训练时间（大约10分钟），并且能够渲染未知的现实世界场景。 See360的卓越性能为以相机为中心的视图渲染和360全景视图插值开辟了一个有希望的方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.03431v1><br />
**Code:** <https://github.com/Holmes-Alan/See360>**<br />
>>**index:** 4<br />
**Title:** **Towards Effective Multiple-in-One Image Restoration: A Sequential and Prompt Learning Strategy**<br />
**Title_cn:** 实现有效的多合一图像恢复：一种顺序且快速的学习策略<br />
**Authors:** Xiangtao Kong, Chao Dong, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>While single task image restoration (IR) has achieved significant successes, it remains a challenging issue to train a single model which can tackle multiple IR tasks. In this work, we investigate in-depth the multiple-in-one (MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO IR faces two pivotal challenges: the optimization of diverse objectives and the adaptation to multiple tasks. To tackle these challenges, we present two simple yet effective strategies. The first strategy, referred to as sequential learning, attempts to address how to optimize the diverse objectives, which guides the network to incrementally learn individual IR tasks in a sequential manner rather than mixing them together. The second strategy, i.e., prompt learning, attempts to address how to adapt to the different IR tasks, which assists the network to understand the specific task and improves the generalization ability. By evaluating on 19 test sets, we demonstrate that the sequential and prompt learning strategies can significantly enhance the MiO performance of commonly used CNN and Transformer backbones. Our experiments also reveal that the two strategies can supplement each other to learn better degradation representations and enhance the model robustness. It is expected that our proposed MiO IR formulation and strategies could facilitate the research on how to train IR models with higher generalization capabilities.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然单任务图像恢复（IR）取得了巨大的成功，但训练可以处理多个 IR 任务的单个模型仍然是一个具有挑战性的问题。在这项工作中，我们深入研究了多合一 (MiO) IR 问题，其中包括七个流行的 IR 任务。我们指出，MIO IR 面临两个关键挑战：多样化目标的优化和多任务的适应。为了应对这些挑战，我们提出了两种简单而有效的策略。第一种策略称为顺序学习，试图解决如何优化不同目标的问题，引导网络以顺序方式逐步学习各个 IR 任务，而不是将它们混合在一起。第二种策略，即即时学习，试图解决如何适应不同的IR任务，帮助网络理解具体任务并提高泛化能力。通过对 19 个测试集进行评估，我们证明了顺序和即时学习策略可以显着提高常用 CNN 和 Transformer 主干的 MiO 性能。我们的实验还表明，这两种策略可以相互补充，以学习更好的退化表示并增强模型的鲁棒性。预计我们提出的 MiO IR 公式和策略可以促进如何训练具有更高泛化能力的 IR 模型的研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.03379v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people**<br />
**Title_cn:** Amirkabir 校园数据集：视障人士视觉惯性里程计 (VIO) 的现实挑战和场景<br />
**Authors:** Ali Samadzadeh, Mohammad Hassan Mojab, Heydar Soudani, Seyed Hesamoddin Mireshghollah, Ahmad Nickabadi<br />
**Abstract:** <details><summary>原文: </summary>Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉惯性里程计 (VIO) 算法通过使用相机和惯性测量单元 (IMU) 传感器来估计准确的相机轨迹。 VIO 的应用范围广泛，包括增强现实和室内导航。 VIO 算法有潜力促进视障人士在室内和室外环境中的导航。然而，最先进的 VIO 算法在动态环境中遇到了巨大的挑战，特别是在人口稠密的走廊中。现有的 VIO 数据集（例如 ADVIO）通常无法有效地利用这些挑战。在本文中，我们引入了 Amirkabir 校园数据集（AUT-VI）来解决上述问题并改进导航系统。 AUT-VI 是一个新颖且极具挑战性的数据集，包含 17 个不同位置的 126 个不同序列。该数据集包含动态对象、具有挑战性的闭环/地图重用、不同的照明条件、反射和突然的相机移动，以涵盖所有极端的导航场景。此外，为了支持正在进行的开发工作，我们向公众发布了用于数据捕获的 Android 应用程序。这使得研究人员能够轻松捕获他们定制的 VIO 数据集变化。此外，我们还在数据集上评估了最先进的视觉惯性里程计（VIO）和视觉里程计（VO）方法，强调了这个具有挑战性的数据集的基本需求。</details>
**PDF:** <http://arxiv.org/pdf/2401.03604v1><br />
**Code:** null<br />

