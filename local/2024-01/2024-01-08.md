# !UPDATED  -- 2024-01-08

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Unifying Graph Contrastive Learning via Graph Message Augmentation**<br />
**Title_cn:** 通过图消息增强统一图对比学习<br />
**Authors:** Ziyan Zhang, Bo Jiang, Jin Tang, Bin Luo<br />
**Abstract:** <details><summary>原文: </summary>Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs. As we know that GDA is an important issue for graph contrastive learning. Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes. However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data. To address this issue, in this paper, we first introduce the graph message representation of graph data. Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs. The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks. Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs. Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning. Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>图对比学习通常首先进行图数据增强（GDA），然后使用对比学习管道来训练 GNN。众所周知，GDA 是图对比学习的一个重要问题。最近开发了各种GDA，主要涉及丢弃或扰动边、节点、节点属性和边属性。然而，据我们所知，它仍然缺乏适合不同类型图数据的通用且有效的增强器。为了解决这个问题，在本文中，我们首先介绍图数据的图消息表示。在此基础上，我们提出了一种新颖的图消息增强（GMA），这是一种重新制定许多现有 GDA 的通用方案。所提出的统一GMA不仅为理解许多现有的GDA提供了新的视角，而且为图自监督学习任务提供了通用且更有效的图数据增强。此外，GMA 引入了一种简单的方法来实现混合增强器，这对于图像来说很自然，但对于图形来说通常具有挑战性。基于所提出的 GMA，我们提出了一种统一的图对比学习，称为图消息对比学习（GMCL），它采用归因引导的通用 GMA 进行图对比学习。许多图学习任务的实验证明了所提出的 GMA 和 GMCL 方法的有效性和优点。</details>
**PDF:** <http://arxiv.org/pdf/2401.03638v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Automated Detection of Myopic Maculopathy in MMAC 2023: Achievements in Classification, Segmentation, and Spherical Equivalent Prediction**<br />
**Title_cn:** MMAC 2023 中近视黄斑病变的自动检测：分类、分割和球面等效预测方面的成就<br />
**Authors:** Yihao Li, Philippe Zhang, Yubo Tan, Jing Zhang, Zhihan Wang, Weili Jiang, Pierre-Henri Conze, Mathieu Lamard, Gwenolé Quellec, Mostafa El Habib Daho<br />
**Abstract:** <details><summary>原文: </summary>Myopic macular degeneration is the most common complication of myopia and the primary cause of vision loss in individuals with pathological myopia. Early detection and prompt treatment are crucial in preventing vision impairment due to myopic maculopathy. This was the focus of the Myopic Maculopathy Analysis Challenge (MMAC), in which we participated. In task 1, classification of myopic maculopathy, we employed the contrastive learning framework, specifically SimCLR, to enhance classification accuracy by effectively capturing enriched features from unlabeled data. This approach not only improved the intrinsic understanding of the data but also elevated the performance of our classification model. For Task 2 (segmentation of myopic maculopathy plus lesions), we have developed independent segmentation models tailored for different lesion segmentation tasks and implemented a test-time augmentation strategy to further enhance the model's performance. As for Task 3 (prediction of spherical equivalent), we have designed a deep regression model based on the data distribution of the dataset and employed an integration strategy to enhance the model's prediction accuracy. The results we obtained are promising and have allowed us to position ourselves in the Top 6 of the classification task, the Top 2 of the segmentation task, and the Top 1 of the prediction task. The code is available at \url{https://github.com/liyihao76/MMAC_LaTIM_Solution}.</details>
**Abstract_cn:** <details><summary>译文: </summary>近视性黄斑变性是近视最常见的并发症，也是病理性近视患者视力丧失的主要原因。早期发现和及时治疗对于预防近视黄斑病引起的视力损害至关重要。这是我们参加的近视黄斑病变分析挑战赛 (MMAC) 的焦点。在任务 1（近视性黄斑病变的分类）中，我们采用了对比学习框架（特别是 SimCLR），通过有效地从未标记数据中捕获丰富的特征来提高分类准确性。这种方法不仅提高了对数据的内在理解，而且提高了分类模型的性能。对于任务2（近视黄斑病变加病变的分割），我们开发了针对不同病变分割任务的独立分割模型，并实施了测试时间增强策略以进一步增强模型的性能。对于任务3（球当量的预测），我们根据数据集的数据分布设计了深度回归模型，并采用集成策略来提高模型的预测精度。我们获得的结果令人鼓舞，使我们能够跻身分类任务前 6 名、分割任务前 2 名和预测任务前 1 名。代码可在 \url{https://github.com/liyihao76/MMAC_LaTIM_Solution} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03615v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning**<br />
**Title_cn:** Dr$^2$Net：用于内存高效微调的动态可逆双残差网络<br />
**Authors:** Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem<br />
**Abstract:** <details><summary>原文: </summary>Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network with much higher numerical precision. We evaluate Dr$^2$Net on various pretrained models and various tasks, and show that it can reach comparable performance to conventional finetuning but with significantly less memory usage.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型预训练模型在现代计算机视觉任务中变得越来越重要。这些模型通常通过端到端微调用于下游任务，这对于具有高分辨率数据的任务来说是高度内存密集型的，例如视频理解、小物体检测和点云分析。在本文中，我们提出了动态可逆双残差网络（Dynamic Reversible Dual-Residual Networks），或 Dr$^2$Net，这是一种新颖的网络架构家族，它充当代理网络来微调预训练模型，同时显着减少内存消耗。 Dr$^2$Net 包含两种类型的残差连接，一种保持预训练模型中的残差结构，另一种使网络可逆。由于其可逆性，可以从输出重建的中间激活在训练期间从内存中清除。我们分别在任一类型的残差连接上使用两个系数，并引入动态训练策略，将预训练模型无缝过渡到具有更高数值精度的可逆网络。我们在各种预训练模型和各种任务上评估了 Dr$^2$Net，并表明它可以达到与传统微调相当的性能，但内存使用量显着减少。</details>
**PDF:** <http://arxiv.org/pdf/2401.04105v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classification**<br />
**Title_cn:** 用于音视频分类的高效多尺度多模态瓶颈变压器<br />
**Authors:** Wentao Zhu<br />
**Abstract:** <details><summary>原文: </summary>In recent years, researchers combine both audio and video signals to deal with challenges where actions are not well represented or captured by visual cues. However, how to effectively leverage the two modalities is still under development. In this work, we develop a multiscale multimodal Transformer (MMT) that leverages hierarchical representation learning. Particularly, MMT is composed of a novel multiscale audio Transformer (MAT) and a multiscale video Transformer [43]. To learn a discriminative cross-modality fusion, we further design multimodal supervised contrastive objectives called audio-video contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly align the two modalities. MMT surpasses previous state-of-the-art approaches by 7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy without external training data. Moreover, the proposed MAT significantly outperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark datasets, and is about 3% more efficient based on the number of FLOPs and 9.8% more efficient based on GPU memory usage.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，研究人员将音频和视频信号结合起来，以应对视觉线索无法很好地表示或捕获动作的挑战。然而，如何有效利用这两种模式仍在研究中。在这项工作中，我们开发了一种利用分层表示学习的多尺度多模态 Transformer (MMT)。特别地，MMT由新颖的多尺度音频变压器（MAT）和多尺度视频变压器组成[43]。为了学习有区别的跨模态融合，我们进一步设计了多模态监督对比目标，称为音频视频对比损失（AVC）和模内对比损失（IMC），它们可以稳健地对齐两种模态。在没有外部训练数据的情况下，MMT 在 Kinetics-Sounds 和 VGGSound 上的 top-1 准确度方面比之前最先进的方法分别提高了 7.3% 和 2.1%。此外，所提出的 MAT 在三个公共基准数据集上显着优于 AST [28] 22.2%、4.4% 和 4.7%，并且基于 FLOP 数量的效率提高了约 3%，基于 GPU 内存使用的效率提高了 9.8%。</details>
**PDF:** <http://arxiv.org/pdf/2401.04023v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MS-DETR: Efficient DETR Training with Mixed Supervision**<br />
**Title_cn:** MS-DETR：混合监督下的高效 DETR 训练<br />
**Authors:** Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, Jingdong Wang<br />
**Abstract:** <details><summary>原文: </summary>DETR accomplishes end-to-end object detection through iteratively generating multiple object candidates based on image features and promoting one candidate for each ground-truth object. The traditional training procedure using one-to-one supervision in the original DETR lacks direct supervision for the object detection candidates.   We aim at improving the DETR training efficiency by explicitly supervising the candidate generation procedure through mixing one-to-one supervision and one-to-many supervision. Our approach, namely MS-DETR, is simple, and places one-to-many supervision to the object queries of the primary decoder that is used for inference. In comparison to existing DETR variants with one-to-many supervision, such as Group DETR and Hybrid DETR, our approach does not need additional decoder branches or object queries. The object queries of the primary decoder in our approach directly benefit from one-to-many supervision and thus are superior in object candidate prediction. Experimental results show that our approach outperforms related DETR variants, such as DN-DETR, Hybrid DETR, and Group DETR, and the combination with related DETR variants further improves the performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>DETR通过基于图像特征迭代生成多个目标候选者并为每个真实目标提升一个候选者来完成端到端目标检测。原始 DETR 中使用一对一监督的传统训练过程缺乏对目标检测候选者的直接监督。我们的目标是通过混合一对一监督和一对多监督来明确监督候选生成过程，从而提高 DETR 训练效率。我们的方法，即 MS-DETR，很简单，并对用于推理的主解码器的对象查询进行一对多的监督。与具有一对多监督的现有 DETR 变体（例如 Group DETR 和 Hybrid DETR）相比，我们的方法不需要额外的解码器分支或对象查询。我们的方法中主解码器的对象查询直接受益于一对多监督，因此在对象候选预测方面表现出色。实验结果表明，我们的方法优于相关的 DETR 变体，例如 DN-DETR、Hybrid DETR 和 Group DETR，并且与相关 DETR 变体的组合进一步提高了性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03989v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multi-scale attention-based instance segmentation for measuring crystals with large size variation**<br />
**Title_cn:** 基于多尺度注意力的实例分割，用于测量尺寸变化较大的晶体<br />
**Authors:** Theresa Neubauer, Astrid Berg, Maria Wimmer, Dimitrios Lenis, David Major, Philip Matthias Winter, Gaia Romana De Paolis, Johannes Novotny, Daniel Lüftner, Katja Reinharter, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Quantitative measurement of crystals in high-resolution images allows for important insights into underlying material characteristics. Deep learning has shown great progress in vision-based automatic crystal size measurement, but current instance segmentation methods reach their limits with images that have large variation in crystal size or hard to detect crystal boundaries. Even small image segmentation errors, such as incorrectly fused or separated segments, can significantly lower the accuracy of the measured results. Instead of improving the existing pixel-wise boundary segmentation methods, we propose to use an instance-based segmentation method, which gives more robust segmentation results to improve measurement accuracy. Our novel method enhances flow maps with a size-aware multi-scale attention module. The attention module adaptively fuses information from multiple scales and focuses on the most relevant scale for each segmented image area. We demonstrate that our proposed attention fusion strategy outperforms state-of-the-art instance and boundary segmentation methods, as well as simple average fusion of multi-scale predictions. We evaluate our method on a refractory raw material dataset of high-resolution images with large variation in crystal size and show that our model can be used to calculate the crystal size more accurately than existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过高分辨率图像对晶体进行定量测量可以深入了解潜在的材料特性。深度学习在基于视觉的自动晶体尺寸测量方面取得了巨大进展，但当前的实例分割方法在处理晶体尺寸变化较大或难以检测晶体边界的图像时达到了极限。即使很小的图像分割错误，例如不正确地融合或分离的片段，也会显着降低测量结果的准确性。我们建议使用基于实例的分割方法，而不是改进现有的逐像素边界分割方法，该方法提供更稳健的分割结果以提高测量精度。我们的新颖方法通过尺寸感知的多尺度注意力模块增强了流程图。注意力模块自适应地融合来自多个尺度的信息，并关注每个分割图像区域最相关的尺度。我们证明了我们提出的注意力融合策略优于最先进的实例和边界分割方法以及多尺度预测的简单平均融合。我们在晶体尺寸变化较大的高分辨率图像的耐火原材料数据集上评估我们的方法，并表明我们的模型可以比现有方法更准确地计算晶体尺寸。</details>
**PDF:** <http://arxiv.org/pdf/2401.03939v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM**<br />
**Title_cn:** RoboFusion：通过 SAM 实现稳健的多模态 3D 物体检测<br />
**Authors:** Ziying Song, Guoxing Zhang, Lin Liu, Lei Yang, Shaoqing Xu, Caiyan Jia, Feiyang Jia, Li Wang<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal 3D object detectors are dedicated to exploring secure and reliable perception systems for autonomous driving (AD). However, while achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they tend to overlook the complexity and harsh conditions of real-world environments. Meanwhile, with the emergence of visual foundation models (VFMs), opportunities and challenges are presented for improving the robustness and generalization of multi-modal 3D object detection in autonomous driving. Therefore, we propose RoboFusion, a robust framework that leverages VFMs like SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the original SAM for autonomous driving scenarios named SAM-AD. To align SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the image features extracted by SAM. We employ wavelet decomposition to denoise the depth-guided images for further noise reduction and weather interference. Lastly, we employ self-attention mechanisms to adaptively reweight the fused features, enhancing informative features while suppressing excess noise. In summary, our RoboFusion gradually reduces noise by leveraging the generalization and robustness of VFMs, thereby enhancing the resilience of multi-modal 3D object detection. Consequently, our RoboFusion achieves state-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态 3D 物体探测器致力于探索安全可靠的自动驾驶 (AD) 感知系统。然而，虽然在干净的基准数据集上实现了最先进的 (SOTA) 性能，但他们往往忽视了现实环境的复杂性和恶劣条件。同时，随着视觉基础模型（VFM）的出现，为提高自动驾驶中多模态 3D 物体检测的鲁棒性和泛化性带来了机遇和挑战。因此，我们提出 RoboFusion，这是一个强大的框架，利用 SAM 等 VFM 来解决分布外 (OOD) 噪声场景。我们首先将原始 SAM 改编为自动驾驶场景，命名为 SAM-AD。为了将 SAM 或 SAM-AD 与多模态方法结合起来，我们引入了 AD-FPN 对 SAM 提取的图像特征进行上采样。我们采用小波分解对深度引导图像进行去噪，以进一步降低噪声和天气干扰。最后，我们采用自注意力机制来自适应地重新加权融合特征，增强信息特征，同时抑制多余的噪声。总之，我们的 RoboFusion 通过利用 VFM 的泛化性和鲁棒性逐渐降低噪声，从而增强多模态 3D 对象检测的弹性。因此，我们的 RoboFusion 在嘈杂的场景中实现了最先进的性能，正如 KITTI-C 和 nuScenes-C 基准测试所证明的那样。</details>
**PDF:** <http://arxiv.org/pdf/2401.03907v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking**<br />
**Title_cn:** 用于透明对象跟踪的新数据集和干扰感知架构<br />
**Authors:** Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan<br />
**Abstract:** <details><summary>原文: </summary>Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>与不透明物体相比，现代跟踪器的性能在透明物体上显着下降。这主要是由于两个不同的原因。透明对象的独特之处在于它们的外观直接受背景影响。此外，透明物体场景通常包含许多视觉上相似的物体（干扰物），这通常会导致跟踪失败。然而，现代跟踪架构的开发需要大量的训练集，而透明对象跟踪中不存在这种情况。我们针对上述问题提出了两项​​贡献。我们提出了第一个透明对象跟踪训练数据集 Trans2k，它由超过 2k 个序列组成，总共有 104,343 个图像，由边界框和分割掩模注释。在此数据集上训练的标准跟踪器持续改进高达 16%。我们的第二个贡献是一种新的干扰感知透明对象跟踪器（DiTra），它将定位精度和目标识别视为单独的任务，并通过新颖的架构来实现它们。 DiTra 在透明对象跟踪方面树立了新的最先进技术，并且可以很好地推广到不透明对象。</details>
**PDF:** <http://arxiv.org/pdf/2401.03872v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **UFO: Unidentified Foreground Object Detection in 3D Point Cloud**<br />
**Title_cn:** UFO：3D 点云中的不明前景物体检测<br />
**Authors:** Hyunjun Choi, Hawook Jeong, Jin Young Choi<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we raise a new issue on Unidentified Foreground Object (UFO) detection in 3D point clouds, which is a crucial technology in autonomous driving in the wild. UFO detection is challenging in that existing 3D object detectors encounter extremely hard challenges in both 3D localization and Out-of-Distribution (OOD) detection. To tackle these challenges, we suggest a new UFO detection framework including three tasks: evaluation protocol, methodology, and benchmark. The evaluation includes a new approach to measure the performance on our goal, i.e. both localization and OOD detection of UFOs. The methodology includes practical techniques to enhance the performance of our goal. The benchmark is composed of the KITTI Misc benchmark and our additional synthetic benchmark for modeling a more diverse range of UFOs. The proposed framework consistently enhances performance by a large margin across all four baseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight for future work on UFO detection in the wild.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了 3D 点云中的不明前景物体 (UFO) 检测的新问题，这是野外自动驾驶的一项关键技术。 UFO 检测具有挑战性，因为现有的 3D 物体检测器在 3D 定位和分布外 (OOD) 检测方面都遇到了极其严峻的挑战。为了应对这些挑战，我们提出了一个新的 UFO 检测框架，包括三个任务：评估协议、方法论和基准。该评估包括一种衡量我们目标绩效的新方法，即 UFO 的定位和 OOD 检测。该方法包括提高我们目标绩效的实用技术。该基准由 KITTI Misc 基准和我们用于建模更多样化的 UFO 的附加综合基准组成。所提出的框架持续大幅提高了所有四个基线探测器的性能：SECOND、PointPillars、PV-RCNN 和 PartA2，为未来的野外 UFO 探测工作提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2401.03846v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Fully Attentional Networks with Self-emerging Token Labeling**<br />
**Title_cn:** 具有自我出现的令牌标签的完全注意力网络<br />
**Authors:** Bingyin Zhao, Zhiding Yu, Shiyi Lan, Yutao Cheng, Anima Anandkumar, Yingjie Lao, Jose M. Alvarez<br />
**Abstract:** <details><summary>原文: </summary>Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework. Our method contains a two-stage training framework. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins. The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model. Code is available at https://github.com/NVlabs/STL.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究表明，视觉变压器 (ViT) 对于分布外场景具有很强的鲁棒性。特别是，全注意力网络（FAN）——一系列 ViT 主干网络，已经实现了最先进的鲁棒性。在本文中，我们重新审视 FAN 模型，并使用自我生成的令牌标记（STL）框架改进其预训练。我们的方法包含一个两阶段的训练框架。具体来说，我们首先训练 FAN 令牌标记器 (FAN-TL) 以生成语义上有意义的补丁令牌标签，然后是使用令牌标签和原始类标签的 FAN 学生模型训练阶段。利用所提出的 STL 框架，我们基于 FAN-L-Hybrid（77.3M 参数）的最佳模型在 ImageNet-1K 和 ImageNet-C 上实现了 84.8% Top-1 准确率和 42.1% mCE，并创下了新的最高水平-art 用于 ImageNet-A (46.1%) 和 ImageNet-R (56.6%)，无需使用额外数据，明显优于原始 FAN 对应项。所提出的框架还展示了语义分割等下游任务的性能显着增强，与对应模型相比，鲁棒性提高了 1.7%。代码可在 https://github.com/NVlabs/STL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03844v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **WidthFormer: Toward Efficient Transformer-based BEV View Transformation**<br />
**Title_cn:** WidthFormer：实现基于 Transformer 的高效 BEV 视图转换<br />
**Authors:** Chenhongyi Yang, Tianwei Lin, Lichao Huang, Elliot J. Crowley<br />
**Abstract:** <details><summary>原文: </summary>In this work, we present WidthFormer, a novel transformer-based Bird's-Eye-View (BEV) 3D detection method tailored for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. In this work, we propose a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to generate high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently-proposed works, we further improve our model's efficiency by vertically compressing the image features when serving as attention keys and values. We also introduce two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using $256\times 704$ input images, it achieves 1.5 ms latency on NVIDIA 3090 GPU. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer .</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了 WidthFormer，这是一种专为实时自动驾驶应用量身定制的基于变压器的鸟瞰 (BEV) 3D 检测方法。 WidthFormer 计算高效、稳健，不需要任何特殊的工程工作即可部署。在这项工作中，我们提出了一种新颖的 3D 位置编码机制，能够准确封装 3D 几何信息，这使得我们的模型能够仅使用单个 Transformer 解码器层生成高质量的 BEV 表示。这种机制对于现有的稀疏 3D 物体检测器也有好处。受最近提出的工作的启发，我们通过在用作注意键和值时垂直压缩图像特征来进一步提高模型的效率。我们还引入了两个模块来补偿由于特征压缩而导致的潜在信息丢失。对广泛使用的 nuScenes 3D 对象检测基准的实验评估表明，我们的方法在不同的 3D 检测架构中优于以前的方法。更重要的是，我们的模型非常高效。例如，当使用 256 美元× 704 美元的输入图像时，它在 NVIDIA 3090 GPU 上实现了 1.5 毫秒的延迟。此外，WidthFormer 对不同程度的相机扰动也表现出很强的鲁棒性。我们的研究为在现实复杂的道路环境中部署纯电动汽车改造方法提供了宝贵的见解。代码可在 https://github.com/ChenhongyiYang/WidthFormer 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03836v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **A multimodal gesture recognition dataset for desktop human-computer interaction**<br />
**Title_cn:** 用于桌面人机交互的多模态手势识别数据集<br />
**Authors:** Qi Wang, Fengchao Zhu, Guangming Zhu, Liang Zhang, Ning Li, Eryang Gao<br />
**Abstract:** <details><summary>原文: </summary>Gesture recognition is an indispensable component of natural and efficient human-computer interaction technology, particularly in desktop-level applications, where it can significantly enhance people's productivity. However, the current gesture recognition community lacks a suitable desktop-level (top-view perspective) dataset for lightweight gesture capture devices. In this study, we have established a dataset named GR4DHCI. What distinguishes this dataset is its inherent naturalness, intuitive characteristics, and diversity. Its primary purpose is to serve as a valuable resource for the development of desktop-level portable applications. GR4DHCI comprises over 7,000 gesture samples and a total of 382,447 frames for both Stereo IR and skeletal modalities. We also address the variances in hand positioning during desktop interactions by incorporating 27 different hand positions into the dataset. Building upon the GR4DHCI dataset, we conducted a series of experimental studies, the results of which demonstrate that the fine-grained classification blocks proposed in this paper can enhance the model's recognition accuracy. Our dataset and experimental findings presented in this paper are anticipated to propel advancements in desktop-level gesture recognition research.</details>
**Abstract_cn:** <details><summary>译文: </summary>手势识别是自然高效的人机交互技术不可或缺的组成部分，特别是在桌面级应用中，它可以显着提高人们的生产力。然而，当前的手势识别社区缺乏适合轻量级手势捕获设备的桌面级（顶视图）数据集。在本研究中，我们建立了一个名为 GR4DHCI 的数据集。该数据集的独特之处在于其固有的自然性、直观特征和多样性。其主要目的是作为开发桌面级便携式应用程序的宝贵资源。 GR4DHCI 包含 7,000 多个手势样本以及立体红外和骨骼模式的总共 382,​​447 帧。我们还通过将 27 个不同的手部位置合并到数据集中来解决桌面交互期间手部位置的差异。基于GR4DHCI数据集，我们进行了一系列实验研究，结果表明本文提出的细粒度分类块可以提高模型的识别精度。我们在本文中提出的数据集和实验结果预计将推动桌面级手势识别研究的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.03828v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Color-$S^{4}L$: Self-supervised Semi-supervised Learning with Image Colorization**<br />
**Title_cn:** Color-$S^{4}L$：具有图像着色的自监督半监督学习<br />
**Authors:** Hanxiao Chen<br />
**Abstract:** <details><summary>原文: </summary>This work addresses the problem of semi-supervised image classification tasks with the integration of several effective self-supervised pretext tasks. Different from widely-used consistency regularization within semi-supervised learning, we explored a novel self-supervised semi-supervised learning framework (Color-$S^{4}L$) especially with image colorization proxy task and deeply evaluate performances of various network architectures in such special pipeline. Also, we demonstrated its effectiveness and optimal performance on CIFAR-10, SVHN and CIFAR-100 datasets in comparison to previous supervised and semi-supervised optimal methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作通过集成几个有效的自监督借口任务来解决半监督图像分类任务的问题。与半监督学习中广泛使用的一致性正则化不同，我们探索了一种新颖的自监督半监督学习框架（Color-$S^{4}L$），特别是图像着色代理任务，并深入评估各种网络的性能这种特殊管道中的架构。此外，与之前的监督和半监督优化方法相比，我们在 CIFAR-10、SVHN 和 CIFAR-100 数据集上证明了其有效性和最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03753v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Flying Bird Object Detection Algorithm in Surveillance Video**<br />
**Title_cn:** 监控视频中的飞鸟目标检测算法<br />
**Authors:** Ziwei Sun, Zexi Hua, Hengchao Li, Yan Li<br />
**Abstract:** <details><summary>原文: </summary>Aiming at the characteristics of the flying bird object in surveillance video, such as the single frame image feature is not obvious, the size is small in most cases, and asymmetric, this paper proposes a Flying Bird Object Detection method for Surveillance Video (FBOD-SV). Firstly, a new feature aggregation module, the Correlation Attention Feature Aggregation (Co-Attention-FA) module, is designed to aggregate the features of the flying bird object according to the bird object's correlation on multiple consecutive frames of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net) with down-sampling and then up-sampling is designed, which uses a large feature layer that fuses fine spatial information and large receptive field information to detect special multi-scale (mostly small-scale) bird objects. Finally, the SimOTA dynamic label allocation method is applied to One-Category object detection, and the SimOTA-OC dynamic label strategy is proposed to solve the difficult problem of label allocation caused by irregular flying bird objects. In this paper, the algorithm's performance is verified by the experimental data set of the surveillance video of the flying bird object of the traction substation. The experimental results show that the surveillance video flying bird object detection method proposed in this paper effectively improves the detection performance of flying bird objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>针对监控视频中飞鸟目标单帧图像特征不明显、多数情况下尺寸较小、不对称等特点，提出一种监控视频飞鸟目标检测方法（FBOD- SV）。首先，设计了一个新的特征聚合模块，即相关注意特征聚合（Co-Attention-FA）模块，根据鸟对象在多个连续帧图像上的相关性来聚合飞鸟对象的特征。其次，设计了先下采样再上采样的飞鸟目标检测网络（FBOD-Net），该网络使用融合精细空间信息和大感受野信息的大特征层来检测特殊的多尺度（大多是小尺度） -规模）鸟类物体。最后，将SimOTA动态标签分配方法应用于One-Category目标检测，提出SimOTA-OC动态标签策略，解决不规则飞鸟目标带来的标签分配难题。本文通过牵引变电站飞鸟物体监控视频实验数据集验证了算法的性能。实验结果表明，本文提出的监控视频飞鸟目标检测方法有效提高了飞鸟目标的检测性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03749v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Flowmind2Digital: The First Comprehensive Flowmind Recognition and Conversion Approach**<br />
**Title_cn:** Flowmind2Digital：第一个全面的 Flowmind 识别和转换方法<br />
**Authors:** Huanyu Liu, Jianfeng Cai, Tingjia Zhang, Hongsheng Li, Siyuan Wang, Guangming Zhu, Syed Afaq Ali Shah, Mohammed Bennamoun, Liang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Flowcharts and mind maps, collectively known as flowmind, are vital in daily activities, with hand-drawn versions facilitating real-time collaboration. However, there's a growing need to digitize them for efficient processing. Automated conversion methods are essential to overcome manual conversion challenges. Existing sketch recognition methods face limitations in practical situations, being field-specific and lacking digital conversion steps. Our paper introduces the Flowmind2digital method and hdFlowmind dataset to address these challenges. Flowmind2digital, utilizing neural networks and keypoint detection, achieves a record 87.3% accuracy on our dataset, surpassing previous methods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds across 22 scenarios, outperforms existing datasets. Additionally, our experiments emphasize the importance of simple graphics, enhancing accuracy by 9.3%.</details>
**Abstract_cn:** <details><summary>译文: </summary>流程图和思维导图（统称为“flowmind”）在日常活动中至关重要，手绘版本有助于实时协作。然而，越来越需要将它们数字化以进行高效处理。自动转换方法对于克服手动转换挑战至关重要。现有的草图识别方法在实际情况中面临着局限性，即针对特定领域且缺乏数字转换步骤。我们的论文介绍了 Flowmind2digital 方法和 hdFlowmind 数据集来应对这些挑战。 Flowmind2digital 利用神经网络和关键点检测，在我们的数据集上实现了创纪录的 87.3% 准确率，比之前的方法高出 11.9%。 hdFlowmind 数据集包含 22 个场景中的 1,776 个带注释的 flowmind，其性能优于现有数据集。此外，我们的实验强调了简单图形的重要性，将准确率提高了 9.3%。</details>
**PDF:** <http://arxiv.org/pdf/2401.03742v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A Large-scale Empirical Study on Improving the Fairness of Deep Learning Models**<br />
**Title_cn:** 提高深度学习模型公平性的大规模实证研究<br />
**Authors:** Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen<br />
**Abstract:** <details><summary>原文: </summary>Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the performance of each method across different datasets and sensitive attributes, indicating over-fitting on specific datasets by many existing methods. Furthermore, different fairness evaluation metrics, due to their distinct focuses, yield significantly different assessment results. Overall, we observe that pre-processing methods and in-processing methods outperform post-processing methods, with pre-processing methods exhibiting the best performance. Our empirical study offers comprehensive recommendations for enhancing fairness in deep learning models. We approach the problem from multiple dimensions, aiming to provide a uniform evaluation platform and inspire researchers to explore more effective fairness solutions via a set of implications.</details>
**Abstract_cn:** <details><summary>译文: </summary>公平性一直是影响深度学习模型在实际实践中采用的关键问题。为了提高模型的公平性，许多现有方法被提出并评估为在各自的环境中有效。然而，目前还没有对它们进行系统评价，在相同背景下进行综合比较，这使得人们很难理解它们之间的性能差异，阻碍了它们的研究进展和实际应用。为了填补这一空白，本文致力于进行首次大规模实证研究，以全面比较现有最先进的公平性改进技术的性能。具体来说，我们针对广泛使用的图像分类应用场景，利用三个不同的数据集和五个常用的性能指标来评估不同类别的总共 13 种方法。我们的研究结果揭示了每种方法在不同数据集和敏感属性上的性能存在巨大差异，表明许多现有方法对特定数据集的过度拟合。此外，不同的公平性评价指标由于侧重点不同，其评价结果也存在显着差异。总体而言，我们观察到预处理方法和处理中方法优于后处理方法，其中预处理方法表现出最佳性能。我们的实证研究为增强深度学习模型的公平性提供了全面的建议。我们从多个维度来处理这个问题，旨在提供一个统一的评估平台，并通过一系列的含义激励研究人员探索更有效的公平解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.03695v1><br />
**Code:** <https://github.com/junjie1003/DL-Fairness-Study>**<br />
>>**index:** 15<br />
**Title:** **Primitive Geometry Segment Pre-training for 3D Medical Image Segmentation**<br />
**Title_cn:** 3D 医学图像分割的原始几何分割预训练<br />
**Authors:** Ryu Tadokoro, Ryosuke Yamada, Kodai Nakashima, Ryo Nakamura, Hirokatsu Kataoka<br />
**Abstract:** <details><summary>原文: </summary>The construction of 3D medical image datasets presents several issues, including requiring significant financial costs in data collection and specialized expertise for annotation, as well as strict privacy concerns for patient confidentiality compared to natural image datasets. Therefore, it has become a pressing issue in 3D medical image segmentation to enable data-efficient learning with limited 3D medical data and supervision. A promising approach is pre-training, but improving its performance in 3D medical image segmentation is difficult due to the small size of existing 3D medical image datasets. We thus present the Primitive Geometry Segment Pre-training (PrimGeoSeg) method to enable the learning of 3D semantic features by pre-training segmentation tasks using only primitive geometric objects for 3D medical image segmentation. PrimGeoSeg performs more accurate and efficient 3D medical image segmentation without manual data collection and annotation. Further, experimental results show that PrimGeoSeg on SwinUNETR improves performance over learning from scratch on BTCV, MSD (Task06), and BraTS datasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was equal to or better than state-of-the-art self-supervised learning despite the equal number of pre-training data. From experimental results, we conclude that effective pre-training can be achieved by looking at primitive geometric objects only. Code and dataset are available at https://github.com/SUPER-TADORY/PrimGeoSeg.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 医学图像数据集的构建提出了几个问题，包括在数据收集和注释方面需要大量的财务成本，以及与自然图像数据集相比对患者保密性的严格隐私问题。因此，在有限的3D医学数据和监督下实现数据高效的学习已成为3D医学图像分割中的一个紧迫问题。预训练是一种很有前景的方法，但由于现有 3D 医学图像数据集规模较小，因此很难提高其在 3D 医学图像分割中的性能。因此，我们提出了原始几何分割预训练（PrimGeoSeg）方法，通过仅使用原始几何对象进行 3D 医学图像分割的预训练分割任务来实现 3D 语义特征的学习。 PrimGeoSeg 可以执行更准确、更高效的 3D 医学图像分割，无需手动数据收集和注释。此外，实验结果表明，SwinUNETR 上的 PrimGeoSeg 比在 BTCV、MSD (Task06) 和 BraTS 数据集上从头开始学习的性能分别提高了 3.7%、4.4% 和 0.3%。值得注意的是，尽管预训练数据数量相同，但其性能等于或优于最先进的自监督学习。根据实验结果，我们得出结论，仅通过查看原始几何对象就可以实现有效的预训练。代码和数据集可在 https://github.com/SUPER-TADORY/PrimGeoSeg 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03665v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Dual-Channel Reliable Breast Ultrasound Image Classification Based on Explainable Attribution and Uncertainty Quantification**<br />
**Title_cn:** 基于可解释归因和不确定性量化的双通道可靠乳腺超声图像分类<br />
**Authors:** Shuge Lei, Haonan Hu, Dasheng Sun, Huabin Zhang, Kehong Yuan, Jian Dai, Jijun Tang, Yan Tong<br />
**Abstract:** <details><summary>原文: </summary>This paper focuses on the classification task of breast ultrasound images and researches on the reliability measurement of classification results. We proposed a dual-channel evaluation framework based on the proposed inference reliability and predictive reliability scores. For the inference reliability evaluation, human-aligned and doctor-agreed inference rationales based on the improved feature attribution algorithm SP-RISA are gracefully applied. Uncertainty quantification is used to evaluate the predictive reliability via the Test Time Enhancement. The effectiveness of this reliability evaluation framework has been verified on our breast ultrasound clinical dataset YBUS, and its robustness is verified on the public dataset BUSI. The expected calibration errors on both datasets are significantly lower than traditional evaluation methods, which proves the effectiveness of our proposed reliability measurement.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文主要针对乳腺超声图像的分类任务，对分类结果的可靠性度量进行研究。我们根据所提出的推理可靠性和预测可靠性分数提出了双通道评估框架。对于推理可靠性评估，优雅地应用了基于改进的特征归因算法 SP-RISA 的人性化和医生同意的推理原理。不确定性量化用于通过测试时间增强来评估预测可靠性。该可靠性评估框架的有效性已在我们的乳腺超声临床数据集 YBUS 上得到验证，其稳健性在公共数据集 BUSI 上得到验证。两个数据集的预期校准误差均显着低于传统的评估方法，这证明了我们提出的可靠性测量的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03664v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling**<br />
**Title_cn:** 通过阅读顺序估计和动态采样进行类逆对抗场景文本识别<br />
**Authors:** Shi-Xue Zhang, Chun Yang, Xiaobin Zhu, Hongyang Zhou, Hongfa Wang, Xu-Cheng Yin<br />
**Abstract:** <details><summary>原文: </summary>Scene text spotting is a challenging task, especially for inverse-like scene text, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed. In this paper, we propose a unified end-to-end trainable inverse-like antagonistic text spotting framework dubbed IATS, which can effectively spot inverse-like scene texts without sacrificing general ones. Specifically, we propose an innovative reading-order estimation module (REM) that extracts reading-order information from the initial text boundary generated by an initial boundary module (IBM). To optimize and train REM, we propose a joint reading-order estimation loss consisting of a classification loss, an orthogonality loss, and a distribution loss. With the help of IBM, we can divide the initial text boundary into two symmetric control points and iteratively refine the new text boundary using a lightweight boundary refinement module (BRM) for adapting to various shapes and scales. To alleviate the incompatibility between text detection and recognition, we propose a dynamic sampling module (DSM) with a thin-plate spline that can dynamically sample appropriate features for recognition in the detected text region. Without extra supervision, the DSM can proactively learn to sample appropriate features for text recognition through the gradient returned by the recognition module. Extensive experiments on both challenging scene text and inverse-like scene text datasets demonstrate that our method achieves superior performance both on irregular and inverse-like text spotting.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景文本识别是一项具有挑战性的任务，特别是对于具有复杂布局（例如镜像、对称或反折）的类似反向的场景文本。在本文中，我们提出了一种统一的端到端可训练的类逆对抗性文本识别框架，称为 IATS，它可以有效地识别类逆场景文本，而无需牺牲通用文本。具体来说，我们提出了一种创新的阅读顺序估计模块（REM），它从初始边界模块（IBM）生成的初始文本边界中提取阅读顺序信息。为了优化和训练 REM，我们提出了一种联合阅读顺序估计损失，其中包括分类损失、正交性损失和分布损失。在 IBM 的帮助下，我们可以将初始文本边界划分为两个对称控制点，并使用轻量级边界细化模块 (BRM) 迭代细化新的文本边界，以适应各种形状和比例。为了缓解文本检测和识别之间的不兼容性，我们提出了一种带有薄板样条的动态采样模块（DSM），可以动态采样适当的特征以在检测到的文本区域中进行识别。在没有额外监督的情况下，DSM 可以通过识别模块返回的梯度主动学习采样适当的特征以进行文本识别。在具有挑战性的场景文本和类似逆的场景文本数据集上进行的大量实验表明，我们的方法在不规则和类似逆的文本识别上都实现了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03637v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose Refinement**<br />
**Title_cn:** D3PRefiner：用于 3D 人体姿势细化的基于扩散的降噪方法<br />
**Authors:** Danqi Yan, Qing Gao, Yuepeng Qian, Xinxing Chen, Chenglong Fu, Yuquan Leng<br />
**Abstract:** <details><summary>原文: </summary>Three-dimensional (3D) human pose estimation using a monocular camera has gained increasing attention due to its ease of implementation and the abundance of data available from daily life. However, owing to the inherent depth ambiguity in images, the accuracy of existing monocular camera-based 3D pose estimation methods remains unsatisfactory, and the estimated 3D poses usually include much noise. By observing the histogram of this noise, we find each dimension of the noise follows a certain distribution, which indicates the possibility for a neural network to learn the mapping between noisy poses and ground truth poses. In this work, in order to obtain more accurate 3D poses, a Diffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output of any existing 3D pose estimator. We first introduce a conditional multivariate Gaussian distribution to model the distribution of noisy 3D poses, using paired 2D poses and noisy 3D poses as conditions to achieve greater accuracy. Additionally, we leverage the architecture of current diffusion models to convert the distribution of noisy 3D poses into ground truth 3D poses. To evaluate the effectiveness of the proposed method, two state-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D pose estimation models, and the proposed method is evaluated on different types of 2D poses and different lengths of the input sequence. Experimental results demonstrate the proposed architecture can significantly improve the performance of current sequence-to-sequence 3D pose estimators, with a reduction of at least 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in the Procrustes MPJPE (P-MPJPE).</details>
**Abstract_cn:** <details><summary>译文: </summary>使用单目相机进行三维 (3D) 人体姿势估计由于其易于实施且日常生活中可获得的数据丰富而受到越来越多的关注。然而，由于图像固有的深度模糊性，现有的基于单目相机的3D姿态估计方法的精度仍然不能令人满意，并且估计的3D姿态通常包含大量噪声。通过观察该噪声的直方图，我们发现噪声的每个维度都遵循一定的分布，这表明神经网络有可能学习噪声姿势和地面真实姿势之间的映射。在这项工作中，为了获得更准确的 3D 姿态，提出了一种基于扩散的 3D 姿态细化器（D3PRefiner）来细化任何现有 3D 姿态估计器的输出。我们首先引入条件多元高斯分布来对噪声 3D 姿势的分布进行建模，使用配对 2D 姿势和噪声 3D 姿势作为条件来实现更高的精度。此外，我们利用当前扩散模型的架构将噪声 3D 姿势的分布转换为地面真实 3D 姿势。为了评估所提出方法的有效性，使用两个最先进的序列到序列3D姿态估计器作为基本3D姿态估计模型，并在不同类型的2D姿态和不同长度的2D姿态上评估所提出的方法。输入序列。实验结果表明，所提出的架构可以显着提高当前序列到序列 3D 位姿估计器的性能，平均每关节位置误差 (MPJPE) 减少至少 10.3%，Procrustes MPJPE 至少减少 11.0%（ P-MPJPE）。</details>
**PDF:** <http://arxiv.org/pdf/2401.03914v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **AGG: Amortized Generative 3D Gaussians for Single Image to 3D**<br />
**Title_cn:** AGG：用于单图像到 3D 的摊销生成 3D 高斯<br />
**Authors:** Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat<br />
**Abstract:** <details><summary>原文: </summary>Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: https://ir1d.github.io/AGG/</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于对自动 3D 内容创建管道的需求不断增长，人们已经研究了各种 3D 表示形式，以从单个图像生成 3D 对象。由于其卓越的渲染效率，基于 3D 高斯喷射的模型最近在 3D 重建和生成方面都表现出色。用于图像到 3D 生成的 3D 高斯分布方法通常是基于优化的，需要许多计算成本高昂的分数蒸馏步骤。为了克服这些挑战，我们引入了摊销生成 3D 高斯框架 (AGG)，它可以立即从单个图像生成 3D 高斯，从而无需对每个实例进行优化。 AGG 利用中间混合表示，分解 3D 高斯位置和其他外观属性的生成，以进行联合优化。此外，我们提出了一个级联管道，首先生成 3D 数据的粗略表示，然后使用 3D 高斯超分辨率模块对其进行上采样。我们的方法根据现有的基于优化的 3D 高斯框架和利用其他 3D 表示的基于采样的管道进行评估，其中 AGG 在定性和定量上都展示了有竞争力的生成能力，同时速度快了几个数量级。项目页面：https://ir1d.github.io/AGG/</details>
**PDF:** <http://arxiv.org/pdf/2401.04099v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **A Survey on 3D Gaussian Splatting**<br />
**Title_cn:** 3D 高斯泼溅综述<br />
**Authors:** Guikun Chen, Wenguan Wang<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯分布 (3D GS) 最近作为显式辐射场和计算机图形领域的一项变革性技术而出现。这种创新方法的特点是利用了数百万个 3D 高斯函数，它与神经辐射场 (NeRF) 方法有很大不同，神经辐射场 (NeRF) 方法主要使用隐式的基于坐标的模型将空间坐标映射到像素值。 3D GS 凭借其明确的场景表示和可微的渲染算法，不仅保证了实时渲染功能，而且还引入了前所未有的控制和可编辑性水平。这使得 3D GS 成为下一代 3D 重建和表示的潜在游戏规则改变者。在本文中，我们首次系统地概述了 3D GS 领域的最新发展和关键贡献。我们首先详细探讨 3D GS 出现背后的基本原理和驱动力，为理解其重要性奠定基础。我们讨论的一个焦点是 3D GS 的实际适用性。通过促进实时性能，3D GS 开辟了从虚拟现实到交互式媒体等众多应用程序。对此进行了补充，对领先的 3D GS 模型进行了比较分析，并在各种基准任务中进行了评估，以突出其性能和实用性。该调查最后确定了当前的挑战并提出了该领域未来研究的潜在途径。通过这项调查，我们的目标是为新手和经验丰富的研究人员提供宝贵的资源，促进在适用和明确的辐射场表示方面的进一步探索和进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.03890v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation**<br />
**Title_cn:** NeRFmentation：基于 NeRF 的单目深度估计增强<br />
**Authors:** Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald<br />
**Abstract:** <details><summary>原文: </summary>The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call "NeRFmentation", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目深度估计 (MDE) 模型的功能受到足够且多样化的数据集可用性的限制。对于自动驾驶的 MDE 模型，捕获的数据轨迹的线性会加剧这个问题。我们提出了一种基于 NeRF 的数据增强管道，将具有更多样化观察方向的合成数据引入训练数据集中，并展示我们的方法对模型性能和鲁棒性的好处。我们的数据增强管道（我们称之为“NeRFmentation”）在数据集中的每个场景上训练 NeRF，根据相关指标过滤掉低于标准的 NeRF，并使用它们生成从新观看方向捕获的合成 RGB-D 图像。在这项工作中，我们将我们的技术与三种最先进的 MDE 架构结合应用在流行的自动驾驶数据集 KITTI 上，增强了其 Eigen split 的训练集。我们在原始测试集、单独的流行驾驶集和我们自己的综合测试集上评估了最终的性能增益。</details>
**PDF:** <http://arxiv.org/pdf/2401.03771v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation**<br />
**Title_cn:** GPT-4V(ision) 是一款用于文本转 3D 生成的人性化评估器<br />
**Authors:** Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, Gordon Wetzstein<br />
**Abstract:** <details><summary>原文: </summary>Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. These metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. Conducting user preference studies is an alternative that offers both adaptability and human-aligned results. User studies, however, can be very expensive to scale. This paper presents an automatic, versatile, and human-aligned evaluation metric for text-to-3D generative models. To this end, we first develop a prompt generator using GPT-4V to generate evaluating prompts, which serve as input to compare text-to-3D models. We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria. Finally, we use these pairwise comparison results to assign these models Elo ratings. Experimental results suggest our metric strongly align with human preference across different evaluation criteria.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管文本到 3D 生成方法最近取得了进展，但仍然明显缺乏可靠的评估指标。现有的指标通常只关注一个标准，例如资产与输入文本的对齐程度。这些指标缺乏推广到不同评估标准的灵活性，并且可能与人类偏好不太相符。进行用户偏好研究是一种替代方案，可以提供适应性和人性化的结果。然而，扩展用户研究的成本可能非常昂贵。本文提出了一种用于文本转 3D 生成模型的自动、多功能且人性化的评估指标。为此，我们首先使用 GPT-4V 开发一个提示生成器来生成评估提示，作为比较文本到 3D 模型的输入。我们进一步设计了一种方法，指示 GPT-4V 根据用户定义的标准比较两个 3D 资产。最后，我们使用这些成对比较结果来为这些模型分配 Elo 评级。实验结果表明，我们的指标在不同的评估标准上与人类的偏好高度一致。</details>
**PDF:** <http://arxiv.org/pdf/2401.04092v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **TIER: Text and Image Encoder-based Regression for AIGC Image Quality Assessment**<br />
**Title_cn:** TIER：用于 AIGC 图像质量评估的基于文本和图像编码器的回归<br />
**Authors:** Jiquan Yuan, Xinyan Cao, Jinming Che, Qinyuan Wang, Sen Liang, Wei Ren, Jinlong Lin, Xixin Cao<br />
**Abstract:** <details><summary>原文: </summary>Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text and image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to extract features from these text prompts and generated images, respectively. To demonstrate the effectiveness of our proposed TIER method, we conduct extensive experiments on several mainstream AIGCIQA databases, including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed TIER method generally demonstrates superior performance compared to baseline in most cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，旨在从人类感知角度评估人工智能生成图像质量的AIGC图像质量评估（AIGCIQA）已成为计算机视觉领域的一个新课题。与常见的图像质量评估任务不同，在 AIGCIQA 任务中，图像通常由生成模型使用文本提示生成。过去几年，我们为推进 AGCIQA 做出了巨大努力。然而，大多数现有的 AIGCIQA 方法直接从各个生成的图像回归预测分数，忽略了这些图像的文本提示中包含的信息。这种疏忽部分限制了这些 AIGCIQA 方法的性能。为了解决这个问题，我们提出了一种基于文本和图像编码器的回归（TIER）框架。具体来说，我们将生成的图像及其相应的文本提示作为输入进行处理，利用文本编码器和图像编码器分别从这些文本提示和生成的图像中提取特征。为了证明我们提出的 TIER 方法的有效性，我们在几个主流 AIGCIQA 数据库上进行了广泛的实验，包括 AGIQA-1K、AGIQA-3K 和 AIGCIQA2023。实验结果表明，在大多数情况下，我们提出的 TIER 方法通常表现出优于基线的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03854v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait Synthesis**<br />
**Title_cn:** 3D-SSGAN：提升 2D 语义以实现 3D 感知构图合成<br />
**Authors:** Ruiqi Liu, Peng Zheng, Ye Wang, Rui Ma<br />
**Abstract:** <details><summary>原文: </summary>Existing 3D-aware portrait synthesis methods can generate impressive high-quality images while preserving strong 3D consistency. However, most of them cannot support the fine-grained part-level control over synthesized images. Conversely, some GAN-based 2D portrait synthesis methods can achieve clear disentanglement of facial regions, but they cannot preserve view consistency due to a lack of 3D modeling abilities. To address these issues, we propose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image synthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module maps the generated 2D part features and semantics to 3D. Then, a volume renderer with a novel 3D-aware semantic mask renderer is utilized to produce the composed face features and corresponding masks. The whole framework is trained end-to-end by discriminating between real and synthesized 2D images and their semantic masks. Quantitative and qualitative evaluations demonstrate the superiority of 3D-SSGAN in controllable part-level synthesis while preserving 3D view consistency.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的 3D 感知肖像合成方法可以生成令人印象深刻的高质量图像，同时保持强大的 3D 一致性。然而，它们中的大多数不能支持对合成图像的细粒度部分级控制。相反，一些基于 GAN 的 2D 人像合成方法可以实现面部区域的清晰解开，但由于缺乏 3D 建模能力，无法保持视图一致性。为了解决这些问题，我们提出了 3D-SSGAN，这是一种用于 3D 感知构图图像合成的新颖框架。首先，一个简单而有效的深度引导 2D 到 3D 提升模块将生成的 2D 零件特征和语义映射到 3D。然后，利用具有新颖的 3D 感知语义掩模渲染器的体积渲染器来生成合成的面部特征和相应的掩模。整个框架通过区分真实和合成的 2D 图像及其语义掩模进行端到端训练。定量和定性评估证明了 3D-SSGAN 在可控零件级合成方面的优越性，同时保持了 3D 视图的一致性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03764v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Deep Learning for Visual Neuroprosthesis**<br />
**Title_cn:** 视觉神经假体的深度学习<br />
**Authors:** Peter Beech, Shanshan Jia, Zhaofei Yu, Jian K. Liu<br />
**Abstract:** <details><summary>原文: </summary>The visual pathway involves complex networks of cells and regions which contribute to the encoding and processing of visual information. While some aspects of visual perception are understood, there are still many unanswered questions regarding the exact mechanisms of visual encoding and the organization of visual information along the pathway. This chapter discusses the importance of visual perception and the challenges associated with understanding how visual information is encoded and represented in the brain. Furthermore, this chapter introduces the concept of neuroprostheses: devices designed to enhance or replace bodily functions, and highlights the importance of constructing computational models of the visual pathway in the implementation of such devices. A number of such models, employing the use of deep learning models, are outlined, and their value to understanding visual coding and natural vision is discussed.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉通路涉及复杂的细胞和区域网络，有助于视觉信息的编码和处理。虽然视觉感知的某些方面已被了解，但关于视觉编码的确切机制和视觉信息沿通路的组织，仍然存在许多未解答的问题。本章讨论视觉感知的重要性以及与理解视觉信息如何在大脑中编码和表示相关的挑战。此外，本章介绍了神经假体的概念：旨在增强或替代身体功能的设备，并强调了在实施此类设备时构建视觉通路计算模型的重要性。概述了许多采用深度学习模型的此类模型，并讨论了它们对理解视觉编码和自然视觉的价值。</details>
**PDF:** <http://arxiv.org/pdf/2401.03639v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex**<br />
**Title_cn:** 与法学硕士一致：一种新的多模式训练范例，用于编码视觉皮层的功能磁共振成像活动<br />
**Authors:** Shuxiao Ma, Linyuan Wang, Senbao Hou, Bin Yan<br />
**Abstract:** <details><summary>原文: </summary>Recently, there has been a surge in the popularity of pre trained large language models (LLMs) (such as GPT-4), sweeping across the entire Natural Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have demonstrated advanced multi-modal understanding capabilities and showcased strong performance across various benchmarks. The LLM has started to embody traits of artificial general intelligence, which holds vital guidance for enhancing brain-like characteristics within visual encoding models. Hence, This paper proposes a new multi-modal training paradigm, aligning with LLM, for encoding fMRI activity in visual cortex. Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM). Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all stimulus images, forming a high-quality textual description set. Moreover, we use the pre-trained text encoder (CLIP) to process these detailed descriptions, obtaining the text embedding features. Next, we use the contrast loss function to minimize the distance between the image embedding features and the text embedding features to complete the alignment operation of the stimulus image and text information. With the assistance of the pre-trained LLM, this alignment process facilitates better learning of the visual encoding model, resulting in higher precision. The final experimental results indicate that our training paradigm has significantly aided in enhancing the performance of the visual encoding model.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，预训练大型语言模型 (LLM)（例如 GPT-4）的流行度激增，席卷了整个自然语言处理 (NLP) 和计算机视觉 (CV) 社区。这些法学硕士展示了先进的多模式理解能力，并在各种基准测试中展现了强劲的表现。法学硕士已开始体现通用人工智能的特征，它为增强视觉编码模型中的类脑特征提供了重要指导。因此，本文提出了一种与法学硕士相结合的新的多模式训练范式，用于编码视觉皮层的功能磁共振成像活动。基于这个范式，我们在 fMRI 数据中训练了一个编码模型，名为 LLM-视觉编码模型 (LLM-VEM)。具体来说，我们利用LLM（miniGPT4）为所有刺激图像生成描述性文本，形成高质量的文本描述集。此外，我们使用预训练的文本编码器（CLIP）来处理这些详细描述，获得文本嵌入特征。接下来，我们使用对比度损失函数来最小化图像嵌入特征和文本嵌入特征之间的距离，以完成刺激图像和文本信息的对齐操作。在预训练的LLM的帮助下，这个对齐过程有助于更好地学习视觉编码模型，从而获得更高的精度。最终的实验结果表明，我们的训练范式对增强视觉编码模型的性能有显着帮助。</details>
**PDF:** <http://arxiv.org/pdf/2401.03851v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis Plate Contact Abnormality Detection**<br />
**Title_cn:** FM-AE：用于锌电解板接触异常检测的频率屏蔽多模态自动编码器<br />
**Authors:** Canzong Zhou, Can Zhou, Hongqiu Zhu, Tianhao Liu<br />
**Abstract:** <details><summary>原文: </summary>Zinc electrolysis is one of the key processes in zinc smelting, and maintaining stable operation of zinc electrolysis is an important factor in ensuring production efficiency and product quality. However, poor contact between the zinc electrolysis cathode and the anode is a common problem that leads to reduced production efficiency and damage to the electrolysis cell. Therefore, online monitoring of the contact status of the plates is crucial for ensuring production quality and efficiency. To address this issue, we propose an end-to-end network, the Frequency-masked Multimodal Autoencoder (FM-AE). This method takes the cell voltage signal and infrared image information as input, and through automatic encoding, fuses the two features together and predicts the poor contact status of the plates through a cascaded detector. Experimental results show that the proposed method maintains high accuracy (86.2%) while having good robustness and generalization ability, effectively detecting poor contact status of the zinc electrolysis cell, providing strong support for production practice.</details>
**Abstract_cn:** <details><summary>译文: </summary>锌电解是锌冶炼的关键工序之一，保持锌电解稳定运行是保证生产效率和产品质量的重要因素。然而，锌电解阴极和阳极之间的接触不良是导致生产效率降低和电解槽损坏的常见问题。因此，在线监测板材的接触状态对于保证生产质量和效率至关重要。为了解决这个问题，我们提出了一种端到端网络，即频率掩蔽多模态自动编码器（FM-AE）。该方法以电池电压信号和红外图像信息为输入，通过自动编码，将两种特征融合在一起，并通过级联检测器预测极板的不良接触状态。实验结果表明，该方法保持了较高的准确率（86.2%），同时具有良好的鲁棒性和泛化能力，有效检测出锌电解槽的接触不良状态，为生产实践提供了有力的支持。</details>
**PDF:** <http://arxiv.org/pdf/2401.03806v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Attention-Guided Erasing: A Novel Augmentation Method for Enhancing Downstream Breast Density Classification**<br />
**Title_cn:** 注意力引导擦除：一种增强下游乳腺密度分类的新型增强方法<br />
**Authors:** Adarsh Bhandary Panambur, Hui Yu, Sheethal Bhat, Prathmesh Madhu, Siming Bayer, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>The assessment of breast density is crucial in the context of breast cancer screening, especially in populations with a higher percentage of dense breast tissues. This study introduces a novel data augmentation technique termed Attention-Guided Erasing (AGE), devised to enhance the downstream classification of four distinct breast density categories in mammography following the BI-RADS recommendation in the Vietnamese cohort. The proposed method integrates supplementary information during transfer learning, utilizing visual attention maps derived from a vision transformer backbone trained using the self-supervised DINO method. These maps are utilized to erase background regions in the mammogram images, unveiling only the potential areas of dense breast tissues to the network. Through the incorporation of AGE during transfer learning with varying random probabilities, we consistently surpass classification performance compared to scenarios without AGE and the traditional random erasing transformation. We validate our methodology using the publicly available VinDr-Mammo dataset. Specifically, we attain a mean F1-score of 0.5910, outperforming values of 0.5594 and 0.5691 corresponding to scenarios without AGE and with random erasing (RE), respectively. This superiority is further substantiated by t-tests, revealing a p-value of p<0.0001, underscoring the statistical significance of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳腺密度的评估在乳腺癌筛查中至关重要，尤其是在乳腺组织致密比例较高的人群中。本研究引入了一种称为注意力引导擦除（AGE）的新型数据增强技术，旨在遵循越南队列中的 BI-RADS 建议，增强乳房 X 光检查中四种不同乳腺密度类别的下游分类。所提出的方法在迁移学习期间集成了补充信息，利用从使用自监督 DINO 方法训练的视觉变换器骨干导出的视觉注意图。这些图用于擦除乳房X光检查图像中的背景区域，仅向网络揭示致密乳腺组织的潜在区域。通过在具有不同随机概率的迁移学习过程中结合 AGE，与没有 AGE 和传统随机擦除变换的场景相比，我们始终超越分类性能。我们使用公开的 VinDr-Mammo 数据集验证我们的方法。具体来说，我们获得了 0.5910 的平均 F1 分数，分别优于没有 AGE 和随机擦除 (RE) 情况下对应的值 0.5594 和 0.5691。 t 检验进一步证实了这种优越性，显示 p 值为 p<0.0001，强调了我们方法的统计显着性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03912v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering**<br />
**Title_cn:** STAIR：用于视频问答的具有可审核中间结果的时空推理<br />
**Authors:** Yueqian Wang, Yuxuan Wang, Kai Chen, Dongyan Zhao<br />
**Abstract:** <details><summary>原文: </summary>Recently we have witnessed the rapid development of video question answering models. However, most models can only handle simple videos in terms of temporal reasoning, and their performance tends to drop when answering temporal-reasoning questions on long and informative videos. To tackle this problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable Intermediate Results for video question answering. STAIR is a neural module network, which contains a program generator to decompose a given question into a hierarchical combination of several sub-tasks, and a set of lightweight neural modules to complete each of these sub-tasks. Though neural module networks are already widely studied on image-text tasks, applying them to videos is a non-trivial task, as reasoning on videos requires different abilities. In this paper, we define a set of basic video-text sub-tasks for video question answering and design a set of lightweight modules to complete them. Different from most prior works, modules of STAIR return intermediate outputs specific to their intentions instead of always returning attention maps, which makes it easier to interpret and collaborate with pre-trained models. We also introduce intermediate supervision to make these intermediate outputs more accurate. We conduct extensive experiments on several video question answering datasets under various settings to show STAIR's performance, explainability, compatibility with pre-trained models, and applicability when program annotations are not available. Code: https://github.com/yellow-binary-tree/STAIR</details>
**Abstract_cn:** <details><summary>译文: </summary>最近我们见证了视频问答模型的快速发展。然而，大多数模型只能在时间推理方面处理简单的视频，并且在回答长且信息丰富的视频上的时间推理问题时，其性能往往会下降。为了解决这个问题，我们提出了 STAIR，一种时空推理模型，具有用于视频问答的可审核中间结果。 STAIR 是一个神经模块网络，它包含一个程序生成器，用于将给定问题分解为多个子任务的分层组合，以及一组轻量级神经模块来完成每个子任务。尽管神经模块网络已经在图像文本任务上得到了广泛的研究，但将它们应用于视频并不是一件简单的任务，因为视频推理需要不同的能力。在本文中，我们定义了一组用于视频问答的基本视频文本子任务，并设计了一组轻量级模块来完成它们。与大多数先前的工作不同，STAIR 的模块返回特定于其意图的中间输出，而不是总是返回注意力图，这使得更容易解释和与预训练模型协作。我们还引入了中间监督，使这些中间输出更加准确。我们在不同设置下对多个视频问答数据集进行了广泛的实验，以展示 STAIR 的性能、可解释性、与预训练模型的兼容性以及程序注释不可用时的适用性。代码：https://github.com/yellow-binary-tree/STAIR</details>
**PDF:** <http://arxiv.org/pdf/2401.03901v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Gramformer: Learning Crowd Counting via Graph-Modulated Transformer**<br />
**Title_cn:** Gramformer：通过图形调制变压器学习人群计数<br />
**Authors:** Hui Lin, Zhiheng Ma, Xiaopeng Hong, Qinnan Shangguan, Deyu Meng<br />
**Abstract:** <details><summary>原文: </summary>Transformer has been popular in recent crowd counting work since it breaks the limited receptive field of traditional CNNs. However, since crowd images always contain a large number of similar patches, the self-attention mechanism in Transformer tends to find a homogenized solution where the attention maps of almost all patches are identical. In this paper, we address this problem by proposing Gramformer: a graph-modulated transformer to enhance the network by adjusting the attention and input node features respectively on the basis of two different types of graphs. Firstly, an attention graph is proposed to diverse attention maps to attend to complementary information. The graph is building upon the dissimilarities between patches, modulating the attention in an anti-similarity fashion. Secondly, a feature-based centrality encoding is proposed to discover the centrality positions or importance of nodes. We encode them with a proposed centrality indices scheme to modulate the node features and similarity relationships. Extensive experiments on four challenging crowd counting datasets have validated the competitiveness of the proposed method. Code is available at {https://github.com/LoraLinH/Gramformer}.</details>
**Abstract_cn:** <details><summary>译文: </summary>Transformer 在最近的人群统计工作中很受欢迎，因为它打破了传统 CNN 有限的感受野。然而，由于人群图像总是包含大量相似的补丁，Transformer 中的自注意力机制倾向于找到一个同质化的解决方案，其中几乎所有补丁的注意力图都是相同的。在本文中，我们通过提出 Gramformer 来解决这个问题：一种图调制变压器，通过根据两种不同类型的图分别调整注意力和输入节点特征来增强网络。首先，针对不同的注意力图提出了注意力图来关注补充信息。该图建立在补丁之间的差异之上，以反相似的方式调节注意力。其次，提出了基于特征的中心性编码来发现节点的中心性位置或重要性。我们使用提出的中心性指数方案对它们进行编码，以调整节点特征和相似性关系。对四个具有挑战性的人群计数数据集的广泛实验验证了所提出方法的竞争力。代码可在 {https://github.com/LoraLinH/Gramformer} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03870v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Monitoring water contaminants in coastal areas through ML algorithms leveraging atmospherically corrected Sentinel-2 data**<br />
**Title_cn:** 利用经大气校正的 Sentinel-2 数据通过机器学习算法监测沿海地区的水污染物<br />
**Authors:** Francesca Razzano, Francesco Mauro, Pietro Di Stasio, Gabriele Meoni, Marco Esposito, Gilda Schirinzi, Silvia Liberata Ullo<br />
**Abstract:** <details><summary>原文: </summary>Monitoring water contaminants is of paramount importance, ensuring public health and environmental well-being. Turbidity, a key parameter, poses a significant problem, affecting water quality. Its accurate assessment is crucial for safeguarding ecosystems and human consumption, demanding meticulous attention and action. For this, our study pioneers a novel approach to monitor the Turbidity contaminant, integrating CatBoost Machine Learning (ML) with high-resolution data from Sentinel-2 Level-2A. Traditional methods are labor-intensive while CatBoost offers an efficient solution, excelling in predictive accuracy. Leveraging atmospherically corrected Sentinel-2 data through the Google Earth Engine (GEE), our study contributes to scalable and precise Turbidity monitoring. A specific tabular dataset derived from Hong Kong contaminants monitoring stations enriches our study, providing region-specific insights. Results showcase the viability of this integrated approach, laying the foundation for adopting advanced techniques in global water quality management.</details>
**Abstract_cn:** <details><summary>译文: </summary>监测水污染物对于确保公众健康和环境福祉至关重要。浊度是一个关键参数，造成了影响水质的重大问题。其准确评估对于保护生态系统和人类消费至关重要，需要认真关注并采取行动。为此，我们的研究开创了一种监测浊度污染物的新方法，将 CatBoost 机器学习 (ML) 与 Sentinel-2 Level-2A 的高分辨率数据相集成。传统方法是劳动密集型的，而 CatBoost 提供了一种高效的解决方案，在预测准确性方面表现出色。我们的研究通过 Google Earth Engine (GEE) 利用经过大气校正的 Sentinel-2 数据，有助于实现可扩展且精确的浊度监测。来自香港污染物监测站的特定表格数据集丰富了我们的研究，提供了针对特定区域的见解。结果展示了这种综合方法的可行性，为在全球水质管理中采用先进技术奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.03792v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Identifying Important Group of Pixels using Interactions**<br />
**Title_cn:** 使用交互识别重要的像素组<br />
**Authors:** Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera<br />
**Abstract:** <details><summary>原文: </summary>To better understand the behavior of image classifiers, it is useful to visualize the contribution of individual pixels to the model prediction. In this study, we propose a method, MoXI~($\textbf{Mo}$del e$\textbf{X}$planation by $\textbf{I}$nteractions), that efficiently and accurately identifies a group of pixels with high prediction confidence. The proposed method employs game-theoretic concepts, Shapley values and interactions, taking into account the effects of individual pixels and the cooperative influence of pixels on model confidence. Theoretical analysis and experiments demonstrate that our method better identifies the pixels that are highly contributing to the model outputs than widely-used visualization methods using Grad-CAM, Attention rollout, and Shapley value. While prior studies have suffered from the exponential computational cost in the computation of Shapley value and interactions, we show that this can be reduced to linear cost for our task.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了更好地理解图像分类器的行为，可视化单个像素对模型预测的贡献非常有用。在本研究中，我们提出了一种方法 MoXI~($\textbf{Mo}$del e$\textbf{X}$planation by $\textbf{I}$nteractions)，该方法可以高效、准确地识别一组像素高预测置信度。该方法采用博弈论概念、Shapley 值和交互作用，考虑到单个像素的影响以及像素对模型置信度的协同影响。理论分析和实验表明，与广泛使用的使用 Grad-CAM、Attention rollout 和 Shapley 值的可视化方法相比，我们的方法可以更好地识别对模型输出贡献较大的像素。虽然之前的研究在计算 Shapley 值和相互作用时遇到了指数计算成本的问题，但我们表明，对于我们的任务来说，这可以减少为线性成本。</details>
**PDF:** <http://arxiv.org/pdf/2401.03785v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring**<br />
**Title_cn:** FMA-Net：流引导动态过滤和迭代特征细化，用于联合视频超分辨率和去模糊<br />
**Authors:** Geunhyuk Youk, Jihyong Oh, Munchurl Kim<br />
**Abstract:** <details><summary>原文: </summary>We present a joint learning scheme of video super-resolution and deblurring, called VSRDB, to restore clean high-resolution (HR) videos from blurry low-resolution (LR) ones. This joint restoration problem has drawn much less attention compared to single restoration problems. In this paper, we propose a novel flow-guided dynamic filtering (FGDF) and iterative feature refinement with multi-attention (FRMA), which constitutes our VSRDB framework, denoted as FMA-Net. Specifically, our proposed FGDF enables precise estimation of both spatio-temporally-variant degradation and restoration kernels that are aware of motion trajectories through sophisticated motion representation learning. Compared to conventional dynamic filtering, the FGDF enables the FMA-Net to effectively handle large motions into the VSRDB. Additionally, the stacked FRMA blocks trained with our novel temporal anchor (TA) loss, which temporally anchors and sharpens features, refine features in a course-to-fine manner through iterative updates. Extensive experiments demonstrate the superiority of the proposed FMA-Net over state-of-the-art methods in terms of both quantitative and qualitative quality. Codes and pre-trained models are available at: https://kaist-viclab.github.io/fmanet-site</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种视频超分辨率和去模糊的联合学习方案，称为 VSRDB，用于从模糊的低分辨率 (LR) 视频中恢复干净的高分辨率 (HR) 视频。与单一恢复问题相比，这种联合恢复问题引起的关注要少得多。在本文中，我们提出了一种新颖的流引导动态过滤（FGDF）和多注意力迭代特征细化（FRMA），它构成了我们的 VSRDB 框架，表示为 FMA-Net。具体来说，我们提出的 FGDF 能够精确估计时空变化的退化和恢复内核，这些内核通过复杂的运动表示学习来了解运动轨迹。与传统的动态过滤相比，FGDF 使 FMA-Net 能够有效处理 VSRDB 中的大运动。此外，使用我们新颖的时间锚点（TA）损失训练的堆叠 FRMA 块可以暂时锚定和锐化特征，通过迭代更新以从粗到细的方式细化特征。大量的实验证明了所提出的 FMA-Net 在定量和定性质量方面均优于最先进的方法。代码和预训练模型可在以下位置获取：https://kaist-viclab.github.io/fmanet-site</details>
**PDF:** <http://arxiv.org/pdf/2401.03707v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **GloTSFormer: Global Video Text Spotting Transformer**<br />
**Title_cn:** GloTSFormer：全球视频文本识别变压器<br />
**Authors:** Han Wang, Yanjie Wang, Yang Li, Can Huang<br />
**Abstract:** <details><summary>原文: </summary>Video Text Spotting (VTS) is a fundamental visual task that aims to predict the trajectories and content of texts in a video. Previous works usually conduct local associations and apply IoU-based distance and complex post-processing procedures to boost performance, ignoring the abundant temporal information and the morphological characteristics in VTS. In this paper, we propose a novel Global Video Text Spotting Transformer GloTSFormer to model the tracking problem as global associations and utilize the Gaussian Wasserstein distance to guide the morphological correlation between frames. Our main contributions can be summarized as three folds. 1). We propose a Transformer-based global tracking method GloTSFormer for VTS and associate multiple frames simultaneously. 2). We introduce a Wasserstein distance-based method to conduct positional associations between frames. 3). We conduct extensive experiments on public datasets. On the ICDAR2015 video dataset, GloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the previous SOTA method and outperforms the previous Transformer-based method by a significant 8.3 MOTA.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频文本识别（VTS）是一项基本的视觉任务，旨在预测视频中文本的轨迹和内容。以前的工作通常进行局部关联，并应用基于 IoU 的距离和复杂的后处理程序来提高性能，忽略了 VTS 中丰富的时间信息和形态特征。在本文中，我们提出了一种新颖的全局视频文本识别转换器 GloTSFormer，将跟踪问题建模为全局关联，并利用高斯 Wasserstein 距离来指导帧之间的形态相关性。我们的主要贡献可以概括为三个方面。 1）。我们提出了一种基于 Transformer 的 VTS 全局跟踪方法 GloTSFormer，并同时关联多个帧。 2）。我们引入了一种基于 Wasserstein 距离的方法来进行帧之间的位置关联。 3）。我们对公共数据集进行了广泛的实验。在 ICDAR2015 视频数据集上，GloTSFormer 实现了 56.0 MOTA，与之前的 SOTA 方法相比，绝对提升了 4.6，并且比之前基于 Transformer 的方法显着提高了 8.3 MOTA。</details>
**PDF:** <http://arxiv.org/pdf/2401.03694v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease**<br />
**Title_cn:** 用于阿尔茨海默氏病建模和分类的结构聚焦神经变性卷积神经网络<br />
**Authors:** Simisola Odimayo, Chollette C. Olisah, Khadija Mohammed<br />
**Abstract:** <details><summary>原文: </summary>Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints of the structure-focused Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Through experiments, our proposed machine learning framework shows exceptional performance. The parasagittal viewpoint set achieves 97.8% accuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal viewpoint is shown to present deeper insights into the structural brain changes given the increase in accuracy, specificity, and sensitivity, which are 98.1% 97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our proposed model is capable of capturing the structural dynamics of MCI and AD which exist about the frontal lobe, occipital lobe, cerebellum, and parietal lobe. Therefore, our model itself as a potential brain structural change Digi-Biomarker for early diagnosis of AD.</details>
**Abstract_cn:** <details><summary>译文: </summary>阿尔茨海默氏病 (AD) 是痴呆症的主要形式，给全球带来了日益严峻的挑战，并强调了准确和早期诊断的紧迫性。放射科医生采用机器共振成像 (MRI) 来区分轻度认知障碍 (MCI) 和 AD 的临床技术遇到了障碍，因为它们不一致且不可靠。机器学习已被证明为早期 AD 诊断提供了希望。然而，现有模型专注于局灶性细粒度特征，而没有考虑提供大脑皮层神经变性信息的局灶性结构特征。因此，本文提出了一种机器学习（ML）框架，该框架集成了伽玛校正（一种图像增强技术），并包括一种名为SNeurodCNN的专注于结构的神经变性卷积神经网络（CNN）架构，用于区分AD和MCI。机器学习框架利用以结构为重点的阿尔茨海默氏病神经影像计划 (ADNI) 数据集的中矢状和旁矢状脑图像观点。通过实验，我们提出的机器学习框架显示出卓越的性能。旁矢状视点集的准确度达到 97.8%，特异性为 97.0%，灵敏度为 98.5%。由于准确性、特异性和敏感性的提高，正中矢状视点可以更深入地了解大脑结构的变化，分别为 98.1%、97.2% 和 99.0%。使用 GradCAM 技术，我们表明我们提出的模型能够捕获存在于额叶、枕叶、小脑和顶叶的 MCI 和 AD 的结构动态。因此，我们的模型本身可以作为潜在的大脑结构变化的数字生物标记，用于 AD 的早期诊断。</details>
**PDF:** <http://arxiv.org/pdf/2401.03922v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **InvariantOODG: Learning Invariant Features of Point Clouds for Out-of-Distribution Generalization**<br />
**Title_cn:** InvariantOODG：学习点云的不变特征以实现分布外泛化<br />
**Authors:** Zhimin Zhang, Xiang Gao, Wei Hu<br />
**Abstract:** <details><summary>原文: </summary>The convenience of 3D sensors has led to an increase in the use of 3D point clouds in various applications. However, the differences in acquisition devices or scenarios lead to divergence in the data distribution of point clouds, which requires good generalization of point cloud representation learning methods. While most previous methods rely on domain adaptation, which involves fine-tuning pre-trained models on target domain data, this may not always be feasible in real-world scenarios where target domain data may be unavailable. To address this issue, we propose InvariantOODG, which learns invariability between point clouds with different distributions using a two-branch network to extract local-to-global features from original and augmented point clouds. Specifically, to enhance local feature learning of point clouds, we define a set of learnable anchor points that locate the most useful local regions and two types of transformations to augment the input point clouds. The experimental results demonstrate the effectiveness of the proposed model on 3D domain generalization benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 传感器的便利性导致 3D 点云在各种应用中的使用增加。然而，采集设备或场景的差异导致点云数据分布的发散，这就需要点云表示学习方法具有良好的泛化性。虽然大多数以前的方法依赖于域适应，这涉及对目标域数据的预训练模型进行微调，但这在目标域数据可能不可用的现实场景中可能并不总是可行。为了解决这个问题，我们提出了 InvariantOODG，它使用两分支网络从原始点云和增强点云中提取局部到全局特征来学习不同分布的点云之间的不变性。具体来说，为了增强点云的局部特征学习，我们定义了一组可学习的锚点来定位最有用的局部区域，并定义了两种类型的变换来增强输入点云。实验结果证明了所提出的模型在 3D 域泛化基准上的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03765v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Sur2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images**<br />
**Title_cn:** Sur2f：从多视图图像中实现高质量和高效表面重建的混合表示<br />
**Authors:** Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia<br />
**Abstract:** <details><summary>原文: </summary>Multi-view surface reconstruction is an ill-posed, inverse problem in 3D vision research. It involves modeling the geometry and appearance with appropriate surface representations. Most of the existing methods rely either on explicit meshes, using surface rendering of meshes for reconstruction, or on implicit field functions, using volume rendering of the fields for reconstruction. The two types of representations in fact have their respective merits. In this work, we propose a new hybrid representation, termed Sur2f, aiming to better benefit from both representations in a complementary manner. Technically, we learn two parallel streams of an implicit signed distance field and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the implicit signed distance function (SDF) and surface rendering of the surrogate mesh with a shared, neural shader; the unified shading promotes their convergence to the same, underlying surface. We synchronize learning of the surrogate mesh by driving its deformation with functions induced from the implicit SDF. In addition, the synchronized surrogate mesh enables surface-guided volume sampling, which greatly improves the sampling efficiency per ray in volume rendering. We conduct thorough experiments showing that Sur$^2$f outperforms existing reconstruction methods and surface representations, including hybrid ones, in terms of both recovery quality and recovery efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>多视图表面​​重建是 3D 视觉研究中的不适定逆问题。它涉及使用适当的表面表示对几何形状和外观进行建模。大多数现有方法要么依赖于显式网格，使用网格的表面渲染进行重建，要么依赖于隐式场函数，使用场的体渲染进行重建。这两种表述其实各有各的优点。在这项工作中，我们提出了一种新的混合表示，称为 Sur2f，旨在以互补的方式更好地从两种表示中受益。从技术上讲，我们学习隐式符号距离场和显式代理表面 Sur2f 网格的两个并行流，并使用共享的神经着色器统一隐式符号距离函数 (SDF) 的体积渲染和代理网格的表面渲染；统一的阴影促进它们收敛到相同的下表面。我们通过用隐式 SDF 导出的函数驱动代理网格的变形来同步代理网格的学习。此外，同步的代理网格可以实现表面引导的体积采样，这大大提高了体渲染中每条光线的采样效率。我们进行了彻底的实验，表明 Sur$^2$f 在恢复质量和恢复效率方面均优于现有的重建方法和表面表示（包括混合方法）。</details>
**PDF:** <http://arxiv.org/pdf/2401.03704v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving**<br />
**Title_cn:** DME-Driver：在自动驾驶中集成人类决策逻辑和 3D 场景感知<br />
**Authors:** Wencheng Han, Dongqian Guo, Cheng-Zhong Xu, Jianbing Shen<br />
**Abstract:** <details><summary>原文: </summary>In the field of autonomous driving, two important features of autonomous driving car systems are the explainability of decision logic and the accuracy of environmental perception. This paper introduces DME-Driver, a new autonomous driving system that enhances the performance and reliability of autonomous driving system. DME-Driver utilizes a powerful vision language model as the decision-maker and a planning-oriented perception model as the control signal generator. To ensure explainable and reliable driving decisions, the logical decision-maker is constructed based on a large vision language model. This model follows the logic employed by experienced human drivers and makes decisions in a similar manner. On the other hand, the generation of accurate control signals relies on precise and detailed environmental perception, which is where 3D scene perception models excel. Therefore, a planning oriented perception model is employed as the signal generator. It translates the logical decisions made by the decision-maker into accurate control signals for the self-driving cars. To effectively train the proposed model, a new dataset for autonomous driving was created. This dataset encompasses a diverse range of human driver behaviors and their underlying motivations. By leveraging this dataset, our model achieves high-precision planning accuracy through a logical thinking process.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶领域，自动驾驶汽车系统的两个重要特征是决策逻辑的可解释性和环境感知的准确性。本文介绍了一种新型自动驾驶系统DME-Driver，可增强自动驾驶系统的性能和可靠性。 DME-Driver利用强大的视觉语言模型作为决策者，利用面向规划的感知模型作为控制信号生成器。为了确保驾驶决策可解释且可靠，逻辑决策器是基于大型视觉语言模型构建的。该模型遵循经验丰富的人类驾驶员所采用的逻辑，并以类似的方式做出决策。另一方面，精确控制信号的生成依赖于精确、详细的环境感知，而这正是 3D 场景感知模型的优势所在。因此，采用面向规划的感知模型作为信号发生器。它将决策者做出的逻辑决策转化为自动驾驶汽车的准确控制信号。为了有效地训练所提出的模型，创建了一个新的自动驾驶数据集。该数据集包含各种人类驾驶员行为及其潜在动机。通过利用该数据集，我们的模型通过逻辑思维过程实现了高精度的规划准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.03641v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **RudolfV: A Foundation Model by Pathologists for Pathologists**<br />
**Title_cn:** RudolfV：病理学家为病理学家提供的基础模型<br />
**Authors:** Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg, Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Philipp Jurmeister, David Horst, Lukas Ruff, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and (3) to augment the input images during training. We evaluate the resulting model on a set of public and internal benchmarks and show that although our foundation model is trained with an order of magnitude less slides, it performs on par or better than competing models. We expect that scaling our approach to more data and larger models will further increase its performance and capacity to deal with increasingly complex real world tasks in diagnostics and biomedical research.</details>
**Abstract_cn:** <details><summary>译文: </summary>组织病理学在临床医学和生物医学研究中发挥着核心作用。尽管人工智能在许多病理任务上显示出有希望的结果，但泛化和处理训练数据稀缺的罕见疾病仍然是一个挑战。在从可能有限的标记数据中学习之前，将未标记数据中的知识提取到基础模型中，为解决这些挑战提供了一条可行的途径。在这项工作中，我们通过半自动数据管理和结合病理学家领域知识，扩展了数字病理学全幻灯片图像基础模型的最新技术。具体来说，我们结合计算和病理学家领域知识 (1) 来整理包含 103,000 张幻灯片的多样化数据集，对应于 7.5 亿个图像块，涵盖来自不同固定、染色和扫描协议的数据以及来自欧盟和欧洲不同适应症和实验室的数据。 US，(2) 用于对语义相似的幻灯片和组织块进行分组，以及 (3) 在训练期间增强输入图像。我们在一组公共和内部基准上评估了结果模型，结果表明，尽管我们的基础模型是用少一个数量级的幻灯片进行训练的，但它的性能与竞争模型相当或更好。我们预计，将我们的方法扩展到更多数据和更大的模型将进一步提高其性能和能力，以处理诊断和生物医学研究中日益复杂的现实世界任务。</details>
**PDF:** <http://arxiv.org/pdf/2401.04079v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fun with Flags: Robust Principal Directions via Flag Manifolds**<br />
**Title_cn:** 旗帜的乐趣：通过旗帜流形实现稳健的主要方向<br />
**Authors:** Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal<br />
**Abstract:** <details><summary>原文: </summary>Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations. The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types. Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold. Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds.</details>
**Abstract_cn:** <details><summary>译文: </summary>主成分分析 (PCA) 及其对流形和异常污染数据的扩展在计算机视觉和机器学习中是不可或缺的。在这项工作中，我们提出了 PCA 及其变体的统一形式，并引入了一个基于线性子空间标志的框架，即维度递增的嵌套线性子空间的层次结构，它不仅允许通用实现，而且还产生以前没有探索过的新颖变体。我们首先概括传统的 PCA 方法，这些方法可以最大化方差或最小化重建误差。我们通过考虑异常值和数据流形来扩展这些解释，以开发一系列新的降维算法。为了设计一种通用的计算方法，我们将稳健的对偶形式的 PCA 重新设计为标志流形上的优化问题。然后，我们将主测地线分析（切线 PCA）的切线空间近似集成到这个基于标志的框架中，创建新颖的鲁棒和双测地线 PCA 变体。这里引入的“标记”提供了显着的灵活性，可以通过特定标记类型识别更多的算法变体。最后但并非最不重要的一点是，我们为这些采用 Stiefel 流形的标志公式提出了一种有效的收敛求解器。我们在现实世界和合成场景上的实证结果证明了我们的新颖算法的优越性，特别是在对流形异常值的鲁棒性方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.04071v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Behavioural Cloning in VizDoom**<br />
**Title_cn:** VizDoom 中的行为克隆<br />
**Authors:** Ryan Spick, Timothy Bradley, Ayush Raina, Pierluigi Vito Amadori, Guy Moss<br />
**Abstract:** <details><summary>原文: </summary>This paper describes methods for training autonomous agents to play the game "Doom 2" through Imitation Learning (IL) using only pixel data as input. We also explore how Reinforcement Learning (RL) compares to IL for humanness by comparing camera movement and trajectory data. Through behavioural cloning, we examine the ability of individual models to learn varying behavioural traits. We attempt to mimic the behaviour of real players with different play styles, and find we can train agents that behave aggressively, passively, or simply more human-like than traditional AIs. We propose these methods of introducing more depth and human-like behaviour to agents in video games. The trained IL agents perform on par with the average players in our dataset, whilst outperforming the worst players. While performance was not as strong as common RL approaches, it provides much stronger human-like behavioural traits to the agent.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文描述了仅使用像素数据作为输入，通过模仿学习（IL）训练自主代理玩游戏“Doom 2”的方法。我们还通过比较相机运动和轨迹数据，探讨强化学习 (RL) 与 IL 的人性比较。通过行为克隆，我们检查个体模型学习不同行为特征的能力。我们尝试模仿具有不同游戏风格的真实玩家的行为，并发现我们可以训练出比传统人工智能表现得更具攻击性、被动性或更像人类的智能体。我们提出了这些方法，为视频游戏中的代理引入更多深度和类人行为。经过训练的 IL 智能体的表现与我们数据集中的平均玩家相当，同时超过了最差的玩家。虽然性能不如常见的强化学习方法那么强大，但它为智能体提供了更强的类人行为特征。</details>
**PDF:** <http://arxiv.org/pdf/2401.03993v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware Analysis**<br />
**Title_cn:** 数据驱动的光谱重建的局限性——光学感知分析<br />
**Authors:** Qiang Fu, Matheus Souza, Eunsue Choi, Suhyun Shin, Seung-Hwan Baek, Wolfgang Heidrich<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral imaging empowers computer vision systems with the distinct capability of identifying materials through recording their spectral signatures. Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware.   In this paper we systematically analyze the performance of such methods, evaluating both the practical limitations with respect to current datasets and overfitting, as well as fundamental limits with respect to the nature of the information encoded in the RGB images, and the dependency of this information on the optical system of the camera.   We find that the current models are not robust under slight variations, e.g., in noise level or compression of the RGB file. Both the methods and the datasets are also limited in their ability to cope with metameric colors. This issue can in part be overcome with metameric data augmentation. Moreover, optical lens aberrations can help to improve the encoding of the metameric information into the RGB image, which paves the road towards higher performing spectral imaging and reconstruction approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱成像使计算机视觉系统具有通过记录材料的光谱特征来识别材料的独特能力。最近在数据驱动的光谱重建方面的努力旨在从经济高效的 RGB 相机捕获的 RGB 图像中提取光谱信息，而不是专用硬件。在本文中，我们系统地分析了此类方法的性能，评估了当前数据集和过度拟合的实际限制，以及 RGB 图像中编码信息的性质的基本限制，以及该信息的依赖性关于相机的光学系统。我们发现当前模型在轻微变化（例如噪声水平或 RGB 文件压缩）下并不稳健。这些方法和数据集处理同色异谱颜色的能力也受到限制。这个问题可以通过同色异谱数据增强来部分解决。此外，光学镜头像差有助于改善同色异谱信息在 RGB 图像中的编码，这为实现更高性能的光谱成像和重建方法铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2401.03835v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **A foundation for exact binarized morphological neural networks**<br />
**Title_cn:** 精确二值化形态神经网络的基础<br />
**Authors:** Theodore Aouad, Hugues Talbot<br />
**Abstract:** <details><summary>原文: </summary>Training and running deep neural networks (NNs) often demands a lot of computation and energy-intensive specialized hardware (e.g. GPU, TPU...). One way to reduce the computation and power cost is to use binary weight NNs, but these are hard to train because the sign function has a non-smooth gradient. We present a model based on Mathematical Morphology (MM), which can binarize ConvNets without losing performance under certain conditions, but these conditions may not be easy to satisfy in real-world scenarios. To solve this, we propose two new approximation methods and develop a robust theoretical framework for ConvNets binarization using MM. We propose as well regularization losses to improve the optimization. We empirically show that our model can learn a complex morphological network, and explore its performance on a classification task.</details>
**Abstract_cn:** <details><summary>译文: </summary>训练和运行深度神经网络 (NN) 通常需要大量计算和能源密集型专用硬件（例如 GPU、TPU...）。减少计算和功耗成本的一种方法是使用二元权重神经网络，但这些神经网络很难训练，因为符号函数具有非平滑梯度。我们提出了一种基于数学形态学（MM）的模型，它可以在某些条件下对ConvNet进行二值化而不损失性能，但这些条件在现实场景中可能不容易满足。为了解决这个问题，我们提出了两种新的近似方法，并为使用 MM 的 ConvNets 二值化开发了一个强大的理论框架。我们还提出正则化损失来改进优化。我们凭经验表明，我们的模型可以学习复杂的形态网络，并探索其在分类任务上的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.03830v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Gnuastro: visualizing the full dynamic range in color images**<br />
**Title_cn:** Gnuastro：可视化彩色图像的完整动态范围<br />
**Authors:** Raúl Infante-Sainz, Mohammad Akhlaghi<br />
**Abstract:** <details><summary>原文: </summary>Color plays a crucial role in the visualization, interpretation, and analysis of multi-wavelength astronomical images. However, generating color images that accurately represent the full dynamic range of astronomical sources is challenging. In response, Gnuastro v0.22 introduces the program 'astscript-color-faint-gray', which is extensively documented in the Gnuastro manual. It employs a non-linear transformation to assign an 8-bit RGB (Red-Green-Blue) value to brighter pixels, while the fainter ones are shown in an inverse grayscale. This approach enables the simultaneous visualization of low surface brightness features within the same image. This research note is reproducible with Maneage, on the Git commit 48f5408.</details>
**Abstract_cn:** <details><summary>译文: </summary>颜色在多波长天文图像的可视化、解释和分析中起着至关重要的作用。然而，生成准确代表天文源的完整动态范围的彩色图像具有挑战性。作为回应，Gnuastro v0.22 引入了程序“astscript-color-faint-gray”，该程序在 Gnuastro 手册中有详细记录。它采用非线性变换将 8 位 RGB（红-绿-蓝）值分配给较亮的像素，而较暗的像素则以反灰度显示。这种方法可以同时可视化同一图像中的低表面亮度特征。这份研究报告可以通过 Maneage 在 Git 提交 48f5408 上重现。</details>
**PDF:** <http://arxiv.org/pdf/2401.03814v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy Degradation**<br />
**Title_cn:** MvKSR：多视图知识引导的雾霾和雨天退化场景恢复<br />
**Authors:** Dong Yang, Wenyu Xu, Yuxu Lu, Yuan Gao, Jingming Zhang, Yu Guo<br />
**Abstract:** <details><summary>原文: </summary>High-quality imaging is crucial for ensuring safety supervision and intelligent deployment in fields like transportation and industry. It enables precise and detailed monitoring of operations, facilitating timely detection of potential hazards and efficient management. However, adverse weather conditions, such as atmospheric haziness and precipitation, can have a significant impact on image quality. When the atmosphere contains dense haze or water droplets, the incident light scatters, leading to degraded captured images. This degradation is evident in the form of image blur and reduced contrast, increasing the likelihood of incorrect assessments and interpretations by intelligent imaging systems (IIS). To address the challenge of restoring degraded images in hazy and rainy conditions, this paper proposes a novel multi-view knowledge-guided scene recovery network (termed MvKSR). Specifically, guided filtering is performed on the degraded image to separate high/low-frequency components. Subsequently, an en-decoder-based multi-view feature coarse extraction module (MCE) is used to coarsely extract features from different views of the degraded image. The multi-view feature fine fusion module (MFF) will learn and infer the restoration of degraded images through mixed supervision under different views. Additionally, we suggest an atrous residual block to handle global restoration and local repair in hazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR outperforms other state-of-the-art methods in terms of efficiency and stability for restoring degraded scenarios in IIS.</details>
**Abstract_cn:** <details><summary>译文: </summary>高质量成像对于交通、工业等领域的安全监管和智能部署至关重要。它可以对操作进行精确、详细的监控，有利于及时发现潜在危险并进行高效管理。然而，恶劣的天气条件，例如大气雾霾和降水，会对图像质量产生重大影响。当大气中含有浓雾或水滴时，入射光会发生散射，导致捕获的图像质量下降。这种退化以图像模糊和对比度降低的形式表现出来，增加了智能成像系统 (IIS) 错误评估和解释的可能性。为了解决在雾霾和雨天条件下恢复退化图像的挑战，本文提出了一种新颖的多视图知识引导场景恢复网络（称为MvKSR）。具体来说，对退化图像进行引导滤波以分离高频/低频分量。随后，使用基于编码器的多视图特征粗略提取模块（MCE）从退化图像的不同视图中粗略提取特征。多视图特征精细融合模块（MFF）将通过不同视图下的混合监督来学习和推断退化图像的恢复。此外，我们建议使用一个空洞的残差块来处理雾霾/雨天/混合场景中的全局恢复和局部修复。大量的实验结果表明，MvKSR 在恢复 IIS 降级场景的效率和稳定性方面优于其他最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.03800v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion**<br />
**Title_cn:** 通过 CLIP-傅立叶引导小波扩散实现低光图像增强<br />
**Authors:** Minglong Xue, Jinhong He, Yanyi He, Zhipu Liu, Wenhai Wang, Mingliang Zhou<br />
**Abstract:** <details><summary>原文: </summary>Low-light image enhancement techniques have significantly progressed, but unstable image quality recovery and unsatisfactory visual perception are still significant challenges. To solve these problems, we propose a novel and robust low-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion, abbreviated as CFWD. Specifically, we design a guided network with a multiscale visual language in the frequency domain based on the wavelet transform to achieve effective image enhancement iteratively. In addition, we combine the advantages of Fourier transform in detail perception to construct a hybrid frequency domain space with significant perceptual capabilities(HFDPM). This operation guides wavelet diffusion to recover the fine-grained structure of the image and avoid diversity confusion. Extensive quantitative and qualitative experiments on publicly available real-world benchmarks show that our method outperforms existing state-of-the-art methods and better reproduces images similar to normal images. Code is available at https://github.com/He-Jinhong/CFWD.</details>
**Abstract_cn:** <details><summary>译文: </summary>低光图像增强技术已经取得了显着进步，但不稳定的图像质量恢复和不令人满意的视觉感知仍然是重大挑战。为了解决这些问题，我们通过 CLIP-傅立叶引导小波扩散（缩写为 CFWD）提出了一种新颖且鲁棒的低光图像增强方法。具体来说，我们设计了一个基于小波变换的频域多尺度视觉语言的引导网络，以迭代地实现有效的图像增强。此外，我们结合傅里叶变换在细节感知方面的优势，构建了具有显着感知能力的混合频域空间（HFDPM）。该操作引导小波扩散来恢复图像的细粒度结构并避免多样性混乱。对公开的现实世界基准进行的广泛的定量和定性实验表明，我们的方法优于现有的最先进的方法，并且更好地再现了与正常图像相似的图像。代码可在 https://github.com/He-Jinhong/CFWD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.03788v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Machine Learning Applications in Traumatic Brain Injury Diagnosis and Prognosis: A Spotlight on Mild TBI and CT Imaging**<br />
**Title_cn:** 机器学习在创伤性脑损伤诊断和预后中的应用：聚焦轻度 TBI 和 CT 成像<br />
**Authors:** Hanem Ellethy, Shekhar S. Chandra, Viktor Vegh<br />
**Abstract:** <details><summary>原文: </summary>Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis and prognosis of TBI relies on a combination of clinical and imaging data often acquired using a Computed Tomography (CT) scanner. Addressing the multifaceted challenges posed by TBI requires innovative, data-driven approaches, for this complex condition. As such, we provide a summary of the state-of-the-art Machine Learning (ML) and Deep Learning (DL) techniques applied to clinical and images in TBI, with a particular focus on mild TBI (mTBI). We explore the rich spectrum of ML and DL techniques used and highlight their impact in TBI . We categorize ML and DL methods by TBI severity and showcase their application in mTBI and moderate-to-severe TBI scenarios. Finally, we emphasize the role of ML and DL in mTBI diagnosis, where conventional methods often fall short, and comment on the potential of CT-based ML applications in TBI. This review may serve as a source of inspiration for future research endeavours aimed at improving the diagnosis and prognosis of TBI.</details>
**Abstract_cn:** <details><summary>译文: </summary>创伤性脑损伤（TBI）对全球公共卫生构成重大挑战，导致高发病率和死亡率，并给全球医疗保健系统带来沉重的经济负担。 TBI 的诊断和预后依赖于通常使用计算机断层扫描 (CT) 扫描仪获取的临床和影像数据的结合。针对这种复杂的情况，应对 TBI 带来的多方面挑战需要创新的数据驱动方法。因此，我们总结了应用于 TBI 临床和图像的最先进的机器学习 (ML) 和深度学习 (DL) 技术，特别关注轻度 TBI (mTBI)。我们探索了所使用的丰富的 ML 和 DL 技术，并强调了它们在 TBI 中的影响。我们根据 TBI 严重程度对 ML 和 DL 方法进行分类，并展示它们在 mTBI 和中重度 TBI 场景中的应用。最后，我们强调了 ML 和 DL 在 mTBI 诊断中的作用（传统方法往往无法满足这一要求），并评论了基于 CT 的 ML 在 TBI 中应用的潜力。这篇综述可能为未来旨在改善 TBI 诊断和预后的研究工作提供灵感。</details>
**PDF:** <http://arxiv.org/pdf/2401.03621v1><br />
**Code:** null<br />

