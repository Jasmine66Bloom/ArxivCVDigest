# !UPDATED  -- 2024-01-09

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks**<br />
**Title_cn:** 通过生成对抗网络推进事前可解释模型<br />
**Authors:** Tanmay Garg, Deepika Vemuri, Vineeth N Balasubramanian<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this work presents a significant step towards building inherently interpretable deep vision models with task-aligned concept representations - a key enabler for developing trustworthy AI for real-world perception tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种新颖的概念学习框架，用于增强视觉分类任务中的模型可解释性和性能。我们的方法将无监督解释生成器附加到主分类器网络，并利用对抗性训练。在训练过程中，解释模块经过优化，可以从分类器的潜在表示中提取视觉概念，而基于 GAN 的模块旨在区分从概念生成的图像和真实图像。这种联合训练方案使模型能够隐式地将其内部学习的概念与人类可解释的视觉属性对齐。综合实验证明了我们方法的稳健性，同时产生了连贯的概念激活。我们分析学到的概念，显示它们与物体部分和视觉属性的语义一致性。我们还研究对抗性训练协议中的扰动如何影响分类和概念获取。总之，这项工作在构建具有与任务相关的概念表示的本质上可解释的深度视觉模型方面迈出了重要一步，这是为现实世界感知任务开发值得信赖的人工智能的关键推动因素。</details>
**PDF:** <http://arxiv.org/pdf/2401.04647v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Effective pruning of web-scale datasets based on complexity of concept clusters**<br />
**Title_cn:** 基于概念簇复杂度的网络规模数据集的有效剪枝<br />
**Authors:** Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, Ari S. Morcos<br />
**Abstract:** <details><summary>原文: </summary>Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用大规模网络规模的数据集使机器学习模型获得了前所未有的性能提升，但也对其训练提出了异常的计算要求。为了提高训练和数据效率，我们在这里突破了修剪大规模多模态数据集以训练 CLIP 式模型的极限。当今 ImageNet 上最有效的修剪方法根据数据样本的嵌入将数据样本聚类成单独的概念，并修剪掉最典型的样本。我们将此方法扩展到 LAION 并通过注意到剪枝率应该针对特定概念并适应概念的复杂性来改进它。使用简单直观的复杂性衡量标准，我们能够将培训成本降低到常规培训的四分之一。通过从 LAION 数据集进行过滤，我们发现对较小的高质量数据集进行训练可以带来更高的性能，同时显着降低训练成本。更具体地说，我们能够在 ImageNet 零样本精度上超越 LAION 训练的 OpenCLIP-ViT-B32 模型 1.1p.p。而只使用了 27.7% 的数据和训练计算。尽管训练成本大幅降低，但我们也看到了 ImageNet dist 的改进。轮班、检索任务和 VTAB。在 DataComp Medium 基准测试中，我们在 38 项评估任务中实现了最先进的 ImageNet 零样本精度和具有竞争力的平均零样本精度。</details>
**PDF:** <http://arxiv.org/pdf/2401.04578v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Iterative Feedback Network for Unsupervised Point Cloud Registration**<br />
**Title_cn:** 用于无监督点云配准的迭代反馈网络<br />
**Authors:** Yifan Xie, Boyu Wang, Shiqi Li, Jihua Zhu<br />
**Abstract:** <details><summary>原文: </summary>As a fundamental problem in computer vision, point cloud registration aims to seek the optimal transformation for aligning a pair of point clouds. In most existing methods, the information flows are usually forward transferring, thus lacking the guidance from high-level information to low-level information. Besides, excessive high-level information may be overly redundant, and directly using it may conflict with the original low-level information. In this paper, we propose a novel Iterative Feedback Network (IFNet) for unsupervised point cloud registration, in which the representation of low-level features is efficiently enriched by rerouting subsequent high-level features. Specifically, our IFNet is built upon a series of Feedback Registration Block (FRB) modules, with each module responsible for generating the feedforward rigid transformation and feedback high-level features. These FRB modules are cascaded and recurrently unfolded over time. Further, the Feedback Transformer is designed to efficiently select relevant information from feedback high-level features, which is utilized to refine the low-level features. What's more, we incorporate a geometry-awareness descriptor to empower the network for making full use of most geometric information, which leads to more precise registration results. Extensive experiments on various benchmark datasets demonstrate the superior registration performance of our IFNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为计算机视觉中的一个基本问题，点云配准旨在寻求对齐一对点云的最佳变换。现有的大多数方法中，信息流通常是前向传递的，缺乏从高层信息到低层信息的引导。此外，过多的高层信息可能会过于冗余，直接使用可能会与原有的低层信息发生冲突。在本文中，我们提出了一种用于无监督点云配准的新型迭代反馈网络（IFNet），其中通过重新路由后续高级特征来有效地丰富低级特征的表示。具体来说，我们的 IFNet 建立在一系列反馈注册块（FRB）模块的基础上，每个模块负责生成前馈刚性变换和反馈高级特征。这些 FRB 模块是级联的，并且随着时间的推移不断展开。此外，反馈变压器被设计为从反馈的高级特征中有效地选择相关信息，用于细化低级特征。此外，我们结合了几何感知描述符，使网络能够充分利用大多数几何信息，从而获得更精确的配准结果。对各种基准数据集的大量实验证明了 IFNet 的卓越配准性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.04357v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness**<br />
**Title_cn:** 预训练模型引导的零样本对抗鲁棒性微调<br />
**Authors:** Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan<br />
**Abstract:** <details><summary>原文: </summary>Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks, and exhibit remarkable zero-shot generalization capability, while they are also vulnerable to imperceptible adversarial examples. Existing works typically employ adversarial training (fine-tuning) as a defense method against adversarial examples. However, direct application to the CLIP model may result in overfitting, compromising the model's capacity for generalization. In this paper, we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch, to enhance the model's zero-shot adversarial robustness. Specifically, PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model, aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outperforms the state-of-the-art method, improving the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach consistently improves clean accuracy by an average of 8.72%.</details>
**Abstract_cn:** <details><summary>译文: </summary>像 CLIP 这样的大规模预训练视觉语言模型在各种任务中都表现出了令人印象深刻的性能，并表现出卓越的零样本泛化能力，但它们也容易受到难以察觉的对抗性例子的影响。现有的作品通常采用对抗性训练（微调）作为对抗性示例的防御方法。然而，直接应用于 CLIP 模型可能会导致过度拟合，从而损害模型的泛化能力。在本文中，我们提出了预训练模型引导对抗性微调（PMG-AFT）方法，该方法通过仔细设计辅助分支来利用原始预训练模型的监督，以增强模型的零样本对抗性鲁棒性。具体来说，PMG-AFT 最小化了目标模型中的对抗样本特征与预训练模型中的对抗样本特征之间的距离，旨在保留预训练模型已经捕获的泛化特征。对 15 个零样本数据集的广泛实验表明，PMG-AFT 显着优于最先进的方法，将 top-1 稳健精度平均提高了 4.99%。此外，我们的方法持续将清洁准确度平均提高了 8.72%。</details>
**PDF:** <http://arxiv.org/pdf/2401.04350v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation**<br />
**Title_cn:** U-Mamba：增强生物医学图像分割的远程依赖性<br />
**Authors:** Jun Ma, Feifei Li, Bo Wang<br />
**Abstract:** <details><summary>原文: </summary>Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at https://wanglab.ai/u-mamba.html.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络（CNN）和 Transformer 一直是生物医学图像分割最流行的架构，但由于固有的局部性或计算复杂性，它们处理远程依赖性的能力有限。为了应对这一挑战，我们引入了 U-Mamba，一种用于生物医学图像分割的通用网络。受状态空间序列模型（SSM）这一新的深度序列模型家族的启发，该模型以其处理长序列的强大能力而闻名，我们设计了一个混合 CNN-SSM 模块，它将卷积层的局部特征提取能力与以下能力集成在一起：用于捕获远程依赖性的 SSM。此外，U-Mamba 具有自我配置机制，无需人工干预即可自动适应各种数据集。我们对四种不同的任务进行了广泛的实验，包括 CT 和 MR 图像中的 3D 腹部器官分割、内窥镜图像中的器械分割以及显微镜图像中的细胞分割。结果表明，U-Mamba 在所有任务中都优于最先进的基于 CNN 和 Transformer 的分割网络。这为生物医学图像分析中高效的远程依赖性建模开辟了新途径。代码、模型和数据可在 https://wanglab.ai/u-mamba.html 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.04722v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Low-resource finetuning of foundation models beats state-of-the-art in histopathology**<br />
**Title_cn:** 基础模型的低资源微调击败了组织病理学领域的最先进技术<br />
**Authors:** Benedikt Roth, Valentin Koch, Sophia J. Wagner, Julia A. Schnabel, Carsten Marr, Tingying Peng<br />
**Abstract:** <details><summary>原文: </summary>To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了处理计算病理学中的大规模整个幻灯片图像，大多数方法首先将图像细分为较小的块，从这些块中提取特征，最后通过弱监督学习聚合特征向量。该工作流程的性能很大程度上取决于提取特征的质量。最近，计算机视觉的基础模型表明，通过监督或自监督学习利用大量数据可以提高各种任务的特征质量和通用性。在这项研究中，我们将最流行的视觉基础模型作为组织病理学数据的特征提取器进行基准测试。我们在两种设置下评估模型：幻灯片级分类和补丁级分类。我们证明基础模型是一个强大的基线。我们的实验表明，通过根据数据集在单个 GPU 上微调基础模型仅两小时或三天，我们可以匹配或超越最先进的计算病理学特征提取器。这些发现意味着，即使资源很少，我们也可以微调针对特定下游任务和数据集定制的特征提取器。与目前的状态相比，这是一个相当大的转变，目前只有少数拥有大量资源和数据集的机构能够训练特征提取器。我们发布了用于训练和评估的所有代码以及微调后的模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.04720v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset**<br />
**Title_cn:** ASSIRA猫狗数据集上各种预训练深度学习模型的基准分析<br />
**Authors:** Galib Muhammad Shahriar Himel, Md. Masudul Islam<br />
**Abstract:** <details><summary>原文: </summary>As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为深度学习最基本的应用和实现，图像分类越来越受欢迎。著名数据科学社区提供各种数据集，用于对机器学习算法和预训练模型进行基准测试。 ASSIRA Cats & Dogs 数据集就是其中之一，在本研究中用于其整体接受度和基准标准。通过使用不同类型的优化器和损失函数来演示各种预训练模型的比较。更改超参数以获得模型的最佳结果。通过应用这种方法，我们在训练模型没有发生重大变化的情况下获得了更高的准确率。为了运行实验，我们使用了三种不同的计算机架构：一台配备 NVIDIA GeForce GTX 1070 的笔记本电脑、一台配备 NVIDIA GeForce RTX 3080Ti 的笔记本电脑和一台配备 NVIDIA GeForce RTX 3090 的台式机。所获得的结果表明，在准确性方面优于其他计算机。之前在此数据集上完成的实验。从这个实验来看，使用 NASNet Large 获得了 99.65% 的最高准确率。</details>
**PDF:** <http://arxiv.org/pdf/2401.04666v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learning to Prompt Segment Anything Models**<br />
**Title_cn:** 学习提示分割任何模型<br />
**Authors:** Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric Xing<br />
**Abstract:** <details><summary>原文: </summary>Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>SEEM 和 SAM 等分割任何模型 (SAM) 在学习分割任何东西方面表现出了巨大的潜力。 SAM 的核心设计在于 Promptable Segmentation，它将手工制作的提示作为输入并返回预期的分割掩码。 SAM 使用两种类型的提示，包括空间提示（例如点）和语义提示（例如文本），它们共同作用以提示 SAM 对下游数据集上的任何内容进行分段。尽管提示起着重要作用，但如何为 SAM 获取合适的提示在很大程度上尚未得到充分探索。在这项工作中，我们研究了 SAM 的架构，并确定了学习 SAM 有效提示的两个挑战。为此，我们提出了空间语义提示学习（SSPrompt），它可以学习有效的语义和空间提示，以获得更好的 SAM。具体来说，SSPrompt 引入了空间提示学习和语义提示学习，直接在嵌入空间上优化空间提示和语义提示，并选择性地利用预先训练的提示编码器中编码的知识。大量实验表明，SSPrompt 在多个广泛采用的数据集上一致地实现了卓越的图像分割性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.04651v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Generic Knowledge Boosted Pre-training For Remote Sensing Images**<br />
**Title_cn:** 通用知识促进遥感图像的预训练<br />
**Authors:** Ziyue Huang, Mingming Zhang, Yuan Gong, Qingjie Liu, Yunhong Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing image understanding tasks. Most backbones of existing remote sensing deep learning models are typically initialized by pre-trained weights obtained from ImageNet pre-training (IMP). However, domain gaps exist between remote sensing images and natural images (e.g., ImageNet), making deep learning models initialized by pre-trained weights of IMP perform poorly for remote sensing image understanding. Although some pre-training methods are studied in the remote sensing community, current remote sensing pre-training methods face the problem of vague generalization by only using remote sensing images. In this paper, we propose a novel remote sensing pre-training framework, Generic Knowledge Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations from remote sensing and natural images for remote sensing understanding tasks. GeRSP contains two pre-training branches: (1) A self-supervised pre-training branch is adopted to learn domain-related representations from unlabeled remote sensing images. (2) A supervised pre-training branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pre-training branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pre-trained model for deep learning model initialization. Finally, we evaluate GeRSP and other remote sensing pre-training methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of remote sensing downstream tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型对于场景分类、变化检测、土地覆盖分割和其他遥感图像理解任务至关重要。现有遥感深度学习模型的大多数主干通常是通过从 ImageNet 预训练 (IMP) 获得的预训练权重来初始化的。然而，遥感图像和自然图像（例如 ImageNet）之间存在领域差距，使得由 IMP 预训练权重初始化的深度学习模型在遥感图像理解方面表现不佳。尽管遥感界研究了一些预训练方法，但当前的遥感预训练方法仅使用遥感图像，面临着泛化模糊的问题。在本文中，我们提出了一种新颖的遥感预训练框架，即通用知识增强遥感预训练（GeRSP），用于从遥感和自然图像中学习鲁棒的表示，以实现遥感理解任务。 GeRSP包含两个预训练分支：（1）采用自监督预训练分支从未标记的遥感图像中学习领域相关的表示。 (2) 将有监督的预训练分支集成到 GeRSP 中，以便从标记的自然图像中学习一般知识。此外，GeRSP 使用师生架构结合两个预训练分支，同时学习具有一般知识和特殊知识的表示，从而生成用于深度学习模型初始化的强大预训练模型。最后，我们在三个下游任务（即目标检测、语义分割和场景分类）上评估 GeRSP 和其他遥感预训练方法。大量的实验结果一致表明，GeRSP 可以有效地以统一的方式学习鲁棒的表示，从而提高遥感下游任务的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.04614v1><br />
**Code:** <https://github.com/floatingstarZ/GeRSP>**<br />
>>**index:** 6<br />
**Title:** **Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding**<br />
**Title_cn:** Let's Go Shopping (LGS)——用于视觉概念理解的网络规模图像文本数据集<br />
**Authors:** Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli, Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot Branson, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经网络的视觉和视觉语言应用（例如图像分类和字幕）依赖于需要非平凡数据收集过程的大规模注释数据集。这种耗时的工作阻碍了大规模数据集的出现，将研究人员和从业者的选择限制在少数。因此，我们寻求更有效的方法来收集和注释图像。以前的举措是从 HTML 替代文本中收集标题并抓取社交媒体帖子，但这些数据源存在噪音、稀疏性或主观性。因此，我们选择数据符合三个标准的商业购物网站：干净、信息丰富、流畅。我们介绍 Let's Go Shopping (LGS) 数据集，这是一个大型公共数据集，包含来自公开电子商务网站的 1500 万个图像标题对。与现有的通用域数据集相比，LGS 图像专注于前景物体，背景不太复杂。我们在 LGS 上的实验表明，在现有基准数据集上训练的分类器不容易泛化到电子商务数据，而特定的自监督视觉特征提取器可以更好地泛化。此外，LGS 的高质量电子商务图像和双模态特性使其在视觉语言双模态任务中具有优势：LGS 使图像字幕模型能够生成更丰富的字幕，并帮助文本到图像生成模型实现电子商务风格转移。</details>
**PDF:** <http://arxiv.org/pdf/2401.04575v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and Hemorrhagic Volume Estimation**<br />
**Title_cn:** 用于出血性卒中分割和出血量估计的自动级联模型<br />
**Authors:** Weijin Xu, Zhuang Sha, Huihua Yang, Rongcai Jiang, Zhanying Li, Wentao Liu, Ruisheng Su<br />
**Abstract:** <details><summary>原文: </summary>Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that poses a great health threat. Promptly and accurately delineating the bleeding region and estimating the volume of bleeding in Computer Tomography (CT) images can assist clinicians in treatment planning, leading to improved treatment outcomes for patients. In this paper, a cascaded 3D model is constructed based on UNet to perform a two-stage segmentation of the hemorrhage area in CT images from rough to fine, and the hemorrhage volume is automatically calculated from the segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans, the proposed model provides high-quality segmentation outcome with higher accuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample) when compared to the traditional Tada formula with respect to hemorrhage volume estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>出血性中风（HS）发病迅速，是一种对健康构成巨大威胁的严重疾病。在计算机断层扫描 (CT) 图像中及时准确地描绘出血区域并估计出血量可以帮助临床医生制定治疗计划，从而改善患者的治疗结果。本文基于UNet构建级联3D模型，对CT图像中的出血区域进行由粗到细的两阶段分割，并根据分割区域自动计算出血量。在包含 341 例出血性中风 CT 扫描的数据集上，与传统的出血性 Tada 公式相比，该模型提供了高质量的分割结果，具有更高的准确度（DSC 85.66%）和更好的计算效率（每个样本 6.2 秒）体积估计。</details>
**PDF:** <http://arxiv.org/pdf/2401.04570v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **PhilEO Bench: Evaluating Geo-Spatial Foundation Models**<br />
**Title_cn:** PhilEO Bench：评估地理空间基础模型<br />
**Authors:** Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis, Bertrand Le Saux<br />
**Abstract:** <details><summary>原文: </summary>Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.</details>
**Abstract_cn:** <details><summary>译文: </summary>地球观测 (EO) 卫星捕获了大量未标记的数据，其中 Sentinel-2 星座每天生成 1.6 TB 的数据。这使得遥感成为一个数据丰富的领域，非常适合机器学习 (ML) 解决方案。然而，将机器学习模型应用于 EO 的一个瓶颈是缺乏注释数据，因为注释是一个劳动密集型且成本高昂的过程。因此，该领域的研究主要集中在自我监督学习和基础模型方法上。本文通过引入 PhilEO Bench（一种针对 EO 基础模型的新颖评估框架）解决了在公平、统一的基准上评估不同基础模型的需求。该框架由一个测试台和一个新颖的 400 GB Sentinel-2 数据集组成，其中包含三个下游任务的标签：建筑密度估计、道路分割和土地覆盖分类。我们使用我们的框架在多个 n 次镜头和收敛速率下评估不同的基础模型（包括 Prithvi 和 SatMAE）进行实验。</details>
**PDF:** <http://arxiv.org/pdf/2401.04464v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection**<br />
**Title_cn:** D3AD：用于异常检测的动态去噪扩散概率模型<br />
**Authors:** Justin Tebbe, Jawad Tayyub<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have found valuable applications in anomaly detection by capturing the nominal data distribution and identifying anomalies via reconstruction. Despite their merits, they struggle to localize anomalies of varying scales, especially larger anomalies like entire missing components. Addressing this, we present a novel framework that enhances the capability of diffusion models, by extending the previous introduced implicit conditioning approach Meng et al. (2022) in three significant ways. First, we incorporate a dynamic step size computation that allows for variable noising steps in the forward process guided by an initial anomaly prediction. Second, we demonstrate that denoising an only scaled input, without any added noise, outperforms conventional denoising process. Third, we project images in a latent space to abstract away from fine details that interfere with reconstruction of large missing components. Additionally, we propose a fine-tuning mechanism that facilitates the model to effectively grasp the nuances of the target domain. Our method undergoes rigorous evaluation on two prominent anomaly detection datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our framework effectively localizes anomalies regardless of their scale, marking a pivotal advancement in diffusion-based anomaly detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型通过捕获标称数据分布并通过重建识别异常，在异常检测中找到了有价值的应用。尽管有其优点，但它们很难定位不同规模的异常，尤其是较大的异常，例如整个缺失的组件。为了解决这个问题，我们提出了一个新颖的框架，通过扩展先前引入的隐式调节方法，增强扩散模型的能力。 （2022）以三个重要的方式。首先，我们结合了动态步长计算，允许在初始异常预测指导下的前向过程中使用可变噪声步骤。其次，我们证明对仅缩放的输入进行去噪，而不添加任何噪声，其性能优于传统的去噪过程。第三，我们将图像投影到潜在空间中，以抽象出干扰大型缺失组件重建的细节。此外，我们提出了一种微调机制，有助于模型有效地掌握目标域的细微差别。我们的方法对两个著名的异常检测数据集 VISA 和 BTAD 进行了严格的评估，产生了最先进的性能。重要的是，我们的框架可以有效地定位异常，无论其规模如何，这标志着基于扩散的异常检测的关键进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.04463v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **A Novel Dataset for Non-Destructive Inspection of Handwritten Documents**<br />
**Title_cn:** 用于手写文档无损检测的新型数据集<br />
**Authors:** Eleonora Breci, Luca Guarnera, Sebastiano Battiato<br />
**Abstract:** <details><summary>原文: </summary>Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.</details>
**Abstract_cn:** <details><summary>译文: </summary>法医笔迹检查是法医学的一个分支，旨在检查手写文件，以便正确定义或假设手稿的作者。这些分析涉及通过全面比较内在的局部和全局特征来比较两个或多个（数字化）文档。如果存在相关性并且满足特定的最佳实践，则可以确认所分析的文档是由同一个人编写的。对创建能够提取和比较重要特征的复杂工具的需求导致了具有几乎完全自动化流程的尖端软件的开发，改进了笔迹的取证检查并实现了越来越客观的评估。这是通过基于纯数学概念的算法解决方案实现的。使用特定数据集训练的机器学习和深度学习模型可能成为最好地解决手头任务的关键要素。在本文中，我们提出了一个新的、具有挑战性的数据集，由两个子集组成：第一个由 21 个文档组成，这些文档是通过经典的“笔和纸”方法（后来数字化）编写的，并直接在平板电脑等常见设备上获取；第二个由 124 个不同人的 362 份手写手稿组成，这些手稿是按照特定流程获得的。我们的研究开创了传统手写文档与数字工具（例如平板电脑）生成的文档之间的比较。所提出的数据集的初步结果表明，分类准确度为 90%可以在第一个子集（写在纸和笔上的文档，然后数字化并在平板电脑上）上实现，而在数据的第二部分上可以实现 96%。数据集可在 https://iplab.dmi.unict.it/ 上获得。 mfs/forensic-handwriting-analysis/novel-dataset-2023/。</details>
**PDF:** <http://arxiv.org/pdf/2401.04448v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Image classification network enhancement methods based on knowledge injection**<br />
**Title_cn:** 基于知识注入的图像分类网络增强方法<br />
**Authors:** Yishuang Tian, Ning Wang, Liang Zhang<br />
**Abstract:** <details><summary>原文: </summary>The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network. The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task. We constructed a knowledge injection dataset with matching knowledge data and image classification data. The knowledge injection dataset is the benchmark dataset for the experiments in the paper. Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales.</details>
**Abstract_cn:** <details><summary>译文: </summary>目前的深度神经网络算法还停留在像Image-Label对这样的端到端训练监督方式，这使得传统算法难以解释结果的原因，预测逻辑也难以理解和分析。目前的算法没有利用人类现有的知识信息，使得模型不符合人类的认知模型，使得模型不适合人类使用。为了解决上述问题，本发明提供了一种基于人类知识的深度神经网络训练方法，利用人类认知模型构建深度神经网络训练模型，利用人类现有知识信息构建深度神经网络训练模型。神经网络训练模型。本文提出了一种多级分层深度学习算法，该算法由多级分层深度神经网络架构和多级分层深度学习框架组成。实验结果表明，该算法能够有效解释神经网络的隐藏信息。我们研究的目标是通过分析知识注入对分类任务的影响来提高深度神经网络（DNN）的可解释性。我们构建了一个具有匹配知识数据和图像分类数据的知识注入数据集。知识注入数据集是本文实验的基准数据集。我们的模型表达了不同尺度隐藏层的可解释性和分类任务性能的提高。</details>
**PDF:** <http://arxiv.org/pdf/2401.04441v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods**<br />
**Title_cn:** 使用降维方法进行高光谱成像异常检测的实证分析<br />
**Authors:** Dongeon Kim, YeongHyeon Park<br />
**Abstract:** <details><summary>原文: </summary>Recent studies try to use hyperspectral imaging (HSI) to detect foreign matters in products because it enables to visualize the invisible wavelengths including ultraviolet and infrared. Considering the enormous image channels of the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be considered to reduce but those cannot ease the fundamental limitations, as follows: (1) latency of HSI capturing. (2) less explanation ability of the important channels. In this paper, to circumvent the aforementioned methods, one of the ways to channel reduction, on anomaly detection proposed HSI. Different from feature extraction methods (i.e., PCA or UMAP), feature selection can sort the feature by impact and show better explainability so we might redesign the task-optimized and cost-effective spectroscopic camera. Via the extensive experiment results with synthesized MVTec AD dataset, we confirm that the feature selection method shows 6.90x faster at the inference phase compared with feature extraction-based approaches while preserving anomaly detection performance. Ultimately, we conclude the advantage of feature selection which is effective yet fast.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究尝试使用高光谱成像 (HSI) 来检测产品中的异物，因为它能够可视化包括紫外线和红外线在内的不可见波长。考虑到HSI巨大的图像通道，可以考虑采用PCA或UMAP等多种降维方法来降低维度，但这些方法并不能缓解根本性的限制，如下：（1）HSI捕获的延迟。 (2)重要渠道的解释能力较差。本文为了规避前述方法，通道缩减的方法之一，在异常检测上提出了HSI。与特征提取方法（即 PCA 或 UMAP）不同，特征选择可以按影响对特征进行排序，并显示出更好的可解释性，因此我们可以重新设计任务优化且经济高效的光谱相机。通过合成 MVTec AD 数据集的大量实验结果，我们确认特征选择方法在推理阶段的速度比基于特征提取的方法快 6.90 倍，同时保持异常检测性能。最终，我们总结出特征选择的优势是有效且快速。</details>
**PDF:** <http://arxiv.org/pdf/2401.04437v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Meta-forests: Domain generalization on random forests with meta-learning**<br />
**Title_cn:** 元森林：通过元学习对随机森林进行领域泛化<br />
**Authors:** Yuyang Sun, Panagiotis Kosmas<br />
**Abstract:** <details><summary>原文: </summary>Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>领域泛化是一种流行的机器学习技术，通过从多个源领域学习，使模型能够在看不见的目标领域上表现良好。领域泛化在数据有限、收集困难或收集成本昂贵的情况下非常有用，例如在对象识别和生物医学中。在本文中，我们提出了一种称为“元森林”的新型领域泛化算法，该算法基于基本随机森林模型，结合元学习策略和最大平均差异度量。元森林的目的是通过减少树之间的相关性并增加树的强度来增强分类器的泛化能力。更具体地说，元森林在每个元任务期间进行元学习优化，同时还利用最大均值差异作为正则化项来惩罚元测试过程中较差的泛化性能。为了评估我们算法的有效性，我们在之前研究中使用的两个公开对象识别数据集和一个血糖监测数据集上对其进行了测试。我们的结果表明，元森林在对象识别和葡萄糖监测数据集的泛化性能方面优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.04425v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **MapAI: Precision in Building Segmentation**<br />
**Title_cn:** MapAI：精确的建筑分割<br />
**Authors:** Sander Riisøen Jyhne, Morten Goodwin, Per Arne Andersen, Ivar Oveland, Alexander Salveson Nossum, Karianne Ormseth, Mathilde Ørstavik, Andrew C. Flatman<br />
**Abstract:** <details><summary>原文: </summary>MapAI: Precision in Building Segmentation is a competition arranged with the Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration with Centre for Artificial Intelligence Research at the University of Agder (CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency for Data Supply and Infrastructure. The competition will be held in the fall of 2022. It will be concluded at the Northern Lights Deep Learning conference focusing on the segmentation of buildings using aerial images and laser data. We propose two different tasks to segment buildings, where the first task can only utilize aerial images, while the second must use laser data (LiDAR) with or without aerial images. Furthermore, we use IoU and Boundary IoU to properly evaluate the precision of the models, with the latter being an IoU measure that evaluates the results' boundaries. We provide the participants with a training dataset and keep a test dataset for evaluation.</details>
**Abstract_cn:** <details><summary>译文: </summary>MapAI：精确建筑分割是由挪威人工智能研究联盟 (NORA) 与阿格德尔大学人工智能研究中心 (CAIR)、挪威测绘局、AI:Hub、Norkart 和丹麦数据供应和基础设施机构。该竞赛将于 2022 年秋季举行。比赛将在北极光深度学习会议上结束，重点关注使用航空图像和激光数据进行建筑物分割。我们提出了两种不同的任务来分割建筑物，其中第一个任务只能利用航空图像，而第二个任务必须使用带有或不带有航空图像的激光数据（LiDAR）。此外，我们使用 IoU 和边界 IoU 来正确评估模型的精度，后者是评估结果边界的 IoU 度量。我们为参与者提供训练数据集并保留测试数据集以进行评估。</details>
**PDF:** <http://arxiv.org/pdf/2401.04406v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation**<br />
**Title_cn:** 用于高效每标题比特率阶梯估计的最佳转码分辨率预测<br />
**Authors:** Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bj{\o}ntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.</details>
**Abstract_cn:** <details><summary>译文: </summary>自适应视频流需要高效的比特率阶梯构建，以满足异构网络条件和最终用户需求。按标题优化的编码通常会遍历大量编码参数来搜索每个视频的帕累托最优操作​​点。最近，研究人员试图预测内容优化的比特率阶梯以减少预编码开销。然而，现有方法通常估计Pareto前沿上的编码参数，并且仍然需要后续的预编码。在本文中，我们建议直接预测每个预设比特率的最佳转码分辨率，以实现高效的比特率阶梯构建。我们采用时间注意力门控循环网络来捕获时空特征并预测转码分辨率作为多任务分类问题。我们证明，无需任何预编码即可有效确定内容优化的比特率阶梯。我们的方法很好地近似了真实的比特率分辨率对，Bj{\o}ntegaard Delta 速率损失为 1.21%，并且显着优于最先进的固定阶梯。</details>
**PDF:** <http://arxiv.org/pdf/2401.04405v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation**<br />
**Title_cn:** MST：自适应多尺度令牌引导交互式分割<br />
**Authors:** Long Xu, Shanghong Li, Yongquan Chen, Jun Luo<br />
**Abstract:** <details><summary>原文: </summary>In the field of Industrial Informatics, interactive segmentation has gained significant attention for its application in human-computer interaction and data annotation. Existing algorithms, however, face challenges in balancing the segmentation accuracy between large and small targets, often leading to an increased number of user interactions. To tackle this, a novel multi-scale token adaptation algorithm, leveraging token similarity, has been devised to enhance segmentation across varying target sizes. This algorithm utilizes a differentiable top-k tokens selection mechanism, allowing for fewer tokens to be used while maintaining efficient multi-scale token interaction. Furthermore, a contrastive loss is introduced to better discriminate between target and background tokens, improving the correctness and robustness of the tokens similar to the target. Extensive benchmarking shows that the algorithm achieves state-of-the-art (SOTA) performance compared to current methods. An interactive demo and all reproducible codes will be released at https://github.com/hahamyt/mst.</details>
**Abstract_cn:** <details><summary>译文: </summary>在工业信息学领域，交互式分割因其在人机交互和数据注释中的应用而受到广泛关注。然而，现有算法在平衡大目标和小目标之间的分割精度方面面临挑战，通常会导致用户交互数量增加。为了解决这个问题，设计了一种新颖的多尺度令牌自适应算法，利用令牌相似性来增强跨不同目标大小的分割。该算法利用可微的 top-k 令牌选择机制，允许使用更少的令牌，同时保持高效的多尺度令牌交互。此外，引入对比损失以更好地区分目标和背景标记，提高与目标相似的标记的正确性和鲁棒性。广泛的基准测试表明，与当前方法相比，该算法实现了最先进的 (SOTA) 性能。交互式演示和所有可重现的代码将在 https://github.com/hahamyt/mst 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.04403v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **SoK: Facial Deepfake Detectors**<br />
**Title_cn:** SoK：面部 Deepfake 探测器<br />
**Authors:** Binh M. Le, Jiwon Kim, Shahroz Tariq, Kristen Moore, Alsharif Abuadbba, Simon S. Woo<br />
**Abstract:** <details><summary>原文: </summary>Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-box, white-box, and gray-box settings. Our systematized analysis and experimentation lay the groundwork for a deeper understanding of deepfake detectors and their generalizability, paving the way for future research focused on creating detectors adept at countering various attack scenarios. Additionally, this work offers insights for developing more proactive defenses against deepfakes.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度造假已迅速成为对社会的深刻而严重的威胁，这主要是因为它们易于创建和传播。这种情况引发了 Deepfake 检测技术的加速发展。然而，许多现有的检测器严重依赖实验室生成的数据集进行验证，这可能无法有效地为新颖的、新兴的和现实世界的深度伪造技术做好准备。在本文中，我们对最新最先进的深度造假探测器进行了广泛而全面的审查和分析，并根据几个关键标准对其进行了评估。这些标准有助于将这些探测器分为 4 个高级组和 13 个细粒度子组，所有这些都符合统一的标准概念框架。这种分类和框架为影响探测器功效的因素提供了深入而实用的见解。我们评估了 16 个领先检测器在各种标准攻击场景（包括黑盒、白盒和灰盒设置）中的通用性。我们的系统化分析和实验为更深入地了解 Deepfake 探测器及其普遍性奠定了基础，为未来专注于创建擅长应对各种攻击场景的探测器的研究铺平了道路。此外，这项工作还为开发更主动的深度伪造防御措施提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2401.04364v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition**<br />
**Title_cn:** 用于场景识别的知识增强多视角视频表示学习<br />
**Authors:** Xuzheng Yu, Chen Jiang, Wei Zhang, Tian Gan, Linlin Chao, Jianan Zhao, Yuan Cheng, Qingpei Guo, Wei Chu<br />
**Abstract:** <details><summary>原文: </summary>With the explosive growth of video data in real-world applications, a comprehensive representation of videos becomes increasingly important. In this paper, we address the problem of video scene recognition, whose goal is to learn a high-level video representation to classify scenes in videos. Due to the diversity and complexity of video contents in realistic scenarios, this task remains a challenge. Most existing works identify scenes for videos only from visual or textual information in a temporal perspective, ignoring the valuable information hidden in single frames, while several earlier studies only recognize scenes for separate images in a non-temporal perspective. We argue that these two perspectives are both meaningful for this task and complementary to each other, meanwhile, externally introduced knowledge can also promote the comprehension of videos. We propose a novel two-stream framework to model video representations from multiple perspectives, i.e. temporal and non-temporal perspectives, and integrate the two perspectives in an end-to-end manner by self-distillation. Besides, we design a knowledge-enhanced feature fusion and label prediction method that contributes to naturally introducing knowledge into the task of video scene recognition. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着实际应用中视频数据的爆炸式增长，视频的全面表示变得越来越重要。在本文中，我们解决了视频场景识别问题，其目标是学习高级视频表示来对视频中的场景进行分类。由于现实场景中视频内容的多样性和复杂性，这项任务仍然是一个挑战。大多数现有作品仅从时间角度的视觉或文本信息识别视频场景，忽略了隐藏在单帧中的有价值的信息，而一些早期的研究仅从非时间角度识别单独图像的场景。我们认为这两种视角对于这项任务都是有意义的并且是相互补充的，同时，外部引入的知识也可以促进对视频的理解。我们提出了一种新颖的双流框架，从多个视角（即时间和非时间视角）对视频表示进行建模，并通过自蒸馏以端到端的方式集成这两个视角。此外，我们设计了一种知识增强的特征融合和标签预测方法，有助于将知识自然地引入视频场景识别任务中。在真实世界数据集上进行的实验证明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.04354v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation**<br />
**Title_cn:** BD-MSA：多尺度特征信息聚合引导的体解耦VHR遥感图像变化检测方法<br />
**Authors:** Yonghui Tan, Xiaolong Li, Yishu Chen, Jinquan Ai<br />
**Abstract:** <details><summary>原文: </summary>The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment metrics and evaluation effects of the model described in this paper on the publicly available datasets DSIFN-CD and S2Looking are the best when compared to other models.</details>
**Abstract_cn:** <details><summary>译文: </summary>遥感图像变化检测（RSCD）的目的是检测在同一地点拍摄的双时态图像之间的差异。深度学习已广泛应用于 RSCD 任务，在结果识别方面取得了显着的成果。然而，由于卫星的拍摄角度、薄云层的影响以及一定的光照条件，目前的RSCD算法无法很好地处理一些遥感照片变化区域边缘模糊的问题。为了解决这个问题，我们提出了一种通过恐惧聚合变化检测进行身体解耦多尺度（BD-MSA），这是一种新颖的模型，可以在训练和训练期间收集特征图的通道和空间维度中的全局和局部特征图信息。预测阶段。这种方法使我们能够成功提取变化区域的边界信息，同时将变化区域的主体与其边界分开。大量研究表明，与其他模型相比，本文描述的模型在公开数据集 DSIFN-CD 和 S2Looking 上的评估指标和评估效果是最好的。</details>
**PDF:** <http://arxiv.org/pdf/2401.04330v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models**<br />
**Title_cn:** 扩散模型训练后量化的增强分布对齐<br />
**Authors:** Xuewen Liu, Zhikai Li, Junrui Xiao, Qingyi Gu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the density and diversity in the latent space, thus facilitating the alignment of their distribution with the overall samples; and at the reconstruction output level, we propose Fine-grained Block Reconstruction, which can align the outputs of the quantized model and the full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM outperforms the existing post-training quantization frameworks in both unconditional and conditional generation scenarios. At low-bit precision, the quantized models with our method even outperform the full-precision models on most datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型通过迭代噪声估计在图像生成任务中取得了巨大成功。然而，繁重的去噪过程和复杂的神经网络阻碍了它们在现实场景中的低延迟应用。量化可以有效降低模型复杂度，并且不需要微调的训练后量化（PTQ）在加速去噪过程方面非常有前景。不幸的是，我们发现，由于不同去噪步骤中激活的高度动态分布，现有的扩散模型 PTQ 方法在校准样本级别和重建输出级别上都存在分布不匹配问题，这使得性能远不能令人满意，尤其是在低噪声情况下。位案例。在本文中，我们提出了扩散模型训练后量化的增强分布对齐（EDA-DM）来解决上述问题。具体来说，在校准样本层面，我们根据潜在空间的密度和多样性来选择校准样本，从而有利于它们的分布与整体样本的对齐；在重建输出层面，我们提出了细粒度块重建（Fine-grained Block Reconstruction），它可以在不同网络粒度下对齐量化模型和全精度模型的输出。大量实验表明，EDA-DM 在无条件和条件生成场景中均优于现有的训练后量化框架。在低位精度下，我们的方法的量化模型甚至优于大多数数据集上的全精度模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.04585v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Memory-Efficient Personalization using Quantized Diffusion Model**<br />
**Title_cn:** 使用量化扩散模型进行内存高效的个性化<br />
**Authors:** Hyogon Ryu, Seohyun Lim, Hyunjung Shim<br />
**Abstract:** <details><summary>原文: </summary>The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. Our approach not only enhances personalization but also upholds prompt fidelity and image quality, significantly outperforming the baseline qualitatively and quantitatively. The code will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>Stable Diffusion XL、Imagen 和 Dall-E3 等十亿参数扩散模型的兴起显着推进了生成式 AI 领域的发展。然而，由于资源需求高和推理速度慢，它们的大规模性质给微调和部署带来了挑战。本文涉足了相对未经探索但前景广阔的微调量化扩散模型领域。我们通过定制三个模型建立了强大的基线：用于微调量化参数的 PEQA、用于训练后量化的 Q-Diffusion 以及用于个性化的 DreamBooth。我们的分析揭示了基线模型中主题和提示保真度之间的显着权衡。为了解决这些问题，受扩散模型中不同时间步长不同作用的启发，我们引入了两种策略：S1 仅在选定的时间间隔内优化一组微调参数，S2 创建多个微调参数集，每个参数集专门用于不同的时间步间隔。我们的方法不仅增强了个性化，而且还保持了即时保真度和图像质量，在质量和数量上都显着优于基线。该代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.04339v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**<br />
**Title_cn:** 可变形扩散：用于单图像头像创建的 3D 一致扩散<br />
**Authors:** Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成扩散模型的最新进展实现了从单个输入图像或文本提示生成 3D 资产的先前不可行的功能。在这项工作中，我们的目标是提高这些模型的质量和功能，以完成创建可控、逼真的人类化身的任务。我们通过将 3D 可变形模型集成到最先进的多视图一致扩散方法中来实现这一目标。我们证明，对铰接式 3D 模型上的生成管道进行精确调节可以增强在从单个图像合成新颖视图的任务中的基线模型性能。更重要的是，这种集成有助于将面部表情和身体姿势控制无缝且准确地融入生成过程中。据我们所知，我们提出的框架是第一个扩散模型，能够从看不见的主体的单个图像创建完全 3D 一致、可动画且逼真的人类化身；广泛的定量和定性评估证明了我们的方法在新颖的视图和新颖的表达合成任务上优于现有最先进的化身创建模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.04728v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Low-Resource Vision Challenges for Foundation Models**<br />
**Title_cn:** 基础模型的低资源视觉挑战<br />
**Authors:** Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek<br />
**Abstract:** <details><summary>原文: </summary>Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project website: https://xiaobai1217.github.io/Low-Resource-Vision/.</details>
**Abstract_cn:** <details><summary>译文: </summary>自然语言处理领域已经存在资源匮乏的情况，许多语言缺乏足够的数据来进行大规模机器学习。然而，计算机视觉中的低资源问题尚未得到充分探索。在本文中，我们努力解决这一差距，并利用视觉基础模型探索低资源图像任务的挑战。因此，我们首先收集真正的低资源图像数据基准，涵盖历史地图、电路图和机械图纸。这些资源匮乏的环境都面临着数据稀缺、细粒度差异以及从自然图像到感兴趣的专业领域的分布转变这三个挑战。虽然现有的基础模型显示出令人印象深刻的通用性，但我们发现它们不能很好地迁移到我们的低资源任务。为了开始应对低资源视觉的挑战，我们为每个挑战引入一个简单的基线。具体来说，我们建议i）通过生成模型扩大数据空间，ii）采用最佳子内核对局部区域进行编码以进行细粒度差异发现，以及iii）学习对专业领域的关注。我们的基准测试中三个低资源数据源的实验表明，我们的建议已经提供了比常见的迁移学习、数据增强和细粒度方法更好的基线。这凸显了基础模型的低资源愿景的独特特征和挑战，值得进一步研究。项目网站：https://xiaobai1217.github.io/Low-Resource-Vision/。</details>
**PDF:** <http://arxiv.org/pdf/2401.04716v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models**<br />
**Title_cn:** EmoGen：使用文本到图像扩散模型生成情感图像内容<br />
**Authors:** Jingyuan Yang, Jiawei Feng, Hui Huang<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，图像生成任务取得了显着进展，用户可以创建视觉上令人惊叹的高质量图像。然而，现有的文本到图像的扩散模型擅长生成具体概念（狗），但在生成更抽象的概念（情感）方面遇到了挑战。人们已经做出了一些努力来通过颜色和风格调整来修改图像情感，但在用固定的图像内容有效传达情感方面面临着局限性。在这项工作中，我们引入了情感图像内容生成（EICG），这是一项新任务，用于在给定情感类别的情况下生成语义清晰且情感忠实的图像。具体来说，我们提出了一个情感空间并构建了一个映射网络，将其与强大的对比语言图像预训练（CLIP）空间对齐，提供了抽象情感的具体解释。进一步提出属性损失和情感置信度，以确保生成图像的语义多样性和情感保真度。我们的方法在数量和质量上都优于最先进的文本到图像方法，我们得出了三个自定义指标，即情感准确性、语义清晰度和语义多样性。除了生成之外，我们的方法还可以帮助情感理解并激发情感艺术设计。</details>
**PDF:** <http://arxiv.org/pdf/2401.04608v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation**<br />
**Title_cn:** MagicVideo-V2：多阶段高美视频生成<br />
**Authors:** Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.</details>
**Abstract_cn:** <details><summary>译文: </summary>对从文本描述生成高保真视频的需求不断增长，促进了该领域的重要研究。在这项工作中，我们介绍了 MagicVideo-V2，它将文本到图像模型、视频运动生成器、参考图像嵌入模块和帧插值模块集成到端到端视频生成管道中。受益于这些架构设计，MagicVideo-V2 可以生成具有出色保真度和平滑度的美观、高分辨率视频。通过大规模用户评估，它表现出了优于 Runway、Pika 1.0、Morph、Moon Valley 和 Stable Video Diffusion 模型等领先文本转视频系统的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.04468v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example**<br />
**Title_cn:** 草图提取扩散过程中的代表性特征提取（以一例为例）<br />
**Authors:** Kwan Yun, Youngseo Kim, Kwanggyoon Seo, Chang Wook Seo, Junyong Noh<br />
**Abstract:** <details><summary>原文: </summary>We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 DiffSketch，一种从图像生成各种风格化草图的方法。我们的方法侧重于从预训练扩散模型中深层特征的丰富语义中选择代表性特征。这种新颖的草图生成方法可以通过一张手动绘图进行训练。此外，通过将经过训练的生成器提炼为简化的提取器，可以确保高效的草图提取。我们通过分析选择去噪扩散特征，并将这些选定的特征与 VAE 特征集成以生成草图。此外，我们提出了使用条件生成方法训练模型的采样方案。通过一系列比较，我们验证了蒸馏后的 DiffSketch 不仅优于现有最先进的草图提取方法，而且在提取草图的任务中也超越了基于扩散的风格化方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.04362v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging**<br />
**Title_cn:** 视觉重新构想：人工智能驱动的 WiFi 室内成像突破<br />
**Authors:** Jianyang Shi, Bowen Zhang, Amartansh Dubey, Ross Murch, Liwen Jing<br />
**Abstract:** <details><summary>原文: </summary>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance.</details>
**Abstract_cn:** <details><summary>译文: </summary>室内成像是机器人和物联网的一项关键任务。 WiFi 作为一种无所不在的信号，是执行被动成像并将最新信息同步到所有连接设备的有前途的候选者。这是第一个将 WiFi 室内成像视为多模态图像生成任务的研究工作，将测量的 WiFi 功率转换为高分辨率的室内图像。我们提出的 WiFi-GEN 网络实现的形状重建精度是基于物理模型的反演方法的 275%。此外，Frechet Inception Distance 分数显着降低了 82%。为了检验该任务模型的有效性，发布了第一个大规模数据集，其中包含 80,000 对 WiFi 信号和成像目标。我们的模型吸收了基于模型的方法的挑战，包括将非线性、不适定性和不确定性纳入我们的生成人工智能网络的大量参数中。该网络还旨在最适合测量的 WiFi 信号和所需的成像输出。为了可重复性，我们将在接受后发布数据和代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.04317v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Jump Cut Smoothing for Talking Heads**<br />
**Title_cn:** 说话头像的跳切平滑<br />
**Authors:** Xiaojuan Wang, Taesung Park, Yang Zhou, Eli Shechtman, Richard Zhang<br />
**Abstract:** <details><summary>原文: </summary>A jump cut offers an abrupt, sometimes unwanted change in the viewing experience. We present a novel framework for smoothing these jump cuts, in the context of talking head videos. We leverage the appearance of the subject from the other source frames in the video, fusing it with a mid-level representation driven by DensePose keypoints and face landmarks. To achieve motion, we interpolate the keypoints and landmarks between the end frames around the cut. We then use an image translation network from the keypoints and source frames, to synthesize pixels. Because keypoints can contain errors, we propose a cross-modal attention scheme to select and pick the most appropriate source amongst multiple options for each key point. By leveraging this mid-level representation, our method can achieve stronger results than a strong video interpolation baseline. We demonstrate our method on various jump cuts in the talking head videos, such as cutting filler words, pauses, and even random cuts. Our experiments show that we can achieve seamless transitions, even in the challenging cases where the talking head rotates or moves drastically in the jump cut.</details>
**Abstract_cn:** <details><summary>译文: </summary>跳切会给观看体验带来突然的、有时是不必要的改变。我们提出了一个新颖的框架，用于在头部说话视频的背景下平滑这些跳切。我们利用视频中其他源帧中主题的外观，将其与由 DensePose 关键点和面部标志驱动的中级表示融合。为了实现运动，我们在剪切周围的结束帧之间插入关键点和地标。然后，我们使用关键点和源帧的图像转换网络来合成像素。由于关键点可能包含错误，因此我们提出了一种跨模式注意方案，以在每个关键点的多个选项中选择最合适的来源。通过利用这种中级表示，我们的方法可以获得比强大的视频插值基线更强的结果。我们在头部说话视频中的各种跳切上演示了我们的方法，例如剪切填充词、停顿，甚至随机剪切。我们的实验表明，即使在跳跃剪辑中说话头旋转或剧烈移动的挑战性情况下，我们也可以实现无缝过渡。</details>
**PDF:** <http://arxiv.org/pdf/2401.04718v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal**<br />
**Title_cn:** WaveletFormerNet：基于变压器的小波网络，用于现实世界的非均匀和密集除雾<br />
**Authors:** Shengli Zhang, Zhiyong Tao, Sen Lin<br />
**Abstract:** <details><summary>原文: </summary>Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Extensive experiments demonstrate that our WaveletFormerNet performs better than state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管深度卷积神经网络在去除合成雾方面取得了显着的成功，但必须能够处理现实世界中复杂雾条件（例如浓雾或非均匀雾）下拍摄的图像。然而，现实世界中的雾度分布很复杂，随着特征图分辨率或图像分辨率的降低，下采样可能会导致输出结果中的颜色失真或细节丢失。除了获取足够训练数据的挑战之外，雾图像处理的深度学习技术也会出现过度拟合，这会限制模型的泛化能力，为其在现实场景中的实际应用带来挑战。考虑到这些问题，本文提出了一种基于 Transformer 的小波网络（WaveletFormerNet），用于现实世界的雾图像恢复。我们通过提出 WaveletFormer 和 IWaveletFormer 块将离散小波变换嵌入到 Vision Transformer 中，旨在减轻由于下采样而导致的图像中的纹理细节损失和颜色失真。我们在 Transformer 块中引入并行卷积，它允许以轻量级机制捕获多频率信息。此外，我们还实现了特征聚合模块（FAM）来保持图像分辨率并增强模型的特征提取能力，进一步使其在现实世界的雾图像恢复任务中具有令人印象深刻的性能。大量实验表明，我们的 WaveletFormerNet 的性能优于最先进的方法，如对次要模型复杂性的定量和定性评估所示。此外，我们在现实世界除尘和应用测试中取得的令人满意的结果展示了 WaveletFormerNet 在计算机视觉相关应用中卓越的泛化能力和改进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.04550v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks**<br />
**Title_cn:** 走捷径回来：减轻训练尖峰神经网络的梯度消失<br />
**Authors:** Yufei Guo, Yuanpei Chen<br />
**Abstract:** <details><summary>原文: </summary>The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in our paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>尖峰神经网络（SNN）是一种受生物学启发的神经网络基础设施，最近引起了广泛关注。它利用二进制尖峰激活来传输信息，从而用加法代替乘法，从而实现高能效。然而，由于放电尖峰过程的梯度未定义，直接训练 SNN 会带来挑战。尽管先前的工作已经采用了各种替代梯度训练方法，这些方法使用替代函数来代替反向传播过程中的激发过程，但这些方法忽略了一个内在问题：梯度消失。为了解决这个问题，我们在论文中提出了一种快捷的反向传播方法，该方法主张将梯度直接从损失传输到浅层。这使我们能够直接将梯度呈现给浅层，从而显着缓解梯度消失问题。此外，该方法不会在推理阶段引入任何负担。为了在最终的准确性和训练的简易性之间取得平衡，我们还提出了一种进化训练框架，并通过引入随训练周期动态变化的平衡系数来实现它，这进一步提高了网络的性能。使用几种流行的网络结构对静态和动态数据集进行的广泛实验表明，我们的方法始终优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.04486v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learning with Noisy Labels: Interconnection of Two Expectation-Maximizations**<br />
**Title_cn:** 使用噪声标签学习：两个期望最大化的互连<br />
**Authors:** Heewon Kim, Hyun Sung Chang, Kiho Cho, Jaeyun Lee, Bohyung Han<br />
**Abstract:** <details><summary>原文: </summary>Labor-intensive labeling becomes a bottleneck in developing computer vision algorithms based on deep learning. For this reason, dealing with imperfect labels has increasingly gained attention and has become an active field of study. We address learning with noisy labels (LNL) problem, which is formalized as a task of finding a structured manifold in the midst of noisy data. In this framework, we provide a proper objective function and an optimization algorithm based on two expectation-maximization (EM) cycles. The separate networks associated with the two EM cycles collaborate to optimize the objective function, where one model is for distinguishing clean labels from corrupted ones while the other is for refurbishing the corrupted labels. This approach results in a non-collapsing LNL-flywheel model in the end. Experiments show that our algorithm achieves state-of-the-art performance in multiple standard benchmarks with substantial margins under various types of label noise.</details>
**Abstract_cn:** <details><summary>译文: </summary>劳动密集型标注成为开发基于深度学习的计算机视觉算法的瓶颈。因此，处理不完美标签越来越受到关注，并成为一个活跃的研究领域。我们解决了噪声标签学习（LNL）问题，该问题被形式化为在噪声数据中找到结构化流形的任务。在此框架中，我们提供了适当的目标函数和基于两个期望最大化（EM）循环的优化算法。与两个 EM 周期相关的独立网络协作优化目标函数，其中一个模型用于区分干净标签和损坏标签，而另一个模型用于翻新损坏标签。这种方法最终产生了一个不塌陷的 LNL 飞轮模型。实验表明，我们的算法在多个标准基准测试中实现了最先进的性能，并且在各种类型的标签噪声下具有很大的裕度。</details>
**PDF:** <http://arxiv.org/pdf/2401.04390v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars**<br />
**Title_cn:** 使用 3D 头像进行口语到手语翻译的简单基线<br />
**Authors:** Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong<br />
**Abstract:** <details><summary>原文: </summary>The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models will be available at https://github.com/FangyunWei/SLRT</details>
**Abstract_cn:** <details><summary>译文: </summary>本文的目的是开发一个将口语翻译成手语的功能系统，称为 Spoken2Sign 翻译。 Spoken2Sign 任务与传统手语到口语 (Sign2Spoken) 翻译是正交和互补的。为了实现 Spoken2Sign 翻译，我们提出了一个简单的基线，包括三个步骤：1）使用现有的 Sign2Spoken 基准创建注释视频词典； 2) 估计字典中每个手势视频的3D手势； 3）借助生成的gloss-3D符号字典训练Spoken2Sign模型，该模型由Text2Gloss翻译器、符号连接器和渲染模块组成。然后翻译结果通过符号头像显示。据我们所知，我们是第一个以 3D 符号输出格式呈现 Spoken2Sign 任务的人。除了 Spoken2Sign 翻译功能之外，我们还证明了我们方法的两个副产品——3D 关键点增强和多视图理解——可以帮助基于关键点的手语理解。代码和模型可在 https://github.com/FangyunWei/SLRT 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.04730v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning**<br />
**Title_cn:** 长尾半监督学习的不确定性采样<br />
**Authors:** Kuo Yang, Duo Li, Menghan Hu, Guangtao Zhai, Xiaokang Yang, Xiao-Ping Zhang<br />
**Abstract:** <details><summary>原文: </summary>For semi-supervised learning with imbalance classes, the long-tailed distribution of data will increase the model prediction bias toward dominant classes, undermining performance on less frequent classes. Existing methods also face challenges in ensuring the selection of sufficiently reliable pseudo-labels for model training and there is a lack of mechanisms to adjust the selection of more reliable pseudo-labels based on different training stages. To mitigate this issue, we introduce uncertainty into the modeling process for pseudo-label sampling, taking into account that the model performance on the tailed classes varies over different training stages. For example, at the early stage of model training, the limited predictive accuracy of model results in a higher rate of uncertain pseudo-labels. To counter this, we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach. This approach allows the model to perceive the uncertainty of pseudo-labels at different training stages, thereby adaptively adjusting the selection thresholds for different classes. Compared to other methods such as the baseline method FixMatch, UDTS achieves an increase in accuracy of at least approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset TissueMNIST, respectively. The source code of UDTS is publicly available at: https://github.com/yangk/UDTS.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于不平衡类的半监督学习，数据的长尾分布将增加模型对主导类的预测偏差，从而损害不太频繁的类的性能。现有方法在确保为模型训练选择足够可靠的伪标签方面也面临挑战，并且缺乏根据不同训练阶段调整更可靠伪标签的选择的机制。为了缓解这个问题，我们在伪标签采样的建模过程中引入了不确定性，考虑到尾类上的模型性能在不同的训练阶段会有所不同。例如，在模型训练的早期阶段，模型的预测精度有限，导致不确定伪标签的比例较高。为了解决这个问题，我们提出了一种不确定性感知动态阈值选择（UDTS）方法。这种方法使得模型能够感知不同训练阶段伪标签的不确定性，从而自适应地调整不同类别的选择阈值。与基线方法 FixMatch 等其他方法相比，UDTS 在自然场景图像数据集 CIFAR10-LT、CIFAR100-LT、STL-10 上实现了至少约 5.26%、1.75%、9.96% 和 1.28% 的精度提升-LT 和医学图像数据集 TissueMNIST。 UDTS的源代码公开在：https://github.com/yangk/UDTS。</details>
**PDF:** <http://arxiv.org/pdf/2401.04435v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **RomniStereo: Recurrent Omnidirectional Stereo Matching**<br />
**Title_cn:** RomniStereo：循环全向立体匹配<br />
**Authors:** Hualie Jiang, Rui Xu, Minglang Tan, Wenjie Jiang<br />
**Abstract:** <details><summary>原文: </summary>Omnidirectional stereo matching (OSM) is an essential and reliable means for $360^{\circ}$ depth sensing. However, following earlier works on conventional stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D encoder-decoder block to regularize the cost volume, causing the whole system complicated and sub-optimal results. Recently, the Recurrent All-pairs Field Transforms (RAFT) based approach employs the recurrent update in 2D and has efficiently improved image-matching tasks, \ie, optical flow, and stereo matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite adaptive weighting scheme to seamlessly transform the outputs of spherical sweeping of OSM into the required inputs for the recurrent update, thus creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm. Furthermore, we introduce two techniques, \ie, grid embedding and adaptive context feature generation, which also contribute to RomniStereo's performance. Our best model improves the average MAE metric by 40.7\% over the previous SOTA baseline across five datasets. When visualizing the results, our models demonstrate clear advantages on both synthetic and realistic examples. The code is available at \url{https://github.com/HalleyJiang/RomniStereo}.</details>
**Abstract_cn:** <details><summary>译文: </summary>全向立体匹配 (OSM) 是 360^{\circ}$ 深度传感的重要且可靠的手段。然而，继早期的传统立体匹配工作之后，现有的最先进 (SOTA) 方法依赖 3D 编码器-解码器块来规范成本量，导致整个系统复杂且结果次优。最近，基于循环全对场变换（RAFT）的方法采用了二维循环更新，并有效地改进了图像匹配任务，即光流和立体匹配。为了弥补 OSM 和 RAFT 之间的差距，我们主要提出了一种相反的自适应加权方案，将 OSM 球面扫描的输出无缝转换为循环更新所需的输入，从而创建循环全向立体匹配（RomniStereo）算法。此外，我们引入了两种技术，即网格嵌入和自适应上下文特征生成，这也有助于 RomniStereo 的性能。我们的最佳模型在五个数据集上的平均 MAE 指标比之前的 SOTA 基线提高了 40.7%。在可视化结果时，我们的模型在合成和现实示例上都表现出了明显的优势。代码可在 \url{https://github.com/HalleyJiang/RomniStereo} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.04345v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale**<br />
**Title_cn:** RadarCam-Depth：雷达相机融合，通过学习的公制尺度进行深度估计<br />
**Authors:** Han Li, Yukai Ma, Yaqing Gu, Kewei Hu, Yong Liu, Xingxing Zuo<br />
**Abstract:** <details><summary>原文: </summary>We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种基于单视图图像和稀疏、噪声雷达点云融合的度量密集深度估计的新方法。异构雷达和图像数据或其编码的直接融合往往会产生具有明显伪影、模糊边界和次优精度的密集深度图。为了解决这个问题，我们学习利用稀疏和嘈杂的雷达数据产生的密集度量尺度来增强通用且鲁棒的单目深度预测。我们提出了一个雷达相机框架，用于高精度和精细的密集深度估计，分为四个阶段，包括单目深度预测、单目深度与稀疏雷达点的全局尺度对齐、通过学习雷达点和雷达点之间的关联来进行准密集尺度估计。图像补丁，以及使用比例图学习器对密集深度进行局部比例细化。我们提出的方法在具有挑战性的 nuScenes 数据集和我们自行收集的 ZJU-4DRadarCam 上将深度估计的平均绝对误差 (MAE) 降低了 25.6% 和 40.2%，显着优于最先进的雷达相机深度估计方法数据集，分别。</details>
**PDF:** <http://arxiv.org/pdf/2401.04325v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Revisiting Adversarial Training at Scale**<br />
**Title_cn:** 重新审视大规模对抗性训练<br />
**Authors:** Zeyu Wang, Xianhang Li, Hongru Zhu, Cihang Xie<br />
**Abstract:** <details><summary>原文: </summary>The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL.   Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at https://github.com/UCSC-VLAA/AdvXL.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习社区见证了训练流程的巨大变化，这些变化以规模空前的“基础模型”为中心。然而，对抗训练领域却相对滞后，主要集中在像 ResNet-50 这样的小模型尺寸，以及像 CIFAR-10 这样的小型低分辨率数据集。为了弥合这一转型差距，本文提供了一种带有对抗性训练的现代重新检验，调查其大规模应用时的潜在好处。此外，我们引入了一种高效且有效的训练策略，以可承受的计算成本使用巨型模型和网络规模数据进行对抗性训练。我们将这个新引入的框架表示为 AdvXL。实证结果表明，AdvXL 在 ImageNet-1K 上的 AutoAttack 下建立了新的最先进的鲁棒精度记录。例如，通过在 DataComp-1B 数据集上进行训练，我们的 AdvXL 使普通 ViT-g 模型能够大幅超越 $l_{\infty}$-、$l_{2}$- 和 $l_{1} 之前的记录稳健准确度分别为 11.4%、14.2% 和 12.9%。这一成就将 AdvXL 视为一种开创性方法，为在更大尺度上有效训练稳健的视觉表示绘制了新的轨迹。我们的代码位于 https://github.com/UCSC-VLAA/AdvXL。</details>
**PDF:** <http://arxiv.org/pdf/2401.04727v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks**<br />
**Title_cn:** CoordGate：在卷积神经网络中高效计算空间变化的卷积<br />
**Authors:** Sunny Howard, Peter Norreys, Andreas Döpp<br />
**Abstract:** <details><summary>原文: </summary>Optical imaging systems are inherently limited in their resolution due to the point spread function (PSF), which applies a static, yet spatially-varying, convolution to the image. This degradation can be addressed via Convolutional Neural Networks (CNNs), particularly through deblurring techniques. However, current solutions face certain limitations in efficiently computing spatially-varying convolutions. In this paper we propose CoordGate, a novel lightweight module that uses a multiplicative gate and a coordinate encoding network to enable efficient computation of spatially-varying convolutions in CNNs. CoordGate allows for selective amplification or attenuation of filters based on their spatial position, effectively acting like a locally connected neural network. The effectiveness of the CoordGate solution is demonstrated within the context of U-Nets and applied to the challenging problem of image deblurring. The experimental results show that CoordGate outperforms conventional approaches, offering a more robust and spatially aware solution for CNNs in various computer vision applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于点扩散函数 (PSF) 对图像应用静态但空间变化的卷积，光学成像系统的分辨率本质上受到限制。这种退化可以通过卷积神经网络（CNN）来解决，特别是通过去模糊技术。然而，当前的解决方案在有效计算空间变化的卷积方面面临某些限制。在本文中，我们提出了 CoordGate，这是一种新颖的轻量级模块，它使用乘法门和坐标编码网络来实现 CNN 中空间变化卷积的高效计算。 CoordGate 允许根据滤波器的空间位置选择性放大或衰减滤波器，有效地充当本地连接的神经网络。 CoordGate 解决方案的有效性在 U-Net 的背景下得到了证明，并应用于具有挑战性的图像去模糊问题。实验结果表明，CoordGate 的性能优于传统方法，为各种计算机视觉应用中的 CNN 提供了更强大且具有空间感知能力的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.04680v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video**<br />
**Title_cn:** 相移远程光电体积描记法，用于根据面部视频估算心率和血压<br />
**Authors:** Gyutae Hwang, Sang Jun Lee<br />
**Abstract:** <details><summary>原文: </summary>Human health can be critically affected by cardiovascular diseases, such as hypertension, arrhythmias, and stroke. Heart rate and blood pressure are important biometric information for the monitoring of cardiovascular system and early diagnosis of cardiovascular diseases. Existing methods for estimating the heart rate are based on electrocardiography and photoplethyomography, which require contacting the sensor to the skin surface. Moreover, catheter and cuff-based methods for measuring blood pressure cause inconvenience and have limited applicability. Therefore, in this thesis, we propose a vision-based method for estimating the heart rate and blood pressure. This thesis proposes a 2-stage deep learning framework consisting of a dual remote photoplethysmography network (DRP-Net) and bounded blood pressure network (BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography (rPPG) signals for the acral and facial regions, and these phase-shifted rPPG signals are utilized to estimate the heart rate. In the second stage, BBP-Net integrates temporal features and analyzes phase discrepancy between the acral and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy of estimating the heart rate, we employed a data augmentation method based on a frame interpolation model. Moreover, we designed BBP-Net to infer blood pressure within a predefined range by incorporating a scaled sigmoid function. Our method resulted in estimating the heart rate with the mean absolute error (MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method, on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure (SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64 mmHg, and 9.4 mmHg, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类健康可能受到高血压、心律失常和中风等心血管疾病的严重影响。心率和血压是心血管系统监测和心血管疾病早期诊断的重要生物识别信息。现有的估计心率的方法基于心电图和光电容积描记术，这需要将传感器接触皮肤表面。此外，基于导管和袖带的血压测量方法造成不便并且适用性有限。因此，在本论文中，我们提出了一种基于视觉的心率和血压估计方法。本论文提出了一个由双远程光电体积描记网络（DRP-Net）和有界血压网络（BBP-Net）组成的两阶段深度学习框架。在第一阶段，DRP-Net 推断肢端和面部区域的远程光电容积描记法 (rPPG) 信号，并利用这些相移 rPPG 信号来估计心率。在第二阶段，BBP-Net 集成时间特征并分析肢端和面部 rPPG 信号之间的相位差异，以估计 SBP 和 DBP 值。为了提高估计心率的准确性，我们采用了基于帧插值模型的数据增强方法。此外，我们设计了 BBP-Net，通过结合缩放的 sigmoid 函数来推断预定义范围内的血压。我们的方法在 MMSE-HR 数据集上估计心率的平均绝对误差 (MAE) 为 1.78 BPM，与最近的方法相比，MAE 降低了 34.31%。估计收缩压（SBP）和舒张压（DBP）的 MAE 分别为 10.19 mmHg 和 7.09 mmHg。在 V4V 数据集上，心率、SBP 和 DBP 的 MAE 分别为 3.83 BPM、13.64 mmHg 和 9.4 mmHg。</details>
**PDF:** <http://arxiv.org/pdf/2401.04560v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Towards Real-World Aerial Vision Guidance with Categorical 6D Pose Tracker**<br />
**Title_cn:** 通过分类 6D 姿势跟踪器实现真实世界的空中视觉引导<br />
**Authors:** Jingtao Sun, Yaonan Wang, Danwei Wang<br />
**Abstract:** <details><summary>原文: </summary>Tracking the object 6-DoF pose is crucial for various downstream robot tasks and real-world applications. In this paper, we investigate the real-world robot task of aerial vision guidance for aerial robotics manipulation, utilizing category-level 6-DoF pose tracking. Aerial conditions inevitably introduce special challenges, such as rapid viewpoint changes in pitch and roll. To support this task and challenge, we firstly introduce a robust category-level 6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal prior knowledge to explore optimal inter-frame keypoint pairs, generated under a priori structural adaptive supervision in a coarse-to-fine manner. Notably, our Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the problems of the inter-frame differences and intra-class shape variations through both temporal dynamic filtering and shape-similarity filtering. We further present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a decoupling approach to implement the final aerial vision guidance task. It contains two servo action policies to better accommodate the structural properties of aerial robotics manipulation. Exhaustive experiments on four well-known public benchmarks demonstrate the superiority of our Robust6DoF. Real-world tests directly verify that our Robust6DoF along with PAD-Servo can be readily used in real-world aerial robotic applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>跟踪物体 6-DoF 位姿对于各种下游机器人任务和实际应用至关重要。在本文中，我们利用类别级 6-DoF 位姿跟踪，研究了用于空中机器人操纵的空中视觉引导的现实机器人任务。空中条件不可避免地会带来特殊的挑战，例如俯仰和横滚的快速视点变化。为了支持这项任务和挑战，我们首先引入一个强大的类别级 6-DoF 姿势跟踪器（Robust6DoF）。该跟踪器利用形状和时间先验知识来探索最佳的帧间关键点对，这些关键点对是在先验结构自适应监督下以从粗到细的方式生成的。值得注意的是，我们的 Robust6DoF 采用时空增强模块，通过时间动态过滤和形状相似性过滤来处理帧间差异和类内形状变化的问题。我们进一步提出了一种姿态感知离散伺服策略（PAD-Servo），作为实现最终航空视觉引导任务的解耦方法。它包含两个伺服动作策略，以更好地适应空中机器人操纵的结构特性。对四个著名公共基准的详尽实验证明了我们 Robust6DoF 的优越性。现实世界的测试直接验证了我们的 Robust6DoF 与 ​​PAD-Servo 可以轻松用于现实世界的空中机器人应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.04377v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mix-GENEO: A flexible filtration for multiparameter persistent homology detects digital images**<br />
**Title_cn:** Mix-GENEO：用于多参数持久同源性检测数字图像的灵活过滤<br />
**Authors:** Jiaxing He, Bingzhe Hou, Tieru Wu, Yue Xin<br />
**Abstract:** <details><summary>原文: </summary>Two important problems in the field of Topological Data Analysis are defining practical multifiltrations on objects and showing ability of TDA to detect the geometry. Motivated by the problems, we constuct three multifiltrations named multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the interleaving distance and multiparameter persistence landscape of multi-GENEO with respect to the pseudometric of the subspace of bounded functions. We also give the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we provide experiment results on MNIST dataset to demonstrate our bifiltrations have ability to detect geometric and topological differences of digital images.</details>
**Abstract_cn:** <details><summary>译文: </summary>拓扑数据分析领域的两个重要问题是定义对象的实际多重过滤和显示 TDA 检测几何形状的能力。受这些问题的启发，我们构建了三种多重过滤：multi-GENEO、multi-DGENEO 和 mix-GENEO，并证明了 multi-GENEO 的交错距离和多参数持久性景观相对于有界函数子空间的伪度量的稳定性。我们还给出了 multi-DGENEO 和 mix-GENEO 的上限估计。最后，我们提供了 MNIST 数据集上的实验结果，以证明我们的双过滤能够检测数字图像的几何和拓扑差异。</details>
**PDF:** <http://arxiv.org/pdf/2401.04332v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments**<br />
**Title_cn:** StarCraftImage：用于多代理环境空间推理方法原型设计的数据集<br />
**Authors:** Sean Kulinski, Nicholas R. Waytowich, James Z. Hare, David I. Inouye<br />
**Abstract:** <details><summary>原文: </summary>Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com</details>
**Abstract_cn:** <details><summary>译文: </summary>多智能体环境中的空间推理任务（例如事件预测、智能体类型识别或缺失数据插补）对于多种应用非常重要（例如，传感器网络的自主监视和强化学习 (RL) 的子任务）。 《星际争霸 II》游戏重播编码智能（和对抗性）多智能体行为，并可以为这些任务提供测试平台；然而，提取简单且标准化的表示来对这些任务进行原型设计非常费力并且阻碍了可重复性。相比之下，MNIST 和 CIFAR10 尽管极其简单，却能够实现 ML 方法的快速原型设计和可重复性。遵循这些数据集的简单性，我们基于《星际争霸 II》回放构建了一个基准空间推理数据集，该数据集表现出复杂的多智能体行为，同时仍然像 MNIST 和 CIFAR10 一样易于使用。具体来说，我们仔细总结了 255 个连续游戏状态的窗口，从 60,000 次重播中创建 360 万张摘要图像，包括所有相关元数据，例如游戏结果和玩家竞赛。我们开发了三种降低复杂性的格式：每种单位类型都有一个通道的高光谱图像（类似于多光谱地理空间图像）、模仿 CIFAR10 的 RGB 图像以及模仿 MNIST 的灰度图像。我们展示了如何使用该数据集来构建空间推理方法的原型。所有数据集、提取代码和数据集加载代码都可以在 https://starcraftdata.davidinouye.com 找到</details>
**PDF:** <http://arxiv.org/pdf/2401.04290v1><br />
**Code:** null<br />

