# !UPDATED  -- 2024-01-10

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **URHand: Universal Relightable Hands**<br />
**Title_cn:** URHand：通用可重复照明手<br />
**Authors:** Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations. To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities. The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations. To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature. By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport. This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities. In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization. Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability. We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的逼真可重复照明手模型需要在不同视图、姿势和照明下进行广泛的特定于身份的观察，并且在推广到自然照明和新身份方面面临挑战。为了弥补这一差距，我们推出了 URHand，这是第一个通用的可重新照明的手模型，它概括了视角、姿势、照明和身份。我们的模型允许使用手机捕获的图像进行少量拍摄的个性化，并且可以在新颖的照明下进行照片级真实感渲染。为了简化个性化过程，同时保留照片真实感，我们基于在具有数百个身份的光阶段中捕获的手部多视图图像的神经重新照明，构建了一个强大的通用可重新照明先验。关键的挑战是扩展跨身份训练，同时保持个性化的保真度和清晰的细节，而不影响自然照明下的泛化能力。为此，我们提出了一种空间变化的线性光照模型作为神经渲染器，它将物理启发的着色作为输入特征。通过消除非线性激活和偏差，我们专门设计的照明模型明确地保持了光传输的线性。这使得能够从光阶段数据进行单阶段训练，同时推广到跨不同身份的任意连续照明下的实时渲染。此外，我们引入了基于物理的模型和神经重新照明模型的联合学习，这进一步提高了保真度和泛化能力。大量的实验表明，我们的方法在质量和通用性方面都优于现有方法。我们还演示了通过对看不见的身份进行简短的手机扫描来快速个性化 URHand。</details>
**PDF:** <http://arxiv.org/pdf/2401.05334v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Score Distillation Sampling with Learned Manifold Corrective**<br />
**Title_cn:** 使用学习流形校正对蒸馏采样进行评分<br />
**Authors:** Thiemo Alldieck, Nikos Kolotouros, Cristian Sminchisescu<br />
**Abstract:** <details><summary>原文: </summary>Score Distillation Sampling (SDS) is a recent but already widely popular method that relies on an image diffusion model to control optimization problems using text prompts. In this paper, we conduct an in-depth analysis of the SDS loss function, identify an inherent problem with its formulation, and propose a surprisingly easy but effective fix. Specifically, we decompose the loss into different factors and isolate the component responsible for noisy gradients. In the original formulation, high text guidance is used to account for the noise, leading to unwanted side effects. Instead, we train a shallow network mimicking the timestep-dependent denoising deficiency of the image diffusion model in order to effectively factor it out. We demonstrate the versatility and the effectiveness of our novel loss formulation through several qualitative and quantitative experiments, including optimization-based image synthesis and editing, zero-shot image translation network training, and text-to-3D synthesis.</details>
**Abstract_cn:** <details><summary>译文: </summary>分数蒸馏采样 (SDS) 是一种最近但已经广泛流行的方法，它依靠图像扩散模型使用文本提示来控制优化问题。在本文中，我们对 SDS 损失函数进行了深入分析，确定了其公式的固有问题，并提出了一个非常简单但有效的解决方案。具体来说，我们将损失分解为不同的因素，并隔离导致噪声梯度的成分。在最初的配方中，使用高文本指导来解决噪音，从而导致不必要的副作用。相反，我们训练一个浅层网络来模仿图像扩散模型的时间步相关的去噪缺陷，以便有效地将其分解出来。我们通过几个定性和定量实验证明了我们新颖的损失公式的多功能性和有效性，包括基于优化的图像合成和编辑、零样本图像翻译网络训练以及文本到 3D 合成。</details>
**PDF:** <http://arxiv.org/pdf/2401.05293v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CLIP-guided Source-free Object Detection in Aerial Images**<br />
**Title_cn:** CLIP 引导的航空图像中的无源物体检测<br />
**Authors:** Nanqing Liu, Xun Xu, Yongyi Su, Chengxin Liu, Peiliang Gong, Heng-Chao Li<br />
**Abstract:** <details><summary>原文: </summary>Domain adaptation is crucial in aerial imagery, as the visual representation of these images can significantly vary based on factors such as geographic location, time, and weather conditions. Additionally, high-resolution aerial images often require substantial storage space and may not be readily accessible to the public. To address these challenges, we propose a novel Source-Free Object Detection (SFOD) method. Specifically, our approach is built upon a self-training framework; however, self-training can lead to inaccurate learning in the absence of labeled training data. To address this issue, we further integrate Contrastive Language-Image Pre-training (CLIP) to guide the generation of pseudo-labels, termed CLIP-guided Aggregation. By leveraging CLIP's zero-shot classification capability, we use it to aggregate scores with the original predicted bounding boxes, enabling us to obtain refined scores for the pseudo-labels. To validate the effectiveness of our method, we constructed two new datasets from different domains based on the DIOR dataset, named DIOR-C and DIOR-Cloudy. Experiments demonstrate that our method outperforms other comparative algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>域适应在航空图像中至关重要，因为这些图像的视觉表示可能会根据地理位置、时间和天气条件等因素而发生显着变化。此外，高分辨率航拍图像通常需要大量存储空间，并且公众可能无法轻松访问。为了解决这些挑战，我们提出了一种新颖的无源对象检测（SFOD）方法。具体来说，我们的方法是建立在自我训练框架之上的；然而，在没有标记的训练数据的情况下，自我训练可能会导致学习不准确。为了解决这个问题，我们进一步集成对比语言图像预训练（CLIP）来指导伪标签的生成，称为 CLIP 引导聚合。通过利用 CLIP 的零样本分类功能，我们用它来聚合原始预测边界框的分数，使我们能够获得伪标签的精确分数。为了验证我们方法的有效性，我们基于 DIOR 数据集构建了来自不同领域的两个新数据集，分别命名为 DIOR-C 和 DIOR-Cloudy。实验表明我们的方法优于其他比较算法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05168v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN**<br />
**Title_cn:** Derm-T2IM：通过稳定扩散模型利用合成皮肤病变数据，使用 ViT 和 CNN 增强皮肤疾病分类<br />
**Authors:** Muhammad Ali Farooq, Wang Yao, Michael Schukat, Mark A Little, Peter Corcoran<br />
**Abstract:** <details><summary>原文: </summary>This study explores the utilization of Dermatoscopic synthetic data generated through stable diffusion models as a strategy for enhancing the robustness of machine learning model training. Synthetic data generation plays a pivotal role in mitigating challenges associated with limited labeled datasets, thereby facilitating more effective model training. In this context, we aim to incorporate enhanced data transformation techniques by extending the recent success of few-shot learning and a small amount of data representation in text-to-image latent diffusion models. The optimally tuned model is further used for rendering high-quality skin lesion synthetic data with diverse and realistic characteristics, providing a valuable supplement and diversity to the existing training data. We investigate the impact of incorporating newly generated synthetic data into the training pipeline of state-of-art machine learning models, assessing its effectiveness in enhancing model performance and generalization to unseen real-world data. Our experimental results demonstrate the efficacy of the synthetic data generated through stable diffusion models helps in improving the robustness and adaptability of end-to-end CNN and vision transformer models on two different real-world skin lesion datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究探索利用通过稳定扩散模型生成的皮肤镜合成数据作为增强机器学习模型训练稳健性的策略。合成数据生成在缓解与有限标记数据集相关的挑战方面发挥着关键作用，从而促进更有效的模型训练。在这种背景下，我们的目标是通过扩展最近成功的少样本学习和文本到图像潜在扩散模型中的少量数据表示来整合增强的数据转换技术。经过优化调整的模型进一步用于渲染具有多样化和真实特征的高质量皮肤病变合成数据，为现有训练数据提供了有价值的补充和多样性。我们研究了将新生成的合成数据纳入最先进的机器学习模型的训练流程中的影响，评估其在增强模型性能和对未见过的现实世界数据的泛化方面的有效性。我们的实验结果表明，通过稳定扩散模型生成的合成数据的有效性有助于提高端到端 CNN 和视觉变换器模型在两个不同的真实皮肤病变数据集上的鲁棒性和适应性。</details>
**PDF:** <http://arxiv.org/pdf/2401.05159v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model**<br />
**Title_cn:** CrossDiff：通过交叉预测扩散模型探索全色锐化的自监督表示<br />
**Authors:** Yinghui Xing, Litao Qu, ShiZhou Zhang, Xiuwei Zhang, Yanning Zhang<br />
**Abstract:** <details><summary>原文: </summary>Fusion of a panchromatic (PAN) image and corresponding multispectral (MS) image is also known as pansharpening, which aims to combine abundant spatial details of PAN and spectral information of MS. Due to the absence of high-resolution MS images, available deep-learning-based methods usually follow the paradigm of training at reduced resolution and testing at both reduced and full resolution. When taking original MS and PAN images as inputs, they always obtain sub-optimal results due to the scale variation. In this paper, we propose to explore the self-supervised representation of pansharpening by designing a cross-predictive diffusion model, named CrossDiff. It has two-stage training. In the first stage, we introduce a cross-predictive pretext task to pre-train the UNet structure based on conditional DDPM, while in the second stage, the encoders of the UNets are frozen to directly extract spatial and spectral features from PAN and MS, and only the fusion head is trained to adapt for pansharpening task. Extensive experiments show the effectiveness and superiority of the proposed model compared with state-of-the-art supervised and unsupervised methods. Besides, the cross-sensor experiments also verify the generalization ability of proposed self-supervised representation learners for other satellite's datasets. We will release our code for reproducibility.</details>
**Abstract_cn:** <details><summary>译文: </summary>全色（PAN）图像和相应的多光谱（MS）图像的融合也称为全色锐化，其目的是将PAN的丰富空间细节和MS的光谱信息结合起来。由于缺乏高分辨率 MS 图像，可用的基于深度学习的方法通常遵循降低分辨率训练以及降低分辨率和全分辨率测试的范式。当以原始 MS 和 PAN 图像作为输入时，由于尺度变化，它们总是获得次优结果。在本文中，我们建议通过设计一个名为 CrossDiff 的交叉预测扩散模型来探索全色锐化的自监督表示。它有两个阶段的训练。在第一阶段，我们引入交叉预测借口任务来基于条件DDPM预训练UNet结构，而在第二阶段，UNet的编码器被冻结以直接从PAN和MS中提取空间和光谱特征，并且只有融合头经过训练以适应全色锐化任务。大量的实验表明，与最先进的监督和无监督方法相比，所提出的模型的有效性和优越性。此外，跨传感器实验还验证了所提出的自监督表示学习器对其他卫星数据集的泛化能力。我们将发布我们的代码以实现可重复性。</details>
**PDF:** <http://arxiv.org/pdf/2401.05153v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image**<br />
**Title_cn:** SwiMDiff：遥感图像具有扩散约束的全场景匹配对比学习<br />
**Authors:** Jiayuan Tian, Jie Lei, Jiaqing Zhang, Weiying Xie, Yunsong Li<br />
**Abstract:** <details><summary>原文: </summary>With recent advancements in aerospace technology, the volume of unlabeled remote sensing image (RSI) data has increased dramatically. Effectively leveraging this data through self-supervised learning (SSL) is vital in the field of remote sensing. However, current methodologies, particularly contrastive learning (CL), a leading SSL method, encounter specific challenges in this domain. Firstly, CL often mistakenly identifies geographically adjacent samples with similar semantic content as negative pairs, leading to confusion during model training. Secondly, as an instance-level discriminative task, it tends to neglect the essential fine-grained features and complex details inherent in unstructured RSIs. To overcome these obstacles, we introduce SwiMDiff, a novel self-supervised pre-training framework designed for RSIs. SwiMDiff employs a scene-wide matching approach that effectively recalibrates labels to recognize data from the same scene as false negatives. This adjustment makes CL more applicable to the nuances of remote sensing. Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through the implementation of pixel-level diffusion constraints, we enhance the encoder's ability to capture both the global semantic information and the fine-grained features of the images more comprehensively. Our proposed framework significantly enriches the information available for downstream tasks in remote sensing. Demonstrating exceptional performance in change detection and land-cover classification tasks, SwiMDiff proves its substantial utility and value in the field of remote sensing.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着航空航天技术的最新进步，未标记的遥感图像（RSI）数据量急剧增加。通过自我监督学习（SSL）有效利用这些数据在遥感领域至关重要。然而，当前的方法，特别是对比学习（CL）（一种领先的 SSL 方法），在该领域遇到了特定的挑战。首先，CL经常错误地将具有相似语义内容的地理上相邻的样本识别为负对，导致模型训练过程中的混乱。其次，作为实例级判别任务，它往往忽略非结构化 RSI 固有的细粒度特征和复杂细节。为了克服这些障碍，我们引入了 SwiMDiff，这是一种专为 RSI 设计的新型自监督预训练框架。 SwiMDiff 采用场景范围匹配方法，可以有效地重新校准标签，以将来自同一场景的数据识别为漏报。这一调整使 CL 更适用于遥感的细微差别。此外，SwiMDiff 将 CL 与扩散模型无缝集成。通过实施像素级扩散约束，我们增强了编码器更全面地捕获图像的全局语义信息和细粒度特征的能力。我们提出的框架显着丰富了遥感下游任务可用的信息。 SwiMDiff 在变化检测和土地覆盖分类任务中表现出卓越的性能，证明了其在遥感领域的巨大实用性和价值。</details>
**PDF:** <http://arxiv.org/pdf/2401.05093v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Less is More : A Closer Look at Multi-Modal Few-Shot Learning**<br />
**Title_cn:** 少即是多：仔细观察多模态少样本学习<br />
**Authors:** Chunpeng Zhou, Haishuai Wang, Xilu Yuan, Zhi Yu, Jiajun Bu<br />
**Abstract:** <details><summary>原文: </summary>Few-shot Learning aims to learn and distinguish new categories with a very limited number of available images, presenting a significant challenge in the realm of deep learning. Recent researchers have sought to leverage the additional textual or linguistic information of these rare categories with a pre-trained language model to facilitate learning, thus partially alleviating the problem of insufficient supervision signals. However, the full potential of the textual information and pre-trained language model have been underestimated in the few-shot learning till now, resulting in limited performance enhancements. To address this, we propose a simple but effective framework for few-shot learning tasks, specifically designed to exploit the textual information and language model. In more detail, we explicitly exploit the zero-shot capability of the pre-trained language model with the learnable prompt. And we just add the visual feature with the textual feature for inference directly without the intricate designed fusion modules in previous works. Additionally, we apply the self-ensemble and distillation to further enhance these components. Our extensive experiments conducted across four widely used few-shot datasets demonstrate that our simple framework achieves impressive results. Particularly noteworthy is its outstanding performance in the 1-shot learning task, surpassing state-of-the-art methods by an average of 3.0\% in classification accuracy. \footnote{We will make the source codes of the proposed framework publicly available upon acceptance. }.</details>
**Abstract_cn:** <details><summary>译文: </summary>Few-shot Learning 旨在利用数量非常有限的可用图像来学习和区分新类别，这在深度学习领域提出了重大挑战。最近的研究人员试图利用这些罕见类别的额外文本或语言信息与预先训练的语言模型来促进学习，从而部分缓解监督信号不足的问题。然而，迄今为止，在少数样本学习中，文本信息和预训练语言模型的全部潜力被低估，导致性能提升有限。为了解决这个问题，我们提出了一个简单但有效的小样本学习任务框架，专门设计用于利用文本信息和语言模型。更详细地说，我们通过可学习的提示明确地利用了预训练语言模型的零样本功能。我们只是将视觉特征与文本特征直接相加进行推理，而不需要像之前的作品那样复杂地设计融合模块。此外，我们应用自集成和蒸馏来进一步增强这些组件。我们在四个广泛使用的小样本数据集上进行的广泛实验表明，我们的简单框架取得了令人印象深刻的结果。特别值得关注的是它在 1-shot 学习任务中的出色表现，分类准确率平均超过最先进的方法 3.0%。 \footnote{我们将在接受后公开拟议框架的源代码。 }。</details>
**PDF:** <http://arxiv.org/pdf/2401.05010v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic Polyp Detection**<br />
**Title_cn:** ECC-PolypDet：具有对比学习的增强型 CenterNet，用于自动息肉检测<br />
**Authors:** Yuncheng Jiang, Zixun Zhang, Yiwen Hu, Guanbin Li, Xiang Wan, Song Wu, Shuguang Cui, Silin Huang, Zhen Li<br />
**Abstract:** <details><summary>原文: </summary>Accurate polyp detection is critical for early colorectal cancer diagnosis. Although remarkable progress has been achieved in recent years, the complex colon environment and concealed polyps with unclear boundaries still pose severe challenges in this area. Existing methods either involve computationally expensive context aggregation or lack prior modeling of polyps, resulting in poor performance in challenging cases. In this paper, we propose the Enhanced CenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \& end-to-end inference framework that leverages images and bounding box annotations to train a general model and fine-tune it based on the inference score to obtain a final robust model. Specifically, we conduct Box-assisted Contrastive Learning (BCL) during training to minimize the intra-class difference and maximize the inter-class difference between foreground polyps and backgrounds, enabling our model to capture concealed polyps. Moreover, to enhance the recognition of small polyps, we design the Semantic Flow-guided Feature Pyramid Network (SFFPN) to aggregate multi-scale features and the Heatmap Propagation (HP) module to boost the model's attention on polyp targets. In the fine-tuning stage, we introduce the IoU-guided Sample Re-weighting (ISR) mechanism to prioritize hard samples by adaptively adjusting the loss weight for each sample during fine-tuning. Extensive experiments on six large-scale colonoscopy datasets demonstrate the superiority of our model compared with previous state-of-the-art detectors.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的息肉检测对于早期结直肠癌诊断至关重要。尽管近年来取得了显着的进展，但复杂的结肠环境和边界不清晰的隐匿性息肉仍然给该领域带来严峻的挑战。现有方法要么涉及计算成本高昂的上下文聚合，要么缺乏息肉的预先建模，导致在具有挑战性的情况下表现不佳。在本文中，我们提出了具有对比学习的增强型 CenterNet（ECC-PolypDet），这是一个两阶段训练和端到端推理框架，利用图像和边界框注释来训练通用模型并基于的推理分数以获得最终的稳健模型。具体来说，我们在训练期间进行框辅助对比学习（BCL），以最小化类内差异并最大化前景息肉和背景之间的类间差异，使我们的模型能够捕获隐藏的息肉。此外，为了增强对小息肉的识别，我们设计了语义流引导特征金字塔网络（SFFPN）来聚合多尺度特征，并设计了热图传播（HP）模块来提高模型对息肉目标的关注。在微调阶段，我们引入了 IoU 引导的样本重新加权（ISR）机制，通过在微调过程中自适应调整每个样本的损失权重来优先考虑硬样本。对六个大规模结肠镜检查数据集的广泛实验证明了我们的模型与以前最先进的探测器相比的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.04961v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics**<br />
**Title_cn:** 视频中的延迟感知道路异常分割：真实感数据集和新指标<br />
**Authors:** Beiwen Tian, Huan-ang Gao, Leiyao Cui, Yupeng Zheng, Lan Luo, Baofeng Wang, Rong Zhi, Guyue Zhou, Hao Zhao<br />
**Abstract:** <details><summary>原文: </summary>In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的几年里，道路异常分割在学术界得到了积极的探索，并越来越受到业界的关注。背后的原理很简单：如果自动驾驶汽车能够在撞到异常物体之前刹车，那么安全性就会得到提升。然而，这个基本原理自然需要一个临时通知的设置，而现有的方法和基准是以不切实际的框架方式设计的。为了弥补这一差距，我们贡献了第一个用于自动驾驶的视频异常分割数据集。由于将各种异常物体放置在繁忙的道路上并在每一帧中对其进行注释既危险又昂贵，因此我们求助于合成数据。为了提高该合成数据集与现实世界应用的相关性，我们训练了一个以渲染 G 缓冲区为条件的生成对抗网络，以增强照片真实感。我们的数据集由 7 个不同城镇记录的 120,000 个 60 FPS 帧速率的高分辨率帧组成。作为初始基准测试，我们使用最新的监督和无监督道路异常分割方法提供基线。除了传统指标之外，我们还关注两个新指标：时间一致性和延迟感知流准确性。我们认为后者很有价值，因为它衡量异常分割算法是否能够真正防止汽车在临时信息环境中发生碰撞。</details>
**PDF:** <http://arxiv.org/pdf/2401.04942v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Modality-Aware Representation Learning for Zero-shot Sketch-based Image Retrieval**<br />
**Title_cn:** 用于基于零样本草图的图像检索的模态感知表示学习<br />
**Authors:** Eunyi Lyou, Doyeon Lee, Jooeun Kim, Joonseok Lee<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>零样本学习为机器学习模型提供了一种有效的解决方案来处理看不见的类别，避免详尽的数据收集。基于零样本草图的图像检索 (ZS-SBIR) 模拟现实世界的场景，在这些场景中，收集配对的草图照片样本既困难又昂贵。我们提出了一种新颖的框架，通过文本对比草图和照片来间接对齐草图和照片，从而消除了访问草图-照片对的必要性。通过从数据中学习到的显式模态编码，我们的方法将模态不可知的语义与模态特定的信息分开，弥合模态差距并在联合潜在空间内实现有效的跨模态内容检索。通过综合实验，我们验证了所提出模型在 ZS-SBIR 上的有效性，并且它也可以应用于广义和细粒度设置。</details>
**PDF:** <http://arxiv.org/pdf/2401.04860v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Towards Online Sign Language Recognition and Translation**<br />
**Title_cn:** 走向在线手语识别和翻译<br />
**Authors:** Ronglai Zuo, Fangyun Wei, Brian Mak<br />
**Abstract:** <details><summary>原文: </summary>The objective of sign language recognition is to bridge the communication gap between the deaf and the hearing. Numerous previous works train their models using the well-established connectionist temporal classification (CTC) loss. During the inference stage, the CTC-based models typically take the entire sign video as input to make predictions. This type of inference scheme is referred to as offline recognition. In contrast, while mature speech recognition systems can efficiently recognize spoken words on the fly, sign language recognition still falls short due to the lack of practical online solutions. In this work, we take the first step towards filling this gap. Our approach comprises three phases: 1) developing a sign language dictionary encompassing all glosses present in a target sign language dataset; 2) training an isolated sign language recognition model on augmented signs using both conventional classification loss and our novel saliency loss; 3) employing a sliding window approach on the input sign sequence and feeding each sign clip to the well-optimized model for online recognition. Furthermore, our online recognition model can be extended to boost the performance of any offline model, and to support online translation by appending a gloss-to-text network onto the recognition model. By integrating our online framework with the previously best-performing offline model, TwoStream-SLR, we achieve new state-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T, and CSL-Daily. Code and models will be available at https://github.com/FangyunWei/SLRT</details>
**Abstract_cn:** <details><summary>译文: </summary>手语识别的目标是弥合聋哑人和听力正常者之间的沟通差距。之前的许多工作都使用完善的联结主义时间分类（CTC）损失来训练他们的模型。在推理阶段，基于 CTC 的模型通常将整个标志视频作为输入来进行预测。这种类型的推理方案称为离线识别。相比之下，虽然成熟的语音识别系统可以有效地即时识别口语单词，但由于缺乏实用的在线解决方案，手语识别仍然存在不足。在这项工作中，我们迈出了填补这一空白的第一步。我们的方法包括三个阶段：1）开发包含目标手语数据集中存在的所有注释的手语词典； 2）使用传统的分类损失和我们新颖的显着性损失来训练增强符号的孤立手语识别模型； 3）对输入符号序列采用滑动窗口方法，并将每个符号片段输入到优化良好的模型中进行在线识别。此外，我们的在线识别模型可以扩展以提高任何离线模型的性能，并通过在识别模型上附加注释到文本网络来支持在线翻译。通过将我们的在线框架与之前性能最佳的离线模型 TwoStream-SLR 集成，我们在三个基准测试中实现了新的最先进的性能：Phoenix-2014、Phoenix-2014T 和 CSL-Daily。代码和模型可在 https://github.com/FangyunWei/SLRT 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.05336v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video**<br />
**Title_cn:** ANIM-400K：用于视频自动端到端配音的大规模数据集<br />
**Authors:** Kevin Cai, Chonghua Liu, David M. Chan<br />
**Abstract:** <details><summary>原文: </summary>The Internet's wealth of content, with up to 60% published in English, starkly contrasts the global population, where only 18.8% are English speakers, and just 5.1% consider it their native language, leading to disparities in online information access. Unfortunately, automated processes for dubbing of video - replacing the audio track of a video with a translated alternative - remains a complex and challenging task due to pipelines, necessitating precise timing, facial movement synchronization, and prosody matching. While end-to-end dubbing offers a solution, data scarcity continues to impede the progress of both end-to-end and pipeline-based methods. In this work, we introduce Anim-400K, a comprehensive dataset of over 425K aligned animated video segments in Japanese and English supporting various video-related tasks, including automated dubbing, simultaneous translation, guided video summarization, and genre/theme/style classification. Our dataset is made publicly available for research purposes at https://github.com/davidmchan/Anim400K.</details>
**Abstract_cn:** <details><summary>译文: </summary>互联网内容丰富，其中高达 60% 的内容以英语发布，这与全球人口形成鲜明对比，全球人口中只有 18.8% 的人讲英语，只有 5.1% 的人认为英语是自己的母语，导致在线信息获取的差异。不幸的是，由于管道的原因，视频配音的自动化过程（用翻译后的替代方案替换视频的音轨）仍然是一项复杂且具有挑战性的任务，需要精确的计时、面部运动同步和韵律匹配。虽然端到端配音提供了一种解决方案，但数据稀缺仍然阻碍着端到端和基于管道的方法的进展。在这项工作中，我们介绍了 Anim-400K，这是一个包含超过 425K 对齐的日语和英语动画视频片段的综合数据集，支持各种视频相关任务，包括自动配音、同声翻译、引导视频摘要和流派/主题/风格分类。我们的数据集已公开用于研究目的，网址为 https://github.com/davidmchan/Anim400K。</details>
**PDF:** <http://arxiv.org/pdf/2401.05314v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks**<br />
**Title_cn:** 战略客户选择以解决支持 HAPS 的 FL 网络中的非独立同分布问题<br />
**Authors:** Amin Farajzadeh, Animesh Yadav, Halim Yanikomeroglu<br />
**Abstract:** <details><summary>原文: </summary>The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation in FL training, our approach fosters a more uniform and representative data distribution across the network. Our simulations demonstrate that this targeted client selection methodology significantly reduces the training loss of FL models in HAPS networks, thereby effectively tackling a crucial challenge in implementing large-scale FL systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>在垂直异构网络中部署联邦学习（FL），例如由高空平台站（HAPS）支持的网络，提供了吸引广泛客户的机会，每个客户都具有独特的通信和计算能力。这种多样性不仅提高了 FL 模型的训练精度，而且加速了它们的收敛。然而，在这些广阔的网络中应用 FL 会带来显着的挑战，特别是客户端数据分布中显着的非独立同分布性。这种数据异构性通常会导致收敛速度变慢并降低模型训练性能的有效性。我们的研究引入了一种针对解决此问题而定制的客户选择策略，利用用户网络流量行为。该策略涉及根据客户的网络使用模式对客户进行预测和分类，同时优先考虑用户隐私。通过战略性地选择数据表现出相似模式的客户来参与 FL 培训，我们的方法在整个网络中促进了更加统一和具有代表性的数据分布。我们的模拟表明，这种有针对性的客户选择方法显着减少了 HAPS 网络中 FL 模型的训练损失，从而有效地解决了实施大规模 FL 系统的关键挑战。</details>
**PDF:** <http://arxiv.org/pdf/2401.05308v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Enhanced Muscle and Fat Segmentation for CT-Based Body Composition Analysis: A Comparative Study**<br />
**Title_cn:** 基于 CT 的身体成分分析的增强肌肉和脂肪分割：比较研究<br />
**Authors:** Benjamin Hou, Tejas Sudharshan Mathai, Jianfei Liu, Christopher Parnell, Ronald M. Summers<br />
**Abstract:** <details><summary>原文: </summary>Purpose: Body composition measurements from routine abdominal CT can yield personalized risk assessments for asymptomatic and diseased patients. In particular, attenuation and volume measures of muscle and fat are associated with important clinical outcomes, such as cardiovascular events, fractures, and death. This study evaluates the reliability of an Internal tool for the segmentation of muscle and fat (subcutaneous and visceral) as compared to the well-established public TotalSegmentator tool.   Methods: We assessed the tools across 900 CT series from the publicly available SAROS dataset, focusing on muscle, subcutaneous fat, and visceral fat. The Dice score was employed to assess accuracy in subcutaneous fat and muscle segmentation. Due to the lack of ground truth segmentations for visceral fat, Cohen's Kappa was utilized to assess segmentation agreement between the tools.   Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for subcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation respectively. A Wilcoxon signed-rank test revealed that our results were statistically different with p<0.01. For visceral fat, the Cohen's kappa score of 0.856 indicated near-perfect agreement between the two tools. Our internal tool also showed very strong correlations for muscle volume (R^2=0.99), muscle attenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate correlation for subcutaneous fat attenuation (R^2=0.45).   Conclusion: Our findings indicated that our Internal tool outperformed TotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen's Kappa score for visceral fat suggests a reliable level of agreement between the two tools. These results demonstrate the potential of our tool in advancing the accuracy of body composition analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：常规腹部 CT 的身体成分测量可以对无症状和患病患者进行个性化风险评估。特别是，肌肉和脂肪的衰减和体积测量与重要的临床结果相关，例如心血管事件、骨折和死亡。本研究评估了用于分割肌肉和脂肪（皮下和内脏）的内部工具与成熟的公共 TotalSegmentator 工具相比的可靠性。方法：我们评估了公开的 SAROS 数据集中的 900 个 CT 系列的工具，重点关注肌肉、皮下脂肪和内脏脂肪。 Dice 评分用于评估皮下脂肪和肌肉分割的准确性。由于缺乏内脏脂肪的真实分割，因此利用 Cohen 的 Kappa 来评估工具之间的分割一致性。结果：我们的内部工具在皮下脂肪方面的 Dice 提高了 3%（83.8 比 80.8），在肌肉分割方面提高了 5%（87.6 比 83.2）。 Wilcoxon 符号秩检验显示我们的结果具有统计学差异，p<0.01。对于内脏脂肪，Cohen 的 kappa 得分为 0.856，表明两种工具之间几乎完美一致。我们的内部工具还显示，肌肉体积 (R^2=0.99)、肌肉衰减 (R^2=0.93) 和皮下脂肪体积 (R^2=0.99) 之间具有非常强的相关性，而皮下脂肪衰减 (R^2=0.99) 具有中等相关性。 ^2=0.45）。结论：我们的研究结果表明，我们的内部工具在测量皮下脂肪和肌肉方面优于 TotalSegmentator。内脏脂肪的高 Cohen Kappa 分数表明两种工具之间具有可靠的一致性。这些结果证明了我们的工具在提高身体成分分析准确性方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.05294v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Do Vision and Language Encoders Represent the World Similarly?**<br />
**Title_cn:** 视觉和语言编码器是否同样代表世界？<br />
**Authors:** Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Mohamed El Amine Seddik, Karttikeya Mangalam, Noel E. O'Connor<br />
**Abstract:** <details><summary>原文: </summary>Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demonstrate the effectiveness of this on several downstream tasks including cross-lingual, cross-domain caption matching and image classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>对齐的文本图像编码器（例如 CLIP）已成为视觉语言任务的事实上的模型。此外，特定于模态的编码器在各自的领域中取得了令人印象深刻的性能。这就提出了一个核心问题：单模态视觉和语言编码器之间是否存在一致性，因为它们从根本上代表了相同的物理世界？使用中心核对齐（CKA）分析图像字幕基准上的视觉和语言模型的潜在空间结构，我们发现未对齐和对齐编码器的表示空间在语义上相似。在像 CLIP 这样的对齐编码器中缺乏统计相似性的情况下，我们表明在没有任何训练的情况下存在未对齐编码器的可能匹配。我们将其构建为利用图之间语义相似性的种子图匹配问题，并提出了两种方法 - 快速二次分配问题优化和新颖的基于局部 CKA 度量的匹配/检索。我们在几个下游任务上证明了这一点的有效性，包括跨语言、跨域标题匹配和图像分类。</details>
**PDF:** <http://arxiv.org/pdf/2401.05224v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits**<br />
**Title_cn:** 使用姿势估计和多种运动特征进行基于视频的奶牛自动跛行检测<br />
**Authors:** Helena Russello, Rik van der Tol, Menno Holzhauer, Eldert J. van Henten, Gert Kootstra<br />
**Abstract:** <details><summary>原文: </summary>This study presents an automated lameness detection system that uses deep-learning image processing techniques to extract multiple locomotion traits associated with lameness. Using the T-LEAP pose estimation model, the motion of nine keypoints was extracted from videos of walking cows. The videos were recorded outdoors, with varying illumination conditions, and T-LEAP extracted 99.6% of correct keypoints. The trajectories of the keypoints were then used to compute six locomotion traits: back posture measurement, head bobbing, tracking distance, stride length, stance duration, and swing duration. The three most important traits were back posture measurement, head bobbing, and tracking distance. For the ground truth, we showed that a thoughtful merging of the scores of the observers could improve intra-observer reliability and agreement. We showed that including multiple locomotion traits improves the classification accuracy from 76.6% with only one trait to 79.9% with the three most important traits and to 80.1% with all six locomotion traits.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究提出了一种自动跛行检测系统，该系统使用深度学习图像处理技术来提取与跛行相关的多种运动特征。使用T-LEAP姿态估计模型，从牛行走的视频中提取了九个关键点的运动。视频是在户外、不同光照条件下录制的，T-LEAP 提取了 99.6% 的正确关键点。然后使用关键点的轨迹来计算六种运动特征：背部姿势测量、头部摆动、跟踪距离、步幅、站立持续时间和摆动持续时间。三个最重要的特征是背部姿势测量、头部摆动和跟踪距离。对于基本事实，我们表明，对观察者分数进行深思熟虑的合并可以提高观察者内部的可靠性和一致性。我们发现，包含多个运动特征可以将分类准确率从只有一种特征的 76.6% 提高到三个最重要特征的分类准确率 79.9%，以及所有六种运动特征的分类准确率提高到 80.1%。</details>
**PDF:** <http://arxiv.org/pdf/2401.05202v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Watermark Text Pattern Spotting in Document Images**<br />
**Title_cn:** 文档图像中的水印文本图案识别<br />
**Authors:** Mateusz Krubinski, Stefan Matcovici, Diana Grigore, Daniel Voinea, Alin-Ionut Popa<br />
**Abstract:** <details><summary>原文: </summary>Watermark text spotting in document images can offer access to an often unexplored source of information, providing crucial evidence about a record's scope, audience and sometimes even authenticity. Stemming from the problem of text spotting, detecting and understanding watermarks in documents inherits the same hardships - in the wild, writing can come in various fonts, sizes and forms, making generic recognition a very difficult problem. To address the lack of resources in this field and propel further research, we propose a novel benchmark (K-Watermark) containing 65,447 data samples generated using Wrender, a watermark text patterns rendering procedure. A validity study using humans raters yields an authenticity score of 0.51 against pre-generated watermarked documents. To prove the usefulness of the dataset and rendering technique, we developed an end-to-end solution (Wextract) for detecting the bounding box instances of watermark text, while predicting the depicted text. To deal with this specific task, we introduce a variance minimization loss and a hierarchical self-attention mechanism. To the best of our knowledge, we are the first to propose an evaluation benchmark and a complete solution for retrieving watermarks from documents surpassing baselines by 5 AP points in detection and 4 points in character accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>文档图像中的水印文本识别可以提供对经常未探索的信息源的访问，提供有关记录范围、受众甚至有时甚至真实性的重要证据。源于文本识别的问题，检测和理解文档中的水印也面临着同样的困难——在野外，书写可能有各种字体、大小和形式，这使得通用识别成为一个非常困难的问题。为了解决该领域资源缺乏的问题并推动进一步研究，我们提出了一个新颖的基准（K-Watermark），其中包含使用 Wrender（一种水印文本模式渲染程序）生成的 65,447 个数据样本。使用人类评分者进行的有效性研究相对于预先生成的带水印的文档得出了 0.51 的真实性得分。为了证明数据集和渲染技术的有用性，我们开发了一种端到端解决方案（Wextract），用于检测水印文本的边界框实例，同时预测所描绘的文本。为了处理这个特定任务，我们引入了方差最小化损失和分层自注意力机制。据我们所知，我们是第一个提出评估基准和完整的解决方案，用于从检测超过基线 5 个 AP 点和字符准确度超过基线 4 点的文档中检索水印。</details>
**PDF:** <http://arxiv.org/pdf/2401.05167v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge**<br />
**Title_cn:** REACT 2024：第二届多重适当面部反应生成挑战赛<br />
**Authors:** Siyang Song, Micol Spitale, Cheng Luo, Cristina Palmero, German Barquero, Hengde Zhu, Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In dyadic interactions, humans communicate their intentions and state of mind using verbal and non-verbal cues, where multiple different facial reactions might be appropriate in response to a specific speaker behaviour. Then, how to develop a machine learning (ML) model that can automatically generate multiple appropriate, diverse, realistic and synchronised human facial reactions from an previously unseen speaker behaviour is a challenging task. Following the successful organisation of the first REACT challenge (REACT 2023), this edition of the challenge (REACT 2024) employs a subset used by the previous challenge, which contains segmented 30-secs dyadic interaction clips originally recorded as part of the NOXI and RECOLA datasets, encouraging participants to develop and benchmark Machine Learning (ML) models that can generate multiple appropriate facial reactions (including facial image sequences and their attributes) given an input conversational partner's stimulus under various dyadic video conference scenarios. This paper presents: (i) the guidelines of the REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii) the performance of the baseline systems on the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation, respectively. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.</details>
**Abstract_cn:** <details><summary>译文: </summary>在二元互动中，人类使用言语和非言语暗示来传达他们的意图和精神状态，其中多种不同的面部反应可能适合响应特定的说话者行为。那么，如何开发一种机器学习（ML）模型，能够根据以前未见过的说话者行为自动生成多种适当的、多样化的、真实的和同步的人类面部反应，是一项具有挑战性的任务。继首届 REACT 挑战赛 (REACT 2023) 成功组织之后，本次挑战赛 (REACT 2024) 采用了上一挑战赛使用的子集，其中包含最初作为 NOXI 和 RECOLA 的一部分录制的分段 30 秒二元交互剪辑数据集，鼓励参与者开发和基准测试机器学习（ML）模型，这些模型可以在各种二元视频会议场景下，根据输入对话伙伴的刺激，生成多种适当的面部反应（包括面部图像序列及其属性）。本文介绍了：(i) REACT 2024 挑战赛的指南； (ii) 挑战中使用的数据集； (iii) 基线系统在两个拟议子挑战上的表现：分别是离线多重适当面部反应生成和在​​线多重适当面部反应生成。挑战基线代码可在 https://github.com/reactmultimodalchallenge/baseline_react2024 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.05166v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **MISS: A Generative Pretraining and Finetuning Approach for Med-VQA**<br />
**Title_cn:** MISS：Med-VQA 的生成预训练和微调方法<br />
**Authors:** Jiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, Lihua Zhang<br />
**Abstract:** <details><summary>原文: </summary>Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models. The code and model weights will be released upon the paper's acceptance.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学视觉问答（VQA）是一项具有挑战性的多模态任务，其中视觉语言预训练（VLP）模型可以有效提高泛化性能。然而，医学领域的大多数方法将VQA视为答案分类任务，很难迁移到实际应用场景。此外，由于医学图像的隐私性和昂贵的注释过程，严重缺乏用于预训练的大规模医学图像-文本对数据集。在本文中，我们提出了一种用于医学 VQA 任务的基于大规模多任务自监督学习的框架（MISS）。与现有方法不同，我们将医学 VQA 视为一项生成任务。我们统一文本编码器和多模态编码器，并通过多任务学习对齐图像文本特征。此外，我们提出了一种传输和标题方法，该方法使用大语言模型（LLM）扩展单模态图像数据集的特征空间，使这些传统的医学视觉领域任务数据能够应用于 VLP。实验表明，我们的方法用较少的多模态数据集取得了优异的结果，并展示了生成式 VQA 模型的优势。代码和模型权重将在论文被接受后发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.05163v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Toward distortion-aware change detection in realistic scenarios**<br />
**Title_cn:** 在现实场景中实现失真感知变化检测<br />
**Authors:** Yitao Zhao, Heng-Chao Li, Nanqing Liu, Rui Wang<br />
**Abstract:** <details><summary>原文: </summary>In the conventional change detection (CD) pipeline, two manually registered and labeled remote sensing datasets serve as the input of the model for training and prediction. However, in realistic scenarios, data from different periods or sensors could fail to be aligned as a result of various coordinate systems. Geometric distortion caused by coordinate shifting remains a thorny issue for CD algorithms. In this paper, we propose a reusable self-supervised framework for bitemporal geometric distortion in CD tasks. The whole framework is composed of Pretext Representation Pre-training, Bitemporal Image Alignment, and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the key components of the framework can be reused for assistance in the bitemporal image alignment, while simultaneously enhancing the performance of the CD decoder. Experimental results in 2 large-scale realistic scenarios demonstrate that our proposed method can alleviate the bitemporal geometric distortion in CD tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在传统的变化检测（CD）管道中，两个手动注册和标记的遥感数据集作为训练和预测模型的输入。然而，在现实场景中，由于坐标系不同，来自不同时期或传感器的数据可能无法对齐。坐标偏移引起的几何失真仍然是 CD 算法的棘手问题。在本文中，我们提出了一种可重用的自我监督框架，用于 CD 任务中的双时态几何失真。整个框架由借口表示预训练、双时图像对齐和下游解码器微调组成。只需单阶段预训练，框架的关键组件就可以重复使用，以帮助双时图像对齐，同时增强 CD 解码器的性能。 2个大规模现实场景的实验结果表明，我们提出的方法可以减轻CD任务中的双时态几何失真。</details>
**PDF:** <http://arxiv.org/pdf/2401.05157v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography Angiography for Automatic Diabetic Retinopathy Diagnosis**<br />
**Title_cn:** 发现：用于自动糖尿病视网膜病变诊断的光学相干断层扫描血管造影的二维多视图总结<br />
**Authors:** Mostafa El Habib Daho, Yihao Li, Rachid Zeghlache, Hugo Le Boité, Pierre Deman, Laurent Borderie, Hugang Ren, Niranchana Mannivanan, Capucine Lepicard, Béatrice Cochener, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading cause of blindness worldwide. Traditionally, DR is monitored using Color Fundus Photography (CFP), a widespread 2-D imaging modality. However, DR classifications based on CFP have poor predictive power, resulting in suboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a recent 3-D imaging modality offering enhanced structural and functional information (blood flow) with a wider field of view. This paper investigates automatic DR severity assessment using 3-D OCTA. A straightforward solution to this task is a 3-D neural network classifier. However, 3-D architectures have numerous parameters and typically require many training samples. A lighter solution consists in using 2-D neural network classifiers processing 2-D en-face (or frontal) projections and/or 2-D cross-sectional slices. Such an approach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face flow maps are often used to detect avascular zones and neovascularization, and 2) cross-sectional slices are commonly analyzed to detect macular edemas, for instance. However, arbitrary data reduction or selection might result in information loss. Two complementary strategies are thus proposed to optimally summarize OCTA volumes with 2-D images: 1) a parametric en-face projection optimized through deep learning and 2) a cross-sectional slice selection process controlled through gradient-based attribution. The full summarization and DR classification pipeline is trained from end to end. The automatic 2-D summary can be displayed in a viewer or printed in a report to support the decision. We show that the proposed 2-D summarization and classification pipeline outperforms direct 3-D classification with the advantage of improved interpretability.</details>
**Abstract_cn:** <details><summary>译文: </summary>糖尿病视网膜病变（DR）是糖尿病的一种眼部并发症，是全世界失明的主要原因。传统上，DR 使用彩色眼底摄影 (CFP) 进行监测，这是一种广泛使用的二维成像方式。然而，基于CFP的DR分类预测能力较差，导致DR管理不理想。光学相干断层扫描血管造影 (OCTA) 是一种最新的 3D 成像方式，可提供增强的结构和功能信息（血流）以及更宽的视野。本文研究了使用 3-D OCTA 进行自动 DR 严重性评估。此任务的一个直接解决方案是 3D 神经网络分类器。然而，3D 架构具有大量参数，通常需要大量训练样本。更轻的解决方案包括使用 2-D 神经网络分类器处理 2-D 正面（或正面）投影和/或 2-D 横截面切片。这种方法模仿了眼科医生分析 OCTA 采集的方式：1) 面部血流图通常用于检测无血管区和新生血管形成，2) 例如，通常分析横截面切片以检测黄斑水肿。然而，任意的数据缩减或选择可能会导致信息丢失。因此，提出了两种互补策略来用二维图像最佳地总结 OCTA 体积：1）通过深度学习优化的参数化正面投影；2）通过基于梯度的归因控制的横截面切片选择过程。完整的摘要和 DR 分类管道是端到端训练的。自动二维摘要可以显示在查看器中或打印在报告中以支持决策。我们表明，所提出的 2-D 概括和分类流程优于直接 3-D 分类，具有改进的可解释性的优点。</details>
**PDF:** <http://arxiv.org/pdf/2401.05137v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer**<br />
**Title_cn:** 通过领域适应进行高效微调，以保护隐私的 Vision Transformer<br />
**Authors:** Teru Nagamori, Sayaka Shiota, Hitoshi Kiya<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel method for privacy-preserving deep neural networks (DNNs) with the Vision Transformer (ViT). The method allows us not only to train models and test with visually protected images but to also avoid the performance degradation caused from the use of encrypted images, whereas conventional methods cannot avoid the influence of image encryption. A domain adaptation method is used to efficiently fine-tune ViT with encrypted images. In experiments, the method is demonstrated to outperform conventional methods in an image classification task on the CIFAR-10 and ImageNet datasets in terms of classification accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种使用 Vision Transformer (ViT) 保护隐私的深度神经网络 (DNN) 的新方法。该方法不仅使我们能够使用受视觉保护的图像来训练模型和进行测试，而且还可以避免由于使用加密图像而导致的性能下降，而传统方法无法避免图像加密的影响。域适应方法用于有效地微调加密图像的 ViT。在实验中，该方法在 CIFAR-10 和 ImageNet 数据集上的图像分类任务中被证明在分类精度方面优于传统方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05126v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection**<br />
**Title_cn:** 半监督 3D 物体检测的双视角知识丰富<br />
**Authors:** Yucheng Han, Na Zhao, Weiling Chen, Keng Teck Ma, Hanwang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised 3D object detection is a promising yet under-explored direction to reduce data annotation costs, especially for cluttered indoor scenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this task by utilizing a teacher model to generate pseudo-labels for unlabeled samples. However, the availability of unlabeled samples in the 3D domain is relatively limited compared to its 2D counterpart due to the greater effort required to collect 3D data. Moreover, the loose consistency regularization in SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to either low-quality supervision or a limited amount of pseudo labels. To address these issues, we present a novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection. Our DPKE enriches the knowledge of limited training data, particularly unlabeled data, from two perspectives: data-perspective and feature-perspective. Specifically, from the data-perspective, we propose a class-probabilistic data augmentation method that augments the input data with additional instances based on the varying distribution of class probabilities. Our DPKE achieves feature-perspective knowledge enrichment by designing a geometry-aware feature matching method that regularizes feature-level similarity between object proposals from the student and teacher models. Extensive experiments on the two benchmark datasets demonstrate that our DPKE achieves superior performance over existing state-of-the-art approaches under various label ratio conditions. The source code will be made available to the public.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督 3D 物体检测是一个有前途但尚未充分探索的方向，可降低数据注释成本，尤其是对于杂乱的室内场景。一些先前的工作，例如 SESS 和 3DIoUMatch，试图通过利用教师模型为未标记的样本生成伪标签来解决此任务。然而，由于收集 3D 数据需要付出更大的努力，因此与 2D 领域相比，3D 领域中未标记样本的可用性相对有限。此外，SESS 中的松散一致性正则化和 3DIoUMatch 中的受限伪标签选择策略导致低质量的监督或有限数量的伪标签。为了解决这些问题，我们提出了一种新颖的双视角知识丰富方法，称为 DPKE，用于半监督 3D 对象检测。我们的DPKE从数据角度和特征角度两个角度丰富了有限训练数据，特别是未标记数据的知识。具体来说，从数据的角度来看，我们提出了一种类概率数据增强方法，该方法根据类概率的变化分布使用额外的实例来增强输入数据。我们的 DPKE 通过设计一种几何感知的特征匹配方法来实现特征视角的知识丰富，该方法可以规范学生模型和教师模型的对象建议之间的特征级相似性。对两个基准数据集的大量实验表明，我们的 DPKE 在各种标签比率条件下比现有最先进的方法实现了卓越的性能。源代码将向公众开放。</details>
**PDF:** <http://arxiv.org/pdf/2401.05011v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Optimising Graph Representation for Hardware Implementation of Graph Convolutional Networks for Event-based Vision**<br />
**Title_cn:** 优化基于事件视觉的图卷积网络硬件实现的图表示<br />
**Authors:** Kamil Jeziorek, Piotr Wzorek, Krzysztof Blachut, Andrea Pinna, Tomasz Kryjak<br />
**Abstract:** <details><summary>原文: </summary>Event-based vision is an emerging research field involving processing data generated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest proposals in this area are Graph Convolutional Networks (GCNs), which allow to process events in its original sparse form while maintaining high detection and classification performance. In this paper, we present the hardware implementation of a~graph generation process from an event camera data stream, taking into account both the advantages and limitations of FPGAs. We propose various ways to simplify the graph representation and use scaling and quantisation of values. We consider both undirected and directed graphs that enable the use of PointNet convolution. The results obtained show that by appropriately modifying the graph representation, it is possible to create a~hardware module for graph generation. Moreover, the proposed modifications have no significant impact on object detection performance, only 0.08% mAP less for the base model and the N-Caltech data set.Finally, we describe the proposed hardware architecture of the graph generation module.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于事件的视觉是一个新兴的研究领域，涉及处理动态视觉传感器（神经形态相机）生成的数据。该领域的最新提议之一是图卷积网络（GCN），它允许以原始稀疏形式处理事件，同时保持较高的检测和分类性能。在本文中，我们考虑到 FPGA 的优点和局限性，介绍了从事件摄像机数据流生成图的硬件实现过程。我们提出了各种方法来简化图形表示并使用值的缩放和量化。我们考虑使用 PointNet 卷积的无向图和有向图。获得的结果表明，通过适当修改图表示，可以创建用于图生成的硬件模块。此外，所提出的修改对目标检测性能没有显着影响，对于基础模型和 N-Caltech 数据集仅减少了 0.08% mAP。最后，我们描述了所提出的图生成模块的硬件架构。</details>
**PDF:** <http://arxiv.org/pdf/2401.04988v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition**<br />
**Title_cn:** HaltingVT：用于高效视频识别的自适应令牌停止变压器<br />
**Authors:** Qian Wu, Ruoxuan Cui, Yuke Li, Haoqi Zhu<br />
**Abstract:** <details><summary>原文: </summary>Action recognition in videos poses a challenge due to its high computational cost, especially for Joint Space-Time video transformers (Joint VT). Despite their effectiveness, the excessive number of tokens in such architectures significantly limits their efficiency. In this paper, we propose HaltingVT, an efficient video transformer adaptively removing redundant video patch tokens, which is primarily composed of a Joint VT and a Glimpser module. Specifically, HaltingVT applies data-adaptive token reduction at each layer, resulting in a significant reduction in the overall computational cost. Besides, the Glimpser module quickly removes redundant tokens in shallow transformer layers, which may even be misleading for video recognition tasks based on our observations. To further encourage HaltingVT to focus on the key motion-related information in videos, we design an effective Motion Loss during training. HaltingVT acquires video analysis capabilities and token halting compression strategies simultaneously in a unified training process, without requiring additional training procedures or sub-networks. On the Mini-Kinetics dataset, we achieved 75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely low 9.9 GFLOPs. The code is available at https://github.com/dun-research/HaltingVT.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频中的动作识别由于其高计算成本而提出了挑战，特别是对于联合时空视频转换器（Joint VT）而言。尽管它们很有效，但此类架构中过多的代币极大地限制了它们的效率。在本文中，我们提出了 HaltingVT，一种高效的视频转换器，可自适应地删除冗余视频补丁标记，它主要由 Joint VT 和 Glimpser 模块组成。具体来说，HaltingVT 在每一层应用数据自适应令牌减少，从而显着降低总体计算成本。此外，Glimpser 模块可以快速删除浅层转换器层中的冗余标记，根据我们的观察，这甚至可能会误导视频识别任务。为了进一步鼓励 HaltingVT 关注视频中与运动相关的关键信息，我们在训练期间设计了有效的运动损失。 HaltingVT 在统一的训练过程中同时获得视频分析能力和令牌停止压缩策略，无需额外的训练过程或子网络。在 Mini-Kinetics 数据集上，我们以 24.2 GFLOP 实现了 75.0% 的 top-1 ACC，以及以极低的 9.9 GFLOP 实现了 67.2% 的 top-1 ACC。该代码可在 https://github.com/dun-research/HaltingVT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.04975v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **EmMixformer: Mix transformer for eye movement recognition**<br />
**Title_cn:** EmMixformer：用于眼动识别的混合变压器<br />
**Authors:** Huafeng Qin, Hongyu Zhu, Xin Jin, Qun Song, Mounim A. El-Yacoubi, Xinbo Gao<br />
**Abstract:** <details><summary>原文: </summary>Eye movement (EM) is a new highly secure biometric behavioral modality that has received increasing attention in recent years. Although deep neural networks, such as convolutional neural network (CNN), have recently achieved promising performance, current solutions fail to capture local and global temporal dependencies within eye movement data. To overcome this problem, we propose in this paper a mixed transformer termed EmMixformer to extract time and frequency domain information for eye movement recognition. To this end, we propose a mixed block consisting of three modules, transformer, attention Long short-term memory (attention LSTM), and Fourier transformer. We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement. Second, we incorporate the attention mechanism into LSTM to propose attention LSTM with the aim to learn short temporal dependencies. Third, we perform self attention in the frequency domain to learn global features. As the three modules provide complementary feature representations in terms of local and global dependencies, the proposed EmMixformer is capable of improving recognition accuracy. The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error.</details>
**Abstract_cn:** <details><summary>译文: </summary>眼动（EM）是一种新型的高度安全的生物识别行为方式，近年来受到越来越多的关注。尽管卷积神经网络 (CNN) 等深度神经网络最近取得了可喜的性能，但当前的解决方案无法捕获眼动数据中的局部和全局时间依赖性。为了克服这个问题，我们在本文中提出了一种称为 EmMixformer 的混合变压器来提取时域和频域信息以进行眼动识别。为此，我们提出了一个由三个模块组成的混合块，变压器、注意力长短期记忆（注意力LSTM）和傅里叶变压器。我们是第一个尝试利用 Transformer 来学习眼球运动中的长期时间依赖性的人。其次，我们将注意力机制融入到 LSTM 中，提出注意力 LSTM，旨在学习短时间依赖性。第三，我们在频域中进行自注意力以学习全局特征。由于这三个模块在局部和全局依赖性方面提供了互补的特征表示，因此所提出的 EmMixformer 能够提高识别精度。我们的眼动数据集和两个公共眼动数据集的实验结果表明，所提出的 EmMixformer 通过实现最低的验证误差而优于现有技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.04956v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes**<br />
**Title_cn:** InseRF：神经 3D 场景中文本驱动的生成对象插入<br />
**Authors:** Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari<br />
**Abstract:** <details><summary>原文: </summary>We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 InseRF，这是一种在 3D 场景的 NeRF 重建中生成对象插入的新方法。基于用户提供的文本描述和参考视点中的 2D 边界框，InseRF 在 3D 场景中生成新对象。最近，由于在 3D 生成建模中使用了文本到图像扩散模型的强先验，3D 场景编辑方法已经发生了深刻的转变。现有方法在通过样式和外观更改或删除现有对象来编辑 3D 场景时最有效。然而，生成新对象仍然是此类方法的一个挑战，我们在本研究中解决了这个问题。具体来说，我们建议将 3D 对象插入基础为场景参考视图中的 2D 对象插入。然后使用单视图对象重建方法将 2D 编辑提升为 3D。然后，在单目深度估计方法的先验指导下，将重建的对象插入场景中。我们在各种 3D 场景上评估我们的方法，并对所提出的组件进行深入分析。我们在多个 3D 场景中生成对象插入的实验表明，与现有方法相比，我们的方法是有效的。 InseRF 能够进行可控且 3D 一致的对象插入，而不需要明确的 3D 信息作为输入。请访问我们的项目页面：https://mohamad-shahbazi.github.io/inserf。</details>
**PDF:** <http://arxiv.org/pdf/2401.05335v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models**<br />
**Title_cn:** PIXART-δ：具有潜在一致性模型的快速且可控的图像生成<br />
**Authors:** Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, Zhenguo Li<br />
**Abstract:** <details><summary>原文: </summary>This technical report introduces PIXART-{\delta}, a text-to-image synthesis framework that integrates the Latent Consistency Model (LCM) and ControlNet into the advanced PIXART-{\alpha} model. PIXART-{\alpha} is recognized for its ability to generate high-quality images of 1024px resolution through a remarkably efficient training process. The integration of LCM in PIXART-{\delta} significantly accelerates the inference speed, enabling the production of high-quality images in just 2-4 steps. Notably, PIXART-{\delta} achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images, marking a 7x improvement over the PIXART-{\alpha}. Additionally, PIXART-{\delta} is designed to be efficiently trainable on 32GB V100 GPUs within a single day. With its 8-bit inference capability (von Platen et al., 2023), PIXART-{\delta} can synthesize 1024px images within 8GB GPU memory constraints, greatly enhancing its usability and accessibility. Furthermore, incorporating a ControlNet-like module enables fine-grained control over text-to-image diffusion models. We introduce a novel ControlNet-Transformer architecture, specifically tailored for Transformers, achieving explicit controllability alongside high-quality image generation. As a state-of-the-art, open-source image generation model, PIXART-{\delta} offers a promising alternative to the Stable Diffusion family of models, contributing significantly to text-to-image synthesis.</details>
**Abstract_cn:** <details><summary>译文: </summary>本技术报告介绍了 PIXART-{\delta}，这是一种文本到图像合成框架，它将潜在一致性模型 (LCM) 和 ControlNet 集成到先进的 PIXART-{\alpha} 模型中。 PIXART-{\alpha} 因其通过非常高效的训练过程生成 1024px 分辨率的高质量图像的能力而受到认可。 PIXART-{\delta}中 LCM 的集成显着加快了推理速度，只需 2-4 个步骤即可生成高质量图像。值得注意的是，PIXART-{\delta} 在生成 1024x1024 像素图像方面突破了 0.5 秒，比 PIXART-{\alpha} 提高了 7 倍。此外，PIXART-{\delta} 设计为可在一天内在 32GB V100 GPU 上进行高效训练。凭借其 8 位推理能力（von Platen 等人，2023），PIXART-{\delta} 可以在 8GB GPU 内存限制内合成 1024px 图像，大大增强了其可用性和可访问性。此外，结合类似 ControlNet 的模块可以对文本到图像扩散模型进行细粒度控制。我们引入了一种新颖的 ControlNet-Transformer 架构，专为 Transformer 量身定制，可在生成高质量图像的同时实现明确的可控性。作为最先进的开源图像生成模型，PIXART-{\delta} 为稳定扩散模型系列提供了一种有前途的替代方案，为文本到图像的合成做出了重大贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.05252v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Application of Deep Learning in Blind Motion Deblurring: Current Status and Future Prospects**<br />
**Title_cn:** 深度学习在盲运动去模糊中的应用：现状与未来展望<br />
**Authors:** Yawen Xiang, Heng Zhou, Chengyang Li, Fangwei Sun, Zhongbo Li, Yongqiang Xie<br />
**Abstract:** <details><summary>原文: </summary>Motion deblurring is one of the fundamental problems of computer vision and has received continuous attention. The variability in blur, both within and across images, imposes limitations on non-blind deblurring techniques that rely on estimating the blur kernel. As a response, blind motion deblurring has emerged, aiming to restore clear and detailed images without prior knowledge of the blur type, fueled by the advancements in deep learning methodologies. Despite strides in this field, a comprehensive synthesis of recent progress in deep learning-based blind motion deblurring is notably absent. This paper fills that gap by providing an exhaustive overview of the role of deep learning in blind motion deblurring, encompassing datasets, evaluation metrics, and methods developed over the last six years. Specifically, we first introduce the types of motion blur and the fundamental principles of deblurring. Next, we outline the shortcomings of traditional non-blind deblurring algorithms, emphasizing the advantages of employing deep learning techniques for deblurring tasks. Following this, we categorize and summarize existing blind motion deblurring methods based on different backbone networks, including convolutional neural networks, generative adversarial networks, recurrent neural networks, and Transformer networks. Subsequently, we elaborate not only on the fundamental principles of these different categories but also provide a comprehensive summary and comparison of their advantages and limitations. Qualitative and quantitative experimental results conducted on four widely used datasets further compare the performance of SOTA methods. Finally, an analysis of present challenges and future pathways. All collected models, benchmark datasets, source code links, and codes for evaluation have been made publicly available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey</details>
**Abstract_cn:** <details><summary>译文: </summary>运动去模糊是计算机视觉的基本问题之一，受到持续关注。图像内部和图像之间的模糊变化对依赖于估计模糊内核的非盲去模糊技术施加了限制。作为回应，盲运动去模糊应运而生，旨在在深度学习方法进步的推动下，在不事先了解模糊类型的情况下恢复清晰详细的图像。尽管该领域取得了长足的进步，但对基于深度学习的盲运动去模糊的最新进展的全面综合仍然明显缺乏。本文通过详尽概述深度学习在盲运动去模糊中的作用（包括过去六年开发的数据集、评估指标和方法）来填补这一空白。具体来说，我们首先介绍运动模糊的类型和去模糊的基本原理。接下来，我们概述了传统非盲去模糊算法的缺点，强调了采用深度学习技术进行去模糊任务的优势。接下来，我们根据不同的骨干网络对现有的盲运动去模糊方法进行分类和总结，包括卷积神经网络、生成对抗网络、循环神经网络和 Transformer 网络。随后，我们不仅阐述了这些不同类别的基本原理，还对它们的优点和局限性进行了全面的总结和比较。在四个广泛使用的数据集上进行的定性和定量实验结果进一步比较了 SOTA 方法的性能。最后，分析当前的挑战和未来的路径。所有收集的模型、基准数据集、源代码链接和评估代码均已在 https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey 上公开</details>
**PDF:** <http://arxiv.org/pdf/2401.05055v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **SnapCap: Efficient Snapshot Compressive Video Captioning**<br />
**Title_cn:** SnapCap：高效的快照压缩视频字幕<br />
**Authors:** Jianqiao Sun, Yudi Su, Hao Zhang, Ziheng Cheng, Zequn Zeng, Zhengjue Wang, Bo Chen, Xin Yuan<br />
**Abstract:** <details><summary>原文: </summary>Video Captioning (VC) is a challenging multi-modal task since it requires describing the scene in language by understanding various and complex videos. For machines, the traditional VC follows the "imaging-compression-decoding-and-then-captioning" pipeline, where compression is pivot for storage and transmission. However, in such a pipeline, some potential shortcomings are inevitable, i.e., information redundancy resulting in low efficiency and information loss during the sampling process for captioning. To address these problems, in this paper, we propose a novel VC pipeline to generate captions directly from the compressed measurement, which can be captured by a snapshot compressive sensing camera and we dub our model SnapCap. To be more specific, benefiting from the signal simulation, we have access to obtain abundant measurement-video-annotation data pairs for our model. Besides, to better extract language-related visual representations from the compressed measurement, we propose to distill the knowledge from videos via a pre-trained CLIP with plentiful language-vision associations to guide the learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we conduct experiments on two widely-used VC datasets. Both the qualitative and quantitative results verify the superiority of our pipeline over conventional VC pipelines. In particular, compared to the "caption-after-reconstruction" methods, our SnapCap can run at least 3$\times$ faster, and achieve better caption results.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频字幕（VC）是一项具有挑战性的多模态任务，因为它需要通过理解各种复杂的视频来用语言描述场景。对于机器来说，传统的视频编码遵循“成像-压缩-解码-字​​幕”的流程，其中压缩是存储和传输的关键。然而，在这样的管道中，一些潜在的缺点是不可避免的，即信息冗余导致字幕采样过程中的效率低下和信息丢失。为了解决这些问题，在本文中，我们提出了一种新颖的 VC 管道，直接从压缩测量生成字幕，该字幕可以由快照压缩传感相机捕获，我们将我们的模型称为 SnapCap。更具体地说，受益于信号模拟，我们可以为我们的模型获得丰富的测量-视频-注释数据对。此外，为了更好地从压缩测量中提取与语言相关的视觉表示，我们建议通过预先训练的 CLIP 从视频中提取知识，并具有丰富的语言视觉关联，以指导 SnapCap 的学习。为了证明 SnapCap 的有效性，我们在两个广泛使用的 VC 数据集上进行了实验。定性和定量结果都验证了我们的管道相对于传统 VC 管道的优越性。特别是，与“重建后的字幕”方法相比，我们的 SnapCap 的运行速度至少快 3$\times$，并获得更好的字幕效果。</details>
**PDF:** <http://arxiv.org/pdf/2401.04903v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction**<br />
**Title_cn:** AdvMT：用于长期人体运动预测的对抗性运动变压器<br />
**Authors:** Sarmad Idrees, Jongeun Choi, Seokman Sohn<br />
**Abstract:** <details><summary>原文: </summary>To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential. Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses. Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches. Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge. In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator. This combination effectively captures spatial and temporal dependencies simultaneously within frames. With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions. The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions</details>
**Abstract_cn:** <details><summary>译文: </summary>为了在共享环境中实现机器人和人类之间的无缝协作，准确预测未来的人类运动至关重要。传统上，人体运动预测被视为序列预测问题，利用历史人体运动数据来估计未来的姿势。从普通的循环网络开始，研究界研究了各种学习人体运动动力学的方法，包括基于图的方法和生成方法。尽管做出了这些努力，实现准确的长期预测仍然是一项重大挑战。在这方面，我们提出了对抗运动变换器（AdvMT），这是一种集成了基于变换器的运动编码器和时间连续性鉴别器的新颖模型。这种组合有效地在帧内同时捕获空间和时间依赖性。通过对抗性训练，我们的方法有效地减少了预测中不需要的伪影，从而确保学习更真实、更流畅的人体动作。评估结果表明，AdvMT 极大地提高了长期预测的准确性，同时也提供了稳健的短期预测</details>
**PDF:** <http://arxiv.org/pdf/2401.05018v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MGNet: Learning Correspondences via Multiple Graphs**<br />
**Title_cn:** MGNet：通过多个图学习对应关系<br />
**Authors:** Luanyuan Dai, Xiaoyu Du, Hanwang Zhang, Jinhui Tang<br />
**Abstract:** <details><summary>原文: </summary>Learning correspondences aims to find correct correspondences (inliers) from the initial correspondence set with an uneven correspondence distribution and a low inlier rate, which can be regarded as graph data. Recent advances usually use graph neural networks (GNNs) to build a single type of graph or simply stack local graphs into the global one to complete the task. But they ignore the complementary relationship between different types of graphs, which can effectively capture potential relationships among sparse correspondences. To address this problem, we propose MGNet to effectively combine multiple complementary graphs. To obtain information integrating implicit and explicit local graphs, we construct local graphs from implicit and explicit aspects and combine them effectively, which is used to build a global graph. Moreover, we propose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse correspondence information at once in the global graph, which can capture and amplify discriminative features. Extensive experiments demonstrate that MGNet outperforms state-of-the-art methods in different visual tasks. The code is provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习对应关系的目的是从对应分布不均匀且内点率较低的初始对应集合中找到正确的对应关系（内点），可以将其视为图数据。最近的进展通常使用图神经网络（GNN）来构建单一类型的图，或者简单地将局部图堆叠到全局图中来完成任务。但他们忽略了不同类型图之间的互补关系，而这种关系可以有效捕获稀疏对应关系之间的潜在关系。为了解决这个问题，我们提出 MGNet 来有效地组合多个互补图。为了获得集成隐式和显式局部图的信息，我们从隐式和显式方面构建局部图，并将它们有效地结合起来，用于构建全局图。此外，我们提出了 Graph~Soft~Degree~Attention (GSDA)，以充分利用全局图中的所有稀疏对应信息，可以捕获和放大判别性特征。大量实验表明 MGNet 在不同的视觉任务中优于最先进的方法。代码在 https://github.com/DAILUANYUAN/MGNet-2024AAAI 中提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.04984v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton**<br />
**Title_cn:** 基于扩散的姿势细化和多假设生成，用于 3D 人体姿势估计<br />
**Authors:** Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Xinlin Yuan, Wenming Yang<br />
**Abstract:** <details><summary>原文: </summary>Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to enhance pose accuracy by generating multiple hypotheses. However, most of the hypotheses generated deviate substantially from the true pose. Compared to deterministic models, the excessive uncertainty in probabilistic models leads to weaker performance in single-hypothesis prediction. To address these two challenges, we propose a diffusion-based refinement framework called DRPose, which refines the output of deterministic models by reverse diffusion and achieves more suitable multi-hypothesis prediction for the current pose benchmark by multi-step refinement with multiple noises. To this end, we propose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement Module (PRM) for denoising and refining. Extensive experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art performance on both single and multi-hypothesis 3DHPE. Code is available at https://github.com/KHB1698/DRPose.</details>
**Abstract_cn:** <details><summary>译文: </summary>先前的 3D 人体姿势估计 (3DHPE) 概率模型旨在通过生成多个假设来提高姿势准确性。然而，大多数生成的假设与真实姿势有很大偏差。与确定性模型相比，概率模型过多的不确定性导致单假设预测的性能较差。为了解决这两个挑战，我们提出了一种名为 DRPose 的基于扩散的细化框架，该框架通过反向扩散细化确定性模型的输出，并通过使用多个噪声的多步细化实现对当前姿态基准更合适的多假设预测。为此，我们提出了可扩展图卷积变换器（SGCT）和用于去噪和细化的姿势细化模块（PRM）。在 Human3.6M 和 MPI-INF-3DHP 数据集上进行的大量实验表明，我们的方法在单假设和多假设 3DHPE 上均实现了最先进的性能。代码可在 https://github.com/KHB1698/DRPost 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.04921v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction**<br />
**Title_cn:** 用于行人轨迹预测的知识感知图转换器<br />
**Authors:** Yu Liu, Yuexin Zhang, Kunming Li, Yongliang Qiao, Stewart Worrall, You-Fu Li, He Kong<br />
**Abstract:** <details><summary>原文: </summary>Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for training and performance evaluation purposes. The proposed framework is validated and compared against existing methods using popular public datasets, i.e., ETH and UCY. Experimental results demonstrate the improved performance of our proposed scheme.</details>
**Abstract_cn:** <details><summary>译文: </summary>预测行人运动轨迹对于自动驾驶车辆的路径规划和运动控制至关重要。由于不同环境中人体运动的不确定性，准确预测人群轨迹具有挑战性。对于训练，最近基于深度学习的预测方法主要利用轨迹历史和行人之间的交互等信息。由于训练数据集之间的差异尚未正确纳入，这可能会限制各种场景的预测性能。为了克服这一限制，本文提出了一种图转换器结构来提高预测性能，捕获数据集中包含的各个站点和场景之间的差异。特别是，设计了自注意力机制和领域适应模块来提高模型的泛化能力。此外，出于训练和性能评估目的，引入了考虑跨数据集序列的附加度量。使用流行的公共数据集（即 ETH 和 UCY）对所提出的框架进行了验证并与现有方法进行了比较。实验结果证明了我们提出的方案的性能得到了提高。</details>
**PDF:** <http://arxiv.org/pdf/2401.04872v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from Monocular Video**<br />
**Title_cn:** CTNeRF：单目视频动态神经辐射场的跨时间变换器<br />
**Authors:** Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Yang Long, Yefeng Zheng<br />
**Abstract:** <details><summary>原文: </summary>The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们工作的目标是从复杂和动态场景的单眼视频中生成高质量的新颖视图。先前的方法，例如 DynamicNeRF，通过利用时变动态辐射场显示出令人印象深刻的性能。然而，这些方法在精确建模复杂物体的运动时存在局限性，这可能导致细节渲染不准确和模糊。为了解决这个限制，我们提出了一种基于最近的泛化 NeRF 的新方法，它将附近的视图聚合到新的观点上。然而，此类方法通常仅对静态场景有效。为了克服这一挑战，我们引入了一个在时域和频域中运行的模块来聚合对象运动的特征。这使我们能够学习帧之间的关系并生成更高质量的图像。我们的实验证明了动态场景数据集上最先进方法的显着改进。具体来说，我们的方法在合成视图的准确性和视觉质量方面都优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.04861v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects**<br />
**Title_cn:** 重复的结构：一堆对象的神经逆向图形<br />
**Authors:** Tianhang Cheng, Wei-Chiu Ma, Kaiyu Guan, Antonio Torralba, Shenlong Wang<br />
**Abstract:** <details><summary>原文: </summary>Our world is full of identical objects (\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们的世界充满了相同的物体（例如，可乐罐、相同型号的汽车）。当将这些重复项放在一起查看时，它们为我们有效地推理 3D 提供了额外且强有力的线索。受这一观察的启发，我们引入了重复结构（SfD），这是一种新颖的逆向图形框架，可以从包含多个相同对象的单个图像中重建几何形状、材质和照明。 SfD 首先识别图像中对象的多个实例，然后联合估计所有实例的 6DoF 姿态。随后采用逆图形管道来联合推理对象的形状、材质和环境光，同时遵循跨实例共享几何和材料约束。我们的主要贡献包括利用对象重复作为单图像逆向图形的稳健先验，并提出用于联合 6-DoF 对象姿态估计的面内旋转稳健运动结构 (SfM) 公式。通过利用单个图像的多视图线索，SfD 生成更真实、更详细的 3D 重建，显着优于现有的单图像重建模型和具有相似或更多数量观察的多视图重建方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05236v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Measuring Natural Scenes SFR of Automotive Fisheye Cameras**<br />
**Title_cn:** 测量汽车鱼眼相机的自然场景 SFR<br />
**Authors:** Daniel Jakab, Eoin Martino Grua, Brian Micheal Deegan, Anthony Scanlan, Pepijn Van De Ven, Ciarán Eising<br />
**Abstract:** <details><summary>原文: </summary>The Modulation Transfer Function (MTF) is an important image quality metric typically used in the automotive domain. However, despite the fact that optical quality has an impact on the performance of computer vision in vehicle automation, for many public datasets, this metric is unknown. Additionally, wide field-of-view (FOV) cameras have become increasingly popular, particularly for low-speed vehicle automation applications. To investigate image quality in datasets, this paper proposes an adaptation of the Natural Scenes Spatial Frequency Response (NS-SFR) algorithm to suit cameras with a wide field-of-view.</details>
**Abstract_cn:** <details><summary>译文: </summary>调制传递函数 (MTF) 是汽车领域通常使用的重要图像质量指标。然而，尽管光学质量对车辆自动化中计算机视觉的性能有影响，但对于许多公共数据集来说，这个指标是未知的。此外，宽视场 (FOV) 相机变得越来越流行，特别是对于低速车辆自动化应用。为了研究数据集中的图像质量，本文提出了一种自然场景空间频率响应（NS-SFR）算法的改进方案，以适应具有宽视场的相机。</details>
**PDF:** <http://arxiv.org/pdf/2401.05232v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method**<br />
**Title_cn:** 探索无参考图像质量评估模型的漏洞：基于查询的黑盒方法<br />
**Authors:** Chenxi Yang, Yujia Liu, Dingquan Li, Tingting jiang<br />
**Abstract:** <details><summary>原文: </summary>No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality scores consistent with human perception without relying on pristine reference images, serving as a crucial component in various visual tasks. Ensuring the robustness of NR-IQA methods is vital for reliable comparisons of different image processing techniques and consistent user experiences in recommendations. The attack methods for NR-IQA provide a powerful instrument to test the robustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on the gradient of the NR-IQA model, leading to limitations when the gradient information is unavailable. In this paper, we present a pioneering query-based black box attack against NR-IQA methods. We propose the concept of \emph{score boundary} and leverage an adaptive iterative approach with multiple score boundaries. Meanwhile, the initial attack directions are also designed to leverage the characteristics of the Human Visual System (HVS). Experiments show our attack method outperforms all compared state-of-the-art methods and is far ahead of previous black-box methods. The effective DBCNN model suffers a Spearman rank-order correlation coefficient (SROCC) decline of $0.6972$ attacked by our method, revealing the vulnerability of NR-IQA to black-box attacks. The proposed attack method also provides a potent tool for further exploration into NR-IQA robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>无参考图像质量评估（NR-IQA）旨在预测与人类感知一致的图像质量分数，而不依赖原始参考图像，作为各种视觉任务的关键组成部分。确保 NR-IQA 方法的稳健性对于不同图像处理技术的可靠比较和推荐中一致的用户体验至关重要。 NR-IQA的攻击方法为测试NR-IQA的鲁棒性提供了强大的工具。然而，目前的NR-IQA攻击方法严重依赖NR-IQA模型的梯度，导致在梯度信息不可用时受到限制。在本文中，我们提出了一种针对 NR-IQA 方法的开创性的基于查询的黑盒攻击。我们提出了\emph{分数边界}的概念，并利用具有多个分数边界的自适应迭代方法。同时，最初的攻击方向也被设计为利用人类视觉系统（HVS）的特性。实验表明，我们的攻击方法优于所有比较的最先进方法，并且远远领先于以前的黑盒方法。有效的 DBCNN 模型受到我们的方法的攻击，Spearman 排序相关系数 (SROCC) 下降了 0.6972 美元，这揭示了 NR-IQA 容易受到黑盒攻击。所提出的攻击方法还为进一步探索 NR-IQA 鲁棒性提供了有效的工具。</details>
**PDF:** <http://arxiv.org/pdf/2401.05217v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Content-Aware Depth-Adaptive Image Restoration**<br />
**Title_cn:** 内容感知深度自适应图像恢复<br />
**Authors:** Tom Richard Vargis, Siavash Ghiasvand<br />
**Abstract:** <details><summary>原文: </summary>This work prioritizes building a modular pipeline that utilizes existing models to systematically restore images, rather than creating new restoration models from scratch. Restoration is carried out at an object-specific level, with each object regenerated using its corresponding class label information. The approach stands out by providing complete user control over the entire restoration process. Users can select models for specialized restoration steps, customize the sequence of steps to meet their needs, and refine the resulting regenerated image with depth awareness. The research provides two distinct pathways for implementing image regeneration, allowing for a comparison of their respective strengths and limitations. The most compelling aspect of this versatile system is its adaptability. This adaptability enables users to target particular object categories, including medical images, by providing models that are trained on those object classes.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作优先考虑构建一个模块化管道，利用现有模型系统地恢复图像，而不是从头开始创建新的恢复模型。恢复是在特定于对象的级别进行的，每个对象使用其相应的类标签信息重新生成。该方法的突出之处在于为用户提供了对整个恢复过程的完全控制。用户可以选择专门的恢复步骤的模型，自定义步骤顺序以满足他们的需求，并通过深度感知来细化生成的再生图像。该研究提供了两种不同的实现图像再生的途径，可以比较它们各自的优点和局限性。这个多功能系统最引人注目的方面是它的适应性。这种适应性使用户能够通过提供针对特定对象类别（包括医学图像）进行训练的模型来定位这些对象类别。</details>
**PDF:** <http://arxiv.org/pdf/2401.05049v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data**<br />
**Title_cn:** 通过释放任务无关数据的潜力实现无源跨模式知识转移<br />
**Authors:** Jinjing Zhu, Yucheng Chen, Lin Wang<br />
**Abstract:** <details><summary>原文: </summary>Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns. A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap. However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality. To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer. Our work is buttressed by two key technical components. Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module. It translates the target modality data (e.g., infrared) into the source-like RGB images based on paired TI data and the guidance of the available source model to alleviate two key gaps: 1) inter-modality gap between the paired TI data; 2) intra-modality gap between TI and TR target data. We then propose a Task-irrelevant data-Guided Knowledge Transfer (TGKT) module that transfers knowledge from the source model to the target model by leveraging the paired TI data. Notably, due to the unavailability of labels for the TR target data and its less reliable prediction from the source model, our TGKT model incorporates a self-supervised pseudo-labeling approach to enable the target model to learn from its predictions. Extensive experiments show that our method achieves state-of-the-art performance on three datasets (RGB-to-depth and RGB-to-infrared).</details>
**Abstract_cn:** <details><summary>译文: </summary>无源跨模态知识转移是一项至关重要但具有挑战性的任务，其目的是将知识从一种源模态（例如 RGB）转移到目标模态（例如深度或红外），而无需访问任务相关的（TR ）出于内存和隐私问题而源数据。最近的一项尝试利用配对的任务无关（TI）数据并直接匹配其中的特征以消除模态差距。然而，它忽略了一个关键线索，即配对的 TI 数据可用于有效估计源数据分布并更好地促进知识向目标模态的迁移。为此，我们提出了一个新颖而简洁的框架，以释放配对 TI 数据的潜力，以增强无源跨模式知识转移。我们的工作由两个关键技术组成部分支撑。首先，为了更好地估计源数据分布，我们引入了任务无关数据引导模态桥接（TGMB）模块。它基于配对 TI 数据和可用源模型的指导，将目标模态数据（例如红外）转换为类源 RGB 图像，以缩小两个关键差距：1）配对 TI 数据之间的模态间差距； 2）TI和TR目标数据之间的模态内差距。然后，我们提出了一个与任务无关的数据引导知识转移（TGKT）模块，该模块通过利用配对的 TI 数据将知识从源模型转移到目标模型。值得注意的是，由于 TR 目标数据的标签不可用，并且源模型的预测不太可靠，我们的 TGKT 模型采用了自我监督的伪标签方法，使目标模型能够从其预测中学习。大量实验表明，我们的方法在三个数据集（RGB 到深度和 RGB 到红外）上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.05014v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Large Model based Sequential Keyframe Extraction for Video Summarization**<br />
**Title_cn:** 基于大型模型的视频摘要序列关键帧提取<br />
**Authors:** Kailong Tan, Yuxiang Zhou, Qianchen Xia, Rui Liu, Yong Chen<br />
**Abstract:** <details><summary>原文: </summary>Keyframe extraction aims to sum up a video's semantics with the minimum number of its frames. This paper puts forward a Large Model based Sequential Keyframe Extraction for video summarization, dubbed LMSKE, which contains three stages as below. First, we use the large model "TransNetV21" to cut the video into consecutive shots, and employ the large model "CLIP2" to generate each frame's visual feature within each shot; Second, we develop an adaptive clustering algorithm to yield candidate keyframes for each shot, with each candidate keyframe locating nearest to a cluster center; Third, we further reduce the above candidate keyframes via redundancy elimination within each shot, and finally concatenate them in accordance with the sequence of shots as the final sequential keyframes. To evaluate LMSKE, we curate a benchmark dataset and conduct rich experiments, whose results exhibit that LMSKE performs much better than quite a few SOTA competitors with average F1 of 0.5311, average fidelity of 0.8141, and average compression ratio of 0.9922.</details>
**Abstract_cn:** <details><summary>译文: </summary>关键帧提取的目的是用最少的帧数总结视频的语义。本文提出了一种基于大型模型的视频摘要序列关键帧提取，称为LMSKE，它包含以下三个阶段。首先，我们使用大模型“TransNetV21”将视频切割成连续的镜头，并使用大模型“CLIP2”生成每个镜头内每一帧的视觉特征；其次，我们开发了一种自适应聚类算法，为每个镜头生成候选关键帧，每个候选关键帧距离聚类中心最近；第三，我们通过每个镜头内的冗余消除进一步减少上述候选关键帧，最后按照镜头顺序将它们连接起来作为最终的顺序关键帧。为了评估 LMSKE，我们整理了一个基准数据集并进行了丰富的实验，结果表明 LMSKE 的平均 F1 为 0.5311，平均保真度为 0.8141，平均压缩比为 0.9922，比相当多的 SOTA 竞争对手表现得更好。</details>
**PDF:** <http://arxiv.org/pdf/2401.04962v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Inconsistency-Based Data-Centric Active Open-Set Annotation**<br />
**Title_cn:** 基于不一致性的以数据为中心的主动开放集注释<br />
**Authors:** Ruiyu Mao, Ouyang Xu, Yunhui Guo<br />
**Abstract:** <details><summary>原文: </summary>Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks. However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes. This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem. The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce. To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data. NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data. It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative samples from those classes based on a consistency criterion that measures inconsistencies between model predictions and local feature distribution. Unlike the recently proposed learning-centric method for the same problem, NEAT is much more computationally efficient and is a data-centric active open-set annotation method. Our experiments demonstrate that NEAT achieves significantly better performance than state-of-the-art active learning methods for active open-set annotation.</details>
**Abstract_cn:** <details><summary>译文: </summary>主动学习是一种常用的方法，可以减少训练深度神经网络所需的标记工作。然而，当前主动学习方法的有效性受到其封闭世界假设的限制，该假设假设未标记池中的所有数据都来自一组预定义的已知类。这种假设在实际情况中通常是无效的，因为未标记的数据中可能存在未知的类，从而导致主动开放集注释问题。由于引入的不确定性，数据中未知类的存在可能会显着影响现有主动学习方法的性能。为了解决这个问题，我们提出了一种名为 NEAT 的新型以数据为中心的主动学习方法，它可以主动注释开放集数据。 NEAT 旨在标记已知和未知类未标记数据池中的已知类数据。它利用标签的可聚类性从未标记的池中识别已知类别，并根据衡量模型预测和局部特征分布之间不一致的一致性标准从这些类别中选择信息样本。与最近提出的针对同一问题的以学习为中心的方法不同，NEAT 的计算效率更高，并且是一种以数据为中心的主动开放集注释方法。我们的实验表明，对于主动开放集注释，NEAT 的性能明显优于最先进的主动学习方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.04923v1><br />
**Code:** null<br />

