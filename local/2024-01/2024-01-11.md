## [UPDATED!] **2024-01-11** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios**<br />
**Title_cn:** 使用混合真实和虚拟场景的基于无人机的自动机场路面检查<br />
**Authors:** Pablo Alonso, Jon Ander Iñiguez de Gordoa, Juan Diego Ortega, Sara García, Francisco Javier Iriarte, Marcos Nieto<br />
**Abstract:** <details><summary>原文: </summary>Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time. To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections. UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost. In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs. The proposed method is based on Deep Learning (DL) to segment defects in the image. The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation. To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets. We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>跑道和滑行道路面在其预计使用寿命期间会承受高压力，这不可避免地会导致其状况随着时间的推移而恶化。为了确保机场路面状况确保不间断和弹性运行，监测其状况并进行定期检查至关重要。基于无人机的检查由于其广泛的监控能力和降低的成本而最近变得越来越重要。在这项工作中，我们提出了一种基于视觉的方法，使用无人机捕获的图像自动识别路面破损。所提出的方法基于深度学习（DL）来分割图像中的缺陷。深度学习架构通过使用 EfficientNet 特征提取和特征金字塔网络分割的优化实现，利用了无人机嵌入式系统的低计算能力。为了解决训练注释数据的缺乏问题，我们开发了一种合成数据集生成方法来扩展可用的遇险数据集。我们证明，在实际应用场景中测试训练模型时，使用由合成图像和真实训练图像组成的混合数据集可以产生更好的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.06019v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Attention to detail: inter-resolution knowledge distillation**<br />
**Title_cn:** 关注细节：分辨率间知识蒸馏<br />
**Authors:** Rocío del Amor, Julio Silva-Rodríguez, Adrián Colomer, Valery Naranjo<br />
**Abstract:** <details><summary>原文: </summary>The development of computer vision solutions for gigapixel images in digital pathology is hampered by significant computational limitations due to the large size of whole slide images. In particular, digitizing biopsies at high resolutions is a time-consuming process, which is necessary due to the worsening results from the decrease in image detail. To alleviate this issue, recent literature has proposed using knowledge distillation to enhance the model performance at reduced image resolutions. In particular, soft labels and features extracted at the highest magnification level are distilled into a model that takes lower-magnification images as input. However, this approach fails to transfer knowledge about the most discriminative image regions in the classification process, which may be lost when the resolution is decreased. In this work, we propose to distill this information by incorporating attention maps during training. In particular, our formulation leverages saliency maps of the target class via grad-CAMs, which guides the lower-resolution Student model to match the Teacher distribution by minimizing the l2 distance between them. Comprehensive experiments on prostate histology image grading demonstrate that the proposed approach substantially improves the model performance across different image resolutions compared to previous literature.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于整个幻灯片图像尺寸较大，计算限制极大，阻碍了数字病理学中十亿像素图像的计算机视觉解决方案的开发。特别是，以高分辨率对活检进行数字化是一个耗时的过程，由于图像细节的减少会导致结果恶化，因此这是必要的。为了缓解这个问题，最近的文献提出使用知识蒸馏来增强图像分辨率降低时的模型性能。特别是，在最高放大倍率级别提取的软标签和特征被提炼成以较低放大倍率图像作为输入的模型。然而，这种方法无法传递有关分类过程中最具辨别力的图像区域的知识，当分辨率降低时，这些知识可能会丢失。在这项工作中，我们建议通过在训练期间合并注意力图来提取这些信息。特别是，我们的公式通过 grad-CAM 利用目标类的显着性图，它通过最小化它们之间的 l2 距离来指导较低分辨率的学生模型匹配教师分布。前列腺组织学图像分级的综合实验表明，与之前的文献相比，所提出的方法大大提高了不同图像分辨率下的模型性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.06010v1><br />
**Code:** <https://github.com/cvblab/kd_resolution>**<br />
>>**index:** 3<br />
**Title:** **Sea ice detection using concurrent multispectral and synthetic aperture radar imagery**<br />
**Title_cn:** 使用并发多光谱和合成孔径雷达图像进行海冰探测<br />
**Authors:** Martin S J Rogers, Maria Fox, Andrew Fleming, Louisa van Zeeland, Jeremy Wilkinson, J. Scott Hosking<br />
**Abstract:** <details><summary>原文: </summary>Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery. ViSual\_IceD outperforms the other networks, with a F1 score 1.60\% points higher than the next best network, and results indicate that ViSual\_IceD is selective in the image type it uses during image segmentation. Outputs from ViSual\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how ViSual\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions.</details>
**Abstract_cn:** <details><summary>译文: </summary>合成孔径雷达 (SAR) 图像是用于海冰测绘的主要数据类型，因为它具有时空覆盖范围，并且能够独立于云和照明条件检测海冰。由于图像中存在模糊信号和噪声，使用 SAR 图像进行自动海冰检测仍然存在问题。相反，使用多光谱图像 (MSI) 可以轻松区分冰和水，但在极地地区，海洋表面经常被云遮挡，或者太阳可能好几个月都不会出现在地平线上方。为了解决其中的一些限制，本文提出了一种使用并发多光谱可见光和 SAR 图像进行海冰检测训练的新工具 (ViSual\_IceD)。 ViSual\_IceD 是一种基于经典 U-Net 架构的卷积神经网络 (CNN)，包含两个并行编码器级，能够融合和串联包含不同空间分辨率的 MSI 和 SAR 图像。将 ViSual\_IceD 的性能与使用串联 MSI 和 SAR 图像训练的 U-Net 模型以及专门在 MSI 或 SAR 图像上训练的模型进行比较。 ViSual\_IceD 优于其他网络，F1 分数比次优网络高 1.60\% 个点，结果表明 ViSual\_IceD 在图像分割过程中使用的图像类型具有选择性。 ViSualIceD 的输出与 AMSR2 无源微波 (PMW) 传感器的海冰浓度产品进行比较。结果凸显了 ViSual\_IceD 是如何与 PMW 数据结合使用的有用工具，特别是在沿海地区。随着 MSI 和 SAR 图像的时空覆盖范围不断增加，ViSual\_IceD 为极地地区稳健、准确的海冰覆盖范围检测提供了新的机会。</details>
**PDF:** <http://arxiv.org/pdf/2401.06009v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Body-Area Capacitive or Electric Field Sensing for Human Activity Recognition and Human-Computer Interaction: A Comprehensive Survey**<br />
**Title_cn:** 用于人体活动识别和人机交互的身体区域电容或电场感应：综合调查<br />
**Authors:** Sizhen Bian, Mengxi Liu, Bo Zhou, Paul Lukowicz, Michele Magno<br />
**Abstract:** <details><summary>原文: </summary>Due to the fact that roughly sixty percent of the human body is essentially composed of water, the human body is inherently a conductive object, being able to, firstly, form an inherent electric field from the body to the surroundings and secondly, deform the distribution of an existing electric field near the body. Body-area capacitive sensing, also called body-area electric field sensing, is becoming a promising alternative for wearable devices to accomplish certain tasks in human activity recognition and human-computer interaction. Over the last decade, researchers have explored plentiful novel sensing systems backed by the body-area electric field. On the other hand, despite the pervasive exploration of the body-area electric field, a comprehensive survey does not exist for an enlightening guideline. Moreover, the various hardware implementations, applied algorithms, and targeted applications result in a challenging task to achieve a systematic overview of the subject. This paper aims to fill in the gap by comprehensively summarizing the existing works on body-area capacitive sensing so that researchers can have a better view of the current exploration status. To this end, we first sorted the explorations into three domains according to the involved body forms: body-part electric field, whole-body electric field, and body-to-body electric field, and enumerated the state-of-art works in the domains with a detailed survey of the backed sensing tricks and targeted applications. We then summarized the three types of sensing frontends in circuit design, which is the most critical part in body-area capacitive sensing, and analyzed the data processing pipeline categorized into three kinds of approaches. Finally, we described the challenges and outlooks of body-area electric sensing.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于人体大约百分之六十的成分基本上是水，人体本质上是一个导电物体，首先能够从身体到周围环境形成固有的电场，其次使分布变形身体附近现有的电场。身体区域电容传感，也称为身体区域电场传感，正在成为可穿戴设备完成人类活动识别和人机交互中某些任务的有前景的替代方案。在过去的十年中，研究人员探索了大量由身体区域电场支持的新型传感系统。另一方面，尽管对身体区域电场的探索普遍存在，但缺乏具有启发性的指导方针的全面调查。此外，各种硬件实现、应用算法和目标应用导致实现该主题的系统概述成为一项具有挑战性的任务。本文旨在通过全面总结人体区域电容传感的现有工作来填补空白，以便研究人员更好地了解当前的探索现状。为此，我们首先根据所涉及的身体形式将探索分为三个领域：身体部位电场、全身电场和身体对身体电场，并列举了这些领域的最新成果。对支持的传感技巧和目标应用进行详细调查。然后，我们总结了电路设计中的三种类型的传感前端，这是人体区域电容传感中最关键的部分，并分析了分为三种方法的数据处理流程。最后，我们描述了身体区域电传感的挑战和前景。</details>
**PDF:** <http://arxiv.org/pdf/2401.06000v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians**<br />
**Title_cn:** CoSSegGaussians：紧凑且快速的场景分割 3D 高斯<br />
**Authors:** Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan<br />
**Abstract:** <details><summary>原文: </summary>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method firstly optimizes Gaussian points' position, convariance and color attributes under the supervision of RGB images. After Gaussian Locating, we distill multi-scale DINO features extracted from images through unprojection to each Gaussian, which is then incorporated with spatial features from the fast point features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is applied to the multi-scale fused features to obtain compact segmentation. Experimental results show that our model can perform high-quality zero-shot scene segmentation, as our model outperforms other segmentation methods on both semantic and panoptic segmentation task, meanwhile consumes approximately only 10% segmenting time compared to NeRF-based segmentation. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了紧凑和快速分割 3D 高斯（CoSSegGaussians），这是一种仅使用 RGB 图像输入以快速渲染速度进行紧凑 3D 一致场景分割的方法。以前基于 NeRF 的 3D 分割方法依赖于隐式或体素神经场景表示和光线行进体积渲染，这些方法非常耗时。最近的3D Gaussian Splatting显着提高了渲染速度，然而，现有的基于高斯的分割方法（例如：高斯分组）无法提供紧凑的分割掩模，特别是在零样本分割中，这主要是由于直接缺乏鲁棒性和紧凑性造成的当遇到不一致的 2D 机器生成标签时，为每个高斯分配可学习的参数。我们的方法旨在通过浅层解码网络为每个高斯点映射融合的空间和语义上有意义的特征，从而快速实现紧凑且可靠的零镜头场景分割。具体来说，我们的方法首先在 RGB 图像的监督下优化高斯点的位置、协方差和颜色属性。在高斯定位之后，我们将通过非投影从图像中提取的多尺度 DINO 特征提取到每个高斯，然后将其与来自快速点特征处理网络（即 RandLA-Net）的空间特征合并。然后将浅层解码MLP应用于多尺度融合特征以获得紧凑分割。实验结果表明，我们的模型可以执行高质量的零镜头场景分割，因为我们的模型在语义和全景分割任务上都优于其他分割方法，同时与基于 NeRF 的分割相比，仅消耗大约 10% 的分割时间。代码和更多结果将在 https://David-Dou.github.io/CoSSegGaussians 提供</details>
**PDF:** <http://arxiv.org/pdf/2401.05925v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **PartSTAD: 2D-to-3D Part Segmentation Task Adaptation**<br />
**Title_cn:** PartSTAD：2D 到 3D 零件分割任务适配<br />
**Authors:** Hyunjin Kim, Minhyuk Sung<br />
**Abstract:** <details><summary>原文: </summary>We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p improvement in mAP_50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 PartSTAD，一种专为 2D 到 3D 分割提升的任务适配而设计的方法。最近的研究强调了利用 2D 分割模型通过少样本自适应实现高质量 3D 分割的优势。然而，以前的方法侧重于调整 2D 分割模型以将域转移到渲染图像和合成文本描述，而不是专门针对 3D 分割优化模型。我们提出的任务适应方法使用 3D 分割的目标函数微调 2D 边界框预测模型。我们引入了用于自适应合并的 2D 边界框的权重，并使用小型附加神经网络来学习权重。此外，我们还结合了 SAM（边界框上的前景分割模型），以改善 2D 分割的边界，从而改善 3D 分割的边界。我们在 PartNet-Mobility 数据集上的实验表明，我们的任务适应方法有了显着改进，与 SotA 少样本 3D 分割模型相比，语义和实例分割的 mIoU 提高了 7.0%p，mAP_50 提高了 5.2%p。</details>
**PDF:** <http://arxiv.org/pdf/2401.05906v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification**<br />
**Title_cn:** 电阻存储器中的噪声对图像分类深度神经网络的影响<br />
**Authors:** Yannick Emonds, Kai Xi, Holger Fröning<br />
**Abstract:** <details><summary>原文: </summary>Resistive memory is a promising alternative to SRAM, but is also an inherently unstable device that requires substantial effort to ensure correct read and write operations. To avoid the associated costs in terms of area, time and energy, the present work is concerned with exploring how much noise in memory operations can be tolerated by image classification tasks based on neural networks. We introduce a special noisy operator that mimics the noise in an exemplary resistive memory unit, explore the resilience of convolutional neural networks on the CIFAR-10 classification task, and discuss a couple of countermeasures to improve this resilience.</details>
**Abstract_cn:** <details><summary>译文: </summary>电阻式存储器是 SRAM 的一种很有前途的替代品，但它本质上也是一种不稳定的设备，需要付出大量努力才能确保正确的读写操作。为了避免在面积、时间和能源方面的相关成本，目前的工作致力于探索基于神经网络的图像分类任务可以容忍内存操作中的多少噪声。我们引入了一种特殊的噪声算子，它模仿示例性电阻存储单元中的噪声，探索卷积神经网络在 CIFAR-10 分类任务上的弹性，并讨论了一些提高这种弹性的对策。</details>
**PDF:** <http://arxiv.org/pdf/2401.05820v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image Classification**<br />
**Title_cn:** 向 Zoom 学习：WCE 图像分类的解耦监督对比学习<br />
**Authors:** Kunpeng Qiu, Zhiying Zhou, Yongxin Guo<br />
**Abstract:** <details><summary>原文: </summary>Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images is vital for early diagnosis and treatment of gastrointestinal (GI) cancers. However, this task is confronted with challenges like tiny lesions and background interference. Additionally, WCE images exhibit higher intra-class variance and inter-class similarities, adding complexity. To tackle these challenges, we propose Decoupled Supervised Contrastive Learning for WCE image classification, learning robust representations from zoomed-in WCE images generated by Saliency Augmentor. Specifically, We use uniformly down-sampled WCE images as anchors and WCE images from the same class, especially their zoomed-in images, as positives. This approach empowers the Feature Extractor to capture rich representations from various views of the same image, facilitated by Decoupled Supervised Contrastive Learning. Training a linear Classifier on these representations within 10 epochs yields an impressive 92.01% overall accuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend of two publicly accessible WCE datasets. Code is available at: https://github.com/Qiukunpeng/DSCL.</details>
**Abstract_cn:** <details><summary>译文: </summary>无线胶囊内窥镜 (WCE) 图像中准确的病变分类对于胃肠道 (GI) 癌症的早期诊断和治疗至关重要。然而，这项任务面临着微小病变和背景干扰等挑战。此外，WCE 图像表现出更高的类内方差和类间相似性，从而增加了复杂性。为了应对这些挑战，我们提出了用于 WCE 图像分类的解耦监督对比学习，从显着性增强器生成的放大 WCE 图像中学习鲁棒的表示。具体来说，我们使用统一下采样的 WCE 图像作为锚点，并使用来自同一类的 WCE 图像，特别是它们的放大图像作为正值。这种方法使特征提取器能够在解耦监督对比学习的促进下从同一图像的不同视图中捕获丰富的表示。在 10 个 epoch 内对这些表示进行训练线性分类器，总体准确率高达 92.01%，令人印象深刻，在两个可公开访问的 WCE 数据集的混合上，比之前最先进的技术 (SOTA) 提高了 0.72%。代码位于：https://github.com/Qiukunpeng/DSCL。</details>
**PDF:** <http://arxiv.org/pdf/2401.05771v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Evaluating Data Augmentation Techniques for Coffee Leaf Disease Classification**<br />
**Title_cn:** 评估咖啡叶病分类的数据增强技术<br />
**Authors:** Adrian Gheorghiu, Iulian-Marius Tăiatu, Dumitru-Clementin Cercel, Iuliana Marin, Florin Pop<br />
**Abstract:** <details><summary>原文: </summary>The detection and classification of diseases in Robusta coffee leaves are essential to ensure that plants are healthy and the crop yield is kept high. However, this job requires extensive botanical knowledge and much wasted time. Therefore, this task and others similar to it have been extensively researched subjects in image classification. Regarding leaf disease classification, most approaches have used the more popular PlantVillage dataset while completely disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of pre-trained models and multiple augmentation techniques need to be used. The current paper uses the RoCoLe dataset and approaches based on deep learning for classifying coffee leaf diseases from images, incorporating the pix2pix model for segmentation and cycle-generative adversarial network (CycleGAN) for augmentation. Our study demonstrates the effectiveness of Transformer-based models, online augmentations, and CycleGAN augmentation in improving leaf disease classification. While synthetic data has limitations, it complements real data, enhancing model performance. These findings contribute to developing robust techniques for plant disease detection and classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>罗布斯塔咖啡叶病害的检测和分类对于确保植物健康和保持作物高产至关重要。然而，这项工作需要广泛的植物知识并且浪费大量时间。因此，该任务和其他类似任务已成为图像分类领域广泛研究的课题。关于叶病分类，大多数方法都使用更流行的 PlantVillage 数据集，而完全忽略其他数据集，例如 Robusta Coffee Leaf (RoCoLe) 数据集。由于RoCoLe数据集不平衡且样本不多，需要使用预训练模型的微调和多种增强技术。当前的论文使用 RoCoLe 数据集和基于深度学习的方法对图像中的咖啡叶病进行分类，并结合用于分割的 pix2pix 模型和用于增强的循环生成对抗网络 (CycleGAN)。我们的研究证明了基于 Transformer 的模型、在线增强和 CycleGAN 增强在改善叶病分类方面的有效性。虽然合成数据有局限性，但它补充了真实数据，提高了模型性能。这些发现有助于开发强大的植物病害检测和分类技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.05768v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **LKCA: Large Kernel Convolutional Attention**<br />
**Title_cn:** LKCA：大核卷积注意力<br />
**Authors:** Chenghao Li, Boheng Zeng, Yi Lu, Pengbo Shi, Qingzi Chen, Jirui Liu, Lingyun Zhu<br />
**Abstract:** <details><summary>原文: </summary>We revisit the relationship between attention mechanisms and large kernel ConvNets in visual transformers and propose a new spatial attention named Large Kernel Convolutional Attention (LKCA). It simplifies the attention operation by replacing it with a single large kernel convolution. LKCA combines the advantages of convolutional neural networks and visual transformers, possessing a large receptive field, locality, and parameter sharing. We explained the superiority of LKCA from both convolution and attention perspectives, providing equivalent code implementations for each view. Experiments confirm that LKCA implemented from both the convolutional and attention perspectives exhibit equivalent performance. We extensively experimented with the LKCA variant of ViT in both classification and segmentation tasks. The experiments demonstrated that LKCA exhibits competitive performance in visual tasks. Our code will be made publicly available at https://github.com/CatworldLee/LKCA.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们重新审视了视觉 Transformer 中的注意力机制和大核卷积网络之间的关系，并提出了一种新的空间注意力，称为大核卷积注意力（LKCA）。它通过用单个大核卷积替换它来简化注意力操作。 LKCA结合了卷积神经网络和视觉变换器的优点，拥有大的感受野、局部性和参数共享。我们从卷积和注意力两个角度解释了LKCA的优越性，为每个视图提供了等效的代码实现。实验证实，从卷积和注意力角度实现的 LKCA 表现出同等的性能。我们在分类和分割任务中对 ViT 的 LKCA 变体进行了广泛的实验。实验表明，LKCA 在视觉任务中表现出有竞争力的表现。我们的代码将在 https://github.com/CatworldLee/LKCA 公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.05738v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Video Anomaly Detection and Explanation via Large Language Models**<br />
**Title_cn:** 通过大型语言模型进行视频异常检测和解释<br />
**Authors:** Hui Lv, Qianru Sun<br />
**Abstract:** <details><summary>原文: </summary>Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\% and +4.96\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频异常检测（VAD）旨在定位远程监控视频时间轴上的异常事件。基于异常评分的方法已经流行多年，但存在阈值复杂度高和检测结果可解释性低的问题。在本文中，我们在VAD框架中装备基于视频的大语言模型（VLLM）进行了开创性的研究，使VAD模型不受阈值限制，并且能够解释检测到的异常的原因。我们引入了一种新颖的网络模块长期上下文（LTC）来缓解 VLLM 在远程上下文建模中的无能。我们设计了一种三阶段训练方法，通过大幅减少对 VAD 数据的需求并降低注释指令调优数据的成本来提高微调 VLLM 的效率。我们训练的模型在 UCF-Crime 和 TAD 基准的异常视频上实现了最佳性能，AUC 分别提高了 +3.86\% 和 +4.96\%。更令人印象深刻的是，我们的方法可以为检测到的异常提供文本解释。</details>
**PDF:** <http://arxiv.org/pdf/2401.05702v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition**<br />
**Title_cn:** HiCMAE：用于自监督视听情感识别的分层对比屏蔽自动编码器<br />
**Authors:** Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao<br />
**Abstract:** <details><summary>原文: </summary>Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，视听情感识别（AVER）因其在创建情感感知智能机器中的关键作用而受到越来越多的关注。此前该领域的工作主要以监督学习范式为主。尽管取得了重大进展，但由于 AVER 长期存在的数据稀缺问题，监督学习正在遇到瓶颈。受自监督学习最新进展的推动，我们提出了分层对比掩模自动编码器（HiCMAE），这是一种新颖的自监督框架，利用对大量未标记的视听数据进行大规模自监督预训练来促进 AVER 的进步。遵循自监督视听表示学习的现有技术，HiCMAE 采用两种主要的自监督形式进行预训练，即掩码数据建模和对比学习。与只关注顶层表示而忽略中间层的显式指导不同，HiCMAE 开发了一种三管齐下的策略来促进分层视听特征学习并提高学习表示的整体质量。为了验证 HiCMAE 的有效性，我们对涵盖分类和维度 AVER 任务的 9 个数据集进行了广泛的实验。实验结果表明，我们的方法显着优于最先进的监督和自监督视听方法，这表明 HiCMAE 是一种强大的视听情感表示学习器。代码和模型将在 https://github.com/sunlicai/HiCMAE 上公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.05698v1><br />
**Code:** <https://github.com/sunlicai/hicmae>**<br />
>>**index:** 13<br />
**Title:** **Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection**<br />
**Title_cn:** 探索人机交互检测的自三元组相关性和互三元组相关性<br />
**Authors:** Weibo Jiang, Weihong Ren, Jiandong Tian, Liangqiong Qu, Zhiyong Wang, Honghai Liu<br />
**Abstract:** <details><summary>原文: </summary>Human-Object Interaction (HOI) detection plays a vital role in scene understanding, which aims to predict the HOI triplet in the form of <human, object, action>. Existing methods mainly extract multi-modal features (e.g., appearance, object semantics, human pose) and then fuse them together to directly predict HOI triplets. However, most of these methods focus on seeking for self-triplet aggregation, but ignore the potential cross-triplet dependencies, resulting in ambiguity of action prediction. In this work, we propose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI detection. Specifically, we regard each triplet proposal as a graph where Human, Object represent nodes and Action indicates edge, to aggregate self-triplet correlation. Also, we try to explore cross-triplet dependencies by jointly considering instance-level, semantic-level, and layout-level relations. Besides, we leverage the CLIP model to assist our SCTC obtain interaction-aware feature by knowledge distillation, which provides useful action clues for HOI detection. Extensive experiments on HICO-DET and V-COCO datasets verify the effectiveness of our proposed SCTC.</details>
**Abstract_cn:** <details><summary>译文: </summary>人与物体交互（HOI）检测在场景理解中起着至关重要的作用，其目的是预测<人，物体，动作>形式的HOI三元组。现有方法主要提取多模态特征（例如外观、对象语义、人体姿势），然后将它们融合在一起以直接预测 HOI 三元组。然而，这些方法大多数都专注于寻找自三元组聚合，而忽略了潜在的跨三元组依赖性，导致动作预测的模糊性。在这项工作中，我们建议探索用于 HOI 检测的自三重态相关性和交叉三重态相关性 (SCTC)。具体来说，我们将每个三元组提案视为一个图，其中人类、对象代表节点，动作代表边缘，以聚合自三元组相关性。此外，我们尝试通过共同考虑实例级、语义级和布局级关系来探索跨三元组依赖关系。此外，我们利用 CLIP 模型来帮助我们的 SCTC 通过知识蒸馏获得交互感知特征，这为 HOI 检测提供了有用的行动线索。 HICO-DET 和 V-COCO 数据集上的大量实验验证了我们提出的 SCTC 的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.05676v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Masked Attribute Description Embedding for Cloth-Changing Person Re-identification**<br />
**Title_cn:** 用于换衣人员重新识别的屏蔽属性描述嵌入<br />
**Authors:** Chunlei Peng, Boyu Wang, Decheng Liu, Nannan Wang, Ruimin Hu, Xinbo Gao<br />
**Abstract:** <details><summary>原文: </summary>Cloth-changing person re-identification (CC-ReID) aims to match persons who change clothes over long periods. The key challenge in CC-ReID is to extract clothing-independent features, such as face, hairstyle, body shape, and gait. Current research mainly focuses on modeling body shape using multi-modal biological features (such as silhouettes and sketches). However, it does not fully leverage the personal description information hidden in the original RGB image. Considering that there are certain attribute descriptions which remain unchanged after the changing of cloth, we propose a Masked Attribute Description Embedding (MADE) method that unifies personal visual appearance and attribute description for CC-ReID. Specifically, handling variable clothing-sensitive information, such as color and type, is challenging for effective modeling. To address this, we mask the clothing and color information in the personal attribute description extracted through an attribute detection model. The masked attribute description is then connected and embedded into Transformer blocks at various levels, fusing it with the low-level to high-level features of the image. This approach compels the model to discard clothing information. Experiments are conducted on several CC-ReID benchmarks, including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE effectively utilizes attribute description, enhancing cloth-changing person re-identification performance, and compares favorably with state-of-the-art methods. The code is available at https://github.com/moon-wh/MADE.</details>
**Abstract_cn:** <details><summary>译文: </summary>换衣服的人重新识别（CC-ReID）旨在匹配长时间换衣服的人。 CC-ReID 的关键挑战是提取与服装无关的特征，例如面部、发型、体型和步态。目前的研究主要集中在使用多模态生物特征（例如轮廓和草图）来建模身体形状。然而，它并没有充分利用隐藏在原始RGB图像中的个人描述信息。考虑到某些属性描述在换衣服后保持不变，我们提出了一种掩蔽属性描述嵌入（MADE）方法，该方法将个人视觉外观和属性描述统一到CC-ReID。具体来说，处理可变的服装敏感信息（例如颜色和类型）对于有效建模来说是一个挑战。为了解决这个问题，我们掩盖了通过属性检测模型提取的个人属性描述中的服装和颜色信息。然后将屏蔽的属性描述连接并嵌入到各个级别的 Transformer 块中，将其与图像的低级到高级特征融合。这种方法迫使模型丢弃服装信息。在多个 CC-ReID 基准上进行了实验，包括 PRCC、LTCC、Celeb-reID-light 和 LaST。结果表明，MADE 有效地利用了属性描述，增强了换衣服的人重新识别性能，并且与最先进的方法相媲美。代码可在 https://github.com/moon-wh/MADE 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.05646v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **MatSAM: Efficient Materials Microstructure Extraction via Visual Large Model**<br />
**Title_cn:** MatSAM：通过视觉大模型高效提取材料微观结构<br />
**Authors:** Changtai Li, Xu Han, Chao Yao, Xiaojuan Ban<br />
**Abstract:** <details><summary>原文: </summary>Accurate and efficient extraction of microstructures in microscopic images of materials plays a critical role in the exploration of structure-property relationships and the optimization of process parameters. Deep learning-based image segmentation techniques that rely on manual annotation are time-consuming and labor-intensive and hardly meet the demand for model transferability and generalization. Segment Anything Model (SAM), a large visual model with powerful deep feature representation and zero-shot generalization capabilities, has provided new solutions for image segmentation. However, directly applying SAM to segmenting microstructures in microscopic images of materials without human annotation cannot achieve the expected results, as the difficulty of adapting its native prompt engineering to the dense and dispersed characteristics of key microstructures in materials microscopy images. In this paper, we propose MatSAM, a general and efficient microstructure extraction solution based on SAM. A new point-based prompts generation strategy is designed, grounded on the distribution and shape of materials microstructures. It generates prompts for different microscopic images, fuses the prompts of the region of interest (ROI) key points and grid key points, and integrates post-processing methods for quantitative characterization of materials microstructures. For common microstructures including grain boundary and phase, MatSAM achieves superior segmentation performance to conventional methods and is even preferable to supervised learning methods evaluated on 18 materials microstructures imaged by the optical microscope (OM) and scanning electron microscope (SEM). We believe that MatSAM can significantly reduce the cost of quantitative characterization of materials microstructures and accelerate the design of new materials.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确高效地提取材料显微图像中的微观结构对于探索结构-性能关系和优化工艺参数起着至关重要的作用。基于深度学习的图像分割技术依赖于人工标注，耗时耗力，难以满足模型可迁移性和泛化性的需求。 Segment Anything Model（SAM）是一种大型视觉模型，具有强大的深度特征表示和零样本泛化能力，为图像分割提供了新的解决方案。然而，直接应用SAM在没有人工标注的情况下分割材料显微图像中的微观结构并不能达到预期的结果，因为其原生即时工程难以适应材料显微图像中关键微观结构的密集和分散特征。在本文中，我们提出了 MatSAM，一种基于 SAM 的通用且高效的微观结构提取解决方案。基于材料微观结构的分布和形状，设计了一种新的基于点的提示生成策略。它针对不同的显微图像生成提示，融合感兴趣区域（ROI）关键点和网格关键点的提示，并集成用于材料微观结构定量表征的后处理方法。对于包括晶界和相在内的常见微观结构，MatSAM 实现了优于传统方法的分割性能，甚至优于对光学显微镜 (OM) 和扫描电子显微镜 (SEM) 成像的 18 种材料微观结构进行评估的监督学习方法。我们相信MatSAM可以显着降低材料微观结构定量表征的成本并加速新材料的设计。</details>
**PDF:** <http://arxiv.org/pdf/2401.05638v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **REBUS: A Robust Evaluation Benchmark of Understanding Symbols**<br />
**Title_cn:** REBUS：理解符号的稳健评估基准<br />
**Authors:** Andrew Gritsevskiy, Arjun Panickssery, Aaron Kirtland, Derik Kauffman, Hans Gundlach, Irina Gritsevskaya, Joe Cavanagh, Jonathan Chiang, Lydia La Roux, Michelle Hung<br />
**Abstract:** <details><summary>原文: </summary>We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowledge and reasoning of multimodal large language models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个新的基准来评估多模式大语言模型在画画谜题上的性能。该数据集涵盖 333 个基于图像的文字游戏的原始示例，涵盖电影、作曲家、主要城市和食物等 13 个类别。为了在识别线索单词或短语的基准上取得良好的性能，模型必须将图像识别和字符串操作与假设检验、多步骤推理和对人类认知的理解结合起来，从而对能力进行复杂的多模式评估。我们发现 GPT-4V 和 Gemini Pro 等专有模型的性能明显优于所有其他测试模型。然而，即使是最好的模型，最终准确率也仅为 24%，这凸显了推理方面需要大幅改进的必要性。此外，模型很少理解谜题的所有部分，并且几乎总是无法追溯解释正确的答案。因此，我们的基准可用于识别多模态大语言模型的知识和推理中的主要缺陷。</details>
**PDF:** <http://arxiv.org/pdf/2401.05604v1><br />
**Code:** <https://github.com/cvndsh/rebus>**<br />
>>**index:** 17<br />
**Title:** **Nucleus subtype classification using inter-modality learning**<br />
**Title_cn:** 使用跨模态学习进行细胞核亚型分类<br />
**Authors:** Lucas W. Remedios, Shunxing Bao, Samuel W. Remedios, Ho Hin Lee, Leon Y. Cai, Thomas Li, Ruining Deng, Can Cui, Jia Li, Qi Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Understanding the way cells communicate, co-locate, and interrelate is essential to understanding human physiology. Hematoxylin and eosin (H&E) staining is ubiquitously available both for clinical studies and research. The Colon Nucleus Identification and Classification (CoNIC) Challenge has recently innovated on robust artificial intelligence labeling of six cell types on H&E stains of the colon. However, this is a very small fraction of the number of potential cell classification types. Specifically, the CoNIC Challenge is unable to classify epithelial subtypes (progenitor, endocrine, goblet), lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes (fibroblasts, stromal). In this paper, we propose to use inter-modality learning to label previously un-labelable cell types on virtual H&E. We leveraged multiplexed immunofluorescence (MxIF) histology imaging to identify 14 subclasses of cell types. We performed style transfer to synthesize virtual H&E from MxIF and transferred the higher density labels from MxIF to these virtual H&E images. We then evaluated the efficacy of learning in this approach. We identified helper T and progenitor nuclei with positive predictive values of $0.34 \pm 0.15$ (prevalence $0.03 \pm 0.01$) and $0.47 \pm 0.1$ (prevalence $0.07 \pm 0.02$) respectively on virtual H&E. This approach represents a promising step towards automating annotation in digital pathology.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解细胞通信、共定位和相互关联的方式对于理解人类生理学至关重要。苏木精和伊红 (H&E) 染色普遍可用于临床研究和研究。结肠核识别和分类 (CoNIC) 挑战赛最近在结肠 H&E 染色上对六种细胞类型进行了强大的人工智能标记。然而，这只是潜在细胞分类类型数量的一小部分。具体来说，CoNIC 挑战无法对上皮亚型（祖细胞、内分泌细胞、杯状细胞）、淋巴细胞亚型（B、辅助 T、细胞毒性 T）或结缔细胞亚型（成纤维细胞、基质细胞）进行分类。在本文中，我们建议使用跨模态学习来标记虚拟 H&E 上以前无法标记的细胞类型。我们利用多重免疫荧光 (MxIF) 组织学成像来识别 14 种细胞类型亚类。我们执行风格转移以从 MxIF 合成虚拟 H&E，并将更高密度的标签从 MxIF 转移到这些虚拟 H&E 图像。然后我们评估了这种方法的学习效果。我们在虚拟 H&E 上确定了辅助 T 和祖细胞核的阳性预测值分别为 $0.34 \pm 0.15$（患病率 $0.03 \pm 0.01$）和 $0.47 \pm 0.1$（患病率 $0.07 \pm 0.02$）。这种方法代表了数字病理学自动化注释方面迈出了有希望的一步。</details>
**PDF:** <http://arxiv.org/pdf/2401.05602v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation**<br />
**Title_cn:** E$^{2}$GAN：用于图像到图像翻译的高效 GAN 的高效训练<br />
**Authors:** Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.</details>
**Abstract_cn:** <details><summary>译文: </summary>实现灵活的实时设备图像编辑的一个非常有前途的方向是通过利用大规模文本到图像扩散模型（例如稳定扩散）来利用数据蒸馏来生成用于训练生成对抗网络（GAN）的配对数据集。这种方法显着缓解了高端商用 GPU 通常对使用扩散模型执行图像编辑提出的严格要求。然而，与文本到图像的扩散模型不同，每个精炼的 GAN 专门用于特定的图像编辑任务，需要昂贵的训练工作才能获得各种概念的模型。在这项工作中，我们介绍并提出了一个新的研究方向：从扩散模型中提取 GAN 的过程是否可以显着提高效率？为了实现这一目标，我们提出了一系列创新技术。首先，我们构建一个具有通用特征的基础 GAN 模型，通过微调适应不同的概念，从而无需从头开始训练。其次，我们确定基本 GAN 模型中的关键层，并通过简单而有效的排名搜索过程采用低秩适应 (LoRA)，而不是微调整个基本模型。第三，我们研究微调所需的最少量数据，进一步减少总体训练时间。大量实验表明，我们可以有效地赋予 GAN 在移动设备上执行实时高质量图像编辑的能力，并显着降低每个概念的训练成本和存储成本。</details>
**PDF:** <http://arxiv.org/pdf/2401.06127v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PALP: Prompt Aligned Personalization of Text-to-Image Models**<br />
**Title_cn:** PALP：文本到图像模型的快速对齐个性化<br />
**Authors:** Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, Ariel Shamir<br />
**Abstract:** <details><summary>原文: </summary>Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks. We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>内容创建者通常旨在使用超出传统文本到图像模型功能的个人主题来创建个性化图像。此外，他们可能希望生成的图像包含特定的位置、风格、氛围等。现有的个性化方法可能会损害个性化能力或与复杂文本提示的对齐。这种权衡可能会妨碍用户提示的实现和主题保真度。我们提出了一种专注于 \emph{single} 提示的个性化方法的新方法来解决这个问题。我们将我们的方法称为“及时调整个性化”。虽然这看起来可能有限制，但我们的方法擅长改进文本对齐，能够创建具有复杂提示的图像，这可能对当前技术构成挑战。特别是，我们的方法使用附加的分数蒸馏采样项使个性化模型与目标提示保持一致。我们展示了我们的方法在多镜头和单镜头设置中的多功能性，并进一步表明它可以组合多个主题或使用来自参考图像（例如艺术品）的灵感。我们将我们的方法与现有基线和最先进的技术进行定量和定性比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.06105v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering**<br />
**Title_cn:** TRIPS：用于实时辐射场渲染的三线性点溅射<br />
**Authors:** Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger<br />
**Abstract:** <details><summary>原文: </summary>Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于点的辐射场渲染在新颖的视图合成方面展示了令人印象深刻的结果，提供了令人信服的渲染质量和计算效率的结合。然而，该领域的最新方法也并非没有缺点。 3D 高斯泼溅 [Kerbl 和 Kopanas 等人。 2023] 由于模糊和浑浊的伪像，在渲染高度详细的场景时会遇到困难。另一方面，ADOP [R\"uckert et al. 2022] 可以容纳更清晰的图像，但神经重建网络会降低性能，它会解决时间不稳定问题，并且无法有效解决点云中的大间隙。论文中，我们提出了 TRIPS（三线性点分布），这是一种结合了高斯分布和 ADOP 思想的方法。我们新技术背后的基本概念涉及将点光栅化为屏幕空间图像金字塔，金字塔层的选择由下式决定：投影点大小。这种方法允许使用单个三线性写入渲染任意大的点。然后使用轻量级神经网络来重建无孔图像，包括超出splat分辨率的细节。重要的是，我们的渲染管道是完全可微分的，允许自动我们的评估表明，TRIPS 在渲染质量方面超越了现有的最先进方法，同时在现成的硬件上保持了每秒 60 帧的实时帧速率。这种性能扩展到具有挑战性的场景，例如具有复杂几何形状、广阔景观和自动曝光镜头的场景。</details>
**PDF:** <http://arxiv.org/pdf/2401.06003v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd Counting**<br />
**Title_cn:** 用于资源受限人群计数的轻量级特征融合架构<br />
**Authors:** Yashwardhan Chaudhuri, Ankit Kumar, Orchid Chetia Phukan, Arun Balaji Buduru<br />
**Abstract:** <details><summary>原文: </summary>Crowd counting finds direct applications in real-world situations, making computational efficiency and performance crucial. However, most of the previous methods rely on a heavy backbone and a complex downstream architecture that restricts the deployment. To address this challenge and enhance the versatility of crowd-counting models, we introduce two lightweight models. These models maintain the same downstream architecture while incorporating two distinct backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to extract diverse scale features from a Pre-Trained Model (PTM) and subsequently combine these features seamlessly. This approach empowers our models to achieve improved performance while maintaining a compact and efficient design. With the comparison of our proposed models with previously available state-of-the-art (SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it achieves comparable results while being the most computationally efficient model. Finally, we present a comparative study, an extensive ablation study, along with pruning to show the effectiveness of our models.</details>
**Abstract_cn:** <details><summary>译文: </summary>人群计数在现实世界中找到了直接应用，这使得计算效率和性能变得至关重要。然而，之前的方法大多依赖于沉重的主干网和复杂的下游架构，这限制了部署。为了应对这一挑战并增强人群计数模型的多功能性，我们引入了两种轻量级模型。这些模型保持相同的下游架构，同时合并两个不同的主干：MobileNet 和 MobileViT。我们利用相邻特征融合从预训练模型（PTM）中提取不同的尺度特征，然后无缝地组合这些特征。这种方法使我们的模型能够提高性能，同时保持紧凑和高效的设计。通过将我们提出的模型与 ShanghaiTech-A、ShanghaiTech-B 和 UCF-CC-50 数据集上现有的最先进 (SOTA) 方法进行比较，它取得了可比较的结果，同时成为计算效率最高的模型。最后，我们提出了一项比较研究，一项广泛的消融研究，以及修剪以显示我们模型的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.05968v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks**<br />
**Title_cn:** RAVEN：重新思考使用高效三平面网络的对抗性视频生成<br />
**Authors:** Partha Ghosh, Soubhik Sanyal, Cordelia Schmid, Bernhard Schölkopf<br />
**Abstract:** <details><summary>原文: </summary>We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN) based generator architecture, thereby compensating for the constraints imposed by a smaller generator size. As a result, our model is capable of synthesizing high-fidelity video clips at a resolution of $256\times256$ pixels, with durations extending to more than $5$ seconds at a frame rate of 30 fps. The efficacy and versatility of our approach are empirically validated through qualitative and quantitative assessments across three different datasets comprising both synthetic and real video clips.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的无条件视频生成模型，旨在解决长期的空间和时间依赖性。为了捕获这些依赖性，我们的方法采用了混合显式-隐式三平面表示，其灵感来自于为三维对象表示而开发的 3D 感知生成框架，并采用单一潜在代码来建模整个视频序列。然后从中间三平面表示合成各个视频帧，该中间三平面表示本身是从主要潜在代码导出的。这种新颖的策略将计算复杂性降低了 2 美元（以 FLOP 为单位）。因此，我们的方法有助于高效且时间连贯地生成视频。此外，与自回归方法相比，我们的联合帧建模方法可以减少视觉伪影的产生。我们通过将基于光流的模块集成到基于生成对抗网络（GAN）的生成器架构中，进一步增强了模型的功能，从而补偿了较小生成器尺寸所带来的限制。因此，我们的模型能够以 256\times256$ 像素的分辨率合成高保真视频剪辑，持续时间在 30 fps 的帧速率下延长到超过 5$ 秒。我们的方法的有效性和多功能性通过对三个不同数据集（包括合成视频剪辑和真实视频剪辑）的定性和定量评估进行了实证验证。</details>
**PDF:** <http://arxiv.org/pdf/2401.06035v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model**<br />
**Title_cn:** GE-AdvGAN：通过基于梯度编辑的对抗生成模型提高对抗样本的可转移性<br />
**Authors:** Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo<br />
**Abstract:** <details><summary>原文: </summary>Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗生成模型，例如生成对抗网络（GAN），广泛应用于生成各种类型的数据，即图像、文本和音频。因此，其令人鼓舞的性能催生了白盒和黑盒攻击场景中基于 GAN 的对抗攻击方法。可转移黑盒攻击的重要性在于它们能够在不同的模型和设置中发挥作用，与现实世界的应用程序更加紧密地结合。然而，保持此类方法在可转移对抗样本方面的性能仍然具有挑战性。同时，我们观察到一些增强的基于梯度的可转移对抗攻击算法需要更长的时间来生成对抗样本。因此，在这项工作中，我们提出了一种名为 GE-AdvGAN 的新算法，以增强对抗样本的可转移性，同时提高算法的效率。主要方法是通过优化生成器参数的训练过程。通过功能和特征相似性分析，我们引入了一种新颖的梯度编辑（GE）机制，并验证了其在各种模型上生成可转移样本的可行性。此外，通过探索频域信息来确定梯度编辑方向，GE-AdvGAN 可以生成高度可转移的对抗样本，同时与最先进的可转移对抗攻击算法相比，最大限度地减少执行时间。通过在不同数据集上的大规模实验对GE-AdvGAN的性能进行了综合评估，结果证明了我们算法的优越性。我们的算法的代码位于：https://github.com/LMBTough/GE-advGAN</details>
**PDF:** <http://arxiv.org/pdf/2401.06031v1><br />
**Code:** <https://github.com/lmbtough/ge-advgan>**<br />
>>**index:** 3<br />
**Title:** **How does the primate brain combine generative and discriminative computations in vision?**<br />
**Title_cn:** 灵长类动物的大脑如何将视觉中的生成计算和判别计算结合起来？<br />
**Authors:** Benjamin Peters, James J. DiCarlo, Todd Gureckis, Ralf Haefner, Leyla Isik, Joshua Tenenbaum, Talia Konkle, Thomas Naselaris, Kimberly Stachenfeld, Zenna Tavares, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of "inference" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmholtz's sense, where the sensory evidence is evaluated in the context of a generative model of the causal processes giving rise to it. In this conception, vision inverts a generative model through an interrogation of the evidence in a process often thought to involve top-down predictions of sensory data to evaluate the likelihood of alternative hypotheses. The authors include scientists rooted in roughly equal numbers in each of the conceptions and motivated to overcome what might be a false dichotomy between them and engage the other perspective in the realm of theory and experiment. The primate brain employs an unknown algorithm that may combine the advantages of both conceptions. We explain and clarify the terminology, review the key empirical evidence, and propose an empirical research program that transcends the dichotomy and sets the stage for revealing the mysterious hybrid algorithm of primate vision.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉被广泛理解为一个推理问题。然而，推理过程的两种截然不同的概念都对生物视觉研究以及机器视觉工程产生了影响。第一个强调自下而上的信号流，将视觉描述为一个很大程度上前馈、判别性的推理过程，它过滤和转换视觉信息以消除不相关的变化，并以适合认知和行为控制下游功能的格式表示行为相关信息。在这个概念中，视觉是由感官数据驱动的，而感知是直接的，因为处理从数据进行到感兴趣的潜在变量。这个概念中的“推理”概念是神经网络工程文献中的概念，其中处理图像的前馈卷积神经网络被称为执行推理。另一种概念是将视觉视为亥姆霍兹意义上的推理过程，其中感官证据是在产生它的因果过程的生成模型的背景下进行评估的。在这个概念中，视觉通过询问证据来反转生成模型，这一过程通常被认为涉及自上而下的感官数据预测，以评估替代假设的可能性。作者包括在每个概念中植根于大致相等数量的科学家，并致力于克服它们之间可能存在的错误二分法，并在理论和实验领域中采用其他观点。灵长类动物的大脑采用了一种未知的算法，可以结合这两种概念的优点。我们解释和澄清了术语，回顾了关键的经验证据，并提出了一个超越二分法的实证研究计划，并为揭示灵长类动物视觉的神秘混合算法奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.06005v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **An attempt to generate new bridge types from latent space of PixelCNN**<br />
**Title_cn:** 尝试从 PixelCNN 的潜在空间生成新的桥类型<br />
**Authors:** Hongjun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the way to achieve artificial general intelligence in the future.</details>
**Abstract_cn:** <details><summary>译文: </summary>尝试使用生成人工智能技术生成新的桥梁类型。利用三跨梁桥、拱桥、斜拉桥、悬索桥的对称结构化图像数据集，基于Python编程语言、TensorFlow和Keras深度学习平台框架，构建并训练PixelCNN。该模型可以捕获图像的统计结构，并在给定先前像素的情况下计算下一个像素的概率分布。根据获得的潜在空间采样，可以生成与训练数据集不同的新桥梁类型。 PixelCNN可以在人类原创桥梁类型的基础上有机地组合不同的结构组件，创造出具有一定人类原创能力的新桥梁类型。自回归模型无法理解序列的含义，而多模态模型则结合回归和自回归模型来理解序列。多模态模型应该是未来实现通用人工智能的途径。</details>
**PDF:** <http://arxiv.org/pdf/2401.05964v1><br />
**Code:** <https://github.com/QQ583304953/Bridge-PixelCNN>**<br />
>>**index:** 5<br />
**Title:** **Efficient Image Deblurring Networks based on Diffusion Models**<br />
**Title_cn:** 基于扩散模型的高效图像去模糊网络<br />
**Authors:** Kang Chen, Yuanjie Liu<br />
**Abstract:** <details><summary>原文: </summary>This article introduces a sliding window model for defocus deblurring that achieves the best performance to date with extremely low memory usage. Named Swintormer, the method utilizes a diffusion model to generate latent prior features that assist in restoring more detailed images. It also extends the sliding window strategy to specialized Transformer blocks for efficient inference. Additionally, we have further optimized Multiply-Accumulate operations (Macs). Compared to the currently top-performing GRL method, our Swintormer model drastically reduces computational complexity from 140.35 GMACs to 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocus deblurring from 27.04 dB to 27.07 dB. This new method allows for the processing of higher resolution images on devices with limited memory, significantly expanding potential application scenarios. The article concludes with an ablation study that provides an in-depth analysis of the impact of each network module on final performance. The source code and model will be available at the following website: https://github.com/bnm6900030/swintormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种用于散焦去模糊的滑动窗口模型，该模型以极低的内存使用量实现了迄今为止的最佳性能。该方法名为 Swintormer，利用扩散模型生成潜在的先验特征，有助于恢复更详细的图像。它还将滑动窗口策略扩展到专门的 Transformer 块，以实现高效推理。此外，我们还进一步优化了乘法累加运算 (Mac)。与目前表现最好的 GRL 方法相比，我们的 Swintormer 模型将计算复杂度从 140.35 GMAC 大幅降低到 8.02 GMac，同时还将散焦去模糊的信噪比 (SNR) 从 27.04 dB 提高到 27.07 dB。这种新方法允许在内存有限的设备上处理更高分辨率的图像，显​​着扩展了潜在的应用场景。本文最后进行了消融研究，深入分析了每个网络模块对最终性能的影响。源代码和模型可在以下网站获取：https://github.com/bnm6900030/swintormer。</details>
**PDF:** <http://arxiv.org/pdf/2401.05907v1><br />
**Code:** <https://github.com/bnm6900030/swintormer>**<br />
>>**index:** 6<br />
**Title:** **HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models**<br />
**Title_cn:** HiCAST：高度定制的任意风格转移，带有适配器增强扩散模型<br />
**Authors:** Hanzhang Wang, Haoran Wang, Jinze Yang, Zhongrui Yu, Zeke Xie, Lei Tian, Xinyan Xiao, Junjun Jiang, Xianming Liu, Mingming Sun<br />
**Abstract:** <details><summary>原文: </summary>The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for video diffusion model training, which significantly improve cross-frame temporal consistency in the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our HiCAST outperforms the existing SoTA methods in generating visually plausible stylization results.</details>
**Abstract_cn:** <details><summary>译文: </summary>任意风格迁移（AST）的目标是将风格参考的艺术特征注入给定的图像/视频中。现有的方法通常注重追求风格和内容之间的平衡，而忽略了对灵活和定制的风格化结果的巨大需求，从而限制了其实际应用。为了解决这个关键问题，提出了一种新的 AST 方法，即 HiCAST，它能够根据各种语义线索来源显式定制样式化结果。具体来说，我们的模型是基于潜在扩散模型（LDM）构建的，并精心设计以吸收内容和风格实例作为LDM的条件。它的特点是引入了\textit{Style Adapter}，允许用户通过对齐LDM中的多级样式信息和内在知识来灵活地操纵输出结果。最后，我们进一步扩展我们的模型来执行视频 AST。利用新颖的学习目标进行视频扩散模型训练，在保持风格化强度的前提下显着提高跨帧时间一致性。定性和定量比较以及全面的用户研究表明，我们的 HiCAST 在生成视觉上合理的风格化结果方面优于现有的 SoTA 方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05870v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **EraseDiff: Erasing Data Influence in Diffusion Models**<br />
**Title_cn:** EraseDiff：消除扩散模型中的数据影响<br />
**Authors:** Jing Wu, Trung Le, Munawar Hayat, Mehrtash Harandi<br />
**Abstract:** <details><summary>原文: </summary>In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data. The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios. In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了响应数据保护法规和“被遗忘权”，在这项工作中，我们引入了一种扩散模型的遗忘算法。我们的算法为扩散模型配备了一种机制，以减轻与数据记忆相关的问题。为了实现这一目标，我们将遗忘问题表述为双层优化问题，其中外部目标是保留扩散模型对剩余数据的效用。内部目标旨在通过使可学习的生成过程偏离真实的去噪过程来清除与遗忘数据相关的信息。为了解决由此产生的双层问题，我们采用一阶方法，具有优越的实用性能，同时警惕扩散过程并解决其中的双层问题。根据经验，我们证明我们的算法可以保留模型的实用性、有效性和效率，同时在两种广泛使用的扩散模型以及条件和无条件图像生成场景中进行删除。在我们的实验中，我们演示了如何从 UTKFace、CelebA、CelebA-HQ 和 CIFAR10 等人脸和对象数据集中忘记类别、属性甚至种族。</details>
**PDF:** <http://arxiv.org/pdf/2401.05779v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **LEGO:Language Enhanced Multi-modal Grounding Model**<br />
**Title_cn:** 乐高：语言增强多模式接地模型<br />
**Authors:** Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型在不同模态的各种任务中表现出了令人印象深刻的性能。然而，现有的多模态模型主要强调捕获每种模态内的全局信息，而忽略了跨模态感知局部信息的重要性。因此，这些模型缺乏有效理解输入数据的细粒度细节的能力，限制了它们在需要更细致理解的任务中的性能。为了解决这一限制，迫切需要开发能够跨多种模式进行细粒度理解的模型，从而增强其对广泛任务的适用性。在本文中，我们提出了 LEGO，一种语言增强的多模态基础模型。除了像其他多模态模型一样捕获全局信息之外，我们提出的模型还擅长执行需要详细了解输入中的本地信息的任务。它演示了对图像或视频中特定区域的精确识别和定位。为了实现这一目标，我们设计了多样化的数据集构建流程，从而产生用于模型训练的多模式、多粒度数据集。我们模型的代码、数据集和演示可以在 https://github.com/lzw-lzw/LEGO 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.06071v1><br />
**Code:** <https://github.com/lzw-lzw/lego>**<br />
>>**index:** 2<br />
**Title:** **Hallucination Benchmark in Medical Visual Question Answering**<br />
**Title_cn:** 医学视觉问答中的幻觉基准<br />
**Authors:** Jinge Wu, Yunsoo Kim, Honghan Wu<br />
**Abstract:** <details><summary>原文: </summary>The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大型语言和视觉模型在视觉问答（VQA）方面取得的成功，特别是它们在医学（Med-VQA）中的应用，已经显示出实现有效的医疗保健视觉助手的巨大潜力。然而，这些模型并未在临床环境中对幻觉现象进行广泛的测试。在这里，我们创建了与问答集配对的医学图像的幻觉基准，并对最先进的模型进行了全面评估。该研究深入分析了当前模型的局限性，并揭示了各种提示策略的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.05827v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Surface Normal Estimation with Transformers**<br />
**Title_cn:** 使用 Transformer 进行表面法线估计<br />
**Authors:** Barry Shichen Hu, Siyun Liang, Johannes Paetzold, Huy H. Nguyen, Isao Echizen, Jiapeng Tang<br />
**Abstract:** <details><summary>原文: </summary>We propose the use of a Transformer to accurately predict normals from point clouds with noise and density variations. Previous learning-based methods utilize PointNet variants to explicitly extract multi-scale features at different input scales, then focus on a surface fitting method by which local point cloud neighborhoods are fitted to a geometric surface approximated by either a polynomial function or a multi-layer perceptron (MLP). However, fitting surfaces to fixed-order polynomial functions can suffer from overfitting or underfitting, and learning MLP-represented hyper-surfaces requires pre-generated per-point weights. To avoid these limitations, we first unify the design choices in previous works and then propose a simplified Transformer-based model to extract richer and more robust geometric features for the surface normal estimation task. Through extensive experiments, we demonstrate that our Transformer-based method achieves state-of-the-art performance on both the synthetic shape dataset PCPNet, and the real-world indoor scene dataset SceneNN, exhibiting more noise-resilient behavior and significantly faster inference. Most importantly, we demonstrate that the sophisticated hand-designed modules in existing works are not necessary to excel at the task of surface normal estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们建议使用 Transformer 来准确预测具有噪声和密度变化的点云的法线。以前基于学习的方法利用 PointNet 变体在不同输入尺度上显式提取多尺度特征，然后关注表面拟合方法，通过该方法将局部点云邻域拟合到由多项式函数或多层近似的几何表面感知器（MLP）。然而，将曲面拟合到固定阶多项式函数可能会出现过度拟合或欠拟合的情况，并且学习 MLP 表示的超曲面需要预先生成的每点权重。为了避免这些限制，我们首先统一以前工作中的设计选择，然后提出一个基于 Transformer 的简化模型，为表面法线估计任务提取更丰富、更鲁棒的几何特征。通过大量实验，我们证明了基于 Transformer 的方法在合成形状数据集 PCPNet 和真实室内场景数据集 SceneNN 上实现了最先进的性能，表现出更强的抗噪行为和显着更快的推理速度。最重要的是，我们证明现有作品中复杂的手工设计模块不一定能够出色地完成表面法线估计的任务。</details>
**PDF:** <http://arxiv.org/pdf/2401.05745v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Object-Centric Diffusion for Efficient Video Editing**<br />
**Title_cn:** 以对象为中心的扩散，实现高效视频编辑<br />
**Authors:** Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki M. Asano, Amirhossein Habibian<br />
**Abstract:** <details><summary>原文: </summary>Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, which reduces cost of cross-frame attention by fusing redundant tokens in unimportant background regions. Both techniques are readily applicable to a given video editing model \textit{without} retraining, and can drastically reduce its memory and computational cost. We evaluate our proposals on inversion-based and control-signal-based editing pipelines, and show a latency reduction up to 10x for a comparable synthesis quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散的视频编辑已经达到了令人印象深刻的质量，并且可以按照文本编辑提示转换给定视频输入的全局样式、局部结构和属性。然而，这样的解决方案通常会产生大量的内存和计算成本来生成时间相干的帧，无论是采用扩散反转和/或跨帧注意的形式。在本文中，我们对这种低效率进行了分析，并提出了简单而有效的修改建议，可以在保持质量的同时显着提高速度。此外，我们引入了以对象为中心的扩散（被称为 OCD），通过将计算更多地分配给前台编辑区域来进一步减少延迟，这对于感知质量来说可能更重要。我们通过两个新颖的提议来实现这一目标：i) 以对象为中心的采样，解耦在显着区域或背景上花费的扩散步骤，将大部分模型容量分配给前者，以及 ii) 以对象为中心的 3D 令牌合并，这降低了成本通过在不重要的背景区域融合冗余标记来实现跨帧注意力。这两种技术都很容易适用于给定的视频编辑模型\textit{无需}重新训练，并且可以大大减少其内存和计算成本。我们评估了我们关于基于反转和基于控制信号的编辑管道的建议，结果表明，在同等合成质量的情况下，延迟减少了高达 10 倍。</details>
**PDF:** <http://arxiv.org/pdf/2401.05735v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Transforming Image Super-Resolution: A ConvFormer-based Efficient Approach**<br />
**Title_cn:** 变换图像超分辨率：一种基于 ConvFormer 的高效方法<br />
**Authors:** Gang Wu, Junjun Jiang, Junpeng Jiang, Xianming Liu<br />
**Abstract:** <details><summary>原文: </summary>Recent progress in single-image super-resolution (SISR) has achieved remarkable performance, yet the computational costs of these methods remain a challenge for deployment on resource-constrained devices. Especially for transformer-based methods, the self-attention mechanism in such models brings great breakthroughs while incurring substantial computational costs. To tackle this issue, we introduce the Convolutional Transformer layer (ConvFormer) and the ConvFormer-based Super-Resolution network (CFSR), which offer an effective and efficient solution for lightweight image super-resolution tasks. In detail, CFSR leverages the large kernel convolution as the feature mixer to replace the self-attention module, efficiently modeling long-range dependencies and extensive receptive fields with a slight computational cost. Furthermore, we propose an edge-preserving feed-forward network, simplified as EFN, to obtain local feature aggregation and simultaneously preserve more high-frequency information. Extensive experiments demonstrate that CFSR can achieve an advanced trade-off between computational cost and performance when compared to existing lightweight SR methods. Compared to state-of-the-art methods, e.g. ShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset for x2 SR task while containing 26% and 31% fewer parameters and FLOPs, respectively. Code and pre-trained models are available at https://github.com/Aitical/CFSR.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像超分辨率（SISR）的最新进展已经取得了显着的性能，但这些方法的计算成本仍然是在资源受限设备上部署的挑战。特别是对于基于 Transformer 的方法，此类模型中的自注意力机制带来了巨大的突破，同时带来了大量的计算成本。为了解决这个问题，我们引入了卷积变换层（ConvFormer）和基于ConvFormer的超分辨率网络（CFSR），它们为轻量级图像超分辨率任务提供了有效且高效的解决方案。具体来说，CFSR利用大核卷积作为特征混合器来取代自注意力模块，以少量的计算成本有效地建模长程依赖性和广泛的感受野。此外，我们提出了一种边缘保留前馈网络，简化为 EFN，以获得局部特征聚合并同时保留更多高频信息。大量实验表明，与现有的轻量级 SR 方法相比，CFSR 可以在计算成本和性能之间实现高级权衡。与最先进的方法相比，例如ShuffleMixer 中，所提出的 CFSR 在 x2 SR 任务的 Urban100 数据集上实现了 0.39 dB 的增益，同时参数和 FLOP 分别减少了 26% 和 31%。代码和预训练模型可在 https://github.com/Atical/CFSR 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.05633v1><br />
**Code:** <https://github.com/aitical/cfsr>**<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Fast High Dynamic Range Radiance Fields for Dynamic Scenes**<br />
**Title_cn:** 适用于动态场景的快速高动态范围辐射场<br />
**Authors:** Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \url{https://guanjunwu.github.io/HDR-HexPlane/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 及其扩展在表示 3D 场景和合成新颖视图图像方面取得了巨大成功。然而，大多数 NeRF 方法采用低动态范围 (LDR) 图像，这可能会丢失细节，尤其是在照明不均匀的情况下。之前的一些 NeRF 方法尝试引入高动态范围（HDR）技术，但主要针对静态场景。为了将 HDR NeRF 方法扩展到更广泛的应用，我们提出了一个动态 HDR NeRF 框架，名为 HDR-HexPlane，它可以从使用各种曝光捕获的动态 2D 图像中学习 3D 场景。构建可学习的曝光映射函数以获得每个图像的自适应曝光值。基于单调递增先验，设计了相机响应函数以实现稳定学习。通过所提出的模型，可以在任何时间点以任何所需的曝光度渲染高质量的新颖视图图像。我们进一步构建了一个数据集，其中包含使用不同曝光捕获的多个动态场景以进行评估。所有数据集和代码都可以在 \url{https://guanjunwu.github.io/HDR-HexPlane/} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06052v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GO-NeRF: Generating Virtual Objects in Neural Radiance Fields**<br />
**Title_cn:** GO-NeRF：在神经辐射场中生成虚拟对象<br />
**Authors:** Peng Dai, Feitong Tan, Xin Yu, Yinda Zhang, Xiaojuan Qi<br />
**Abstract:** <details><summary>原文: </summary>Despite advances in 3D generation, the direct creation of 3D objects within an existing 3D scene represented as NeRF remains underexplored. This process requires not only high-quality 3D object generation but also seamless composition of the generated 3D content into the existing NeRF. To this end, we propose a new method, GO-NeRF, capable of utilizing scene context for high-quality and harmonious 3D object generation within an existing NeRF. Our method employs a compositional rendering formulation that allows the generated 3D objects to be seamlessly composited into the scene utilizing learned 3D-aware opacity maps without introducing unintended scene modification. Moreover, we also develop tailored optimization objectives and training strategies to enhance the model's ability to exploit scene context and mitigate artifacts, such as floaters, originating from 3D object generation within a scene. Extensive experiments on both feed-forward and $360^o$ scenes show the superior performance of our proposed GO-NeRF in generating objects harmoniously composited with surrounding scenes and synthesizing high-quality novel view images. Project page at {\url{https://daipengwa.github.io/GO-NeRF/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管 3D 生成技术取得了进步，但在以 NeRF 表示的现有 3D 场景中直接创建 3D 对象仍然没有得到充分探索。这个过程不仅需要高质量的 3D 对象生成，还需要将生成的 3D 内容无缝组合到现有的 NeRF 中。为此，我们提出了一种新方法 GO-NeRF，它能够利用场景上下文在现有 NeRF 中生成高质量且和谐的 3D 对象。我们的方法采用合成渲染公式，允许使用学习的 3D 感知不透明贴图将生成的 3D 对象无缝合成到场景中，而不会引入意外的场景修改。此外，我们还开发定制的优化目标和训练策略，以增强模型利用场景上下文和减少源自场景内 3D 对象生成的伪影（例如漂浮物）的能力。对前馈和 360^o$ 场景的大量实验表明，我们提出的 GO-NeRF 在生成与周围场景和谐合成的对象以及合成高质量的新颖视图图像方面具有卓越的性能。项目页面位于 {\url{https://daipengwa.github.io/GO-NeRF/}。</details>
**PDF:** <http://arxiv.org/pdf/2401.05750v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **Gaussian Shadow Casting for Neural Characters**<br />
**Title_cn:** 神经角色的高斯阴影投射<br />
**Authors:** Luis Bolanos, Shih-Yang Su, Helge Rhodin<br />
**Abstract:** <details><summary>原文: </summary>Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经角色模型现在可以从视频中重建详细的几何形状和纹理，但它们缺乏明确的阴影和阴影，导致在生成新颖的视图和姿势或重新照明期间出现伪影。包含阴影特别困难，因为它们是全局效果，并且所需的二次光线投射成本很高。我们提出了一种使用高斯密度代理的新阴影模型，用简单的分析公式代替采样。它支持动态运动并专为阴影计算而定制，从而避免了密切相关的高斯泼溅所需的仿射投影近似和排序。与延迟神经渲染模型相结合，我们的高斯阴影能够以最小的开销实现朗伯着色和阴影投射。我们展示了改进的重建，在具有直射阳光和硬阴影的具有挑战性的户外场景中更好地分离了反照率、阴影和阴影。我们的方法能够优化光线方向，无需用户输入任何信息。因此，与最先进的方法相比，新颖的姿势具有更少的阴影伪影，并且新颖场景中的重新照明更加真实，从而提供了在新颖环境中摆出神经角色姿势的新方法，增加了其适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06116v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors**<br />
**Title_cn:** 适合所有人的配音：使用神经渲染先验进行数据高效的视觉配音<br />
**Authors:** Jack Saunders, Vinay Namboodiri<br />
**Abstract:** <details><summary>原文: </summary>Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio. Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption. Existing methods are split into either person-generic or person-specific models. Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets. Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts. Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches. Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures. This method allows for $\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\textbf{visual quality}$ and $\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies. Our prior learning and adaptation method $\textbf{generalises to limited data}$ better and is more $\textbf{scalable}$ than existing person-specific models. Our experiments on real-world, limited data scenarios find that our model is preferred over all others. The project page may be found at https://dubbingforeveryone.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉配音是在视频中生成演员的嘴唇动作以与给定音频同步的过程。最近的进展在实现这一目标方面取得了进展，但未能产生适合大规模采用的方法。现有的方法分为个人通用模型或个人特定模型。特定于人的模型产生的结果几乎与现实没有区别，但依赖于使用大型单人数据集的长时间训练。人物通用作品允许将任何视频进行视觉配音，无需进一步培训，但这些作品无法捕捉特定于人物的细微差别，并且经常受到视觉伪影的影响。我们的方法基于数​​据高效的神经渲染先验，克服了现有方法的局限性。我们的流程包括学习延迟神经渲染先验网络和使用神经纹理进行特定于演员的适应。此方法允许$\textbf{只需几秒钟的数据即可进行高质量的视觉配音}$，可以为任何演员（从一线明星到背景演员）进行视频配音。我们通过两项用户研究表明，我们在定量和定性方面均达到了最先进的水平。我们之前的学习和适应方法 $\textbf{概括到有限的数据}$ 比现有的特定于个人的模型更好，并且更 $\textbf{可扩展}$。我们对现实世界、有限数据场景的实验发现，我们的模型优于所有其他模型。项目页面可以在 https://dubbingforeveryone.github.io/ 找到</details>
**PDF:** <http://arxiv.org/pdf/2401.06126v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MatSynth: A Modern PBR Materials Dataset**<br />
**Title_cn:** MatSynth：现代 PBR 材料数据集<br />
**Authors:** Giuseppe Vecchio, Valentin Deschaintre<br />
**Abstract:** <details><summary>原文: </summary>We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 MatSynth，这是一个包含 4,000+$ CC0 超高分辨率 PBR 材料的数据集。材质是虚拟可重新照明资产的重要组成部分，定义了光在几何体表面的相互作用。鉴于它们的重要性，大量的研究工作致力于它们的表现、创造和获取。然而，在过去的六年中，大多数材料获取或生成的研究要么依赖于相同的独特数据集，要么依赖于公司拥有的庞大的程序材料库。通过这个数据集，我们提出了比以前公开的更大、更多样化、分辨率更高的材料集。我们仔细讨论了数据收集过程，并展示了该数据集在材料采集和生成应用程序中的优势。完整的数据还包含元数据，其中包含每种材质的来源、许可证、类别、标签、创建方法以及可用的描述和物理尺寸，以及在各种环境照明下以 1K 表示的增强材质的 3M+ 渲染。 MatSynth 数据集通过项目页面发布：https://www.gvecchio.com/matsynth。</details>
**PDF:** <http://arxiv.org/pdf/2401.06056v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery**<br />
**Title_cn:** Surgical-DINO：内窥镜手术深度估计基础模型的适配器学习<br />
**Authors:** Cui Beilei, Islam Mobarakol, Bai Long, Ren Hongliang<br />
**Abstract:** <details><summary>原文: </summary>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：机器人手术中的深度估计对于 3D 重建、手术导航和增强现实可视化至关重要。尽管基础模型在许多视觉任务中表现出出色的性能，包括深度估计（例如 DINOv2），但最近的工作观察到其在医疗和外科领域特定应用中的局限性。这项工作提出了用于手术深度估计的基础模型的低阶适应（LoRA）。方法：我们设计了一种基于基础模型的深度估计方法，称为 Surgical-DINO，是 DINOv2 的低阶改编，用于内窥镜手术中的深度估计。我们构建 LoRA 层并将其集成到 DINO 中，以适应手术特定的领域知识，而不是传统的微调。在训练过程中，我们冻结了 DINO 图像编码器，该编码器显示出出色的视觉表示能力，并且仅优化 LoRA 层和深度解码器以集成手术场景的特征。结果：我们的模型在 SCARED 的 MICCAI 挑战数据集上得到了广泛验证，该数据集是从达芬奇 Xi 内窥镜手术中收集的。我们的经验表明，在内窥镜深度估计任务中，Surgical-DINO 显着优于所有最先进的模型。消融研究的分析证明了 LoRA 层和适应的显着效果。结论：Surgical-DINO 为将基础模型成功应用于手术领域进行深度估计提供了一些启示。结果中有明确的证据表明，对计算机视觉数据集中预训练权重的零样本预测或朴素微调不足以直接在外科领域使用基础模型。代码可在 https://github.com/BeileiCui/SurgicalDINO 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06013v1><br />
**Code:** <https://github.com/beileicui/surgicaldino>**<br />
>>**index:** 4<br />
**Title:** **UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization**<br />
**Title_cn:** UAVD4L：无人机 6 自由度定位的大规模数据集<br />
**Authors:** Rouwan Wu, Xiaoya Cheng, Juelin Zhu, Xuxiang Liu, Maojun Zhang, Shen Yan<br />
**Abstract:** <details><summary>原文: </summary>Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. Code and dataset are available at https://github.com/RingoWRW/UAVD4L</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管在没有 GPS 的环境中无人机 (UAV) 的全球定位取得了重大进展，但现有方法仍然受到数据集可用性的限制。当前的数据集通常专注于小规模场景，缺乏视点可变性、准确的地面实况 (GT) 姿态和无人机内置传感器数据。为了解决这些限制，我们引入了用于定位的大规模六自由度无人机数据集（UAVD4L），并开发了两阶段六自由度定位管道（UAVLoc），其中包括离线合成数据生成和在线视觉定位。此外，基于 6-DoF 估计器，我们设计了一个用于在 3D 空间中跟踪地面目标的分层系统。新数据集上的实验结果证明了所提出方法的有效性。代码和数据集可在 https://github.com/RingoWRW/UAVD4L 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.05971v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **LiDAR data acquisition and processing for ecology applications**<br />
**Title_cn:** 用于生态应用的激光雷达数据采集和处理<br />
**Authors:** Ion Ciobotari, Adriana Príncipe, Maria Alexandra Oliveira, João Nuno Silva<br />
**Abstract:** <details><summary>原文: </summary>The collection of ecological data in the field is essential to diagnose, monitor and manage ecosystems in a sustainable way. Since acquisition of this information through traditional methods are generally time-consuming, due to the capability of recording large volumes of data in short time periods, automation of data acquisition sees a growing trend. Terrestrial laser scanners (TLS), particularly LiDAR sensors, have been used in ecology, allowing to reconstruct the 3D structure of vegetation, and thus, infer ecosystem characteristics based on the spatial variation of the density of points. However, the low amount of information obtained per beam, lack of data analysis tools and the high cost of the equipment limit their use. This way, a low-cost TLS (<10k$) was developed along with data acquisition and processing mechanisms applicable in two case studies: an urban garden and a target area for ecological restoration. The orientation of LiDAR was modified to make observations in the vertical plane and a motor was integrated for its rotation, enabling the acquisition of 360 degree data with high resolution. Motion and location sensors were also integrated for automatic error correction and georeferencing. From the data generated, histograms of point density variation along the vegetation height were created, where shrub stratum was easily distinguishable from tree stratum, and maximum tree height and shrub cover were calculated. These results agreed with the field data, whereby the developed TLS has proved to be effective in calculating metrics of structural complexity of vegetation.</details>
**Abstract_cn:** <details><summary>译文: </summary>现场生态数据的收集对于以可持续的方式诊断、监测和管理生态系统至关重要。由于通过传统方法获取这些信息通常非常耗时，由于能够在短时间内记录大量数据，因此数据采集的自动化呈现出日益增长的趋势。地面激光扫描仪 (TLS)，特别是 LiDAR 传感器，已应用于生态学中，可以重建植被的 3D 结构，从而根据点密度的空间变化推断生态系统特征。然而，每束获得的信息量低、缺乏数据分析工具以及设备成本高限制了它们的使用。通过这种方式，开发了低成本 TLS（<10k 美元）以及适用于两个案例研究的数据采集和处理机制：城市花园和生态恢复目标区域。修改激光雷达的方向以在垂直平面内进行观测，并集成电机用于其旋转，从而能够采集高分辨率的 360 度数据。还集成了运动和位置传感器，用于自动纠错和地理配准。根据生成的数据，创建了沿植被高度的点密度变化直方图，其中灌木层与乔木层很容易区分，并计算了最大树木高度和灌木覆盖度。这些结果与现场数据一致，证明所开发的 TLS 在计算植被结构复杂性指标方面是有效的。</details>
**PDF:** <http://arxiv.org/pdf/2401.05891v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Distilling Vision-Language Models on Millions of Videos**<br />
**Title_cn:** 从数百万个视频中提取视觉语言模型<br />
**Authors:** Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型的最新进展很大程度上归功于丰富的图像文本数据。我们的目标是在视频语言模型中复制这一成功，但根本没有足够的人工管理视频文本数据可用。因此，我们依靠强大的图像语言基线和合成的教学数据来微调视频语言模型。然后使用生成的视频语言模型自动标记数百万个视频以生成高质量的字幕。我们展示了改编后的视频语言模型在各种视频语言基准上表现良好。例如，它比开放式 NExT-QA 的最佳先前结果高出 2.8%。此外，我们的模型为以前未见过的视频生成详细描述，这比现有方法提供更好的文本监督。实验表明，在这些自动生成的字幕上进行对比训练的视频语言双编码器模型比同样利用视觉语言模型的最强基线好 3.8%。我们最好的模型在 MSR-VTT 零样本文本到视频检索方面的性能比最先进的方法高出 6%。</details>
**PDF:** <http://arxiv.org/pdf/2401.06129v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ConKeD: Multiview contrastive descriptor learning for keypoint-based retinal image registration**<br />
**Title_cn:** ConKeD：基于关键点的视网膜图像配准的多视图对比描述符学习<br />
**Authors:** David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo<br />
**Abstract:** <details><summary>原文: </summary>Retinal image registration is of utmost importance due to its wide applications in medical practice. In this context, we propose ConKeD, a novel deep learning approach to learn descriptors for retinal image registration. In contrast to current registration methods, our approach employs a novel multi-positive multi-negative contrastive learning strategy that enables the utilization of additional information from the available training samples. This makes it possible to learn high quality descriptors from limited training data. To train and evaluate ConKeD, we combine these descriptors with domain-specific keypoints, particularly blood vessel bifurcations and crossovers, that are detected using a deep neural network. Our experimental results demonstrate the benefits of the novel multi-positive multi-negative strategy, as it outperforms the widely used triplet loss technique (single-positive and single-negative) as well as the single-positive multi-negative alternative. Additionally, the combination of ConKeD with the domain-specific keypoints produces comparable results to the state-of-the-art methods for retinal image registration, while offering important advantages such as avoiding pre-processing, utilizing fewer training samples, and requiring fewer detected keypoints, among others. Therefore, ConKeD shows a promising potential towards facilitating the development and application of deep learning-based methods for retinal image registration.</details>
**Abstract_cn:** <details><summary>译文: </summary>视网膜图像配准由于其在医疗实践中的广泛应用而变得至关重要。在这种背景下，我们提出了 ConKeD，一种新颖的深度学习方法，用于学习视网膜图像配准的描述符。与当前的配准方法相比，我们的方法采用了一种新颖的多正多负对比学习策略，可以利用可用训练样本中的附加信息。这使得从有限的训练数据中学习高质量的描述符成为可能。为了训练和评估 ConKeD，我们将这些描述符与使用深度神经网络检测到的特定领域关键点结合起来，特别是血管分叉和交叉。我们的实验结果证明了新颖的多正多负策略的好处，因为它优于广泛使用的三重损失技术（单正和单负）以及单正多负替代方案。此外，ConKeD 与特定领域关键点的结合产生了与最先进的视网膜图像配准方法相当的结果，同时提供了重要的优势，例如避免预处理、使用更少的训练样本以及需要更少的检测关键点等。因此，ConKeD 在促进基于深度学习的视网膜图像配准方法的开发和应用方面显示出巨大的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.05901v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing**<br />
**Title_cn:** 通过有效的组合正配对增强对比学习<br />
**Authors:** Jaeill Kim, Duhun Hwang, Eunjung Lee, Jangwon Suh, Jimyeong Kim, Wonjong Rhee<br />
**Abstract:** <details><summary>原文: </summary>In the past few years, contrastive learning has played a central role for the success of visual unsupervised representation learning. Around the same time, high-performance non-contrastive learning methods have been developed as well. While most of the works utilize only two views, we carefully review the existing multi-view methods and propose a general multi-view strategy that can improve learning speed and performance of any contrastive or non-contrastive method. We first analyze CMC's full-graph paradigm and empirically show that the learning speed of $K$-views can be increased by $_{K}\mathrm{C}_{2}$ times for small learning rate and early training. Then, we upgrade CMC's full-graph by mixing views created by a crop-only augmentation, adopting small-size views as in SwAV multi-crop, and modifying the negative sampling. The resulting multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness of ECPP by applying it to SimCLR and assessing the linear evaluation performance for CIFAR-10 and ImageNet-100. For each benchmark, we achieve a state-of-the-art performance. In case of ImageNet-100, ECPP boosted SimCLR outperforms supervised learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的几年中，对比学习对于视觉无监督表示学习的成功发挥了核心作用。大约在同一时间，高性能的非对比学习方法也被开发出来。虽然大多数作品仅使用两个视图，但我们仔细回顾了现有的多视图方法，并提出了一种通用的多视图策略，可以提高任何对比或非对比方法的学习速度和性能。我们首先分析了 CMC 的全图范式，并凭经验表明，对于小学习率和早期训练，$K$-views 的学习速度可以提高 $_{K}\mathrm{C}_{2}$ 倍。然后，我们通过混合仅裁剪增强创建的视图、采用 SwAV 多裁剪中的小尺寸视图以及修改负采样来升级 CMC 的全图。由此产生的多视图策略称为 ECPP（高效组合正配对）。我们通过将 ECPP 应用于 SimCLR 并评估 CIFAR-10 和 ImageNet-100 的线性评估性能来研究 ECPP 的有效性。对于每个基准测试，我们都实现了最先进的性能。就 ImageNet-100 而言，ECPP 提升的 SimCLR 优于监督学习。</details>
**PDF:** <http://arxiv.org/pdf/2401.05730v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Manipulating Feature Visualizations with Gradient Slingshots**<br />
**Title_cn:** 使用渐变弹弓操作特征可视化<br />
**Authors:** Dilyara Bareeva, Marina M. -C. Höhne, Alexander Warnecke, Lukas Pirch, Klaus-Robert Müller, Konrad Rieck, Kirill Bykov<br />
**Abstract:** <details><summary>原文: </summary>Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which substantiates our findings.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络（DNN）能够学习复杂且通用的表示，然而，所学习概念的语义本质仍然未知。用于解释 DNN 学习的概念的常用方法是激活最大化 (AM)，它生成一个合成输入信号，最大限度地激活网络中的特定神经元。在本文中，我们研究了这种方法对对抗性模型操作的脆弱性，并介绍了一种在不改变模型架构或显着影响模型决策过程的情况下操作特征可视化的新方法。我们评估了我们的方法在多个神经网络模型上的有效性，并通过在模型审核期间用选定的目标解释掩盖神经元的原始解释来展示其隐藏特定神经元功能的能力。作为补救措施，我们提出了针对此类操纵的保护措施，并提供了证实我们发现的定量证据。</details>
**PDF:** <http://arxiv.org/pdf/2401.06122v1><br />
**Code:** <https://github.com/dilyabareeva/grad-slingshot>**<br />
>>**index:** 2<br />
**Title:** **MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring**<br />
**Title_cn:** MGARD：用于高性能、错误控制数据压缩和重构的多重网格框架<br />
**Authors:** Qian Gong, Jieyang Chen, Ben Whitney, Xin Liang, Viktor Reshniak, Tania Banerjee, Jaemoon Lee, Anand Rangarajan, Lipeng Wan, Nicolas Vidal, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids. With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis. It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures. MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们描述了 MGARD，这是一款为结构化和非结构化网格上的浮点科学数据提供 MultiGrid Adaptive Reduction 的软件。凭借卓越的数据压缩能力和精确的错误控制，MGARD 满足了广泛的需求，包括存储减少、高性能 I/O 和现场数据分析。它具有统一的应用程序编程接口 (API)，可以跨不同的计算架构无缝运行。 MGARD 经过高度调优的 GPU 内核以及高效的内存和设备管理机制进行了优化，确保了可扩展和快速的操作。</details>
**PDF:** <http://arxiv.org/pdf/2401.05994v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **YOIO: You Only Iterate Once by mining and fusing multiple necessary global information in the optical flow estimation**<br />
**Title_cn:** YOIO：通过在光流估计中挖掘和融合多个必要的全局信息，您只需迭代一次<br />
**Authors:** Yu Jing, Tan Yujuan, Ren Ao, Liu Duo<br />
**Abstract:** <details><summary>原文: </summary>Occlusions pose a significant challenge to optical flow algorithms that even rely on global evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work only used the current frame as the only input, which could not guarantee providing correct global reference information for occluded points, and had problems such as long calculation time and poor accuracy in predicting optical flow at occluded points. To enable both high accuracy and efficiency, We fully mine and utilize the spatiotemporal information provided by the frame pair, design a loopback judgment algorithm to ensure that correct global reference information is obtained, mine multiple necessary global information, and design an efficient refinement module that fuses these global information. Specifically, we propose a YOIO framework, which consists of three main components: an initial flow estimator, a multiple global information extraction module, and a unified refinement module. We demonstrate that optical flow estimates in the occluded regions can be significantly improved in only one iteration without damaging the performance in non-occluded regions. Compared with GMA, the optical flow prediction accuracy of this method in the occluded area is improved by more than 10%, and the occ_out area exceeds 15%, while the calculation time is 27% shorter. This approach, running up to 18.9fps with 436*1024 image resolution, obtains new state-of-the-art results on the challenging Sintel dataset among all published and unpublished approaches that can run in real-time, suggesting a new paradigm for accurate and efficient optical flow estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>遮挡对甚至依赖全局证据的光流算法提出了重大挑战。我们认为遮挡点是在参考帧中成像但不在下一帧中成像的点。估计这些点的运动非常困难，特别是在两帧设置中。以往的工作仅以当前帧作为唯一输入，无法保证为遮挡点提供正确的全局参考信息，存在计算时间长、遮挡点光流预测精度差等问题。为了兼顾高精度和高效率，我们充分挖掘和利用帧对提供的时空信息，设计环回判断算法以确保获得正确的全局参考信息，挖掘多个必要的全局信息，并设计高效的细化模块融合这些全局信息。具体来说，我们提出了一个YOIO框架，它由三个主要组件组成：初始流量估计器、多个全局信息提取模块和统一细化模块。我们证明，遮挡区域中的光流估计只需一次迭代即可显着改善，而不会损害非遮挡区域的性能。与GMA相比，该方法在遮挡区域的光流预测精度提高了10%以上，occ_out区域超过15%，同时计算时间缩短了27%。这种方法的运行速度高达 18.9fps，图像分辨率为 436*1024，在所有已发表和未发表的可实时运行的方法中，在具有挑战性的 Sintel 数据集上获得了最新的结果，这为精确计算提供了一种新的范式。和高效的光流估计。</details>
**PDF:** <http://arxiv.org/pdf/2401.05879v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **On the representation and methodology for wide and short range head pose estimation**<br />
**Title_cn:** 宽短程头部姿态估计的表示和方法<br />
**Authors:** Alejandro Cobo, Roberto Valle, José M. Buenaposada, Luis Baumela<br />
**Abstract:** <details><summary>原文: </summary>Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360{\deg} rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles' gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LPBiwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set.</details>
**Abstract_cn:** <details><summary>译文: </summary>头部姿势估计（HPE）是计算机视觉中一个令人感兴趣的问题，旨在提高半正面或侧面设置中的面部处理任务的性能。最近的应用需要在整个 360{\deg} 旋转范围内分析面部。解决半正面和侧面情况的传统方法并不直接适用于全旋转情况。在本文中，我们分析了短期和广泛 HPE 的方法，并讨论了哪些表示和指标适合每种情况。我们表明，流行的欧拉角表示对于短程 HPE 是一个不错的选择，但不适用于极端旋转。然而，欧拉角的万向节锁定问题阻止它们在任何设置中用作有效的度量。我们还重新审视了当前的跨数据集评估方法，并注意到训练和测试数据集的参考系统之间缺乏一致性，从而对文献中所有文章的结果产生负面偏差。我们引入了量化这种偏差的程序以及跨数据集 HPE 的新方法，该方法为 300W-LPBiwi 基准建立了新的、更准确的 SOTA。我们还提出了测地角距离度量的推广，该度量能够构建控制每个训练样本对模型优化的贡献的损失。最后，我们介绍基于 CMU Panoptic 数据集的广泛 HPE 基准测试。</details>
**PDF:** <http://arxiv.org/pdf/2401.05807v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CLIP-Driven Semantic Discovery Network for Visible-Infrared Person Re-Identification**<br />
**Title_cn:** CLIP 驱动的语义发现网络，用于可见红外人员重新识别<br />
**Authors:** Xiaoyan Yu, Neng Dong, Liehuang Zhu, Hao Peng, Dapeng Tao<br />
**Abstract:** <details><summary>原文: </summary>Visible-infrared person re-identification (VIReID) primarily deals with matching identities across person images from different modalities. Due to the modality gap between visible and infrared images, cross-modality identity matching poses significant challenges. Recognizing that high-level semantics of pedestrian appearance, such as gender, shape, and clothing style, remain consistent across modalities, this paper intends to bridge the modality gap by infusing visual features with high-level semantics. Given the capability of CLIP to sense high-level semantic information corresponding to visual representations, we explore the application of CLIP within the domain of VIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network (CSDN) that consists of Modality-specific Prompt Learner, Semantic Information Integration (SII), and High-level Semantic Embedding (HSE). Specifically, considering the diversity stemming from modality discrepancies in language descriptions, we devise bimodal learnable text tokens to capture modality-private semantic information for visible and infrared images, respectively. Additionally, acknowledging the complementary nature of semantic details across different modalities, we integrate text features from the bimodal language descriptions to achieve comprehensive semantics. Finally, we establish a connection between the integrated text features and the visual features across modalities. This process embed rich high-level semantic information into visual representations, thereby promoting the modality invariance of visual representations. The effectiveness and superiority of our proposed CSDN over existing methods have been substantiated through experimental evaluations on multiple widely used benchmarks. The code will be released at \url{https://github.com/nengdong96/CSDN}.</details>
**Abstract_cn:** <details><summary>译文: </summary>可见红外人员重新识别（VIReID）主要处理来自不同模式的人员图像之间的身份匹配。由于可见光和红外图像之间的模态差距，跨模态身份匹配带来了重大挑战。认识到行人外观的高级语义（例如性别、形状和服装风格）在不同模态中保持一致，本文打算通过将视觉特征与高级语义相结合来弥合模态差距。鉴于 CLIP 能够感知与视觉表示相对应的高级语义信息，我们探索了 CLIP 在 VIReID 领域的应用。因此，我们提出了一种 CLIP 驱动的语义发现网络（CSDN），由特定模态提示学习器、语义信息集成（SII）和高级语义嵌入（HSE）组成。具体来说，考虑到语言描述中模态差异带来的多样性，我们设计了双模态可学习文本标记来分别捕获可见光和红外图像的模态私有语义信息。此外，认识到不同模态语义细节的互补性，我们整合双模态语言描述中的文本特征以实现全面的语义。最后，我们在集成的文本特征和跨模态的视觉特征之间建立联系。这个过程将丰富的高级语义信息嵌入到视觉表示中，从而促进视觉表示的模态不变性。我们提出的 CSDN 相对于现有方法的有效性和优越性已通过对多个广泛使用的基准的实验评估得到证实。代码将发布在\url{https://github.com/nengdong96/CSDN}。</details>
**PDF:** <http://arxiv.org/pdf/2401.05806v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Knowledge Translation: A New Pathway for Model Compression**<br />
**Title_cn:** 知识翻译：模型压缩的新途径<br />
**Authors:** Wujie Sun, Defang Chen, Jiawei Chen, Yan Feng, Chun Chen, Can Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategies to enhance model performance despite restricted training data, and successfully demonstrate the feasibility of KT on the MNIST dataset. Code is available at \url{https://github.com/zju-SWJ/KT}.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度学习取得了显着进步，但代价是训练、推理和模型存储开销增加。虽然现有的模型压缩方法努力在保持高精度的同时减少模型参数的数量，但它们不可避免地需要重新训练压缩模型或施加架构限制。为了克服这些限制，本文提出了一种新颖的框架，称为 \textbf{K}nowledge \textbf{T}translation (KT)，其中训练“翻译”模型以接收更大模型的参数并生成压缩的参数。 KT的概念受到语言翻译的启发，它有效地利用神经网络来转换不同的语言，并保持相同的含义。因此，我们探索神经网络转换不同大小模型的潜力，同时保留其功能。我们提出了一个全面的 KT 框架，引入数据增强策略来增强模型性能，尽管训练数据有限，并成功证明了 KT 在 MNIST 数据集上的可行性。代码可在 \url{https://github.com/zju-SWJ/KT} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.05772v1><br />
**Code:** <https://github.com/zju-swj/kt>**<br />
>>**index:** 7<br />
**Title:** **Learning Generalizable Models via Disentangling Spurious and Enhancing Potential Correlations**<br />
**Title_cn:** 通过消除虚假和增强潜在相关性来学习可推广模型<br />
**Authors:** Na Wang, Lei Qi, Jintao Guo, Yinghuan Shi, Yang Gao<br />
**Abstract:** <details><summary>原文: </summary>Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain. The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model. Adopting multiple perspectives, such as the sample and the feature, proves to be effective. The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features. In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations. 1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations. 2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model. The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG.</details>
**Abstract_cn:** <details><summary>译文: </summary>域泛化（DG）旨在在多个源域上训练模型，以确保它可以很好地泛化到任意未见过的目标域。域不变表示的获取对于 DG 至关重要，因为它们能够捕获数据的固有语义信息，减轻域转移的影响并增强模型的泛化能力。采用样本和特征等多个视角被证明是有效的。样本视角通过数据操作技术促进数据增强，而特征视角则能够提取有意义的泛化特征。在本文中，我们致力于通过解开虚假相关性并增强潜在相关性，迫使模型从样本和特征角度获取域不变表示，从而提高模型的泛化能力。 1）从样本角度，我们开发了频率限制模块，引导模型关注对象特征和标签之间的相关相关性，从而消除虚假相关性。 2）从特征角度来看，简单的尾部交互模块隐式增强了来自所有源域的所有样本之间的潜在相关性，有助于模型获取跨多个域的域不变表示。实验结果表明，嵌入这两个模块的具有强大基线的卷积神经网络（CNN）或多层感知器（MLP）可以取得优异的结果，例如在 Digits-DG 上的平均准确率达到 92.30%。</details>
**PDF:** <http://arxiv.org/pdf/2401.05752v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Self Expanding Convolutional Neural Networks**<br />
**Title_cn:** 自扩展卷积神经网络<br />
**Authors:** Blaise Appolinary, Alex Deaconu, Sophia Yang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a novel method for dynamically expanding Convolutional Neural Networks (CNNs) during training, aimed at meeting the increasing demand for efficient and sustainable deep learning models. Our approach, drawing from the seminal work on Self-Expanding Neural Networks (SENN), employs a natural expansion score as an expansion criteria to address the common issue of over-parameterization in deep convolutional neural networks, thereby ensuring that the model's complexity is finely tuned to the task's specific needs. A significant benefit of this method is its eco-friendly nature, as it obviates the necessity of training multiple models of different sizes. We employ a strategy where a single model is dynamically expanded, facilitating the extraction of checkpoints at various complexity levels, effectively reducing computational resource use and energy consumption while also expediting the development cycle by offering diverse model complexities from a single training session. We evaluate our method on the CIFAR-10 dataset and our experimental results validate this approach, demonstrating that dynamically adding layers not only maintains but also improves CNN performance, underscoring the effectiveness of our expansion criteria. This approach marks a considerable advancement in developing adaptive, scalable, and environmentally considerate neural network architectures, addressing key challenges in the field of deep learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种在训练期间动态扩展卷积神经网络（CNN）的新方法，旨在满足对高效和可持续深度学习模型日益增长的需求。我们的方法借鉴了自扩展神经网络（SENN）的开创性工作，采用自然扩展分数作为扩展标准来解决深度卷积神经网络中过度参数化的常见问题，从而确保模型的复杂性很好根据任务的具体需求进行调整。这种方法的一个显着好处是它的环保性质，因为它消除了训练不同大小的多个模型的必要性。我们采用动态扩展单个模型的策略，有助于提取各种复杂程度的检查点，有效减少计算资源使用和能源消耗，同时还通过从单个训练会话中提供不同的模型复杂性来加快开发周期。我们在 CIFAR-10 数据集上评估我们的方法，我们的实验结果验证了这种方法，证明动态添加层不仅可以保持而且还可以提高 CNN 性能，强调了我们扩展标准的有效性。这种方法标志着在开发自适应、可扩展和环境友好的神经网络架构方面取得了相当大的进步，解决了深度学习领域的关键挑战。</details>
**PDF:** <http://arxiv.org/pdf/2401.05686v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation**<br />
**Title_cn:** Parrot：用于文本到图像生成的帕累托最优多奖励强化学习框架<br />
**Authors:** Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recent works demonstrate that using reinforcement learning (RL) with quality rewards can enhance the quality of generated images in text-to-image (T2I) generation. However, a simple aggregation of multiple rewards may cause over-optimization in certain metrics and degradation in others, and it is challenging to manually find the optimal weights. An effective strategy to jointly optimize multiple rewards in RL for T2I generation is highly desirable. This paper introduces Parrot, a novel multi-reward RL framework for T2I generation. Through the use of the batch-wise Pareto optimal selection, Parrot automatically identifies the optimal trade-off among different rewards during the RL optimization of the T2I generation. Additionally, Parrot employs a joint optimization approach for the T2I model and the prompt expansion network, facilitating the generation of quality-aware text prompts, thus further enhancing the final image quality. To counteract the potential catastrophic forgetting of the original user prompt due to prompt expansion, we introduce original prompt centered guidance at inference time, ensuring that the generated image remains faithful to the user input. Extensive experiments and a user study demonstrate that Parrot outperforms several baseline methods across various quality criteria, including aesthetics, human preference, image sentiment, and text-image alignment.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的工作表明，使用具有质量奖励的强化学习（RL）可以提高文本到图像（T2I）生成中生成图像的质量。然而，多个奖励的简单聚合可能会导致某些指标的过度优化和其他指标的退化，并且手动找到最佳权重具有挑战性。非常需要一种有效的策略来联合优化 RL 中的多种奖励以生成 T2I。本文介绍了 Parrot，一种用于 T2I 生成的新型多奖励 RL 框架。通过使用批量 Pareto 最优选择，Parrot 在 T2I 一代的 RL 优化过程中自动识别不同奖励之间的最佳权衡。此外，Parrot对T2I模型和提示扩展网络采用联合优化方法，有助于生成质量感知的文本提示，从而进一步提高最终图像质量。为了抵消由于提示扩展而导致的对原始用户提示的潜在灾难性遗忘，我们在推理时引入了以原始提示为中心的指导，确保生成的图像保持忠实于用户输入。大量实验和用户研究表明，Parrot 在各种质量标准（包括美学、人类偏好、图像情感和文本图像对齐）方面均优于多种基线方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05675v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle Dynamics in Videos**<br />
**Title_cn:** Face-GPS：量化视频中面部肌肉动态的综合技术<br />
**Authors:** Juni Kim, Zhikang Dong, Pawel Polak<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel method that combines differential geometry, kernels smoothing, and spectral analysis to quantify facial muscle activity from widely accessible video recordings, such as those captured on personal smartphones. Our approach emphasizes practicality and accessibility. It has significant potential for applications in national security and plastic surgery. Additionally, it offers remote diagnosis and monitoring for medical conditions such as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept at detecting and classifying emotions, from the overt to the subtle. The proposed face muscle analysis technique is an explainable alternative to deep learning methods and a non-invasive substitute to facial electromyography (fEMG).</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种新颖的方法，该方法结合了微分几何、核平滑和光谱分析，从广泛访问的视频记录（例如个人智能手机上捕获的视频记录）中量化面部肌肉活动。我们的方法强调实用性和可访问性。它在国家安全和整形外科方面具有巨大的应用潜力。此外，它还提供中风、贝尔麻痹和听神经瘤等医疗状况的远程诊断和监测。此外，它还擅长检测和分类情绪，从明显的情绪到微妙的情绪。所提出的面部肌肉分析技术是深度学习方法的可解释替代方法，也是面部肌电图（fEMG）的非侵入性替代方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.05625v1><br />
**Code:** null<br />

