## [UPDATED!] **2024-01-12** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery**<br />
**Title_cn:** 透过树林看路：利用航空图像对空间依赖性进行建模的基准<br />
**Authors:** Caleb Robinson, Isaac Corley, Anthony Ortiz, Rahul Dodhia, Juan M. Lavista Ferres, Peyman Najafirad<br />
**Abstract:** <details><summary>原文: </summary>Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to segment roads from background in aerial imagery achieves an 84% recall on unoccluded roads, but just 63.5% recall on roads covered by tree canopy despite being trained to model both the same way. We further analyze how the performance of models changes as the relevant context for a decision (unoccluded roads in our case) varies in distance. We release the code to reproduce our experiments and dataset of imagery and masks to encourage future research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.</details>
**Abstract_cn:** <details><summary>译文: </summary>充分理解复杂的高分辨率卫星或航空图像场景通常需要在广泛的相关背景下进行空间推理。人体对象识别系统能够在远程相关上下文中理解场景中的对象。例如，如果一个人观察到一个显示道路部分被树冠破坏的空中场景，那么他们不太可能得出结论，道路实际上已被树木破坏成不相交的碎片，而是认为附近树木的树冠正在堵塞道路。然而，对于现代机器学习模型的远程上下文理解的研究还很有限。在这项工作中，我们提出了道路分割基准数据集切萨皮克道路空间上下文（RSC），用于评估地理空间机器学习模型的空间远程上下文理解，并展示常用的语义分割模型如何无法完成此任务。例如，我们表明，经过训练以从航空图像背景中分割道路的 U-Net 在畅通无阻的道路上实现了 84% 的召回率，但在被树冠覆盖的道路上只有 63.5% 的召回率，尽管经过训练以相同的方式建模。我们进一步分析模型的性能如何随着决策的相关背景（在我们的例子中是畅通的道路）距离的变化而变化。我们发布了代码来重现我们的实验以及图像和掩模数据集，以鼓励未来在这个方向上的研究 - https://github.com/isaaccorley/ChesapeakeRSC。</details>
**PDF:** <http://arxiv.org/pdf/2401.06762v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering**<br />
**Title_cn:** 具有超点图聚类的可扩展 3D 全景分割<br />
**Authors:** Damien Robert, Hugo Raguet, Loic Landrieu<br />
**Abstract:** <details><summary>原文: </summary>We introduce a highly efficient method for panoptic segmentation of large 3D point clouds by redefining this task as a scalable graph clustering problem. This approach can be trained using only local auxiliary tasks, thereby eliminating the resource-intensive instance-matching step during training. Moreover, our formulation can easily be adapted to the superpoint paradigm, further increasing its efficiency. This allows our model to process scenes with millions of points and thousands of objects in a single inference. Our method, called SuperCluster, achieves a new state-of-the-art panoptic segmentation performance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DIS Area~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the first state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and DALES. With only $209$k parameters, our model is over $30$ times smaller than the best-competing method and trains up to $15$ times faster. Our code and pretrained models are available at https://github.com/drprojects/superpoint_transformer.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们通过将此任务重新定义为可扩展的图聚类问题，引入了一种用于大型 3D 点云全景分割的高效方法。这种方法可以仅使用本地辅助任务进行训练，从而消除训练过程中资源密集型的实例匹配步骤。此外，我们的公式可以很容易地适应超点范式，进一步提高其效率。这使得我们的模型能够在一次推理中处理具有数百万个点和数千个对象的场景。我们的方法称为 SuperCluster，为两个室内扫描数据集实现了最先进的全景分割性能：S3DIS Area~5 为 50.1$ PQ ($+7.8$)，S3DIS Area~5 为 58.7$ PQ ($+25.2$)对于 ScanNetV2。我们还为两个大型移动测绘基准设置了首个最先进的基准：KITTI-360 和 DALES。我们的模型仅使用 209$k 美元的参数，比最佳竞争方法小 30 美元多，训练速度快 15 美元。我们的代码和预训练模型可在 https://github.com/drprojects/superpoint_transformer 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06704v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Embedded Planogram Compliance Control System**<br />
**Title_cn:** 嵌入式货架合规控制系统<br />
**Authors:** M. Erkin Yücel, Serkan Topaloğlu, Cem Ünsalan<br />
**Abstract:** <details><summary>原文: </summary>The retail sector presents several open and challenging problems that could benefit from advanced pattern recognition and computer vision techniques. One such critical challenge is planogram compliance control. In this study, we propose a complete embedded system to tackle this issue. Our system consists of four key components as image acquisition and transfer via stand-alone embedded camera module, object detection via computer vision and deep learning methods working on single board computers, planogram compliance control method again working on single board computers, and energy harvesting and power management block to accompany the embedded camera modules. The image acquisition and transfer block is implemented on the ESP-EYE camera module. The object detection block is based on YOLOv5 as the deep learning method and local feature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson Orin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogram compliance control block utilizes sequence alignment through a modified Needleman-Wunsch algorithm. This block is also working along with the object detection block on the same single board computers. The energy harvesting and power management block consists of solar and RF energy harvesting modules with suitable battery pack for operation. We tested the proposed embedded planogram compliance control system on two different datasets to provide valuable insights on its strengths and weaknesses. The results show that our method achieves F1 scores of 0.997 and 1.0 in object detection and planogram compliance control blocks, respectively. Furthermore, we calculated that the complete embedded system can work in stand-alone form up to two years based on battery. This duration can be further extended with the integration of the proposed solar and RF energy harvesting options.</details>
**Abstract_cn:** <details><summary>译文: </summary>零售业提出了一些开放且具有挑战性的问题，这些问题可以受益于先进的模式识别和计算机视觉技术。其中一项关键挑战是货架图合规性控制。在本研究中，我们提出了一个完整的嵌入式系统来解决这个问题。我们的系统由四个关键组件组成，即通过独立嵌入式摄像头模块进行图像采集和传输、通过计算机视觉进行物体检测和在单板计算机上工作的深度学习方法、再次在单板计算机上工作的货架图合规控制方法以及能量收集和电源管理块与嵌入式相机模块一起使用。图像采集和传输模块在 ESP-EYE 相机模块上实现。目标检测模块基于YOLOv5作为深度学习方法和局部特征提取。我们在 Raspberry Pi 4、NVIDIA Jetson Orin Nano 和 NVIDIA Jetson AGX Orin 作为单板计算机上实现了这些方法。货架图合规性控制块通过修改的 Needleman-Wunsch 算法利用序列对齐。该块还与同一单板计算机上的对象检测块一起工作。能量收集和电源管理模块由太阳能和射频能量收集模块组成，并配有合适的操作电池组。我们在两个不同的数据集上测试了拟议的嵌入式货架图合规控制系统，以提供有关其优点和缺点的宝贵见解。结果表明，我们的方法在对象检测和货架图合规性控制块中分别实现了 0.997 和 1.0 的 F1 分数。此外，我们计算出完整的嵌入式系统可以基于电池以独立形式工作长达两年。通过整合拟议的太阳能和射频能量收集选项，可以进一步延长这一持续时间。</details>
**PDF:** <http://arxiv.org/pdf/2401.06690v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Adversarial Examples are Misaligned in Diffusion Model Manifolds**<br />
**Title_cn:** 扩散模型流形中的对抗性示例未对齐<br />
**Authors:** Peter Lorenz, Ricard Durall, Jansi Keuper<br />
**Abstract:** <details><summary>原文: </summary>In recent years, diffusion models (DMs) have drawn significant attention for their success in approximating data distributions, yielding state-of-the-art generative results. Nevertheless, the versatility of these models extends beyond their generative capabilities to encompass various vision applications, such as image inpainting, segmentation, adversarial robustness, among others. This study is dedicated to the investigation of adversarial attacks through the lens of diffusion models. However, our objective does not involve enhancing the adversarial robustness of image classifiers. Instead, our focus lies in utilizing the diffusion model to detect and analyze the anomalies introduced by these attacks on images. To that end, we systematically examine the alignment of the distributions of adversarial examples when subjected to the process of transformation using diffusion models. The efficacy of this approach is assessed across CIFAR-10 and ImageNet datasets, including varying image sizes in the latter. The results demonstrate a notable capacity to discriminate effectively between benign and attacked images, providing compelling evidence that adversarial instances do not align with the learned manifold of the DMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，扩散模型（DM）因其在逼近数据分布、产生最先进的生成结果方面的成功而引起了广泛关注。然而，这些模型的多功能性超出了它们的生成能力，涵盖了各种视觉应用，例如图像修复、分割、对抗鲁棒性等。这项研究致力于通过扩散模型的视角来研究对抗性攻击。然而，我们的目标并不涉及增强图像分类器的对抗鲁棒性。相反，我们的重点在于利用扩散模型来检测和分析这些图像攻击所引入的异常。为此，我们系统地检查在使用扩散模型进行转换过程时对抗性示例的分布对齐情况。该方法的有效性在 CIFAR-10 和 ImageNet 数据集上进行评估，包括后者中不同的图像大小。结果表明，其具有有效区分良性图像和受攻击图像的显着能力，提供了令人信服的证据，证明对抗性实例与学习到的 DM 流形不一致。</details>
**PDF:** <http://arxiv.org/pdf/2401.06637v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking**<br />
**Title_cn:** Motion2VecSets：用于非刚性形状重建和跟踪的 4D 潜在向量集扩散<br />
**Authors:** Wei Cao, Chang Luo, Biao Zhang, Matthias Nießner, Jiapeng Tang<br />
**Abstract:** <details><summary>原文: </summary>We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based prior enables more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent vector sets instead of using a global latent. This novel 4D representation allows us to learn local surface shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporal-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid the computational overhead, we design an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against the state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations, notably achieving a 19% improvement in Intersection over Union (IoU) compared to CaDex for reconstructing unseen individuals from sparse point clouds on the DeformingThings4D-Animals dataset. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 Motion2VecSets，这是一种用于从点云序列进行动态表面重建的 4D 扩散模型。虽然现有最先进的方法已经证明使用神经场表示重建非刚性物体是成功的，但传统的前馈网络遇到了来自噪声、部分或稀疏点云的模糊观察的挑战。为了解决这些挑战，我们引入了一种扩散模型，该模型通过压缩潜在表示的迭代去噪过程显式地学习非刚性对象的形状和运动分布。在处理模糊输入时，基于扩散的先验可以实现更合理和概率性的重建。我们使用潜在向量集来参数化 4D 动力学，而不是使用全局潜在变量。这种新颖的 4D 表示使我们能够学习局部表面形状和变形模式，从而实现更准确的非线性运动捕捉，并显着提高对看不见的运动和身份的概括性。为了获得时间相干的对象跟踪，我们同步对变形潜在集进行去噪并跨多个帧交换信息。为了避免计算开销，我们设计了一个交错的空间和时间注意块，以沿着空间和时间域交替聚合潜在变形。与最先进方法的广泛比较证明了我们的 Motion2VecSets 在根据各种不完美观测进行 4D 重建方面的优越性，特别是与 CaDex 相比，在从稀疏点云重建看不见的个体时，交集比并集 (IoU) 提高了 19%在 DeformingThings4D-Animals 数据集上。更详细的信息可以在 https://vveicao.github.io/projects/Motion2VecSets/ 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.06614v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints**<br />
**Title_cn:** 具有强延迟约束的联结主义语音识别的动态行为<br />
**Authors:** Giampiero Salvi<br />
**Abstract:** <details><summary>原文: </summary>This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文描述了连接主义技术在具有强延迟约束的语音识别中的使用。这些约束是通过将语音字符串输入到发音合成器中，从语音信号中实时导出合成面部的嘴唇运动的任务所施加的。特别关注分析在不同延迟条件下多层感知器学习的时间演化模型与维特比解码器施加的转换模型之间的相互作用。进行了两个实验，其中语言模型 (LM) 中的时间依赖性由参数控制。结果表明，所涉及的三个因素之间存在很强的相互作用，即神经网络拓扑、LM 中的时间依赖性长度和解码器延迟。</details>
**PDF:** <http://arxiv.org/pdf/2401.06588v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Resource-Efficient Gesture Recognition using Low-Resolution Thermal Camera via Spiking Neural Networks and Sparse Segmentation**<br />
**Title_cn:** 通过尖峰神经网络和稀疏分割，使用低分辨率热像仪进行资源高效的手势识别<br />
**Authors:** Ali Safa, Wout Mommen, Lars Keuninckx<br />
**Abstract:** <details><summary>原文: </summary>This work proposes a novel approach for hand gesture recognition using an inexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking Neural Network (SNN) followed by Sparse Segmentation and feature-based gesture classification via Robust Principal Component Analysis (R-PCA). Compared to the use of standard RGB cameras, the proposed system is insensitive to lighting variations while being significantly less expensive compared to high-frequency radars, time-of-flight cameras and high-resolution thermal sensors previously used in literature. Crucially, this paper shows that the innovative use of the recently proposed Monostable Multivibrator (MMV) neural networks as a new class of SNN achieves more than one order of magnitude smaller memory and compute complexity compared to deep learning approaches, while reaching a top gesture recognition accuracy of 93.9% using a 5-class thermal camera dataset acquired in a car cabin, within an automotive context. Our dataset is released for helping future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出了一种新的手势识别方法，使用廉价、低分辨率 (24 x 32) 的热传感器，由尖峰神经网络 (SNN) 处理，然后通过鲁棒主成分分析 (R) 进行稀疏分割和基于特征的手势分类。 -PCA）。与使用标准 RGB 相机相比，所提出的系统对照明变化不敏感，同时与文献中先前使用的高频雷达、飞行时间相机和高分辨率热传感器相比，成本显着降低。至关重要的是，本文表明，与深度学习方法相比，创新地使用最近提出的单稳态多谐振荡器 (MMV) 神经网络作为一类新型 SNN，可实现比深度学习方法小一个数量级以上的内存和计算复杂性，同时达到顶级手势识别水平使用在汽车环境中的车厢内采集的 5 级热像仪数据集，准确率达到 93.9%。我们发布的数据集是为了帮助未来的研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.06563v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information**<br />
**Title_cn:** 利用遥感图像和多语义信息检测城市功能区的多模态学习<br />
**Authors:** Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Qiqi Zhu<br />
**Abstract:** <details><summary>原文: </summary>Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road nodes, human mobility, and logistics addresses to build a multimodal detection model based on transformer encoder-decoder architecture, titled AOITR. In the model, in addition to the remote sensing images, multi-semantic information including core POI and road nodes is embedded and reorganized as the query content part for the transformer decoder to generate the AOI polygon. Meanwhile, relatively dynamic distribution features of human mobility, nearby POIs, and logistics addresses are used for AOI reliability evaluation through a cascaded feedforward network. The experimental results demonstrate that our algorithm significantly outperforms two existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>城市兴趣区（AOI）是指具有明确边界的综合城市功能区。城市商业的快速发展对 AOI 的定义提出了更加精确的要求。然而，现有研究主要集中于城市规划或区域经济分析的广泛AOI挖掘，未能满足移动互联网线上线下业务的精准需求。这些业务需要精确到特定的社区、学校或医院。在本文中，我们提出了一种使用遥感图像和多语义参考信息检测 AOI 围栏多边形的端到端多模态深度学习算法。然后，我们通过包含动态人员流动和物流地址信息的级联模块评估其及时性。具体来说，我们首先选择特定类别的兴趣点（POI），并用它来调用相应的遥感图像、附近的 POI、道路节点、人员流动性和物流地址，构建基于 Transformer 的多模态检测模型编码器-解码器架构，名为 AOITR。模型中，除了遥感图像外，还嵌入并重组了包括核心POI和道路节点在内的多语义信息，作​​为Transformer解码器生成AOI多边形的查询内容部分。同时，通过级联前馈网络，利用人员流动性、附近 POI 和物流地址的相对动态分布特征来进行 AOI 可靠性评估。实验结果表明我们的算法明显优于两种现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.06550v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Enhancing Consistency and Mitigating Bias: A Data Replay Approach for Incremental Learning**<br />
**Title_cn:** 增强一致性并减少偏差：增量学习的数据重放方法<br />
**Authors:** Chenyang Wang, Junjun Jiang, Xingyu Hu, Xianming Liu, Xiangyang Ji<br />
**Abstract:** <details><summary>原文: </summary>Deep learning systems are prone to catastrophic forgetting when learning from a sequence of tasks, where old data from experienced tasks is unavailable when learning from a new task. To mitigate the problem, a line of methods propose to replay the data of experienced tasks when learning new tasks. These methods usually adopt an extra memory to store the data for replay. However, it is not expected in practice considering the memory constraint or data privacy issue. As a replacement, data-free data replay methods are proposed by inverting samples from the classification model. Though achieving good results, these methods still suffer from the inconsistency of the inverted and real training data, which is neglected in the inversion stage in recent works. To that effect, we propose to measure the data consistency quantitatively by some simplification and assumptions. Using the measurement, we analyze existing techniques for inverting samples and get some insightful information that inspires a novel loss function to reduce the inconsistency. Specifically, the loss minimizes the KL divergence of the distributions of inverted and real data under the tied multivariate Gaussian assumption, which is easy to implement in continual learning. In addition, we observe that the norms of old class weights turn to decrease continually as learning progresses. We thus analyze the underlying reasons and propose a simple regularization term to balance the class weights so that the samples of old classes are more distinguishable. To conclude, we propose the Consistency enhanced data replay with debiased classifier for Class Incremental Learning (CCIL). Extensive experiments on CIFAR-100, Tiny-ImageNet, and ImageNet100 show consistently improved performance of CCIL compared to previous approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习系统在从一系列任务中学习时很容易出现灾难性遗忘，在从新任务中学习时，无法获得来自经验丰富任务的旧数据。为了缓解这个问题，一系列方法建议在学习新任务时重放经验任务的数据。这些方法通常采用额外的存储器来存储回放数据。然而，考虑到内存限制或数据隐私问题，在实践中并不期望这样做。作为替代，通过反转分类模型中的样本提出了无数据数据重放方法。尽管取得了良好的效果，但这些方法仍然存在反演数据与真实训练数据不一致的问题，这一点在最近的工作中在反演阶段被忽略了。为此，我们建议通过一些简化和假设来定量测量数据一致性。使用测量结果，我们分析了现有的样本反演技术，并获得了一些富有洞察力的信息，这些信息激发了一种新颖的损失函数来减少不一致性。具体来说，该损失在绑定多元高斯假设下最小化了倒置数据和真实数据分布的 KL 散度，这在持续学习中很容易实现。此外，我们观察到，随着学习的进展，旧班级权重的标准不断下降。因此，我们分析了根本原因，并提出了一个简单的正则化项来平衡类别权重，以使旧类别的样本更具可区分性。总之，我们提出了用于类增量学习（CCIL）的带有去偏分类器的一致性增强数据重放。在 CIFAR-100、Tiny-ImageNet 和 ImageNet100 上进行的大量实验表明，与以前的方法相比，CCIL 的性能持续提高。</details>
**PDF:** <http://arxiv.org/pdf/2401.06548v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach**<br />
**Title_cn:** 优化带有噪声标签的二元分类的特征选择：遗传算法方法<br />
**Authors:** Vandad Imani, Elaheh Moradi, Carlos Sevilla-Salcedo, Vittorio Fortino, Jussi Tohka<br />
**Abstract:** <details><summary>原文: </summary>Feature selection in noisy label scenarios remains an understudied topic. We propose a novel genetic algorithm-based approach, the Noise-Aware Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting optimal feature subsets in binary classification with noisy labels. NMFS-GA offers a unified framework for selecting feature subsets that are both accurate and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise, a Breast Cancer dataset enriched with noisy features, and a real-world ADNI dataset for dementia conversion prediction. Our results indicate that NMFS-GA can effectively select feature subsets that improve the accuracy and interpretability of binary classifiers in scenarios with noisy labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>嘈杂标签场景中的特征选择仍然是一个未被充分研究的主题。我们提出了一种基于遗传算法的新颖方法，即噪声感知多目标特征选择遗传算法（NMFS-GA），用于在带有噪声标签的二元分类中选择最佳特征子集。 NMFS-GA 提供了一个统一的框架来选择准确且可解释的特征子集。我们在带有标签噪声的合成数据集、富含噪声特征的乳腺癌数据集以及用于痴呆转换预测的真实 ADNI 数据集上评估 NMFS-GA。我们的结果表明，NMFS-GA 可以有效地选择特征子集，从而提高带有噪声标签的场景中二元分类器的准确性和可解释性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06546v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook**<br />
**Title_cn:** 自动驾驶中的鲁棒性感知 3D 物体检测：回顾与展望<br />
**Authors:** Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, Caiyan Jia<br />
**Abstract:** <details><summary>原文: </summary>In the realm of modern autonomous driving, the perception system is indispensable for accurately assessing the state of the surrounding environment, thereby enabling informed prediction and planning. Key to this system is 3D object detection methods, that utilize vehicle-mounted sensors such as LiDAR and cameras to identify the size, category, and location of nearby objects. Despite the surge in 3D object detection methods aimed at enhancing detection precision and efficiency, there is a gap in the literature that systematically examines their resilience against environmental variations, noise, and weather changes. This study emphasizes the importance of robustness, alongside accuracy and latency, in evaluating perception systems under practical scenarios. Our work presents an extensive survey of camera-based, LiDAR-based, and multimodal 3D object detection algorithms, thoroughly evaluating their trade-off between accuracy, latency, and robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair comparisons. Among these,multimodal 3D detection approaches exhibit superior robustness and a novel taxonomy is introduced to reorganize its literature for enhanced clarity. This survey aims to offer a more practical perspective on the current capabilities and constraints of 3D object detection algorithms in real-world applications, thus steering future research towards robustness-centric advancements</details>
**Abstract_cn:** <details><summary>译文: </summary>在现代自动驾驶领域，感知系统对于准确评估周围环境的状态，从而实现明智的预测和规划是不可或缺的。该系统的关键是 3D 物体检测方法，利用激光雷达和摄像头等车载传感器来识别附近物体的大小、类别和位置。尽管旨在提高检测精度和效率的 3D 物体检测方法激增，但系统地检查其对环境变化、噪声和天气变化的适应能力的文献中仍存在空白。这项研究强调了在实际场景下评估感知系统时鲁棒性以及准确性和延迟的重要性。我们的工作对基于相机、基于 LiDAR 和多模态 3D 对象检测算法进行了广泛的调查，彻底评估了它们在准确性、延迟和鲁棒性之间的权衡，特别是在 KITTI-C 和 nuScenes-C 等数据集上，以确保公平比较。其中，多模态 3D 检测方法表现出卓越的鲁棒性，并引入了一种新颖的分类法来重新组织其文献以提高清晰度。本次调查旨在为现实应用中 3D 对象检测算法的当前功能和限制提供更实用的视角，从而引导未来的研究朝着以鲁棒性为中心的方向发展</details>
**PDF:** <http://arxiv.org/pdf/2401.06542v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Exploring Diverse Representations for Open Set Recognition**<br />
**Title_cn:** 探索开集识别的多样化表示<br />
**Authors:** Yu Wang, Junxian Mu, Pengfei Zhu, Qinghua Hu<br />
**Abstract:** <details><summary>原文: </summary>Open set recognition (OSR) requires the model to classify samples that belong to closed sets while rejecting unknown samples during test. Currently, generative models often perform better than discriminative models in OSR, but recent studies show that generative models may be computationally infeasible or unstable on complex tasks. In this paper, we provide insights into OSR and find that learning supplementary representations can theoretically reduce the open space risk. Based on the analysis, we propose a new model, namely Multi-Expert Diverse Attention Fusion (MEDAF), that learns diverse representations in a discriminative way. MEDAF consists of multiple experts that are learned with an attention diversity regularization term to ensure the attention maps are mutually different. The logits learned by each expert are adaptively fused and used to identify the unknowns through the score function. We show that the differences in attention maps can lead to diverse representations so that the fused representations can well handle the open space. Extensive experiments are conducted on standard and OSR large-scale benchmarks. Results show that the proposed discriminative method can outperform existing generative models by up to 9.5% on AUROC and achieve new state-of-the-art performance with little computational cost. Our method can also seamlessly integrate existing classification models. Code is available at https://github.com/Vanixxz/MEDAF.</details>
**Abstract_cn:** <details><summary>译文: </summary>开放集识别（OSR）要求模型对属于封闭集的样本进行分类，同时在测试过程中拒绝未知样本。目前，生成模型在 OSR 中通常比判别模型表现更好，但最近的研究表明，生成模型在复杂任务上可能在计算上不可行或不稳定。在本文中，我们提供了对 OSR 的见解，并发现学习补充表示理论上可以降低开放空间风险。基于分析，我们提出了一种新模型，即多专家多样化注意力融合（MEDAF），它以判别性的方式学习不同的表示。 MEDAF 由多个专家组成，这些专家通过注意力多样性正则化项进行学习，以确保注意力图相互不同。每个专家学习到的逻辑被自适应地融合并用于通过评分函数来识别未知数。我们表明，注意力图的差异可以导致不同的表示，以便融合的表示可以很好地处理开放空间。在标准和 OSR 大规模基准上进行了大量实验。结果表明，所提出的判别方法在 AUROC 上的性能比现有的生成模型高出 9.5%，并且以很少的计算成本实现了新的最先进的性能。我们的方法还可以无缝集成现有的分类模型。代码可在 https://github.com/Vanixxz/MEDAF 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06521v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Frequency Masking for Universal Deepfake Detection**<br />
**Title_cn:** 用于通用 Deepfake 检测的频率掩蔽<br />
**Authors:** Chandler Timm Doloriel, Ngai-Man Cheung<br />
**Abstract:** <details><summary>原文: </summary>We study universal deepfake detection. Our goal is to detect synthetic images from a range of generative AI approaches, particularly from emerging ones which are unseen during training of the deepfake detector. Universal deepfake detection requires outstanding generalization capability. Motivated by recently proposed masked image modeling which has demonstrated excellent generalization in self-supervised pre-training, we make the first attempt to explore masked image modeling for universal deepfake detection. We study spatial and frequency domain masking in training deepfake detectors. Based on empirical analysis, we propose a novel deepfake detector via frequency masking. Our focus on frequency domain is different from the majority, which primarily target spatial domain detection. Our comparative analyses reveal substantial performance gains over existing methods. Code and models are publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究通用深度伪造检测。我们的目标是从一系列生成人工智能方法中检测合成图像，特别是在深度伪造检测器训练期间看不见的新兴图像。通用的深度换脸检测需要出色的泛化能力。受最近提出的蒙版图像模型的启发，该模型在自监督预训练中表现出了出色的泛化能力，我们首次尝试探索用于通用深度伪造检测的蒙版图像建模。我们研究训练深度伪造探测器时的空间和频域掩蔽。基于实证分析，我们提出了一种通过频率掩蔽的新型深度伪造检测器。我们对频域的关注与大多数人不同，大多数人主要针对空间域检测。我们的比较分析表明，与现有方法相比，性能有了显着提升。代码和模型是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2401.06506v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Improving the Detection of Small Oriented Objects in Aerial Images**<br />
**Title_cn:** 改进航空图像中小定向物体的检测<br />
**Authors:** Chandler Timm C. Doloriel, Rhandley D. Cajote<br />
**Abstract:** <details><summary>原文: </summary>Small oriented objects that represent tiny pixel-area in large-scale aerial images are difficult to detect due to their size and orientation. Existing oriented aerial detectors have shown promising results but are mainly focused on orientation modeling with less regard to the size of the objects. In this work, we proposed a method to accurately detect small oriented objects in aerial images by enhancing the classification and regression tasks of the oriented object detection model. We designed the Attention-Points Network consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss (BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn the attention features needed to improve the detection of small objects. These attention features are then used to predict box points for BPLoss, which determines the points' position relative to the target oriented bounding box. Experimental results show the effectiveness of our Attention-Points Network on a standard oriented aerial dataset with small object instances (DOTA-v1.5) and on a maritime-related dataset (HRSC2016). The code is publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大规模航空图像中代表微小像素区域的小方向物体由于其尺寸和方向而难以检测。现有的定向空中探测器已显示出有希望的结果，但主要集中于方向建模，而较少考虑物体的大小。在这项工作中，我们提出了一种通过增强定向物体检测模型的分类和回归任务来准确检测航拍图像中的小定向物体的方法。我们设计的注意力点网络由两种损失组成：引导注意力损失（GALoss）和框点损失（BPLoss）。 GALoss 使用实例分割掩码作为基本事实来学习改进小物体检测所需的注意特征。然后，这些注意特征用于预测 BPLoss 的框点，从而确定点相对于面向目标的边界框的位置。实验结果表明，我们的注意力点网络在具有小对象实例的标准导向航空数据集 (DOTA-v1.5) 和海事相关数据集 (HRSC2016) 上的有效性。该代码是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2401.06503v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner UNet**<br />
**Title_cn:** 使用 Multiplanner UNet 对脑 MRI 数据进行全自动肿瘤分割<br />
**Authors:** Sumit Pandey, Satyasaran Changdar, Mathias Perslev, Erik B Dam<br />
**Abstract:** <details><summary>原文: </summary>Automated segmentation of distinct tumor regions is critical for accurate diagnosis and treatment planning in pediatric brain tumors. This study evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in segmenting different tumor subregions across three challenging datasets: Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse scenarios and anatomical variations, making them suitable for assessing the robustness and generalization capabilities of the MPUnet model. By utilizing multi-planar information, the MPUnet architecture aims to enhance segmentation accuracy. Our results show varying performance levels across the evaluated challenges, with the tumor core (TC) class demonstrating relatively higher segmentation accuracy. However, variability is observed in the segmentation of other classes, such as the edema and enhancing tumor (ET) regions. These findings emphasize the complexity of brain tumor segmentation and highlight the potential for further refinement of the MPUnet approach and inclusion of MRI more data and preprocessing.</details>
**Abstract_cn:** <details><summary>译文: </summary>不同肿瘤区域的自动分割对于儿科脑肿瘤的准确诊断和治疗计划至关重要。本研究评估了多规划器 U-Net (MPUnet) 方法在三个具有挑战性的数据集中分割不同肿瘤亚区域的功效：儿科肿瘤挑战 (PED)、脑转移挑战 (MET) 和撒哈拉以南非洲成人胶质瘤 ( SSA）。这些数据集代表了不同的场景和解剖变化，使其适合评估 MPUnet 模型的稳健性和泛化能力。通过利用多平面信息，MPUnet 架构旨在提高分割精度。我们的结果显示，在所评估的挑战中，性能水平各不相同，其中肿瘤核心 (TC) 类别表现出相对较高的分割精度。然而，在其他类别的分割中观察到变异性，例如水肿和增强肿瘤（ET）区域。这些发现强调了脑肿瘤分割的复杂性，并强调了进一步完善 MPUnet 方法以及纳入 MRI 更多数据和预处理的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.06499v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Self-supervised Learning of Dense Hierarchical Representations for Medical Image Segmentation**<br />
**Title_cn:** 医学图像分割的密集层次表示的自监督学习<br />
**Authors:** Eytan Kats, Jochen G. Hirsch, Mattias P. Heinrich<br />
**Abstract:** <details><summary>原文: </summary>This paper demonstrates a self-supervised framework for learning voxel-wise coarse-to-fine representations tailored for dense downstream tasks. Our approach stems from the observation that existing methods for hierarchical representation learning tend to prioritize global features over local features due to inherent architectural bias. To address this challenge, we devise a training strategy that balances the contributions of features from multiple scales, ensuring that the learned representations capture both coarse and fine-grained details. Our strategy incorporates 3-fold improvements: (1) local data augmentations, (2) a hierarchically balanced architecture, and (3) a hybrid contrastive-restorative loss function. We evaluate our method on CT and MRI data and demonstrate that our new approach particularly beneficial for fine-tuning with limited annotated data and consistently outperforms the baseline counterpart in linear evaluation settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文演示了一种自监督框架，用于学习针对密集下游任务定制的体素从粗到细的表示。我们的方法源于这样的观察：由于固有的架构偏差，现有的分层表示学习方法倾向于优先考虑全局特征而不是局部特征。为了应对这一挑战，我们设计了一种训练策略，平衡多个尺度特征的贡献，确保学习到的表示捕获粗粒度和细粒度的细节。我们的策略包含三重改进：(1) 本地数据增强，(2) 分层平衡架构，以及 (3) 混合对比恢复损失函数。我们在 CT 和 MRI 数据上评估我们的方法，并证明我们的新方法特别有利于使用有限的注释数据进行微调，并且在线性评估设置中始终优于基线对应方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.06473v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Improving Low-Light Image Recognition Performance Based on Image-adaptive Learnable Module**<br />
**Title_cn:** 基于图像自适应学习模块提高弱光图像识别性能<br />
**Authors:** Seitaro Ono, Yuka Ogino, Takahiro Toizumi, Atsushi Ito, Masato Tsukada<br />
**Abstract:** <details><summary>原文: </summary>In recent years, significant progress has been made in image recognition technology based on deep neural networks. However, improving recognition performance under low-light conditions remains a significant challenge. This study addresses the enhancement of recognition model performance in low-light conditions. We propose an image-adaptive learnable module which apply appropriate image processing on input images and a hyperparameter predictor to forecast optimal parameters used in the module. Our proposed approach allows for the enhancement of recognition performance under low-light conditions by easily integrating as a front-end filter without the need to retrain existing recognition models designed for low-light conditions. Through experiments, our proposed method demonstrates its contribution to enhancing image recognition performance under low-light conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，基于深度神经网络的图像识别技术取得了重大进展。然而，提高弱光条件下的识别性能仍然是一个重大挑战。这项研究致力于增强弱​​光条件下的识别模型性能。我们提出了一种图像自适应可学习模块，该模块对输入图像应用适当的图像处理，并使用超参数预测器来预测模块中使用的最佳参数。我们提出的方法可以通过轻松集成为前端滤波器来增强低光条件下的识别性能，而无需重新训练专为低光条件设计的现有识别模型。通过实验，我们提出的方法证明了其对增强弱光条件下图像识别性能的贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.06438v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding**<br />
**Title_cn:** UMG-CLIP：用于理解开放世界的统一多粒度视觉通才<br />
**Authors:** Bowen Shi, Peisen Zhao, Zichen Wang, Yuhang Zhang, Yaoming Wang, Jin Li, Wenrui Dai, Junni Zou, Hongkai Xiong, Qi Tian, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Vision-language foundation models, represented by Contrastive language-image pre-training (CLIP), have gained increasing attention for jointly understanding both vision and textual tasks. However, existing approaches primarily focus on training models to match global image representations with textual descriptions, thereby overlooking the critical alignment between local regions and corresponding text tokens. This paper extends CLIP with multi-granularity alignment. Notably, we deliberately construct a new dataset comprising pseudo annotations at various levels of granularities, encompassing image-level, region-level, and pixel-level captions/tags. Accordingly, we develop a unified multi-granularity learning framework, named UMG-CLIP, that simultaneously empowers the model with versatile perception abilities across different levels of detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses current widely used CLIP models and achieves state-of-the-art performance on diverse image understanding benchmarks, including open-world recognition, retrieval, semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP can serve as a valuable option for advancing vision-language foundation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>以对比语言图像预训练（CLIP）为代表的视觉语言基础模型由于共同理解视觉和文本任务而受到越来越多的关注。然而，现有的方法主要侧重于训练模型来将全局图像表示与文本描述相匹配，从而忽略了局部区域与相应文本标记之间的关键对齐。本文通过多粒度对齐扩展了 CLIP。值得注意的是，我们特意构建了一个新的数据集，其中包含各种粒度级别的伪注释，包括图像级、区域级和像素级标题/标签。因此，我们开发了一个统一的多粒度学习框架，名为 UMG-CLIP，它同时赋予模型跨不同细节级别的多功能感知能力。 UMG-CLIP 配备参数高效调整，超越了当前广泛使用的 CLIP 模型，并在各种图像理解基准上实现了最先进的性能，包括开放世界识别、检索、语义分割和全景分割任务。我们希望 UMG-CLIP 能够成为推进视觉语言基础模型的一个有价值的选择。</details>
**PDF:** <http://arxiv.org/pdf/2401.06397v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical Refinement and EM optimization**<br />
**Title_cn:** SD-MVS：具有球面细化和 EM 优化的分段驱动变形多视图立体<br />
**Authors:** Zhenlong Yuan, Jiakai Cao, Zhaoxin Li, Hao Jiang, Zhaoqi Wang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo (SD-MVS), a method that can effectively tackle challenges in 3D reconstruction of textureless areas. We are the first to adopt the Segment Anything Model (SAM) to distinguish semantic instances in scenes and further leverage these constraints for pixelwise patch deformation on both matching cost and propagation. Concurrently, we propose a unique refinement strategy that combines spherical coordinates and gradient descent on normals and pixelwise search interval on depths, significantly improving the completeness of reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM) algorithm to alternately optimize the aggregate matching cost and hyperparameters, effectively mitigating the problem of parameters being excessively dependent on empirical tuning. Evaluations on the ETH3D high-resolution multi-view stereo benchmark and the Tanks and Temples dataset demonstrate that our method can achieve state-of-the-art results with less time consumption.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了分段驱动变形多视图立体 (SD-MVS)，这是一种可以有效解决无纹理区域 3D 重建挑战的方法。我们是第一个采用分段任意模型（SAM）来区分场景中的语义实例，并进一步利用这些约束在匹配成本和传播上进行像素级补丁变形。同时，我们提出了一种独特的细化策略，结合了球坐标和法线梯度下降以及深度像素搜索间隔，显着提高了重建 3D 模型的完整性。此外，我们采用期望最大化（EM）算法来交替优化聚合匹配成本和超参数，有效缓解参数过度依赖经验调整的问题。对 ETH3D 高分辨率多视图立体基准和 Tanks and Temples 数据集的评估表明，我们的方法可以以更少的时间消耗实现最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.06385v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **SamLP: A Customized Segment Anything Model for License Plate Detection**<br />
**Title_cn:** SamLP：用于车牌检测的定制分段任意模型<br />
**Authors:** Haoxuan Ding, Junyu Gao, Yuan Yuan, Qi Wang<br />
**Abstract:** <details><summary>原文: </summary>With the emergence of foundation model, this novel paradigm of deep learning has encouraged many powerful achievements in natural language processing and computer vision. There are many advantages of foundation model, such as excellent feature extraction power, mighty generalization ability, great few-shot and zero-shot learning capacity, etc. which are beneficial to vision tasks. As the unique identity of vehicle, different countries and regions have diverse license plate (LP) styles and appearances, and even different types of vehicles have different LPs. However, recent deep learning based license plate detectors are mainly trained on specific datasets, and these limited datasets constrain the effectiveness and robustness of LP detectors. To alleviate the negative impact of limited data, an attempt to exploit the advantages of foundation model is implement in this paper. We customize a vision foundation model, i.e. Segment Anything Model (SAM), for LP detection task and propose the first LP detector based on vision foundation model, named SamLP. Specifically, we design a Low-Rank Adaptation (LoRA) fine-tuning strategy to inject extra parameters into SAM and transfer SAM into LP detection task. And then, we further propose a promptable fine-tuning step to provide SamLP with prompatable segmentation capacity. The experiments show that our proposed SamLP achieves promising detection performance compared to other LP detectors. Meanwhile, the proposed SamLP has great few-shot and zero-shot learning ability, which shows the potential of transferring vision foundation model. The code is available at https://github.com/Dinghaoxuan/SamLP</details>
**Abstract_cn:** <details><summary>译文: </summary>随着基础模型的出现，这种新颖的深度学习范式催生了自然语言处理和计算机视觉领域的许多强大成就。基础模型具有许多优点，例如优异的特征提取能力、强大的泛化能力、强大的少样本和零样本学习能力等，有利于视觉任务。作为车辆的唯一标识，不同国家和地区的车牌样式和外观各异，甚至不同类型的车辆也有不同的车牌。然而，最近基于深度学习的车牌检测器主要在特定数据集上进行训练，这些有限的数据集限制了车牌检测器的有效性和鲁棒性。为了减轻有限数据的负面影响，本文尝试利用基础模型的优势。我们为车牌检测任务定制了一个视觉基础模型，即Segment Anything Model (SAM)，并提出了第一个基于视觉基础模型的车牌检测器，命名为SamLP。具体来说，我们设计了一种低秩适应（LoRA）微调策略，将额外的参数注入到 SAM 中，并将 SAM 转移到 LP 检测任务中。然后，我们进一步提出了一个及时的微调步骤，为 SamLP 提供及时的分割能力。实验表明，与其他 LP 检测器相比，我们提出的 SamLP 实现了有希望的检测性能。同时，所提出的 SamLP 具有强大的少样本和零样本学习能力，这显示了迁移视觉基础模型的潜力。代码可在https://github.com/Dinghaoxuan/SamLP获取</details>
**PDF:** <http://arxiv.org/pdf/2401.06374v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Graph Relation Distillation for Efficient Biomedical Instance Segmentation**<br />
**Title_cn:** 用于高效生物医学实例分割的图关系蒸馏<br />
**Authors:** Xiaoyu Liu, Yueyi Zhang, Zhiwei Xiong, Wei Huang, Bo Hu, Xiaoyan Sun, Feng Wu<br />
**Abstract:** <details><summary>原文: </summary>Instance-aware embeddings predicted by deep neural networks have revolutionized biomedical instance segmentation, but its resource requirements are substantial. Knowledge distillation offers a solution by transferring distilled knowledge from heavy teacher networks to lightweight yet high-performance student networks. However, existing knowledge distillation methods struggle to extract knowledge for distinguishing instances and overlook global relation information. To address these challenges, we propose a graph relation distillation approach for efficient biomedical instance segmentation, which considers three essential types of knowledge: instance-level features, instance relations, and pixel-level boundaries. We introduce two graph distillation schemes deployed at both the intra-image level and the inter-image level: instance graph distillation (IGD) and affinity graph distillation (AGD). IGD constructs a graph representing instance features and relations, transferring these two types of knowledge by enforcing instance graph consistency. AGD constructs an affinity graph representing pixel relations to capture structured knowledge of instance boundaries, transferring boundary-related knowledge by ensuring pixel affinity consistency. Experimental results on a number of biomedical datasets validate the effectiveness of our approach, enabling student models with less than $ 1\%$ parameters and less than $10\%$ inference time while achieving promising performance compared to teacher models.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络预测的实例感知嵌入彻底改变了生物医学实例分割，但其资源需求巨大。知识蒸馏提供了一种解决方案，将蒸馏知识从繁重的教师网络转移到轻量级但高性能的学生网络。然而，现有的知识蒸馏方法难以提取用于区分实例的知识，并且忽略了全局关系信息。为了解决这些挑战，我们提出了一种用于高效生物医学实例分割的图关系蒸馏方法，该方法考虑了三种基本类型的知识：实例级特征、实例关系和像素级边界。我们介绍了在图像内级别和图像间级别部署的两种图蒸馏方案：实例图蒸馏（IGD）和亲和图蒸馏（AGD）。 IGD 构造一个表示实例特征和关系的图，通过强制实例图一致性来传递这两类知识。 AGD构建表示像素关系的亲和图来捕获实例边界的结构化知识，通过确保像素亲和一致性来传递边界相关知识。许多生物医学数据集的实验结果验证了我们方法的有效性，使学生模型具有少于 1\%$ 的参数和少于 10\%$ 的推理时间，同时与教师模型相比实现了有希望的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.06370v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **AffordanceLLM: Grounding Affordance from Vision Language Models**<br />
**Title_cn:** AffordanceLLM：视觉语言模型的基础可供性<br />
**Authors:** Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li<br />
**Abstract:** <details><summary>原文: </summary>Affordance grounding refers to the task of finding the area of an object with which one can interact. It is a fundamental but challenging task, as a successful solution requires the comprehensive understanding of a scene in multiple aspects including detection, localization, and recognition of objects with their parts, of geo-spatial configuration/layout of the scene, of 3D shapes and physics, as well as of the functionality and potential interaction of the objects and humans. Much of the knowledge is hidden and beyond the image content with the supervised labels from a limited training set. In this paper, we make an attempt to improve the generalization capability of the current affordance grounding by taking the advantage of the rich world, abstract, and human-object-interaction knowledge from pretrained large-scale vision language models. Under the AGD20K benchmark, our proposed model demonstrates a significant performance gain over the competing methods for in-the-wild object affordance grounding. We further demonstrate it can ground affordance for objects from random Internet images, even if both objects and actions are unseen during training. Project site: https://jasonqsy.github.io/AffordanceLLM/</details>
**Abstract_cn:** <details><summary>译文: </summary>可供性基础是指找到一个对象可以与之交互的区域的任务。这是一项基本但具有挑战性的任务，因为成功的解决方案需要从多个方面全面了解场景，包括对象及其部件的检测、定位和识别、场景的地理空间配置/布局、3D 形状和物理学，以及物体和人类的功能和潜在相互作用。许多知识是隐藏的，超出了来自有限训练集的监督标签的图像内容。在本文中，我们尝试利用预训练的大规模视觉语言模型中丰富的世界、抽象和人机交互知识来提高当前可供性基础的泛化能力。在 AGD20K 基准下，我们提出的模型在野外对象可供性接地方面表现出比竞争方法显着的性能增益。我们进一步证明，即使在训练期间看不到物体和动作，它也可以从随机互联网图像中获得物体的可供性。项目站点：https://jasonqsy.github.io/AffordanceLLM/</details>
**PDF:** <http://arxiv.org/pdf/2401.06341v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Mutual Distillation Learning For Person Re-Identification**<br />
**Title_cn:** 用于人员重新识别的相互蒸馏学习<br />
**Authors:** Huiyuan Fu, Kuilong Cui, Chuanming Wang, Mengshi Qi, Huadong Ma<br />
**Abstract:** <details><summary>原文: </summary>With the rapid advancements in deep learning technologies, person re-identification (ReID) has witnessed remarkable performance improvements. However, the majority of prior works have traditionally focused on solving the problem via extracting features solely from a single perspective, such as uniform partitioning, hard attention mechanisms, or semantic masks. While these approaches have demonstrated efficacy within specific contexts, they fall short in diverse situations. In this paper, we propose a novel approach, Mutual Distillation Learning For Person Re-identification (termed as MDPR), which addresses the challenging problem from multiple perspectives within a single unified model, leveraging the power of mutual distillation to enhance the feature representations collectively. Specifically, our approach encompasses two branches: a hard content branch to extract local features via a uniform horizontal partitioning strategy and a Soft Content Branch to dynamically distinguish between foreground and background and facilitate the extraction of multi-granularity features via a carefully designed attention mechanism. To facilitate knowledge exchange between these two branches, a mutual distillation and fusion process is employed, promoting the capability of the outputs of each branch. Extensive experiments are conducted on widely used person ReID datasets to validate the effectiveness and superiority of our approach. Notably, our method achieves an impressive $88.7\%/94.4\%$ in mAP/Rank-1 on the DukeMTMC-reID dataset, surpassing the current state-of-the-art results. Our source code is available at https://github.com/KuilongCui/MDPR.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着深度学习技术的快速发展，行人重新识别（ReID）的性能得到了显着的提高。然而，大多数先前的工作传统上都专注于通过仅从单一角度提取特征来解决问题，例如统一划分、硬注意力机制或语义掩码。虽然这些方法在特定情况下已证明有效，但在不同情况下却存在不足。在本文中，我们提出了一种新颖的方法，即行人重新识别的相互蒸馏学习（称为 MDPR），该方法在单个统一模型中从多个角度解决具有挑战性的问题，利用相互蒸馏的力量来共同增强特征表示。具体来说，我们的方法包含两个分支：硬内容分支通过统一的水平分区策略提取局部特征，软内容分支动态区分前景和背景，并通过精心设计的注意力机制促进多粒度特征的提取。为了促进这两个分支之间的知识交换，采用相互蒸馏和融合的过程，提高每个分支的输出能力。在广泛使用的人员 ReID 数据集上进行了大量的实验，以验证我们方法的有效性和优越性。值得注意的是，我们的方法在 DukeMTMC-reID 数据集上的 mAP/Rank-1 中取得了令人印象深刻的 $88.7\%/94.4\%$，超越了当前最先进的结果。我们的源代码可在 https://github.com/KuilongCui/MDPR 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06430v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer**<br />
**Title_cn:** UPDP：CNN 和 Vision Transformer 的统一渐进深度剪枝器<br />
**Authors:** Ji Liu, Dehua Tang, Yuanxian Huang, Li Zhang, Xiaocheng Zeng, Dong Li, Mingjie Lu, Jinzhang Peng, Yu Wang, Fan Jiang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 models with our method applying on ConvNeXtV1, which surpass most SOTA efficient models with comparable inference performance. Our method also achieves state-of-the-art pruning performance on the vision transformer model.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的通道修剪方法通过减少网络通道来有效地修剪具有深度卷积层和某些高效模块（例如流行的反向残差块）的高效 CNN 模型。由于一些归一化层的存在，先前通过减少网络深度的深度剪枝方法不适合剪枝一些高效模型。此外，通过直接删除激活层来微调子网会破坏原始模型权重，阻碍剪枝后的模型实现高性能。为了解决这些问题，我们提出了一种新的有效模型深度修剪方法。我们的方法提出了一种新颖的子网块修剪策略和渐进式训练方法。此外，我们将修剪方法扩展到视觉变换器模型。实验结果表明，我们的方法在各种修剪配置中始终优于现有的深度修剪方法。我们将我们的方法应用在 ConvNeXtV1 上，获得了三个剪枝的 ConvNeXtV1 模型，它们超越了具有可比推理性能的大多数 SOTA 高效模型。我们的方法还在视觉变换器模型上实现了最先进的修剪性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.06426v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks**<br />
**Title_cn:** 解耦像素翻转和遮挡策略以实现一致的 XAI 基准<br />
**Authors:** Stefan Blücher, Johanna Vielhaben, Nils Strodthoff<br />
**Abstract:** <details><summary>原文: </summary>Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement problem by grouping consistent PF rankings. Secondly, we show that the insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To leverage this, we combine the MIF and LIF measures into the symmetric relevance gain (SRG) measure. This breaks the inherent connection to the underlying occlusion strategy and leads to consistent rankings. This resolves the disagreement problem, which we verify for a set of 40 different occlusion strategies.</details>
**Abstract_cn:** <details><summary>译文: </summary>特征去除是 eXplainable AI (XAI) 的核心构建模块，既可用于基于遮挡的解释（Shapley 值），也可用于评估（像素翻转，PF）。然而，遮挡策略可能存在很大差异，从简单的均值替换到使用最先进的扩散模型进行修复。这种模糊性限制了基于遮挡的方法的实用性。例如，PF 基准会导致相互矛盾的排名。竞争性 PF 措施会放大这一点：功能要么从最有影响力的优先 (MIF) 开始，要么从最不影响力的优先 (LIF) 开始删除。本研究提出了两种互补的观点来解决这一分歧问题。首先，我们解决了基于遮挡的 XAI 的常见批评，即人工样本导致模型评估不可靠。我们建议通过 R（参考）-模型范围外 (OMS) 分数来衡量可靠性。 R-OMS 评分可以对遮挡策略进行系统比较，并通过对一致的 PF 排名进行分组来解决分歧问题。其次，我们表明 MIF 和 LIF 的洞察力反过来依赖于 R-OMS 分数。为了利用这一点，我们将 MIF 和 LIF 度量结合到对称相关增益 (SRG) 度量中。这打破了与底层遮挡策略的固有联系，并导致排名一致。这解决了不一致问题，我们针对 40 种不同的遮挡策略进行了验证。</details>
**PDF:** <http://arxiv.org/pdf/2401.06654v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model**<br />
**Title_cn:** 360DVD：具有 360 度视频扩散模型的可控全景视频生成<br />
**Authors:** Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, Jian Zhang<br />
**Abstract:** <details><summary>原文: </summary>360-degree panoramic videos recently attract more interest in both studies and applications, courtesy of the heightened immersive experiences they engender. Due to the expensive cost of capturing 360-degree panoramic videos, generating desirable panoramic videos by given prompts is urgently required. Recently, the emerging text-to-video (T2V) diffusion methods demonstrate notable effectiveness in standard video generation. However, due to the significant gap in content and motion patterns between panoramic and standard videos, these methods encounter challenges in yielding satisfactory 360-degree panoramic videos. In this paper, we propose a controllable panorama video generation pipeline named 360-Degree Video Diffusion model (360DVD) for generating panoramic videos based on the given prompts and motion conditions. Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted 360 Enhancement Techniques to transform pre-trained T2V models for 360-degree video generation. We further propose a new panorama dataset named WEB360 consisting of 360-degree video-text pairs for training 360DVD, addressing the absence of captioned panoramic video datasets. Extensive experiments demonstrate the superiority and effectiveness of 360DVD for panorama video generation. The code and dataset will be released soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>360 度全景视频最近吸引了更多研究和应用的兴趣，这得益于它们所带来的高度沉浸式体验。由于捕获360度全景视频的成本昂贵，迫切需要根据给定的提示生成理想的全景视频。最近，新兴的文本到视频（T2V）扩散方法在标准视频生成中表现出显着的有效性。然而，由于全景视频和标准视频在内容和运动模式上存在显着差距，这些方法在生成令人满意的 360 度全景视频方面遇到了挑战。在本文中，我们提出了一种名为 360 度视频扩散模型（360DVD）的可控全景视频生成管道，用于根据给定的提示和运动条件生成全景视频。具体来说，我们引入了一个名为 360-Adapter 的轻量级模块，并辅助 360 增强技术来转换预训练的 T2V 模型以生成 360 度视频。我们进一步提出了一个名为 WEB360 的新全景数据集，由 360 度视频-文本对组成，用于训练 360DVD，解决缺乏字幕的全景视频数据集的问题。大量的实验证明了 360DVD 在全景视频生成方面的优越性和有效性。代码和数据集即将发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.06578v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **RotationDrag: Point-based Image Editing with Rotated Diffusion Features**<br />
**Title_cn:** RotationDrag：具有旋转扩散功能的基于点的图像编辑<br />
**Authors:** Minxing Luo, Wentao Cheng, Jian Yang<br />
**Abstract:** <details><summary>原文: </summary>A precise and user-friendly manipulation of image content while preserving image fidelity has always been crucial to the field of image editing. Thanks to the power of generative models, recent point-based image editing methods allow users to interactively change the image content with high generalizability by clicking several control points. But the above mentioned editing process is usually based on the assumption that features stay constant in the motion supervision step from initial to target points. In this work, we conduct a comprehensive investigation in the feature space of diffusion models, and find that features change acutely under in-plane rotation. Based on this, we propose a novel approach named RotationDrag, which significantly improves point-based image editing performance when users intend to in-plane rotate the image content. Our method tracks handle points more precisely by utilizing the feature map of the rotated images, thus ensuring precise optimization and high image fidelity. Furthermore, we build a in-plane rotation focused benchmark called RotateBench, the first benchmark to evaluate the performance of point-based image editing method under in-plane rotation scenario on both real images and generated images. A thorough user study demonstrates the superior capability in accomplishing in-plane rotation that users intend to achieve, comparing the DragDiffusion baseline and other existing diffusion-based methods. See the project page https://github.com/Tony-Lowe/RotationDrag for code and experiment results.</details>
**Abstract_cn:** <details><summary>译文: </summary>在保持图像保真度的同时对图像内容进行精确且用户友好的操作一直是图像编辑领域的关键。得益于生成模型的强大功能，最近的基于点的图像编辑方法允许用户通过单击多个控制点以交互方式更改具有高通用性的图像内容。但上述编辑过程通常基于这样的假设：特征在从初始点到目标点的运动监督步骤中保持不变。在这项工作中，我们对扩散模型的特征空间进行了全面的研究，发现特征在面内旋转下发生剧烈变化。基于此，我们提出了一种名为 RotationDrag 的新颖方法，当用户打算平面内旋转图像内容时，它可以显着提高基于点的图像编辑性能。我们的方法利用旋转图像的特征图更精确地跟踪手柄点，从而确保精确的优化和高图像保真度。此外，我们建立了一个名为 RotateBench 的平面内旋转基准，这是第一个在真实图像和生成图像上评估平面内旋转场景下基于点的图像编辑方法性能的基准。彻底的用户研究通过比较 DragDiffusion 基线和其他现有的基于扩散的方法，证明了其在完成用户想要实现的平面内旋转方面的卓越能力。有关代码和实验结果，请参阅项目页面 https://github.com/Tony-Lowe/RotationDrag。</details>
**PDF:** <http://arxiv.org/pdf/2401.06442v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ModaVerse: Efficiently Transforming Modalities with LLMs**<br />
**Title_cn:** ModaVerse：利用法学硕士有效转变模式<br />
**Authors:** Xinyu Wang, Bohan Zhuang, Qi Wu<br />
**Abstract:** <details><summary>原文: </summary>Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM's output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类拥有理解不同模式并在它们之间无缝传输信息的能力。在这项工作中，我们介绍了 ModaVerse，一种多模态大语言模型 (MLLM)，能够跨各种模态（包括图像、视频和音频）理解和转换内容。主要的 MLLM 框架在很大程度上依赖于文本和非文本特征的潜在空间的对齐。这种对齐过程将文本数据训练的语言模型与多模态数据训练的编码器和解码器同步，通常需要在多个阶段对多个投影层进行广泛的训练。受法学硕士作为代理方法的启发，我们提出了一种直接在自然语言级别运行的新颖的输入/输出（I/O）对齐机制。它将 LLM 的输出与生成模型的输入保持一致，避免了与潜在特征对齐相关的复杂性，并将现有 MLLM 的多个训练阶段简化为单个高效的流程。这种概念上的进步导致数据和计算成本的显着降低。通过对多个基准进行实验，我们证明我们的方法获得了与最先进技术相当的性能，同时在数据使用和训练持续时间方面实现了相当高的效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.06395v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Seek for Incantations: Towards Accurate Text-to-Image Diffusion Synthesis through Prompt Engineering**<br />
**Title_cn:** 寻求咒语：通过快速工程实现准确的文本到图像扩散合成<br />
**Authors:** Chang Yu, Junran Peng, Xiangyu Zhu, Zhaoxiang Zhang, Qi Tian, Zhen Lei<br />
**Abstract:** <details><summary>原文: </summary>The text-to-image synthesis by diffusion models has recently shown remarkable performance in generating high-quality images. Although performs well for simple texts, the models may get confused when faced with complex texts that contain multiple objects or spatial relationships. To get the desired images, a feasible way is to manually adjust the textual descriptions, i.e., narrating the texts or adding some words, which is labor-consuming. In this paper, we propose a framework to learn the proper textual descriptions for diffusion models through prompt learning. By utilizing the quality guidance and the semantic guidance derived from the pre-trained diffusion model, our method can effectively learn the prompts to improve the matches between the input text and the generated images. Extensive experiments and analyses have validated the effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过扩散模型进行文本到图像的合成最近在生成高质量图像方面表现出了卓越的性能。尽管对于简单文本表现良好，但当面对包含多个对象或空间关系的复杂文本时，模型可能会感到困惑。为了得到想要的图像，一种可行的方法是手动调整文本描述，即对文本进行叙述或添加一些文字，这比较费力。在本文中，我们提出了一个框架，通过即时学习来学习扩散模型的正确文本描述。通过利用预训练扩散模型得出的质量指导和语义指导，我们的方法可以有效地学习提示，以提高输入文本和生成图像之间的匹配度。大量的实验和分析验证了所提出方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06345v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model**<br />
**Title_cn:** 通过具有大型语言模型的 QA 链将视觉问答从合成问题推广到人类编写的问题<br />
**Authors:** Taehee Kim, Yeongjae Cho, Heejun Shin, Yohan Jo, Dongmyung Shin<br />
**Abstract:** <details><summary>原文: </summary>Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no finetuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉问答（VQA）是一项给定图像并针对该图像提出一系列问题的任务。为了构建高效的VQA算法，需要大量的QA数据，这是非常昂贵的。基于模板生成合成问答对是一种获取数据的实用方法。然而，基于这些数据训练的 VQA 模型在复杂的、人工编写的问题上表现不佳。为了解决这个问题，我们提出了一种新方法，称为{\it chain of QA for human-writing questions} (CoQAH)。 CoQAH 利用大型语言模型和在合成数据上训练的 VQA 模型之间的一系列 QA 交互来推理并得出人类编写的问题的逻辑答案。我们在两种类型的人工编写的 VQA 数据集（针对 3D 渲染和胸部 X 射线图像）上测试了 CoQAH 的有效性，发现它在两种类型的数据中都实现了最先进的准确性。值得注意的是，CoQAH 的性能优于一般视觉语言模型、VQA 模型和无需微调的医学基础模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.06400v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning**<br />
**Title_cn:** Hyper-STTN：利用超图推理进行人体轨迹预测的社会群体感知时空变换器网络<br />
**Authors:** Weizheng Wang, Le Mao, Baijian Yang, Guohua Chen, Byung-Cheol Min<br />
**Abstract:** <details><summary>原文: </summary>Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer network. Hyper-STTN outperformes other state-of-the-art baselines and ablation models on 5 real-world pedestrian motion datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>预测拥挤的意图和轨迹对于各种现实应用（包括服务机器人和自动驾驶汽车）至关重要。理解环境动力学具有挑战性，不仅因为建模成对的空间和时间相互作用的复杂性，而且因为群体相互作用的不同影响。为了解码拥挤场景中全面的成对和成组交互​​，我们引入了 Hyper-STTN，一种基于超图的时空变换网络，用于人群轨迹预测。在 Hyper-STTN 中，拥挤的分组相关性是使用一组具有不同组大小的多尺度超图来构建的，这些超图是通过基于随机游走概率的超图谱卷积捕获的。此外，时空转换器适用于捕获行人在时空维度上的成对潜在交互。然后，这些异构组和成对通过多模态变压器网络进行融合和对齐。 Hyper-STTN 在 5 个真实行人运动数据集上的性能优于其他最先进的基线和消融模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.06344v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards**<br />
**Title_cn:** PCB-Vision：印刷电路板的多场景 RGB 高光谱基准数据集<br />
**Authors:** Elias Arbash, Margret Fuchs, Behnood Rasti, Sandra Lorenz, Pedram Ghamisi, Richard Gloaguen<br />
**Abstract:** <details><summary>原文: </summary>Addressing the critical theme of recycling electronic waste (E-waste), this contribution is dedicated to developing advanced automated data processing pipelines as a basis for decision-making and process control. Aligning with the broader goals of the circular economy and the United Nations (UN) Sustainable Development Goals (SDG), our work leverages non-invasive analysis methods utilizing RGB and hyperspectral imaging data to provide both quantitative and qualitative insights into the E-waste stream composition for optimizing recycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53 RGB images of high spatial resolution paired with their corresponding high spectral resolution hyperspectral data cubes in the visible and near-infrared (VNIR) range. Grounded in open science principles, our dataset provides a comprehensive resource for researchers through high-quality ground truths, focusing on three primary PCB components: integrated circuits (IC), capacitors, and connectors. We provide extensive statistical investigations on the proposed dataset together with the performance of several state-of-the-art (SOTA) models, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and DeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the baseline codes, we hope to foster transparent, traceable, and comparable developments of advanced data processing across various scientific communities, including, but not limited to, computer vision and remote sensing. Emphasizing our commitment to supporting a collaborative and inclusive scientific community, all materials, including code, data, ground truth, and masks, will be accessible at https://github.com/hifexplo/PCBVision.</details>
**Abstract_cn:** <details><summary>译文: </summary>针对回收电子废物（E-waste）的关键主题，该贡献致力于开发先进的自动化数据处理管道，作为决策和过程控制的基础。为了与循环经济和联合国 (UN) 可持续发展目标 (SDG) 的更广泛目标保持一致，我们的工作利用 RGB 和高光谱成像数据的非侵入性分析方法，提供有关电子废物流的定量和定性见解用于优化回收效率的组合物。在本文中，我们介绍了“PCB-Vision”；开创性的 RGB 高光谱印刷电路板 (PCB) 基准数据集，包含 53 个高空间分辨率的 RGB 图像及其相应的可见光和近红外 (VNIR) 范围内的高光谱分辨率高光谱数据立方体。我们的数据集以开放科学原则为基础，通过高质量的事实为研究人员提供了全面的资源，重点关注三种主要的 PCB 组件：集成电路 (IC)、电容器和连接器。我们对所提出的数据集以及几种最先进 (SOTA) 模型的性能进行了广泛的统计调查，包括 U-Net、Attention U-Net、Residual U-Net、LinkNet 和 DeepLabv3+。通过公开共享这个多场景基准数据集以及基线代码，我们希望促进各个科学界（包括但不限于计算机视觉和遥感）先进数据处理的透明、可追溯和可比较的发展。为了强调我们对支持协作和包容性科学界的承诺，所有材料，包括代码、数据、基本事实和掩模，都可以在 https://github.com/hifexplo/PCBVision 上访问。</details>
**PDF:** <http://arxiv.org/pdf/2401.06528v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MedTransformer: Accurate AD Diagnosis for 3D MRI Images through 2D Vision Transformers**<br />
**Title_cn:** MedTransformer：通过 2D Vision Transformer 对 3D MRI 图像进行准确的 AD 诊断<br />
**Authors:** Yifeng Wang, Ke Chen, Yihan Zhang, Haohan Wang<br />
**Abstract:** <details><summary>原文: </summary>Automated diagnosis of AD in brain images is becoming a clinically important technique to support precision and efficient diagnosis and treatment planning. A few efforts have been made to automatically diagnose AD in magnetic resonance imaging (MRI) using three-dimensional CNNs. However, due to the complexity of 3D models, the performance is still unsatisfactory, both in terms of accuracy and efficiency. To overcome the complexities of 3D images and 3D models, in this study, we aim to attack this problem with 2D vision Transformers. We propose a 2D transformer-based medical image model with various transformer attention encoders to diagnose AD in 3D MRI images, by cutting the 3D images into multiple 2D slices.The model consists of four main components: shared encoders across three dimensions, dimension-specific encoders, attention across images from the same dimension, and attention across three dimensions. It is used to obtain attention relationships among multiple sequences from different dimensions (axial, coronal, and sagittal) and multiple slices. We also propose morphology augmentation, an erosion and dilation based method to increase the structural difference between AD and normal images. In this experiment, we use multiple datasets from ADNI, AIBL, MIRAID, OASIS to show the performance of our model. Our proposed MedTransformer demonstrates a strong ability in diagnosing AD. These results demonstrate the effectiveness of MedTransformer in learning from 3D data using a much smaller model and its capability to generalize among different medical tasks, which provides a possibility to help doctors diagnose AD in a simpler way.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑图像中 AD 的自动诊断正在成为支持精确、高效的诊断和治疗计划的临床重要技术。使用三维 CNN 在磁共振成像 (MRI) 中自动诊断 AD 已经做出了一些努力。然而，由于3D模型的复杂性，无论是在精度还是效率上，表现仍然不尽如人意。为了克服 3D 图像和 3D 模型的复杂性，在本研究中，我们的目标是使用 2D 视觉 Transformer 来解决这个问题。我们提出了一种基于 2D Transformer 的医学图像模型，具有各种 Transformer 注意编码器，通过将 3D 图像切割成多个 2D 切片来诊断 3D MRI 图像中的 AD。该模型由四个主要部分组成：跨三个维度的共享编码器、特定于维度的编码器编码器、同一维度图像的注意力以及跨三个维度的注意力。它用于获取不同维度（轴向、冠状和矢状）和多个切片的多个序列之间的注意力关系。我们还提出了形态增强，一种基于侵蚀和膨胀的方法，以增加 AD 和正常图像之间的结构差异。在本实验中，我们使用 ADNI、AIBL、MIRAID、OASIS 的多个数据集来展示我们模型的性能。我们提出的 MedTransformer 展示了诊断 AD 的强大能力。这些结果证明了 MedTransformer 使用更小的模型从 3D 数据中学习的有效性及其在不同医疗任务之间进行泛化的能力，这为帮助医生以更简单的方式诊断 AD 提供了可能性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06349v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention**<br />
**Title_cn:** 具有屏蔽帧间和帧内注意力的视频超分辨率变压器<br />
**Authors:** Xingyu Zhou, Leheng Zhang, Xiaorui Zhao, Keze Wang, Leida Li, Shuhang Gu<br />
**Abstract:** <details><summary>原文: </summary>Recently, Vision Transformer has achieved great success in recovering missing details in low-resolution sequences, i.e., the video super-resolution (VSR) task.Despite its superiority in VSR accuracy, the heavy computational burden as well as the large memory footprint hinder the deployment of Transformer-based VSR models on constrained devices.In this paper, we address the above issue by proposing a novel feature-level masked processing framework: VSR with Masked Intra and inter frame Attention (MIA-VSR).The core of MIA-VSR is leveraging feature-level temporal continuity between adjacent frames to reduce redundant computations and make more rational use of previously enhanced SR features. Concretely, we propose an intra-frame and inter-frame attention block which takes the respective roles of past features and input features into consideration and only exploits previously enhanced features to provide supplementary information. In addition, an adaptive block-wise mask prediction module is developed to skip unimportant computations according to feature similarity between adjacent frames. We conduct detailed ablation studies to validate our contributions and compare the proposed method with recent state-of-the-art VSR approaches. The experimental results demonstrate that MIA-VSR improves the memory and computation efficiency over state-of-the-art methods, without trading off PSNR accuracy. The code is available at https://github.com/LabShuHangGU/MIA-VSR.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，Vision Transformer 在恢复低分辨率序列中丢失的细节，即视频超分辨率（VSR）任务方面取得了巨大成功。尽管其在 VSR 精度方面具有优势，但繁重的计算负担以及较大的内存占用阻碍了该任务的实现。在受限设备上部署基于 Transformer 的 VSR 模型。在本文中，我们通过提出一种新颖的特征级屏蔽处理框架来解决上述问题：具有屏蔽帧内和帧间注意的 VSR（MIA-VSR）。MIA 的核心 - VSR 正在利用相邻帧之间的特征级时间连续性来减少冗余计算并更合理地利用先前增强的 SR 特征。具体来说，我们提出了一种帧内和帧间注意块，它考虑了过去特征和输入特征各自的作用，并且仅利用先前增强的特征来提供补充信息。此外，还开发了自适应逐块掩模预测模块，以根据相邻帧之间的特征相似性跳过不重要的计算。我们进行了详细的消融研究来验证我们的贡献，并将所提出的方法与最近最先进的 VSR 方法进行比较。实验结果表明，与最先进的方法相比，MIA-VSR 提高了内存和计算效率，而无需牺牲 PSNR 精度。代码可在 https://github.com/LabShuHangGU/MIA-VSR 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06312v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image**<br />
**Title_cn:** 从单个图像重建多人服装交互的 3D 重建<br />
**Authors:** Junuk Cha, Hansol Lee, Jaewon Kim, Nhat Nguyen Bao Truong, Jae Shin Yoon, Seungryul Baek<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel pipeline to reconstruct the geometry of interacting multi-person in clothing on a globally coherent scene space from a single image. The main challenge arises from the occlusion: a part of a human body is not visible from a single view due to the occlusion by others or the self, which introduces missing geometry and physical implausibility (e.g., penetration). We overcome this challenge by utilizing two human priors for complete 3D geometry and surface contacts. For the geometry prior, an encoder learns to regress the image of a person with missing body parts to the latent vectors; a decoder decodes these vectors to produce 3D features of the associated geometry; and an implicit network combines these features with a surface normal map to reconstruct a complete and detailed 3D humans. For the contact prior, we develop an image-space contact detector that outputs a probability distribution of surface contacts between people in 3D. We use these priors to globally refine the body poses, enabling the penetration-free and accurate reconstruction of interacting multi-person in clothing on the scene space. The results demonstrate that our method is complete, globally coherent, and physically plausible compared to existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种新颖的管道，可以根据单个图像在全局连贯的场景空间上重建穿着服装的多人交互的几何形状。主要的挑战来自于遮挡：由于他人或自身的遮挡，人体的一部分从单一视图中不可见，这导致了几何形状的缺失和物理上的不可信性（例如，穿透）。我们利用两个人类先验知识来实现​​完整的 3D 几何和表面接触，从而克服了这一挑战。对于几何先验，编码器学习将缺少身体部位的人的图像回归到潜在向量；解码器对这些向量进行解码以生成相关几何形状的 3D 特征；隐式网络将这些特征与表面法线贴图结合起来，重建完整且详细的 3D 人体。对于接触先验，我们开发了一种图像空间接触检测器，可输出 3D 中人与人之间表面接触的概率分布。我们使用这些先验来全局细化身体姿势，从而能够在场景空间中无渗透且准确地重建穿着衣服的互动多人。结果表明，与现有方法相比，我们的方法是完整的、全局一致的并且物理上合理的。</details>
**PDF:** <http://arxiv.org/pdf/2401.06415v1><br />
**Code:** null<br />

>## **GNN**
>---
>>**index:** 1<br />
**Title:** **Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction**<br />
**Title_cn:** 用于行人意图预测的综合数据生成框架、数据集和高效深度模型<br />
**Authors:** Muhammad Naveed Riaz, Maciej Wielgosz, Abel Garcia Romera, Antonio M. Lopez<br />
**Abstract:** <details><summary>原文: </summary>Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to predict crossing intentions.</details>
**Abstract_cn:** <details><summary>译文: </summary>行人意图预测对于自动驾驶至关重要。特别是，了解行人是否要在自我车辆前面过马路是执行安全和舒适操作的核心。创建准确且快速的模型来根据连续图像预测此类意图具有挑战性。造成这种情况的一个因素是缺乏具有不同交叉和非交叉（C/NC）场景的数据集。我们通过引入一个名为 ARCANE 的框架来解决这一稀缺问题，该框架允许以编程方式生成由 C/NC 视频剪辑样本组成的合成数据集。例如，我们使用 ARCANE 生成一个名为 PedSynth 的大型且多样化的数据集。我们将展示 PedSynth 如何补充广泛使用的现实世界数据集（例如 JAAD 和 PIE），从而为 C/NC 预测提供更准确的模型。考虑到 C/NC 预测模型的机载部署，我们还提出了一种名为 PedGNN 的深度模型，该模型速度快且内存占用非常低。 PedGNN 基于 GNN-GRU 架构，该架构采用行人骨架序列作为输入来预测交叉口意图。</details>
**PDF:** <http://arxiv.org/pdf/2401.06757v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **AttributionScanner: A Visual Analytics System for Metadata-Free Data-Slicing Based Model Validation**<br />
**Title_cn:** AttributionScanner：用于基于无元数据数据切片的模型验证的可视化分析系统<br />
**Authors:** Xiwei Xuan, Jorge Piazentin Ono, Liang Gou, Kwan-Liu Ma, Liu Ren<br />
**Abstract:** <details><summary>原文: </summary>Data slice-finding is an emerging technique for evaluating machine learning models. It works by identifying subgroups within a specified dataset that exhibit poor performance, often defined by distinct feature sets or meta-information. However, in the context of unstructured image data, data slice-finding poses two notable challenges: it requires additional metadata -- a laborious and costly requirement, and also demands non-trivial efforts for interpreting the root causes of the underperformance within data slices. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for data-slicing-based machine learning (ML) model validation. Our approach excels in identifying interpretable data slices, employing explainable features extracted through the lens of Explainable AI (XAI) techniques, and removing the necessity for additional metadata of textual annotations or cross-model embeddings. AttributionScanner demonstrates proficiency in pinpointing critical model issues, including spurious correlations and mislabeled data. Our novel VA interface visually summarizes data slices, enabling users to gather insights into model behavior patterns effortlessly. Furthermore, our framework closes the ML Development Cycle by empowering domain experts to address model issues by using a cutting-edge neural network regularization technique. The efficacy of AttributionScanner is underscored through two prototype use cases, elucidating its substantial effectiveness in model validation for vision-centric tasks. Our approach paves the way for ML researchers and practitioners to drive interpretable model validation in a data-efficient way, ultimately leading to more reliable and accurate models.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据切片查找是一种用于评估机器学习模型的新兴技术。它的工作原理是识别指定数据集中性能较差的子组，这些子组通常由不同的特征集或元信息定义。然而，在非结构化图像数据的背景下，数据切片查找提出了两个显着的挑战：它需要额外的元数据——这是一项费力且昂贵的要求，并且还需要付出巨大的努力来解释数据切片内性能不佳的根本原因。为了应对这些挑战，我们推出了 AttributionScanner，这是一种创新的人机交互可视化分析 (VA) 系统，专为基于数据切片的机器学习 (ML) 模型验证而设计。我们的方法擅长识别可解释的数据切片，采用通过可解释的人工智能（XAI）技术提取的可解释的特征，并消除文本注释或跨模型嵌入的额外元数据的必要性。 AttributionScanner 能够熟练地查明关键模型问题，包括虚假相关性和错误标记的数据。我们新颖的 VA 界面直观地总结了数据切片，使用户能够轻松收集对模型行为模式的见解。此外，我们的框架通过授权领域专家使用尖端的神经网络正则化技术来解决模型问题，从而结束了机器学习开发周期。通过两个原型用例强调了 AttributionScanner 的功效，阐明了其在以视觉为中心的任务的模型验证中的实质性有效性。我们的方法为机器学习研究人员和从业者以数据高效的方式推动可解释的模型验证铺平了道路，最终导致更可靠和准确的模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.06462v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **UAV-borne Mapping Algorithms for Canopy-Level and High-Speed Drone Applications**<br />
**Title_cn:** 适用于冠层级和高速无人机应用的无人机载测绘算法<br />
**Authors:** Jincheng Zhang, Artur Wolek, Andrew R. Willis<br />
**Abstract:** <details><summary>原文: </summary>This article presents a comprehensive review of and analysis of state-of-the-art mapping algorithms for UAV (Unmanned Aerial Vehicle) applications, focusing on canopy-level and high-speed scenarios. This article presents a comprehensive exploration of sensor technologies suitable for UAV mapping, assessing their capabilities to provide measurements that meet the requirements of fast UAV mapping. Furthermore, the study conducts extensive experiments in a simulated environment to evaluate the performance of three distinct mapping algorithms: Direct Sparse Odometry (DSO), Stereo DSO (SDSO), and DSO Lite (DSOL). The experiments delve into mapping accuracy and mapping speed, providing valuable insights into the strengths and limitations of each algorithm. The results highlight the versatility and shortcomings of these algorithms in meeting the demands of modern UAV applications. The findings contribute to a nuanced understanding of UAV mapping dynamics, emphasizing their applicability in complex environments and high-speed scenarios. This research not only serves as a benchmark for mapping algorithm comparisons but also offers practical guidance for selecting sensors tailored to specific UAV mapping applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文对 UAV（无人机）应用的最先进测绘算法进行了全面回顾和分析，重点关注冠层级和高速场景。本文对适用于无人机测绘的传感器技术进行了全面的探索，评估了它们提供满足快速无人机测绘要求的测量的能力。此外，该研究在模拟环境中进行了大量实验，以评估三种不同映射算法的性能：直接稀疏里程计 (DSO)、立体声 DSO (SDSO) 和 DSO Lite (DSOL)。这些实验深入研究了映射精度和映射速度，为了解每种算法的优点和局限性提供了宝贵的见解。结果凸显了这些算法在满足现代无人机应用需求方面的多功能性和缺点。这些发现有助于对无人机测绘动态的细致了解，强调其在复杂环境和高速场景中的适用性。这项研究不仅可以作为测绘算法比较的基准，还可以为选择适合特定无人机测绘应用的传感器提供实用指导。</details>
**PDF:** <http://arxiv.org/pdf/2401.06407v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Application Of Vision-Language Models For Assessing Osteoarthritis Disease Severity**<br />
**Title_cn:** 视觉语言模型在评估骨关节炎疾病严重程度中的应用<br />
**Authors:** Banafshe Felfeliyan, Yuyue Zhou, Shrimanti Ghosh, Jessica Kupper, Shaobo Liu, Abhilash Hareendranathan, Jacob L. Jaremko<br />
**Abstract:** <details><summary>原文: </summary>Osteoarthritis (OA) poses a global health challenge, demanding precise diagnostic methods. Current radiographic assessments are time consuming and prone to variability, prompting the need for automated solutions. The existing deep learning models for OA assessment are unimodal single task systems and they don't incorporate relevant text information such as patient demographics, disease history, or physician reports. This study investigates employing Vision Language Processing (VLP) models to predict OA severity using Xray images and corresponding reports. Our method leverages Xray images of the knee and diverse report templates generated from tabular OA scoring values to train a CLIP (Contrastive Language Image PreTraining) style VLP model. Furthermore, we incorporate additional contrasting captions to enforce the model to discriminate between positive and negative reports. Results demonstrate the efficacy of these models in learning text image representations and their contextual relationships, showcase potential advancement in OA assessment, and establish a foundation for specialized vision language models in medical contexts.</details>
**Abstract_cn:** <details><summary>译文: </summary>骨关节炎 (OA) 构成了全球健康挑战，需要精确的诊断方法。目前的射线照相评估非常耗时且容易出现变化，因此需要自动化解决方案。现有的 OA 评估深度学习模型是单模态单任务系统，它们不包含相关的文本信息，例如患者人口统计、疾病史或医生报告。本研究探讨了如何利用视觉语言处理 (VLP) 模型，利用 X 射线图像和相应报告来预测 OA 严重程度。我们的方法利用膝盖的 X 射线图像和从表格 OA 评分值生成的各种报告模板来训练 CLIP（对比语言图像预训练）样式的 VLP 模型。此外，我们还加入了额外的对比标题，以强制模型区分正面和负面报告。结果证明了这些模型在学习文本图像表示及其上下文关系方面的有效性，展示了 OA 评估的潜在进步，并为医学环境中的专业视觉语言模型奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.06331v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation**<br />
**Title_cn:** 超越表面：文本到图像生成中视觉刻板印象的全球范围分析<br />
**Authors:** Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi Dave, Rida Qadri, Chandan K. Reddy, Sunipa Dev<br />
**Abstract:** <details><summary>原文: </summary>Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images of these identities as compared to other attributes. We further investigate how disparately offensive the depictions of generated images are for different nationalities. Finally, through a detailed case study, we reveal how the 'default' representations of all identity groups have a stereotypical appearance. Moreover, for the Global South, images across different attributes are visually similar, even when explicitly prompted otherwise. CONTENT WARNING: Some examples may contain offensive stereotypes.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究强调了文本到图像（T2I）模型生成中对不同身份群体的人们的刻板描述问题。然而，这些现有的方法有几个关键的局限性，包括在评估中明显缺乏对全球身份群体的覆盖，以及其相关刻板印象的范围。此外，他们常常缺乏对固有视觉刻板印象（例如“体重不足”或“阔边帽”）与文化依赖性刻板印象（例如“有吸引力”或“恐怖分子”）之间的严格区分。在这项工作中，我们通过多方面的方法解决了这些局限性，该方法利用现有的文本资源，将我们对 T2I 模型生成的图像中的地理文化刻板印象的评估作为基础。我们利用现有的刻板印象基准来识别和评估全球范围内的视觉刻板印象，涵盖 135 个国籍的身份群体。我们证明，与其他属性相比，刻板属性出现在这些身份图像中的可能性是其他属性的三倍。我们进一步调查了不同国籍的生成图像的描述有多么不同的冒犯性。最后，通过详细的案例研究，我们揭示了所有身份群体的“默认”表示如何具有刻板印象。此外，对于南半球国家来说，即使有明确提示，不同属性的图像在视觉上也是相似的。内容警告：某些示例可能包含令人反感的刻板印象。</details>
**PDF:** <http://arxiv.org/pdf/2401.06310v1><br />
**Code:** null<br />

