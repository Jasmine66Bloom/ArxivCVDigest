## [UPDATED!] **2024-01-13** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Concrete Surface Crack Detection with Convolutional-based Deep Learning Models**<br />
**Title_cn:** 使用基于卷积的深度学习模型进行混凝土表面裂缝检测<br />
**Authors:** Sara Shomal Zadeh, Sina Aalipour birgani, Meisam Khorshidi, Farhad Kooban<br />
**Abstract:** <details><summary>原文: </summary>Effective crack detection is pivotal for the structural health monitoring and inspection of buildings. This task presents a formidable challenge to computer vision techniques due to the inherently subtle nature of cracks, which often exhibit low-level features that can be easily confounded with background textures, foreign objects, or irregularities in construction. Furthermore, the presence of issues like non-uniform lighting and construction irregularities poses significant hurdles for autonomous crack detection during building inspection and monitoring. Convolutional neural networks (CNNs) have emerged as a promising framework for crack detection, offering high levels of accuracy and precision. Additionally, the ability to adapt pre-trained networks through transfer learning provides a valuable tool for users, eliminating the need for an in-depth understanding of algorithm intricacies. Nevertheless, it is imperative to acknowledge the limitations and considerations when deploying CNNs, particularly in contexts where the outcomes carry immense significance, such as crack detection in buildings. In this paper, our approach to surface crack detection involves the utilization of various deep-learning models. Specifically, we employ fine-tuning techniques on pre-trained deep learning architectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models are chosen for their established performance and versatility in image analysis tasks. We compare deep learning models using precision, recall, and F1 scores.</details>
**Abstract_cn:** <details><summary>译文: </summary>有效的裂缝检测对于建筑物的结构健康监测和检查至关重要。由于裂缝固有的微妙性质，该任务对计算机视觉技术提出了巨大的挑战，裂缝通常表现出低级特征，很容易与背景纹理、异物或结构中的不规则性混淆。此外，照明不均匀和施工不规则等问题的存在给建筑检查和监控期间的自动裂缝检测带来了重大障碍。卷积神经网络 (CNN) 已成为一种很有前景的裂纹检测框架，可提供高水平的准确度和精确度。此外，通过迁移学习调整预训练网络的能力为用户提供了宝贵的工具，无需深入了解算法的复杂性。然而，在部署 CNN 时必须承认其局限性和考虑因素，特别是在结果具有重大意义的情况下，例如建筑物中的裂缝检测。在本文中，我们的表面裂纹检测方法涉及利用各种深度学习模型。具体来说，我们在预训练的深度学习架构上采用微调技术：VGG19、ResNet50、Inception V3 和 EfficientNetV2。选择这些模型是因为它们在图像分析任务中的既定性能和多功能性。我们使用精确率、召回率和 F1 分数来比较深度学习模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.07124v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Exploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability**<br />
**Title_cn:** 从对抗性可迁移性的角度探讨针对潜在扩散模型的对抗性攻击<br />
**Authors:** Junxi Chen, Junhao Dong, Xiaohua Xie<br />
**Abstract:** <details><summary>原文: </summary>Recently, many studies utilized adversarial examples (AEs) to raise the cost of malicious image editing and copyright violation powered by latent diffusion models (LDMs). Despite their successes, a few have studied the surrogate model they used to generate AEs. In this paper, from the perspective of adversarial transferability, we investigate how the surrogate model's property influences the performance of AEs for LDMs. Specifically, we view the time-step sampling in the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogate models. We find that the smoothness of surrogate models at different time steps differs, and we substantially improve the performance of the MC-based AEs by selecting smoother surrogate models. In the light of the theoretical framework on adversarial transferability in image classification, we also conduct a theoretical analysis to explain why smooth surrogate models can also boost AEs for LDMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，许多研究利用对抗性示例（AE）来提高由潜在扩散模型（LDM）支持的恶意图像编辑和版权侵犯的成本。尽管取得了成功，但仍有一些人研究了他们用来生成 AE 的替代模型。在本文中，我们从对抗性可迁移性的角度研究了代理模型的属性如何影响 LDM 的 AE 性能。具体来说，我们将基于蒙特卡罗（MC-based）对抗攻击中的时间步采样视为选择代理模型。我们发现不同时间步长的代理模型的平滑度不同，并且通过选择更平滑的代理模型，我们显着提高了基于 MC 的 AE 的性能。根据图像分类中对抗性可迁移性的理论框架，我们还进行了理论分析，以解释为什么平滑代理模型也可以提高 LDM 的 AE。</details>
**PDF:** <http://arxiv.org/pdf/2401.07087v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching**<br />
**Title_cn:** GoMatching：通过长期和短期匹配进行视频文本识别的简单基线<br />
**Authors:** Haibin He, Maoyuan Ye, Jing Zhang, Juhua Liu, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we highlight a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching achieves impressive performance on two public benchmarks, e.g., setting a new record on the ICDAR15-video dataset, and one novel test set with arbitrary-shaped text, while saving considerable training budgets. The code will be released at https://github.com/Hxyz-123/GoMatching.</details>
**Abstract_cn:** <details><summary>译文: </summary>除了图像文本识别中的文本检测和识别任务之外，视频文本识别还因包含跟踪而面临更大的挑战。虽然先进的端到端可训练方法已显示出值得称赞的性能，但追求多任务优化可能会带来为单个任务产生次优结果的风险。在本文中，我们强调了最先进的视频文本识别器的主要瓶颈：识别能力有限。针对这个问题，我们建议有效地将现成的基于查询的图像文本识别器转变为视频专家，并提出一个名为 GoMatching 的简单基线，它将训练工作重点放在跟踪上，同时保持强大的识别性能。为了使图像文本识别器适应视频数据集，我们添加了一个重新评分头，通过有效的调整对每个检测到的实例的置信度重新评分，从而形成更好的跟踪候选池。此外，我们设计了一个长短期匹配模块，称为 LST-Matcher，通过 Transformer 集成长期和短期匹配结果来增强观测器的跟踪能力。基于上述简单的设计，GoMatching 在两个公共基准测试中取得了令人印象深刻的性能，例如在 ICDAR15 视频数据集和一个具有任意形状文本的新颖测试集上创造了新记录，同时节省了大量的训练预算。代码将在 https://github.com/Hxyz-123/GoMatching 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.07080v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition**<br />
**Title_cn:** 具有语义关系指导的双视图数据幻觉用于少镜头图像识别<br />
**Authors:** Hefeng Wu, Guangzhi Ye, Ziyang Zhou, Ling Tian, Qing Wang, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of the semantic information in the textual modality that reflects human concepts, this work proposes a novel framework that exploits semantic relations to guide dual-view data hallucination for few-shot image recognition. The proposed framework enables generating more diverse and reasonable data samples for novel classes through effective information transfer from base classes. Specifically, an instance-view data hallucination module hallucinates each sample of a novel class to generate new data by employing local semantic correlated attention and global semantic feature fusion derived from base classes. Meanwhile, a prototype-view data hallucination module exploits semantic-aware measure to estimate the prototype of a novel class and the associated distribution from the few samples, which thereby harvests the prototype as a more stable sample and enables resampling a large number of samples. We conduct extensive experiments and comparisons with state-of-the-art methods on several popular few-shot benchmarks to verify the effectiveness of the proposed framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习从少量图像样本中识别新概念非常具有挑战性，因为学习的模型很容易在少量数据上过度拟合，导致泛化性较差。一种有前途但尚未充分探索的解决方案是通过生成合理的样本来补偿新类别。然而，该系列的大多数现有作品仅利用视觉信息，使得生成的数据很容易被少数可用样本中包含的一些具有挑战性的因素分散注意力。意识到反映人类概念的文本模态中的语义信息，这项工作提出了一种新颖的框架，利用语义关系来指导双视图数据幻觉以进行少镜头图像识别。所提出的框架能够通过基类的有效信息传输为新类生成更加多样化和合理的数据样本。具体来说，实例视图数据幻觉模块通过采用从基类导出的局部语义相关注意力和全局语义特征融合来幻觉新类的每个样本以生成新数据。同时，原型视图数据幻觉模块利用语义感知措施来估计新类的原型以及少数样本的相关分布，从而将原型收获为更稳定的样本，并能够对大量样本进行重采样。我们在几个流行的小样本基准上进行了广泛的实验并与最先进的方法进行比较，以验证所提出框架的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.07061v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **An automated framework for brain vessel centerline extraction from CTA images**<br />
**Title_cn:** 从 CTA 图像中提取脑血管中心线的自动化框架<br />
**Authors:** Sijie Liu, Ruisheng Su, Jianghang Su, Jingmin Xin, Jiayi Wu, Wim van Zwam, Pieter Jan van Doormaal, Aad van der Lugt, Wiro J. Niessen, Nanning Zheng, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Accurate automated extraction of brain vessel centerlines from CTA images plays an important role in diagnosis and therapy of cerebrovascular diseases, such as stroke. However, this task remains challenging due to the complex cerebrovascular structure, the varying imaging quality, and vessel pathology effects. In this paper, we consider automatic lumen segmentation generation without additional annotation effort by physicians and more effective use of the generated lumen segmentation for improved centerline extraction performance. We propose an automated framework for brain vessel centerline extraction from CTA images. The framework consists of four major components: (1) pre-processing approaches that register CTA images with a CT atlas and divide these images into input patches, (2) lumen segmentation generation from annotated vessel centerlines using graph cuts and robust kernel regression, (3) a dual-branch topology-aware UNet (DTUNet) that can effectively utilize the annotated vessel centerlines and the generated lumen segmentation through a topology-aware loss (TAL) and its dual-branch design, and (4) post-processing approaches that skeletonize the predicted lumen segmentation. Extensive experiments on a multi-center dataset demonstrate that the proposed framework outperforms state-of-the-art methods in terms of average symmetric centerline distance (ASCD) and overlap (OV). Subgroup analyses further suggest that the proposed framework holds promise in clinical applications for stroke treatment. Code is publicly available at https://github.com/Liusj-gh/DTUNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>从 CTA 图像中准确自动提取脑血管中心线在中风等脑血管疾病的诊断和治疗中发挥着重要作用。然而，由于复杂的脑血管结构、不同的成像质量和血管病理学影响，这项任务仍然具有挑战性。在本文中，我们考虑自动管腔分割生成，而无需医生进行额外的注释工作，并且更有效地使用生成的管腔分割来提高中心线提取性能。我们提出了一个从 CTA 图像中提取脑血管中心线的自动化框架。该框架由四个主要部分组成：(1) 将 CTA 图像与 CT 图集配准并将这些图像划分为输入块的预处理方法，(2) 使用图形切割和鲁棒核回归从注释的血管中心线生成管腔分割，( 3）双分支拓扑感知UNet（DTUNet），可以通过拓扑感知损失（TAL）及其双分支设计有效地利用注释的血管中心线和生成的管腔分割，以及（4）后处理方法骨架化预测的流明分割。对多中心数据集的大量实验表明，所提出的框架在平均对称中心线距离（ASCD）和重叠（OV）方面优于最先进的方法。亚组分析进一步表明，所提出的框架在中风治疗的临床应用中具有前景。代码可在 https://github.com/Liusj-gh/DTUNet 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07041v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Image edge enhancement for effective image classification**<br />
**Title_cn:** 用于有效图像分类的图像边缘增强<br />
**Authors:** Tianhao Bu, Michalis Lazarou, Tania Stathaki<br />
**Abstract:** <details><summary>原文: </summary>Image classification has been a popular task due to its feasibility in real-world applications. Training neural networks by feeding them RGB images has demonstrated success over it. Nevertheless, improving the classification accuracy and computational efficiency of this process continues to present challenges that researchers are actively addressing. A widely popular embraced method to improve the classification performance of neural networks is to incorporate data augmentations during the training process. Data augmentations are simple transformations that create slightly modified versions of the training data and can be very effective in training neural networks to mitigate overfitting and improve their accuracy performance. In this study, we draw inspiration from high-boost image filtering and propose an edge enhancement-based method as means to enhance both accuracy and training speed of neural networks. Specifically, our approach involves extracting high frequency features, such as edges, from images within the available dataset and fusing them with the original images, to generate new, enriched images. Our comprehensive experiments, conducted on two distinct datasets CIFAR10 and CALTECH101, and three different network architectures ResNet-18, LeNet-5 and CNN-9 demonstrates the effectiveness of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像分类由于其在现实应用中的可行性而成为一项流行的任务。通过向神经网络提供 RGB 图像来训练神经网络已经证明是成功的。然而，提高该过程的分类准确性和计算效率仍然是研究人员正在积极解决的挑战。一种广泛流行的提高神经网络分类性能的方法是在训练过程中合并数据增强。数据增强是简单的转换，可以创建训练数据的稍微修改的版本，并且可以非常有效地训练神经网络以减轻过度拟合并提高其准确性性能。在本研究中，我们从高增强图像过滤中汲取灵感，并提出了一种基于边缘增强的方法作为提高神经网络的准确性和训练速度的手段。具体来说，我们的方法涉及从可用数据集中的图像中提取高频特征，例如边缘，并将它们与原始图像融合，以生成新的、丰富的图像。我们在两个不同的数据集 CIFAR10 和 CALTECH101 以及三种不同的网络架构 ResNet-18、LeNet-5 和 CNN-9 上进行的综合实验证明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.07028v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Empowering Medical Imaging with Artificial Intelligence: A Review of Machine Learning Approaches for the Detection, and Segmentation of COVID-19 Using Radiographic and Tomographic Images**<br />
**Title_cn:** 利用人工智能增强医学成像：使用放射线和断层扫描图像检测和分割 COVID-19 的机器学习方法综述<br />
**Authors:** Sayed Amir Mousavi Mobarakeh, Kamran Kazemi, Ardalan Aarabi, Habibollah Danyal<br />
**Abstract:** <details><summary>原文: </summary>Since 2019, the global dissemination of the Coronavirus and its novel strains has resulted in a surge of new infections. The use of X-ray and computed tomography (CT) imaging techniques is critical in diagnosing and managing COVID-19. Incorporating artificial intelligence (AI) into the field of medical imaging is a powerful combination that can provide valuable support to healthcare professionals.This paper focuses on the methodological approach of using machine learning (ML) to enhance medical imaging for COVID-19 diagnosis.For example, deep learning can accurately distinguish lesions from other parts of the lung without human intervention in a matter of minutes.Moreover, ML can enhance performance efficiency by assisting radiologists in making more precise clinical decisions, such as detecting and distinguishing Covid-19 from different respiratory infections and segmenting infections in CT and X-ray images, even when the lesions have varying sizes and shapes.This article critically assesses machine learning methodologies utilized for the segmentation, classification, and detection of Covid-19 within CT and X-ray images, which are commonly employed tools in clinical and hospital settings to represent the lung in various aspects and extensive detail.There is a widespread expectation that this technology will continue to hold a central position within the healthcare sector, driving further progress in the management of the pandemic.</details>
**Abstract_cn:** <details><summary>译文: </summary>2019年以来，新冠病毒及其新毒株在全球传播，导致新增感染病例激增。 X 射线和计算机断层扫描 (CT) 成像技术的使用对于诊断和管理 COVID-19 至关重要。将人工智能 (AI) 纳入医学影像领域是一个强大的组合，可以为医疗保健专业人员提供宝贵的支持。本文重点介绍使用机器学习 (ML) 增强医学影像以进行 COVID-19 诊断的方法。例如，深度学习可以在几分钟内准确地区分病变与肺部其他部位，无需人工干预。此外，机器学习可以通过协助放射科医生做出更精确的临床决策来提高性能效率，例如检测和区分 Covid-19 和不同部位的病变。呼吸道感染和 CT 和 X 射线图像中的感染分割，即使病变的大小和形状各异。本文严格评估了用于 CT 和 X 射线图像中 Covid-19 分割、分类和检测的机器学习方法，它们是临床和医院环境中常用的工具，可以在各个方面和广泛的细节上代表肺部。人们普遍期望这项技术将继续在医疗保健领域占据核心地位，推动肺部管理的进一步进步大流行。</details>
**PDF:** <http://arxiv.org/pdf/2401.07020v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Weak Labeling for Cropland Mapping in Africa**<br />
**Title_cn:** 非洲农田制图的弱标签<br />
**Authors:** Gilles Quentin Hacheme, Akram Zaytar, Girmaw Abebe Tadesse, Caleb Robinson, Rahul Dodhia, Juan M. Lavista Ferres, Stephen Wood<br />
**Abstract:** <details><summary>原文: </summary>Cropland mapping can play a vital role in addressing environmental, agricultural, and food security challenges. However, in the context of Africa, practical applications are often hindered by the limited availability of high-resolution cropland maps. Such maps typically require extensive human labeling, thereby creating a scalability bottleneck. To address this, we propose an approach that utilizes unsupervised object clustering to refine existing weak labels, such as those obtained from global cropland maps. The refined labels, in conjunction with sparse human annotations, serve as training data for a semantic segmentation network designed to identify cropland areas. We conduct experiments to demonstrate the benefits of the improved weak labels generated by our method. In a scenario where we train our model with only 33 human-annotated labels, the F_1 score for the cropland category increases from 0.53 to 0.84 when we add the mined negative labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>农田测绘可以在应对环境、农业和粮食安全挑战方面发挥至关重要的作用。然而，在非洲，实际应用往往受到高分辨率农田地图有限的阻碍。此类地图通常需要大量的人工标记，从而造成可扩展性瓶颈。为了解决这个问题，我们提出了一种利用无监督对象聚类来细化现有弱标签的方法，例如从全球农田地图中获得的标签。精炼的标签与稀疏的人工注释相结合，作为旨在识别农田区域的语义分割网络的训练数据。我们进行实验来证明我们的方法生成的改进的弱标签的好处。在我们仅使用 33 个人工注释标签训练模型的场景中，当我们添加挖掘的负标签时，农田类别的 F_1 分数从 0.53 增加到 0.84。</details>
**PDF:** <http://arxiv.org/pdf/2401.07014v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Datasets, Clues and State-of-the-Arts for Multimedia Forensics: An Extensive Review**<br />
**Title_cn:** 多媒体取证的数据集、线索和最新技术：广泛回顾<br />
**Authors:** Ankit Yadav, Dinesh Kumar Vishwakarma<br />
**Abstract:** <details><summary>原文: </summary>With the large chunks of social media data being created daily and the parallel rise of realistic multimedia tampering methods, detecting and localising tampering in images and videos has become essential. This survey focusses on approaches for tampering detection in multimedia data using deep learning models. Specifically, it presents a detailed analysis of benchmark datasets for malicious manipulation detection that are publicly available. It also offers a comprehensive list of tampering clues and commonly used deep learning architectures. Next, it discusses the current state-of-the-art tampering detection methods, categorizing them into meaningful types such as deepfake detection methods, splice tampering detection methods, copy-move tampering detection methods, etc. and discussing their strengths and weaknesses. Top results achieved on benchmark datasets, comparison of deep learning approaches against traditional methods and critical insights from the recent tampering detection methods are also discussed. Lastly, the research gaps, future direction and conclusion are discussed to provide an in-depth understanding of the tampering detection research arena.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着每天产生大量社交媒体数据以及现实多媒体篡改方法的并行兴起，检测和定位图像和视频中的篡改变得至关重要。这项调查的重点是使用深度学习模型对多媒体数据进行篡改检测的方法。具体来说，它对公开的恶意操纵检测基准数据集进行了详细分析。它还提供了篡改线索和常用深度学习架构的完整列表。接下来，讨论当前最先进的篡改检测方法，将它们分类为有意义的类型，例如 Deepfake 检测方法、拼接篡改检测方法、复制移动篡改检测方法等，并讨论它们的优缺点。还讨论了在基准数据集上取得的最佳结果、深度学习方法与传统方法的比较以及最近篡改检测方法的重要见解。最后，讨论了研究差距、未来方向和结论，以深入了解篡改检测研究领域。</details>
**PDF:** <http://arxiv.org/pdf/2401.06999v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Towards Effective Image Forensics via A Novel Computationally Efficient Framework and A New Image Splice Dataset**<br />
**Title_cn:** 通过新颖的计算效率框架和新的图像拼接数据集实现有效的图像取证<br />
**Authors:** Ankit Yadav, Dinesh Kumar Vishwakarma<br />
**Abstract:** <details><summary>原文: </summary>Splice detection models are the need of the hour since splice manipulations can be used to mislead, spread rumors and create disharmony in society. However, there is a severe lack of image splicing datasets, which restricts the capabilities of deep learning models to extract discriminative features without overfitting. This manuscript presents two-fold contributions toward splice detection. Firstly, a novel splice detection dataset is proposed having two variants. The two variants include spliced samples generated from code and through manual editing. Spliced images in both variants have corresponding binary masks to aid localization approaches. Secondly, a novel Spatio-Compression Lightweight Splice Detection Framework is proposed for accurate splice detection with minimum computational cost. The proposed dual-branch framework extracts discriminative spatial features from a lightweight spatial branch. It uses original resolution compression data to extract double compression artifacts from the second branch, thereby making it 'information preserving.' Several CNNs are tested in combination with the proposed framework on a composite dataset of images from the proposed dataset and the CASIA v2.0 dataset. The best model accuracy of 0.9382 is achieved and compared with similar state-of-the-art methods, demonstrating the superiority of the proposed framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>剪接检测模型是当前的需要，因为剪接操作可能被用来误导、传播谣言并在社会上制造不和谐。然而，图像拼接数据集严重缺乏，这限制了深度学习模型在不过度拟合的情况下提取判别性特征的能力。这份手稿对剪接检测提出了两方面的贡献。首先，提出了一种具有两种变体的新颖的剪接检测数据集。这两个变体包括通过代码和手动编辑生成的拼接样本。两种变体中的拼接图像都有相应的二进制掩模来帮助定位方法。其次，提出了一种新颖的空间压缩轻量级拼接检测框架，以最小的计算成本进行精确的拼接检测。所提出的双分支框架从轻量级空间分支中提取有区别的空间特征。它使用原始分辨率压缩数据从第二个分支中提取双重压缩伪影，从而使其“信息保留”。结合所提出的框架，在来自所提出的数据集和 CASIA v2.0 数据集的图像复合数据集上测试了多个 CNN。实现了 0.9382 的最佳模型精度，并与类似的最先进方法进行了比较，证明了所提出框架的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06998v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **UniVision: A Unified Framework for Vision-Centric 3D Perception**<br />
**Title_cn:** UniVision：以视觉为中心的 3D 感知的统一框架<br />
**Authors:** Yu Hong, Qian Liu, Huayuan Cheng, Danjiao Ma, Hang Dai, Yu Wang, Guangzhi Cao, Yong Ding<br />
**Abstract:** <details><summary>原文: </summary>The past few years have witnessed the rapid development of vision-centric 3D perception in autonomous driving. Although the 3D perception models share many structural and conceptual similarities, there still exist gaps in their feature representations, data formats, and objectives, posing challenges for unified and efficient 3D perception framework design. In this paper, we present UniVision, a simple and efficient framework that unifies two major tasks in vision-centric 3D perception, \ie, occupancy prediction and object detection. Specifically, we propose an explicit-implicit view transform module for complementary 2D-3D feature transformation. We propose a local-global feature extraction and fusion module for efficient and adaptive voxel and BEV feature extraction, enhancement, and interaction. Further, we propose a joint occupancy-detection data augmentation strategy and a progressive loss weight adjustment strategy which enables the efficiency and stability of the multi-task framework training. We conduct extensive experiments for different perception tasks on four public benchmarks, including nuScenes LiDAR segmentation, nuScenes detection, OpenOccupancy, and Occ3D. UniVision achieves state-of-the-art results with +1.5 mIoU, +1.8 NDS, +1.5 mIoU, and +1.8 mIoU gains on each benchmark, respectively. We believe that the UniVision framework can serve as a high-performance baseline for the unified vision-centric 3D perception task. The code will be available at \url{https://github.com/Cc-Hy/UniVision}.</details>
**Abstract_cn:** <details><summary>译文: </summary>过去几年，以视觉为中心的3D感知在自动驾驶领域快速发展。尽管3D感知模型在结构和概念上有许多相似之处，但在特征表示、数据格式和目标方面仍然存在差距，这给统一、高效的3D感知框架设计带来了挑战。在本文中，我们提出了 UniVision，这是一个简单而高效的框架，它统一了以视觉为中心的 3D 感知中的两个主要任务，即占用预测和对象检测。具体来说，我们提出了一种用于互补 2D-3D 特征变换的显式-隐式视图变换模块。我们提出了一种局部-全局特征提取和融合模块，用于高效、自适应体素和 BEV 特征提取、增强和交互。此外，我们提出了联合占用检测数据增强策略和渐进式损失权重调整策略，从而提高了多任务框架训练的效率和稳定性。我们在四个公共基准上针对不同的感知任务进行了广泛的实验，包括 nuScenes LiDAR 分割、nuScenes 检测、OpenOccupancy 和 Occ3D。 UniVision 在每个基准测试中分别获得了 +1.5 mIoU、+1.8 NDS、+1.5 mIoU 和 +1.8 mIoU 的最先进结果。我们相信 UniVision 框架可以作为以视觉为中心的统一 3D 感知任务的高性能基准。该代码可在 \url{https://github.com/Cc-Hy/UniVision} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.06994v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Class-Imbalanced Semi-Supervised Learning for Large-Scale Point Cloud Semantic Segmentation via Decoupling Optimization**<br />
**Title_cn:** 通过解耦优化进行大规模点云语义分割的类不平衡半监督学习<br />
**Authors:** Mengtian Li, Shaohui Lin, Zihan Wang, Yunhang Shen, Baochang Zhang, Lizhuang Ma<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets (i.e., S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督学习（SSL）由于数据标注成本的显着降低，一直是大规模 3D 场景理解的活跃研究课题。然而，现有的基于 SSL 的方法存在严重的训练偏差，这主要是由于点云数据的类不平衡和长尾分布造成的。因此，它们导致尾部类别分割的预测存在偏差。在本文中，我们引入了一种新的解耦优化框架，该框架以另一种优化方式解开特征表示学习和分类器，以有效地改变偏差决策边界。特别是，我们首先采用两轮伪标签生成来选择头尾类中的未标记点。我们进一步引入多类不平衡焦点损失，以自适应地更多地关注跨头到尾类的特征学习。我们在特征学习后修复主干参数，并使用地面实况点重新训练分类器来更新其参数。大量实验证明，我们的方法在室内和室外 3D 点云数据集（即 S3DIS、ScanNet-V2、Semantic3D 和 SemanticKITTI）上使用 1% 和 1pt 评估的效果优于以前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.06975v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Domain Adaptation for Large-Vocabulary Object Detectors**<br />
**Title_cn:** 大词汇量目标检测器的域适应<br />
**Authors:** Kai Jiang, Jiaxing Huang, Weiying Xie, Yunsong Li, Ling Shao, Shijian Lu<br />
**Abstract:** <details><summary>原文: </summary>Large-vocabulary object detectors (LVDs) aim to detect objects of many categories, which learn super objectness features and can locate objects accurately while applied to various downstream data. However, LVDs often struggle in recognizing the located objects due to domain discrepancy in data distribution and object vocabulary. At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability. This paper presents KGD, a Knowledge Graph Distillation technique that exploits the implicit knowledge graphs (KG) in CLIP for effectively adapting LVDs to various downstream domains. KGD consists of two consecutive stages: 1) KG extraction that employs CLIP to encode downstream domain data as nodes and their feature distances as edges, constructing KG that inherits the rich semantic relations in CLIP explicitly; and 2) KG encapsulation that transfers the extracted KG into LVDs to enable accurate cross-domain object classification. In addition, KGD can extract both visual and textual KG independently, providing complementary vision and language knowledge for object localization and object classification in detection tasks over various downstream domains. Experiments over multiple widely adopted detection benchmarks show that KGD outperforms the state-of-the-art consistently by large margins.</details>
**Abstract_cn:** <details><summary>译文: </summary>大词汇量目标检测器（LVD）旨在检测多类别的目标，它学习超级目标特征，并在应用于各种下游数据时可以准确地定位目标。然而，由于数据分布和对象词汇方面的域差异，LVD 在识别所定位的对象时常常遇到困难。另一方面，最近的视觉语言基础模型（例如 CLIP）展示了卓越的开放词汇识别能力。本文提出了 KGD，一种知识图蒸馏技术，它利用 CLIP 中的隐式知识图（KG）来有效地使 LVD 适应各种下游领域。 KGD由两个连续的阶段组成：1）KG提取，利用CLIP将下游领域数据编码为节点，将其特征距离编码为边，构建显式继承CLIP丰富语义关系的KG； 2）KG封装，将提取的KG传输到LVD中，以实现准确的跨域对象分类。此外，KGD可以独立提取视觉和文本KG，为各个下游领域的检测任务中的对象定位和对象分类提供补充的视觉和语言知识。对多个广泛采用的检测基准进行的实验表明，KGD 的性能始终大幅优于最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.06969v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation**<br />
**Title_cn:** EVOKE：使用优化的知识蒸馏实现情感支持的虚拟化身映射<br />
**Authors:** Maryam Nadeem, Raza Imam, Rouqaiah Al-Refai, Meriem Chkir, Mohamad Hoda, Abdulmotaleb El Saddik<br />
**Abstract:** <details><summary>原文: </summary>As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着虚拟环境的不断发展，对沉浸式和情感参与体验的需求不断增长。为了满足这一需求，我们引入了使用优化知识蒸馏 (EVOKE) 的情感支持虚拟化身映射，EVOKE 是一种轻量级情感识别框架，旨在将情感识别无缝集成到虚拟环境中的 3D 化身中。我们的方法利用知识蒸馏，涉及公开可用的 DEAP 数据集上的多标签分类，其中涵盖效价、唤醒度和支配性作为主要情绪类别。值得注意的是，我们的蒸馏模型（仅具有两个卷积层且参数比教师模型少 18 倍的 CNN）取得了有竞争力的结果，准确率达到 87%，同时需要的计算资源少得多。性能和可部署性之间的这种平衡使我们的框架成为虚拟环境系统的理想选择。此外，多标签分类结果用于将情感映射到定制设计的 3D 头像上。</details>
**PDF:** <http://arxiv.org/pdf/2401.06957v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **3D Object Detection and High-Resolution Traffic Parameters Extraction Using Low-Resolution LiDAR Data**<br />
**Title_cn:** 使用低分辨率 LiDAR 数据进行 3D 对象检测和高分辨率交通参数提取<br />
**Authors:** Linlin Zhang, Xiang Yu, Armstrong Aboah, Yaw Adu-Gyamfi<br />
**Abstract:** <details><summary>原文: </summary>Traffic volume data collection is a crucial aspect of transportation engineering and urban planning, as it provides vital insights into traffic patterns, congestion, and infrastructure efficiency. Traditional manual methods of traffic data collection are both time-consuming and costly. However, the emergence of modern technologies, particularly Light Detection and Ranging (LiDAR), has revolutionized the process by enabling efficient and accurate data collection. Despite the benefits of using LiDAR for traffic data collection, previous studies have identified two major limitations that have impeded its widespread adoption. These are the need for multiple LiDAR systems to obtain complete point cloud information of objects of interest, as well as the labor-intensive process of annotating 3D bounding boxes for object detection tasks. In response to these challenges, the current study proposes an innovative framework that alleviates the need for multiple LiDAR systems and simplifies the laborious 3D annotation process. To achieve this goal, the study employed a single LiDAR system, that aims at reducing the data acquisition cost and addressed its accompanying limitation of missing point cloud information by developing a Point Cloud Completion (PCC) framework to fill in missing point cloud information using point density. Furthermore, we also used zero-shot learning techniques to detect vehicles and pedestrians, as well as proposed a unique framework for extracting low to high features from the object of interest, such as height, acceleration, and speed. Using the 2D bounding box detection and extracted height information, this study is able to generate 3D bounding boxes automatically without human intervention.</details>
**Abstract_cn:** <details><summary>译文: </summary>交通量数据收集是交通工程和城市规划的一个重要方面，因为它提供了有关交通模式、拥堵和基础设施效率的重要见解。传统的手动交通数据收集方法既耗时又昂贵。然而，现代技术的出现，特别是光探测和测距 (LiDAR)，通过实现高效、准确的数据收集，彻底改变了这一过程。尽管使用激光雷达进行交通数据收集有很多好处，但之前的研究已经发现了阻碍其广泛采用的两个主要限制。这些是需要多个 LiDAR 系统来获取感兴趣物体的完整点云信息，以及为物体检测任务注释 3D 边界框的劳动密集型过程。为了应对这些挑战，当前的研究提出了一种创新框架，可以减轻对多个 LiDAR 系统的需求，并简化繁琐的 3D 注释过程。为了实现这一目标，该研究采用了单一激光雷达系统，旨在降低数据采集成本，并通过开发点云补全（PCC）框架来使用点来填充缺失的点云信息，从而解决其伴随的缺失点云信息的局限性。密度。此外，我们还使用零样本学习技术来检测车辆和行人，并提出了一种独特的框架，用于从感兴趣的对象中提取从低到高的特征，例如高度、加速度和速度。利用 2D 边界框检测和提取的高度信息，本研究能够自动生成 3D 边界框，无需人工干预。</details>
**PDF:** <http://arxiv.org/pdf/2401.06946v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration**<br />
**Title_cn:** ENTED：基于参考的盲脸恢复的增强神经纹理提取和分布<br />
**Authors:** Yuen-Fui Lau, Tianjia Zhang, Zhefan Rao, Qifeng Chen<br />
**Abstract:** <details><summary>原文: </summary>We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 ENTED，一个用于盲人面部恢复的新框架，旨在恢复高质量和逼真的人像图像。我们的方法涉及使用高质量参考图像修复单个退化的输入图像。我们利用纹理提取和分布框架在降级的输入和参考图像之间传输高质量的纹理特征。然而，我们框架中的类似 StyleGAN 的架构需要高质量的潜在代码来生成逼真的图像。从退化的输入图像中提取的潜在代码通常包含损坏的特征，使得很难将输入中的语义信息与参考中的高质量纹理对齐。为了克服这一挑战，我们采用了两种特殊技术。第一种技术受矢量量化启发，用高质量的码字替换损坏的语义特征。第二种技术生成风格代码，这些代码携带来自使用参考图像流形中的高质量特征开发的信息更丰富的潜在空间的真实纹理信息。对合成和真实数据集进行的大量实验表明，我们的方法产生的结果具有更真实的上下文细节，并且优于最先进的方法。彻底的消融研究证实了每个提议模块的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.06978v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Quantum Denoising Diffusion Models**<br />
**Title_cn:** 量子去噪扩散模型<br />
**Authors:** Michael Kölle, Gerhard Stenzel, Jonas Stein, Sebastian Zielinski, Björn Ommer, Claudia Linnhoff-Popien<br />
**Abstract:** <details><summary>原文: </summary>In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，DALL-E、Craiyon 和 Stable Diffusion 等机器学习模型因其从简洁描述生成高分辨率图像的能力而受到广泛关注。与此同时，量子计算正在显示出有希望的进步，尤其是量子机器学习，它利用量子力学来满足传统机器学习算法日益增长的计算要求。本文探讨了量子机器学习和变分量子电路的集成，以增强基于扩散的图像生成模型的功效。具体来说，我们解决了经典扩散模型的两个挑战：采样速度低和参数要求广泛。我们引入了两种量子扩散模型，并使用 MNIST 数字、Fashion MNIST 和 CIFAR-10 将它们的能力与经典模型进行了基准测试。我们的模型在性能指标 FID、SSIM 和 PSNR 方面超越了具有相似参数数量的经典模型。此外，我们引入了一种一致性模型单一采样架构，它将扩散过程结合到一个步骤中，从而实现快速的一步图像生成。</details>
**PDF:** <http://arxiv.org/pdf/2401.07049v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Visually Attentive Splice Localization Network with Multi-Domain Feature Extractor and Multi-Receptive Field Upsampler**<br />
**Title_cn:** 具有多域特征提取器和多感受野上采样器的视觉注意力剪接定位网络<br />
**Authors:** Ankit Yadav, Dinesh Kumar Vishwakarma<br />
**Abstract:** <details><summary>原文: </summary>Image splice manipulation presents a severe challenge in today's society. With easy access to image manipulation tools, it is easier than ever to modify images that can mislead individuals, organizations or society. In this work, a novel, "Visually Attentive Splice Localization Network with Multi-Domain Feature Extractor and Multi-Receptive Field Upsampler" has been proposed. It contains a unique "visually attentive multi-domain feature extractor" (VA-MDFE) that extracts attentional features from the RGB, edge and depth domains. Next, a "visually attentive downsampler" (VA-DS) is responsible for fusing and downsampling the multi-domain features. Finally, a novel "visually attentive multi-receptive field upsampler" (VA-MRFU) module employs multiple receptive field-based convolutions to upsample attentional features by focussing on different information scales. Experimental results conducted on the public benchmark dataset CASIA v2.0 prove the potency of the proposed model. It comfortably beats the existing state-of-the-arts by achieving an IoU score of 0.851, pixel F1 score of 0.9195 and pixel AUC score of 0.8989.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像拼接处理在当今社会提出了严峻的挑战。通过轻松访问图像处理工具，修改可能误导个人、组织或社会的图像比以往任何时候都更容易。在这项工作中，提出了一种新颖的“具有多域特征提取器和多感受野上采样器的视觉注意力拼接定位网络”。它包含一个独特的“视觉注意力多域特征提取器”（VA-MDFE），可从 RGB、边缘和深度域中提取注意力特征。接下来，“视觉关注下采样器”（VA-DS）负责融合和下采样多域特征。最后，一种新颖的“视觉注意力多感受野上采样器”（VA-MRFU）模块采用多个基于感受野的卷积，通过关注不同的信息尺度来对注意力特征进行上采样。在公共基准数据集 CASIA v2.0 上进行的实验结果证明了所提出模型的有效性。它轻松击败了现有的最先进技术，IoU 得分为 0.851，像素 F1 得分为 0.9195，像素 AUC 得分为 0.8989。</details>
**PDF:** <http://arxiv.org/pdf/2401.06995v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Deep Blind Super-Resolution for Satellite Video**<br />
**Title_cn:** 卫星视频的深度盲超分辨率<br />
**Authors:** Yi Xiao, Qiangqiang Yuan, Qiang Zhang, Liangpei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recent efforts have witnessed remarkable progress in Satellite Video Super-Resolution (SVSR). However, most SVSR methods usually assume the degradation is fixed and known, e.g., bicubic downsampling, which makes them vulnerable in real-world scenes with multiple and unknown degradations. To alleviate this issue, blind SR has thus become a research hotspot. Nevertheless, existing approaches are mainly engaged in blur kernel estimation while losing sight of another critical aspect for VSR tasks: temporal compensation, especially compensating for blurry and smooth pixels with vital sharpness from severely degraded satellite videos. Therefore, this paper proposes a practical Blind SVSR algorithm (BSVSR) to explore more sharp cues by considering the pixel-wise blur levels in a coarse-to-fine manner. Specifically, we employed multi-scale deformable convolution to coarsely aggregate the temporal redundancy into adjacent frames by window-slid progressive fusion. Then the adjacent features are finely merged into mid-feature using deformable attention, which measures the blur levels of pixels and assigns more weights to the informative pixels, thus inspiring the representation of sharpness. Moreover, we devise a pyramid spatial transformation module to adjust the solution space of sharp mid-feature, resulting in flexible feature adaptation in multi-level domains. Quantitative and qualitative evaluations on both simulated and real-world satellite videos demonstrate that our BSVSR performs favorably against state-of-the-art non-blind and blind SR models. Code will be available at https://github.com/XY-boy/Blind-Satellite-VSR</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的努力见证了卫星视频超分辨率（SVSR）方面的显着进展。然而，大多数 SVSR 方法通常假设退化是固定且已知的，例如双三次下采样，这使得它们在具有多个未知退化的现实场景中很容易受到攻击。为了缓解这一问题，盲SR因此成为研究热点。然而，现有方法主要从事模糊核估计，而忽视了 VSR 任务的另一个关键方面：时间补偿，特别是对严重退化的卫星视频中具有重要清晰度的模糊和平滑像素进行补偿。因此，本文提出了一种实用的盲 SVSR 算法（BSVSR），通过以从粗到细的方式考虑像素级模糊级别来探索更清晰的线索。具体来说，我们采用多尺度可变形卷积，通过窗口滑动渐进融合将时间冗余粗略地聚合到相邻帧中。然后，使用可变形注意力将相邻特征精细地合并到中间特征中，该注意力测量像素的模糊程度，并为信息丰富的像素分配更多权重，从而激发锐度的表示。此外，我们设计了金字塔空间变换模块来调整锐中间特征的解空间，从而实现多级域中灵活的特征适应。对模拟和现实世界卫星视频的定量和定性评估表明，我们的 BSVSR 的性能优于最先进的非盲和盲 SR 模型。代码可在 https://github.com/XY-boy/Blind-Satellite-VSR 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.07139v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Transformer for Object Re-Identification: A Survey**<br />
**Title_cn:** 用于对象重新识别的 Transformer：一项调查<br />
**Authors:** Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du<br />
**Abstract:** <details><summary>原文: </summary>Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from varying viewpoints. For a prolonged period, this field has been predominantly driven by deep convolutional neural networks. In recent years, the Transformer has witnessed remarkable advancements in computer vision, prompting an increasing body of research to delve into the application of Transformer in Re-ID. This paper provides a comprehensive review and in-depth analysis of the Transformer-based Re-ID. In categorizing existing works into Image/Video-Based Re-ID, Re-ID with limited data/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly elucidate the advantages demonstrated by the Transformer in addressing a multitude of challenges across these domains. Considering the trending unsupervised Re-ID, we propose a new Transformer baseline, UntransReID, achieving state-of-the-art performance on both single-/cross modal tasks. Besides, this survey also covers a wide range of Re-ID research objects, including progress in animal Re-ID. Given the diversity of species in animal Re-ID, we devise a standardized experimental benchmark and conduct extensive experiments to explore the applicability of Transformer for this task to facilitate future research. Finally, we discuss some important yet under-investigated open issues in the big foundation model era, we believe it will serve as a new handbook for researchers in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象重新识别（Re-ID）旨在从不同的角度识别和检索特定对象。长期以来，该领域主要由深度卷积神经网络驱动。近年来，Transformer 见证了计算机视觉领域的显着进步，促使越来越多的研究机构深入研究 Transformer 在 Re-ID 中的应用。本文对基于 Transformer 的 Re-ID 进行了全面的回顾和深入的分析。在将现有作品分类为基于图像/视频的 Re-ID、有限数据/注释的 Re-ID、跨模态 Re-ID 和特殊 Re-ID 场景时，我们彻底阐明了 Transformer 在解决众多问题方面所表现出的优势这些领域的挑战。考虑到无监督 Re-ID 的趋势，我们提出了一个新的 Transformer 基线 UntransReID，在单/跨模态任务上实现了最先进的性能。此外，本次调查还涵盖了广泛的Re-ID研究对象，包括动物Re-ID的进展。鉴于动物Re-ID中物种的多样性，我们设计了标准化的实验基准，并进行了大量的实验来探索Transformer在此任务中的适用性，以促进未来的研究。最后，我们讨论了大基础模型时代一些重要但尚未充分研究的开放问题，我们相信它将成为该领域研究人员的新手册。</details>
**PDF:** <http://arxiv.org/pdf/2401.06960v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **A New Method of Pixel-level In-situ U-value Measurement for Building Envelopes Based on Infrared Thermography**<br />
**Title_cn:** 基于红外热成像的建筑围护结构像素级U值原位测量新方法<br />
**Authors:** Zihao Wang, Yu Hou, Lucio Soibelman<br />
**Abstract:** <details><summary>原文: </summary>The potential energy loss of aging buildings traps building owners in a cycle of underfunding operations and overpaying maintenance costs. Energy auditors intending to generate an energy model of a target building for performance assessment may struggle to obtain accurate results as the spatial distribution of temperatures is not considered when calculating the U-value of the building envelope. This paper proposes a pixel-level method based on infrared thermography (IRT) that considers two-dimensional (2D) spatial temperature distributions of the outdoor and indoor surfaces of the target wall to generate a 2D U-value map of the wall. The result supports that the proposed method can better reflect the actual thermal insulation performance of the target wall compared to the current IRT-based methods that use a single-point room temperature as input.</details>
**Abstract_cn:** <details><summary>译文: </summary>老化建筑潜在的能源损失使业主陷入运营资金不足和维护成本过高的恶性循环。想要生成目标建筑的能源模型以进行性能评估的能源审计员可能很难获得准确的结果，因为在计算建筑围护结构的 U 值时没有考虑温度的空间分布。本文提出了一种基于红外热成像（IRT）的像素级方法，该方法考虑目标墙壁的室外和室内表面的二维（2D）空间温度分布，以生成墙壁的2D U值图。结果表明，与当前使用单点室温作为输入的基于 IRT 的方法相比，该方法可以更好地反映目标墙体的实际隔热性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07163v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **IVIM-Morph: Motion-compensated quantitative Intra-voxel Incoherent Motion (IVIM) analysis for functional fetal lung maturity assessment from diffusion-weighted MRI data**<br />
**Title_cn:** IVIM-Morph：运动补偿定量体素内不相干运动 (IVIM) 分析，用于根据扩散加权 MRI 数据评估功能性胎肺成熟度<br />
**Authors:** Noga Kertes, Yael Zaffrani-Reznikov, Onur Afacan, Sila Kurugol, Simon K. Warfield, Moti Freiman<br />
**Abstract:** <details><summary>原文: </summary>Quantitative analysis of pseudo-diffusion in diffusion-weighted magnetic resonance imaging (DWI) data shows potential for assessing fetal lung maturation and generating valuable imaging biomarkers. Yet, the clinical utility of DWI data is hindered by unavoidable fetal motion during acquisition. We present IVIM-morph, a self-supervised deep neural network model for motion-corrected quantitative analysis of DWI data using the Intra-voxel Incoherent Motion (IVIM) model. IVIM-morph combines two sub-networks, a registration sub-network, and an IVIM model fitting sub-network, enabling simultaneous estimation of IVIM model parameters and motion. To promote physically plausible image registration, we introduce a biophysically informed loss function that effectively balances registration and model-fitting quality. We validated the efficacy of IVIM-morph by establishing a correlation between the predicted IVIM model parameters of the lung and gestational age (GA) using fetal DWI data of 39 subjects. IVIM-morph exhibited a notably improved correlation with gestational age (GA) when performing in-vivo quantitative analysis of fetal lung DWI data during the canalicular phase. IVIM-morph shows potential in developing valuable biomarkers for non-invasive assessment of fetal lung maturity with DWI data. Moreover, its adaptability opens the door to potential applications in other clinical contexts where motion compensation is essential for quantitative DWI analysis. The IVIM-morph code is readily available at: https://github.com/TechnionComputationalMRILab/qDWI-Morph.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散加权磁共振成像 (DWI) 数据中伪扩散的定量分析显示了评估胎儿肺成熟度和生成有价值的成像生物标志物的潜力。然而，DWI 数据的临床应用受到采集过程中不可避免的胎儿运动的阻碍。我们提出了 IVIM-morph，这是一种自监督深度神经网络模型，用于使用体素内不相干运动 (IVIM) 模型对 DWI 数据进行运动校正定量分析。 IVIM-morph结合了两个子网络，一个配准子网络和一个IVIM模型拟合子网络，能够同时估计IVIM模型参数和运动。为了促进物理上合理的图像配准，我们引入了一种生物物理知情的损失函数，可以有效地平衡配准和模型拟合质量。我们使用 39 名受试者的胎儿 DWI 数据建立了预测的肺 IVIM 模型参数与胎龄 (GA) 之间的相关性，验证了 IVIM-morph 的功效。在对小管期胎儿肺 DWI 数据进行体内定量分析时，IVIM-morph 与胎龄 (GA) 的相关性显着改善。 IVIM-morph 显示出开发有价值的生物标志物的潜力，用于利用 DWI 数据对胎肺成熟度进行无创评估。此外，它的适应性为其他临床环境中的潜在应用打开了大门，在这些临床环境中运动补偿对于定量 DWI 分析至关重要。 IVIM-morph 代码可在以下网址轻松获得：https://github.com/TechnionComputationalMRILab/qDWI-Morph。</details>
**PDF:** <http://arxiv.org/pdf/2401.07126v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Revisiting Sampson Approximations for Geometric Estimation Problems**<br />
**Title_cn:** 重新审视几何估计问题的桑普森近似<br />
**Authors:** Felix Rydell, Angélica Torres, Viktor Larsson<br />
**Abstract:** <details><summary>原文: </summary>Many problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation ``agrees" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error).   In this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉中的许多问题可以表述为几何估计问题，即给定一组测量值（例如点对应），我们希望拟合一个与我们的观察结果一致的模型（例如基本矩阵）。这就需要对观察结果与给定模型“一致”的程度进行某种测量。自然的选择是考虑使观察结果完全满足约束条件的最小扰动。然而，对于许多问题来说，这个度量方法成本高昂，或者难以解决所谓的桑普森误差通过线性化方案来近似该几何误差。对于极几何，桑普森误差是一种流行的选择，并且在实践中已知它可以产生相应几何残差（重投影误差）的非常紧密的近似值。在这篇论文中，我们重新审视了桑普森近似，并提供了关于这种近似为何以及何时起作用的新的理论见解，以及在一些温和假设下提供了紧密性的明确界限。我们的理论结果在对真实数据的多次实验中以及在以下背景下得到了验证：不同的几何估计任务。</details>
**PDF:** <http://arxiv.org/pdf/2401.07114v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Progressive Feature Fusion Network for Enhancing Image Quality Assessment**<br />
**Title_cn:** 用于增强图像质量评估的渐进式特征融合网络<br />
**Authors:** Kaiqun Wu, Xiaoling Jiang, Rui Yu, Yonggang Luo, Tian Jiang, Xi Wu, Peng Wei<br />
**Abstract:** <details><summary>原文: </summary>Image compression has been applied in the fields of image storage and video broadcasting. However, it's formidably tough to distinguish the subtle quality differences between those distorted images generated by different algorithms. In this paper, we propose a new image quality assessment framework to decide which image is better in an image group. To capture the subtle differences, a fine-grained network is adopted to acquire multi-scale features. Subsequently, we design a cross subtract block for separating and gathering the information within positive and negative image pairs. Enabling image comparison in feature space. After that, a progressive feature fusion block is designed, which fuses multi-scale features in a novel progressive way. Hierarchical spatial 2D features can thus be processed gradually. Experimental results show that compared with the current mainstream image quality assessment methods, the proposed network can achieve more accurate image quality assessment and ranks second in the benchmark of CLIC in the image perceptual model track.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像压缩已应用于图像存储和视频广播领域。然而，区分不同算法生成的扭曲图像之间细微的质量差异非常困难。在本文中，我们提出了一种新的图像质量评估框架来决定图像组中哪张图像更好。为了捕捉细微的差异，采用细粒度网络来获取多尺度特征。随后，我们设计了一个交叉减法块，用于分离和收集正负图像对中的信息。在特征空间中启用图像比较。之后，设计了渐进特征融合块，以新颖的渐进方式融合多尺度特征。因此，可以逐步处理分层空间二维特征。实验结果表明，与目前主流的图像质量评估方法相比，该网络能够实现更准确的图像质量评估，在图像感知模型赛道的 CLIC 基准测试中排名第二。</details>
**PDF:** <http://arxiv.org/pdf/2401.06992v1><br />
**Code:** null<br />

