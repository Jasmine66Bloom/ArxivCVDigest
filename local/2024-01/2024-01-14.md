## [UPDATED!] **2024-01-14** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **A Strong Inductive Bias: Gzip for binary image classification**<br />
**Title_cn:** 强归纳偏差：用于二值图像分类的 Gzip<br />
**Authors:** Marco Scilipoti, Marina Fuster, Rodrigo Ramele<br />
**Abstract:** <details><summary>原文: </summary>Deep learning networks have become the de-facto standard in Computer Vision for industry and research. However, recent developments in their cousin, Natural Language Processing (NLP), have shown that there are areas where parameter-less models with strong inductive biases can serve as computationally cheaper and simpler alternatives. We propose such a model for binary image classification: a nearest neighbor classifier combined with a general purpose compressor like Gzip. We test and compare it against popular deep learning networks like Resnet, EfficientNet and Mobilenet and show that it achieves better accuracy and utilizes significantly less space, more than two order of magnitude, within a few-shot setting. As a result, we believe that this underlines the untapped potential of models with stronger inductive biases in few-shot scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习网络已成为工业和研究计算机视觉领域事实上的标准。然而，其近亲自然语言处理 (NLP) 的最新发展表明，在某些领域，具有强归纳偏差的无参数模型可以作为计算成本更低、更简单的替代方案。我们提出了这样一个用于二值图像分类的模型：最近邻分类器与 Gzip 等通用压缩器相结合。我们将其与 Resnet、EfficientNet 和 Mobilenet 等流行的深度学习网络进行测试和比较，结果表明，它在几次镜头设置中实现了更高的准确性，并且利用的空间显着减少，超过两个数量级。因此，我们认为这凸显了在少数场景中具有更强归纳偏差的模型尚未开发的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.07392v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Knee or ROC**<br />
**Title_cn:** 膝关节或 ROC<br />
**Authors:** Veronica Wendt, Byunggu Yu, Caleb Kelly, Junwhan Kim<br />
**Abstract:** <details><summary>原文: </summary>Self-attention transformers have demonstrated accuracy for image classification with smaller data sets. However, a limitation is that tests to-date are based upon single class image detection with known representation of image populations. For instances where the input image classes may be greater than one and test sets that lack full information on representation of image populations, accuracy calculations must adapt. The Receiver Operating Characteristic (ROC) accuracy thresh-old can address the instances of multi-class input images. However, this approach is unsuitable in instances where image population representation is unknown. We consider calculating accuracy using the knee method to determine threshold values on an ad-hoc basis. Results of ROC curve and knee thresholds for a multi-class data set, created from CIFAR-10 images, are discussed for multi-class image detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>自注意力变压器已经证明了使用较小数据集进行图像分类的准确性。然而，一个限制是迄今为止的测试基于具有已知图像群体表示的单类图像检测。对于输入图像类别可能大于 1 且测试集缺乏有关图像总体表示的完整信息的情况，必须调整精度计算。接收器操作特性（ROC）精度阈值可以解决多类输入图像的实例。然而，这种方法不适合图像群体表示未知的情况。我们考虑使用拐点法来计算准确性，以临时确定阈值。讨论了从 CIFAR-10 图像创建的多类数据集的 ROC 曲线和拐点阈值的结果，以进行多类图像检测。</details>
**PDF:** <http://arxiv.org/pdf/2401.07390v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search**<br />
**Title_cn:** 基于最近邻搜索的推土机距离高效近似<br />
**Authors:** Guangyu Meng, Ruyu Zhou, Liu Liu, Peixian Liang, Fang Liu, Danny Chen, Michael Niemier, X. Sharon Hu<br />
**Abstract:** <details><summary>原文: </summary>Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>推土机距离 (EMD) 是两个分布之间的重要相似性度量，用于计算机视觉和许多其他应用领域。然而，它的精确计算是计算和内存密集型的，这阻碍了它的可扩展性和对大规模问题的适用性。人们已经提出了各种近似 EMD 算法来降低计算成本，但它们的精度较低，并且可能需要额外的内存使用或手动参数调整。在本文中，我们提出了一种新方法 NNS-EMD，使用最近邻搜索 (NNS) 来近似 EMD，以实现高精度、低时间复杂度和高内存效率。 NNS 操作减少了每次 NNS 迭代中比较的数据点数量，并提供了并行处理的机会。我们通过 GPU 上的矢量化进一步加速 NNS-EMD，这对于大型数据集特别有益。我们将 NNS-EMD 与图像分类和检索任务上的精确 EMD 和最先进的近似 EMD 算法进行比较。我们还应用NNS-EMD来计算传输映射并实现图像之间的颜色传输。 NNS-EMD 的速度比精确的 EMD 实现快 44 倍到 135 倍，并且比现有的近似 EMD 方法具有更高的精度、加速和内存效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.07378v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Generation of Synthetic Images for Pedestrian Detection Using a Sequence of GANs**<br />
**Title_cn:** 使用 GAN 序列生成用于行人检测的合成图像<br />
**Authors:** Viktor Seib, Malte Roosen, Ida Germann, Stefan Wirtz, Dietrich Paulus<br />
**Abstract:** <details><summary>原文: </summary>Creating annotated datasets demands a substantial amount of manual effort. In this proof-of-concept work, we address this issue by proposing a novel image generation pipeline. The pipeline consists of three distinct generative adversarial networks (previously published), combined in a novel way to augment a dataset for pedestrian detection. Despite the fact that the generated images are not always visually pleasant to the human eye, our detection benchmark reveals that the results substantially surpass the baseline. The presented proof-of-concept work was done in 2020 and is now published as a technical report after a three years retention period.</details>
**Abstract_cn:** <details><summary>译文: </summary>创建带注释的数据集需要大量的手动工作。在这项概念验证工作中，我们通过提出一种新颖的图像生成管道来解决这个问题。该管道由三个不同的生成对抗网络（之前发布）组成，以一种新颖的方式组合起来，以增强行人检测的数据集。尽管生成的图像并不总是让人眼愉悦，但我们的检测基准显示结果大大超过了基线。所提出的概念验证工作于 2020 年完成，现已作为技术报告在保留三年后发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.07370v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Harnessing Machine Learning for Discerning AI-Generated Synthetic Images**<br />
**Title_cn:** 利用机器学习来识别人工智能生成的合成图像<br />
**Authors:** Yuyang Wang, Yizhi Hao, Amando Xu Cong<br />
**Abstract:** <details><summary>原文: </summary>In the realm of digital media, the advent of AI-generated synthetic images has introduced significant challenges in distinguishing between real and fabricated visual content. These images, often indistinguishable from authentic ones, pose a threat to the credibility of digital media, with potential implications for disinformation and fraud. Our research addresses this challenge by employing machine learning techniques to discern between AI-generated and genuine images. Central to our approach is the CIFAKE dataset, a comprehensive collection of images labeled as "Real" and "Fake". We refine and adapt advanced deep learning architectures like ResNet, VGGNet, and DenseNet, utilizing transfer learning to enhance their precision in identifying synthetic images. We also compare these with a baseline model comprising a vanilla Support Vector Machine (SVM) and a custom Convolutional Neural Network (CNN). The experimental results were significant, demonstrating that our optimized deep learning models outperform traditional methods, with DenseNet achieving an accuracy of 97.74%. Our application study contributes by applying and optimizing these advanced models for synthetic image detection, conducting a comparative analysis using various metrics, and demonstrating their superior capability in identifying AI-generated images over traditional machine learning techniques. This research not only advances the field of digital media integrity but also sets a foundation for future explorations into the ethical and technical dimensions of AI-generated content in digital media.</details>
**Abstract_cn:** <details><summary>译文: </summary>在数字媒体领域，人工智能生成的合成图像的出现给区分真实和伪造的视觉内容带来了重大挑战。这些图像通常与真实图像难以区分，对数字媒体的可信度构成威胁，并可能导致虚假信息和欺诈。我们的研究通过采用机器学习技术来区分人工智能生成的图像和真实图像来解决这一挑战。我们方法的核心是 CIFAKE 数据集，这是标记为“真实”和“假”的图像的综合集合。我们改进和调整了 ResNet、VGGNet 和 DenseNet 等先进的深度学习架构，利用迁移学习来提高识别合成图像的精度。我们还将它们与包含普通支持向量机 (SVM) 和自定义卷积神经网络 (CNN) 的基线模型进行比较。实验结果显着，表明我们优化的深度学习模型优于传统方法，DenseNet 的准确率达到 97.74%。我们的应用研究通过应用和优化这些用于合成图像检测的先进模型，使用各种指标进行比较分析，并证明它们在识别人工智能生成的图像方面优于传统机器学习技术的能力。这项研究不仅推进了数字媒体完整性领域，还为未来探索数字媒体中人工智能生成内容的道德和技术维度奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.07358v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Beyond Traditional Approaches: Multi-Task Network for Breast Ultrasound Diagnosis**<br />
**Title_cn:** 超越传统方法：用于乳腺超声诊断的多任务网络<br />
**Authors:** Dat T. Chung, Minh-Anh Dang, Mai-Anh Vu, Minh T. Nguyen, Thanh-Huy Nguyen, Vinh Q. Dinh<br />
**Abstract:** <details><summary>原文: </summary>Breast Ultrasound plays a vital role in cancer diagnosis as a non-invasive approach with cost-effective. In recent years, with the development of deep learning, many CNN-based approaches have been widely researched in both tumor localization and cancer classification tasks. Even though previous single models achieved great performance in both tasks, these methods have some limitations in inference time, GPU requirement, and separate fine-tuning for each model. In this study, we aim to redesign and build end-to-end multi-task architecture to conduct both segmentation and classification. With our proposed approach, we achieved outstanding performance and time efficiency, with 79.8% and 86.4% in DeepLabV3+ architecture in the segmentation task.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳房超声作为一种经济高效的非侵入性方法，在癌症诊断中发挥着至关重要的作用。近年来，随着深度学习的发展，许多基于CNN的方法在肿瘤定位和癌症分类任务中得到了广泛的研究。尽管以前的单一模型在这两项任务中都取得了出色的性能，但这些方法在推理时间、GPU 要求以及每个模型的单独微调方面存在一些限制。在本研究中，我们的目标是重新设计和构建端到端多任务架构以进行分割和分类。通过我们提出的方法，我们取得了出色的性能和时间效率，在分割任务中 DeepLabV3+ 架构中分别达到了 79.8% 和 86.4%。</details>
**PDF:** <http://arxiv.org/pdf/2401.07326v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized HD Map Construction**<br />
**Title_cn:** MapNeXt：重新审视在线矢量化高清地图构建的培训和缩放实践<br />
**Authors:** Toyota Li<br />
**Abstract:** <details><summary>原文: </summary>High-Definition (HD) maps are pivotal to autopilot navigation. Integrating the capability of lightweight HD map construction at runtime into a self-driving system recently emerges as a promising direction. In this surge, vision-only perception stands out, as a camera rig can still perceive the stereo information, let alone its appealing signature of portability and economy. The latest MapTR architecture solves the online HD map construction task in an end-to-end fashion but its potential is yet to be explored. In this work, we present a full-scale upgrade of MapTR and propose MapNeXt, the next generation of HD map learning architecture, delivering major contributions from the model training and scaling perspectives. After shedding light on the training dynamics of MapTR and exploiting the supervision from map elements thoroughly, MapNeXt-Tiny raises the mAP of MapTR-Tiny from 49.0% to 54.8%, without any architectural modifications. Enjoying the fruit of map segmentation pre-training, MapNeXt-Base further lifts the mAP up to 63.9% that has already outperformed the prior art, a multi-modality MapTR, by 1.4% while being $\sim1.8\times$ faster. Towards pushing the performance frontier to the next level, we draw two conclusions on practical model scaling: increased query favors a larger decoder network for adequate digestion; a large backbone steadily promotes the final accuracy without bells and whistles. Building upon these two rules of thumb, MapNeXt-Huge achieves state-of-the-art performance on the challenging nuScenes benchmark. Specifically, we push the mapless vision-only single-model performance to be over 78% for the first time, exceeding the best model from existing methods by 16%.</details>
**Abstract_cn:** <details><summary>译文: </summary>高清 (HD) 地图对于自动驾驶导航至关重要。将运行时轻量级高清地图构建能力集成到自动驾驶系统中最近成为一个有前途的方向。在这一浪潮中，仅视觉感知脱颖而出，因为相机设备仍然可以感知立体信息，更不用说其便携性和经济性的吸引人的标志了。最新的MapTR架构以端到端的方式解决了在线高精地图构建任务，但其潜力还有待挖掘。在这项工作中，我们对 MapTR 进行了全面升级，并提出了下一代高清地图学习架构 MapNeXt，从模型训练和缩放角度做出了重大贡献。在阐明MapTR的训练动态并充分利用地图元素的监督后，MapNeXt-Tiny将MapTR-Tiny的mAP从49.0％提高到54.8％，而无需任何架构修改。享受地图分割预训练的成果后，MapNeXt-Base 将 mAP 进一步提升至 63.9%，已经比现有技术（多模态 MapTR）提高了 1.4%，同时速度提高了 $\sim1.8\times$。为了将性能前沿推向新的水平，我们在实际模型扩展方面得出了两个结论：增加查询有利于更大的解码器网络以进行充分消化；一个庞大的骨干稳定地提升了最终的准确性，没有花哨的东西。基于这两条经验法则，MapNeXt-Huge 在具有挑战性的 nuScenes 基准测试中实现了最先进的性能。具体来说，我们首次将无地图视觉单一模型性能推至 78% 以上，比现有方法的最佳模型高出 16%。</details>
**PDF:** <http://arxiv.org/pdf/2401.07323v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Semi-supervised Semantic Segmentation using Redesigned Self-Training for White Blood Cel**<br />
**Title_cn:** 使用重新设计的白细胞自我训练进行半监督语义分割<br />
**Authors:** Vinh Quoc Luu, Duy Khanh Le, Huy Thanh Nguyen, Minh Thanh Nguyen, Thinh Tien Nguyen, Vinh Quang Dinh<br />
**Abstract:** <details><summary>原文: </summary>Artificial Intelligence (AI) in healthcare, especially in white blood cell cancer diagnosis, is hindered by two primary challenges: the lack of large-scale labeled datasets for white blood cell (WBC) segmentation and outdated segmentation methods. To address the first challenge, a semi-supervised learning framework should be brought to efficiently annotate the large dataset. In this work, we address this issue by proposing a novel self-training pipeline with the incorporation of FixMatch. We discover that by incorporating FixMatch in the self-training pipeline, the performance improves in the majority of cases. Our performance achieved the best performance with the self-training scheme with consistency on DeepLab-V3 architecture and ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC datasets, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>医疗保健领域的人工智能（AI），特别是白细胞癌症诊断，受到两个主要挑战的阻碍：缺乏用于白细胞（WBC）分割的大规模标记数据集和过时的分割方法。为了解决第一个挑战，应该引入半监督学习框架来有效地注释大型数据集。在这项工作中，我们通过提出一种结合 FixMatch 的新颖的自我训练管道来解决这个问题。我们发现，通过将 FixMatch 纳入自训练管道中，大多数情况下性能都会提高。我们的性能在 DeepLab-V3 架构和 ResNet-50 上具有一致性的自训练方案取得了最佳性能，在 Cheng 1、Zheng 2 和 LISC 数据集上分别达到 90.69%、87.37% 和 76.49%。</details>
**PDF:** <http://arxiv.org/pdf/2401.07278v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning and Uncertainty Estimation**<br />
**Title_cn:** SpineCLUE：使用对比学习和不确定性估计自动椎骨识别<br />
**Authors:** Sheng Zhang, Minheng Chen, Junxian Wu, Ziyue Zhang, Tonglong Li, Cheng Xue, Youyong Kong<br />
**Abstract:** <details><summary>原文: </summary>Vertebrae identification in arbitrary fields-of-view plays a crucial role in diagnosing spine disease. Most spine CT contain only local regions, such as the neck, chest, and abdomen. Therefore, identification should not depend on specific vertebrae or a particular number of vertebrae being visible. Existing methods at the spine-level are unable to meet this challenge. In this paper, we propose a three-stage method to address the challenges in 3D CT vertebrae identification at vertebrae-level. By sequentially performing the tasks of vertebrae localization, segmentation, and identification, the anatomical prior information of the vertebrae is effectively utilized throughout the process. Specifically, we introduce a dual-factor density clustering algorithm to acquire localization information for individual vertebra, thereby facilitating subsequent segmentation and identification processes. In addition, to tackle the issue of interclass similarity and intra-class variability, we pre-train our identification network by using a supervised contrastive learning method. To further optimize the identification results, we estimated the uncertainty of the classification network and utilized the message fusion module to combine the uncertainty scores, while aggregating global information about the spine. Our method achieves state-of-the-art results on the VerSe19 and VerSe20 challenge benchmarks. Additionally, our approach demonstrates outstanding generalization performance on an collected dataset containing a wide range of abnormal cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>任意视野中的椎骨识别在诊断脊柱疾病中起着至关重要的作用。大多数脊柱 CT 仅包含局部区域，例如颈部、胸部和腹部。因此，识别不应依赖于特定的椎骨或可见的特定数量的椎骨。现有的脊椎级方法无法应对这一挑战。在本文中，我们提出了一种三阶段方法来解决椎骨级 3D CT 椎骨识别的挑战。通过顺序执行椎骨定位、分割和识别的任务，在整个过程中有效地利用了椎骨的解剖先验信息。具体来说，我们引入了双因子密度聚类算法来获取单个椎骨的定位信息，从而促进后续的分割和识别过程。此外，为了解决类间相似性和类内变异性问题，我们通过使用监督对比学习方法来预训练我们的识别网络。为了进一步优化识别结果，我们估计了分类网络的不确定性，并利用消息融合模块来组合不确定性分数，同时聚合有关脊柱的全局信息。我们的方法在 VerSe19 和 VerSe20 挑战基准上取得了最先进的结果。此外，我们的方法在包含各种异常情况的收集数据集上表现出出色的泛化性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07271v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **3D Landmark Detection on Human Point Clouds: A Benchmark and A Dual Cascade Point Transformer Framework**<br />
**Title_cn:** 人体点云上的 3D 地标检测：基准和双级联点转换器框架<br />
**Authors:** Fan Zhang, Shuyi Mao, Qing Li, Xiaojiang Peng<br />
**Abstract:** <details><summary>原文: </summary>3D landmark detection plays a pivotal role in various applications such as 3D registration, pose estimation, and virtual try-on. While considerable success has been achieved in 2D human landmark detection or pose estimation, there is a notable scarcity of reported works on landmark detection in unordered 3D point clouds. This paper introduces a novel challenge, namely 3D landmark detection on human point clouds, presenting two primary contributions. Firstly, we establish a comprehensive human point cloud dataset, named HPoint103, designed to support the 3D landmark detection community. This dataset comprises 103 human point clouds created with commercial software and actors, each manually annotated with 11 stable landmarks. Secondly, we propose a Dual Cascade Point Transformer (D-CPT) model for precise point-based landmark detection. D-CPT gradually refines the landmarks through cascade Transformer decoder layers across the entire point cloud stream, simultaneously enhancing landmark coordinates with a RefineNet over local regions. Comparative evaluations with popular point-based methods on HPoint103 and the public dataset DHP19 demonstrate the dramatic outperformance of our D-CPT. Additionally, the integration of our RefineNet into existing methods consistently improves performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 地标检测在 3D 配准、姿态估计和虚拟试穿等各种应用中发挥着关键作用。尽管在 2D 人体地标检测或姿态估计方面取得了相当大的成功，但有关无序 3D 点云中地标检测的报道工作却明显缺乏。本文介绍了一个新的挑战，即人体点云上的 3D 地标检测，提出了两个主要贡献。首先，我们建立了一个全面的人体点云数据集，名为 HPoint103，旨在支持 3D 地标检测社区。该数据集包含使用商业软件和参与者创建的 103 个人类点云，每个点云均手动注释有 11 个稳定地标。其次，我们提出了一种双级联点变换器（D-CPT）模型，用于精确的基于点的地标检测。 D-CPT 通过跨整个点云流的级联 Transformer 解码器层逐渐细化地标，同时使用局部区域的 RefineNet 增强地标坐标。在 HPoint103 和公共数据集 DHP19 上与流行的基于点的方法进行的比较评估表明我们的 D-CPT 具有显着的性能优势。此外，将我们的 RefineNet 集成到现有方法中可以持续提高性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07251v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **MIMIC: Mask Image Pre-training with Mix Contrastive Fine-tuning for Facial Expression Recognition**<br />
**Title_cn:** MIMIC：针对面部表情识别的混合对比微调的掩模图像预训练<br />
**Authors:** Fan Zhang, Xiaobao Guo, Xiaojiang Peng, Alex Kot<br />
**Abstract:** <details><summary>原文: </summary>Cutting-edge research in facial expression recognition (FER) currently favors the utilization of convolutional neural networks (CNNs) backbone which is supervisedly pre-trained on face recognition datasets for feature extraction. However, due to the vast scale of face recognition datasets and the high cost associated with collecting facial labels, this pre-training paradigm incurs significant expenses. Towards this end, we propose to pre-train vision Transformers (ViTs) through a self-supervised approach on a mid-scale general image dataset. In addition, when compared with the domain disparity existing between face datasets and FER datasets, the divergence between general datasets and FER datasets is more pronounced. Therefore, we propose a contrastive fine-tuning approach to effectively mitigate this domain disparity. Specifically, we introduce a novel FER training paradigm named Mask Image pre-training with MIx Contrastive fine-tuning (MIMIC). In the initial phase, we pre-train the ViT via masked image reconstruction on general images. Subsequently, in the fine-tuning stage, we introduce a mix-supervised contrastive learning process, which enhances the model with a more extensive range of positive samples by the mixing strategy. Through extensive experiments conducted on three benchmark datasets, we demonstrate that our MIMIC outperforms the previous training paradigm, showing its capability to learn better representations. Remarkably, the results indicate that the vanilla ViT can achieve impressive performance without the need for intricate, auxiliary-designed modules. Moreover, when scaling up the model size, MIMIC exhibits no performance saturation and is superior to the current state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部表情识别 (FER) 的前沿研究目前倾向于使用卷积神经网络 (CNN) 主干，该主干在面部识别数据集上进行有监督的预训练以进行特征提取。然而，由于人脸识别数据集规模庞大，而且收集人脸标签的成本很高，这种预训练范例会产生大量费用。为此，我们建议通过在中型通用图像数据集上的自监督方法来预训练视觉 Transformers (ViT)。此外，与人脸数据集和 FER 数据集之间存在的域差异相比，通用数据集和 FER 数据集之间的差异更加明显。因此，我们提出了一种对比微调方法来有效减轻这种域差异。具体来说，我们引入了一种新颖的 FER 训练范例，名为带有 MIx 对比微调的掩模图像预训练 (MIMIC)。在初始阶段，我们通过一般图像上的掩模图像重建来预训练 ViT。随后，在微调阶段，我们引入了混合监督对比学习过程，通过混合策略以更广泛的正样本增强模型。通过对三个基准数据集进行的广泛实验，我们证明了我们的 MIMIC 优于之前的训练范式，显示了其学习更好表示的能力。值得注意的是，结果表明普通 ViT 无需复杂的辅助设计模块即可实现令人印象深刻的性能。此外，当扩大模型尺寸时，MIMIC 没有表现出性能饱和，并且优于当前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07245v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **DCDet: Dynamic Cross-based 3D Object Detector**<br />
**Title_cn:** DCDet：基于动态交叉的 3D 对象检测器<br />
**Authors:** Shuai Liu, Boyang Li, Zhiyu Fang, Kai Huang<br />
**Abstract:** <details><summary>原文: </summary>Recently, significant progress has been made in the research of 3D object detection. However, most prior studies have focused on the utilization of center-based or anchor-based label assignment schemes. Alternative label assignment strategies remain unexplored in 3D object detection. We find that the center-based label assignment often fails to generate sufficient positive samples for training, while the anchor-based label assignment tends to encounter an imbalanced issue when handling objects of varying scales. To solve these issues, we introduce a dynamic cross label assignment (DCLA) scheme, which dynamically assigns positive samples for each object from a cross-shaped region, thus providing sufficient and balanced positive samples for training. Furthermore, to address the challenge of accurately regressing objects with varying scales, we put forth a rotation-weighted Intersection over Union (RWIoU) metric to replace the widely used L1 metric in regression loss. Extensive experiments demonstrate the generality and effectiveness of our DCLA and RWIoU-based regression loss. The Code will be available at https://github.com/Say2L/DCDet.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，3D物体检测的研究取得了重大进展。然而，大多数先前的研究都集中在基于中心或基于锚的标签分配方案的利用。在 3D 对象检测中，替代标签分配策略仍未得到探索。我们发现基于中心的标签分配通常无法生成足够的正样本用于训练，而基于锚的标签分配在处理不同尺度的对象时往往会遇到不平衡的问题。为了解决这些问题，我们引入了动态交叉标签分配（DCLA）方案，该方案从十字形区域为每个对象动态分配正样本，从而为训练提供充足且平衡的正样本。此外，为了解决准确回归不同尺度对象的挑战，我们提出了旋转加权并集交集（RWIoU）指标来取代回归损失中广泛使用的 L1 指标。大量的实验证明了我们基于 DCLA 和 RWIoU 的回归损失的通用性和有效性。该代码将在 https://github.com/Say2L/DCDet.git 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.07240v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Application of 2D Homography for High Resolution Traffic Data Collection using CCTV Cameras**<br />
**Title_cn:** 使用闭路电视摄像机采集高分辨率交通数据的 2D 单应性应用<br />
**Authors:** Linlin Zhang, Xiang Yu, Abdulateef Daud, Abdul Rashid Mussah, Yaw Adu-Gyamfi<br />
**Abstract:** <details><summary>原文: </summary>Traffic cameras remain the primary source data for surveillance activities such as congestion and incident monitoring. To date, State agencies continue to rely on manual effort to extract data from networked cameras due to limitations of the current automatic vision systems including requirements for complex camera calibration and inability to generate high resolution data. This study implements a three-stage video analytics framework for extracting high-resolution traffic data such vehicle counts, speed, and acceleration from infrastructure-mounted CCTV cameras. The key components of the framework include object recognition, perspective transformation, and vehicle trajectory reconstruction for traffic data collection. First, a state-of-the-art vehicle recognition model is implemented to detect and classify vehicles. Next, to correct for camera distortion and reduce partial occlusion, an algorithm inspired by two-point linear perspective is utilized to extracts the region of interest (ROI) automatically, while a 2D homography technique transforms the CCTV view to bird's-eye view (BEV). Cameras are calibrated with a two-layer matrix system to enable the extraction of speed and acceleration by converting image coordinates to real-world measurements. Individual vehicle trajectories are constructed and compared in BEV using two time-space-feature-based object trackers, namely Motpy and BYTETrack. The results of the current study showed about +/- 4.5% error rate for directional traffic counts, less than 10% MSE for speed bias between camera estimates in comparison to estimates from probe data sources. Extracting high-resolution data from traffic cameras has several implications, ranging from improvements in traffic management and identify dangerous driving behavior, high-risk areas for accidents, and other safety concerns, enabling proactive measures to reduce accidents and fatalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>交通摄像头仍然是拥堵和事件监控等监控活动的主要来源数据。迄今为止，由于当前自动视觉系统的局限性，包括复杂的相机校准要求和无法生成高分辨率数据，国家机构继续依靠手动从网络相机中提取数据。本研究实施了一个三阶段视频分析框架，用于从基础设施安装的闭路电视摄像机中提取高分辨率交通数据，例如车辆数量、速度和加速度。该框架的关键组件包括对象识别、透视变换和用于交通数据收集的车辆轨迹重建。首先，采用最先进的车辆识别模型来检测和分类车辆。接下来，为了校正摄像机失真并减少部分遮挡，采用受两点线性透视启发的算法自动提取感兴趣区域 (ROI)，同时 2D 单应性技术将闭路电视视图转换为鸟瞰图 (BEV) ）。相机使用两层矩阵系统进行校准，通过将图像坐标转换为现实世界的测量值来提取速度和加速度。使用两个基于时空特征的对象跟踪器（即 Motpy 和 BYTETrack）在 BEV 中构建和比较各个车辆轨迹。当前研究的结果显示，定向交通计数的错误率约为 +/- 4.5%，与探测数据源的估计值相比，摄像机估计值之间的速度偏差 MSE 小于 10%。从交通摄像头提取高分辨率数据具有多种意义，包括改善交通管理和识别危险驾驶行为、事故高风险区域以及其他安全问题，从而能够采取主动措施减少事故和死亡人数。</details>
**PDF:** <http://arxiv.org/pdf/2401.07220v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Exploring Compressed Image Representation as a Perceptual Proxy: A Study**<br />
**Title_cn:** 探索压缩图像表示作为感知代理：一项研究<br />
**Authors:** Chen-Hsiu Huang, Ja-Ling Wu<br />
**Abstract:** <details><summary>原文: </summary>We propose an end-to-end learned image compression codec wherein the analysis transform is jointly trained with an object classification task. This study affirms that the compressed latent representation can predict human perceptual distance judgments with an accuracy comparable to a custom-tailored DNN-based quality metric. We further investigate various neural encoders and demonstrate the effectiveness of employing the analysis transform as a perceptual loss network for image tasks beyond quality judgments. Our experiments show that the off-the-shelf neural encoder proves proficient in perceptual modeling without needing an additional VGG network. We expect this research to serve as a valuable reference developing of a semantic-aware and coding-efficient neural encoder.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种端到端学习图像压缩编解码器，其中分析变换与对象分类任务联合训练。这项研究证实，压缩的潜在表示可以预测人类感知距离判断，其准确度与定制的基于 DNN 的质量指标相当。我们进一步研究了各种神经编码器，并证明了使用分析变换作为感知损失网络来处理超出质量判断的图像任务的有效性。我们的实验表明，现成的神经编码器在感知建模方面表现出色，无需额外的 VGG 网络。我们希望这项研究能够为开发语义感知和编码高效的神经编码器提供有价值的参考。</details>
**PDF:** <http://arxiv.org/pdf/2401.07200v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Unsupervised Domain Adaptation Using Compact Internal Representations**<br />
**Title_cn:** 使用紧凑内部表示的无监督域适应<br />
**Authors:** Mohammad Rostami<br />
**Abstract:** <details><summary>原文: </summary>A major technique for tackling unsupervised domain adaptation involves mapping data points from both the source and target domains into a shared embedding space. The mapping encoder to the embedding space is trained such that the embedding space becomes domain agnostic, allowing a classifier trained on the source domain to generalize well on the target domain. To further enhance the performance of unsupervised domain adaptation (UDA), we develop an additional technique which makes the internal distribution of the source domain more compact, thereby improving the model's ability to generalize in the target domain.We demonstrate that by increasing the margins between data representations for different classes in the embedding space, we can improve the model performance for UDA. To make the internal representation more compact, we estimate the internally learned multi-modal distribution of the source domain as Gaussian mixture model (GMM). Utilizing the estimated GMM, we enhance the separation between different classes in the source domain, thereby mitigating the effects of domain shift. We offer theoretical analysis to support outperofrmance of our method. To evaluate the effectiveness of our approach, we conduct experiments on widely used UDA benchmark UDA datasets. The results indicate that our method enhances model generalizability and outperforms existing techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>解决无监督域适应的主要技术涉及将源域和目标域的数据点映射到共享嵌入空间。训练映射编码器到嵌入空间，使得嵌入空间变得与域无关，从而允许在源域上训练的分类器在目标域上很好地泛化。为了进一步增强无监督域适应（UDA）的性能，我们开发了一种额外的技术，使源域的内部分布更加紧凑，从而提高模型在目标域中的泛化能力。我们证明，通过增加嵌入空间中不同类的数据表示，我们可以提高 UDA 的模型性能。为了使内部表示更加紧凑，我们将内部学习的源域多模态分布估计为高斯混合模型（GMM）。利用估计的 GMM，我们增强了源域中不同类之间的分离，从而减轻了域转移的影响。我们提供理论分析来支持我们的方法的表现。为了评估我们方法的有效性，我们对广泛使用的 UDA 基准 UDA 数据集进行了实验。结果表明，我们的方法增强了模型的通用性并优于现有技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.07207v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Enhanced Few-Shot Class-Incremental Learning via Ensemble Models**<br />
**Title_cn:** 通过集成模型增强少样本类增量学习<br />
**Authors:** Mingli Zhu, Zihao Zhu, Sihong Chen, Chen Chen, Baoyuan Wu<br />
**Abstract:** <details><summary>原文: </summary>Few-shot class-incremental learning (FSCIL) aims to continually fit new classes with limited training data, while maintaining the performance of previously learned classes. The main challenges are overfitting the rare new training samples and forgetting old classes. While catastrophic forgetting has been extensively studied, the overfitting problem has attracted less attention in FSCIL. To tackle overfitting challenge, we design a new ensemble model framework cooperated with data augmentation to boost generalization. In this way, the enhanced model works as a library storing abundant features to guarantee fast adaptation to downstream tasks. Specifically, the multi-input multi-output ensemble structure is applied with a spatial-aware data augmentation strategy, aiming at diversifying the feature extractor and alleviating overfitting in incremental sessions. Moreover, self-supervised learning is also integrated to further improve the model generalization. Comprehensive experimental results show that the proposed method can indeed mitigate the overfitting problem in FSCIL, and outperform the state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头类增量学习（FSCIL）旨在不断地用有限的训练数据适应新的类，同时保持以前学习的类的性能。主要挑战是过度拟合罕见的新训练样本和忘记旧课程。虽然灾难性遗忘已被广泛研究，但过拟合问题在 FSCIL 中引起的关注较少。为了解决过度拟合的挑战，我们设计了一个新的集成模型框架，与数据增强相结合以提高泛化能力。这样，增强模型就像一个存储丰富特征的库，以保证快速适应下游任务。具体来说，多输入多输出集成结构应用了空间感知数据增强策略，旨在使特征提取器多样化并减轻增量会话中的过度拟合。此外，还集成了自监督学习，进一步提高模型的泛化能力。综合实验结果表明，所提出的方法确实可以缓解 FSCIL 中的过拟合问题，并且优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07208v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Depth-agnostic Single Image Dehazing**<br />
**Title_cn:** 与深度无关的单图像去雾<br />
**Authors:** Honglei Xu, Yan Shu, Shaohui Liu<br />
**Abstract:** <details><summary>原文: </summary>Single image dehazing is a challenging ill-posed problem. Existing datasets for training deep learning-based methods can be generated by hand-crafted or synthetic schemes. However, the former often suffers from small scales, while the latter forces models to learn scene depth instead of haze distribution, decreasing their dehazing ability. To overcome the problem, we propose a simple yet novel synthetic method to decouple the relationship between haze density and scene depth, by which a depth-agnostic dataset (DA-HAZE) is generated. Meanwhile, a Global Shuffle Strategy (GSS) is proposed for generating differently scaled datasets, thereby enhancing the generalization ability of the model. Extensive experiments indicate that models trained on DA-HAZE achieve significant improvements on real-world benchmarks, with less discrepancy between SOTS and DA-SOTS (the test set of DA-HAZE). Additionally, Depth-agnostic dehazing is a more complicated task because of the lack of depth prior. Therefore, an efficient architecture with stronger feature modeling ability and fewer computational costs is necessary. We revisit the U-Net-based architectures for dehazing, in which dedicatedly designed blocks are incorporated. However, the performances of blocks are constrained by limited feature fusion methods. To this end, we propose a Convolutional Skip Connection (CSC) module, allowing vanilla feature fusion methods to achieve promising results with minimal costs. Extensive experimental results demonstrate that current state-of-the-art methods. equipped with CSC can achieve better performance and reasonable computational expense, whether the haze distribution is relevant to the scene depth.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像去雾是一个具有挑战性的不适定问题。用于训练基于深度学习的方法的现有数据集可以通过手工设计或合成方案生成。然而，前者通常会受到小尺度的影响，而后者则迫使模型学习场景深度而不是雾霾分布，从而降低了其去雾能力。为了克服这个问题，我们提出了一种简单而新颖的合成方法来解耦雾霾密度和场景深度之间的关系，通过该方法生成深度不可知的数据集（DA-HAZE）。同时，提出了全局洗牌策略（GSS）来生成不同尺度的数据集，从而增强模型的泛化能力。大量实验表明，在 DA-HAZE 上训练的模型在现实世界基准上取得了显着改进，SOTS 和 DA-SOTS（DA-HAZE 的测试集）之间的差异较小。此外，由于缺乏先验深度，与深度无关的去雾是一项更加复杂的任务。因此，需要一种具有更强特征建模能力和更少计算成本的高效架构。我们重新审视基于 U-Net 的去雾架构，其中包含专门设计的模块。然而，块的性能受到有限的特征融合方法的限制。为此，我们提出了一个卷积跳跃连接（CSC）模块，允许普通特征融合方法以最小的成本实现有希望的结果。大量的实验结果证明了当前最先进的方法。无论雾度分布是否与场景深度相关，配备CSC都可以获得更好的性能和合理的计算费用。</details>
**PDF:** <http://arxiv.org/pdf/2401.07213v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation**<br />
**Title_cn:** MapGPT：统一视觉和语言导航的地图引导提示<br />
**Authors:** Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong<br />
**Abstract:** <details><summary>原文: </summary>Embodied agents equipped with GPT as their brain have exhibited extraordinary thinking and decision-making abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT to handle excessive environmental information and select potential locations within localized environments, without constructing an effective ''global-view'' (e.g., a commonly-used map) for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based path-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically, we convert a topological map constructed online into prompts to encourage map-guided global exploration, and require the agent to explicitly output and update multi-step path planning to avoid getting stuck in local exploration. Extensive experiments demonstrate that our MapGPT is effective, achieving impressive performance on both the R2R and REVERIE datasets (38.8% and 28.4% success rate, respectively) and showcasing the newly emerged global thinking and path planning capabilities of the GPT model. Unlike previous VLN agents, which require separate parameters fine-tuning or specific prompt design to accommodate various instruction styles across different datasets, our MapGPT is more unified as it can adapt to different instruction styles seamlessly, which is the first of its kind in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>以 GPT 作为大脑的实体智能体在各种任务中表现出了非凡的思维和决策能力。然而，现有的用于视觉和语言导航（VLN）的零样本智能体仅提示 GPT 处理过多的环境信息并在局部环境中选择潜在位置，而没有构建有效的“全局视图”（例如，常见的-使用地图）让智能体了解整体环境。在这项工作中，我们提出了一种新颖的基于地图引导的 GPT 路径规划代理，称为 MapGPT，用于零样本 VLN 任务。具体来说，我们将在线构建的拓扑图转换为提示，以鼓励地图引导的全局探索，并要求代理显式输出和更新多步路径规划，以避免陷入局部探索。大量实验证明我们的 MapGPT 是有效的，在 R2R 和 REVERIE 数据集上都取得了令人印象深刻的性能（成功率分别为 38.8% 和 28.4%），并展示了 GPT 模型新出现的全局思维和路径规划能力。与之前的 VLN 代理需要单独的参数微调或特定的提示设计以适应不同数据集的各种指令风格不同，我们的 MapGPT 更加统一，因为它可以无缝适应不同的指令风格，这是该领域的首创。</details>
**PDF:** <http://arxiv.org/pdf/2401.07314v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency**<br />
**Title_cn:** 使用跨模态一致性的自监督基于事件的单目深度估计<br />
**Authors:** Junyu Zhu, Lina Liu, Bofeng Jiang, Feng Wen, Hongbo Zhang, Wanlong Li, Yong Liu<br />
**Abstract:** <details><summary>原文: </summary>An event camera is a novel vision sensor that can capture per-pixel brightness changes and output a stream of asynchronous ``events''. It has advantages over conventional cameras in those scenes with high-speed motions and challenging lighting conditions because of the high temporal resolution, high dynamic range, low bandwidth, low power consumption, and no motion blur. Therefore, several supervised monocular depth estimation from events is proposed to address scenes difficult for conventional cameras. However, depth annotation is costly and time-consuming. In this paper, to lower the annotation cost, we propose a self-supervised event-based monocular depth estimation framework named EMoDepth. EMoDepth constrains the training process using the cross-modal consistency from intensity frames that are aligned with events in the pixel coordinate. Moreover, in inference, only events are used for monocular depth prediction. Additionally, we design a multi-scale skip-connection architecture to effectively fuse features for depth estimation while maintaining high inference speed. Experiments on MVSEC and DSEC datasets demonstrate that our contributions are effective and that the accuracy can outperform existing supervised event-based and unsupervised frame-based methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>事件相机是一种新颖的视觉传感器，可以捕获每个像素的亮度变化并输出异步“事件”流。在高速运动和具有挑战性的照明条件的场景中，它比传统相机具有高时间分辨率、高动态范围、低带宽、低功耗和无运动模糊的优势。因此，提出了几种基于事件的有监督单目深度估计来解决传统相机难以处理的场景。然而，深度标注成本高昂且耗时。在本文中，为了降低注释成本，我们提出了一种名为 EMoDepth 的自监督的基于事件的单目深度估计框架。 EMoDepth 使用与像素坐标中的事件对齐的强度帧的跨模式一致性来约束训练过程。而且，在推理中，仅使用事件来进行单目深度预测。此外，我们设计了一种多尺度跳跃连接架构，以有效融合深度估计的特征，同时保持高推理速度。在 MVSEC 和 DSEC 数据集上的实验表明，我们的贡献是有效的，并且准确性可以优于现有的基于事件的监督和基于帧的无监督方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07218v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving**<br />
**Title_cn:** RSUD20K：自动驾驶中道路场景理解的数据集<br />
**Authors:** Hasib Zunair, Shakib Khan, A. Ben Hamza<br />
**Abstract:** <details><summary>原文: </summary>Road scene understanding is crucial in autonomous driving, enabling machines to perceive the visual environment. However, recent object detectors tailored for learning on datasets collected from certain geographical locations struggle to generalize across different locations. In this paper, we present RSUD20K, a new dataset for road scene understanding, comprised of over 20K high-resolution images from the driving perspective on Bangladesh roads, and includes 130K bounding box annotations for 13 objects. This challenging dataset encompasses diverse road scenes, narrow streets and highways, featuring objects from different viewpoints and scenes from crowded environments with densely cluttered objects and various weather conditions. Our work significantly improves upon previous efforts, providing detailed annotations and increased object complexity. We thoroughly examine the dataset, benchmarking various state-of-the-art object detectors and exploring large vision models as image annotators.</details>
**Abstract_cn:** <details><summary>译文: </summary>道路场景理解对于自动驾驶至关重要，它使机器能够感知视觉环境。然而，最近为学习从某些地理位置收集的数据集而定制的物体检测器很难在不同的位置进行泛化。在本文中，我们提出了 RSUD20K，这是一个用于道路场景理解的新数据集，由孟加拉国道路驾驶视角的超过 20K 高分辨率图像组成，并包含 13 个物体的 130K 边界框注释。这个具有挑战性的数据集包含不同的道路场景、狭窄的街道和高速公路，具有不同视角的物体以及来自拥挤环境、物体密集和各种天气条件的场景。我们的工作显着改进了以前的工作，提供了详细的注释并增加了对象的复杂性。我们彻底检查数据集，对各种最先进的对象检测器进行基准测试，并探索作为图像注释器的大型视觉模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.07322v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FROST-BRDF: A Fast and Robust Optimal Sampling Technique for BRDF Acquisition**<br />
**Title_cn:** FROST-BRDF：一种快速、鲁棒的 BRDF 采集最优采样技术<br />
**Authors:** Ehsan Miandji, Tanaboon Tongbuasirilai, Saghi Hajisharif, Behnaz Kavoosighafi, Jonas Unger<br />
**Abstract:** <details><summary>原文: </summary>Efficient and accurate BRDF acquisition of real world materials is a challenging research problem that requires sampling millions of incident light and viewing directions. To accelerate the acquisition process, one needs to find a minimal set of sampling directions such that the recovery of the full BRDF is accurate and robust given such samples. In this paper, we formulate BRDF acquisition as a compressed sensing problem, where the sensing operator is one that performs sub-sampling of the BRDF signal according to a set of optimal sample directions. To solve this problem, we propose the Fast and Robust Optimal Sampling Technique (FROST) for designing a provably optimal sub-sampling operator that places light-view samples such that the recovery error is minimized. FROST casts the problem of designing an optimal sub-sampling operator for compressed sensing into a sparse representation formulation under the Multiple Measurement Vector (MMV) signal model. The proposed reformulation is exact, i.e. without any approximations, hence it converts an intractable combinatorial problem into one that can be solved with standard optimization techniques. As a result, FROST is accompanied by strong theoretical guarantees from the field of compressed sensing. We perform a thorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly available BRDF datasets and show significant advantages compared to the state-of-the-art with respect to reconstruction quality. Finally, FROST is simple, both conceptually and in terms of implementation, it produces consistent results at each run, and it is at least two orders of magnitude faster than the prior art.</details>
**Abstract_cn:** <details><summary>译文: </summary>高效、准确地获取现实世界材料的 BRDF 是一个具有挑战性的研究问题，需要对数百万个入射光和观察方向进行采样。为了加速采集过程，需要找到一组最小的采样方向，以便在给定此类样本的情况下，完整 BRDF 的恢复准确且稳健。在本文中，我们将 BRDF 采集表述为压缩感知问题，其中感知算子是根据一组最佳采样方向对 BRDF 信号执行子采样的算子。为了解决这个问题，我们提出了快速稳健的最佳采样技术（FROST），用于设计可证明最佳的子采样算子，该算子放置光视图样本以使恢复误差最小化。 FROST 将压缩感知的最佳子采样算子设计问题转化为多测量向量 (MMV) 信号模型下的稀疏表示公式。所提出的重新表述是精确的，即没有任何近似，因此它将棘手的组合问题转换为可以用标准优化技术解决的问题。因此，FROST 伴随着来自压缩感知领域的强有力的理论保证。我们使用公开可用的 BRDF 数据集进行 10 倍交叉验证，对 FROST-BRDF 进行了全面分析，并在重建质量方面显示出与最先进技术相比的显着优势。最后，FROST 在概念上和实现上都很简单，每次运行都会产生一致的结果，并且比现有技术至少快两个数量级。</details>
**PDF:** <http://arxiv.org/pdf/2401.07283v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **City Scene Super-Resolution via Geometric Error Minimization**<br />
**Title_cn:** 通过几何误差最小化的城市场景超分辨率<br />
**Authors:** Zhengyang Lu, Feng Wang<br />
**Abstract:** <details><summary>原文: </summary>Super-resolution techniques are crucial in improving image granularity, particularly in complex urban scenes, where preserving geometric structures is vital for data-informed cultural heritage applications. In this paper, we propose a city scene super-resolution method via geometric error minimization. The geometric-consistent mechanism leverages the Hough Transform to extract regular geometric features in city scenes, enabling the computation of geometric errors between low-resolution and high-resolution images. By minimizing mixed mean square error and geometric align error during the super-resolution process, the proposed method efficiently restores details and geometric regularities. Extensive validations on the SET14, BSD300, Cityscapes and GSV-Cities datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, especially in urban scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>超分辨率技术对于提高图像粒度至关重要，特别是在复杂的城市场景中，其中保留几何结构对于数据通知的文化遗产应用至关重要。在本文中，我们提出了一种通过几何误差最小化的城市场景超分辨率方法。几何一致机制利用霍夫变换提取城市场景中的规则几何特征，从而能够计算低分辨率和高分辨率图像之间的几何误差。通过最小化超分辨率过程中的混合均方误差和几何对准误差，该方法有效地恢复了细节和几何规律。对 SET14、BSD300、Cityscapes 和 GSV-Cities 数据集的广泛验证表明，所提出的方法优于现有的最先进方法，尤其是在城市场景中。</details>
**PDF:** <http://arxiv.org/pdf/2401.07272v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models**<br />
**Title_cn:** Crafter：针对深度模型上基于反转的身份盗窃的面部特征制作<br />
**Authors:** Shiming Wang, Zhe Ji, Liyao Xiang, Hao Zhang, Xinbing Wang, Chenghu Zhou, Bo Li<br />
**Abstract:** <details><summary>原文: </summary>With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着边缘（例如移动设备）功能的增强和更严格的隐私要求，支持深度学习的应用程序在边缘预处理敏感原始数据并将特征传输到后端云以进行进一步处理已成为最新趋势。加工。一个典型的应用是对从不同个体收集的面部图像运行机器学习 (ML) 服务。为了防止身份盗窃，传统方法通常依赖于基于对抗性游戏的方法来从特征中泄露身份信息。然而，此类方法无法防御自适应攻击，其中攻击者针对已知的防御策略采取反制措施。我们提出了 Crafter，一种部署在边缘的特征制作机制，以保护身份信息免受自适应模型反转攻击，同时确保 ML 任务在云端正确执行。关键的防御策略是将攻击者误导到非私有先验，攻击者从中几乎无法获得私有身份。在这种情况下，精心设计的特征对于具有自适应模型更新的攻击者来说就像毒药训练样本。实验结果表明，Crafter 成功防御了基本攻击和可能的自适应攻击，这是最先进的基于对抗性博弈的方法无法实现的。</details>
**PDF:** <http://arxiv.org/pdf/2401.07205v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Left-right Discrepancy for Adversarial Attack on Stereo Networks**<br />
**Title_cn:** 立体网络对抗性攻击的左右差异<br />
**Authors:** Pengfei Wang, Xiaofei Hui, Beijia Lu, Nimrod Lilith, Jun Liu, Sameer Alam<br />
**Abstract:** <details><summary>原文: </summary>Stereo matching neural networks often involve a Siamese structure to extract intermediate features from left and right images. The similarity between these intermediate left-right features significantly impacts the accuracy of disparity estimation. In this paper, we introduce a novel adversarial attack approach that generates perturbation noise specifically designed to maximize the discrepancy between left and right image features. Extensive experiments demonstrate the superior capability of our method to induce larger prediction errors in stereo neural networks, e.g. outperforming existing state-of-the-art attack methods by 219% MAE on the KITTI dataset and 85% MAE on the Scene Flow dataset. Additionally, we extend our approach to include a proxy network black-box attack method, eliminating the need for access to stereo neural network. This method leverages an arbitrary network from a different vision task as a proxy to generate adversarial noise, effectively causing the stereo network to produce erroneous predictions. Our findings highlight a notable sensitivity of stereo networks to discrepancies in shallow layer features, offering valuable insights that could guide future research in enhancing the robustness of stereo vision systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>立体匹配神经网络通常涉及连体结构，以从左图像和右图像中提取中间特征。这些中间左右特征之间的相似性显着影响视差估计的准确性。在本文中，我们介绍了一种新颖的对抗性攻击方法，该方法生成专门设计用于最大化左右图像特征之间的差异的扰动噪声。大量的实验证明了我们的方法在立体神经网络中引起更大的预测误差的卓越能力，例如在 KITTI 数据集上比现有最先进的攻击方法高出 219% MAE，在 Scene Flow 数据集上比现有最先进的攻击方法高出 85% MAE。此外，我们扩展了我们的方法，包括代理网络黑盒攻击方法，消除了访问立体神经网络的需要。该方法利用来自不同视觉任务的任意网络作为代理来生成对抗性噪声，从而有效地导致立体网络产生错误的预测。我们的研究结果强调了立体网络对浅层特征差异的显着敏感性，提供了宝贵的见解，可以指导未来增强立体视觉系统鲁棒性的研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.07188v1><br />
**Code:** null<br />

