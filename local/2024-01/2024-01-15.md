## [UPDATED!] **2024-01-15** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Convolutional Neural Network Compression via Dynamic Parameter Rank Pruning**<br />
**Title_cn:** 通过动态参数秩剪枝的卷积神经网络压缩<br />
**Authors:** Manish Sharma, Jamison Heard, Eli Saber, Panos P. Markopoulos<br />
**Abstract:** <details><summary>原文: </summary>While Convolutional Neural Networks (CNNs) excel at learning complex latent-space representations, their over-parameterization can lead to overfitting and reduced performance, particularly with limited data. This, alongside their high computational and memory demands, limits the applicability of CNNs for edge deployment. Low-rank matrix approximation has emerged as a promising approach to reduce CNN parameters, but its application presents challenges including rank selection and performance loss. To address these issues, we propose an efficient training method for CNN compression via dynamic parameter rank pruning. Our approach integrates efficient matrix factorization and novel regularization techniques, forming a robust framework for dynamic rank reduction and model compression. We use Singular Value Decomposition (SVD) to model low-rank convolutional filters and dense weight matrices and we achieve model compression by training the SVD factors with back-propagation in an end-to-end way. We evaluate our method on an array of modern CNNs, including ResNet-18, ResNet-20, and ResNet-32, and datasets like CIFAR-10, CIFAR-100, and ImageNet (2012), showcasing its applicability in computer vision. Our experiments show that the proposed method can yield substantial storage savings while maintaining or even enhancing classification performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然卷积神经网络 (CNN) 擅长学习复杂的潜在空间表示，但它们的过度参数化可能会导致过度拟合和性能下降，尤其是在数据有限的情况下。这加上其高计算和内存需求，限制了 CNN 在边缘部署中的适用性。低秩矩阵近似已成为减少 CNN 参数的一种有前途的方法，但其应用面临包括秩选择和性能损失在内的挑战。为了解决这些问题，我们提出了一种通过动态参数秩剪枝进行 CNN 压缩的有效训练方法。我们的方法集成了高效的矩阵分解和新颖的正则化技术，形成了用于动态降级和模型压缩的强大框架。我们使用奇异值分解（SVD）对低秩卷积滤波器和密集权重矩阵进行建模，并通过以端到端的方式使用反向传播训练 SVD 因子来实现模型压缩。我们在一系列现代 CNN（包括 ResNet-18、ResNet-20 和 ResNet-32）以及 CIFAR-10、CIFAR-100 和 ImageNet (2012) 等数据集上评估我们的方法，展示了其在计算机视觉中的适用性。我们的实验表明，所提出的方法可以节省大量存储空间，同时保持甚至增强分类性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08014v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Jewelry Recognition via Encoder-Decoder Models**<br />
**Title_cn:** 通过编码器-解码器模型进行珠宝识别<br />
**Authors:** José M. Alcalde-Llergo, Enrique Yeguas-Bolívar, Andrea Zingoni, Alejandro Fuerte-Jurado<br />
**Abstract:** <details><summary>原文: </summary>Jewelry recognition is a complex task due to the different styles and designs of accessories. Precise descriptions of the various accessories is something that today can only be achieved by experts in the field of jewelry. In this work, we propose an approach for jewelry recognition using computer vision techniques and image captioning, trying to simulate this expert human behavior of analyzing accessories. The proposed methodology consist on using different image captioning models to detect the jewels from an image and generate a natural language description of the accessory. Then, this description is also utilized to classify the accessories at different levels of detail. The generated caption includes details such as the type of jewel, color, material, and design. To demonstrate the effectiveness of the proposed method in accurately recognizing different types of jewels, a dataset consisting of images of accessories belonging to jewelry stores in C\'ordoba (Spain) has been created. After testing the different image captioning architectures designed, the final model achieves a captioning accuracy of 95\%. The proposed methodology has the potential to be used in various applications such as jewelry e-commerce, inventory management or automatic jewels recognition to analyze people's tastes and social status.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于配饰的款式和设计不同，珠宝识别是一项复杂的任务。如今，只有珠宝领域的专家才能对各种配件进行精确描述。在这项工作中，我们提出了一种使用计算机视觉技术和图像字幕进行珠宝识别的方法，试图模拟这种专家分析配件的人类行为。所提出的方法包括使用不同的图像字幕模型来检测图像中的珠宝并生成配件的自然语言描述。然后，该描述还用于对配件进行不同细节级别的分类。生成的标题包括珠宝类型、颜色、材料和设计等详细信息。为了证明所提出的方法在准确识别不同类型珠宝方面的有效性，创建了一个由属于科尔多瓦（西班牙）珠宝店的配饰图像组成的数据集。在测试了设计的不同图像字幕架构后，最终模型的字幕准确率达到了 95%。所提出的方法有可能用于各种应用，例如珠宝电子商务、库存管理或自动珠宝识别，以分析人们的品味和社会地位。</details>
**PDF:** <http://arxiv.org/pdf/2401.08003v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **How does self-supervised pretraining improve robustness against noisy labels across various medical image classification datasets?**<br />
**Title_cn:** 自监督预训练如何提高各种医学图像分类数据集中针对噪声标签的鲁棒性？<br />
**Authors:** Bidur Khanal, Binod Bhattarai, Bishesh Khanal, Cristian Linte<br />
**Abstract:** <details><summary>原文: </summary>Noisy labels can significantly impact medical image classification, particularly in deep learning, by corrupting learned features. Self-supervised pretraining, which doesn't rely on labeled data, can enhance robustness against noisy labels. However, this robustness varies based on factors like the number of classes, dataset complexity, and training size. In medical images, subtle inter-class differences and modality-specific characteristics add complexity. Previous research hasn't comprehensively explored the interplay between self-supervised learning and robustness against noisy labels in medical image classification, considering all these factors. In this study, we address three key questions: i) How does label noise impact various medical image classification datasets? ii) Which types of medical image datasets are more challenging to learn and more affected by label noise? iii) How do different self-supervised pretraining methods enhance robustness across various medical image datasets? Our results show that DermNet, among five datasets (Fetal plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging but exhibits greater robustness against noisy labels. Additionally, contrastive learning stands out among the eight self-supervised methods as the most effective approach to enhance robustness against noisy labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>噪声标签会破坏学习到的特征，从而显着影响医学图像分类，特别是在深度学习中。自监督预训练不依赖于标记数据，可以增强针对噪声标签的鲁棒性。然而，这种鲁棒性会根据类别数量、数据集复杂性和训练规模等因素而变化。在医学图像中，微妙的类间差异和特定模态特征增加了复杂性。考虑到所有这些因素，先前的研究尚未全面探讨医学图像分类中自监督学习和针对噪声标签的鲁棒性之间的相互作用。在本研究中，我们解决了三个关键问题：i）标签噪声如何影响各种医学图像分类数据集？ ii) 哪些类型的医学图像数据集更难学习并且更容易受到标签噪声的影响？ iii）不同的自监督预训练方法如何增强各种医学图像数据集的鲁棒性？我们的结果表明，在五个数据集（Fetalplane、DermNet、COVID-DU-Ex、MURA、NCT-CRC-HE-100K）中，DermNet 是最具挑战性的，但对噪声标签表现出更强的鲁棒性。此外，对比学习在八种自我监督方法中脱颖而出，成为增强对噪声标签的鲁棒性的最有效方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07990v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Machine Perceptual Quality: Evaluating the Impact of Severe Lossy Compression on Audio and Image Models**<br />
**Title_cn:** 机器感知质量：评估严重有损压缩对音频和图像模型的影响<br />
**Authors:** Dan Jacobellis, Daniel Cummings, Neeraja J. Yadwadkar<br />
**Abstract:** <details><summary>原文: </summary>In the field of neural data compression, the prevailing focus has been on optimizing algorithms for either classical distortion metrics, such as PSNR or SSIM, or human perceptual quality. With increasing amounts of data consumed by machines rather than humans, a new paradigm of machine-oriented compression$\unicode{x2013}$which prioritizes the retention of features salient for machine perception over traditional human-centric criteria$\unicode{x2013}$has emerged, creating several new challenges to the development, evaluation, and deployment of systems utilizing lossy compression. In particular, it is unclear how different approaches to lossy compression will affect the performance of downstream machine perception tasks. To address this under-explored area, we evaluate various perception models$\unicode{x2013}$including image classification, image segmentation, speech recognition, and music source separation$\unicode{x2013}$under severe lossy compression. We utilize several popular codecs spanning conventional, neural, and generative compression architectures. Our results indicate three key findings: (1) using generative compression, it is feasible to leverage highly compressed data while incurring a negligible impact on machine perceptual quality; (2) machine perceptual quality correlates strongly with deep similarity metrics, indicating a crucial role of these metrics in the development of machine-oriented codecs; and (3) using lossy compressed datasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive scenarios where lossy compression increases machine perceptual quality rather than degrading it. To encourage engagement on this growing area of research, our code and experiments are available at: https://github.com/danjacobellis/MPQ.</details>
**Abstract_cn:** <details><summary>译文: </summary>在神经数据压缩领域，普遍关注的焦点是优化经典失真指标（例如 PSNR 或 SSIM）或人类感知质量的算法。随着机器而不是人类消耗的数据量不断增加，一种面向机器的压缩新范式$\unicode{x2013}$优先保留机器感知的显着特征，而不是传统的以人为中心的标准$\unicode{x2013}$的出现，给有损压缩系统的开发、评估和部署带来了一些新的挑战。特别是，目前尚不清楚不同的有损压缩方法将如何影响下游机器感知任务的性能。为了解决这个尚未探索的领域，我们在严重有损压缩下评估了各种感知模型$\unicode{x2013}$，包括图像分类、图像分割、语音识别和音乐源分离$\unicode{x2013}$。我们利用多种流行的编解码器，涵盖传统、神经和生成压缩架构。我们的结果表明了三个关键发现：（1）使用生成压缩，可以利用高度压缩的数据，同时对机器感知质量的影响可以忽略不计； （2）机器感知质量与深度相似性度量密切相关，表明这些度量在面向机器的编解码器的开发中发挥着至关重要的作用； (3) 使用有损压缩数据集（例如 ImageNet）进行预训练可能会导致违反直觉的情况，即有损压缩会提高而不是降低机器感知质量。为了鼓励参与这一不断发展的研究领域，我们的代码和实验可在以下网址获取：https://github.com/danjacobellis/MPQ。</details>
**PDF:** <http://arxiv.org/pdf/2401.07957v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Vertical Federated Image Segmentation**<br />
**Title_cn:** 垂直联合图像分割<br />
**Authors:** Paul K. Mandal, Cole Leo<br />
**Abstract:** <details><summary>原文: </summary>With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and currently the only) implementation of a system that can work under the constraints of a VFL environment and perform image segmentation while maintaining nominal accuracies. We achieved this by utilizing an FCN that boasts the ability to operate on federates that lack labelled data and privately share the respective weights with a central server, that of which hosts the necessary features for classification. Tests were conducted on the CamVid dataset in order to determine the impact of heavy feature compression required for the transfer of information between federates, as well as to reach nominal conclusions about the overall performance metrics when working under such constraints.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着针对基于图像的问题的人工智能解决方案的普及，人们越来越关注数据隐私和获取。在很多情况下，信息位于单独的数据孤岛中，开发人员可能很难以适合机器学习模型开发的方式整合所有信息。除此之外，这些局部数据区域的一部分可能无法访问标记的地面事实。这表明他们有能力得出数字结论，但由于缺乏相关信息而无法进行分类。这种确定通常可以忽略不计，尤其是在尝试开发通常需要这种功能的基于图像的解决方案时。在这种情况下，我们提出了一种创新的垂直联合学习（VFL）模型架构，可以在这种常见的条件下运行。这是第一个（也是目前唯一一个）可以在 VFL 环境的约束下工作并在保持标称精度的同时执行图像分割的系统实现。我们通过利用 FCN 来实现这一目标，该 FCN 拥有在缺乏标记数据的联邦上运行的能力，并与中央服务器私下共享各自的权重，中央服务器托管分类所需的功能。在 CamVid 数据集上进行了测试，以确定联邦之间信息传输所需的重度特征压缩的影响，以及在此类约束下工作时得出有关整体性能指标的名义结论。</details>
**PDF:** <http://arxiv.org/pdf/2401.07931v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Machine Learning Based Object Tracking**<br />
**Title_cn:** 基于机器学习的对象跟踪<br />
**Authors:** Md Rakibul Karim Akanda, Joshua Reynolds, Treylin Jackson, Milijah Gray<br />
**Abstract:** <details><summary>原文: </summary>Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文执行了基于机器学习的对象检测以及对象跟踪。作者能够使用开放计算机视觉（更广为人知的名称为 OpenCV）围绕对象设置一系列兴趣 (ROI)。接下来，使用跟踪算法来维持对物体的跟踪，同时操作两个伺服电机以保持物体在框架中居中。本文包含详细的过程和代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.07929v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **VeCAF: VLM-empowered Collaborative Active Finetuning with Training Objective Awareness**<br />
**Title_cn:** VeCAF：VLM 赋能的具有训练目标意识的协作主动微调<br />
**Authors:** Rongyu Zhang, Zefan Cai, Huanrui Yang, Zidong Liu, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Baobao Chang, Yuan Du, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Finetuning a pretrained vision model (PVM) is a common technique for learning downstream vision tasks. The conventional finetuning process with the randomly sampled data points results in diminished training efficiency. To address this drawback, we propose a novel approach, VLM-empowered Collaborative Active Finetuning (VeCAF). VeCAF optimizes a parametric data selection model by incorporating the training objective of the model being tuned. Effectively, this guides the PVM towards the performance goal with improved data and computational efficiency. As vision-language models (VLMs) have achieved significant advancements by establishing a robust connection between image and language domains, we exploit the inherent semantic richness of the text embedding space and utilize text embedding of pretrained VLM models to augment PVM image features for better data selection and finetuning. Furthermore, the flexibility of text-domain augmentation gives VeCAF a unique ability to handle out-of-distribution scenarios without external augmented data. Extensive experiments show the leading performance and high efficiency of VeCAF that is superior to baselines in both in-distribution and out-of-distribution image classification tasks. On ImageNet, VeCAF needs up to 3.3x less training batches to reach the target performance compared to full finetuning and achieves 2.8% accuracy improvement over SOTA methods with the same number of batches.</details>
**Abstract_cn:** <details><summary>译文: </summary>微调预训练视觉模型（PVM）是学习下游视觉任务的常用技术。使用随机采样数据点的传统微调过程会导致训练效率降低。为了解决这个缺点，我们提出了一种新方法，即基于 VLM 的协作主动微调（VeCAF）。 VeCAF 通过结合正在调整的模型的训练目标来优化参数数据选择模型。实际上，这可以通过提高数据和计算效率来引导 PVM 实现性能目标。由于视觉语言模型 (VLM) 通过在图像和语言域之间建立强大的连接而取得了显着的进步，我们利用文本嵌入空间固有的语义丰富性，并利用预训练的 VLM 模型的文本嵌入来增强 PVM 图像特征以获得更好的数据选择和微调。此外，文本域增强的灵活性使 VeCAF 具有独特的能力，无需外部增强数据即可处理分布外场景。大量实验表明，VeCAF 在分布内和分布外图像分类任务中均优于基线，具有领先的性能和高效率。在 ImageNet 上，与完全微调相比，VeCAF 需要最多减少 3.3 倍的训练批次才能达到目标性能，并且与相同批次数量的 SOTA 方法相比，精度提高了 2.8%。</details>
**PDF:** <http://arxiv.org/pdf/2401.07853v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Phenotyping calcification in vascular tissues using artificial intelligence**<br />
**Title_cn:** 使用人工智能对血管组织中的钙化进行表型分析<br />
**Authors:** Mehdi Ramezanpour, Anne M. Robertson, Yasutaka Tobe, Xiaowei Jia, Juan R. Cebral<br />
**Abstract:** <details><summary>原文: </summary>Vascular calcification is implicated as an important factor in major adverse cardiovascular events (MACE), including heart attack and stroke. A controversy remains over how to integrate the diverse forms of vascular calcification into clinical risk assessment tools. Even the commonly used calcium score for coronary arteries, which assumes risk scales positively with total calcification, has important inconsistencies. Fundamental studies are needed to determine how risk is influenced by the diverse calcification phenotypes. However, studies of these kinds are hindered by the lack of high-throughput, objective, and non-destructive tools for classifying calcification in imaging data sets. Here, we introduce a new classification system for phenotyping calcification along with a semi-automated, non-destructive pipeline that can distinguish these phenotypes in even atherosclerotic tissues. The pipeline includes a deep-learning-based framework for segmenting lipid pools in noisy micro-CT images and an unsupervised clustering framework for categorizing calcification based on size, clustering, and topology. This approach is illustrated for five vascular specimens, providing phenotyping for thousands of calcification particles across as many as 3200 images in less than seven hours. Average Dice Similarity Coefficients of 0.96 and 0.87 could be achieved for tissue and lipid pool, respectively, with training and validation needed on only 13 images despite the high heterogeneity in these tissues. By introducing an efficient and comprehensive approach to phenotyping calcification, this work enables large-scale studies to identify a more reliable indicator of the risk of cardiovascular events, a leading cause of global mortality and morbidity.</details>
**Abstract_cn:** <details><summary>译文: </summary>血管钙化是导致心脏病和中风等主要不良心血管事件 (MACE) 的重要因素。关于如何将不同形式的血管钙化整合到临床风险评估工具中仍然存在争议。即使是常用的冠状动脉钙评分（假设风险与总钙化呈正相关）也存在严重的不一致。需要进行基础研究来确定不同钙化表型如何影响风险。然而，由于缺乏用于对成像数据集中的钙化进行分类的高通量、客观和非破坏性工具，此类研究受到阻碍。在这里，我们引入了一种新的钙化表型分类系统，以及半自动化、非破坏性的管道，甚至可以在动脉粥样硬化组织中区分这些表型。该管道包括一个基于深度学习的框架，用于分割嘈杂的微 CT 图像中的脂质池，以及一个无监督的聚类框架，用于根据大小、聚类和拓扑对钙化进行分类。该方法以 5 个血管标本为例进行了说明，可在不到 7 小时的时间内对多达 3200 张图像中的数千个钙化颗粒进行表型分析。组织和脂质池的平均 Dice 相似系数分别为 0.96 和 0.87，尽管这些组织具有很高的异质性，但仅需要 13 张图像进行训练和验证。通过引入一种有效且全面的钙化表型分析方法，这项工作使大规模研究能够确定心血管事件风险的更可靠指标，而心血管事件是全球死亡率和发病率的主要原因。</details>
**PDF:** <http://arxiv.org/pdf/2401.07825v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Pedestrian Detection in Low-Light Conditions: A Comprehensive Survey**<br />
**Title_cn:** 弱光条件下的行人检测：综合调查<br />
**Authors:** Bahareh Ghari, Ali Tourani, Asadollah Shahbahrami, Georgi Gaydadjiev<br />
**Abstract:** <details><summary>原文: </summary>Pedestrian detection remains a critical problem in various domains, such as computer vision, surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety. Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies (i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48%) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than six percent of the works.</details>
**Abstract_cn:** <details><summary>译文: </summary>行人检测仍然是计算机视觉、监控和自动驾驶等各个领域的一个关键问题。特别是，在弱光条件和能见度较低的情况下准确、即时地检测行人对于自动驾驶汽车防止事故和挽救生命至关重要。本文旨在全面调查专门针对弱光条件的各种行人检测方法、基线和数据集。该调查讨论了夜间检测行人所面临的挑战，并探讨了近年来提出的解决这一问题的最先进方法。这些方法涵盖了不同的范围，包括基于深度学习、基于特征和混合方法，这些方法在增强具有挑战性的照明条件下的行人检测性能方面显示出了有希望的结果。此外，本文还强调了该领域当前的研究方向，并确定了值得研究人员进一步研究的潜在解决方案。通过彻底检查弱光条件下的行人检测技术，该调查旨在促进更安全、更可靠的自动驾驶系统以及与行人安全相关的其他应用的发展。因此，该领域当前的大多数方法都使用基于深度学习的图像融合方法（即早期、中途和晚期融合）来实现准确可靠的行人检测。此外，该领域的大多数作品（约 48%）都在 KAIST 数据集上进行了评估，而作者录制的现实世界视频源仅在不到 6% 的作品中使用。</details>
**PDF:** <http://arxiv.org/pdf/2401.07801v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification**<br />
**Title_cn:** 融合超声心动图图像和医疗记录以进行连续患者分层<br />
**Authors:** Nathan Painchaud, Pierre-Yves Courand, Pierre-Marc Jodoin, Nicolas Duchateau, Olivier Bernard<br />
**Abstract:** <details><summary>原文: </summary>Deep learning now enables automatic and robust extraction of cardiac function descriptors from echocardiographic sequences, such as ejection fraction or strain. These descriptors provide fine-grained information that physicians consider, in conjunction with more global variables from the clinical record, to assess patients' condition. Drawing on novel transformer models applied to tabular data (e.g., variables from electronic health records), we propose a method that considers all descriptors extracted from medical records and echocardiograms to learn the representation of a difficult-to-characterize cardiovascular pathology, namely hypertension. Our method first projects each variable into its own representation space using modality-specific approaches. These standardized representations of multimodal data are then fed to a transformer encoder, which learns to merge them into a comprehensive representation of the patient through a pretext task of predicting a clinical rating. This pretext task is formulated as an ordinal classification to enforce a pathological continuum in the representation space. We observe the major trends along this continuum for a cohort of 239 hypertensive patients to describe, with unprecedented gradation, the effect of hypertension on a number of cardiac function descriptors. Our analysis shows that i) pretrained weights from a foundation model allow to reach good performance (83% accuracy) even with limited data (less than 200 training samples), ii) trends across the population are reproducible between trainings, and iii) for descriptors whose interactions with hypertension are well documented, patterns are consistent with prior physiological knowledge.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习现在可以从超声心动图序列中自动、稳健地提取心脏功能描述符，例如射血分数或应变。这些描述符提供了医生考虑的细粒度信息，结合临床记录中的更多全局变量来评估患者的病情。利用应用于表格数据（例如，电子健康记录中的变量）的新颖变压器模型，我们提出了一种方法，该方法考虑从医疗记录和超声心动图中提取的所有描述符，以学习难以表征的心血管病理学的表示，即高血压。我们的方法首先使用特定于模态的方法将每个变量投影到其自己的表示空间中。然后，这些多模态数据的标准化表示被馈送到变压器编码器，编码器学习通过预测临床评级的借口任务将它们合并成患者的综合表示。该借口任务被表述为顺序分类，以在表示空间中强制执行病态连续体。我们观察了 239 名高血压患者在这一连续过程中的主要趋势，以前所未有的分级来描述高血压对许多心功能指标的影响。我们的分析表明，i) 即使数据有限（少于 200 个训练样本），来自基础模型的预训练权重也能达到良好的性能（83% 准确度），ii) 整个群体的趋势在训练之间是可重现的，以及 iii) 描述符其与高血压的相互作用已有充分记录，其模式与先前的生理知识一致。</details>
**PDF:** <http://arxiv.org/pdf/2401.07796v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Improving OCR Quality in 19th Century Historical Documents Using a Combined Machine Learning Based Approach**<br />
**Title_cn:** 使用基于机器学习的组合方法提高 19 世纪历史文档的 OCR 质量<br />
**Authors:** David Fleischhacker, Wolfgang Goederle, Roman Kern<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses a major challenge to historical research on the 19th century. Large quantities of sources have become digitally available for the first time, while extraction techniques are lagging behind. Therefore, we researched machine learning (ML) models to recognise and extract complex data structures in a high-value historical primary source, the Schematismus. It records every single person in the Habsburg civil service above a certain hierarchical level between 1702 and 1918 and documents the genesis of the central administration over two centuries. Its complex and intricate structure as well as its enormous size have so far made any more comprehensive analysis of the administrative and social structure of the later Habsburg Empire on the basis of this source impossible. We pursued two central objectives: Primarily, the improvement of the OCR quality, for which we considered an improved structure recognition to be essential; in the further course, it turned out that this also made the extraction of the data structure possible. We chose Faster R-CNN as base for the ML architecture for structure recognition. In order to obtain the required amount of training data quickly and economically, we synthesised Hof- und Staatsschematismus-style data, which we used to train our model. The model was then fine-tuned with a smaller set of manually annotated historical source data. We then used Tesseract-OCR, which was further optimised for the style of our documents, to complete the combined structure extraction and OCR process. Results show a significant decrease in the two standard parameters of OCR-performance, WER and CER (where lower values are better). Combined structure detection and fine-tuned OCR improved CER and WER values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER).</details>
**Abstract_cn:** <details><summary>译文: </summary>本文讨论了 19 世纪历史研究面临的重大挑战。大量资源首次以数字方式提供，但提取技术却相对滞后。因此，我们研究了机器学习 (ML) 模型，以识别和提取高价值历史主要来源 Schematismus 中的复杂数据结构。它记录了 1702 年至 1918 年间哈布斯堡王朝公务员系统中某一等级以上的每一个人，并记录了两个世纪以来中央政府的起源。其错综复杂的结构以及庞大的规模，迄今无法根据这一资料对后来的哈布斯堡帝国的行政和社会结构进行更全面的分析。我们追求两个中心目标：首先是提高 OCR 质量，为此我们认为改进结构识别至关重要；在进一步的过程中，事实证明这也使得数据结构的提取成为可能。我们选择 Faster R-CNN 作为用于结构识别的 ML 架构的基础。为了快速、经济地获得所需数量的训练数据，我们合成了 Hof- und Staatsschematismus 风格的数据，用于训练我们的模型。然后使用较小的一组手动注释的历史源数据对该模型进行微调。然后，我们使用 Tesseract-OCR（针对我们文档的风格进行了进一步优化）来完成结构提取和 OCR 的组合过程。结果显示 OCR 性能的两个标准参数 WER 和 CER 显着下降（其中值越低越好）。结合结构检测和微调 OCR，CER 和 WER 值显着提高了 71.98% (CER)，分别提高了 52.49% (WER)。</details>
**PDF:** <http://arxiv.org/pdf/2401.07787v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Seeing the Unseen: Visual Common Sense for Semantic Placement**<br />
**Title_cn:** 看到看不见的东西：语义放置的视觉常识<br />
**Authors:** Ram Ramrakhya, Aniruddha Kembhavi, Dhruv Batra, Zsolt Kira, Kuo-Hao Zeng, Luca Weihs<br />
**Abstract:** <details><summary>原文: </summary>Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding what is not present. Specifically, given an image (e.g. of a living room) and name of an object ("cushion"), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Semantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), and AR devices (automatically rendering an object in the user's space). Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context from web, and then remove that object from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. Using this, we collect a novel dataset, with ${\sim}1.3$M images across $9$ object categories, and train a SP prediction model called CLIP-UNet. CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors on real-world and simulated images. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored $43.7\%$ and $31.3\%$ times when comparing against the $4$ SP baselines on real and simulated images. In addition, we demonstrate leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉任务通常涉及描述图像中存在的内容（例如分类、检测、分割和字幕）。我们研究一项视觉常识任务，需要理解不存在的东西。具体来说，给定图像（例如客厅的图像）和对象的名称（“垫子”），要求视觉系统预测图像中可以放置该对象的语义有意义的区域（掩模或边界框）或可能是人类放置的（例如沙发上）。我们将此任务称为：语义放置 (SP)，并相信这种常识性视觉理解对于主动机器人（整理房子）和 AR 设备（自动在用户空间中渲染对象）至关重要。研究看不见的事物是困难的。用于图像描述的数据集通常是通过整理相关图像并要求人类注释图像内容来构建的；对于图像中不存在的对象，这两个步骤都不是简单的。我们通过相反的方向操作来克服这一挑战：我们从网络上下文中的对象图像开始，然后通过修复从图像中删除该对象。该自动化管道将非结构化 Web 数据转换为包含带有/不带有对象的图像对的数据集。使用它，我们收集了一个新颖的数据集，其中包含 ${\sim}1.3$M 图像，涉及 $9$ 对象类别，并训练一个名为 CLIP-UNet 的 SP 预测模型。 CLIP-UNet 的性能优于现有的 VLM 和基线，它们将语义先验与现实世界和模拟图像上的对象检测器相结合。在我们的用户研究中，我们发现与真实图像和模拟图像上的 $4$ SP 基线相比，CLIP-UNet 预测的 SP 掩模的优势为 $43.7\%$ 和 $31.3\%$ 倍。此外，我们还证明利用 CLIP-UNet 的 SP 掩模预测可以实现下游应用，例如在室内环境中构建整理机器人。</details>
**PDF:** <http://arxiv.org/pdf/2401.07770v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **DeepThalamus: A novel deep learning method for automatic segmentation of brain thalamic nuclei from multimodal ultra-high resolution MRI**<br />
**Title_cn:** DeepThalamus：一种新颖的深度学习方法，用于从多模态超高分辨率 MRI 中自动分割大脑丘脑核<br />
**Authors:** Marina Ruiz-Perez, Sergio Morell-Ortega, Marien Gadea, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Mariam de la Iglesia-Vaya, Thomas Tourdias, Pierrick Coupé, José V. Manjón<br />
**Abstract:** <details><summary>原文: </summary>The implication of the thalamus in multiple neurological pathologies makes it a structure of interest for volumetric analysis. In the present work, we have designed and implemented a multimodal volumetric deep neural network for the segmentation of thalamic nuclei at ultra-high resolution (0.125 mm3). Current tools either operate at standard resolution (1 mm3) or use monomodal data. To achieve the proposed objective, first, a database of semiautomatically segmented thalamic nuclei was created using ultra-high resolution T1, T2 and White Matter nulled (WMn) images. Then, a novel Deep learning based strategy was designed to obtain the automatic segmentations and trained to improve its robustness and accuaracy using a semisupervised approach. The proposed method was compared with a related state-of-the-art method showing competitive results both in terms of segmentation quality and efficiency. To make the proposed method fully available to the scientific community, a full pipeline able to work with monomodal standard resolution T1 images is also proposed.</details>
**Abstract_cn:** <details><summary>译文: </summary>丘脑在多种神经病理学中的含义使其成为体积分析感兴趣的结构。在目前的工作中，我们设计并实现了一种多模态体积深度神经网络，用于超高分辨率（0.125 mm3）的丘脑核分割。当前的工具要么以标准分辨率（1 mm3）运行，要么使用单峰数据。为了实现所提出的目标，首先，使用超高分辨率 T1、T2 和白质无效 (WMn) 图像创建半自动分割丘脑核的数据库。然后，设计了一种新颖的基于深度学习的策略来获得自动分割，并使用半监督方法进行训练以提高其鲁棒性和准确性。将所提出的方法与相关的最先进方法进行比较，在分割质量和效率方面都显示出有竞争力的结果。为了使所提出的方法完全可供科学界使用，还提出了能够处理单峰标准分辨率 T1 图像的完整流程。</details>
**PDF:** <http://arxiv.org/pdf/2401.07751v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation**<br />
**Title_cn:** MaskClustering：用于开放词汇 3D 实例分割的基于视图共识的掩模图聚类<br />
**Authors:** Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang<br />
**Abstract:** <details><summary>原文: </summary>Open-vocabulary 3D instance segmentation has emerged as a frontier topic due to its capability to segment 3D instances beyond a predefined set of categories. However, compared to significant progress in the 2D domain, methods for 3D open-vocabulary instance segmentation are hindered by the limited scale of high-quality annotated 3D data. To harness the capabilities of 2D models, recent efforts have focused on merging 2D masks based on metrics such as geometric and semantic similarity to form 3D instances. In contrast to these local metrics, we propose a novel metric called view consensus to better exploit multi-view observation. The key insight is that two 2D masks should be considered as belonging to the same instance if a considerable number of other 2D masks from other views contain both these two masks. Based on this metric, we build a global mask graph and iteratively cluster masks, prioritizing mask pairs with solid view consensus. The corresponding 3D points cluster of these 2D mask clusters can be regarded as 3D instances, along with the fused open-vocabulary features from clustered 2D masks. Through this multi-view verification and fusion mechanism, our method effectively leverages the prior instance knowledge from massive 2D masks predicted by visual foundation models, eliminating the need for training on 3D data. Experiments on publicly available datasets, including ScanNet200 and MatterPort3D, demonstrate that our method achieves state-of-the-art performance in both open-vocabulary instance segmentation and class-agnostic mask generation. Our project page is at https://pku-epic.github.io/MaskClustering.</details>
**Abstract_cn:** <details><summary>译文: </summary>开放词汇表 3D 实例分割已成为一个前沿主题，因为它能够对超出预定义类别集的 3D 实例进行分割。然而，与 2D 领域的重大进展相比，3D 开放词汇实例分割方法受到高质量带注释 3D 数据规模有限的阻碍。为了利用 2D 模型的功能，最近的工作重点是根据几何和语义相似性等指标合并 2D 掩模以形成 3D 实例。与这些局部指标相比，我们提出了一种称为视图共识的新指标，以更好地利用多视图观察。关键的见解是，如果来自其他视图的大量其他 2D 蒙版同时包含这两个蒙版，则应将这两个 2D 蒙版视为属于同一实例。基于这个指标，我们构建了一个全局掩模图并迭代地对掩模进行聚类，优先考虑具有可靠视图共识的掩模对。这些 2D 掩模簇的相应 3D 点簇可以被视为 3D 实例，以及来自簇状 2D 掩模的融合开放词汇特征。通过这种多视图验证和融合机制，我们的方法有效地利用了视觉基础模型预测的大量 2D 掩模中的先验实例知识，从而消除了对 3D 数据进行训练的需要。在公开数据集（包括 ScanNet200 和 MatterPort3D）上进行的实验表明，我们的方法在开放词汇实例分割和与类无关的掩码生成方面都实现了最先进的性能。我们的项目页面位于 https://pku-epic.github.io/MaskClustering。</details>
**PDF:** <http://arxiv.org/pdf/2401.07745v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Graph Transformer GANs with Graph Masked Modeling for Architectural Layout Generation**<br />
**Title_cn:** 用于生成架构布局的具有图形屏蔽建模的图形转换器 GAN<br />
**Authors:** Hao Tang, Ling Shao, Nicu Sebe, Luc Van Gool<br />
**Abstract:** <details><summary>原文: </summary>We present a novel graph Transformer generative adversarial network (GTGAN) to learn effective graph node relations in an end-to-end fashion for challenging graph-constrained architectural layout generation tasks. The proposed graph-Transformer-based generator includes a novel graph Transformer encoder that combines graph convolutions and self-attentions in a Transformer to model both local and global interactions across connected and non-connected graph nodes. Specifically, the proposed connected node attention (CNA) and non-connected node attention (NNA) aim to capture the global relations across connected nodes and non-connected nodes in the input graph, respectively. The proposed graph modeling block (GMB) aims to exploit local vertex interactions based on a house layout topology. Moreover, we propose a new node classification-based discriminator to preserve the high-level semantic and discriminative node features for different house components. To maintain the relative spatial relationships between ground truth and predicted graphs, we also propose a novel graph-based cycle-consistency loss. Finally, we propose a novel self-guided pre-training method for graph representation learning. This approach involves simultaneous masking of nodes and edges at an elevated mask ratio (i.e., 40%) and their subsequent reconstruction using an asymmetric graph-centric autoencoder architecture. This method markedly improves the model's learning proficiency and expediency. Experiments on three challenging graph-constrained architectural layout generation tasks (i.e., house layout generation, house roof generation, and building layout generation) with three public datasets demonstrate the effectiveness of the proposed method in terms of objective quantitative scores and subjective visual realism. New state-of-the-art results are established by large margins on these three tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的图 Transformer 生成对抗网络（GTGAN），以端到端的方式学习有效的图节点关系，以应对图约束架构布局生成任务。所提出的基于图 Transformer 的生成器包括一种新颖的图 Transformer 编码器，它将图卷积和自注意力结合在 Transformer 中，以对连接和非连接图节点之间的局部和全局交互进行建模。具体来说，所提出的连接节点注意（CNA）和非连接节点注意（NNA）旨在分别捕获输入图中连接节点和非连接节点之间的全局关系。所提出的图建模块（GMB）旨在利用基于房屋布局拓扑的局部顶点交互。此外，我们提出了一种新的基于节点分类的鉴别器，以保留不同房屋组件的高级语义和判别节点特征。为了维持真实图和预测图之间的相对空间关系，我们还提出了一种新颖的基于图的循环一致性损失。最后，我们提出了一种用于图表示学习的新型自引导预训练方法。这种方法涉及以较高的掩码比（即 40%）同时掩码节点和边缘，并使用非对称的以图为中心的自动编码器架构进行后续重建。该方法显着提高了模型的学习熟练度和便捷性。使用三个公共数据集对三个具有挑战性的图形约束建筑布局生成任务（即房屋布局生成、房屋屋顶生成和建筑布局生成）进行的实验证明了该方法在客观定量得分和主观视觉真实感方面的有效性。在这三项任务上大幅取得了新的最先进成果。</details>
**PDF:** <http://arxiv.org/pdf/2401.07721v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos**<br />
**Title_cn:** FiGCLIP：通过密集注释视频进行细粒度 CLIP 适应<br />
**Authors:** Darshan Singh S, Zeeshan Khan, Makarand Tapaswi<br />
**Abstract:** <details><summary>原文: </summary>While contrastive language image pretraining (CLIP) have exhibited impressive performance by learning highly semantic and generalized representations, recent works have exposed a fundamental drawback in its syntactic properties, that includes interpreting fine-grained attributes, actions, spatial relations, states, and details that require compositional reasoning. One reason for this is that natural captions often do not capture all the visual details of a scene. This leads to unaddressed visual concepts being misattributed to the wrong words. And the pooled image and text features, ends up acting as a bag of words, hence losing the syntactic information. In this work, we ask: Is it possible to enhance CLIP's fine-grained and syntactic abilities without compromising its semantic properties? We show that this is possible by adapting CLIP efficiently on a high-quality, comprehensive, and relatively small dataset. We demonstrate our adaptation strategy on VidSitu, a video situation recognition dataset annotated with verbs and rich semantic role labels (SRL). We use the SRL and verb information to create rule-based detailed captions, making sure they capture most of the visual concepts. Combined with hard negatives and hierarchical losses, these annotations allow us to learn a powerful visual representation, dubbed Fine-Grained CLIP (FiGCLIP), that preserves semantic understanding while being detail-oriented. We evaluate on five diverse vision-language tasks in both fine-tuning and zero-shot settings, achieving consistent improvements over the base CLIP model.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然对比语言图像预训练（CLIP）通过学习高度语义和广义表示表现出了令人印象深刻的性能，但最近的工作暴露了其句法属性的根本缺陷，包括解释细粒度属性、动作、空间关系、状态和细节需要组合推理。原因之一是自然字幕通常无法捕捉场景的所有视觉细节。这导致未解决的视觉概念被错误地归因于错误的词语。合并的图像和文本特征最终会充当一个词袋，从而丢失语法信息。在这项工作中，我们问：是否有可能在不损害其语义属性的情况下增强 CLIP 的细粒度和句法能力？我们证明，通过在高质量、全面且相对较小的数据集上有效地调整 CLIP，这是可能的。我们在 VidSitu 上展示了我们的适应策略，VidSitu 是一个用动词和丰富的语义角色标签（SRL）注释的视频情境识别数据集。我们使用 SRL 和动词信息来创建基于规则的详细标题，确保它们捕获大部分视觉概念。结合硬底片和层次损失，这些注释使我们能够学习强大的视觉表示，称为细粒度 CLIP (FiGCLIP)，它在注重细节的同时保留语义理解。我们在微调和零样本设置下评估五种不同的视觉语言任务，实现了对基本 CLIP 模型的一致改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.07669v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Foundation Models for Biomedical Image Segmentation: A Survey**<br />
**Title_cn:** 生物医学图像分割的基础模型：调查<br />
**Authors:** Ho Hin Lee, Yu Gu, Theodore Zhao, Yanbo Xu, Jianwei Yang, Naoto Usuyama, Cliff Wong, Mu Wei, Bennett A. Landman, Yuankai Huo, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>分段任意模型 (SAM) 显着推动了生物医学图像分析领域的最新进展。这项变革性技术最初是为通用计算机视觉而开发的，现已在医学图像处理中得到快速应用。去年，SAM 发表了 100 多篇出版物，展示了其在医学成像零样本学习改编方面的实力。 SAM 的基本前提在于其能够在不事先了解对象类型或成像模式的情况下分割或识别图像中的对象。这种方法与人类视觉系统可实现的任务非常吻合，尽管其在非生物视觉环境中的应用在理论上仍然更具挑战性。 SAM 的一个显着特征是它能够根据指定的分辨率比例或感兴趣的区域调整分割，类似于语义启动。这种适应性激发了将 SAM 应用于医学成像的创造力和创新浪潮。我们的审查重点关注 2023 年 4 月 1 日至 2023 年 9 月 30 日期间，这是首次发布后关键的前六个月。我们研究了 SAM 的适应和集成，以解决长期存在的临床挑战，特别是在我们的分析中涵盖的 33 个开放数据集的背景下。虽然 SAM 在许多应用中接近或实现了最先进的性能，但它在某些领域存在不足，例如颈动脉、肾上腺、视神经和下颌骨的分割。我们的调查深入研究了 SAM 基础方法所擅长的创新技术，并探讨了在不同的医学成像场景中有效转化和应用这些模型的核心概念。</details>
**PDF:** <http://arxiv.org/pdf/2401.07654v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting**<br />
**Title_cn:** SwinTextSpotter v2：实现更好的场景文本识别协同作用<br />
**Authors:** Mingxin Huang, Dezhi Peng, Hongliang Li, Zhenghao Peng, Chongyu Liu, Dahua Lin, Yuliang Liu, Xiang Bai, Lianwen Jin<br />
**Abstract:** <details><summary>原文: </summary>End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieved state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at \href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2}.</details>
**Abstract_cn:** <details><summary>译文: </summary>端到端场景文本识别旨在读取自然图像中的文本，近年来引起了广泛关注。然而，最近最先进的方法通常只是通过共享主干来将检测和识别结合起来，这并没有直接利用两个任务之间的特征交互。在本文中，我们提出了一种新的端到端场景文本识别框架，称为 SwinTextSpotter v2，旨在找到文本检测和识别之间更好的协同作用。具体来说，我们使用新颖的识别转换和识别对齐模块来增强两个任务之间的关系。识别转换通过识别损失明确指导文本定位，而识别对齐则通过检测预测动态提取文本特征进行识别。这种简单而有效的设计产生了一个简洁的框架，既不需要额外的校正模块，也不需要对任意形状的文本进行字符级注释。此外，通过引入 Box Selection Schedule，探测器的参数大大减少，而性能却没有下降。定性和定量实验表明，SwinTextSpotter v2 在各种多语言（英语、中文和越南语）基准测试中实现了最先进的性能。该代码可在 \href{https://github.com/mxin262/SwinTextSpotterv2}{SwinTextSpotter v2} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07641v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Fine-Grained Prototypes Distillation for Few-Shot Object Detection**<br />
**Title_cn:** 用于少样本目标检测的细粒度原型蒸馏<br />
**Authors:** Zichen Wang, Bo Yang, Haonan Yue, Zhenghao Ma<br />
**Abstract:** <details><summary>原文: </summary>Few-shot object detection (FSOD) aims at extending a generic detector for novel object detection with only a few training examples. It attracts great concerns recently due to the practical meanings. Meta-learning has been demonstrated to be an effective paradigm for this task. In general, methods based on meta-learning employ an additional support branch to encode novel examples (a.k.a. support images) into class prototypes, which are then fused with query branch to facilitate the model prediction. However, the class-level prototypes are difficult to precisely generate, and they also lack detailed information, leading to instability in performance.New methods are required to capture the distinctive local context for more robust novel object detection. To this end, we propose to distill the most representative support features into fine-grained prototypes. These prototypes are then assigned into query feature maps based on the matching results, modeling the detailed feature relations between two branches. This process is realized by our Fine-Grained Feature Aggregation (FFA) module. Moreover, in terms of high-level feature fusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and Non-Linear Fusion (NLF) module from differenct perspectives. They are complementary to each other and depict the high-level feature relations more effectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show that our method sets a new state-of-the-art performance in most settings. Our code is available at https://github.com/wangchen1801/FPD.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头目标检测（FSOD）旨在扩展通用检测器，仅使用少量训练示例即可进行新颖的目标检测。由于其实际意义，它最近引起了人们的极大关注。元学习已被证明是这项任务的有效范例。一般来说，基于元学习的方法采用额外的支持分支将新颖的示例（也称为支持图像）编码为类原型，然后将其与查询分支融合以促进模型预测。然而，类级原型很难精确生成，而且缺乏详细信息，导致性能不稳定。需要新方法来捕获独特的局部上下文，以实现更鲁棒的新物体检测。为此，我们建议将最具代表性的支持功能提炼成细粒度的原型。然后根据匹配结果将这些原型分配到查询特征图中，对两个分支之间的详细特征关系进行建模。这个过程是通过我们的细粒度特征聚合（FFA）模块实现的。此外，在高级特征融合方面，我们从不同角度提出了平衡类不可知采样（B-CAS）策略和非线性融合（NLF）模块。它们相互补充，更有效地描述高层特征关系。对 PASCAL VOC 和 MS COCO 基准的大量实验表明，我们的方法在大多数设置中都设置了新的最先进的性能。我们的代码可在 https://github.com/wangchen1801/FPD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07629v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Collaboratively Self-supervised Video Representation Learning for Action Recognition**<br />
**Title_cn:** 用于动作识别的协作自监督视频表示学习<br />
**Authors:** Jie Zhang, Zhifan Wan, Lanqing Hu, Stephen Lin, Shuzhe Wu, Shiguang Shan<br />
**Abstract:** <details><summary>原文: </summary>Considering the close connection between action recognition and human pose estimation, we design a Collaboratively Self-supervised Video Representation (CSVR) learning framework specific to action recognition by jointly considering generative pose prediction and discriminative context matching as pretext tasks. Specifically, our CSVR consists of three branches: a generative pose prediction branch, a discriminative context matching branch, and a video generating branch. Among them, the first one encodes dynamic motion feature by utilizing Conditional-GAN to predict the human poses of future frames, and the second branch extracts static context features by pulling the representations of clips and compressed key frames from the same video together while pushing apart the pairs from different videos. The third branch is designed to recover the current video frames and predict the future ones, for the purpose of collaboratively improving dynamic motion features and static context features. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the UCF101 and HMDB51 datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>考虑到动作识别和人体姿势估计之间的密切联系，我们通过联合考虑生成姿势预测和判别性上下文匹配作为借口任务，设计了一个专门针对动作识别的协作自监督视频表示（CSVR）学习框架。具体来说，我们的 CSVR 由三个分支组成：生成姿势预测分支、判别上下文匹配分支和视频生成分支。其中，第一个分支通过利用条件 GAN 来预测未来帧的人体姿势来编码动态运动特征，第二个分支通过将同一视频中的剪辑和压缩关键帧的表示拉在一起同时推开来提取静态上下文特征来自不同视频的对。第三个分支旨在恢复当前视频帧并预测未来视频帧，以协同改进动态运动特征和静态上下文特征。大量实验表明，我们的方法在 UCF101 和 HMDB51 数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07584v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Geo-locating Road Objects using Inverse Haversine Formula with NVIDIA Driveworks**<br />
**Title_cn:** 使用 NVIDIA Driveworks 的反半正弦公式对道路对象进行地理定位<br />
**Authors:** Mamoona Birkhez Shami, Gabriel Kiss, Trond Arve Haakonsen, Frank Lindseth<br />
**Abstract:** <details><summary>原文: </summary>Geolocation is integral to the seamless functioning of autonomous vehicles and advanced traffic monitoring infrastructures. This paper introduces a methodology to geolocate road objects using a monocular camera, leveraging the NVIDIA DriveWorks platform. We use the Centimeter Positioning Service (CPOS) and the inverse Haversine formula to geo-locate road objects accurately. The real-time algorithm processing capability of the NVIDIA DriveWorks platform enables instantaneous object recognition and spatial localization for Advanced Driver Assistance Systems (ADAS) and autonomous driving platforms. We present a measurement pipeline suitable for autonomous driving (AD) platforms and provide detailed guidelines for calibrating cameras using NVIDIA DriveWorks. Experiments were carried out to validate the accuracy of the proposed method for geolocating targets in both controlled and dynamic settings. We show that our approach can locate targets with less than 1m error when the AD platform is stationary and less than 4m error at higher speeds (i.e. up to 60km/h) within a 15m radius.</details>
**Abstract_cn:** <details><summary>译文: </summary>地理定位是自动驾驶车辆和先进交通监控基础设施无缝运行不可或缺的一部分。本文介绍了一种利用 NVIDIA DriveWorks 平台，使用单目摄像头对道路物体进行地理定位的方法。我们使用厘米定位服务（CPOS）和反半正弦公式来精确定位道路物体。 NVIDIA DriveWorks 平台的实时算法处理能力可实现高级驾驶辅助系统 (ADAS) 和自动驾驶平台的即时对象识别和空间定位。我们提出了适用于自动驾驶 (AD) 平台的测量管道，并提供了使用 NVIDIA DriveWorks 校准摄像头的详细指南。进行了实验以验证所提出的在受控和动态设置中对目标进行地理定位的方法的准确性。我们证明，当 AD 平台静止时，我们的方法可以以小于 1m 的误差定位目标，而在 15m 半径内以较高速度（即高达 60km/h）定位目标时，误差小于 4m。</details>
**PDF:** <http://arxiv.org/pdf/2401.07582v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **PMFSNet: Polarized Multi-scale Feature Self-attention Network For Lightweight Medical Image Segmentation**<br />
**Title_cn:** PMFSNet：用于轻量级医学图像分割的偏振多尺度特征自注意力网络<br />
**Authors:** Jiahui Zhong, Wenhong Tian, Yuanlun Xie, Zhijia Liu, Jie Ou, Taoran Tian, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Current state-of-the-art medical image segmentation methods prioritize accuracy but often at the expense of increased computational demands and larger model sizes. Applying these large-scale models to the relatively limited scale of medical image datasets tends to induce redundant computation, complicating the process without the necessary benefits. This approach not only adds complexity but also presents challenges for the integration and deployment of lightweight models on edge devices. For instance, recent transformer-based models have excelled in 2D and 3D medical image segmentation due to their extensive receptive fields and high parameter count. However, their effectiveness comes with a risk of overfitting when applied to small datasets and often neglects the vital inductive biases of Convolutional Neural Networks (CNNs), essential for local feature representation. In this work, we propose PMFSNet, a novel medical imaging segmentation model that effectively balances global and local feature processing while avoiding the computational redundancy typical in larger models. PMFSNet streamlines the UNet-based hierarchical structure and simplifies the self-attention mechanism's computational complexity, making it suitable for lightweight applications. It incorporates a plug-and-play PMFS block, a multi-scale feature enhancement module based on attention mechanisms, to capture long-term dependencies. Extensive comprehensive results demonstrate that even with a model (less than 1 million parameters), our method achieves superior performance in various segmentation tasks across different data scales. It achieves (IoU) metrics of 84.68%, 82.02%, and 78.82% on public datasets of teeth CT (CBCT), ovarian tumors ultrasound(MMOTU), and skin lesions dermoscopy images (ISIC 2018), respectively. The source code is available at https://github.com/yykzjh/PMFSNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前最先进的医学图像分割方法优先考虑准确性，但往往以增加计算需求和更大的模型尺寸为代价。将这些大规模模型应用于规模相对有限的医学图像数据集往往会引起冗余计算，使过程复杂化，但没有带来必要的好处。这种方法不仅增加了复杂性，而且给边缘设备上轻量级模型的集成和部署带来了挑战。例如，最近基于 Transformer 的模型由于其广泛的感受野和高参数数而在 2D 和 3D 医学图像分割中表现出色。然而，当应用于小型数据集时，它们的有效性会带来过度拟合的风险，并且经常忽略卷积神经网络（CNN）的重要归纳偏差，这对于局部特征表示至关重要。在这项工作中，我们提出了 PMFSNet，一种新颖的医学成像分割模型，它有效地平衡全局和局部特征处理，同时避免较大模型中典型的计算冗余。 PMFSNet简化了基于UNet的层次结构，并简化了self-attention机制的计算复杂度，使其适合轻量级应用。它包含一个即插即用的 PMFS 块，一个基于注意力机制的多尺度特征增强模块，以捕获长期依赖性。广泛的综合结果表明，即使使用模型（少于 100 万个参数），我们的方法在不同数据规模的各种分割任务中也能实现卓越的性能。它在牙齿 CT (CBCT)、卵巢肿瘤超声 (MMOTU) 和皮肤病变皮肤镜图像 (ISIC 2018) 的公共数据集上分别实现了 84.68%、82.02% 和 78.82% 的 (IoU) 指标。源代码可在 https://github.com/yykzjh/PMFSNet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07579v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Exploiting GPT-4 Vision for Zero-shot Point Cloud Understanding**<br />
**Title_cn:** 利用 GPT-4 视觉实现零样本点云理解<br />
**Authors:** Qi Sun, Xiao Cui, Wengang Zhou, Houqiang Li<br />
**Abstract:** <details><summary>原文: </summary>In this study, we tackle the challenge of classifying the object category in point clouds, which previous works like PointCLIP struggle to address due to the inherent limitations of the CLIP architecture. Our approach leverages GPT-4 Vision (GPT-4V) to overcome these challenges by employing its advanced generative abilities, enabling a more adaptive and robust classification process. We adapt the application of GPT-4V to process complex 3D data, enabling it to achieve zero-shot recognition capabilities without altering the underlying model architecture. Our methodology also includes a systematic strategy for point cloud image visualization, mitigating domain gap and enhancing GPT-4V's efficiency. Experimental validation demonstrates our approach's superiority in diverse scenarios, setting a new benchmark in zero-shot point cloud classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们解决了对点云中的对象类别进行分类的挑战，由于 CLIP 架构的固有局限性，PointCLIP 等之前的工作很难解决这个问题。我们的方法利用 GPT-4 Vision (GPT-4V) 来克服这些挑战，通过利用其先进的生成能力，实现更具适应性和稳健的分类过程。我们采用GPT-4V的应用来处理复杂的3D数据，使其能够在不改变底层模型架构的情况下实现零样本识别能力。我们的方法还包括点云图像可视化、缩小域差距和提高 GPT-4V 效率的系统策略。实验验证证明了我们的方法在不同场景中的优越性，为零样本点云分类树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2401.07572v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Combining Image- and Geometric-based Deep Learning for Shape Regression: A Comparison to Pixel-level Methods for Segmentation in Chest X-Ray**<br />
**Title_cn:** 结合基于图像和几何的深度学习进行形状回归：胸部 X 射线分割的像素级方法的比较<br />
**Authors:** Ron Keuth, Mattias Heinrich<br />
**Abstract:** <details><summary>原文: </summary>When solving a segmentation task, shaped-base methods can be beneficial compared to pixelwise classification due to geometric understanding of the target object as shape, preventing the generation of anatomical implausible predictions in particular for corrupted data. In this work, we propose a novel hybrid method that combines a lightweight CNN backbone with a geometric neural network (Point Transformer) for shape regression. Using the same CNN encoder, the Point Transformer reaches segmentation quality on per with current state-of-the-art convolutional decoders ($4\pm1.9$ vs $3.9\pm2.9$ error in mm and $85\pm13$ vs $88\pm10$ Dice), but crucially, is more stable w.r.t image distortion, starting to outperform them at a corruption level of 30%. Furthermore, we include the nnU-Net as an upper baseline, which has $3.7\times$ more trainable parameters than our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>在解决分割任务时，由于将目标对象几何理解为形状，因此与像素分类相比，基于形状的方法可能更有利，可以防止生成不可信的解剖学预测，特别是对于损坏的数据。在这项工作中，我们提出了一种新颖的混合方法，它将轻量级 CNN 主干与几何神经网络（Point Transformer）相结合以进行形状回归。使用相同的 CNN 编码器，Point Transformer 达到了当前最先进的卷积解码器的分割质量（$4\pm1.9$ vs $3.9\pm2.9$ 误差以毫米为单位，$85\pm13$ vs $88\ pm10$ Dice），但最重要的是，在图像失真方面更稳定，在 30% 的损坏水平上开始优于它们。此外，我们将 nnU-Net 作为上基线，其可训练参数比我们提出的方法多 $3.7\times$。</details>
**PDF:** <http://arxiv.org/pdf/2401.07542v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception**<br />
**Title_cn:** MM-SAP：评估感知中多模态大语言模型自我意识的综合基准<br />
**Authors:** Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have shown their remarkable abilities in visual perception and understanding recently. However, how to comprehensively evaluate the capabilities of MLLMs remains a challenge. Most of the existing benchmarks predominantly focus on assessing perception, cognition, and reasoning, neglecting the abilities of self-awareness, referring to the model's recognition of its own capability boundary. In our study, we focus on self-awareness in image perception and introduce the knowledge quadrant for MLLMs, which clearly defines the knowns and unknowns in perception. Based on this, we propose a novel benchmark specifically designed to evaluate the Self-Aware capabilities in Perception for MLLMs(MM-SAP). MM-SAP encompasses three distinct sub-datasets, each focusing on different aspects of self-awareness. We evaluated eight well-known MLLMs using MM-SAP, analyzing their self-awareness and providing detailed insights. Code and data are available at https://github.com/YHWmz/MM-SAP</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）最近在视觉感知和理解方面表现出了非凡的能力。然而，如何全面评估MLLM的能力仍然是一个挑战。现有的基准测试大多主要侧重于评估感知、认知和推理，而忽略了自我意识的能力，指的是模型对自身能力边界的认知。在我们的研究中，我们关注图像感知中的自我意识，并引入 MLLM 的知识象限，它清楚地定义了感知中的已知和未知。基于此，我们提出了一个新的基准，专门用于评估 MLLM（MM-SAP）感知中的自我意识能力。 MM-SAP 包含三个不同的子数据集，每个子​​数据集侧重于自我意识的不同方面。我们使用 MM-SAP 评估了八个知名的 MLLM，分析他们的自我意识并提供详细的见解。代码和数据可在 https://github.com/YHWmz/MM-SAP 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.07529v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Compositional Oil Spill Detection Based on Object Detector and Adapted Segment Anything Model from SAR Images**<br />
**Title_cn:** 基于目标检测器和 SAR 图像自适应分段任意模型的合成溢油检测<br />
**Authors:** Wenhui Wu, Man Sing Wong, Xinyu Yu, Guoqiang Shi, Coco Yin Tung Kwok, Kang Zou<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation-based methods have attracted extensive attention in oil spill detection from SAR images. However, the existing approaches require a large number of finely annotated segmentation samples in the training stage. To alleviate this issue, we propose a composite oil spill detection framework, SAM-OIL, comprising an object detector (e.g., YOLOv8), an adapted Segment Anything Model (SAM), and an Ordered Mask Fusion (OMF) module. SAM-OIL is the first application of the powerful SAM in oil spill detection. Specifically, the SAM-OIL strategy uses YOLOv8 to obtain the categories and bounding boxes of oil spill-related objects, then inputs bounding boxes into the adapted SAM to retrieve category-agnostic masks, and finally adopts the Ordered Mask Fusion (OMF) module to fuse the masks and categories. The adapted SAM, combining a frozen SAM with a learnable Adapter module, can enhance SAM's ability to segment ambiguous objects. The OMF module, a parameter-free method, can effectively resolve pixel category conflicts within SAM. Experimental results demonstrate that SAM-OIL surpasses existing semantic segmentation-based oil spill detection methods, achieving mIoU of 69.52%. The results also indicated that both OMF and Adapter modules can effectively improve the accuracy in SAM-OIL.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于语义分割的方法在 SAR 图像溢油检测中引起了广泛关注。然而，现有的方法在训练阶段需要大量精细注释的分割样本。为了缓解这个问题，我们提出了一种复合漏油检测框架 SAM-OIL，包括一个目标检测器（例如 YOLOv8）、一个改编的分段任意模型（SAM）和一个有序掩模融合（OMF）模块。 SAM-OIL 是功能强大的 SAM 在溢油检测中的首次应用。具体来说，SAM-OIL策略使用YOLOv8获取漏油相关对象的类别和边界框，然后将边界框输入到改编后的SAM中以检索类别不可知的掩模，最后采用有序掩模融合（OMF）模块融合掩码和类别。改编后的 SAM 将冻结的 SAM 与可学习的适配器模块相结合，可以增强 SAM 分割模糊对象的能力。 OMF模块是一种无参数方法，可以有效解决SAM内部的像素类别冲突。实验结果表明，SAM-OIL 超越了现有的基于语义分割的溢油检测方法，达到 69.52% 的 mIoU。结果还表明OMF和Adapter模块都可以有效提高SAM-OIL的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.07502v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Harnessing Deep Learning and Satellite Imagery for Post-Buyout Land Cover Mapping**<br />
**Title_cn:** 利用深度学习和卫星图像进行收购后土地覆盖测绘<br />
**Authors:** Hakan T. Otal, Elyse Zavar, Sherri B. Binder, Alex Greer, M. Abdullah Canbaz<br />
**Abstract:** <details><summary>原文: </summary>Environmental disasters such as floods, hurricanes, and wildfires have increasingly threatened communities worldwide, prompting various mitigation strategies. Among these, property buyouts have emerged as a prominent approach to reducing vulnerability to future disasters. This strategy involves governments purchasing at-risk properties from willing sellers and converting the land into open space, ostensibly reducing future disaster risk and impact. However, the aftermath of these buyouts, particularly concerning land-use patterns and community impacts, remains under-explored. This research aims to fill this gap by employing innovative techniques like satellite imagery analysis and deep learning to study these patterns. To achieve this goal, we employed FEMA's Hazard Mitigation Grant Program (HMGP) buyout dataset, encompassing over 41,004 addresses of these buyout properties from 1989 to 2017. Leveraging Google's Maps Static API, we gathered 40,053 satellite images corresponding to these buyout lands. Subsequently, we implemented five cutting-edge machine learning models to evaluate their performance in classifying land cover types. Notably, this task involved multi-class classification, and our model achieved an outstanding ROC-AUC score of 98.86%</details>
**Abstract_cn:** <details><summary>译文: </summary>洪水、飓风和野火等环境灾害日益威胁着世界各地的社区，催生了各种缓解策略。其中，财产收购已成为减少未来灾害脆弱性的重要方法。这一战略涉及政府从自愿卖家那里购买有风险的房产，并将土地转变为开放空间，表面上减少了未来的灾害风险和影响。然而，这些收购的后果，特别是在土地利用模式和社区影响方面，仍有待探索。这项研究旨在通过采用卫星图像分析和深度学习等创新技术来研究这些模式来填补这一空白。为了实现这一目标，我们采用了 FEMA 的减灾拨款计划 (HMGP) 买断数据集，其中包含 1989 年至 2017 年间这些买断房产的超过 41,004 个地址。利用 Google 的 Maps Static API，我们收集了与这些买断土地相对应的 40,053 个卫星图像。随后，我们实施了五个前沿的机器学习模型来评估它们在土地覆盖类型分类方面的表现。值得注意的是，该任务涉及多类分类，我们的模型取得了 98.86% 的出色 ROC-AUC 分数</details>
**PDF:** <http://arxiv.org/pdf/2401.07500v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation**<br />
**Title_cn:** Robo-ABC：通过机器人操作的语义对应进行超越类别的可供性概括<br />
**Authors:** Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, Huazhe Xu<br />
**Abstract:** <details><summary>原文: </summary>Enabling robotic manipulation that generalizes to out-of-distribution scenes is a crucial step toward open-world embodied intelligence. For human beings, this ability is rooted in the understanding of semantic correspondence among objects, which naturally transfers the interaction experience of familiar objects to novel ones. Although robots lack such a reservoir of interaction experience, the vast availability of human videos on the Internet may serve as a valuable resource, from which we extract an affordance memory including the contact points. Inspired by the natural way humans think, we propose Robo-ABC: when confronted with unfamiliar objects that require generalization, the robot can acquire affordance by retrieving objects that share visual or semantic similarities from the affordance memory. The next step is to map the contact points of the retrieved objects to the new object. While establishing this correspondence may present formidable challenges at first glance, recent research finds it naturally arises from pre-trained diffusion models, enabling affordance mapping even across disparate object categories. Through the Robo-ABC framework, robots may generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively, Robo-ABC significantly enhances the accuracy of visual affordance retrieval by a large margin of 31.6% compared to state-of-the-art (SOTA) end-to-end affordance models. We also conduct real-world experiments of cross-category object-grasping tasks. Robo-ABC achieved a success rate of 85.7%, proving its capacity for real-world tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>实现泛化到分布外场景的机器人操作是迈向开放世界具身智能的关键一步。对于人类来说，这种能力植根于对物体间语义对应关系的理解，从而自然地将熟悉物体的交互体验转移到新物体上。尽管机器人缺乏如此丰富的交互经验，但互联网上大量的人类视频可能是一种宝贵的资源，我们可以从中提取包括接触点在内的可供性记忆。受人类自然思维方式的启发，我们提出了 Robo-ABC：当面对需要泛化的不熟悉的物体时，机器人可以通过从可供性记忆中检索具有视觉或语义相似性的物体来获得可供性。下一步是将检索到的对象的接触点映射到新对象。虽然乍一看建立这种对应关系可能会带来巨大的挑战，但最近的研究发现它自然地产生于预先训练的扩散模型，甚至可以跨不同的对象类别实现可供性映射。通过 Robo-ABC 框架，机器人可以以零镜头的方式泛化操作类别外的对象，而无需任何手动注释、额外训练、零件分割、预编码知识或视点限制。从数量上来说，与最先进的（SOTA）端到端可供性模型相比，Robo-ABC 显着提高了视觉可供性检索的准确性，大幅提高了 31.6%。我们还进行了跨类别对象抓取任务的真实实验。 Robo-ABC 的成功率达到 85.7%，证明了其执行现实任务的能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.07487v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **CascadeV-Det: Cascade Point Voting for 3D Object Detection**<br />
**Title_cn:** CascadeV-Det：用于 3D 对象检测的级联点投票<br />
**Authors:** Yingping Liang, Ying Fu<br />
**Abstract:** <details><summary>原文: </summary>Anchor-free object detectors are highly efficient in performing point-based prediction without the need for extra post-processing of anchors. However, different from the 2D grids, the 3D points used in these detectors are often far from the ground truth center, making it challenging to accurately regress the bounding boxes. To address this issue, we propose a Cascade Voting (CascadeV) strategy that provides high-quality 3D object detection with point-based prediction. Specifically, CascadeV performs cascade detection using a novel Cascade Voting decoder that combines two new components: Instance Aware Voting (IA-Voting) and a Cascade Point Assignment (CPA) module. The IA-Voting module updates the object features of updated proposal points within the bounding box using conditional inverse distance weighting. This approach prevents features from being aggregated outside the instance and helps improve the accuracy of object detection. Additionally, since model training can suffer from a lack of proposal points with high centerness, we have developed the CPA module to narrow down the positive assignment threshold with cascade stages. This approach relaxes the dependence on proposal centerness in the early stages while ensuring an ample quantity of positives with high centerness in the later stages. Experiments show that FCAF3D with our CascadeV achieves state-of-the-art 3D object detection results with 70.4\% mAP@0.25 and 51.6\% mAP@0.5 on SUN RGB-D and competitive results on ScanNet. Code will be released at https://github.com/Sharpiless/CascadeV-Det</details>
**Abstract_cn:** <details><summary>译文: </summary>无锚目标检测器在执行基于点的预测方面非常高效，无需对锚进行额外的后处理。然而，与 2D 网格不同，这些检测器中使用的 3D 点通常远离地面实况中心，这使得准确回归边界框具有挑战性。为了解决这个问题，我们提出了一种级联投票 (CascadeV) 策略，该策略通过基于点的预测提供高质量的 3D 对象检测。具体来说，CascadeV 使用新颖的级联投票解码器执行级联检测，该解码器结合了两个新组件：实例感知投票 (IA-Voting) 和级联点分配 (CPA) 模块。 IA-Voting 模块使用条件反距离加权来更新边界框中更新的提议点的对象特征。这种方法可以防止特征在实例外部聚合，并有助于提高对象检测的准确性。此外，由于模型训练可能会受到缺乏高中心度提案点的影响，我们开发了 CPA 模块，通过级联阶段缩小正分配阈值。这种方法放松了前期对提案中心度的依赖，同时保证了后期有充足的中心度高的正例。实验表明，FCAF3D 与我们的 CascadeV 实现了最先进的 3D 对象检测结果，在 SUN RGB-D 上达到了 70.4\% mAP@0.25 和 51.6\% mAP@0.5，在 ScanNet 上取得了具有竞争力的结果。代码将在 https://github.com/Sharpiless/CascadeV-Det 发布</details>
**PDF:** <http://arxiv.org/pdf/2401.07477v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Semantic Segmentation in Multiple Adverse Weather Conditions with Domain Knowledge Retention**<br />
**Title_cn:** 具有领域知识保留的多种恶劣天气条件下的语义分割<br />
**Authors:** Xin Yang, Wending Yan, Yuan Yuan, Michael Bi Mi, Robby T. Tan<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation's performance is often compromised when applied to unlabeled adverse weather conditions. Unsupervised domain adaptation is a potential approach to enhancing the model's adaptability and robustness to adverse weather. However, existing methods encounter difficulties when sequentially adapting the model to multiple unlabeled adverse weather conditions. They struggle to acquire new knowledge while also retaining previously learned knowledge.To address these problems, we propose a semantic segmentation method for multiple adverse weather conditions that incorporates adaptive knowledge acquisition, pseudolabel blending, and weather composition replay. Our adaptive knowledge acquisition enables the model to avoid learning from extreme images that could potentially cause the model to forget. In our approach of blending pseudo-labels, we not only utilize the current model but also integrate the previously learned model into the ongoing learning process. This collaboration between the current teacher and the previous model enhances the robustness of the pseudo-labels for the current target. Our weather composition replay mechanism allows the model to continuously refine its previously learned weather information while simultaneously learning from the new target domain. Our method consistently outperforms the stateof-the-art methods, and obtains the best performance with averaged mIoU (%) of 65.7 and the lowest forgetting (%) of 3.6 against 60.1 and 11.3, on the ACDC datasets for a four-target continual multi-target domain adaptation.</details>
**Abstract_cn:** <details><summary>译文: </summary>当应用于未标记的恶劣天气条件时，语义分割的性能通常会受到影响。无监督域适应是增强模型对恶劣天气的适应性和鲁棒性的潜在方法。然而，现有方法在依次使模型适应多种未标记的恶劣天气条件时遇到困难。他们努力获取新知识，同时保留以前学到的知识。为了解决这些问题，我们提出了一种针对多种恶劣天气条件的语义分割方法，该方法结合了自适应知识获取、伪标签混合和天气成分回放。我们的自适应知识获取使模型能够避免从可能导致模型遗忘的极端图像中学习。在我们混合伪标签的方法中，我们不仅利用当前模型，还将之前学习的模型集成到正在进行的学习过程中。当前教师和先前模型之间的这种协作增强了当前目标的伪标签的鲁棒性。我们的天气成分重播机制允许模型不断完善其先前学习的天气信息，同时从新的目标域中学习。我们的方法始终优于最先进的方法，并在四目标连续多目标的 ACDC 数据集上获得了最佳性能，平均 mIoU (%) 为 65.7，最低遗忘 (%) 为 3.6，相对于 60.1 和 11.3 -目标域适应。</details>
**PDF:** <http://arxiv.org/pdf/2401.07459v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels**<br />
**Title_cn:** BoNuS：使用部分点标签进行核分割的边界挖掘<br />
**Authors:** Yi Lin, Zeyu Wang, Dong Zhang, Kwang-Ting Cheng, Hao Chen<br />
**Abstract:** <details><summary>原文: </summary>Nuclei segmentation is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclei segmentation enables quantitative analysis of the wide existence and large variances in nuclei morphometry in histopathology images. However, manual annotation of tens of thousands of nuclei is tedious and time-consuming, which requires significant amount of human effort and domain-specific expertise. To alleviate this problem, in this paper, we propose a weakly-supervised nuclei segmentation method that only requires partial point labels of nuclei. Specifically, we propose a novel boundary mining framework for nuclei segmentation, named BoNuS, which simultaneously learns nuclei interior and boundary information from the point labels. To achieve this goal, we propose a novel boundary mining loss, which guides the model to learn the boundary information by exploring the pairwise pixel affinity in a multiple-instance learning manner. Then, we consider a more challenging problem, i.e., partial point label, where we propose a nuclei detection module with curriculum learning to detect the missing nuclei with prior morphological knowledge. The proposed method is validated on three public datasets, MoNuSeg, CPM, and CoNIC datasets. Experimental results demonstrate the superior performance of our method to the state-of-the-art weakly-supervised nuclei segmentation methods. Code: https://github.com/hust-linyi/bonus.</details>
**Abstract_cn:** <details><summary>译文: </summary>细胞核分割是数字病理工作流程的基本先决条件。细胞核分割自动化方法的发展使得能够对组织病理学图像中细胞核形态测量的广泛存在和巨大差异进行定量分析。然而，对数以万计的细胞核进行手动注释既繁琐又耗时，需要大量的人力和特定领域的专业知识。为了缓解这个问题，在本文中，我们提出了一种弱监督的核分割方法，仅需要核的部分点标签。具体来说，我们提出了一种用于核分割的新型边界挖掘框架，名为BoNuS，它同时从点标签中学习核内部和边界信息。为了实现这一目标，我们提出了一种新颖的边界挖掘损失，它引导模型通过以多实例学习方式探索成对像素亲和力来学习边界信息。然后，我们考虑一个更具挑战性的问题，即部分点标签，我们提出了一个具有课程学习的核检测模块，以利用先验形态学知识来检测丢失的核。所提出的方法在三个公共数据集 MoNuSeg、CPM 和 CoNIC 数据集上进行了验证。实验结果证明我们的方法比最先进的弱监督核分割方法具有优越的性能。代码：https://github.com/hust-linyi/bonus。</details>
**PDF:** <http://arxiv.org/pdf/2401.07437v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **A Deep Hierarchical Feature Sparse Framework for Occluded Person Re-Identification**<br />
**Title_cn:** 用于被遮挡人员重新识别的深层层次特征稀疏框架<br />
**Authors:** Yihu Song, Shuaishi Liu<br />
**Abstract:** <details><summary>原文: </summary>Most existing methods tackle the problem of occluded person re-identification (ReID) by utilizing auxiliary models, resulting in a complicated and inefficient ReID framework that is unacceptable for real-time applications. In this work, a speed-up person ReID framework named SUReID is proposed to mitigate occlusion interference while speeding up inference. The SUReID consists of three key components: hierarchical token sparsification (HTS) strategy, non-parametric feature alignment knowledge distillation (NPKD), and noise occlusion data augmentation (NODA). The HTS strategy works by pruning the redundant tokens in the vision transformer to achieve highly effective self-attention computation and eliminate interference from occlusions or background noise. However, the pruned tokens may contain human part features that contaminate the feature representation and degrade the performance. To solve this problem, the NPKD is employed to supervise the HTS strategy, retaining more discriminative tokens and discarding meaningless ones. Furthermore, the NODA is designed to introduce more noisy samples, which further trains the ability of the HTS to disentangle different tokens. Experimental results show that the SUReID achieves superior performance with surprisingly fast inference.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数现有方法通过利用辅助模型来解决被遮挡人重新识别（ReID）的问题，从而导致复杂且低效的ReID框架，这对于实时应用来说是不可接受的。在这项工作中，提出了一种名为 SUReID 的加速行人 ReID 框架，以减轻遮挡干扰，同时加速推理。 SUReID 由三个关键组件组成：分层令牌稀疏（HTS）策略、非参数特征对齐知识蒸馏（NPKD）和噪声遮挡数据增强（NODA）。 HTS 策略通过修剪视觉转换器中的冗余标记来实现高效的自注意力计算并消除遮挡或背景噪声的干扰。然而，修剪后的令牌可能包含人类部分特征，这些特征会污染特征表示并降低性能。为了解决这个问题，采用NPKD来监督HTS策略，保留更多有辨别力的标记并丢弃无意义的标记。此外，NODA 旨在引入更多噪声样本，进一步训练 HTS 解开不同 token 的能力。实验结果表明，SUReID 凭借惊人的快速推理实现了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07469v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Towards A Better Metric for Text-to-Video Generation**<br />
**Title_cn:** 寻求更好的文本到视频生成指标<br />
**Authors:** Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao Ge, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgements of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型在合成高质量文本、图像和视频方面表现出了卓越的能力。对于视频生成，当代文本到视频模型展示了令人印象深刻的功能，可以制作视觉上令人惊叹的视频。尽管如此，评估此类视频仍面临重大挑战。目前的研究主要采用 FVD、IS 和 CLIP Score 等自动化指标。然而，这些指标提供的分析不完整，特别是在视频内容的时间评估方面，因此使它们成为真实视频质量的不可靠指标。此外，虽然用户研究有可能准确反映人类的感知，但它们因其耗时和费力的性质而受到阻碍，其结果往往受到主观偏见的影响。在本文中，我们研究了现有指标固有的局限性，并引入了一种新颖的评估管道：文本到视频分数（T2VScore）。该指标集成了两个关键标准：(1) 文本视频对齐，仔细检查视频在表示给定文本描述方面的保真度；(2) 视频质量，由专家共同评估视频的整体制作水平。此外，为了评估所提出的指标并促进未来的改进，我们提出了 TVGE 数据集，收集了人类对 2,543 个文本到视频生成的视频根据这两个标准的判断。 TVGE 数据集上的实验证明了所提出的 T2VScore 在为文本到视频生成提供更好的指标方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.07781v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation**<br />
**Title_cn:** HexaGen3D：StableDiffusion 距离快速、多样化的文本到 3D 生成仅一步之遥<br />
**Authors:** Antoine Mercier, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, Guillaume Berger<br />
**Abstract:** <details><summary>原文: </summary>Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong generalization to new objects or compositions.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管生成建模领域取得了显着进展，但根据文本提示高效生成高质量 3D 资源仍然是一项艰巨的任务。一个关键挑战在于数据稀缺：最广泛的 3D 数据集仅包含数百万个资产，而其 2D 数据集包含数十亿个文本图像对。为了解决这个问题，我们提出了一种利用大型预训练二维扩散模型的力量的新方法。更具体地说，我们的方法 HexaGen3D 对预训练的文本到图像模型进行微调，以联合预测 6 个正交投影和相应的潜在三平面。然后我们解码这些潜在变量以生成纹理网格。 HexaGen3D 不需要针对每个样本进行优化，并且可以在 7 秒内根据文本提示推断高质量和多样化的对象，与现有方法相比，提供明显更好的质量与延迟权衡。此外，HexaGen3D 展示了对新对象或组合的强大泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.07727v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Towards Efficient Diffusion-Based Image Editing with Instant Attention Masks**<br />
**Title_cn:** 使用即时注意力蒙版实现高效的基于扩散的图像编辑<br />
**Authors:** Siyu Zou, Jiji Tang, Yiyi Zhou, Jing He, Chaoyi Zhao, Rongsheng Zhang, Zhipeng Hu, Xiaoshuai Sun<br />
**Abstract:** <details><summary>原文: </summary>Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which often applies a semantic mask to control the target area for diffusion-based editing. However, most existing solutions obtain these masks via manual operations or off-line processing, greatly reducing their efficiency. In this paper, we propose a novel and efficient image editing method for Text-to-Image (T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In particular, InstDiffEdit aims to employ the cross-modal attention ability of existing diffusion models to achieve instant mask guidance during the diffusion steps. To reduce the noise of attention maps and realize the full automatics, we equip InstDiffEdit with a training-free refinement scheme to adaptively aggregate the attention distributions for the automatic yet accurate mask generation. Meanwhile, to supplement the existing evaluations of DIE, we propose a new benchmark called Editing-Mask to examine the mask accuracy and local editing ability of existing methods. To validate InstDiffEdit, we also conduct extensive experiments on ImageNet and Imagen, and compare it with a bunch of the SOTA methods. The experimental results show that InstDiffEdit not only outperforms the SOTA methods in both image quality and editing results, but also has a much faster inference speed, i.e., +5 to +6 times. Our code available at https://anonymous.4open.science/r/InstDiffEdit-C306/</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散的图像编辑（DIE）是一个新兴的研究热点，它通常应用语义掩模来控制基于扩散的编辑的目标区域。然而，现有的解决方案大多通过手动操作或离线处理来获取这些掩模，大大降低了效率。在本文中，我们提出了一种用于文本到图像（T2I）扩散模型的新颖且高效的图像编辑方法，称为即时扩散编辑（InstDiffEdit）。特别是，InstDiffEdit 旨在利用现有扩散模型的跨模式注意能力，在扩散步骤中实现即时掩模引导。为了减少注意力图的噪声并实现全自动，我们为 InstDiffEdit 配备了免训练的细化方案，以自适应地聚合注意力分布，以自动且准确地生成掩模。同时，为了补充 DIE 的现有评估，我们提出了一个名为 Editing-Mask 的新基准来检查现有方法的掩模精度和本地编辑能力。为了验证 InstDiffEdit，我们还在 ImageNet 和 Imagen 上进行了广泛的实验，并将其与一堆 SOTA 方法进行了比较。实验结果表明，InstDiffEdit不仅在图像质量和编辑结果上都优于SOTA方法，而且推理速度也快得多，即+5至+6倍。我们的代码位于 https://anonymous.4open.science/r/InstDiffEdit-C306/</details>
**PDF:** <http://arxiv.org/pdf/2401.07709v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multimodal Crowd Counting with Pix2Pix GANs**<br />
**Title_cn:** 使用 Pix2Pix GAN 进行多模式人群计数<br />
**Authors:** Muhammad Asif Khan, Hamid Menouar, Ridha Hamila<br />
**Abstract:** <details><summary>原文: </summary>Most state-of-the-art crowd counting methods use color (RGB) images to learn the density map of the crowd. However, these methods often struggle to achieve higher accuracy in densely crowded scenes with poor illumination. Recently, some studies have reported improvement in the accuracy of crowd counting models using a combination of RGB and thermal images. Although multimodal data can lead to better predictions, multimodal data might not be always available beforehand. In this paper, we propose the use of generative adversarial networks (GANs) to automatically generate thermal infrared (TIR) images from color (RGB) images and use both to train crowd counting models to achieve higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to TIR images. Our experiments on several state-of-the-art crowd counting models and benchmark crowd datasets report significant improvement in accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数最先进的人群计数方法都使用彩色 (RGB) 图像来学习人群的密度图。然而，这些方法通常很难在光照较差的拥挤场景中获得更高的准确度。最近，一些研究报告称，结合使用 RGB 和热图像，可以提高人群计数模型的准确性。尽管多模态数据可以带来更好的预测，但多模态数据可能并不总是事先可用。在本文中，我们建议使用生成对抗网络（GAN）从彩色（RGB）图像自动生成热红外（TIR）图像，并使用两者来训练人群计数模型以实现更高的准确性。我们首先使用 Pix2Pix GAN 网络将 RGB 图像转换为 TIR 图像。我们对几种最先进的人群计数模型和基准人群数据集进行的实验表明，准确性显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2401.07591v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **InstantID: Zero-shot Identity-Preserving Generation in Seconds**<br />
**Title_cn:** InstantID：几秒钟内零次身份保存生成<br />
**Authors:** Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen<br />
**Abstract:** <details><summary>原文: </summary>There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at https://github.com/InstantID/InstantID.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过 Textual Inversion、DreamBooth 和 LoRA 等方法，个性化图像合成取得了重大进展。然而，它们在现实世界中的适用性受到高存储需求、冗长的微调过程以及需要多个参考图像的阻碍。相反，现有的基于 ID 嵌入的方法虽然只需要一次前向推理，但也面临着挑战：它们要么需要对众多模型参数进行广泛的微调，要么缺乏与社区预训练模型的兼容性，要么无法保持高面部保真度。为了解决这些限制，我们引入了 InstantID，这是一种基于扩散模型的强大解决方案。我们的即插即用模块仅使用单个面部图像就能熟练地处理各种风格的图像个性化，同时确保高保真度。为了实现这一目标，我们设计了一个新颖的 IdentityNet，通过强加语义和弱空间条件，将面部和地标图像与文本提示相结合来引导图像生成。 InstantID 展示了卓越的性能和效率，在身份保存至关重要的现实应用中非常有用。此外，我们的工作与流行的预训练文本到图像扩散模型（如 SD1.5 和 SDXL）无缝集成，作为一个适应性强的插件。我们的代码和预先训练的检查点将在 https://github.com/InstantID/InstantID 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.07519v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Hierarchical Fashion Design with Multi-stage Diffusion Models**<br />
**Title_cn:** 多级扩散模型的分层时装设计<br />
**Authors:** Zhifeng Xie, Hao li, Huiming Ding, Mengtian Li, Ying Cao<br />
**Abstract:** <details><summary>原文: </summary>Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed them in different time step to the diffusion model according to the criteria of professional clothing designers.HieraFashDiff allows designers to add low-level attributes after high-level prompts for interactive editing incrementally.In addition, we design a differentiable loss function in the sampling process with a mask to keep non-edit areas.Comprehensive experiments performed on our newly conducted Hierarchical fashion dataset,demonstrate that our proposed method outperforms other state-of-the-art competitors.</details>
**Abstract_cn:** <details><summary>译文: </summary>跨模态的时装合成与编辑为时装设计师提供了智能支持，实现了设计稿的自动生成和本地修改。虽然当前的扩散模型在图像合成方面表现出了值得称赞的稳定性和可控性，但在从抽象设计生成时装设计方面仍然面临着巨大的挑战办公、商务、派对等抽象的感官表达构成了高层次的设计理念，而袖长、领型、裤长等可测量的方面则被认为是服装的低层次属性使用冗长的文本描述来控制和编辑时装图像存在困难。在本文中，我们提出了 HieraFashDiff，这是一种新颖的时装设计方法，使用共享的多阶段扩散模型，在层次结构中包含高层设计概念和低层服装属性具体来说，我们根据专业服装设计师的标准，将输入文本分为不同的级别，并以不同的时间步长馈送到扩散模型。HieraFashDiff 允许设计师在高级提示后添加低级属性，以增量方式进行交互式编辑此外，我们在采样过程中设计了一个可微损失函数，并带有掩模以保留非编辑区域。在我们新进行的分层时尚数据集上进行的综合实验表明，我们提出的方法优于其他最先进的竞争对手。</details>
**PDF:** <http://arxiv.org/pdf/2401.07450v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Cross Domain Early Crop Mapping using CropGAN and CNN Classifier**<br />
**Title_cn:** 使用 CropGAN 和 CNN 分类器进行跨域早期作物绘图<br />
**Authors:** Yiqun Wang, Hui Huang, Radu State<br />
**Abstract:** <details><summary>原文: </summary>Driven by abundant satellite imagery, machine learning-based approaches have recently been promoted to generate high-resolution crop cultivation maps to support many agricultural applications. One of the major challenges faced by these approaches is the limited availability of ground truth labels. In the absence of ground truth, existing work usually adopts the "direct transfer strategy" that trains a classifier using historical labels collected from other regions and then applies the trained model to the target region. Unfortunately, the spectral features of crops exhibit inter-region and inter-annual variability due to changes in soil composition, climate conditions, and crop progress, the resultant models perform poorly on new and unseen regions or years. This paper presents the Crop Generative Adversarial Network (CropGAN) to address the above cross-domain issue. Our approach does not need labels from the target domain. Instead, it learns a mapping function to transform the spectral features of the target domain to the source domain (with labels) while preserving their local structure. The classifier trained by the source domain data can be directly applied to the transformed data to produce high-accuracy early crop maps of the target domain. Comprehensive experiments across various regions and years demonstrate the benefits and effectiveness of the proposed approach. Compared with the widely adopted direct transfer strategy, the F1 score after applying the proposed CropGAN is improved by 13.13% - 50.98%</details>
**Abstract_cn:** <details><summary>译文: </summary>在丰富的卫星图像的推动下，基于机器学习的方法最近得到推广，可以生成高分辨率作物种植地图，以支持许多农业应用。这些方法面临的主要挑战之一是地面真实标签的可用性有限。在缺乏基本事实的情况下，现有的工作通常采用“直接迁移策略”，即使用从其他区域收集的历史标签来训练分类器，然后将训练后的模型应用于目标区域。不幸的是，由于土壤成分、气候条件和作物生长的变化，农作物的光谱特征表现出区域间和年际变化，由此产生的模型在新的和未见过的区域或年份上表现不佳。本文提出了作物生成对抗网络（CropGAN）来解决上述跨域问题。我们的方法不需要来自目标域的标签。相反，它学习一个映射函数，将目标域的光谱特征转换到源域（带有标签），同时保留其局部结构。由源域数据训练的分类器可以直接应用于转换后的数据，以生成目标域的高精度早期作物图。不同地区和年份的综合实验证明了所提出方法的好处和有效性。与广泛采用的直接迁移策略相比，应用所提出的 CropGAN 后的 F1 分数提高了 13.13% - 50.98%</details>
**PDF:** <http://arxiv.org/pdf/2401.07398v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **$M^{2}$Fusion: Bayesian-based Multimodal Multi-level Fusion on Colorectal Cancer Microsatellite Instability Prediction**<br />
**Title_cn:** $M^{2}$Fusion：基于贝叶斯的结直肠癌微卫星不稳定性预测多模态多级融合<br />
**Authors:** Quan Liu, Jiawen Yao, Lisha Yao, Xin Chen, Jingren Zhou, Le Lu, Ling Zhang, Zaiyi Liu, Yuankai Huo<br />
**Abstract:** <details><summary>原文: </summary>Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose $M^{2}$Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model $M^{2}$Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) $M^{2}$Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score.</details>
**Abstract_cn:** <details><summary>译文: </summary>组织病理学图像的结直肠癌 (CRC) 微卫星不稳定性 (MSI) 预测是一项具有挑战性的弱监督学习任务，涉及十亿像素图像的多实例学习。迄今为止，放射学图像已被证明具有 CRC MSI 信息和高效的患者成像技术。不同数据模式的集成提供了提高 MSI 预测的准确性和稳健性的机会。尽管在从整个幻灯片图像 (WSI) 进行表示学习以及探索利用放射学数据的潜力方面取得了进展，但 CRC MSI 预测仍然是融合来自多种数据模式（例如病理学 WSI 和放射学 CT 图像）的信息的挑战。在本文中，我们提出 $M^{2}$Fusion：一种基于贝叶斯的 CRC MSI 多模态多级融合管道。所提出的融合模型 $M^{2}$Fusion 能够在模态内部和跨模态发现更多新颖的模式，这比单独使用单一模态以及其他融合方法更有利于预测 MSI。该论文的贡献有三方面：(1) $M^{2}$Fusion 是第一个对病理 WSI 和 3D 放射学 CT 图像进行多级融合以进行 MSI 预测的流程； (2)首次将CT图像整合到多模态融合中进行CRC MSI预测； (3) 基于 Transformer 和基于 CNN 的方法评估特征级融合策略。我们的方法在 352 个案例的交叉验证中得到了验证，并且在 AUC 评分上优于特征级（0.8177 与 0.7908）或决策级融合策略（0.8177 与 0.7289）。</details>
**PDF:** <http://arxiv.org/pdf/2401.07854v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Uncovering the Full Potential of Visual Grounding Methods in VQA**<br />
**Title_cn:** 发掘 VQA 中视觉接地方法的全部潜力<br />
**Authors:** Daniel Reich, Tanja Schultz<br />
**Abstract:** <details><summary>原文: </summary>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model's reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits.   In this work, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that the potential benefits of these methods are severely underestimated as a result.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉问答 (VQA) 中的视觉接地 (VG) 方法试图通过加强模型对问题相关视觉信息的依赖来提高 VQA 性能。在训练和测试中通常假设视觉输入中存在此类相关信息。然而，在处理大规模 VQA 中常见的不完美图像表示时，这种假设本质上是有缺陷的，其中视觉特征携带的信息经常偏离预期的真实内容。因此，VG 方法的训练和测试是使用基本上不准确的数据进行的，这阻碍了对其潜在好处的正确评估。在这项工作中，我们证明了当前 VG 方法的评估方案存在问题，因为对相关视觉信息的可用性的假设存在缺陷。我们的实验表明，这些方法的潜在好处因此被严重低估。</details>
**PDF:** <http://arxiv.org/pdf/2401.07803v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Bi-Pyramid Multimodal Fusion Method for the Diagnosis of Bipolar Disorders**<br />
**Title_cn:** 用于诊断双相情感障碍的双金字塔多模态融合方法<br />
**Authors:** Guoxin Wang, Sheng Shi, Shan An, Fengmei Fan, Wenshu Ge, Qi Wang, Feng Yu, Zhiren Wang<br />
**Abstract:** <details><summary>原文: </summary>Previous research on the diagnosis of Bipolar disorder has mainly focused on resting-state functional magnetic resonance imaging. However, their accuracy can not meet the requirements of clinical diagnosis. Efficient multimodal fusion strategies have great potential for applications in multimodal data and can further improve the performance of medical diagnosis models. In this work, we utilize both sMRI and fMRI data and propose a novel multimodal diagnosis model for bipolar disorder. The proposed Patch Pyramid Feature Extraction Module extracts sMRI features, and the spatio-temporal pyramid structure extracts the fMRI features. Finally, they are fused by a fusion module to output diagnosis results with a classifier. Extensive experiments show that our proposed method outperforms others in balanced accuracy from 0.657 to 0.732 on the OpenfMRI dataset, and achieves the state of the art.</details>
**Abstract_cn:** <details><summary>译文: </summary>以往双相情感障碍诊断的研究主要集中在静息态功能磁共振成像。但其准确性无法满足临床诊断的要求。高效的多模态融合策略在多模态数据中具有巨大的应用潜力，可以进一步提高医学诊断模型的性能。在这项工作中，我们利用 sMRI 和 fMRI 数据，提出了一种新型的双相情感障碍多模态诊断模型。所提出的补丁金字塔特征提取模块提取sMRI特征，时空金字塔结构提取fMRI特征。最后，通过融合模块将它们融合，用分类器输出诊断结果。大量实验表明，我们提出的方法在 OpenfMRI 数据集上的平衡精度从 0.657 到 0.732 优于其他方法，并达到了最先进的水平。</details>
**PDF:** <http://arxiv.org/pdf/2401.07571v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video**<br />
**Title_cn:** 视频中时间句子扎根的偏差冲突样本合成和对抗性消除去偏差策略<br />
**Authors:** Zhaobo Qi, Yibo Yuan, Xiaowen Ruan, Shuhui Wang, Weigang Zhang, Qingming Huang<br />
**Abstract:** <details><summary>原文: </summary>Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias strategy (BSSARD), which dynamically generates bias-conflict samples by explicitly leveraging potentially spurious correlations between single-modality features and the temporal position of the target moments. Through adversarial training, its bias generators continuously introduce biases and generate bias-conflict samples to deceive its grounding model. Meanwhile, the grounding model continuously eliminates the introduced biases, which requires it to model multi-modality alignment information. BSSARD will cover most kinds of coupling relationships and disrupt language and visual biases simultaneously. Extensive experiments on Charades-CD and ActivityNet-CD demonstrate the promising debiasing capability of BSSARD. Source codes are available at https://github.com/qzhb/BSSARD.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频中的时间句子基础（TSGV）受到数据集偏差问题的困扰，这是由于输入视频或查询文本中具有相似语义成分的样本的目标时刻时间分布不均匀造成的。现有的方法诉诸于利用有关偏见的先验知识来人为地打破这种不均匀分布，这只能消除有限数量的显着语言偏见。在这项工作中，我们提出了偏差冲突样本合成和对抗性消除去偏差策略（BSSARD），它通过明确利用单模态特征和目标时刻的时间位置之间的潜在虚假相关性来动态生成偏差冲突样本。通过对抗性训练，其偏差生成器不断引入偏差并生成偏差冲突样本来欺骗其基础模型。同时，接地模型不断消除引入的偏差，这要求它能够对多模态对齐信息进行建模。 BSSARD 将涵盖大多数类型的耦合关系，并同时破坏语言和视觉偏见。 Charades-CD 和 ActivityNet-CD 上的大量实验证明了 BSSARD 的去偏能力。源代码可在 https://github.com/qzhb/BSSARD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07567v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **One for All: Toward Unified Foundation Models for Earth Vision**<br />
**Title_cn:** 为所有人服务：迈向地球愿景的统一基础模型<br />
**Authors:** Zhitong Xiong, Yi Wang, Fahong Zhang, Xiao Xiang Zhu<br />
**Abstract:** <details><summary>原文: </summary>Foundation models characterized by extensive parameters and trained on large-scale datasets have demonstrated remarkable efficacy across various downstream tasks for remote sensing data. Current remote sensing foundation models typically specialize in a single modality or a specific spatial resolution range, limiting their versatility for downstream datasets. While there have been attempts to develop multi-modal remote sensing foundation models, they typically employ separate vision encoders for each modality or spatial resolution, necessitating a switch in backbones contingent upon the input data. To address this issue, we introduce a simple yet effective method, termed OFA-Net (One-For-All Network): employing a single, shared Transformer backbone for multiple data modalities with different spatial resolutions. Using the masked image modeling mechanism, we pre-train a single Transformer backbone on a curated multi-modal dataset with this simple design. Then the backbone model can be used in different downstream tasks, thus forging a path towards a unified foundation backbone model in Earth vision. The proposed method is evaluated on 12 distinct downstream tasks and demonstrates promising performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>以广泛参数为特征并在大规模数据集上进行训练的基础模型在遥感数据的各种下游任务中表现出了显着的功效。当前的遥感基础模型通常专注于单一模态或特定的空间分辨率范围，限制了其下游数据集的多功能性。虽然已经尝试开发多模态遥感基础模型，但它们通常对每种模态或空间分辨率采用单独的视觉编码器，因此需要根据输入数据在主干网中进行切换。为了解决这个问题，我们引入了一种简单而有效的方法，称为 OFA-Net（One-For-All Network）：针对具有不同空间分辨率的多种数据模态采用单个共享 Transformer 主干。使用掩模图像建模机制，我们通过这种简单的设计在精选的多模态数据集上预训练单个 Transformer 主干。然后骨干模型可以用于不同的下游任务，从而为地球视觉中的统一基础骨干模型开辟道路。所提出的方法在 12 个不同的下游任务上进行了评估，并展示了良好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.07527v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting**<br />
**Title_cn:** GD-CAF：用于降水临近预报的图双流卷积注意力融合<br />
**Authors:** Lorand Vatamany, Siamak Mehrkanoon<br />
**Abstract:** <details><summary>原文: </summary>Accurate precipitation nowcasting is essential for various purposes, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolutional operations. This enhancement enables the model to directly process the high-dimensional spatiotemporal graph of precipitation maps and exploits higher-order correlations between the data dimensions. We evaluate our model on seven years of precipitation maps across Europe and its neighboring areas collected from the ERA5 dataset, provided by Copernicus. The model receives a fully connected graph in which each node represents historical observations from a specific region on the map. Consequently, each node contains a 3D tensor with time, height, and width dimensions. Experimental results demonstrate that the proposed GD-CAF model outperforms the other examined models. Furthermore, the averaged seasonal spatial and temporal attention scores over the test set are visualized to provide additional insights about the strongest connections between different regions or time steps. These visualizations shed light on the decision-making process of our model.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的降水临近预报对于洪水预测、灾害管理、优化农业活动、管理运输路线和可再生能源等多种目的至关重要。虽然一些研究从序列到序列的角度解决了这一具有挑战性的任务，但大多数研究都集中在单个区域，而没有考虑多个不相交区域之间现有的相关性。在本文中，我们将降水临近预报表述为时空图序列临近预报问题。特别是，我们引入了图双流卷积注意力融合（GD-CAF），这是一种新颖的方法，旨在从降水图的历史时空图中学习，并预测不同空间位置的未来时间步长降水。 GD-CAF 由时空卷积注意力以及配备深度可分离卷积运算的门控融合模块组成。这一增强功能使模型能够直接处理降水图的高维时空图，并利用数据维度之间的高阶相关性。我们根据哥白尼提供的 ERA5 数据集收集的欧洲及其邻近地区七年降水图来评估我们的模型。该模型接收一个完全连接的图，其中每个节点代表地图上特定区域的历史观测结果。因此，每个节点都包含一个具有时间、高度和宽度维度的 3D 张量。实验结果表明，所提出的 GD-CAF 模型优于其他检查模型。此外，测试集上的平均季节性空间和时间注意力分数被可视化，以提供有关不同区域或时间步之间最强联系的额外见解。这些可视化揭示了我们模型的决策过程。</details>
**PDF:** <http://arxiv.org/pdf/2401.07958v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Transformer-based Video Saliency Prediction with High Temporal Dimension Decoding**<br />
**Title_cn:** 基于变压器的视频显着性预测与高时间维度解码<br />
**Authors:** Morteza Moradi, Simone Palazzo, Concetto Spampinato<br />
**Abstract:** <details><summary>原文: </summary>In recent years, finding an effective and efficient strategy for exploiting spatial and temporal information has been a hot research topic in video saliency prediction (VSP). With the emergence of spatio-temporal transformers, the weakness of the prior strategies, e.g., 3D convolutional networks and LSTM-based networks, for capturing long-range dependencies has been effectively compensated. While VSP has drawn benefits from spatio-temporal transformers, finding the most effective way for aggregating temporal features is still challenging. To address this concern, we propose a transformer-based video saliency prediction approach with high temporal dimension decoding network (THTD-Net). This strategy accounts for the lack of complex hierarchical interactions between features that are extracted from the transformer-based spatio-temporal encoder: in particular, it does not require multiple decoders and aims at gradually reducing temporal features' dimensions in the decoder. This decoder-based architecture yields comparable performance to multi-branch and over-complicated models on common benchmarks such as DHF1K, UCF-sports and Hollywood-2.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，寻找一种有效且高效的策略来利用空间和时间信息一直是视频显着性预测（VSP）的热门研究课题。随着时空变换器的出现，现有策略（例如 3D 卷积网络和基于 LSTM 的网络）在捕获长程依赖性方面的弱点得到了有效弥补。虽然 VSP 从时空转换器中获益，但找到聚合时间特征的最有效方法仍然具有挑战性。为了解决这个问题，我们提出了一种基于变压器的视频显着性预测方法，具有高时间维度解码网络（THTD-Net）。该策略解释了从基于变换器的时空编码器提取的特征之间缺乏复杂的层次交互：特别是，它不需要多个解码器，并且旨在逐渐减少解码器中的时间特征的维度。这种基于解码器的架构在 DHF1K、UCF-sports 和 Hollywood-2 等常见基准测试上的性能可与多分支和过于复杂的模型相媲美。</details>
**PDF:** <http://arxiv.org/pdf/2401.07942v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Information hiding cameras: optical concealment of object information into ordinary images**<br />
**Title_cn:** 信息隐藏相机：将物体信息光学隐藏到普通图像中<br />
**Authors:** Bijie Bai, Ryan Lee, Yuhang Li, Tianyi Gan, Yuntian Wang, Mona Jarrahi, Aydogan Ozcan<br />
**Abstract:** <details><summary>原文: </summary>Data protection methods like cryptography, despite being effective, inadvertently signal the presence of secret communication, thereby drawing undue attention. Here, we introduce an optical information hiding camera integrated with an electronic decoder, optimized jointly through deep learning. This information hiding-decoding system employs a diffractive optical processor as its front-end, which transforms and hides input images in the form of ordinary-looking patterns that deceive/mislead human observers. This information hiding transformation is valid for infinitely many combinations of secret messages, all of which are transformed into ordinary-looking output patterns, achieved all-optically through passive light-matter interactions within the optical processor. By processing these ordinary-looking output images, a jointly-trained electronic decoder neural network accurately reconstructs the original information hidden within the deceptive output pattern. We numerically demonstrated our approach by designing an information hiding diffractive camera along with a jointly-optimized convolutional decoder neural network. The efficacy of this system was demonstrated under various lighting conditions and noise levels, showing its robustness. We further extended this information hiding camera to multi-spectral operation, allowing the concealment and decoding of multiple images at different wavelengths, all performed simultaneously in a single feed-forward operation. The feasibility of our framework was also demonstrated experimentally using THz radiation. This optical encoder-electronic decoder-based co-design provides a novel information hiding camera interface that is both high-speed and energy-efficient, offering an intriguing solution for visual information security.</details>
**Abstract_cn:** <details><summary>译文: </summary>像密码学这样的数据保护方法尽管有效，但却无意中表明了秘密通信的存在，从而引起了过度的关注。在这里，我们介绍了一种与电子解码器集成的光学信息隐藏相机，通过深度学习联合优化。该信息隐藏解码系统采用衍射光学处理器作为其前端，以看似普通的图案的形式转换和隐藏输入图像，从而欺骗/误导人类观察者。这种信息隐藏转换对于秘密消息的无限多种组合是有效的，所有这些组合都被转换成看起来普通的输出模式，通过光学处理器内的被动光与物质相互作用以全光学方式实现。通过处理这些看似普通的输出图像，联合训练的电子解码器神经网络可以准确地重建隐藏在欺骗性输出模式中的原始信息。我们通过设计信息隐藏衍射相机和联合优化的卷积解码器神经网络，以数字方式展示了我们的方法。该系统的功效在各种照明条件和噪声水平下得到了证明，显示了其稳健性。我们进一步将这种信息隐藏相机扩展到多光谱操作，允许隐藏和解码不同波长的多个图像，所有这些都在单个前馈操作中同时执行。我们的框架的可行性也通过太赫兹辐射的实验得到了证明。这种基于光学编码器-电子解码器的协同设计提供了一种新颖的信息隐藏相机接口，既高速又节能，为视觉信息安全提供了有趣的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.07856v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in Remote Sensing**<br />
**Title_cn:** 探索用于遥感中与传感器无关的图像检索的掩模自动编码器<br />
**Authors:** Jakob Hackstein, Gencer Sumbul, Kai Norman Clasen, Begüm Demir<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised learning through masked autoencoders (MAEs) has recently attracted great attention for remote sensing (RS) image representation learning, and thus embodies a significant potential for content-based image retrieval (CBIR) from ever-growing RS image archives. However, the existing studies on MAEs in RS assume that the considered RS images are acquired by a single image sensor, and thus are only suitable for uni-modal CBIR problems. The effectiveness of MAEs for cross-sensor CBIR, which aims to search semantically similar images across different image modalities, has not been explored yet. In this paper, we take the first step to explore the effectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a systematic overview on the possible adaptations of the vanilla MAE to exploit masked image modeling on multi-sensor RS image archives (denoted as cross-sensor masked autoencoders [CSMAEs]). Based on different adjustments applied to the vanilla MAE, we introduce different CSMAE models. We also provide an extensive experimental analysis of these CSMAE models. We finally derive a guideline to exploit masked image modeling for uni-modal and cross-modal CBIR problems in RS. The code of this work is publicly available at https://github.com/jakhac/CSMAE.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过掩模自动编码器（MAE）进行的自监督学习最近引起了遥感（RS）图像表示学习的极大关注，因此体现了从不断增长的遥感图像档案中进行基于内容的图像检索（CBIR）的巨大潜力。然而，现有的 RS 中 MAE 的研究假设所考虑的 RS 图像是由单个图像传感器获取的，因此仅适用于单模态 CBIR 问题。 MAE 在跨传感器 CBIR 中的有效性尚未得到探索，该跨传感器 CBIR 旨在跨不同图像模态搜索语义相似的图像。在本文中，我们迈出了第一步，探索 MAE 在 RS 中与传感器无关的 CBIR 的有效性。为此，我们对普通 MAE 的可能适应进行了系统概述，以利用多传感器 RS 图像档案上的掩模图像建模（表示为跨传感器掩模自动编码器 [CSMAE]）。基于对普通 MAE 的不同调整，我们引入了不同的 CSMAE 模型。我们还提供了这些 CSMAE 模型的广泛实验分析。我们最终得出了利用蒙版图像建模解决 RS 中单模态和跨模态 CBIR 问题的指南。这项工作的代码可在 https://github.com/jakhac/CSMAE 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.07782v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction**<br />
**Title_cn:** SSL-Interactions：交互式轨迹预测的借口任务<br />
**Authors:** Prarthana Bhattacharyya, Chengjie Huang, Krzysztof Czarnecki<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses motion forecasting in multi-agent environments, pivotal for ensuring safety of autonomous vehicles. Traditional as well as recent data-driven marginal trajectory prediction methods struggle to properly learn non-linear agent-to-agent interactions. We present SSL-Interactions that proposes pretext tasks to enhance interaction modeling for trajectory prediction. We introduce four interaction-aware pretext tasks to encapsulate various aspects of agent interactions: range gap prediction, closest distance prediction, direction of movement prediction, and type of interaction prediction. We further propose an approach to curate interaction-heavy scenarios from datasets. This curated data has two advantages: it provides a stronger learning signal to the interaction model, and facilitates generation of pseudo-labels for interaction-centric pretext tasks. We also propose three new metrics specifically designed to evaluate predictions in interactive scenes. Our empirical evaluations indicate SSL-Interactions outperforms state-of-the-art motion forecasting methods quantitatively with up to 8% improvement, and qualitatively, for interaction-heavy scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文讨论了多智能体环境中的运动预测，这对于确保自动驾驶车辆的安全至关重要。传统的以及最近的数据驱动的边缘轨迹预测方法都很难正确学习非线性智能体之间的交互。我们提出了 SSL-Interactions，它提出了借口任务来增强轨迹预测的交互建模。我们引入了四种交互感知借口任务来封装代理交互的各个方面：距离间隙预测、最近距离预测、运动方向预测和交互类型预测。我们进一步提出了一种从数据集中管理交互频繁的场景的方法。这些精选数据有两个优点：它为交互模型提供了更强的学习信号，并有助于为以交互为中心的借口任务生成伪标签。我们还提出了三个专门用于评估交互式场景中的预测的新指标。我们的实证评估表明，对于交互频繁的场景，SSL-Interactions 在数量上优于最先进的运动预测方法，提升高达 8%；在质量上也优于最先进的运动预测方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07729v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Sparsity-based background removal for STORM super-resolution images**<br />
**Title_cn:** 基于稀疏性的 STORM 超分辨率图像背景去除<br />
**Authors:** Patris Valera, Josué Page Vizcaíno, Tobias Lasser<br />
**Abstract:** <details><summary>原文: </summary>Single-molecule localization microscopy techniques, like stochastic optical reconstruction microscopy (STORM), visualize biological specimens by stochastically exciting sparse blinking emitters. The raw images suffer from unwanted background fluorescence, which must be removed to achieve super-resolution. We introduce a sparsity-based background removal method by adapting a neural network (SLNet) from a different microscopy domain. The SLNet computes a low-rank representation of the images, and then, by subtracting it from the raw images, the sparse component is computed, representing the frames without the background. We compared our approach with widely used background removal methods, such as the median background removal or the rolling ball algorithm, on two commonly used STORM datasets, one glial cell, and one microtubule dataset. The SLNet delivers STORM frames with less background, leading to higher emitters' localization precision and higher-resolution reconstructed images than commonly used methods. Notably, the SLNet is lightweight and easily trainable (<5 min). Since it is trained in an unsupervised manner, no prior information is required and can be applied to any STORM dataset. We uploaded a pre-trained SLNet to the Bioimage model zoo, easily accessible through ImageJ. Our results show that our sparse decomposition method could be an essential and efficient STORM pre-processing tool.</details>
**Abstract_cn:** <details><summary>译文: </summary>单分子定位显微镜技术，如随机光学重建显微镜 (STORM)，通过随机激发稀疏闪烁发射器来可视化生物样本。原始图像受到不需要的背景荧光的影响，必须将其去除才能实现超分辨率。我们通过适应不同显微镜领域的神经网络（SLNet）引入了一种基于稀疏性的背景去除方法。 SLNet 计算图像的低秩表示，然后通过从原始图像中减去它，计算稀疏分量，表示没有背景的帧。我们在两个常用的 STORM 数据集（一个胶质细胞数据集和一个微管数据集）上将我们的方法与广泛使用的背景去除方法（例如中值背景去除或滚球算法）进行了比较。 SLNet 提供背景较少的 STORM 帧，从而比常用方法具有更高的发射器定位精度和更高分辨率的重建图像。值得注意的是，SLNet 重量轻且易于训练（<5 分钟）。由于它是以无监督的方式进行训练的，因此不需要先验信息，并且可以应用于任何 STORM 数据集。我们将预先训练的 SLNet 上传到 Bioimage 模型库，可以通过 ImageJ 轻松访问。我们的结果表明，我们的稀疏分解方法可能是一种重要且高效的 STORM 预处理工具。</details>
**PDF:** <http://arxiv.org/pdf/2401.07746v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Cesium Tiles for High-realism Simulation and Comparing SLAM Results in Corresponding Virtual and Real-world Environments**<br />
**Title_cn:** 用于高真实度模拟并比较相应虚拟和现实环境中 SLAM 结果的 Cesium Tiles<br />
**Authors:** Chris Beam, Jincheng Zhang, Nicholas Kakavitsas, Collin Hague, Artur Wolek, Andrew Willis<br />
**Abstract:** <details><summary>原文: </summary>This article discusses the use of a simulated environment to predict algorithm results in the real world. Simulators are crucial in allowing researchers to test algorithms, sensor integration, and navigation systems without deploying expensive hardware. This article examines how the AirSim simulator, Unreal Engine, and Cesium plugin can be used to generate simulated digital twin models of real-world locations. Several technical challenges in completing the analysis are discussed and the technical solutions are detailed in this article. Work investigates how to assess mapping results for a real-life experiment using Cesium Tiles provided by digital twins of the experimental location. This is accompanied by a description of a process for duplicating real-world flights in simulation. The performance of these methods is evaluated by analyzing real-life and experimental image telemetry with the Direct Sparse Odometry (DSO) mapping algorithm. Results indicate that Cesium Tiles environments can provide highly accurate models of ground truth geometry after careful alignment. Further, results from real-life and simulated telemetry analysis indicate that the virtual simulation results accurately predict real-life results. Findings indicate that the algorithm results in real life and in the simulated duplicate exhibited a high degree of similarity. This indicates that the use of Cesium Tiles environments as a virtual digital twin for real-life experiments will provide representative results for such algorithms. The impact of this can be significant, potentially allowing expansive virtual testing of robotic systems at specific deployment locations to develop solutions that are tailored to the environment and potentially outperforming solutions meant to work in completely generic environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文讨论使用模拟环境来预测现实世界中的算法结果。模拟器对于研究人员无需部署昂贵的硬件即可测试算法、传感器集成和导航系统至关重要。本文探讨了如何使用 AirSim 模拟器、虚幻引擎和 Cesium 插件来生成真实世界位置的模拟数字孪生模型。本文讨论了完成分析时遇到的几个技术挑战，并详细介绍了技术解决方案。这项工作研究了如何使用实验位置的数字孪生提供的铯块来评估现实实验的测绘结果。伴随着在模拟中复制现实世界飞行的过程的描述。通过使用直接稀疏里程计 (DSO) 映射算法分析现实生活和实验图像遥测来评估这些方法的性能。结果表明，在仔细对齐后，Cesium Tiles 环境可以提供高精度的地面实况几何模型。此外，现实生活和模拟遥测分析的结果表明，虚拟模拟结果准确地预测了现实生活结果。研究结果表明，现实生活中的算法结果和模拟副本中的算法结果表现出高度的相似性。这表明使用 Cesium Tiles 环境作为现实实验的虚拟数字孪生将为此类算法提供代表性结果。其影响可能是巨大的，可能允许在特定部署位置对机器人系统进行广泛的虚拟测试，以开发适合环境的解决方案，并且可能优于在完全通用环境中工作的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.07962v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Image Similarity using An Ensemble of Context-Sensitive Models**<br />
**Title_cn:** 使用上下文敏感模型集合进行图像相似度<br />
**Authors:** Zukang Liao, Min Chen<br />
**Abstract:** <details><summary>原文: </summary>Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像相似性在计算机视觉领域得到了广泛的研究。近年来，机器学习模型已经显示出比传统多元指标能够编码更多语义的能力。然而，在标记相似度时，为一对图像分配数值分数不如确定图像 A 是否比另一图像 B 更接近参考图像 R 更直观。在这项工作中，我们提出了一种构建图像相似度的新方法基于 A:R 与 B:R 形式的标记数据的模型。我们通过使用集成模型解决了图像空间（R、A、B）中稀疏采样的挑战以及使用基于上下文的数据训练的模型中的偏差。特别是，我们采用了两种 ML 技术来构建这样的集成模型，即降维和 MLP 回归器。我们的测试结果表明，构建的集成模型的性能比最好的单独上下文敏感模型好约 5%。它们的表现也比使用混合图像数据训练的模型以及现有的相似性模型（例如 CLIP 和 DINO）要好。这项工作表明，当使用适当的集成方法来减轻稀疏采样带来的限制时，基于上下文的标记和模型训练可以是有效的。</details>
**PDF:** <http://arxiv.org/pdf/2401.07951v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Low-light Stereo Image Enhancement and De-noising in the Low-frequency Information Enhanced Image Space**<br />
**Title_cn:** 低频信息增强图像空间中的微光立体图像增强与去噪<br />
**Authors:** Minghua Zhao, Xiangdong Qin, Shuangli Du, Xuefei Bai, Jiahao Lyu, Yiguang Liu<br />
**Abstract:** <details><summary>原文: </summary>Unlike single image task, stereo image enhancement can use another view information, and its key stage is how to perform cross-view feature interaction to extract useful information from another view. However, complex noise in low-light image and its impact on subsequent feature encoding and interaction are ignored by the existing methods. In this paper, a method is proposed to perform enhancement and de-noising simultaneously. First, to reduce unwanted noise interference, a low-frequency information enhanced module (IEM) is proposed to suppress noise and produce a new image space. Additionally, a cross-channel and spatial context information mining module (CSM) is proposed to encode long-range spatial dependencies and to enhance inter-channel feature interaction. Relying on CSM, an encoder-decoder structure is constructed, incorporating cross-view and cross-scale feature interactions to perform enhancement in the new image space. Finally, the network is trained with the constraints of both spatial and frequency domain losses. Extensive experiments on both synthesized and real datasets show that our method obtains better detail recovery and noise removal compared with state-of-the-art methods. In addition, a real stereo image enhancement dataset is captured with stereo camera ZED2. The code and dataset are publicly available at: https://www.github.com/noportraits/LFENet.</details>
**Abstract_cn:** <details><summary>译文: </summary>与单图像任务不同，立体图像增强可以使用另一个视图信息，其关键阶段是如何进行跨视图特征交互以从另一个视图中提取有用信息。然而，现有方法忽略了低光图像中的复杂噪声及其对后续特征编码和交互的影响。本文提出了一种同时进行增强和去噪的方法。首先，为了减少不需要的噪声干扰，提出了低频信息增强模块（IEM）来抑制噪声并产生新的图像空间。此外，还提出了跨通道和空间上下文信息挖掘模块（CSM）来编码远程空间依赖性并增强通道间特征交互。依托CSM，构建编码器-解码器结构，结合跨视图和跨尺度特征交互，在新的图像空间中进行增强。最后，在空间域和频域损失的约束下训练网络。对合成数据集和真实数据集的大量实验表明，与最先进的方法相比，我们的方法获得了更好的细节恢复和噪声消除。此外，使用立体相机 ZED2 捕获真实的立体图像增强数据集。代码和数据集可在以下网址公开获取：https://www.github.com/noportraits/LFENet。</details>
**PDF:** <http://arxiv.org/pdf/2401.07753v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Curriculum for Crowd Counting -- Is it Worthy?**<br />
**Title_cn:** 人群计数课程——值得吗？<br />
**Authors:** Muhammad Asif Khan, Hamid Menouar, Ridha Hamila<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in deep learning techniques have achieved remarkable performance in several computer vision problems. A notably intuitive technique called Curriculum Learning (CL) has been introduced recently for training deep learning models. Surprisingly, curriculum learning achieves significantly improved results in some tasks but marginal or no improvement in others. Hence, there is still a debate about its adoption as a standard method to train supervised learning models. In this work, we investigate the impact of curriculum learning in crowd counting using the density estimation method. We performed detailed investigations by conducting 112 experiments using six different CL settings using eight different crowd models. Our experiments show that curriculum learning improves the model learning performance and shortens the convergence time.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习技术的最新进展在多个计算机视觉问题上取得了显着的性能。最近引入了一种称为课程学习（CL）的非常直观的技术，用于训练深度学习模型。令人惊讶的是，课程学习在某些任务中取得了显着的改善，但在其他任务中却取得了微小的改善或没有改善。因此，关于采用它作为训练监督学习模型的标准方法仍然存在争议。在这项工作中，我们使用密度估计方法研究了课程学习对人群计数的影响。我们使用六种不同的 CL 设置和八种不同的人群模型进行了 112 项实验，进行了详细的调查。我们的实验表明，课程学习提高了模型的学习性能并缩短了收敛时间。</details>
**PDF:** <http://arxiv.org/pdf/2401.07586v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **PolMERLIN: Self-Supervised Polarimetric Complex SAR Image Despeckling with Masked Networks**<br />
**Title_cn:** PolMERLIN：使用掩模网络进行自监督偏振复合 SAR 图像去斑<br />
**Authors:** Shunya Kato, Masaki Saito, Katsuhiko Ishiguro, Sol Cummings<br />
**Abstract:** <details><summary>原文: </summary>Despeckling is a crucial noise reduction task in improving the quality of synthetic aperture radar (SAR) images. Directly obtaining noise-free SAR images is a challenging task that has hindered the development of accurate despeckling algorithms. The advent of deep learning has facilitated the study of denoising models that learn from only noisy SAR images. However, existing methods deal solely with single-polarization images and cannot handle the multi-polarization images captured by modern satellites. In this work, we present an extension of the existing model for generating single-polarization SAR images to handle multi-polarization SAR images. Specifically, we propose a novel self-supervised despeckling approach called channel masking, which exploits the relationship between polarizations. Additionally, we utilize a spatial masking method that addresses pixel-to-pixel correlations to further enhance the performance of our approach. By effectively incorporating multiple polarization information, our method surpasses current state-of-the-art methods in quantitative evaluation in both synthetic and real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>去斑是提高合成孔径雷达 (SAR) 图像质量的一项关键降噪任务。直接获取无噪声的SAR图像是一项具有挑战性的任务，阻碍了精确去斑算法的发展。深度学习的出现促进了仅从噪声 SAR 图像中学习的去噪模型的研究。然而，现有方法仅处理单偏振图像，无法处理现代卫星捕获的多偏振图像​​。在这项工作中，我们提出了对现有模型的扩展，用于生成单偏振 SAR 图像来处理多偏振 SAR 图像。具体来说，我们提出了一种称为通道掩蔽的新型自监督去斑方法，它利用了偏振之间的关系。此外，我们利用空间掩蔽方法来解决像素到像素的相关性，以进一步增强我们方法的性能。通过有效地结合多个偏振信息，我们的方法在合成和现实场景中的定量评估方面都超越了当前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07503v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Concept-Guided Prompt Learning for Generalization in Vision-Language Models**<br />
**Title_cn:** 用于视觉语言模型泛化的概念引导即时学习<br />
**Authors:** Yi Zhang, Ce Zhang, Ke Yu, Yushun Tang, Zhihai He<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Language-Image Pretraining (CLIP) model has exhibited remarkable efficacy in establishing cross-modal connections between texts and images, yielding impressive performance across a broad spectrum of downstream applications through fine-tuning. However, for generalization tasks, the current fine-tuning methods for CLIP, such as CoOp and CoCoOp, demonstrate relatively low performance on some fine-grained datasets. We recognize the underlying reason is that these previous methods only projected global features into the prompt, neglecting the various visual concepts, such as colors, shapes, and sizes, which are naturally transferable across domains and play a crucial role in generalization tasks. To address this issue, in this work, we propose Concept-Guided Prompt Learning (CPL) for vision-language models. Specifically, we leverage the well-learned knowledge of CLIP to create a visual concept cache to enable concept-guided prompting. In order to refine the text features, we further develop a projector that transforms multi-level visual features into text features. We observe that this concept-guided prompt learning approach is able to achieve enhanced consistency between visual and linguistic modalities. Extensive experimental results demonstrate that our CPL method significantly improves generalization capabilities compared to the current state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比语言-图像预训练（CLIP）模型在建立文本和图像之间的跨模态连接方面表现出了显着的功效，通过微调在广泛的下游应用程序中产生了令人印象深刻的性能。然而，对于泛化任务，当前的 CLIP 微调方法（例如 CoOp 和 CoCoOp）在一些细粒度数据集上表现出相对较低的性能。我们认识到根本原因是这些先前的方法仅将全局特征投射到提示中，忽略了各种视觉概念，例如颜色、形状和大小，这些概念可以自然地跨领域转移并在泛化任务中发挥至关重要的作用。为了解决这个问题，在这项工作中，我们提出了针对视觉语言模型的概念引导提示学习（CPL）。具体来说，我们利用 CLIP 的丰富知识来创建视觉概念缓存，以实现概念引导提示。为了细化文本特征，我们进一步开发了一种将多级视觉特征转换为文本特征的投影仪。我们观察到这种概念引导的即时学习方法能够增强视觉和语言模式之间的一致性。大量的实验结果表明，与当前最先进的方法相比，我们的 CPL 方法显着提高了泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.07457v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Mask-adaptive Gated Convolution and Bi-directional Progressive Fusion Network for Depth Completion**<br />
**Title_cn:** 用于深度完成的掩模自适应门控卷积和双向渐进融合网络<br />
**Authors:** Tingxuan Huang, Jiacheng Miao, Shizhuo Deng, Tong, Dongyue Chen<br />
**Abstract:** <details><summary>原文: </summary>Depth completion is a critical task for handling depth images with missing pixels, which can negatively impact further applications. Recent approaches have utilized Convolutional Neural Networks (CNNs) to reconstruct depth images with the assistance of color images. However, vanilla convolution has non-negligible drawbacks in handling missing pixels. To solve this problem, we propose a new model for depth completion based on an encoder-decoder structure. Our model introduces two key components: the Mask-adaptive Gated Convolution (MagaConv) architecture and the Bi-directional Progressive Fusion (BP-Fusion) module. The MagaConv architecture is designed to acquire precise depth features by modulating convolution operations with iteratively updated masks, while the BP-Fusion module progressively integrates depth and color features, utilizing consecutive bi-directional fusion structures in a global perspective. Extensive experiments on popular benchmarks, including NYU-Depth V2, DIML, and SUN RGB-D, demonstrate the superiority of our model over state-of-the-art methods. We achieved remarkable performance in completing depth maps and outperformed existing approaches in terms of accuracy and reliability.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度补全是处理缺失像素的深度图像的一项关键任务，这可能会对进一步的应用产生负面影响。最近的方法利用卷积神经网络（CNN）在彩色图像的帮助下重建深度图像。然而，普通卷积在处理丢失像素方面具有不可忽视的缺点。为了解决这个问题，我们提出了一种基于编码器-解码器结构的深度补全新模型。我们的模型引入了两个关键组件：掩模自适应门控卷积（MagaConv）架构和双向渐进融合（BP-Fusion）模块。 MagaConv 架构旨在通过迭代更新掩模来调制卷积运算来获取精确的深度特征，而 BP-Fusion 模块则逐步集成深度和颜色特征，在全局视角下利用连续的双向融合结构。对流行基准（包括 NYU-Depth V2、DIML 和 SUN RGB-D）进行的大量实验证明了我们的模型相对于最先进方法的优越性。我们在完成深度图方面取得了显着的性能，并且在准确性和可靠性方面优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.07439v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Improved Implicity Neural Representation with Fourier Bases Reparameterized Training**<br />
**Title_cn:** 通过傅里叶基重新参数化训练改进隐式神经表示<br />
**Authors:** Kexuan Shi, Xingyu Zhou, Shuhang Gu<br />
**Abstract:** <details><summary>原文: </summary>Implicit Neural Representation (INR) as a mighty representation paradigm has achieved success in various computer vision tasks recently. Due to the low-frequency bias issue of vanilla multi-layer perceptron (MLP), existing methods have investigated advanced techniques, such as positional encoding and periodic activation function, to improve the accuracy of INR. In this paper, we connect the network training bias with the reparameterization technique and theoretically prove that weight reparameterization could provide us a chance to alleviate the spectral bias of MLP. Based on our theoretical analysis, we propose a Fourier reparameterization method which learns coefficient matrix of fixed Fourier bases to compose the weights of MLP. We evaluate the proposed Fourier reparameterization method on different INR tasks with various MLP architectures, including vanilla MLP, MLP with positional encoding and MLP with advanced activation function, etc. The superiority approximation results on different MLP architectures clearly validate the advantage of our proposed method. Armed with our Fourier reparameterization method, better INR with more textures and less artifacts can be learned from the training data.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐式神经表示（INR）作为一种强大的表示范式，最近在各种计算机视觉任务中取得了成功。由于普通多层感知器（MLP）的低频偏差问题，现有方法研究了位置编码和周期性激活函数等先进技术，以提高 INR 的准确性。在本文中，我们将网络训练偏差与重新参数化技术联系起来，并从理论上证明权重重新参数化可以为我们提供减轻 MLP 谱偏差的机会。基于我们的理论分析，我们提出了一种傅里叶重新参数化方法，该方法学习固定傅里叶基的系数矩阵来组成 MLP 的权重。我们在具有各种 MLP 架构的不同 INR 任务上评估了所提出的傅里叶重参数化方法，包括普通 MLP、具有位置编码的 MLP 和具有高级激活函数的 MLP 等。不同 MLP 架构上的优越性近似结果清楚地验证了我们提出的方法的优势。借助我们的傅立叶重新参数化方法，可以从训练数据中学习具有更多纹理和更少伪影的更好 INR。</details>
**PDF:** <http://arxiv.org/pdf/2401.07402v1><br />
**Code:** null<br />

