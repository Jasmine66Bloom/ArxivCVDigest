## [UPDATED!] **2024-01-16** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection**<br />
**Title_cn:** SAMF：用于物体检测的小区域感知多焦点图像融合<br />
**Authors:** Xilai Li, Xiaosong Li, Haishu Tan, Jinyang Li<br />
**Abstract:** <details><summary>原文: </summary>Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately. To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability. First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels. To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy. Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results. Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations. The source code is available at https://github.com/ixilai/SAMF.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的多焦点图像融合（MFIF）方法通常无法保留不确定的过渡区域并准确检测大散焦区域内的小焦点区域。为了解决这个问题，本研究提出了一种新的小区域感知 MFIF 算法来增强目标检测能力。首先，我们增强小焦点和边界区域内的像素属性，随后将其与视觉显着性检测相结​​合，以获得用于区分聚焦像素分布的预融合结果。为了准确地确保像素聚焦，我们将源图像视为聚焦、散焦和不确定区域的组合，并提出了三区域分割策略。最后，我们设计有效的像素选择规则来生成分割决策图并获得最终的融合结果。实验表明，该方法能够准确检测小且平滑的聚焦区域，同时提高目标检测性能，在主观和客观评估方面均优于现有方法。源代码可在 https://github.com/ixilai/SAMF 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08357v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-$\mathrm{M^2}$DF)**<br />
**Title_cn:** 基于多模态融合的多视图蒸馏进行小样本动作识别(CLIP-$\mathrm{M^2}$DF)<br />
**Authors:** Fei Guo, YiKang Wang, Han Qi, WenPing Jin, Li Zhu<br />
**Abstract:** <details><summary>原文: </summary>In recent years, few-shot action recognition has attracted increasing attention. It generally adopts the paradigm of meta-learning. In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples. We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity. Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query. Secondly, we establish a Multi-view. In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers. Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias. Our code is available at the URL: \url{https://github.com/cofly2014/MDMF}.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，少镜头动作识别引起了越来越多的关注。它一般采用元学习的范式。在这一领域，基于有限的样本，克服类和异常值的重叠分布仍然是一个具有挑战性的问题。我们相信多模态和多视图的结合可以通过信息互补来改善这个问题。因此，我们提出了一种基于多模态融合的多视图蒸馏方法。首先，构造查询的概率提示选择器，以基于支持的提示嵌入与查询的视觉嵌入之间的比较分数来生成概率提示嵌入。其次，我们建立多视角。在每个视图中，我们将提示嵌入作为一致的信息与视觉和全局或局部时间上下文融合，以克服类和异常值的重叠分布。第三，我们对多视图进行距离融合以及相互匹配能力的相互升华，使模型对分布偏差更加鲁棒。我们的代码可从以下 URL 获取：\url{https://github.com/cofly2014/MDMF}。</details>
**PDF:** <http://arxiv.org/pdf/2401.08345v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction**<br />
**Title_cn:** 生成去噪蒸馏：简单的随机噪声诱导高效的知识转移以实现密集预测<br />
**Authors:** Zhaoge Liu, Xiaohao Xu, Yunkang Cao, Weiming Shen<br />
**Abstract:** <details><summary>原文: </summary>Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student). Numerous current approaches involve the student imitating the knowledge of the teacher directly. However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately. To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network. Then, the generated instance feature is aligned with the knowledge of the instance from the teacher. We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method. Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above. We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories. The source code of GDD is available at https://github.com/ZhgLiu/GDD.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识蒸馏是将知识从更强大的大型模型（教师）转移到更简单的对应模型（学生）的过程。当前的许多方法都涉及学生直接模仿老师的知识。然而，通过这些流行的方法学习到的表示仍然存在冗余，这些方法往往不加区别地学习每个空间位置的特征。为了从教师那里获得更紧凑的表示（概念特征），受人类认知的启发，我们提出了一种创新方法，称为生成降噪蒸馏（GDD），其中将随机噪声添加到学生的概念特征中，将其嵌入到学生的概念特征中。从浅层网络生成实例特征。然后，生成的实例特征与教师提供的实例知识相匹配。我们对对象检测、实例分割和语义分割进行了广泛的实验，以证明我们方法的多功能性和有效性。值得注意的是，GDD 在上述任务中实现了新的最先进的性能。我们通过增强 PspNet 和 DeepLabV3 在语义分割方面取得了实质性的改进，这两个模型都基于 ResNet-18，其 mIoU 分数分别为 74.67 和 77.69，超过了它们之前在 20 个类别的 Cityscapes 数据集上的分数 69.85 和 73.20 。 GDD的源代码可以在https://github.com/ZhgLiu/GDD获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08332v1><br />
**Code:** <https://github.com/ZhgLiu/GDD>**<br />
>>**index:** 4<br />
**Title:** **Modeling Spoof Noise by De-spoofing Diffusion and its Application in Face Anti-spoofing**<br />
**Title_cn:** 反欺骗扩散模拟欺骗噪声及其在人脸反欺骗中的应用<br />
**Authors:** Bin Zhang, Xiangyu Zhu, Xiaoyu Zhang, Zhen Lei<br />
**Abstract:** <details><summary>原文: </summary>Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems. Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image. But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods. In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image. The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing. We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸反欺骗对于确保人脸识别系统的安全性和可靠性至关重要。几种现有的面部反欺骗方法利用类似 GAN 的网络，通过估计欺骗图像的噪声模式并恢复相应的真实图像来检测呈现攻击。但GAN有限的人脸外观空间导致去噪后的人脸无法覆盖真实人脸的完整数据分布，从而削弱了此类方法的泛化性能。在这项工作中，我们提出了利用扩散模型对恶搞图像进行去噪并恢复真实图像的开创性尝试。这两个图像之间的差异被认为是欺骗噪声，它可以作为面部反欺骗的判别线索。我们在几个内部测试和相互测试协议上评估了我们提出的方法，其中实验结果展示了我们的方法在准确性和泛化方面实现竞争性能的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08275v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multi-Technique Sequential Information Consistency For Dynamic Visual Place Recognition In Changing Environments**<br />
**Title_cn:** 变化环境中动态视觉位置识别的多技术序列信息一致性<br />
**Authors:** Bruno Arcanjo, Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan<br />
**Abstract:** <details><summary>原文: </summary>Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data. VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints. Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance. Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets. Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis. For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image. The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉地点识别 (VPR) 是机器人导航和定位系统的重要组成部分，使机器人能够仅使用图像数据来识别地点。 VPR 具有挑战性，因为不同的日常照明、季节性天气变化和不同的观点会导致一个地方的外观发生显着变化。目前，没有一种 VPR 技术能够适应所有环境条件，每种技术都具有独特的优点和缺点，因此结合多种技术可以实现更可靠的 VPR 性能。目前的多方法方法要么依赖于通常不可用的在线真实信息，要么依赖于强力技术组合，可能会降低高方差技术集的性能。为了解决这些缺点，我们提出了一种称为多序列信息一致性（MuSIC）的 VPR 系统，它利用序列信息在每帧的在线基础上选择最具凝聚力的技术。对于一组中的每种技术，MuSIC 通过分析其最佳匹配候选者的帧到帧连续性来计算其各自的顺序一致性，然后直接比较这些技术以选择当前查询图像的最佳技术。使用顺序信息在 VPR 方法之间进行选择可以提高不同基准数据集上的整体 VPR 性能，同时避免需要运行时环境的额外真实数据。</details>
**PDF:** <http://arxiv.org/pdf/2401.08263v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video Localization**<br />
**Title_cn:** 用于自然语言视频定位的多尺度 2D 时间图扩散模型<br />
**Authors:** Chongzhi Zhang, Mingyuan Zhang, Zhiyang Teng, Jiayi Li, Xizhou Zhu, Lewei Lu, Ziwei Liu, Aixin Sun<br />
**Abstract:** <details><summary>原文: </summary>Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding. Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data. In this study, we present a novel approach to NLVL that aims to address this issue. Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query. The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder. To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder. Our approach effectively encapsulates the interaction between the query and video data across various time scales. Experiments on the Charades and DiDeMo datasets underscore the potency of our design.</details>
**Abstract_cn:** <details><summary>译文: </summary>自然语言视频本地化（NLVL）是将自然语言描述的短语基础到相应的视频片段，是视频理解中一项复杂但关键的任务。尽管不断取得进步，但许多现有解决方案缺乏全局捕获视频数据的时间动态的能力。在这项研究中，我们提出了一种新的 NLVL 方法，旨在解决这个问题。我们的方法涉及基于输入视频和语言查询，通过条件去噪扩散过程直接生成全局 2D 时间图。主要挑战是设计扩散解码器时二维时间图固有的稀疏性和不连续性。为了应对这些挑战，我们引入了多尺度技术并开发了一种创新的扩散解码器。我们的方法有效地封装了不同时间尺度的查询和视频数据之间的交互。 Charades 和 DiDeMo 数据集上的实验强调了我们设计的效力。</details>
**PDF:** <http://arxiv.org/pdf/2401.08232v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point Cloud Classification**<br />
**Title_cn:** ModelNet-O：用于遮挡感知点云分类的大规模综合数据集<br />
**Authors:** Zhongbin Fang, Xia Li, Xiangtai Li, Shen Zhao, Mengyuan Liu<br />
**Abstract:** <details><summary>原文: </summary>Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective. More experiments also demonstrate the robustness and effectiveness of PointMLS.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，3D 点云分类在许多数据集的帮助下取得了重大进展。然而，这些数据集并不能反映现实世界点云因遮挡而导致的不完整性质，这限制了当前方法的实际应用。为了弥补这一差距，我们提出了 ModelNet-O，这是一个包含 123,041 个样本的大规模合成数据集，它模拟现实世界的点云，具有由单目相机扫描引起的自遮挡。 ModelNet-O 比现有数据集大 10 倍，并提供更具挑战性的案例来评估现有方法的稳健性。我们对 ModelNet-O 的观察表明，精心设计的稀疏结构可以在遮挡下保留点云的结构信息，这促使我们提出一种鲁棒的点云处理方法，该方法以多级方式利用临界点采样（CPS）策略。我们将我们的方法称为 PointMLS。通过大量的实验，我们证明了我们的 PointMLS 在 ModelNet-O 上取得了最先进的结果，在常规数据集上取得了有竞争力的结果，并且它是稳健且有效的。更多的实验也证明了 PointMLS 的稳健性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08210v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **End-to-End Optimized Image Compression with the Frequency-Oriented Transform**<br />
**Title_cn:** 使用面向频率的变换进行端到端优化的图像压缩<br />
**Authors:** Yuefeng Zhang, Kai Lin<br />
**Abstract:** <details><summary>原文: </summary>Image compression constitutes a significant challenge amidst the era of information explosion. Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs. However, an inherent challenge associated with these methods lies in their lack of interpretability. Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform. The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion. The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept. Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components. Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric. Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method could preserve semantic fidelity besides signal-level precision.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像压缩是信息爆炸时代的重大挑战。最近采用深度学习方法的研究证明了基于学习的图像压缩方法比传统编解码器具有优越的性能。然而，与这些方法相关的固有挑战在于它们缺乏可解释性。在分析不同频段的不同程度的压缩退化之后，我们提出了由面向频率的变换促进的端到端优化的图像压缩模型。所提出的端到端图像压缩模型由四个部分组成：空间采样、面向频率的变换、熵估计和频率感知融合。面向频率的变换将原始图像信号分成不同的频带，与人类可解释的概念保持一致。利用非重叠假​​设，该模型通过选择性传输任意频率分量来实现可扩展编码。进行了大量的实验来证明我们的模型在 MS-SSIM 指标上优于所有传统编解码器，包括下一代标准 H.266/VVC。此外，还进行了视觉分析任务（即对象检测和语义分割），以验证所提出的压缩方法除了信号级精度之外还可以保留语义保真度。</details>
**PDF:** <http://arxiv.org/pdf/2401.08194v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Completely Occluded and Dense Object Instance Segmentation Using Box Prompt-Based Segmentation Foundation Models**<br />
**Title_cn:** 使用基于框提示的分割基础模型进行完全遮挡和密集的对象实例分割<br />
**Authors:** Zhen Zhou, Junfeng Fan, Yunkai Ma, Sihan Zhao, Fengshui Jing, Min Tan<br />
**Abstract:** <details><summary>原文: </summary>Completely occluded and dense object instance segmentation (IS) is an important and challenging task. Although current amodal IS methods can predict invisible regions of occluded objects, they are difficult to directly predict completely occluded objects. For dense object IS, existing box-based methods are overly dependent on the performance of bounding box detection. In this paper, we propose CFNet, a coarse-to-fine IS framework for completely occluded and dense objects, which is based on box prompt-based segmentation foundation models (BSMs). Specifically, CFNet first detects oriented bounding boxes (OBBs) to distinguish instances and provide coarse localization information. Then, it predicts OBB prompt-related masks for fine segmentation. To predict completely occluded object instances, CFNet performs IS on occluders and utilizes prior geometric properties, which overcomes the difficulty of directly predicting completely occluded object instances. Furthermore, based on BSMs, CFNet reduces the dependence on bounding box detection performance, improving dense object IS performance. Moreover, we propose a novel OBB prompt encoder for BSMs. To make CFNet more lightweight, we perform knowledge distillation on it and introduce a Gaussian smoothing method for teacher targets. Experimental results demonstrate that CFNet achieves the best performance on both industrial and publicly available datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>完全遮挡和密集的对象实例分割（IS）是一项重要且具有挑战性的任务。尽管当前的非模态IS方法可以预测被遮挡物体的不可见区域，但它们很难直接预测完全被遮挡的物体。对于密集对象 IS，现有的基于框的方法过度依赖于边界框检测的性能。在本文中，我们提出了 CFNet，这是一种针对完全遮挡和密集对象的从粗到细的 IS 框架，它基于基于框提示的分割基础模型（BSM）。具体来说，CFNet 首先检测定向边界框 (OBB) 以区分实例并提供粗略的定位信息。然后，它预测 OBB 提示相关的掩模以进行精细分割。为了预测完全遮挡的对象实例，CFNet 对遮挡器执行 IS 并利用先验几何属性，克服了直接预测完全遮挡的对象实例的困难。此外，基于 BSM，CFNet 减少了对边界框检测性能的依赖，提高了密集对象 IS 性能。此外，我们为 BSM 提出了一种新颖的 OBB 提示编码器。为了使CFNet更加轻量级，我们对其进行知识蒸馏，并为教师目标引入高斯平滑方法。实验结果表明，CFNet 在工业数据集和公开数据集上均取得了最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08174v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Mobile Contactless Palmprint Recognition: Use of Multiscale, Multimodel Embeddings**<br />
**Title_cn:** 移动非接触式掌纹识别：使用多尺度、多模型嵌入<br />
**Authors:** Steven A. Grosz, Akash Godbole, Anil K. Jain<br />
**Abstract:** <details><summary>原文: </summary>Contactless palmprints are comprised of both global and local discriminative features. Most prior work focuses on extracting global features or local features alone for palmprint matching, whereas this research introduces a novel framework that combines global and local features for enhanced palmprint matching accuracy. Leveraging recent advancements in deep learning, this study integrates a vision transformer (ViT) and a convolutional neural network (CNN) to extract complementary local and global features. Next, a mobile-based, end-to-end palmprint recognition system is developed, referred to as Palm-ID. On top of the ViT and CNN features, Palm-ID incorporates a palmprint enhancement module and efficient dimensionality reduction (for faster matching). Palm-ID balances the trade-off between accuracy and latency, requiring just 18ms to extract a template of size 516 bytes, which can be efficiently searched against a 10,000 palmprint gallery in 0.33ms on an AMD EPYC 7543 32-Core CPU utilizing 128-threads. Cross-database matching protocols and evaluations on large-scale operational datasets demonstrate the robustness of the proposed method, achieving a TAR of 98.06% at FAR=0.01% on a newly collected, time-separated dataset. To show a practical deployment of the end-to-end system, the entire recognition pipeline is embedded within a mobile device for enhanced user privacy and security.</details>
**Abstract_cn:** <details><summary>译文: </summary>非接触式掌纹由全局和局部判别特征组成。大多数先前的工作都集中于单独提取全局特征或局部特征来进行掌纹匹配，而本研究引入了一种结合全局和局部特征的新颖框架，以提高掌纹匹配的准确性。本研究利用深度学习的最新进展，集成了视觉变换器（ViT）和卷积神经网络（CNN）来提取互补的局部和全局特征。接下来，开发了一种基于移动设备的端到端掌纹识别系统，称为Palm-ID。除了 ViT 和 CNN 功能之外，Palm-ID 还集成了掌纹增强模块和高效的降维（以实现更快的匹配）。 Palm-ID 平衡了准确性和延迟之间的权衡，只需 18 毫秒即可提取大小为 516 字节的模板，并且可以在 AMD EPYC 7543 32 核 CPU 上利用 128 个处理器在 0.33 毫秒内有效地搜索 10,000 个掌纹图库。线程。跨数据库匹配协议和对大规模操作数据集的评估证明了所提出方法的鲁棒性，在新收集的时间分离数据集上，FAR=0.01% 时实现了 98.06% 的 TAR。为了展示端到端系统的实际部署，整个识别管道嵌入到移动设备中，以增强用户隐私和安全性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08111v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Hardware Acceleration for Real-Time Wildfire Detection Onboard Drone Networks**<br />
**Title_cn:** 用于实时野火检测机载无人机网络的硬件加速<br />
**Authors:** Austin Briley, Fatemeh Afghah<br />
**Abstract:** <details><summary>原文: </summary>Early wildfire detection in remote and forest areas is crucial for minimizing devastation and preserving ecosystems. Autonomous drones offer agile access to remote, challenging terrains, equipped with advanced imaging technology that delivers both high-temporal and detailed spatial resolution, making them valuable assets in the early detection and monitoring of wildfires. However, the limited computation and battery resources of Unmanned Aerial Vehicles (UAVs) pose significant challenges in implementing robust and efficient image classification models. Current works in this domain often operate offline, emphasizing the need for solutions that can perform inference in real time, given the constraints of UAVs. To address these challenges, this paper aims to develop a real-time image classification and fire segmentation model. It presents a comprehensive investigation into hardware acceleration using the Jetson Nano P3450 and the implications of TensorRT, NVIDIA's high-performance deep-learning inference library, on fire classification accuracy and speed. The study includes implementations of Quantization Aware Training (QAT), Automatic Mixed Precision (AMP), and post-training mechanisms, comparing them against the latest baselines for fire segmentation and classification. All experiments utilize the FLAME dataset - an image dataset collected by low-altitude drones during a prescribed forest fire. This work contributes to the ongoing efforts to enable real-time, on-board wildfire detection capabilities for UAVs, addressing speed and the computational and energy constraints of these crucial monitoring systems. The results show a 13% increase in classification speed compared to similar models without hardware optimization. Comparatively, loss and accuracy are within 1.225% of the original values.</details>
**Abstract_cn:** <details><summary>译文: </summary>偏远地区和森林地区的早期野火检测对于最大限度地减少破坏和保护生态系统至关重要。自主无人机可灵活进入偏远、具有挑战性的地形，配备先进的成像技术，可提供高时间和详细的空间分辨率，使其成为早期发现和监测野火的宝贵资产。然而，无人机 (UAV) 有限的计算和电池资源对实现稳健且高效的图像分类模型提出了重大挑战。目前该领域的工作通常是离线运行的，考虑到无人机的限制，强调需要能够实时进行推理的解决方案。为了应对这些挑战，本文旨在开发实时图像分类和火灾分割模型。它对使用 Jetson Nano P3450 的硬件加速以及 NVIDIA 的高性能深度学习推理库 TensorRT 对火灾分类准确性和速度的影响进行了全面调查。该研究包括量化感知训练（QAT）、自动混合精度（AMP）和训练后机制的实现，并将它们与火灾分割和分类的最新基线进行比较。所有实验都利用 FLAME 数据集——低空无人机在规定的森林火灾期间收集的图像数据集。这项工作有助于持续努力，为无人机提供实时机载野火检测功能，解决这些关键监测系统的速度、计算和能源限制。结果显示，与未经硬件优化的类似模型相比，分类速度提高了 13%。相比之下，损失和准确率均在原始值的 1.225% 以内。</details>
**PDF:** <http://arxiv.org/pdf/2401.08105v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **UV-SAM: Adapting Segment Anything Model for Urban Village Identification**<br />
**Title_cn:** UV-SAM：采用分段任意模型进行城中村识别<br />
**Authors:** Xin Zhang, Yu Liu, Yuming Lin, Qingming Liao, Yong Li<br />
**Abstract:** <details><summary>原文: </summary>Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>城中村是指城市中心及其周边的非正规居住区，其特点是基础设施不足、生活条件差，与贫困、适足住房和可持续城市等可持续发展目标密切相关。传统上，政府严重依赖实地调查方法来监测城中村，但这种方法费时、费力，而且可能会造成延误。由于卫星图像广泛可用且及时更新，最近的研究开发了计算机视觉技术来有效地检测城中村。然而，现有研究要么侧重于简单的城中村图像分类，要么无法提供准确的边界信息。为了从卫星图像中准确识别城中村边界，我们利用视觉基础模型的强大功能，将分段任意模型（SAM）应用于城中村分割，称为UV-SAM。具体来说，UV-SAM 首先利用小型语义分割模型为城中村生成混合提示，包括掩模、边界框和图像表示，然后将其输入 SAM 进行细粒度边界识别。在中国两个数据集上的大量实验结果表明，UV-SAM优于现有基线，并且多年的识别结果表明，城中村的数量和面积都随着时间的推移而减少，为城中村和棚屋的发展趋势提供了更深入的见解阐述可持续城市的愿景基础模型。本研究的数据集和代码可在https://github.com/tsinghua-fib-lab/UV-SAM获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08083v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Adversarial Masking Contrastive Learning for vein recognition**<br />
**Title_cn:** 用于静脉识别的对抗性掩蔽对比学习<br />
**Authors:** Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang<br />
**Abstract:** <details><summary>原文: </summary>Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results.</details>
**Abstract_cn:** <details><summary>译文: </summary>静脉识别因其较高的安全性和隐私性而受到越来越多的关注。最近，卷积神经网络（CNN）和 Transformers 等深度神经网络被引入静脉识别，并取得了最先进的性能。然而，尽管最近取得了进展，但由于训练图像样本稀缺，现有的手指静脉特征提取解决方案仍然不是最佳的。为了克服这个问题，在本文中，我们提出了一种对抗性掩蔽对比学习（AMCL）方法，该方法通过选择性地优化对比中的编码器，生成具有挑战性的样本，为下游手掌静脉识别任务训练更鲁棒的对比学习模型。学习模型和一组潜在变量。首先，生成大量掩模来训练强大的生成对抗网络（GAN）。经过训练的生成器将潜在变量从潜在变量空间转换为掩码空间。然后，我们将经过训练的生成器与对比学习模型相结合以获得我们的 AMCL，其中生成器生成具有挑战性的掩蔽图像以增加对比损失，并且对比学习模型基于更难的图像进行训练以学习更鲁棒的特征表示。训练后，对比学习模型中经过训练的编码器与分类层相结合来构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。三个数据库上的实验结果表明，我们的方法在提高静脉分类器的识别准确性方面优于现有的对比学习方法，并实现了最先进的识别结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.08079v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Achieve Fairness without Demographics for Dermatological Disease Diagnosis**<br />
**Title_cn:** 无需人口统计即可实现皮肤病诊断的公平性<br />
**Authors:** Ching-Hao Chiu, Yu-Jen Chen, Yawen Wu, Yiyu Shi, Tsung-Yi Ho<br />
**Abstract:** <details><summary>原文: </summary>In medical image diagnosis, fairness has become increasingly crucial. Without bias mitigation, deploying unfair AI would harm the interests of the underprivileged population and potentially tear society apart. Recent research addresses prediction biases in deep learning models concerning demographic groups (e.g., gender, age, and race) by utilizing demographic (sensitive attribute) information during training. However, many sensitive attributes naturally exist in dermatological disease images. If the trained model only targets fairness for a specific attribute, it remains unfair for other attributes. Moreover, training a model that can accommodate multiple sensitive attributes is impractical due to privacy concerns. To overcome this, we propose a method enabling fair predictions for sensitive attributes during the testing phase without using such information during training. Inspired by prior work highlighting the impact of feature entanglement on fairness, we enhance the model features by capturing the features related to the sensitive and target attributes and regularizing the feature entanglement between corresponding classes. This ensures that the model can only classify based on the features related to the target attribute without relying on features associated with sensitive attributes, thereby improving fairness and accuracy. Additionally, we use disease masks from the Segment Anything Model (SAM) to enhance the quality of the learned feature. Experimental results demonstrate that the proposed method can improve fairness in classification compared to state-of-the-art methods in two dermatological disease datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学图像诊断中，公平性变得越来越重要。如果不减少偏见，部署不公平的人工智能将损害弱势群体的利益，并可能导致社会分裂。最近的研究通过在训练期间利用人口统计（敏感属性）信息来解决深度学习模型中有关人口统计群体（例如性别、年龄和种族）的预测偏差。然而，皮肤病图像中自然存在许多敏感属性。如果训练后的模型仅针对特定属性的公平性，那么对于其他属性来说仍然不公平。此外，由于隐私问题，训练可以容纳多个敏感属性的模型是不切实际的。为了克服这个问题，我们提出了一种方法，可以在测试阶段对敏感属性进行公平预测，而无需在训练期间使用此类信息。受到先前强调特征纠缠对公平性影响的工作的启发，我们通过捕获与敏感属性和目标属性相关的特征并规范相应类之间的特征纠缠来增强模型特征。这保证了模型只能基于与目标属性相关的特征进行分类，而不依赖于与敏感属性相关的特征，从而提高公平性和准确性。此外，我们使用分段任意模型 (SAM) 中的疾病掩模来提高学习特征的质量。实验结果表明，与两个皮肤病数据集中最先进的方法相比，所提出的方法可以提高分类的公平性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08066v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Toward Clinically Trustworthy Deep Learning: Applying Conformal Prediction to Intracranial Hemorrhage Detection**<br />
**Title_cn:** 迈向临床值得信赖的深度学习：将适形预测应用于颅内出血检测<br />
**Authors:** Cooper Gamble, Shahriar Faghani, Bradley J. Erickson<br />
**Abstract:** <details><summary>原文: </summary>As deep learning (DL) continues to demonstrate its ability in radiological tasks, it is critical that we optimize clinical DL solutions to include safety. One of the principal concerns in the clinical adoption of DL tools is trust. This study aims to apply conformal prediction as a step toward trustworthiness for DL in radiology. This is a retrospective study of 491 non-contrast head CTs from the CQ500 dataset, in which three senior radiologists annotated slices containing intracranial hemorrhage (ICH). The dataset was split into definite and challenging subsets, where challenging images were defined to those in which there was disagreement among readers. A DL model was trained on 146 patients (10,815 slices) from the definite data (training dataset) to perform ICH localization and classification for five classes of ICH. To develop an uncertainty-aware DL model, 1,546 cases of the definite data (calibration dataset) was used for Mondrian conformal prediction (MCP). The uncertainty-aware DL model was tested on 8,401 definite and challenging cases to assess its ability to identify challenging cases. After the MCP procedure, the model achieved an F1 score of 0.920 for ICH classification on the test dataset. Additionally, it correctly identified 6,837 of the 6,856 total challenging cases as challenging (99.7% accuracy). It did not incorrectly label any definite cases as challenging. The uncertainty-aware ICH detector performs on par with state-of-the-art models. MCP's performance in detecting challenging cases demonstrates that it is useful in automated ICH detection and promising for trustworthiness in radiological DL.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着深度学习 (DL) 不断证明其在放射学任务中的能力，我们优化临床 DL 解决方案以确保安全性至关重要。深度学习工具临床采用的主要问题之一是信任。本研究旨在应用保形预测作为放射学中 DL 可信度的一步。这是对 CQ500 数据集中 491 个非造影头部 CT 的回顾性研究，其中三名高级放射科医生对包含颅内出血 (ICH) 的切片进行了注释。数据集被分为明确的和具有挑战性的子集，其中具有挑战性的图像被定义为读者之间存在分歧的图像。根据确定数据（训练数据集）对 146 名患者（10,815 个切片）训练 DL 模型，以对五类 ICH 进行 ICH 定位和分类。为了开发不确定性感知的深度学习模型，使用了 1,546 例确定数据（校准数据集）进行蒙德里安保形预测 (MCP)。不确定性感知深度学习模型在 8,401 个明确且具有挑战性的案例上进行了测试，以评估其识别具有挑战性的案例的能力。经过 MCP 程序后，该模型在测试数据集上的 ICH 分类 F1 得分为 0.920。此外，它还正确地将 6,856 个具有挑战性的案例中的 6,837 个识别为具有挑战性（准确度为 99.7%）。它并没有错误地将任何明确的案例标记为具有挑战性。不确定性感知 ICH 检测器的性能与最先进的模型相当。 MCP 在检测具有挑战性的病例方面的表现表明，它在自动 ICH 检测中非常有用，并有望提高放射学 DL 的可信度。</details>
**PDF:** <http://arxiv.org/pdf/2401.08058v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Robust Tiny Object Detection in Aerial Images amidst Label Noise**<br />
**Title_cn:** 标签噪声中航空图像中稳健的微小物体检测<br />
**Authors:** Haoran Zhu, Chang Xu, Wen Yang, Ruixiang Zhang, Yan Zhang, Gui-Song Xia<br />
**Abstract:** <details><summary>原文: </summary>Precise detection of tiny objects in remote sensing imagery remains a significant challenge due to their limited visual information and frequent occurrence within scenes. This challenge is further exacerbated by the practical burden and inherent errors associated with manual annotation: annotating tiny objects is laborious and prone to errors (i.e., label noise). Training detectors for such objects using noisy labels often leads to suboptimal performance, with networks tending to overfit on noisy labels. In this study, we address the intricate issue of tiny object detection under noisy label supervision. We systematically investigate the impact of various types of noise on network training, revealing the vulnerability of object detectors to class shifts and inaccurate bounding boxes for tiny objects. To mitigate these challenges, we propose a DeNoising Tiny Object Detector (DN-TOD), which incorporates a Class-aware Label Correction (CLC) scheme to address class shifts and a Trend-guided Learning Strategy (TLS) to handle bounding box noise. CLC mitigates inaccurate class supervision by identifying and filtering out class-shifted positive samples, while TLS reduces noisy box-induced erroneous supervision through sample reweighting and bounding box regeneration. Additionally, Our method can be seamlessly integrated into both one-stage and two-stage object detection pipelines. Comprehensive experiments conducted on synthetic (i.e., noisy AI-TOD-v2.0 and DOTA-v2.0) and real-world (i.e., AI-TOD) noisy datasets demonstrate the robustness of DN-TOD under various types of label noise. Notably, when applied to the strong baseline RFLA, DN-TOD exhibits a noteworthy performance improvement of 4.9 points under 40% mixed noise. Datasets, codes, and models will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于视觉信息有限且在场景中频繁出现，遥感图像中微小物体的精确检测仍然是一个重大挑战。与手动注释相关的实际负担和固有错误进一步加剧了这一挑战：注释微小对象非常费力且容易出错（即标签噪声）。使用噪声标签训练此类对象的检测器通常会导致性能不佳，网络往往会在噪声标签上过度拟合。在这项研究中，我们解决了噪声标签监督下微小物体检测的复杂问题。我们系统地研究了各种类型的噪声对网络训练的影响，揭示了对象检测器对类转移和微小对象不准确的边界框的脆弱性。为了缓解这些挑战，我们提出了一种去噪微型对象检测器（DN-TOD），它结合了类感知标签校正（CLC）方案来解决类转移问题，并采用趋势引导学习策略（TLS）来处理边界框噪声。 CLC 通过识别和过滤掉类转移的正样本来减轻不准确的类监督，而 TLS 通过样本重新加权和边界框重新生成来减少噪声框引起的错误监督。此外，我们的方法可以无缝集成到一级和两级目标检测管道中。对合成（即噪声 AI-TOD-v2.0 和 DOTA-v2.0）和现实世界（即 AI-TOD）噪声数据集进行的综合实验证明了 DN-TOD 在各种类型的标签噪声下的鲁棒性。值得注意的是，当应用于强基线 RFLA 时，DN-TOD 在 40% 混合噪声下表现出 4.9 个百分点的显着性能改进。数据集、代码和模型将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.08056v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **3D Lane Detection from Front or Surround-View using Joint-Modeling & Matching**<br />
**Title_cn:** 使用联合建模和匹配从前视或环视进行 3D 车道检测<br />
**Authors:** Haibin Zhou, Jun Chang, Tao Lu, Huabing Zhou<br />
**Abstract:** <details><summary>原文: </summary>3D lanes offer a more comprehensive understanding of the road surface geometry than 2D lanes, thereby providing crucial references for driving decisions and trajectory planning. While many efforts aim to improve prediction accuracy, we recognize that an efficient network can bring results closer to lane modeling. However, if the modeling data is imprecise, the results might not accurately capture the real-world scenario. Therefore, accurate lane modeling is essential to align prediction results closely with the environment. This study centers on efficient and accurate lane modeling, proposing a joint modeling approach that combines Bezier curves and interpolation methods. Furthermore, based on this lane modeling approach, we developed a Global2Local Lane Matching method with Bezier Control-Point and Key-Point, which serve as a comprehensive solution that leverages hierarchical features with two mathematical models to ensure a precise match. We also introduce a novel 3D Spatial Constructor, representing an exploration of 3D surround-view lane detection research. The framework is suitable for front-view or surround-view 3D lane detection. By directly outputting the key points of lanes in 3D space, it overcomes the limitations of anchor-based methods, enabling accurate prediction of closed-loop or U-shaped lanes and effective adaptation to complex road conditions. This innovative method establishes a new benchmark in front-view 3D lane detection on the Openlane dataset and achieves competitive performance in surround-view 2D lane detection on the Argoverse2 dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>与 2D 车道相比，3D 车道可以更全面地了解路面几何形状，从而为驾驶决策和轨迹规划提供重要参考。虽然许多努力旨在提高预测准确性，但我们认识到高效的网络可以使结果更接近车道建模。然而，如果建模数据不精确，结果可能无法准确捕捉现实世界的场景。因此，准确的车道建模对于使预测结果与环境紧密结合至关重要。本研究以高效、准确的车道建模为中心，提出了一种结合贝塞尔曲线和插值方法的联合建模方法。此外，基于这种车道建模方法，我们开发了一种具有贝塞尔曲线控制点和关键点的全局到局部车道匹配方法，作为一个综合解决方案，利用两个数学模型的层次特征来确保精确匹配。我们还引入了一种新颖的 3D 空间构造器，代表了对 3D 环视车道检测研究的探索。该框架适用于前视或环视 3D 车道检测。通过直接输出3D空间中的车道关键点，克服了基于anchor的方法的局限性，能够准确预测闭环或U形车道，有效适应复杂路况。这种创新方法在 Openlane 数据集上建立了前视 3D 车道检测的新基准，并在 Argoverse2 数据集上的环视 2D 车道检测中实现了具有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08036v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **BanglaNet: Bangla Handwritten Character Recognition using Ensembling of Convolutional Neural Network**<br />
**Title_cn:** BanglaNet：使用卷积神经网络集成进行孟加拉语手写字符识别<br />
**Authors:** Chandrika Saha, Md. Mostafijur Rahman<br />
**Abstract:** <details><summary>原文: </summary>Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb, BanglaLekha-Isolated, and Ekush datasets respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>手写字符识别因其丰富的应用而成为一项至关重要的任务。由于孟加拉语字符的草书性质以及存在多种书写方式的复合字符，孟加拉语手写字符的识别任务尤其具有挑战性。本文提出了一种基于多个卷积神经网络（CNN）集成的分类模型，即 BanglaNet，对孟加拉语基本字符、复合字符、数字和修饰语进行分类。基于最先进的 CNN 模型（如 Inception、ResNet 和 DenseNet）理念的三种不同模型已经使用增强和非增强输入进行了训练。最后，对所有这些模型进行平均或集成以获得最终模型。与最近一些基于 CNN 的研究相比，对三个基准孟加拉手写字符数据集（CMATERdb、BanglaLekha-Isolated 和 Ekush）进行的严格实验显示出显着的识别精度。 CMATERdb、BanglaLekha-Isolated 和 Ekush 数据集的 top-1 识别准确率分别为 98.40%、97.65% 和 97.32%，top-3 准确率分别为 99.79%、99.74% 和 99.56%。</details>
**PDF:** <http://arxiv.org/pdf/2401.08035v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Small Object Detection by DETR via Information Augmentation and Adaptive Feature Fusion**<br />
**Title_cn:** DETR 通过信息增强和自适应特征融合进行小物体检测<br />
**Authors:** Ji Huang, Hui Wang<br />
**Abstract:** <details><summary>原文: </summary>The main challenge for small object detection algorithms is to ensure accuracy while pursuing real-time performance. The RT-DETR model performs well in real-time object detection, but performs poorly in small object detection accuracy. In order to compensate for the shortcomings of the RT-DETR model in small object detection, two key improvements are proposed in this study. Firstly, The RT-DETR utilises a Transformer that receives input solely from the final layer of Backbone features. This means that the Transformer's input only receives semantic information from the highest level of abstraction in the Deep Network, and ignores detailed information such as edges, texture or color gradients that are critical to the location of small objects at lower levels of abstraction. Including only deep features can introduce additional background noise. This can have a negative impact on the accuracy of small object detection. To address this issue, we propose the fine-grained path augmentation method. This method helps to locate small objects more accurately by providing detailed information to the deep network. So, the input to the transformer contains both semantic and detailed information. Secondly, In RT-DETR, the decoder takes feature maps of different levels as input after concatenating them with equal weight. However, this operation is not effective in dealing with the complex relationship of multi-scale information captured by feature maps of different sizes. Therefore, we propose an adaptive feature fusion algorithm that assigns learnable parameters to each feature map from different levels. This allows the model to adaptively fuse feature maps from different levels and effectively integrate feature information from different scales. This enhances the model's ability to capture object features at different scales, thereby improving the accuracy of detecting small objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>小物体检测算法的主要挑战是在追求实时性的同时保证精度。 RT-DETR模型在实时目标检测方面表现良好，但在小目标检测精度方面表现较差。为了弥补RT-DETR模型在小目标检测方面的缺点，本研究提出了两个关键改进。首先，RT-DETR 使用仅从 Backbone 功能的最后一层接收输入的 Transformer。这意味着 Transformer 的输入仅接收来自深度网络中最高抽象级别的语义信息，而忽略对较低抽象级别的小物体的位置至关重要的边缘、纹理或颜色渐变等详细信息。仅包含深层特征会引入额外的背景噪声。这会对小物体检测的准确性产生负面影响。为了解决这个问题，我们提出了细粒度路径增强方法。该方法通过向深层网络提供详细信息，有助于更准确地定位小物体。因此，变压器的输入包含语义和详细信息。其次，在RT-DETR中，解码器将不同级别的特征图等权重连接后作为输入。然而，这种操作在处理不同尺寸的特征图捕获的多尺度信息的复杂关系时并不有效。因此，我们提出了一种自适应特征融合算法，将可学习的参数分配给不同级别的每个特征图。这使得模型能够自适应地融合不同级别的特征图，并有效地集成不同尺度的特征信息。这增强了模型捕获不同尺度物体特征的能力，从而提高检测小物体的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08017v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Key-point Guided Deformable Image Manipulation Using Diffusion Model**<br />
**Title_cn:** 使用扩散模型的关键点引导可变形图像处理<br />
**Authors:** Seok-Hwan Oh, Guil Jung, Myeong-Gee Kim, Sang-Yun Kim, Young-Min Kim, Hyeon-Jik Lee, Hyuk-Sool Kwon, Hyeon-Min Bae<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a Key-point-guided Diffusion probabilistic Model (KDM) that gains precise control over images by manipulating the object's key-point. We propose a two-stage generative model incorporating an optical flow map as an intermediate output. By doing so, a dense pixel-wise understanding of the semantic relation between the image and sparse key point is configured, leading to more realistic image generation. Additionally, the integration of optical flow helps regulate the inter-frame variance of sequential images, demonstrating an authentic sequential image generation. The KDM is evaluated with diverse key-point conditioned image synthesis tasks, including facial image generation, human pose synthesis, and echocardiography video prediction, demonstrating the KDM is proving consistency enhanced and photo-realistic images compared with state-of-the-art models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了一种关键点引导的扩散概率模型（KDM），它通过操纵对象的关键点来获得对图像的精确控制。我们提出了一种两阶段生成模型，其中包含光流图作为中间输出。通过这样做，可以配置对图像和稀疏关键点之间语义关系的密集像素级理解，从而生成更真实的图像。此外，光流的集成有助于调节顺序图像的帧间方差，展示真实的顺序图像生成。 KDM 通过各种关键点条件图像合成任务进行评估，包括面部图像生成、人体姿势合成和超声心动图视频预测，证明与最先进的模型相比，KDM 具有一致性增强和逼真的图像。</details>
**PDF:** <http://arxiv.org/pdf/2401.08178v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Inpainting Normal Maps for Lightstage data**<br />
**Title_cn:** 修复 Lightstage 数据的法线贴图<br />
**Authors:** Hancheng Zuo, Bernard Tiddeman<br />
**Abstract:** <details><summary>原文: </summary>This study introduces a novel method for inpainting normal maps using a generative adversarial network (GAN). Normal maps, often derived from a lightstage, are crucial in performance capture but can have obscured areas due to movement (e.g., by arms, hair, or props). Inpainting fills these missing areas with plausible data. Our approach extends previous general image inpainting techniques, employing a bow tie-like generator network and a discriminator network, with alternating training phases. The generator aims to synthesize images aligning with the ground truth and deceive the discriminator, which differentiates between real and processed images. Periodically, the discriminator undergoes retraining to enhance its ability to identify processed images. Importantly, our method adapts to the unique characteristics of normal map data, necessitating modifications to the loss function. We utilize a cosine loss instead of mean squared error loss for generator training. Limited training data availability, even with synthetic datasets, demands significant augmentation, considering the specific nature of the input data. This includes appropriate image flipping and in-plane rotations to accurately alter normal vectors. Throughout training, we monitored key metrics such as average loss, Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR) for the generator, along with average loss and accuracy for the discriminator. Our findings suggest that the proposed model effectively generates high-quality, realistic inpainted normal maps, suitable for performance capture applications. These results establish a foundation for future research, potentially involving more advanced networks and comparisons with inpainting of source images used to create the normal maps.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究介绍了一种使用生成对抗网络（GAN）修复法线贴图的新方法。法线贴图通常源自光场，对于表演捕捉至关重要，但可能会因运动（例如，手臂、头发或道具）而遮挡区域。修复用可信的数据填充了这些缺失的区域。我们的方法扩展了以前的通用图像修复技术，采用类似领结的生成器网络和鉴别器网络，并具有交替的训练阶段。生成器的目的是合成与真实情况一致的图像并欺骗鉴别器，鉴别器区分真实图像和处理后的图像。鉴别器定期接受再训练，以增强其识别处理图像的能力。重要的是，我们的方法适应了法线贴图数据的独特特征，需要对损失函数进行修改。我们使用余弦损失而不是均方误差损失来进行生成器训练。考虑到输入数据的具体性质，即使使用合成数据集，训练数据的可用性也有限，需要大量增强。这包括适当的图像翻转和平面内旋转以准确地改变法向矢量。在整个训练过程中，我们监控了生成器的平均损失、结构相似性指数测量（SSIM）和峰值信噪比（PSNR）等关键指标，以及鉴别器的平均损失和准确性。我们的研究结果表明，所提出的模型可以有效地生成高质量、逼真的修复法线贴图，适合表演捕捉应用。这些结果为未来的研究奠定了基础，可能涉及更先进的网络以及与用于创建法线贴图的源图像修复的比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.08099v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model**<br />
**Title_cn:** EmoTalker：通过扩散模型生成情感可编辑的说话面孔<br />
**Authors:** Bingyuan Zhang, Xulong Zhang, Ning Cheng, Jun Yu, Jing Xiao, Jianzong Wang<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the field of talking faces generation has attracted considerable attention, with certain methods adept at generating virtual faces that convincingly imitate human expressions. However, existing methods face challenges related to limited generalization, particularly when dealing with challenging identities. Furthermore, methods for editing expressions are often confined to a singular emotion, failing to adapt to intricate emotions. To overcome these challenges, this paper proposes EmoTalker, an emotionally editable portraits animation approach based on the diffusion model. EmoTalker modifies the denoising process to ensure preservation of the original portrait's identity during inference. To enhance emotion comprehension from text input, Emotion Intensity Block is introduced to analyze fine-grained emotions and strengths derived from prompts. Additionally, a crafted dataset is harnessed to enhance emotion comprehension within prompts. Experiments show the effectiveness of EmoTalker in generating high-quality, emotionally customizable facial expressions.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，说话面孔生成领域引起了相当大的关注，某些方法擅长生成令人信服地模仿人类表情的虚拟面孔。然而，现有方法面临着泛化能力有限的挑战，特别是在处理具有挑战性的身份时。此外，编辑表情的方法往往局限于单一的情感，无法适应复杂的情感。为了克服这些挑战，本文提出了 EmoTalker，一种基于扩散模型的情感可编辑肖像动画方法。 EmoTalker 修改了去噪过程​​，以确保在推理过程中保留原始肖像的身份。为了增强对文本输入的情感理解，引入了情感强度模块来分析来自提示的细粒度情感和强度。此外，还利用精心设计的数据集来增强提示中的情感理解。实验证明了 EmoTalker 在生成高质量、可根据情绪定制的面部表情方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08049v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities**<br />
**Title_cn:** 打造自动驾驶视觉基础模型：挑战、方法和机遇<br />
**Authors:** Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大量数据集上进行训练的大型基础模型的兴起正在彻底改变人工智能领域。 SAM、DALL-E2 和 GPT-4 等模型通过提取复杂的模式并在不同的任务中有效执行来展示其适应性，从而成为广泛的人工智能应用程序的有效构建块。自动驾驶是人工智能应用领域的一个充满活力的前沿领域，但由于缺乏专用的视觉基础模型（VFM），仍然面临着挑战。综合训练数据的缺乏、多传感器集成的需求以及多样化的特定任务架构对该领域的 VFM 发展构成了重大障碍。本文深入探讨了专门为自动驾驶打造 VFM 的关键挑战，同时也概述了未来的方向。通过对 250 多篇论文的系统分析，我们剖析了 VFM 开发的基本技术，包括数据准备、预训练策略和下游任务适应。此外，我们还探索了 NeRF、扩散模型、3D 高斯分布和世界模型等关键进展，为未来的研究提供了全面的路线图。为了增强研究人员的能力，我们构建并维护了 https://github.com/zhanghm1995/Forge_VFM4AD，这是一个开放访问存储库，不断更新自动驾驶 VFM 的最新进展。</details>
**PDF:** <http://arxiv.org/pdf/2401.08045v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception**<br />
**Title_cn:** AesBench：图像美学感知多模态大语言模型的专家基准<br />
**Authors:** Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, Weisi Lin<br />
**Abstract:** <details><summary>原文: </summary>With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development. However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications. An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity. To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets. (1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts. (2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI). Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans. We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs. Source data will be available at https://github.com/yipoh/AesBench.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大家的共同努力下，多模态大语言模型（MLLM）正在蓬勃发展。然而，它们在图像美学感知上的表现仍然不确定，这在现实世界的应用中是非常需要的。一个明显的障碍在于缺乏具体的基准来评估 MLLM 对审美感知的有效性。这种盲目的摸索可能会阻碍更先进的具有审美感知能力的MLLM的进一步发展。为了解决这一困境，我们提出了 AesBench，这是一个专家基准，旨在通过跨双面的精心设计来综合评估 MLLM 的美感能力。 （1）我们构建了专家标记的美学感知数据库（EAPD），该数据库具有多样化的图像内容和由专业美学专家提供的高质量注释。 （2）我们提出了一套综合标准，从感知（AesP）、移情（AesE）、评估（AesA）和解释（AesI）四个角度衡量MLLM的审美感知能力。大量的实验结果表明，目前的MLLM仅具备初级的审美感知能力，与人类相比仍存在显着差距。我们希望这项工作能够激发社区对 MLLM 的美学潜力进行更深入的探索。源数据可在 https://github.com/yipoh/AesBench 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08276v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and Usage in Digital Communication**<br />
**Title_cn:** 人类与 LMM：探索数字通信中表情符号解释和使用的差异<br />
**Authors:** Hanjia Lyu, Weihong Qi, Zhongyu Wei, Jiebo Luo<br />
**Abstract:** <details><summary>原文: </summary>Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications. Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions. Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction. This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis. The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用大型多模态模型（LMM）来模拟处理多模态信息时的人类行为，特别是在社交媒体背景下，由于其广泛的潜力和深远的影响而引起了人们的极大兴趣。表情符号作为数字通信最独特的方面之一，对于丰富并常常澄清情感和语气维度至关重要。然而，在理解这些先进模型（例如 GPT-4V）如何在微妙的在线交互环境中解释和使用表情符号方面存在显着差距。本研究旨在通过检查 GPT-4V 在复制类人表情符号使用方面的行为来弥补这一差距。研究结果揭示了人类和 GPT-4V 行为之间存在明显的差异，这可能是由于人类解释的主观性和 GPT-4V 以英语为中心的训练的局限性，这表明文化偏见和对非英语文化的代表性不足。</details>
**PDF:** <http://arxiv.org/pdf/2401.08212v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **The Devil is in the Details: Boosting Guided Depth Super-Resolution via Rethinking Cross-Modal Alignment and Aggregation**<br />
**Title_cn:** 细节决定成败：通过重新思考跨模式对齐和聚合来提高引导深度超分辨率<br />
**Authors:** Xinni Jiang, Zengsheng Kuang, Chunle Guo, Ruixun Zhang, Lei Cai, Xiao Fan, Chongyi Li<br />
**Abstract:** <details><summary>原文: </summary>Guided depth super-resolution (GDSR) involves restoring missing depth details using the high-resolution RGB image of the same scene. Previous approaches have struggled with the heterogeneity and complementarity of the multi-modal inputs, and neglected the issues of modal misalignment, geometrical misalignment, and feature selection. In this study, we rethink some essential components in GDSR networks and propose a simple yet effective Dynamic Dual Alignment and Aggregation network (D2A2). D2A2 mainly consists of 1) a dynamic dual alignment module that adapts to alleviate the modal misalignment via a learnable domain alignment block and geometrically align cross-modal features by learning the offset; and 2) a mask-to-pixel feature aggregate module that uses the gated mechanism and pixel attention to filter out irrelevant texture noise from RGB features and combine the useful features with depth features. By combining the strengths of RGB and depth features while minimizing disturbance introduced by the RGB image, our method with simple reuse and redesign of basic components achieves state-of-the-art performance on multiple benchmark datasets. The code is available at https://github.com/JiangXinni/D2A2.</details>
**Abstract_cn:** <details><summary>译文: </summary>引导深度超分辨率 (GDSR) 涉及使用同一场景的高分辨率 RGB 图像来恢复丢失的深度细节。以前的方法一直在努力解决多模态输入的异质性和互补性，并忽略了模态错位、几何错位和特征选择的问题。在这项研究中，我们重新思考了 GDSR 网络中的一些重要组成部分，并提出了一种简单而有效的动态双重对齐和聚合网络（D2A2）。 D2A2主要由1）一个动态双对齐模块组成，该模块通过可学习的域对齐块来缓解模态未对齐，并通过学习偏移量来几何对齐跨模态特征； 2）掩模到像素的特征聚合模块，使用门控机制和像素注意力从RGB特征中过滤掉不相关的纹理噪声，并将有用的特征与深度特征结合起来。通过结合 RGB 和深度特征的优势，同时最大限度地减少 RGB 图像引入的干扰，我们的方法通过简单地重用和重新设计基本组​​件，在多个基准数据集上实现了最先进的性能。代码可在 https://github.com/JiangXinni/D2A2 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08123v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary**<br />
**Title_cn:** 超越本地窗口的限制：具有自适应令牌字典的高级超分辨率变压器<br />
**Authors:** Leheng Zhang, Yawei Li, Xingyu Zhou, Xiaorui Zhao, Shuhang Gu<br />
**Abstract:** <details><summary>原文: </summary>Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像超分辨率是一个经典的计算机视觉问题，涉及从低分辨率 (LR) 图像估计高分辨率 (HR) 图像。尽管深度神经网络（DNN），特别是用于超分辨率的 Transformer，近年来取得了显着的进步，但挑战仍然存在，特别是在基于窗口的自注意力引起的有限感受野方面。为了解决这些问题，我们向 SR Transformer 引入一组辅助的 Adapeive Token Dictionary，并建立了 ATD-SR 方法。引入的标记字典可以从训练数据中学习先验信息，并通过自适应细化步骤将学习到的先验信息适应特定的测试图像。细化策略不仅可以为所有输入标记提供全局信息，还可以将图像标记分组。基于类别划分，我们进一步提出了一种基于类别的自注意力机制，旨在利用遥远但相似的标记来增强输入特征。实验结果表明，我们的方法在各种单图像超分辨率基准上实现了最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08209v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining**<br />
**Title_cn:** DPAFNet：用于单图像去雨的双路径注意力融合网络<br />
**Authors:** Bingcai Wei<br />
**Abstract:** <details><summary>原文: </summary>Rainy weather will have a significant impact on the regular operation of the imaging system. Based on this premise, image rain removal has always been a popular branch of low-level visual tasks, especially methods using deep neural networks. However, most neural networks are but-branched, such as only using convolutional neural networks or Transformers, which is unfavourable for the multidimensional fusion of image features. In order to solve this problem, this paper proposes a dual-branch attention fusion network. Firstly, a two-branch network structure is proposed. Secondly, an attention fusion module is proposed to selectively fuse the features extracted by the two branches rather than simply adding them. Finally, complete ablation experiments and sufficient comparison experiments prove the rationality and effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>阴雨天气会对成像系统的正常运行产生较大影响。基于这个前提，图像去雨一直是低级视觉任务的一个流行分支，特别是使用深度神经网络的方法。然而，大多数神经网络都是but-branched，例如仅使用卷积神经网络或Transformers，这不利于图像特征的多维融合。为了解决这个问题，本文提出了双分支注意力融合网络。首先，提出了一种双分支网络结构。其次，提出了一个注意力融合模块来选择性地融合两个分支提取的特征，而不是简单地将它们相加。最后，完整的消融实验和充分的对比实验证明了该方法的合理性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08185v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep Linear Array Pushbroom Image Restoration: A Degradation Pipeline and Jitter-Aware Restoration Network**<br />
**Title_cn:** 深度线性阵列推扫式图像恢复：退化管道和抖动感知恢复网络<br />
**Authors:** Zida Chen, Ziran Zhang, Haoying Li, Menghao Li, Yueting Chen, Qi Li, Huajun Feng, Zhihai Xu, Shiqi Chen<br />
**Abstract:** <details><summary>原文: </summary>Linear Array Pushbroom (LAP) imaging technology is widely used in the realm of remote sensing. However, images acquired through LAP always suffer from distortion and blur because of camera jitter. Traditional methods for restoring LAP images, such as algorithms estimating the point spread function (PSF), exhibit limited performance. To tackle this issue, we propose a Jitter-Aware Restoration Network (JARNet), to remove the distortion and blur in two stages. In the first stage, we formulate an Optical Flow Correction (OFC) block to refine the optical flow of the degraded LAP images, resulting in pre-corrected images where most of the distortions are alleviated. In the second stage, for further enhancement of the pre-corrected images, we integrate two jitter-aware techniques within the Spatial and Frequency Residual (SFRes) block: 1) introducing Coordinate Attention (CoA) to the SFRes block in order to capture the jitter state in orthogonal direction; 2) manipulating image features in both spatial and frequency domains to leverage local and global priors. Additionally, we develop a data synthesis pipeline, which applies Continue Dynamic Shooting Model (CDSM) to simulate realistic degradation in LAP images. Both the proposed JARNet and LAP image synthesis pipeline establish a foundation for addressing this intricate challenge. Extensive experiments demonstrate that the proposed two-stage method outperforms state-of-the-art image restoration models. Code is available at https://github.com/JHW2000/JARNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>线性阵列推扫式（LAP）成像技术广泛应用于遥感领域。然而，由于相机抖动，通过 LAP 获取的图像总是会出现失真和模糊。用于恢复 LAP 图像的传统方法（例如估计点扩散函数 (PSF) 的算法）表现出有限的性能。为了解决这个问题，我们提出了抖动感知恢复网络（JARNet），以分两个阶段消除失真和模糊。在第一阶段，我们制定光流校正（OFC）块来细化退化的 LAP 图像的光流，从而产生预校正图像，其中大部分失真得到缓解。在第二阶段，为了进一步增强预校正图像，我们在空间和频率残差（SFRes）块中集成了两种抖动感知技术：1）向 SFRes 块引入坐标注意（CoA）以捕获正交方向的抖动状态； 2）在空间和频率域中操纵图像特征以利用局部和全局先验。此外，我们开发了一个数据合成管道，它应用连续动态拍摄模型（CDSM）来模拟 LAP 图像中的真实退化。所提出的 JARNet 和 LAP 图像合成管道都为解决这一复杂的挑战奠定了基础。大量实验表明，所提出的两阶段方法优于最先进的图像恢复模型。代码可从 https://github.com/JHW2000/JARNet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.08171v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Spatial-Semantic Collaborative Cropping for User Generated Content**<br />
**Title_cn:** 用户生成内容的空间语义协作裁剪<br />
**Authors:** Yukun Su, Yiwen Cao, Jingliang Deng, Fengyun Rao, Qingyao Wu<br />
**Abstract:** <details><summary>原文: </summary>A large amount of User Generated Content (UGC) is uploaded to the Internet daily and displayed to people world-widely through the client side (e.g., mobile and PC). This requires the cropping algorithms to produce the aesthetic thumbnail within a specific aspect ratio on different devices. However, existing image cropping works mainly focus on landmark or landscape images, which fail to model the relations among the multi-objects with the complex background in UGC. Besides, previous methods merely consider the aesthetics of the cropped images while ignoring the content integrity, which is crucial for UGC cropping. In this paper, we propose a Spatial-Semantic Collaborative cropping network (S2CNet) for arbitrary user generated content accompanied by a new cropping benchmark. Specifically, we first mine the visual genes of the potential objects. Then, the suggested adaptive attention graph recasts this task as a procedure of information association over visual nodes. The underlying spatial and semantic relations are ultimately centralized to the crop candidate through differentiable message passing, which helps our network efficiently to preserve both the aesthetics and the content integrity. Extensive experiments on the proposed UGCrop5K and other public datasets demonstrate the superiority of our approach over state-of-the-art counterparts. Our project is available at https://github.com/suyukun666/S2CNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>每天都有大量的用户生成内容（UGC）上传到互联网，并通过客户端（例如移动和PC）向全世界的人们展示。这需要裁剪算法在不同设备上以特定的宽高比生成美观的缩略图。然而，现有的图像裁剪工作主要集中在地标或风景图像，未能对UGC中复杂背景的多对象之间的关系进行建模。此外，以前的方法仅考虑裁剪图像的美观性，而忽略了内容完整性，而内容完整性对于 UGC 裁剪至关重要。在本文中，我们提出了一种空间语义协作裁剪网络（S2CNet），用于任意用户生成的内容，并附带新的裁剪基准。具体来说，我们首先挖掘潜在物体的视觉基因。然后，建议的自适应注意力图将该任务重新定义为视觉节点上的信息关联过程。底层的空间和语义关系最终通过可微分的消息传递集中到候选作物上，这有助于我们的网络有效地保持美观和内容完整性。对所提出的 UGCrop5K 和其他公共数据集进行的广泛实验证明了我们的方法相对于最先进的同行的优越性。我们的项目位于 https://github.com/suyukun666/S2CNet。</details>
**PDF:** <http://arxiv.org/pdf/2401.08086v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process**<br />
**Title_cn:** ProvNeRF：NeRF 中每点来源的建模作为随机过程<br />
**Authors:** Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas<br />
**Abstract:** <details><summary>原文: </summary>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: "from where has each point been seen?" -- which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 在各种应用中越来越受欢迎。然而，它们在稀疏视图设置中面临挑战，缺乏体渲染的足够约束。从稀疏且不受约束的相机中重建和理解 3D 场景是具有多种应用的经典计算机视觉中长期存在的问题。虽然最近的工作在稀疏、无约束的视图场景中探索了 NeRF，但他们的重点主要集中在增强重建和新颖的视图合成上。我们的方法通过提出以下问题来采取更广泛的视角：“从哪里看到每个点？” ——这决定了我们理解和重建它的程度。换句话说，我们的目标是在稀疏、无约束的视图下确定每个 3D 点及其相关信息的起源或出处。我们引入了 ProvNeRF，该模型通过合并每个点的出处、对每个点可能的源位置进行建模来丰富传统的 NeRF 表示。我们通过扩展随机过程的隐式最大似然估计（IMLE）来实现这一点。值得注意的是，我们的方法与任何预先训练的 NeRF 模型和相关的训练相机姿势兼容。我们证明，与最先进的方法相比，每点起源建模具有多种优势，包括不确定性估计、基于标准的视图选择和改进的新颖视图合成。</details>
**PDF:** <http://arxiv.org/pdf/2401.08140v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy**<br />
**Title_cn:** 免清洁参考图像超分辨率：在电子显微镜中的应用<br />
**Authors:** Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, Jussi Tohka<br />
**Abstract:** <details><summary>原文: </summary>The inability to acquire clean high-resolution (HR) electron microscopy (EM) images over a large brain tissue volume hampers many neuroscience studies. To address this challenge, we propose a deep-learning-based image super-resolution (SR) approach to computationally reconstruct clean HR 3D-EM with a large field of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are I) Investigating training with no-clean references for $\ell_2$ and $\ell_1$ loss functions; II) Introducing a novel network architecture, named EMSR, for enhancing the resolution of LR EM images while reducing inherent noise; and, III) Comparing different training strategies including using acquired LR and HR image pairs, i.e., real pairs with no-clean references contaminated with real corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR and denoised HR pairs. Experiments with nine brain datasets showed that training with real pairs can produce high-quality super-resolved results, demonstrating the feasibility of training with non-clean references for both loss functions. Additionally, comparable results were observed, both visually and numerically, when employing denoised and noisy references for training. Moreover, utilizing the network trained with synthetically generated LR images from HR counterparts proved effective in yielding satisfactory SR results, even in certain cases, outperforming training with real pairs. The proposed SR network was compared quantitatively and qualitatively with several established SR techniques, showcasing either the superiority or competitiveness of the proposed method in mitigating noise while recovering fine details.</details>
**Abstract_cn:** <details><summary>译文: </summary>无法在较大的脑组织体积上获取清晰的高分辨率 (HR) 电子显微镜 (EM) 图像阻碍了许多神经科学研究。为了应对这一挑战，我们提出了一种基于深度学习的图像超分辨率 (SR) 方法，可以从嘈杂的低分辨率 (LR) 采集中计算重建具有大视场 (FoV) 的干净 HR 3D-EM。我们的贡献是 I) 研究 $\ell_2$ 和 $\ell_1$ 损失函数的 no-clean 参考的训练； II) 引入一种新颖的网络架构，称为 EMSR，用于增强 LR EM 图像的分辨率，同时减少固有噪声； III) 比较不同的训练策略，包括使用获取的 LR 和 HR 图像对，即具有受真实损坏污染的免洗参考的真实对、合成 LR 和获取的 HR 对，以及获取的 LR 和去噪 HR 对。对九个大脑数据集的实验表明，使用真实配对进行训练可以产生高质量的超分辨率结果，证明了使用非干净参考对两种损失函数进行训练的可行性。此外，当使用去噪和噪声参考进行训练时，在视觉和数字上都观察到了可比较的结果。此外，事实证明，利用由 HR 对应对象合成生成的 LR 图像训练的网络可以有效地产生令人满意的 SR 结果，即使在某些情况下，其性能也优于使用真实图像对进行的训练。所提出的 SR 网络与几种已建立的 SR 技术进行了定量和定性比较，展示了所提出的方法在减轻噪声同时恢复精细细节方面的优越性或竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2401.08115v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label Generation**<br />
**Title_cn:** 通过基于克里金法的伪标签生成增强地面 PM2.5 预测<br />
**Authors:** Lei Duan, Ziyang Jiang, David Carlson<br />
**Abstract:** <details><summary>原文: </summary>Fusing abundant satellite data with sparse ground measurements constitutes a major challenge in climate modeling. To address this, we propose a strategy to augment the training dataset by introducing unlabeled satellite images paired with pseudo-labels generated through a spatial interpolation technique known as ordinary kriging, thereby making full use of the available satellite data resources. We show that the proposed data augmentation strategy helps enhance the performance of the state-of-the-art convolutional neural network-random forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy improvement in spatial correlation and a reduction in prediction error.</details>
**Abstract_cn:** <details><summary>译文: </summary>将丰富的卫星数据与稀疏的地面测量数据融合是气候建模的主要挑战。为了解决这个问题，我们提出了一种策略，通过引入未标记的卫星图像与通过称为普通克里金法的空间插值技术生成的伪标签配对来增强训练数据集，从而充分利用可用的卫星数据资源。我们表明，所提出的数据增强策略有助于以合理的量提高最先进的卷积神经网络随机森林（CNN-RF）模型的性能，从而显着改善空间相关性并减少预测错误。</details>
**PDF:** <http://arxiv.org/pdf/2401.08061v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions**<br />
**Title_cn:** 具有挑战性的条件下事件相机的跨模态半密集 6 自由度跟踪<br />
**Authors:** Yi-Fan Zuo, Wanting Xu, Xia Wang, Yifu Wang, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于视觉的本地化是一种经济高效的解决方案，因此对许多智能移动平台来说都很有吸引力。然而，其准确性，尤其是鲁棒性仍然受到低光照条件、光照变化和剧烈运动的影响。基于事件的相机是受生物启发的视觉传感器，在 HDR 条件下表现良好并具有高时间分辨率，因此在这种具有挑战性的场景中提供了有趣的替代方案。虽然纯粹基于事件的解决方案目前尚未产生令人满意的映射结果，但目前的工作证明了如果允许使用替代传感器进行映射，则纯粹基于事件的跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关的场景是由深度相机支持的跟踪或基于地图的定位以及由常规的基于图像的视觉 SLAM 或运动结构系统事先创建的半密集地图给出的。传统的基于边缘的 3D-2D 对齐通过新颖的极性感知配准进行了扩展，该配准利用从事件流获得的带符号时间表面图 (STSM)。我们还引入了一种新颖的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其针对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.08043v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Spatial Channel State Information Prediction with Generative AI: Towards Holographic Communication and Digital Radio Twin**<br />
**Title_cn:** 利用生成式人工智能进行空间信道状态信息预测：迈向全息通信和数字无线电孪生<br />
**Authors:** Lihao Zhang, Haijian Sun, Yong Zeng, Rose Qingyang Hu<br />
**Abstract:** <details><summary>原文: </summary>As 5G technology becomes increasingly established, the anticipation for 6G is growing, which promises to deliver faster and more reliable wireless connections via cutting-edge radio technologies. However, efficient management method of the large-scale antenna arrays deployed by those radio technologies is crucial. Traditional management methods are mainly reactive, usually based on feedback from users to adapt to the dynamic wireless channel. However, a more promising approach lies in the prediction of spatial channel state information (spatial-CSI), which is an all-inclusive channel characterization and consists of all the feasible line-of-sight (LoS) and non-line-of-sight (NLoS) paths between the transmitter (Tx) and receiver (Rx), with the three-dimension (3D) trajectory, attenuation, phase shift, delay, and polarization of each path. Advances in hardware and neural networks make it possible to predict such spatial-CSI using precise environmental information, and further look into the possibility of holographic communication, which implies complete control over every aspect of the radio waves emitted. Based on the integration of holographic communication and digital twin, we proposed a new framework, digital radio twin, which takes advantages from both the digital world and deterministic control over radio waves, supporting a wide range of high-level applications. As a preliminary attempt towards this visionary direction, in this paper, we explore the use of generative artificial intelligence (AI) to pinpoint the valid paths in a given environment, demonstrating promising results, and highlighting the potential of this approach in driving forward the evolution of 6G wireless communication technologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着 5G 技术的日益成熟，人们对 6G 的期望也越来越高，它有望通过尖端的无线电技术提供更快、更可靠的无线连接。然而，对这些无线电技术部署的大规模天线阵列的有效管理方法至关重要。传统的管理方法主要是反应式的，通常根据用户的反馈来适应动态的无线信道。然而，更有前途的方法在于空间信道状态信息（spatial-CSI）的预测，它是一种包罗万象的信道表征，由所有可行的视距（LoS）和非视距组成。发射器 (Tx) 和接收器 (Rx) 之间的视线 (NLoS) 路径，以及每条路径的三维 (3D) 轨迹、衰减、相移、延迟和偏振。硬件和神经网络的进步使得使用精确的环境信息来预测此类空间CSI成为可能，并进一步研究全息通信的可能性，这意味着对所发射的无线电波的各个方面进行完全控制。基于全息通信和数字孪生的融合，我们提出了一种新的框架——数字无线电孪生，它结合了数字世界和无线电波确定性控制的优势，支持广泛的高级应用。作为朝着这一愿景方向的初步尝试，在本文中，我们探索使用生成人工智能（AI）来查明给定环境中的有效路径，展示了有希望的结果，并强调了这种方法在推动进化方面的潜力6G无线通信技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.08023v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Multitask Learning in Minimally Invasive Surgical Vision: A Review**<br />
**Title_cn:** 微创手术视觉中的多任务学习：综述<br />
**Authors:** Oluwatosin Alabi, Tom Vercauteren, Miaojing Shi<br />
**Abstract:** <details><summary>原文: </summary>Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data. This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments.</details>
**Abstract_cn:** <details><summary>译文: </summary>微创手术 (MIS) 彻底改变了许多手术方式，缩短了恢复时间并降低了患者受伤的风险。然而，MIS 给手术团队带来了额外的复杂性和负担。数据驱动的手术视觉算法被认为是开发未来具有更高自主性的 MIS 系统的关键构建模块。机器学习和计算机视觉的最新进展已成功应用于分析从 MIS 获得的视频，有望缓解 MIS 视频中的挑战。手术场景和动作理解包含多个相关任务，当单独解决这些任务时，这些任务可能会占用大量内存、效率低下，并且无法捕获任务关系。多任务学习 (MTL) 是一种利用多个相关任务的信息来提高性能并帮助泛化的学习范式，非常适合对 MIS 数据进行细粒度和高层次的理解。这篇评论概述了当前最先进的 MTL 系统，这些系统利用了从 MIS 获得的视频。除了列出已发布的方法之外，我们还讨论了这些 MTL 系统的优点和局限性。此外，本手稿还对 MIS 中 MTL 的各个应用领域（包括大型模型）的文献进行了分析，突出了显着的趋势、新的研究方向和发展。</details>
**PDF:** <http://arxiv.org/pdf/2401.08256v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation**<br />
**Title_cn:** 取消混合测试时间归一化统计：对抗标签时间相关性<br />
**Authors:** Devavrat Tomar, Guillaume Vray, Jean-Philippe Thiran, Behzad Bozorgtabar<br />
**Abstract:** <details><summary>原文: </summary>In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions. This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed "Un-Mixing Test-Time Normalization Statistics" (UnMix-TNS). UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment. The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch. Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts. UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples. Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在测试时间适应方法越来越依赖于批量归一化 (BN) 参数的细微操作的时代，一个关键假设经常被忽视：相对于未知标签的独立同分布 (i.i.d.) 测试批次。这种假设最终会导致 BN 统计数据的估计出现偏差，并危及非独立同分布下的系统稳定性。状况。这篇论文开创了对独立同分布的背离。范式引入了一种名为“非混合测试时间标准化统计”（UnMix-TNS）的突破性策略。 UnMix-TNS 通过将实例与多个未混合的统计组件混合来重新校准用于标准化批次中每个实例的实例统计数据，从而本质上模拟独立同分布。环境。关键在于我们创新的在线分解程序，该程序通过利用传入测试批次中最接近的实例来持续完善这些统计组件。 UnMix-TNS 的设计非常通用，它与一系列最先进的测试时间适应方法和配备 BN 层的预训练架构无缝集成。实证评估证实了 UnMix-TNS 在从单一域转换到连续域转换和混合域转换的各种场景下的鲁棒性。 UnMix-TNS 在处理具有时间相关性的测试数据流（包括那些具有损坏的现实世界非独立同分布的数据流）时脱颖而出。即使在最小批量和单个样品的情况下也能维持其功效。我们的结果为测试时间适应设定了新标准，证明了多个基准测试中稳定性和性能的显着改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.08328v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **The Faiss library**<br />
**Title_cn:** 费斯图书馆<br />
**Authors:** Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou<br />
**Abstract:** <details><summary>原文: </summary>Vector databases manage large collections of embedding vectors. As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.</details>
**Abstract_cn:** <details><summary>译文: </summary>矢量数据库管理大量嵌入矢量。随着人工智能应用程序的快速增长，需要存储和索引的嵌入数量也在快速增长。 Faiss 库致力于矢量相似性搜索，这是矢量数据库的核心功能。 Faiss 是一个包含索引方法和相关原语的工具包，用于搜索、聚类、压缩和转换向量。本文首先描述了向量搜索的权衡空间，然后描述了Faiss在结构、优化方法和接口方面的设计原则。我们对库的主要功能进行了基准测试，并讨论了一些选定的应用程序，以强调其广泛的适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08281v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Siamese Content-based Search Engine for a More Transparent Skin and Breast Cancer Diagnosis through Histological Imaging**<br />
**Title_cn:** 基于连体内容的搜索引擎，通过组织学成像实现更透明的皮肤和乳腺癌诊断<br />
**Authors:** Zahra Tabatabaei, Adrián Colomer, JAvier Oliver Moll, Valery Naranjo<br />
**Abstract:** <details><summary>原文: </summary>Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making. Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features. In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor. The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs. The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy. Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists. To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images. The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels. So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机辅助诊断 (CAD) 利用基于深度学习 (DL) 的工具开发了数字病理学，以协助病理学家做出决策。基于内容的组织病理学图像检索（CBHIR）是一种新颖的工具，可根据组织病理学特征的相似性寻找高度相关的斑块。在这项工作中，我们针对乳腺癌（Breast-twins）和皮肤癌（Skin-twins）数据集提出了两种 CBHIR 方法，以实现稳健且准确的补丁级检索，并将定制的连体网络集成为特征提取器。所提出的 Siamese 网络能够通过关注输入对的相似组织病理学特征来概括未见过的图像。所提出的 CBHIR 方法在乳房（公共）和皮肤（私人）数据集上进行评估，具有最高 K 精度。找到 K 的最佳量具有挑战性，而且随着 K 的增加，查询和返回图像之间的差异也会增加，这可能会误导病理学家。据作者所知，本文通过评估最先检索到的图像，首次在组织病理学图像上解决了这个问题。 Breast-twins 模型首先达到了 F1score 的 70%，这在更高的 K 值（例如 5 和 400）下超过了其他最先进的方法。Skin-twins 超越了最近提出的卷积自动编码器(CAE) 提高了 67%，提高了精度。此外，皮肤双胞胎模型解决了不确定恶性潜能的 Spitzoid 肿瘤 (STUMP) 的挑战，以帮助病理学家检索前 K 个图像及其相应的标签。因此，这种方法可以为病理学家提供一种在透明度、可信度或可靠性等特征方面更易于解释的 CAD 工具。</details>
**PDF:** <http://arxiv.org/pdf/2401.08272v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learned Image Compression with ROI-Weighted Distortion and Bit Allocation**<br />
**Title_cn:** 通过 ROI 加权失真和位分配学习图像压缩<br />
**Authors:** Wei Jiang, Yongqi Zhai, Hangyu Li, Ronggang Wang<br />
**Abstract:** <details><summary>原文: </summary>This one page paper describes our method for the track of image compression. To achieve better perceptual quality, we use the adversarial loss to generate realistic textures, use region of interest (ROI) mask to guide the bit allocation for different regions. Our Team name is TLIC.</details>
**Abstract_cn:** <details><summary>译文: </summary>这篇一页纸描述了我们跟踪图像压缩的方法。为了获得更好的感知质量，我们使用对抗性损失来生成逼真的纹理，使用感兴趣区域（ROI）掩模来指导不同区域的位分配。我们的团队名称是 TLIC。</details>
**PDF:** <http://arxiv.org/pdf/2401.08154v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning**<br />
**Title_cn:** E2HQV：通过理论启发的模型辅助深度学习从事件摄像机生成高质量视频<br />
**Authors:** Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Tongliang Liu<br />
**Abstract:** <details><summary>原文: </summary>The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose \textbf{E2HQV}, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40\% for some evaluation metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>仿生事件相机或动态视觉传感器能够以高时间分辨率和高动态范围异步捕获每个像素的亮度变化（称为事件流）。然而，非结构性的时空事件流使得为人类视觉提供具有丰富语义信息的直观可视化变得具有挑战性。它需要事件到视频 (E2V) 解决方案，该解决方案将事件流作为输入并生成高质量的视频帧以实现直观的可视化。然而，当前的解决方案主要是数据驱动的，没有考虑与事件流和视频帧相关的底层统计数据的先验知识。它高度依赖深度神经网络的非线性和泛化能力，因此，当场景复杂时，很难重建详细的纹理。在这项工作中，我们提出了 \textbf{E2HQV}，这是一种新颖的 E2V 范例，旨在从事件中生成高质量的视频帧。这种方法利用了模型辅助的深度学习框架，以理论启发的 E2V 模型为基础，该模型是从事件相机的基本成像原理中精心推导出来的。为了解决 E2HQV 循环组件中的状态重置问题，我们还设计了时间移位嵌入模块以进一步提高视频帧的质量。对现实世界事件摄像机数据集的综合评估验证了我们的方法，E2HQV 的性能明显优于最先进的方法，例如，在某些评估指标上超过第二好的方法 40% 以上。</details>
**PDF:** <http://arxiv.org/pdf/2401.08117v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Deep Shape-Texture Statistics for Completely Blind Image Quality Evaluation**<br />
**Title_cn:** 用于完全盲图像质量评估的深层形状纹理统计<br />
**Authors:** Yixuan Li, Peilin Chen, Hanwei Zhu, Keyan Ding, Leida Li, Shiqi Wang<br />
**Abstract:** <details><summary>原文: </summary>Opinion-Unaware Blind Image Quality Assessment (OU-BIQA) models aim to predict image quality without training on reference images and subjective quality scores. Thereinto, image statistical comparison is a classic paradigm, while the performance is limited by the representation ability of visual descriptors. Deep features as visual descriptors have advanced IQA in recent research, but they are discovered to be highly texture-biased and lack of shape-bias. On this basis, we find out that image shape and texture cues respond differently towards distortions, and the absence of either one results in an incomplete image representation. Therefore, to formulate a well-round statistical description for images, we utilize the shapebiased and texture-biased deep features produced by Deep Neural Networks (DNNs) simultaneously. More specifically, we design a Shape-Texture Adaptive Fusion (STAF) module to merge shape and texture information, based on which we formulate qualityrelevant image statistics. The perceptual quality is quantified by the variant Mahalanobis Distance between the inner and outer Shape-Texture Statistics (DSTS), wherein the inner and outer statistics respectively describe the quality fingerprints of the distorted image and natural images. The proposed DSTS delicately utilizes shape-texture statistical relations between different data scales in the deep domain, and achieves state-of-the-art (SOTA) quality prediction performance on images with artificial and authentic distortions.</details>
**Abstract_cn:** <details><summary>译文: </summary>Opinion-Unaware 盲图像质量评估 (OU-BIQA) 模型旨在预测图像质量，而无需对参考图像和主观质量分数进行训练。其中，图像统计比较是经典范例，但其性能受到视觉描述符的表示能力的限制。在最近的研究中，作为视觉描述符的深层特征提高了 IQA，但人们发现它们具有高度纹理偏差且缺乏形状偏差。在此基础上，我们发现图像形状和纹理线索对扭曲的反应不同，并且其中任何一个的缺失都会导致图像表示不完整。因此，为了对图像制定全面的统计描述，我们同时利用深度神经网络（DNN）产生的形状偏差和纹理偏差的深层特征。更具体地说，我们设计了一个形状-纹理自适应融合（STAF）模块来合并形状和纹理信息，在此基础上我们制定与质量相关的图像统计数据。感知质量通过内部和外部形状纹理统计（DSTS）之间的变体马哈拉诺比斯距离来量化，其中内部和外部统计分别描述失真图像和自然图像的质量指纹。所提出的 DSTS 巧妙地利用了深域中不同数据尺度之间的形状纹理统计关系，并在具有人工和真实失真的图像上实现了最先进的（SOTA）质量预测性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08107v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain**<br />
**Title_cn:** KTVIC：生命领域的越南图像字幕数据集<br />
**Authors:** Anh-Cuong Pham, Van-Quang Nguyen, Thi-Hong Vuong, Quang-Thuy Ha<br />
**Abstract:** <details><summary>原文: </summary>Image captioning is a crucial task with applications in a wide range of domains, including healthcare and education. Despite extensive research on English image captioning datasets, the availability of such datasets for Vietnamese remains limited, with only two existing datasets. In this study, we introduce KTVIC, a comprehensive Vietnamese Image Captioning dataset focused on the life domain, covering a wide range of daily activities. This dataset comprises 4,327 images and 21,635 Vietnamese captions, serving as a valuable resource for advancing image captioning in the Vietnamese language. We conduct experiments using various deep neural networks as the baselines on our dataset, evaluating them using the standard image captioning metrics, including BLEU, METEOR, CIDEr, and ROUGE. Our findings underscore the effectiveness of the proposed dataset and its potential contributions to the field of image captioning in the Vietnamese context.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像字幕是一项关键任务，其应用范围广泛，包括医疗保健和教育。尽管对英语图像字幕数据集进行了广泛的研究，但此类越南语数据集的可用性仍然有限，现有的数据集只有两个。在本研究中，我们介绍了 KTVIC，这是一个专注于生活领域的综合越南图像字幕数据集，涵盖了广泛的日常活动。该数据集包含 4,327 张图像和 21,635 个越南语字幕，是推进越南语图像字幕的宝贵资源。我们使用各种深度神经网络作为数据集的基线进行实验，并使用标准图像字幕指标（包括 BLEU、METEOR、CIDEr 和 ROUGE）对其进行评估。我们的研究结果强调了所提出的数据集的有效性及其对越南背景下图像字幕领域的潜在贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.08100v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Representation Learning on Event Stream via an Elastic Net-incorporated Tensor Network**<br />
**Title_cn:** 通过弹性网络结合的张量网络对事件流进行表示学习<br />
**Authors:** Beibei Yang, Weiling Li, Yan Fang<br />
**Abstract:** <details><summary>原文: </summary>Event cameras are neuromorphic sensors that capture asynchronous and sparse event stream when per-pixel brightness changes. The state-of-the-art processing methods for event signals typically aggregate events into a frame or a grid. However, events are dense in time, these works are limited to local information of events due to the stacking. In this paper, we present a novel spatiotemporal representation learning method which can capture the global correlations of all events in the event stream simultaneously by tensor decomposition. In addition, with the events are sparse in space, we propose an Elastic Net-incorporated tensor network (ENTN) model to obtain more spatial and temporal details about event stream. Empirically, the results indicate that our method can represent the spatiotemporal correlation of events with high quality, and can achieve effective results in applications like filtering noise compared with the state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>事件相机是神经形态传感器，可在每像素亮度发生变化时捕获异步且稀疏的事件流。最先进的事件信号处理方法通常将事件聚合到帧或网格中。然而，事件在时间上是密集的，由于堆叠，这些工作仅限于事件的局部信息。在本文中，我们提出了一种新颖的时空表示学习方法，该方法可以通过张量分解同时捕获事件流中所有事件的全局相关性。此外，由于事件在空间上稀疏，我们提出了一种弹性网络合并张量网络（ENTN）模型来获取有关事件流的更多空间和时间细节。根据经验，结果表明，我们的方法可以高质量地表示事件的时空相关性，并且与最先进的方法相比，可以在过滤噪声等应用中取得有效的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.08068v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation**<br />
**Title_cn:** SCoFT：自我对比微调以实现公平的图像生成<br />
**Authors:** Zhixuan Liu, Peter Schaldenbrand, Beverley-Claire Okogwu, Wenxuan Peng, Youngsik Yun, Andrew Hundt, Jihie Kim, Jean Oh<br />
**Abstract:** <details><summary>原文: </summary>Accurate representation in media is known to improve the well-being of the people who consume it. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with harmful stereotypes and misrepresentations of cultures. We improve inclusive representation in generated images by (1) engaging with communities to collect a culturally representative dataset that we call the Cross-Cultural Understanding Benchmark (CCUB) and (2) proposing a novel Self-Contrastive Fine-Tuning (SCoFT) method that leverages the model's known biases to self-improve. SCoFT is designed to prevent overfitting on small datasets, encode only high-level information from the data, and shift the generated distribution away from misrepresentations encoded in a pretrained model. Our user study conducted on 51 participants from 5 different countries based on their self-selected national cultural affiliation shows that fine-tuning on CCUB consistently generates images with higher cultural relevance and fewer stereotypes when compared to the Stable Diffusion baseline, which is further improved with our SCoFT technique.</details>
**Abstract_cn:** <details><summary>译文: </summary>众所周知，媒体的准确报道可以改善媒体受众的福祉。众所周知，在 LAION 等大型网络爬取数据集上训练的生成图像模型会生成带有有害刻板印象和歪曲文化的图像。我们通过（1）与社区合作收集具有文化代表性的数据集（我们称之为跨文化理解基准（CCUB））和（2）提出一种新颖的自我对比微调（SCoFT）方法来改善生成图像中的包容性表示利用模型的已知偏差来自我改进。 SCoFT 旨在防止小数据集上的过度拟合，仅对数据中的高级信息进行编码，并使生成的分布远离预训练模型中编码的错误表示。我们对来自 5 个不同国家的 51 名参与者进行了基于他们自行选择的国家文化归属的用户研究，结果表明，与稳定扩散基线相比，CCUB 上的微调始终能够生成具有更高文化相关性和更少刻板印象的图像，并通过以下方法进一步改进：我们的 SCoFT 技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.08053v1><br />
**Code:** null<br />

