## [UPDATED!] **2024-01-17** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **GARField: Group Anything with Radiance Fields**<br />
**Title_cn:** GARField：用辐射场对任何东西进行分组<br />
**Authors:** Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Goldberg, Matthew Tancik, Angjoo Kanazawa<br />
**Abstract:** <details><summary>原文: </summary>Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at https://www.garfield.studio/</details>
**Abstract_cn:** <details><summary>译文: </summary>由于可以分解场景的多个粒度级别，分组本质上是不明确的——挖掘机的轮子应该被视为独立的还是整体的一部分？我们提出了使用辐射场对任何事物进行分组 (GARField)，这是一种将 3D 场景从姿势图像输入分解为具有语义意义的组的层次结构的方法。为此，我们通过物理尺度来接受群体模糊性：通过优化尺度条件的 3D 亲和力特征场，世界上的一个点可以属于不同大小的不同群体。我们从 Segment Anything (SAM) 提供的一组 2D 掩模中以尊重从粗到细的层次结构的方式优化该字段，使用比例来一致地融合来自不同视点的冲突掩模。从这个字段中，我们可以通过自动树构建或用户交互导出可能分组的层次结构。我们在各种野外场景中评估了 GARField，发现它可以有效地提取多个级别的组：对象集群、对象和各种子部分。 GARField 本质上代表多视图一致分组，并产生比输入 SAM 掩模更高保真度的组。 GARField 的分层分组可能具有令人兴奋的下游应用，例如 3D 资产提取或动态场景理解。请参阅项目网站 https://www.garfield.studio/</details>
**PDF:** <http://arxiv.org/pdf/2401.09419v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model**<br />
**Title_cn:** Vision Mamba：利用双向状态空间模型进行高效视觉表示学习<br />
**Authors:** Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang<br />
**Abstract:** <details><summary>原文: </summary>Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., Mamba, have shown great potential for long sequence modeling. Building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，具有高效硬件感知设计的状态空间模型（SSM）（即 Mamba）在长序列建模方面表现出了巨大的潜力。纯粹基于 SSM 构建高效且通用的视觉主干是一个有吸引力的方向。然而，由于视觉数据的位置敏感性以及视觉理解的全局上下文的要求，表示视觉数据对于 SSM 来说是一个挑战。在本文中，我们证明视觉表示学习对自注意力的依赖是不必要的，并提出了一种具有双向 Mamba 块（Vim）的新通用视觉主干，它用位置嵌入标记图像序列并用双向压缩视觉表示状态空间模型。在 ImageNet 分类、​​COCO 对象检测和 ADE20k 语义分割任务上，与 DeiT 等成熟的视觉转换器相比，Vim 实现了更高的性能，同时还显着提高了计算和内存效率。例如，在对分辨率为 1248$\times$1248 的图像执行批量推理提取特征时，Vim 比 DeiT 快 2.8$\times$，并节省 86.8% GPU 内存。结果表明，Vim 能够克服对高分辨率图像执行 Transformer 式理解时的计算和内存限制，并且具有成为视觉基础模型的下一代骨干的巨大潜力。代码可在 https://github.com/hustvl/Vim 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09417v1><br />
**Code:** <https://github.com/hustvl/vim>**<br />
>>**index:** 3<br />
**Title:** **POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images**<br />
**Title_cn:** POP-3D：根据图像进行开放词汇 3D 占用预测<br />
**Authors:** Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic<br />
**Abstract:** <details><summary>原文: </summary>We describe an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images with the objective of enabling 3D grounding, segmentation and retrieval of free-form language queries. This is a challenging problem because of the 2D-3D ambiguity and the open-vocabulary nature of the target tasks, where obtaining annotated training data in 3D is difficult. The contributions of this work are three-fold. First, we design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Second, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language and (iii) LiDAR point clouds, and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. Finally, we demonstrate quantitatively the strengths of the proposed model on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes. You can find the project page here https://vobecant.github.io/POP3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们描述了一种从输入 2D 图像预测开放词汇 3D 语义体素占用图的方法，其目标是实现自由形式语言查询的 3D 基础、分割和检索。这是一个具有挑战性的问题，因为 2D-3D 的模糊性和目标任务的开放词汇性质，在 3D 中获取带注释的训练数据很困难。这项工作的贡献有三个方面。首先，我们设计了一种用于开放词汇 3D 语义占用预测的新模型架构。该架构由 2D-3D 编码器以及占用预测和 3D 语言头组成。输出是 3D 基础语言嵌入的密集体素图，可实现一系列开放词汇任务。其次，我们开发了一种三模态自监督学习算法，该算法利用三种模态：(i) 图像、(ii) 语言和 (iii) LiDAR 点云，并能够使用强大的预训练视觉语言来训练所提出的架构模型无需任何 3D 手动语言注释。最后，我们定量地证明了所提出的模型在几个开放词汇任务上的优势：使用现有数据集的零样本 3D 语义分割；使用我们建议作为 nuScenes 扩展的小数据集，进行自由格式语言查询的 3D 基础和检索。您可以在此处找到项目页面 https://vobecant.github.io/POP3D。</details>
**PDF:** <http://arxiv.org/pdf/2401.09413v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **To deform or not: treatment-aware longitudinal registration for breast DCE-MRI during neoadjuvant chemotherapy via unsupervised keypoints detection**<br />
**Title_cn:** 变形与否：新辅助化疗期间通过无监督关键点检测对乳腺 DCE-MRI 进行治疗感知纵向配准<br />
**Authors:** Luyi Han, Tao Tan, Tianyu Zhang, Yuan Gao, Xin Wang, Valentina Longo, Sofía Ventura-Díaz, Anna D'Angelo, Jonas Teuwen, Ritse Mann<br />
**Abstract:** <details><summary>原文: </summary>Clinicians compare breast DCE-MRI after neoadjuvant chemotherapy (NAC) with pre-treatment scans to evaluate the response to NAC. Clinical evidence supports that accurate longitudinal deformable registration without deforming treated tumor regions is key to quantifying tumor changes. We propose a conditional pyramid registration network based on unsupervised keypoint detection and selective volume-preserving to quantify changes over time. In this approach, we extract the structural and the abnormal keypoints from DCE-MRI, apply the structural keypoints for the registration algorithm to restrict large deformation, and employ volume-preserving loss based on abnormal keypoints to keep the volume of the tumor unchanged after registration. We use a clinical dataset with 1630 MRI scans from 314 patients treated with NAC. The results demonstrate that our method registers with better performance and better volume preservation of the tumors. Furthermore, a local-global-combining biomarker based on the proposed method achieves high accuracy in pathological complete response (pCR) prediction, indicating that predictive information exists outside tumor regions. The biomarkers could potentially be used to avoid unnecessary surgeries for certain patients. It may be valuable for clinicians and/or computer systems to conduct follow-up tumor segmentation and response prediction on images registered by our method. Our code is available on \url{https://github.com/fiy2W/Treatment-aware-Longitudinal-Registration}.</details>
**Abstract_cn:** <details><summary>译文: </summary>临床医生将新辅助化疗 (NAC) 后的乳腺 DCE-MRI 与治疗前扫描进行比较，以评估对 NAC 的反应。临床证据支持，在不使治疗的肿瘤区域变形的情况下准确的纵向可变形配准是量化肿瘤变化的关键。我们提出了一种基于无监督关键点检测和选择性体积保留的条件金字塔配准网络，以量化随时间的变化。在该方法中，我们从DCE-MRI中提取结构和异常关键点，将结构关键点应用于配准算法以限制大变形，并基于异常关键点采用体积保持损失以保持配准后肿瘤体积不变。我们使用的临床数据集包含 314 名接受 NAC 治疗的患者的 1630 次 MRI 扫描。结果表明，我们的方法具有更好的性能和更好的肿瘤体积保存。此外，基于所提出的方法的局部全局组合生物标志物在病理完全缓解（pCR）预测中实现了高精度，表明预测信息存在于肿瘤区域之外。这些生物标志物有可能用于避免某些患者不必要的手术。对于临床医生和/或计算机系统对通过我们的方法注册的图像进行后续肿瘤分割和反应预测可能是有价值的。我们的代码可以在 \url{https://github.com/fiy2W/Treatment-aware-Longitudinal-Registration} 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.09336v1><br />
**Code:** <https://github.com/fiy2w/treatment-aware-longitudinal-registration>**<br />
>>**index:** 5<br />
**Title:** **Siamese Meets Diffusion Network: SMDNet for Enhanced Change Detection in High-Resolution RS Imagery**<br />
**Title_cn:** Siamese 遇上扩散网络：SMDNet 用于增强高分辨率 RS 图像中的变化检测<br />
**Authors:** Jia Jia, Geunho Lee, Zhibo Wang, Lyu Zhi, Yuchu He<br />
**Abstract:** <details><summary>原文: </summary>Recently, the application of deep learning to change detection (CD) has significantly progressed in remote sensing images. In recent years, CD tasks have mostly used architectures such as CNN and Transformer to identify these changes. However, these architectures have shortcomings in representing boundary details and are prone to false alarms and missed detections under complex lighting and weather conditions. For that, we propose a new network, Siamese Meets Diffusion Network (SMDNet). This network combines the Siam-U2Net Feature Differential Encoder (SU-FDE) and the denoising diffusion implicit model to improve the accuracy of image edge change detection and enhance the model's robustness under environmental changes. First, we propose an innovative SU-FDE module that utilizes shared weight features to capture differences between time series images and identify similarities between features to enhance edge detail detection. Furthermore, we add an attention mechanism to identify key coarse features to improve the model's sensitivity and accuracy. Finally, the diffusion model of progressive sampling is used to fuse key coarse features, and the noise reduction ability of the diffusion model and the advantages of capturing the probability distribution of image data are used to enhance the adaptability of the model in different environments. Our method's combination of feature extraction and diffusion models demonstrates effectiveness in change detection in remote sensing images. The performance evaluation of SMDNet on LEVIR-CD, DSIFN-CD, and CDD datasets yields validated F1 scores of 90.99%, 88.40%, and 88.47%, respectively. This substantiates the advanced capabilities of our model in accurately identifying variations and intricate details.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，深度学习在变化检测（CD）中的应用在遥感图像中取得了显着进展。近年来，CD任务大多使用CNN和Transformer等架构来识别这些变化。然而，这些架构在表示边界细节方面存在缺陷，并且在复杂的光照和天气条件下容易出现误报和漏检。为此，我们提出了一个新的网络，Siamese Meets Diffusion Network (SMDNet)。该网络结合了Siam-U2Net特征差分编码器（SU-FDE）和去噪扩散隐式模型，提高了图像边缘变化检测的准确性，增强了模型在环境变化下的鲁棒性。首先，我们提出了一种创新的 SU-FDE 模块，该模块利用共享权重特征来捕获时间序列图像之间的差异并识别特征之间的相似性以增强边缘细节检测。此外，我们添加了注意力机制来识别关键的粗略特征，以提高模型的灵敏度和准确性。最后利用渐进采样的扩散模型融合关键粗特征，利用扩散模型的降噪能力和捕捉图像数据概率分布的优势，增强模型在不同环境下的适应性。我们的方法结合了特征提取和扩散模型，证明了遥感图像变化检测的有效性。 SMDNet 在 LEVIR-CD、DSIFN-CD 和 CDD 数据集上的性能评估得出的经过验证的 F1 分数分别为 90.99%、88.40% 和 88.47%。这证实了我们的模型在准确识别变化和复杂细节方面的先进能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.09325v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **PixelDINO: Semi-Supervised Semantic Segmentation for Detecting Permafrost Disturbances**<br />
**Title_cn:** PixelDINO：用于检测永久冻土扰动的半监督语义分割<br />
**Authors:** Konrad Heidler, Ingmar Nitze, Guido Grosse, Xiao Xiang Zhu<br />
**Abstract:** <details><summary>原文: </summary>Arctic Permafrost is facing significant changes due to global climate change. As these regions are largely inaccessible, remote sensing plays a crucial rule in better understanding the underlying processes not just on a local scale, but across the Arctic. In this study, we focus on the remote detection of retrogressive thaw slumps (RTS), a permafrost disturbance comparable to landslides induced by thawing. For such analyses from space, deep learning has become an indispensable tool, but limited labelled training data remains a challenge for training accurate models. To improve model generalization across the Arctic without the need for additional labelled data, we present a semi-supervised learning approach to train semantic segmentation models to detect RTS. Our framework called PixelDINO is trained in parallel on labelled data as well as unlabelled data. For the unlabelled data, the model segments the imagery into self-taught pseudo-classes and the training procedure ensures consistency of these pseudo-classes across strong augmentations of the input data. Our experimental results demonstrate that PixelDINO can improve model performance both over supervised baseline methods as well as existing semi-supervised semantic segmentation approaches, highlighting its potential for training robust models that generalize well to regions that were not included in the training data. The project page containing code and other materials for this study can be found at \url{https://khdlr.github.io/PixelDINO/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于全球气候变化，北极永久冻土层正面临重大变化。由于这些地区基本上交通不便，遥感在更好地了解当地乃至整个北极的潜在过程方面发挥着至关重要的作用。在这项研究中，我们重点关注倒退性融雪崩塌（RTS）的远程检测，这是一种与解冻引起的山体滑坡相当的永久冻土扰动。对于此类来自太空的分析，深度学习已成为不可或缺的工具，但有限的标记训练数据仍然是训练准确模型的挑战。为了在不需要额外标记数据的情况下提高整个北极地区的模型泛化能力，我们提出了一种半监督学习方法来训练语义分割模型来检测 RTS。我们名为 PixelDINO 的框架在标记数据和未标记数据上进行并行训练。对于未标记的数据，模型将图像分割成自学的伪类，并且训练过程确保这些伪类在输入数据的强增强中的一致性。我们的实验结果表明，PixelDINO 可以比监督基线方法以及现有的半监督语义分割方法提高模型性能，突显其训练鲁棒模型的潜力，这些模型可以很好地推广到训练数据中未包含的区域。包含本研究的代码和其他材料的项目页面可以在 \url{https://khdlr.github.io/PixelDINO/} 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.09271v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Uncertainty estimates for semantic segmentation: providing enhanced reliability for automated motor claims handling**<br />
**Title_cn:** 语义分割的不确定性估计：为自动汽车索赔处理提供增强的可靠性<br />
**Authors:** Jan Küchler, Daniel Kröll, Sebastian Schoenen, Andreas Witte<br />
**Abstract:** <details><summary>原文: </summary>Deep neural network models for image segmentation can be a powerful tool for the automation of motor claims handling processes in the insurance industry. A crucial aspect is the reliability of the model outputs when facing adverse conditions, such as low quality photos taken by claimants to document damages. We explore the use of a meta-classification model to assess the precision of segments predicted by a model trained for the semantic segmentation of car body parts. Different sets of features correlated with the quality of a segment are compared, and an AUROC score of 0.915 is achieved for distinguishing between high- and low-quality segments. By removing low-quality segments, the average mIoU of the segmentation output is improved by 16 percentage points and the number of wrongly predicted segments is reduced by 77%.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于图像分割的深度神经网络模型可以成为保险行业汽车索赔处理流程自动化的强大工具。一个关键的方面是模型输出在面临不利条件时的可靠性，例如索赔人为记录损失而拍摄的低质量照片。我们探索使用元分类模型来评估为车身零件语义分割训练的模型所预测的分段的精度。比较与片段质量相关的不同特征集，区分高质量片段和低质量片段的 AUROC 得分为 0.915。通过去除低质量片段，分割输出的平均 mIoU 提高了 16 个百分点，错误预测的片段数量减少了 77%。</details>
**PDF:** <http://arxiv.org/pdf/2401.09245v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Dynamic Relation Transformer for Contextual Text Block Detection**<br />
**Title_cn:** 用于上下文文本块检测的动态关系转换器<br />
**Authors:** Jiawei Wang, Shunchi Zhang, Kai Hu, Chixiang Ma, Zhuoyao Zhong, Lei Sun, Qiang Huo<br />
**Abstract:** <details><summary>原文: </summary>Contextual Text Block Detection (CTBD) is the task of identifying coherent text blocks within the complexity of natural scenes. Previous methodologies have treated CTBD as either a visual relation extraction challenge within computer vision or as a sequence modeling problem from the perspective of natural language processing. We introduce a new framework that frames CTBD as a graph generation problem. This methodology consists of two essential procedures: identifying individual text units as graph nodes and discerning the sequential reading order relationships among these units as graph edges. Leveraging the cutting-edge capabilities of DQ-DETR for node detection, our framework innovates further by integrating a novel mechanism, a Dynamic Relation Transformer (DRFormer), dedicated to edge generation. DRFormer incorporates a dual interactive transformer decoder that deftly manages a dynamic graph structure refinement process. Through this iterative process, the model systematically enhances the graph's fidelity, ultimately resulting in improved precision in detecting contextual text blocks. Comprehensive experimental evaluations conducted on both SCUT-CTW-Context and ReCTS-Context datasets substantiate that our method achieves state-of-the-art results, underscoring the effectiveness and potential of our graph generation framework in advancing the field of CTBD.</details>
**Abstract_cn:** <details><summary>译文: </summary>上下文文本块检测（CTBD）是在复杂的自然场景中识别连贯文本块的任务。以前的方法论将 CTBD 视为计算机视觉中的视觉关系提取挑战或从自然语言处理的角度来看的序列建模问题。我们引入了一个新框架，将 CTBD 框架为图生成问题。该方法由两个基本过程组成：将各个文本单元识别为图节点，并将这些单元之间的顺序阅读顺序关系识别为图边。利用 DQ-DETR 的尖端功能进行节点检测，我们的框架通过集成专用于边缘生成的新颖机制——动态关系变换器 (DRFormer) 来进一步创新。 DRFormer 结合了双交互式变压器解码器，可以巧妙地管理动态图结构细化过程。通过这个迭代过程，该模型系统地增强了图形的保真度，最终提高了检测上下文文本块的精度。对 SCUT-CTW-Context 和 ReCTS-Context 数据集进行的综合实验评估证实了我们的方法取得了最先进的结果，强调了我们的图生成框架在推进 CTBD 领域的有效性和潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.09232v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Exploring the Role of Convolutional Neural Networks (CNN) in Dental Radiography Segmentation: A Comprehensive Systematic Literature Review**<br />
**Title_cn:** 探索卷积神经网络 (CNN) 在牙科放射线摄影分割中的作用：全面系统的文献综述<br />
**Authors:** Walid Brahmi, Imen Jdey, Fadoua Drira<br />
**Abstract:** <details><summary>原文: </summary>In the field of dentistry, there is a growing demand for increased precision in diagnostic tools, with a specific focus on advanced imaging techniques such as computed tomography, cone beam computed tomography, magnetic resonance imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep learning has emerged as a pivotal tool in this context, enabling the implementation of automated segmentation techniques crucial for extracting essential diagnostic data. This integration of cutting-edge technology addresses the urgent need for effective management of dental conditions, which, if left undetected, can have a significant impact on human health. The impressive track record of deep learning across various domains, including dentistry, underscores its potential to revolutionize early detection and treatment of oral health issues. Objective: Having demonstrated significant results in diagnosis and prediction, deep convolutional neural networks (CNNs) represent an emerging field of multidisciplinary research. The goals of this study were to provide a concise overview of the state of the art, standardize the current debate, and establish baselines for future research. Method: In this study, a systematic literature review is employed as a methodology to identify and select relevant studies that specifically investigate the deep learning technique for dental imaging analysis. This study elucidates the methodological approach, including the systematic collection of data, statistical analysis, and subsequent dissemination of outcomes. Conclusion: This work demonstrates how Convolutional Neural Networks (CNNs) can be employed to analyze images, serving as effective tools for detecting dental pathologies. Although this research acknowledged some limitations, CNNs utilized for segmenting and categorizing teeth exhibited their highest level of performance overall.</details>
**Abstract_cn:** <details><summary>译文: </summary>在牙科领域，对提高诊断工具精度的需求不断增长，特别关注先进的成像技术，如计算机断层扫描、锥形束计算机断层扫描、磁共振成像、超声波和传统的口腔内根尖周X线检查。射线。深度学习已成为这种背景下的关键工具，能够实现对于提取基本诊断数据至关重要的自动分割技术。这种尖端技术的整合满足了有效管理牙科疾病的迫切需求，如果不及时发现，可能会对人类健康产生重大影响。深度学习在包括牙科在内的各个领域的令人印象深刻的记录强调了其彻底改变口腔健康问题的早期检测和治疗的潜力。目的：深度卷积神经网络（CNN）在诊断和预测方面取得了显着的成果，代表了多学科研究的新兴领域。这项研究的目标是提供最新技术的简明概述，标准化当前的争论，并为未来的研究建立基线。方法：在本研究中，采用系统文献综述作为方法来识别和选择专门研究牙科成像分析深度学习技术的相关研究。这项研究阐明了方法论，包括系统地收集数据、统计分析和随后的结果传播。结论：这项工作展示了如何使用卷积神经网络（CNN）来分析图像，作为检测牙科病理的有效工具。尽管这项研究承认存在一些局限性，但用于牙齿分割和分类的 CNN 总体表现出了最高水平的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.09190v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing**<br />
**Title_cn:** DK-SLAM：具有深度关键点自适应学习、跟踪和闭环的单目视觉 SLAM<br />
**Authors:** Hao Qu, Lilian Zhang, Jun Mao, Junbo Tie, Xiaofeng He, Xiaoping Hu, Yifei Shi, Changhao Chen<br />
**Abstract:** <details><summary>原文: </summary>Unreliable feature extraction and matching in handcrafted features undermine the performance of visual SLAM in complex real-world scenarios. While learned local features, leveraging CNNs, demonstrate proficiency in capturing high-level information and excel in matching benchmarks, they encounter challenges in continuous motion scenes, resulting in poor generalization and impacting loop detection accuracy. To address these issues, we present DK-SLAM, a monocular visual SLAM system with adaptive deep local features. MAML optimizes the training of these features, and we introduce a coarse-to-fine feature tracking approach. Initially, a direct method approximates the relative pose between consecutive frames, followed by a feature matching method for refined pose estimation. To counter cumulative positioning errors, a novel online learning binary feature-based online loop closure module identifies loop nodes within a sequence. Experimental results underscore DK-SLAM's efficacy, outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly available datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>手工特征中不可靠的特征提取和匹配破坏了视觉 SLAM 在复杂的现实场景中的性能。虽然利用 CNN 学习局部特征，表现出捕获高级信息的能力并擅长匹配基准，但它们在连续运动场景中遇到挑战，导致泛化能力较差并影响循环检测准确性。为了解决这些问题，我们提出了 DK-SLAM，一种具有自适应深度局部特征的单目视觉 SLAM 系统。 MAML 优化了这些特征的训练，我们引入了从粗到细的特征跟踪方法。最初，直接方法近似连续帧之间的相对姿势，然后采用特征匹配方法进行精细姿势估计。为了对抗累积定位误差，一种新颖的基于在线学习二进制特征的在线闭环模块识别序列内的循环节点。实验结果强调了 DK-SLAM 的有效性，在公开数据集上优于代表性 SLAM 解决方案，例如 ORB-SLAM3。</details>
**PDF:** <http://arxiv.org/pdf/2401.09160v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Trapped in texture bias? A large scale comparison of deep instance segmentation**<br />
**Title_cn:** 陷入纹理偏差？深度实例分割的大规模比较<br />
**Authors:** Johannes Theodoridis, Jessica Hofmann, Johannes Maucher, Andreas Schilling<br />
**Abstract:** <details><summary>原文: </summary>Do deep learning models for instance segmentation generalize to novel objects in a systematic way? For classification, such behavior has been questioned. In this study, we aim to understand if certain design decisions such as framework, architecture or pre-training contribute to the semantic understanding of instance segmentation. To answer this question, we consider a special case of robustness and compare pre-trained models on a challenging benchmark for object-centric, out-of-distribution texture. We do not introduce another method in this work. Instead, we take a step back and evaluate a broad range of existing literature. This includes Cascade and Mask R-CNN, Swin Transformer, BMask, YOLACT(++), DETR, BCNet, SOTR and SOLOv2. We find that YOLACT++, SOTR and SOLOv2 are significantly more robust to out-of-distribution texture than other frameworks. In addition, we show that deeper and dynamic architectures improve robustness whereas training schedules, data augmentation and pre-training have only a minor impact. In summary we evaluate 68 models on 61 versions of MS COCO for a total of 4148 evaluations.</details>
**Abstract_cn:** <details><summary>译文: </summary>实例分割的深度学习模型是否可以系统地推广到新的对象？对于分类来说，这种行为受到了质疑。在本研究中，我们的目标是了解框架、架构或预训练等某些设计决策是否有助于实例分割的语义理解。为了回答这个问题，我们考虑了鲁棒性的特殊情况，并在以对象为中心的非分布纹理的具有挑战性的基准上比较预先训练的模型。我们在这项工作中不介绍另一种方法。相反，我们退一步并评估广泛的现有文献。这包括 Cascade 和 Mask R-CNN、Swin Transformer、BMask、YOLACT(++)、DETR、BCNet、SOTR 和 SOLOv2。我们发现 YOLACT++、SOTR 和 SOLOv2 对于分布外纹理的鲁棒性明显优于其他框架。此外，我们还表明，更深层次的动态架构可以提高鲁棒性，而训练计划、数据增强和预训练的影响很小。总之，我们在 61 个版本的 MS COCO 上评估了 68 个模型，总共进行了 4148 次评估。</details>
**PDF:** <http://arxiv.org/pdf/2401.09109v1><br />
**Code:** <https://github.com/johannestheo/trapped-in-texture-bias>**<br />
>>**index:** 12<br />
**Title:** **Enhancing Lidar-based Object Detection in Adverse Weather using Offset Sequences in Time**<br />
**Title_cn:** 使用时间偏移序列增强恶劣天气下基于激光雷达的物体检测<br />
**Authors:** Raphael van Kempen, Tim Rehbronn, Abin Jose, Johannes Stegmaier, Bastian Lampe, Timo Woopen, Lutz Eckstein<br />
**Abstract:** <details><summary>原文: </summary>Automated vehicles require an accurate perception of their surroundings for safe and efficient driving. Lidar-based object detection is a widely used method for environment perception, but its performance is significantly affected by adverse weather conditions such as rain and fog. In this work, we investigate various strategies for enhancing the robustness of lidar-based object detection by processing sequential data samples generated by lidar sensors. Our approaches leverage temporal information to improve a lidar object detection model, without the need for additional filtering or pre-processing steps. We compare $10$ different neural network architectures that process point cloud sequences including a novel augmentation strategy introducing a temporal offset between frames of a sequence during training and evaluate the effectiveness of all strategies on lidar point clouds under adverse weather conditions through experiments. Our research provides a comprehensive study of effective methods for mitigating the effects of adverse weather on the reliability of lidar-based object detection using sequential data that are evaluated using public datasets such as nuScenes, Dense, and the Canadian Adverse Driving Conditions Dataset. Our findings demonstrate that our novel method, involving temporal offset augmentation through randomized frame skipping in sequences, enhances object detection accuracy compared to both the baseline model (Pillar-based Object Detection) and no augmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶车辆需要准确感知周围环境，以实现安全高效的驾驶。基于激光雷达的物体检测是一种广泛使用的环境感知方法，但其性能受到雨、雾等恶劣天气条件的显着影响。在这项工作中，我们研究了通过处理激光雷达传感器生成的顺序数据样本来增强基于激光雷达的物体检测的鲁棒性的各种策略。我们的方法利用时间信息来改进激光雷达目标检测模型，而不需要额外的过滤或预处理步骤。我们比较了处理点云序列的 10 美元不同神经网络架构，包括一种新颖的增强策略，在训练期间引入序列帧之间的时间偏移，并通过实验评估所有策略在恶劣天气条件下对激光雷达点云的有效性。我们的研究对有效方法进行了全面研究，以减轻恶劣天气对基于激光雷达的物体检测可靠性的影响，这些方法使用序列数据，这些数据使用 nuScenes、Dense 和加拿大不良驾驶条件数据集等公共数据集进行评估。我们的研究结果表明，与基线模型（基于支柱的对象检测）和无增强模型相比，我们的新颖方法通过序列中的随机跳帧来增强时间偏移，提高了对象检测的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09049v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM)**<br />
**Title_cn:** 通过分段任意模型 (SAM) 检测光学遥感图像和地图数据之间的变化<br />
**Authors:** Hongruixuan Chen, Jian Song, Naoto Yokoya<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised multimodal change detection is pivotal for time-sensitive tasks and comprehensive multi-temporal Earth monitoring. In this study, we explore unsupervised multimodal change detection between two key remote sensing data sources: optical high-resolution imagery and OpenStreetMap (OSM) data. Specifically, we propose to utilize the vision foundation model Segmentation Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional zero-shot transfer capability, high-quality segmentation maps of optical images can be obtained. Thus, we can directly compare these two heterogeneous data forms in the so-called segmentation domain. We then introduce two strategies for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt' methods. The two strategies are designed to detect land-cover changes in general scenarios and to identify new land-cover objects within existing backgrounds, respectively. Experimental results on three datasets indicate that the proposed approach can achieve more competitive results compared to representative unsupervised multimodal change detection methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督多模式变化检测对于时间敏感任务和全面的多时相地球监测至关重要。在本研究中，我们探索了两个关键遥感数据源之间的无监督多模态变化检测：光学高分辨率图像和 OpenStreetMap (OSM) 数据。具体来说，我们建议利用视觉基础模型分段任何模型（SAM）来解决我们的任务。利用 SAM 卓越的零镜头传输能力，可以获得高质量的光学图像分割图。因此，我们可以在所谓的分段域中直接比较这两种异构数据形式。然后，我们介绍两种指导 SAM 分割过程的策略：“无提示”和“框/掩码提示”方法。这两种策略旨在分别检测一般场景中的土地覆盖变化并识别现有背景中的新土地覆盖对象。三个数据集的实验结果表明，与代表性的无监督多模态变化检测方法相比，所提出的方法可以获得更具竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.09019v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Generalized Face Liveness Detection via De-spoofing Face Generator**<br />
**Title_cn:** 通过反欺骗人脸生成器进行广义人脸活体检测<br />
**Authors:** Xingming Long, Shiguang Shan, Jie Zhang<br />
**Abstract:** <details><summary>原文: </summary>Previous Face Anti-spoofing (FAS) works face the challenge of generalizing in unseen domains. One of the major problems is that most existing FAS datasets are relatively small and lack data diversity. However, we find that there are numerous real faces that can be easily achieved under various conditions, which are neglected by previous FAS works. In this paper, we conduct an Anomalous cue Guided FAS (AG-FAS) method, which leverages real faces for improving model generalization via a De-spoofing Face Generator (DFG). Specifically, the DFG trained only on the real faces gains the knowledge of what a real face should be like and can generate a "real" version of the face corresponding to any given input face. The difference between the generated "real" face and the input face can provide an anomalous cue for the downstream FAS task. We then propose an Anomalous cue Guided FAS feature extraction Network (AG-Net) to further improve the FAS feature generalization via a cross-attention transformer. Extensive experiments on a total of nine public datasets show our method achieves state-of-the-art results under cross-domain evaluations with unseen scenarios and unknown presentation attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>之前的人脸反欺骗（FAS）工作面临着在未知领域进行推广的挑战。主要问题之一是大多数现有 FAS 数据集相对较小且缺乏数据多样性。然而，我们发现有许多在各种条件下都可以轻松实现的真实面孔，而这些都是以前的FAS作品所忽略的。在本文中，我们采用了一种异常线索引导 FAS (AG-FAS) 方法，该方法通过反欺骗面部生成器 (DFG) 利用真实面部来改进模型泛化。具体来说，仅在真实面部上训练的 DFG 可以获得真实面部应该是什么样子的知识，并且可以生成与任何给定输入面部相对应的面部的“真实”版本。生成的“真实”面部与输入面部之间的差异可以为下游 FAS 任务提供异常线索。然后，我们提出了一种异常线索引导 FAS 特征提取网络（AG-Net），以通过交叉注意力变换器进一步改进 FAS 特征泛化。对总共九个公共数据集的广泛实验表明，我们的方法在未见过的场景和未知的演示攻击的跨域评估下取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.09006v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Hearing Loss Detection from Facial Expressions in One-on-one Conversations**<br />
**Title_cn:** 从一对一对话中的面部表情检测听力损失<br />
**Authors:** Yufeng Yin, Ishwarya Ananthabhotla, Vamsi Krishna Ithapu, Stavros Petridis, Yu-Hsiang Wu, Christi Miller<br />
**Abstract:** <details><summary>原文: </summary>Individuals with impaired hearing experience difficulty in conversations, especially in noisy environments. This difficulty often manifests as a change in behavior and may be captured via facial expressions, such as the expression of discomfort or fatigue. In this work, we build on this idea and introduce the problem of detecting hearing loss from an individual's facial expressions during a conversation. Building machine learning models that can represent hearing-related facial expression changes is a challenge. In addition, models need to disentangle spurious age-related correlations from hearing-driven expressions. To this end, we propose a self-supervised pre-training strategy tailored for the modeling of expression variations. We also use adversarial representation learning to mitigate the age bias. We evaluate our approach on a large-scale egocentric dataset with real-world conversational scenarios involving subjects with hearing loss and show that our method for hearing loss detection achieves superior performance over baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>听力受损的人在交谈时会遇到困难，尤其是在嘈杂的环境中。这种困难通常表现为行为的改变，并且可以通过面部表情来捕捉，例如不适或疲劳的表情。在这项工作中，我们以这个想法为基础，引入了通过对话期间个人的面部表情来检测听力损失的问题。构建能够代表与听力相关的面部表情变化的机器学习模型是一项挑战。此外，模型需要将虚假的年龄相关性与听力驱动的表达分开。为此，我们提出了一种针对表达变化建模的自监督预训练策略。我们还使用对抗性表示学习来减轻年龄偏见。我们在大规模以自我为中心的数据集上评估了我们的方法，其中包含涉及听力损失受试者的真实对话场景，并表明我们的听力损失检测方法实现了优于基线的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08972v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Learning to detect cloud and snow in remote sensing images from noisy labels**<br />
**Title_cn:** 学习从噪声标签中检测遥感图像中的云和雪<br />
**Authors:** Zili Liu, Hao Chen, Wenyuan Li, Keyan Chen, Zipeng Qi, Chenyang Liu, Zhengxia Zou, Zhenwei Shi<br />
**Abstract:** <details><summary>原文: </summary>Detecting clouds and snow in remote sensing images is an essential preprocessing task for remote sensing imagery. Previous works draw inspiration from semantic segmentation models in computer vision, with most research focusing on improving model architectures to enhance detection performance. However, unlike natural images, the complexity of scenes and the diversity of cloud types in remote sensing images result in many inaccurate labels in cloud and snow detection datasets, introducing unnecessary noises into the training and testing processes. By constructing a new dataset and proposing a novel training strategy with the curriculum learning paradigm, we guide the model in reducing overfitting to noisy labels. Additionally, we design a more appropriate model performance evaluation method, that alleviates the performance assessment bias caused by noisy labels. By conducting experiments on models with UNet and Segformer, we have validated the effectiveness of our proposed method. This paper is the first to consider the impact of label noise on the detection of clouds and snow in remote sensing images.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测遥感图像中的云和雪是遥感图像的一项重要预处理任务。之前的工作从计算机视觉中的语义分割模型中汲取灵感，大多数研究都集中在改进模型架构以增强检测性能。然而，与自然图像不同，遥感图像中场景的复杂性和云类型的多样性导致云和雪检测数据集中存在许多不准确的标签，从而在训练和测试过程中引入不必要的噪声。通过构建新的数据集并利用课程学习范式提出新颖的训练策略，我们指导模型减少对噪声标签的过度拟合。此外，我们设计了更合适的模型性能评估方法，减轻了噪声标签引起的性能评估偏差。通过使用 UNet 和 Segformer 对模型进行实验，我们验证了所提出方法的有效性。本文首次考虑标签噪声对遥感图像中云雪检测的影响。</details>
**PDF:** <http://arxiv.org/pdf/2401.08932v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems**<br />
**Title_cn:** PPR：增强躲避攻击的同时维持对人脸识别系统的模拟攻击<br />
**Authors:** Fengfan Zhou, Heifei Ling<br />
**Abstract:** <details><summary>原文: </summary>Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experimental results, showcasing its superior performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸识别（FR）的对抗性攻击包括两种类型：模仿攻击和逃避攻击。我们观察到，成功地对 FR 进行模拟攻击并不一定能确保在黑盒设置中成功躲避对 FR 的攻击。引入一种名为预训练修剪恢复攻击（PPR）的新颖攻击方法，我们的目标是增强躲避攻击的性能，同时避免模拟攻击的退化。我们的方法采用对抗性示例剪枝，使一部分对抗性扰动设置为零，同时倾向于保持攻击性能。通过利用对抗性样本修剪，我们可以修剪预先训练的对抗性样本，并选择性地释放某些对抗性扰动。此后，我们在修剪区域中嵌入对抗性扰动，这增强了对抗性面部示例的躲避性能。我们提出的攻击方法的有效性通过我们的实验结果得到了证明，展示了其优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08903v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion**<br />
**Title_cn:** TextureDreamer：通过几何感知扩散进行图像引导纹理合成<br />
**Authors:** Yu-Ying Yeh, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl S Marshall, Zhao Dong, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 TextureDreamer，这是一种新颖的图像引导纹理合成方法，可将可重新点亮的纹理从少量输入图像（3 到 5 个）传输到跨任意类别的目标 3D 形状。纹理创建是视觉和图形领域的一个关键挑战。工业公司聘请经验丰富的艺术家来手动制作 3D 资产的纹理。经典方法需要密集采样的视图和精确对齐的几何形状，而基于学习的方法仅限于数据集中特定于类别的形状。相比之下，TextureDreamer 可以将高度详细、复杂的纹理从现实世界环境转移到任意对象，只需几张随意捕获的图像，这可能会显着使纹理创建民主化。我们的核心理念是个性化几何感知分数蒸馏 (PGSD)，它从扩散模型的最新进展中汲取灵感，包括用于纹理信息提取的个性化建模、用于详细外观合成的变分分数蒸馏以及 ControlNet 的显式几何指导。我们的集成和一些必要的修改大大提高了纹理质量。对跨越不同类别的真实图像的实验表明，TextureDreamer 可以成功地将高度逼真、语义有意义的纹理传输到任意对象，超越了之前最先进的视觉质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.09416v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **An Efficient Generalizable Framework for Visuomotor Policies via Control-aware Augmentation and Privilege-guided Distillation**<br />
**Title_cn:** 通过控制感知增强和特权引导蒸馏的有效通用视觉运动策略框架<br />
**Authors:** Yinuo Zhao, Kun Wu, Tianjiao Yi, Zhiyuan Xu, Xiaozhu Ju, Zhengping Che, Qinru Qiu, Chi Harold Liu, Jian Tang<br />
**Abstract:** <details><summary>原文: </summary>Visuomotor policies, which learn control mechanisms directly from high-dimensional visual observations, confront challenges in adapting to new environments with intricate visual variations. Data augmentation emerges as a promising method for bridging these generalization gaps by enriching data variety. However, straightforwardly augmenting the entire observation shall impose excessive burdens on policy learning and may even result in performance degradation. In this paper, we propose to improve the generalization ability of visuomotor policies as well as preserve training stability from two aspects: 1) We learn a control-aware mask through a self-supervised reconstruction task with three auxiliary losses and then apply strong augmentation only to those control-irrelevant regions based on the mask to reduce the generalization gaps. 2) To address training instability issues prevalent in visual reinforcement learning (RL), we distill the knowledge from a pretrained RL expert processing low-level environment states, to the student visuomotor policy. The policy is subsequently deployed to unseen environments without any further finetuning. We conducted comparison and ablation studies across various benchmarks: the DMControl Generalization Benchmark (DMC-GB), the enhanced Robot Manipulation Distraction Benchmark (RMDB), and a specialized long-horizontal drawer-opening robotic task. The extensive experimental results well demonstrate the effectiveness of our method, e.g., showing a 17\% improvement over previous methods in the video-hard setting of DMC-GB.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉运动策略直接从高维视觉观察中学习控制机制，在适应具有复杂视觉变化的新环境方面面临着挑战。数据增强成为一种通过丰富数据多样性来弥合这些泛化差距的有前途的方法。然而，直接增强整个观察会给政策学习带来过多的负担，甚至可能导致性能下降。在本文中，我们建议从两个方面提高视觉运动策略的泛化能力并保持训练稳定性：1）我们通过具有三个辅助损失的自监督重建任务学习控制感知掩模，然后仅应用强增强基于掩模的那些与控制无关的区域，以减少泛化差距。 2) 为了解决视觉强化学习 (RL) 中普遍存在的训练不稳定问题，我们将处理低级环境状态的预训练 RL 专家的知识提炼为学生视觉运动策略。该策略随后被部署到未见过的环境中，无需任何进一步的微调。我们对各种基准进行了比较和消融研究：DMControl 泛化基准 (DMC-GB)、增强型机器人操作分心基准 (RMDB) 以及专门的长水平抽屉打开机器人任务。大量的实验结果很好地证明了我们方法的有效性，例如，在 DMC-GB 的视频硬设置中，比以前的方法提高了 17%。</details>
**PDF:** <http://arxiv.org/pdf/2401.09258v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior**<br />
**Title_cn:** Consolidated3D：通过确定性采样先验实现一致的高保真文本到 3D 生成<br />
**Authors:** Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, Hanwang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective "Consistent3D" method that explores the ODE deterministic sampling prior for text-to-3D generation. Specifically, at each training iteration, given a rendered image by a 3D model, we first estimate its desired 3D score function by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling. Next, we design a consistency distillation sampling loss which samples along the ODE trajectory to generate two adjacent samples and uses the less noisy sample to guide another more noisy one for distilling the deterministic prior into the 3D model. Experimental results show the efficacy of our Consistent3D in generating high-fidelity and diverse 3D objects and large-scale scenes, as shown in Fig. 1. The codes are available at https://github.com/sail-sg/Consistent3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>分数蒸馏采样 (SDS) 及其变体极大地促进了文本到 3D 生成的发展，但仍然容易受到几何崩溃和不良纹理的影响。为了解决这个问题，我们首先对SDS进行深入分析，发现其蒸馏采样过程确实对应于随机微分方程（SDE）的轨迹采样：SDS沿着SDE轨迹采样，得到噪声较小的样本，然后将其作为优化 3D 模型的指导。然而，SDE 采样中的随机性通常会导致样本多样化且不可预测，这些样本并不总是噪音较小，因此并不是始终正确的指导，这解释了 SDS 的脆弱性。由于对于任何SDE，总是存在一个常微分方程（ODE），其轨迹采样可以确定性地一致地收敛到所需的目标点作为SDE，我们提出了一种新颖且有效的“一致3D”方法，该方法探索了ODE确定性采样先验文本到 3D 生成。具体来说，在每次训练迭代中，给定 3D 模型渲染的图像，我们首先通过预训练的 2D 扩散模型估计其所需的 3D 得分函数，并构建用于轨迹采样的 ODE。接下来，我们设计了一种一致性蒸馏采样损失，它沿着 ODE 轨迹采样以生成两个相邻样本，并使用噪声较小的样本引导另一个噪声较大的样本，将确定性先验提取到 3D 模型中。实验结果表明，我们的 Consolidated3D 在生成高保真、多样化的 3D 对象和大规模场景方面的功效，如图 1 所示。代码可在 https://github.com/sail-sg/Concient3D 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09050v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Hybrid of DiffStride and Spectral Pooling in Convolutional Neural Networks**<br />
**Title_cn:** 卷积神经网络中 DiffStride 和谱池的混合<br />
**Authors:** Sulthan Rafif, Mochamad Arfan Ravy Wahyu Pratama, Mohammad Faris Azhar, Ahmad Mustafidul Ibad, Lailil Muflikhah, Novanto Yudistira<br />
**Abstract:** <details><summary>原文: </summary>Stride determines the distance between adjacent filter positions as the filter moves across the input. A fixed stride causes important information contained in the image can not be captured, so that important information is not classified. Therefore, in previous research, the DiffStride Method was applied, namely the Strided Convolution Method with which it can learn its own stride value. Severe Quantization and a constraining lower bound on preserved information are arises with Max Pooling Downsampling Method. Spectral Pooling reduce the constraint lower bound on preserved information by cutting off the representation in the frequency domain. In this research a CNN Model is proposed with the Downsampling Learnable Stride Technique performed by Backpropagation combined with the Spectral Pooling Technique. Diffstride and Spectral Pooling techniques are expected to maintain most of the information contained in the image. In this study, we compare the Hybrid Method, which is a combined implementation of Spectral Pooling and DiffStride against the Baseline Method, which is the DiffStride implementation on ResNet 18. The accuracy result of the DiffStride combination with Spectral Pooling improves over DiffStride which is baseline method by 0.0094. This shows that the Hybrid Method can maintain most of the information by cutting of the representation in the frequency domain and determine the stride of the learning result through Backpropagation.</details>
**Abstract_cn:** <details><summary>译文: </summary>步幅确定滤波器在输入上移动时相邻滤波器位置之间的距离。固定步长导致图像中包含的重要信息无法被捕获，从而导致重要信息无法分类。因此，在之前的研究中，采用了DiffStride Method，即Strided Convolution Method，通过它可以学习自己的步幅值。最大池下采样方法会产生严重的量化和保留信息的约束下限。频谱池通过切断频域中的表示来减少保留信息的约束下限。在本研究中，提出了一种 CNN 模型，该模型采用反向传播结合频谱池技术执行的下采样可学习跨步技术。 Diffstride 和 Spectral Pooling 技术有望保留图像中包含的大部分信息。在本研究中，我们将混合方法（光谱池和 DiffStride 的组合实现）与基线方法（ResNet 18 上的 DiffStride 实现）进行比较。DiffStride 与光谱池组合的准确度结果比基线 DiffStride 有所提高。方法通过0.0094。这表明混合方法可以通过切割频域中的表示来保留大部分信息，并通过反向传播来确定学习结果的步长。</details>
**PDF:** <http://arxiv.org/pdf/2401.09008v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models**<br />
**Title_cn:** VideoCrafter2：克服高质量视频扩散模型的数据限制<br />
**Authors:** Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, Ying Shan<br />
**Abstract:** <details><summary>原文: </summary>Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到视频生成旨在根据给定的提示生成视频。最近，一些商业视频模型已经能够生成具有最小噪声、出色细节和高审美分数的可信视频。然而，这些模型依赖于社区无法访问的大规模、经过良好过滤的高质量视频。许多现有的研究工作使用低质量的 WebVid-10M 数据集训练模型，但很难生成高质量的视频，因为模型经过优化以适合 WebVid-10M。在这项工作中，我们探索了从稳定扩散扩展的视频模型的训练方案，并研究了利用低质量视频和合成高质量图像来获得高质量视频模型的可行性。我们首先分析视频模型的空间和时间模块之间的联系以及向低质量视频的分布转移。我们观察到，与仅训练时间模块相比，对所有模块进行全面训练会导致空间和时间模块之间的耦合更强。基于这种更强的耦合，我们通过使用高质量图像微调空间模块，将分布转移到更高的质量，而不会导致运动退化，从而产生通用的高质量视频模型。进行评估以证明所提出方法的优越性，特别是在图像质量、运动和概念构成方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.09047v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Vlogger: Make Your Dream A Vlog**<br />
**Title_cn:** 视频博主：让你的梦想成为视频博客<br />
**Authors:** Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang<br />
**Abstract:** <details><summary>原文: </summary>In this work, we present Vlogger, a generic AI system for generating a minute-level video blog (i.e., vlog) of user descriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog professionals, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. Moreover, we introduce a novel video diffusion model, ShowMaker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its capacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor. The code and model is all available at https://github.com/zhuangshaobin/Vlogger.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了 Vlogger，这是一种通用人工智能系统，用于生成用户描述的分钟级视频博客（即 vlog）。与几秒钟的短视频不同，vlog往往包含复杂的故事情节和多样化的场景，这对大多数现有的视频生成方法来说是一个挑战。为了突破这个瓶颈，我们的Vlogger巧妙地利用大语言模型（LLM）作为Director，将vlog的长视频生成任务分解为四个关键阶段，我们调用各种基础模型来扮演vlog专业人士的关键角色，包括（1 ) 剧本、(2) 演员、(3) 表演者和 (4) 配音者。通过这样模仿人类的设计，我们的Vlogger可以通过自上而下的规划和自下而上的拍摄的可解释的配合来生成视频博客。此外，我们引入了一种新颖的视频传播模型 ShowMaker，它在我们的 Vlogger 中充当摄像师，用于生成每个拍摄场景的视频片段。通过精心地将Script和Actor作为文本和视觉提示结合起来，可以有效增强片段中的时空连贯性。此外，我们为 ShowMaker 设计了一个简洁的混合训练范例，提高了其 T2V 生成和预测的能力。最后，大量实验表明，我们的方法在零样本 T2V 生成和预测任务上实现了最先进的性能。更重要的是，Vlogger 可以根据开放世界的描述生成超过 5 分钟的视频博客，而不会损失脚本和演员的视频连贯性。代码和模型均可在https://github.com/zhuangshaobin/Vlogger获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09414v1><br />
**Code:** <https://github.com/zhuangshaobin/vlogger>**<br />
>>**index:** 2<br />
**Title:** **Diverse Part Synthesis for 3D Shape Creation**<br />
**Title_cn:** 用于创建 3D 形状的多种零件合成<br />
**Authors:** Yanran Guan, Oliver van Kaick<br />
**Abstract:** <details><summary>原文: </summary>Methods that use neural networks for synthesizing 3D shapes in the form of a part-based representation have been introduced over the last few years. These methods represent shapes as a graph or hierarchy of parts and enable a variety of applications such as shape sampling and reconstruction. However, current methods do not allow easily regenerating individual shape parts according to user preferences. In this paper, we investigate techniques that allow the user to generate multiple, diverse suggestions for individual parts. Specifically, we experiment with multimodal deep generative models that allow sampling diverse suggestions for shape parts and focus on models which have not been considered in previous work on shape synthesis. To provide a comparative study of these techniques, we introduce a method for synthesizing 3D shapes in a part-based representation and evaluate all the part suggestion techniques within this synthesis method. In our method, which is inspired by previous work, shapes are represented as a set of parts in the form of implicit functions which are then positioned in space to form the final shape. Synthesis in this representation is enabled by a neural network architecture based on an implicit decoder and a spatial transformer. We compare the various multimodal generative models by evaluating their performance in generating part suggestions. Our contribution is to show with qualitative and quantitative evaluations which of the new techniques for multimodal part generation perform the best and that a synthesis method based on the top-performing techniques allows the user to more finely control the parts that are generated in the 3D shapes while maintaining high shape fidelity when reconstructing shapes.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用神经网络以基于零件的表示形式合成 3D 形状的方法在过去几年中已被引入。这些方法将形状表示为图形或零件层次结构，并支持各种应用，例如形状采样和重建。然而，当前的方法不允许根据用户偏好轻松地重新生成各个形状部分。在本文中，我们研究了允许用户为各个部件生成多个不同建议的技术。具体来说，我们试验了多模态深度生成模型，该模型允许对形状零件的不同建议进行采样，并重点关注先前形状合成工作中未考虑的模型。为了对这些技术进行比较研究，我们引入了一种在基于零件的表示中合成 3D 形状的方法，并评估了该合成方法中的所有零件建议技术。在我们的方法中，受先前工作的启发，形状被表示为隐函数形式的一组部分，然后将其定位在空间中以形成最终形状。这种表示形式的综合是通过基于隐式解码器和空间变换器的神经网络架构实现的。我们通过评估各种多模态生成模型在生成零件建议方面的性能来比较它们。我们的贡献是通过定性和定量评估来展示哪些多模态零件生成新技术表现最好，并且基于顶级性能技术的合成方法允许用户更精细地控制以 3D 形状生成的零件同时在重建形状时保持高形状保真度。</details>
**PDF:** <http://arxiv.org/pdf/2401.09384v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Training-Free Semantic Video Composition via Pre-trained Diffusion Model**<br />
**Title_cn:** 通过预训练扩散模型进行免训练语义视频合成<br />
**Authors:** Jiaqi Guo, Sitong Su, Junchen Zhu, Lianli Gao, Jingkuan Song<br />
**Abstract:** <details><summary>原文: </summary>The video composition task aims to integrate specified foregrounds and backgrounds from different videos into a harmonious composite. Current approaches, predominantly trained on videos with adjusted foreground color and lighting, struggle to address deep semantic disparities beyond superficial adjustments, such as domain gaps. Therefore, we propose a training-free pipeline employing a pre-trained diffusion model imbued with semantic prior knowledge, which can process composite videos with broader semantic disparities. Specifically, we process the video frames in a cascading manner and handle each frame in two processes with the diffusion model. In the inversion process, we propose Balanced Partial Inversion to obtain generation initial points that balance reversibility and modifiability. Then, in the generation process, we further propose Inter-Frame Augmented attention to augment foreground continuity across frames. Experimental results reveal that our pipeline successfully ensures the visual harmony and inter-frame coherence of the outputs, demonstrating efficacy in managing broader semantic disparities.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频合成任务旨在将不同视频中指定的前景和背景整合成一个和谐的组合。目前的方法主要是在调整了前景颜色和光照的视频上进行训练，很难解决除表面调整之外的深层语义差异，例如域间隙。因此，我们提出了一种免训练的管道，采用充满语义先验知识的预训练扩散模型，它可以处理具有更广泛语义差异的复合视频。具体来说，我们以级联方式处理视频帧，并使用扩散模型在两个进程中处理每个帧。在反演过程中，我们提出平衡部分反演以获得平衡可逆性和可修改性的生成初始点。然后，在生成过程中，我们进一步提出帧间增强注意力以增强跨帧的前景连续性。实验结果表明，我们的流程成功地确保了输出的视觉和谐和帧间连贯性，证明了管理更广泛的语义差异的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09195v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder**<br />
**Title_cn:** 通过变分自动编码器中的受控解缠实现无监督多域翻译<br />
**Authors:** Almudévar Antonio, Mariotte Théo, Ortega Alfonso, Tahon Marie<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the other one hardly contains any domain information.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督多域翻译是一种将数据从一个域转换到其他域的任务，无需配对数据来训练系统。通常，基于生成对抗网络 (GAN) 的方法用于解决此任务。然而，我们的建议完全依赖于变分自动编码器的修改版本。这种修改包括使用通过设计以受控方式解开的两个潜在变量。这些潜在变量之一被强加为完全依赖于域，而另一个必须依赖于数据的其余可变性因素。此外，对域潜在变量施加的条件可以更好地控制和理解潜在空间。我们凭经验证明我们的方法适用于不同的视觉数据集，提高了其他众所周知方法的性能。最后，我们证明，实际上，其中一个潜在变量存储了与领域相关的所有信息，而另一个潜在变量几乎不包含任何领域信息。</details>
**PDF:** <http://arxiv.org/pdf/2401.09180v1><br />
**Code:** <https://github.com/antonioalmudevar/variational_translation>**<br />
>>**index:** 5<br />
**Title:** **Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis**<br />
**Title_cn:** 组合与征服：基于扩散的 3D 深度感知可组合图像合成<br />
**Authors:** Jonghyun Lee, Hansam Cho, Youngjoon Yoo, Seoung Bum Kim, Yonghyun Jeong<br />
**Abstract:** <details><summary>原文: </summary>Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce \textit{depth disentanglement training} to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce \textit{soft guidance}, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, \textsc{Compose and Conquer (CnC)}, unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. Code: https://github.com/tomtom1103/compose-and-conquer/</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决文本作为文本条件扩散模型中准确布局表示的来源的局限性，许多作品结合了额外的信号来调节生成图像中的某些属性。尽管取得了成功，但以前的工作并未考虑到扩展到三维平面的所述属性的具体定位。在这种情况下，我们提出了一种条件扩散模型，该模型将对三维对象放置的控制与来自多个示例图像的全局风格语义的解开表示相结合。具体来说，我们首先引入 \textit{深度解纠缠训练} 以利用对象的相对深度作为估计器，允许模型通过使用合成图像三元组来识别看不见的对象的绝对位置。我们还引入了 \textit{softguiding}，一种在不使用任何额外的本地化提示的情况下将全局语义强加到目标区域的方法。我们的集成框架 \textsc{Compose and Conquer (CnC)} 统一了这些技术，以一种解开的方式定位多个条件。我们证明，我们的方法允许感知不同深度的对象，同时提供一个通用框架来组合具有不同全局语义的局部对象。代码：https://github.com/tomtom1103/compose-and-conquer/</details>
**PDF:** <http://arxiv.org/pdf/2401.09048v1><br />
**Code:** <https://github.com/tomtom1103/compose-and-conquer>**<br />
>>**index:** 6<br />
**Title:** **3D Human Pose Analysis via Diffusion Synthesis**<br />
**Title_cn:** 通过扩散合成进行 3D 人体姿势分析<br />
**Authors:** Haorui Ji, Hongdong Li<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have demonstrated remarkable success in generative modeling. In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel framework designed to address various challenges in 3D human pose analysis through a unified pipeline. Central to PADS are two distinctive strategies: i) learning a task-agnostic pose prior using a diffusion synthesis process to effectively capture the kinematic constraints in human pose data, and ii) unifying multiple pose analysis tasks like estimation, completion, denoising, etc, as instances of inverse problems. The learned pose prior will be treated as a regularization imposing on task-specific constraints, guiding the optimization process through a series of conditional denoising steps. PADS represents the first diffusion-based framework for tackling general 3D human pose analysis within the inverse problem framework. Its performance has been validated on different benchmarks, signaling the adaptability and robustness of this pipeline.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在生成建模中取得了显着的成功。在本文中，我们提出了 PADS（扩散合成姿势分析），这是一种新颖的框架，旨在通过统一的管道解决 3D 人体姿势分析中的各种挑战。 PADS 的核心是两个独特的策略：i) 使用扩散合成过程先学习与任务无关的姿势，以有效捕获人体姿势数据中的运动学约束，ii) 统一多个姿势分析任务，如估计、完成、去噪等，作为反问题的实例。学习到的姿势先验将被视为对特定于任务的约束施加的正则化，通过一系列条件去噪步骤指导优化过程。 PADS 代表了第一个基于扩散的框架，用于在逆问题框架内处理一般 3D 人体姿势分析。其性能已在不同的基准上得到验证，表明该管道的适应性和稳健性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08930v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Uncertainty-aware No-Reference Point Cloud Quality Assessment**<br />
**Title_cn:** 不确定性感知无参考点云质量评估<br />
**Authors:** Songlin Fan, Zixuan Guo, Wei Gao, Ge Li<br />
**Abstract:** <details><summary>原文: </summary>The evolution of compression and enhancement algorithms necessitates an accurate quality assessment for point clouds. Previous works consistently regard point cloud quality assessment (PCQA) as a MOS regression problem and devise a deterministic mapping, ignoring the stochasticity in generating MOS from subjective tests. Besides, the viewpoint switching of 3D point clouds in subjective tests reinforces the judging stochasticity of different subjects compared with traditional images. This work presents the first probabilistic architecture for no-reference PCQA, motivated by the labeling process of existing datasets. The proposed method can model the quality judging stochasticity of subjects through a tailored conditional variational autoencoder (CVAE) and produces multiple intermediate quality ratings. These intermediate ratings simulate the judgments from different subjects and are then integrated into an accurate quality prediction, mimicking the generation process of a ground truth MOS. Specifically, our method incorporates a Prior Module, a Posterior Module, and a Quality Rating Generator, where the former two modules are introduced to model the judging stochasticity in subjective tests, while the latter is developed to generate diverse quality ratings. Extensive experiments indicate that our approach outperforms previous cutting-edge methods by a large margin and exhibits gratifying cross-dataset robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>压缩和增强算法的发展需要对点云进行准确的质量评估。以前的工作一致将点云质量评估（PCQA）视为MOS回归问题，并设计确定性映射，忽略了从主观测试生成MOS的随机性。此外，与传统图像相比，主观测试中3D点云的视点切换增强了不同主体的判断随机性。这项工作提出了第一个无参考 PCQA 的概率架构，其动机是现有数据集的标记过程。该方法可以通过定制的条件变分自动编码器（CVAE）对受试者的质量判断随机性进行建模，并产生多个中间质量评级。这些中间评级模拟了不同主体的判断，然后集成到准确的质量预测中，模仿了真实 MOS 的生成过程。具体来说，我们的方法包含先验模块、后验模块和质量评级生成器，其中引入前两个模块来对主观测试中的判断随机性进行建模，而后者则用于生成不同的质量评级。大量的实验表明，我们的方法大大优于以前的尖端方法，并表现出令人满意的跨数据集鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2401.08926v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Idempotence and Perceptual Image Compression**<br />
**Title_cn:** 幂等性和感知图像压缩<br />
**Authors:** Tongda Xu, Ziran Zhu, Dailan He, Yanghao Li, Lina Guo, Yuanyuan Wang, Zhe Wang, Hongwei Qin, Yan Wang, Jingjing Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Idempotence is the stability of image codec to re-compression. At the first glance, it is unrelated to perceptual image compression. However, we find that theoretically: 1) Conditional generative model-based perceptual codec satisfies idempotence; 2) Unconditional generative model with idempotence constraint is equivalent to conditional generative codec. Based on this newfound equivalence, we propose a new paradigm of perceptual image codec by inverting unconditional generative model with idempotence constraints. Our codec is theoretically equivalent to conditional generative codec, and it does not require training new models. Instead, it only requires a pre-trained mean-square-error codec and unconditional generative model. Empirically, we show that our proposed approach outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of Fr\'echet Inception Distance (FID). The source code is provided in https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression.</details>
**Abstract_cn:** <details><summary>译文: </summary>幂等性是图像编解码器对重新压缩的稳定性。乍一看，它与感知图像压缩无关。然而，我们发现理论上：1）基于条件生成模型的感知编解码器满足幂等性； 2）具有幂等约束的无条件生成模型相当于条件生成编解码器。基于这种新发现的等价性，我们通过反转具有幂等约束的无条件生成模型，提出了一种新的感知图像编解码器范式。我们的编解码器理论上相当于条件生成编解码器，并且不需要训练新模型。相反，它只需要预先训练的均方误差编解码器和无条件生成模型。根据经验，我们表明，就Fr\'echet Inception Distance (FID) 而言，我们提出的方法优于 HiFiC 和 ILLM 等最先进的方法。源代码在 https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression 中提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.08920v1><br />
**Code:** <https://github.com/tongdaxu/idempotence-and-perceptual-image-compression>**<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects**<br />
**Title_cn:** SM$^3$：针对铰接物体的多视图 2D 图像的自监督多任务建模<br />
**Authors:** Haowen Wang, Zhen Zhao, Zhao Jin, Zhengping Che, Liang Qiao, Yakun Huang, Zhipeng Fan, Xiuquan Qiao, Jian Tang<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing real-world objects and estimating their movable joint structures are pivotal technologies within the field of robotics. Previous research has predominantly focused on supervised approaches, relying on extensively annotated datasets to model articulated objects within limited categories. However, this approach falls short of effectively addressing the diversity present in the real world. To tackle this issue, we propose a self-supervised interaction perception method, referred to as SM$^3$, which leverages multi-view RGB images captured before and after interaction to model articulated objects, identify the movable parts, and infer the parameters of their rotating joints. By constructing 3D geometries and textures from the captured 2D images, SM$^3$ achieves integrated optimization of movable part and joint parameters during the reconstruction process, obviating the need for annotations. Furthermore, we introduce the MMArt dataset, an extension of PartNet-Mobility, encompassing multi-view and multi-modal data of articulated objects spanning diverse categories. Evaluations demonstrate that SM$^3$ surpasses existing benchmarks across various categories and objects, while its adaptability in real-world scenarios has been thoroughly validated.</details>
**Abstract_cn:** <details><summary>译文: </summary>重建现实世界的物体并估计其可移动关节结构是机器人领域的关键技术。以前的研究主要集中在监督方法上，依靠广泛注释的数据集来对有限类别内的铰接对象进行建模。然而，这种方法无法有效解决现实世界中存在的多样性。为了解决这个问题，我们提出了一种自监督交互感知方法，称为 SM$^3$，它利用交互前后捕获的多视图 RGB 图像来建模关节对象，识别可移动部件并推断参数他们的旋转关节。通过从捕获的 2D 图像构建 3D 几何形状和纹理，SM$^3$ 在重建过程中实现了可移动部件和关节参数的集成优化，从而无需注释。此外，我们还介绍了 MMArt 数据集，它是 PartNet-Mobility 的扩展，包含跨越不同类别的铰接对象的多视图和多模态数据。评估表明，SM$^3$ 超越了各个类别和对象的现有基准，同时其在现实场景中的适应性也得到了充分验证。</details>
**PDF:** <http://arxiv.org/pdf/2401.09133v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Autonomous Catheterization with Open-source Simulator and Expert Trajectory**<br />
**Title_cn:** 使用开源模拟器和专家轨迹进行自主导尿<br />
**Authors:** Tudor Jianu, Baoru Huang, Tuan Vo, Minh Nhat Vu, Jingxuan Kang, Hoan Nguyen, Olatunji Omisore, Pierre Berthet-Rayne, Sebastiano Fichera, Anh Nguyen<br />
**Abstract:** <details><summary>原文: </summary>Endovascular robots have been actively developed in both academia and industry. However, progress toward autonomous catheterization is often hampered by the widespread use of closed-source simulators and physical phantoms. Additionally, the acquisition of large-scale datasets for training machine learning algorithms with endovascular robots is usually infeasible due to expensive medical procedures. In this chapter, we introduce CathSim, the first open-source simulator for endovascular intervention to address these limitations. CathSim emphasizes real-time performance to enable rapid development and testing of learning algorithms. We validate CathSim against the real robot and show that our simulator can successfully mimic the behavior of the real robot. Based on CathSim, we develop a multimodal expert navigation network and demonstrate its effectiveness in downstream endovascular navigation tasks. The intensive experimental results suggest that CathSim has the potential to significantly accelerate research in the autonomous catheterization field. Our project is publicly available at https://github.com/airvlab/cathsim.</details>
**Abstract_cn:** <details><summary>译文: </summary>血管内机器人在学术界和工业界都得到了积极的发展。然而，自主导管插入术的进展常常受到闭源模拟器和物理模型的广泛使用的阻碍。此外，由于昂贵的医疗程序，获取用于训练血管内机器人机器学习算法的大规模数据集通常是不可行的。在本章中，我们介绍 CathSim，这是第一个用于血管内介入的开源模拟器，旨在解决这些局限性。 CathSim 强调实时性能，以实现学习算法的快速开发和测试。我们针对真实机器人验证了 CathSim，并表明我们的模拟器可以成功模仿真实机器人的行为。基于CathSim，我们开发了多模态专家导航网络，并证明了其在下游血管内导航任务中的有效性。密集的实验结果表明，CathSim 有潜力显着加速自主导管插入领域的研究。我们的项目已公开发布于 https://github.com/airvlab/cathsim。</details>
**PDF:** <http://arxiv.org/pdf/2401.09059v1><br />
**Code:** <https://github.com/airvlab/cathsim>**<br />
>>**index:** 3<br />
**Title:** **Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading**<br />
**Title_cn:** 具有双重关注的跨模态指导辅助多模态学习用于 MRI 脑肿瘤分级<br />
**Authors:** Dunyuan Xu, Xi Wang, Jinyue Cai, Pheng-Ann Heng<br />
**Abstract:** <details><summary>原文: </summary>Brain tumor represents one of the most fatal cancers around the world, and is very common in children and the elderly. Accurate identification of the type and grade of tumor in the early stages plays an important role in choosing a precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of different sequences provide clinicians with important contradictory information to identify tumor regions. However, manual assessment is time-consuming and error-prone due to big amount of data and the diversity of brain tumor types. Hence, there is an unmet need for MRI automated brain tumor diagnosis. We observe that the predictive capability of uni-modality models is limited and their performance varies widely across modalities, and the commonly used modality fusion methods would introduce potential noise, which results in significant performance degradation. To overcome these challenges, we propose a novel cross-modality guidance-aided multi-modal learning with dual attention for addressing the task of MRI brain tumor grading. To balance the tradeoff between model efficiency and efficacy, we employ ResNet Mix Convolution as the backbone network for feature extraction. Besides, dual attention is applied to capture the semantic interdependencies in spatial and slice dimensions respectively. To facilitate information interaction among modalities, we design a cross-modality guidance-aided module where the primary modality guides the other secondary modalities during the process of training, which can effectively leverage the complementary information of different MRI modalities and meanwhile alleviate the impact of the possible noise.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑肿瘤是世界上最致命的癌症之一，在儿童和老年人中非常常见。早期准确识别肿瘤的类型和分级对于选择精准的治疗方案具有重要作用。不同序列的磁共振成像（MRI）协议为临床医生提供了重要的矛盾信息来识别肿瘤区域。然而，由于数据量大且脑肿瘤类型多样，人工评估耗时且容易出错。因此，MRI 自动脑肿瘤诊断的需求尚未得到满足。我们观察到单模态模型的预测能力有限，并且其性能在不同模态之间差异很大，并且常用的模态融合方法会引入潜在的噪声，从而导致性能显着下降。为了克服这些挑战，我们提出了一种新颖的跨模态指导辅助多模态学习，具有双重注意力，用于解决 MRI 脑肿瘤分级的任务。为了平衡模型效率和功效之间的权衡，我们采用 ResNet Mix Convolution 作为特征提取的骨干网络。此外，应用双重注意力来分别捕获空间和切片维度上的语义相互依赖性。为了促进模态之间的信息交互，我们设计了一个跨模态引导辅助模块，其中主要模态在训练过程中指导其他次要模态，可以有效利用不同MRI模态的互补信息，同时减轻不同模态的影响。可能的噪音。</details>
**PDF:** <http://arxiv.org/pdf/2401.09029v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **COCO is "ALL'' You Need for Visual Instruction Fine-tuning**<br />
**Title_cn:** COCO 是视觉指令微调所需的“全部”<br />
**Authors:** Xiaotian Han, Yiqi Wang, Bohan Zhai, Quanzeng You, Hongxia Yang<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal Large Language Models (MLLMs) are increasingly prominent in the field of artificial intelligence. Visual instruction fine-tuning (IFT) is a vital process for aligning MLLMs' output with user's intentions. High-quality and diversified instruction following data is the key to this fine-tuning process. Recent studies propose to construct visual IFT datasets through a multifaceted approach: transforming existing datasets with rule-based templates, employing GPT-4 for rewriting annotations, and utilizing GPT-4V for visual dataset pseudo-labeling. LLaVA-1.5 adopted similar approach and construct LLaVA-mix-665k, which is one of the simplest, most widely used, yet most effective IFT datasets today. Notably, when properly fine-tuned with this dataset, MLLMs can achieve state-of-the-art performance on several benchmarks. However, we noticed that models trained with this dataset often struggle to follow user instructions properly in multi-round dialog. In addition, tradition caption and VQA evaluation benchmarks, with their closed-form evaluation structure, are not fully equipped to assess the capabilities of modern open-ended generative MLLMs. This problem is not unique to the LLaVA-mix-665k dataset, but may be a potential issue in all IFT datasets constructed from image captioning or VQA sources, though the extent of this issue may vary. We argue that datasets with diverse and high-quality detailed instruction following annotations are essential and adequate for MLLMs IFT. In this work, we establish a new IFT dataset, with images sourced from the COCO dataset along with more diverse instructions. Our experiments show that when fine-tuned with out proposed dataset, MLLMs achieve better performance on open-ended evaluation benchmarks in both single-round and multi-round dialog setting.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型（MLLM）在人工智能领域日益突出。视觉指令微调 (IFT) 是将 MLLM 的输出与用户意图保持一致的重要过程。高质量、多样化的指令跟踪数据是这一微调过程的关键。最近的研究提出通过多方面的方法构建视觉 IFT 数据集：使用基于规则的模板转换现有数据集，使用 GPT-4 重写注释，以及使用 GPT-4V 进行视觉数据集伪标记。 LLaVA-1.5采用了类似的方法并构建了LLaVA-mix-665k，这是当今最简单、使用最广泛、但最有效的IFT数据集之一。值得注意的是，当使用该数据集进行适当微调时，MLLM 可以在多个基准测试中实现最先进的性能。然而，我们注意到使用该数据集训练的模型通常很难在多轮对话中正确遵循用户指令。此外，传统的标题和 VQA 评估基准及其封闭式评估结构，不完全适合评估现代开放式生成 MLLM 的能力。此问题并非 LLaVA-mix-665k 数据集独有，但可能是从图像字幕或 VQA 源构建的所有 IFT 数据集中的潜在问题，尽管此问题的程度可能有所不同。我们认为，具有多样化且高质量的详细说明（遵循注释）的数据集对于 MLLM IFT 来说是必要且充分的。在这项工作中，我们建立了一个新的 IFT 数据集，其中包含来自 COCO 数据集的图像以及更多样化的指令。我们的实验表明，当使用我们提出的数据集进行微调时，MLLM 在单轮和多轮对话设置中的开放式评估基准上都能取得更好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08968v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models**<br />
**Title_cn:** 遥感 ChatGPT：使用 ChatGPT 和视觉模型解决遥感任务<br />
**Authors:** Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang, Deren Li<br />
**Abstract:** <details><summary>原文: </summary>Recently, the flourishing large language models(LLM), especially ChatGPT, have shown exceptional performance in language understanding, reasoning, and interaction, attracting users and researchers from multiple fields and domains. Although LLMs have shown great capacity to perform human-like task accomplishment in natural language and natural image, their potential in handling remote sensing interpretation tasks has not yet been fully explored. Moreover, the lack of automation in remote sensing task planning hinders the accessibility of remote sensing interpretation techniques, especially to non-remote sensing experts from multiple research fields. To this end, we present Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to connect various AI-based remote sensing models to solve complicated interpretation tasks. More specifically, given a user request and a remote sensing image, we utilized ChatGPT to understand user requests, perform task planning according to the tasks' functions, execute each subtask iteratively, and generate the final response according to the output of each subtask. Considering that LLM is trained with natural language and is not capable of directly perceiving visual concepts as contained in remote sensing images, we designed visual cues that inject visual information into ChatGPT. With Remote Sensing ChatGPT, users can simply send a remote sensing image with the corresponding request, and get the interpretation results as well as language feedback from Remote Sensing ChatGPT. Experiments and examples show that Remote Sensing ChatGPT can tackle a wide range of remote sensing tasks and can be extended to more tasks with more sophisticated models such as the remote sensing foundation model. The code and demo of Remote Sensing ChatGPT is publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，蓬勃发展的大型语言模型（LLM），特别是ChatGPT，在语言理解、推理和交互方面表现出了卓越的性能，吸引了来自多个领域的用户和研究人员。尽管法学硕士在自然语言和自然图像方面表现出了类似于人类的任务完成能力，但其在处理遥感解译任务方面的潜力尚未得到充分开发。此外，遥感任务规划缺乏自动化阻碍了遥感解译技术的可及性，特别是对于来自多个研究领域的非遥感专家而言。为此，我们推出了遥感 ChatGPT，这是一种由 LLM 驱动的代理，利用 ChatGPT 连接各种基于人工智能的遥感模型来解决复杂的解释任务。更具体地说，给定用户请求和遥感图像，我们利用 ChatGPT 来理解用户请求，根据任务功能进行任务规划，迭代执行每个子任务，并根据每个子任务的输出生成最终响应。考虑到 LLM 是用自然语言训练的，无法直接感知遥感图像中包含的视觉概念，我们设计了视觉线索，将视觉信息注入 ChatGPT。通过遥感ChatGPT，用户只需发送带有相应请求的遥感图像，即可从遥感ChatGPT获得解译结果以及语言反馈。实验和示例表明，遥感ChatGPT可以处理广泛的遥感任务，并且可以通过更复杂的模型（例如遥感基础模型）扩展到更多任务。遥感ChatGPT的代码和演示已公开在https://github.com/HaonanGuo/Remote-Sensing-ChatGPT。</details>
**PDF:** <http://arxiv.org/pdf/2401.09083v1><br />
**Code:** <https://github.com/haonanguo/remote-sensing-chatgpt>**<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery**<br />
**Title_cn:** DaFoE：混合数据集以推广微创机器人手术中的视觉状态深度学习力估计<br />
**Authors:** Mikel De Iturrate Reyzabal, Mingcong Chen, Wei Huang, Sebastien Ourselin, Hongbin Liu<br />
**Abstract:** <details><summary>原文: </summary>Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decoder, named with the prefix R, and a new temporal sampling to represent the acceleration of the tool. During our training, we demonstrate that single dataset training tends to overfit to the training data domain, but has difficulties on translating the results across new domains. However, dataset mixing presents a good translation with a mean relative estimated force error of 5% and 12% for the recurrent and non-recurrent models respectively. Our method, also marginally increase the effectiveness of transformers for force estimation up to a maximum of ~15%, as the volume of available data is increase by 150%. In conclusion, we demonstrate that mixing experimental set ups for vision-state force estimation in MIRS is a possible approach towards the general solution of the problem.</details>
**Abstract_cn:** <details><summary>译文: </summary>精确确定微创机器人手术 (MIRS) 中安全交互期间的接触力仍然是一个开放的研究挑战。受手术视频术后定性分析的启发，使用跨模态数据驱动的深度神经网络模型已成为预测无传感器力趋势的最新方法之一。然而，这些方法需要当前不可用的大型且可变的数据集。在本文中，我们提出了一个新的视觉触觉数据集（DaFoEs），具有可变的软环境，用于训练深度神经模型。为了减少单个数据集的偏差，我们提出了一个管道，使用先前经过验证的具有不同设置的数据集来概括混合数据集训练的不同视觉和状态数据输入。最后，我们提出了一种可变编码器-解码器架构，以使用单个输入或输入序列来预测腹腔镜工具所产生的力。对于输入序列，我们使用一个以前缀 R 命名的循环解码器，以及一个新的时间采样来表示工具的加速度。在我们的训练过程中，我们证明单个数据集训练往往会过度拟合训练数据域，但在跨新域转换结果时遇到困难。然而，数据集混合提供了良好的转换，循环模型和非循环模型的平均相对估计力误差分别为 5% 和 12%。我们的方法还略微提高了变压器力估计的有效性，最高可达约 15%，因为可用数据量增加了 150%。总之，我们证明了 MIRS 中视觉状态力估计的混合实验设置是解决该问题的一种可能方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.09239v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **UniVG: Towards UNIfied-modal Video Generation**<br />
**Title_cn:** UniVG：迈向统一模态视频生成<br />
**Authors:** Ludan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao<br />
**Abstract:** <details><summary>原文: </summary>Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Genearation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fr\'echet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit https://univg-baidu.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散的视频生成已受到广泛关注，并在学术界和工业界取得了相当大的成功。然而，目前的工作主要集中在单目标或单任务视频生成，例如由文本、图像或文本和图像的组合驱动的生成。这不能完全满足现实应用场景的需求，因为用户可能会以灵活的方式单独或组合输入图像和文本条件。为了解决这个问题，我们提出了一种统一模式视频生成系统，该系统能够处理跨文本和图像模式的多个视频生成任务。为此，我们从生成自由度的角度重新审视系统内的各种视频生成任务，并将它们分为高自由度和低自由度视频生成类别。对于高自由度视频生成，我们采用多条件交叉注意力来生成与输入图像或文本的语义一致的视频。对于低自由度视频生成，我们引入偏置高斯噪声来代替纯随机高斯噪声，这有助于更好地保留输入条件的内容。我们的方法在公共学术基准MSR-VTT上实现了最低的Fr\'echet视频距离（FVD），在人类评估中超越了当前的开源方法，并且与当前的闭源方法Gen2相当。更多示例请访问https://univg-baidu.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2401.09084v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Efficient Image Super-Resolution via Symmetric Visual Attention Network**<br />
**Title_cn:** 通过对称视觉注意网络实现高效图像超分辨率<br />
**Authors:** Chengxu Wu, Qinrui Fan, Shu Hu, Xi Wu, Xin Wang, Jing Hu<br />
**Abstract:** <details><summary>原文: </summary>An important development direction in the Single-Image Super-Resolution (SISR) algorithms is to improve the efficiency of the algorithms. Recently, efficient Super-Resolution (SR) research focuses on reducing model complexity and improving efficiency through improved deep small kernel convolution, leading to a small receptive field. The large receptive field obtained by large kernel convolution can significantly improve image quality, but the computational cost is too high. To improve the reconstruction details of efficient super-resolution reconstruction, we propose a Symmetric Visual Attention Network (SVAN) by applying large receptive fields. The SVAN decomposes a large kernel convolution into three different combinations of convolution operations and combines them with an attention mechanism to form a Symmetric Large Kernel Attention Block (SLKAB), which forms a symmetric attention block with a bottleneck structure by the size of the receptive field in the convolution combination to extract depth features effectively as the basic component of the SVAN. Our network gets a large receptive field while minimizing the number of parameters and improving the perceptual ability of the model. The experimental results show that the proposed SVAN can obtain high-quality super-resolution reconstruction results using only about 30% of the parameters of existing SOTA methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像超分辨率（SISR）算法的一个重要发展方向是提高算法的效率。最近，高效的超分辨率（SR）研究重点是通过改进深度小核卷积来降低模型复杂性并提高效率，从而形成较小的感受野。大核卷积获得的大感受野可以显着提高图像质量，但计算成本太高。为了改善高效超分辨率重建的重建细节，我们通过应用大感受野提出了对称视觉注意网络（SVAN）。 SVAN将大核卷积分解为三种不同的卷积运算组合，并将它们与注意力机制结合起来，形成对称大核注意力块（SLKAB），SLKAB通过感受野的大小形成具有瓶颈结构的对称注意力块在卷积组合中有效地提取深度特征作为SVAN的基本组成部分。我们的网络获得了较大的感受野，同时最大限度地减少了参数数量并提高了模型的感知能力。实验结果表明，所提出的SVAN仅使用现有SOTA方法约30%的参数即可获得高质量的超分辨率重建结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.08913v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization**<br />
**Title_cn:** ICON：联合姿势和辐射场优化的增量置信度<br />
**Authors:** Weiyao Wang, Pierre Gleize, Hao Tang, Xingyu Chen, Kevin J Liang, Matt Feiszli<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.</details>
**Abstract_cn:** <details><summary>译文: </summary>给定一组 2D 图像，神经辐射场 (NeRF) 在新视图合成 (NVS) 方面表现出卓越的性能。然而，NeRF 训练需要每个输入视图的准确相机姿态，通常通过运动结构 (SfM) 管道获得。最近的作品试图放松这一限制，但它们仍然经常依赖于可以改进的合适的初始姿势。这里我们的目标是消除姿势初始化的要求。我们提出了增量置信度 (ICON)，这是一种从 2D 视频帧训练 NeRF 的优化程序。 ICON 仅假设平滑的相机运动来估计姿势的初始猜测。此外，ICON 引入了“置信度”：一种用于动态重新加权梯度的模型质量自适应度量。ICON 依靠高置信度姿势来学习 NeRF，以及高置信度 3D 结构（由 NeRF 编码）来学习姿势。我们展示了与使用 SfM 姿势的方法相比，ICON 在没有事先姿势初始化的情况下，在 CO3D 和 HO3D 中都实现了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.08937v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Tri$^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid**<br />
**Title_cn:** Tri$^{2}$-plane：利用特征金字塔重建体积头像<br />
**Authors:** Luchuan Song, Pinxin Liu, Lele Chen, Celong Liu, Chenliang Xu<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed considerable achievements in facial avatar reconstruction with neural volume rendering. Despite notable advancements, the reconstruction of complex and dynamic head movements from monocular videos still suffers from capturing and restoring fine-grained details. In this work, we propose a novel approach, named Tri$^2$-plane, for monocular photo-realistic volumetric head avatar reconstructions. Distinct from the existing works that rely on a single tri-plane deformation field for dynamic facial modeling, the proposed Tri$^2$-plane leverages the principle of feature pyramids and three top-to-down lateral connections tri-planes for details improvement. It samples and renders facial details at multiple scales, transitioning from the entire face to specific local regions and then to even more refined sub-regions. Moreover, we incorporate a camera-based geometry-aware sliding window method as an augmentation in training, which improves the robustness beyond the canonical space, with a particular improvement in cross-identity generation capabilities. Experimental outcomes indicate that the Tri$^2$-plane not only surpasses existing methodologies but also achieves superior performance across both quantitative metrics and qualitative assessments through experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，利用神经体积渲染进行面部头像重建取得了相当大的成就。尽管取得了显着的进步，但从单目视频重建复杂和动态的头部运动仍然在捕获和恢复细粒度细节方面存在问题。在这项工作中，我们提出了一种名为 Tri$^2$-plane 的新颖方法，用于单眼逼真的体积头部头像重建。与依赖单个三平面变形场进行动态面部建模的现有作品不同，所提出的Tri$^2$平面利用特征金字塔的原理和三个自上而下的横向连接三平面来改进细节。它以多个尺度采样和渲染面部细节，从整个面部过渡到特定的局部区域，然后过渡到更精细的子区域。此外，我们结合了基于相机的几何感知滑动窗口方法作为训练的增强，这提高了规范空间之外的鲁棒性，特别是跨身份生成能力的提高。实验结果表明，Tri$^2$平面不仅超越了现有方法，而且通过实验在定量指标和定性评估方面都取得了优异的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.09386v1><br />
**Code:** <https://github.com/songluchuan/tri2plane>**<br />
>>**index:** 2<br />
**Title:** **SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding**<br />
**Title_cn:** SceneVerse：扩展 3D 视觉语言学习以实现基础场景理解<br />
**Authors:** Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang<br />
**Abstract:** <details><summary>原文: </summary>3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: https://scene-verse.github.io .</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 视觉语言基础侧重于使语言与 3D 物理环境保持一致，是实体智能体开发的基石。与 2D 领域的最新进展相比，3D 场景中的基础语言面临着几个重大挑战：（i）由于不同的对象配置、丰富的属性和复杂的关系，3D 场景固有的复杂性； (ii) 缺乏支持扎根学习的配对 3D 视觉-语言数据； (iii) 缺乏统一的学习框架来从基础 3D 数据中提取知识。在这项工作中，我们的目标是通过研究在室内环境中系统升级 3D 视觉语言学习的潜力来解决 3D 视觉语言的三大挑战。我们推出了第一个百万级 3D 视觉语言数据集 SceneVerse，包含约 68K 3D 室内场景，并包含源自人类注释和我们的可扩展的基于场景图的生成方法的 250 万个视觉语言对。我们证明，这种扩展可以为 3D 视觉语言学习提供统一的预训练框架，即场景基础预训练 (GPS)。通过大量实验，我们在所有现有 3D 视觉接地基准上实现了最先进的性能，展示了 GPS 的有效性。通过在具有挑战性的 3D 视觉语言任务中进行的零样本传输实验，SceneVerse 和 GPS 的巨大潜力得到了展现。项目网站：https://scene-verse.github.io 。</details>
**PDF:** <http://arxiv.org/pdf/2401.09340v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey**<br />
**Title_cn:** 根据 360$^\circ$ 图像进行 3D 场景几何估计：一项调查<br />
**Authors:** Thiago Lopes Trugillo da Silveira, Paulo Gamarra Lessa Pinto, Jeffri Erwin Murrugarra Llerena, Claudio Rosito Jung<br />
**Abstract:** <details><summary>原文: </summary>This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and discuss commonly adopted datasets and figures of merit indicated for each purpose and list recent results for completeness. We conclude this paper by pointing out current and future trends.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文对基于全向光学器件捕获的单张、两张或多张图像的先驱和最先进的 3D 场景几何估计方法进行了全面的调查。我们首先回顾球形相机模型的基本概念，并回顾适合全向（也称为 360$^\circ$，球形或全景）图像和视频的最常见的采集技术和表示格式。然后，我们调查单目布局和深度推理方法，重点介绍适合球形数据的基于学习的解决方案的最新进展。然后，经典的立体匹配在球形域上进行了修改，其中检测和描述稀疏和密集特征的方法变得至关重要。然后将立体匹配概念推断为多视图相机设置，将它们分类为光场、多视图立体和运动结构（或视觉同步定位和映射）。我们还编译和讨论了为每个目的指定的常用数据集和品质因数，并列出了最新结果以确保完整性。我们通过指出当前和未来的趋势来结束本文。</details>
**PDF:** <http://arxiv.org/pdf/2401.09252v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Continuous Piecewise-Affine Based Motion Model for Image Animation**<br />
**Title_cn:** 用于图像动画的连续分段仿射运动模型<br />
**Authors:** Hexiang Wang, Fengqi Liu, Qianyu Zhou, Ran Yi, Xin Tan, Lizhuang Ma<br />
**Abstract:** <details><summary>原文: </summary>Image animation aims to bring static images to life according to driving videos and create engaging visual content that can be used for various purposes such as animation, entertainment, and education. Recent unsupervised methods utilize affine and thin-plate spline transformations based on keypoints to transfer the motion in driving frames to the source image. However, limited by the expressive power of the transformations used, these methods always produce poor results when the gap between the motion in the driving frame and the source image is large. To address this issue, we propose to model motion from the source image to the driving frame in highly-expressive diffeomorphism spaces. Firstly, we introduce Continuous Piecewise-Affine based (CPAB) transformation to model the motion and present a well-designed inference algorithm to generate CPAB transformation from control keypoints. Secondly, we propose a SAM-guided keypoint semantic loss to further constrain the keypoint extraction process and improve the semantic consistency between the corresponding keypoints on the source and driving images. Finally, we design a structure alignment loss to align the structure-related features extracted from driving and generated images, thus helping the generator generate results that are more consistent with the driving action. Extensive experiments on four datasets demonstrate the effectiveness of our method against state-of-the-art competitors quantitatively and qualitatively. Code will be publicly available at: https://github.com/DevilPG/AAAI2024-CPABMM.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像动画旨在根据驾驶视频将静态图像变为现实，并创建可用于动画、娱乐和教育等各种目的的引人入胜的视觉内容。最近的无监督方法利用基于关键点的仿射和薄板样条变换将驱动帧中的运动转移到源图像。然而，受限于所使用的变换的表达能力，当驱动帧中的运动与源图像之间的差距较大时，这些方法总是产生较差的结果。为了解决这个问题，我们建议在高表达微分同胚空间中对从源图像到驱动框架的运动进行建模。首先，我们引入基于连续分段仿射（CPAB）变换来对运动进行建模，并提出一种精心设计的推理算法来从控制关键点生成 CPAB 变换。其次，我们提出了 SAM 引导的关键点语义损失，以进一步约束关键点提取过程并提高源图像和驱动图像上相应关键点之间的语义一致性。最后，我们设计了结构对齐损失来对齐从驾驶图像和生成图像中提取的结构相关特征，从而帮助生成器生成与驾驶动作更加一致的结果。对四个数据集的广泛实验在数量和质量上证明了我们的方法相对于最先进的竞争对手的有效性。代码将公开发布于：https://github.com/DevilPG/AAAI2024-CPABMM。</details>
**PDF:** <http://arxiv.org/pdf/2401.09146v1><br />
**Code:** <https://github.com/devilpg/aaai2024-cpabmm>**<br />
>>**index:** 5<br />
**Title:** **Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting**<br />
**Title_cn:** 具有照明的对象：用于评估对象重新照明的重建和渲染的真实数据集<br />
**Authors:** Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng, Stephan Richter, Shenlong Wang, German Ros<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing an object from photos and placing it virtually in a new environment goes beyond the standard novel view synthesis task as the appearance of the object has to not only adapt to the novel viewpoint but also to the new lighting conditions and yet evaluations of inverse rendering methods rely on novel view synthesis data or simplistic synthetic datasets for quantitative analysis. This work presents a real-world dataset for measuring the reconstruction and rendering of objects for relighting. To this end, we capture the environment lighting and ground truth images of the same objects in multiple environments allowing to reconstruct the objects from images taken in one environment and quantify the quality of the rendered views for the unseen lighting environments. Further, we introduce a simple baseline composed of off-the-shelf methods and test several state-of-the-art methods on the relighting task and show that novel view synthesis is not a reliable proxy to measure performance. Code and dataset are available at https://github.com/isl-org/objects-with-lighting .</details>
**Abstract_cn:** <details><summary>译文: </summary>从照片重建对象并将其虚拟地放置在新环境中超出了标准的新颖视图合成任务，因为对象的外观不仅必须适应新颖的视点，而且还必须适应新的照明条件以及逆渲染方法的评估依靠新颖的视图合成数据或简单的合成数据集进行定量分析。这项工作提供了一个真实世界的数据集，用于测量重新照明对象的重建和渲染。为此，我们在多个环境中捕获相同对象的环境照明和地面实况图像，从而可以根据在一个环境中拍摄的图像重建对象，并量化不可见照明环境的渲染视图的质量。此外，我们引入了一个由现成方法组成的简单基线，并在重新照明任务上测试了几种最先进的方法，并表明新颖的视图合成并不是衡量性能的可靠指标。代码和数据集可在 https://github.com/isl-org/objects-with-lighting 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09126v1><br />
**Code:** <https://github.com/isl-org/objects-with-lighting>**<br />
>>**index:** 6<br />
**Title:** **Stream Query Denoising for Vectorized HD Map Construction**<br />
**Title_cn:** 用于矢量化高精地图构建的流查询去噪<br />
**Authors:** Shuo Wang, Fan Jia, Yingfei Liu, Yucheng Zhao, Zehui Chen, Tiancai Wang, Chi Zhang, Xiangyu Zhang, Feng Zhao<br />
**Abstract:** <details><summary>原文: </summary>To enhance perception performance in complex and extensive scenarios within the realm of autonomous driving, there has been a noteworthy focus on temporal modeling, with a particular emphasis on streaming methods. The prevailing trend in streaming models involves the utilization of stream queries for the propagation of temporal information. Despite the prevalence of this approach, the direct application of the streaming paradigm to the construction of vectorized high-definition maps (HD-maps) fails to fully harness the inherent potential of temporal information. This paper introduces the Stream Query Denoising (SQD) strategy as a novel approach for temporal modeling in high-definition map (HD-map) construction. SQD is designed to facilitate the learning of temporal consistency among map elements within the streaming model. The methodology involves denoising the queries that have been perturbed by the addition of noise to the ground-truth information from the preceding frame. This denoising process aims to reconstruct the ground-truth information for the current frame, thereby simulating the prediction process inherent in stream queries. The SQD strategy can be applied to those streaming methods (e.g., StreamMapNet) to enhance the temporal modeling. The proposed SQD-MapNet is the StreamMapNet equipped with SQD. Extensive experiments on nuScenes and Argoverse2 show that our method is remarkably superior to other existing methods across all settings of close range and long range. The code will be available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了提高自动驾驶领域复杂而广泛的场景中的感知性能，时间建模受到了值得注意的关注，特别是流方法。流模型的流行趋势涉及利用流查询来传播时间信息。尽管这种方法很流行，但将流范式直接应用于矢量化高清地图（HD-map）的构建未能充分利用时间信息的固有潜力。本文介绍了流查询去噪（SQD）策略作为高清地图（HD-map）构建中时间建模的一种新方法。 SQD 旨在促进流模型内地图元素之间时间一致性的学习。该方法涉及对因前一帧的地面实况信息添加噪声而受到干扰的查询进行去噪。该去噪过程旨在重建当前帧的真实信息，从而模拟流查询中固有的预测过程。 SQD 策略可以应用于那些流方法（例如 StreamMapNet）以增强时间建模。所提出的SQD-MapNet是配备SQD的StreamMapNet。 nuScenes 和 Argoverse2 上的大量实验表明，我们的方法在近距离和远距离的所有设置上都明显优于其他现有方法。该代码即将推出。</details>
**PDF:** <http://arxiv.org/pdf/2401.09112v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization**<br />
**Title_cn:** 攻击和重置以实现遗忘：通过参数重新初始化利用对抗性噪声来实现机器遗忘<br />
**Authors:** Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier<br />
**Abstract:** <details><summary>原文: </summary>With growing concerns surrounding privacy and regulatory compliance, the concept of machine unlearning has gained prominence, aiming to selectively forget or erase specific learned information from a trained model. In response to this critical need, we introduce a novel approach called Attack-and-Reset for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. ARU outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC. In particular, we present the steps involved in attacking and masking that strategically filter and re-initialize network parameters biased towards the forget set. Our work represents a significant advancement in rendering data unexploitable to deep learning models through parameter re-initialization, achieved by harnessing adversarial noise to craft a mask.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着人们对隐私和监管合规性的担忧日益增加，机器遗忘的概念越来越受到重视，旨在有选择地忘记或删除训练模型中特定的学习信息。为了满足这一关键需求，我们引入了一种称为攻击和重置遗忘（ARU）的新颖方法。该算法利用精心设计的对抗性噪声来生成参数掩码，有效地重置某些参数并使它们无法学习。 ARU 在两个面部机器学习基准数据集 MUFAC 和 MUCAC 上的表现优于当前最先进的结果。特别是，我们提出了攻击和屏蔽所涉及的步骤，这些步骤战略性地过滤和重新初始化偏向遗忘集的网络参数。我们的工作代表了通过参数重新初始化使数据无法被深度学习模型利用的重大进步，这是通过利用对抗性噪声来制作掩模来实现的。</details>
**PDF:** <http://arxiv.org/pdf/2401.08998v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency**<br />
**Title_cn:** PIN-SLAM：使用基于点的隐式神经表示实现全球地图一致性的 LiDAR SLAM<br />
**Authors:** Yue Pan, Xingguang Zhong, Louis Wiesmann, Thorbjörn Posewsky, Jens Behley, Cyrill Stachniss<br />
**Abstract:** <details><summary>原文: </summary>Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a SLAM system for building globally consistent maps, called PIN-SLAM, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that PIN-SLAM is robust to various environments and versatile to different range sensors such as LiDAR and RGB-D cameras. PIN-SLAM achieves pose estimation accuracy better or on par with the state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent neural implicit SLAM approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, PIN-SLAM can run at the sensor frame rate on a moderate GPU. Codes will be available at: https://github.com/PRBonn/PIN_SLAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确而强大的定位和地图绘制是大多数自主机器人的重要组成部分。在本文中，我们提出了一种用于构建全局一致地图的 SLAM 系统，称为 PIN-SLAM，该系统基于弹性且紧凑的基于点的隐式神经地图表示。以距离测量作为输入，我们的方法在局部隐式带符号距离场的增量学习和使用无对应、点到隐式模型配准给定当前局部地图的姿态估计之间交替。我们的隐式地图基于稀疏可优化神经点，这些神经点本质上是弹性的，并且在闭环时可随着全局姿态调整而变形。还使用神经点特征来检测循环。大量实验验证了 PIN-SLAM 对于各种环境都具有鲁棒性，并且适用于不同范围的传感器，例如 LiDAR 和 RGB-D 相机。 PIN-SLAM 可以更好地实现位姿估计精度，或者与最先进的 LiDAR 里程计或 SLAM 系统相当，并且优于最近的神经隐式 SLAM 方法，同时保持更一致、高度紧凑的隐式地图，可以准确地重建和完整的网格。最后，得益于用于高效神经点索引的体素哈希和无需最近点关联的快速隐式基于地图的配准，PIN-SLAM 可以在中等 GPU 上以传感器帧速率运行。代码可在以下网址获取：https://github.com/PRBonn/PIN_SLAM。</details>
**PDF:** <http://arxiv.org/pdf/2401.09101v1><br />
**Code:** <https://github.com/prbonn/pin_slam>**<br />
>>**index:** 2<br />
**Title:** **Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding**<br />
**Title_cn:** 通过 HSIC 瓶颈正交化和等角嵌入实现持续学习需求<br />
**Authors:** Depeng Li, Tianqi Wang, Junwei Chen, Qining Ren, Kenji Kawaguchi, Zhigang Zeng<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptation between old and new tasks with predefined basis vectors. Extensive experiments demonstrate that our method achieves competitive accuracy performance, even with absolute superiority of zero exemplar buffer and 1.02x the base model.</details>
**Abstract_cn:** <details><summary>译文: </summary>当深度神经网络接受顺序任务训练时，很容易出现灾难性遗忘。各种持续学习（CL）方法通常依赖于样本缓冲区或/和网络扩展来平衡模型稳定性和可塑性，然而，由于隐私和内存问题，这损害了它们的实用价值。相反，本文考虑了严格而现实的设置，其中先前任务的训练数据不可用，并且模型大小在顺序训练期间保持相对恒定。为了实现这种愿望，我们提出了一种概念上简单但有效的方法，将遗忘归因于逐层参数覆盖和由此产生的决策边界失真。这是通过两个关键组件之间的协同作用实现的：HSIC-瓶颈正交化 (HBO) 在正交空间中实现由希尔伯特-施密特独立准则介导的非重写参数更新，等角嵌入 (EAE) 增强新旧任务之间的决策边界适应具有预定义的基向量。大量实验表明，我们的方法即使具有零样本缓冲区和 1.02 倍基础模型的绝对优势，也能实现具有竞争力的精度性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.09067v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding**<br />
**Title_cn:** CrossVideo：用于点云视频理解的自监督跨模态对比学习<br />
**Authors:** Yunze Liu, Changxi Chen, Zifan Wang, Li Yi<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel approach named CrossVideo, which aims to enhance self-supervised cross-modal contrastive learning in the field of point cloud video understanding. Traditional supervised learning methods encounter limitations due to data scarcity and challenges in label acquisition. To address these issues, we propose a self-supervised learning method that leverages the cross-modal relationship between point cloud videos and image videos to acquire meaningful feature representations. Intra-modal and cross-modal contrastive learning techniques are employed to facilitate effective comprehension of point cloud video. We also propose a multi-level contrastive approach for both modalities. Through extensive experiments, we demonstrate that our method significantly surpasses previous state-of-the-art approaches, and we conduct comprehensive ablation studies to validate the effectiveness of our proposed designs.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种名为 CrossVideo 的新方法，旨在增强点云视频理解领域的自监督跨模态对比学习。传统的监督学习方法由于数据稀缺和标签获取的挑战而受到限制。为了解决这些问题，我们提出了一种自监督学习方法，利用点云视频和图像视频之间的跨模态关系来获取有意义的特征表示。采用模内和跨模态对比学习技术来促进点云视频的有效理解。我们还为这两种模式提出了一种多层次的对比方法。通过大量的实验，我们证明我们的方法显着超越了以前最先进的方法，并且我们进行了全面的消融研究以验证我们提出的设计的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09057v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Event-Based Visual Odometry on Non-Holonomic Ground Vehicles**<br />
**Title_cn:** 非完整地面车辆上基于事件的视觉里程计<br />
**Authors:** Wanting Xu, Si'ao Zhang, Li Cui, Xin Peng, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios. The code is available at \url{https://github.com/gowanting/NHEVO}.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管在具有挑战性的条件下承诺提供卓越的性能，但由于从事件流中提取和跟踪稳定特征的困难，基于事件的运动估计仍然是一个难题。为了增强估计，通常认为需要与其他传感器融合。在这项工作中，我们通过采用阿克曼转向平台的约束非完整运动模型，在平面地面车辆上展示了可靠的、纯粹基于事件的视觉里程计。我们将常规基于帧的相机的单特征 n 线性扩展到准时间连续事件轨迹的情况，并通过变阶泰勒展开获得多项式形式。通过直方图投票即可轻松实现多个事件轨迹的稳健平均。正如模拟数据和实际数据所证明的那样，我们的算法实现了对车辆瞬时旋转速度的准确而稳健的估计，因此结果与基于框架的传感器在正常条件下获得的增量旋转相当。此外，在具有挑战性的照明场景中，我们的性能显着优于更传统的替代方案。该代码可在 \url{https://github.com/gowanting/NHEVO} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09331v1><br />
**Code:** <https://github.com/gowanting/nhevo>**<br />
>>**index:** 2<br />
**Title:** **Online Stability Improvement of Groebner Basis Solvers using Deep Learning**<br />
**Title_cn:** 使用深度学习提高 Groebner 基解算器的在线稳定性<br />
**Authors:** Wanting Xu, Lan Hu, Manolis C. Tsakiris, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Over the past decade, the Gr\"obner basis theory and automatic solver generation have lead to a large number of solutions to geometric vision problems. In practically all cases, the derived solvers apply a fixed elimination template to calculate the Gr\"obner basis and thereby identify the zero-dimensional variety of the original polynomial constraints. However, it is clear that different variable or monomial orderings lead to different elimination templates, and we show that they may present a large variability in accuracy for a certain instance of a problem. The present paper has two contributions. We first show that for a common class of problems in geometric vision, variable reordering simply translates into a permutation of the columns of the initial coefficient matrix, and that -- as a result -- one and the same elimination template can be reused in different ways, each one leading to potentially different accuracy. We then prove that the original set of coefficients may contain sufficient information to train a classifier for online selection of a good solver, most notably at the cost of only a small computational overhead. We demonstrate wide applicability at the hand of generic dense polynomial problem solvers, as well as a concrete solver from geometric vision.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年中，Gr“观察基理论和自动求解器生成导致了几何视觉问题的大量解决方案。几乎在所有情况下，导出的求解器都应用固定的消除模板来计算 Gr”观察基从而识别原始多项式约束的零维变化。然而，很明显，不同的变量或单项式排序会导致不同的消除模板，并且我们表明，对于问题的特定实例，它们可能会带来很大的准确性变化。本文有两个贡献。我们首先表明，对于几何视觉中的一类常见问题，变量重新排序简单地转化为初始系数矩阵的列的排列，并且 - 结果 - 一个相同的消除模板可以在不同的环境中重复使用方式，每一种都可能导致不同的准确性。然后，我们证明原始系数集可能包含足够的信息来训练分类器以在线选择良好的求解器，最显着的是，只需少量的计算开销。我们展示了通用密集多项式问题求解器以及来自几何视觉的具体求解器的广泛适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09328v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Tight Fusion of Events and Inertial Measurements for Direct Velocity Estimation**<br />
**Title_cn:** 事件和惯性测量的紧密融合用于直接速度估计<br />
**Authors:** Wanting Xu, Xin Peng, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Traditional visual-inertial state estimation targets absolute camera poses and spatial landmark locations while first-order kinematics are typically resolved as an implicitly estimated sub-state. However, this poses a risk in velocity-based control scenarios, as the quality of the estimation of kinematics depends on the stability of absolute camera and landmark coordinates estimation. To address this issue, we propose a novel solution to tight visual-inertial fusion directly at the level of first-order kinematics by employing a dynamic vision sensor instead of a normal camera. More specifically, we leverage trifocal tensor geometry to establish an incidence relation that directly depends on events and camera velocity, and demonstrate how velocity estimates in highly dynamic situations can be obtained over short time intervals. Noise and outliers are dealt with using a nested two-layer RANSAC scheme. Additionally, smooth velocity signals are obtained from a tight fusion with pre-integrated inertial signals using a sliding window optimizer. Experiments on both simulated and real data demonstrate that the proposed tight event-inertial fusion leads to continuous and reliable velocity estimation in highly dynamic scenarios independently of absolute coordinates. Furthermore, in extreme cases, it achieves more stable and more accurate estimation of kinematics than traditional, point-position-based visual-inertial odometry.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的视觉惯性状态估计目标是绝对相机位姿和空间地标位置，而一阶运动学通常被解析为隐式估计的子状态。然而，这在基于速度的控制场景中带来了风险，因为运动学估计的质量取决于绝对相机和地标坐标估计的稳定性。为了解决这个问题，我们提出了一种新颖的解决方案，通过使用动态视觉传感器而不是普通相机，直接在一阶运动学水平上实现紧密的视觉惯性融合。更具体地说，我们利用三焦点张量几何来建立直接取决于事件和相机速度的重合关系，并演示如何在短时间间隔内获得高度动态情况下的速度估计。使用嵌套两层 RANSAC 方案处理噪声和异常值。此外，使用滑动窗口优化器通过与预积分惯性信号的紧密融合获得平滑的速度信号。对模拟数据和真实数据的实验表明，所提出的紧密事件惯性融合可以在独立于绝对坐标的高度动态场景中实现连续可靠的速度估计。此外，在极端情况下，它比传统的基于点位置的视觉惯性里程计实现了更稳定、更准确的运动学估计。</details>
**PDF:** <http://arxiv.org/pdf/2401.09296v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A gradient-based approach to fast and accurate head motion compensation in cone-beam CT**<br />
**Title_cn:** 锥束 CT 中基于梯度的快速、准确头部运动补偿方法<br />
**Authors:** Mareike Thies, Fabian Wagner, Noah Maul, Haijun Yu, Manuela Meier, Linda-Sophie Schneider, Mingxuan Gu, Siyuan Mei, Lukas Folle, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>Cone-beam computed tomography (CBCT) systems, with their portability, present a promising avenue for direct point-of-care medical imaging, particularly in critical scenarios such as acute stroke assessment. However, the integration of CBCT into clinical workflows faces challenges, primarily linked to long scan duration resulting in patient motion during scanning and leading to image quality degradation in the reconstructed volumes. This paper introduces a novel approach to CBCT motion estimation using a gradient-based optimization algorithm, which leverages generalized derivatives of the backprojection operator for cone-beam CT geometries. Building on that, a fully differentiable target function is formulated which grades the quality of the current motion estimate in reconstruction space. We drastically accelerate motion estimation yielding a 19-fold speed-up compared to existing methods. Additionally, we investigate the architecture of networks used for quality metric regression and propose predicting voxel-wise quality maps, favoring autoencoder-like architectures over contracting ones. This modification improves gradient flow, leading to more accurate motion estimation. The presented method is evaluated through realistic experiments on head anatomy. It achieves a reduction in reprojection error from an initial average of 3mm to 0.61mm after motion compensation and consistently demonstrates superior performance compared to existing approaches. The analytic Jacobian for the backprojection operation, which is at the core of the proposed method, is made publicly available. In summary, this paper contributes to the advancement of CBCT integration into clinical workflows by proposing a robust motion estimation approach that enhances efficiency and accuracy, addressing critical challenges in time-sensitive scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>锥形束计算机断层扫描 (CBCT) 系统凭借其便携性，为直接护理点医学成像提供了一种有前景的途径，特别是在急性中风评估等关键情况下。然而，将 CBCT 集成到临床工作流程中面临挑战，主要与长扫描持续时间有关，导致扫描期间患者运动并导致重建体积中的图像质量下降。本文介绍了一种使用基于梯度的优化算法进行 CBCT 运动估计的新方法，该算法利用锥束 CT 几何形状的反投影算子的广义导数。在此基础上，制定了一个完全可微的目标函数，该函数对重建空间中当前运动估计的质量进行分级。我们极大地加速了运动估计，与现有方法相比，速度提高了 19 倍。此外，我们研究了用于质量度量回归的网络架构，并提出预测体素质量图，与收缩架构相比，更倾向于类似自动编码器的架构。此修改改进了梯度流，从而实现更准确的运动估计。通过头部解剖学的真实实验来评估所提出的方法。运动补偿后，它可将重投影误差从最初的平均 3 毫米减少到 0.61 毫米，并始终表现出比现有方法更优越的性能。作为所提出方法的核心的反投影运算的解析雅可比行列式是公开的。总之，本文提出了一种强大的运动估计方法，可提高效率和准确性，解决时间敏感场景中的关键挑战，有助于推动 CBCT 融入临床工作流程。</details>
**PDF:** <http://arxiv.org/pdf/2401.09283v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering**<br />
**Title_cn:** P$^2$OT：深度不平衡聚类的渐进部分最优传输<br />
**Authors:** Chuyu Zhang, Hui Ren, Xuming He<br />
**Abstract:** <details><summary>原文: </summary>Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we first introduce a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution. To tackle this problem, we propose a novel pseudo-labeling-based learning framework. Our framework formulates pseudo-label generation as a progressive partial optimal transport problem, which progressively transports each sample to imbalanced clusters under prior distribution constraints, thus generating imbalance-aware pseudo-labels and learning from high-confident samples. In addition, we transform the initial formulation into an unbalanced optimal transport problem with augmented constraints, which can be solved efficiently by a fast matrix scaling algorithm. Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度聚类无需标签信息即可学习表示和语义聚类，这对基于深度学习的方法提出了巨大的挑战。尽管近年来取得了重大进展，但大多数现有方法都专注于均匀分布的数据集，这极大地限制了其方法的实际适用性。在本文中，我们首先介绍一个更实际的问题设置，称为深度不平衡聚类，其中底层类表现出不平衡分布。为了解决这个问题，我们提出了一种新颖的基于伪标签的学习框架。我们的框架将伪标签生成制定为渐进的部分最优传输问题，在先验分布约束下逐步将每个样本传输到不平衡的集群，从而生成不平衡感知的伪标签并从高置信度样本中学习。此外，我们将初始公式转化为具有增强约束的不平衡最优传输问题，可以通过快速矩阵缩放算法有效地解决。对各种数据集（包括人工管理的长尾 CIFAR100、具有挑战性的 ImageNet-R 以及细粒度 iNaturalist2018 数据集的大规模子集）的实验证明了我们方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09266v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case**<br />
**Title_cn:** 非刚性多视角相机的相对姿势：静态情况<br />
**Authors:** Min Li, Jiaqi Yang, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Multi-perspective cameras with potentially non-overlapping fields of view have become an important exteroceptive sensing modality in a number of applications such as intelligent vehicles, drones, and mixed reality headsets. In this work, we challenge one of the basic assumptions made in these scenarios, which is that the multi-camera rig is rigid. More specifically, we are considering the problem of estimating the relative pose between a static non-rigid rig in different spatial orientations while taking into account the effect of gravity onto the system. The deformable physical connections between each camera and the body center are approximated by a simple cantilever model, and inserted into the generalized epipolar constraint. Our results lead us to the important insight that the latent parameters of the deformation model, meaning the gravity vector in both views, become observable. We present a concise analysis of the observability of all variables based on noise, outliers, and rig rigidity for two different algorithms. The first one is a vision-only alternative, while the second one makes use of additional gravity measurements. To conclude, we demonstrate the ability to sense gravity in a real-world example, and discuss practical implications.</details>
**Abstract_cn:** <details><summary>译文: </summary>具有潜在不重叠视场的多视角相机已成为智能车辆、无人机和混合现实耳机等许多应用中重要的外感受传感方式。在这项工作中，我们挑战了在这些场景中做出的基本假设之一，即多摄像头装备是刚性的。更具体地说，我们正在考虑估计不同空间方向的静态非刚性装备之间的相对位姿的问题，同时考虑重力对系统的影响。每个相机和身体中心之间的可变形物理连接通过简单的悬臂模型来近似，并插入广义极线约束中。我们的结果使我们得出重要的见解，即变形模型的潜在参数（即两个视图中的重力矢量）变得可观察到。我们根据两种不同算法的噪声、异常值和钻机刚性，对所有变量的可观测性进行了简明分析。第一个是仅视觉的替代方案，而第二个则利用额外的重力测量。最后，我们在现实世界的例子中展示了感知重力的能力，并讨论了实际意义。</details>
**PDF:** <http://arxiv.org/pdf/2401.09140v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality**<br />
**Title_cn:** OCTO+：混合现实中自动开放词汇对象放置套件<br />
**Authors:** Aditya Sharma, Luke Yoffe, Tobias Höllerer<br />
**Abstract:** <details><summary>原文: </summary>One key challenge in Augmented Reality is the placement of virtual content in natural locations. Most existing automated techniques can only work with a closed-vocabulary, fixed set of objects. In this paper, we introduce and evaluate several methods for automatic object placement using recent advances in open-vocabulary vision-language models. Through a multifaceted evaluation, we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark for automatically evaluating the placement of virtual objects in augmented reality, alleviating the need for costly user studies. Through this, in addition to human evaluations, we find that OCTO+ places objects in a valid region over 70% of the time, outperforming other methods on a range of metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>增强现实的一项关键挑战是将虚拟内容放置在自然位置。大多数现有的自动化技术只能处理封闭词汇、固定对象集。在本文中，我们使用开放词汇视觉语言模型的最新进展介绍并评估了几种自动对象放置的方法。通过多方面的评估，我们确定了一种新的最先进的方法：OCTO+。我们还引入了一个基准，用于自动评估增强现实中虚拟对象的放置，从而减轻了昂贵的用户研究的需要。通过这一点，除了人工评估之外，我们发现 OCTO+ 在超过 70% 的时间内将对象放置在有效区域中，在一系列指标上优于其他方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.08973v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Dynamic DNNs and Runtime Management for Efficient Inference on Mobile/Embedded Devices**<br />
**Title_cn:** 动态 DNN 和运行时管理可在移动/嵌入式设备上进行高效推理<br />
**Authors:** Lei Xun, Jonathon Hare, Geoff V. Merrett<br />
**Abstract:** <details><summary>原文: </summary>Deep neural network (DNN) inference is increasingly being executed on mobile and embedded platforms due to several key advantages in latency, privacy and always-on availability. However, due to limited computing resources, efficient DNN deployment on mobile and embedded platforms is challenging. Although many hardware accelerators and static model compression methods were proposed by previous works, at system runtime, multiple applications are typically executed concurrently and compete for hardware resources. This raises two main challenges: Runtime Hardware Availability and Runtime Application Variability. Previous works have addressed these challenges through either dynamic neural networks that contain sub-networks with different performance trade-offs or runtime hardware resource management. In this thesis, we proposed a combined method, a system was developed for DNN performance trade-off management, combining the runtime trade-off opportunities in both algorithms and hardware to meet dynamically changing application performance targets and hardware constraints in real time. We co-designed novel Dynamic Super-Networks to maximise runtime system-level performance and energy efficiency on heterogeneous hardware platforms. Compared with SOTA, our experimental results using ImageNet on the GPU of Jetson Xavier NX show our model is 2.4x faster for similar ImageNet Top-1 accuracy, or 5.1% higher accuracy at similar latency. We also designed a hierarchical runtime resource manager that tunes both dynamic neural networks and DVFS at runtime. Compared with the Linux DVFS governor schedutil, our runtime approach achieves up to a 19% energy reduction and a 9% latency reduction in single model deployment scenario, and an 89% energy reduction and a 23% latency reduction in a two concurrent model deployment scenario.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于延迟、隐私和始终在线可用性方面的几个关键优势，深度神经网络 (DNN) 推理越来越多地在移动和嵌入式平台上执行。然而，由于计算资源有限，在移动和嵌入式平台上高效部署 DNN 具有挑战性。尽管之前的工作提出了许多硬件加速器和静态模型压缩方法，但在系统运行时，多个应用程序通常同时执行并竞争硬件资源。这提出了两个主要挑战：运行时硬件可用性和运行时应用程序可变性。以前的工作通过包含具有不同性能权衡的子网络的动态神经网络或运行时硬件资源管理来解决这些挑战。在本文中，我们提出了一种组合方法，开发了一个用于 DNN 性能权衡管理的系统，结合算法和硬件中的运行时权衡机会，以满足实时动态变化的应用程序性能目标和硬件约束。我们共同设计了新颖的动态超级网络，以最大限度地提高异构硬件平台上的运行时系统级性能和能源效率。与 SOTA 相比，我们在 Jetson Xavier NX 的 GPU 上使用 ImageNet 的实验结果表明，在类似的 ImageNet Top-1 精度下，我们的模型速度提高了 2.4 倍，或者在类似的延迟下，精度提高了 5.1%。我们还设计了一个分层运行时资源管理器，可以在运行时调整动态神经网络和 DVFS。与 Linux DVFS Governor schedutil 相比，我们的运行时方法在单模型部署场景中实现了高达 19% 的能耗和 9% 的延迟减少，在两个并发模型部署场景中实现了 89% 的能耗和 23% 的延迟减少。</details>
**PDF:** <http://arxiv.org/pdf/2401.08965v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Fluid Dynamic DNNs for Reliable and Adaptive Distributed Inference on Edge Devices**<br />
**Title_cn:** 用于边缘设备上可靠、自适应分布式推理的流体动态 DNN<br />
**Authors:** Lei Xun, Mingyu Hu, Hengrui Zhao, Amit Kumar Singh, Jonathon Hare, Geoff V. Merrett<br />
**Abstract:** <details><summary>原文: </summary>Distributed inference is a popular approach for efficient DNN inference at the edge. However, traditional Static and Dynamic DNNs are not distribution-friendly, causing system reliability and adaptability issues. In this paper, we introduce Fluid Dynamic DNNs (Fluid DyDNNs), tailored for distributed inference. Distinct from Static and Dynamic DNNs, Fluid DyDNNs utilize a novel nested incremental training algorithm to enable independent and combined operation of its sub-networks, enhancing system reliability and adaptability. Evaluation on embedded Arm CPUs with a DNN model and the MNIST dataset, shows that in scenarios of single device failure, Fluid DyDNNs ensure continued inference, whereas Static and Dynamic DNNs fail. When devices are fully operational, Fluid DyDNNs can operate in either a High-Accuracy mode and achieve comparable accuracy with Static DNNs, or in a High-Throughput mode and achieve 2.5x and 2x throughput compared with Static and Dynamic DNNs, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>分布式推理是一种在边缘进行高效 DNN 推理的流行方法。然而，传统的静态和动态 DNN 不适合分布，导致系统可靠性和适应性问题。在本文中，我们介绍了专为分布式推理而定制的流体动态 DNN（Fluid DyDNN）。与静态和动态 DNN 不同，Fluid DyDNN 采用新颖的嵌套增量训练算法来实现子网络的独立和组合操作，从而增强系统的可靠性和适应性。对具有 DNN 模型和 MNIST 数据集的嵌入式 Arm CPU 的评估表明，在单个设备发生故障的情况下，Fluid DyDNN 可确保持续推理，而静态和动态 DNN 则会失败。当设备完全运行时，Fluid DyDNN 可以在高精度模式下运行，并达到与静态 DNN 相当的精度，也可以在高吞吐量模式下运行，与静态和动态 DNN 相比，分别实现 2.5 倍和 2 倍的吞吐量。</details>
**PDF:** <http://arxiv.org/pdf/2401.08943v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Subwavelength Imaging using a Solid-Immersion Diffractive Optical Processor**<br />
**Title_cn:** 使用固体浸没衍射光学处理器进行亚波长成像<br />
**Authors:** Jingtian Hu, Kun Liao, Niyazi Ulas Dinc, Carlo Gigli, Bijie Bai, Tianyi Gan, Xurong Li, Hanlong Chen, Xilin Yang, Yuhang Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Phase imaging is widely used in biomedical imaging, sensing, and material characterization, among other fields. However, direct imaging of phase objects with subwavelength resolution remains a challenge. Here, we demonstrate subwavelength imaging of phase and amplitude objects based on all-optical diffractive encoding and decoding. To resolve subwavelength features of an object, the diffractive imager uses a thin, high-index solid-immersion layer to transmit high-frequency information of the object to a spatially-optimized diffractive encoder, which converts/encodes high-frequency information of the input into low-frequency spatial modes for transmission through air. The subsequent diffractive decoder layers (in air) are jointly designed with the encoder using deep-learning-based optimization, and communicate with the encoder layer to create magnified images of input objects at its output, revealing subwavelength features that would otherwise be washed away due to diffraction limit. We demonstrate that this all-optical collaboration between a diffractive solid-immersion encoder and the following decoder layers in air can resolve subwavelength phase and amplitude features of input objects in a highly compact design. To experimentally demonstrate its proof-of-concept, we used terahertz radiation and developed a fabrication method for creating monolithic multi-layer diffractive processors. Through these monolithically fabricated diffractive encoder-decoder pairs, we demonstrated phase-to-intensity transformations and all-optically reconstructed subwavelength phase features of input objects by directly transforming them into magnified intensity features at the output. This solid-immersion-based diffractive imager, with its compact and cost-effective design, can find wide-ranging applications in bioimaging, endoscopy, sensing and materials characterization.</details>
**Abstract_cn:** <details><summary>译文: </summary>相位成像广泛应用于生物医学成像、传感和材料表征等领域。然而，具有亚波长分辨率的相位物体的直接成像仍然是一个挑战。在这里，我们演示了基于全光衍射编码和解码的相位和幅度物体的亚波长成像。为了解析物体的亚波长特征，衍射成像仪使用薄的高折射率固体浸没层将物体的高频信息传输到空间优化的衍射编码器，该编码器对输入的高频信息进行转换/编码转换为低频空间模式以通过空气传输。随后的衍射解码器层（在空气中）使用基于深度学习的优化与编码器联合设计，并与编码器层通信以在其输出处创建输入对象的放大图像，从而揭示亚波长特征，否则这些特征将被冲走。至衍射极限。我们证明，衍射固体浸没编码器和空气中的后续解码器层之间的这种全光学协作可以在高度紧凑的设计中解析输入对象的亚波长相位和幅度特征。为了通过实验证明其概念验证，我们使用太赫兹辐射并开发了一种用于创建单片多层衍射处理器的制造方法。通过这些单片制造的衍射编码器-解码器对，我们通过直接将输入对象转换为输出处的放大强度特征来演示输入对象的相位到强度变换和全光学重建的亚波长相位特征。这种基于固体浸入式的衍射成像仪具有紧凑且经济高效的设计，可在生物成像、内窥镜、传感和材料表征领域得到广泛的应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.08923v1><br />
**Code:** null<br />

