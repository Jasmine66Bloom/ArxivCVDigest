## [UPDATED!] **2024-01-18** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **OMG-Seg: Is One Model Good Enough For All Segmentation?**<br />
**Title_cn:** OMG-Seg：一种模型足以适用于所有细分吗？<br />
**Authors:** Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy<br />
**Abstract:** <details><summary>原文: </summary>In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models. We propose OMG-Seg, One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation. To our knowledge, this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg, a transformer-based encoder-decoder architecture with task-specific queries and outputs, can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们解决了各种分割任务，每个任务传统上都是通过不同或部分统一的模型来处理的。我们提出 OMG-Seg，一种足以高效且有效地处理所有分割任务的模型，包括图像语义、实例和全景分割，以及它们的视频对应项、开放词汇设置、提示驱动、交互式分割，例如SAM 和视频对象分割。据我们所知，这是第一个在一个模型中处理所有这些任务并取得令人满意的性能的模型。我们证明 OMG-Seg 是一种基于 Transformer 的编码器-解码器架构，具有特定于任务的查询和输出，可以支持十多个不同的分割任务，并且显着减少各种任务和数据集的计算和参数开销。我们在协同训练期间严格评估任务间的影响和相关性。代码和模型可在 https://github.com/lxtGH/OMG-Seg 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10229v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **RAP-SAM: Towards Real-Time All-Purpose Segment Anything**<br />
**Title_cn:** RAP-SAM：迈向实时通用分段任何内容<br />
**Authors:** Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过 Transformer 架构的改进，视觉基础模型（VFM）在性能和泛化能力方面取得了显着进步。 Segment Anything Model (SAM) 是一种可以实现广义分割的卓越模型。然而，大多数 VFM 无法实时运行，这使得将它们转移到多个产品中变得困难。另一方面，当前的实时分割主要有一个目的，例如驾驶场景的语义分割。我们认为实际应用需要多样化的输出。因此，这项工作探索了一种新的实时分段设置，称为实时通用分段，以在实时部署中传输 VFM。它包含三个不同的任务，包括交互式分割、全景分割和视频分割。我们的目标是使用一种模型来实时完成上述任务。我们首先对几个强基线进行基准测试。然后，我们提出了实时通用 SAM (RAP-SAM)。它包含一个高效的编码器和一个高效的解耦解码器来执行提示驱动的解码。此外，我们进一步探索不同的训练策略和调整方法，以进一步提高协同训练的性能。我们的代码和模型可在 https://github.com/xushilin1/RAP-SAM/ 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10228v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting**<br />
**Title_cn:** 用于全景分割和掩模修复的简单潜在扩散方法<br />
**Authors:** Wouter Van Gansbeke, Bert De Brabandere<br />
**Abstract:** <details><summary>原文: </summary>Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.</details>
**Abstract_cn:** <details><summary>译文: </summary>全景和实例分割网络通常使用专门的对象检测模块、复杂的损失函数和临时后处理步骤进行训练，以处理实例掩模的排列不变性。这项工作建立在稳定扩散的基础上，并提出了一种用于全景分割的潜在扩散方法，从而产生了一个忽略这些复杂性的简单架构。我们的训练过程包括两个步骤：（1）训练浅层自动编码器将分割掩模投影到潜在空间； (2) 训练扩散模型以允许在潜在空间中进行图像条件采样。生成模型的使用开启了对掩模完成或修复的探索，这在交互式分割中具有应用。实验验证为全景分割和掩模修复带来了有希望的结果。虽然没有创造新的最先进水平，但我们的模型的简单性、通用性和掩模完成能力是理想的特性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10227v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**<br />
**Title_cn:** 解释隐式神经画布：通过追踪像素的贡献将像素与神经元连接起来<br />
**Authors:** Namitha Padmanabhan, Matthew Gwilliam, Pulkit Kumar, Shishira R Maiya, Max Ehrlich, Abhinav Shrivastava<br />
**Abstract:** <details><summary>原文: </summary>The many variations of Implicit Neural Representations (INRs), where a neural network is trained as a continuous representation of a signal, have tremendous practical utility for downstream tasks including novel view synthesis, video compression, and image superresolution. Unfortunately, the inner workings of these networks are seriously under-studied. Our work, eXplaining the Implicit Neural Canvas (XINC), is a unified framework for explaining properties of INRs by examining the strength of each neuron's contribution to each output pixel. We call the aggregate of these contribution maps the Implicit Neural Canvas and we use this concept to demonstrate that the INRs which we study learn to ''see'' the frames they represent in surprising ways. For example, INRs tend to have highly distributed representations. While lacking high-level object semantics, they have a significant bias for color and edges, and are almost entirely space-agnostic. We arrive at our conclusions by examining how objects are represented across time in video INRs, using clustering to visualize similar neurons across layers and architectures, and show that this is dominated by motion. These insights demonstrate the general usefulness of our analysis framework. Our project page is available at https://namithap10.github.io/xinc.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐式神经表示 (INR) 的许多变体（其中神经网络被训练为信号的连续表示）对于下游任务（包括新颖的视图合成、视频压缩和图像超分辨率）具有巨大的实用性。不幸的是，这些网络的内部运作机制还没有被充分研究。我们的工作，eXplaining the Implicit Neural Canvas (XINC)，是一个统一的框架，用于通过检查每个神经元对每个输出像素的贡献强度来解释 INR 的属性。我们将这些贡献图的集合称为隐式神经画布，并使用这个概念来证明我们研究的 INR 学会以令人惊讶的方式“看到”它们所代表的框架。例如，INR 往往具有高度分布式的表示。虽然缺乏高级对象语义，但它们对颜色和边缘有很大的偏见，并且几乎完全与空间无关。我们通过检查对象在视频 INR 中如何随时间表示而得出结论，使用聚类来可视化跨层和架构的相似神经元，并表明这是由运动主导的。这些见解证明了我们的分析框架的普遍实用性。我们的项目页面位于 https://namithap10.github.io/xinc。</details>
**PDF:** <http://arxiv.org/pdf/2401.10217v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Comprehensive OOD Detection Improvements**<br />
**Title_cn:** 全面的 OOD 检测改进<br />
**Authors:** Anish Lakkapragada, Amol Khanna, Edward Raff, Nathan Inkawhich<br />
**Abstract:** <details><summary>原文: </summary>As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着机器学习在有影响力的决策中变得越来越普遍，识别推理数据何时超出模型的预期输入分布对于为预测提供上下文至关重要。为此任务创建了分布外 (OOD) 检测方法。这些方法可以根据它们是否分别利用模型的嵌入或预测进行 OOD 检测分为基于表示的方法或基于逻辑的方法。与大多数只关注其中一个群体的论文不同，我们同时关注这两类群体。我们在基于表示的方法中对特征嵌入进行降维，以加快时间并提高性能。此外，我们提出了 DICE-COL，这是对流行的基于 logit 的方法定向稀疏化 (DICE) 的修改，解决了一个未被注意到的缺陷。我们在 OpenOODv1.5 基准框架上展示了我们的方法的有效性，它们显着提高了性能并设置了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.10176v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Few-shot learning for COVID-19 Chest X-Ray Classification with Imbalanced Data: An Inter vs. Intra Domain Study**<br />
**Title_cn:** 具有不平衡数据的 COVID-19 胸部 X 射线分类的少样本学习：域间与域内研究<br />
**Authors:** Alejandro Galán-Cuenca, Antonio Javier Gallego, Marcelo Saval-Calvo, Antonio Pertusa<br />
**Abstract:** <details><summary>原文: </summary>Medical image datasets are essential for training models used in computer-aided diagnosis, treatment planning, and medical research. However, some challenges are associated with these datasets, including variability in data distribution, data scarcity, and transfer learning issues when using models pre-trained from generic images. This work studies the effect of these challenges at the intra- and inter-domain level in few-shot learning scenarios with severe data imbalance. For this, we propose a methodology based on Siamese neural networks in which a series of techniques are integrated to mitigate the effects of data scarcity and distribution imbalance. Specifically, different initialization and data augmentation methods are analyzed, and four adaptations to Siamese networks of solutions to deal with imbalanced data are introduced, including data balancing and weighted loss, both separately and combined, and with a different balance of pairing ratios. Moreover, we also assess the inference process considering four classifiers, namely Histogram, $k$NN, SVM, and Random Forest. Evaluation is performed on three chest X-ray datasets with annotated cases of both positive and negative COVID-19 diagnoses. The accuracy of each technique proposed for the Siamese architecture is analyzed separately and their results are compared to those obtained using equivalent methods on a state-of-the-art CNN. We conclude that the introduced techniques offer promising improvements over the baseline in almost all cases, and that the selection of the technique may vary depending on the amount of data available and the level of imbalance.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像数据集对于计算机辅助诊断、治疗计划和医学研究中使用的训练模型至关重要。然而，这些数据集存在一些挑战，包括数据分布的可变性、数据稀缺性以及使用从通用图像预训练的模型时的迁移学习问题。这项工作研究了这些挑战在数据严重不平衡的小样本学习场景中在域内和域间层面的影响。为此，我们提出了一种基于暹罗神经网络的方法，其中集成了一系列技术来减轻数据稀缺和分布不平衡的影响。具体来说，分析了不同的初始化和数据增强方法，并介绍了处理不平衡数据的暹罗网络解决方案的四种适应方法，包括单独和组合的数据平衡和加权损失，以及不同的配对比率平衡。此外，我们还评估了考虑四个分类器的推理过程，即直方图、$k$NN、SVM 和随机森林。对三个胸部 X 光数据集进行评估，其中标注了阳性和阴性 COVID-19 诊断病例。分别分析了为 Siamese 架构提出的每种技术的准确性，并将其结果与在最先进的 CNN 上使用等效方法获得的结果进行了比较。我们的结论是，所引入的技术在几乎所有情况下都比基线提供了有希望的改进，并且技术的选择可能会根据可用数据量和不平衡程度而有所不同。</details>
**PDF:** <http://arxiv.org/pdf/2401.10129v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Exposing Lip-syncing Deepfakes from Mouth Inconsistencies**<br />
**Title_cn:** 揭露口型不一致的 Deepfakes<br />
**Authors:** Soumyya Kanti Datta, Shan Jia, Siwei Lyu<br />
**Abstract:** <details><summary>原文: </summary>A lip-syncing deepfake is a digitally manipulated video in which a person's lip movements are created convincingly using AI models to match altered or entirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as the artifacts are limited to the lip region and more difficult to discern. In this paper, we describe a novel approach, LIP-syncing detection based on mouth INConsistency (LIPINC), for lip-syncing deepfake detection by identifying temporal inconsistencies in the mouth region. These inconsistencies are seen in the adjacent frames and throughout the video. Our model can successfully capture these irregularities and outperforms the state-of-the-art methods on several benchmark deepfake datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>口型同步 Deepfake 是一种经过数字处理的视频，其中使用人工智能模型令人信服地创建一个人的嘴唇动作，以匹配更改或全新的音频。口型同步深度伪造是一种危险的深度伪造，因为伪影仅限于嘴唇区域并且更难以辨别。在本文中，我们描述了一种新颖的方法，即基于嘴 INConsistency (LIPINC) 的唇同步检测，通过识别嘴区域的时间不一致来进行唇同步深度伪造检测。这些不一致在相邻帧和整个视频中都可以看到。我们的模型可以成功捕获这些不规则性，并且在几个基准深度伪造数据集上优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.10113v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition**<br />
**Title_cn:** VIPTR：用于快速高效场景文本识别的视觉可变换提取器<br />
**Authors:** Xianfu Cheng, Weixiao Zhou, Xiang Li, Xiaoming Chen, Jian Yang, Tongliang Li, Zhoujun Li<br />
**Abstract:** <details><summary>原文: </summary>Scene Text Recognition (STR) is a challenging task that involves recognizing text within images of natural scenes. Although current state-of-the-art models for STR exhibit high performance, they typically suffer from low inference efficiency due to their reliance on hybrid architectures comprised of visual encoders and sequence decoders. In this work, we propose the VIsion Permutable extractor for fast and efficient scene Text Recognition (VIPTR), which achieves an impressive balance between high performance and rapid inference speeds in the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor with a pyramid structure, characterized by multiple self-attention layers, while eschewing the traditional sequence decoder. This design choice results in a lightweight and efficient model capable of handling inputs of varying sizes. Extensive experimental results on various standard datasets for both Chinese and English scene text recognition validate the superiority of VIPTR. Notably, the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with other lightweight models and achieves SOTA inference speeds. Meanwhile, the VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining a low parameter count and favorable inference speed. Our proposed method provides a compelling solution for the STR challenge, which blends high accuracy with efficiency and greatly benefits real-world applications requiring fast and reliable text recognition. The code is publicly available at https://github.com/cxfyxl/VIPTR.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景文本识别 (STR) 是一项具有挑战性的任务，涉及识别自然场景图像中的文本。尽管当前最先进的 STR 模型表现出高性能，但由于依赖于由视觉编码器和序列解码器组成的混合架构，它们通常会遇到推理效率较低的问题。在这项工作中，我们提出了用于快速高效的场景文本识别（VIPTR）的 VIsion Permutable 提取器，它在 STR 领域的高性能和快速推理速度之间实现了令人印象深刻的平衡。具体来说，VIPTR 利用具有金字塔结构的视觉语义提取器，其特征是多个自注意力层，同时避开了传统的序列解码器。这种设计选择产生了一个轻量级且高效的模型，能够处理不同大小的输入。在各种标准数据集上进行的中英文场景文本识别的大量实验结果验证了 VIPTR 的优越性。值得注意的是，VIPTR-T (Tiny) 变体可提供与其他轻量级模型相当的极具竞争力的准确性，并实现 SOTA 推理速度。同时，VIPTR-L（大）变体获得了更高的识别精度，同时保持了较低的参数数量和良好的推理速度。我们提出的方法为 STR 挑战提供了一个引人注目的解决方案，它将高精度与效率融为一体，极大地有利于需要快速可靠的文本识别的实际应用。该代码可在 https://github.com/cxfyxl/VIPTR 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10110v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **ContextMix: A context-aware data augmentation method for industrial visual inspection systems**<br />
**Title_cn:** ContextMix：工业视觉检测系统的上下文感知数据增强方法<br />
**Authors:** Hyungmin Kim, Donghun Kim, Pyunghwan Ahn, Sungho Suh, Hansang Cho, Junmo Kim<br />
**Abstract:** <details><summary>原文: </summary>While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance. These techniques hold particular significance in industrial manufacturing contexts. Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets. However, their application to industrial tasks remains challenging. The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences. This leads to severe data imbalance. Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling. Nonetheless, this is a crucial step for enhancing productivity. For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets. ContextMix generates novel data by resizing entire images and integrating them into other images within the batch. This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images. With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques. We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets. Our proposed method demonstrates improved results across a range of robustness tasks. Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然深度神经网络取得了卓越的性能，但数据增强已成为减轻过度拟合和增强网络性能的关键策略。这些技术在工业制造环境中具有特别重要的意义。最近，引入了基于图像混合的方法，在公共基准数据集上表现出了改进的性能。然而，它们在工业任务中的应用仍然具有挑战性。制造环境每天都会产生大量未标记的数据，只有少数情况下出现异常数据。这导致了严重的数据不平衡。因此，由于与标记相关的成本很高，创建平衡的数据集并不简单。尽管如此，这是提高生产力的关键一步。为此，我们引入了 ContextMix，一种针对工业应用和基准数据集量身定制的方法。 ContextMix 通过调整整个图像的大小并将其集成到批次中的其他图像中来生成新颖的数据。这种方法使我们的方法能够根据调整大小的图像的不同大小来学习判别特征，并使用遮挡图像训练信息丰富的辅助特征以进行对象识别。与现有的增强技术相比，ContextMix 凭借最小的图像调整大小额外计算成本提高了性能。我们在公共基准数据集上使用各种网络架构评估其在分类、检测和分割任务中的有效性。我们提出的方法展示了一系列鲁棒性任务的改进结果。正如使用无源元件数据集所证明的那样，它在实际工业环境中的功效尤其值得注意。</details>
**PDF:** <http://arxiv.org/pdf/2401.10050v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Deep spatial context: when attention-based models meet spatial regression**<br />
**Title_cn:** 深层空间上下文：当基于注意力的模型遇到空间回归时<br />
**Authors:** Paulina Tomaszewska, Elżbieta Sienkiewicz, Mai P. Hoang, Przemysław Biecek<br />
**Abstract:** <details><summary>原文: </summary>We propose 'Deep spatial context' (DSCon) method, which serves for investigation of the attention-based vision models using the concept of spatial context. It was inspired by histopathologists, however, the method can be applied to various domains. The DSCon allows for a quantitative measure of the spatial context's role using three Spatial Context Measures: $SCM_{features}$, $SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial context is observable within the features of neighboring regions, their target values (attention scores) or residuals, respectively. It is achieved by integrating spatial regression into the pipeline. The DSCon helps to verify research questions. The experiments reveal that spatial relationships are much bigger in the case of the classification of tumor lesions than normal tissues. Moreover, it turns out that the larger the size of the neighborhood taken into account within spatial regression, the less valuable contextual information is. Furthermore, it is observed that the spatial context measure is the largest when considered within the feature space as opposed to the targets and residuals.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了“深层空间上下文”（DSCon）方法，该方法用于使用空间上下文概念研究基于注意力的视觉模型。它受到组织病理学家的启发，但是该方法可以应用于各个领域。 DSCon 允许使用三个空间上下文测量来定量测量空间上下文的作用：$SCM_{features}$、$SCM_{targets}$、$SCM_{residuals}$，以区分空间上下文在以下特征中是否可观察到：相邻区域，分别是它们的目标值（注意力分数）或残差。它是通过将空间回归集成到管道中来实现的。 DSCon 有助于验证研究问题。实验表明，肿瘤病变分类时的空间关系比正常组织大得多。此外，事实证明，空间回归中考虑的邻域规模越大，上下文信息的价值就越低。此外，据观察，当在特征空间内考虑时，空间上下文度量相对于目标和残差是最大的。</details>
**PDF:** <http://arxiv.org/pdf/2401.10044v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition**<br />
**Title_cn:** CMFN：用于不规则场景文本识别的跨模态融合网络<br />
**Authors:** Jinzhi Zheng, Ruyi Ji, Libo Zhang, Yanjun Wu, Chen Zhao<br />
**Abstract:** <details><summary>原文: </summary>Scene text recognition, as a cross-modal task involving vision and text, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semantic mining, which limits the performance of the algorithm in recognizing irregular scene text. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scene text recognition, which incorporates visual cues into the semantic mining process. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scene text and integrates cross-modal visual cues for text recognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景文本识别作为涉及视觉和文本的跨模态任务，是计算机视觉领域的一个重要研究课题。大多数现有方法使用语言模型来提取语义信息以优化视觉识别。然而，语义挖掘过程中忽略了视觉线索的引导，限制了算法识别不规则场景文本的性能。为了解决这个问题，我们提出了一种用于不规则场景文本识别的新型跨模态融合网络（CMFN），它将视觉线索纳入语义挖掘过程。具体来说，CMFN由位置自增强编码器、视觉识别分支和迭代语义识别分支组成。位置自增强编码器为视觉识别分支和迭代语义识别分支提供字符序列位置编码。视觉识别分支根据CNN提取的视觉特征和位置自增强编码器提供的位置编码信息进行视觉识别。迭代语义识别分支由语言识别模块和跨模态融合门组成，模拟人类识别场景文本的方式，并集成跨模态视觉线索进行文本识别。实验表明，所提出的 CMFN 算法实现了与最先进算法相当的性能，表明了其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10041v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition**<br />
**Title_cn:** GPT4Ego：释放预训练模型的潜力，实现零样本自我中心动作识别<br />
**Authors:** Guangzhao Dai, Xiangbo Shu, Wenhao Wu<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in Zero-Shot Egocentric Action Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this paper, we introduce GPT4Ego, a straightforward yet remarkably potent VLM framework for ZS-EAR, designed to enhance the fine-grained alignment of concept and description between vision and language. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%), and CharadesEgo (31.5%, +2.6%).</details>
**Abstract_cn:** <details><summary>译文: </summary>在大规模数据集上进行预训练的视觉语言模型（VLM）在各种视觉识别任务中表现出了令人印象深刻的性能。这一进步为零样本自我中心动作识别（ZS-EAR）的显着性能铺平了道路。通常，VLM 将 ZS-EAR 作为全局视频文本匹配任务来处理，这通常会导致视觉和语言知识的对齐不理想。我们提出了一种使用 VLM 的 ZS-EAR 改进方法，强调细粒度的概念描述对齐，利用以自我为中心的视频中丰富的语义和上下文细节。在本文中，我们介绍了 GPT4Ego，这是一个简单但非常强大的 ZS-EAR VLM 框架，旨在增强视觉和语言之间概念和描述的细粒度对齐。大量实验表明，GPT4Ego 在三个大规模自我中心视频基准测试中显着优于现有 VLM，即 EPIC-KITCHENS-100（33.2%，+9.4%）、EGTEA（39.6%，+5.5%）和 CharadesEgo（31.5%，+） 2.6%）。</details>
**PDF:** <http://arxiv.org/pdf/2401.10039v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth Camera**<br />
**Title_cn:** Depth Over RGB：使用深度相机自动评估开放手术技能<br />
**Authors:** Ido Zuckerman, Nicole Werner, Jonathan Kouchly, Emma Huston, Shannon DiMarco, Paul DiMusto, Shlomi Laufer<br />
**Abstract:** <details><summary>原文: </summary>Purpose: In this paper, we present a novel approach to the automatic evaluation of open surgery skills using depth cameras. This work is intended to show that depth cameras achieve similar results to RGB cameras, which is the common method in the automatic evaluation of open surgery skills. Moreover, depth cameras offer advantages such as robustness to lighting variations, camera positioning, simplified data compression, and enhanced privacy, making them a promising alternative to RGB cameras.   Methods: Experts and novice surgeons completed two simulators of open suturing. We focused on hand and tool detection, and action segmentation in suturing procedures. YOLOv8 was used for tool detection in RGB and depth videos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Our study includes the collection and annotation of a dataset recorded with Azure Kinect.   Results: We demonstrated that using depth cameras in object detection and action segmentation achieves comparable results to RGB cameras. Furthermore, we analyzed 3D hand path length, revealing significant differences between experts and novice surgeons, emphasizing the potential of depth cameras in capturing surgical skills. We also investigated the influence of camera angles on measurement accuracy, highlighting the advantages of 3D cameras in providing a more accurate representation of hand movements.   Conclusion: Our research contributes to advancing the field of surgical skill assessment by leveraging depth cameras for more reliable and privacy evaluations. The findings suggest that depth cameras can be valuable in assessing surgical skills and provide a foundation for future research in this area.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：在本文中，我们提出了一种使用深度相机自动评估开放手术技能的新方法。这项工作旨在表明深度相机可以达到与 RGB 相机相似的结果，这是开放手术技能自动评估中的常用方法。此外，深度相机具有对光照变化的鲁棒性、相机定位、简化的数据压缩和增强的隐私性等优势，使其成为 RGB 相机的有前途的替代品。方法：专家和新手外科医生完成了两个开放缝合模拟器。我们专注于手和工具检测以及缝合过程中的动作分割。 YOLOv8 用于 RGB 和深度视频中的工具检测。此外，UVAST和MSTCN++用于动作分割。我们的研究包括使用 Azure Kinect 记录的数据集的收集和注释。结果：我们证明了在对象检测和动作分割中使用深度相机可以达到与 RGB 相机相当的结果。此外，我们分析了 3D 手部路径长度，揭示了专家和新手外科医生之间的显着差异，强调了深度相机在捕捉手术技能方面的潜力。我们还研究了摄像头角度对测量精度的影响，强调了 3D 摄像头在提供更准确的手部运动表示方面的优势。结论：我们的研究通过利用深度摄像头进行更可靠和隐私的评估，有助于推进手术技能评估领域。研究结果表明，深度相机在评估手术技能方面具有重要价值，并为该领域的未来研究奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.10037v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Text Region Multiple Information Perception Network for Scene Text Detection**<br />
**Title_cn:** 用于场景文本检测的文本区域多信息感知网络<br />
**Authors:** Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao<br />
**Abstract:** <details><summary>原文: </summary>Segmentation-based scene text detection algorithms can handle arbitrary shape scene texts and have strong robustness and adaptability, so it has attracted wide attention. Existing segmentation-based scene text detection algorithms usually only segment the pixels in the center region of the text, while ignoring other information of the text region, such as edge information, distance information, etc., thus limiting the detection accuracy of the algorithm for scene text. This paper proposes a plug-and-play module called the Region Multiple Information Perception Module (RMIPM) to enhance the detection performance of segmentation-based algorithms. Specifically, we design an improved module that can perceive various types of information about scene text regions, such as text foreground classification maps, distance maps, direction maps, etc. Experiments on MSRA-TD500 and TotalText datasets show that our method achieves comparable performance with current state-of-the-art algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于分割的场景文本检测算法可以处理任意形状的场景文本，具有很强的鲁棒性和适应性，因此受到了广泛的关注。现有基于分割的场景文本检测算法通常只对文本中心区域的像素进行分割，而忽略了文本区域的其他信息，如边缘信息、距离信息等，从而限制了算法的检测精度。场景文本。本文提出了一种称为区域多信息感知模块（RMIPM）的即插即用模块，以增强基于分割的算法的检测性能。具体来说，我们设计了一个改进的模块，可以感知场景文本区域的各种类型的信息，例如文本前景分类图、距离图、方向图等。在MSRA-TD500和TotalText数据集上的实验表明，我们的方法实现了与当前最先进的算法。</details>
**PDF:** <http://arxiv.org/pdf/2401.10017v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **BPDO:Boundary Points Dynamic Optimization for Arbitrary Shape Scene Text Detection**<br />
**Title_cn:** BPDO：任意形状场景文本检测的边界点动态优化<br />
**Authors:** Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao<br />
**Abstract:** <details><summary>原文: </summary>Arbitrary shape scene text detection is of great importance in scene understanding tasks. Due to the complexity and diversity of text in natural scenes, existing scene text algorithms have limited accuracy for detecting arbitrary shape text. In this paper, we propose a novel arbitrary shape scene text detector through boundary points dynamic optimization(BPDO). The proposed model is designed with a text aware module (TAM) and a boundary point dynamic optimization module (DOM). Specifically, the model designs a text aware module based on segmentation to obtain boundary points describing the central region of the text by extracting a priori information about the text region. Then, based on the idea of deformable attention, it proposes a dynamic optimization model for boundary points, which gradually optimizes the exact position of the boundary points based on the information of the adjacent region of each boundary point. Experiments on CTW-1500, Total-Text, and MSRA-TD500 datasets show that the model proposed in this paper achieves a performance that is better than or comparable to the state-of-the-art algorithm, proving the effectiveness of the model.</details>
**Abstract_cn:** <details><summary>译文: </summary>任意形状场景文本检测在场景理解任务中非常重要。由于自然场景中文本的复杂性和多样性，现有的场景文本算法对于检测任意形状文本的精度有限。在本文中，我们通过边界点动态优化（BPDO）提出了一种新颖的任意形状场景文本检测器。所提出的模型设计有文本感知模块（TAM）和边界点动态优化模块（DOM）。具体来说，该模型设计了基于分割的文本感知模块，通过提取文本区域的先验信息来获得描述文本中心区域的边界点。然后，基于可变形注意力的思想，提出了边界点动态优化模型，根据每个边界点相邻区域的信息逐步优化边界点的准确位置。在CTW-1500、Total-Text和MSRA-TD500数据集上的实验表明，本文提出的模型取得了优于或相当于state-of-the-art算法的性能，证明了模型的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09997v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Developing an AI-based Integrated System for Bee Health Evaluation**<br />
**Title_cn:** 开发基于人工智能的蜜蜂健康评估综合系统<br />
**Authors:** Andrew Liang<br />
**Abstract:** <details><summary>原文: </summary>Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks. It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times. Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions. The study also shows that audio signals are more reliable than images for assessing bee health. By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies.</details>
**Abstract_cn:** <details><summary>译文: </summary>蜜蜂为世界上约三分之一的食物授粉，但由于杀虫剂和害虫等多种因素的影响，蜂群在过去十年中惊人地减少了近 40%。监测蜂箱的传统方法（例如人工检查）是主观的、破坏性的且耗时的。为了克服这些限制，人工智能已被用来评估蜂箱的健康状况。然而，之前的研究缺乏端到端的解决方案，并且主要依赖于单一来源的数据，要么是蜜蜂图像，要么是声音。本研究介绍了一个由蜜蜂目标检测和健康评估组成的综合系统。此外，它还结合视觉和音频信号来分析蜜蜂的行为。开发了基于注意力的多模态神经网络（AMNN），以自适应地关注每种类型信号的关键特征，以进行准确的蜜蜂健康评估。 AMNN 的总体准确率达到 92.61%，超过了现有的 8 个单信号卷积神经网络和循环神经网络。它比最好的基于图像的模型高出 32.51%，比最好的基于声音的模型高出 13.98%，同时保持高效的处理时间。此外，它还提高了预测的稳健性，在所有四种评估的健康状况下获得了高于 90% 的 F1 分数。该研究还表明，在评估蜜蜂健康状况方面，音频信号比图像更可靠。通过将 AMNN 与图像和声音数据无缝集成在全面的蜜蜂健康监测系统中，该方法为蜜蜂疾病的早期检测和蜂群的保存提供了更高效、非侵入性的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.09988v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Ventricular Segmentation: A Brief Comparison of U-Net Derivatives**<br />
**Title_cn:** 心室分割：U-Net 导数的简要比较<br />
**Authors:** Ketan Suhaas Saichandran<br />
**Abstract:** <details><summary>原文: </summary>Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the accomplishments and areas for further refinement.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学成像是指用于观察人体及其内部以诊断、监测甚至治疗医学疾病的技术和方法。本文旨在探索深度学习技术在心脏短轴MRI（磁共振成像）图像语义分割中的应用，旨在增强与心脏相关的医学疾病的诊断、监测和治疗。重点是实现 U-Net 衍生的各种架构，以有效隔离心脏的特定部分以进行全面的解剖和功能分析。通过图像、图表和定量指标的结合，展示了模型的有效性及其预测。此外，本文还解决了遇到的挑战并概述了未来改进的策略。本摘要简要概述了利用深度学习进行心脏图像分割的工作，强调了所取得的成就和需要进一步完善的领域。</details>
**PDF:** <http://arxiv.org/pdf/2401.09980v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects**<br />
**Title_cn:** CustomVideo：自定义多个主题的文本到视频生成<br />
**Authors:** Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li<br />
**Abstract:** <details><summary>原文: </summary>Customized text-to-video generation aims to generate high-quality videos guided by text prompts and subject references. Current approaches designed for single subjects suffer from tackling multiple subjects, which is a more challenging and practical scenario. In this work, we aim to promote multi-subject guided text-to-video customization. We propose CustomVideo, a novel framework that can generate identity-preserving videos with the guidance of multiple subjects. To be specific, firstly, we encourage the co-occurrence of multiple subjects via composing them in a single image. Further, upon a basic text-to-video diffusion model, we design a simple yet effective attention control strategy to disentangle different subjects in the latent space of diffusion model. Moreover, to help the model focus on the specific object area, we segment the object from given reference images and provide a corresponding object mask for attention learning. Also, we collect a multi-subject text-to-video generation dataset as a comprehensive benchmark, with 69 individual subjects and 57 meaningful pairs. Extensive qualitative, quantitative, and user study results demonstrate the superiority of our method, compared with the previous state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>定制的文本到视频生成旨在根据文本提示和主题参考生成高质量的视频。当前为单个主题设计的方法难以解决多个主题，这是一个更具挑战性和实用性的场景。在这项工作中，我们的目标是促进多主题引导的文本到视频的定制。我们提出了 CustomVideo，这是一种新颖的框架，可以在多个主题的指导下生成保留身份的视频。具体来说，首先，我们通过将多个主题组合在一张图像中来鼓励它们同时出现。此外，在基本的文本到视频扩散模型的基础上，我们设计了一种简单而有效的注意力控制策略，以在扩散模型的潜在空间中解开不同的主题。此外，为了帮助模型聚焦于特定的物体区域，我们从给定的参考图像中分割物体，并为注意力学习提供相应的物体掩模。此外，我们还收集了一个多主题文本到视频生成数据集作为综合基准，其中包含 69 个单独主题和 57 个有意义的对。与之前最先进的方法相比，广泛的定性、定量和用户研究结果证明了我们的方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09962v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking**<br />
**Title_cn:** 用于运动视觉跟踪的联合重新识别、团队归属和角色分类的多任务学习<br />
**Authors:** Amir M. Mansourian, Vladimir Somers, Christophe De Vleeschouwer, Shohreh Kasaei<br />
**Abstract:** <details><summary>原文: </summary>Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>有效跟踪和重新识别球员对于分析足球视频至关重要。但是，由于球员的非线性运动、来自同一球队的球员的外观相似以及频繁的遮挡，这是一项具有挑战性的任务。因此，提取有意义的嵌入来代表玩家的能力对于开发有效的跟踪和重新识别系统至关重要。本文提出了一种多用途的基于部位的人物表示方法，称为 PRTreID，该方法同时执行角色分类、团队归属和重新识别三个任务。与现有文献相比，单个网络经过多任务监督训练以共同解决所有三个任务。由于共享主干，所提出的联合方法在计算上是高效的。此外，正如定量和定性结果所证明的那样，多任务学习会带来更丰富、更具辨别力的表示。为了证明 PRTreID 的有效性，它与最先进的跟踪方法集成，使用基于部件的后处理模块来处理长期跟踪。在具有挑战性的 SoccerNet 跟踪数据集上，所提出的跟踪方法优于所有现有的跟踪方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.09942v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **ICGNet: A Unified Approach for Instance-Centric Grasping**<br />
**Title_cn:** ICGNet：以实例为中心的抓取的统一方法<br />
**Authors:** René Zurbrügg, Yifan Liu, Francis Engelmann, Suryansh Kumar, Marco Hutter, Vaishakh Patil, Fisher Yu<br />
**Abstract:** <details><summary>原文: </summary>Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的抓取是包括装配和家用机器人在内的多项机器人任务的关键。在杂乱的环境中成功抓取需要多层次的场景理解：首先，机器人需要分析单个物体的几何属性以找到可行的抓取。这些抓取需要符合局部对象的几何形状。其次，对于每个提出的抓取，机器人需要推理与场景中其他对象的交互。最后，机器人必须计算无碰撞抓取轨迹，同时考虑目标物体的几何形状。大多数抓取检测算法直接以整体方式预测抓取姿势，这不会捕获环境的可组合性。在本文中，我们介绍了一种用于以对象为中心的抓取的端到端架构。该方法使用来自单个任意观察方向的点云数据作为输入，并为场景中每个部分观察的对象生成以实例为中心的表示。这种表示进一步用于杂乱桌面场景中的对象重建和抓取检测。我们通过在合成数据集上与最先进的方法进行广泛评估来展示所提出的方法的有效性，表明其在抓取和重建方面具有卓越的性能。此外，我们通过整理具有不同数量对象的场景来展示现实世界的适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09939v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection**<br />
**Title_cn:** MAMBA：通过内存库进行多级聚合，用于视频对象检测<br />
**Authors:** Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson<br />
**Abstract:** <details><summary>原文: </summary>State-of-the-art video object detection methods maintain a memory structure, either a sliding window or a memory queue, to enhance the current frame using attention mechanisms. However, we argue that these memory structures are not efficient or sufficient because of two implied operations: (1) concatenating all features in memory for enhancement, leading to a heavy computational cost; (2) frame-wise memory updating, preventing the memory from capturing more temporal information. In this paper, we propose a multi-level aggregation architecture via memory bank called MAMBA. Specifically, our memory bank employs two novel operations to eliminate the disadvantages of existing methods: (1) light-weight key-set construction which can significantly reduce the computational cost; (2) fine-grained feature-wise updating strategy which enables our method to utilize knowledge from the whole video. To better enhance features from complementary levels, i.e., feature maps and proposals, we further propose a generalized enhancement operation (GEO) to aggregate multi-level features in a unified manner. We conduct extensive evaluations on the challenging ImageNetVID dataset. Compared with existing state-of-the-art methods, our method achieves superior performance in terms of both speed and accuracy. More remarkably, MAMBA achieves mAP of 83.7/84.6% at 12.6/9.1 FPS with ResNet-101. Code is available at https://github.com/guanxiongsun/video_feature_enhancement.</details>
**Abstract_cn:** <details><summary>译文: </summary>最先进的视频对象检测方法维护内存结构（滑动窗口或内存队列），以使用注意机制增强当前帧。然而，我们认为这些内存结构并不高效或不够充分，因为有两个隐含的操作：（1）连接内存中的所有特征进行增强，导致大量的计算成本； (2)逐帧记忆更新，防止记忆捕获更多的时间信息。在本文中，我们提出了一种通过内存库的多级聚合架构，称为 MAMBA。具体来说，我们的存储库采用了两种新颖的操作来消除现有方法的缺点：（1）轻量级密钥集构造，可以显着降低计算成本； （2）细粒度的特征更新策略，使我们的方法能够利用整个视频中的知识。为了更好地增强互补级别的特征，即特征图和提案，我们进一步提出了一种广义增强操作（GEO）以统一的方式聚合多级特征。我们对具有挑战性的 ImageNetVID 数据集进行了广泛的评估。与现有的最先进的方法相比，我们的方法在速度和准确性方面都取得了优异的性能。更值得注意的是，MAMBA 使用 ResNet-101 在 12.6/9.1 FPS 下实现了 83.7/84.6% 的 mAP。代码可在 https://github.com/guanxiongsun/video_feature_enhancement 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09923v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **BlenDA: Domain Adaptive Object Detection through diffusion-based blending**<br />
**Title_cn:** BlenDA：通过基于扩散的混合进行域自适应对象检测<br />
**Authors:** Tzuhsuan Huang, Chen-Che Huang, Chung-Hao Ku, Jun-Cheng Chen<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain. To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT). Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%. It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection. The code is available at:https://github.com/aiiu-lab/BlenDA</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督域适应（UDA）旨在将使用源域中的标记数据学习的模型转移到目标域中的未标记数据。为了解决源域和目标域之间的大域差距问题，我们提出了一种新的域自适应对象检测正则化方法，BlenDA，通过生成中间域的伪样本及其相应的软域标签进行自适应训练。中间样本是通过使用现成的预训练文本到图像扩散模型动态混合源图像与其相应的翻译图像来生成的，该模型以目标域的文本标签作为输入，并展示了卓越的图像-到图像的翻译质量。基于两个自适应基准的实验结果，我们提出的方法可以显着提高最先进的域自适应对象检测器——对抗查询变换器（AQT）的性能。特别是，在 Cityscapes 到 Foggy Cityscapes 的适应中，我们在 Foggy Cityscapes 数据集上实现了令人印象深刻的 53.4% mAP，超过了之前的最佳水平 1.5%。值得注意的是，我们提出的方法也适用于域自适应对象检测的各种范例。代码位于：https://github.com/aiiu-lab/BlenDA</details>
**PDF:** <http://arxiv.org/pdf/2401.09921v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection**<br />
**Title_cn:** 用于视觉质量检测的 XAI 增强语义分割模型<br />
**Authors:** Tobias Clement, Truong Thanh Hung Nguyen, Mohamed Abdelaal, Hung Cao<br />
**Abstract:** <details><summary>原文: </summary>Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉质量检测系统在制造和物流等领域至关重要，它利用计算机视觉和机器学习来进行精确、快速的缺陷检测。然而，它们无法解释的性质可能会阻碍信任、错误识别和系统改进。本文提出了一个框架，通过使用基于 CAM 的解释来完善语义分割模型，从而增强视觉质量检查。我们的方法包括 1) 模型训练、2) 基于 XAI 的模型解释、3) XAI 评估和 4) 通过解释和专家见解进行注释增强以增强模型。评估显示 XAI 增强模型超越了原始 DeepLabv3-ResNet101 模型，特别是在复杂的对象分割方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.09900v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Skeleton-Guided Instance Separation for Fine-Grained Segmentation in Microscopy**<br />
**Title_cn:** 用于显微镜中细粒度分割的骨架引导实例分离<br />
**Authors:** Jun Wang, Chengfeng Zhou, Zhaoyan Ming, Lina Wei, Xudong Jiang, Dahong Qian<br />
**Abstract:** <details><summary>原文: </summary>One of the fundamental challenges in microscopy (MS) image analysis is instance segmentation (IS), particularly when segmenting cluster regions where multiple objects of varying sizes and shapes may be connected or even overlapped in arbitrary orientations. Existing IS methods usually fail in handling such scenarios, as they rely on coarse instance representations such as keypoints and horizontal bounding boxes (h-bboxes). In this paper, we propose a novel one-stage framework named A2B-IS to address this challenge and enhance the accuracy of IS in MS images. Our approach represents each instance with a pixel-level mask map and a rotated bounding box (r-bbox). Unlike two-stage methods that use box proposals for segmentations, our method decouples mask and box predictions, enabling simultaneous processing to streamline the model pipeline. Additionally, we introduce a Gaussian skeleton map to aid the IS task in two key ways: (1) It guides anchor placement, reducing computational costs while improving the model's capacity to learn RoI-aware features by filtering out noise from background regions. (2) It ensures accurate isolation of densely packed instances by rectifying erroneous box predictions near instance boundaries. To further enhance the performance, we integrate two modules into the framework: (1) An Atrous Attention Block (A2B) designed to extract high-resolution feature maps with fine-grained multiscale information, and (2) A Semi-Supervised Learning (SSL) strategy that leverages both labeled and unlabeled images for model training. Our method has been thoroughly validated on two large-scale MS datasets, demonstrating its superiority over most state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>显微镜 (MS) 图像分析的基本挑战之一是实例分割 (IS)，特别是在分割簇区域时，其中多个不同大小和形状的对象可能以任意方向连接甚至重叠。现有的 IS 方法通常无法处理此类场景，因为它们依赖于粗略的实例表示，例如关键点和水平边界框 (h-bboxes)。在本文中，我们提出了一种名为 A2B-IS 的新型单阶段框架来应对这一挑战并提高 MS 图像中 IS 的准确性。我们的方法用像素级掩模图和旋转边界框（r-bbox）表示每个实例。与使用框建议进行分割的两阶段方法不同，我们的方法将掩模和框预测解耦，从而能够同时处理以简化模型流程。此外，我们引入了高斯骨架图，以两种关键方式帮助 IS 任务：（1）它指导锚点放置，降低计算成本，同时通过滤除背景区域的噪声来提高模型学习 RoI 感知特征的能力。 (2)它通过纠正实例边界附近的错误框预测来确保密集实例的准确隔离。为了进一步提高性能，我们将两个模块集成到框架中：（1）Atrous Attention Block（A2B），旨在提取具有细粒度多尺度信息的高分辨率特征图，以及（2）半监督学习（SSL） ）利用标记和未标记图像进行模型训练的策略。我们的方法已经在两个大型 MS 数据集上得到了彻底验证，证明了其相对于大多数最先进方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09895v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** 用于弱监督语义分割的问答跨语言图像匹配<br />
**Authors:** Songhe Deng, Wei Zhuo, Jinheng Xie, Linlin Shen<br />
**Abstract:** <details><summary>原文: </summary>Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS</details>
**Abstract_cn:** <details><summary>译文: </summary>类激活图 (CAM) 已成为弱监督语义分割 (WSSS) 的流行工具，允许仅使用图像级标签来定位图像中的对象区域。然而，现有的 CAM 方法存在目标对象区域激活不足和背景区域错误激活的问题，因为缺乏详细的监督会阻碍模型理解整个图像的能力。在本文中，我们提出了一种新颖的 WSSS 问答跨语言图像匹配框架（QA-CLIMS），利用视觉语言基础模型最大化基于文本的图像理解并指导激活图的生成。首先，通过问答提示工程（QAPE）向 VQA（视觉问答）模型提出一系列精心设计的问题，以生成适应查询图像的前景目标对象和背景的语料库。然后，我们在区域图像文本对比（RITC）网络中采用对比学习，将获得的前景和背景区域与生成的语料库进行比较。我们的方法利用开放词汇表中丰富的文本信息作为额外的监督，使模型能够生成具有更完整的对象区域的高质量 CAM，并减少背景区域的错误激活。我们进行了广泛的分析来验证所提出的方法，并表明我们的方法在 PASCAL VOC 2012 和 MS COCO 数据集上均表现出最先进的性能。代码位于：https://github.com/CVI-SZU/QA-CLIMS</details>
**PDF:** <http://arxiv.org/pdf/2401.09883v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention**<br />
**Title_cn:** 通过实例感知数据增强和局部共识引导交叉注意力来促进少样本分割<br />
**Authors:** Li Guo, Haoming Liu, Yuxuan Xia, Chengyu Zhang, Xiaochen Lu<br />
**Abstract:** <details><summary>原文: </summary>Few-shot segmentation aims to train a segmentation model that can fast adapt to a novel task for which only a few annotated images are provided. Most recent models have adopted a prototype-based paradigm for few-shot inference. These approaches may have limited generalization capacity beyond the standard 1- or 5-shot settings. In this paper, we closely examine and reevaluate the fine-tuning based learning scheme that fine-tunes the classification layer of a deep segmentation network pre-trained on diverse base classes. To improve the generalizability of the classification layer optimized with sparsely annotated samples, we introduce an instance-aware data augmentation (IDA) strategy that augments the support images based on the relative sizes of the target objects. The proposed IDA effectively increases the support set's diversity and promotes the distribution consistency between support and query images. On the other hand, the large visual difference between query and support images may hinder knowledge transfer and cripple the segmentation performance. To cope with this challenge, we introduce the local consensus guided cross attention (LCCA) to align the query feature with support features based on their dense correlation, further improving the model's generalizability to the query image. The significant performance improvements on the standard few-shot segmentation benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头分割旨在训练一种分割模型，该模型可以快速适应仅提供少量带注释图像的新任务。最近的模型都采用了基于原型的范式来进行少样本推理。这些方法的泛化能力可能有限，超出标准的 1 次或 5 次设置。在本文中，我们仔细检查和重新评估基于微调的学习方案，该方案对在不同基类上预训练的深度分割网络的分类层进行微调。为了提高使用稀疏注释样本优化的分类层的通用性，我们引入了一种实例感知数据增强（IDA）策略，该策略根据目标对象的相对大小来增强支持图像。所提出的IDA有效地增加了支持集的多样性并促进了支持图像和查询图像之间的分布一致性。另一方面，查询图像和支持图像之间巨大的视觉差异可能会阻碍知识转移并削弱分割性能。为了应对这一挑战，我们引入了局部共识引导交叉注意（LCCA），根据查询特征与支持特征的密集相关性将其对齐，进一步提高了模型对查询图像的泛化能力。标准少样本分割基准 PASCAL-$5^i$ 和 COCO-$20^i$ 的显着性能改进验证了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09866v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Improving fine-grained understanding in image-text pre-training**<br />
**Title_cn:** 提高图像文本预训练的细粒度理解<br />
**Authors:** Ioana Bica, Anastasija Ilić, Matthias Bauer, Goker Erdogan, Matko Bošnjak, Christos Kaplanis, Alexey A. Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 SPARse 细粒度对比对齐 (SPARC)，这是一种从图像文本对中预训练更细粒度的多模态表示的简单方法。鉴于多个图像块通常对应于单个单词，我们建议为标题中的每个标记学习一组图像块。为了实现这一目标，我们在图像补丁和语言标记之间使用稀疏相似性度量，并为每个标记计算语言分组的视觉嵌入作为补丁的加权平均值。然后通过细粒度的序列损失来对比标记和语言分组的视觉嵌入，该损失仅取决于单个样本，不需要其他批次样本作为负样本。这使得能够以计算成本低廉的方式学习更详细的信息。 SPARC 将这种细粒度损失与全局图像和文本嵌入之间的对比损失相结合，以学习同时编码全局和局部信息的表示。我们彻底评估了我们提出的方法，并在依赖于粗粒度信息的图像级任务上表现出了比竞争方法更高的性能，例如分类，以及依赖于细粒度信息的区域级任务，例如检索、对象检测和分割。此外，SPARC 还提高了基础视觉语言模型中的模型可信度和字幕说明。</details>
**PDF:** <http://arxiv.org/pdf/2401.09865v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Enhancing the Fairness and Performance of Edge Cameras with Explainable AI**<br />
**Title_cn:** 通过可解释的人工智能增强边缘摄像头的公平性和性能<br />
**Authors:** Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Quoc Hung Cao, Van Binh Truong, Quoc Khanh Nguyen, Hung Cao<br />
**Abstract:** <details><summary>原文: </summary>The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能 (AI) 在边缘摄像头系统上的人体检测中的使用不断增加，导致模型准确但复杂，难以解释和调试。我们的研究提出了一种使用可解释人工智能 (XAI) 进行模型调试的诊断方法，以及专家驱动的问题识别和解决方案创建。在现实办公室边缘网络中的 Bytetrack 模型上进行验证后，我们发现训练数据集是主要偏差源，并建议将模型增强作为解决方案。我们的方法有助于识别模型偏差，这对于实现公平且值得信赖的模型至关重要。</details>
**PDF:** <http://arxiv.org/pdf/2401.09852v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **Slicer Networks**<br />
**Title_cn:** 切片器网络<br />
**Authors:** Hang Zhang, Xiang Chen, Rongguang Wang, Renjiu Hu, Dongdong Liu, Gaolei Li<br />
**Abstract:** <details><summary>原文: </summary>In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging applications, including unsupervised and keypoints-based image registration and lesion segmentation, have verified the Slicer Network's improved accuracy and efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学成像中，扫描通常会显示具有不同对比度但内部强度或纹理一致的物体。这一特性使得低频近似能够用于分割和变形场估计等任务。然而，将这一概念集成到用于医学图像分析的神经网络架构中仍然没有得到充分探索。在本文中，我们提出了切片器网络，这是一种旨在利用这些特征的新颖架构。切片器网络由一个利用视觉变换器等模型进行特征提取的编码器和一个利用可学习双边网格的切片器组成，通过泼溅-模糊-切片过程战略性地细化和上采样特征图。这为网络结果引入了保留边缘的低频近似，有效地扩大了有效感受野。这一增强不仅降低了计算复杂性，还提高了整体性能。不同医学成像应用的实验，包括无监督和基于关键点的图像配准和病变分割，已经验证了切片器网络提高的准确性和效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.09833v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Enhanced Automated Quality Assessment Network for Interactive Building Segmentation in High-Resolution Remote Sensing Imagery**<br />
**Title_cn:** 用于高分辨率遥感图像中交互式建筑分割的增强型自动化质量评估网络<br />
**Authors:** Zhili Zhang, Xiangyun Hu, Jiabo Xu<br />
**Abstract:** <details><summary>原文: </summary>In this research, we introduce the enhanced automated quality assessment network (IBS-AQSNet), an innovative solution for assessing the quality of interactive building segmentation within high-resolution remote sensing imagery. This is a new challenge in segmentation quality assessment, and our proposed IBS-AQSNet allievate this by identifying missed and mistaken segment areas. First of all, to acquire robust image features, our method combines a robust, pre-trained backbone with a lightweight counterpart for comprehensive feature extraction from imagery and segmentation results. These features are then fused through a simple combination of concatenation, convolution layers, and residual connections. Additionally, ISR-AQSNet incorporates a multi-scale differential quality assessment decoder, proficient in pinpointing areas where segmentation result is either missed or mistaken. Experiments on a newly-built EVLab-BGZ dataset, which includes over 39,198 buildings, demonstrate the superiority of the proposed method in automating segmentation quality assessment, thereby setting a new benchmark in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们介绍了增强型自动质量评估网络（IBS-AQSNet），这是一种用于评估高分辨率遥感图像中交互式建筑分割质量的创新解决方案。这是分割质量评估中的一个新挑战，我们提出的 IBS-AQSNet 通过识别遗漏和错误的分割区域来缓解这一问题。首先，为了获得鲁棒的图像特征，我们的方法将鲁棒的、预先训练的主干与轻量级的主干相结合，以从图像和分割结果中进行全面的特征提取。然后通过串联、卷积层和残差连接的简单组合来融合这些特征。此外，ISR-AQSNet 还采用了多尺度差分质量评估解码器，能够准确定位分割结果丢失或错误的区域。在新建的 EVLab-BGZ 数据集（包含超过 39,198 座建筑物）上进行的实验证明了所提出的方法在自动分割质量评估方面的优越性，从而在该领域树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2401.09828v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **Boosting Few-Shot Semantic Segmentation Via Segment Anything Model**<br />
**Title_cn:** 通过 Segment Anything 模型促进少样本语义分割<br />
**Authors:** Chen-Bin Feng, Qi Lai, Kangdao Liu, Houcheng Su, Chi-Man Vong<br />
**Abstract:** <details><summary>原文: </summary>In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.</details>
**Abstract_cn:** <details><summary>译文: </summary>在语义分割中，准确的预测掩模对于医学图像分析和图像编辑等下游任务至关重要。由于缺乏注释数据，少镜头语义分割（FSS）在预测具有精确轮廓的掩模方面表现不佳。最近，我们注意到大型基础模型分段任何模型（SAM）在处理细节特征方面表现良好。受 SAM 的启发，我们提出 FSS-SAM，通过解决轮廓不准确的问题来增强 FSS 方法。 FSS-SAM 无需培训。它可以作为任何 FSS 方法的后处理工具，可以提高预测掩模的准确性。具体来说，我们使用 FSS 方法预测的掩模来生成提示，然后使用 SAM 预测新的掩模。为了避免使用 SAM 预测错误的掩模，我们提出了一种预测结果选择（PRS）算法。该算法可以显着减少错误预测。在公共数据集上的实验结果表明，我们的方法在定量和定性方面都优于基本 FSS 方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.09826v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **Enhancing Small Object Encoding in Deep Neural Networks: Introducing Fast&Focused-Net with Volume-wise Dot Product Layer**<br />
**Title_cn:** 增强深度神经网络中的小对象编码：引入具有体积点积层的 Fast&Focused-Net<br />
**Authors:** Ali Tofik, Roy Partha Pratim<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce Fast&Focused-Net, a novel deep neural network architecture tailored for efficiently encoding small objects into fixed-length feature vectors. Contrary to conventional Convolutional Neural Networks (CNNs), Fast&Focused-Net employs a series of our newly proposed layer, the Volume-wise Dot Product (VDP) layer, designed to address several inherent limitations of CNNs. Specifically, CNNs often exhibit a smaller effective receptive field than their theoretical counterparts, limiting their vision span. Additionally, the initial layers in CNNs produce low-dimensional feature vectors, presenting a bottleneck for subsequent learning. Lastly, the computational overhead of CNNs, particularly in capturing diverse image regions by parameter sharing, is significantly high. The VDP layer, at the heart of Fast&Focused-Net, aims to remedy these issues by efficiently covering the entire image patch information with reduced computational demand. Experimental results demonstrate the prowess of Fast&Focused-Net in a variety of applications. For small object classification tasks, our network outperformed state-of-the-art methods on datasets such as CIFAR-10, CIFAR-100, STL-10, SVHN-Cropped, and Fashion-MNIST. In the context of larger image classification, when combined with a transformer encoder (ViT), Fast&Focused-Net produced competitive results for OpenImages V6, ImageNet-1K, and Places365 datasets. Moreover, the same combination showcased unparalleled performance in text recognition tasks across SVT, IC15, SVTP, and HOST datasets. This paper presents the architecture, the underlying motivation, and extensive empirical evidence suggesting that Fast&Focused-Net is a promising direction for efficient and focused deep learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 Fast&Focused-Net，这是一种新颖的深度神经网络架构，专为将小对象有效编码为固定长度的特征向量而设计。与传统的卷积神经网络 (CNN) 相反，Fast&Focused-Net 采用了一系列我们新提出的层，即体积点积 (VDP) 层，旨在解决 CNN 的几个固有局限性。具体来说，CNN 通常表现出比理论对应物更小的有效感受野，从而限制了它们的视野范围。此外，CNN 的初始层会产生低维特征向量，为后续学习带来瓶颈。最后，CNN 的计算开销（尤其是通过参数共享捕获不同图像区域）非常高。 VDP 层是 Fast&Focused-Net 的核心，旨在通过有效覆盖整个图像块信息并减少计算需求来解决这些问题。实验结果证明了 Fast&Focused-Net 在各种应用中的强大能力。对于小对象分类任务，我们的网络在 CIFAR-10、CIFAR-100、STL-10、SVHN-Cropped 和 Fashion-MNIST 等数据集上的表现优于最先进的方法。在较大图像分类的背景下，当与 Transformer Encoder (ViT) 结合使用时，Fast&Focused-Net 为 OpenImages V6、ImageNet-1K 和 Places365 数据集产生了有竞争力的结果。此外，相同的组合在 SVT、IC15、SVTP 和 HOST 数据集的文本识别任务中展示了无与伦比的性能。本文介绍了其架构、潜在动机和广泛的经验证据，表明 Fast&Focused-Net 是高效且专注的深度学习的一个有前途的方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.09823v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Multilingual Visual Speech Recognition with a Single Model by Learning with Discrete Visual Speech Units**<br />
**Title_cn:** 通过学习离散视觉语音单元，使用单一模型进行多语言视觉语音识别<br />
**Authors:** Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Se Jin Park, Yong Man Ro<br />
**Abstract:** <details><summary>原文: </summary>This paper explores sentence-level Multilingual Visual Speech Recognition with a single model for the first time. As the massive multilingual modeling of visual data requires huge computational costs, we propose a novel strategy, processing with visual speech units. Motivated by the recent success of the audio speech unit, the proposed visual speech unit is obtained by discretizing the visual speech features extracted from the self-supervised visual speech model. To correctly capture multilingual visual speech, we first train the self-supervised visual speech model on 5,512 hours of multilingual audio-visual data. Through analysis, we verify that the visual speech units mainly contain viseme information while suppressing non-linguistic information. By using the visual speech units as the inputs of our system, we pre-train the model to predict corresponding text outputs on massive multilingual data constructed by merging several VSR databases. As both the inputs and outputs are discrete, we can greatly improve the training efficiency compared to the standard VSR training. Specifically, the input data size is reduced to 0.016% of the original video inputs. In order to complement the insufficient visual information in speech recognition, we apply curriculum learning where the inputs of the system begin with audio-visual speech units and gradually change to visual speech units. After pre-training, the model is finetuned on continuous features. We set new state-of-the-art multilingual VSR performances by achieving comparable performances to the previous language-specific VSR models, with a single trained model.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文首次探讨了单一模型的句子级多语言视觉语音识别。由于视觉数据的大规模多语言建模需要巨大的计算成本，我们提出了一种新颖的策略，即使用视觉语音单元进行处理。受音频语音单元最近成功的启发，所提出的视觉语音单元是通过离散化从自监督视觉语音模型中提取的视觉语音特征来获得的。为了正确捕获多语言视觉语音，我们首先在 5,512 小时的多语言视听数据上训练自监督视觉语音模型。通过分析，我们验证了视觉语音单元主要包含视位信息，同时抑制非语言信息。通过使用视觉语音单元作为系统的输入，我们对模型进行预训练，以预测通过合并多个 VSR 数据库构建的海量多语言数据的相应文本输出。由于输入和输出都是离散的，因此与标准 VSR 训练相比，我们可以大大提高训练效率。具体来说，输入数据大小减少到原始视频输入的 0.016%。为了弥补语音识别中视觉信息的不足，我们采用课程学习的方式，系统的输入从视听语音单元开始，逐渐转变为视觉语音单元。预训练后，模型针对连续特征进行微调。我们通过单一训练模型实现了与之前的特定语言 VSR 模型相当的性能，从而设定了新的最先进的多语言 VSR 性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.09802v1><br />
**Code:** null<br />
>>**index:** 34<br />
**Title:** **BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images**<br />
**Title_cn:** BreastRegNet：用于注册乳房 Faxitron 和组织病理学图像的深度学习框架<br />
**Authors:** Negar Golestani, Aihui Wang, Gregory R Bean, Mirabela Rusu<br />
**Abstract:** <details><summary>原文: </summary>A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a deep learning-based image registration approach trained on mono-modal synthetic image pairs. The models were trained using data from 50 women who received neoadjuvant chemotherapy and underwent surgery. The results demonstrate that our method is faster and yields significantly lower average landmark error ($2.1\pm1.96$ mm) over the state-of-the-art iterative ($4.43\pm4.1$ mm) and deep learning ($4.02\pm3.15$ mm) approaches. Improved performance of our approach in integrating radiology and pathology information facilitates generating large datasets, which allows training models for more accurate breast cancer detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳腺癌的标准治疗方案需要进行新辅助治疗，然后手术切除肿瘤和周围组织。病理学家通常依靠柜式 X 射线照片（称为 Faxitron）来检查切除的乳腺组织并诊断残留疾病的程度。然而，准确确定残留癌症的位置、大小和病灶可能具有挑战性，并且不正确的评估可能会导致临床后果。自动化方法的利用可以改善组织病理学过程，使病理学家能够更有效、更精确地选择采样区域。尽管认识到必要性，但目前还没有这样的方法可用。训练此类自动检测模型需要在离体放射学图像上进行准确的地面实况标签，这些标签可以通过配准 Faxitron 和组织病理学图像并将癌症范围从组织病理学映射到 X 射线图像来获取。本研究介绍了一种基于深度学习的图像配准方法，该方法在单模态合成图像对上进行训练。这些模型使用 50 名接受新辅助化疗并接受手术的女性的数据进行训练。结果表明，与最先进的迭代（$4.43\pm4.1$mm）和深度学习（$4.02\pm3）相比，我们的方法更快，并且产生显着更低的平均地标误差（$2.1\pm1.96$mm） .15$ mm) 接近。我们的方法在整合放射学和病理学信息方面的性能改进有助于生成大型数据集，从而可以训练模型以实现更准确的乳腺癌检测。</details>
**PDF:** <http://arxiv.org/pdf/2401.09791v1><br />
**Code:** null<br />
>>**index:** 35<br />
**Title:** **Adaptive Self-training Framework for Fine-grained Scene Graph Generation**<br />
**Title_cn:** 用于细粒度场景图生成的自适应自训练框架<br />
**Authors:** Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park<br />
**Abstract:** <details><summary>原文: </summary>Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景图生成（SGG）模型存在基准数据集的固有问题，例如长尾谓词分布和缺失注释问题。在这项工作中，我们的目标是通过利用未注释的三元组来缓解 SGG 的长尾问题。为此，我们引入了 SGG (ST-SGG) 的自训练框架，该框架为未注释的三元组分配伪标签，并基于该伪标签训练 SGG 模型。虽然图像识别的自训练已经取得了重大进展，但由于其固有的性质（例如语义模糊性和谓词类的长尾分布），为 SGG 任务设计自训练框架更具挑战性。因此，我们为 SGG 提出了一种新颖的伪标记技术，称为类特定动量自适应阈值（CATM），它是一个与模型无关的框架，可以应用于任何现有的 SGG 模型。此外，我们设计了一种图结构学习器（GSL），当将我们提出的自训练框架采用到最先进的基于消息传递神经网络（MPNN）的 SGG 模型时，它是有益的。我们广泛的实验验证了 ST-SGG 在各种 SGG 模型上的有效性，特别是在增强细粒度谓词类的性能方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.09786v1><br />
**Code:** null<br />
>>**index:** 36<br />
**Title:** **On the Audio Hallucinations in Large Audio-Video Language Models**<br />
**Title_cn:** 论大型音视频语言模型中的幻听<br />
**Authors:** Taichi Nishimura, Shota Nakada, Masayoshi Kondo<br />
**Abstract:** <details><summary>原文: </summary>Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型音视频语言模型可以生成视频和音频的描述。然而，他们有时会忽略音频内容，产生仅依赖于视觉信息的音频描述。本文将其称为幻听，并在大型音视频语言模型中对其进行分析。我们通过查询音频信息收集了 1000 个句子，并注释它们是否包含幻觉。如果一个句子是幻觉的，我们也会对幻觉的类型进行分类。结果显示，有 332 个句子出现幻觉，每种幻觉类型的名词和动词都有不同的趋势。基于此，我们在零样本和微调设置中使用预训练的音频文本模型来解决音频幻觉分类的任务。我们的实验结果表明，零样本模型实现了比随机模型（40.3％）更高的性能（F1 中为 52.2％），而微调模型实现了 87.9％，优于零样本模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.09774v1><br />
**Code:** null<br />
>>**index:** 37<br />
**Title:** **SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation**<br />
**Title_cn:** SEINE：用于核实例分割的结构编码和交互网络<br />
**Authors:** Ye Zhang, Linghan Cai, Ziyue Wang, Yongbing Zhang<br />
**Abstract:** <details><summary>原文: </summary>Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance the structure learning for the fuzzy nuclei. To strengthen the structural learning ability, a semantic feature fusion (SFF) is presented to boost the semantic consistency of semantic and structure branches. Furthermore, a position enhancement (PE) method is applied to suppress incorrect nuclei boundary predictions. Extensive experiments demonstrate the superiority of our approaches, and SEINE achieves state-of-the-art (SOTA) performance on four datasets. The code is available at \href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE}.</details>
**Abstract_cn:** <details><summary>译文: </summary>组织病理学图像中的细胞核实例分割对于生物分析和癌症诊断非常重要，但由于两个原因仍然具有挑战性。 （1）嫌色核的核内和核外区域的相似视觉呈现通常会导致分割不足，（2）当前的方法缺乏对核结构的探索，导致实例预测支离破碎。为了解决这些问题，本文提出了一种结构编码和交互网络，称为SEINE，它开发了核的结构建模方案，并利用核之间的结构相似性来提高每个分割实例的完整性。具体来说，SEINE引入了基于轮廓的结构编码（SE），考虑了细胞核结构和语义之间的相关性，实现了细胞核结构的合理表示。基于编码，我们提出了一种结构引导注意力（SGA），以清晰的核为原型来增强模糊核的结构学习。为了增强结构学习能力，提出了语义特征融合（SFF）来提高语义和结构分支的语义一致性。此外，应用位置增强（PE）方法来抑制不正确的核边界预测。大量的实验证明了我们方法的优越性，SEINE 在四个数据集上实现了最先进的 (SOTA) 性能。代码可在 \href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09773v1><br />
**Code:** null<br />
>>**index:** 38<br />
**Title:** **SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition**<br />
**Title_cn:** SlideAVSR：用于视听语音识别的论文讲解视频数据集<br />
**Authors:** Hao Wang, Shuhei Kurita, Shuichiro Shimizu, Daisuke Kawahara<br />
**Abstract:** <details><summary>原文: </summary>Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR.</details>
**Abstract_cn:** <details><summary>译文: </summary>视听语音识别 (AVSR) 是自动语音识别 (ASR) 的多模式扩展，使用视频作为音频的补充。在 AVSR 中，人们在唇读等面部特征数据集上投入了大量精力，但在评估更广泛的背景下的图像理解能力方面往往存在不足。在本文中，我们构建了 SlideAVSR，这是一个使用科学论文解释视频的 AVSR 数据集。 SlideAVSR 提供​​了一个新的基准，模型可以在演示文稿录音的幻灯片上转录语音和文本。由于论文解释中常见的技术术语在没有参考文本的情况下转录起来非常困难，因此我们的 SlideAVSR 数据集突出了 AVSR 问题的一个新方面。作为一个简单而有效的基线，我们提出了 DocWhisper，一种 AVSR 模型，可以引用幻灯片中的文本信息，并确认其在 SlideAVSR 上的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09759v1><br />
**Code:** null<br />
>>**index:** 39<br />
**Title:** **Instance Brownian Bridge as Texts for Open-vocabulary Video Instance Segmentation**<br />
**Title_cn:** 实例布朗桥作为开放词汇视频实例分割的文本<br />
**Authors:** Zesen Cheng, Kehan Li, Hao Li, Peng Jin, Chang Liu, Xiawu Zheng, Rongrong Ji, Jie Chen<br />
**Abstract:** <details><summary>原文: </summary>Temporally locating objects with arbitrary class texts is the primary pursuit of open-vocabulary Video Instance Segmentation (VIS). Because of the insufficient vocabulary of video data, previous methods leverage image-text pretraining model for recognizing object instances by separately aligning each frame and class texts, ignoring the correlation between frames. As a result, the separation breaks the instance movement context of videos, causing inferior alignment between video and text. To tackle this issue, we propose to link frame-level instance representations as a Brownian Bridge to model instance dynamics and align bridge-level instance representation to class texts for more precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon a frozen video segmentor to generate frame-level instance queries, and design Temporal Instance Resampler (TIR) to generate queries with temporal context from frame queries. To mold instance queries to follow Brownian bridge and accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to learn discriminative bridge-level representations of instances via contrastive objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).</details>
**Abstract_cn:** <details><summary>译文: </summary>临时定位具有任意类文本的对象是开放词汇视频实例分割（VIS）的主要追求。由于视频数据的词汇量不足，以前的方法利用图像文本预训练模型通过单独对齐每个帧和类文本来识别对象实例，忽略帧之间的相关性。结果，这种分离破坏了视频的实例移动上下文，导致视频和文本之间的对齐较差。为了解决这个问题，我们建议将帧级实例表示作为布朗桥链接起来，以对实例动态进行建模，并将桥级实例表示与类文本对齐，以实现更精确的开放词汇 VIS (BriVIS)。具体来说，我们在冻结视频分段器上构建系统以生成帧级实例查询，并设计时态实例重采样器（TIR）以从帧查询中生成具有时态上下文的查询。为了塑造实例查询以遵循布朗桥并实现与类文本的对齐，我们设计了桥文本对齐（BTA）以通过对比目标学习实例的判别性桥级表示。以 MinVIS 作为基本视频分割器，BriVIS 明显超越了 Open-vocabulary SOTA (OV2Seg)。例如，在具有挑战性的大词汇量 VIS 数据集 (BURST) 上，BriVIS 达到了 7.43 mAP，与 OV2Seg (4.97 mAP) 相比，提高了 49.49%。</details>
**PDF:** <http://arxiv.org/pdf/2401.09732v1><br />
**Code:** null<br />
>>**index:** 40<br />
**Title:** **P2Seg: Pointly-supervised Segmentation via Mutual Distillation**<br />
**Title_cn:** P2Seg：通过相互蒸馏进行点监督分割<br />
**Authors:** Zipeng Wang, Xuehui Yu, Xumeng Han, Wenwen Yu, Zhixun Huang, Jianbin Jiao, Zhenjun Han<br />
**Abstract:** <details><summary>原文: </summary>Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.</details>
**Abstract_cn:** <details><summary>译文: </summary>点级监督实例分割（PSIS）旨在通过利用低成本但实例信息丰富的注释来增强实例分割的适用性和可扩展性。现有的 PSIS 方法通常依靠位置信息来区分对象，但由于缺乏轮廓注释，预测精确的边界仍然具有挑战性。然而，弱监督语义分割方法擅长利用类内特征一致性来捕获相同语义区域的边界轮廓。在本文中，我们设计了一个相互蒸馏模块（MDM），以利用实例位置和语义信息的互补优势，实现准确的实例级对象感知。 MDM 由语义到实例 (S2I) 和实例到语义 (I2S) 组成。 S2I 以语义区域的精确边界为指导来学习注释点和实例轮廓之间的关联。 I2S 利用实例之间的区别关系来促进语义图中各种对象的区分。大量实验证实了 MDM 在促进实例和语义信息之间的协同作用方面的功效，从而提高了实例级对象表示的质量。我们的方法在 PASCAL VOC 和 MS COCO 数据集上实现了 55.7 mAP$_{50}$ 和 17.6 mAP，显着优于最近的 PSIS 方法和几个框监督实例分割竞争对手。</details>
**PDF:** <http://arxiv.org/pdf/2401.09709v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Model Compression Techniques in Biometrics Applications: A Survey**<br />
**Title_cn:** 生物识别应用中的模型压缩技术：调查<br />
**Authors:** Eduarda Caldeira, Pedro C. Neto, Marco Huber, Naser Damer, Ana F. Sequeira<br />
**Abstract:** <details><summary>原文: </summary>The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习算法的发展广泛增强了人类任务自动化的能力。然而，这些模型性能的巨大改进与它们不断增加的复杂性高度相关，限制了它们在通常部署在资源受限设备中的以人为本的应用程序中的有用性。这导致了压缩技术的发展，该技术大大降低了深度学习模型的计算和内存成本，而不会显着降低性能。本文旨在通过对生物识别应用中的模型压缩技术（即量化、知识蒸馏和剪枝）进行全面调查，系统化当前有关该主题的文献。我们对这些技术的比较价值进行批判性分析，重点分析它们的优点和缺点，并提出可能改进当前方法的未来工作方向的建议。此外，我们讨论和分析了模型偏差和模型压缩之间的联系，强调了在未来的工作中将压缩研究导向模型公平性的必要性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10139v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions**<br />
**Title_cn:** ParaHome：参数化日常家庭活动以实现人机交互的 3D 生成模型<br />
**Authors:** Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, Hanbyul Joo<br />
**Abstract:** <details><summary>原文: </summary>To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了使机器能够了解人类在日常活动中如何与物理世界交互，提供包含人类 3D 运动以及可学习 3D 表示形式的物体运动的丰富数据至关重要。理想情况下，这些数据应该在自然设置中收集，在人与物体交互期间捕获真实的动态 3D 信号。为了应对这一挑战，我们推出了 ParaHome 系统，该系统旨在捕获和参数化公共家庭环境中人类和物体的动态 3D 运动。我们的系统由具有 70 个同步 RGB 摄像机的多视图设置以及配备基于 IMU 的连体衣和手部动作捕捉手套的可穿戴动作捕捉设备组成。通过利用 ParaHome 系统，我们收集了一个新颖的大规模人机交互数据集。值得注意的是，我们的数据集在三个主要方面比现有数据集提供了关键的进步：（1）在自然活动期间捕捉 3D 身体和灵巧的手部操作运动以及家庭环境中的 3D 对象移动； （2）包含人类在各种情景场景中与多个对象的交互，并在文本中进行相应的描述； (3)包括用参数化关节表达的具有多个部分的关节对象。在我们的数据集的基础上，我们引入了新的研究任务，旨在构建一个生成模型，用于在现实世界的房间环境中学习和综合人与物体的交互。</details>
**PDF:** <http://arxiv.org/pdf/2401.10232v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Edit One for All: Interactive Batch Image Editing**<br />
**Title_cn:** 编辑一应俱全：交互式批量图像编辑<br />
**Authors:** Thao Nguyen, Utkarsh Ojha, Yuheng Li, Haotian Liu, Yong Jae Lee<br />
**Abstract:** <details><summary>原文: </summary>In recent years, image editing has advanced remarkably. With increased human control, it is now possible to edit an image in a plethora of ways; from specifying in text what we want to change, to straight up dragging the contents of the image in an interactive point-based manner. However, most of the focus has remained on editing single images at a time. Whether and how we can simultaneously edit large batches of images has remained understudied. With the goal of minimizing human supervision in the editing process, this paper presents a novel method for interactive batch image editing using StyleGAN as the medium. Given an edit specified by users in an example image (e.g., make the face frontal), our method can automatically transfer that edit to other test images, so that regardless of their initial state (pose), they all arrive at the same final state (e.g., all facing front). Extensive experiments demonstrate that edits performed using our method have similar visual quality to existing single-image-editing methods, while having more visual consistency and saving significant time and human effort.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，图像编辑取得了显着进步。随着人类控制力的增强，现在可以通过多种方式编辑图像；从在文本中指定我们想要更改的内容，到以基于点的交互式方式直接拖动图像的内容。然而，大部分注意力仍然集中在一次编辑单个图像上。我们是否以及如何同时编辑大批量图像仍有待研究。为了最大限度地减少编辑过程中的人工监督，本文提出了一种使用 StyleGAN 作为媒介的交互式批量图像编辑的新方法。给定用户在示例图像中指定的编辑（例如，将脸部设为正面），我们的方法可以自动将该编辑转移到其他测试图像，以便无论它们的初始状态（姿势）如何，它们都会达到相同的最终状态（例如，全部面向前方）。大量的实验表明，使用我们的方法进行的编辑与现有的单图像编辑方法具有相似的视觉质量，同时具有更高的视觉一致性并节省大量时间和人力。</details>
**PDF:** <http://arxiv.org/pdf/2401.10219v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer**<br />
**Title_cn:** MM-Interleaved：通过多模态特征同步器进行交错图像文本生成建模<br />
**Authors:** Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{https://github.com/OpenGVLab/MM-Interleaved}.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发交错图像文本数据的生成模型具有研究和实用价值。它需要模型来理解交错序列并随后生成图像和文本。然而，现有的尝试受到固定数量的视觉标记无法有效捕获图像细节的问题的限制，这在多图像场景中尤其成问题。为了解决这个问题，本文提出了 MM-Interleaved，一种用于交错图像文本数据的端到端生成模型。它引入了多尺度和多图像特征同步器模块，允许在生成过程中直接访问先前上下文中的细粒度图像特征。 MM-Interleaved 在配对和交错图像文本语料库上进行了端到端预训练。它通过监督微调阶段得到进一步增强，其中模型提高了遵循复杂多模态指令的能力。实验证明了 MM-Interleaved 在遵循多模式指令识别视觉细节以及遵循文本和视觉条件生成一致图像方面的多功能性。代码和模型可在 \url{https://github.com/OpenGVLab/MM-Interleaved} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10208v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation**<br />
**Title_cn:** Motion-Zero：用于基于扩散的视频生成的零镜头移动对象控制框架<br />
**Authors:** Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, Yang Li<br />
**Abstract:** <details><summary>原文: </summary>Recent large-scale pre-trained diffusion models have demonstrated a powerful generative ability to produce high-quality videos from detailed text descriptions. However, exerting control over the motion of objects in videos generated by any video diffusion model is a challenging problem. In this paper, we propose a novel zero-shot moving object trajectory control framework, Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video diffusion model.To this end, an initial noise prior module is designed to provide a position-based prior to improve the stability of the appearance of the moving object and the accuracy of position. In addition, based on the attention map of the U-net, spatial constraints are directly applied to the denoising process of diffusion models, which further ensures the positional and spatial consistency of moving objects during the inference. Furthermore, temporal consistency is guaranteed with a proposed shift temporal attention mechanism. Our method can be flexibly applied to various state-of-the-art video diffusion models without any training process. Extensive experiments demonstrate our proposed method can control the motion trajectories of objects and generate high-quality videos.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的大规模预训练扩散模型已经表现出强大的生成能力，可以根据详细的文本描述生成高质量的视频。然而，对任何视频扩散模型生成的视频中的对象运动进行控制都是一个具有挑战性的问题。在本文中，我们提出了一种新颖的零样本移动物体轨迹控制框架Motion-Zero，以实现边界框轨迹控制的文本到视频扩散模型。为此，设计了初始噪声先验模块提供基于位置的先验，提高运动物体外观的稳定性和位置的准确性。此外，基于U-net的注意力图，将空间约束直接应用于扩散模型的去噪过程，进一步保证了推理过程中运动物体的位置和空间一致性。此外，通过提出的转移时间注意力机制保证了时间一致性。我们的方法可以灵活地应用于各种最先进的视频扩散模型，无需任何训练过程。大量的实验证明我们提出的方法可以控制物体的运动轨迹并生成高质量的视频。</details>
**PDF:** <http://arxiv.org/pdf/2401.10150v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **DiffusionGPT: LLM-Driven Text-to-Image Generation System**<br />
**Title_cn:** DiffusionGPT：法学硕士驱动的文本到图像生成系统<br />
**Authors:** Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型为图像生成领域开辟了新途径，导致开源平台上共享的高质量模型激增。然而，当前的文本到图像系统仍然存在一个重大挑战，通常无法处理不同的输入，或者仅限于单一模型结果。目前的统一尝试往往分为两个正交的方面：i）在输入阶段解析多样化的提示； ii) 激活专家模型进行输出。为了结合两个领域的优点，我们提出了 DiffusionGPT，它利用大型语言模型 (LLM) 提供一个统一的生成系统，能够无缝地容纳各种类型的提示并集成领域专家模型。 DiffusionGPT 基于先验知识为各种生成模型构建特定领域的树。当提供输入时，法学硕士会解析提示并使用思想树来指导选择适当的模型，从而放宽输入约束并确保在不同领域的卓越性能。此外，我们引入了优势数据库，其中思想树通过人类反馈丰富，使模型选择过程与人类偏好保持一致。通过大量的实验和比较，我们证明了 DiffusionGPT 的有效性，展示了其在不同领域突破图像合成界限的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.10061v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework**<br />
**Title_cn:** 探索潜在跨通道嵌入，以在扩散框架中实现准确的 3D 人体姿势重建<br />
**Authors:** Junkun Jiang, Jie Chen<br />
**Abstract:** <details><summary>原文: </summary>Monocular 3D human pose estimation poses significant challenges due to the inherent depth ambiguities that arise during the reprojection process from 2D to 3D. Conventional approaches that rely on estimating an over-fit projection matrix struggle to effectively address these challenges and often result in noisy outputs. Recent advancements in diffusion models have shown promise in incorporating structural priors to address reprojection ambiguities. However, there is still ample room for improvement as these methods often overlook the exploration of correlation between the 2D and 3D joint-level features. In this study, we propose a novel cross-channel embedding framework that aims to fully explore the correlation between joint-level features of 3D coordinates and their 2D projections. In addition, we introduce a context guidance mechanism to facilitate the propagation of joint graph attention across latent channels during the iterative diffusion process. To evaluate the effectiveness of our proposed method, we conduct experiments on two benchmark datasets, namely Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement in terms of reconstruction accuracy compared to state-of-the-art methods. The code for our method will be made available online for further reference.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于从 2D 到 3D 的重投影过程中出现的固有深度模糊性，单目 3D 人体姿态估计提出了重大挑战。依赖于估计过度拟合投影矩阵的传统方法很难有效地解决这些挑战，并且常常会导致输出噪声。扩散模型的最新进展显示出在结合结构先验来解决重投影模糊性方面的前景。然而，仍然有很大的改进空间，因为这些方法经常忽视 2D 和 3D 联合级特征之间相关性的探索。在本研究中，我们提出了一种新颖的跨通道嵌入框架，旨在充分探索 3D 坐标的联合级特征与其 2D 投影之间的相关性。此外，我们引入了上下文引导机制，以促进迭代扩散过程中联合图注意力跨潜在通道的传播。为了评估我们提出的方法的有效性，我们在两个基准数据集 Human3.6M 和 MPI-INF-3DHP 上进行了实验。我们的结果表明，与最先进的方法相比，重建精度有了显着提高。我们的方法的代码将在线提供以供进一步参考。</details>
**PDF:** <http://arxiv.org/pdf/2401.09836v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing**<br />
**Title_cn:** 基于扩散的图像编辑中文本反转的小波引导加速<br />
**Authors:** Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo<br />
**Abstract:** <details><summary>原文: </summary>In the field of image editing, Null-text Inversion (NTI) enables fine-grained editing while preserving the structure of the original image by optimizing null embeddings during the DDIM sampling process. However, the NTI process is time-consuming, taking more than two minutes per image. To address this, we introduce an innovative method that maintains the principles of the NTI while accelerating the image editing process. We propose the WaveOpt-Estimator, which determines the text optimization endpoint based on frequency characteristics. Utilizing wavelet transform analysis to identify the image's frequency characteristics, we can limit text optimization to specific timesteps during the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI) concept, a target prompt representing the original image serves as the initial text value for optimization. This approach maintains performance comparable to NTI while reducing the average editing time by over 80% compared to the NTI method. Our method presents a promising approach for efficient, high-quality image editing based on diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在图像编辑领域，空文本反转（NTI）通过在 DDIM 采样过程中优化空嵌入，实现细粒度编辑，同时保留原始图像的结构。然而，NTI 过程非常耗时，每张图像需要两分钟以上。为了解决这个问题，我们引入了一种创新方法，该方法在加速图像编辑过程的同时保持了 NTI 的原则。我们提出了 WaveOpt-Estimator，它根据频率特征确定文本优化端点。利用小波变换分析来识别图像的频率特征，我们可以将文本优化限制在 DDIM 采样过程中的特定时间步长。采用负提示反转（NPI）概念，代表原始图像的目标提示作为优化的初始文本值。这种方法保持了与 NTI 相当的性能，同时与 NTI 方法相比，平均编辑时间减少了 80% 以上。我们的方法提供了一种基于扩散模型的高效、高质量图像编辑的有前景的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.09794v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **CLIP Model for Images to Textual Prompts Based on Top-k Neighbors**<br />
**Title_cn:** 基于Top-k邻居的图像到文本提示的CLIP模型<br />
**Authors:** Xin Zhang, Xin Zhang, YeMing Cai, Tianzhi Jia<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像合成是多模态生成的一个子领域，近年来引起了广泛关注。我们提出了一种经济有效的图像到提示生成方法，利用生成模型生成文本提示，而不需要大量注释数据。我们将我们的方法分为两个阶段：在线阶段和离线阶段。我们使用 CLIP 模型和 K 最近邻（KNN）算法的组合。所提出的系统由两个主要部分组成：离线任务和在线任务。我们的方法在这些模型中拥有最高的度量 0.612，分别比 Clip、Clip + KNN（前 10）高 0.013、0.055、0.011。</details>
**PDF:** <http://arxiv.org/pdf/2401.09763v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Image Translation as Diffusion Visual Programmers**<br />
**Title_cn:** 作为扩散视觉程序员的图像翻译<br />
**Authors:** Cheng Han, James C. Liang, Qifan Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Ying Nian Wu, Dongfang Liu<br />
**Abstract:** <details><summary>原文: </summary>We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs (i.e., computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP's remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the framework enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍了新颖的扩散视觉编程器（DVP），这是一种神经符号图像翻译框架。我们提出的 DVP 在 GPT 架构中无缝嵌入了条件灵活的扩散模型，为各种亲符号步骤编排了连贯的视觉程序序列（即计算机视觉模型），这些步骤涵盖了 RoI 识别、风格转移和位置操纵，从而促进透明且可控的图像翻译过程。大量的实验证明了DVP的卓越性能，超越了并行艺术。这一成功归功于 DVP 的几个关键特性：首先，DVP 通过实例归一化实现了条件灵活的翻译，使模型能够消除人工指导带来的敏感性，并最佳地关注文本描述以生成高质量的内容。其次，该框架通过将特征空间中复杂的高维概念解读为更容易访问的低维符号（例如，[Prompt]、[RoI object]）来增强上下文推理，允许本地化、上下文无关的编辑，同时保持整体连贯性。最后但并非最不重要的一点是，DVP 通过在每个编程阶段提供明确的符号表示来提高系统的可控性和可解释性，使用户能够直观地解释和修改结果。我们的研究标志着在协调人工图像翻译过程与认知智能方面迈出了实质性的一步，有望实现更广泛的应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.09742v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach**<br />
**Title_cn:** 迈向可识别的无监督领域翻译：多样化的分布匹配方法<br />
**Authors:** Sagar Shrestha, Xiao Fu<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督域翻译（UDT）旨在找到将样本从一个域（例如草图）转换到另一个域（例如照片）而不改变高级语义（也称为“内容”）的函数。平移函数通常通过变换后的源域和目标域的概率分布匹配来寻求。 CycleGAN 可以说是这一领域中最具代表性的方法。然而，文献中注意到，CycleGAN 及其变体可能无法识别所需的翻译功能并产生内容不一致的翻译。这种限制是由于学习标准的解决方案空间中存在多个翻译函数（称为“测量保留自同构”（MPA））而出现的。尽管意识到了此类可识别性问题，但解决方案仍然难以捉摸。研究深入研究了核心可识别性查询，并引入了 MPA 消除理论。我们的分析表明，如果学习函数匹配多对不同的跨域条件分布，则 MPA 不太可能存在。我们的理论导致使用分布的 UDT 学习器匹配辅助变量引起的域子集——而不是经典方法中的整个数据域。据我们所知，所提出的框架是第一个在合理的 UDT 设置下严格建立翻译可识别性的框架。实验证实了我们的理论主张。</details>
**PDF:** <http://arxiv.org/pdf/2401.09671v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Towards Language-Driven Video Inpainting via Multimodal Large Language Models**<br />
**Title_cn:** 通过多模态大语言模型实现语言驱动的视频修复<br />
**Authors:** Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一个新任务——语言驱动的视频修复，它使用自然语言指令来指导修复过程。这种方法克服了传统视频修复方法的局限性，传统视频修复方法依赖于手动标记的二进制掩模，这一过程通常是乏味且劳动密集型的。我们提出了“按指令从视频中删除对象”(ROVI) 数据集，其中包含 5,650 个视频和 9,091 个修复结果，以支持该任务的训练和评估。我们还提出了一种新颖的基于扩散的语言驱动的视频修复框架，这是该任务的第一个端到端基线，集成多模态大语言模型以有效地理解和执行复杂的基于语言的修复请求。我们的综合结果展示了数据集的多功能性和模型在各种语言指导的修复场景中的有效性。我们将公开数据集、代码和模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.10226v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification**<br />
**Title_cn:** CPCL：用于弱监督的基于文本的人员重新识别的跨模态原型对比学习<br />
**Authors:** Yanwei Zheng, Xinpeng Zhao, Chuanlin Lan, Xiaowei Zhang, Bowen Huang, Jibin Yang, Dongxiao Yu<br />
**Abstract:** <details><summary>原文: </summary>Weakly supervised text-based person re-identification (TPRe-ID) seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-text pairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters by mining implicit relationships between image-text pairs. Experimental results demonstrate that our proposed CPCL attains state-of-the-art performance on all three public datasets, with a significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is available at https://github.com/codeGallery24/CPCL.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于弱监督文本的行人重识别（TPRe-ID）寻求使用文本描述来检索目标人的图像，而不依赖于身份注释，更具挑战性和实用性。主要挑战是类内差异，包括模态内特征变化和跨模态语义差距。先前的工作集中于实例级样本，而忽略了每个人固有且不变的原型特征。为此，我们提出了一种跨模态原型对比学习（CPCL）方法。在实践中，CPCL 首次将 CLIP 模型引入弱监督 TPRe-ID，将视觉和文本实例映射到共享潜在空间。随后，所提出的原型多模态记忆（PMM）模块通过混合跨模态匹配（HCM）模块以多对多映射方式捕获属于同一个人的图像文本对的异构模态之间的关联。此外，异常值伪标签挖掘（OPLM）模块进一步区分每种模态中有价值的异常值样本，通过挖掘图像-文本对之间的隐式关系来增强更可靠聚类的创建。实验结果表明，我们提出的 CPCL 在所有三个公共数据集上都达到了最先进的性能，在 CUHK-PEDES、ICFG-PEDES 和 RSTPReid 上的 Rank@1 准确率显着提高了 11.58%、8.77% 和 5.25%数据集，分别。该代码可在 https://github.com/codeGallery24/CPCL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10011v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation**<br />
**Title_cn:** 通过显式推理链和视觉问题生成推进大型多模态模型<br />
**Authors:** Kohei Uehara, Nabarun Goswami, Hanqin Wang, Toshiaki Baba, Kohtaro Tanaka, Tomohiro Hashimoto, Kai Wang, Rei Ito, Takagi Naoya, Ryo Umagami, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed by instruction tuning, and fine-tuning with a focus on chain-of-thought reasoning. The results demonstrate a stride toward a more robust, accurate, and interpretable LMM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.</details>
**Abstract_cn:** <details><summary>译文: </summary>对能够解释和推理视觉内容的智能系统的需求不断增长，需要开发不仅准确而且具有显式推理能力的大型多模态模型（LMM）。本文提出了一种新颖的方法，使 LMM 能够基于视觉内容和文本指令进行显式推理。我们引入了一个可以提出问题来获取必要知识的系统，从而增强推理过程的稳健性和可解释性。我们的方法包括开发由大型语言模型（LLM）生成的新颖数据集，旨在促进与提问机制相结合的思想链推理。我们设计了一个 LMM，它具有很强的区域感知能力，可以满足图像文本对齐的复杂要求。该模型经历了三个阶段的训练阶段，首先使用大规模数据集进行大规模图像文本对齐，然后进行指令调整，最后进行以思想链推理为重点的微调。结果表明，LMM 朝着更稳健、更准确、更可解释的方向迈进了一步，能够在面对模糊的视觉输入时进行明确的推理并主动寻找信息。</details>
**PDF:** <http://arxiv.org/pdf/2401.10005v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens**<br />
**Title_cn:** WorldDreamer：通过预测屏蔽令牌实现视频生成的通用世界模型<br />
**Authors:** Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu<br />
**Abstract:** <details><summary>原文: </summary>World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>世界模型在理解和预测世界动态方面发挥着至关重要的作用，这对于视频生成至关重要。然而，现有的世界模型仅限于游戏或驾驶等特定场景，限制了它们捕捉一般世界动态环境复杂性的能力。因此，我们引入了WorldDreamer，这是一种开创性的世界模型，旨在促进对一般世界物理和运动的全面理解，从而显着增强视频生成的能力。 WorldDreamer 从大型语言模型的成功中汲取灵感，将世界建模视为无监督的视觉序列建模挑战。这是通过将视觉输入映射到离散标记并预测被屏蔽的标记来实现的。在此过程中，我们结合了多模式提示来促进世界模型内的交互。我们的实验表明，WorldDreamer 擅长生成不同场景的视频，包括自然场景和驾驶环境。 WorldDreamer 展示了执行文本到视频转换、图像到视频合成和视频编辑等任务的多功能性。这些结果强调了 WorldDreamer 在捕捉不同的一般世界环境中的动态元素方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.09985v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models**<br />
**Title_cn:** 时间洞察力增强：减轻多模态大语言模型中的时间幻觉<br />
**Authors:** Li Sun, Liuan Wang, Jun Sun, Takayuki Okatani<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型 (MLLM) 的最新进展显着增强了对多媒体内容的理解，将文本、图像和视频等多种模态结合在一起。然而，这些模型面临的一个关键挑战，特别是在处理视频输入时，是幻觉的发生——错误的感知或解释，特别是在事件层面。本研究引入了一种创新方法来解决 MLLM 中的事件级幻觉，重点关注视频内容中的特定时间理解。我们的方法利用了一种新颖的框架，该框架从事件查询和提供的视频中提取并利用特定于事件的信息来完善 MLLM 的响应。我们提出了一种独特的机制，可将按需事件查询分解为标志性操作。随后，我们使用 CLIP 和 BLIP2 等模型来预测事件发生的特定时间戳。我们使用 Charades-STA 数据集进行的评估表明，时间幻觉显着减少，事件相关反应的质量有所提高。这项研究不仅为解决 MLLM 的关键局限性提供了新的视角，而且还提供了一种在时间相关问题的背景下评估 MLLM 的定量可测量方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.09861v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**<br />
**Title_cn:** SkyEyeGPT：通过大型语言模型的指令调整来统一遥感视觉语言任务<br />
**Authors:** Yang Zhan, Zhitong Xiong, Yuan Yuan<br />
**Abstract:** <details><summary>原文: </summary>Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released in https://github.com/ZhanYang-nwpu/SkyEyeGPT.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型（LLM）最近已扩展到视觉语言领域，获得了令人印象深刻的通用多模式功能。然而，针对遥感（RS）数据的多模态大语言模型（MLLM）的探索仍处于起步阶段，性能并不令人满意。在这项工作中，我们介绍了 SkyEyeGPT，这是一种专门为 RS 视觉语言理解而设计的统一多模态大语言模型。为此，我们精心策划了 RS 多模态指令调优数据集，包括单任务和多任务对话指令。经过手动验证，我们获得了包含 968k 样本的高质量 RS 指令跟踪数据集。我们的研究表明，通过简单而有效的设计，SkyEyeGPT 在相当不同的任务上表现得非常好，而不需要额外的编码模块。具体来说，在通过对齐层将 RS 视觉特征投影到语言域后，它们与特定于任务的指令联合输入到基于 LLM 的 RS 解码器中，以预测 RS 开放式任务的答案。此外，我们设计了一种两阶段调整方法来增强不同粒度的指令跟踪和多轮对话能力。在 RS 视觉语言任务的 8 个数据集上进行的实验证明了 SkyEyeGPT 在图像级和区域级任务（例如字幕和视觉基础）方面的优越性。特别是，与 GPT-4V 相比，SkyEyeGPT 在一些定性测试中表现出了令人鼓舞的结果。在线演示、代码和数据集将在 https://github.com/ZhanYang-nwpu/SkyEyeGPT 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.09712v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Supervised Fine-tuning in turn Improves Visual Foundation Models**<br />
**Title_cn:** 有监督的微调反过来改进了视觉基础模型<br />
**Authors:** Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan<br />
**Abstract:** <details><summary>原文: </summary>Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，像 CLIP 这样的图文训练在视觉基础模型的预训练中占据了主导地位。随后人们努力将区域级视觉学习引入 CLIP 的预训练中，但由于缺乏大规模区域级数据集而面临可扩展性挑战。受到自然语言处理（例如指令调优）中的监督微调（SFT）的启发，我们探索了细粒度 SFT 在增强预训练后视觉基础模型生成方面的潜力。因此，提出了一种两阶段方法 ViSFT（Vision SFT）来释放视觉基础模型的细粒度知识。在 ViSFT 中，通过对一些域内任务执行视觉联合学习来增强视觉基础模型，然后在域外基准上进行测试。通过在 8 V100 GPU 上使用 ViSFT 在不到 2 天内进行更新，具有超过 4.4B 参数的视觉转换器在各种域外基准测试（包括视觉和视觉语言场景）上显示出改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.10222v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GPAvatar: Generalizable and Precise Head Avatar from Image(s)**<br />
**Title_cn:** GPAvatar：来自图像的可概括且精确的头像<br />
**Authors:** Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada<br />
**Abstract:** <details><summary>原文: </summary>Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose a framework named GPAvatar that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. The proposed method achieves faithful identity reconstruction, precise expression control, and multi-view consistency, demonstrating promising results for free-viewpoint rendering and novel view synthesis.</details>
**Abstract_cn:** <details><summary>译文: </summary>头部头像重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，已经引起了计算机视觉界的广泛关注。该领域的根本目标是忠实地再现头部虚拟形象并精确控制表情和姿势。现有方法分为基于 2D 的扭曲、基于网格和神经渲染方法，在维持多视图一致性、合并非面部信息以及推广到新身份方面提出了挑战。在本文中，我们提出了一个名为 GPAvatar 的框架，它可以在单次前向传递中根据一张或多张图像重建 3D 头部头像。这项工作的关键思想是引入由点云驱动的基于点的动态表情场，以精确有效地捕获表情。此外，我们在三平面规范领域中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，为自由视点渲染和新颖的视图合成展示了有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.10215v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **VMamba: Visual State Space Model**<br />
**Title_cn:** VMamba：视觉状态空间模型<br />
**Authors:** Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Yunfan Liu<br />
**Abstract:** <details><summary>原文: </summary>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at https://github.com/MzeroMiko/VMamba.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络（CNN）和视觉变换器（ViT）是视觉表示学习的两种最流行的基础模型。虽然 CNN 表现出卓越的可扩展性和线性复杂度。尽管在图像分辨率方面，ViT 的拟合能力超过了它们，但其复杂度却是二次方。仔细观察发现，ViT 通过结合全局感受野和动态权重，实现了卓越的视觉建模性能。这一观察促使我们提出一种新颖的架构，该架构继承了这些组件，同时提高了计算效率。为此，我们从最近引入的状态空间模型中汲取灵感，提出了视觉状态空间模型（VMamba），该模型在不牺牲全局感受野的情况下实现了线性复杂度。为了解决所遇到的方向敏感问题，我们引入了交叉扫描模块（CSM）来遍历空间域并将任何非因果视觉图像转换为顺序补丁序列。大量的实验结果证实，VMamba 不仅在各种视觉感知任务中表现出有前景的能力，而且随着图像分辨率的提高，与既定基准相比也表现出更明显的优势。源代码已在 https://github.com/MzeroMiko/VMamba 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.10166v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Explicitly Disentangled Representations in Object-Centric Learning**<br />
**Title_cn:** 以对象为中心的学习中的显式解缠表示<br />
**Authors:** Riccardo Majellaro, Jonathan Collu, Aske Plaat, Thomas M. Moerland<br />
**Abstract:** <details><summary>原文: </summary>Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.</details>
**Abstract_cn:** <details><summary>译文: </summary>从原始视觉数据中提取结构化表示是机器学习中一个重要且长期存在的挑战。最近，以对象为中心的表示的无监督学习技术引起了越来越多的兴趣。在这种背景下，增强潜在特征的鲁棒性可以提高下游任务训练的效率和效果。朝这个方向迈出的一个有希望的一步是理清导致数据变化的因素。此前，不变槽注意力将位置、尺度和方向与其余特征分开。扩展这种方法，我们专注于分离形状和纹理组件。特别是，我们提出了一种新颖的架构，该架构偏向以对象为中心的模型，将形状和纹理组件分解为潜在空间维度的两个不重叠的子集。这些子集是先验已知的，因此是在训练过程之前已知的。对一系列以对象为中心的基准的实验表明，我们的方法实现了所需的解缠，同时在大多数情况下还从数值上提高了基线性能。此外，我们还表明我们的方法可以为特定对象生成新颖的纹理或在具有不同形状的对象之间传输纹理。</details>
**PDF:** <http://arxiv.org/pdf/2401.10148v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Cross-Modality Perturbation Synergy Attack for Person Re-identification**<br />
**Title_cn:** 用于人员重新识别的跨模态扰动协同攻击<br />
**Authors:** Yunpeng Gong, others<br />
**Abstract:** <details><summary>原文: </summary>In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on two widely used cross-modality datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness of our method but also provided insights for future enhancements in the robustness of cross-modality ReID systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，围绕解决基于 RGB 图像的单模式行人重新识别 (ReID) 系统的安全问题开展了大量研究。然而，在涉及红外相机捕获图像的实际应用中更常见的跨模态场景的安全性尚未得到足够的重视。跨模态 ReID 的主要挑战在于有效处理不同模态之间的视觉差异。例如，与包含颜色信息的可见光图像不同，红外图像通常是灰度的。现有的攻击方法主要关注可见图像模态的特征，忽视了其他模态的特征以及不同模态之间数据分布的变化。这种疏忽可能会破坏这些方法在跨多种模式的图像检索中的有效性。这项研究代表了对跨模态 ReID 模型安全性的首次探索，并提出了一种专门为跨模态 ReID 设计的通用扰动攻击。这种攻击通过利用不同模态数据的梯度来优化扰动，从而破坏鉴别器并强化模态之间的差异。我们在两个广泛使用的跨模态数据集 RegDB 和 SYSU 上进行了实验，这不仅证明了我们方法的有效性，而且为未来增强跨模态 ReID 系统的鲁棒性提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2401.10090v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization**<br />
**Title_cn:** HCVP：利用分层对比视觉提示进行领域泛化<br />
**Authors:** Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.</details>
**Abstract_cn:** <details><summary>译文: </summary>领域泛化（DG）致力于通过学习不变特征来创建在未见过的场景中表现出色的机器学习模型。在 DG 中，将模型约束为固定结构或统一参数化以封装不变特征的普遍做法可能会无意中混合特定方面。这种方法很难区分域间变化的细微差别，并且可能会对某些域表现出偏见，从而阻碍域不变特征的精确学习。认识到这一点，我们引入了一种新颖的方法，旨在补充模型的领域级和特定于任务的特征。这种方法旨在指导模型更有效地将不变特征与特定特征分开，从而提高泛化能力。基于 DG 范式中视觉提示的新兴趋势，我们的工作引入了新颖的 \textbf{H}ierarchical \textbf{C}contrastive \textbf{V}isual \textbf{P}rompt (HCVP) 方法。这代表了该领域的重大进步，以其独特的生成提示方法、明确的模型结构和专门的损失函数而脱颖而出。与通常在整个数据集中共享的传统视觉提示不同，HCVP 利用通过提示对比学习增强的分层提示生成网络。这些生成提示是依赖于实例的，迎合不同领域和任务固有的独特特征。此外，我们设计了一个提示调制网络作为桥梁，有效地将生成的视觉提示合并到视觉变压器主干中。在五个 DG 数据集上进行的实验证明了 HCVP 的有效性，其性能优于现有的 DG 算法和适应协议。</details>
**PDF:** <http://arxiv.org/pdf/2401.09716v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting**<br />
**Title_cn:** GaussianBody：通过 3d 高斯泼溅重建穿着衣服的人体<br />
**Authors:** Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang<br />
**Abstract:** <details><summary>原文: </summary>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了一种基于 3D Gaussian Splatting 的新型服装人体重建方法，称为 GaussianBody。与昂贵的基于神经辐射的模型相比，3D 高斯分布最近在训练时间和渲染质量方面表现出了出色的性能。然而，由于复杂的非刚性变形和丰富的布料细节，将静态 3D 高斯泼溅模型应用于动态人体重建问题并非易事。为了解决这些挑战，我们的方法考虑显式姿势引导变形来关联规范空间和观察空间中的动态高斯，引入基于物理的先验和正则化变换有助于减轻两个空间之间的模糊性。在训练过程中，我们进一步提出了一种姿态细化策略来更新姿态回归，以补偿不准确的初始估计，并提出一种尺度分割机制来增强回归点云的密度。实验验证了我们的方法可以实现最先进的真实感小说视图渲染结果，具有动态服装人体的高质量细节，以及显式几何重建。</details>
**PDF:** <http://arxiv.org/pdf/2401.09720v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild**<br />
**Title_cn:** SHINOBI：通过 BRDF 优化在野外使用神经对象分解的形状和照明<br />
**Authors:** Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: https://shinobi.aengelhardt.com Video: https://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 SHINOBI，这是一个端到端框架，用于根据不同光照、姿势和背景捕获的物体图像重建形状、材质和光照。基于无约束图像集合的对象的逆渲染是计算机视觉和图形领域的一个长期挑战，需要对形状、辐射度和姿态进行联合优化。我们证明，基于多分辨率哈希编码的隐式形状表示可以通过联合相机对齐优化实现更快、更稳健的形状重建，其性能优于先前的工作。此外，为了能够编辑照明和物体反射率（即材质），我们联合优化 BRDF 和照明以及物体的形状。我们的方法与类无关，适用于对象的野外图像集合，为 AR/VR、电影、游戏等多种用例生成可重新点亮的 3D 资源。项目页面：https://shinobi.aengelhardt。 com 视频：https://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be</details>
**PDF:** <http://arxiv.org/pdf/2401.10171v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Measuring the Discrepancy between 3D Geometric Models using Directional Distance Fields**<br />
**Title_cn:** 使用定向距离场测量 3D 几何模型之间的差异<br />
**Authors:** Siyu Ren, Junhui Hou, Xiaodong Chen, Hongkai Xiong, Wenping Wang<br />
**Abstract:** <details><summary>原文: </summary>Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks. As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling. The source code is available at \url{https://github.com/rsy6318/DirDist}.</details>
**Abstract_cn:** <details><summary>译文: </summary>验证可以用点云或三角形网格表示的 3D 几何模型之间的差异是电路板应用的关键问题。现有的方法主要集中于直接建立两个模型之间的对应关系，然后聚合对应点之间的逐点距离，导致其效率低下或无效。在本文中，我们提出了 DirDist，一种高效、有效、鲁棒且可微分的 3D 几何数据距离度量。具体来说，我们基于所提出的 3D 模型隐式表示（即方向距离场（DDF））构建 DirDist，它定义 3D 点到模型的方向距离以捕获其局部表面几何形状。然后，我们将两个 3D 几何模型之间的差异转化为在同一域上定义的 DDF 之间的差异，自然地建立模型对应关系。为了展示 DirDist 的优势，我们探索了各种距离度量驱动的 3D 几何建模任务，包括模板曲面拟合、刚性配准、非刚性配准、场景流估计和人体姿势优化。大量实验表明，我们的 DirDist 在所有任务下都实现了显着更高的准确度。作为通用距离度量，DirDist 有潜力推动 3D 几何建模领域的发展。源代码可在 \url{https://github.com/rsy6318/DirDist} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.09736v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **fast graph-based denoising for point cloud color information**<br />
**Title_cn:** 基于图的快速点云颜色信息去噪<br />
**Authors:** Ryosuke Watanabe, Keisuke Nonaka, Eduardo Pavez, Tatsuya Kobayashi, Antonio Ortega<br />
**Abstract:** <details><summary>原文: </summary>Point clouds are utilized in various 3D applications such as cross-reality (XR) and realistic 3D displays. In some applications, e.g., for live streaming using a 3D point cloud, real-time point cloud denoising methods are required to enhance the visual quality. However, conventional high-precision denoising methods cannot be executed in real time for large-scale point clouds owing to the complexity of graph constructions with K nearest neighbors and noise level estimation. This paper proposes a fast graph-based denoising (FGBD) for a large-scale point cloud. First, high-speed graph construction is achieved by scanning a point cloud in various directions and searching adjacent neighborhoods on the scanning lines. Second, we propose a fast noise level estimation method using eigenvalues of the covariance matrix on a graph. Finally, we also propose a new low-cost filter selection method to enhance denoising accuracy to compensate for the degradation caused by the acceleration algorithms. In our experiments, we succeeded in reducing the processing time dramatically while maintaining accuracy relative to conventional denoising methods. Denoising was performed at 30fps, with frames containing approximately 1 million points.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云用于各种 3D 应用，例如跨现实 (XR) 和逼真的 3D 显示。在某些应用中，例如，对于使用 3D 点云的直播，需要实时点云去噪方法来增强视觉质量。然而，由于K近邻图构建和噪声水平估计的复杂性，传统的高精度去噪方法无法对大规模点云实时执行。本文提出了一种针对大规模点云的快速基于图的去噪（FGBD）。首先，通过在各个方向上扫描点云并搜索扫描线上的相邻邻域来实现高速图形构建。其次，我们提出了一种使用图上协方差矩阵的特征值的快速噪声水平估计方法。最后，我们还提出了一种新的低成本滤波器选择方法来提高去噪精度，以补偿加速算法造成的退化。在我们的实验中，我们成功地显着减少了处理时间，同时保持了相对于传统去噪方法的准确性。去噪以 30fps 进行，帧包含大约 100 万个点。</details>
**PDF:** <http://arxiv.org/pdf/2401.09721v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Eye Motion Matters for 3D Face Reconstruction**<br />
**Title_cn:** 眼动对于 3D 面部重建很重要<br />
**Authors:** Xuan Wang, Mengyuan Liu<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in single-image 3D face reconstruction have shown remarkable progress in various applications. Nevertheless, prevailing techniques tend to prioritize the global facial contour and expression, often neglecting the nuanced dynamics of the eye region. In response, we introduce an Eye Landmark Adjustment Module, complemented by a Local Dynamic Loss, designed to capture the dynamic features of the eyes area. Our module allows for flexible adjustment of landmarks, resulting in accurate recreation of various eye states. In this paper, we present a comprehensive evaluation of our approach, conducting extensive experiments on two datasets. The results underscore the superior performance of our approach, highlighting its significant contributions in addressing this particular challenge.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像 3D 人脸重建的最新进展在各种应用中都显示出显着的进展。然而，流行的技术倾向于优先考虑整体面部轮廓和表情，常常忽略眼睛区域的细微动态。为此，我们引入了眼睛地标调整模块，并辅以局部动态损失，旨在捕获眼睛区域的动态特征。我们的模块允许灵活调整地标，从而准确地再现各种眼睛状态。在本文中，我们对我们的方法进行了全面评估，对两个数据集进行了广泛的实验。结果强调了我们的方法的卓越性能，强调了它在应对这一特殊挑战方面的重大贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.09677v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Divide and not forget: Ensemble of selectively trained experts in Continual Learning**<br />
**Title_cn:** 分开但不要忘记：经过选择性培训的持续学习专家团队<br />
**Authors:** Grzegorz Rypeść, Sebastian Cygert, Valeriya Khan, Tomasz Trzciński, Bartosz Zieliński, Bartłomiej Twardowski<br />
**Abstract:** <details><summary>原文: </summary>Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>班级增量学习变得越来越流行，因为它可以帮助模型扩大其适用性，同时不会忘记他们已经知道的东西。该领域的趋势是使用专家混合技术，其中不同的模型一起工作来解决任务。然而，专家通常使用整个任务数据一次性进行训练，这使得他们很容易忘记并增加计算负担。为了解决这个限制，我们引入了一种名为 SEED 的新方法。 SEED 仅选择一个最适合所考虑任务的专家，并使用来自该任务的数据仅微调该专家。为此，每个专家用高斯分布代表每个类别，并根据这些分布的相似性选择最佳专家。因此，SEED 增加了专家内部的多样性和异质性，同时保持了这种集成方法的高度稳定性。大量实验表明，SEED 在各种场景的无样本设置中实现了最先进的性能，展示了通过持续学习中的数据实现专家多样化的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.10191v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **The Manga Whisperer: Automatically Generating Transcriptions for Comics**<br />
**Title_cn:** 漫画低语者：自动生成漫画转录<br />
**Authors:** Ragav Sachdeva, Andrew Zisserman<br />
**Abstract:** <details><summary>原文: </summary>In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way.   To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages. The code, evaluation datasets and the pre-trained model can be found at: https://github.com/ragavsachdeva/magi.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的几十年里，日本漫画（通常称为漫画）已经超越了文化和语言的界限，成为真正的世界性轰动。然而，漫画对视觉线索和插图的固有依赖使得视力障碍人士基本上无法理解。在这项工作中，我们力求解决这一重大障碍，旨在确保每个人都能欣赏并积极参与漫画。具体来说，我们解决了分类问题，即以全自动方式生成谁说了什么、何时说的转录。为此，我们做出以下贡献：（1）我们提出了一个统一的模型 Magi，它能够（a）检测面板、文本框和字符框，（b）按身份对字符进行聚类（不知道字符的数量） (c) 将对话与其发言者联系起来； （2）我们提出了一种新颖的方法，能够按照阅读顺序对检测到的文本框进行排序并生成对话记录； （3）我们使用公开可用的[英文]漫画页面注释此任务的评估基准。代码、评估数据集和预训练模型可以在以下位置找到：https://github.com/ragavsachdeva/magi。</details>
**PDF:** <http://arxiv.org/pdf/2401.10224v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data**<br />
**Title_cn:** AutoFT：通过优化 OOD 数据的超参数进行鲁棒微调<br />
**Authors:** Caroline Choi, Yoonho Lee, Annie Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn<br />
**Abstract:** <details><summary>原文: </summary>Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\%$ and $1.5\%$, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>基础模型编码丰富的表示，可以通过对特定于任务的数据进行微调来适应所需的任务。然而，在一种特定数据分布上微调模型通常会损害模型在其他分布上的原始性能。当前的鲁棒微调方法利用手工制作的正则化技术来限制对基础模型的微调过程。然而，很难精确地指定在微调过程中要保留基础模型的哪些特征，因为这取决于预训练、微调和评估数据分布如何相互关联。我们提出了 AutoFT，一种用于指导基础模型微调的数据驱动方法。 AutoFT 优化微调超参数，以最大限度地提高小型分布外 (OOD) 验证集的性能。为了以精细的方式指导微调，AutoFT 搜索一个高度表达的超参数空间，除了学习率和权重衰减值之外，其中还包括许多不同损失的权重系数。我们评估 AutoFT 的九种自然分布变化，其中包括域变化和子群体变化。我们的实验表明，AutoFT 显着提高了对新 OOD 数据的泛化能力，优于现有的稳健微调方法。值得注意的是，AutoFT 在 WILDS-iWildCam 和 WILDS-FMoW 基准测试中实现了新的最先进性能，分别比之前的最佳方法高出 6.0\%$ 和 1.5\%$。</details>
**PDF:** <http://arxiv.org/pdf/2401.10220v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields**<br />
**Title_cn:** 神经回声：深度卷积滤波器复制生物感受野<br />
**Authors:** Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu<br />
**Abstract:** <details><summary>原文: </summary>In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们提供的证据表明深度卷积核可以有效地复制在哺乳动物视网膜中观察到的生物感受野的复杂结构。我们提供来自各种最先进模型的经过训练的内核的分析，证实了这一证据。受这一有趣发现的启发，我们提出了一种从生物感受野中汲取灵感的初始化方案。对具有多个具有深度卷积特征的 CNN 架构的 ImageNet 数据集进行的实验分析表明，当使用生物学衍生的权重进行初始化时，学习模型的准确性显着增强。这奠定了受生物学启发的计​​算模型的潜力，可以进一步加深我们对视觉处理系统的理解并提高卷积网络的效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.10178v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Sub2Full: split spectrum to boost OCT despeckling without clean data**<br />
**Title_cn:** Sub2Full：分割光谱以增强 OCT 去斑效果，无需干净数据<br />
**Authors:** Lingyun Wang, Jose A Sahel, Shaohua Pi<br />
**Abstract:** <details><summary>原文: </summary>Optical coherence tomography (OCT) suffers from speckle noise, causing the deterioration of image quality, especially in high-resolution modalities like visible light OCT (vis-OCT). The potential of conventional supervised deep learning denoising methods is limited by the difficulty of obtaining clean data. Here, we proposed an innovative self-supervised strategy called Sub2Full (S2F) for OCT despeckling without clean data. This approach works by acquiring two repeated B-scans, splitting the spectrum of the first repeat as a low-resolution input, and utilizing the full spectrum of the second repeat as the high-resolution target. The proposed method was validated on vis-OCT retinal images visualizing sublaminar structures in outer retina and demonstrated superior performance over conventional Noise2Noise and Noise2Void schemes. The code is available at https://github.com/PittOCT/Sub2Full-OCT-Denoising.</details>
**Abstract_cn:** <details><summary>译文: </summary>光学相干断层扫描 (OCT) 会受到散斑噪声的影响，导致图像质量下降，尤其是在可见光 OCT (vis-OCT) 等高分辨率模式中。传统的监督深度学习去噪方法的潜力受到难以获得干净数据的限制。在这里，我们提出了一种名为 Sub2Full (S2F) 的创新自我监督策略，用于无需干净数据的 OCT 去斑。该方法的工作原理是采集两次重复的 B 扫描，将第一次重复的光谱分割为低分辨率输入，并利用第二次重复的全光谱作为高分辨率目标。该方法在可视化外视网膜层下结构的 vis-OCT 视网膜图像上进行了验证，并证明了优于传统 Noise2Noise 和 Noise2Void 方案的性能。代码可在 https://github.com/PittOCT/Sub2Full-OCT-Denoising 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10128v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack**<br />
**Title_cn:** 使用局部自适应对抗性颜色攻击来保护艺术品免受神经风格迁移<br />
**Authors:** Zhongliang Guo, Kaixuan Wang, Weiye Li, Yifei Qian, Ognjen Arandjelović, Lei Fang<br />
**Abstract:** <details><summary>原文: </summary>Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study confirm that by attacking NST using the proposed method results in visually worse neural style transfer, thus making it an effective solution for visual artwork protection.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经风格迁移（NST）在计算机视觉中被广泛采用，以生成任意风格的新图像。该过程利用神经网络将风格图像的美学元素与内容图像的结构方面融合成和谐集成的视觉结果。然而，未经授权的 NST 可以利用艺术品。这种滥用引起了对艺术家权利的社会技术关注，并推动了主动保护原创作品的技术方法的发展。对抗性攻击是机器学习安全中主要探讨的一个概念。我们的工作引入了这种技术来保护艺术家的知识产权。在本文中，局部自适应对抗性颜色攻击（LAACA）是一种以人眼无法察觉但破坏 NST 的方式改变图像的方法。具体来说，我们针对富含高频内容的图像区域设计扰动，这些扰动是通过破坏中间特征而生成的。我们的实验和用户研究证实，使用所提出的方法攻击 NST 会导致视觉上更差的神经风格迁移，从而使其成为视觉艺术品保护的有效解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2401.09673v1><br />
**Code:** null<br />

