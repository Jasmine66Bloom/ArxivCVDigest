## [UPDATED!] **2024-01-19** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Event detection from novel data sources: Leveraging satellite imagery alongside GPS traces**<br />
**Title_cn:** 来自新颖数据源的事件检测：利用卫星图像和 GPS 轨迹<br />
**Authors:** Ekin Ugurel, Steffen Coenen, Minda Zhou Chen, Cynthia Chen<br />
**Abstract:** <details><summary>原文: </summary>Rapid identification and response to breaking events, particularly those that pose a threat to human life such as natural disasters or conflicts, is of paramount importance. The prevalence of mobile devices and the ubiquity of network connectivity has generated a massive amount of temporally- and spatially-stamped data. Numerous studies have used mobile data to derive individual human mobility patterns for various applications. Similarly, the increasing number of orbital satellites has made it easier to gather high-resolution images capturing a snapshot of a geographical area in sub-daily temporal frequency. We propose a novel data fusion methodology integrating satellite imagery with privacy-enhanced mobile data to augment the event inference task, whether in real-time or historical. In the absence of boots on the ground, mobile data is able to give an approximation of human mobility, proximity to one another, and the built environment. On the other hand, satellite imagery can provide visual information on physical changes to the built and natural environment. The expected use cases for our methodology include small-scale disaster detection (i.e., tornadoes, wildfires, and floods) in rural regions, search and rescue operation augmentation for lost hikers in remote wilderness areas, and identification of active conflict areas and population displacement in war-torn states. Our implementation is open-source on GitHub: https://github.com/ekinugurel/SatMobFusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>快速识别和应对突发事件，特别是那些对人类生命构成威胁的事件，例如自然灾害或冲突，至关重要。移动设备的流行和网络连接的普遍存在产生了大量的时间和空间标记数据。许多研究都使用移动数据来得出各种应用的个体人类移动模式。同样，轨道卫星数量的增加使得收集高分辨率图像变得更加容易，这些图像以次日时间频率捕捉地理区域的快照。我们提出了一种新颖的数据融合方法，将卫星图像与隐私增强的移动数据相结合，以增强事件推理任务，无论是实时的还是历史的。在地面上没有靴子的情况下，移动数据能够大致了解人类的流动性、彼此的接近程度以及建筑环境。另一方面，卫星图像可以提供有关建筑和自然环境物理变化的视觉信息。我们的方法的预期用例包括农村地区的小规模灾害检测（即龙卷风、野火和洪水）、加强偏远荒野地区失踪徒步旅行者的搜救行动，以及识别活跃冲突地区和人口流离失所。饱受战争蹂躏的国家。我们的实现在 GitHub 上是开源的：https://github.com/ekinugurel/SatMobFusion。</details>
**PDF:** <http://arxiv.org/pdf/2401.10890v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ActAnywhere: Subject-Aware Video Background Generation**<br />
**Title_cn:** ActAnywhere：主题感知视频背景生成<br />
**Authors:** Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang<br />
**Abstract:** <details><summary>原文: </summary>Generating video background that tailors to foreground subject motion is an important problem for the movie industry and visual effects community. This task involves synthesizing background that aligns with the motion and appearance of the foreground subject, while also complies with the artist's creative intention. We introduce ActAnywhere, a generative model that automates this process which traditionally requires tedious manual efforts. Our model leverages the power of large-scale video diffusion models, and is specifically tailored for this task. ActAnywhere takes a sequence of foreground subject segmentation as input and an image that describes the desired scene as condition, to produce a coherent video with realistic foreground-background interactions while adhering to the condition frame. We train our model on a large-scale dataset of human-scene interaction videos. Extensive evaluations demonstrate the superior performance of our model, significantly outperforming baselines. Moreover, we show that ActAnywhere generalizes to diverse out-of-distribution samples, including non-human subjects. Please visit our project webpage at https://actanywhere.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成适合前景主体运动的视频背景是电影行业和视觉效果社区的一个重要问题。此任务涉及合成与前景主体的运动和外观一致的背景，同时也符合艺术家的创作意图。我们引入了 ActAnywhere，这是一种生成模型，可以自动化这个传统上需要繁琐的手动工作的过程。我们的模型利用了大规模视频扩散模型的强大功能，并且是专门针对此任务量身定制的。 ActAnywhere 将一系列前景主体分割作为输入，并将描述所需场景的图像作为条件，以生成具有真实前景-背景交互的连贯视频，同时遵循条件框架。我们在人类场景交互视频的大规模数据集上训练我们的模型。广泛的评估证明了我们模型的卓越性能，显着优于基线。此外，我们还表明 ActAnywhere 可以推广到不同的分布外样本，包括非人类受试者。请访问我们的项目网页：https://actanywhere.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2401.10822v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision**<br />
**Title_cn:** RAD-DINO：探索文本监督之外的可扩展医学图像编码器<br />
**Authors:** Fernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.</details>
**Abstract_cn:** <details><summary>译文: </summary>语言监督的预训练已被证明是一种从图像中提取语义上有意义的特征的有价值的方法，可以作为计算机视觉和医学成像领域多模态系统的基础元素。然而，所得到的特征受到文本中包含的信息的限制。这在医学成像中尤其成问题，因为放射科医生的书面发现集中于特定的观察结果；由于担心个人健康信息泄露，配对图像-文本数据的稀缺加剧了这一挑战。在这项工作中，我们从根本上挑战了学习通用生物医学成像编码器时普遍依赖语言监督的现象。我们推出了 RAD-DINO，这是一种仅根据单模态生物医学成像数据进行预训练的生物医学图像编码器，它在各种基准上获得了与最先进的生物医学语言监督模型相似或更好的性能。具体来说，学习表示的质量是根据标准成像任务（分类和语义分割）和视觉语言对齐任务（从图像生成文本报告）进行评估的。为了进一步证明语言监督的缺点，我们表明 RAD-DINO 的特征与其他医疗记录（例如性别或年龄）的相关性比语言监督模型更好，而放射学报告中通常没有提到这些模型。最后，我们进行了一系列消融，以确定 RAD-DINO 性能的因素；值得注意的是，我们观察到 RAD-DINO 的下游性能与训练数据的数量和多样性很好地扩展，这表明仅图像监督是训练基础生物医学图像编码器的可扩展方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.10815v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions**<br />
**Title_cn:** 测量场景级物体对物体检测的影响：对检测决策的定量解释<br />
**Authors:** Lynn Vonder Haar, Timothy Elvira, Luke Newcomb, Omar Ochoa<br />
**Abstract:** <details><summary>原文: </summary>Although accuracy and other common metrics can provide a useful window into the performance of an object detection model, they lack a deeper view of the model's decision process. Regardless of the quality of the training data and process, the features that an object detection model learns cannot be guaranteed. A model may learn a relationship between certain background context, i.e., scene level objects, and the presence of the labeled classes. Furthermore, standard performance verification and metrics would not identify this phenomenon. This paper presents a new black box explainability method for additional verification of object detection models by finding the impact of scene level objects on the identification of the objects within the image. By comparing the accuracies of a model on test data with and without certain scene level objects, the contributions of these objects to the model's performance becomes clearer. The experiment presented here will assess the impact of buildings and people in image context on the detection of emergency road vehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the presence of a scene level object will indicate the model's reliance on that object to make its detections. The results of this research lead to providing a quantitative explanation of the object detection model's decision process, enabling a deeper understanding of the model's performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管准确性和其他常见指标可以提供了解对象检测模型性能的有用窗口，但它们缺乏对模型决策过程的更深入了解。无论训练数据和过程的质量如何，都无法保证目标检测模型学习到的特征。模型可以学习某些背景上下文（即场景级对象）与标记类的存在之间的关系。此外，标准性能验证和指标无法识别这种现象。本文提出了一种新的黑盒可解释性方法，通过发现场景级对象对图像内对象识别的影响，对对象检测模型进行额外验证。通过比较具有和不具有某些场景级对象的测试数据上的模型的准确性，这些对象对模型性能的贡献变得更加清晰。这里介绍的实验将通过微调的 YOLOv8 模型评估图像环境中的建筑物和人员对紧急道路车辆检测的影响。场景级对象存在时准确度的大幅提高将表明模型依赖该对象进行检测。这项研究的结果为目标检测模型的决策过程提供了定量解释，使人们能够更深入地了解模型的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.10790v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **HiCD: Change Detection in Quality-Varied Images via Hierarchical Correlation Distillation**<br />
**Title_cn:** HiCD：通过分层相关蒸馏对质量变化的图像进行变化检测<br />
**Authors:** Chao Pang, Xingxing Weng, Jiang Wu, Qiang Wang, Gui-Song Xia<br />
**Abstract:** <details><summary>原文: </summary>Advanced change detection techniques primarily target image pairs of equal and high quality. However, variations in imaging conditions and platforms frequently lead to image pairs with distinct qualities: one image being high-quality, while the other being low-quality. These disparities in image quality present significant challenges for understanding image pairs semantically and extracting change features, ultimately resulting in a notable decline in performance. To tackle this challenge, we introduce an innovative training strategy grounded in knowledge distillation. The core idea revolves around leveraging task knowledge acquired from high-quality image pairs to guide the model's learning process when dealing with image pairs that exhibit differences in quality. Additionally, we develop a hierarchical correlation distillation approach (involving self-correlation, cross-correlation, and global correlation). This approach compels the student model to replicate the correlations inherent in the teacher model, rather than focusing solely on individual features. This ensures effective knowledge transfer while maintaining the student model's training flexibility.</details>
**Abstract_cn:** <details><summary>译文: </summary>先进的变化检测技术主要针对相同且高质量的图像对。然而，成像条件和平台的变化经常导致图像对具有不同的质量：一个图像是高质量的，而另一个图像是低质量的。图像质量的这些差异给从语义上理解图像对和提取变化特征带来了重大挑战，最终导致性能显着下降。为了应对这一挑战，我们引入了一种基于知识蒸馏的创新培训策略。核心思想围绕利用从高质量图像对获取的任务知识来指导模型在处理表现出质量差异的图像对时的学习过程。此外，我们还开发了一种分层相关蒸馏方法（涉及自相关、互相关和全局相关）。这种方法迫使学生模型复制教师模型中固有的相关性，而不是仅仅关注个体特征。这确保了有效的知识转移，同时保持了学生模型的培训灵活性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10752v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Character Recognition in Byzantine Seals with Deep Neural Networks**<br />
**Title_cn:** 使用深度神经网络进行拜占庭印章中的字符识别<br />
**Authors:** Théophile Rageau, Laurence Likforman-Sulem, Attilio Fiandrotti, Victoria Eyharabide, Béatrice Caseau, Jean-Claude Cheynet<br />
**Abstract:** <details><summary>原文: </summary>Seals are small coin-shaped artifacts, mostly made of lead, held with strings to seal letters. This work presents the first attempt towards automatic reading of text on Byzantine seal images.Byzantine seals are generally decorated with iconography on the obverse side and Greek text on the reverse side. Text may include the sender's name, position in the Byzantine aristocracy, and elements of prayers. Both text and iconography are precious literary sources that wait to be exploited electronically, so the development of computerized systems for interpreting seals images is of paramount importance. This work's contribution is hence a deep, two-stages, character reading pipeline for transcribing Byzantine seal images. A first deep convolutional neural network (CNN) detects characters in the seal (character localization). A second convolutional network reads the localized characters (character classification). Finally, a diplomatic transcription of the seal is provided by post-processing the two network outputs. We provide an experimental evaluation of each CNN in isolation and both CNNs in combination. All performances are evaluated by cross-validation. Character localization achieves a mean average precision (mAP@0.5) greater than 0.9. Classification of characters cropped from ground truth bounding boxes achieves Top-1 accuracy greater than 0.92. End-to-end evaluation shows the efficiency of the proposed approach when compared to the SoTA for similar tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>印章是硬币形状的小工艺品，大多由铅制成，用绳子固定以密封字母。这项工作首次尝试自动阅读拜占庭印章图像上的文字。拜占庭印章的正面通常装饰有图像，反面则装饰有希腊文字。文本可能包括发送者的姓名、拜占庭贵族的地位以及祈祷的内容。文本和图像都是宝贵的文学资源，有待以电子方式利用，因此开发用于解释印章图像的计算机化系统至关重要。因此，这项工作的贡献是一个用于转录拜占庭印章图像的深度、两阶段的字符读取管道。第一个深度卷积神经网络 (CNN) 检测印章中的字符（字符定位）。第二个卷积网络读取本地化字符（字符分类）。最后，通过对两个网络输出进行后处理来提供印章的外交转录。我们对每个单独的 CNN 以及两个 CNN 的组合进行了实验评估。所有性能均通过交叉验证进行评估。字符定位的平均精度 (mAP@0.5) 大于 0.9。从真实边界框裁剪的字符分类实现了大于 0.92 的 Top-1 精度。端到端评估显示了所提出的方法与类似任务的 SoTA 相比的效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.10741v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion**<br />
**Title_cn:** 移除和选择：通过粗到精融合改进 RGB 红外物体检测<br />
**Authors:** Tianyi Zhao, Maoxun Yuan, Xingxing Wei<br />
**Abstract:** <details><summary>原文: </summary>Object detection in visible (RGB) and infrared (IR) images has been widely applied in recent years. Leveraging the complementary characteristics of RGB and IR images, the object detector provides reliable and robust object localization from day to night. Existing fusion strategies directly inject RGB and IR images into convolution neural networks, leading to inferior detection performance. Since the RGB and IR features have modality-specific noise, these strategies will worsen the fused features along with the propagation. Inspired by the mechanism of human brain processing multimodal information, this work introduces a new coarse-to-fine perspective to purify and fuse two modality features. Specifically, following this perspective, we design a Redundant Spectrum Removal module to coarsely remove interfering information within each modality and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called Removal and Selection Detector (RSDet). Extensive experiments on three RGB-IR object detection datasets verify the superior performance of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，可见光（RGB）和红外（IR）图像中的目标检测得到了广泛的应用。利用 RGB 和 IR 图像的互补特性，物体检测器可从白天到夜间提供可靠且稳健的物体定位。现有的融合策略直接将RGB和IR图像注入卷积神经网络，导致检测性能较差。由于 RGB 和 IR 特征具有特定于模态的噪声，因此这些策略会随着传播而恶化融合特征。受人脑处理多模态信息机制的启发，这项工作引入了一种新的从粗到细的视角来纯化和融合两种模态特征。具体来说，按照这个观点，我们设计了一个冗余频谱去除模块来粗略地去除每种模态中的干扰信息，并设计了一个动态特征选择模块来精细地选择特征融合所需的特征。为了验证从粗到精融合策略的有效性，我们构建了一个新的目标检测器，称为移除和选择检测器（RSDet）。对三个 RGB-IR 目标检测数据集的大量实验验证了我们方法的优越性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.10731v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **BadODD: Bangladeshi Autonomous Driving Object Detection Dataset**<br />
**Title_cn:** BadODD：孟加拉国自动驾驶物体检测数据集<br />
**Authors:** Mirza Nihal Baig, Rony Hajong, Mahdi Murshed Patwary, Mohammad Shahidur Rahman, Husne Ara Chowdhury<br />
**Abstract:** <details><summary>原文: </summary>We propose a comprehensive dataset for object detection in diverse driving environments across 9 districts in Bangladesh. The dataset, collected exclusively from smartphone cameras, provided a realistic representation of real-world scenarios, including day and night conditions. Most existing datasets lack suitable classes for autonomous navigation on Bangladeshi roads, making it challenging for researchers to develop models that can handle the intricacies of road scenarios. To address this issue, the authors proposed a new set of classes based on characteristics rather than local vehicle names. The dataset aims to encourage the development of models that can handle the unique challenges of Bangladeshi road scenarios for the effective deployment of autonomous vehicles. The dataset did not consist of any online images to simulate real-world conditions faced by autonomous vehicles. The classification of vehicles is challenging because of the diverse range of vehicles on Bangladeshi roads, including those not found elsewhere in the world. The proposed classification system is scalable and can accommodate future vehicles, making it a valuable resource for researchers in the autonomous vehicle sector.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个全面的数据集，用于孟加拉国 9 个地区不同驾驶环境中的物体检测。该数据集专门从智能手机摄像头收集，提供了真实世界场景的真实再现，包括白天和夜间条件。大多数现有数据集缺乏适合孟加拉国道路自主导航的类别，这使得研究人员难以开发能够处理复杂道路场景的模型。为了解决这个问题，作者提出了一组基于特征而不是本地车辆名称的新类。该数据集旨在鼓励开发能够应对孟加拉国道路场景的独特挑战的模型，以有效部署自动驾驶汽车。该数据集不包含任何模拟自动驾驶汽车面临的现实条件的在线图像。车辆分类具有挑战性，因为孟加拉国道路上的车辆种类繁多，其中包括世界其他地方没有的车辆。所提出的分类系统具有可扩展性，可以适应未来的车辆，使其成为自动驾驶汽车领域研究人员的宝贵资源。</details>
**PDF:** <http://arxiv.org/pdf/2401.10659v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Towards Universal Unsupervised Anomaly Detection in Medical Imaging**<br />
**Title_cn:** 迈向医学成像中普遍的无监督异常检测<br />
**Authors:** Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, Julia A. Schnabel<br />
**Abstract:** <details><summary>原文: </summary>The increasing complexity of medical imaging data underscores the need for advanced anomaly detection methods to automatically identify diverse pathologies. Current methods face challenges in capturing the broad spectrum of anomalies, often limiting their use to specific lesion types in brain scans. To address this challenge, we introduce a novel unsupervised approach, termed \textit{Reversed Auto-Encoders (RA)}, designed to create realistic pseudo-healthy reconstructions that enable the detection of a wider range of pathologies. We evaluate the proposed method across various imaging modalities, including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray, and chest X-ray, and demonstrate superior performance in detecting anomalies compared to existing state-of-the-art methods. Our unsupervised anomaly detection approach may enhance diagnostic accuracy in medical imaging by identifying a broader range of unknown pathologies. Our code is publicly available at: \url{https://github.com/ci-ber/RA}.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学成像数据日益复杂，强调需要先进的异常检测方法来自动识别不同的病理。目前的方法在捕获广泛的异常情况方面面临挑战，通常将它们的使用限制在脑部扫描中的特定病变类型。为了应对这一挑战，我们引入了一种新颖的无监督方法，称为 \textit{反向自动编码器（RA）}，旨在创建现实的伪健康重建，从而能够检测更广泛的病理。我们评估了各种成像方式所提出的方法，包括大脑磁共振成像 (MRI)、儿科腕部 X 射线和胸部 X 射线，并证明了与现有最​​先进技术相比，该方法在检测异常方面具有卓越的性能方法。我们的无监督异常检测方法可以通过识别更广泛的未知病理来提高医学成像的诊断准确性。我们的代码公开于：\url{https://github.com/ci-ber/RA}。</details>
**PDF:** <http://arxiv.org/pdf/2401.10637v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **MAEDiff: Masked Autoencoder-enhanced Diffusion Models for Unsupervised Anomaly Detection in Brain Images**<br />
**Title_cn:** MAEDiff：用于脑图像中无监督异常检测的掩模自动编码器增强扩散模型<br />
**Authors:** Rui Xu, Yunke Wang, Bo Du<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised anomaly detection has gained significant attention in the field of medical imaging due to its capability of relieving the costly pixel-level annotation. To achieve this, modern approaches usually utilize generative models to produce healthy references of the diseased images and then identify the abnormalities by comparing the healthy references and the original diseased images. Recently, diffusion models have exhibited promising potential for unsupervised anomaly detection in medical images for their good mode coverage and high sample quality. However, the intrinsic characteristics of the medical images, e.g. the low contrast, and the intricate anatomical structure of the human body make the reconstruction challenging. Besides, the global information of medical images often remain underutilized. To address these two issues, we propose a novel Masked Autoencoder-enhanced Diffusion Model (MAEDiff) for unsupervised anomaly detection in brain images. The MAEDiff involves a hierarchical patch partition. It generates healthy images by overlapping upper-level patches and implements a mechanism based on the masked autoencoders operating on the sub-level patches to enhance the condition on the unnoised regions. Extensive experiments on data of tumors and multiple sclerosis lesions demonstrate the effectiveness of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督异常检测由于其能够减轻昂贵的像素级注释而在医学成像领域获得了极大的关注。为了实现这一目标，现代方法通常利用生成模型来生成患病图像的健康参考，然后通过比较健康参考和原始患病图像来识别异常。最近，扩散模型因其良好的模式覆盖率和高样本质量而在医学图像中的无监督异常检测方面展现出了巨大的潜力。然而，医学图像的内在特征，例如人体的低对比度和复杂的解剖结构使得重建具有挑战性。此外，医学图像的全局信息通常仍未得到充分利用。为了解决这两个问题，我们提出了一种新颖的掩模自动编码器增强扩散模型（MAEDiff），用于大脑图像中的无监督异常检测。 MAEDiff 涉及分层补丁分区。它通过重叠上层补丁生成健康图像，并实现一种基于对子层补丁进行操作的屏蔽自动编码器的机制，以增强无噪声区域的条件。对肿瘤和多发性硬化症病变数据的广泛实验证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10561v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Symbol as Points: Panoptic Symbol Spotting via Point-based Representation**<br />
**Title_cn:** 符号作为点：通过基于点的表示进行全景符号识别<br />
**Authors:** Wenlong Liu, Tianyu Yang, Yuhan Wang, Qizhi Yu, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>This work studies the problem of panoptic symbol spotting, which is to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from computer-aided design (CAD) drawings. Existing methods typically involve either rasterizing the vector graphics into images and using image-based methods for symbol spotting, or directly building graphs and using graph neural networks for symbol recognition. In this paper, we take a different approach, which treats graphic primitives as a set of 2D points that are locally connected and use point cloud segmentation methods to tackle it. Specifically, we utilize a point transformer to extract the primitive features and append a mask2former-like spotting head to predict the final output. To better use the local connection information of primitives and enhance their discriminability, we further propose the attention with connection module (ACM) and contrastive connection learning scheme (CCL). Finally, we propose a KNN interpolation mechanism for the mask attention module of the spotting head to better handle primitive mask downsampling, which is primitive-level in contrast to pixel-level for the image. Our approach, named SymPoint, is simple yet effective, outperforming recent state-of-the-art method GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD dataset. The source code and models will be available at https://github.com/nicehuster/SymPoint.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作研究全景符号识别问题，即从计算机辅助设计 (CAD) 图纸中识别和解析可数对象实例（窗户、门、桌子等）和不可数物体（墙壁、栏杆等） 。现有方法通常涉及将矢量图形光栅化为图像并使用基于图像的方法进行符号识别，或者直接构建图并使用图神经网络进行符号识别。在本文中，我们采用不同的方法，将图形基元视为一组局部连接的二维点，并使用点云分割方法来处理它。具体来说，我们利用点变换器来提取原始特征，并附加一个类似 mask2former 的定位头来预测最终输出。为了更好地利用基元的局部连接信息并增强其可辨别性，我们进一步提出了连接模块注意（ACM）和对比连接学习方案（CCL）。最后，我们为定位头的掩模注意模块提出了一种 KNN 插值机制，以更好地处理原始掩模下采样，与图像的像素级相比，原始掩模下采样是原始级的。我们的方法名为 SymPoint，简单而有效，在 FloorPlanCAD 数据集上的 PQ 绝对值提高了 9.6%，RQ 绝对值提高了 10.4%，优于最新的最先进方法 GAT-CADNet。源代码和模型可在 https://github.com/nicehuster/SymPoint 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10556v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **I-SplitEE: Image classification in Split Computing DNNs with Early Exits**<br />
**Title_cn:** I-SplitEE：早期退出的分割计算 DNN 中的图像分类<br />
**Authors:** Divya Jyoti Bajpai, Aastha Jaiswal, Manjesh Kumar Hanawal<br />
**Abstract:** <details><summary>原文: </summary>The recent advances in Deep Neural Networks (DNNs) stem from their exceptional performance across various domains. However, their inherent large size hinders deploying these networks on resource-constrained devices like edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud computation offloading (split computing) to integrating early exits within DNN layers. Our work presents an innovative unified approach merging early exits and split computing. We determine the 'splitting layer', the optimal depth in the DNN for edge device computations, and whether to infer on edge device or be offloaded to the cloud for inference considering accuracy, computational efficiency, and communication costs. Also, Image classification faces diverse environmental distortions, influenced by factors like time of day, lighting, and weather. To adapt to these distortions, we introduce I-SplitEE, an online unsupervised algorithm ideal for scenarios lacking ground truths and with sequential data. Experimental validation using Caltech-256 and Cifar-10 datasets subjected to varied distortions showcases I-SplitEE's ability to reduce costs by a minimum of 55% with marginal performance degradation of at most 5%.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络 (DNN) 的最新进展源于其在各个领域的卓越性能。然而，它们固有的大尺寸阻碍了在边缘、移动和物联网平台等资源受限的设备上部署这些网络。从部分云计算卸载（拆分计算）到将早期退出集成到 DNN 层中，各种策略已经出现。我们的工作提出了一种创新的统一方法，融合了早期退出和分割计算。我们考虑准确性、计算效率和通信成本，确定“分割层”、DNN 中用于边缘设备计算的最佳深度，以及是在边缘设备上进行推理还是卸载到云端进行推理。此外，图像分类面临着各种环境扭曲，受一天中的时间、照明和天气等因素的影响。为了适应这些扭曲，我们引入了 I-SplitEE，这是一种在线无监督算法，非常适合缺乏基本事实和序列数据的场景。使用经受各种扭曲的 Caltech-256 和 Cifar-10 数据集进行的实验验证表明，I-SplitEE 能够将成本降低至少 55%，同时边际性能下降最多 5%。</details>
**PDF:** <http://arxiv.org/pdf/2401.10541v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Focaler-IoU: More Focused Intersection over Union Loss**<br />
**Title_cn:** Focaler-IoU：更集中于联合损失的交叉点<br />
**Authors:** Hao Zhang, Shuaijie Zhang<br />
**Abstract:** <details><summary>原文: </summary>Bounding box regression plays a crucial role in the field of object detection, and the positioning accuracy of object detection largely depends on the loss function of bounding box regression. Existing researchs improve regression performance by utilizing the geometric relationship between bounding boxes, while ignoring the impact of difficult and easy sample distribution on bounding box regression. In this article, we analyzed the impact of difficult and easy sample distribution on regression results, and then proposed Focaler-IoU, which can improve detector performance in different detection tasks by focusing on different regression samples. Finally, comparative experiments were conducted using existing advanced detectors and regression methods for different detection tasks, and the detection performance was further improved by using the method proposed in this paper.Code is available at \url{https://github.com/malagoutou/Focaler-IoU}.</details>
**Abstract_cn:** <details><summary>译文: </summary>边界框回归在目标检测领域起着至关重要的作用，目标检测的定位精度很大程度上取决于边界框回归的损失函数。现有研究利用边界框之间的几何关系来提高回归性能，而忽略了难易样本分布对边界框回归的影响。在本文中，我们分析了难易样本分布对回归结果的影响，然后提出了Focaler-IoU，它可以通过关注不同的回归样本来提高检测器在不同检测任务中的性能。最后，利用现有的先进检测器和回归方法针对不同的检测任务进行了对比实验，利用本文提出的方法进一步提高了检测性能。代码可见\url{https://github.com/malagoutou /Focaler-IoU}。</details>
**PDF:** <http://arxiv.org/pdf/2401.10525v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Exploring Color Invariance through Image-Level Ensemble Learning**<br />
**Title_cn:** 通过图像级集成学习探索颜色不变性<br />
**Authors:** Yunpeng Gong, Jiaquan Li, Lifei Chen, Min Jiang<br />
**Abstract:** <details><summary>原文: </summary>In the field of computer vision, the persistent presence of color bias, resulting from fluctuations in real-world lighting and camera conditions, presents a substantial challenge to the robustness of models. This issue is particularly pronounced in complex wide-area surveillance scenarios, such as person re-identification and industrial dust segmentation, where models often experience a decline in performance due to overfitting on color information during training, given the presence of environmental variations. Consequently, there is a need to effectively adapt models to cope with the complexities of camera conditions. To address this challenge, this study introduces a learning strategy named Random Color Erasing, which draws inspiration from ensemble learning. This strategy selectively erases partial or complete color information in the training data without disrupting the original image structure, thereby achieving a balanced weighting of color features and other features within the neural network. This approach mitigates the risk of overfitting and enhances the model's ability to handle color variation, thereby improving its overall robustness. The approach we propose serves as an ensemble learning strategy, characterized by robust interpretability. A comprehensive analysis of this methodology is presented in this paper. Across various tasks such as person re-identification and semantic segmentation, our approach consistently improves strong baseline methods. Notably, in comparison to existing methods that prioritize color robustness, our strategy significantly enhances performance in cross-domain scenarios. The code available at \url{https://github.com/layumi/Person\_reID\_baseline\_pytorch/blob/master/random\_erasing.py} or \url{https://github.com/finger-monkey/Data-Augmentation}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在计算机视觉领域，由于现实世界的照明和相机条件的波动而导致的颜色偏差持续存在，这对模型的鲁棒性提出了重大挑战。这个问题在复杂的广域监控场景中尤其明显，例如人员重新识别和工业灰尘分割，在这些场景中，由于环境变化的存在，模型在训练过程中经常会因颜色信息的过度拟合而导致性能下降。因此，需要有效地调整模型以应对相机条件的复杂性。为了应对这一挑战，本研究引入了一种名为“随机颜色擦除”的学习策略，该策略从集成学习中汲取灵感。该策略选择性地擦除训练数据中的部分或全部颜色信息，而不破坏原始图像结构，从而实现神经网络内颜色特征和其他特征的平衡权重。这种方法减轻了过度拟合的风险，并增强了模型处理颜色变化的能力，从而提高了其整体稳健性。我们提出的方法作为一种集成学习策略，其特点是具有强大的可解释性。本文对该方法进行了全面分析。在人员重新识别和语义分割等各种任务中，我们的方法不断改进强大的基线方法。值得注意的是，与优先考虑颜色稳健性的现有方法相比，我们的策略显着提高了跨域场景中的性能。代码可在 \url{https://github.com/layumi/Person\_reID\_baseline\_pytorch/blob/master/random\_erasing.py} 或 \url{https://github.com/finger-monkey /数据增强}。</details>
**PDF:** <http://arxiv.org/pdf/2401.10512v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Enhancing medical vision-language contrastive learning via inter-matching relation modelling**<br />
**Title_cn:** 通过相互匹配关系建模增强医学视觉语言对比学习<br />
**Authors:** Mingjian Li, Mingyuan Meng, Michael Fulham, David Dagan Feng, Lei Bi, Jinman Kim<br />
**Abstract:** <details><summary>原文: </summary>Medical image representations can be learned through medical vision-language contrastive learning (mVLCL) where medical imaging reports are used as weak supervision through image-text alignment. These learned image representations can be transferred to and benefit various downstream medical vision tasks such as disease classification and segmentation. Recent mVLCL methods attempt to align image sub-regions and the report keywords as local-matchings. However, these methods aggregate all local-matchings via simple pooling operations while ignoring the inherent relations between them. These methods therefore fail to reason between local-matchings that are semantically related, e.g., local-matchings that correspond to the disease word and the location word (semantic-relations), and also fail to differentiate such clinically important local-matchings from others that correspond to less meaningful words, e.g., conjunction words (importance-relations). Hence, we propose a mVLCL method that models the inter-matching relations between local-matchings via a relation-enhanced contrastive learning framework (RECLF). In RECLF, we introduce a semantic-relation reasoning module (SRM) and an importance-relation reasoning module (IRM) to enable more fine-grained report supervision for image representation learning. We evaluated our method using four public benchmark datasets on four downstream tasks, including segmentation, zero-shot classification, supervised classification, and cross-modal retrieval. Our results demonstrated the superiority of our RECLF over the state-of-the-art mVLCL methods with consistent improvements across single-modal and cross-modal tasks. These results suggest that our RECLF, by modelling the inter-matching relations, can learn improved medical image representations with better generalization capabilities.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像表示可以通过医学视觉语言对比学习（mVLCL）来学习，其中医学成像报告通过图像文本对齐用作弱监督。这些学习到的图像表示可以转移到并有益于各种下游医学视觉任务，例如疾病分类和分割。最近的 mVLCL 方法尝试将图像子区域和报告关键字对齐为局部匹配。然而，这些方法通过简单的池操作聚合所有本地匹配，而忽略了它们之间的内在关系。因此，这些方法无法在语义相关的局部匹配（例如，对应于疾病词和位置词（语义关系）的局部匹配）之间进行推理，并且也无法将此类临床上重要的局部匹配与其他局部匹配区分开来。对应于意义较小的单词，例如连词（重要性关系）。因此，我们提出了一种 mVLCL 方法，通过关系增强对比学习框架（RECLF）对局部匹配之间的相互匹配关系进行建模。在 RECLF 中，我们引入了语义关系推理模块（SRM）和重要性关系推理模块（IRM），以便为图像表示学习提供更细粒度的报告监督。我们使用四个公共基准数据集对四个下游任务（包括分割、零样本分类、监督分类和跨模式检索）评估了我们的方法。我们的结果证明了 RECLF 相对于最先进的 mVLCL 方法的优越性，在单模态和跨模态任务中都有一致的改进。这些结果表明，我们的 RECLF 通过对相互匹配关系进行建模，可以学习具有更好泛化能力的改进的医学图像表示。</details>
**PDF:** <http://arxiv.org/pdf/2401.10501v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss**<br />
**Title_cn:** 通过测地线蒸馏损失缓解 CLIP 引导图像变形中的稳定性-可塑性困境<br />
**Authors:** Yeongtak Oh, Saehyung Lee, Uiwon Hwang, Sungroh Yoon<br />
**Abstract:** <details><summary>原文: </summary>Large-scale language-vision pre-training models, such as CLIP, have achieved remarkable text-guided image morphing results by leveraging several unconditional generative models. However, existing CLIP-guided image morphing methods encounter difficulties when morphing photorealistic images. Specifically, existing guidance fails to provide detailed explanations of the morphing regions within the image, leading to misguidance. In this paper, we observed that such misguidance could be effectively mitigated by simply using a proper regularization loss. Our approach comprises two key components: 1) a geodesic cosine similarity loss that minimizes inter-modality features (i.e., image and text) on a projected subspace of CLIP space, and 2) a latent regularization loss that minimizes intra-modality features (i.e., image and image) on the image manifold. By replacing the na\"ive directional CLIP loss in a drop-in replacement manner, our method achieves superior morphing results on both images and videos for various benchmarks, including CLIP-inversion.</details>
**Abstract_cn:** <details><summary>译文: </summary>CLIP 等大规模语言视觉预训练模型通过利用多种无条件生成模型，取得了显着的文本引导图像变形结果。然而，现有的 CLIP 引导的图像变形方法在变形真实感图像时遇到困难。具体而言，现有指南未能提供图像内变形区域的详细解释，从而导致误导。在本文中，我们观察到，只需使用适当的正则化损失就可以有效地减轻这种误导。我们的方法包括两个关键组成部分：1）测地余弦相似性损失，最大限度地减少 CLIP 空间投影子空间上的模态间特征（即图像和文本）；2）潜在正则化损失，最大限度地减少模态内特征（即图像和文本）。 、图像和图像）在图像流形上。通过以直接替换方式替换原始定向 CLIP 损失，我们的方法在各种基准（包括 CLIP 反转）的图像和视频上实现了出色的变形结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.10526v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Synthesizing Moving People with 3D Control**<br />
**Title_cn:** 通过 3D 控制合成移动人物<br />
**Authors:** Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一个基于扩散模型的框架，用于根据给定的目标 3D 运动序列从单个图像中制作人物动画。我们的方法有两个核心组成部分：a）学习人体和衣服的不可见部分的先验知识，b）用适当的衣服和纹理渲染新颖的身体姿势。对于第一部分，我们学习一种填充扩散模型，以在给定单个图像的情况下产生人看不见的部分的幻觉。我们在纹理贴图空间上训练这个模型，这使得它的样本效率更高，因为它对于姿势和视点来说是不变的。其次，我们开发了一个基于扩散的渲染管道，它由 3D 人体姿势控制。这会产生人物新颖姿势的逼真渲染，包括衣服、头发和看不见区域的合理填充。这种解开的方法使我们的方法能够生成一系列图像，这些图像忠实于 3D 姿势中的目标运动，并且在视觉相似性方面忠实于输入图像。除此之外，3D 控制还允许使用各种合成摄像机轨迹来渲染人物。我们的实验表明，与之前的方法相比，我们的方法在生成长时间运动以及各种具有挑战性和复杂的姿势方面具有弹性。请查看我们的网站了解更多详细信息：https://boyiliee.github.io/3DHM.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2401.10889v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation**<br />
**Title_cn:** 用于类别级物体姿态估计的无源和仅图像无监督域适应<br />
**Authors:** Prakhar Kaushik, Aayush Mishra, Adam Kortylewski, Alan Yuille<br />
**Abstract:** <details><summary>原文: </summary>We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain even when the global pose is not correct. Our model is then trained in an EM fashion, alternating between updating the vertex features and the feature extractor. We show that our method simulates fine-tuning on a global pseudo-labeled dataset under mild assumptions, which converges to the target domain asymptotically. Through extensive empirical validation, including a complex extreme UDA setup which combines real nuisances, synthetic noise, and occlusion, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly improving pose estimation accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们考虑仅从 RGB 图像到目标域的无源无监督类别级姿态估计问题，在适应过程中无需访问源域数据或 3D 注释。收集和注释现实世界的 3D 数据和相应图像是费力、昂贵但不可避免的过程，因为即使 3D 姿态域自适应方法也需要目标域中的 3D 数据。我们引入 3DUDA，这是一种无需 3D 或深度数据即可适应充满麻烦的目标域的方法。我们的关键见解源于对特定对象子部分在域外 (OOD) 场景中保持稳定的观察，从而能够战略性地利用这些不变的子组件来进行有效的模型更新。我们将对象类别表示为简单的长方体网格，并利用在使用差分渲染学习的每个网格顶点建模的神经特征激活生成模型。我们专注于各个局部稳健的网格顶点特征，并根据它们与目标域中相应特征的接近度迭代更新它们，即使全局姿态不正确也是如此。然后我们的模型以 EM 方式进行训练，交替更新顶点特征和特征提取器。我们表明，我们的方法在温和的假设下模拟全局伪标记数据集的微调，渐近收敛到目标域。通过广泛的经验验证，包括结合了真实干扰、合成噪声和遮挡的复杂极端 UDA 设置，我们证明了我们的简单方法在解决域转移挑战和显着提高姿态估计准确性方面的效力。</details>
**PDF:** <http://arxiv.org/pdf/2401.10848v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion**<br />
**Title_cn:** Sat2Scene：通过扩散卫星图像生成 3D 城市场景<br />
**Authors:** Zuoyue Li, Zhenqiang Li, Zhaopeng Cui, Marc Pollefeys, Martin R. Oswald<br />
**Abstract:** <details><summary>原文: </summary>Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services. However, challenges arise from significant view changes and scene scale. Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views. Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery. To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques. Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner. The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency. Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.</details>
**Abstract_cn:** <details><summary>译文: </summary>直接从卫星图像生成场景为集成到游戏和地图服务等应用程序中提供了令人兴奋的可能性。然而，显着的视图变化和场景规模带来了挑战。之前的工作主要集中在图像或视频生成上，缺乏对场景生成对任意视图的适应性的探索。现有的 3D 生成工作要么在对象级别运行，要么难以利用从卫星图像获得的几何形状。为了克服这些限制，我们通过将扩散模型引入 3D 稀疏表示并将其与神经渲染技术相结合，提出了一种用于直接 3D 场景生成的新颖架构。具体来说，我们的方法首先使用 3D 扩散模型在给定几何体的点级别生成纹理颜色，然后以前馈方式将其转换为场景表示。该表示可用于渲染任意视图，这在单帧质量和帧间一致性方面都表现出色。在两个城市规模的数据集中进行的实验表明，我们的模型能够熟练地从卫星图像生成逼真的街景图像序列和交叉视图城市场景。</details>
**PDF:** <http://arxiv.org/pdf/2401.10786v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360 Image Outpainting**<br />
**Title_cn:** Dream360：通过基于 Transformer 的 360 度图像绘制创建多样化、身临其境的户外虚拟场景<br />
**Authors:** Hao Ai, Zidong Cao, Haonan Lu, Chen Chen, Jian Ma, Pengyuan Zhou, Tae-Kyun Kim, Pan Hui, Lin Wang<br />
**Abstract:** <details><summary>原文: </summary>360 images, with a field-of-view (FoV) of 180x360, provide immersive and realistic environments for emerging virtual reality (VR) applications, such as virtual tourism, where users desire to create diverse panoramic scenes from a narrow FoV photo they take from a viewpoint via portable devices. It thus brings us to a technical challenge: `How to allow the users to freely create diverse and immersive virtual scenes from a narrow FoV image with a specified viewport?' To this end, we propose a transformer-based 360 image outpainting framework called Dream360, which can generate diverse, high-fidelity, and high-resolution panoramas from user-selected viewports, considering the spherical properties of 360 images. Compared with existing methods, e.g., [3], which primarily focus on inputs with rectangular masks and central locations while overlooking the spherical property of 360 images, our Dream360 offers higher outpainting flexibility and fidelity based on the spherical representation. Dream360 comprises two key learning stages: (I) codebook-based panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN learns a sphere-specific codebook from spherical harmonic (SH) values, providing a better representation of spherical data distribution for scene modeling. The frequency-aware refinement matches the resolution and further improves the semantic consistency and visual fidelity of the generated results. Our Dream360 achieves significantly lower Frechet Inception Distance (FID) scores and better visual fidelity than existing methods. We also conducted a user study involving 15 participants to interactively evaluate the quality of the generated results in VR, demonstrating the flexibility and superiority of our Dream360 framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>视场 (FoV) 为 180x360 的 360 度图像为新兴虚拟现实 (VR) 应用提供身临其境的逼真环境，例如虚拟旅游，用户希望通过拍摄的狭窄 FoV 照片创建多样化的全景场景通过便携式设备的视角。这就给我们带来了一个技术挑战：“如何让用户在指定视口的窄视场图像中自由创建多样化、沉浸式的虚拟场景？”为此，我们提出了一种基于 Transformer 的 360 度图像绘制框架，称为 Dream360，考虑到 360 度图像的球形特性，它可以从用户选择的视口生成多样化、高保真度和高分辨率的全景图。与现有方法（例如，[3]）相比，主要关注具有矩形掩模和中心位置的输入，同时忽略 360 度图像的球形属性，我们的 Dream360 基于球形表示提供了更高的绘制灵活性和保真度。 Dream360 包括两个关键的学习阶段：(I) 通过 Spherical-VQGAN (S-VQGAN) 基于密码本的全景绘制，以及 (II) 通过新颖的频率感知一致性损失进行频率感知细化。具体来说，S-VQGAN 从球谐 (SH) 值中学习特定于球体的码本，为场景建模提供更好的球面数据分布表示。频率感知细化与分辨率相匹配，并进一步提高了生成结果的语义一致性和视觉保真度。与现有方法相比，我们的 Dream360 实现了显着更低的 Frechet 起始距离 (FID) 分数和更好的视觉保真度。我们还进行了一项涉及 15 名参与者的用户研究，以交互方式评估 VR 生成结果的质量，展示了我们的 Dream360 框架的灵活性和优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10564v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning**<br />
**Title_cn:** Tool-LMM：用于工具代理学习的大型多模态模型<br />
**Authors:** Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Xiaohua, Xuan, Zhengxin Li, Lin Ma, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' perceiving tool-use ability is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the visual- or auditory-grounded instructions' information. Therefore, in this paper, we propose Tool-LMM, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featured by consisting of multi-modal input tools from HuggingFace. Another important feature of our dataset is that our dataset also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our LMM is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/Tool-LMM/Tool-LMM.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大型语言模型（LLM）在自然语言理解和生成任务中的惊人表现引发了人们对使用它们作为中央控制器来构建代理系统的大量探索。多项研究侧重于将法学硕士与外部工具联系起来以扩展应用场景。然而，目前的法学硕士感知工具使用能力仅限于单个文本查询，这可能会导致对用户真实意图的理解含糊不清。法学硕士有望通过感知基于视觉或听觉的指令信息来消除这种情况。因此，在本文中，我们提出了Tool-LMM，一个结合了开源LLM和多模态编码器的系统，使得学习到的LLM能够意识到多模态输入指令，然后正确选择功能匹配的工具。为了便于评估模型的能力，我们收集了一个由 HuggingFace 的多模态输入工具组成的数据集。我们数据集的另一个重要特征是，由于相同函数和同义函数的存在，我们的数据集还包含同一指令的多个潜在选择，这为同一查询提供了更多潜在的解决方案。实验表明，我们的 LMM 能够为多模式指令推荐合适的工具。代码和数据可在 https://github.com/Tool-LMM/Tool-LMM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10727v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge**<br />
**Title_cn:** 问答提示：通过挖掘问答提示发现丰富的视觉线索，VQA需要多元化的世界知识<br />
**Authors:** Haibi Wang, Weifeng Ge<br />
**Abstract:** <details><summary>原文: </summary>With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model. Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers. Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers. Experimental results show that, compared with state-of-the-art methods, our Q&A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着多模态大语言模型的突破，回答需要高级推理能力和世界知识的复杂视觉问题已成为开发人工智能模型比以往任何时候都更加重要的测试平台。然而，由于人类的认知模式尚未被系统地理解，为人工智能模型配备强大的跨模态推理能力仍然具有挑战性。在本文中，我们相信，如果我们能够尽可能多地收集给定图像中的视觉线索，我们将更准确地识别图像，更好地理解问题，更容易地回忆相关知识，并最终推理出答案。我们通过挖掘图像中的问答对并将其作为提示发送到多模态大语言模型中来发现这些丰富的视觉线索。我们将所提出的方法称为“问答提示”。具体来说，我们首先使用训练集中的图像-答案对和相应的问题作为输入和输出来训练视觉问题生成模型。然后，我们使用图像标签模型来识别各种实例，并将打包的图像标签对发送到视觉问题生成模型中，以生成相关问题，并以提取的图像标签作为答案。最后，我们使用视觉感知提示模块将这些生成的问答对编码为提示，并将它们发送到预先训练的多模态大语言模型中以推理出最终答案。实验结果表明，与最先进的方法相比，我们的问答提示在需要对不同世界知识进行推理的具有挑战性的视觉问答数据集（例如 OK-VQA 和 A-OKVQA）上取得了实质性改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.10712v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering**<br />
**Title_cn:** 用于视频问答的大型多模态模型的弱监督高斯对比基础<br />
**Authors:** Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge<br />
**Abstract:** <details><summary>原文: </summary>Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure of the video, and sample question-critical frames as positive moments to be the visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks verify the effectiveness of our framework, and we achieve substantial improvements compared to previous state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频问答（VideoQA）旨在根据视频中观察到的信息回答自然语言问题。尽管大型多模态模型（LMM）最近在图像语言理解和推理方面取得了成功，但它们仅通过简单地将均匀采样的帧作为视觉输入来处理视频QA，而忽略了与问题相关的视觉线索。此外，现有 VideoQA 数据集中没有针对关键问题时间戳的人工注释。有鉴于此，我们提出了一种新颖的弱监督框架，以强制 LMM 以问题关键时刻作为视觉输入来推理出答案。具体来说，我们将问题和答案对融合为事件描述，以找到多个关键帧作为目标时刻，这将是伪标签。利用这些伪标签作为额外的弱监督，我们设计了一个轻量级的基于高斯的对比接地（GCG）模块。 GCG 学习多个高斯函数来表征视频的时间结构，并将关键问题帧采样为正向时刻，作为 LMM 的视觉输入。对多个 VideoQA 基准的大量实验验证了我们框架的有效性，并且与之前最先进的方法相比，我们取得了实质性的改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.10711v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval**<br />
**Title_cn:** DGL：用于文本视频检索的动态全局局部提示调整<br />
**Authors:** Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, Yi Yang<br />
**Abstract:** <details><summary>原文: </summary>Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL</details>
**Abstract_cn:** <details><summary>译文: </summary>文本视频检索是一项关键的多模式任务，旨在找到与文本查询最相关的视频。尽管像 CLIP 这样的预训练模型在这一领域表现出了令人印象深刻的潜力，但由于模型尺寸的增加，完全微调这些模型的成本不断上升，这仍然是一个问题。为了应对这一挑战，即时调整作为一种替代方案应运而生。然而，现有的工作在将预训练的图像文本模型应用于下游视频文本任务时仍然面临两个问题：（1）视觉编码器只能编码帧级特征，无法提取全局级通用视频信息。 (2) 为视觉和文本编码器配备单独的提示未能缩小视觉-文本模态差距。为此，我们提出了 DGL，一种具有全局-局部视频注意力的跨模态动态提示调整方法。与之前的提示调整方法相比，我们利用共享潜在空间来生成鼓励跨模式交互的本地级别文本和框架提示。此外，我们提出在全局局部注意力机制中对视频进行建模，以从提示调整的角度捕获全局视频信息。大量实验表明，当仅调整 0.67% 的参数时，我们的跨模态提示调整策略 DGL 在 MSR-VTT、VATEX、LSMDC 和 ActivityNet 数据集上优于或与完全微调方法相当。代码可在 https://github.com/knightyxp/DGL 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.10588v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences**<br />
**Title_cn:** Mementos：图像序列多模态大语言模型推理的综合基准<br />
**Authors:** Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs' sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations. Our dataset is available at https://github.com/umd-huang-lab/Mementos.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型 (MLLM) 已证明能够熟练处理各种视觉语言任务。然而，当前的 MLLM 基准主要设计用于评估基于单个图像的静态信息的推理，而现代 MLLM 从图像序列推断的能力（这对于理解我们不断变化的世界至关重要）的研究较少。为了应对这一挑战，本文引入了 Mementos，这是一种旨在评估 MLLM 的顺序图像推理能力的新基准。 Mementos 具有 4,761 个不同长度的不同图像序列。我们还采用 GPT-4 辅助方法来评估 MLLM 推理性能。通过对 Mementos 上最近的 9 个 MLLM（包括 GPT-4V 和 Gemini）的仔细评估，我们发现它们很难准确描述给定图像序列的动态信息，通常会导致对物体及其相应行为的幻觉/误解。我们的定量分析和案例研究确定了影响 MLLM 顺序图像推理的三个关键因素：物体幻觉和行为幻觉之间的相关性、同时发生的行为的影响以及行为幻觉的复合影响。我们的数据集可在 https://github.com/umd-huang-lab/Mementos 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10529v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology**<br />
**Title_cn:** 机器中的尸体：运动捕捉技术测量和验证的社会实践<br />
**Authors:** Emma Harvey, Hauke Sandhaus, Abigail Z. Jacobs, Emanuel Moss, Mona Sloane<br />
**Abstract:** <details><summary>原文: </summary>Motion capture systems, used across various domains, make body representations concrete through technical processes. We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices. By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time. Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements. We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction.</details>
**Abstract_cn:** <details><summary>译文: </summary>跨领域使用的动作捕捉系统通过技术流程使身体表征变得具体。我们认为，身体的测量和运动捕捉系统测量的验证可以理解为社会实践。通过从社会实践理论的角度分析系统文献综述（N = 278）的结果，我们展示了这些实践及其对错误的不同关注如何随着时间的推移在动作捕捉设计和创新中根深蒂固。此外，我们还展示了当代动作捕捉系统如何延续对人体及其动作的假设。我们认为，测量和验证的社会实践在更广泛的数据和传感器驱动系统的开发中普遍存在，并将这项工作作为研究隐藏的设计假设及其在人机交互中的潜在负面后果的基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.10877v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning**<br />
**Title_cn:** 基于注意力的深度学习的单目视觉里程计的运动一致性损失<br />
**Authors:** André O. Françani, Marcos R. O. A. Maximo<br />
**Abstract:** <details><summary>原文: </summary>Deep learning algorithms have driven expressive progress in many complex tasks. The loss function is a core component of deep learning techniques, guiding the learning process of neural networks. This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches. The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips. Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习算法推动了许多复杂任务的表达进步。损失函数是深度学习技术的核心组成部分，指导神经网络的学习过程。本文通过基于深度学习的方法引入视觉里程计的一致性损失来做出贡献。运动一致性损失探索连续重叠视频剪辑中出现的重复运动。实验结果表明，我们的方法提高了模型在 KITTI 里程计基准上的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.10857v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Understanding Video Transformers via Universal Concept Discovery**<br />
**Title_cn:** 通过通用概念发现了解视频转换器<br />
**Authors:** Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov<br />
**Abstract:** <details><summary>原文: </summary>This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models. Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers. Finally, we demonstrate that VTCDcan be used to improve model performance for fine-grained tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了视频变压器表示的基于概念的可解释性问题。具体来说，我们试图根据自动发现的高级时空概念来解释视频转换器的决策过程。先前对基于概念的可解释性的研究仅集中在图像级任务上。相比之下，视频模型处理增加的时间维度，增加了复杂性，并对随着时间的推移识别动态概念提出了挑战。在这项工作中，我们通过引入第一个视频变压器概念发现（VTCD）算法来系统地解决这些挑战。为此，我们提出了一种有效的方法，用于无监督地识别视频变换器表示单元（概念），并对它们对模型输出的重要性进行排名。由此产生的概念具有高度可解释性，揭示了非结构化视频模型中的时空推理机制和以对象为中心的表示。通过对一组不同的监督和自监督表示联合执行此分析，我们发现其中一些机制在视频转换器中是通用的。最后，我们证明 VTCD 可用于提高细粒度任务的模型性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.10831v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images**<br />
**Title_cn:** M2ORT：用于根据组织病理学图像进行空间转录组预测的多对一回归变压器<br />
**Authors:** Hongyi Wang, Xiuju Du, Jing Liu, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin<br />
**Abstract:** <details><summary>原文: </summary>The advancement of Spatial Transcriptomics (ST) has facilitated the spatially-aware profiling of gene expressions based on histopathology images. Although ST data offers valuable insights into the micro-environment of tumors, its acquisition cost remains expensive. Therefore, directly predicting the ST expressions from digital pathology images is desired. Current methods usually adopt existing regression backbones for this task, which ignore the inherent multi-scale hierarchical data structure of digital pathology images. To address this limit, we propose M2ORT, a many-to-one regression Transformer that can accommodate the hierarchical structure of the pathology images through a decoupled multi-scale feature extractor. Different from traditional models that are trained with one-to-one image-label pairs, M2ORT accepts multiple pathology images of different magnifications at a time to jointly predict the gene expressions at their corresponding common ST spot, aiming at learning a many-to-one relationship through training. We have tested M2ORT on three public ST datasets and the experimental results show that M2ORT can achieve state-of-the-art performance with fewer parameters and floating-point operations (FLOPs). The code is available at: https://github.com/Dootmaan/M2ORT/.</details>
**Abstract_cn:** <details><summary>译文: </summary>空间转录组学（ST）的进步促进了基于组织病理学图像的基因表达的空间感知分析。尽管 ST 数据为肿瘤微环境提供了有价值的见解，但其获取成本仍然昂贵。因此，需要直接从数字病理图像预测 ST 表达。当前的方法通常采用现有的回归主干来完成此任务，而忽略了数字病理图像固有的多尺度分层数据结构。为了解决这个限制，我们提出了 M2ORT，一种多对一回归 Transformer，它可以通过解耦的多尺度特征提取器来适应病理图像的层次结构。与使用一对一图像标签对训练的传统模型不同，M2ORT 一次接受多个不同放大倍数的病理图像，共同预测其相应公共 ST 点的基因表达，旨在学习多对多的模型。通过训练建立一种关系。我们在三个公共 ST 数据集上测试了 M2ORT，实验结果表明，M2ORT 可以用更少的参数和浮点运算 (FLOP) 实现最先进的性能。该代码位于：https://github.com/Dootmaan/M2ORT/。</details>
**PDF:** <http://arxiv.org/pdf/2401.10608v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Learning Position-Aware Implicit Neural Network for Real-World Face Inpainting**<br />
**Title_cn:** 学习用于现实世界面部修复的位置感知隐式神经网络<br />
**Authors:** Bo Zhao, Huan Yang, Jianlong Fu<br />
**Abstract:** <details><summary>原文: </summary>Face inpainting requires the model to have a precise global understanding of the facial position structure. Benefiting from the powerful capabilities of deep learning backbones, recent works in face inpainting have achieved decent performance in ideal setting (square shape with $512px$). However, existing methods often produce a visually unpleasant result, especially in the position-sensitive details (e.g., eyes and nose), when directly applied to arbitrary-shaped images in real-world scenarios. The visually unpleasant position-sensitive details indicate the shortcomings of existing methods in terms of position information processing capability. In this paper, we propose an \textbf{I}mplicit \textbf{N}eural \textbf{I}npainting \textbf{N}etwork (IN$^2$) to handle arbitrary-shape face images in real-world scenarios by explicit modeling for position information. Specifically, a downsample processing encoder is proposed to reduce information loss while obtaining the global semantic feature. A neighbor hybrid attention block is proposed with a hybrid attention mechanism to improve the facial understanding ability of the model without restricting the shape of the input. Finally, an implicit neural pyramid decoder is introduced to explicitly model position information and bridge the gap between low-resolution features and high-resolution output. Extensive experiments demonstrate the superiority of the proposed method in real-world face inpainting task.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部修复需要模型对面部位置结构有精确的全局理解。受益于深度学习主干的强大能力，最近的面部修复作品在理想环境（$512px$的正方形）中取得了不错的表现。然而，当直接应用于现实场景中的任意形状图像时，现有方法通常会产生视觉上令人不快的结果，特别是在位置敏感的细节（例如眼睛和鼻子）中。视觉上不愉快的位置敏感细节表明了现有方法在位置信息处理能力方面的缺点。在本文中，我们提出了一个 \textbf{I}mplicit \textbf{N}eural \textbf{I}npainting \textbf{N}etwork (IN$^2$) 来处理现实场景中的任意形状的人脸图像通过位置信息的显式建模。具体来说，提出了一种下采样处理编码器来减少信息损失，同时获得全局语义特征。提出了具有混合注意机制的邻居混合注意块，以在不限制输入形状的情况下提高模型的面部理解能力。最后，引入隐式神经金字塔解码器来显式建模位置信息并弥合低分辨率特征和高分辨率输出之间的差距。大量的实验证明了该方法在现实世界的人脸修复任务中的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10537v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **NWPU-MOC: A Benchmark for Fine-grained Multi-category Object Counting in Aerial Images**<br />
**Title_cn:** NWPU-MOC：航空图像中细粒度多类别物体计数的基准<br />
**Authors:** Junyu Gao, Liangliang Zhao, Xuelong Li<br />
**Abstract:** <details><summary>原文: </summary>Object counting is a hot topic in computer vision, which aims to estimate the number of objects in a given image. However, most methods only count objects of a single category for an image, which cannot be applied to scenes that need to count objects with multiple categories simultaneously, especially in aerial scenes. To this end, this paper introduces a Multi-category Object Counting (MOC) task to estimate the numbers of different objects (cars, buildings, ships, etc.) in an aerial image. Considering the absence of a dataset for this task, a large-scale Dataset (NWPU-MOC) is collected, consisting of 3,416 scenes with a resolution of 1024 $\times$ 1024 pixels, and well-annotated using 14 fine-grained object categories. Besides, each scene contains RGB and Near Infrared (NIR) images, of which the NIR spectrum can provide richer characterization information compared with only the RGB spectrum. Based on NWPU-MOC, the paper presents a multi-spectrum, multi-category object counting framework, which employs a dual-attention module to fuse the features of RGB and NIR and subsequently regress multi-channel density maps corresponding to each object category. In addition, to modeling the dependency between different channels in the density map with each object category, a spatial contrast loss is designed as a penalty for overlapping predictions at the same spatial position. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared with some mainstream counting algorithms. The dataset, code and models are publicly available at https://github.com/lyongo/NWPU-MOC.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象计数是计算机视觉中的一个热门话题，其目的是估计给定图像中对象的数量。然而，大多数方法仅对图像中的单个类别的物体进行计数，这不能应用于需要同时对多个类别的物体进行计数的场景，尤其是在航拍场景中。为此，本文引入了多类别对象计数（MOC）任务来估计航空图像中不同对象（汽车、建筑物、船舶等）的数量。考虑到该任务缺乏数据集，收集了一个大规模数据集（NWPU-MOC），由 3,416 个场景组成，分辨率为 1024 $\times$ 1024 像素，并使用 14 个细粒度对象类别进行了良好注释。此外，每个场景都包含RGB和近红外（NIR）图像，其中NIR光谱与仅RGB光谱相比可以提供更丰富的表征信息。基于NWPU-MOC，本文提出了一种多光谱、多类别物体计数框架，该框架采用双注意力模块来融合RGB和NIR的特征，随后回归与每个物体类别对应的多通道密度图。此外，为了对密度图中不同通道与每个对象类别之间的依赖性进行建模，设计了空间对比度损失作为同一空间位置处重叠预测的惩罚。实验结果表明，与一些主流计数算法相比，该方法实现了最先进的性能。数据集、代码和模型可在 https://github.com/lyongo/NWPU-MOC 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10530v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **SCENES: Subpixel Correspondence Estimation With Epipolar Supervision**<br />
**Title_cn:** 场景：具有极线监督的子像素对应估计<br />
**Authors:** Dominik A. Kloepfer, João F. Henriques, Dylan Campbell<br />
**Abstract:** <details><summary>原文: </summary>Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.</details>
**Abstract_cn:** <details><summary>译文: </summary>从场景的两个或多个视图中提取点对应关系是一个基本的计算机视觉问题，对于相对相机姿态估计和运动结构尤其重要。现有的局部特征匹配方法，通过大规模数据集上的对应监督训练，可以在测试集上获得高度准确的匹配。然而，与经典特征提取器不同，它们不能很好地泛化到与训练数据集具有不同特征的新数据集。相反，它们需要微调，假设地面实况对应或地面实况相机姿势和 3D 结构可用。我们通过消除 3D 结构（例如深度图或点云）的要求来放松这一假设，并且只需要相机姿态信息，该信息可以从里程计获得。我们通过用极线损失替换对应损失来实现这一点，这鼓励假定的匹配位于相关的极线上。虽然弱于对应监督，但我们观察到这一线索足以根据新数据微调现有模型。然后，我们通过在新颖的引导方法中使用姿势估计来进一步放宽已知相机姿势的假设。我们对极具挑战性的数据集进行评估，包括室内无人机数据集和室外智能手机摄像头数据集，并在没有强有力监督的情况下获得最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.10886v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Dense 3D Reconstruction Through Lidar: A Comparative Study on Ex-vivo Porcine Tissue**<br />
**Title_cn:** 通过激光雷达进行密集 3D 重建：离体猪组织的比较研究<br />
**Authors:** Guido Caccianiga, Julian Nubert, Marco Hutter, Katherine J. Kuchenbecker<br />
**Abstract:** <details><summary>原文: </summary>New sensing technologies and more advanced processing algorithms are transforming computer-integrated surgery. While researchers are actively investigating depth sensing and 3D reconstruction for vision-based surgical assistance, it remains difficult to achieve real-time, accurate, and robust 3D representations of the abdominal cavity for minimally invasive surgery. Thus, this work uses quantitative testing on fresh ex-vivo porcine tissue to thoroughly characterize the quality with which a 3D laser-based time-of-flight sensor (lidar) can perform anatomical surface reconstruction. Ground-truth surface shapes are captured with a commercial laser scanner, and the resulting signed error fields are analyzed using rigorous statistical tools. When compared to modern learning-based stereo matching from endoscopic images, time-of-flight sensing demonstrates higher precision, lower processing delay, higher frame rate, and superior robustness against sensor distance and poor illumination. Furthermore, we report on the potential negative effect of near-infrared light penetration on the accuracy of lidar measurements across different tissue samples, identifying a significant measured depth offset for muscle in contrast to fat and liver. Our findings highlight the potential of lidar for intraoperative 3D perception and point toward new methods that combine complementary time-of-flight and spectral imaging.</details>
**Abstract_cn:** <details><summary>译文: </summary>新的传感技术和更先进的处理算法正在改变计算机集成手术。尽管研究人员正在积极研究用于基于视觉的手术辅助的深度传感和 3D 重建，但在微创手术中实现腹腔的实时、准确和稳健的 3D 表示仍然很困难。因此，这项工作对新鲜的离体猪组织进行定量测试，以彻底表征基于 3D 激光的飞行时间传感器 (激光雷达) 执行解剖表面重建的质量。使用商用激光扫描仪捕获地面实况表面形状，并使用严格的统计工具分析所得的带符号误差场。与现代基于学习的内窥镜图像立体匹配相比，飞行时间传感表现出更高的精度、更低的处理延迟、更高的帧速率以及针对传感器距离和不良照明的卓越鲁棒性。此外，我们报告了近红外光穿透对不同组织样本的激光雷达测量精度的潜在负面影响，确定了与脂肪和肝脏相比肌肉的显着测量深度偏移。我们的研究结果强调了激光雷达在术中 3D 感知方面的潜力，并指出了结合互补飞行时间和光谱成像的新方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.10709v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MixNet: Towards Effective and Efficient UHD Low-Light Image Enhancement**<br />
**Title_cn:** MixNet：迈向有效且高效的超高清低光图像增强<br />
**Authors:** Chen Wu, Zhuoran Zheng, Xiuyi Jia, Wenqi Ren<br />
**Abstract:** <details><summary>原文: </summary>With the continuous advancement of imaging devices, the prevalence of Ultra-High-Definition (UHD) images is rising. Although many image restoration methods have achieved promising results, they are not directly applicable to UHD images on devices with limited computational resources due to the inherently high computational complexity of UHD images. In this paper, we focus on the task of low-light image enhancement (LLIE) and propose a novel LLIE method called MixNet, which is designed explicitly for UHD images. To capture the long-range dependency of features without introducing excessive computational complexity, we present the Global Feature Modulation Layer (GFML). GFML associates features from different views by permuting the feature maps, enabling efficient modeling of long-range dependency. In addition, we also design the Local Feature Modulation Layer (LFML) and Feed-forward Layer (FFL) to capture local features and transform features into a compact representation. This way, our MixNet achieves effective LLIE with few model parameters and low computational complexity. We conducted extensive experiments on both synthetic and real-world datasets, and the comprehensive results demonstrate that our proposed method surpasses the performance of current state-of-the-art methods. The code will be available at \url{https://github.com/zzr-idam/MixNet}.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着成像设备的不断进步，超高清（UHD）图像的普及率不断上升。尽管许多图像恢复方法取得了可喜的结果，但由于超高清图像固有的高计算复杂性，它们不能直接适用于计算资源有限的设备上的超高清图像。在本文中，我们专注于低光图像增强（LLIE）的任务，并提出了一种称为 MixNet 的新型 LLIE 方法，该方法是专门为超高清图像设计的。为了捕获特征的远程依赖性而不引入过多的计算复杂性，我们提出了全局特征调制层（GFML）。 GFML 通过排列特征映射来关联来自不同视图的特征，从而实现远程依赖性的高效建模。此外，我们还设计了局部特征调制层（LFML）和前馈层（FFL）来捕获局部特征并将特征转换为紧凑的表示。通过这种方式，我们的 MixNet 以较少的模型参数和较低的计算复杂度实现了有效的 LLIE。我们对合成数据集和真实数据集进行了广泛的实验，综合结果表明我们提出的方法超越了当前最先进方法的性能。该代码可在 \url{https://github.com/zzr-idam/MixNet} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.10666v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **3D Shape Completion on Unseen Categories:A Weakly-supervised Approach**<br />
**Title_cn:** 看不见的类别的 3D 形状补全：弱监督方法<br />
**Authors:** Lintai Wu, Junhui Hou, Linqi Song, Yong Xu<br />
**Abstract:** <details><summary>原文: </summary>3D shapes captured by scanning devices are often incomplete due to occlusion. 3D shape completion methods have been explored to tackle this limitation. However, most of these methods are only trained and tested on a subset of categories, resulting in poor generalization to unseen categories. In this paper, we introduce a novel weakly-supervised framework to reconstruct the complete shapes from unseen categories. We first propose an end-to-end prior-assisted shape learning network that leverages data from the seen categories to infer a coarse shape. Specifically, we construct a prior bank consisting of representative shapes from the seen categories. Then, we design a multi-scale pattern correlation module for learning the complete shape of the input by analyzing the correlation between local patterns within the input and the priors at various scales. In addition, we propose a self-supervised shape refinement model to further refine the coarse shape. Considering the shape variability of 3D objects across categories, we construct a category-specific prior bank to facilitate shape refinement. Then, we devise a voxel-based partial matching loss and leverage the partial scans to drive the refinement process. Extensive experimental results show that our approach is superior to state-of-the-art methods by a large margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于遮挡，扫描设备捕获的 3D 形状通常不完整。人们已经探索了 3D 形状完成方法来解决这一限制。然而，大多数这些方法仅在类别的子集上进行训练和测试，导致对未见过的类别的泛化能力较差。在本文中，我们引入了一种新颖的弱监督框架来从看不见的类别中重建完整的形状。我们首先提出了一种端到端先验辅助形状学习网络，该网络利用所见类别的数据来推断粗略形状。具体来说，我们构建了一个由所见类别中的代表性形状组成的先前库。然后，我们设计了一个多尺度模式相关模块，通过分析输入中的局部模式与不同尺度的先验之间的相关性来学习输入的完整形状。此外，我们提出了一种自监督形状细化模型来进一步细化粗略形状。考虑到跨类别 3D 对象的形状变异性，我们构建了一个特定于类别的先验库以促进形状细化。然后，我们设计了一种基于体素的部分匹配损失，并利用部分扫描来驱动细化过程。大量的实验结果表明，我们的方法大大优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.10578v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data**<br />
**Title_cn:** 深入一切：释放大规模未标记数据的力量<br />
**Authors:** Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao<br />
**Abstract:** <details><summary>原文: </summary>This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出了 Depth Anything，这是一种用于鲁棒单目深度估计的高度实用的解决方案。在不追求新颖的技术模块的情况下，我们的目标是构建一个简单而强大的基础模型，处理任何情况下的任何图像。为此，我们通过设计数据引擎来收集和自动注释大规模未标记数据（~62M）来扩展数据集，这显着扩大了数据覆盖范围，从而能够减少泛化误差。我们研究了两种简单而有效的策略，使数据扩展前景光明。首先，利用数据增强工具创建更具挑战性的优化目标。它迫使模型主动寻求额外的视觉知识并获得稳健的表示。其次，开发了辅助监督来强制模型从预训练的编码器继承丰富的语义先验。我们广泛评估其零镜头能力，包括六个公共数据集和随机捕获的照片。它表现出了令人印象深刻的泛化能力。此外，通过使用 NYUv2 和 KITTI 的度量深度信息对其进行微调，设置了新的 SOTA。我们更好的深度模型也会产生更好的深度调节 ControlNet。我们的模型发布于 https://github.com/LiheYoung/Depth-Anything。</details>
**PDF:** <http://arxiv.org/pdf/2401.10891v1><br />
**Code:** <https://github.com/LiheYoung/Depth-Anything>**<br />
>>**index:** 2<br />
**Title:** **A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges**<br />
**Title_cn:** 基于深度学习的车辆重新识别的全面调查：模型、数据集和挑战<br />
**Authors:** Ali Amiri, Aydin Kaya, Ali Seydi Keceli<br />
**Abstract:** <details><summary>原文: </summary>Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research directions. This comprehensive assessment examines the landscape of deep learning in vehicle ReID and establishes a foundation and starting point for future works. It aims to serve as a complete reference by highlighting challenges and emerging trends, fostering advancements and applications in vehicle ReID utilizing deep learning models.</details>
**Abstract_cn:** <details><summary>译文: </summary>车辆重新识别 (ReID) 致力于将从跨越不同交通环境的分布式摄像机网络收集的车辆图像关联起来。这项任务在以车辆为中心的技术领域中占据着至关重要的地位，在部署智能交通系统 (ITS) 和推进智慧城市计划方面发挥着关键作用。近年来，深度学习的快速进步极大地推动了车辆 ReID 技术的发展。因此，对以深度学习为中心的车辆重新识别方法进行全面调查已势在必行。本文广泛探讨了应用于车辆 ReID 的深度学习技术。它概述了这些方法的分类，包括监督和无监督方法，深入研究了这些类别中的现有研究，介绍了数据集和评估标准，并描绘了即将到来的挑战和潜在的研究方向。这项综合评估审视了车辆再识别中深度学习的前景，并为未来的工作奠定了基础和起点。它旨在通过强调挑战和新兴趋势，利用深度学习模型促进车辆 ReID 的进步和应用，作为完整的参考。</details>
**PDF:** <http://arxiv.org/pdf/2401.10643v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Learning to Visually Connect Actions and their Effects**<br />
**Title_cn:** 学习以视觉方式连接动作及其效果<br />
**Authors:** Eric Peh, Paritosh Parmar, Basura Fernando<br />
**Abstract:** <details><summary>原文: </summary>In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们介绍了视频理解中视觉连接动作及其效果（CATE）的新颖概念。 CATE 可应用于任务规划和演示学习等领域。我们提出了不同的基于 CATE 的任务公式，例如动作选择和动作规范，其中视频理解模型在语义和细粒度级别连接动作和效果。我们观察到不同的公式产生捕获直观动作属性的表示。我们还设计了用于动作选择和动作规范的各种基线模型。尽管这项任务具有直观性，但我们观察到模型很挣扎，而人类的表现却远远超过了它们。该研究旨在为未来的工作奠定基础，展示视频理解中连接动作和效果的灵活性和多功能性，以期激发先进的公式和模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.10805v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Determination of efficiency indicators of the stand for intelligent control of manual operations in industrial production**<br />
**Title_cn:** 工业生产中手动操作智能控制工位效率指标的确定<br />
**Authors:** Anton Sergeev, Victor Minchenkov, Aleksei Soldatov<br />
**Abstract:** <details><summary>原文: </summary>Systems of intelligent control of manual operations in industrial production are being implemented in many industries nowadays. Such systems use high-resolution cameras and computer vision algorithms to automatically track the operator's manipulations and prevent technological errors in the assembly process. At the same time compliance with safety regulations in the workspace is monitored. As a result, the defect rate of manufactured products and the number of accidents during the manual assembly of any device are decreased. Before implementing an intelligent control system into a real production it is necessary to calculate its efficiency. In order to do it experiments on the stand for manual operations control systems were carried out. This paper proposes the methodology for calculating the efficiency indicators. This mathematical approach is based on the IoU calculation of real- and predicted-time intervals between assembly stages. The results show high precision in tracking the validity of manual assembly and do not depend on the duration of the assembly process.</details>
**Abstract_cn:** <details><summary>译文: </summary>如今，工业生产中手动操作的智能控制系统正在许多行业中实施。此类系统使用高分辨率摄像头和计算机视觉算法来自动跟踪操作员的操作并防止装配过程中出现技术错误。同时监控工作场所安全法规的遵守情况。因此，减少了制造产品的缺陷率以及任何设备的手动组装过程中的事故数量。在将智能控制系统应用于实际生产之前，有必要计算其效率。为了做到这一点，在手动操作控制系统的支架上进行了实验。本文提出了效率指标的计算方法。这种数学方法基于装配阶段之间的实时和预测时间间隔的 IoU 计算。结果表明，跟踪手动装配的有效性具有很高的精度，并且不依赖于装配过程的持续时间。</details>
**PDF:** <http://arxiv.org/pdf/2401.10777v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **NN-VVC: Versatile Video Coding boosted by self-supervisedly learned image coding for machines**<br />
**Title_cn:** NN-VVC：通过机器自监督学习图像编码推动多功能视频编码<br />
**Authors:** Jukka I. Ahonen, Nam Le, Honglei Zhang, Antti Hallapuro, Francesco Cricri, Hamed Rezazadegan Tavakoli, Miska M. Hannuksela, Esa Rahtu<br />
**Abstract:** <details><summary>原文: </summary>The recent progress in artificial intelligence has led to an ever-increasing usage of images and videos by machine analysis algorithms, mainly neural networks. Nonetheless, compression, storage and transmission of media have traditionally been designed considering human beings as the viewers of the content. Recent research on image and video coding for machine analysis has progressed mainly in two almost orthogonal directions. The first is represented by end-to-end (E2E) learned codecs which, while offering high performance on image coding, are not yet on par with state-of-the-art conventional video codecs and lack interoperability. The second direction considers using the Versatile Video Coding (VVC) standard or any other conventional video codec (CVC) together with pre- and post-processing operations targeting machine analysis. While the CVC-based methods benefit from interoperability and broad hardware and software support, the machine task performance is often lower than the desired level, particularly in low bitrates. This paper proposes a hybrid codec for machines called NN-VVC, which combines the advantages of an E2E-learned image codec and a CVC to achieve high performance in both image and video coding for machines. Our experiments show that the proposed system achieved up to -43.20% and -26.8% Bj{\o}ntegaard Delta rate reduction over VVC for image and video data, respectively, when evaluated on multiple different datasets and machine vision tasks. To the best of our knowledge, this is the first research paper showing a hybrid video codec that outperforms VVC on multiple datasets and multiple machine vision tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能的最新进展导致机器分析算法（主要是神经网络）对图像和视频的使用不断增加。尽管如此，媒体的压缩、存储和传输传统上都是将人类作为内容的观看者来设计的。用于机器分析的图像和视频编码的最新研究主要在两个几乎正交的方向上取得进展。第一个是以端到端（E2E）学习编解码器为代表，它虽然在图像编码方面提供了高性能，但尚未与最先进的传统视频编解码器相提并论，并且缺乏互操作性。第二个方向考虑使用通用视频编码 (VVC) 标准或任何其他传统视频编解码器 (CVC) 以及针对机器分析的预处理和后处理操作。虽然基于 CVC 的方法受益于互操作性和广泛的硬件和软件支持，但机器任务性能通常低于所需水平，特别是在低比特率下。本文提出了一种称为 NN-VVC 的机器混合编解码器，它结合了端到端学习的图像编解码器和 CVC 的优点，以实现机器图像和视频编码的高性能。我们的实验表明，在多个不同的数据集和机器视觉任务上进行评估时，所提出的系统在图像和视频数据上分别比 VVC 实现了 -43.20% 和 -26.8% Bj{\o}ntegaard Delta 速率降低。据我们所知，这是第一篇研究论文，展示了在多个数据集和多个机器视觉任务上优于 VVC 的混合视频编解码器。</details>
**PDF:** <http://arxiv.org/pdf/2401.10761v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Bridging the gap between image coding for machines and humans**<br />
**Title_cn:** 弥合机器和人类图像编码之间的差距<br />
**Authors:** Nam Le, Honglei Zhang, Francesco Cricri, Ramin G. Youvalari, Hamed Rezazadegan Tavakoli, Emre Aksu, Miska M. Hannuksela, Esa Rahtu<br />
**Abstract:** <details><summary>原文: </summary>Image coding for machines (ICM) aims at reducing the bitrate required to represent an image while minimizing the drop in machine vision analysis accuracy. In many use cases, such as surveillance, it is also important that the visual quality is not drastically deteriorated by the compression process. Recent works on using neural network (NN) based ICM codecs have shown significant coding gains against traditional methods; however, the decompressed images, especially at low bitrates, often contain checkerboard artifacts. We propose an effective decoder finetuning scheme based on adversarial training to significantly enhance the visual quality of ICM codecs, while preserving the machine analysis accuracy, without adding extra bitcost or parameters at the inference phase. The results show complete removal of the checkerboard artifacts at the negligible cost of -1.6% relative change in task performance score. In the cases where some amount of artifacts is tolerable, such as when machine consumption is the primary target, this technique can enhance both pixel-fidelity and feature-fidelity scores without losing task performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器图像编码 (ICM) 旨在降低表示图像所需的比特率，同时最大限度地减少机器视觉分析精度的下降。在许多用例中，例如监控，视觉质量不会因压缩过程而急剧恶化也很重要。最近使用基于神经网络 (NN) 的 ICM 编解码器的工作已经显示出相对于传统方法的显着编码增益；然而，解压缩的图像，尤其是在低比特率下，通常包含棋盘伪像。我们提出了一种基于对抗性训练的有效解码器微调方案，以显着提高 ICM 编解码器的视觉质量，同时保持机器分析准确性，而无需在推理阶段添加额外的比特成本或参数。结果显示，任务性能得分相对变化 -1.6% 的成本可以忽略不计，完全消除了棋盘伪影。在可以容忍一定数量的伪像的情况下，例如当机器消耗是主要目标时，该技术可以增强像素保真度和特征保真度分数，而不会损失任务性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.10732v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **A comprehensive study on fidelity metrics for XAI**<br />
**Title_cn:** XAI 保真度指标的综合研究<br />
**Authors:** Miquel Miró-Nicolau, Antoni Jaume-i-Capó, Gabriel Moyà-Alcover<br />
**Abstract:** <details><summary>原文: </summary>The use of eXplainable Artificial Intelligence (XAI) systems has introduced a set of challenges that need resolution. Herein, we focus on how to correctly select an XAI method, an open questions within the field. The inherent difficulty of this task is due to the lack of a ground truth. Several authors have proposed metrics to approximate the fidelity of different XAI methods. These metrics lack verification and have concerning disagreements. In this study, we proposed a novel methodology to verify fidelity metrics, using a well-known transparent model, namely a decision tree. This model allowed us to obtain explanations with perfect fidelity. Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods. We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images. The images from these datasets had a size a 128 by 128 pixels and were synthetic data that simplified the training process. All metric values, indicated a lack of fidelity, with the best one showing a 30 \% deviation from the expected values for perfect explanation. Our experimentation led us to conclude that the current fidelity metrics are not reliable enough to be used in real scenarios. From this finding, we deemed it necessary to development new metrics, to avoid the detected problems, and we recommend the usage of our proposal as a benchmark within the scientific community to address these limitations.</details>
**Abstract_cn:** <details><summary>译文: </summary>可解释人工智能 (XAI) 系统的使用带来了一系列需要解决的挑战。在此，我们重点讨论如何正确选择 XAI 方法，这是该领域内的一个悬而未决的问题。这项任务的固有困难是由于缺乏基本事实。几位作者提出了近似不同 XAI 方法保真度的指标。这些指标缺乏验证并且存在令人担忧的分歧。在这项研究中，我们提出了一种新颖的方法来验证保真度指标，使用众所周知的透明模型，即决策树。这个模型使我们能够获得完美保真度的解释。我们的提案构成了这些指标的第一个客观基准，有助于对现有提案进行比较，并超越现有方法。我们应用基准来评估两个不同实验中现有的保真度指标，每个实验都使用包含 52,000 张图像的公共数据集。这些数据集中的图像大小为 128 x 128 像素，是简化训练过程的合成数据。所有指标值均表明缺乏保真度，其中最好的指标与完美解释的预期值存在 30% 的偏差。我们的实验使我们得出结论，当前的保真度指标不够可靠，无法在实际场景中使用。根据这一发现，我们认为有必要开发新的指标，以避免检测到的问题，并且我们建议使用我们的提案作为科学界的基准来解决这些限制。</details>
**PDF:** <http://arxiv.org/pdf/2401.10640v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows**<br />
**Title_cn:** 用于流降阶建模的具有平滑聚类的多面自编码器<br />
**Authors:** Jan Heiland, Yongho Kim<br />
**Abstract:** <details><summary>原文: </summary>With the advancement of neural networks, there has been a notable increase, both in terms of quantity and variety, in research publications concerning the application of autoencoders to reduced-order models. We propose a polytopic autoencoder architecture that includes a lightweight nonlinear encoder, a convex combination decoder, and a smooth clustering network. Supported by several proofs, the model architecture ensures that all reconstructed states lie within a polytope, accompanied by a metric indicating the quality of the constructed polytopes, referred to as polytope error. Additionally, it offers a minimal number of convex coordinates for polytopic linear-parameter varying systems while achieving acceptable reconstruction errors compared to proper orthogonal decomposition (POD). To validate our proposed model, we conduct simulations involving two flow scenarios with the incompressible Navier-Stokes equation. Numerical results demonstrate the guaranteed properties of the model, low reconstruction errors compared to POD, and the improvement in error using a clustering network.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着神经网络的进步，有关自动编码器在降阶模型中的应用的研究出版物在数量和种类上都有显着增加。我们提出了一种多面自动编码器架构，其中包括轻量级非线性编码器、凸组合解码器和平滑聚类网络。在多个证明的支持下，模型架构确保所有重建状态都位于多胞体内，并附有指示所构造多胞体质量的度量，称为多胞体误差。此外，与适当的正交分解 (POD) 相比，它为多面线性参数变化系统提供了最少数量的凸坐标，同时实现了可接受的重建误差。为了验证我们提出的模型，我们使用不可压缩的纳维-斯托克斯方程进行了涉及两种流动场景的模拟。数值结果证明了模型的保证特性、与 POD 相比较低的重建误差以及使用聚类网络的误差改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.10620v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network**<br />
**Title_cn:** 360ORB-SLAM：具有深度补全网络的全景图像视觉SLAM系统<br />
**Authors:** Yichen Chen, Yiqi Pan, Ruyu Liu, Haoyu Zhang, Guodao Zhang, Bo Sun, Jianhua Zhang<br />
**Abstract:** <details><summary>原文: </summary>To enhance the performance and effect of AR/VR applications and visual assistance and inspection systems, visual simultaneous localization and mapping (vSLAM) is a fundamental task in computer vision and robotics. However, traditional vSLAM systems are limited by the camera's narrow field-of-view, resulting in challenges such as sparse feature distribution and lack of dense depth information. To overcome these limitations, this paper proposes a 360ORB-SLAM system for panoramic images that combines with a depth completion network. The system extracts feature points from the panoramic image, utilizes a panoramic triangulation module to generate sparse depth information, and employs a depth completion network to obtain a dense panoramic depth map. Experimental results on our novel panoramic dataset constructed based on Carla demonstrate that the proposed method achieves superior scale accuracy compared to existing monocular SLAM methods and effectively addresses the challenges of feature association and scale ambiguity. The integration of the depth completion network enhances system stability and mitigates the impact of dynamic elements on SLAM performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了增强 AR/VR 应用以及视觉辅助和检查系统的性能和效果，视觉同步定位和建图 (vSLAM) 是计算机视觉和机器人技术中的一项基本任务。然而，传统的vSLAM系统受到相机视场较窄的限制，导致特征分布稀疏、缺乏密集的深度信息等挑战。为了克服这些限制，本文提出了一种与深度补全网络相结合的全景图像 360ORB-SLAM 系统。该系统从全景图像中提取特征点，利用全景三角测量模块生成稀疏深度信息，并利用深度补全网络获得密集的全景深度图。基于 Carla 构建的新型全景数据集的实验结果表明，与现有的单目 SLAM 方法相比，所提出的方法具有更高的尺度精度，并有效解决了特征关联和尺度模糊的挑战。深度补全网络的集成增强了系统稳定性并减轻了动态元素对SLAM性能的影响。</details>
**PDF:** <http://arxiv.org/pdf/2401.10560v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **GMC-IQA: Exploiting Global-correlation and Mean-opinion Consistency for No-reference Image Quality Assessment**<br />
**Title_cn:** GMC-IQA：利用全局相关性和平均意见一致性进行无参考图像质量评估<br />
**Authors:** Zewen Chen, Juan Wang, Bing Li, Chunfeng Yuan, Weiming Hu, Junxian Liu, Peng Li, Yan Wang, Youqun Zhang, Congxuan Zhang<br />
**Abstract:** <details><summary>原文: </summary>Due to the subjective nature of image quality assessment (IQA), assessing which image has better quality among a sequence of images is more reliable than assigning an absolute mean opinion score for an image. Thus, IQA models are evaluated by global correlation consistency (GCC) metrics like PLCC and SROCC, rather than mean opinion consistency (MOC) metrics like MAE and MSE. However, most existing methods adopt MOC metrics to define their loss functions, due to the infeasible computation of GCC metrics during training. In this work, we construct a novel loss function and network to exploit Global-correlation and Mean-opinion Consistency, forming a GMC-IQA framework. Specifically, we propose a novel GCC loss by defining a pairwise preference-based rank estimation to solve the non-differentiable problem of SROCC and introducing a queue mechanism to reserve previous data to approximate the global results of the whole data. Moreover, we propose a mean-opinion network, which integrates diverse opinion features to alleviate the randomness of weight learning and enhance the model robustness. Experiments indicate that our method outperforms SOTA methods on multiple authentic datasets with higher accuracy and generalization. We also adapt the proposed loss to various networks, which brings better performance and more stable training.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于图像质量评估 (IQA) 的主观性质，评估图像序列中哪个图像具有更好的质量比为图像分配绝对平均意见分数更可靠。因此，IQA 模型是通过 PLCC 和 SROCC 等全局相关一致性 (GCC) 指标来评估的，而不是 MAE 和 MSE 等平均意见一致性 (MOC) 指标。然而，由于训练期间 GCC 指标的计算不可行，大多数现有方法采用 MOC 指标来定义其损失函数。在这项工作中，我们构建了一种新颖的损失函数和网络来利用全局相关性和平均意见一致性，形成 GMC-IQA 框架。具体来说，我们提出了一种新颖的 GCC 损失，通过定义基于成对偏好的排名估计来解决 SROCC 的不可微问题，并引入队列机制来保留先前的数据以近似整个数据的全局结果。此外，我们提出了一种平均意见网络，它集成了不同的意见特征，以减轻权重学习的随机性并增强模型的鲁棒性。实验表明，我们的方法在多个真实数据集上优于 SOTA 方法，具有更高的准确性和泛化性。我们还使建议的损失适应各种网络，这带来了更好的性能和更稳定的训练。</details>
**PDF:** <http://arxiv.org/pdf/2401.10511v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **CBVS: A Large-Scale Chinese Image-Text Benchmark for Real-World Short Video Search Scenarios**<br />
**Title_cn:** CBVS：真实世界短视频搜索场景的大规模中文图文基准<br />
**Authors:** Xiangshuo Qiao, Xianxin Li, Xiaozhe Qu, Jie Zhang, Yang Liu, Yu Luo, Cihang Jin, Jin Ma<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models pre-trained on large-scale image-text datasets have shown superior performance in downstream tasks such as image retrieval. Most of the images for pre-training are presented in the form of open domain common-sense visual elements. Differently, video covers in short video search scenarios are presented as user-originated contents that provide important visual summaries of videos. In addition, a portion of the video covers come with manually designed cover texts that provide semantic complements. In order to fill in the gaps in short video cover data, we establish the first large-scale cover-text benchmark for Chinese short video search scenarios. Specifically, we release two large-scale datasets CBVS-5M/10M to provide short video covers, and the manual fine-labeling dataset CBVS-20K to provide real user queries, which serves as an image-text benchmark test in the Chinese short video search field. To integrate the semantics of cover text in the case of modality missing, we propose UniCLIP where cover texts play a guiding role during training, however are not relied upon by inference. Extensive evaluation on CBVS-20K demonstrates the excellent performance of our proposal. UniCLIP has been deployed to Tencent's online video search systems with hundreds of millions of visits and achieved significant gains. The complete dataset, code and checkpoints will be available upon release.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大规模图像文本数据集上预训练的视觉语言模型在图像检索等下游任务中表现出了卓越的性能。大多数预训练的图像都以开放域常识视觉元素的形式呈现。不同的是，短视频搜索场景中的视频封面呈现为用户原创内容，提供视频的重要视觉摘要。此外，部分视频封面还带有手动设计的封面文本，提供语义补充。为了填补短视频封面数据的空白，我们建立了第一个针对中国短视频搜索场景的大规模封面文本基准。具体来说，我们发布了两个大规模数据集CBVS-5M/10M来提供短视频封面，以及手动精细标记数据集CBVS-20K来提供真实的用户查询，作为中文短视频中的图文基准测试。搜索字段。为了在模态缺失的情况下整合封面文本的语义，我们提出了 UniCLIP，其中封面文本在训练期间发挥指导作用，但不依赖于推理。对 CBVS-20K 的广泛评估证明了我们的方案的出色性能。 UniCLIP已部署至腾讯在线视频搜索系统，访问量达数亿，并取得显着收益。完整的数据集、代码和检查点将在发布时提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.10475v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **LDReg: Local Dimensionality Regularized Self-Supervised Learning**<br />
**Title_cn:** LDReg：局部维度正则化自监督学习<br />
**Authors:** Hanxun Huang, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun Ma, Michael E. Houle, James Bailey<br />
**Abstract:** <details><summary>原文: </summary>Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the representation quality of SSL. The results also show that LDReg can regularize dimensionality at both local and global levels.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过自监督学习（SSL）学习的表示可能容易受到维度崩溃的影响，其中学习的表示子空间的维度极低，因此无法表示完整的数据分布和模式。维度崩溃也称为“填充不足”现象，是下游任务性能下降的主要原因之一。之前的工作已经在全球范围内研究了 SSL 的维度崩溃问题。在本文中，我们证明了表示可以全局跨越高维空间，但局部崩溃。为了解决这个问题，我们提出了一种名为$\textit{局部维数正则化（LDReg）}$的方法。我们的公式基于 Fisher-Rao 度量的推导，以比较和优化每个数据点渐近小半径处的局部距离分布。通过增加局部内在维度，我们通过一系列实验证明 LDReg 提高了 SSL 的表示质量。结果还表明 LDReg 可以在局部和全局层面上正则化维度。</details>
**PDF:** <http://arxiv.org/pdf/2401.10474v1><br />
**Code:** <https://github.com/HanxunH/LDReg>**<br />
>>**index:** 11<br />
**Title:** **Learning to Robustly Reconstruct Low-light Dynamic Scenes from Spike Streams**<br />
**Title_cn:** 学习从尖峰流中稳健地重建低光动态场景<br />
**Authors:** Liwen Hu, Ziluo Ding, Mianzhi Liu, Lei Ma, Tiejun Huang<br />
**Abstract:** <details><summary>原文: </summary>As a neuromorphic sensor with high temporal resolution, spike camera can generate continuous binary spike streams to capture per-pixel light intensity. We can use reconstruction methods to restore scene details in high-speed scenarios. However, due to limited information in spike streams, low-light scenes are difficult to effectively reconstruct. In this paper, we propose a bidirectional recurrent-based reconstruction framework, including a Light-Robust Representation (LR-Rep) and a fusion module, to better handle such extreme conditions. LR-Rep is designed to aggregate temporal information in spike streams, and a fusion module is utilized to extract temporal features. Additionally, we have developed a reconstruction benchmark for high-speed low-light scenes. Light sources in the scenes are carefully aligned to real-world conditions. Experimental results demonstrate the superiority of our method, which also generalizes well to real spike streams. Related codes and proposed datasets will be released after publication.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为具有高时间分辨率的神经形态传感器，尖峰相机可以生成连续的二进制尖峰流来捕获每个像素的光强度。我们可以使用重建方法来恢复高速场景下的场景细节。然而，由于尖峰流中的信息有限，低光场景很难有效地重建。在本文中，我们提出了一种基于双向循环的重建框架，包括轻鲁棒表示（LR-Rep）和融合模块，以更好地处理这种极端条件。 LR-Rep 旨在聚合尖峰流中的时间信息，并利用融合模块来提取时间特征。此外，我们还开发了针对高速低光场景的重建基准。场景中的光源与现实世界的条件仔细对齐。实验结果证明了我们的方法的优越性，该方法也可以很好地推广到真实的尖峰流。相关代码和建议的数据集将在出版后发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.10461v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Path Choice Matters for Clear Attribution in Path Methods**<br />
**Title_cn:** 路径选择对于路径方法中的清晰归因至关重要<br />
**Authors:** Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu<br />
**Abstract:** <details><summary>原文: </summary>Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices. To address the ambiguity, we introduce \textbf{Concentration Principle}, which centrally allocates high attributions to indispensable features, thereby endowing aesthetic and sparsity. We then present \textbf{SAMP}, a model-agnostic interpreter, which efficiently searches the near-optimal path from a pre-defined set of manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and momentum strategy (MS) to improve the rigorousness and optimality. Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient image pixels. We also perform quantitative experiments and observe that our method significantly outperforms the counterparts. Code: https://github.com/zbr17/SAMP.</details>
**Abstract_cn:** <details><summary>译文: </summary>严谨性和清晰度对于 DNN 的解释以建立人类信任至关重要。路径方法通常用于生成满足三个公理的严格归因。然而，由于不同的路径选择，归因的含义仍然不明确。为了解决这种歧义，我们引入了 \textbf{集中原则}，它将高归因集中分配给不可或缺的特征，从而赋予美感和稀疏性。然后，我们提出 \textbf{SAMP}，一个与模型无关的解释器，它可以从一组预定义的操作路径中有效地搜索接近最优的路径。此外，我们提出了无穷小约束（IC）和动量策略（MS）来提高严格性和最优性。可视化结果表明，SAMP 可以通过精确定位显着图像像素来精确揭示 DNN。我们还进行了定量实验，并观察到我们的方法明显优于同行。代码：https://github.com/zbr17/SAMP。</details>
**PDF:** <http://arxiv.org/pdf/2401.10442v1><br />
**Code:** null<br />

