## [UPDATED!] **2024-01-20** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery**<br />
**Title_cn:** 圆形扫描合成孔径声纳图像的弱监督语义分割<br />
**Authors:** Isaac J. Sledge, Dominic M. Byrne, Jonathan L. King, Steven H. Ostertag, Denton L. Woods, James L. Prater, Jermaine L. Kennedy, Timothy M. Marston, Jose C. Principe<br />
**Abstract:** <details><summary>原文: </summary>We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.   We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种弱监督框架，用于圆形扫描合成孔径声纳（CSAS）图像的语义分割。我们框架的第一部分以图像级标签上的监督方式进行训练，以发现每个图像中的一组半稀疏的、空间区分的区域。然后评估每个区域的分类不确定性。然后，选择那些不确定性最低的区域作为框架第二部分在像素级别的弱标记分割种子。每个种子范围都根据无监督的信息论损失和结构化预测正则化器逐步调整大小。此重塑过程使用多尺度、自适应加权特征来描绘局部图像内容中特定于类的过渡。内容可寻址存储器被插入到我们框架的各个部分，以便它可以利用先前看到的图像的特征来提高相关图像的分割性能。我们使用包含十多个海底类别和十多个目标类别的真实 CSAS 图像来评估我们的弱监督框架。我们证明我们的框架的性能与九个完全监督的深度网络相当。我们的框架还优于十一个最好的弱监督深度网络。在对自然图像进行预训练时，我们实现了最先进的性能。对于自然图像和声纳图像来说，与次优弱监督网络的平均绝对性能差距远远超过百分之十。发现这种差距在统计上是显着的。</details>
**PDF:** <http://arxiv.org/pdf/2401.11313v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models**<br />
**Title_cn:** 基础模型时代少样本语义分割的新基准<br />
**Authors:** Reda Bensaid, Vincent Gripon, François Leduc-Primeau, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，计算机视觉的快速发展出现了各种视觉基础模型，每种模型都针对特定的数据类型和任务量身定制。虽然大型语言模型通常共享一个共同的借口任务，但视觉基础模型的多样性源于其不同的训练目标。在这项研究中，我们深入研究了如何确定最有效的视觉基础模型来进行少样本语义分割，这是计算机视觉中的一项关键任务。具体来说，我们对四种著名的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders 以及在 COCO 数据集上预训练的简单 ResNet50。我们的研究重点是它们对新语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，DINO V2 在各种数据集和适应方法中始终优于其他考虑的基础模型。这一结果强调了 DINO V2 与同类产品相比，适应语义分割任务的卓越能力。此外，我们的观察表明，各种适配器方法表现出相似的性能，强调选择鲁棒的特征提取器比适应技术本身的复杂性至关重要。这一见解揭示了特征提取在少镜头语义分割背景下的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了宝贵的见解，而且还强调了强大的特征提取器在该领域的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11311v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation**<br />
**Title_cn:** LRP-QViT：通过分层相关性传播进行混合精度视觉变压器量化<br />
**Authors:** Navin Ranjan, Andreas Savakis<br />
**Abstract:** <details><summary>原文: </summary>Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉转换器 (ViT) 在各种视觉任务中表现出了卓越的性能。然而，ViT 模型面临大量的计算和内存需求，使得在资源受限的平台上部署它们具有挑战性。量化是减小模型大小的流行方法，但大多数研究主要集中在整个网络的等位宽量化上，从而导致解决方案次优。虽然关于 ViT 混合精度量化（MPQ）的工作很少，但它们通常依赖于基于搜索空间的方法或任意采用混合精度。在本文中，我们介绍了 LRP-QViT，这是一种基于可解释性的方法，用于根据分类过程中的重要性将混合精度比特分配分配给不同的层。具体来说，为了衡量每一层在预测目标类别中的贡献分数，我们采用逐层相关性传播（LRP）方法。 LRP 在输出层分配局部相关性，并将其传播到所有层，分配相关性直到到达输入层。这些相关性分数充当计算层贡献分数的指标。此外，我们引入了一种剪切通道量化，旨在消除 LayerNorm 后激活中的异常值，以减轻严重的通道间变化。为了验证和评估我们的方法，我们在各种数据集上跨 ViT、DeiT 和 Swin 变压器模型使用 LRP-QViT。我们的实验结果表明，我们的固定位和混合位训练后量化方法在 4 位和 6 位量化方面都超越了现有模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.11243v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Unifying Visual and Vision-Language Tracking via Contrastive Learning**<br />
**Title_cn:** 通过对比学习统一视觉和视觉语言跟踪<br />
**Authors:** Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Jinpeng Zhang, Mengxue Kang<br />
**Abstract:** <details><summary>原文: </summary>Single object tracking aims to locate the target object in a video sequence according to the state specified by different modal references, including the initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to the gap between different modalities, most existing trackers are designed for single or partial of these reference settings and overspecialize on the specific modality. Differently, we present a unified tracker called UVLTrack, which can simultaneously handle all three reference settings (BBOX, NL, NL+BBOX) with the same parameters. The proposed UVLTrack enjoys several merits. First, we design a modality-unified feature extractor for joint visual and language feature learning and propose a multi-modal contrastive loss to align the visual and language features into a unified semantic space. Second, a modality-adaptive box head is proposed, which makes full use of the target reference to mine ever-changing scenario features dynamically from video contexts and distinguish the target in a contrastive way, enabling robust performance in different reference settings. Extensive experimental results demonstrate that UVLTrack achieves promising performance on seven visual tracking datasets, three vision-language tracking datasets, and three visual grounding datasets. Codes and models will be open-sourced at https://github.com/OpenSpaceAI/UVLTrack.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目标跟踪旨在根据不同模态参考指定的状态来定位视频序列中的目标对象，包括初始边界框（BBOX）、自然语言（NL）或两者（NL+BBOX）。由于不同模态之间的差距，大多数现有跟踪器都是针对这些参考设置中的单个或部分而设计的，并且过度专注于特定模态。不同的是，我们提出了一个名为 UVLTrack 的统一跟踪器，它可以使用相同的参数同时处理所有三个参考设置（BBOX、NL、NL+BBOX）。所提出的 UVLTrack 有几个优点。首先，我们设计了一个用于联合视觉和语言特征学习的模态统一特征提取器，并提出了一种多模态对比损失，将视觉和语言特征对齐到统一的语义空间中。其次，提出了一种模态自适应盒头，它充分利用目标参考从视频上下文中动态挖掘不断变化的场景特征，并以对比的方式区分目标，从而在不同的参考设置下实现鲁棒的性能。大量的实验结果表明，UVLTrack 在七个视觉跟踪数据集、三个视觉语言跟踪数据集和三个视觉基础数据集上取得了良好的性能。代码和模型将在 https://github.com/OpenSpaceAI/UVLTrack 开源。</details>
**PDF:** <http://arxiv.org/pdf/2401.11228v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Susceptibility of Adversarial Attack on Medical Image Segmentation Models**<br />
**Title_cn:** 医学图像分割模型对抗性攻击的敏感性<br />
**Authors:** Zhongxuan Wang, Leo Xu<br />
**Abstract:** <details><summary>原文: </summary>The nature of deep neural networks has given rise to a variety of attacks, but little work has been done to address the effect of adversarial attacks on segmentation models trained on MRI datasets. In light of the grave consequences that such attacks could cause, we explore four models from the U-Net family and examine their responses to the Fast Gradient Sign Method (FGSM) attack. We conduct FGSM attacks on each of them and experiment with various schemes to conduct the attacks. In this paper, we find that medical imaging segmentation models are indeed vulnerable to adversarial attacks and that there is a negligible correlation between parameter size and adversarial attack success. Furthermore, we show that using a different loss function than the one used for training yields higher adversarial attack success, contrary to what the FGSM authors suggested. In future efforts, we will conduct the experiments detailed in this paper with more segmentation models and different attacks. We will also attempt to find ways to counteract the attacks by using model ensembles or special data augmentations. Our code is available at https://github.com/ZhongxuanWang/adv_attk</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络的性质引发了各种攻击，但很少有工作来解决对抗性攻击对 MRI 数据集训练的分割模型的影响。鉴于此类攻击可能造成的严重后果，我们探索了 U-Net 系列中的四种模型，并检查了它们对快速梯度符号方法 (FGSM) 攻击的响应。我们对它们中的每一个进行 FGSM 攻击，并尝试各种方案来进行攻击。在本文中，我们发现医学成像分割模型确实容易受到对抗性攻击，并且参数大小和对抗性攻击成功之间的相关性可以忽略不计。此外，我们表明，使用与训练中使用的损失函数不同的损失函数会产生更高的对抗性攻击成功率，这与 FGSM 作者的建议相反。在未来的工作中，我们将使用更多的分割模型和不同的攻击来进行本文详细介绍的实验。我们还将尝试找到通过使用模型集成或特殊数据增强来抵消攻击的方法。我们的代码位于 https://github.com/ZhongxuanWang/adv_attk</details>
**PDF:** <http://arxiv.org/pdf/2401.11224v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Towards Category Unification of 3D Single Object Tracking on Point Clouds**<br />
**Title_cn:** 迈向点云上 3D 单目标跟踪的类别统一<br />
**Authors:** Jiahao Nie, Zhiwei He, Xudong Lv, Xueyi Zhou, Dong-Kyu Chae, Fei Xie<br />
**Abstract:** <details><summary>原文: </summary>Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We find that the attribute variances of point cloud objects primarily occur from the varying size and shape (e.g., large and square vehicles v.s. small and slender humans). Based on this observation, we design a novel point set representation learning network inheriting transformer architecture, termed AdaFormer, which adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. We further incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models SiamCUT and MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin (e.g., on KITTI dataset, 12% and 3% performance gains on the Siamese and motion paradigms). Our code will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>无论是 Siamese 范式还是以运动为中心的范式，特定类别模型都是 3D 单对象跟踪 (SOT) 中经证明有价值的方法。然而，这种过于专业化的模型设计会产生冗余参数，从而限制了 3D SOT 任务的更广泛适用性。本文首先介绍了统一模型，该模型可以使用具有共享模型参数的单个网络同时跟踪所有类别的对象。具体来说，我们建议显式编码与不同对象类别相关的不同属性，使模型能够适应跨类别数据。我们发现点云对象的属性差异主要来自不同的尺寸和形状（例如，大型和方形的车辆与小型和细长的人类）。基于这一观察，我们设计了一种继承 Transformer 架构的新型点集表示学习网络，称为 AdaFormer，它以统一的方式自适应地编码来自跨类别数据的动态变化的形状和大小信息。我们进一步将从已知模板目标导出的尺寸和形状先验合并到模型的输入和学习目标中，促进统一表示的学习。配备这样的设计，我们构建了两个类别统一的模型SiamCUT和MoCUT。大量的实验表明SiamCUT和MoCUT表现出很强的泛化性和训练稳定性。此外，我们的类别统一模型明显优于特定类别模型（例如，在 KITTI 数据集上，暹罗模型和运动范例的性能分别提高了 12% 和 3%）。我们的代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2401.11204v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Pixel-Wise Recognition for Holistic Surgical Scene Understanding**<br />
**Title_cn:** 用于整体手术场景理解的逐像素识别<br />
**Authors:** Nicolás Ayobi, Santiago Rodríguez, Alejandra Pérez, Isabela Hernández, Nicolás Aparicio, Eugénie Dessevres, Sebastián Peña, Jessica Santander, Juan Ignacio Caicedo, Nicolás Fernández, et.al.<br />
**Abstract:** <details><summary>原文: </summary>This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity requirements of each task, and establish TAPIS's superiority over previously proposed baselines and conventional CNN-based models. Additionally, we validate the robustness of our method across multiple public benchmarks, confirming the reliability and applicability of our dataset. This work represents a significant step forward in Endoscopic Vision, offering a novel and comprehensive framework for future research towards a holistic understanding of surgical procedures.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了前列腺切除术的整体和多粒度手术场景理解 (GraSP) 数据集，这是一个精心策划的基准，它将手术场景理解建模为具有不同粒度级别的补充任务的层次结构。我们的方法能够对手术活动进行多层次的理解，包括手术阶段和步骤识别等长期任务以及手术器械分割和原子视觉动作检测等短期任务。为了利用我们提出的基准，我们引入了动作、阶段、步骤和仪器分割（TAPIS）模型的变压器，这是一种通用架构，它将全局视频特征提取器与来自仪器分割模型的局部区域建议相结合，以解决多粒度问题我们的基准。通过广泛的实验，我们展示了在短期识别任务中包含分割注释的影响，突出了每个任务不同的粒度要求，并确立了 TAPIS 相对于之前提出的基线和传统的基于 CNN 的模型的优越性。此外，我们在多个公共基准上验证了我们方法的稳健性，确认了我们数据集的可靠性和适用性。这项工作代表了内窥镜视觉领域向前迈出的重要一步，为未来研究提供了一个新颖而全面的框架，以全面了解外科手术。</details>
**PDF:** <http://arxiv.org/pdf/2401.11174v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Simultaneous Gesture Classification and Localization with an Automatic Gesture Annotation Model**<br />
**Title_cn:** 使用自动手势注释模型同时进行手势分类和定位<br />
**Authors:** Junxiao Shen, Xuhai Xu, Ran Tan, Amy Karlson, Evan Strasnick<br />
**Abstract:** <details><summary>原文: </summary>Training a real-time gesture recognition model heavily relies on annotated data. However, manual data annotation is costly and demands substantial human effort. In order to address this challenge, we propose a novel annotation model that can automatically annotate gesture classes and identify their temporal ranges. Our ablation study demonstrates that our annotation model design surpasses the baseline in terms of both gesture classification accuracy (3-4\% improvement) and localization accuracy (71-75\% improvement). We believe that this annotation model has immense potential to improve the training of downstream gesture recognition models using unlabeled datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>训练实时手势识别模型很大程度上依赖于带注释的数据。然而，手动数据注释成本高昂并且需要大量的人力。为了应对这一挑战，我们提出了一种新颖的注释模型，可以自动注释手势类别并识别其时间范围。我们的消融研究表明，我们的注释模型设计在手势分类精度（提高 3-4%）和定位精度（提高 71-75%）方面都超过了基线。我们相信，该注释模型具有巨大的潜力，可以改进使用未标记数据集的下游手势识别模型的训练。</details>
**PDF:** <http://arxiv.org/pdf/2401.11150v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Towards Open-World Gesture Recognition**<br />
**Title_cn:** 迈向开放世界手势识别<br />
**Authors:** Junxiao Shen, Matthias De Lange, Xuhai "Orson" Xu, Enmin Zhou, Ran Tan, Naveen Suda, Maciej Lazarewicz, Per Ola Kristensson, Amy Karlson, Evan Strasnick<br />
**Abstract:** <details><summary>原文: </summary>Static machine learning methods in gesture recognition assume that training and test data come from the same underlying distribution. However, in real-world applications involving gesture recognition on wrist-worn devices, data distribution may change over time. We formulate this problem of adapting recognition models to new tasks, where new data patterns emerge, as open-world gesture recognition (OWGR). We propose leveraging continual learning to make machine learning models adaptive to new tasks without degrading performance on previously learned tasks. However, the exploration of parameters for questions around when and how to train and deploy recognition models requires time-consuming user studies and is sometimes impractical. To address this challenge, we propose a design engineering approach that enables offline analysis on a collected large-scale dataset with various parameters and compares different continual learning methods. Finally, design guidelines are provided to enhance the development of an open-world wrist-worn gesture recognition process.</details>
**Abstract_cn:** <details><summary>译文: </summary>手势识别中的静态机器学习方法假设训练和测试数据来自相同的底层分布。然而，在涉及腕戴式设备上的手势识别的现实应用中，数据分布可能会随着时间的推移而改变。我们将识别模型适应新任务（其中出现新数据模式）的问题表述为开放世界手势识别（OWGR）。我们建议利用持续学习使机器学习模型适应新任务，而不会降低先前学习任务的性能。然而，探索何时以及如何训练和部署识别模型的参数需要耗时的用户研究，有时是不切实际的。为了应对这一挑战，我们提出了一种设计工程方法，可以对收集的具有各种参数的大规模数据集进行离线分析，并比较不同的持续学习方法。最后，提供了设计指南，以增强开放世界腕戴式手势识别过程的开发。</details>
**PDF:** <http://arxiv.org/pdf/2401.11144v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities**<br />
**Title_cn:** 高斯自适应注意力就是您所需要的：跨多种模式的稳健上下文表示<br />
**Authors:** Georgios Ioannides, Aman Chadha, Aaron Elkins<br />
**Abstract:** <details><summary>原文: </summary>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy across a diverse range of tasks, including emotion recognition in speech, image classification, and text classification, thereby establishing its robustness and versatility in handling multi-modal data. Furthermore, we introduce the Importance Factor (IF), a new learning-based metric that enhances the explainability of models trained with GAAM-based methods. Overall, GAAM represents an advancement towards development of better performing and more explainable attention models across multiple modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了多头高斯自适应注意力机制（GAAM）（一种新颖的概率注意力框架）和高斯自适应变换器（GAT），旨在增强跨多种模式（包括语音、文本和视觉）的信息聚合。 GAAM 将可学习的均值和方差集成到其注意力机制中，并在多头框架中实现，使其能够对任何概率分布进行集体建模，以动态重新校准特征重要性。该方法展示了显着的改进，特别是对于高度非平稳的数据，通过识别特征空间内的关键元素，在模型性能方面超越了最先进的注意力技术（准确率高达约+20%）。 GAAM 与基于点积的注意力模型的兼容性和相对较少的参数数量展示了其适应性和提升现有注意力框架的潜力。根据经验，GAAM 在各种任务中表现出卓越的适应性和有效性，包括语音中的情感识别、图像分类和文本分类，从而确立了其在处理多模态数据方面的鲁棒性和多功能性。此外，我们还引入了重要性因子 (IF)，这是一种新的基于学习的指标，可增强使用基于 GAAM 的方法训练的模型的可解释性。总体而言，GAAM 代表了跨多种模式开发性能更好、更易于解释的注意力模型的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.11143v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end Object Detection**<br />
**Title_cn:** 用于少样本端到端物体检测的稳定性可塑性解耦微调<br />
**Authors:** Yuantao Yin, Ping Yin<br />
**Abstract:** <details><summary>原文: </summary>Few-shot object detection(FSOD) aims to design methods to adapt object detectors efficiently with only few annotated samples. Fine-tuning has been shown to be an effective and practical approach. However, previous works often take the classical base-novel two stage fine-tuning procedure but ignore the implicit stability-plasticity contradiction among different modules. Specifically, the random re-initialized classifiers need more plasticity to adapt to novel samples. The other modules inheriting pre-trained weights demand more stability to reserve their class-agnostic knowledge. Regular fine-tuning which couples the optimization of these two parts hurts the model generalization in FSOD scenarios. In this paper, we find that this problem is prominent in the end-to-end object detector Sparse R-CNN for its multi-classifier cascaded architecture. We propose to mitigate this contradiction by a new three-stage fine-tuning procedure by introducing an addtional plasticity classifier fine-tuning(PCF) stage. We further design the multi-source ensemble(ME) technique to enhance the generalization of the model in the final fine-tuning stage. Extensive experiments verify that our method is effective in regularizing Sparse R-CNN, outperforming previous methods in the FSOD benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>少样本目标检测（FSOD）旨在设计方法，仅用少量带注释的样本即可有效地适应目标检测器。微调已被证明是一种有效且实用的方法。然而，以前的工作往往采取经典的基础小说两阶段微调程序，而忽略了不同模块之间隐含的稳定性-可塑性矛盾。具体来说，随机重新初始化的分类器需要更多的可塑性来适应新的样本。继承预训练权重的其他模块需要更高的稳定性来保留其与类别无关的知识。定期微调将这两部分的优化结合在一起，会损害 FSOD 场景中的模型泛化能力。在本文中，我们发现这个问题在端到端目标检测器 Sparse R-CNN 的多分类器级联架构中很突出。我们建议通过引入额外的塑性分类器微调（PCF）阶段，通过新的三阶段微调程序来缓解这一矛盾。我们进一步设计了多源集成（ME）技术，以增强最终微调阶段模型的泛化能力。大量实验验证了我们的方法在规范稀疏 R-CNN 方面是有效的，在 FSOD 基准测试中优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11140v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Uncertainty-aware Bridge based Mobile-Former Network for Event-based Pattern Recognition**<br />
**Title_cn:** 基于不确定性感知桥的移动前网络，用于基于事件的模式识别<br />
**Authors:** Haoxiang Yang, Chengguo Yuan, Yabin Zhu, Lan Chen, Xiao Wang, Jin Tang<br />
**Abstract:** <details><summary>原文: </summary>The mainstream human activity recognition (HAR) algorithms are developed based on RGB cameras, which are easily influenced by low-quality images (e.g., low illumination, motion blur). Meanwhile, the privacy protection issue caused by ultra-high definition (HD) RGB cameras aroused more and more people's attention. Inspired by the success of event cameras which perform better on high dynamic range, no motion blur, and low energy consumption, we propose to recognize human actions based on the event stream. We propose a lightweight uncertainty-aware information propagation based Mobile-Former network for efficient pattern recognition, which aggregates the MobileNet and Transformer network effectively. Specifically, we first embed the event images using a stem network into feature representations, then, feed them into uncertainty-aware Mobile-Former blocks for local and global feature learning and fusion. Finally, the features from MobileNet and Transformer branches are concatenated for pattern recognition. Extensive experiments on multiple event-based recognition datasets fully validated the effectiveness of our model. The source code of this work will be released at https://github.com/Event-AHU/Uncertainty_aware_MobileFormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>主流的人体活动识别（HAR）算法是基于RGB相机开发的，很容易受到低质量图像（例如低照度、运动模糊）的影响。与此同时，超高清RGB摄像头带来的隐私保护问题也引起了越来越多人们的关注。受到事件相机在高动态范围、无运动模糊和低能耗方面表现更好的成功的启发，我们建议根据事件流识别人类动作。我们提出了一种基于轻量级不确定性感知信息传播的 Mobile-Former 网络，用于高效模式识别，它有效地聚合了 MobileNet 和 Transformer 网络。具体来说，我们首先使用干网络将事件图像嵌入到特征表示中，然后将它们输入到不确定性感知的 Mobile-Former 块中，以进行局部和全局特征学习和融合。最后，将 MobileNet 和 Transformer 分支的特征连接起来以进行模式识别。对多个基于事件的识别数据集的广泛实验充分验证了我们模型的有效性。这项工作的源代码将在 https://github.com/Event-AHU/Uncertainty_aware_MobileFormer 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.11123v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Spatial Structure Constraints for Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** 弱监督语义分割的空间结构约束<br />
**Authors:** Tao Chen, Yazhou Yao, Xingguo Huang, Zechao Li, Liqiang Nie, Jinhui Tang<br />
**Abstract:** <details><summary>原文: </summary>The image-level label has prevailed in weakly supervised semantic segmentation tasks due to its easy availability. Since image-level labels can only indicate the existence or absence of specific categories of objects, visualization-based techniques have been widely adopted to provide object location clues. Considering class activation maps (CAMs) can only locate the most discriminative part of objects, recent approaches usually adopt an expansion strategy to enlarge the activation area for more integral object localization. However, without proper constraints, the expanded activation will easily intrude into the background region. In this paper, we propose spatial structure constraints (SSC) for weakly supervised semantic segmentation to alleviate the unwanted object over-activation of attention expansion. Specifically, we propose a CAM-driven reconstruction module to directly reconstruct the input image from deep CAM features, which constrains the diffusion of last-layer object attention by preserving the coarse spatial structure of the image content. Moreover, we propose an activation self-modulation module to refine CAMs with finer spatial structure details by enhancing regional consistency. Without external saliency models to provide background clues, our approach achieves 72.7\% and 47.0\% mIoU on the PASCAL VOC 2012 and COCO datasets, respectively, demonstrating the superiority of our proposed approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像级标签由于其易于使用而在弱监督语义分割任务中占据了主导地位。由于图像级标签只能指示特定类别的对象是否存在，因此基于可视化的技术已被广泛采用来提供对象位置线索。考虑到类激活图（CAM）只能定位对象中最具辨别力的部分，最近的方法通常采用扩展策略来扩大激活区域，以实现更完整的对象定位。然而，如果没有适当的约束，扩展的激活很容易侵入背景区域。在本文中，我们提出了弱监督语义分割的空间结构约束（SSC），以减轻注意力扩展中不需要的对象过度激活。具体来说，我们提出了一种 CAM 驱动的重建模块，可以直接从深层 CAM 特征重建输入图像，通过保留图像内容的粗糙空间结构来限制最后一层对象注意力的扩散。此外，我们提出了一种激活自调制模块，通过增强区域一致性来细化具有更精细空间结构细节的 CAM。在没有外部显着性模型提供背景线索的情况下，我们的方法在 PASCAL VOC 2012 和 COCO 数据集上分别实现了 72.7\% 和 47.0\% mIoU，证明了我们提出的方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11122v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE**<br />
**Title_cn:** VONet：利用并行 U-Net Attention 和对象顺序 VAE 进行无监督视频对象学习<br />
**Authors:** Haonan Yu, Wei Xu<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督视频对象学习旨在将视频场景分解为结构对象表示，而无需任何深度、光流或分割的监督。我们提出了 VONet，这是一种受 MONet 启发的创新方法。在利用 U-Net 架构的同时，VONet 采用高效且有效的并行注意力推理过程，同时为所有时隙生成注意力掩模。此外，为了增强连续视频帧中每个掩模的时间一致性，VONet 开发了一个对象方式的顺序 VAE 框架。这些创新的编码器端技术与基于 Transformer 的富有表现力的解码器相结合，使 VONet 成为跨五个 MOVI 数据集（涵盖不同复杂性的视频）进行对象学习的领先无监督方法。代码可在 https://github.com/hnyu/vonet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11110v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Adaptive Global-Local Representation Learning and Selection for Cross-Domain Facial Expression Recognition**<br />
**Title_cn:** 跨域面部表情识别的自适应全局局部表示学习和选择<br />
**Authors:** Yuefang Gao, Yuhao Xie, Zeke Zexi Hu, Tianshui Chen, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>Domain shift poses a significant challenge in Cross-Domain Facial Expression Recognition (CD-FER) due to the distribution variation across different domains. Current works mainly focus on learning domain-invariant features through global feature adaptation, while neglecting the transferability of local features. Additionally, these methods lack discriminative supervision during training on target datasets, resulting in deteriorated feature representation in target domain. To address these limitations, we propose an Adaptive Global-Local Representation Learning and Selection (AGLRLS) framework. The framework incorporates global-local adversarial adaptation and semantic-aware pseudo label generation to enhance the learning of domain-invariant and discriminative feature during training. Meanwhile, a global-local prediction consistency learning is introduced to improve classification results during inference. Specifically, the framework consists of separate global-local adversarial learning modules that learn domain-invariant global and local features independently. We also design a semantic-aware pseudo label generation module, which computes semantic labels based on global and local features. Moreover, a novel dynamic threshold strategy is employed to learn the optimal thresholds by leveraging independent prediction of global and local features, ensuring filtering out the unreliable pseudo labels while retaining reliable ones. These labels are utilized for model optimization through the adversarial learning process in an end-to-end manner. During inference, a global-local prediction consistency module is developed to automatically learn an optimal result from multiple predictions. We conduct comprehensive experiments and analysis based on a fair evaluation benchmark. The results demonstrate that the proposed framework outperforms the current competing methods by a substantial margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于不同域之间的分布变化，域转移对跨域面部表情识别（CD-FER）提出了重大挑战。目前的工作主要集中在通过全局特征适应来学习领域不变特征，而忽略了局部特征的可迁移性。此外，这些方法在目标数据集的训练过程中缺乏区分性监督，导致目标域中的特征表示恶化。为了解决这些限制，我们提出了自适应全局局部表示学习和选择（AGLRLS）框架。该框架结合了全局-局部对抗适应和语义感知伪标签生成，以增强训练过程中领域不变和判别性特征的学习。同时，引入全局-局部预测一致性学习来改善推理过程中的分类结果。具体来说，该框架由单独的全局局部对抗性学习模块组成，这些模块独立学习领域不变的全局和局部特征。我们还设计了一个语义感知的伪标签生成模块，它根据全局和局部特征计算语义标签。此外，采用一种新颖的动态阈值策略，通过利用全局和局部特征的独立预测来学习最佳阈值，确保过滤掉不可靠的伪标签，同时保留可靠的伪标签。这些标签用于通过端到端的对抗性学习过程进行模型优化。在推理过程中，开发了全局-局部预测一致性模块，以自动从多个预测中学习最佳结果。我们基于公平的评估基准进行全面的实验和分析。结果表明，所提出的框架大大优于当前的竞争方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11085v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures**<br />
**Title_cn:** UltrAvatar：具有真实性引导纹理的逼真的可动画 3D 头像扩散模型<br />
**Authors:** Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 头像生成的最新进展引起了广泛关注。这些突破旨在产生更逼真的动画化身，缩小虚拟和现实世界体验之间的差距。大多数现有作品都采用分数蒸馏采样 (SDS) 损失，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，SDS 经常生成过度平滑的结果，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他作品从单个图像生成 3D 头像，其中不需要的光照效果、透视图和较差的图像质量的挑战使得它们难以可靠地重建具有对齐的完整纹理的 3D 面部网格。在本文中，我们提出了一种称为 UltrAvatar 的新型 3D 头像生成方法，该方法具有增强的几何保真度和基于物理的渲染 (PBR) 纹理的卓越质量，并且没有不需要的照明。为此，所提出的方法提出了漫射颜色提取模型和真实性引导纹理扩散模型。前者消除了不需要的光照效果，以显示真实的漫反射颜色，以便生成的头像可以在各种光照条件下渲染。后者遵循两个基于梯度的指导来生成 PBR 纹理，以渲染不同的面部身份特征和细节，更好地与 3D 网格几何体对齐。我们证明了所提出方法的有效性和鲁棒性，在实验中大大优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11078v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient**<br />
**Title_cn:** 高斯混合模型和负高斯混合梯度的扩散模型条件<br />
**Authors:** Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model training with an additional classifier. Training stability has improved. We also theoretically prove that NGMG shares the same benefit as the Earth Mover distance (Wasserstein) as a more sensible cost function when learning distributions supported by low-dimensional manifolds.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型 (DM) 是一种生成模型，对图像合成及其他领域具有巨大影响。他们在各种生成任务中取得了最先进的生成结果。可以使用多种调节输入（例如文本或边界框）来控制生成。在这项工作中，我们提出了一种利用高斯混合模型（GMM）作为特征调节来指导去噪过程的调节机制。基于集合论，我们提供了全面的理论分析，表明基于特征和类别的条件潜在分布有显着不同，因此特征上的条件潜在分布比类别上的条件潜在分布产生的缺陷更少。分别训练两个以高斯混合模型为条件的扩散模型进行比较。实验支持我们的发现。提出了一种称为负高斯混合梯度（NGMG）的新型梯度函数，并将其应用于具有附加分类器的扩散模型训练。训练稳定性有所提高。我们还从理论上证明，在学习低维流形支持的分布时，NGMG 与 Earth Mover 距离 (Wasserstein) 具有相同的优势，作为更合理的成本函数。</details>
**PDF:** <http://arxiv.org/pdf/2401.11261v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Product-Level Try-on: Characteristics-preserving Try-on with Realistic Clothes Shading and Wrinkles**<br />
**Title_cn:** 产品级试穿：保留特征的试穿，真实的衣服阴影和皱纹<br />
**Authors:** Yanlong Zang, Han Yang, Jiaxu Miao, Yi Yang<br />
**Abstract:** <details><summary>原文: </summary>Image-based virtual try-on systems,which fit new garments onto human portraits,are gaining research attention.An ideal pipeline should preserve the static features of clothes(like textures and logos)while also generating dynamic elements(e.g.shadows,folds)that adapt to the model's pose and environment.Previous works fail specifically in generating dynamic features,as they preserve the warped in-shop clothes trivially with predicted an alpha mask by composition.To break the dilemma of over-preserving and textures losses,we propose a novel diffusion-based Product-level virtual try-on pipeline,\ie PLTON, which can preserve the fine details of logos and embroideries while producing realistic clothes shading and wrinkles.The main insights are in three folds:1)Adaptive Dynamic Rendering:We take a pre-trained diffusion model as a generative prior and tame it with image features,training a dynamic extractor from scratch to generate dynamic tokens that preserve high-fidelity semantic information. Due to the strong generative power of the diffusion prior,we can generate realistic clothes shadows and wrinkles.2)Static Characteristics Transformation: High-frequency Map(HF-Map)is our fundamental insight for static representation.PLTON first warps in-shop clothes to the target model pose by a traditional warping network,and uses a high-pass filter to extract an HF-Map for preserving static cloth features.The HF-Map is used to generate modulation maps through our static extractor,which are injected into a fixed U-net to synthesize the final result.To enhance retention,a Two-stage Blended Denoising method is proposed to guide the diffusion process for correct spatial layout and color.PLTON is finetuned only with our collected small-size try-on dataset.Extensive quantitative and qualitative experiments on 1024 768 datasets demonstrate the superiority of our framework in mimicking real clothes dynamics.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于图像的虚拟试穿系统正在引起研究关注，该系统可以将新服装适配到人体肖像上。理想的流程应该保留衣服的静态特征（如纹理和徽标），同时生成动态元素（如阴影、褶皱），适应模型的姿势和环境。以前的工作在生成动态特征方面尤其失败，因为它们通过合成预测的 alpha 掩模微不足道地保留了扭曲的店内衣服。为了打破过度保留和纹理损失的困境，我们提出了一种新颖的基于扩散的产品级虚拟试穿管道，即 PLTON，它可以保留徽标和刺绣的精细细节，同时产生逼真的衣服阴影和皱纹。主要见解分为三个方面：1）自适应动态渲染：我们采用预先训练的扩散模型作为生成先验，并用图像特征来驯服它，从头开始训练动态提取器以生成保留高保真语义信息的动态标记。由于扩散先验的强大生成能力，我们可以生成逼真的衣服阴影和皱纹。2）静态特征转换：高频图（HF-Map）是我们静态表示的基本见解。PLTON首先对店内衣​​服进行变形通过传统的扭曲网络对目标模型构成，并使用高通滤波器提取 HF-Map 以保留静态布料特征。HF-Map 用于通过我们的静态提取器生成调制图，并将其注入到固定U-net来合成最终结果。为了增强保留，提出了一种两阶段混合去噪方法来指导扩散过程以获得正确的空间布局和颜色。PLTON仅使用我们收集的小尺寸试戴数据集进行微调。对 1024 768 个数据集进行的广泛定量和定性实验证明了我们的框架在模仿真实衣服动态方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11239v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation**<br />
**Title_cn:** MotionMix：用于可控运动生成的弱监督扩散<br />
**Authors:** Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi<br />
**Abstract:** <details><summary>原文: </summary>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^*$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^*$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着世界拥抱数字化转型，可控生成 3D 人体动作成为一个重要课题。现有的工作虽然随着扩散模型的出现取得了有希望的进展，但在很大程度上依赖于精心捕获和注释（例如文本）的高质量运动语料库，这是现实世界中资源密集型的工作。这激发了我们提出的 MotionMix，这是一种简单而有效的弱监督扩散模型，它利用了噪声和未注释的运动序列。具体来说，我们将扩散模型的去噪目标分为两个阶段：通过学习噪声注释运动，在初始 $T-T^*$ 步骤中获得条件粗略运动近似，然后在最后 $T 期间对这些初步运动进行无条件细化^*$ 使用未注释的动作进行步骤。值得注意的是，尽管从两个不完美数据源学习，但与访问黄金数据的完全监督方法相比，我们的模型并没有损害运动生成质量。对多个基准的大量实验表明，我们的 MotionMix 作为一个多功能框架，在文本到动作、动作到动作和音乐到舞蹈任务上始终如一地实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11115v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Make-A-Shape: a Ten-Million-scale 3D Shape Model**<br />
**Title_cn:** Make-A-Shape：千万级 3D 形状模型<br />
**Authors:** Ka-Hei Hui, Aditya Sanghi, Arianna Rampini, Kamal Rahimi Malekshan, Zhengzhe Liu, Hooman Shayani, Chi-Wing Fu<br />
**Abstract:** <details><summary>原文: </summary>Significant progress has been made in training large generative models for natural language and images. Yet, the advancement of 3D generative models is hindered by their substantial resource demands for training, along with inefficient, non-compact, and less expressive representations. This paper introduces Make-A-Shape, a new 3D generative model designed for efficient training on a vast scale, capable of utilizing 10 millions publicly-available shapes. Technical-wise, we first innovate a wavelet-tree representation to compactly encode shapes by formulating the subband coefficient filtering scheme to efficiently exploit coefficient relations. We then make the representation generatable by a diffusion model by devising the subband coefficients packing scheme to layout the representation in a low-resolution grid. Further, we derive the subband adaptive training strategy to train our model to effectively learn to generate coarse and detail wavelet coefficients. Last, we extend our framework to be controlled by additional input conditions to enable it to generate shapes from assorted modalities, e.g., single/multi-view images, point clouds, and low-resolution voxels. In our extensive set of experiments, we demonstrate various applications, such as unconditional generation, shape completion, and conditional generation on a wide range of modalities. Our approach not only surpasses the state of the art in delivering high-quality results but also efficiently generates shapes within a few seconds, often achieving this in just 2 seconds for most conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>在训练自然语言和图像的大型生成模型方面取得了重大进展。然而，3D 生成模型的进步受到大量训练资源需求以及低效、不紧凑和表达能力较差的阻碍。本文介绍了 Make-A-Shape，这是一种新的 3D 生成模型，专为大规模高效训练而设计，能够利用 1000 万个公开可用的形状。在技​​术方面，我们首先创新小波树表示，通过制定子带系数滤波方案来有效地利用系数关系来对形状进行紧凑编码。然后，我们通过设计子带系数打包方案以在低分辨率网格中布局表示，从而使表示可由扩散模型生成。此外，我们推导了子带自适应训练策略来训练我们的模型，以有效地学习生成粗略和细节小波系数。最后，我们扩展了我们的框架，使其能够通过额外的输入条件进行控制，使其能够从各种模式生成形状，例如单/多视图图像、点云和低分辨率体素。在我们广泛的实验中，我们演示了各种应用，例如各种模式上的无条件生成、形状完成和条件生成。我们的方法不仅在提供高质量结果方面超越了最先进的技术，而且还能在几秒钟内有效地生成形状，在大多数情况下通常只需 2 秒即可实现这一目标。</details>
**PDF:** <http://arxiv.org/pdf/2401.11067v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Prompting Large Vision-Language Models for Compositional Reasoning**<br />
**Title_cn:** 促进大型视觉语言模型进行组合推理<br />
**Authors:** Timothy Ossowski, Ming Jiang, Junjie Hu<br />
**Abstract:** <details><summary>原文: </summary>Vision-language models such as CLIP have shown impressive capabilities in encoding texts and images into aligned embeddings, enabling the retrieval of multimodal data in a shared embedding space. However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset. In this paper, we argue that this limitation stems from two factors: the use of single vector representations for complex multimodal data, and the absence of step-by-step reasoning in these embedding-based methods. To address this issue, we make an exploratory step using a novel generative method that prompts large vision-language models (e.g., GPT-4) to depict images and perform compositional reasoning. Our method outperforms other embedding-based methods on the Winoground dataset, and obtains further improvement of up to 10% accuracy when enhanced with the optimal description.</details>
**Abstract_cn:** <details><summary>译文: </summary>CLIP 等视觉语言模型在将文本和图像编码为对齐的嵌入方面表现出了令人印象深刻的能力，从而能够在共享嵌入空间中检索多模态数据。然而，这些基于嵌入的模型在有效匹配具有相似视觉语言组合性的图像和文本方面仍然面临挑战，正如它们在最近的 Winoground 数据集上的表现所证明的那样。在本文中，我们认为这种限制源于两个因素：对复杂的多模态数据使用单向量表示，以及这些基于嵌入的方法中缺乏逐步推理。为了解决这个问题，我们采取了探索性的步骤，使用一种新颖的生成方法，提示大型视觉语言模型（例如 GPT-4）来描述图像并执行组合推理。我们的方法在 Winoground 数据集上优于其他基于嵌入的方法，并且在使用最佳描述进行增强时，准确率进一步提高了 10%。</details>
**PDF:** <http://arxiv.org/pdf/2401.11337v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images**<br />
**Title_cn:** 使用详细图像诱导大型视觉语言模型的高能量延迟<br />
**Authors:** Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu<br />
**Abstract:** <details><summary>原文: </summary>Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.</details>
**Abstract_cn:** <details><summary>译文: </summary>GPT-4 等大型视觉语言模型 (VLM) 在各种多模式任务中取得了卓越的性能。然而，VLM 的部署需要大量的能源消耗和计算资源。一旦攻击者在VLM的推理过程中恶意引发高能耗和延迟时间（energy-latency cost），就会耗尽计算资源。在本文中，我们探讨了有关 VLM 可用性的攻击面，旨在在 VLM 推理过程中引发高能量延迟成本。我们发现，VLM 推理过程中的高能量延迟成本可以通过最大化生成序列的长度来控制。为此，我们提出了详细图像，目的是制作一种难以察觉的扰动，以诱导 VLM 在推理过程中生成长句子。具体来说，我们设计了三个损失目标。首先，提出损失来延迟序列结束（EOS）代币的出现，其中EOS代币是VLM停止生成更多代币的信号。此外，提出了不确定性损失和令牌多样性损失，以分别增加每个生成的令牌的不确定性和整个生成序列的所有令牌之间的多样性，这可以打破令牌级和序列级的输出依赖性。此外，提出了一种时间权重调整算法，可以有效平衡这些损失。大量实验表明，与 MS-COCO 和 ImageNet 数据集上的原始图像相比，我们的详细图像可以将生成序列的长度增加 7.87 倍和 8.56 倍，这给各种应用带来了潜在的挑战。我们的代码可在 https://github.com/KuofengGao/Verbose_Images 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11170v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for Resource-Limited Countries**<br />
**Title_cn:** DengueNet：利用时空卫星图像对资源有限的国家进行登革热预测<br />
**Authors:** Kuan-Ting Kuo, Dana Moukheiber, Sebastian Cajas Ordonez, David Restrepo, Atika Rahman Paddo, Tsung-Yu Chen, Lama Moukheiber, Mira Moukheiber, Sulaiman Moukheiber, Saptarshi Purkayastha, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Dengue fever presents a substantial challenge in developing countries where sanitation infrastructure is inadequate. The absence of comprehensive healthcare systems exacerbates the severity of dengue infections, potentially leading to life-threatening circumstances. Rapid response to dengue outbreaks is also challenging due to limited information exchange and integration. While timely dengue outbreak forecasts have the potential to prevent such outbreaks, the majority of dengue prediction studies have predominantly relied on data that impose significant burdens on individual countries for collection. In this study, our aim is to improve health equity in resource-constrained countries by exploring the effectiveness of high-resolution satellite imagery as a nontraditional and readily accessible data source. By leveraging the wealth of publicly available and easily obtainable satellite imagery, we present a scalable satellite extraction framework based on Sentinel Hub, a cloud-based computing platform. Furthermore, we introduce DengueNet, an innovative architecture that combines Vision Transformer, Radiomics, and Long Short-term Memory to extract and integrate spatiotemporal features from satellite images. This enables dengue predictions on an epi-week basis. To evaluate the effectiveness of our proposed method, we conducted experiments on five municipalities in Colombia. We utilized a dataset comprising 780 high-resolution Sentinel-2 satellite images for training and evaluation. The performance of DengueNet was assessed using the mean absolute error (MAE) metric. Across the five municipalities, DengueNet achieved an average MAE of 43.92. Our findings strongly support the efficacy of satellite imagery as a valuable resource for dengue prediction, particularly in informing public health policies within countries where manually collected data is scarce and dengue virus prevalence is severe.</details>
**Abstract_cn:** <details><summary>译文: </summary>登革热对卫生基础设施不足的发展中国家构成了重大挑战。缺乏全面的医疗保健系统会加剧登革热感染的严重性，可能导致危及生命的情况。由于信息交换和整合有限，对登革热疫情的快速反应也具有挑战性。虽然及时的登革热疫情预测有可能预防此类疫情的爆发，但大多数登革热预测研究主要依赖于给个别国家带来沉重收集负担的数据。在这项研究中，我们的目标是通过探索高分辨率卫星图像作为非传统且易于访问的数据源的有效性，改善资源有限国家的健康公平。通过利用大量公开且易于获取的卫星图像，我们提出了一个基于 Sentinel Hub（一个基于云的计算平台）的可扩展卫星提取框架。此外，我们还介绍了 DengueNet，这是一种创新架构，结合了 Vision Transformer、Radiomics 和长短期记忆，可从卫星图像中提取和集成时空特征。这使得登革热周的预测成为可能。为了评估我们提出的方法的有效性，我们在哥伦比亚的五个城市进行了实验。我们利用包含 780 个高分辨率 Sentinel-2 卫星图像的数据集进行训练和评估。 DengueNet 的性能使用平均绝对误差 (MAE) 指标进行评估。在五个城市中，DengueNet 的平均 MAE 为 43.92。我们的研究结果强烈支持卫星图像作为登革热预测的宝贵资源的有效性，特别是在为手动收集的数据稀缺且登革热病毒流行严重的国家/地区的公共卫生政策提供信息方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.11114v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Evaluating Driver Readiness in Conditionally Automated Vehicles from Eye-Tracking Data and Head Pose**<br />
**Title_cn:** 根据眼动追踪数据和头部姿势评估有条件自动驾驶车辆的驾驶员准备情况<br />
**Authors:** Mostafa Kazemi, Mahdi Rezaei, Mohsen Azarmi<br />
**Abstract:** <details><summary>原文: </summary>As automated driving technology advances, the role of the driver to resume control of the vehicle in conditionally automated vehicles becomes increasingly critical. In the SAE Level 3 or partly automated vehicles, the driver needs to be available and ready to intervene when necessary. This makes it essential to evaluate their readiness accurately. This article presents a comprehensive analysis of driver readiness assessment by combining head pose features and eye-tracking data. The study explores the effectiveness of predictive models in evaluating driver readiness, addressing the challenges of dataset limitations and limited ground truth labels. Machine learning techniques, including LSTM architectures, are utilised to model driver readiness based on the Spatio-temporal status of the driver's head pose and eye gaze. The experiments in this article revealed that a Bidirectional LSTM architecture, combining both feature sets, achieves a mean absolute error of 0.363 on the DMD dataset, demonstrating superior performance in assessing driver readiness. The modular architecture of the proposed model also allows the integration of additional driver-specific features, such as steering wheel activity, enhancing its adaptability and real-world applicability.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着自动驾驶技术的进步，驾驶员在有条件自动驾驶车辆中恢复车辆控制的作用变得越来越重要。在 SAE 3 级或半自动驾驶车辆中，驾驶员需要随时准备好在必要时进行干预。这使得准确评估他们的准备情况至关重要。本文结合头部姿势特征和眼动追踪数据，对驾驶员准备状态评估进行了全面分析。该研究探讨了预测模型在评估驾驶员准备情况、解决数据集限制和有限真实标签挑战方面的有效性。包括 LSTM 架构在内的机器学习技术用于根据驾驶员头部姿势和眼睛注视的时空状态对驾驶员准备情况进行建模。本文中的实验表明，结合两个功能集的双向 LSTM 架构在 DMD 数据集上实现了 0.363 的平均绝对误差，展示了在评估驾驶员准备情况方面的卓越性能。该模型的模块化架构还允许集成额外的驾驶员特定功能，例如方向盘活动，从而增强其适应性和实际适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11284v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT**<br />
**Title_cn:** 锥束 CT 的等变多尺度学习可逆重建<br />
**Authors:** Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen<br />
**Abstract:** <details><summary>原文: </summary>Cone Beam CT (CBCT) is an essential imaging modality nowadays, but the image quality of CBCT still lags behind the high quality standards established by the conventional Computed Tomography. We propose LIRE+, a learned iterative scheme for fast and memory-efficient CBCT reconstruction, which is a substantially faster and more parameter-efficient alternative to the recently proposed LIRE method. LIRE+ is a rotationally-equivariant multiscale learned invertible primal-dual iterative scheme for CBCT reconstruction. Memory usage is optimized by relying on simple reversible residual networks in primal/dual cells and patch-wise computations inside the cells during forward and backward passes, while increased inference speed is achieved by making the primal-dual scheme multiscale so that the reconstruction process starts at low resolution and with low resolution primal/dual latent vectors. A LIRE+ model was trained and validated on a set of 260 + 22 thorax CT scans and tested using a set of 142 thorax CT scans with additional evaluation with and without finetuning on an out-of-distribution set of 79 Head and Neck (HN) CT scans. Our method surpasses classical and deep learning baselines, including LIRE, on the thorax test set. For a similar inference time and with only 37 % of the parameter budget, LIRE+ achieves a +0.2 dB PSNR improvement over LIRE, while being able to match the performance of LIRE in 45 % less inference time and with 28 % of the parameter budget. Rotational equivariance ensures robustness of LIRE+ to patient orientation, while LIRE and other deep learning baselines suffer from substantial performance degradation when patient orientation is unusual. On the HN dataset in the absence of finetuning, LIRE+ is generally comparable to LIRE in performance apart from a few outlier cases, whereas after identical finetuning LIRE+ demonstates a +1.02 dB PSNR improvement over LIRE.</details>
**Abstract_cn:** <details><summary>译文: </summary>锥形束CT（CBCT）是当今重要的成像方式，但CBCT的图像质量仍然落后于传统计算机断层扫描建立的高质量标准。我们提出了 LIRE+，这是一种用于快速且内存高效的 CBCT 重建的学习迭代方案，它是最近提出的 LIRE 方法的更快且参数效率更高的替代方案。 LIRE+ 是一种用于 CBCT 重建的旋转等变多尺度学习可逆原始对偶迭代方案。通过依赖原/对偶单元中的简单可逆残差网络以及前向和后向传递过程中单元内的分片计算来优化内存使用，同时通过使原-对偶方案多尺度来实现推理速度的提高，以便重建过程开始在低分辨率和低分辨率原始/双潜在向量。 LIRE+ 模型在一组 260 + 22 个胸部 CT 扫描上进行了训练和验证，并使用一组 142 个胸部 CT 扫描进行了测试，并对 79 个头颈 (HN) 的分布外组进行了微调和微调CT 扫描。我们的方法在胸部测试集上超越了经典和深度学习基线，包括 LIRE。对于相似的推理时间和仅 37% 的参数预算，LIRE+ 比 LIRE 实现了 +0.2 dB PSNR 改进，同时能够以 45% 的推理时间和 28% 的参数预算与 LIRE 的性能相匹配。旋转等方差确保了 LIRE+ 对患者方向的鲁棒性，而当患者方向异常时，LIRE 和其他深度学习基线的性能会大幅下降。在没有微调的 HN 数据集上，除了少数异常情况外，LIRE+ 的性能通常与 LIRE 相当，而经过相同的微调后，LIRE+ 的 PSNR 比 LIRE 提高了 +1.02 dB。</details>
**PDF:** <http://arxiv.org/pdf/2401.11256v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions**<br />
**Title_cn:** EMA-Net：用于密集场景预测的高效多任务亲和学习<br />
**Authors:** Dimitrios Sinodinos, Narges Armanfard<br />
**Abstract:** <details><summary>原文: </summary>Multitask learning (MTL) has gained prominence for its ability to jointly predict multiple tasks, achieving better per-task performance while using fewer per-task model parameters than single-task learning. More recently, decoder-focused architectures have considerably improved multitask performance by refining task predictions using the features of other related tasks. However, most of these refinement methods fail to simultaneously capture local and global task-specific representations, as well as cross-task patterns in a parameter-efficient manner. In this paper, we introduce the Efficient Multitask Affinity Learning Network (EMA-Net), which is a lightweight framework that enhances the task refinement capabilities of multitask networks. EMA-Net adeptly captures local, global, and cross-task interactions using our novel Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in its ability to manipulate task affinity matrices in a manner that is optimally suited to apply parameter-efficient grouped convolutions without worrying about information loss. Our results show that we achieve state-of-the-art MTL performance for CNN-based decoder-focused models while using substantially fewer model parameters. Our code is publicly available at https://github.com/Armanfard-Lab/EMA-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>多任务学习（MTL）因其联合预测多个任务的能力而受到关注，与单任务学习相比，在使用更少的每任务模型参数的同时实现更好的每任务性能。最近，以解码器为中心的架构通过使用其他相关任务的特征来细化任务预测，显着提高了多任务性能。然而，大多数细化方法无法以参数有效的方式同时捕获局部和全局特定于任务的表示以及跨任务模式。在本文中，我们介绍了高效多任务亲和学习网络（EMA-Net），它是一个轻量级框架，可以增强多任务网络的任务细化能力。 EMA-Net 使用我们新颖的跨任务亲和学习 (CTAL) 模块熟练地捕获本地、全局和跨任务交互。 CTAL 的关键创新在于它能够以最适合应用参数高效分组卷积的方式操纵任务亲和力矩阵，而无需担心信息丢失。我们的结果表明，我们在使用更少的模型参数的情况下，为基于 CNN 的以解码器为中心的模型实现了最先进的 MTL 性能。我们的代码可在 https://github.com/Armanfard-Lab/EMA-Net 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11124v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations**<br />
**Title_cn:** HOSC：一种用于保留隐式神经表示中的尖锐特征的周期性激活函数<br />
**Authors:** Danzel Serrano, Jakub Szymkowiak, Przemyslaw Musialski<br />
**Abstract:** <details><summary>原文: </summary>Recently proposed methods for implicitly representing signals such as images, scenes, or geometries using coordinate-based neural network architectures often do not leverage the choice of activation functions, or do so only to a limited extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a novel activation function with a controllable sharpness parameter. Unlike any previous activations, HOSC has been specifically designed to better capture sudden changes in the input signal, and hence sharp or acute features of the underlying data, as well as smooth low-frequency transitions. Due to its simplicity and modularity, HOSC offers a plug-and-play functionality that can be easily incorporated into any existing method employing a neural network as a way of implicitly representing a signal. We benchmark HOSC against other popular activations in an array of general tasks, empirically showing an improvement in the quality of obtained representations, provide the mathematical motivation behind the efficacy of HOSC, and discuss its limitations.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近提出的使用基于坐标的神经网络架构隐式表示信号（例如图像、场景或几何形状）的方法通常不利用激活函数的选择，或者仅在有限的程度上利用激活函数的选择。在本文中，我们介绍了双曲振荡函数（HOSC），这是一种具有可控锐度参数的新型激活函数。与之前的任何激活不同，HOSC 经过专门设计，可以更好地捕获输入信号的突然变化，从而捕获基础数据的尖锐或尖锐特征，以及平滑的低频转换。由于其简单性和模块化，HOSC 提供了即插即用功能，可以轻松地合并到任何采用神经网络作为隐式表示信号的方式的现有方法中。我们在一系列一般任务中将 HOSC 与其他流行的激活进行基准测试，凭经验显示所获得的表示质量的提高，提供 HOSC 功效背后的数学动机，并讨论其局限性。</details>
**PDF:** <http://arxiv.org/pdf/2401.10967v1><br />
**Code:** null<br />

