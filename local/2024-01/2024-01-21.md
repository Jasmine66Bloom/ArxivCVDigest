## [UPDATED!] **2024-01-21** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **A Survey on African Computer Vision Datasets, Topics and Researchers**<br />
**Title_cn:** 非洲计算机视觉数据集、主题和研究人员调查<br />
**Authors:** Abdul-Hakeem Omotayo, Ashery Mbilinyi, Lukman Ismaila, Houcemeddine Turki, Mahmoud Abdien, Karim Gamal, Idriss Tondji, Yvan Pimi, Naome A. Etori, Marwa M. Matar, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Computer vision encompasses a range of tasks such as object detection, semantic segmentation, and 3D reconstruction. Despite its relevance to African communities, research in this field within Africa represents only 0.06% of top-tier publications over the past decade. This study undertakes a thorough analysis of 63,000 Scopus-indexed computer vision publications from Africa, spanning from 2012 to 2022. The aim is to provide a survey of African computer vision topics, datasets and researchers. A key aspect of our study is the identification and categorization of African Computer Vision datasets using large language models that automatically parse abstracts of these publications. We also provide a compilation of unofficial African Computer Vision datasets distributed through challenges or data hosting platforms, and provide a full taxonomy of dataset categories. Our survey also pinpoints computer vision topics trends specific to different African regions, indicating their unique focus areas. Additionally, we carried out an extensive survey to capture the views of African researchers on the current state of computer vision research in the continent and the structural barriers they believe need urgent attention. In conclusion, this study catalogs and categorizes Computer Vision datasets and topics contributed or initiated by African institutions and identifies barriers to publishing in top-tier Computer Vision venues. This survey underscores the importance of encouraging African researchers and institutions in advancing computer vision research in the continent. It also stresses on the need for research topics to be more aligned with the needs of African communities.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉涵盖一系列任务，例如对象检测、语义分割和 3D 重建。尽管与非洲社区相关，但过去十年非洲境内该领域的研究仅占顶级出版物的 0.06%。本研究对 2012 年至 2022 年期间非洲 63,000 份 Scopus 索引的计算机视觉出版物进行了全面分析。目的是对非洲计算机视觉主题、数据集和研究人员进行调查。我们研究的一个关键方面是使用自动解析这些出版物摘要的大型语言模型对非洲计算机视觉数据集进行识别和分类。我们还提供通过挑战或数据托管平台分发的非官方非洲计算机视觉数据集的汇编，并提供数据集类别的完整分类。我们的调查还指出了非洲不同地区特有的计算机视觉主题趋势，表明了它们独特的重点领域。此外，我们还进行了广泛的调查，以了解非洲研究人员对该大陆计算机视觉研究现状以及他们认为需要紧急关注的结构性障碍的看法。总之，本研究对非洲机构贡献或发起的计算机视觉数据集和主题进行了编目和分类，并确定了在顶级计算机视觉场所发表的障碍。这项调查强调了鼓励非洲研究人员和机构推进非洲大陆计算机视觉研究的重要性。它还强调研究主题需要更加符合非洲社区的需求。</details>
**PDF:** <http://arxiv.org/pdf/2401.11617v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **TetraLoss: Improving the Robustness of Face Recognition against Morphing Attacks**<br />
**Title_cn:** TetraLoss：提高人脸识别抵御变形攻击的鲁棒性<br />
**Authors:** Mathias Ibsen, Lázaro J. González-Soler, Christian Rathgeb, Christoph Busch<br />
**Abstract:** <details><summary>原文: </summary>Face recognition systems are widely deployed in high-security applications such as for biometric verification at border controls. Despite their high accuracy on pristine data, it is well-known that digital manipulations, such as face morphing, pose a security threat to face recognition systems. Malicious actors can exploit the facilities offered by the identity document issuance process to obtain identity documents containing morphed images. Thus, subjects who contributed to the creation of the morphed image can with high probability use the identity document to bypass automated face recognition systems. In recent years, no-reference (i.e., single image) and differential morphing attack detectors have been proposed to tackle this risk. These systems are typically evaluated in isolation from the face recognition system that they have to operate jointly with and do not consider the face recognition process. Contrary to most existing works, we present a novel method for adapting deep learning-based face recognition systems to be more robust against face morphing attacks. To this end, we introduce TetraLoss, a novel loss function that learns to separate morphed face images from its contributing subjects in the embedding space while still preserving high biometric verification performance. In a comprehensive evaluation, we show that the proposed method can significantly enhance the original system while also significantly outperforming other tested baseline methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸识别系统广泛部署在高安全性应用中，例如边境管制的生物识别验证。尽管原始数据的准确性很高，但众所周知，面部变形等数字操作会对面部识别系统构成安全威胁。恶意行为者可以利用身份证件颁发过程提供的设施来获取包含变形图像的身份证件。因此，对变形图像的创建做出贡献的主体很可能使用身份证件来绕过自动面部识别系统。近年来，人们提出了无参考（即单图像）和差分变形攻击检测器来解决这种风险。这些系统通常独立于人脸识别系统进行评估，它们必须与人脸识别系统联合运行，并且不考虑人脸识别过程。与大多数现有工作相反，我们提出了一种新颖的方法，可以使基于深度学习的人脸识别系统更加鲁棒地抵御人脸变形攻击。为此，我们引入了 TetraLoss，一种新颖的损失函数，它学习将变形的人脸图像与其在嵌入空间中的贡献主体分开，同时仍然保持较高的生物特征验证性能。在综合评估中，我们表明所提出的方法可以显着增强原始系统，同时也显着优于其他测试的基线方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11598v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Thermal Image Calibration and Correction using Unpaired Cycle-Consistent Adversarial Networks**<br />
**Title_cn:** 使用不成对的循环一致对抗网络进行热图像校准和校正<br />
**Authors:** Hossein Rajoli, Pouya Afshin, Fatemeh Afghah<br />
**Abstract:** <details><summary>原文: </summary>Unmanned aerial vehicles (UAVs) offer a flexible and cost-effective solution for wildfire monitoring. However, their widespread deployment during wildfires has been hindered by a lack of operational guidelines and concerns about potential interference with aircraft systems. Consequently, the progress in developing deep-learning models for wildfire detection and characterization using aerial images is constrained by the limited availability, size, and quality of existing datasets. This paper introduces a solution aimed at enhancing the quality of current aerial wildfire datasets to align with advancements in camera technology. The proposed approach offers a solution to create a comprehensive, standardized large-scale image dataset. This paper presents a pipeline based on CycleGAN to enhance wildfire datasets and a novel fusion method that integrates paired RGB images as attribute conditioning in the generators of both directions, improving the accuracy of the generated images.</details>
**Abstract_cn:** <details><summary>译文: </summary>无人机 (UAV) 为野火监测提供了灵活且经济高效的解决方案。然而，由于缺乏操作指南以及对飞机系统潜在干扰的担忧，它们在野火期间的广泛部署受到阻碍。因此，利用航空图像开发用于野火检测和表征的深度学习模型的进展受到现有数据集有限的可用性、大小和质量的限制。本文介绍了一种解决方案，旨在提高当前空中野火数据集的质量，以适应​​相机技术的进步。所提出的方法提供了创建全面、标准化的大规模图像数据集的解决方案。本文提出了一种基于 CycleGAN 的管道来增强野火数据集，以及一种新颖的融合方法，该方法将成对的 RGB 图像集成为双向生成器中的属性条件，从而提高了生成图像的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11582v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CaBuAr: California Burned Areas dataset for delineation**<br />
**Title_cn:** CaBuAr：用于描绘的加州燃烧区域数据集<br />
**Authors:** Daniele Rege Cambrin, Luca Colomba, Paolo Garza<br />
**Abstract:** <details><summary>原文: </summary>Forest wildfires represent one of the catastrophic events that, over the last decades, caused huge environmental and humanitarian damages. In addition to a significant amount of carbon dioxide emission, they are a source of risk to society in both short-term (e.g., temporary city evacuation due to fire) and long-term (e.g., higher risks of landslides) cases. Consequently, the availability of tools to support local authorities in automatically identifying burned areas plays an important role in the continuous monitoring requirement to alleviate the aftereffects of such catastrophic events. The great availability of satellite acquisitions coupled with computer vision techniques represents an important step in developing such tools. This paper introduces a novel open dataset that tackles the burned area delineation problem, a binary segmentation problem applied to satellite imagery. The presented resource consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest fires that took place starting in 2015. Raster annotations were generated from the data released by California's Department of Forestry and Fire Protection. Moreover, in conjunction with the dataset, we release three different baselines based on spectral indexes analyses, SegFormer, and U-Net models.</details>
**Abstract_cn:** <details><summary>译文: </summary>森林野火是过去几十年来造成巨大环境和人道主义损失的灾难性事件之一。除了大量二氧化碳排放外，它们还是短期（例如，因火灾而临时进行城市疏散）和长期（例如，山体滑坡风险较高）的社会风险来源。因此，支持地方当局自动识别烧毁区域的工具的可用性在持续监测需求中发挥着重要作用，以减轻此类灾难性事件的后果。卫星采集与计算机视觉技术的广泛可用性代表着开发此类工具的重要一步。本文介绍了一种新颖的开放数据集，用于解决烧毁区域划分问题，这是一种应用于卫星图像的二元分割问题。所提供的资源包括 Sentinel-2 L2A 对 2015 年开始发生的加利福尼亚州森林火灾的火灾前和火灾后数据。栅格注释是根据加利福尼亚州林业和消防部门发布的数据生成的。此外，结合数据集，我们基于光谱指数分析、SegFormer 和 U-Net 模型发布了三个不同的基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.11519v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Edge-Enabled Real-time Railway Track Segmentation**<br />
**Title_cn:** 边缘支持的实时铁路轨道分割<br />
**Authors:** Chen Chenglin, Wang Fei, Yang Min, Qin Yong, Bai Yun<br />
**Abstract:** <details><summary>原文: </summary>Accurate and rapid railway track segmentation can assist automatic train driving and is a key step in early warning to fixed or moving obstacles on the railway track. However, certain existing algorithms tailored for track segmentation often struggle to meet the requirements of real-time and efficiency on resource-constrained edge devices. Considering this challenge, we propose an edge-enabled real-time railway track segmentation algorithm, which is optimized to be suitable for edge applications by optimizing the network structure and quantizing the model after training. Initially, Ghost convolution is introduced to reduce the complexity of the backbone, thereby achieving the extraction of key information of the interested region at a lower cost. To further reduce the model complexity and calculation, a new lightweight detection head is proposed to achieve the best balance between accuracy and efficiency. Subsequently, we introduce quantization techniques to map the model's floating-point weights and activation values into lower bit-width fixed-point representations, reducing computational demands and memory footprint, ultimately accelerating the model's inference. Finally, we draw inspiration from GPU parallel programming principles to expedite the pre-processing and post-processing stages of the algorithm by doing parallel processing. The approach is evaluated with public and challenging dataset RailSem19 and tested on Jetson Nano. Experimental results demonstrate that our enhanced algorithm achieves an accuracy level of 83.3% while achieving a real-time inference rate of 25 frames per second when the input size is 480x480, thereby effectively meeting the requirements for real-time and high-efficiency operation.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确快速的铁路轨道分割可以辅助列车自动驾驶，是对铁路轨道上的固定或移动障碍物进行预警的关键步骤。然而，现有的某些针对轨道分割量身定制的算法往往难以满足资源受限的边缘设备的实时性和效率要求。考虑到这一挑战，我们提出了一种基于边缘的实时铁路轨道分割算法，通过优化网络结构和训练后量化模型来优化该算法以适合边缘应用。最初引入Ghost卷积是为了降低backbone的复杂度，从而以较低的成本实现感兴趣区域关键信息的提取。为了进一步降低模型复杂度和计算量，提出了一种新的轻量级检测头，以实现精度和效率之间的最佳平衡。随后，我们引入量化技术，将模型的浮点权重和激活值映射为较低位宽的定点表示，减少计算需求和内存占用，最终加速模型​​的推理。最后，我们从GPU并行编程原理中汲取灵感，通过并行处理来加快算法的预处理和后处理阶段。该方法使用公共且具有挑战性的数据集 RailSem19 进行评估，并在 Jetson Nano 上进行测试。实验结果表明，我们的增强算法在输入尺寸为480x480时，准确率达到83.3%，同时实现了每秒25帧的实时推理速率，有效满足了实时高效运行的要求。</details>
**PDF:** <http://arxiv.org/pdf/2401.11492v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network**<br />
**Title_cn:** MapChange：基于深度三元组网络的时间不变历史地图增强语义变化检测<br />
**Authors:** Yinhe Liu, Sunan Shi, Zhuo Zheng, Jue Wang, Shiqi Tian, Yanfei Zhong<br />
**Abstract:** <details><summary>原文: </summary>Semantic Change Detection (SCD) is recognized as both a crucial and challenging task in the field of image analysis. Traditional methods for SCD have predominantly relied on the comparison of image pairs. However, this approach is significantly hindered by substantial imaging differences, which arise due to variations in shooting times, atmospheric conditions, and angles. Such discrepancies lead to two primary issues: the under-detection of minor yet significant changes, and the generation of false alarms due to temporal variances. These factors often result in unchanged objects appearing markedly different in multi-temporal images. In response to these challenges, the MapChange framework has been developed. This framework introduces a novel paradigm that synergizes temporal-invariant historical map data with contemporary high-resolution images. By employing this combination, the temporal variance inherent in conventional image pair comparisons is effectively mitigated. The efficacy of the MapChange framework has been empirically validated through comprehensive testing on two public datasets. These tests have demonstrated the framework's marked superiority over existing state-of-the-art SCD methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义变化检测（SCD）被认为是图像分析领域中一项至关重要且具有挑战性的任务。传统的 SCD 方法主要依赖于图像对的比较。然而，这种方法受到由于拍摄时间、大气条件和角度的变化而产生的显着成像差异的严重阻碍。这种差异导致两个主要问题：对微小但重大变化的检测不足，以及由于时间差异而产生误报。这些因素通常会导致未改变的对象在多时相图像中显得明显不同。为了应对这些挑战，MapChange 框架应运而生。该框架引入了一种新颖的范式，将时间不变的历史地图数据与当代高分辨率图像相结合。通过采用这种组合，有效地减轻了传统图像对比较中固有的时间方差。 MapChange 框架的功效已经通过对两个公共数据集的全面测试得到了实证验证。这些测试证明了该框架相对于现有最先进的 SCD 方法具有显着的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11489v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Exploring Missing Modality in Multimodal Egocentric Datasets**<br />
**Title_cn:** 探索多模态自我中心数据集中缺失的模态<br />
**Authors:** Merey Ramazanova, Alejandro Pardo, Humam Alwassel, Bernard Ghanem<br />
**Abstract:** <details><summary>原文: </summary>Multimodal video understanding is crucial for analyzing egocentric videos, where integrating multiple sensory signals significantly enhances action recognition and moment localization. However, practical applications often grapple with incomplete modalities due to factors like privacy concerns, efficiency demands, or hardware malfunctions. Addressing this, our study delves into the impact of missing modalities on egocentric action recognition, particularly within transformer-based models. We introduce a novel concept -Missing Modality Token (MMT)-to maintain performance even when modalities are absent, a strategy that proves effective in the Ego4D, Epic-Kitchens, and Epic-Sounds datasets. Our method mitigates the performance loss, reducing it from its original $\sim 30\%$ drop to only $\sim 10\%$ when half of the test set is modal-incomplete. Through extensive experimentation, we demonstrate the adaptability of MMT to different training scenarios and its superiority in handling missing modalities compared to current methods. Our research contributes a comprehensive analysis and an innovative approach, opening avenues for more resilient multimodal systems in real-world settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态视频理解对于分析以自我为中心的视频至关重要，其中集成多个感官信号可以显着增强动作识别和时刻定位。然而，由于隐私问题、效率要求或硬件故障等因素，实际应用经常会遇到不完整的模式。为了解决这个问题，我们的研究深入研究了缺失模式对以自我为中心的动作识别的影响，特别是在基于变压器的模型中。我们引入了一个新颖的概念——缺失模态令牌（MMT）——即使在模态缺失的情况下也能保持性能，这一策略在 Ego4D、Epic-Kitchens 和 Epic-Sounds 数据集中被证明是有效的。我们的方法减轻了性能损失，当一半的测试集模态不完整时，性能损失从原来的 $\sim 30\%$ 下降到只有 $\sim 10\%$。通过广泛的实验，我们证明了 MMT 对不同训练场景的适应性，以及与当前方法相比在处理缺失模式方面的优越性。我们的研究提供了全面的分析和创新方法，为现实环境中更具弹性的多式联运系统开辟了途径。</details>
**PDF:** <http://arxiv.org/pdf/2401.11470v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Task-specific regularization loss towards model calibration for reliable lung cancer detection**<br />
**Title_cn:** 针对可靠肺癌检测的模型校准的特定任务正则化损失<br />
**Authors:** Mehar Prateek Kalra, Mansi Singhal, Rohan Raju Dhanakashirur<br />
**Abstract:** <details><summary>原文: </summary>Lung cancer is one of the significant causes of cancer-related deaths globally. Early detection and treatment improve the chances of survival. Traditionally CT scans have been used to extract the most significant lung infection information and diagnose cancer. This process is carried out manually by an expert radiologist. The imbalance in the radiologists-to-population ratio in a country like India implies significant work pressure on them and thus raises the need to automate a few of their responsibilities. The tendency of modern-day Deep Neural networks to make overconfident mistakes limit their usage to detect cancer. In this paper, we propose a new task-specific loss function to calibrate the neural network to reduce the risk of overconfident mistakes. We use the state-of-the-art Multi-class Difference in Confidence and Accuracy (MDCA) loss in conjunction with the proposed task-specific loss function to achieve the same. We also integrate post-hoc calibration by performing temperature scaling on top of the train-time calibrated model. We demonstrate 5.98% improvement in the Expected Calibration Error (ECE) and a 17.9% improvement in Maximum Calibration Error (MCE) as compared to the best-performing SOTA algorithm.</details>
**Abstract_cn:** <details><summary>译文: </summary>肺癌是全球癌症相关死亡的重要原因之一。早期发现和治疗可以提高生存机会。传统上，CT 扫描用于提取最重要的肺部感染信息并诊断癌症。此过程由放射科专家手动执行。在印度这样的国家，放射科医生与人口的比例不平衡意味着他们承受着巨大的工作压力，因此需要将他们的一些职责自动化。现代深度神经网络容易犯过度自信的错误，这限制了它们在癌症检测中的应用。在本文中，我们提出了一种新的特定于任务的损失函数来校准神经网络，以降低过度自信错误的风险。我们使用最先进的多类置信度和准确性差异（MDCA）损失与所提出的特定于任务的损失函数来实现相同的目标。我们还通过在训练时间校准模型之上执行温度缩放来集成事后校准。与性能最佳的 SOTA 算法相比，我们证明预期校准误差 (ECE) 提高了 5.98%，最大校准误差 (MCE) 提高了 17.9%。</details>
**PDF:** <http://arxiv.org/pdf/2401.11464v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Inter-Domain Mixup for Semi-Supervised Domain Adaptation**<br />
**Title_cn:** 用于半监督域适应的域间混合<br />
**Authors:** Jichang Li, Guanbin Li, Yizhou Yu<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised domain adaptation (SSDA) aims to bridge source and target domain distributions, with a small number of target labels available, achieving better classification performance than unsupervised domain adaptation (UDA). However, existing SSDA work fails to make full use of label information from both source and target domains for feature alignment across domains, resulting in label mismatch in the label space during model testing. This paper presents a novel SSDA approach, Inter-domain Mixup with Neighborhood Expansion (IDMNE), to tackle this issue. Firstly, we introduce a cross-domain feature alignment strategy, Inter-domain Mixup, that incorporates label information into model adaptation. Specifically, we employ sample-level and manifold-level data mixing to generate compatible training samples. These newly established samples, combined with reliable and actual label information, display diversity and compatibility across domains, while such extra supervision thus facilitates cross-domain feature alignment and mitigates label mismatch. Additionally, we utilize Neighborhood Expansion to leverage high-confidence pseudo-labeled samples in the target domain, diversifying the label information of the target domain and thereby further increasing the performance of the adaptation model. Accordingly, the proposed approach outperforms existing state-of-the-art methods, achieving significant accuracy improvements on popular SSDA benchmarks, including DomainNet, Office-Home, and Office-31.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督域适应（SSDA）旨在桥接源域和目标域分布，利用少量可用的目标标签，实现比无监督域适应（UDA）更好的分类性能。然而，现有的SSDA工作未能充分利用源域和目标域的标签信息进行跨域特征对齐，导致模型测试时标签空间中的标签不匹配。本文提出了一种新颖的 SSDA 方法，即带有邻域扩展的域间混合（IDMNE）来解决这个问题。首先，我们引入了一种跨域特征对齐策略Inter-domain Mixup，它将标签信息合并到模型自适应中。具体来说，我们采用样本级和流形级数据混合来生成兼容的训练样本。这些新建立的样本与可靠且真实的标签信息相结合，显示出跨领域的多样性和兼容性，而这种额外的监督则有助于跨领域的特征对齐并减少标签不匹配。此外，我们利用邻域扩展来利用目标域中的高置信度伪标记样本，使目标域的标签信息多样化，从而进一步提高自适应模型的性能。因此，所提出的方法优于现有的最先进方法，在流行的 SSDA 基准（包括 DomainNet、Office-Home 和 Office-31）上实现了显着的准确性改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.11453v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Adaptive Betweenness Clustering for Semi-Supervised Domain Adaptation**<br />
**Title_cn:** 用于半监督域适应的自适应介数聚类<br />
**Authors:** Jichang Li, Guanbin Li, Yizhou Yu<br />
**Abstract:** <details><summary>原文: </summary>Compared to unsupervised domain adaptation, semi-supervised domain adaptation (SSDA) aims to significantly improve the classification performance and generalization capability of the model by leveraging the presence of a small amount of labeled data from the target domain. Several SSDA approaches have been developed to enable semantic-aligned feature confusion between labeled (or pseudo labeled) samples across domains; nevertheless, owing to the scarcity of semantic label information of the target domain, they were arduous to fully realize their potential. In this study, we propose a novel SSDA approach named Graph-based Adaptive Betweenness Clustering (G-ABC) for achieving categorical domain alignment, which enables cross-domain semantic alignment by mandating semantic transfer from labeled data of both the source and target domains to unlabeled target samples. In particular, a heterogeneous graph is initially constructed to reflect the pairwise relationships between labeled samples from both domains and unlabeled ones of the target domain. Then, to degrade the noisy connectivity in the graph, connectivity refinement is conducted by introducing two strategies, namely Confidence Uncertainty based Node Removal and Prediction Dissimilarity based Edge Pruning. Once the graph has been refined, Adaptive Betweenness Clustering is introduced to facilitate semantic transfer by using across-domain betweenness clustering and within-domain betweenness clustering, thereby propagating semantic label information from labeled samples across domains to unlabeled target data. Extensive experiments on three standard benchmark datasets, namely DomainNet, Office-Home, and Office-31, indicated that our method outperforms previous state-of-the-art SSDA approaches, demonstrating the superiority of the proposed G-ABC algorithm.</details>
**Abstract_cn:** <details><summary>译文: </summary>与无监督域适应相比，半监督域适应（SSDA）旨在通过利用目标域中少量标记数据的存在来显着提高模型的分类性能和泛化能力。已经开发了几种 SSDA 方法来实现跨域的标记（或伪标记）样本之间的语义对齐特征混淆；然而，由于目标领域语义标签信息的稀缺，它们很难充分发挥其潜力。在本研究中，我们提出了一种名为基于图的自适应介数聚类（G-ABC）的新颖 SSDA 方法，用于实现分类域对齐，该方法通过强制从源域和目标域的标记数据到语义转移来实现跨域语义对齐。未标记的目标样本。特别是，最初构建异构图来反映来自两个域的标记样本和目标域的未标记样本之间的成对关系。然后，为了降低图中的噪声连接性，通过引入两种策略来进行连接性细化，即基于置信不确定性的节点去除和基于预测相异性的边缘修剪。一旦图被细化，自适应介数聚类就被引入，通过使用跨域介数聚类和域内介数聚类来促进语义转移，从而将语义标签信息从跨域的标记样本传播到未标记的目标数据。对三个标准基准数据集（DomainNet、Office-Home 和 Office-31）的广泛实验表明，我们的方法优于以前最先进的 SSDA 方法，证明了所提出的 G-ABC 算法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11448v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Geometric Prior Guided Feature Representation Learning for Long-Tailed Classification**<br />
**Title_cn:** 用于长尾分类的几何先验引导特征表示学习<br />
**Authors:** Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Puhua Chen<br />
**Abstract:** <details><summary>原文: </summary>Real-world data are long-tailed, the lack of tail samples leads to a significant limitation in the generalization ability of the model. Although numerous approaches of class re-balancing perform well for moderate class imbalance problems, additional knowledge needs to be introduced to help the tail class recover the underlying true distribution when the observed distribution from a few tail samples does not represent its true distribution properly, thus allowing the model to learn valuable information outside the observed domain. In this work, we propose to leverage the geometric information of the feature distribution of the well-represented head class to guide the model to learn the underlying distribution of the tail class. Specifically, we first systematically define the geometry of the feature distribution and the similarity measures between the geometries, and discover four phenomena regarding the relationship between the geometries of different feature distributions. Then, based on four phenomena, feature uncertainty representation is proposed to perturb the tail features by utilizing the geometry of the head class feature distribution. It aims to make the perturbed features cover the underlying distribution of the tail class as much as possible, thus improving the model's generalization performance in the test domain. Finally, we design a three-stage training scheme enabling feature uncertainty modeling to be successfully applied. Experiments on CIFAR-10/100-LT, ImageNet-LT, and iNaturalist2018 show that our proposed approach outperforms other similar methods on most metrics. In addition, the experimental phenomena we discovered are able to provide new perspectives and theoretical foundations for subsequent studies.</details>
**Abstract_cn:** <details><summary>译文: </summary>现实世界的数据是长尾的，尾部样本的缺乏导致模型的泛化能力受到显着限制。尽管许多类别重新平衡方法对于中等类别不平衡问题表现良好，但当从几个尾部样本观察到的分布不能正确代表其真实分布时，需要引入额外的知识来帮助尾部恢复潜在的真实分布，因此允许模型学习观察域之外的有价值的信息。在这项工作中，我们建议利用具有良好代表性的头类特征分布的几何信息来指导模型学习尾类的底层分布。具体来说，我们首先系统地定义了特征分布的几何形状和几何形状之间的相似性度量，并发现了关于不同特征分布的几何形状之间关系的四种现象。然后，基于四种现象，提出特征不确定性表示，利用头类特征分布的几何形状来扰动尾部特征。其目的是使扰动特征尽可能覆盖尾类的底层分布，从而提高模型在测试域的泛化性能。最后，我们设计了一个三阶段训练方案，使特征不确定性建模能够成功应用。 CIFAR-10/100-LT、ImageNet-LT 和 iNaturalist2018 上的实验表明，我们提出的方法在大多数指标上都优于其他类似方法。此外，我们发现的实验现象能够为后续研究提供新的视角和理论基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.11436v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Exploring Diffusion Time-steps for Unsupervised Representation Learning**<br />
**Title_cn:** 探索无监督表示学习的扩散时间步长<br />
**Authors:** Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all 1,...,t-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, e.g., interpolating only one specified attribute between two images, validating the disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti.</details>
**Abstract_cn:** <details><summary>译文: </summary>表示学习就是发现忠实生成数据的隐藏模块化属性。我们探索去噪扩散概率模型（DM）在模块化属性无监督学习中的潜力。我们建立了一个连接扩散时间步长和隐藏属性的理论框架，作为无监督学习的有效归纳偏差。具体来说，前向扩散过程在每个时间步逐渐向样本添加高斯噪声，这本质上是通过丢失属性将不同的样本折叠成相似的样本，例如，纹理等细粒度属性随着添加的噪声较少而丢失（即，早期时间） -步骤），而粗粒度的形状（例如形状）会通过添加更多噪声（即后期时间步骤）而丢失。为了解开模块化属性，在每个时间步 t，我们学习一个特定于 t 的特征来补偿新丢失的属性，以及所有 1,...,t 特定特征的集合，对应于累积集丢失的属性，经过训练以弥补时间步 t 处预训练 DM 的重建误差。在 CelebA、FFHQ 和 Bedroom 数据集上，学习到的特征显着改善了属性分类，并实现了忠实的反事实生成，例如，在两幅图像之间仅插入一个指定的属性，验证解开质量。代码位于https://github.com/yue-zhongqi/diti。</details>
**PDF:** <http://arxiv.org/pdf/2401.11430v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Enhancing the vision-language foundation model with key semantic knowledge-emphasized report refinement**<br />
**Title_cn:** 通过关键语义知识强调报告细化来增强视觉语言基础模型<br />
**Authors:** Cheng Li, Weijian Huang, Hao Yang, Jiarun Liu, Shanshan Wang<br />
**Abstract:** <details><summary>原文: </summary>Recently, vision-language representation learning has made remarkable advancements in building up medical foundation models, holding immense potential for transforming the landscape of clinical research and medical care. The underlying hypothesis is that the rich knowledge embedded in radiology reports can effectively assist and guide the learning process, reducing the need for additional labels. However, these reports tend to be complex and sometimes even consist of redundant descriptions that make the representation learning too challenging to capture the key semantic information. This paper develops a novel iterative vision-language representation learning framework by proposing a key semantic knowledge-emphasized report refinement method. Particularly, raw radiology reports are refined to highlight the key information according to a constructed clinical dictionary and two model-optimized knowledge-enhancement metrics. The iterative framework is designed to progressively learn, starting from gaining a general understanding of the patient's condition based on raw reports and gradually refines and extracts critical information essential to the fine-grained analysis tasks. The effectiveness of the proposed framework is validated on various downstream medical image analysis tasks, including disease classification, region-of-interest segmentation, and phrase grounding. Our framework surpasses seven state-of-the-art methods in both fine-tuning and zero-shot settings, demonstrating its encouraging potential for different clinical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，视觉语言表征学习在建立医学基础模型方面取得了显着进展，在改变临床研究和医疗保健领域具有巨大潜力。基本假设是，放射学报告中嵌入的丰富知识可以有效地协助和指导学习过程，减少对额外标签的需求。然而，这些报告往往很复杂，有时甚至包含冗余描述，这使得表示学习难以捕获关键语义信息。本文通过提出一种强调关键语义知识的报告细化方法，开发了一种新颖的迭代视觉语言表示学习框架。特别是，根据构建的临床词典和两个模型优化的知识增强指标，对原始放射学报告进行细化以突出显示关键信息。迭代框架旨在逐步学习，从基于原始报告对患者病情的总体了解开始，逐步细化和提取对细粒度分析任务至关重要的关键信息。所提出的框架的有效性在各种下游医学图像分析任务上得到了验证，包括疾病分类、感兴趣区域分割和短语基础。我们的框架在微调和零样本设置方面都超过了七种最先进的方法，展示了其在不同临床应用中令人鼓舞的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.11421v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Embedded Hyperspectral Band Selection with Adaptive Optimization for Image Semantic Segmentation**<br />
**Title_cn:** 具有图像语义分割自适应优化的嵌入式高光谱波段选择<br />
**Authors:** Yaniv Zimmer, Oren Glickman<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral band selection plays a pivotal role in remote sensing and image analysis, aiming to identify the most informative spectral bands while minimizing computational overhead. In this paper, we introduce a pioneering approach for hyperspectral band selection that offers an embedded solution, making it well-suited for resource-constrained or real-time applications. Our proposed method, embedded Hyperspectral Band Selection (EHBS), excels in selecting the best bands without the need for prior processing, seamlessly integrating with the downstream task model. This is achieved through the adaptation of the Stochastic Gates (STG) algorithm, originally designed for feature selection, for hyperspectral band selection in the context of image semantic segmentation and the integration of a dynamic optimizer, DoG, which removes the need for the required tuning the learning rate. To assess the performance of our method, we introduce a novel metric for evaluating band selection methods across different target numbers of selected bands quantified by the Area Under the Curve (AUC). We conduct experiments on two distinct semantic-segmentation hyperspectral benchmark datasets, demonstrating its superiority in terms of its resulting accuracy and its ease of use compared to many common and state-of-the-art methods. Furthermore, our contributions extend beyond the realm of hyperspectral band selection. The adaptability of our approach to other tasks, especially those involving grouped features, opens up promising avenues for broader applications within the realm of deep learning, such as feature selection for feature groups. The demonstrated success on the tested datasets and the potential for application to a variety of tasks underscore the value of our method as a substantial addition to the field of computer vision.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱波段选择在遥感和图像分析中发挥着关键作用，旨在识别信息最丰富的光谱波段，同时最大限度地减少计算开销。在本文中，我们介绍了一种开创性的高光谱波段选择方法，该方法提供了嵌入式解决方案，使其非常适合资源受限或实时应用。我们提出的方法，嵌入式高光谱波段选择（EHBS），擅长选择最佳波段，无需事先处理，与下游任务模型无缝集成。这是通过采用随机门 (STG) 算法来实现的，该算法最初设计用于特征选择、图像语义分割背景下的高光谱波段选择以及动态优化器 DoG 的集成，从而无需进行所需的调整学习率。为了评估我们方法的性能，我们引入了一种新的指标，用于评估通过曲线下面积（AUC）量化的不同目标数量的选定波段的波段选择方法。我们在两个不同的语义分割高光谱基准数据集上进行了实验，证明了与许多常见和最先进的方法相比，其在结果准确性和易用性方面的优越性。此外，我们的贡献超出了高光谱波段选择的领域。我们的方法对其他任务的适应性，特别是那些涉及分组特征的任务，为深度学习领域更广泛的应用开辟了有希望的途径，例如特征组的特征选择。在测试数据集上所取得的成功以及应用于各种任务的潜力强调了我们的方法作为计算机视觉领域的实质性补充的价值。</details>
**PDF:** <http://arxiv.org/pdf/2401.11420v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving**<br />
**Title_cn:** S$^3$M-Net：自动驾驶语义分割和立体匹配的联合学习<br />
**Authors:** Zhiyuan Wu, Yi Feng, Chuang-Wei Liu, Fisher Yu, Qijun Chen, Rui Fan<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S$^3$M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S$^3$M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is trained by minimizing a novel semantic consistency-guided (SCG) loss, which places emphasis on the structural consistency in both tasks. Extensive experimental results conducted on the vKITTI2 and KITTI datasets demonstrate the effectiveness of our proposed joint learning framework and its superior performance compared to other state-of-the-art single-task networks. Our project webpage is accessible at mias.group/S3M-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割和立体匹配是自动驾驶3D环境感知系统的两个重要组成部分。然而，传统方法通常独立解决这两个问题，为每个任务采用单独的模型。这种方法在现实场景中存在实际限制，特别是当计算资源稀缺或实时性能必不可少时。因此，在本文中，我们介绍了 S$^3$M-Net，这是一种新颖的联合学习框架，旨在同时执行语义分割和立体匹配。具体来说，S$^3$M-Net 在两个任务之间共享从 RGB 图像提取的特征，从而提高了整体场景理解能力。该特征共享过程是使用特征融合适应（FFA）模块实现的，该模块有效地将共享特征转换到语义空间，然后将它们与编码的视差特征融合。整个联合学习框架是通过最小化新颖的语义一致性引导（SCG）损失来训练的，该损失强调两项任务的结构一致性。在 vKITTI2 和 KITTI 数据集上进行的大量实验结果证明了我们提出的联合学习框架的有效性及其与其他最先进的单任务网络相比的优越性能。我们的项目网页可通过 mias.group/S3M-Net 访问。</details>
**PDF:** <http://arxiv.org/pdf/2401.11414v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Adversarial Augmentation Training Makes Action Recognition Models More Robust to Realistic Video Distribution Shifts**<br />
**Title_cn:** 对抗性增强训练使动作识别模型对现实视频分发变化更加鲁棒<br />
**Authors:** Kiyoon Kim, Shreyank N Gowda, Panagiotis Eustratiadis, Antreas Antoniou, Robert B Fisher<br />
**Abstract:** <details><summary>原文: </summary>Despite recent advances in video action recognition achieving strong performance on existing benchmarks, these models often lack robustness when faced with natural distribution shifts between training and test data. We propose two novel evaluation methods to assess model resilience to such distribution disparity. One method uses two different datasets collected from different sources and uses one for training and validation, and the other for testing. More precisely, we created dataset splits of HMDB-51 or UCF-101 for training, and Kinetics-400 for testing, using the subset of the classes that are overlapping in both train and test datasets. The other proposed method extracts the feature mean of each class from the target evaluation dataset's training data (i.e. class prototype) and estimates test video prediction as a cosine similarity score between each sample to the class prototypes of each target class. This procedure does not alter model weights using the target dataset and it does not require aligning overlapping classes of two different datasets, thus is a very efficient method to test the model robustness to distribution shifts without prior knowledge of the target distribution. We address the robustness problem by adversarial augmentation training - generating augmented views of videos that are "hard" for the classification model by applying gradient ascent on the augmentation parameters - as well as "curriculum" scheduling the strength of the video augmentations. We experimentally demonstrate the superior performance of the proposed adversarial augmentation approach over baselines across three state-of-the-art action recognition models - TSM, Video Swin Transformer, and Uniformer. The presented work provides critical insight into model robustness to distribution shifts and presents effective techniques to enhance video action recognition performance in a real-world deployment.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管视频动作识别领域的最新进展在现有基准上取得了出色的性能，但这些模型在面对训练数据和测试数据之间的自然分布变化时往往缺乏鲁棒性。我们提出了两种新颖的评估方法来评估模型对这种分布差异的弹性。一种方法使用从不同来源收集的两个不同数据集，并使用一个用于训练和验证，另一个用于测试。更准确地说，我们使用训练和测试数据集中重叠的类子集创建了用于训练的 HMDB-51 或 UCF-101 数据集分割，以及用于测试的 Kinetics-400 数据集分割。另一种提出的方​​法从目标评估数据集的训练数据（即类原型）中提取每个类的特征平均值，并将测试视频预测估计为每个样本与每个目标类的类原型之间的余弦相似度得分。此过程不会使用目标数据集改变模型权重，并且不需要对齐两个不同数据集的重叠类，因此是一种非常有效的方法，可以在不事先了解目标分布的情况下测试模型对分布变化的鲁棒性。我们通过对抗性增强训练来解决鲁棒性问题——通过对增强参数应用梯度上升来生成对分类模型来说“困难”的视频增强视图——以及“课程”调度视频增强的强度。我们通过实验证明了所提出的对抗性增强方法相对于三种最先进的动作识别模型（TSM、Video Swin Transformer 和 Uniformer）的基线的优越性能。所提出的工作提供了对模型对分布变化的鲁棒性的重要见解，并提出了在实际部署中增强视频动作识别性能的有效技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.11406v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with Fine-Grained Feature Representation**<br />
**Title_cn:** UniM-OV3D：具有细粒度特征表示的单模态开放词汇 3D 场景理解<br />
**Authors:** Qingdong He, Jinlong Peng, Zhengkai Jiang, Kai Wu, Xiaozhong Ji, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Mingang Chen, Yunsheng Wu<br />
**Abstract:** <details><summary>原文: </summary>3D open-vocabulary scene understanding aims to recognize arbitrary novel categories beyond the base label space. However, existing works not only fail to fully utilize all the available modal information in the 3D domain but also lack sufficient granularity in representing the features of each modality. In this paper, we propose a unified multimodal 3D open-vocabulary scene understanding network, namely UniM-OV3D, which aligns point clouds with image, language and depth. To better integrate global and local features of the point clouds, we design a hierarchical point cloud feature extraction module that learns comprehensive fine-grained feature representations. Further, to facilitate the learning of coarse-to-fine point-semantic representations from captions, we propose the utilization of hierarchical 3D caption pairs, capitalizing on geometric constraints across various viewpoints of 3D scenes. Extensive experimental results demonstrate the effectiveness and superiority of our method in open-vocabulary semantic and instance segmentation, which achieves state-of-the-art performance on both indoor and outdoor benchmarks such as ScanNet, ScanNet200, S3IDS and nuScenes. Code is available at https://github.com/hithqd/UniM-OV3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 开放词汇场景理解旨在识别基本标签空间之外的任意新类别。然而，现有的工作不仅未能充分利用 3D 域中所有可用的模态信息，而且在表示每种模态的特征时也缺乏足够的粒度。在本文中，我们提出了一种统一的多模态 3D 开放词汇场景理解网络，即 UniM-OV3D，它将点云与图像、语言和深度对齐。为了更好地整合点云的全局和局部特征，我们设计了一个分层点云特征提取模块，该模块可以学习全面的细粒度特征表示。此外，为了促进从字幕中学习从粗到细的点语义表示，我们建议利用分层 3D 字幕对，利用 3D 场景各个视点的几何约束。大量的实验结果证明了我们的方法在开放词汇语义和实例分割方面的有效性和优越性，在 ScanNet、ScanNet200、S3IDS 和 nuScenes 等室内和室外基准测试上均实现了最先进的性能。代码可在 https://github.com/hithqd/UniM-OV3D 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11395v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **ANNA: A Deep Learning Based Dataset in Heterogeneous Traffic for Autonomous Vehicles**<br />
**Title_cn:** ANNA：基于深度学习的自动驾驶汽车异构交通数据集<br />
**Authors:** Mahedi Kamal, Tasnim Fariha, Afrina Kabir Zinia, Md. Abu Syed, Fahim Hasan Khan, Md. Mahbubur Rahman<br />
**Abstract:** <details><summary>原文: </summary>Recent breakthroughs in artificial intelligence offer tremendous promise for the development of self-driving applications. Deep Neural Networks, in particular, are being utilized to support the operation of semi-autonomous cars through object identification and semantic segmentation. To assess the inadequacy of the current dataset in the context of autonomous and semi-autonomous cars, we created a new dataset named ANNA. This study discusses a custom-built dataset that includes some unidentified vehicles in the perspective of Bangladesh, which are not included in the existing dataset. A dataset validity check was performed by evaluating models using the Intersection Over Union (IOU) metric. The results demonstrated that the model trained on our custom dataset was more precise and efficient than the models trained on the KITTI or COCO dataset concerning Bangladeshi traffic. The research presented in this paper also emphasizes the importance of developing accurate and efficient object detection algorithms for the advancement of autonomous vehicles.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能的最新突破为自动驾驶应用的开发提供了巨大的希望。特别是深度神经网络被用来通过对象识别和语义分割来支持半自动汽车的操作。为了评估当前数据集在自动驾驶和半自动汽车背景下的不足，我们创建了一个名为 ANNA 的新数据集。本研究讨论了一个定制的数据集，其中包括孟加拉国视角中的一些身份不明的车辆，这些车辆未包含在现有数据集中。通过使用并交交集 (IOU) 指标评估模型来执行数据集有效性检查。结果表明，在我们的自定义数据集上训练的模型比在有关孟加拉国交通的 KITTI 或 COCO 数据集上训练的模型更加精确和高效。本文提出的研究还强调了开发准确高效的物体检测算法对于自动驾驶汽车进步的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11358v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **Multi-View Neural 3D Reconstruction of Micro-/Nanostructures with Atomic Force Microscopy**<br />
**Title_cn:** 利用原子力显微镜对微/纳米结构进行多视角神经 3D 重建<br />
**Authors:** Shuo Chen, Mao Peng, Yijin Li, Bing-Feng Ju, Hujun Bao, Yuan-Liu Chen, Guofeng Zhang<br />
**Abstract:** <details><summary>原文: </summary>Atomic Force Microscopy (AFM) is a widely employed tool for micro-/nanoscale topographic imaging. However, conventional AFM scanning struggles to reconstruct complex 3D micro-/nanostructures precisely due to limitations such as incomplete sample topography capturing and tip-sample convolution artifacts. Here, we propose a multi-view neural-network-based framework with AFM (MVN-AFM), which accurately reconstructs surface models of intricate micro-/nanostructures. Unlike previous works, MVN-AFM does not depend on any specially shaped probes or costly modifications to the AFM system. To achieve this, MVN-AFM uniquely employs an iterative method to align multi-view data and eliminate AFM artifacts simultaneously. Furthermore, we pioneer the application of neural implicit surface reconstruction in nanotechnology and achieve markedly improved results. Extensive experiments show that MVN-AFM effectively eliminates artifacts present in raw AFM images and reconstructs various micro-/nanostructures including complex geometrical microstructures printed via Two-photon Lithography and nanoparticles such as PMMA nanospheres and ZIF-67 nanocrystals. This work presents a cost-effective tool for micro-/nanoscale 3D analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>原子力显微镜 (AFM) 是一种广泛使用的微/纳米尺度形貌成像工具。然而，由于样品形貌捕获不完整和尖端样品卷积伪影等限制，传统 AFM 扫描难以精确重建复杂的 3D 微/纳米结构。在这里，我们提出了一种基于多视图神经网络的 AFM (MVN-AFM) 框架，它可以准确地重建复杂微/纳米结构的表面模型。与以前的工作不同，MVN-AFM 不依赖于任何特殊形状的探针或对 AFM 系统进行昂贵的修改。为了实现这一目标，MVN-AFM 独特地采用迭代方法来对齐多视图数据并同时消除 AFM 伪影。此外，我们开创了神经隐式表面重建在纳米技术中的应用，并取得了显着改善的结果。大量实验表明，MVN-AFM 有效消除了原始 AFM 图像中存在的伪影，并重建了各种微/纳米结构，包括通过双光子光刻印刷的复杂几何微结构以及 PMMA 纳米球和 ZIF-67 纳米晶体等纳米颗粒。这项工作为微米/纳米级 3D 分析提供了一种经济高效的工具。</details>
**PDF:** <http://arxiv.org/pdf/2401.11541v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers**<br />
**Title_cn:** 使用沙漏扩散变压器进行可扩展高分辨率像素空间图像合成<br />
**Authors:** Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z. Kaplan, Enrico Shippole<br />
**Abstract:** <details><summary>原文: </summary>We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 Hourglass Diffusion Transformer (HDiT)，这是一种图像生成模型，它表现出与像素数的线性缩放，支持直接在像素空间中进行高分辨率（例如 $1024 \times 1024$）的训练。它建立在 Transformer 架构之上，众所周知，该架构可扩展到数十亿个参数，它弥补了卷积 U-Net 的效率与 Transformer 的可扩展性之间的差距。 HDiT 无需典型的高分辨率训练技术（例如多尺度架构、潜在自动编码器或自调节）即可成功训练。我们证明 HDiT 在 ImageNet $256^2$ 上的表现与现有模型具有竞争力，并为 FFHQ-$1024^2$ 上的扩散模型设定了新的最先进水平。</details>
**PDF:** <http://arxiv.org/pdf/2401.11605v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Grayscale Image Colorization with GAN and CycleGAN in Different Image Domain**<br />
**Title_cn:** 在不同图像域中使用 GAN 和 CycleGAN 进行灰度图像着色<br />
**Authors:** Chen Liang, Yunchen Sheng, Yichen Mo<br />
**Abstract:** <details><summary>原文: </summary>Automatic colorization of grayscale image has been a challenging task. Previous research have applied supervised methods in conquering this problem [ 1]. In this paper, we reproduces a GAN-based coloring model, and experiments one of its variant. We also proposed a CycleGAN based model and experiments those methods on various datasets. The result shows that the proposed CycleGAN model does well in human-face coloring and comic coloring, but lack the ability to diverse colorization.</details>
**Abstract_cn:** <details><summary>译文: </summary>灰度图像的自动着色一直是一项具有挑战性的任务。先前的研究已经应用监督方法来解决这个问题[1]。在本文中，我们重现了基于 GAN 的着色模型，并对其其中一个变体进行了实验。我们还提出了一个基于 CycleGAN 的模型，并在各种数据集上实验了这些方法。结果表明，所提出的 CycleGAN 模型在人脸着色和漫画着色方面表现良好，但缺乏多样化着色的能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.11425v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals**<br />
**Title_cn:** 使用跨模态信号的自监督鸟瞰运动预测<br />
**Authors:** Shaoheng Fang, Zuhong Liu, Mingyu Wang, Chenxin Xu, Yiqi Zhong, Siheng Chen<br />
**Abstract:** <details><summary>原文: </summary>Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task.</details>
**Abstract_cn:** <details><summary>译文: </summary>以自我监督的方式学习密集鸟瞰 (BEV) 运动流是机器人和自动驾驶的一项新兴研究。目前的自监督方法主要依赖于点云之间的点对应关系，这可能会引入假流和不一致的问题，阻碍模型学习准确和真实运动的能力。在本文中，我们介绍了一种新颖的跨模态自监督训练框架，该框架通过利用多模态数据获取监督信号来有效解决这些问题。我们设计了三种创新的监督信号来保留场景运动的固有属性，包括掩模切角距离损失、分段刚度损失和时间一致性损失。通过大量的实验，我们证明了我们提出的自监督框架优于之前所有用于运动预测任务的自监督方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11499v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LLMRA: Multi-modal Large Language Model based Restoration Assistant**<br />
**Title_cn:** LLMRA：基于多模态大语言模型的恢复助手<br />
**Authors:** Xiaoyu Jin, Yuan Shi, Bin Xia, Wenming Yang<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal Large Language Models (MLLMs) have a significant impact on various tasks, due to their extensive knowledge and powerful perception and generation capabilities. However, it still remains an open research problem on applying MLLMs to low-level vision tasks. In this paper, we present a simple MLLM-based Image Restoration framework to address this gap, namely Multi-modal Large Language Model based Restoration Assistant (LLMRA). We exploit the impressive capabilities of MLLMs to obtain the degradation information for universal image restoration. By employing a pretrained multi-modal large language model and a vision language model, we generate text descriptions and encode them as context embedding with degradation information for the degraded image. Through the proposed Context Enhance Module (CEM) and Degradation Context based Transformer Network (DC-former), we integrate these context embedding into the restoration network, contributing to more accurate and adjustable image restoration. Based on the dialogue with the users, our method leverages image degradation priors from MLLMs, providing low-level attributes descriptions of the input low-quality images and the restored high-quality images simultaneously. Extensive experiments demonstrate the superior performance of our LLMRA in universal image restoration tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）因其丰富的知识和强大的感知和生成能力而对各种任务产生重大影响。然而，将 MLLM 应用于低级视觉任务仍然是一个开放的研究问题。在本文中，我们提出了一个简单的基于 MLLM 的图像恢复框架来解决这一差距，即基于多模态大型语言模型的恢复助手（LLMRA）。我们利用 MLLM 令人印象深刻的功能来获取通用图像恢复的退化信息。通过采用预训练的多模态大语言模型和视觉语言模型，我们生成文本描述并将其编码为上下文嵌入，其中包含退化图像的退化信息。通过提出的上下文增强模块（CEM）和基于退化上下文的变压器网络（DC-former），我们将这些上下文嵌入集成到恢复网络中，有助于更准确和可调整的图像恢复。基于与用户的对话，我们的方法利用 MLLM 的图像退化先验，同时提供输入低质量图像和恢复的高质量图像的低级属性描述。大量实验证明了我们的 LLMRA 在通用图像恢复任务中的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11401v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **General Flow as Foundation Affordance for Scalable Robot Learning**<br />
**Title_cn:** 一般流程作为可扩展机器人学习的基础功能<br />
**Authors:** Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao<br />
**Abstract:** <details><summary>原文: </summary>We address the challenge of acquiring real-world manipulation skills with a scalable framework.Inspired by the success of large-scale auto-regressive prediction in Large Language Models (LLMs), we hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target in robot learning. To exploit scalable data resources, we turn our attention to cross-embodiment datasets. We develop, for the first time, a language-conditioned prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable geometric and physics guidance, thus facilitating stable zero-shot skill transfer in real-world scenarios.We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any additional training, our method achieves an impressive 81% success rate in human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) universality: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. These lead to a new pathway towards scalable general robot learning. Data, code, and model weights will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们通过可扩展的框架来应对获取现实世界操作技能的挑战。受到大型语言模型（LLM）中大规模自回归预测成功的启发，我们坚信，确定能够利用大型语言模型的适当预测目标规模数据集对于实现高效和通用的学习至关重要。因此，我们建议利用代表感兴趣对象上 3D 点的未来轨迹的流作为机器人学习中的理想预测目标。为了利用可扩展的数据资源，我们将注意力转向跨实施例数据集。我们首次直接从大规模 RGBD 人类视频数据集开发语言条件预测模型。我们的预测流提供了可操作的几何和物理指导，从而促进现实场景中稳定的零样本技能转移。我们使用基于闭环流预测的策略来部署我们的方法。值得注意的是，在没有任何额外训练的情况下，我们的方法在人机技能迁移方面取得了令人印象深刻的 81% 成功率，涵盖 6 个场景中的 18 项任务。我们的框架具有以下优点：（1）可扩展性：利用跨实体的数据资源； (2)通用性：多种物体类别，包括刚性体、铰接体和软体； (3) 稳定的技能转移：提供具有较小推理领域差距的可行指导。这些为可扩展的通用机器人学习开辟了一条新途径。数据、代码和模型权重将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.11439v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **How Robust Are Energy-Based Models Trained With Equilibrium Propagation?**<br />
**Title_cn:** 通过平衡传播训练的基于能量的模型有多鲁棒？<br />
**Authors:** Siddharth Mansingh, Michal Kucer, Garrett Kenyon, Juston Moore, Michael Teti<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks (DNNs) are easily fooled by adversarial perturbations that are imperceptible to humans. Adversarial training, a process where adversarial examples are added to the training set, is the current state-of-the-art defense against adversarial attacks, but it lowers the model's accuracy on clean inputs, is computationally expensive, and offers less robustness to natural noise. In contrast, energy-based models (EBMs), which were designed for efficient implementation in neuromorphic hardware and physical systems, incorporate feedback connections from each layer to the previous layer, yielding a recurrent, deep-attractor architecture which we hypothesize should make them naturally robust. Our work is the first to explore the robustness of EBMs to both natural corruptions and adversarial attacks, which we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs are more robust than transformers and display comparable robustness to adversarially-trained DNNs on gradient-based (white-box) attacks, query-based (black-box) attacks, and natural perturbations without sacrificing clean accuracy, and without the need for adversarial training or additional training techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络（DNN）很容易被人类无法察觉的对抗性扰动所愚弄。对抗性训练是将对抗性示例添加到训练集中的过程，是当前针对对抗性攻击的最先进的防御方法，但它降低了模型在干净输入上的准确性，计算量大，并且对自然的鲁棒性较差。噪音。相比之下，基于能量的模型（EBM）是为了在神经形态硬件和物理系统中有效实现而设计的，它结合了从每一层到前一层的反馈连接，产生了一个循环的、深度吸引子架构，我们假设这应该使它们自然地产生强壮的。我们的工作是第一个探索 EBM 对自然损坏和对抗性攻击的鲁棒性的工作，我们使用 CIFAR-10 和 CIFAR-100 数据集进行这项工作。我们证明 EBM 比 Transformer 更稳健，并且在基于梯度（白盒）的攻击、基于查询的（黑盒）攻击和自然扰动方面表现出与经过对抗性训练的 DNN 相当的鲁棒性，而不会牺牲干净的准确性，并且无需需要对抗性训练或额外的训练技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.11543v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting**<br />
**Title_cn:** 高斯溅射可变形内窥镜组织重建<br />
**Authors:** Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu<br />
**Abstract:** <details><summary>原文: </summary>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at https://github.com/HKU-MedAI/EndoGS.</details>
**Abstract_cn:** <details><summary>译文: </summary>外科 3D 重建是机器人手术研究的一个关键领域，最近的工作采用动态辐射场的变体，成功地从单视点视频对可变形组织进行 3D 重建。然而，这些方法通常会遇到耗时的优化或质量较差的问题，限制了它们在下游任务中的采用。受到最近流行的 3D 表示形式 3D 高斯分布的启发，我们提出了 EndoGS，应用高斯分布进行可变形内窥镜组织重建。具体来说，我们的方法结合了变形场来处理动态场景、深度引导监督来优化单视点的 3D 目标，以及时空权重掩模来减轻工具遮挡。因此，EndoGS 通过单视点视频、估计深度图和标记的工具掩模重建并渲染高质量的可变形内窥镜组织。达芬奇机器人手术视频实验表明，EndoGS 实现了卓越的渲染质量。代码可在 https://github.com/HKU-MedAI/EndoGS 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11535v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Hierarchical Prompts for Rehearsal-free Continual Learning**<br />
**Title_cn:** 分层提示，无需排练持续学习<br />
**Authors:** Yukun Zuo, Hantao Yao, Lu Yu, Liansheng Zhuang, Changsheng Xu<br />
**Abstract:** <details><summary>原文: </summary>Continual learning endeavors to equip the model with the capability to integrate current task knowledge while mitigating the forgetting of past task knowledge. Inspired by prompt tuning, prompt-based methods maintain a frozen backbone and train with slight learnable prompts to minimize the catastrophic forgetting that arises due to updating a large number of backbone parameters. Nonetheless, these learnable prompts tend to concentrate on the discriminatory knowledge of the current task while ignoring past task knowledge, leading to that learnable prompts still suffering from catastrophic forgetting. This paper introduces a novel rehearsal-free paradigm for continual learning termed Hierarchical Prompts (H-Prompts), comprising three categories of prompts -- class prompt, task prompt, and general prompt. To effectively depict the knowledge of past classes, class prompt leverages Bayesian Distribution Alignment to model the distribution of classes in each task. To reduce the forgetting of past task knowledge, task prompt employs Cross-task Knowledge Excavation to amalgamate the knowledge encapsulated in the learned class prompts of past tasks and current task knowledge. Furthermore, general prompt utilizes Generalized Knowledge Exploration to deduce highly generalized knowledge in a self-supervised manner. Evaluations on two benchmarks substantiate the efficacy of the proposed H-Prompts, exemplified by an average accuracy of 87.8% in Split CIFAR-100 and 70.6% in Split ImageNet-R.</details>
**Abstract_cn:** <details><summary>译文: </summary>持续学习致力于使模型具有整合当前任务知识的能力，同时减少对过去任务知识的遗忘。受提示调整的启发，基于提示的方法保持冻结的骨干网，并使用轻微的可学习提示进​​行训练，以最大限度地减少由于更新大量骨干网参数而产生的灾难性遗忘。然而，这些可学习提示往往集中于当前任务的歧视性知识，而忽略过去的任务知识，导致可学习提示仍然遭受灾难性遗忘。本文介绍了一种新颖的免排练持续学习范式，称为分层提示（H-Prompts），包括三类提示——课堂提示、任务提示和一般提示。为了有效地描述过去班级的知识，班级提示利用贝叶斯分布对齐来对每个任务中班级的分布进行建模。为了减少对过去任务知识的遗忘，任务提示采用跨任务知识挖掘来合并封装在过去任务和当前任务知识的学习类提示中的知识。此外，通用提示利用广义知识探索以自我监督的方式推导高度广义的知识。对两个基准的评估证实了所提出的 H-Prompts 的有效性，Split CIFAR-100 中的平均准确度为 87.8%，Split ImageNet-R 中的平均准确度为 70.6%。</details>
**PDF:** <http://arxiv.org/pdf/2401.11544v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Visual Imitation Learning with Calibrated Contrastive Representation**<br />
**Title_cn:** 具有校准对比表示的视觉模仿学习<br />
**Authors:** Yunke Wang, Linwei Tao, Bo Du, Yutian Lin, Chang Xu<br />
**Abstract:** <details><summary>原文: </summary>Adversarial Imitation Learning (AIL) allows the agent to reproduce expert behavior with low-dimensional states and actions. However, challenges arise in handling visual states due to their less distinguishable representation compared to low-dimensional proprioceptive features. While existing methods resort to adopt complex network architectures or separate the process of learning representation and decision-making, they overlook valuable intra-agent information within demonstrations. To address this problem, this paper proposes a simple and effective solution by incorporating calibrated contrastive representative learning into visual AIL framework. Specifically, we present an image encoder in visual AIL, utilizing a combination of unsupervised and supervised contrastive learning to extract valuable features from visual states. Based on the fact that the improved agent often produces demonstrations of varying quality, we propose to calibrate the contrastive loss by treating each agent demonstrations as a mixed sample. The incorporation of contrastive learning can be jointly optimized with the AIL framework, without modifying the architecture or incurring significant computational costs. Experimental results on DMControl Suite demonstrate our proposed method is sample efficient and can outperform other compared methods from different aspects.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性模仿学习（AIL）允许代理通过低维状态和动作重现专家行为。然而，由于与低维本体感受特征相比，视觉状态的表示不太明显，因此在处理视觉状态时出现了挑战。虽然现有方法采用复杂的网络架构或将学习表示和决策的过程分开，但它们忽略了演示中有价值的代理内信息。为了解决这个问题，本文提出了一种简单有效的解决方案，将校准对比代表性学习纳入视觉AIL框架中。具体来说，我们在视觉 AIL 中提出了一种图像编码器，利用无监督和监督对比学习的组合从视觉状态中提取有价值的特征。基于改进后的代理经常产生不同质量的演示的事实，我们建议通过将每个代理演示视为混合样本来校准对比损失。对比学习的结合可以与 AIL 框架联合优化，而无需修改架构或产生大量计算成本。 DMControl Suite 上的实验结果表明，我们提出的方法具有样本效率，并且可以从不同方面优于其他比较方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11396v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Text-to-Image Cross-Modal Generation: A Systematic Review**<br />
**Title_cn:** 文本到图像的跨模式生成：系统回顾<br />
**Authors:** Maciej Żelaszczyk, Jacek Mańdziuk<br />
**Abstract:** <details><summary>原文: </summary>We review research on generating visual data from text from the angle of "cross-modal generation." This point of view allows us to draw parallels between various methods geared towards working on input text and producing visual output, without limiting the analysis to narrow sub-areas. It also results in the identification of common templates in the field, which are then compared and contrasted both within pools of similar methods and across lines of research. We provide a breakdown of text-to-image generation into various flavors of image-from-text methods, video-from-text methods, image editing, self-supervised and graph-based approaches. In this discussion, we focus on research papers published at 8 leading machine learning conferences in the years 2016-2022, also incorporating a number of relevant papers not matching the outlined search criteria. The conducted review suggests a significant increase in the number of papers published in the area and highlights research gaps and potential lines of investigation. To our knowledge, this is the first review to systematically look at text-to-image generation from the perspective of "cross-modal generation."</details>
**Abstract_cn:** <details><summary>译文: </summary>我们从“跨模态生成”的角度回顾了从文本生成视觉数据的研究。这种观点使我们能够在处理输入文本和产生视觉输出的各种方法之间进行比较，而不是将分析限制在狭窄的子区域。它还导致了该领域常见模板的识别，然后在类似方法库和跨研究领域内对这些模板进行比较和对比。我们将文本到图像的生成细分为各种风格的图像到文本方法、视频到文本方法、图像编辑、自我监督和基于图形的方法。在本次讨论中，我们重点关注 2016-2022 年在 8 个领先的机器学习会议上发表的研究论文，还纳入了一些不符合概述的搜索标准的相关论文。进行的审查表明该领域发表的论文数量显着增加，并强调了研究差距和潜在的研究方向。据我们所知，这是第一次从“跨模态生成”的角度系统地看待文本到图像生成的综述。</details>
**PDF:** <http://arxiv.org/pdf/2401.11631v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MobileARLoc: On-device Robust Absolute Localisation for Pervasive Markerless Mobile AR**<br />
**Title_cn:** MobileARLoc：用于普及无标记移动 AR 的设备上鲁棒绝对定位<br />
**Authors:** Changkun Liu, Yukun Zhao, Tristan Braud<br />
**Abstract:** <details><summary>原文: </summary>Recent years have seen significant improvement in absolute camera pose estimation, paving the way for pervasive markerless Augmented Reality (AR). However, accurate absolute pose estimation techniques are computation- and storage-heavy, requiring computation offloading. As such, AR systems rely on visual-inertial odometry (VIO) to track the device's relative pose between requests to the server. However, VIO suffers from drift, requiring frequent absolute repositioning. This paper introduces MobileARLoc, a new framework for on-device large-scale markerless mobile AR that combines an absolute pose regressor (APR) with a local VIO tracking system. Absolute pose regressors (APRs) provide fast on-device pose estimation at the cost of reduced accuracy. To address APR accuracy and reduce VIO drift, MobileARLoc creates a feedback loop where VIO pose estimations refine the APR predictions. The VIO system identifies reliable predictions of APR, which are then used to compensate for the VIO drift. We comprehensively evaluate MobileARLoc through dataset simulations. MobileARLoc halves the error compared to the underlying APR and achieve fast (80\,ms) on-device inference speed.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，绝对相机姿态估计有了显着改进，为普遍的无标记增强现实 (AR) 铺平了道路。然而，精确的绝对姿态估计技术计算量和存储量很大，需要计算卸载。因此，AR 系统依靠视觉惯性里程计 (VIO) 来跟踪设备在向服务器发出的请求之间的相对姿势。然而，VIO 会出现漂移，需要频繁的绝对重新定位。本文介绍了 MobileARLoc，这是一种用于设备上大规模无标记移动 AR 的新框架，它将绝对姿态回归器 (APR) 与本地 VIO 跟踪系统相结合。绝对姿态回归器 (APR) 提供快速的设备上姿态估计，但代价是精度降低。为了解决 APR 准确性并减少 VIO 漂移，MobileARLoc 创建了一个反馈循环，其中 VIO 姿态估计可细化 APR 预测。 VIO 系统可识别 APR 的可靠预测，然后用于补偿 VIO 漂移。我们通过数据集模拟全面评估 MobileARLoc。与底层 APR 相比，MobileARLoc 将误差减半，并实现快速 (80\,ms) 设备上推理速度。</details>
**PDF:** <http://arxiv.org/pdf/2401.11511v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **ColorVideoVDP: A visual difference predictor for image, video and display distortions**<br />
**Title_cn:** ColorVideoVDP：图像、视频和显示失真的视觉差异预测器<br />
**Authors:** Rafal K. Mantiuk, Param Hanji, Maliha Ashraf, Yuta Asano, Alexandre Chapiro<br />
**Abstract:** <details><summary>原文: </summary>ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision, for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video streaming distortions (e.g. video compression, rescaling, and transmission errors), and also 8 new distortion types related to AR/VR displays (e.g. light source and waveguide non-uniformities). To address the latter application, we collected our novel XR-Display-Artifact-Video quality dataset (XR-DAVID), comprised of 336 distorted videos. Extensive testing on XR-DAVID, as well as several datasets from the literature, indicate a significant gain in prediction performance compared to existing metrics. ColorVideoVDP opens the doors to many novel applications which require the joint automated spatiotemporal assessment of luminance and color distortions, including video streaming, display specification and design, visual comparison of results, and perceptually-guided quality optimization.</details>
**Abstract_cn:** <details><summary>译文: </summary>ColorVideoVDP 是一种视频和图像质量指标，可对视觉的空间和时间方面（亮度和颜色）进行建模。该指标建立在彩色时空对比敏感度和跨通道对比掩蔽的新型心理物理模型的基础上。它考虑了显示器的观看条件、几何和光度特性。它经过训练可以预测常见的视频流失真（例如视频压缩、重新缩放和传输错误），以及与 AR/VR 显示相关的 8 种新失真类型（例如光源和波导不均匀性）。为了解决后一种应用，我们收集了新颖的 XR-Display-Artifact-Video 质量数据集 (XR-DAVID)，其中包含 336 个失真视频。对 XR-DAVID 以及文献中的多个数据集进行的广泛测试表明，与现有指标相比，预测性能有了显着提高。 ColorVideoVDP 为许多新颖的应用打开了大门，这些应用需要对亮度和颜色失真进行联合自动时空评估，包括视频流、显示规范和设计、结果的视觉比较以及感知引导的质量优化。</details>
**PDF:** <http://arxiv.org/pdf/2401.11485v1><br />
**Code:** null<br />

