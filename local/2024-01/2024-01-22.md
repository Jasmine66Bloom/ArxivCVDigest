## [UPDATED!] **2024-01-22** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Exploring Simple Open-Vocabulary Semantic Segmentation**<br />
**Title_cn:** 探索简单的开放词汇语义分割<br />
**Authors:** Zihang Lai<br />
**Abstract:** <details><summary>原文: </summary>Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>开放词汇语义分割模型旨在从一组任意开放词汇文本中准确地将语义标签分配给图像中的每个像素。为了学习这种像素级对齐，当前的方法通常依赖于 (i) 图像级 VL 模型（例如 CLIP）、(ii) 地面真实掩模和 (iii) 自定义分组编码器的组合。在本文中，我们介绍了 S-Seg，这是一种新颖的模型，无需依赖上述任何元素即可实现令人惊讶的强大性能。 S-Seg 利用伪掩模和语言来训练 MaskFormer，并且可以从公开可用的图像文本数据集轻松进行训练。与之前的工作相反，我们的模型直接训练像素级特征和语言对齐。经过训练后，S-Seg 可以很好地推广到多个测试数据集，无需进行微调。此外，S-Seg 还具有数据可扩展性以及通过自我训练增强后持续改进的额外优势。我们相信，我们简单而有效的方法将为未来的研究奠定坚实的基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.12217v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition**<br />
**Title_cn:** 连接点：利用时空图神经网络进行准确的孟加拉手语识别<br />
**Authors:** Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习和计算机视觉的最新进展已被成功利用来为各种情况下的边缘化社区提供服务。其中一个领域是手语——聋人社区的主要交流方式。然而，到目前为止，大部分研究工作和投资都投入到了美国手语方面，而对资源匮乏的手语（尤其是孟加拉手语）的研究活动却明显滞后。在这篇研究论文中，我们提出了一个新的单词级孟加拉手语数据集 - BdSL40 - 由超过 40 个单词的 611 个视频组成，以及两种不同的方法：一种采用 3D 卷积神经网络模型，另一种采用新颖的图神经网络方法用于 BdSL40 数据集的分类。这是第一个关于单词级 BdSL 识别的研究，数据集是使用孟加拉手语词典 (1997) 从印度手语 (ISL) 转录而来。所提出的 GNN 模型取得了 89% 的 F1 分数。该研究强调了 BdSL、西孟加拉手语和 ISL 之间显着的词汇和语义相似性，以及文献中缺乏 BdSL 的单词级数据集。我们发布数据集和源代码以促进进一步的研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.12210v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics**<br />
**Title_cn:** OK-Robot：集成机器人开放知识模型真正重要的是什么<br />
**Authors:** Peiqi Liu, Yaswanth Orru, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto<br />
**Abstract:** <details><summary>原文: </summary>Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，视觉、语言和机器人领域取得了显着进展。我们现在拥有能够基于语言查询识别物体的视觉模型、能够有效控制移动系统的导航系统以及能够处理各种物体的抓取模型。尽管取得了这些进步，机器人技术的通用应用仍然落后，尽管它们依赖于识别、导航和抓取等基本功能。在本文中，我们采用系统优先的方法来开发一种新的基于开放知识的机器人框架，称为 OK-Robot。通过结合用于物体检测的视觉语言模型 (VLM)、用于运动的导航原语以及用于物体操作的抓取原语，OK-Robot 为拾取和放置操作提供了集成解决方案，无需任何培训。为了评估其性能，我们在 10 个真实家庭环境中运行 OK-Robot。结果表明，OK-Robot 在开放式拾放任务中实现了 58.5% 的成功率，代表了开放词汇移动操作 (OVMM) 领域的最新技术，其性能几乎是之前的 1.8 倍工作。在更干净、整洁的环境中，OK-Robot 的性能提升至 82%。然而，从 OK-Robot 获得的最重要的见解是将 VLM 等开放知识系统与机器人模块相结合时，细微细节的关键作用。我们的实验视频可在我们的网站上观看：https://ok-robot.github.io</details>
**PDF:** <http://arxiv.org/pdf/2401.12202v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses**<br />
**Title_cn:** Broiler-Net：用于家禽舍中肉鸡行为分析​​的深度卷积框架<br />
**Authors:** Tahereh Zarrat Ehsan, Seyed Mehdi Mohtavipour<br />
**Abstract:** <details><summary>原文: </summary>Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms. Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis</details>
**Abstract_cn:** <details><summary>译文: </summary>检测禽舍中的异常对于维持鸡的最佳健康状况、最大限度地减少经济损失和提高盈利能力至关重要。本文提出了一种新颖的实时框架，用于分析散养禽舍中鸡的行为以检测异常行为。具体来说，本研究调查了两种显着的异常现象，即不活跃的肉鸡和挤作一团的行为。所提出的框架包括三个关键步骤：（1）利用最先进的深度学习模型进行鸡检测，（2）使用快速跟踪器模块在连续帧中跟踪单个鸡，以及（3）检测鸡内的异常行为视频流。进行实验研究以评估所提出的算法在准确评估鸡行为方面的功效。结果表明，我们的框架为实时异常检测提供了精确有效的解决方案，有助于及时采取干预措施，以保持鸡的健康并提高家禽养殖场的整体生产力。 Github：https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis</details>
**PDF:** <http://arxiv.org/pdf/2401.12176v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE**<br />
**Title_cn:** 使用多特征非线性典型相关分析和 t-SNE 对土地覆盖图像进行半监督分割<br />
**Authors:** Hong Wei, James Xiao, Yichao Zhang, Xia Hong<br />
**Abstract:** <details><summary>原文: </summary>Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像分割是一项聚类任务，其中每个像素都被分配一个聚类标签。遥感数据通常由多个光谱图像波段组成，其中存在语义上有意义的土地覆盖子区域，并与其他源数据（如可用的激光雷达（光探测和测距）数据）共同注册。这表明，为了考虑像素之间的空间相关性，与每个像素相关联的特征向量可以是表示多个频带和适当的局部补丁的向量化张量。同样，基于像素局部补丁的多种类型的纹理特征也有利于编码局部统计信息和空间变化，而不必逐像素标记大量地面实况，然后训练监督模型，这有时是不切实际的。在这项工作中，通过仅标记少量像素，提出了一种新的半监督分割方法。最初，在所有像素上，在高维特征空间中创建图像数据矩阵。然后，t-SNE 将高维数据投影到 3D 嵌入上。通过使用径向基函数作为输入特征，使用标记数据样本作为中心，与输出类标签配对，引入了一种改进的规范相关分析算法，称为 RBF-CCA，该算法通过小标记数据集。通过 k 均值聚类算法应用针对完整图像获得的相关典型变量。所提出的半监督 RBF-CCA 算法已在多幅遥感多光谱图像上实现，展示了出色的分割结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.12164v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Automated facial recognition system using deep learning for pain assessment in adults with cerebral palsy**<br />
**Title_cn:** 使用深度学习的自动面部识别系统对脑瘫成人患者进行疼痛评估<br />
**Authors:** Álvaro Sabater-Gárriz, F. Xavier Gaya-Morey, José María Buades-Rubio, Cristina Manresa Yee, Pedro Montoya, Inmaculada Riquelme<br />
**Abstract:** <details><summary>原文: </summary>Background: Pain assessment in individuals with neurological conditions, especially those with limited self-report ability and altered facial expressions, presents challenges. Existing measures, relying on direct observation by caregivers, lack sensitivity and specificity. In cerebral palsy, pain is a common comorbidity and a reliable evaluation protocol is crucial. Thus, having an automatic system that recognizes facial expressions could be of enormous help when diagnosing pain in this type of patient.   Objectives: 1) to build a dataset of facial pain expressions in individuals with cerebral palsy, and 2) to develop an automated facial recognition system based on deep learning for pain assessment addressed to this population.   Methods: Ten neural networks were trained on three pain image databases, including the UNBC-McMaster Shoulder Pain Expression Archive Database, the Multimodal Intensity Pain Dataset, and the Delaware Pain Database. Additionally, a curated dataset (CPPAIN) was created, consisting of 109 preprocessed facial pain expression images from individuals with cerebral palsy, categorized by two physiotherapists using the Facial Action Coding System observational scale.   Results: InceptionV3 exhibited promising performance on the CP-PAIN dataset, achieving an accuracy of 62.67% and an F1 score of 61.12%. Explainable artificial intelligence techniques revealed consistent essential features for pain identification across models.   Conclusion: This study demonstrates the potential of deep learning models for robust pain detection in populations with neurological conditions and communication disabilities. The creation of a larger dataset specific to cerebral palsy would further enhance model accuracy, offering a valuable tool for discerning subtle and idiosyncratic pain expressions. The insights gained could extend to other complex neurological conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>背景：患有神经系统疾病的个体的疼痛评估，特别是那些自我报告能力有限和面部表情改变的个体，面临着挑战。现有的措施依赖于护理人员的直接观察，缺乏敏感性和特异性。在脑瘫中，疼痛是一种常见的合并症，可靠的评估方案至关重要。因此，拥有一个识别面部表情的自动系统在诊断此类患者的疼痛时可能会有巨大的帮助。目标：1) 建立脑瘫患者面部疼痛表情数据集，2) 开发基于深度学习的自动面部识别系统，用于针对该人群的疼痛评估。方法：在三个疼痛图像数据库上训练 10 个神经网络，包括 UNBC-McMaster 肩部疼痛表达档案数据库、多模态强度疼痛数据集和特拉华州疼痛数据库。此外，还创建了一个精选数据集 (CPPAIN)，其中包含来自脑瘫患者的 109 张经过预处理的面部疼痛表情图像，由两名物理治疗师使用面部动作编码系统观察量表进行分类。结果：InceptionV3 在 CP-PAIN 数据集上表现出了良好的性能，准确率达到 62.67%，F1 分数达到 61.12%。可解释的人工智能技术揭示了跨模型疼痛识别的一致基本特征。结论：这项研究证明了深度学习模型在神经系统疾病和沟通障碍人群中进行稳健疼痛检测的潜力。创建针对脑瘫的更大数据集将进一步提高模型的准确性，为辨别微妙和特殊的疼痛表达提供有价值的工具。获得的见解可以扩展到其他复杂的神经系统疾病。</details>
**PDF:** <http://arxiv.org/pdf/2401.12161v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games**<br />
**Title_cn:** VRMN-bD：VR 单口互动游戏中沉浸式人类恐惧反应的多模态自然行为数据集<br />
**Authors:** He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll<br />
**Abstract:** <details><summary>原文: </summary>Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.</details>
**Abstract_cn:** <details><summary>译文: </summary>理解和识别情绪是虚拟宇宙时代重要且具有挑战性的问题。在虚拟现实（VR）环境中理解、识别和预测恐惧是人类基本情感之一，在沉浸式游戏开发、场景开发和下一代虚拟人机交互应用中发挥着至关重要的作用。在本文中，我们以 VR 恐怖游戏为媒介，通过收集 23 名玩家的多模态数据（姿势、音频和生理信号）来分析恐惧情绪。我们使用基于 LSTM 的模型来预测恐惧，在 6 级分类（无恐惧和五个不同级别的恐惧）和 2 级分类（无恐惧和恐惧）下，准确率分别为 65.31% 和 90.47%。我们构建了沉浸式人类恐惧反应的多模态自然行为数据集（VRMN-bD），并将其与现有的相关高级数据集进行了比较。结果表明，我们的数据集在收集方法、数据规模和受众范围方面的限制较少。我们在针对 VR 站立交互环境中的恐惧和行为的多模式数据集方面具有独特性和先进性。此外，我们还讨论了这项工作对社区和应用程序的影响。数据集和预训练模型可在 https://github.com/KindOPSTAR/VRMN-bD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12133v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy**<br />
**Title_cn:** 具有消融学习温度能量的分布外检测和应用<br />
**Authors:** Will LeVine, Benjamin Pikus, Jacob Phillips, Berk Norman, Fernando Amat Gil, Sean Hendryx<br />
**Abstract:** <details><summary>原文: </summary>As deep neural networks become adopted in high-stakes domains, it is crucial to be able to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence. Among many others, existing methods use the following two scores to do so without training on any apriori OOD examples: a learned temperature and an energy score. In this paper we introduce Ablated Learned Temperature Energy (or "AbeT" for short), a method which combines these prior methods in novel ways with effective modifications. Due to these contributions, AbeT lowers the False Positive Rate at $95\%$ True Positive Rate (FPR@95) by $35.39\%$ in classification (averaged across all ID and OOD datasets measured) compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to how our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of $5.15\%$ in object detection and both a decrease in FPR@95 of $41.48\%$ and an increase in AUPRC of $34.20\%$ on average in semantic segmentation compared to previous state of the art.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着深度神经网络在高风险领域得到采用，能够识别推理输入何时出现分布外 (OOD) 至关重要，以便用户在高置信度的情况下能够收到性能和校准可能下降的警报。除许多其他方法外，现有方法使用以下两个分数来实现此目的，而无需对任何先验 OOD 示例进行训练：学习温度和能量分数。在本文中，我们介绍了消融学习温度能量（Ablated LearnedTemperature Energy，简称“AbeT”），这种方法以新颖的方式结合了这些现有方法并进行了有效的修改。由于这些贡献，与没有训练网络的现有技术相比，AbeT 将分类中的假阳性率 (FPR@95) 降低了 35.39\%$（测量的所有 ID 和 OOD 数据集的平均值），为 $95\%$ 真阳性率 (FPR@95)处于多个阶段或需要超参数或测试时间向后传递。我们还提供了关于我们的模型如何学习区分分布内 (ID) 和 OOD 样本的经验见解，同时仅通过在训练时接触错误分类的 ID 示例来对 ID 样本进行显式训练。最后，我们展示了我们的方法在对象检测和语义分割中分别识别与 OOD 对象相对应的预测边界框和像素的功效 - 对象检测中的 AUROC 增加了 5.15\%$，而 FPR@95 均减少了与之前的现有技术相比，语义分割在语义分割方面平均增加了 41.48\%$，AUPRC 平均增加了 $34.20\%$。</details>
**PDF:** <http://arxiv.org/pdf/2401.12129v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI**<br />
**Title_cn:** DeepCERES：使用超高分辨率多模态 MRI 进行小脑小叶分割的深度学习方法<br />
**Authors:** Sergio Morell-Ortega, Marina Ruiz-Perez, Marien Gadea, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Mariam de la Iglesia-Vaya, Gwenaelle Catheline, Pierrick Coupé, José V. Manjón<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel multimodal and high-resolution human brain cerebellum lobule segmentation method. Unlike current tools that operate at standard resolution ($1 \text{ mm}^{3}$) or using mono-modal data, the proposed method improves cerebellum lobule segmentation through the use of a multimodal and ultra-high resolution ($0.125 \text{ mm}^{3}$) training dataset. To develop the method, first, a database of semi-automatically labelled cerebellum lobules was created to train the proposed method with ultra-high resolution T1 and T2 MR images. Then, an ensemble of deep networks has been designed and developed, allowing the proposed method to excel in the complex cerebellum lobule segmentation task, improving precision while being memory efficient. Notably, our approach deviates from the traditional U-Net model by exploring alternative architectures. We have also integrated deep learning with classical machine learning methods incorporating a priori knowledge from multi-atlas segmentation, which improved precision and robustness. Finally, a new online pipeline, named DeepCERES, has been developed to make available the proposed method to the scientific community requiring as input only a single T1 MR image at standard resolution.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种新颖的多模态、高分辨率人脑小脑小叶分割方法。与当前以标准分辨率（$1 \text{ mm}^{3}$）或使用单模态数据运行的工具不同，所提出的方法通过使用多模态和超高分辨率（$0.125 \text { mm}^{3}$) 训练数据集。为了开发该方法，首先创建了半自动标记小脑小叶的数据库，以使用超高分辨率 T1 和 T2 MR 图像来训练所提出的方法。然后，设计和开发了一个深度网络集合，使所提出的方法能够在复杂的小脑小叶分割任务中表现出色，提高精度，同时提高内存效率。值得注意的是，我们的方法通过探索替代架构来偏离传统的 U-Net 模型。我们还将深度学习与经典机器学习方法相结合，结合了多图谱分割的先验知识，从而提高了精度和鲁棒性。最后，开发了一种名为 DeepCERES 的新在线管道，以便向科学界提供所提出的方法，仅需要标准分辨率的单个 T1 MR 图像作为输入。</details>
**PDF:** <http://arxiv.org/pdf/2401.12074v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **CloSe: A 3D Clothing Segmentation Dataset and Model**<br />
**Title_cn:** CloSe：3D 服装分割数据集和模型<br />
**Authors:** Dimitrije Antić, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, Gerard Pons-Moll<br />
**Abstract:** <details><summary>原文: </summary>3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 服装建模和数据集在娱乐、动画和数字时尚行业中发挥着至关重要的作用。现有的工作通常缺乏详细的语义理解或使用合成数据集，缺乏现实性和个性化。为了解决这个问题，我们首先引入 CloSe-D：一个新颖的大规模数据集，包含 3167 次扫描的 3D 服装分割，涵盖 18 个不同的服装类别。此外，我们提出了 CloSe-Net，这是第一个基于学习的 3D 服装分割模型，用于对彩色点云进行细粒度分割。 CloSe-Net 使用局部点特征、身体-服装相关性以及基于服装类和点特征的注意模块，相对于基线和之前的工作提高了性能。所提出的注意力模块使我们的模型能够先从数据中学习外观和依赖于几何形状的服装。我们通过成功分割公开的穿着服装的人数据集，进一步验证了我们方法的有效性。我们还介绍了 CloSe-T，一种用于细化分割标签的 3D 交互式工具。在持续学习设置中将该工具与 CloSe-T 相结合，可以提高对现实世界数据的泛化能力。数据集、模型和工具可以在 https://virtual humans.mpi-inf.mpg.de/close3dv24/ 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.12051v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report (Team KuzHum)**<br />
**Title_cn:** HomeRobot 开放词汇移动操作挑战赛 2023 参赛者报告（KuzHum 团队）<br />
**Authors:** Volodymyr Kuzma, Vladyslav Humennyy, Ruslan Partsey<br />
**Abstract:** <details><summary>原文: </summary>We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们报告了 NeurIPS 2023 HomeRobot：开放词汇移动操作 (OVMM) 挑战强化学习基线的改进。更具体地说，我们提出了更准确的语义分割模块，以及更好的位置技能策略和高级启发式算法，其整体成功率比基线高出 2.4%（提高了 7 倍），部分成功率提高了 8.2%（提高了 1.75 倍）挑战数据集的测试标准分割。通过上述增强功能，我们的代理在模拟和现实世界阶段的挑战中都获得了第三名。</details>
**PDF:** <http://arxiv.org/pdf/2401.12048v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling**<br />
**Title_cn:** 看、听、认：角色感知视听字幕<br />
**Authors:** Bruno Korbar, Jaesung Huh, Andrew Zisserman<br />
**Abstract:** <details><summary>原文: </summary>The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}</details>
**Abstract_cn:** <details><summary>译文: </summary>本文的目标是自动生成字符感知字幕。给定视频和最少量的元数据，我们提出了一种视听方法，可以生成完整的对话记录，具有精确的语音时间戳和已识别的说话角色。关键思想是首先使用视听线索为每个角色选择一组高精度的音频样本，然后使用这些样本根据说话者身份对所有语音片段进行分类。值得注意的是，该方法不需要面部检测或跟踪。我们在各种电视情景喜剧中评估了该方法，包括《宋飞正传》、《弗莱泽》和《实习医生风云》。我们预计该系统可用于自动生成字幕，以提高现代流媒体服务上大量视频的可访问性。项目页面：\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}</details>
**PDF:** <http://arxiv.org/pdf/2401.12039v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network**<br />
**Title_cn:** 基于显着性增强特征融合的多尺度 RGB-D 显着目标检测网络<br />
**Authors:** Rui Huang, Qingyi Zhao, Yan Xing, Sihua Gao, Weifeng Xu, Yuxiang Zhang, Wei Fan<br />
**Abstract:** <details><summary>原文: </summary>Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems. However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection. In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection. SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features. Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales. SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales. Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors.</details>
**Abstract_cn:** <details><summary>译文: </summary>多尺度卷积神经网络（CNN）在解决各种视觉问题方面表现出了卓越的能力。然而，融合不同尺度的特征总是会导致模型尺寸过大，阻碍了多尺度 CNN 在 RGB-D 显着性检测中的应用。在本文中，我们提出了一种定制的特征融合模块，称为显着性增强特征融合（SEFF），用于 RGB-D 显着性检测。 SEFF 利用相邻尺度的显着性图来增强融合所需的特征，从而产生更具代表性的融合特征。我们的多尺度 RGB-D 显着性检测器使用 SEFF 并处理具有三种不同尺度的图像。 SEFF用于融合RGB和深度图像的特征，以及不同尺度的解码器的特征。对五个基准数据集的广泛实验证明了我们的方法相对于十个 SOTA 显着性检测器的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11914v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Large receptive field strategy and important feature extraction strategy in 3D object detection**<br />
**Title_cn:** 3D物体检测中的大感受野策略和重要特征提取策略<br />
**Authors:** Leichao Cui, Xiuxian Li, Min Meng<br />
**Abstract:** <details><summary>原文: </summary>The enhancement of 3D object detection is pivotal for precise environmental perception and improved task execution capabilities in autonomous driving. LiDAR point clouds, offering accurate depth information, serve as a crucial information for this purpose. Our study focuses on key challenges in 3D target detection. To tackle the challenge of expanding the receptive field of a 3D convolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM). This module achieves adaptive expansion of the 3D convolutional kernel's receptive field, balancing the expansion with acceptable computational loads. This innovation reduces operations, expands the receptive field, and allows the model to dynamically adjust to different object requirements. Simultaneously, we identify redundant information in 3D features. Employing the Feature Selection Module (FSM) quantitatively evaluates and eliminates non-important features, achieving the separation of output box fitting and feature extraction. This innovation enables the detector to focus on critical features, resulting in model compression, reduced computational burden, and minimized candidate frame interference. Extensive experiments confirm that both DFFM and FSM not only enhance current benchmarks, particularly in small target detection, but also accelerate network performance. Importantly, these modules exhibit effective complementarity.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 物体检测的增强对于自动驾驶中精确的环境感知和提高任务执行能力至关重要。激光雷达点云提供准确的深度信息，是实现这一目的的关键信息。我们的研究重点是 3D 目标检测中的关键挑战。为了应对扩展 3D 卷积核感受野的挑战，我们引入了动态特征融合模块（DFFM）。该模块实现了 3D 卷积核感受野的自适应扩展，平衡扩展与可接受的计算负载。这一创新减少了操作，扩大了感受野，并允许模型动态调整以适应不同的对象要求。同时，我们识别 3D 特征中的冗余信息。采用特征选择模块（FSM）定量评估并剔除不重要特征，实现输出框拟合和特征提取的分离。这项创新使检测器能够专注于关键特征，从而实现模型压缩、减少计算负担并最大限度地减少候选帧干扰。大量实验证实，DFFM 和 FSM 不仅增强了当前的基准测试，特别是在小目标检测方面，而且还提高了网络性能。重要的是，这些模块表现出有效的互补性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11913v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Evaluating the Feasibility of Standard Facial Expression Recognition in Individuals with Moderate to Severe Intellectual Disabilities**<br />
**Title_cn:** 评估标准面部表情识别对中度至重度智力障碍个体的可行性<br />
**Authors:** F. Xavier Gaya-Morey, Silvia Ramis, Jose M. Buades-Rubio, Cristina Manresa-Yee<br />
**Abstract:** <details><summary>原文: </summary>Recent research has underscored the increasing preference of users for human-like interactions with machines. Consequently, facial expression recognition has gained significance as a means of imparting social robots with the capacity to discern the emotional states of users. In this investigation, we assess the suitability of deep learning approaches, known for their remarkable performance in this domain, for recognizing facial expressions in individuals with intellectual disabilities, which has not been yet studied in the literature, to the best of our knowledge. To address this objective, we train a set of twelve distinct convolutional neural networks in different approaches, including an ensemble of datasets without individuals with intellectual disabilities and a dataset featuring such individuals. Our examination of the outcomes achieved by the various models under distinct training conditions, coupled with a comprehensive analysis of critical facial regions during expression recognition facilitated by explainable artificial intelligence techniques, revealed significant distinctions in facial expressions between individuals with and without intellectual disabilities, as well as among individuals with intellectual disabilities. Remarkably, our findings demonstrate the feasibility of facial expression recognition within this population through tailored user-specific training methodologies, which enable the models to effectively address the unique expressions of each user.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究强调了用户越来越倾向于与机器进行类人交互。因此，面部表情识别作为赋予社交机器人辨别用户情绪状态能力的一种手段，具有重要意义。在这项调查中，我们评估了深度学习方法的适用性，该方法以其在该领域的卓越表现而闻名，用于识别智障人士的面部表情，据我们所知，尚未在文献中进行过研究。为了实现这一目标，我们用不同的方法训练了一组十二个不同的卷积神经网络，包括一组没有智障人士的数据集和一个包含智障人士的数据集。我们对各种模型在不同训练条件下取得的结果进行了检查，再加上可解释的人工智能技术促进的表情识别过程中关键面部区域的全面分析，揭示了智力障碍者和非智力障碍者之间面部表情的显着差异。就像智力障碍人士一样。值得注意的是，我们的研究结果证明了通过针对特定用户量身定制的训练方法在该人群中进行面部表情识别的可行性，这使得模型能够有效地解决每个用户的独特表情。</details>
**PDF:** <http://arxiv.org/pdf/2401.11877v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Detect-Order-Construct: A Tree Construction based Approach for Hierarchical Document Structure Analysis**<br />
**Title_cn:** 检测-顺序-构造：一种基于树构造的分层文档结构分析方法<br />
**Authors:** Jiawei Wang, Kai Hu, Zhuoyao Zhong, Lei Sun, Qiang Huo<br />
**Abstract:** <details><summary>原文: </summary>Document structure analysis (aka document layout analysis) is crucial for understanding the physical layout and logical structure of documents, with applications in information retrieval, document summarization, knowledge extraction, etc. In this paper, we concentrate on Hierarchical Document Structure Analysis (HDSA) to explore hierarchical relationships within structured documents created using authoring software employing hierarchical schemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze hierarchical document structures, we propose a tree construction based approach that addresses multiple subtasks concurrently, including page object detection (Detect), reading order prediction of identified objects (Order), and the construction of intended hierarchical structure (Construct). We present an effective end-to-end solution based on this framework to demonstrate its performance. To assess our approach, we develop a comprehensive benchmark called Comp-HRDoc, which evaluates the above subtasks simultaneously. Our end-to-end system achieves state-of-the-art performance on two large-scale document layout analysis datasets (PubLayNet and DocLayNet), a high-quality hierarchical document structure reconstruction dataset (HRDoc), and our Comp-HRDoc benchmark. The Comp-HRDoc benchmark will be released to facilitate further research in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>文档结构分析（又名文档布局分析）对于理解文档的物理布局和逻辑结构至关重要，可应用于信息检索、文档摘要、知识提取等。在本文中，我们重点关注分层文档结构分析（HDSA）探索使用采用分层模式的创作软件（例如 LaTeX、Microsoft Word 和 HTML）创建的结构化文档中的分层关系。为了全面分析层次文档结构，我们提出了一种基于树结构的方法，该方法可以同时处理多个子任务，包括页面对象检测（Detect）、识别对象的阅读顺序预测（Order）以及构建预期的层次结构（Construct）。我们提出了一个基于该框架的有效的端到端解决方案来展示其性能。为了评估我们的方法，我们开发了一个名为 Comp-HRDoc 的综合基准，它同时评估上述子任务。我们的端到端系统在两个大型文档布局分析数据集（PubLayNet 和 DocLayNet）、高质量分层文档结构重建数据集（HRDoc）以及我们的 Comp-HRDoc 基准上实现了最先进的性能。 Comp-HRDoc 基准测试将发布，以促进该领域的进一步研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.11874v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation**<br />
**Title_cn:** MOSformer：用于医学图像分割的基于动量编码器的层间融合变压器<br />
**Authors:** De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Xiu-Ling Liu, Zeng-Guang Hou<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation takes an important position in various clinical applications. Deep learning has emerged as the predominant solution for automated segmentation of volumetric medical images. 2.5D-based segmentation models bridge computational efficiency of 2D-based models and spatial perception capabilities of 3D-based models. However, prevailing 2.5D-based models often treat each slice equally, failing to effectively learn and exploit inter-slice information, resulting in suboptimal segmentation performances. In this paper, a novel Momentum encoder-based inter-slice fusion transformer (MOSformer) is proposed to overcome this issue by leveraging inter-slice information at multi-scale feature maps extracted by different encoders. Specifically, dual encoders are employed to enhance feature distinguishability among different slices. One of the encoders is moving-averaged to maintain the consistency of slice representations. Moreover, an IF-Swin transformer module is developed to fuse inter-slice multi-scale features. The MOSformer is evaluated on three benchmark datasets (Synapse, ACDC, and AMOS), establishing a new state-of-the-art with 85.63%, 92.19%, and 85.43% of DSC, respectively. These promising results indicate its competitiveness in medical image segmentation. Codes and models of MOSformer will be made publicly available upon acceptance.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分割在各种临床应用中占有重要地位。深度学习已成为体积医学图像自动分割的主要解决方案。基于 2.5D 的分割模型将基于 2D 的模型的计算效率和基于 3D 的模型的空间感知能力结合起来。然而，流行的基于 2.5D 的模型通常平等地对待每个切片，无法有效地学习和利用切片间信息，导致分割性能不佳。本文提出了一种新型的基于动量编码器的片间融合变压器（MOSformer），通过利用不同编码器提取的多尺度特征图的片间信息来克服这个问题。具体来说，采用双编码器来增强不同切片之间的特征可区分性。其中一个编码器是移动平均的，以保持切片表示的一致性。此外，还开发了 IF-Swin 变压器模块来融合片间多尺度特征。 MOSformer 在三个基准数据集（Synapse、ACDC 和 AMOS）上进行了评估，分别以 DSC 的 85.63%、92.19% 和 85.43% 建立了新的最先进水平。这些有希望的结果表明了其在医学图像分割方面的竞争力。 MOSformer 的代码和模型将在接受后公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.11856v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning**<br />
**Title_cn:** SignVTCL：通过视觉文本对比学习增强多模式连续手语识别<br />
**Authors:** Hao Chen, Jiaze Wang, Ziyu Guo, Jinpeng Li, Donghao Zhou, Bian Wu, Chenyong Guan, Guangyong Chen, Pheng-Ann Heng<br />
**Abstract:** <details><summary>原文: </summary>Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community. SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment. Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets. To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model. SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations. Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence. Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>手语识别 (SLR) 在促进听力障碍社区的沟通方面发挥着至关重要的作用。 SLR 是一项弱监督任务，整个视频都用注释进行注释，因此很难识别视频片段中相应的注释。最近的研究表明，SLR 的主要瓶颈是大规模数据集的可用性有限导致的训练不足。为了应对这一挑战，我们提出了 SignVTCL，这是一种通过视觉文本对比学习增强的多模态连续手语识别框架，它充分利用了多模态数据的潜力和语言模型的泛化能力。 SignVTCL 同时集成多模态数据（视频、关键点和光流）来训练统一的视觉主干，从而产生更强大的视觉表示。此外，SignVTCL 包含一种视觉文本对齐方法，结合注释级别和句子级别对齐，以确保视觉特征和注释在单个注释和句子级别上的精确对应。在 Phoenix-2014、Phoenix-2014T 和 CSL-Daily 三个数据集上进行的实验结果表明，与之前的方法相比，SignVTCL 取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.11847v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Unveiling the Human-like Similarities of Automatic Facial Expression Recognition: An Empirical Exploration through Explainable AI**<br />
**Title_cn:** 揭示自动面部表情识别的类人相似性：通过可解释的人工智能进行实证探索<br />
**Authors:** F. Xavier Gaya-Morey, Silvia Ramis-Guarinos, Cristina Manresa-Yee, Jose M. Buades-Rubio<br />
**Abstract:** <details><summary>原文: </summary>Facial expression recognition is vital for human behavior analysis, and deep learning has enabled models that can outperform humans. However, it is unclear how closely they mimic human processing. This study aims to explore the similarity between deep neural networks and human perception by comparing twelve different networks, including both general object classifiers and FER-specific models. We employ an innovative global explainable AI method to generate heatmaps, revealing crucial facial regions for the twelve networks trained on six facial expressions. We assess these results both quantitatively and qualitatively, comparing them to ground truth masks based on Friesen and Ekman's description and among them. We use Intersection over Union (IoU) and normalized correlation coefficients for comparisons. We generate 72 heatmaps to highlight critical regions for each expression and architecture. Qualitatively, models with pre-trained weights show more similarity in heatmaps compared to those without pre-training. Specifically, eye and nose areas influence certain facial expressions, while the mouth is consistently important across all models and expressions. Quantitatively, we find low average IoU values (avg. 0.2702) across all expressions and architectures. The best-performing architecture averages 0.3269, while the worst-performing one averages 0.2066. Dendrograms, built with the normalized correlation coefficient, reveal two main clusters for most expressions: models with pre-training and models without pre-training. Findings suggest limited alignment between human and AI facial expression recognition, with network architectures influencing the similarity, as similar architectures prioritize similar facial regions.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部表情识别对于人类行为分析至关重要，深度学习使模型能够超越人类。然而，尚不清楚它们模仿人类处理的程度如何。本研究旨在通过比较 12 种不同的网络（包括通用对象分类器和 FER 特定模型）来探索深度神经网络和人类感知之间的相似性。我们采用创新的全局可解释人工智能方法来生成热图，揭示受六种面部表情训练的十二个网络的关键面部区域。我们对这些结果进行定量和定性评估，将它们与基于 Friesen 和 Ekman 的描述以及其中的真实掩模进行比较。我们使用交并并集（IoU）和归一化相关系数进行比较。我们生成 72 个热图来突出显示每个表达式和架构的关键区域。定性地讲，与没有预训练的模型相比，具有预训练权重的模型在热图中表现出更多的相似性。具体来说，眼睛和鼻子区域会影响某些面部表情，而嘴巴在所有模型和表情中始终很重要。定量地，我们发现所有表达式和架构的平均 IoU 值较低（平均 0.2702）。性能最好的架构平均为 0.3269，而性能最差的架构平均为 0.2066。使用归一化相关系数构建的树状图揭示了大多数表达式的两个主要簇：经过预训练的模型和未经预训练的模型。研究结果表明，人类和人工智能面部表情识别之间的一致性有限，网络架构会影响相似性，因为相似的架构会优先考虑相似的面部区域。</details>
**PDF:** <http://arxiv.org/pdf/2401.11835v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Rethinking Centered Kernel Alignment in Knowledge Distillation**<br />
**Title_cn:** 重新思考知识蒸馏中的中心内核对齐<br />
**Authors:** Zikai Zhou, Yunhang Shen, Shitong Shao, Huanran Chen, Linrui Gong, Shaohui Lin<br />
**Abstract:** <details><summary>原文: </summary>Knowledge distillation has emerged as a highly effective method for bridging the representation discrepancy between large-scale models and lightweight models. Prevalent approaches involve leveraging appropriate metrics to minimize the divergence or distance between the knowledge extracted from the teacher model and the knowledge learned by the student model. Centered Kernel Alignment (CKA) is widely used to measure representation similarity and has been applied in several knowledge distillation methods. However, these methods are complex and fail to uncover the essence of CKA, thus not answering the question of how to use CKA to achieve simple and effective distillation properly. This paper first provides a theoretical perspective to illustrate the effectiveness of CKA, which decouples CKA to the upper bound of Maximum Mean Discrepancy~(MMD) and a constant term. Drawing from this, we propose a novel Relation-Centered Kernel Alignment~(RCKA) framework, which practically establishes a connection between CKA and MMD. Furthermore, we dynamically customize the application of CKA based on the characteristics of each task, with less computational source yet comparable performance than the previous methods. The extensive experiments on the CIFAR-100, ImageNet-1k, and MS-COCO demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs for image classification and object detection, validating the effectiveness of our approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识蒸馏已成为弥合大规模模型和轻量级模型之间表示差异的高效方法。流行的方法涉及利用适当的指标来最小化从教师模型提取的知识与学生模型学到的知识之间的分歧或距离。中心核对齐（CKA）广泛用于测量表示相似性，并已应用于多种知识蒸馏方法中。但这些方法比较复杂，未能揭示CKA的本质，无法正确回答如何利用CKA实现简单有效的蒸馏的问题。本文首先提供了一个理论视角来说明 CKA 的有效性，它将 CKA 解耦到最大平均差异~（MMD）的上限和常数项。由此，我们提出了一种新颖的以关系为中心的内核对齐（RCKA）框架，它实际上在CKA和MMD之间建立了联系。此外，我们根据每个任务的特点动态定制CKA的应用，与以前的方法相比，计算资源更少，但性能相当。在 CIFAR-100、ImageNet-1k 和 MS-COCO 上进行的广泛实验表明，我们的方法在几乎所有师生对的图像分类和目标检测上实现了最先进的性能，验证了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11824v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Symbrain: A large-scale dataset of MRI images for neonatal brain symmetry analysis**<br />
**Title_cn:** Symbrain：用于新生儿大脑对称性分析的大规模 MRI 图像数据集<br />
**Authors:** Arnaud Gucciardi, Safouane El Ghazouali, Francesca Venturini, Vida Groznik, Umberto Michelucci<br />
**Abstract:** <details><summary>原文: </summary>This paper presents an annotated dataset of brain MRI images designed to advance the field of brain symmetry study. Magnetic resonance imaging (MRI) has gained interest in analyzing brain symmetry in neonatal infants, and challenges remain due to the vast size differences between fetal and adult brains. Classification methods for brain structural MRI use scales and visual cues to assess hemisphere symmetry, which can help diagnose neonatal patients by comparing hemispheres and anatomical regions of interest in the brain. Using the Developing Human Connectome Project dataset, this work presents a dataset comprising cerebral images extracted as slices across selected portions of interest for clinical evaluation . All the extracted images are annotated with the brain's midline. All the extracted images are annotated with the brain's midline. From the assumption that a decrease in symmetry is directly related to possible clinical pathologies, the dataset can contribute to a more precise diagnosis because it can be used to train deep learning model application in neonatal cerebral MRI anomaly detection from postnatal infant scans thanks to computer vision. Such models learn to identify and classify anomalies by identifying potential asymmetrical patterns in medical MRI images. Furthermore, this dataset can contribute to the research and development of methods using the relative symmetry of the two brain hemispheres for crucial diagnosis and treatment planning.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一个带注释的脑 MRI 图像数据集，旨在推进脑对称性研究领域。磁共振成像（MRI）在分析新生儿大脑对称性方面引起了人们的兴趣，但由于胎儿和成人大脑之间存在巨大的尺寸差异，挑战仍然存在。脑结构 MRI 的分类方法使用尺度和视觉线索来评估半球对称性，这可以通过比较大脑半球和感兴趣的解剖区域来帮助诊断新生儿患者。这项工作使用开发人类连接组项目数据集，提出了一个数据集，其中包含提取为跨选定感兴趣部分的切片的大脑图像，用于临床评估。所有提取的图像都用大脑中线注释。所有提取的图像都用大脑中线注释。假设对称性降低与可能的临床病理直接相关，该数据集可以有助于更精确的诊断，因为它可以用于训练深度学习模型应用，通过计算机视觉从产后婴儿扫描中检测新生儿脑 MRI 异常。此类模型通过识别医学 MRI 图像中潜在的不对称模式来学习识别和分类异常。此外，该数据集有助于研究和开发利用两个大脑半球的相对对称性进行关键诊断和治疗计划的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11814v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation**<br />
**Title_cn:** SemPLeS：弱监督语义分割的语义提示学习<br />
**Authors:** Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen<br />
**Abstract:** <details><summary>原文: </summary>Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Class-associated Semantic Refinement to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. In this way, our proposed framework is able to perform better semantic matching between object regions and the associated text labels, resulting in desired pseudo masks for training the segmentation model. The proposed SemPLeS framework achieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the semantic visualization of our learned prompts. The codes will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>弱监督语义分割（WSSS）旨在使用仅具有图像级监督的训练图像数据来训练分割模型。由于无法获得精确的像素级注释，现有方法通常侧重于通过细化类似 CAM 的热图来生成用于训练分割模型的伪掩模。然而，生成的热图可能仅捕获目标对象类别或相关联的共现背景的辨别图像区域。为了解决这些问题，我们提出了一种WSSS语义提示学习（SemPLeS）框架，该框架学习有效提示CLIP空间以增强分割区域和目标对象类别之间的语义对齐。更具体地说，我们提出对比提示学习和类相关语义细化来学习充分描述和抑制与每个目标对象类别相关的图像背景的提示。通过这种方式，我们提出的框架能够在对象区域和相关文本标签之间执行更好的语义匹配，从而产生用于训练分割模型的所需伪掩模。所提出的 SemPLeS 框架在标准 WSSS 基准、PASCAL VOC 和 MS COCO 上实现了 SOTA 性能，并通过我们学习的提示的语义可视化展示了可解释性。代码将被释放。</details>
**PDF:** <http://arxiv.org/pdf/2401.11791v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Deep Learning for Computer Vision based Activity Recognition and Fall Detection of the Elderly: a Systematic Review**<br />
**Title_cn:** 基于计算机视觉的深度学习老年人活动识别和跌倒检测：系统综述<br />
**Authors:** F. Xavier Gaya-Morey, Cristina Manresa-Yee, Jose M. Buades-Rubio<br />
**Abstract:** <details><summary>原文: </summary>As the percentage of elderly people in developed countries increases worldwide, the healthcare of this collective is a worrying matter, especially if it includes the preservation of their autonomy. In this direction, many studies are being published on Ambient Assisted Living (AAL) systems, which help to reduce the preoccupations raised by the independent living of the elderly. In this study, a systematic review of the literature is presented on fall detection and Human Activity Recognition (HAR) for the elderly, as the two main tasks to solve to guarantee the safety of elderly people living alone. To address the current tendency to perform these two tasks, the review focuses on the use of Deep Learning (DL) based approaches on computer vision data. In addition, different collections of data like DL models, datasets or hardware (e.g. depth or thermal cameras) are gathered from the reviewed studies and provided for reference in future studies. Strengths and weaknesses of existing approaches are also discussed and, based on them, our recommendations for future works are provided.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着全球发达国家老年人口比例的增加，这个群体的医疗保健是一个令人担忧的问题，特别是如果它包括保护他们的自主权的话。在这个方向上，许多关于环境辅助生活（AAL）系统的研究正在发表，这有助于减少老年人独立生活引起的关注。本研究对老年人跌倒检测和人类活动识别（HAR）的文献进行了系统回顾，作为保障独居老年人安全需要解决的两个主要任务。为了解决当前执行这两项任务的趋势，本次审查重点关注在计算机视觉数据上使用基于深度学习 (DL) 的方法。此外，还从已审查的研究中收集了不同的数据集合，例如深度学习模型、数据集或硬件（例如深度相机或热感相机），并为未来的研究提供参考。还讨论了现有方法的优点和缺点，并在此基础上提出了我们对未来工作的建议。</details>
**PDF:** <http://arxiv.org/pdf/2401.11790v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Collaborative Position Reasoning Network for Referring Image Segmentation**<br />
**Title_cn:** 用于参考图像分割的协作位置推理网络<br />
**Authors:** Jianjian Cao, Beiya Dai, Yulin Li, Xiameng Qin, Jingdong Wang<br />
**Abstract:** <details><summary>原文: </summary>Given an image and a natural language expression as input, the goal of referring image segmentation is to segment the foreground masks of the entities referred by the expression. Existing methods mainly focus on interactive learning between vision and language to enhance the multi-modal representations for global context reasoning. However, predicting directly in pixel-level space can lead to collapsed positioning and poor segmentation results. Its main challenge lies in how to explicitly model entity localization, especially for non-salient entities. In this paper, we tackle this problem by executing a Collaborative Position Reasoning Network (CPRN) via the proposed novel Row-and-Column interactive (RoCo) and Guided Holistic interactive (Holi) modules. Specifically, RoCo aggregates the visual features into the row- and column-wise features corresponding two directional axes respectively. It offers a fine-grained matching behavior that perceives the associations between the linguistic features and two decoupled visual features to perform position reasoning over a hierarchical space. Holi integrates features of the two modalities by a cross-modal attention mechanism, which suppresses the irrelevant redundancy under the guide of positioning information from RoCo. Thus, with the incorporation of RoCo and Holi modules, CPRN captures the visual details of position reasoning so that the model can achieve more accurate segmentation. To our knowledge, this is the first work that explicitly focuses on position reasoning modeling. We also validate the proposed method on three evaluation datasets. It consistently outperforms existing state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>给定图像和自然语言表达式作为输入，引用图像分割的目标是分割表达式引用的实体的前景掩模。现有方法主要侧重于视觉和语言之间的交互学习，以增强全局上下文推理的多模态表示。然而，直接在像素级空间中进行预测可能会导致定位崩溃和分割结果不佳。它的主要挑战在于如何显式地建模实体本地化，特别是对于非显着实体。在本文中，我们通过提出的新颖的行列交互（RoCo）和引导整体交互（Holi）模块执行协作位置推理网络（CPRN）来解决这个问题。具体来说，RoCo将视觉特征聚合成分别对应两个方向轴的行和列特征。它提供了一种细粒度的匹配行为，可以感知语言特征和两个解耦的视觉特征之间的关联，以在分层空间上执行位置推理。 Holi通过跨模态注意机制整合了两种模态的特征，在RoCo定位信息的指导下抑制了不相关的冗余。因此，通过RoCo和Holi模块的结合，CPRN捕获位置推理的视觉细节，使模型能够实现更准确的分割。据我们所知，这是第一个明确关注位置推理建模的工作。我们还在三个评估数据集上验证了所提出的方法。它始终优于现有的最先进方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.11775v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Concealed Object Segmentation with Hierarchical Coherence Modeling**<br />
**Title_cn:** 使用分层一致性建模的隐藏对象分割<br />
**Authors:** Fengyang Xiao, Pan Zhang, Chunming He, Runze Hu, Yutao Liu<br />
**Abstract:** <details><summary>原文: </summary>Concealed object segmentation (COS) is a challenging task that involves localizing and segmenting those concealed objects that are visually blended with their surrounding environments. Despite achieving remarkable success, existing COS segmenters still struggle to achieve complete segmentation results in extremely concealed scenarios. In this paper, we propose a Hierarchical Coherence Modeling (HCM) segmenter for COS, aiming to address this incomplete segmentation limitation. In specific, HCM promotes feature coherence by leveraging the intra-stage coherence and cross-stage coherence modules, exploring feature correlations at both the single-stage and contextual levels. Additionally, we introduce the reversible re-calibration decoder to detect previously undetected parts in low-confidence regions, resulting in further enhancing segmentation performance. Extensive experiments conducted on three COS tasks, including camouflaged object detection, polyp image segmentation, and transparent object detection, demonstrate the promising results achieved by the proposed HCM segmenter.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐藏对象分割（COS）是一项具有挑战性的任务，涉及定位和分割那些在视觉上与周围环境融合的隐藏对象。尽管取得了显着的成功，现有的 COS 分割器仍然难以在极其隐蔽的场景下获得完整的分割结果。在本文中，我们提出了一种用于 COS 的分层一致性建模（HCM）分割器，旨在解决这种不完整的分割限制。具体来说，HCM 通过利用阶段内一致性和跨阶段一致性模块来促进特征一致性，探索单阶段和上下文级别的特征相关性。此外，我们引入了可逆重新校准解码器来检测低置信区域中以前未检测到的部分，从而进一步增强分割性能。对三个 COS 任务进行的广泛实验，包括伪装目标检测、息肉图像分割和透明目标检测，证明了所提出的 HCM 分割器取得的有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.11767v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models**<br />
**Title_cn:** EmerDiff：扩散模型中新兴的像素级语义知识<br />
**Authors:** Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, Seung Wook Kim<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have recently received increasing research attention for their remarkable transfer abilities in semantic segmentation tasks. However, generating fine-grained segmentation masks with diffusion models often requires additional training on annotated datasets, leaving it unclear to what extent pre-trained diffusion models alone understand the semantic relations of their generated images. To address this question, we leverage the semantic knowledge extracted from Stable Diffusion (SD) and aim to develop an image segmentor capable of generating fine-grained segmentation maps without any additional training. The primary difficulty stems from the fact that semantically meaningful feature maps typically exist only in the spatially lower-dimensional layers, which poses a challenge in directly extracting pixel-level semantic relations from these feature maps. To overcome this issue, our framework identifies semantic correspondences between image pixels and spatial locations of low-dimensional feature maps by exploiting SD's generation process and utilizes them for constructing image-resolution segmentation maps. In extensive experiments, the produced segmentation maps are demonstrated to be well delineated and capture detailed parts of the images, indicating the existence of highly accurate pixel-level semantic knowledge in diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型最近因其在语义分割任务中卓越的迁移能力而受到越来越多的研究关注。然而，使用扩散模型生成细粒度的分割掩模通常需要对带注释的数据集进行额外的训练，因此不清楚预训练的扩散模型在多大程度上单独理解其生成图像的语义关系。为了解决这个问题，我们利用从稳定扩散（SD）中提取的语义知识，旨在开发一种能够生成细粒度分割图的图像分割器，而无需任何额外的训练。主要困难源于这样一个事实：语义上有意义的特征图通常只存在于空间较低维的层中，这对从这些特征图中直接提取像素级语义关系提出了挑战。为了克服这个问题，我们的框架通过利用 SD 的生成过程来识别图像像素和低维特征图的空间位置之间的语义对应关系，并利用它们来构建图像分辨率分割图。在大量的实验中，所生成的分割图被证明可以很好地描绘并捕获图像的详细部分，这表明扩散模型中存在高度准确的像素级语义知识。</details>
**PDF:** <http://arxiv.org/pdf/2401.11739v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **MetaSeg: Content-Aware Meta-Net for Omni-Supervised Semantic Segmentation**<br />
**Title_cn:** MetaSeg：用于全监督语义分割的内容感知元网络<br />
**Authors:** Shenwang Jiang, Jianan Li, Ying Wang, Wenxuan Wu, Jizhou Zhang, Bo Huang, Tingfa Xu<br />
**Abstract:** <details><summary>原文: </summary>Noisy labels, inevitably existing in pseudo segmentation labels generated from weak object-level annotations, severely hampers model optimization for semantic segmentation. Previous works often rely on massive hand-crafted losses and carefully-tuned hyper-parameters to resist noise, suffering poor generalization capability and high model complexity. Inspired by recent advances in meta learning, we argue that rather than struggling to tolerate noise hidden behind clean labels passively, a more feasible solution would be to find out the noisy regions actively, so as to simply ignore them during model optimization. With this in mind, this work presents a novel meta learning based semantic segmentation method, MetaSeg, that comprises a primary content-aware meta-net (CAM-Net) to sever as a noise indicator for an arbitrary segmentation model counterpart. Specifically, CAM-Net learns to generate pixel-wise weights to suppress noisy regions with incorrect pseudo labels while highlighting clean ones by exploiting hybrid strengthened features from image content, providing straightforward and reliable guidance for optimizing the segmentation model. Moreover, to break the barrier of time-consuming training when applying meta learning to common large segmentation models, we further present a new decoupled training strategy that optimizes different model layers in a divide-and-conquer manner. Extensive experiments on object, medical, remote sensing and human segmentation shows that our method achieves superior performance, approaching that of fully supervised settings, which paves a new promising way for omni-supervised semantic segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>由弱对象级注释生成的伪分割标签中不可避免地存在噪声标签，严重阻碍了语义分割的模型优化。以前的工作通常依赖大量的手工损失和精心调整的超参数来抵抗噪声，泛化能力差且模型复杂度高。受元学习最新进展的启发，我们认为，与其被动地忍受隐藏在干净标签后面的噪声，更可行的解决方案是主动找出噪声区域，以便在模型优化过程中忽略它们。考虑到这一点，这项工作提出了一种新颖的基于元学习的语义分割方法 MetaSeg，该方法包括一个主要内容感知元网络（CAM-Net），用作任意分割模型对应物的噪声指示器。具体来说，CAM-Net 学习生成像素级权重，以抑制具有不正确伪标签的噪声区域，同时通过利用图像内容的混合强化特征来突出显示干净的区域，为优化分割模型提供直接而可靠的指导。此外，为了打破将元学习应用于常见大型分割模型时耗时训练的障碍，我们进一步提出了一种新的解耦训练策略，以分而治之的方式优化不同模型层。在物体、医学、遥感和人体分割方面的大量实验表明，我们的方法取得了优异的性能，接近完全监督的设置，这为全监督语义分割开辟了一条新的有前景的道路。</details>
**PDF:** <http://arxiv.org/pdf/2401.11738v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey**<br />
**Title_cn:** 深度学习时代的结直肠息肉分割：综合调查<br />
**Authors:** Zhenyu Wu, Fengmao Lv, Chenglizhao Chen, Aimin Hao, Shuo Li<br />
**Abstract:** <details><summary>原文: </summary>Colorectal polyp segmentation (CPS), an essential problem in medical image analysis, has garnered growing research attention. Recently, the deep learning-based model completely overwhelmed traditional methods in the field of CPS, and more and more deep CPS methods have emerged, bringing the CPS into the deep learning era. To help the researchers quickly grasp the main techniques, datasets, evaluation metrics, challenges, and trending of deep CPS, this paper presents a systematic and comprehensive review of deep-learning-based CPS methods from 2014 to 2023, a total of 115 technical papers. In particular, we first provide a comprehensive review of the current deep CPS with a novel taxonomy, including network architectures, level of supervision, and learning paradigm. More specifically, network architectures include eight subcategories, the level of supervision comprises six subcategories, and the learning paradigm encompasses 12 subcategories, totaling 26 subcategories. Then, we provided a comprehensive analysis the characteristics of each dataset, including the number of datasets, annotation types, image resolution, polyp size, contrast values, and polyp location. Following that, we summarized CPS's commonly used evaluation metrics and conducted a detailed analysis of 40 deep SOTA models, including out-of-distribution generalization and attribute-based performance analysis. Finally, we discussed deep learning-based CPS methods' main challenges and opportunities.</details>
**Abstract_cn:** <details><summary>译文: </summary>结直肠息肉分割（CPS）是医学图像分析中的一个基本问题，已经引起了越来越多的研究关注。近年来，基于深度学习的模型在CPS领域完全压倒了传统方法，越来越多的深度CPS方法涌现，将CPS带入了深度学习时代。为了帮助研究人员快速掌握深度CPS的主要技术、数据集、评估指标、挑战和趋势，本文对2014年至2023年基于深度学习的CPS方法进行了系统、全面的回顾，共115篇技术论文。特别是，我们首先通过新颖的分类法对当前的深度 CPS 进行了全面的回顾，包括网络架构、监督级别和学习范式。更具体地说，网络架构包括8个子类别，监督级别包括6个子类别，学习范式包括12个子类别，总共26个子类别。然后，我们对每个数据集的特征进行了全面的分析，包括数据集的数量、注释类型、图像分辨率、息肉大小、对比度值和息肉位置。接下来，我们总结了CPS常用的评估指标，并对40个深度SOTA模型进行了详细分析，包括分布外泛化和基于属性的性能分析。最后，我们讨论了基于深度学习的 CPS 方法的主要挑战和机遇。</details>
**PDF:** <http://arxiv.org/pdf/2401.11734v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport**<br />
**Title_cn:** 通过具有最佳传输的条件分布熵检测分布外样本<br />
**Authors:** Chuanwen Feng, Wenlong Chen, Ao Ke, Yilong Ren, Xike Xie, S. Kevin Zhou<br />
**Abstract:** <details><summary>原文: </summary>When deploying a trained machine learning model in the real world, it is inevitable to receive inputs from out-of-distribution (OOD) sources. For instance, in continual learning settings, it is common to encounter OOD samples due to the non-stationarity of a domain. More generally, when we have access to a set of test inputs, the existing rich line of OOD detection solutions, especially the recent promise of distance-based methods, falls short in effectively utilizing the distribution information from training samples and test inputs. In this paper, we argue that empirical probability distributions that incorporate geometric information from both training samples and test inputs can be highly beneficial for OOD detection in the presence of test inputs available. To address this, we propose to model OOD detection as a discrete optimal transport problem. Within the framework of optimal transport, we propose a novel score function known as the \emph{conditional distribution entropy} to quantify the uncertainty of a test input being an OOD sample. Our proposal inherits the merits of certain distance-based methods while eliminating the reliance on distribution assumptions, a-prior knowledge, and specific training mechanisms. Extensive experiments conducted on benchmark datasets demonstrate that our method outperforms its competitors in OOD detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>在现实世界中部署经过训练的机器学习模型时，不可避免地要接收来自分布外（OOD）源的输入。例如，在持续学习环境中，由于域的非平稳性，经常会遇到 OOD 样本。更一般地说，当我们能够访问一组测试输入时，现有丰富的 OOD 检测解决方案，尤其是最近承诺的基于距离的方法，在有效利用来自训练样本和测试输入的分布信息方面存在不足。在本文中，我们认为，在存在可用测试输入的情况下，结合来自训练样本和测试输入的几何信息的经验概率分布对于 OOD 检测非常有益。为了解决这个问题，我们建议将 OOD 检测建模为离散最优传输问题。在最优传输的框架内，我们提出了一种称为 \emph{条件分布熵} 的新颖评分函数，用于量化 OOD 样本测试输入的不确定性。我们的建议继承了某些基于距离的方法的优点，同时消除了对分布假设、先验知识和特定训练机制的依赖。对基准数据集进行的大量实验表明，我们的方法在 OOD 检测方面优于竞争对手。</details>
**PDF:** <http://arxiv.org/pdf/2401.11726v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Augmenting Prototype Network with TransMix for Few-shot Hyperspectral Image Classification**<br />
**Title_cn:** 使用 TransMix 增强原型网络以实现少样本高光谱图像分类<br />
**Authors:** Chun Liu, Longwei Yang, Dongmei Dong, Zheng Li, Wei Yang, Zhigang Han, Jiayao Wang<br />
**Abstract:** <details><summary>原文: </summary>Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of these pixels. And in order to obtain the spatial-spectral joint features of each pixel, the fixed-size patches centering around each pixel are often used for classification. However, observing the classification results of existing methods, we found that boundary patches corresponding to the pixels which are located at the boundary of the objects in the hyperspectral images, are hard to classify. These boundary patchs are mixed with multi-class spectral information. Inspired by this, we propose to augment the prototype network with TransMix for few-shot hyperspectrial image classification(APNT). While taking the prototype network as the backbone, it adopts the transformer as feature extractor to learn the pixel-to-pixel relation and pay different attentions to different pixels. At the same time, instead of directly using the patches which are cut from the hyperspectral images for training, it randomly mixs up two patches to imitate the boundary patches and uses the synthetic patches to train the model, with the aim to enlarge the number of hard training samples and enhance their diversity. And by following the data agumentation technique TransMix, the attention returned by the transformer is also used to mix up the labels of two patches to generate better labels for synthetic patches. Compared with existing methods, the proposed method has demonstrated sate of the art performance and better robustness for few-shot hyperspectral image classification in our experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头高光谱图像分类旨在通过仅标记少数像素来识别图像中每个像素的类别。为了获得每个像素的空间-光谱联合特征，通常使用以每个像素为中心的固定大小的块进行分类。然而，观察现有方法的分类结果，我们发现高光谱图像中位于目标边界的像素对应的边界斑块很难分类。这些边界斑块与多类光谱信息混合。受此启发，我们建议使用 TransMix 增强原型网络，以实现少样本高光谱图像分类（APNT）。以原型网络为骨干，采用变压器作为特征提取器来学习像素到像素的关系，并对不同的像素给予不同的关注。同时，它不是直接使用从高光谱图像中切下的补丁进行训练，而是随机混合两个补丁来模拟边界补丁，并使用合成的补丁来训练模型，目的是扩大模型的数量。努力训练样本并增强其多样性。并且通过遵循数据增强技术 TransMix，变压器返回的注意力也用于混合两个补丁的标签，以便为合成补丁生成更好的标签。与现有方法相比，所提出的方法在我们的实验中展示了最先进的性能和更好的少样本高光谱图像分类鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11724v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** SFC：弱监督语义分割中的共享特征校准<br />
**Authors:** Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, Jimin Xiao<br />
**Abstract:** <details><summary>原文: </summary>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像级弱监督语义分割由于其注释成本低而受到越来越多的关注。现有方法主要依靠类激活映射（CAM）来获取伪标签来训练语义分割模型。在这项工作中，我们首次证明训练数据中的长尾分布会导致通过分类器权重计算出的 CAM 由于头类和尾类之间的共享特征而对头类过度激活，而对尾类激活不足。类。这会降低伪标签质量并进一步影响最终的语义分割性能。为了解决这个问题，我们提出了一种用于 CAM 生成的共享特征校准 (SFC) 方法。具体来说，我们利用具有积极共享特征的类原型，并提出多尺度分布加权（MSDW）一致性损失，以缩小训练期间通过分类器权重生成的 CAM 与类原型之间的差距。 MSDW 损失通过校准头/尾类分类器权重中的共享特征来平衡过度激活和激活不足。实验结果表明，我们的 SFC 显着改善了 CAM 边界并实现了新的最先进性能。该项目位于 https://github.com/Barrett-python/SFC。</details>
**PDF:** <http://arxiv.org/pdf/2401.11719v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **MsSVT++: Mixed-scale Sparse Voxel Transformer with Center Voting for 3D Object Detection**<br />
**Title_cn:** MsSVT++：用于 3D 对象检测的具有中心投票功能的混合尺度稀疏体素变换器<br />
**Authors:** Jianan Li, Shaocong Dong, Lihe Ding, Tingfa Xu<br />
**Abstract:** <details><summary>原文: </summary>Accurate 3D object detection in large-scale outdoor scenes, characterized by considerable variations in object scales, necessitates features rich in both long-range and fine-grained information. While recent detectors have utilized window-based transformers to model long-range dependencies, they tend to overlook fine-grained details. To bridge this gap, we propose MsSVT++, an innovative Mixed-scale Sparse Voxel Transformer that simultaneously captures both types of information through a divide-and-conquer approach. This approach involves explicitly dividing attention heads into multiple groups, each responsible for attending to information within a specific range. The outputs of these groups are subsequently merged to obtain final mixed-scale features. To mitigate the computational complexity associated with applying a window-based transformer in 3D voxel space, we introduce a novel Chessboard Sampling strategy and implement voxel sampling and gathering operations sparsely using a hash map. Moreover, an important challenge stems from the observation that non-empty voxels are primarily located on the surface of objects, which impedes the accurate estimation of bounding boxes. To overcome this challenge, we introduce a Center Voting module that integrates newly voted voxels enriched with mixed-scale contextual information towards the centers of the objects, thereby improving precise object localization. Extensive experiments demonstrate that our single-stage detector, built upon the foundation of MsSVT++, consistently delivers exceptional performance across diverse datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模户外场景中的精确 3D 物体检测，其特点是物体尺度变化很大，需要具有丰富的长距离和细粒度信息的特征。虽然最近的检测器利用基于窗口的变压器来模拟远程依赖性，但它们往往会忽略细粒度的细节。为了弥补这一差距，我们提出了 MsSVT++，这是一种创新的混合尺度稀疏体素变换器，它通过分而治之的方法同时捕获两种类型的信息。这种方法涉及明确地将注意力头分为多个组，每个组负责关注特定范围内的信息。随后合并这些组的输出以获得最终的混合尺度特征。为了减轻与在 3D 体素空间中应用基于窗口的变换器相关的计算复杂性，我们引入了一种新颖的棋盘采样策略，并使用哈希图稀疏地实现体素采样和收集操作。此外，一个重要的挑战源于观察到非空体素主要位于物体表面，这阻碍了边界框的准确估计。为了克服这一挑战，我们引入了中心投票模块，该模块将新投票的体素（富含混合尺度上下文信息）集成到对象的中心，从而提高精确的对象定位。大量实验表明，我们的单级检测器建立在 MsSVT++ 的基础上，能够在不同的数据集上始终如一地提供卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11718v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Medical Image Debiasing by Learning Adaptive Agreement from a Biased Council**<br />
**Title_cn:** 通过从有偏见的委员会学习自适应协议来消除医学图像偏见<br />
**Authors:** Luyang Luo, Xin Huang, Minghao Wang, Zhuoyue Wan, Hao Chen<br />
**Abstract:** <details><summary>原文: </summary>Deep learning could be prone to learning shortcuts raised by dataset bias and result in inaccurate, unreliable, and unfair models, which impedes its adoption in real-world clinical applications. Despite its significance, there is a dearth of research in the medical image classification domain to address dataset bias. Furthermore, the bias labels are often agnostic, as identifying biases can be laborious and depend on post-hoc interpretation. This paper proposes learning Adaptive Agreement from a Biased Council (Ada-ABC), a debiasing framework that does not rely on explicit bias labels to tackle dataset bias in medical images. Ada-ABC develops a biased council consisting of multiple classifiers optimized with generalized cross entropy loss to learn the dataset bias. A debiasing model is then simultaneously trained under the guidance of the biased council. Specifically, the debiasing model is required to learn adaptive agreement with the biased council by agreeing on the correctly predicted samples and disagreeing on the wrongly predicted samples by the biased council. In this way, the debiasing model could learn the target attribute on the samples without spurious correlations while also avoiding ignoring the rich information in samples with spurious correlations. We theoretically demonstrated that the debiasing model could learn the target features when the biased model successfully captures dataset bias. Moreover, to our best knowledge, we constructed the first medical debiasing benchmark from four datasets containing seven different bias scenarios. Our extensive experiments practically showed that our proposed Ada-ABC outperformed competitive approaches, verifying its effectiveness in mitigating dataset bias for medical image classification. The codes and organized benchmark datasets will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习可能容易出现数据集偏差带来的学习捷径，并导致模型不准确、不可靠和不公平，这阻碍了其在现实世界临床应用中的采用。尽管其意义重大，但医学图像分类领域仍缺乏解决数据集偏差的研究。此外，偏见标签通常是不可知的，因为识别偏见可能很费力并且依赖于事后解释。本文提出从偏置委员会（Ada-ABC）学习自适应协议，这是一种不依赖于显式偏差标签来解决医学图像中数据集偏差的去偏差框架。 Ada-ABC 开发了一个由多个分类器组成的有偏差委员会，这些分类器通过广义交叉熵损失进行了优化，以学习数据集偏差。然后在有偏差委员会的指导下同时训练去偏差模型。具体来说，去偏差模型需要通过对有偏差委员会的正确预测样本达成一致并在错误预测样本上达成一致来学习与有偏差委员会的自适应一致性。这样，去偏模型可以学习没有虚假相关性的样本上的目标属性，同时也避免忽略具有虚假相关性的样本中的丰富信息。我们从理论上证明，当偏置模型成功捕获数据集偏差时，去偏置模型可以学习目标特征。此外，据我们所知，我们从包含七种不同偏差场景的四个数据集构建了第一个医学去偏差基准。我们广泛的实验实际上表明，我们提出的 Ada-ABC 优于竞争方法，验证了其在减轻医学图像分类数据集偏差方面的有效性。代码和组织的基准数据集将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.11713v1><br />
**Code:** null<br />
>>**index:** 34<br />
**Title:** **EK-Net:Real-time Scene Text Detection with Expand Kernel Distance**<br />
**Title_cn:** EK-Net：扩展核距离的实时场景文本检测<br />
**Authors:** Boyuan Zhu, Fagui Liu, Xi Chen, Quan Tang<br />
**Abstract:** <details><summary>原文: </summary>Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the "shrinked kernel". Specifically, it refers to a decrease in accuracy resulting from an output that overly favors the text kernel. In this paper, we propose a new approach named Expand Kernel Network (EK-Net) with expand kernel distance to compensate for the previous deficiency, which includes three-stages regression to complete instance detection. Moreover, EK-Net not only realize the precise positioning of arbitrary-shaped text, but also achieve a trade-off between performance and speed. Evaluation results demonstrate that EK-Net achieves state-of-the-art or competitive performance compared to other advanced methods, e.g., F-measure of 85.72% at 35.42 FPS on ICDAR 2015, F-measure of 85.75% at 40.13 FPS on CTW1500.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，场景文本检测由于其广泛的应用而受到了广泛的关注。然而，在多个尺度、方向和曲率的复杂场景中进行准确检测仍然是一个挑战。许多检测方法采用华帝裁剪（VC）算法进行多实例训练，以解决任意形状文本的问题。然而，我们发现这些方法产生的一些偏差结果被称为“收缩内核”。具体来说，它指的是由于过于偏向文本内核的输出而导致的准确性下降。在本文中，我们提出了一种名为扩展核网络（EK-Net）的新方法，通过扩展核距离来弥补之前的缺陷，其中包括三阶段回归来完成实例检测。而且，EK-Net不仅实现了任意形状文本的精确定位，还实现了性能和速度之间的权衡。评估结果表明，与其他先进方法相比，EK-Net 实现了最先进的或具有竞争力的性能，例如，ICDAR 2015 上 35.42 FPS 下的 F 测量为 85.72%，CTW1500 上 40.13 FPS 上的 F 测量为 85.75% 。</details>
**PDF:** <http://arxiv.org/pdf/2401.11704v1><br />
**Code:** null<br />
>>**index:** 35<br />
**Title:** **Memory-Efficient Prompt Tuning for Incremental Histopathology Classification**<br />
**Title_cn:** 用于增量组织病理学分类的内存高效提示调整<br />
**Authors:** Yu Zhu, Kang Li, Lequan Yu, Pheng-Ann Heng<br />
**Abstract:** <details><summary>原文: </summary>Recent studies have made remarkable progress in histopathology classification. Based on current successes, contemporary works proposed to further upgrade the model towards a more generalizable and robust direction through incrementally learning from the sequentially delivered domains. Unlike previous parameter isolation based approaches that usually demand massive computation resources during model updating, we present a memory-efficient prompt tuning framework to cultivate model generalization potential in economical memory cost. For each incoming domain, we reuse the existing parameters of the initial classification model and attach lightweight trainable prompts into it for customized tuning. Considering the domain heterogeneity, we perform decoupled prompt tuning, where we adopt a domain-specific prompt for each domain to independently investigate its distinctive characteristics, and one domain-invariant prompt shared across all domains to continually explore the common content embedding throughout time. All domain-specific prompts will be appended to the prompt bank and isolated from further changes to prevent forgetting the distinctive features of early-seen domains. While the domain-invariant prompt will be passed on and iteratively evolve by style-augmented prompt refining to improve model generalization capability over time. In specific, we construct a graph with existing prompts and build a style-augmented graph attention network to guide the domain-invariant prompt exploring the overlapped latent embedding among all delivered domains for more domain generic representations. We have extensively evaluated our framework with two histopathology tasks, i.e., breast cancer metastasis classification and epithelium-stroma tissue classification, where our approach yielded superior performance and memory efficiency over the competing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究在组织病理学分类方面取得了显着进展。基于当前的成功，当代的工作提出通过从顺序交付的领域中增量学习，进一步将模型升级到更通用和更稳健的方向。与之前基于参数隔离的方法在模型更新过程中通常需要大量计算资源不同，我们提出了一种内存高效的提示调整框架，以经济的内存成本培养模型泛化潜力。对于每个传入域，我们重用初始分类模型的现有参数，并将轻量级可训练提示附加到其中以进行定制调整。考虑到领域的异构性，我们进行了解耦的提示调整，其中我们为每个领域采用特定于领域的提示来独立研究其独特的特征，并在所有领域之间共享一个领域不变的提示，以不断探索整个时间嵌入的共同内容。所有特定于域的提示都将附加到提示库中，并与进一步的更改隔离，以防止忘记早期看到的域的独特特征。而领域不变的提示将通过风格增强的提示细化来传递和迭代发展，以随着时间的推移提高模型的泛化能力。具体来说，我们用现有的提示构建一个图，并构建一个风格增强的图注意网络，以指导域不变的提示探索所有交付域之间的重叠潜在嵌入，以获得更多域通用表示。我们通过两项组织病理学任务（即乳腺癌转移分类和上皮-间质组织分类）广泛评估了我们的框架，其中我们的方法比竞争方法产生了卓越的性能和记忆效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.11674v1><br />
**Code:** null<br />
>>**index:** 36<br />
**Title:** **RTA-Former: Reverse Transformer Attention for Polyp Segmentation**<br />
**Title_cn:** RTA-Former：用于息肉分割的反向变压器注意力<br />
**Authors:** Zhikai Li, Murong Yi, Ali Uneri, Sihan Niu, Craig Jones<br />
**Abstract:** <details><summary>原文: </summary>Polyp segmentation is a key aspect of colorectal cancer prevention, enabling early detection and guiding subsequent treatments. Intelligent diagnostic tools, including deep learning solutions, are widely explored to streamline and potentially automate this process. However, even with many powerful network architectures, there still comes the problem of producing accurate edge segmentation. In this paper, we introduce a novel network, namely RTA-Former, that employs a transformer model as the encoder backbone and innovatively adapts Reverse Attention (RA) with a transformer stage in the decoder for enhanced edge segmentation. The results of the experiments illustrate that RTA-Former achieves state-of-the-art (SOTA) performance in five polyp segmentation datasets. The strong capability of RTA-Former holds promise in improving the accuracy of Transformer-based polyp segmentation, potentially leading to better clinical decisions and patient outcomes. Our code will be publicly available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>息肉分割是结直肠癌预防的一个关键方面，可以实现早期发现并指导后续治疗。人们广泛探索包括深度学习解决方案在内的智能诊断工具来简化这一过程并可能实现自动化。然而，即使有许多强大的网络架构，仍然存在产生准确边缘分割的问题。在本文中，我们介绍了一种新颖的网络，即 RTA-Former，它采用 Transformer 模型作为编码器主干，并创新性地采用解码器中 Transformer 级的反向注意力（RA）来增强边缘分割。实验结果表明，RTA-Former 在五个息肉分割数据集中实现了最先进的 (SOTA) 性能。 RTA-Former 的强大功能有望提高基于 Transformer 的息肉分割的准确性，从而有可能带来更好的临床决策和患者结果。我们的代码将在 GitHub 上公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.11671v1><br />
**Code:** null<br />
>>**index:** 37<br />
**Title:** **ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition**<br />
**Title_cn:** ActionHub：用于零镜头动作识别的大规模动作视频描述数据集<br />
**Authors:** Jiaming Zhou, Junwei Liang, Kun-Yu Lin, Jinrui Yang, Wei-Shi Zheng<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot action recognition (ZSAR) aims to learn an alignment model between videos and class descriptions of seen actions that is transferable to unseen actions. The text queries (class descriptions) used in existing ZSAR works, however, are often short action names that fail to capture the rich semantics in the videos, leading to misalignment. With the intuition that video content descriptions (e.g., video captions) can provide rich contextual information of visual concepts in videos, we propose to utilize human annotated video descriptions to enrich the semantics of the class descriptions of each action. However, all existing action video description datasets are limited in terms of the number of actions, the semantics of video descriptions, etc. To this end, we collect a large-scale action video descriptions dataset named ActionHub, which covers a total of 1,211 common actions and provides 3.6 million action video descriptions. With the proposed ActionHub dataset, we further propose a novel Cross-modality and Cross-action Modeling (CoCo) framework for ZSAR, which consists of a Dual Cross-modality Alignment module and a Cross-action Invariance Mining module. Specifically, the Dual Cross-modality Alignment module utilizes both action labels and video descriptions from ActionHub to obtain rich class semantic features for feature alignment. The Cross-action Invariance Mining module exploits a cycle-reconstruction process between the class semantic feature spaces of seen actions and unseen actions, aiming to guide the model to learn cross-action invariant representations. Extensive experimental results demonstrate that our CoCo framework significantly outperforms the state-of-the-art on three popular ZSAR benchmarks (i.e., Kinetics-ZSAR, UCF101 and HMDB51) under two different learning protocols in ZSAR. We will release our code, models, and the proposed ActionHub dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>零样本动作识别（ZSAR）旨在学习视频和已见动作的类描述之间的对齐模型，该模型可转移到未见动作。然而，现有 ZSAR 作品中使用的文本查询（类描述）通常是简短的动作名称，无法捕获视频中丰富的语义，从而导致错位。凭借视频内容描述（例如视频字幕）可以提供视频中视觉概念的丰富上下文信息的直觉，我们建议利用人类注释的视频描述来丰富每个动作的类描述的语义。然而，所有现有的动作视频描述数据集在动作数量、视频描述语义等方面都受到限制。为此，我们收集了一个名为 ActionHub 的大规模动作视频描述数据集，该数据集总共涵盖了 1,211 个常见的动作视频描述数据集。动作并提供360万条动作视频描述。利用所提出的 ActionHub 数据集，我们进一步提出了一种新颖的 ZSAR 跨模态和交叉动作建模（CoCo）框架，该框架由双跨模态对齐模块和跨动作不变性挖掘模块组成。具体来说，双跨模态对齐模块利用来自 ActionHub 的动作标签和视频描述来获取丰富的类别语义特征以进行特征对齐。交叉动作不变性挖掘模块利用已见动作和未见动作的类语义特征空间之间的循环重建过程，旨在引导模型学习交叉动作不变表示。大量实验结果表明，在 ZSAR 的两种不同学习协议下，我们的 CoCo 框架在三个流行的 ZSAR 基准（即 Kinetics-ZSAR、UCF101 和 HMDB51）上的性能显着优于最先进的框架。我们将发布我们的代码、模型和提议的 ActionHub 数据集。</details>
**PDF:** <http://arxiv.org/pdf/2401.11654v1><br />
**Code:** null<br />
>>**index:** 38<br />
**Title:** **M2-CLIP: A Multimodal, Multi-task Adapting Framework for Video Action Recognition**<br />
**Title_cn:** M2-CLIP：视频动作识别的多模态、多任务适应框架<br />
**Authors:** Mengmeng Wang, Jiazheng Xing, Boyuan Jiang, Jun Chen, Jianbiao Mei, Xingxing Zuo, Guang Dai, Jingdong Wang, Yong Liu<br />
**Abstract:** <details><summary>原文: </summary>Recently, the rise of large-scale vision-language pretrained models like CLIP, coupled with the technology of Parameter-Efficient FineTuning (PEFT), has captured substantial attraction in video action recognition. Nevertheless, prevailing approaches tend to prioritize strong supervised performance at the expense of compromising the models' generalization capabilities during transfer. In this paper, we introduce a novel Multimodal, Multi-task CLIP adapting framework named \name to address these challenges, preserving both high supervised performance and robust transferability. Firstly, to enhance the individual modality architectures, we introduce multimodal adapters to both the visual and text branches. Specifically, we design a novel visual TED-Adapter, that performs global Temporal Enhancement and local temporal Difference modeling to improve the temporal representation capabilities of the visual encoder. Moreover, we adopt text encoder adapters to strengthen the learning of semantic label information. Secondly, we design a multi-task decoder with a rich set of supervisory signals to adeptly satisfy the need for strong supervised performance and generalization within a multimodal framework. Experimental results validate the efficacy of our approach, demonstrating exceptional performance in supervised learning while maintaining strong generalization in zero-shot scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，像 CLIP 这样的大规模视觉语言预训练模型的兴起，加上参数高效微调（PEFT）技术，在视频动作识别领域引起了巨大的关注。然而，流行的方法倾向于优先考虑强监督性能，但代价是在传输过程中损害模型的泛化能力。在本文中，我们介绍了一种名为 \name 的新颖的多模式、多任务 CLIP 适应框架来应对这些挑战，同时保持高监督性能和强大的可转移性。首先，为了增强单独的模态架构，我们将多模态适配器引入视觉和文本分支。具体来说，我们设计了一种新颖的视觉 TED 适配器，它执行全局时间增强和局部时间差异建模，以提高视觉编码器的时间表示能力。此外，我们采用文本编码器适配器来加强语义标签信息的学习。其次，我们设计了一个具有丰富监督信号集的多任务解码器，以巧妙地满足多模态框架内对强监督性能和泛化的需求。实验结果验证了我们方法的有效性，展示了监督学习的卓越性能，同时在零样本场景中保持了强大的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.11649v1><br />
**Code:** null<br />
>>**index:** 39<br />
**Title:** **Friends Across Time: Multi-Scale Action Segmentation Transformer for Surgical Phase Recognition**<br />
**Title_cn:** 跨越时间的朋友：用于手术阶段识别的多尺度动作分段变压器<br />
**Authors:** Bokai Zhang, Jiayuan Meng, Bin Cheng, Dean Biskup, Svetlana Petculescu, Angela Chapman<br />
**Abstract:** <details><summary>原文: </summary>Automatic surgical phase recognition is a core technology for modern operating rooms and online surgical video assessment platforms. Current state-of-the-art methods use both spatial and temporal information to tackle the surgical phase recognition task. Building on this idea, we propose the Multi-Scale Action Segmentation Transformer (MS-AST) for offline surgical phase recognition and the Multi-Scale Action Segmentation Causal Transformer (MS-ASCT) for online surgical phase recognition. We use ResNet50 or EfficientNetV2-M for spatial feature extraction. Our MS-AST and MS-ASCT can model temporal information at different scales with multi-scale temporal self-attention and multi-scale temporal cross-attention, which enhances the capture of temporal relationships between frames and segments. We demonstrate that our method can achieve 95.26% and 96.15% accuracy on the Cholec80 dataset for online and offline surgical phase recognition, respectively, which achieves new state-of-the-art results. Our method can also achieve state-of-the-art results on non-medical datasets in the video action segmentation domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动手术阶段识别是现代手术室和在线手术视频评估平台的核心技术。当前最先进的方法使用空间和时间信息来处理手术相位识别任务。基于这个想法，我们提出了用于离线手术阶段识别的多尺度动作分割变压器（MS-AST）和用于在线手术阶段识别的多尺度动作分割因果变压器（MS-ASCT）。我们使用ResNet50或EfficientNetV2-M进行空间特征提取。我们的 MS-AST 和 MS-ASCT 可以通过多尺度时间自注意力和多尺度时间交叉注意力对不同尺度的时间信息进行建模，从而增强对帧和片段之间时间关系的捕获。我们证明，我们的方法可以在 Cholec80 数据集上分别实现在线和离线手术阶段识别的 95.26% 和 96.15% 准确率，从而实现了新的最先进的结果。我们的方法还可以在视频动作分割领域的非医学数据集上取得最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.11644v1><br />
**Code:** null<br />
>>**index:** 40<br />
**Title:** **Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss**<br />
**Title_cn:** Zoom-shot：快速高效的无监督零样本将 CLIP 传输到具有多模态损失的视觉编码器<br />
**Authors:** Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes<br />
**Abstract:** <details><summary>原文: </summary>The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过视觉语言模型（VLM）的出现，视觉和语言的融合给计算机视觉带来了革命性的转变。然而，现有 VLM 的资源密集型性质提出了重大挑战。我们需要一种可访问的方法来开发下一代 VLM。为了解决这个问题，我们提出了 Zoom-shot，这是一种将 CLIP 的零样本功能转移到任何预先训练的视觉编码器的新颖方法。我们通过使用专门设计的多模态损失函数来利用 CLIP 潜在空间中存在的多模态信息（即文本和图像）来实现这一点。这些损失函数是（1）循环一致性损失和（2）我们新颖的提示引导知识蒸馏损失（PG-KD）。 PG-KD将知识蒸馏的概念与CLIP的零样本分类相结合，以捕获文本和图像特征之间的交互。通过多模态损失，我们在 CLIP 潜在空间和预训练视觉编码器的潜在空间之间训练 $\textbf{线性映射}$，仅适用于 $\textbf{single epoch}$。此外，Zoom-shot 完全是无监督的，并且使用 $\textbf{unpaired}$ 数据进行训练。我们在粗粒度和细粒度分类数据集上测试了一系列作为新 VLM 增强的视觉编码器的零样本能力，在该问题领域的表现优于之前的最先进技术。在我们的消融中，我们发现 Zoom-shot 允许在训练期间在数据和计算之间进行权衡；我们最先进的结果可以通过将 20 个 epoch 的 ImageNet 训练数据的训练量从 20% 减少到 1% 来获得。所有代码和模型均可在 GitHub 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11633v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **LONEStar: The Lunar Flashlight Optical Navigation Experiment**<br />
**Title_cn:** LONEStar：月球手电筒光学导航实验<br />
**Authors:** Michael Krause, Ava Thrasher, Priyal Soni, Liam Smego, Reuben Isaac, Jennifer Nolan, Micah Pledger, E. Glenn Lightsey, W. Jud Ready, John Christian<br />
**Abstract:** <details><summary>原文: </summary>This paper documents the results from the highly successful Lunar flashlight Optical Navigation Experiment with a Star tracker (LONEStar). Launched in December 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration mission. After a propulsion system anomaly prevented capture in lunar orbit, LF was ejected from the Earth-Moon system and into heliocentric space. NASA subsequently transferred ownership of LF to Georgia Tech to conduct an unfunded extended mission to demonstrate further advanced technology objectives, including LONEStar. From August-December 2023, the LONEStar team performed on-orbit calibration of the optical instrument and a number of different OPNAV experiments. This campaign included the processing of nearly 400 images of star fields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and Saturn). LONEStar provided the first on-orbit demonstrations of heliocentric navigation using only optical observations of planets. Of special note is the successful in-flight demonstration of (1) instantaneous triangulation with simultaneous sightings of two planets with the LOST algorithm and (2) dynamic triangulation with sequential sightings of multiple planets.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文记录了使用星跟踪器 (LONEStar) 进行的非常成功的月球手电筒光学导航实验的结果。月球手电筒 (LF) 于 2022 年 12 月发射，是 NASA 资助的一项技术演示任务。在推进系统异常导致无法捕获月球轨道后，LF 被从地月系统中弹出并进入日心空间。 NASA 随后将 LF 的所有权转让给佐治亚理工学院，以执行一项无资金支持的扩展任务，以展示进一步的先进技术目标，包括 LONEStar。 2023年8月至12月，LONEStar团队进行了光学仪器在轨校准和多项不同的OPNAV实验。该活动包括处理近 400 张星域、地球和月球以及其他四颗行星（水星、火星、木星和土星）的图像。 LONEStar 首次仅使用行星光学观测进行了日心导航在轨演示。特别值得注意的是成功的飞行演示：(1) 使用 LOST 算法同时观测两颗行星的瞬时三角测量和 (2) 连续观测多个行星的动态三角测量。</details>
**PDF:** <http://arxiv.org/pdf/2401.12198v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency**<br />
**Title_cn:** 通过多重视差一致性过滤的立体匹配知识蒸馏单目深度估计<br />
**Authors:** Woonghyun Ka, Jae Young Lee, Jaehyun Choi, Junmo Kim<br />
**Abstract:** <details><summary>原文: </summary>In stereo-matching knowledge distillation methods of the self-supervised monocular depth estimation, the stereo-matching network's knowledge is distilled into a monocular depth network through pseudo-depth maps. In these methods, the learning-based stereo-confidence network is generally utilized to identify errors in the pseudo-depth maps to prevent transferring the errors. However, the learning-based stereo-confidence networks should be trained with ground truth (GT), which is not feasible in a self-supervised setting. In this paper, we propose a method to identify and filter errors in the pseudo-depth map using multiple disparity maps by checking their consistency without the need for GT and a training process. Experimental results show that the proposed method outperforms the previous methods and works well on various configurations by filtering out erroneous areas where the stereo-matching is vulnerable, especially such as textureless regions, occlusion boundaries, and reflective surfaces.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自监督单目深度估计的立体匹配知识蒸馏方法中，立体匹配网络的知识通过伪深度图被蒸馏为单目深度网络。在这些方法中，通常利用基于学习的立体置信网络来识别伪深度图中的错误，以防止错误转移。然而，基于学习的立体置信网络应该使用地面实况（GT）进行训练，这在自我监督的环境中是不可行的。在本文中，我们提出了一种使用多个视差图通过检查它们的一致性来识别和过滤伪深度图中的错误的方法，而不需要 GT 和训练过程。实验结果表明，所提出的方法优于以前的方法，并且通过过滤掉立体匹配脆弱的错误区域，特别是无纹理区域、遮挡边界和反射表面等，在各种配置上都表现良好。</details>
**PDF:** <http://arxiv.org/pdf/2401.12019v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Robustness to distribution shifts of compressed networks for edge devices**<br />
**Title_cn:** 边缘设备压缩网络分布变化的鲁棒性<br />
**Authors:** Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark<br />
**Abstract:** <details><summary>原文: </summary>It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>有必要开发部署在计算资源有限的边缘设备上的高效 DNN。然而，压缩网络通常在目标域中执行新任务，该目标域与训练原始网络的源域不同。研究压缩网络在两种类型的数据分布变化中的鲁棒性非常重要：域变化和对抗性扰动。在这项研究中，我们发现压缩模型对分布变化的鲁棒性不如原始网络。有趣的是，较大的网络比较小的网络更容易失去鲁棒性，即使它们被压缩到与较小的网络相似的大小。此外，通过知识蒸馏获得的紧凑网络比修剪网络对分布变化具有更强的鲁棒性。最后，训练后量化是一种可靠的方法，可以实现对分布变化的显着鲁棒性，并且在鲁棒性方面优于修剪模型和蒸馏模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.12014v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark**<br />
**Title_cn:** CMMMU：中国大规模多学科多模态理解基准<br />
**Authors:** Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et.al.<br />
**Abstract:** <details><summary>原文: </summary>As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.   CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement. CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着大型多模态模型 (LMM) 的功能不断进步，评估 LMM 性能的需求也日益增长。此外，在汉语等非英语环境中评估 LMM 的高级知识和推理能力存在更大的差距。我们推出了 CMMMU，这是一种新的中国大规模多学科多模态理解基准，旨在评估 LMM 在中国背景下需要大学水平学科知识和深思熟虑推理的任务。 CMMMU受到MMMU的启发并严格遵循MMMU的注释和分析模式。 CMMMU 包括从大学考试、测验和教科书中手动收集的 12k 多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程，与其姊妹篇 MMMU 一样。这些问题涵盖 30 个主题，包含 39 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。 CMMMU 专注于在中国背景下利用特定领域知识进行复杂的感知和推理。我们评估了 11 个开源 LLM 和一个专有的 GPT-4V(ision)。即使GPT-4V也只能达到42%的准确率，这表明还有很大的改进空间。 CMMMU 将推动社区构建面向专家人工智能的下一代 LMM，并通过提供多样化的语言环境来促进 LMM 的民主化。</details>
**PDF:** <http://arxiv.org/pdf/2401.11944v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Single-View 3D Human Digitalization with Large Reconstruction Models**<br />
**Title_cn:** 具有大型重建模型的单视图 3D 人体数字化<br />
**Authors:** Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image. Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture. Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model. This generative extension addresses the inherent variations in human body shapes when observed from a single view, and makes it possible to reconstruct the full body human from an occluded image. Through extensive experiments, we show that Human-LRM surpasses previous methods by a significant margin on several benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 Human-LRM，这是一种单级前馈大型重建模型，旨在从单个图像预测人类神经辐射场（NeRF）。我们的方法在使用包含 3D 扫描和多视图捕获的广泛数据集进行训练时展示了卓越的适应性。此外，为了增强模型在野外场景（尤其是遮挡情况下）的适用性，我们提出了一种新策略，通过条件三平面扩散模型将多视图重建提炼为单视图。这种生成扩展解决了从单一视图观察时人体形状的固有变化，并且使得从遮挡图像重建人体全身成为可能。通过大量的实验，我们表明 Human-LRM 在多个基准测试中显着超越了以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.12175v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Feature Denoising Diffusion Model for Blind Image Quality Assessment**<br />
**Title_cn:** 用于盲图像质量评估的特征去噪扩散模型<br />
**Authors:** Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC).</details>
**Abstract_cn:** <details><summary>译文: </summary>盲图像质量评估（BIQA）旨在评估符合人类感知的图像质量，无需参考基准。目前，深度学习 BIQA 方法通常依赖于使用高级任务的特征进行迁移学习。然而，BIQA 和这些高级任务之间的固有差异不可避免地会给质量感知功能带来噪音。在本文中，我们迈出了探索 BIQA 中特征去噪的扩散模型的第一步，即 IQA 感知特征扩散（PFD-IQA），其目的是消除质量感知特征中的噪声。具体来说，（i）我们提出了一个感知先验发现和聚合模块来建立两个辅助任务来发现图像中潜在的低级特征，这些特征用于聚合扩散模型的感知文本条件。 （ii）我们提出了一种基于感知先验的特征细化策略，它将噪声特征与预定义的去噪轨迹相匹配，然后根据文本条件执行精确的特征去噪。对八个标准 BIQA 数据集的广泛实验证明了其优于最先进的 BIQA 方法的性能，即实现了 0.935（相对于 KADID 中的 0.905）和 0.922（相对于 LIVEC 中的 0.894）的 PLCC 值。</details>
**PDF:** <http://arxiv.org/pdf/2401.11949v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Fair Evaluation of Various Deep Learning-Based Document Image Binarization Approaches**<br />
**Title_cn:** 对各种基于深度学习的文档图像二值化方法的公平评估<br />
**Authors:** Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein<br />
**Abstract:** <details><summary>原文: </summary>Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (https://github.com/RichSu95/Document_Binarization_Collection) to ensure reproducibility and simplify future binarization evaluations.</details>
**Abstract_cn:** <details><summary>译文: </summary>文档图像二值化是文档分析领域中重要的预处理步骤。传统的图像二值化技术通常依赖于直方图或局部统计来识别有效阈值来区分图像的不同方面。深度学习技术能够通过学习上下文相关的特征来生成图像的二值化版本，这些特征不太容易出现文档图像中通常发生的退化。近年来，已经开发了许多基于深度学习的方法用于文档二值化。但该选择哪一个呢？还没有研究严格比较这些方法。因此，这项工作的重点是在同一评估协议下评估不同的基于深度学习的方法。我们在不同的文档图像二值化竞赛（DIBCO）数据集上对它们进行评估，并获得了非常异构的结果。我们表明，在 DIBCO2013 数据集上进行评估时，DE-GAN 模型能够比其他模型表现更好，而 DP-LinkNet 在 DIBCO2017 数据集上表现最好。 2-StageGAN 在 DIBCO2018 数据集上表现最佳，而 SauvolaNet 在 DIBCO2019 挑战赛上表现优于其他模型。最后，我们将代码、所有模型和评估公开（https://github.com/RichSu95/Document_Binarization_Collection），以确保可重复性并简化未来的二值化评估。</details>
**PDF:** <http://arxiv.org/pdf/2401.11831v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs**<br />
**Title_cn:** 掌握文本到图像的扩散：使用多模态法学硕士进行重述、规划和生成<br />
**Authors:** Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: https://github.com/YangLing0818/RPG-DiffusionMaster</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在文本到图像的生成和编辑方面表现出了卓越的性能。然而，现有的方法在处理涉及具有多个属性和关系的多个对象的复杂文本提示时经常面临挑战。在本文中，我们提出了一种全新的免训练文本到图像生成/编辑框架，即Recaption、Plan和Generate（RPG），利用多模态LLM强大的思想链推理能力来增强文本的组合性文本到图像的扩散模型。我们的方法采用 MLLM 作为全局规划器，将生成复杂图像的过程分解为子区域内多个更简单的生成任务。我们提出互补的区域扩散，以实现区域性的合成生成。此外，我们以闭环方式将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强了泛化能力。大量实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL，特别是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架表现出与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干网（例如 ControlNet）的广泛兼容性。我们的代码位于：https://github.com/YangLing0818/RPG-DiffusionMaster</details>
**PDF:** <http://arxiv.org/pdf/2401.11708v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities**<br />
**Title_cn:** SpatialVLM：赋予视觉语言模型空间推理能力<br />
**Authors:** Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia<br />
**Abstract:** <details><summary>原文: </summary>Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>理解和推理空间关系是视觉问答 (VQA) 和机器人技术的基本能力。虽然视觉语言模型 (VLM) 在某些 VQA 基准测试中表现出了出色的性能，但它们仍然缺乏 3D 空间推理功能，例如识别物理对象的定量关系（例如距离或大小差异）。我们假设 VLM 的空间推理能力有限是由于训练数据中缺乏 3D 空间知识，并旨在通过使用互联网规模的空间推理数据训练 VLM 来解决这个问题。为此，我们提出了一个系统来促进这种方法。我们首先开发了一个自动 3D 空间 VQA 数据生成框架，可在 1000 万张真实世界图像上扩展至 20 亿个 VQA 示例。然后，我们研究训练方案中的各种因素，包括数据质量、训练管道和 VLM 架构。我们的工作是度量空间中第一个互联网规模的 3D 空间推理数据集。通过在此类数据上训练 VLM，我们显着增强了其定性和定量空间 VQA 的能力。最后，我们证明了该 VLM 由于其定量估计能力，在思想链空间推理和机器人技术中解锁了新颖的下游应用。项目网站：https://spatial-vlm.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2401.12168v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Benchmarking Large Multimodal Models against Common Corruptions**<br />
**Title_cn:** 针对常见腐败对大型多模式模型进行基准测试<br />
**Authors:** Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, Min Lin<br />
**Abstract:** <details><summary>原文: </summary>This technical report aims to fill a deficiency in the assessment of large multimodal models (LMMs) by specifically examining the self-consistency of their outputs when subjected to common corruptions. We investigate the cross-modal interactions between text, image, and speech, encompassing four essential generation tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text. We create a comprehensive benchmark, named MMCBench, that covers more than 100 popular LMMs (totally over 150 model checkpoints). A thorough evaluation under common corruptions is critical for practical deployment and facilitates a better understanding of the reliability of cutting-edge LMMs. The benchmarking code is available at https://github.com/sail-sg/MMCBench</details>
**Abstract_cn:** <details><summary>译文: </summary>本技术报告旨在通过专门检查大型多模态模型（LMM）在遭受常见腐败时输出的自洽性来填补评估的缺陷。我们研究文本、图像和语音之间的跨模式交互，包括四个基本生成任务：文本到图像、图像到文本、文本到语音和语音到文本。我们创建了一个名为 MMCBench 的综合基准测试，涵盖 100 多个流行的 LMM（总共超过 150 个模型检查点）。对常见损坏进行彻底评估对于实际部署至关重要，并且有助于更好地了解尖端 LMM 的可靠性。基准测试代码可在 https://github.com/sail-sg/MMCBench 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.11943v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**<br />
**Title_cn:** CheXagent：建立胸部 X 射线解读的基础模型<br />
**Authors:** Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{https://stanford-aimi.github.io/chexagent.html}.</details>
**Abstract_cn:** <details><summary>译文: </summary>胸部 X 光检查 (CXR) 是临床实践中最常进行的影像学检查。视觉语言基础模型 (FM) 开发的最新进展使得执行自动 CXR 解释成为可能，这可以帮助医生进行临床决策并改善患者的治疗结果。然而，开发能够准确解释 CXR 的 FM 具有挑战性，因为 (1) 医学图像领域中大规模视觉语言数据集的可用性有限，(2) 缺乏能够捕获医学数据复杂性的视觉和语言编码器，(3) 缺乏对 FM 的 CXR 解释能力进行基准测试的评估框架。在这项工作中，我们通过首先引入 \emph{CheXinstruct} 来应对这些挑战，这是一个由 28 个公开可用的数据集组成的大规模指令调整数据集。然后我们提出 \emph{CheXagent} - 一种能够分析和总结 CXR 的指令调整 FM。为了构建 CheXagent，我们设计了一个用于解析放射学报告的临床大语言模型 (LLM)、一个用于表示 CXR 图像的视觉编码器，以及一个连接视觉和语言模式的网络。最后，我们介绍 \emph{CheXbench} - 一种新颖的基准，旨在系统地评估 8 个临床相关 CXR 解释任务中的 FM。五位放射专家专家进行的广泛定量评估和定性审查表明，CheXagent 在 CheXbench 任务上的性能优于之前开发的通用和医疗领域 FM。此外，为了提高模型透明度，我们对性别、种族和年龄因素进行公平性评估，以突出潜在的绩效差异。我们的项目位于 \url{https://stanford-aimi.github.io/chexagent.html}。</details>
**PDF:** <http://arxiv.org/pdf/2401.12208v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models**<br />
**Title_cn:** 越少越好：参数高效的微调推进医学视觉基础模型<br />
**Authors:** Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang<br />
**Abstract:** <details><summary>原文: </summary>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.</details>
**Abstract_cn:** <details><summary>译文: </summary>参数高效微调（PEFT）最初是为了利用预先训练的大型语言模型而开发的，最近已成为在计算机视觉任务上执行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型上的有效性仍不清楚，有待探索。作为概念验证，我们对将 PEFT 应用于胸部放射线摄影基础模型进行了详细的实证研究。具体来说，我们深入研究了 LoRA（一种代表性的 PEFT 方法），并将其与三个成熟的胸部 X 光数据集上的两个自监督 X 线摄影基础模型的全参数微调 (FFT) 进行比较。我们的结果表明，使用少于 1% 的可调参数，LoRA 在 18 项迁移学习任务中的 13 项中最多优于 FFT 2.9%。将 LoRA 与基础模型相结合，我们在一系列数据高效的学习任务上建立了最先进的技术，例如在 NIH ChestX-ray14 上使用 1% 的标记数据，AUROC 得分为 80.6%。我们希望这项研究能够引起社区对使用 PEFT 进行医学成像任务的迁移学习的更多关注。代码和模型可在 https://github.com/RL4M/MED-PEFT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12215v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution**<br />
**Title_cn:** LKFormer：用于红外图像超分辨率的大型内核变压器<br />
**Authors:** Feiwei Qin, Kang Yan, Changmiao Wang, Ruiquan Ge, Yong Peng, Kai Zhang<br />
**Abstract:** <details><summary>原文: </summary>Given the broad application of infrared technology across diverse fields, there is an increasing emphasis on investigating super-resolution techniques for infrared images within the realm of deep learning. Despite the impressive results of current Transformer-based methods in image super-resolution tasks, their reliance on the self-attentive mechanism intrinsic to the Transformer architecture results in images being treated as one-dimensional sequences, thereby neglecting their inherent two-dimensional structure. Moreover, infrared images exhibit a uniform pixel distribution and a limited gradient range, posing challenges for the model to capture effective feature information. Consequently, we suggest a potent Transformer model, termed Large Kernel Transformer (LKFormer), to address this issue. Specifically, we have designed a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module with linear complexity. This mainly employs depth-wise convolution with large kernels to execute non-local feature modeling, thereby substituting the standard self-attentive layer. Additionally, we have devised a novel feed-forward network structure called Gated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to manage the information flow within the network. Comprehensive experimental results reveal that our method surpasses the most advanced techniques available, using fewer parameters and yielding considerably superior performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于红外技术在各个领域的广泛应用，人们越来越重视在深度学习领域研究红外图像的超分辨率技术。尽管当前基于 Transformer 的方法在图像超分辨率任务中取得了令人印象深刻的结果，但它们对 Transformer 架构固有的自我关注机制的依赖导致图像被视为一维序列，从而忽略了其固有的二维结构。此外，红外图像呈现出均匀的像素分布和有限的梯度范围，这给模型捕获有效的特征信息带来了挑战。因此，我们建议使用一种有效的 Transformer 模型，称为 Large Kernel Transformer (LKFormer) 来解决这个问题。具体来说，我们设计了一个具有线性复杂度的大内核残差深度卷积注意力（LKRDA）模块。这主要采用具有大内核的深度卷积来执行非局部特征建模，从而取代标准的自注意力层。此外，我们还设计了一种新颖的前馈网络结构，称为门控像素前馈网络 (GPFN)，以增强 LKFormer 管理网络内信息流的能力。综合实验结果表明，我们的方法超越了现有的最先进技术，使用更少的参数并产生了相当优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11859v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**<br />
**Title_cn:** HG3-NeRF：用于稀疏视图输入的分层几何、语义和光度引导神经辐射场<br />
**Authors:** Zelin Gao, Weichen Dai, Yu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场（NeRF）作为通过从离散观察中学习场景表示来合成新颖视图的范例，已经引起了相当大的关注。然而，当面对稀疏视图输入时，NeRF 表现出明显的性能下降，从而限制了其进一步的适用性。在这项工作中，我们引入了分层几何、语义和光度引导 NeRF（HG3-NeRF），这是一种新颖的方法，可以解决上述限制并增强不同视图中几何、语义内容和外观的一致性。我们提出分层几何引导（HGG），将运动结构（SfM）的附件（即稀疏深度先验）合并到场景表示中。与直接深度监督不同，HGG 从局部到全局几何区域采样体积点，减轻了深度先验固有偏差造成的错位。此外，我们从不同分辨率图像中观察到的语义一致性的显着变化中汲取灵感，并提出层次语义指导（HSG）来学习从粗到细的语义内容，这对应于从粗到细的场景表示。实验结果表明，HG3-NeRF 在不同的标准基准上可以优于其他最先进的方法，并针对稀疏视图输入实现高保真合成结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.11711v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **TIM: An Efficient Temporal Interaction Module for Spiking Transformer**<br />
**Title_cn:** TIM：尖峰变压器的高效时间交互模块<br />
**Authors:** Sicheng Shen, Dongcheng Zhao, Guobin Shen, Yi Zeng<br />
**Abstract:** <details><summary>原文: </summary>Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities. Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>尖峰神经网络（SNN）作为第三代神经网络，因其生物学合理性和计算效率而受到关注，特别是在处理不同的数据集方面。受神经网络架构进步的启发，注意力机制的整合导致了尖峰变压器的发展。这些在增强 SNN 功能方面显示出了希望，特别是在静态和神经形态数据集领域。尽管取得了进步，但这些系统中仍存在明显的差距，特别是尖峰自注意力 (SSA) 机制在利用 SNN 时间处理潜力方面的有效性。为了解决这个问题，我们引入了时态交互模块 (TIM)，这是一种新颖的基于卷积的增强功能，旨在增强 SNN 架构中的时态数据处理能力。 TIM 与现有 SNN 框架的集成是无缝且高效的，需要最少的额外参数，同时显着增强其时态信息处理能力。通过严格的实验，TIM 证明了其在利用时间信息方面的有效性，从而在各种神经形态数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11687v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo**<br />
**Title_cn:** MVSFormer++：揭示 Transformer 多视图立体细节中的魔鬼<br />
**Authors:** Chenjie Cao, Xinlin Ren, Yanwei Fu<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于学习的多视图立体（MVS）方法的最新进展突出地突出了具有注意机制的基于变压器的模型。然而，现有方法尚未彻底研究变压器对不同 MVS 模块的深远影响，导致深度估计能力有限。在本文中，我们介绍了 MVSFormer++，这​​是一种谨慎地最大化注意力固有特征的方法，以增强 MVS 管道的各个组件。形式上，我们的方法涉及将交叉视图信息注入预先训练的 DINOv2 模型中，以促进 MVS 学习。此外，我们对特征编码器和成本量正则化采用不同的注意机制，分别关注特征和空间聚合。此外，我们发现一些设计细节会极大地影响 MVS 中 Transformer 模块的性能，包括归一化 3D 位置编码、自适应注意力缩放和层归一化的位置。在DTU、Tanks-and-Temples、BlishedMVS和ETH3D上的综合实验验证了该方法的有效性。值得注意的是，MVSFormer++ 在具有挑战性的 DTU 和 Tanks-and-Temples 基准测试中实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.11673v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning**<br />
**Title_cn:** OnDev-LCT：面向联邦学习的设备上轻量级卷积变压器<br />
**Authors:** Chu Myaet Thwal, Minh N. H. Nguyen, Ye Lin Tun, Seong Tae Kim, My T. Thai, Choong Seon Hong<br />
**Abstract:** <details><summary>原文: </summary>Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy. The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher computational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL. Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources. Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images. Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks.</details>
**Abstract_cn:** <details><summary>译文: </summary>联邦学习 (FL) 已成为一种很有前途的方法，可以跨多个边缘设备协作训练机器学习模型，同时保护隐私。 FL 的成功取决于参与模型的效率及其处理分布式学习独特挑战的能力。虽然 Vision Transformer (ViT) 的几种变体显示出作为现代卷积神经网络 (CNN) 集中训练替代品的巨大潜力，但前所未有的规模和更高的计算需求阻碍了它们在资源受限的边缘设备上的部署，从而挑战了它们在 FL 中的广泛应用。由于 FL 中的客户端设备通常具有有限的计算资源和通信带宽，因此用于此类设备的模型必须在模型大小、计算效率以及适应 FL 中遇到的多样化和非 IID 数据分布的能力之间取得平衡。为了应对这些挑战，我们提出了 OnDev-LCT：用于训练数据和资源有限的设备上视觉任务的轻量级卷积变压器。我们的模型通过 LCT 标记器结合图像特定的归纳偏差，利用残余线性瓶颈块中的高效深度可分离卷积来提取局部特征，而 LCT 编码器中的多头自注意力（MHSA）机制隐式地有助于捕获全局表示图片。对基准图像数据集的大量实验表明，我们的模型优于现有的轻量级视觉模型，同时具有较少的参数和较低的计算需求，使其适合具有数据异构性和通信瓶颈的 FL 场景。</details>
**PDF:** <http://arxiv.org/pdf/2401.11652v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Scaling Face Interaction Graph Networks to Real World Scenes**<br />
**Title_cn:** 将人脸交互图网络扩展到现实世界场景<br />
**Authors:** Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff, Kimberly Stachenfeld, Kelsey R. Allen<br />
**Abstract:** <details><summary>原文: </summary>Accurately simulating real world object dynamics is essential for various applications such as robotics, engineering, graphics, and design. To better capture complex real dynamics such as contact and friction, learned simulators based on graph networks have recently shown great promise. However, applying these learned simulators to real scenes comes with two major challenges: first, scaling learned simulators to handle the complexity of real world scenes which can involve hundreds of objects each with complicated 3D shapes, and second, handling inputs from perception rather than 3D state information. Here we introduce a method which substantially reduces the memory required to run graph-based learned simulators. Based on this memory-efficient simulation model, we then present a perceptual interface in the form of editable NeRFs which can convert real-world scenes into a structured representation that can be processed by graph network simulator. We show that our method uses substantially less memory than previous graph-based simulators while retaining their accuracy, and that the simulators learned in synthetic environments can be applied to real world scenes captured from multiple camera angles. This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确模拟现实世界的物体动力学对于机器人、工程、图形和设计等各种应用至关重要。为了更好地捕捉接触和摩擦等复杂的真实动态，基于图网络的学习模拟器最近显示出了巨大的前景。然而，将这些学习的模拟器应用于真实场景会带来两个主要挑战：首先，扩展学习的模拟器以处理现实世界场景的复杂性，其中可能涉及数百个对象，每个对象都具有复杂的 3D 形状；其次，处理来自感知而不是 3D 的输入状态信息。在这里，我们介绍一种方法，可以大大减少运行基于图的学​​习模拟器所需的内存。基于这种节省内存的模拟模型，我们以可编辑 NeRF 的形式呈现一个感知界面，它可以将现实世界场景转换为可由图网络模拟器处理的结构化表示。我们表明，我们的方法使用的内存比以前基于图形的模拟器要少得多，同时保持了准确性，并且在合成环境中学习的模拟器可以应用于从多个摄像机角度捕获的现实世界场景。这为将学习模拟器的应用扩展到推理时仅提供感知信息的设置铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2401.11985v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep**<br />
**Title_cn:** 通过视差平面扫描对端到端立体匹配网络的立体置信度进行建模<br />
**Authors:** Jae Young Lee, Woonghyun Ka, Jaehyun Choi, Junmo Kim<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map. Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed. 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement. Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的立体置信度，可以在各种立体匹配网络的外部进行测量，为基于学习的方法（尤其是在安全关键系统中）提供成本量的替代输入模式选择。基于视差定义和视差平面扫描的基本概念，所提出的立体置信方法建立在这样的思想之上：立体图像对中的任何移位都应该以视差图中相应的移位量进行更新。基于这个想法，所提出的立体置信方法可以概括为三个方面。 1）使用视差平面扫描，可以获得多个视差图并将其视为3D体积（预测视差体积），就像构造成本体积一样。 2）这些视差图之一充当锚点，使我们能够在每个空间点定义理想的（或理想的）视差轮廓。 3）通过比较期望的和预测的视差分布，我们可以量化左右图像之间的匹配模糊度水平以进行置信度测量。使用各种立体匹配网络和数据集的大量实验结果表明，所提出的立体置信方法不仅本身具有竞争性能，而且当其用作基于学习的立体置信方法的输入模态时，也具有一致的性能改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.12001v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method**<br />
**Title_cn:** 观测引导的站级气象场降尺度：基准和新方法<br />
**Authors:** Zili Liu, Hao Chen, Lei Bai, Wenyuan Li, Keyan Chen, Zhengyi Wang, Wanli Ouyang, Zhengxia Zou, Zhenwei Shi<br />
**Abstract:** <details><summary>原文: </summary>Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.</details>
**Abstract_cn:** <details><summary>译文: </summary>气象变量的降尺度（DS）涉及从低分辨率气象场获取高分辨率状态，是天气预报中的一项重要任务。先前基于深度学习的方法将降尺度视为计算机视觉中的超分辨率任务，并利用高分辨率网格气象场作为监督来提高特定网格尺度下的分辨率。然而，这种方法难以适应气象场的连续分布特征，导致降尺度结果与气象站的实际观测之间存在固有的系统偏差。在本文中，我们将气象降尺度扩展到任意分散的站点尺度，建立全新的基准和数据集，并从粗分辨率气象场中检索任意给定站点位置的气象状态。受数据同化技术的启发，我们将观测数据整合到降尺度过程中，提供多尺度观测先验。在此基础上，我们提出了一种基于超网络架构的新型降尺度模型，即HyperDS，它将不同的观测信息有效地整合到模型训练中，实现了气象领域的连续尺度建模。通过大量的实验，我们提出的方法在多个表面变量上优于其他专门设计的基线模型。值得注意的是，与其他方法相比，风速和表面压力的均方误差 (MSE) 分别提高了 67% 和 19.5%。我们将随后发布数据集和代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.11960v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Local Agnostic Video Explanations: a Study on the Applicability of Removal-Based Explanations to Video**<br />
**Title_cn:** 局部不可知视频解释：基于移除的解释对视频的适用性研究<br />
**Authors:** F. Xavier Gaya-Morey, Jose M. Buades-Rubio, Cristina Manresa-Yee<br />
**Abstract:** <details><summary>原文: </summary>Explainable artificial intelligence techniques are becoming increasingly important with the rise of deep learning applications in various domains. These techniques aim to provide a better understanding of complex "black box" models and enhance user trust while maintaining high learning performance. While many studies have focused on explaining deep learning models in computer vision for image input, video explanations remain relatively unexplored due to the temporal dimension's complexity. In this paper, we present a unified framework for local agnostic explanations in the video domain. Our contributions include: (1) Extending a fine-grained explanation framework tailored for computer vision data, (2) Adapting six existing explanation techniques to work on video data by incorporating temporal information and enabling local explanations, and (3) Conducting an evaluation and comparison of the adapted explanation methods using different models and datasets. We discuss the possibilities and choices involved in the removal-based explanation process for visual data. The adaptation of six explanation methods for video is explained, with comparisons to existing approaches. We evaluate the performance of the methods using automated metrics and user-based evaluation, showing that 3D RISE, 3D LIME, and 3D Kernel SHAP outperform other methods. By decomposing the explanation process into manageable steps, we facilitate the study of each choice's impact and allow for further refinement of explanation methods to suit specific datasets and models.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着深度学习应用在各个领域的兴起，可解释的人工智能技术变得越来越重要。这些技术旨在更好地理解复杂的“黑匣子”模型并增强用户信任，同时保持较高的学习性能。虽然许多研究都集中在解释计算机视觉中用于图像输入的深度学习模型，但由于时间维度的复杂性，视频解释仍然相对未经探索。在本文中，我们提出了一个用于视频领域局部不可知解释的统一框架。我们的贡献包括：（1）扩展为计算机视觉数据量身定制的细粒度解释框架，（2）通过合并时间信息并启用本地解释，采用六种现有的解释技术来处理视频数据，以及（3）进行评估和分析使用不同模型和数据集的适应解释方法的比较。我们讨论视觉数据基于移除的解释过程中涉及的可能性和选择。解释了六种视频解释方法的适应，并与现有方法进行了比较。我们使用自动化指标和基于用户的评估来评估这些方法的性能，结果表明 3D RISE、3D LIME 和 3D Kernel SHAP 优于其他方法。通过将解释过程分解为可管理的步骤，我们促进了对每个选择的影响的研究，并允许进一步细化解释方法以适应特定的数据集和模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.11796v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective**<br />
**Title_cn:** 图视角的稀疏感知全身运动重建<br />
**Authors:** Feiyu Yao, Zongkai Wu, Li Yi<br />
**Abstract:** <details><summary>原文: </summary>Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>从稀疏传感器数据估计 3D 全身姿势是增强现实和虚拟现实中重建真实人体运动的关键技术。然而，将稀疏的传感器信号转换为全面的人体运动仍然是一个挑战，因为常见 VR 系统中稀疏分布的传感器无法捕获完整的人体运动。在本文中，我们使用精心设计的身体姿势图（BPG）来表示人体，并将挑战转化为图缺失节点的预测问题。然后，我们提出了一种基于 BPG 的新型全身运动重建框架。为了建立 BPG，节点最初被赋予从稀疏传感器信号中提取的特征。来自不同传感器的可识别关节节点的特征从时间和空间角度进行合并和处理。使用时间金字塔结构捕获时间动态，而关节运动的空间关系则告知空间属性。由此产生的特征充当 BPG 节点的基本元素。为了进一步细化 BPG，节点特征通过图神经网络进行更新，该网络包含反映不同关节关系的边缘。我们的方法的有效性通过所获得的最先进的性能得到了证明，特别是在下半身运动方面，优于其他基线方法。此外，消融研究验证了我们提出的框架中每个模块的功效。</details>
**PDF:** <http://arxiv.org/pdf/2401.11783v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **PointGL: A Simple Global-Local Framework for Efficient Point Cloud Analysis**<br />
**Title_cn:** PointGL：用于高效点云分析的简单全局局部框架<br />
**Authors:** Jianan Li, Jie Wang, Tingfa Xu<br />
**Abstract:** <details><summary>原文: </summary>Efficient analysis of point clouds holds paramount significance in real-world 3D applications. Currently, prevailing point-based models adhere to the PointNet++ methodology, which involves embedding and abstracting point features within a sequence of spatially overlapping local point sets, resulting in noticeable computational redundancy. Drawing inspiration from the streamlined paradigm of pixel embedding followed by regional pooling in Convolutional Neural Networks (CNNs), we introduce a novel, uncomplicated yet potent architecture known as PointGL, crafted to facilitate efficient point cloud analysis. PointGL employs a hierarchical process of feature acquisition through two recursive steps. First, the Global Point Embedding leverages straightforward residual Multilayer Perceptrons (MLPs) to effectuate feature embedding for each individual point. Second, the novel Local Graph Pooling technique characterizes point-to-point relationships and abstracts regional representations through succinct local graphs. The harmonious fusion of one-time point embedding and parameter-free graph pooling contributes to PointGL's defining attributes of minimized model complexity and heightened efficiency. Our PointGL attains state-of-the-art accuracy on the ScanObjectNN dataset while exhibiting a runtime that is more than 5 times faster and utilizing only approximately 4% of the FLOPs and 30% of the parameters compared to the recent PointMLP model. The code for PointGL is available at https://github.com/Roywangj/PointGL.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云的有效分析在现实 3D 应用中具有至关重要的意义。目前，流行的基于点的模型遵循 PointNet++ 方法，该方法涉及在一系列空间重叠的局部点集内嵌入和抽象点特征，从而导致明显的计算冗余。受卷积神经网络 (CNN) 中像素嵌入和区域池化的简化范例的启发，我们引入了一种新颖、简单但有效的架构，称为 PointGL，旨在促进高效的点云分析。 PointGL 通过两个递归步骤采用层次化的特征获取过程。首先，全局点嵌入利用简单的残差多层感知器（MLP）来实现每个单独点的特征嵌入。其次，新颖的局部图池化技术描述了点对点关系，并通过简洁的局部图抽象出区域表示。一次性点嵌入和无参数图池的和谐融合有助于 PointGL 最小化模型复杂性和提高效率的定义属性。与最新的 PointMLP 模型相比，我们的 PointGL 在 ScanObjectNN 数据集上实现了最先进的精度，同时运行时间快了 5 倍以上，并且仅利用了大约 4% 的 FLOP 和 30% 的参数。 PointGL 的代码可在 https://github.com/Roywangj/PointGL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.11650v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Momentum-SAM: Sharpness Aware Minimization without Computational Overhead**<br />
**Title_cn:** Momentum-SAM：锐度感知最小化，无需计算开销<br />
**Authors:** Marlon Becker, Frederick Altrock, Benjamin Risse<br />
**Abstract:** <details><summary>原文: </summary>The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近提出的深度神经网络锐度感知最小化（SAM）优化算法建议在梯度计算之前通过梯度上升步骤扰动参数，以引导优化进入平坦损失的参数空间区域。虽然可以证明显着的泛化改进，从而减少过拟合，但由于额外需要梯度计算，计算成本加倍，使得 SAM 在计算能力有限的情况下不可行。受 Nesterov 加速梯度 (NAG) 的启发，我们提出了 Momentum-SAM (MSAM)，它会扰动累积动量向量方向上的参数，以实现低锐度，而无需比 SGD 或 Adam 显着的计算开销或内存需求。我们详细评估了 MSAM，并揭示了 NAG、SAM 和 MSAM 在训练优化和泛化方面的可分离机制的见解。代码可在 https://github.com/MarlonBecker/MSAM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12033v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Training-Free Defense Framework for Robust Learned Image Compression**<br />
**Title_cn:** 用于鲁棒学习图像压缩的免训练防御框架<br />
**Authors:** Myungseo Song, Jinyoung Choi, Bohyung Han<br />
**Abstract:** <details><summary>原文: </summary>We study the robustness of learned image compression models against adversarial attacks and present a training-free defense technique based on simple image transform functions. Recent learned image compression models are vulnerable to adversarial attacks that result in poor compression rate, low reconstruction quality, or weird artifacts. To address the limitations, we propose a simple but effective two-way compression algorithm with random input transforms, which is conveniently applicable to existing image compression models. Unlike the na\"ive approaches, our approach preserves the original rate-distortion performance of the models on clean images. Moreover, the proposed algorithm requires no additional training or modification of existing models, making it more practical. We demonstrate the effectiveness of the proposed techniques through extensive experiments under multiple compression models, evaluation metrics, and attack scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究了学习图像压缩模型针对对抗性攻击的鲁棒性，并提出了一种基于简单图像变换函数的免训练防御技术。最近学习的图像压缩模型很容易受到对抗性攻击，从而导致压缩率低、重建质量低或奇怪的伪影。为了解决这些限制，我们提出了一种简单但有效的具有随机输入变换的双向压缩算法，该算法可以方便地应用于现有的图像压缩模型。与原始方法不同，我们的方法保留了模型在干净图像上的原始率失真性能。此外，所提出的算法不需要对现有模型进行额外的训练或修改，使其更加实用。我们证明了该方法的有效性通过在多种压缩模型、评估指标和攻击场景下进行大量实验提出了技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.11902v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Adaptive Fusion of Multi-view Remote Sensing data for Optimal Sub-field Crop Yield Prediction**<br />
**Title_cn:** 多视图遥感数据的自适应融合用于最佳子田作物产量预测<br />
**Authors:** Francisco Mena, Deepak Pathak, Hiba Najjar, Cristhian Sanchez, Patrick Helber, Benjamin Bischke, Peter Habelitz, Miro Miranda, Jayanth Siddamsetty, Marlon Nuske, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Accurate crop yield prediction is of utmost importance for informed decision-making in agriculture, aiding farmers, and industry stakeholders. However, this task is complex and depends on multiple factors, such as environmental conditions, soil properties, and management practices. Combining heterogeneous data views poses a fusion challenge, like identifying the view-specific contribution to the predictive task. We present a novel multi-view learning approach to predict crop yield for different crops (soybean, wheat, rapeseed) and regions (Argentina, Uruguay, and Germany). Our multi-view input data includes multi-spectral optical images from Sentinel-2 satellites and weather data as dynamic features during the crop growing season, complemented by static features like soil properties and topographic information. To effectively fuse the data, we introduce a Multi-view Gated Fusion (MVGF) model, comprising dedicated view-encoders and a Gated Unit (GU) module. The view-encoders handle the heterogeneity of data sources with varying temporal resolutions by learning a view-specific representation. These representations are adaptively fused via a weighted sum. The fusion weights are computed for each sample by the GU using a concatenation of the view-representations. The MVGF model is trained at sub-field level with 10 m resolution pixels. Our evaluations show that the MVGF outperforms conventional models on the same task, achieving the best results by incorporating all the data sources, unlike the usual fusion results in the literature. For Argentina, the MVGF model achieves an R2 value of 0.68 at sub-field yield prediction, while at field level evaluation (comparing field averages), it reaches around 0.80 across different countries. The GU module learned different weights based on the country and crop-type, aligning with the variable significance of each data source to the prediction task.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的作物产量预测对于农业明智决策、帮助农民和行业利益相关者至关重要。然而，这项任务很复杂，取决于多种因素，例如环境条件、土壤特性和管理实践。组合异构数据视图带来了融合挑战，例如识别视图特定对预测任务的贡献。我们提出了一种新颖的多视图学习方法来预测不同作物（大豆、小麦、油菜籽）和地区（阿根廷、乌拉圭和德国）的作物产量。我们的多视图输入数据包括来自 Sentinel-2 卫星的多光谱光学图像和天气数据，作为作物生长季节的动态特征，并辅以土壤特性和地形信息等静态特征。为了有效地融合数据，我们引入了多视图门控融合（MVGF）模型，包括专用视图编码器和门控单元（GU）模块。视图编码器通过学习特定于视图的表示来处理具有不同时间分辨率的数据源的异构性。这些表示通过加权和自适应地融合。 GU 使用视图表示的串联来计算每个样本的融合权重。 MVGF 模型在子场级别以 10 m 分辨率像素进行训练。我们的评估表明，MVGF 在相同任务上优于传统模型，通过合并所有数据源实现了最佳结果，这与文献中通常的融合结果不同。对于阿根廷来说，MVGF模型在子田产量预测中的R2值为0.68，而在田间评估（比较田间平均值）时，不同国家的R2值达到0.80左右。 GU 模块根据国家和作物类型学习不同的权重，与每个数据源对预测任务的可变重要性保持一致。</details>
**PDF:** <http://arxiv.org/pdf/2401.11844v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Boosting Multi-view Stereo with Late Cost Aggregation**<br />
**Title_cn:** 通过后期成本聚合增强多视图立体效果<br />
**Authors:** Jiang Wu, Rui Li, Yu Zhu, Wenxun Zhao, Jinqiu Sun, Yanning Zhang<br />
**Abstract:** <details><summary>原文: </summary>Pairwise matching cost aggregation is a crucial step for modern learning-based Multi-view Stereo (MVS). Prior works adopt an early aggregation scheme, which adds up pairwise costs into an intermediate cost. However, we analyze that this process can degrade informative pairwise matchings, thereby blocking the depth network from fully utilizing the original geometric matching cues.To address this challenge, we present a late aggregation approach that allows for aggregating pairwise costs throughout the network feed-forward process, achieving accurate estimations with only minor changes of the plain CasMVSNet.Instead of building an intermediate cost by weighted sum, late aggregation preserves all pairwise costs along a distinct view channel. This enables the succeeding depth network to fully utilize the crucial geometric cues without loss of cost fidelity. Grounded in the new aggregation scheme, we propose further techniques addressing view order dependence inside the preserved cost, handling flexible testing views, and improving the depth filtering process. Despite its technical simplicity, our method improves significantly upon the baseline cascade-based approach, achieving comparable results with state-of-the-art methods with favorable computation overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>成对匹配成本聚合是现代基于学习的多视图立体（MVS）的关键步骤。先前的工作采用早期聚合方案，将成对成本加起来为中间成本。然而，我们分析认为，这个过程会降低成对匹配的信息量，从而阻止深度网络充分利用原始的几何匹配线索。为了解决这一挑战，我们提出了一种后期聚合方法，该方法允许聚合整个网络前馈的成对成本过程中，只需对普通 CasMVSNet 进行微小的更改即可实现准确的估计。后期聚合不是通过加权和构建中间成本，而是保留沿不同视图通道的所有成对成本。这使得后续的深度网络能够充分利用关键的几何线索，而不会损失成本保真度。基于新的聚合方案，我们提出了进一步的技术，解决保留成本内的视图顺序依赖性，处理灵活的测试视图，并改进深度过滤过程。尽管技术简单，但我们的方法在基于级联的基线方法的基础上有了显着改进，以有利的计算开销实现了与最先进的方法相当的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.11751v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multi-level Cross-modal Alignment for Image Clustering**<br />
**Title_cn:** 图像聚类的多级跨模态对齐<br />
**Authors:** Liping Qiu, Qin Zhang, Xiaojun Chen, Shaotian Cai<br />
**Abstract:** <details><summary>原文: </summary>Recently, the cross-modal pretraining model has been employed to produce meaningful pseudo-labels to supervise the training of an image clustering model. However, numerous erroneous alignments in a cross-modal pre-training model could produce poor-quality pseudo-labels and degrade clustering performance. To solve the aforementioned issue, we propose a novel \textbf{Multi-level Cross-modal Alignment} method to improve the alignments in a cross-modal pretraining model for downstream tasks, by building a smaller but better semantic space and aligning the images and texts in three levels, i.e., instance-level, prototype-level, and semantic-level. Theoretical results show that our proposed method converges, and suggests effective means to reduce the expected clustering risk of our method. Experimental results on five benchmark datasets clearly show the superiority of our new method.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，跨模态预训练模型已被用来产生有意义的伪标签来监督图像聚类模型的训练。然而，跨模式预训练模型中的大量错误对齐可能会产生质量差的伪标签并降低聚类性能。为了解决上述问题，我们提出了一种新颖的 \textbf{多级跨模态对齐} 方法，通过构建更小但更好的语义空间并对齐图像和图像来改进下游任务的跨模态预训练模型中的对齐。文本分为三个级别，即实例级别、原型级别和语义级别。理论结果表明我们提出的方法是收敛的，并提出了降低我们方法的预期聚类风险的有效方法。五个基准数据集的实验结果清楚地表明了我们新方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.11740v1><br />
**Code:** null<br />

