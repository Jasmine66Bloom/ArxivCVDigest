## [UPDATED!] **2024-01-23** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **GALA: Generating Animatable Layered Assets from a Single Scan**<br />
**Title_cn:** GALA：通过单次扫描生成可动画化的分层资源<br />
**Authors:** Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo<br />
**Abstract:** <details><summary>原文: </summary>We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 GALA，这是一个框架，它将单层服装 3D 人体网格作为输入，并将其分解为完整的多层 3D 资产。然后，输出可以与其他资产结合起来，创建具有任何姿势的新颖的服装人类化身。现有的重建方法通常将穿着衣服的人类视为单层几何体，而忽略了人类与发型、服装和配饰的固有组合性，从而限制了网格在下游应用中的实用性。将单层网格分解为单独的层是一项具有挑战性的任务，因为它需要为严重遮挡的区域合成合理的几何形状和纹理。此外，即使分解成功，网格在姿势和身体形状方面也没有标准化，无法与新颖的身份和姿势进行连贯的组合。为了应对这些挑战，我们建议利用预训练的二维扩散模型的一般知识作为人类和其他资产的几何和外观先验。我们首先使用从多视图 2D 分割中提取的 3D 表面分割来分离输入网格。然后，我们使用新颖的姿势引导分数蒸馏采样（SDS）损失来合成姿势空间和规范空间中不同层的缺失几何形状。一旦我们完成了高保真 3D 几何体的修复，我们还会对其纹理应用相同的 SDS 损失以获得完整的外观，包括最初被遮挡的区域。通过一系列分解步骤，我们在共享规范空间中获得了多层 3D 资产，并根据姿势和人体形状进行了规范化，从而支持轻松组合新的身份并以新的姿势复活。我们的实验证明了与现有解决方案相比，我们的分解、标准化和组合任务方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.12979v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SegmentAnyBone: A Universal Model that Segments Any Bone at Any Location on MRI**<br />
**Title_cn:** SegmentAnyBone：一种通用模型，可在 MRI 上的任何位置分割任何骨骼<br />
**Authors:** Hanxue Gu, Roy Colglazier, Haoyu Dong, Jikai Zhang, Yaqian Chen, Zafer Yildiz, Yuwen Chen, Lin Li, Jichen Yang, Jay Willhite, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Magnetic Resonance Imaging (MRI) is pivotal in radiology, offering non-invasive and high-quality insights into the human body. Precise segmentation of MRIs into different organs and tissues would be highly beneficial since it would allow for a higher level of understanding of the image content and enable important measurements, which are essential for accurate diagnosis and effective treatment planning. Specifically, segmenting bones in MRI would allow for more quantitative assessments of musculoskeletal conditions, while such assessments are largely absent in current radiological practice. The difficulty of bone MRI segmentation is illustrated by the fact that limited algorithms are publicly available for use, and those contained in the literature typically address a specific anatomic area. In our study, we propose a versatile, publicly available deep-learning model for bone segmentation in MRI across multiple standard MRI locations. The proposed model can operate in two modes: fully automated segmentation and prompt-based segmentation. Our contributions include (1) collecting and annotating a new MRI dataset across various MRI protocols, encompassing over 300 annotated volumes and 8485 annotated slices across diverse anatomic regions; (2) investigating several standard network architectures and strategies for automated segmentation; (3) introducing SegmentAnyBone, an innovative foundational model-based approach that extends Segment Anything Model (SAM); (4) comparative analysis of our algorithm and previous approaches; and (5) generalization analysis of our algorithm across different anatomical locations and MRI sequences, as well as an external dataset. We publicly release our model at https://github.com/mazurowski-lab/SegmentAnyBone.</details>
**Abstract_cn:** <details><summary>译文: </summary>磁共振成像 (MRI) 在放射学中至关重要，可提供对人体的非侵入性高质量洞察。将 MRI 精确分割成不同的器官和组织将非常有益，因为它可以提高对图像内容的理解并实现重要的测量，这对于准确的诊断和有效的治疗计划至关重要。具体来说，在 MRI 中分割骨骼将允许对肌肉骨骼状况进行更定量的评估，而这种评估在当前的放射学实践中基本上不存在。公开使用的算法有限，并且文献中包含的算法通常针对特定的解剖区域，这一事实说明了骨 MRI 分割的困难。在我们的研究中，我们提出了一种通用的、公开可用的深度学习模型，用于跨多个标准 MRI 位置的 MRI 骨分割。所提出的模型可以以两种模式运行：全自动分割和基于提示的分割。我们的贡献包括 (1) 跨各种 MRI 协议收集和注释新的 MRI 数据集，涵盖不同解剖区域的 300 多个带注释的卷和 8485 个带注释的切片； (2) 研究几种标准网络架构和自动分段策略； (3) 引入 SegmentAnyBone，这是一种基于模型的创新方法，扩展了 Segment Anything Model (SAM)； （4）我们的算法和以前的方法的比较分析； (5) 对不同解剖位置和 MRI 序列以及外部数据集的算法进行泛化分析。我们在 https://github.com/mazurowski-lab/SegmentAnyBone 公开发布我们的模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.12974v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Neural deformation fields for template-based reconstruction of cortical surfaces from MRI**<br />
**Title_cn:** 用于基于 MRI 皮质表面模板重建的神经变形场<br />
**Authors:** Fabian Bongratz, Anne-Marie Rickmann, Christian Wachinger<br />
**Abstract:** <details><summary>原文: </summary>The reconstruction of cortical surfaces is a prerequisite for quantitative analyses of the cerebral cortex in magnetic resonance imaging (MRI). Existing segmentation-based methods separate the surface registration from the surface extraction, which is computationally inefficient and prone to distortions. We introduce Vox2Cortex-Flow (V2C-Flow), a deep mesh-deformation technique that learns a deformation field from a brain template to the cortical surfaces of an MRI scan. To this end, we present a geometric neural network that models the deformation-describing ordinary differential equation in a continuous manner. The network architecture comprises convolutional and graph-convolutional layers, which allows it to work with images and meshes at the same time. V2C-Flow is not only very fast, requiring less than two seconds to infer all four cortical surfaces, but also establishes vertex-wise correspondences to the template during reconstruction. In addition, V2C-Flow is the first approach for cortex reconstruction that models white matter and pial surfaces jointly, therefore avoiding intersections between them. Our comprehensive experiments on internal and external test data demonstrate that V2C-Flow results in cortical surfaces that are state-of-the-art in terms of accuracy. Moreover, we show that the established correspondences are more consistent than in FreeSurfer and that they can directly be utilized for cortex parcellation and group analyses of cortical thickness.</details>
**Abstract_cn:** <details><summary>译文: </summary>皮质表面的重建是磁共振成像（MRI）中大脑皮层定量分析的先决条件。现有的基于分割的方法将表面配准与表面提取分开，这在计算上效率低下并且容易失真。我们介绍了 Vox2Cortex-Flow (V2C-Flow)，这是一种深度网格变形技术，可以学习从大脑模板到 MRI 扫描的皮质表面的变形场。为此，我们提出了一种几何神经网络，以连续方式对描述变形的常微分方程进行建模。该网络架构包括卷积层和图卷积层，这使得它可以同时处理图像和网格。 V2C-Flow 不仅速度非常快，只需不到两秒即可推断出所有四个皮质表面，而且还能在重建过程中与模板建立顶点对应关系。此外，V2C-Flow 是第一个联合模拟白质和软脑膜表面的皮层重建方法，从而避免它们之间的交叉。我们对内部和外部测试数据进行的综合实验表明，V2C-Flow 产生的皮质表面在准确性​​方面是最先进的。此外，我们表明所建立的对应关系比 FreeSurfer 中的更加一致，并且它们可以直接用于皮层分割和皮层厚度的分组分析。</details>
**PDF:** <http://arxiv.org/pdf/2401.12938v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Segmentation of tibiofemoral joint tissues from knee MRI using MtRA-Unet and incorporating shape information: Data from the Osteoarthritis Initiative**<br />
**Title_cn:** 使用 MtRA-Unet 对膝 MRI 中的胫股关节组织进行分割并结合形状信息：来自骨关节炎倡议的数据<br />
**Authors:** Akshay Daydar, Alik Pramanick, Arijit Sur, Subramani Kanagaraj<br />
**Abstract:** <details><summary>原文: </summary>Knee Osteoarthritis (KOA) is the third most prevalent Musculoskeletal Disorder (MSD) after neck and back pain. To monitor such a severe MSD, a segmentation map of the femur, tibia and tibiofemoral cartilage is usually accessed using the automated segmentation algorithm from the Magnetic Resonance Imaging (MRI) of the knee. But, in recent works, such segmentation is conceivable only from the multistage framework thus creating data handling issues and needing continuous manual inference rendering it unable to make a quick and precise clinical diagnosis. In order to solve these issues, in this paper the Multi-Resolution Attentive-Unet (MtRA-Unet) is proposed to segment the femur, tibia and tibiofemoral cartilage automatically. The proposed work has included a novel Multi-Resolution Feature Fusion (MRFF) and Shape Reconstruction (SR) loss that focuses on multi-contextual information and structural anatomical details of the femur, tibia and tibiofemoral cartilage. Unlike previous approaches, the proposed work is a single-stage and end-to-end framework producing a Dice Similarity Coefficient (DSC) of 98.5% for the femur, 98.4% for the tibia, 89.1% for Femoral Cartilage (FC) and 86.1% for Tibial Cartilage (TC) for critical MRI slices that can be helpful to clinicians for KOA grading. The time to segment MRI volume (160 slices) per subject is 22 sec. which is one of the fastest among state-of-the-art. Moreover, comprehensive experimentation on the segmentation of FC and TC which is of utmost importance for morphology-based studies to check KOA progression reveals that the proposed method has produced an excellent result with binary segmentation</details>
**Abstract_cn:** <details><summary>译文: </summary>膝骨关节炎 (KOA) 是继颈部和背部疼痛之后第三大最常见的肌肉骨骼疾病 (MSD)。为了监测如此严重的 MSD，通常使用膝关节磁共振成像 (MRI) 的自动分割算法来获取股骨、胫骨和胫股软骨的分割图。但是，在最近的工作中，这种分割只能从多级框架中实现，从而产生数据处理问题，并且需要持续的手动推理，使其无法做出快速而精确的临床诊断。为了解决这些问题，本文提出了多分辨率关注Unet（MtRA-Unet）来自动分割股骨、胫骨和胫股软骨。拟议的工作包括新颖的多分辨率特征融合（MRFF）和形状重建（SR）损失，重点关注股骨、胫骨和胫股软骨的多上下文信息和结构解剖细节。与之前的方法不同，所提出的工作是一个单阶段端到端框架，股骨的 Dice 相似系数 (DSC) 为 98.5%，胫骨为 98.4%，股骨软骨 (FC) 为 89.1%，股骨为 86.1%。关键 MRI 切片的胫骨软骨 (TC) 百分比，有助于临床医生进行 KOA 分级。对每个受试者的 MRI 体积（160 个切片）进行分段的时间为 22 秒。这是最先进的最快之一。此外，对 FC 和 TC 分割的综合实验（对于检查 KOA 进展的基于形态学的研究至关重要）表明，所提出的方法在二元分割中产生了出色的结果</details>
**PDF:** <http://arxiv.org/pdf/2401.12932v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?**<br />
**Title_cn:** 面对房间里的大象：视觉提示调整还是全面微调？<br />
**Authors:** Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, Dongfang Liu<br />
**Abstract:** <details><summary>原文: </summary>As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning. However, the conditions favoring VPT (the ``when") and the underlying rationale (the ``why") remain unclear. In this paper, we conduct a comprehensive analysis across 19 distinct datasets and tasks. To understand the ``when" aspect, we identify the scenarios where VPT proves favorable by two dimensions: task objectives and data distributions. We find that VPT is preferrable when there is 1) a substantial disparity between the original and the downstream task objectives (e.g., transitioning from classification to counting), or 2) a similarity in data distributions between the two tasks (e.g., both involve natural images). In exploring the ``why" dimension, our results indicate VPT's success cannot be attributed solely to overfitting and optimization considerations. The unique way VPT preserves original features and adds parameters appears to be a pivotal factor. Our study provides insights into VPT's mechanisms, and offers guidance for its optimal utilization.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着视觉模型规模的不断增长，视觉提示调整（VPT）作为一种参数高效的迁移学习技术的出现，由于其相对于传统全微调的优越性能而受到关注。然而，有利于 VPT 的条件（“何时”）和基本原理（“为什么”）仍不清楚。在本文中，我们对 19 个不同的数据集和任务进行了全面分析。为了理解“何时”方面，我们通过两个维度来确定 VPT 被证明是有利的场景：任务目标和数据分布。我们发现，当存在 1) 原始任务目标和下游任务目标之间存在巨大差异时，VPT 更可取（例如，从分类过渡到计数），或 2）两个任务之间数据分布的相似性（例如，都涉及自然图像）。在探索“为什么”维度时，我们的结果表明 VPT 的成功不能仅仅归因于过度拟合和优化考虑。 VPT 保留原始特征并添加参数的独特方式似乎是一个关键因素。我们的研究深入了解了 VPT 的机制，并为其最佳利用提供了指导。</details>
**PDF:** <http://arxiv.org/pdf/2401.12902v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Unlocking the Potential: Multi-task Deep Learning for Spaceborne Quantitative Monitoring of Fugitive Methane Plumes**<br />
**Title_cn:** 释放潜力：用于星载逃逸甲烷羽流定量监测的多任务深度学习<br />
**Authors:** Guoxin Si, Shiliang Fu, Wei Yao<br />
**Abstract:** <details><summary>原文: </summary>With the intensification of global warming, the monitoring of methane emission and detection of gas plumes from landfills have increasingly received attention. We decompose methane emission monitoring into three sub-tasks: methane concentration inversion, plume segmentation, and emission rate estimation. Conventional algorithms have limitations: methane concentration inversion usually uses the matched filter, which is sensitive to global spectrum distribution and contains a large amount of noises. There is limited research on plume segmentation, with many studies resorting to manual segmentation that is likely to be subjective. The estimation of methane emission rate often utilizes IME algorithm, which relies on obtaining meteorological measurement data. Using the WENT landfill site in Hong Kong and PRISMA hyperspectral satellite imagery, we propose a new deep learning-based framework for quantitative monitoring of methane emissions from remote sensing images based on physical simulation. We generate simulated methane plumes using large eddy simulation (LES) and different concentration maps of fugitive emission using the radiative transfer equation (RTE), while combining augmentation techniques to create a simulated PRISMA dataset. We train a U-Net network for methane concentration inversion, a Mask R-CNN network for methane plume segmentation, and a ResNet-50 network for methane emission rate estimation. All three deep networks achieve higher validation accuracy compared to conventional algorithms. We further respectively combine the first two sub-tasks and the last two sub-tasks to design the multi-task learning models - MTL-01 and MTL-02, both of which achieve higher accuracy than single-task models. Our research serves as a demonstration of applying multi-task deep learning to quantitative methane monitoring and can be extended to a broad range of methane monitoring tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着全球变暖的加剧，甲烷排放的监测和垃圾填埋场气体羽流的检测日益受到关注。我们将甲烷排放监测分解为三个子任务：甲烷浓度反演、羽流分割和排放率估算。传统算法存在局限性：甲烷浓度反演通常采用匹配滤波器，该滤波器对全局谱分布敏感，且包含大量噪声。关于羽流分割的研究有限，许多研究采用可能是主观的手动分割。甲烷排放率的估算通常采用IME算法，该算法依赖于获取气象测量数据。利用香港WENT垃圾填埋场和PRISMA高光谱卫星图像，我们提出了一种基于深度学习的新框架，用于基于物理模拟的遥感图像定量监测甲烷排放。我们使用大涡模拟 (LES) 生成模拟甲烷羽流，并使用辐射传输方程 (RTE) 生成不同逸散排放浓度图，同时结合增强技术创建模拟 PRISMA 数据集。我们训练用于甲烷浓度反演的 U-Net 网络、用于甲烷羽流分割的 Mask R-CNN 网络以及用于甲烷排放率估计的 ResNet-50 网络。与传统算法相比，这三个深度网络都实现了更高的验证精度。我们进一步分别结合前两个子任务和后两个子任务来设计多任务学习模型——MTL-01和MTL-02，它们都比单任务模型获得了更高的精度。我们的研究是将多任务深度学习应用于定量甲烷监测的示范，并且可以扩展到广泛的甲烷监测任务。</details>
**PDF:** <http://arxiv.org/pdf/2401.12870v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Classification of grapevine varieties using UAV hyperspectral imaging**<br />
**Title_cn:** 利用无人机高光谱成像对葡萄品种进行分类<br />
**Authors:** Alfonso López, Carlos Javier Ogayar, Francisco Ramón Feito, Joaquim João Sousa<br />
**Abstract:** <details><summary>原文: </summary>The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spatial and spectral features is addressed with 1) a spatial attention layer and 2) Inception blocks. The pipeline goes from processing to dataset elaboration, finishing with the training phase. The fitted model is evaluated in terms of response time, accuracy and data separability, and compared with other state-of-the-art CNNs for classifying hyperspectral data. Our network was proven to be much more lightweight with a reduced number of input bands, a lower number of trainable weights and therefore, reduced training time. Despite this, the evaluated metrics showed much better results for our network (~99% overall accuracy), in comparison with previous works barely achieving 81% OA.</details>
**Abstract_cn:** <details><summary>译文: </summary>不同葡萄品种的分类是精准葡萄栽培中的一项相关表型分析任务，因为它可以估计专门用于不同品种的葡萄园行的生长，以及与葡萄酒行业相关的其他应用。该任务可以通过破坏性方法来执行，这些方法需要耗时的任务，包括实验室中的数据收集和分析。然而，无人机 (UAV) 提供了一种更有效且更宽松的方法来收集高光谱数据，尽管它获取的数据噪声更大。因此，首要任务是对这些数据进行处理，以对大量数据进行校正和下采样。此外，葡萄品种的高光谱特征非常相似。在这项工作中，提出了一种卷积神经网络（CNN）来对 17 个红葡萄品种和白葡萄品种进行分类。这些样本不是对单个样本进行分类，而是与其邻近样本一起进行处理。因此，空间和光谱特征的提取通过 1) 空间注意力层和 2) Inception 块来解决。该管道从处理到数据集细化，最后到训练阶段。拟合模型在响应时间、准确性和数据可分离性方面进行了评估，并与其他用于高光谱数据分类的最先进的 CNN 进行了比较。事实证明，我们的网络更加轻量级，输入频段数量减少，可训练权重数量减少，因此训练时间也减少了。尽管如此，评估指标显示我们的网络取得了更好的结果（总体准确率约为 99%），而之前的工作仅勉强实现 81% OA。</details>
**PDF:** <http://arxiv.org/pdf/2401.12851v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer**<br />
**Title_cn:** DatUS^2：数据驱动的无监督语义分割与预训练的自监督视觉 Transformer<br />
**Authors:** Sonal Kumar, Arijit Sur, Rashmi Dutta Baruah<br />
**Abstract:** <details><summary>原文: </summary>Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>多个自监督培训方案的提案不断涌现，距离开发通用基础模型又近了一步。在此过程中，无监督下游任务被认为是验证通过自监督训练方案学习的视觉特征质量的评估方法之一。然而，无监督密集语义分割尚未作为下游任务进行探索，它可以在视觉变换器的自监督训练期间利用和评估块级特征表示中引入的语义信息的质量。因此，本文提出了一种新的数据驱动的无监督语义分割方法（DatUS^2）作为下游任务。 DatUS^2 为未标记的图像数据集生成语义一致且密集的伪注释分割掩模，而不使用任何视觉先验或同步数据。我们将这些伪注释的分割掩码与真实掩码进行比较，以评估最近的自监督训练方案，以学习补丁级别的共享语义属性和分段级别的判别语义属性。最后，我们使用我们提出的下游任务（即 DatUS^2）评估现有最先进的自监督训练方案。此外，DatUS^2 的最佳版本在无监督密集语义分割任务中优于现有最先进的方法，在 SUIM 数据集上具有 15.02% MiOU 和 21.47% 像素精度。对于大规模且复杂的数据集（即 COCO 数据集），它还实现了具有竞争力的准确性水平。</details>
**PDF:** <http://arxiv.org/pdf/2401.12820v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty**<br />
**Title_cn:** MUSES：用于不确定性驾驶的多传感器语义感知数据集<br />
**Authors:** Tim Brödermann, David Bruggemann, Christos Sakaridis, Kevin Ta, Odysseas Liagouris, Jason Corkill, Luc Van Gool<br />
**Abstract:** <details><summary>原文: </summary>Achieving level-5 driving automation in autonomous vehicles necessitates a robust semantic visual perception system capable of parsing data from different sensors across diverse conditions. However, existing semantic perception datasets often lack important non-camera modalities typically used in autonomous vehicles, or they do not exploit such modalities to aid and improve semantic annotations in challenging conditions. To address this, we introduce MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse conditions under increased uncertainty. MUSES includes synchronized multimodal recordings with 2D panoptic annotations for 2500 images captured under diverse weather and illumination. The dataset integrates a frame camera, a lidar, a radar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic annotation protocol captures both class-level and instance-level uncertainty in the ground truth and enables the novel task of uncertainty-aware panoptic segmentation we introduce, along with standard semantic and panoptic segmentation. MUSES proves both effective for training and challenging for evaluating models under diverse visual conditions, and it opens new avenues for research in multimodal and uncertainty-aware dense semantic perception. Our dataset and benchmark will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>要在自动驾驶汽车中实现 5 级驾驶自动化，需要一个强大的语义视觉感知系统，能够在不同条件下解析来自不同传感器的数据。然而，现有的语义感知数据集通常缺乏自动驾驶汽车中通常使用的重要的非相机模态，或者它们没有利用此类模态来帮助和改进具有挑战性的条件下的语义注释。为了解决这个问题，我们引入了 MUSES，即多传感器语义感知数据集，用于在不确定性增加的不利条件下驾驶。 MUSES 包括同步多模态记录和 2D 全景注释，可记录在不同天气和照明下捕获的 2500 张图像。该数据集集成了帧相机、激光雷达、雷达、事件相机和 IMU/GNSS 传感器。我们新的两阶段全景注释协议捕获了真实情况中的类级和实例级不确定性，并实现了我们引入的不确定性感知全景分割的新任务，以及标准语义和全景分割。事实证明，MUSES 对于训练有效，并且在不同视觉条件下评估模型具有挑战性，它为多模态和不确定性感知的密集语义感知的研究开辟了新的途径。我们的数据集和基准将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.12761v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Correlation-Embedded Transformer Tracking: A Single-Branch Framework**<br />
**Title_cn:** 相关嵌入式变压器跟踪：单分支框架<br />
**Authors:** Fei Xie, Wankou Yang, Chunyu Wang, Lei Chu, Yue Cao, Chao Ma, Wenjun Zeng<br />
**Abstract:** <details><summary>原文: </summary>Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking. In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously. While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer. Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction. The output features can be directly used for predicting target locations without additional correlation steps. Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features. A unified relation modeling is proposed to remove complex handcrafted layer pattern designs. SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads. Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves superior results on eight VOT benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发鲁棒且具有辨别力的外观模型一直是视觉对象跟踪领域长期存在的研究挑战。在流行的基于暹罗的范式中，类暹罗网络提取的特征通常不足以对跟踪目标和干扰对象进行建模，从而阻碍了它们同时具有鲁棒性和判别性。虽然大多数暹罗跟踪器专注于设计强大的相关操作，但我们提出了一种受变压器启发的新颖的单分支跟踪框架。与类似 Siamese 的特征提取不同，我们的跟踪器将跨图像特征相关性深度嵌入到特征网络的多层中。通过多层对两幅图像的特征进行广泛匹配，可以抑制非目标特征，从而实现目标感知特征提取。输出特征可以直接用于预测目标位置，无需额外的相关步骤。因此，我们将两分支连体跟踪重新表述为概念上简单、完全基于变压器的单分支跟踪管道，称为 SBT。在对 SBT 基线进行深入分析后，我们总结了许多有效的设计原则，并提出了一种改进的跟踪器 SuperSBT。 SuperSBT采用具有局部建模层的分层架构来增强浅层特征。提出了一种统一的关系建模来消除复杂的手工层模式设计。 SuperSBT通过掩模图像建模预训练、集成时间建模以及配备专用预测头进一步改进。因此，SuperSBT 在 LaSOT、TrackingNet 和 GOT-10K 中的 AUC 分数比 SBT 基线高出 4.7%、3.0% 和 4.5%。值得注意的是，SuperSBT 将 SBT 的速度从 37 FPS 大幅提升至 81 FPS。大量实验表明，我们的方法在八个 VOT 基准上取得了优异的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.12743v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios**<br />
**Title_cn:** 通过合成数据生成和比例类平衡技术增强小物体的物体检测性能：工业场景的比较研究<br />
**Authors:** Jibinraj Antony, Vinit Hegiste, Ali Nazeri, Hooman Tavakoli, Snehal Walunj, Christiane Plociennik, Martin Ruskowski<br />
**Abstract:** <details><summary>原文: </summary>Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable better anchor matching of the OD models. A comparison was carried out on the performances of the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and synthetic datasets within an industrial use case.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象检测（OD）已被证明是提取本地化类别信息的重要计算机视觉方法，并且在行业中具有多种应用。尽管许多最先进的 (SOTA) OD 模型在中型和大型物体上表现良好，但它们似乎在小型物体上表现不佳。在大多数工业用例中，收集和注释小对象的数据很困难，因为这既耗时又容易出现人为错误。此外，这些数据集可能不平衡，并且常常导致模型收敛效率低下。为了应对这一挑战，本研究提出了一种新颖的方法，注入额外的数据点来提高 OD 模型的性能。使用合成数据生成，可以最大限度地减少小对象数据点的数据收集和注释的难度，并创建分布均衡的数据集。本文讨论了简单的比例类平衡技术的效果，以实现 OD 模型更好的锚匹配。针对工业用例中真实数据集和合成数据集的组合，对 SOTA OD 模型（YOLOv5、YOLOv7 和 SSD）的性能进行了比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.12729v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization**<br />
**Title_cn:** 基于双视图拓扑图的解剖引导 CT 重建，实现前瞻性风险最小化<br />
**Authors:** Chang Liu, Laura Klein, Yixing Huang, Edith Baader, Michael Lell, Marc Kachelrieß, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了促进 CT 有效剂量和风险最小化过程的前瞻性估计，需要前瞻性空间剂量估计和已知的解剖结构。为此，需要一种 CT 重建方法来从尽可能少的投影（即使用地形图）重建 CT 体积，并具有尽可能正确的解剖结构。在这项工作中，提出了一种基于生成对抗网络（GAN）的优化CT重建模型。 GAN 经过训练，可以根据前后和横向 CT 投影重建 3D 体积。为了增强解剖结构，在训练阶段应用预训练的器官分割网络和 3D 感知损失，以便模型可以生成器官增强 CT 体积和器官分割掩模。该方法可以重建 CT 体积，PSNR 为 26.49，RMSE 为 196.17，SSIM 为 0.64，而使用基线方法的结果为 26.21、201.55 和 0.63。在解剖结构方面，该方法有效增强了器官形状和边界，并可以直接识别相关解剖结构。我们注意到，传统的重建指标无法表明解剖结构的增强。除了这些指标之外，还通过评估器官分割性能来扩展评估。该方法的平均器官骰子为0.71，而基线模型为0.63，表明解剖结构得到增强。</details>
**PDF:** <http://arxiv.org/pdf/2401.12725v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Pragmatic Communication in Multi-Agent Collaborative Perception**<br />
**Title_cn:** 多智能体协作感知中的语用沟通<br />
**Authors:** Yue Hu, Xianghe Pang, Xiaoqi Qin, Yonina C. Eldar, Siheng Chen, Ping Zhang, Wenjun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Collaborative perception allows each agent to enhance its perceptual abilities by exchanging messages with others. It inherently results in a trade-off between perception ability and communication costs. Previous works transmit complete full-frame high-dimensional feature maps among agents, resulting in substantial communication costs. To promote communication efficiency, we propose only transmitting the information needed for the collaborator's downstream task. This pragmatic communication strategy focuses on three key aspects: i) pragmatic message selection, which selects task-critical parts from the complete data, resulting in spatially and temporally sparse feature vectors; ii) pragmatic message representation, which achieves pragmatic approximation of high-dimensional feature vectors with a task-adaptive dictionary, enabling communicating with integer indices; iii) pragmatic collaborator selection, which identifies beneficial collaborators, pruning unnecessary communication links. Following this strategy, we first formulate a mathematical optimization framework for the perception-communication trade-off and then propose PragComm, a multi-agent collaborative perception system with two key components: i) single-agent detection and tracking and ii) pragmatic collaboration. The proposed PragComm promotes pragmatic communication and adapts to a wide range of communication conditions. We evaluate PragComm for both collaborative 3D object detection and tracking tasks in both real-world, V2V4Real, and simulation datasets, OPV2V and V2X-SIM2.0. PragComm consistently outperforms previous methods with more than 32.7K times lower communication volume on OPV2V. Code is available at github.com/PhyllisH/PragComm.</details>
**Abstract_cn:** <details><summary>译文: </summary>协作感知允许每个代理通过与其他代理交换消息来增强其感知能力。它本质上导致感知能力和通信成本之间的权衡。以前的工作在代理之间传输完整的全帧高维特征图，导致大量的通信成本。为了提高通信效率，我们建议仅传输协作者下游任务所需的信息。这种实用的通信策略侧重于三个关键方面：i）实用的消息选择，从完整的数据中选择任务关键部分，从而产生空间和时间上稀疏的特征向量； ii) 实用消息表示，通过任务自适应字典实现高维特征向量的实用近似，从而能够与整数索引进行通信； iii) 务实的合作者选择，识别有益的合作者，修剪不必要的沟通链接。按照这一策略，我们首先制定一个用于感知-通信权衡的数学优化框架，然后提出 PragComm，一个多智能体协作感知系统，具有两个关键组件：i）单智能体检测和跟踪以及 ii）务实协作。拟议的 PragComm 提倡务实的沟通并适应广泛的沟通条件。我们评估了 PragComm 在现实世界（V2V4Real）和模拟数据集（OPV2V 和 V2X-SIM2.0）中的协作 3D 对象检测和跟踪任务。 PragComm 的性能始终优于以前的方法，OPV2V 上的通信量降低了 32.7K 倍以上。代码可在 github.com/PhyllisH/PragComm 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12694v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Energy-based Automated Model Evaluation**<br />
**Title_cn:** 基于能量的自动化模型评估<br />
**Authors:** Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao<br />
**Abstract:** <details><summary>原文: </summary>The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习模型的传统评估协议严重依赖于带标签的、独立同分布假设的测试数据集，而这在现实世界的应用中并不常见。自动模型评估 (AutoEval) 展示了这种传统工作流程的替代方案，通过在不存在真实标签的情况下形成测试性能的近端预测管道。尽管 AutoEval 框架最近取得了成功，但仍然存在过度自信问题、大量存储和计算成本。在这方面，我们提出了一种新颖的措施——元分布式能源（MDE）——使 AutoEval 框架更加高效和有效。 MDE 的核心是针对与各个样本相关的信息（能量）建立元分布统计，然后通过基于能量的学习提供更平滑的表示。我们通过将 MDE 与分类损失联系起来，进一步提供我们的理论见解。我们提供了跨模式、数据集和不同架构主干的广泛实验，以验证 MDE 的有效性以及与先前方法相比的优越性。我们还通过展示 MDE 与大型模型的无缝集成以及轻松适应带有噪声或不平衡标签的学习场景来证明 MDE 的多功能性。</details>
**PDF:** <http://arxiv.org/pdf/2401.12689v1><br />
**Code:** <https://github.com/pengr/energy_autoeval>**<br />
>>**index:** 15<br />
**Title:** **ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation**<br />
**Title_cn:** ClipSAM：CLIP 和 SAM 协作进行零样本异常分割<br />
**Authors:** Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen<br />
**Abstract:** <details><summary>原文: </summary>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，CLIP 和 SAM 等基础模型在零样本异常分割（ZSAS）任务中表现出了良好的性能。然而，无论是基于CLIP还是基于SAM的ZSAS方法仍然存在不可忽视的关键缺点：1）CLIP主要关注不同输入之间的全局特征对齐，导致局部异常部分的分割不精确； 2) SAM 在没有适当提示约束的情况下往往会生成大量冗余掩模，从而导致复杂的后处理要求。在这项工作中，我们创新性地提出了一个名为 ClipSAM for ZSAS 的 CLIP 和 SAM 协作框架。 ClipSAM背后的见解是利用CLIP的语义理解能力进行异常定位和粗分割，进一步用作SAM的提示约束来细化异常分割结果。具体来说，我们引入了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在 CLIP 的多个尺度上将语言与视觉特征进行交互，以推理异常位置。然后，我们设计了一种新颖的多级掩模细化（MMR）模块，该模块利用位置信息作为SAM的多级提示来获取掩模的层次级别并将它们合并。大量实验验证了我们方法的有效性，在 MVTec-AD 和 VisA 数据集上实现了最佳分割性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.12665v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Self-Supervised Vision Transformers Are Efficient Segmentation Learners for Imperfect Labels**<br />
**Title_cn:** 自监督视觉变压器是不完美标签的有效分割学习器<br />
**Authors:** Seungho Lee, Seoungyoon Kang, Hyunjung Shim<br />
**Abstract:** <details><summary>原文: </summary>This study demonstrates a cost-effective approach to semantic segmentation using self-supervised vision transformers (SSVT). By freezing the SSVT backbone and training a lightweight segmentation head, our approach effectively utilizes imperfect labels, thereby improving robustness to label imperfections. Empirical experiments show significant performance improvements over existing methods for various annotation types, including scribble, point-level, and image-level labels. The research highlights the effectiveness of self-supervised vision transformers in dealing with imperfect labels, providing a practical and efficient solution for semantic segmentation while reducing annotation costs. Through extensive experiments, we confirm that our method outperforms baseline models for all types of imperfect labels. Especially under the zero-shot vision-language-model-based label, our model exhibits 11.5\%p performance gain compared to the baseline.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究展示了一种使用自监督视觉转换器（SSVT）进行语义分割的经济有效的方法。通过冻结 SSVT 主干并训练轻量级分割头，我们的方法有效地利用了不完美的标签，从而提高了对标签缺陷的鲁棒性。实证实验表明，与各种注释类型（包括涂鸦、点级和图像级标签）的现有方法相比，性能有了显着提高。该研究强调了自监督视觉转换器在处理不完美标签方面的有效性，为语义分割提供了实用且高效的解决方案，同时降低了注释成本。通过大量的实验，我们确认我们的方法对于所有类型的不完美标签都优于基线模型。特别是在基于零样本视觉语言模型的标签下，我们的模型与基线相比表现出 11.5\%p 的性能增益。</details>
**PDF:** <http://arxiv.org/pdf/2401.12535v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR**<br />
**Title_cn:** 使用 YOLOv8、DeiT 和 SimCLR 检测和识别希腊纸莎草中的字符<br />
**Authors:** Robert Turnbull, Evelyn Mannix<br />
**Abstract:** <details><summary>原文: </summary>The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the `ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri' was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision (mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5, we achieved the highest mean average precision and mean average recall results for both detection and classification. We ran our prediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to illustrate the utility of our approach, and we release the results publicly in multiple formats.</details>
**Abstract_cn:** <details><summary>译文: </summary>从纸莎草手稿的传真图像中分离和识别单个字符的能力为数字分析提供了丰富的机会。因此，作为第 17 届国际文献分析与识别会议的一部分，举办了“ICDAR 2023 纸莎草希腊字母检测和识别竞赛”。本文讨论了我们提交的竞赛内容。我们使用 YOLOv8 模型集合来检测和分类单个字符，并采用两种不同的方法来细化字符预测，包括基于 Transformer 的 DeiT 方法和使用 SimCLR（一种自学习工具）在大量未标记数据上训练的 ResNet-50 模型。监督学习方法。我们提交的作品以 42.2% 的 mAP 赢得了识别挑战赛，并以 51.4% 的平均精度 (mAP) 在检测挑战赛中获得亚军。在比并集阈值 0.5 更宽松的交集处，我们实现了检测和分类的最高平均精度和平均召回率结果。我们对来自 Oxyrhynchus Papyri 的 4,500 多张图像运行了预测管道，以说明我们方法的实用性，并以多种格式公开发布结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.12513v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Open-Set Facial Expression Recognition**<br />
**Title_cn:** 开放集面部表情识别<br />
**Authors:** Yuhang Zhang, Yue Yao, Xuannan Liu, Lixiong Qin, Wenjing Wang, Weihong Deng<br />
**Abstract:** <details><summary>原文: </summary>Facial expression recognition (FER) models are typically trained on datasets with a fixed number of seven basic classes. However, recent research works point out that there are far more expressions than the basic ones. Thus, when these models are deployed in the real world, they may encounter unknown classes, such as compound expressions that cannot be classified into existing basic classes. To address this issue, we propose the open-set FER task for the first time. Though there are many existing open-set recognition methods, we argue that they do not work well for open-set FER because FER data are all human faces with very small inter-class distances, which makes the open-set samples very similar to close-set samples. In this paper, we are the first to transform the disadvantage of small inter-class distance into an advantage by proposing a new way for open-set FER. Specifically, we find that small inter-class distance allows for sparsely distributed pseudo labels of open-set samples, which can be viewed as symmetric noisy labels. Based on this novel observation, we convert the open-set FER to a noisy label detection problem. We further propose a novel method that incorporates attention map consistency and cycle training to detect the open-set samples. Extensive experiments on various FER datasets demonstrate that our method clearly outperforms state-of-the-art open-set recognition methods by large margins. Code is available at https://github.com/zyh-uaiaaaa.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部表情识别（FER）模型通常在具有固定数量的七个基本类别的数据集上进行训练。然而，最近的研究工作指出，表达方式远多于基本表达方式。因此，当这些模型部署在现实世界中时，它们可能会遇到未知的类，例如无法分类到现有基本类的复合表达式。为了解决这个问题，我们首次提出了开放集 FER 任务。尽管现有的开放集识别方法有很多，但我们认为它们对于开放集 FER 效果不佳，因为 FER 数据都是人脸，类间距离非常小，这使得开放集样本与封闭样本非常相似- 设置样本。在本文中，我们首次提出了一种开放集 FER 的新方法，将类间距离较小的缺点转化为优势。具体来说，我们发现较小的类间距离允许开放集样本的稀疏分布的伪标签，这可以被视为对称噪声标签。基于这一新颖的观察，我们将开放集 FER 转换为噪声标签检测问题。我们进一步提出了一种新颖的方法，该方法结合了注意力图一致性和循环训练来检测开放集样本。对各种 FER 数据集的大量实验表明，我们的方法明显明显优于最先进的开放集识别方法。代码可在 https://github.com/zyh-uaiaaaa 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12507v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Small Language Model Meets with Reinforced Vision Vocabulary**<br />
**Title_cn:** 小语言模型与强化视觉词汇的结合<br />
**Authors:** Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, Xiangyu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Playing Large Vision Language Models (LVLMs) in 2023 is trendy among the AI community. However, the relatively large number of parameters (more than 7B) of popular LVLMs makes it difficult to train and deploy on consumer GPUs, discouraging many researchers with limited resources. Imagine how cool it would be to experience all the features of current LVLMs on an old GTX1080ti (our only game card). Accordingly, we present Vary-toy in this report, a small-size Vary along with Qwen-1.8B as the base ``large'' language model. In Vary-toy, we introduce an improved vision vocabulary, allowing the model to not only possess all features of Vary but also gather more generality. Specifically, we replace negative samples of natural images with positive sample data driven by object detection in the procedure of generating vision vocabulary, more sufficiently utilizing the capacity of the vocabulary network and enabling it to efficiently encode visual information corresponding to natural objects. For experiments, Vary-toy can achieve 65.6% ANLS on DocVQA, 59.1% accuracy on ChartQA, 88.1% accuracy on RefCOCO, and 29% on MMVet. The code will be publicly available on the homepage.</details>
**Abstract_cn:** <details><summary>译文: </summary>2023 年，大视觉语言模型（LVLM）在 AI 社区中很流行。然而，流行的 LVLM 的参数数量相对较多（超过 7B），因此很难在消费级 GPU 上进行训练和部署，这让许多资源有限的研究人员望而却步。想象一下，在旧的 GTX1080ti（我们唯一的游戏卡）上体验当前 LVLM 的所有功能会有多酷。因此，我们在本报告中介绍了 Vary-toy，一个小型 Vary 以及 Qwen-1.8B 作为基础“大型”语言模型。在Vary-toy中，我们引入了改进的视觉词汇表，使模型不仅拥有Vary的所有特征，而且还具有更多的通用性。具体来说，我们在生成视觉词汇的过程中用目标检测驱动的正样本数据替换自然图像的负样本，更充分地利用词汇网络的能力，使其能够有效地编码与自然物体相对应的视觉信息。对于实验，Vary-toy 在 DocVQA 上可以达到 65.6% 的 ANLS，在 ChartQA 上达到 59.1% 的准确率，在 RefCOCO 上达到 88.1% 的准确率，在 MMVet 上达到 29% 的准确率。该代码将在主页上公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.12503v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **An Automated Real-Time Approach for Image Processing and Segmentation of Fluoroscopic Images and Videos Using a Single Deep Learning Network**<br />
**Title_cn:** 使用单个深度学习网络对荧光图像和视频进行图像处理和分割的自动化实时方法<br />
**Authors:** Viet Dung Nguyen, Michael T. LaCour, Richard D. Komistek<br />
**Abstract:** <details><summary>原文: </summary>Image segmentation in total knee arthroplasty is crucial for precise preoperative planning and accurate implant positioning, leading to improved surgical outcomes and patient satisfaction. The biggest challenges of image segmentation in total knee arthroplasty include accurately delineating complex anatomical structures, dealing with image artifacts and noise, and developing robust algorithms that can handle anatomical variations and pathologies commonly encountered in patients. The potential of using machine learning for image segmentation in total knee arthroplasty lies in its ability to improve segmentation accuracy, automate the process, and provide real-time assistance to surgeons, leading to enhanced surgical planning, implant placement, and patient outcomes. This paper proposes a methodology to use deep learning for robust and real-time total knee arthroplasty image segmentation. The deep learning model, trained on a large dataset, demonstrates outstanding performance in accurately segmenting both the implanted femur and tibia, achieving an impressive mean-Average-Precision (mAP) of 88.83 when compared to the ground truth while also achieving a real-time segmented speed of 20 frames per second (fps). We have introduced a novel methodology for segmenting implanted knee fluoroscopic or x-ray images that showcases remarkable levels of accuracy and speed, paving the way for various potential extended applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>全膝关节置换术中的图像分割对于精确的术前计划和准确的植入物定位至关重要，从而改善手术结果和患者满意度。全膝关节置换术中图像分割的最大挑战包括准确描绘复杂的解剖结构、处理图像伪影和噪声，以及开发能够处理患者常见的解剖变化和病理的强大算法。在全膝关节置换术中使用机器学习进行图像分割的潜力在于其能够提高分割准确性、自动化流程并为外科医生提供实时帮助，从而改善手术计划、植入物放置和患者结果。本文提出了一种使用深度学习进行稳健且实时的全膝关节置换术图像分割的方法。在大型数据集上训练的深度学习模型在准确分割植入的股骨和胫骨方面表现出了出色的性能，与地面实况相比，实现了令人印象深刻的 88.83 的平均精度 (mAP)，同时还实现了实时分割分段速度为每秒 20 帧 (fps)。我们引入了一种新颖的方法来分割植入的膝关节透视或 X 射线图像，该方法展示了卓越的准确性和速度，为各种潜在的扩展应用铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2401.12488v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Explore Synergistic Interaction Across Frames for Interactive Video Object Segmentation**<br />
**Title_cn:** 探索交互式视频对象分割的跨帧协同交互<br />
**Authors:** Kexin Li, Tao Jiang, Zongxin Yang, Yi Yang, Yueting Zhuang, Jun Xiao<br />
**Abstract:** <details><summary>原文: </summary>Interactive Video Object Segmentation (iVOS) is a challenging task that requires real-time human-computer interaction. To improve the user experience, it is important to consider the user's input habits, segmentation quality, running time and memory consumption.However, existing methods compromise user experience with single input mode and slow running speed. Specifically, these methods only allow the user to interact with one single frame, which limits the expression of the user's intent.To overcome these limitations and better align with people's usage habits, we propose a framework that can accept multiple frames simultaneously and explore synergistic interaction across frames (SIAF). Concretely, we designed the Across-Frame Interaction Module that enables users to annotate different objects freely on multiple frames. The AFI module will migrate scribble information among multiple interactive frames and generate multi-frame masks. Additionally, we employ the id-queried mechanism to process multiple objects in batches. Furthermore, for a more efficient propagation and lightweight model, we design a truncated re-propagation strategy to replace the previous multi-round fusion module, which employs an across-round memory that stores important interaction information. Our SwinB-SIAF achieves new state-of-the-art performance on DAVIS 2017 (89.6%, J&F@60). Moreover, our R50-SIAF is more than 3 faster than the state-of-the-art competitor under challenging multi-object scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>交互式视频对象分割（iVOS）是一项具有挑战性的任务，需要实时人机交互。为了提高用户体验，需要考虑用户的输入习惯、分割质量、运行时间和内存消耗。然而，现有方法输入模式单一、运行速度慢，损害了用户体验。具体来说，这些方法只允许用户与单个框架进行交互，这限制了用户意图的表达。为了克服这些限制并更好地符合人们的使用习惯，我们提出了一种可以同时接受多个框架并探索协同交互的框架跨框架（SIAF）。具体来说，我们设计了跨帧交互模块，使用户能够在多个帧上自由注释不同的对象。 AFI模块将在多个交互帧之间迁移涂鸦信息并生成多帧掩模。此外，我们采用id查询机制来批量处理多个对象。此外，为了更高效的传播和轻量级模型，我们设计了一种截断的重新传播策略来取代之前的多轮融合模块，该模块采用跨轮存储器来存储重要的交互信息。我们的 SwinB-SIAF 在 DAVIS 2017 上实现了新的最先进性能（89.6%，J&F@60）。此外，在具有挑战性的多目标场景下，我们的 R50-SIAF 比最先进的竞争对手快 3 倍以上。</details>
**PDF:** <http://arxiv.org/pdf/2401.12480v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **TD^2-Net: Toward Denoising and Debiasing for Dynamic Scene Graph Generation**<br />
**Title_cn:** TD^2-Net：动态场景图生成的去噪和去偏<br />
**Authors:** Xin Lin, Chong Shi, Yibing Zhan, Zuopeng Yang, Yaqi Wu, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Dynamic scene graph generation (SGG) focuses on detecting objects in a video and determining their pairwise relationships. Existing dynamic SGG methods usually suffer from several issues, including 1) Contextual noise, as some frames might contain occluded and blurred objects. 2) Label bias, primarily due to the high imbalance between a few positive relationship samples and numerous negative ones. Additionally, the distribution of relationships exhibits a long-tailed pattern. To address the above problems, in this paper, we introduce a network named TD$^2$-Net that aims at denoising and debiasing for dynamic SGG. Specifically, we first propose a denoising spatio-temporal transformer module that enhances object representation with robust contextual information. This is achieved by designing a differentiable Top-K object selector that utilizes the gumbel-softmax sampling strategy to select the relevant neighborhood for each object. Second, we introduce an asymmetrical reweighting loss to relieve the issue of label bias. This loss function integrates asymmetry focusing factors and the volume of samples to adjust the weights assigned to individual samples. Systematic experimental results demonstrate the superiority of our proposed TD$^2$-Net over existing state-of-the-art approaches on Action Genome databases. In more detail, TD$^2$-Net outperforms the second-best competitors by 12.7 \% on mean-Recall@10 for predicate classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>动态场景图生成（SGG）专注于检测视频中的对象并确定它们的成对关系。现有的动态 SGG 方法通常会遇到几个问题，包括 1）上下文噪声，因为某些帧可能包含被遮挡和模糊的对象。 2）标签偏差，主要是由于少数正关系样本与大量负关系样本之间的高度不平衡造成的。此外，关系的分布呈现出长尾模式。为了解决上述问题，在本文中，我们引入了一种名为 TD$^2$-Net 的网络，旨在对动态 SGG 进行去噪和去偏。具体来说，我们首先提出了一种去噪时空变换器模块，该模块可以通过强大的上下文信息增强对象表示。这是通过设计一个可微分的 Top-K 对象选择器来实现的，该选择器利用gumbel-softmax 采样策略为每个对象选择相关邻域。其次，我们引入不对称重新加权损失来缓解标签偏差问题。该损失函数集成了不对称聚焦因子和样本量，以调整分配给各个样本的权重。系统的实验结果证明了我们提出的 TD$^2$-Net 相对于 Action Genome 数据库现有最先进方法的优越性。更详细地说，在谓词分类的mean-Recall@10 上，TD$^2$-Net 比第二好的竞争对手高出 12.7%。</details>
**PDF:** <http://arxiv.org/pdf/2401.12479v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Zero Shot Open-ended Video Inference**<br />
**Title_cn:** 零镜头开放式视频推理<br />
**Authors:** Ee Yeo Keat, Zhang Hao, Alexander Matyasko, Basura Fernando<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot open-ended inference on untrimmed videos poses a significant challenge, especially when no annotated data is utilized to navigate the inference direction. In this work, we aim to address this underexplored domain by introducing an adaptable framework that efficiently combines both the frozen vision-language (VL) model and off-the-shelf large language model (LLM) for conducting zero-shot open-ended inference tasks without requiring any additional training or fine-tuning. Our comprehensive experiments span various video action datasets for goal inference and action recognition tasks. The results demonstrate the framework's superior performance in goal inference compared to conventional vision-language models in open-ended and close-ended scenarios. Notably, the proposed framework exhibits the capability to generalize effectively to action recognition tasks, underscoring its versatility and potential contributions to advancing the video-based zero-shot understanding.</details>
**Abstract_cn:** <details><summary>译文: </summary>对未修剪视频的零样本开放式推理提出了重大挑战，特别是当没有使用带注释的数据来导航推理方向时。在这项工作中，我们的目标是通过引入一个适应性强的框架来解决这个尚未开发的领域，该框架有效地结合了冻结视觉语言（VL）模型和现成的大语言模型（LLM），以进行零样本开放式推理任务无需任何额外的培训或微调。我们的综合实验涵盖各种视频动作数据集，用于目标推理和动作识别任务。结果表明，与开放式和封闭式场景中的传统视觉语言模型相比，该框架在目标推理方面具有卓越的性能。值得注意的是，所提出的框架展示了有效推广到动作识别任务的能力，强调了其多功能性和对推进基于视频的零镜头理解的潜在贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.12471v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration**<br />
**Title_cn:** 通过 2D-3D 神经校准进行 LiDAR 3D 点云的自监督学习<br />
**Authors:** Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Guangming Shi<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, named NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid transformation aligning camera and LiDAR coordinate systems. First, we propose the learnable transformation alignment to bridge the domain gap between image and point cloud data, converting features into a unified representation space for effective comparison and matching. Second, we identify the overlapping area between the image and point cloud with the fused features. Third, we establish dense 2D-3D correspondences to estimate the rigid transformation. The framework not only learns fine-grained matching from points to pixels but also achieves alignment of the image and point cloud at a holistic level, understanding their relative pose. We demonstrate NCLR's efficacy by applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D semantic segmentation, object detection, and panoptic segmentation. Comprehensive experiments on various datasets illustrate the superiority of NCLR over existing self-supervised methods. The results confirm that joint learning from different modalities significantly enhances the network's understanding abilities and effectiveness of learned representation. Code will be available at \url{https://github.com/Eaphan/NCLR}.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种新颖的自监督学习框架，用于增强自动驾驶场景中的 3D 感知。具体来说，我们的方法名为 NCLR，专注于 2D-3D 神经校准，这是一种新颖的借口任务，用于估计刚性变换对齐相机和 LiDAR 坐标系。首先，我们提出可学习的变换对齐来弥合图像和点云数据之间的域差距，将特征转换为统一的表示空间以进行有效的比较和匹配。其次，我们利用融合特征识别图像和点云之间的重叠区域。第三，我们建立密集的 2D-3D 对应关系来估计刚性变换。该框架不仅学习从点到像素的细粒度匹配，而且还实现了图像和点云的整体对齐，了解它们的相对姿态。我们通过将预训练的主干应用于下游任务（例如基于 LiDAR 的 3D 语义分割、对象检测和全景分割）来展示 NCLR 的功效。对各种数据集的综合实验说明了 NCLR 相对于现有自我监督方法的优越性。结果证实，不同模式的联合学习显着增强了网络的理解能力和学习表示的有效性。代码可在 \url{https://github.com/Eaphan/NCLR} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12452v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **NIV-SSD: Neighbor IoU-Voting Single-Stage Object Detector From Point Cloud**<br />
**Title_cn:** NIV-SSD：来自点云的邻居 IoU 投票单级物体检测器<br />
**Authors:** Shuai Liu, Di Wang, Quan Wang, Kai Huang<br />
**Abstract:** <details><summary>原文: </summary>Previous single-stage detectors typically suffer the misalignment between localization accuracy and classification confidence. To solve the misalignment problem, we introduce a novel rectification method named neighbor IoU-voting (NIV) strategy. Typically, classification and regression are treated as separate branches, making it challenging to establish a connection between them. Consequently, the classification confidence cannot accurately reflect the regression quality. NIV strategy can serve as a bridge between classification and regression branches by calculating two types of statistical data from the regression output to correct the classification confidence. Furthermore, to alleviate the imbalance of detection accuracy for complete objects with dense points (easy objects) and incomplete objects with sparse points (difficult objects), we propose a new data augmentation scheme named object resampling. It undersamples easy objects and oversamples difficult objects by randomly transforming part of easy objects into difficult objects. Finally, combining the NIV strategy and object resampling augmentation, we design an efficient single-stage detector termed NIV-SSD. Extensive experiments on several datasets indicate the effectiveness of the NIV strategy and the competitive performance of the NIV-SSD detector. The code will be available at https://github.com/Say2L/NIV-SSD.</details>
**Abstract_cn:** <details><summary>译文: </summary>以前的单级检测器通常会遇到定位精度和分类置信度之间的不一致问题。为了解决错位问题，我们引入了一种新颖的校正方法，称为邻居 IoU 投票（NIV）策略。通常，分类和回归被视为独立的分支，因此很难在它们之间建立联系。因此，分类置信度不能准确反映回归质量。 NIV策略可以作为分类和回归分支之间的桥梁，通过从回归输出计算两类统计数据来校正分类置信度。此外，为了缓解具有密集点的完整对象（容易对象）和具有稀疏点的不完整对象（困难对象）的检测精度的不平衡，我们提出了一种新的数据增强方案，称为对象重采样。它通过将部分简单对象随机转换为困难对象，对简单对象进行欠采样，对困难对象进行过采样。最后，结合 NIV 策略和对象重采样增强，我们设计了一种高效的单级检测器，称为 NIV-SSD。对多个数据集的大量实验表明了 NIV 策略的有效性以及 NIV-SSD 检测器的竞争性能。该代码可在 https://github.com/Say2L/NIV-SSD 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12447v1><br />
**Code:** <https://github.com/say2l/niv-ssd>**<br />
>>**index:** 26<br />
**Title:** **MAST: Video Polyp Segmentation with a Mixture-Attention Siamese Transformer**<br />
**Title_cn:** MAST：使用混合注意力 Siamese Transformer 进行视频息肉分割<br />
**Authors:** Geng Chen, Junqing Yang, Xiaozhou Pu, Ge-Peng Ji, Huan Xiong, Yongsheng Pan, Hengfei Cui, Yong Xia<br />
**Abstract:** <details><summary>原文: </summary>Accurate segmentation of polyps from colonoscopy videos is of great significance to polyp treatment and early prevention of colorectal cancer. However, it is challenging due to the difficulties associated with modelling long-range spatio-temporal relationships within a colonoscopy video. In this paper, we address this challenging task with a novel Mixture-Attention Siamese Transformer (MAST), which explicitly models the long-range spatio-temporal relationships with a mixture-attention mechanism for accurate polyp segmentation. Specifically, we first construct a Siamese transformer architecture to jointly encode paired video frames for their feature representations. We then design a mixture-attention module to exploit the intra-frame and inter-frame correlations, enhancing the features with rich spatio-temporal relationships. Finally, the enhanced features are fed to two parallel decoders for predicting the segmentation maps. To the best of our knowledge, our MAST is the first transformer model dedicated to video polyp segmentation. Extensive experiments on the large-scale SUN-SEG benchmark demonstrate the superior performance of MAST in comparison with the cutting-edge competitors. Our code is publicly available at https://github.com/Junqing-Yang/MAST.</details>
**Abstract_cn:** <details><summary>译文: </summary>肠镜视频中息肉的准确分割对于息肉治疗和结直肠癌的早期预防具有重要意义。然而，由于在结肠镜检查视频中建模远程时空关系存在困难，因此它具有挑战性。在本文中，我们使用一种新颖的混合注意暹罗变换器（MAST）来解决这一具有挑战性的任务，它通过混合注意机制明确地模拟远程时空关系，以实现精确的息肉分割。具体来说，我们首先构建一个连体变压器架构来联合编码配对视频帧的特征表示。然后，我们设计了一个混合注意模块来利用帧内和帧间相关性，增强具有丰富时空关系的特征。最后，增强的特征被馈送到两个并行解码器以预测分割图。据我们所知，我们的 MAST 是第一个专用于视频息肉分割的 Transformer 模型。在大型 SUN-SEG 基准上的大量实验证明了 MAST 与尖端竞争对手相比具有优越的性能。我们的代码可在 https://github.com/Junqing-Yang/MAST 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12439v1><br />
**Code:** <https://github.com/junqing-yang/mast>**<br />
>>**index:** 27<br />
**Title:** **The Neglected Tails of Vision-Language Models**<br />
**Title_cn:** 视觉语言模型被忽视的尾巴<br />
**Authors:** Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong<br />
**Abstract:** <details><summary>原文: </summary>Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-to-image generators, also struggle with the rare concepts identified by our method. To mitigate VLMs' imbalanced performance in zero-shot recognition, we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in VLMs' pretraining texts. This already outperforms human-engineered and LLM-generated prompts over nine benchmark datasets, likely because VLMs have seen more images associated with the frequently used synonyms. Second, REAL uses all the concept synonyms to retrieve a small, class-balanced set of pretraining data to train a robust classifier. REAL surpasses the recent retrieval-augmented solution REACT, using 400x less storage and 10,000x less training time!</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型 (VLM) 在零样本识别方面表现出色，但在视觉概念之间表现出严重不平衡的性能。例如，尽管 CLIP 在 ImageNet 上的平均零样本准确率令人印象深刻（72.7%），但在 10 个概念（例如陀螺仪和夜蛇）上的收益率不到 10%，大概是因为这些概念在 VLM 的不平衡中代表性不足。预训练数据。然而，评估这种不平衡具有挑战性，因为计算 VLM 大规模预训练数据中特定概念的频率并非易事。我们的工作首次尝试通过分析预训练文本来测量概念频率。我们使用现成的语言模型来帮助计算包含给定概念同义词的相关文本并解决语言歧义。我们确认像 LAION 这样的流行 VLM 数据集确实表现出长尾概念分布，这与每类的准确度密切相关。此外，当代的多模态系统，例如视觉聊天机器人和文本到图像生成器，也难以解决我们的方法识别的罕见概念。为了缓解 VLM 在零样本识别中的不平衡性能，我们提出 REtrieval-Augmented Learning REAL。首先，REAL 没有提示 VLM 使用原始类名，而是使用 VLM 预训练文本中最常见的同义词。在九个基准数据集上，这已经优于人工设计和法学硕士生成的提示，可能是因为 VLM 已经看到了更多与常用同义词相关的图像。其次，REAL 使用所有概念同义词来检索小型、类平衡的预训练数据集，以训练鲁棒的分类器。 REAL 超越了最近的检索增强解决方案 REACT，使用的存储空间减少了 400 倍，训练时间减少了 10,000 倍！</details>
**PDF:** <http://arxiv.org/pdf/2401.12425v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **A Novel Garment Transfer Method Supervised by Distilled Knowledge of Virtual Try-on Model**<br />
**Title_cn:** 虚拟试穿模型蒸馏知识监督下的新型服装传输方法<br />
**Authors:** Naiyu Fang, Lemiao Qiu, Shuyou Zhang, Zili Wang, Kerui Hu, Jianrong Tan<br />
**Abstract:** <details><summary>原文: </summary>When a shopper chooses garments online, garment transfer technology wears the garment from the model image onto the shopper's image, allowing the shopper to decide whether the garment is suitable for them. As garment transfer leverages wild and cheap person image as garment condition, it has attracted tremendous community attention and holds vast commercial potential. However, since the ground truth of garment transfer is almost unavailable in reality, previous studies have treated garment transfer as either pose transfer or garment-pose disentanglement, and trained garment transfer in self-supervised learning, yet do not cover garment transfer intentions completely. Therefore, the training supervising the garment transfer is a rock-hard issue. Notably, virtual try-on technology has exhibited superior performance using self-supervised learning. We supervise the garment transfer training via knowledge distillation from virtual try-on. Specifically, we first train the transfer parsing reasoning model at multi-phases to provide shape guidance for downstream tasks. The transfer parsing reasoning model learns the response and feature knowledge from the try-on parsing reasoning model and absorbs the hard knowledge from the ground truth. By leveraging the warping knowledge from virtual try-on, we estimate a progressive flow to precisely warp the garment by learning the shape and content correspondence. To enhance transfer realism, we propose a well-designed arm regrowth task to infer exposed skin pixel content. Experiments demonstrate that our method has state-of-the-art performance in transferring garments between person compared with other virtual try-on and garment transfer methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>当购物者在线选择服装时，服装转移技术将模特图像中的服装复制到购物者的图像上，让购物者决定该服装是否适合自己。由于服装转移利用狂野、廉价的人物形象作为服装条件，引起了社会的广泛关注，并具有巨大的商业潜力。然而，由于服装转移的基本事实在现实中几乎不可用，以前的研究将服装转移视为姿势转移或服装姿势解开，并在自监督学习中训练服装转移，但并没有完全涵盖服装转移意图。因此，监督服装转移的培训是一个棘手的问题。值得注意的是，虚拟试穿技术通过自我监督学习表现出了卓越的性能。我们通过虚拟试穿的知识蒸馏来监督服装转移培训。具体来说，我们首先在多阶段训练转移解析推理模型，为下游任务提供形状指导。迁移解析推理模型从试穿解析推理模型中学习响应和特征知识，并从地面事实中吸收硬知识。通过利用虚拟试穿中的变形知识，我们通过学习形状和内容对应关系来估计渐进流程，以精确地变形服装。为了增强转移真实感，我们提出了一个精心设计的手臂再生任务来推断暴露的皮肤像素内容。实验表明，与其他虚拟试穿和服装转移方法相比，我们的方法在人与人之间转移服装方面具有最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.12433v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy**<br />
**Title_cn:** 用于采样自主性的冰月表面模拟和立体深度估计<br />
**Authors:** Ramchander Bhaskara, Georgios Georgakis, Jeremy Nash, Marissa Cameron, Joseph Bowkett, Adnan Ansar, Manoranjan Majji, Paul Backes<br />
**Abstract:** <details><summary>原文: </summary>Sampling autonomy for icy moon lander missions requires understanding of topographic and photometric properties of the sampling terrain. Unavailability of high resolution visual datasets (either bird-eye view or point-of-view from a lander) is an obstacle for selection, verification or development of perception systems. We attempt to alleviate this problem by: 1) proposing Graphical Utility for Icy moon Surface Simulations (GUISS) framework, for versatile stereo dataset generation that spans the spectrum of bulk photometric properties, and 2) focusing on a stereo-based visual perception system and evaluating both traditional and deep learning-based algorithms for depth estimation from stereo matching. The surface reflectance properties of icy moon terrains (Enceladus and Europa) are inferred from multispectral datasets of previous missions. With procedural terrain generation and physically valid illumination sources, our framework can fit a wide range of hypotheses with respect to visual representations of icy moon terrains. This is followed by a study over the performance of stereo matching algorithms under different visual hypotheses. Finally, we emphasize the standing challenges to be addressed for simulating perception data assets for icy moons such as Enceladus and Europa. Our code can be found here: https://github.com/nasa-jpl/guiss.</details>
**Abstract_cn:** <details><summary>译文: </summary>冰冷月球着陆器任务的自主采样需要了解采样地形的地形和光度特性。高分辨率视觉数据集（无论是鸟瞰图还是着陆器的视角）的不可用是选择、验证或开发感知系统的障碍。我们试图通过以下方式缓解这个问题：1）提出冰月表面模拟图形实用程序（GUISS）框架，用于生成跨越体光度特性范围的通用立体数据集，2）专注于基于立体的视觉感知系统和评估传统和基于深度学习的立体匹配深度估计算法。冰冷的卫星地形（土卫二和欧罗巴）的表面反射特性是根据之前任务的多光谱数据集推断出来的。通过程序地形生成和物理上有效的照明源，我们的框架可以适应有关冰冷月球地形的视觉表示的各种假设。接下来是对不同视觉假设下立体匹配算法性能的研究。最后，我们强调模拟土卫二和木卫二等冰冷卫星的感知数据资产需要解决的长期挑战。我们的代码可以在这里找到：https://github.com/nasa-jpl/guiss。</details>
**PDF:** <http://arxiv.org/pdf/2401.12414v1><br />
**Code:** <https://github.com/nasa-jpl/guiss>**<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Zero-Shot Learning for the Primitives of 3D Affordance in General Objects**<br />
**Title_cn:** 一般对象中 3D 可供性基元的零样本学习<br />
**Authors:** Hyeonwoo Kim, Sookwan Han, Patrick Kwon, Hanbyul Joo<br />
**Abstract:** <details><summary>原文: </summary>One of the major challenges in AI is teaching machines to precisely respond and utilize environmental functionalities, thereby achieving the affordance awareness that humans possess. Despite its importance, the field has been lagging in terms of learning, especially in 3D, as annotating affordance accompanies a laborious process due to the numerous variations of human-object interaction. The low availability of affordance data limits the learning in terms of generalization for object categories, and also simplifies the representation of affordance, capturing only a fraction of the affordance. To overcome these challenges, we propose a novel, self-supervised method to generate the 3D affordance examples given only a 3D object, without any manual annotations. The method starts by capturing the 3D object into images and creating 2D affordance images by inserting humans into the image via inpainting diffusion models, where we present the Adaptive Mask algorithm to enable human insertion without altering the original details of the object. The method consequently lifts inserted humans back to 3D to create 3D human-object pairs, where the depth ambiguity is resolved within a depth optimization framework that utilizes pre-generated human postures from multiple viewpoints. We also provide a novel affordance representation defined on relative orientations and proximity between dense human and object points, that can be easily aggregated from any 3D HOI datasets. The proposed representation serves as a primitive that can be manifested to conventional affordance representations via simple transformations, ranging from physically exerted affordances to nonphysical ones. We demonstrate the efficacy of our method and representation by generating the 3D affordance samples and deriving high-quality affordance examples from the representation, including contact, orientation, and spatial occupancies.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能的主要挑战之一是教会机器精确响应和利用环境功能，从而实现人类拥有的可供性意识。尽管它很重要，但该领域在学习方面一直落后，尤其是在 3D 领域，因为由于人与物体交互的众多变化，注释可供性伴随着一个费力的过程。可供性数据的低可用性限制了对象类别泛化方面的学习，并且还简化了可供性的表示，仅捕获了可供性的一小部分。为了克服这些挑战，我们提出了一种新颖的自监督方法，仅在给定 3D 对象的情况下生成 3D 可供性示例，无需任何手动注释。该方法首先将 3D 对象捕获到图像中，并通过修复扩散模型将人体插入到图像中来创建 2D 可供性图像，其中我们提出了自适应掩模算法，以在不改变对象原始细节的情况下实现人体插入。因此，该方法将插入的人体提升回 3D 以创建 3D 人体-物体对，其中深度模糊性在深度优化框架内得到解决，该框架利用从多个视点预先生成的人体姿势。我们还提供了一种新颖的可供性表示，它是根据密集的人和物体点之间的相对方向和接近度定义的，可以轻松地从任何 3D HOI 数据集中聚合。所提出的表示作为一个基元，可以通过简单的转换（从物理施加的可供性到非物理的可供性）来显现为传统的可供性表示。我们通过生成 3D 可供性样本并从表示中导出高质量可供性示例（包括接触、方向和空间占用）来证明我们的方法和表示的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.12978v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Lumiere: A Space-Time Diffusion Model for Video Generation**<br />
**Title_cn:** Lumiere：用于视频生成的时空扩散模型<br />
**Authors:** Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 Lumiere——一种文本到视频的扩散模型，旨在合成描绘真实、多样化和连贯运动的视频——这是视频合成中的关键挑战。为此，我们引入了时空 U-Net 架构，该架构通过模型中的单次传递一次性生成视频的整个时间持续时间。这与现有的视频模型形成鲜明对比，现有的视频模型合成遥远的关键帧，然后进行时间超分辨率——这种方法本质上使全局时间一致性难以实现。通过部署空间和（重要的）时间下采样和上采样，并利用预先训练的文本到图像扩散模型，我们的模型学习通过在多个时空尺度。我们展示了最先进的文本到视频生成结果，并表明我们的设计可以轻松促进各种内容创建任务和视频编辑应用程序，包括图像到视频、视频修复和风格化生成。</details>
**PDF:** <http://arxiv.org/pdf/2401.12945v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators**<br />
**Title_cn:** UniHDA：迈向图像生成器的通用混合域适应<br />
**Authors:** Hengjia Li, Yang Liu, Yuqi Lin, Zhanwei Zhang, Yibo Zhao, weihang Pan, Tu Zheng, Zheng Yang, Yuchun Jiang, Boxi Wu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Generative domain adaptation has achieved remarkable progress, enabling us to adapt a pre-trained generator to a new target domain. However, existing methods simply adapt the generator to a single target domain and are limited to a single modality, either text-driven or image-driven. Moreover, they are prone to overfitting domain-specific attributes, which inevitably compromises cross-domain consistency. In this paper, we propose UniHDA, a unified and versatile framework for generative hybrid domain adaptation with multi-modal references from multiple domains. We use CLIP encoder to project multi-modal references into a unified embedding space and then linear interpolate the direction vectors from multiple target domains to achieve hybrid domain adaptation. To ensure the cross-domain consistency, we propose a novel cross-domain spatial structure (CSS) loss that maintains detailed spatial structure information between source and target generator. Experiments show that the adapted generator can synthesise realistic images with various attribute compositions. Additionally, our framework is versatile to multiple generators, \eg, StyleGAN2 and Diffusion Models.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成域适应取得了显着的进展，使我们能够将预训练的生成器适应新的目标域。然而，现有的方法只是简单地使生成器适应单个目标域，并且仅限于单一模式（文本驱动或图像驱动）。此外，它们很容易过度拟合特定于域的属性，这不可避免地会损害跨域的一致性。在本文中，我们提出了 UniHDA，这是一个统一且通用的框架，用于具有来自多个领域的多模态参考的生成混合域适应。我们使用 CLIP 编码器将多模态参考投影到统一的嵌入空间中，然后对来自多个目标域的方向向量进行线性插值以实现混合域自适应。为了确保跨域一致性，我们提出了一种新颖的跨域空间结构（CSS）损失，它可以维护源和目标生成器之间的详细空间结构信息。实验表明，适应的生成器可以合成具有各种属性组合的逼真图像。此外，我们的框架适用于多种生成器，例如 StyleGAN2 和扩散模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.12596v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Exploration and Improvement of Nerf-based 3D Scene Editing Techniques**<br />
**Title_cn:** 基于Nerf的3D场景编辑技术的探索与改进<br />
**Authors:** Shun Fang, Ming Cui, Xing Feng, Yanan Zhang<br />
**Abstract:** <details><summary>原文: </summary>NeRF's high-quality scene synthesis capability was quickly accepted by scholars in the years after it was proposed, and significant progress has been made in 3D scene representation and synthesis. However, the high computational cost limits intuitive and efficient editing of scenes, making NeRF's development in the scene editing field facing many challenges. This paper reviews the preliminary explorations of scholars on NeRF in the scene or object editing field in recent years, mainly changing the shape and texture of scenes or objects in new synthesized scenes; through the combination of residual models such as GaN and Transformer with NeRF, the generalization ability of NeRF scene editing has been further expanded, including realizing real-time new perspective editing feedback, multimodal editing of text synthesized 3D scenes, 4D synthesis performance, and in-depth exploration in light and shadow editing, initially achieving optimization of indirect touch editing and detail representation in complex scenes. Currently, most NeRF editing methods focus on the touch points and materials of indirect points, but when dealing with more complex or larger 3D scenes, it is difficult to balance accuracy, breadth, efficiency, and quality. Overcoming these challenges may become the direction of future NeRF 3D scene editing technology.</details>
**Abstract_cn:** <details><summary>译文: </summary>NeRF的高质量场景合成能力在提出后的几年里迅速被学者们接受，并在3D场景表示和合成方面取得了重大进展。然而高昂的计算成本限制了场景的直观高效编辑，使得NeRF在场景编辑领域的发展面临诸多挑战。本文回顾了近年来学者们对NeRF在场景或物体编辑领域的初步探索，主要是改变新合成场景中场景或物体的形状和纹理；通过GaN、Transformer等残差模型与NeRF的结合，进一步扩展了NeRF场景编辑的泛化能力，包括实现实时新视角编辑反馈、文本合成3D场景的多模态编辑、4D合成性能以及-在光影编辑方面进行深入探索，初步实现复杂场景下间接触摸编辑和细节表现的优化。目前，大多数 NeRF 编辑方法侧重于间接点的接触点和材质，但在处理更复杂或更大的 3D 场景时，很难平衡准确性、广度、效率和质量。克服这些挑战可能会成为未来NeRF 3D场景编辑技术的方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.12456v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **On the Efficacy of Text-Based Input Modalities for Action Anticipation**<br />
**Title_cn:** 基于文本的输入方式对动作预期的功效<br />
**Authors:** Apoorva Beedu, Karan Samel, Irfan Essa<br />
**Abstract:** <details><summary>原文: </summary>Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices. Each modality provides different environmental context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation. Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions. We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions. Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion. Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets. In addition, we examine the impact of object and action information obtained via text and perform extensive ablations. We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管预测未来行动的任务高度不确定，但来自其他模式的信息有助于缩小合理的行动选择范围。每种模式都提供了不同的环境背景供模型学习。虽然以前的多模态方法利用来自视频和音频等模态的信息，但我们主要探讨动作和对象的文本输入如何也能够实现更准确的动作预期。因此，我们提出了一种多模态预期变换器（MAT），一种基于注意力的视频变换器架构，可以联合学习多模态特征和文本字幕。我们分两个阶段训练模型，模型首先学习通过与字幕对齐来预测视频剪辑中的动作，在第二阶段，我们微调模型以预测未来的动作。与现有方法相比，MAT 的优点是可以从两种文本输入中学习额外的环境上下文：预训练阶段的动作描述，以及模态特征融合期间检测到的对象和动作的文本输入。通过大量的实验，我们评估了预训练阶段的有效性，并表明我们的模型在所有数据集上都优于以前的方法。此外，我们还检查通过文本获得的对象和动作信息的影响，并进行广泛的消融。我们评估了三个数据集的性能：EpicKitchens-100、EpicKitchens-55 和 EGTEA GAZE+；并表明文本描述确实有助于更有效的行动预期。</details>
**PDF:** <http://arxiv.org/pdf/2401.12972v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Red Teaming Visual Language Models**<br />
**Title_cn:** 红队视觉语言模型<br />
**Authors:** Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu<br />
**Abstract:** <details><summary>原文: </summary>VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.</details>
**Abstract_cn:** <details><summary>译文: </summary>VLM（视觉语言模型）扩展了 LLM（大型语言模型）的功能，以接受多模式输入。由于已经证实 LLM 可以通过特定的测试用例（称为红队）诱导生成有害或不准确的内容，因此 VLM 在类似场景中的表现如何，特别是在文本和视觉输入相结合的情况下，仍然是一个问题。为了探讨这个问题，我们提出了一个新颖的红队数据集 RTVLM，它包含 4 个主要方面（忠实、隐私、安全、公平）下的 10 个子任务（例如图像误导、多模式越狱、人脸公平等）。我们的 RTVLM 是第一个在这 4 个不同方面对当前 VLM 进行基准测试的红队数据集。详细分析显示，10 个著名的开源 VLM 都不同程度地与红队作斗争，与 GPT-4V 的性能差距高达 31%。此外，我们简单地使用 RTVLM 将红队对齐与监督微调 (SFT) 应用于 LLaVA-v1.5，这增强了模型的性能，在 RTVLM 测试集中提高了 10%，在 MM-Hal 中提高了 13%，并且没有明显的影响MM-Bench 的下降，超过了具有常规对齐数据的其他基于 LLaVA 的模型。这表明当前的开源 VLM 仍然缺乏红队协调。我们的代码和数据集将是开源的。</details>
**PDF:** <http://arxiv.org/pdf/2401.12915v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units**<br />
**Title_cn:** FedRSU：路边场景流估计的联邦学习<br />
**Authors:** Shaoheng Fang, Rui Ye, Wenhao Wang, Zuhong Liu, Yuxiao Wang, Yafei Wang, Siheng Chen, Yanfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated learning, where multiple devices collaboratively train an ML model while keeping the training data local and private. With the power of the recurrent self-supervised learning paradigm, FL is able to leverage innumerable underutilized data from RSU. To verify the FedRSU framework, we construct a large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU clients, covering various scenarios, modalities, and sensor settings. Based on RSU-SF, we show that FedRSU can greatly improve model performance in ITS and provide a comprehensive benchmark under diverse FL scenarios. To the best of our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset and benchmark for the FL community.</details>
**Abstract_cn:** <details><summary>译文: </summary>路边单元 (RSU) 可以通过车对万物 (V2X) 通信显着提高自动驾驶车辆的安全性和稳健性。目前，单个RSU的使用主要集中在实时推理和V2X协作，而忽略了RSU传感器收集的高质量数据的潜在价值。整合来自众多RSU的海量数据可以为模型训练提供丰富的数据源。然而，缺乏真实注释和传输大量数据的困难是充分利用这一隐藏价值的两个不可避免的障碍。在本文中，我们介绍了 FedRSU，一种用于自监督场景流估计的创新联邦学习框架。在FedRSU中，我们提出了一种循环自监督训练范例，其中对于每个RSU，每个时间戳点的场景流预测可以通过其后续的未来多模态观察来监督。 FedRSU 的另一个关键组成部分是联合学习，其中多个设备协作训练 ML 模型，同时保持训练数据本地和私有。凭借循环自我监督学习范式的强大功能，FL 能够利用 RSU 中无数未充分利用的数据。为了验证 FedRSU 框架，我们构建了一个大规模多模态数据集 RSU-SF。该数据集由 17 个 RSU 客户端组成，涵盖各种场景、模式和传感器设置。基于RSU-SF，我们证明FedRSU可以极大地提高ITS中的模型性能，并在不同的FL场景下提供全面的基准。据我们所知，我们为 FL 社区提供了第一个真实世界的 LiDAR 相机多模态数据集和基准。</details>
**PDF:** <http://arxiv.org/pdf/2401.12862v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis**<br />
**Title_cn:** NeRF-AD：具有基于注意力的解开的神经辐射场，用于说话人脸合成<br />
**Authors:** Chongke Bi, Xiaoxing Liu, Zhilei Liu<br />
**Abstract:** <details><summary>原文: </summary>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.</details>
**Abstract_cn:** <details><summary>译文: </summary>音频驱动的人脸合成是当前多维信号处理和多媒体领域的研究热点之一。神经辐射场 (NeRF) 最近被引入这一研究领域，以增强生成的面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 带来复杂的学习任务，同时缺乏监督多模态特征融合的方法，要么无法精确地将音频映射到与语音运动相关的面部区域。这些原因最终导致现有方法产生不准确的唇部形状。本文将 NeRF 学习任务的一部分向前推进，并提出了一种基于 NeRF 的基于注意力解纠缠的说话人脸合成方法（NeRF-AD）。特别是，引入了基于注意力的解缠模块，使用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。为了精确调节音频如何影响说话的面孔，我们仅将音频面孔与音频功能融合。此外，AU信息也被用来监督这两种模式的融合。广泛的定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人脸视频（包括图像质量和唇形同步）方面优于最先进的方法。查看视频结果请参考https://xiaoxingliu02.github.io/NeRF-AD。</details>
**PDF:** <http://arxiv.org/pdf/2401.12568v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews)**<br />
**Title_cn:** 通过专业标记的视频进行多模式新闻理解 (ReutersViLNews)<br />
**Authors:** Shih-Han Chou, Matthew Kowal, Yasmin Niknam, Diana Moyano, Shayaan Mehdi, Richard Pito, Cheng Zhang, Ian Knopke, Sedef Akinli Kocak, Leonid Sigal, et.al.<br />
**Abstract:** <details><summary>原文: </summary>While progress has been made in the domain of video-language understanding, current state-of-the-art algorithms are still limited in their ability to understand videos at high levels of abstraction, such as news-oriented videos. Alternatively, humans easily amalgamate information from video and language to infer information beyond what is visually observable in the pixels. An example of this is watching a news story, where the context of the event can play as big of a role in understanding the story as the event itself. Towards a solution for designing this ability in algorithms, we present a large-scale analysis on an in-house dataset collected by the Reuters News Agency, called Reuters Video-Language News (ReutersViLNews) dataset which focuses on high-level video-language understanding with an emphasis on long-form news. The ReutersViLNews Dataset consists of long-form news videos collected and labeled by news industry professionals over several years and contains prominent news reporting from around the world. Each video involves a single story and contains action shots of the actual event, interviews with people associated with the event, footage from nearby areas, and more. ReutersViLNews dataset contains videos from seven subject categories: disaster, finance, entertainment, health, politics, sports, and miscellaneous with annotations from high-level to low-level, title caption, visual video description, high-level story description, keywords, and location. We first present an analysis of the dataset statistics of ReutersViLNews compared to previous datasets. Then we benchmark state-of-the-art approaches for four different video-language tasks. The results suggest that news-oriented videos are a substantial challenge for current video-language understanding algorithms and we conclude by providing future directions in designing approaches to solve the ReutersViLNews dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管在视频语言理解领域取得了进展，但当前最先进的算法在理解高抽象级别的视频（例如面向新闻的视频）的能力方面仍然受到限制。或者，人类可以轻松地合并来自视频和语言的信息，以推断超出像素中视觉可观察到的信息。一个例子是观看新闻报道，其中事件的背景在理解故事方面与事件本身一样发挥着重要作用。为了在算法中设计这种能力的解决方案，我们对路透社收集的内部数据集进行了大规模分析，该数据集称为路透社视频语言新闻 (ReutersViLNews) 数据集，该数据集专注于高级视频语言理解重点是长篇新闻。 ReutersViLNews 数据集由新闻行业专业人士多年来收集和标记的长篇新闻视频组成，包含来自世界各地的重要新闻报道。每个视频都涉及一个故事，并包含实际事件的动作镜头、对与该事件相关的人员的采访、附近地区的镜头等等。 ReutersViLNews 数据集包含来自七个主题类别的视频：灾难、金融、娱乐、健康、政治、体育和杂项，并带有从高级到低级的注释、标题说明、视觉视频描述、高级故事描述、关键字和地点。我们首先对ReutersViLNews 的数据集统计数据与之前的数据集进行比较分析。然后，我们对四种不同视频语言任务的最先进方法进行基准测试。结果表明，面向新闻的视频对当前视频语言理解算法来说是一个重大挑战，我们通过提供解决ReutersViLNews数据集的设计方法的未来方向来得出结论。</details>
**PDF:** <http://arxiv.org/pdf/2401.12419v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments**<br />
**Title_cn:** HAZARD 挑战：动态变化环境中的具体决策<br />
**Authors:** Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.</details>
**Abstract_cn:** <details><summary>译文: </summary>高保真虚拟环境的最新进展是构建智能实体来感知、推理并与物理世界交互的主要驱动力之一。通常，这些环境保持不变，除非代理与它们交互。然而，在现实场景中，代理还可能面临以意外事件为特征的动态变化的环境，需要快速采取相应的行动。为了弥补这一差距，我们提出了一种新的模拟体现基准，称为 HAZARD，专门用于评估动态情况下体现主体的决策能力。 HAZARD由火灾、洪水和风三种意外灾害场景组成，特别支持利用大语言模型（LLM）来辅助常识推理和决策。该基准使我们能够评估自主代理跨各种管道的决策能力，包括动态变化环境中的强化学习（RL）、基于规则和基于搜索的方法。作为使用大型语言模型应对这一挑战的第一步，我们进一步开发了一个基于 LLM 的代理，并对其解决这些具有挑战性的任务的承诺和挑战进行了深入分析。 HAZARD 可在 https://vis-www.cs.umass.edu/hazard/ 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.12975v1><br />
**Code:** <https://github.com/umass-foundation-model/hazard>**<br />
>>**index:** 2<br />
**Title:** **AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents**<br />
**Title_cn:** AutoRT：机器人代理大规模编排的具体基础模型<br />
**Authors:** Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.</details>
**Abstract_cn:** <details><summary>译文: </summary>结合了语言、视觉和最近的行动的基础模型彻底改变了利用互联网规模数据来推理有用任务的能力。然而，训练具体基础模型的关键挑战之一是缺乏基于物理世界的数据。在本文中，我们提出了 AutoRT，这是一个利用现有基础模型在完全看不见的场景中以最少的人工监督来扩大操作机器人部署的系统。 AutoRT 利用视觉语言模型 (VLM) 进行场景理解和基础，并进一步使用大型语言模型 (LLM) 来提出由一组机器人执行的多样化和新颖的指令。通过利用基础模型的知识来指导数据收集，使 AutoRT 能够有效地推理自主权衡和安全性，同时显着扩大机器人学习的数据收集范围。我们演示了 AutoRT 向多个建筑物中的 20 多个机器人提出指令，并通过远程操作和自主机器人策略收集 77,000 个真实的机器人事件。我们通过实验表明，AutoRT 收集的此类“野外”数据明显更加多样化，并且 AutoRT 对 LLM 的使用允许遵循可以符合人类偏好的数据收集机器人的指令。</details>
**PDF:** <http://arxiv.org/pdf/2401.12963v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **SGTR+: End-to-end Scene Graph Generation with Transformer**<br />
**Title_cn:** SGTR+：使用 Transformer 生成端到端场景图<br />
**Authors:** Rongjie Li, Songyang Zhang, Xuming He<br />
**Abstract:** <details><summary>原文: </summary>Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up, two-stage or point-based, one-stage approach, which often suffers from high time complexity or suboptimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To address the issues above, we create a transformer-based end-to-end framework to generate the entity and entity-aware predicate proposal set, and infer directed edges to form relation triplets. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Based on bipartite graph assembling paradigm, we further propose a new technical design to address the efficacy of entity-aware modeling and optimization stability of graph assembling. Equipped with the enhanced entity-aware design, our method achieves optimal performance and time-complexity. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on three challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. Code is available: https://github.com/Scarecrow0/SGTR</details>
**Abstract_cn:** <details><summary>译文: </summary>由于其组合特性，场景图生成（SGG）仍然是一项具有挑战性的视觉理解任务。以前的大多数工作都采用自下而上的两阶段或基于点的单阶段方法，这种方法通常会遇到高时间复杂度或次优设计的问题。在这项工作中，我们提出了一种新颖的 SGG 方法来解决上述问题，将任务表述为二分图构建问题。为了解决上述问题，我们创建了一个基于转换器的端到端框架来生成实体和实体感知谓词建议集，并推断有向边以形成关系三元组。此外，我们设计了一个图组装模块来基于实体感知结构推断二分场景图的连接性，使我们能够以端到端的方式生成场景图。基于二分图组装范式，我们进一步提出了一种新技术设计来解决实体感知建模的有效性和图组装的优化稳定性。配备增强的实体感知设计，我们的方法实现了最佳性能和时间复杂度。大量的实验结果表明，我们的设计能够在三个具有挑战性的基准上实现最先进的或可比的性能，超越大多数现有方法并享有更高的推理效率。代码可用：https://github.com/Scarecrow0/SGTR</details>
**PDF:** <http://arxiv.org/pdf/2401.12835v1><br />
**Code:** <https://github.com/scarecrow0/sgtr>**<br />
>>**index:** 2<br />
**Title:** **Shift-ConvNets: Small Convolutional Kernel with Large Kernel Effects**<br />
**Title_cn:** Shift-ConvNets：具有大核效应的小卷积核<br />
**Authors:** Dachong Li, Li Li, Zhuangzhuang Chen, Jianqiang Li<br />
**Abstract:** <details><summary>原文: </summary>Recent studies reveal that the remarkable performance of Vision transformers (ViTs) benefits from large receptive fields. For this reason, the large convolutional kernel design becomes an ideal solution to make Convolutional Neural Networks (CNNs) great again. However, the typical large convolutional kernels turn out to be hardware-unfriendly operators, resulting in discount compatibility of various hardware platforms. Thus, it is unwise to simply enlarge the convolutional kernel size. In this paper, we reveal that small convolutional kernels and convolution operations can achieve the closing effects of large kernel sizes. Then, we propose a shift-wise operator that ensures the CNNs capture long-range dependencies with the help of the sparse mechanism, while remaining hardware-friendly. Experimental results show that our shift-wise operator significantly improves the accuracy of a regular CNN while markedly reducing computational requirements. On the ImageNet-1k, our shift-wise enhanced CNN model outperforms the state-of-the-art models. Code & models at https://github.com/lidc54/shift-wiseConv.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究表明，视觉变压器（ViT）的卓越性能得益于大的感受野。因此，大卷积核设计成为让卷积神经网络（CNN）再次伟大的理想解决方案。然而，典型的大卷积核结果是硬件不友好的算子，导致各种硬件平台的兼容性打折扣。因此，简单地增大卷积核大小是不明智的。在本文中，我们揭示了小卷积核和卷积运算可以实现大核尺寸的闭合效果。然后，我们提出了一种移位运算符，确保 CNN 在稀疏机制的帮助下捕获长程依赖性，同时保持硬件友好。实验结果表明，我们的移位算子显着提高了常规 CNN 的准确性，同时显着降低了计算要求。在 ImageNet-1k 上，我们的移位增强 CNN 模型优于最先进的模型。代码和模型位于 https://github.com/lidc54/shift-wiseConv。</details>
**PDF:** <http://arxiv.org/pdf/2401.12736v1><br />
**Code:** <https://github.com/lidc54/shift-wiseconv>**<br />
>>**index:** 3<br />
**Title:** **Convolutional Initialization for Data-Efficient Vision Transformers**<br />
**Title_cn:** 数据高效视觉转换器的卷积初始化<br />
**Authors:** Jianqiao Zheng, Xueqian Li, Simon Lucey<br />
**Abstract:** <details><summary>原文: </summary>Training vision transformer networks on small datasets poses challenges. In contrast, convolutional neural networks (CNNs) can achieve state-of-the-art performance by leveraging their architectural inductive bias. In this paper, we investigate whether this inductive bias can be reinterpreted as an initialization bias within a vision transformer network. Our approach is motivated by the finding that random impulse filters can achieve almost comparable performance to learned filters in CNNs. We introduce a novel initialization strategy for transformer networks that can achieve comparable performance to CNNs on small datasets while preserving its architectural flexibility.</details>
**Abstract_cn:** <details><summary>译文: </summary>在小数据集上训练视觉变换器网络带来了挑战。相比之下，卷积神经网络 (CNN) 可以通过利用其架构归纳偏差来实现最先进的性能。在本文中，我们研究了这种归纳偏差是否可以重新解释为视觉变换器网络中的初始化偏差。我们的方法的动机是发现随机脉冲滤波器可以实现与 CNN 中的学习滤波器几乎相当的性能。我们为 Transformer 网络引入了一种新颖的初始化策略，该策略可以在小数据集上实现与 CNN 相当的性能，同时保留其架构灵活性。</details>
**PDF:** <http://arxiv.org/pdf/2401.12511v1><br />
**Code:** <https://github.com/osiriszjq/impulse_init>**<br />
>>**index:** 4<br />
**Title:** **Methods and strategies for improving the novel view synthesis quality of neural radiation field**<br />
**Title_cn:** 提高神经辐射场新视合成质量的方法与策略<br />
**Authors:** Shun Fang, Ming Cui, Xing Feng, Yanna Lv<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a scene from 2D images and synthesize realistic novel view images. This technology has received widespread attention from the industry and has good application prospects. In response to the problem that the rendering quality of NeRF images needs to be improved, many researchers have proposed various methods to improve the rendering quality in the past three years. The latest relevant papers are classified and reviewed, the technical principles behind quality improvement are analyzed, and the future evolution direction of quality improvement methods is discussed. This study can help researchers quickly understand the current state and evolutionary context of technology in this field, which is helpful in inspiring the development of more efficient algorithms and promoting the application of NeRF technology in related fields.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场（NeRF）技术可以从 2D 图像中学习场景的 3D 隐式模型，并合成逼真的新颖视图图像。该技术受到了业界的广泛关注，具有良好的应用前景。针对NeRF图像渲染质量需要提高的问题，近三年来许多研究人员提出了各种提高渲染质量的方法。对最新相关论文进行分类评述，分析质量改进背后的技术原理，探讨质量改进方法未来的演进方向。这项研究可以帮助研究人员快速了解该领域技术的现状和演化脉络，有助于启发更高效算法的开发，推动NeRF技术在相关领域的应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.12451v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction**<br />
**Title_cn:** InverseMatrixVT3D：一种基于投影矩阵的高效 3D 占用预测方法<br />
**Authors:** Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird's Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimise GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to further enhance performance. Comprehensive experiments on the nuScenes dataset demonstrate the simplicity and effectiveness of our method. The code will be made available at:https://github.com/DanielMing123/InverseMatrixVT3D</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 InverseMatrixVT3D，这是一种将多视图图像特征转换为 3D 特征量以进行 3D 语义占用预测的有效方法。构建 3D 体积的现有方法通常依赖于深度估计、特定于设备的运算符或变压器查询，这阻碍了 3D 占用模型的广泛采用。相比之下，我们的方法利用两个投影矩阵来存储静态映射关系和矩阵乘法，以有效生成全局鸟瞰图 (BEV) 特征和局部 3D 特征体。具体来说，我们通过在多视图图像特征图和两个稀疏投影矩阵之间执行矩阵乘法来实现这一点。我们为投影矩阵引入稀疏矩阵处理技术，以优化 GPU 内存使用。此外，还提出了全局-局部注意力融合模块，将全局 BEV 特征与局部 3D 特征体积相集成，以获得最终的 3D 体积。我们还采用多尺度监督机制，进一步提高绩效。 nuScenes 数据集上的综合实验证明了我们方法的简单性和有效性。代码将在以下位置提供：https://github.com/DanielMing123/InverseMatrixVT3D</details>
**PDF:** <http://arxiv.org/pdf/2401.12422v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting**<br />
**Title_cn:** PSAvatar：基于点的可变形形状模型，用于通过 3D 高斯泼溅创建实时头部头像<br />
**Authors:** Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu<br />
**Abstract:** <details><summary>原文: </summary>Despite much progress, creating real-time high-fidelity head avatar is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency.   Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a resolution of 512 x 512 )</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管取得了很大进展，但创建实时高保真头部头像仍然很困难，现有方法必须在速度和质量之间进行权衡。基于 3DMM 的方法通常无法对眼镜和发型等非面部结构进行建模，而神经隐式模型则存在变形不灵活和渲染效率低下的问题。尽管3D高斯已被证明在几何表示和辐射场重建方面具有良好的能力，但将3D高斯应用于头部头像创建仍然是一个重大挑战，因为3D高斯很难对因姿势和表情变化而引起的头部形状变化进行建模。在本文中，我们介绍了 PSAvatar，这是一种用于创建动画头部头像的新颖框架，它利用离散几何基元创建参数化可变形形状模型，并采用 3D 高斯进行精细细节表示和高保真度渲染。参数化可变形形状模型是基于点的可变形形状模型（PMSM），它使用点而不是网格进行 3D 表示，以实现增强的表示灵活性。 PMSM 首先通过在表面和网格外进行采样，将 FLAME 网格转换为点，不仅可以重建表面结构，还可以重建复杂的几何形状，例如眼镜和发型。通过以综合分析的方式将这些点与头部形状对齐，PMSM 使得利用 3D 高斯进行精细细节表示和外观建模成为可能，从而能够创建高保真化身。我们证明 PSAvatar 可以重建各种主体的高保真头部头像，并且头像可以实时动画（$\ge$ 25 fps，分辨率为 512 x 512 ）</details>
**PDF:** <http://arxiv.org/pdf/2401.12900v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction**<br />
**Title_cn:** EndoGaussian：用于可变形手术场景重建的高斯喷射<br />
**Authors:** Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{https://yifliu3.github.io/EndoGaussian/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>从内窥镜立体视频重建可变形组织在许多下游手术应用中至关重要。然而，现有方法的推理速度慢，这极大地限制了它们的实际使用。在本文中，我们介绍了 EndoGaussian，这是一种基于 3D 高斯 Splatting 构建的实时手术场景重建框架。我们的框架将动态手术场景表示为规范高斯和与时间相关的变形场，该变形场预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，我们的框架与以前的方法相比显着加快了渲染速度。此外，我们将变形场设计为轻量级编码体素和极小的 MLP 的组合，从而可以在较小的渲染负担下实现高效的高斯跟踪。此外，我们设计了一种整体高斯初始化方法，以充分利用表面分布先验，这是通过从输入图像序列中搜索信息点来实现的。在公共内窥镜数据集上的实验表明，我们的方法可以实现实时渲染速度（195 FPS 实时，100$\times$ 增益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（2 分钟内/场景），在术中手术应用中显示出巨大的前景。代码位于：\url{https://yifliu3.github.io/EndoGaussian/}。</details>
**PDF:** <http://arxiv.org/pdf/2401.12561v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**<br />
**Title_cn:** IRIS：低动态范围图像的室内场景逆渲染<br />
**Authors:** Zhi-Hao Lin, Jia-Bin Huang, Zhengqin Li, Zhao Dong, Christian Richardt, Tuotuo Li, Michael Zollhöfer, Johannes Kopf, Shenlong Wang, Changil Kim<br />
**Abstract:** <details><summary>原文: </summary>While numerous 3D reconstruction and novel-view synthesis methods allow for photorealistic rendering of a scene from multi-view images easily captured with consumer cameras, they bake illumination in their representations and fall short of supporting advanced applications like material editing, relighting, and virtual object insertion. The reconstruction of physically based material properties and lighting via inverse rendering promises to enable such applications.   However, most inverse rendering techniques require high dynamic range (HDR) images as input, a setting that is inaccessible to most users. We present a method that recovers the physically based material properties and spatially-varying HDR lighting of a scene from multi-view, low-dynamic-range (LDR) images. We model the LDR image formation process in our inverse rendering pipeline and propose a novel optimization strategy for material, lighting, and a camera response model. We evaluate our approach with synthetic and real scenes compared to the state-of-the-art inverse rendering methods that take either LDR or HDR input. Our method outperforms existing methods taking LDR images as input, and allows for highly realistic relighting and object insertion.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然许多 3D 重建和新颖视图合成方法允许从使用消费级相机轻松捕获的多视图图像中对场景进行真实感渲染，但它们在表示中烘焙照明，并且无法支持材质编辑、重新照明和虚拟对象等高级应用程序插入。通过逆向渲染重建基于物理的材料属性和照明有望实现此类应用。然而，大多数逆渲染技术需要高动态范围 (HDR) 图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，可以从多视图、低动态范围 (LDR) 图像中恢复场景的基于物理的材质属性和空间变化的 HDR 照明。我们在逆渲染管道中对 LDR 图像形成过程进行建模，并针对材质、照明和相机响应模型提出了一种新颖的优化策略。我们使用合成和真实场景与采用 LDR 或 HDR 输入的最先进的逆渲染方法来评估我们的方法。我们的方法优于以 LDR 图像作为输入的现有方法，并且允许高度逼真的重新照明和对象插入。</details>
**PDF:** <http://arxiv.org/pdf/2401.12977v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization**<br />
**Title_cn:** Coverage Axis++：3D 形状骨架化的高效内点选择<br />
**Authors:** Zimeng Wang, Zhiyang Dou, Rui Xu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Shiqing Xin, Lingjie Liu, Taku Komura, Xiaoming Yuan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization. The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality. To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations. We introduce a simple yet effective strategy that considers both shape coverage and uniformity to derive skeletal points. The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT. As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy. Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++. The code will be publicly available once the paper is published.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出了 Coverage Axis++，这是一种新颖且高效的 3D 形状骨架化方法。当前用于此任务的最先进的方法通常依赖于输入的防水性或遭受大量的计算成本，从而限制了它们的实用性。为了应对这一挑战，Coverage Axis++ 提出了一种启发式算法来选择骨架点，提供中轴变换 (MAT) 的高精度近似，同时显着减轻各种形状表示的计算强度。我们引入了一种简单而有效的策略，该策略考虑形状覆盖范围和均匀性来导出骨架点。选择过程强制与形状结构的一致性，同时有利于占主导地位的内侧球，从而在 MAT 方面引入了紧凑的基础形状表示。因此，Coverage Axis++ 允许对各种形状表示（例如，水密网格、三角汤、点云）进行骨架化，指定骨架点的数量，很少的超参数，以及提高重建精度的高效计算。针对各种 3D 形状的大量实验验证了 Coverage Axis++ 的效率和有效性。论文发表后，代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.12946v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction**<br />
**Title_cn:** PSDF：用于多视图重建的先验驱动神经隐式表面学习<br />
**Authors:** Wanjuan Su, Chen Zhang, Qingshan Xu, Wenbing Tao<br />
**Abstract:** <details><summary>原文: </summary>Surface reconstruction has traditionally relied on the Multi-View Stereo (MVS)-based pipeline, which often suffers from noisy and incomplete geometry. This is due to that although MVS has been proven to be an effective way to recover the geometry of the scenes, especially for locally detailed areas with rich textures, it struggles to deal with areas with low texture and large variations of illumination where the photometric consistency is unreliable. Recently, Neural Implicit Surface Reconstruction (NISR) combines surface rendering and volume rendering techniques and bypasses the MVS as an intermediate step, which has emerged as a promising alternative to overcome the limitations of traditional pipelines. While NISR has shown impressive results on simple scenes, it remains challenging to recover delicate geometry from uncontrolled real-world scenes which is caused by its underconstrained optimization. To this end, the framework PSDF is proposed which resorts to external geometric priors from a pretrained MVS network and internal geometric priors inherent in the NISR model to facilitate high-quality neural implicit surface learning. Specifically, the visibility-aware feature consistency loss and depth prior-assisted sampling based on external geometric priors are introduced. These proposals provide powerfully geometric consistency constraints and aid in locating surface intersection points, thereby significantly improving the accuracy and delicate reconstruction of NISR. Meanwhile, the internal prior-guided importance rendering is presented to enhance the fidelity of the reconstructed surface mesh by mitigating the biased rendering issue in NISR. Extensive experiments on the Tanks and Temples dataset show that PSDF achieves state-of-the-art performance on complex uncontrolled scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>表面重建传统上依赖于基于多视图立体 (MVS) 的管道，该管道经常受到噪声和不完整几何体的影响。这是因为，虽然 MVS 已被证明是恢复场景几何形状的有效方法，特别是对于纹理丰富的局部细节区域，但它很难处理纹理低且光照变化大的区域，这些区域的光度一致性是不可靠的。最近，神经隐式表面重建（NISR）结合了表面渲染和体积渲染技术，并绕过 MVS 作为中间步骤，这已成为克服传统管道限制的有前途的替代方案。虽然 NISR 在简单场景上显示了令人印象深刻的结果，但从不受控制的现实世界场景中恢复精致的几何图形仍然具有挑战性，这是由于其优化不足造成的。为此，提出了 PSDF 框架，该框架利用预训练的 MVS 网络的外部几何先验和 NISR 模型固有的内部几何先验来促进高质量的神经隐式表面学习。具体来说，引入了可见性感知特征一致性损失和基于外部几何先验的深度先验辅助采样。这些建议提供了强大的几何一致性约束并有助于定位表面交点，从而显着提高 NISR 的准确性和精细重建。同时，提出了内部先验引导的重要性渲染，通过减轻 NISR 中的偏差渲染问题来增强重建表面网格的保真度。对 Tanks 和 Temples 数据集进行的大量实验表明，PSDF 在复杂的不受控制的场景上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.12751v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**<br />
**Title_cn:** 野外 RGBD 对象：通过 RGB-D 视频缩放真实世界 3D 对象学习<br />
**Authors:** Hongchi Xia, Yang Fu, Sifei Liu, Xiaolong Wang<br />
**Abstract:** <details><summary>原文: </summary>We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一个在野外捕获的新 RGB-D 对象数据集，称为 WildRGB-D。与大多数现有的仅支持 RGB 捕获的以对象为中心的数据集不同，深度通道的直接捕获可以实现更好的 3D 注释和更广泛的下游应用。 WildRGB-D 包含大规模类别级 RGB-D 对象视频，这些视频是使用 iPhone 360​​ 度围绕对象拍摄的。它包含约 8500 个记录的对象和近 20000 个 RGB-D 视频，涉及 46 个常见对象类别。这些视频是在不同的杂乱背景下拍摄的，并采用三种设置来覆盖尽可能多的现实世界场景：(i) 一个视频中的单个对象； (ii) 一个视频中有多个对象； (iii) 一段视频中存在一只手处于静止状态的物体。该数据集使用对象蒙版、真实世界比例相机姿势以及从 RGBD 视频重建的聚合点云进行注释。我们使用 WildRGB-D 对四个任务进行基准测试，包括新颖的视图合成、相机姿态估计、物体 6d 姿态估计和物体表面重建。我们的实验表明，大规模捕获 RGB-D 对象为推进 3D 对象学习提供了巨大的潜力。我们的项目页面是 https://wildrgbd.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2401.12592v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning**<br />
**Title_cn:** 通过对比学习进行基于一致性增强的深度多视图聚类<br />
**Authors:** Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen, Xi Peng<br />
**Abstract:** <details><summary>原文: </summary>Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiments conducted on five datasets demonstrate the effectiveness and superiority of our method in comparison with the state-of-the-art (SOTA) methods. The code for this method can be accessed at https://anonymous.4open.science/r/CCEC-E84E/.</details>
**Abstract_cn:** <details><summary>译文: </summary>多视图聚类 (MVC) 通过综合多个视图的信息，将数据样本分成有意义的簇。此外，基于深度学习的方法在MVC场景中展示了其强大的特征学习能力。然而，在保持一致性的同时有效地概括特征表示仍然是一个棘手的问题。此外，现有的大多数基于对比学习的深度聚类方法都忽略了聚类过程中聚类表示的一致性。在本文中，我们展示了如何克服上述问题，并通过对比学习（CCEC）提出了一种基于一致增强的深度 MVC 方法。具体来说，语义连接块被合并到特征表示中以保留多个视图之间的一致信息。此外，通过谱聚类增强了聚类的表示过程，并且提高了多个视图之间的一致性。在五个数据集上进行的实验证明了我们的方法与最先进的（SOTA）方法相比的有效性和优越性。此方法的代码可以访问 https://anonymous.4open.science/r/CCEC-E84E/。</details>
**PDF:** <http://arxiv.org/pdf/2401.12648v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fast Semi-supervised Unmixing using Non-convex Optimization**<br />
**Title_cn:** 使用非凸优化的快速半监督分解<br />
**Authors:** Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with conventional sparse unmixing methods showcases considerable advantages of our proposed model, which entails nonconvex optimization. Notably, our implementations of the proposed algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which is open-source and available at https://github.com/BehnoodRasti/FUnmix</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了一种专为半监督/基于库的解混合而定制的新型线性模型。我们的模型考虑了文库不匹配，同时启用丰度和对一约束（ASC）。与传统的稀疏分解方法不同，该模型涉及非凸优化，带来了巨大的计算挑战。我们证明了交替乘子法 (ADMM) 在循环解决这些复杂问题方面的功效。我们提出了两种半监督分解方法，除了 ASC 之外，每种方法还依赖于应用于新模型的不同先验：稀疏先验和凸性约束。我们的实验结果验证了对于端元库来说，强制凸性约束优于稀疏先验。这些结果在三个模拟数据集（考虑光谱变异性和不同的像素纯度水平）和 Cuprite 数据集上得到了证实。此外，我们与传统的稀疏分解方法的比较展示了我们提出的模型的相当大的优势，这需要非凸优化。值得注意的是，我们所提出的算法的实现——快速半监督解混合（FaSUn）和使用软收缩的稀疏解混合（SUnS）——被证明比传统的稀疏解混合方法更有效。 SUnS 和 FaSUn 是使用 PyTorch 实现的，并在名为 Fast Semisupervised Unmixing (FUnmix) 的专用 Python 包中提供，该包是开源的，可在 https://github.com/BehnoodRasti/FUnmix 上获取</details>
**PDF:** <http://arxiv.org/pdf/2401.12609v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space**<br />
**Title_cn:** AdaEmbed：嵌入空间中的半监督域适应<br />
**Authors:** Ali Mottaghi, Mohammad Abdullah Jamal, Serena Yeung, Omid Mohareri<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised domain adaptation (SSDA) presents a critical hurdle in computer vision, especially given the frequent scarcity of labeled data in real-world settings. This scarcity often causes foundation models, trained on extensive datasets, to underperform when applied to new domains. AdaEmbed, our newly proposed methodology for SSDA, offers a promising solution to these challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates the transfer of knowledge from a labeled source domain to an unlabeled target domain by learning a shared embedding space. By generating accurate and uniform pseudo-labels based on the established embedding space, the model overcomes the limitations of conventional SSDA, thus enhancing performance significantly. Our method's effectiveness is validated through extensive experiments on benchmark datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed consistently outperforms all the baselines, setting a new state of the art for SSDA. With its straightforward implementation and high data efficiency, AdaEmbed stands out as a robust and pragmatic solution for real-world scenarios, where labeled data is scarce. To foster further research and application in this area, we are sharing the codebase of our unified framework for semi-supervised domain adaptation.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督域适应（SSDA）是计算机视觉中的一个关键障碍，特别是考虑到现实世界中标记数据经常稀缺的情况。这种稀缺性通常会导致在广泛数据集上训练的基础模型在应用于新领域时表现不佳。 AdaEmbed 是我们新提出的 SSDA 方法，为应对这些挑战提供了一个有前景的解决方案。利用未标记数据的潜力，AdaEmbed 通过学习共享嵌入空间，促进知识从标记源域转移到未标记目标域。该模型通过基于建立的嵌入空间生成准确且均匀的伪标签，克服了传统SSDA的局限性，从而显着提高了性能。我们的方法的有效性通过对 DomainNet、Office-Home 和 VisDA-C 等基准数据集的大量实验得到验证，其中 AdaEmbed 始终优于所有基准，为 SSDA 设定了新的技术水平。凭借其简单的实现和高数据效率，AdaEmbed 成为针对标记数据稀缺的现实场景的强大且实用的解决方案。为了促进该领域的进一步研究和应用，我们正在共享半监督域适应统一框架的代码库。</details>
**PDF:** <http://arxiv.org/pdf/2401.12421v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies**<br />
**Title_cn:** 自动驾驶中以数据为中心的演进：大数据系统、数据挖掘和闭环技术的全面综述<br />
**Authors:** Lincan Li, Wei Shao, Wei Dong, Yijun Tian, Kaixiang Yang, Wenjie Zhang<br />
**Abstract:** <details><summary>原文: </summary>The aspiration of the next generation's autonomous driving (AD) technology relies on the dedicated integration and interaction among intelligent perception, prediction, planning, and low-level control. There has been a huge bottleneck regarding the upper bound of autonomous driving algorithm performance, a consensus from academia and industry believes that the key to surmount the bottleneck lies in data-centric autonomous driving technology. Recent advancement in AD simulation, closed-loop model training, and AD big data engine have gained some valuable experience. However, there is a lack of systematic knowledge and deep understanding regarding how to build efficient data-centric AD technology for AD algorithm self-evolution and better AD big data accumulation. To fill in the identified research gaps, this article will closely focus on reviewing the state-of-the-art data-driven autonomous driving technologies, with an emphasis on the comprehensive taxonomy of autonomous driving datasets characterized by milestone generations, key features, data acquisition settings, etc. Furthermore, we provide a systematic review of the existing benchmark closed-loop AD big data pipelines from the industrial frontier, including the procedure of closed-loop frameworks, key technologies, and empirical studies. Finally, the future directions, potential applications, limitations and concerns are discussed to arouse efforts from both academia and industry for promoting the further development of autonomous driving.</details>
**Abstract_cn:** <details><summary>译文: </summary>下一代自动驾驶（AD）技术的愿景依赖于智能感知、预测、规划和底层控制之间的专门集成和交互。自动驾驶算法性能上限存在巨大瓶颈，学术界和工业界一致认为，突破瓶颈的关键在于以数据为中心的自动驾驶技术。近年来在AD仿真、闭环模型训练、AD大数据引擎等方面取得了一些进展，积累了一些宝贵的经验。然而，如何构建高效的以数据为中心的AD技术，实现AD算法的自我进化和更好的AD大数据积累，还缺乏系统的认识和深刻的理解。为了填补已确定的研究空白，本文将密切关注最先进的数据驱动自动驾驶技术，重点是自动驾驶数据集的综合分类，以里程碑代、关键特征、数据为特征。此外，我们还从产业前沿对现有基准闭环AD大数据管道进行了系统回顾，包括闭环框架流程、关键技术和实证研究。最后，讨论了自动驾驶的未来方向、潜在应用、局限性和关注点，以引起学术界和工业界的共同努力，推动自动驾驶的进一步发展。</details>
**PDF:** <http://arxiv.org/pdf/2401.12888v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fast Implicit Neural Representation Image Codec in Resource-limited Devices**<br />
**Title_cn:** 资源有限设备中的快速隐式神经表示图像编解码器<br />
**Authors:** Xiang Liu, Jiahong Chen, Bin Chen, Zimo Liu, Baoyi An, Shu-Tao Xia<br />
**Abstract:** <details><summary>原文: </summary>Displaying high-quality images on edge devices, such as augmented reality devices, is essential for enhancing the user experience. However, these devices often face power consumption and computing resource limitations, making it challenging to apply many deep learning-based image compression algorithms in this field. Implicit Neural Representation (INR) for image compression is an emerging technology that offers two key benefits compared to cutting-edge autoencoder models: low computational complexity and parameter-free decoding. It also outperforms many traditional and early neural compression methods in terms of quality. In this study, we introduce a new Mixed Autoregressive Model (MARM) to significantly reduce the decoding time for the current INR codec, along with a new synthesis network to enhance reconstruction quality. MARM includes our proposed Autoregressive Upsampler (ARU) blocks, which are highly computationally efficient, and ARM from previous work to balance decoding time and reconstruction quality. We also propose enhancing ARU's performance using a checkerboard two-stage decoding strategy. Moreover, the ratio of different modules can be adjusted to maintain a balance between quality and speed. Comprehensive experiments demonstrate that our method significantly improves computational efficiency while preserving image quality. With different parameter settings, our method can outperform popular AE-based codecs in constrained environments in terms of both quality and decoding time, or achieve state-of-the-art reconstruction quality compared to other INR codecs.</details>
**Abstract_cn:** <details><summary>译文: </summary>在增强现实设备等边缘设备上显示高质量图像对于增强用户体验至关重要。然而，这些设备通常面临功耗和计算资源的限制，使得在该领域应用许多基于深度学习的图像压缩算法具有挑战性。用于图像压缩的隐式神经表示 (INR) 是一项新兴技术，与尖端自动编码器模型相比，它具有两个关键优势：计算复杂度低和无参数解码。它在质量方面也优于许多传统和早期的神经压缩方法。在这项研究中，我们引入了一种新的混合自回归模型（MARM），以显着减少当前 INR 编解码器的解码时间，以及一个新的合成网络来提高重建质量。 MARM 包括我们提出的自回归上采样器 (ARU) 模块，该模块具有很高的计算效率，并且包含之前工作中的 ARM，用于平衡解码时间和重建质量。我们还建议使用棋盘两阶段解码策略来增强 ARU 的性能。而且，可以调整不同模块的比例，以保持质量和速度之间的平衡。综合实验表明，我们的方法在保持图像质量的同时显着提高了计算效率。通过不同的参数设置，我们的方法在质量和解码时间方面都可以在受限环境中优于流行的基于 AE 的编解码器，或者与其他 INR 编解码器相比实现最先进的重建质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.12587v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Secure Federated Learning Approaches to Diagnosing COVID-19**<br />
**Title_cn:** 用于诊断 COVID-19 的安全联合学习方法<br />
**Authors:** Rittika Adhikari, Christopher Settles<br />
**Abstract:** <details><summary>原文: </summary>The recent pandemic has underscored the importance of accurately diagnosing COVID-19 in hospital settings. A major challenge in this regard is differentiating COVID-19 from other respiratory illnesses based on chest X-rays, compounded by the restrictions of HIPAA compliance which limit the comparison of patient X-rays. This paper introduces a HIPAA-compliant model to aid in the diagnosis of COVID-19, utilizing federated learning. Federated learning is a distributed machine learning approach that allows for algorithm training across multiple decentralized devices using local data samples, without the need for data sharing. Our model advances previous efforts in chest X-ray diagnostic models. We examined leading models from established competitions in this domain and developed our own models tailored to be effective with specific hospital data. Considering the model's operation in a federated learning context, we explored the potential impact of biased data updates on the model's performance. To enhance hospital understanding of the model's decision-making process and to verify that the model is not focusing on irrelevant features, we employed a visualization technique that highlights key features in chest X-rays indicative of a positive COVID-19 diagnosis.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的大流行凸显了在医院环境中准确诊断 COVID-19 的重要性。这方面的一个主要挑战是根据胸部 X 光检查将 COVID-19 与其他呼吸道疾病区分开来，再加上 HIPAA 合规性的限制限制了患者 X 光检查的比较。本文介绍了一种符合 HIPAA 要求的模型，利用联邦学习来帮助诊断 COVID-19。联邦学习是一种分布式机器学习方法，允许使用本地数据样本跨多个分散设备进行算法训练，而无需数据共享。我们的模型推进了之前在胸部 X 射线诊断模型方面的努力。我们研究了该领域已建立的竞争中的领先模型，并开发了我们自己的模型，以便对特定的医院数据有效。考虑到模型在联邦学习环境中的运行，我们探讨了有偏差的数据更新对模型性能的潜在影响。为了增强医院对模型决策过程的理解，并验证模型没有关注不相关的特征，我们采用了一种可视化技术，突出显示胸部 X 光片中表明 COVID-19 诊断呈阳性的关键特征。</details>
**PDF:** <http://arxiv.org/pdf/2401.12438v1><br />
**Code:** null<br />

