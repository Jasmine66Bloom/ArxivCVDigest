## [UPDATED!] **2024-01-24** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Algebraic methods for solving recognition problems with non-crossing classes**<br />
**Title_cn:** 解决非交叉类识别问题的代数方法<br />
**Authors:** Anvar Kabulov, Alimdzhan Babadzhanov, Islambek Saymanov<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose to consider various models of pattern recognition. At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule. Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created. An upper estimate is constructed for the model, which guarantees the completeness of the extension.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们建议考虑各种模式识别模型。同时，建议考虑两个算子形式的模型：识别算子和决策规则。引入代数运算来识别算子，并基于这些算子的应用，创建了一系列识别算法。为模型构建了上限估计，保证了扩展的完整性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13666v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Tyche: Stochastic In-Context Learning for Medical Image Segmentation**<br />
**Title_cn:** Tyche：医学图像分割的随机上下文学习<br />
**Authors:** Marianne Rakic, Hallee E. Wong, Jose Javier Gonzalez Ortiz, Beth Cimini, John Guttag, Adrian V. Dalca<br />
**Abstract:** <details><summary>原文: </summary>Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的基于学习的医学图像分割解决方案有两个重要的缺点。首先，对于大多数新的分割任务，必须训练或微调新模型。这需要大量的资源和机器学习专业知识，因此对于医学研究人员和临床医生来说通常是不可行的。其次，大多数现有的分割方法为给定图像生成单个确定性分割掩模。然而在实践中，关于什么构成正确的分割通常存在相当大的不确定性，并且不同的专家注释者通常会对同一图像进行不同的分割。我们用 Tyche 解决了这两个问题，该模型使用上下文集为以前未见过的任务生成随机预测，而无需重新训练。 Tyche 在两个重要方面不同于其他上下文分割方法。 (1)我们引入了一种新颖的卷积块架构，可以实现预测之间的交互。 (2)我们引入了上下文测试时间增强，这是一种提供预测随机性的新机制。当与适当的模型设计和损失函数相结合时，Tyche 可以为新的或未见过的医学图像和分割任务预测一组看似合理的多样化分割候选者，而无需重新训练。</details>
**PDF:** <http://arxiv.org/pdf/2401.13650v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability**<br />
**Title_cn:** ChatGPT 在人脸生物识别方面有多出色？初步探讨识别、软生物识别技术和可解释性<br />
**Authors:** Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia<br />
**Abstract:** <details><summary>原文: </summary>Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).   The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>OpenAI 开发的 GPT 等大型语言模型 (LLM) 已经显示出惊人的结果，给我们的社会带来了快速的变化。 ChatGPT 的发布进一步强化了这一点，它允许任何人以简单的对话方式与法学硕士进行交互，而无需任何该领域的经验。因此，ChatGPT 已迅速应用于许多不同的任务，例如代码和歌曲编写、教育、虚拟助理等，在未经训练的任务（零样本学习）中显示出令人印象深刻的结果。本研究旨在基于最近的 GPT-4 多模态 LLM 来探索 ChatGPT 在人脸生物识别任务中的能力。我们特别分析了 ChatGPT 执行面部验证、软生物识别估计和结果可解释性等任务的能力。 ChatGPT 对于进一步提高人类场景中自动决策的可解释性和透明度非常有价值。进行实验是为了评估 ChatGPT 的性能和鲁棒性，使用流行的公共基准并将结果与​​该领域最先进的方法进行比较。本研究取得的结果显示了 ChatGPT 等法学硕士在人脸生物识别方面的潜力，特别是在增强可解释性方面。出于可重复性的原因，我们在 GitHub 中发布了所有代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.13641v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode**<br />
**Title_cn:** 增强图像检索:使用CLIP模式进行照片搜索的综合研究<br />
**Authors:** Naresh Kumar Lahajal, Harini S<br />
**Abstract:** <details><summary>原文: </summary>Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications</details>
**Abstract_cn:** <details><summary>译文: </summary>照片搜索是基于文本查询检索图像的任务，随着 CLIP（对比语言图像预训练）模型的引入，取得了显着的进步。 CLIP 利用视觉语言预训练方法，学习图像和文本的共享表示空间，从而实现跨模式理解。该模型展示了理解不同图像和文本对之间语义关系的能力，允许基于自然语言查询高效、准确地检索图像。通过对包含图像及其相关文本描述的大规模数据集进行训练，CLIP 实现了显着的泛化，为零样本学习和少样本分类等任务提供了强大的工具。本摘要总结了 CLIP 的基本原理，并强调了其对推进照片搜索领域、促进自然语言理解和计算机视觉的无缝集成以改进多媒体应用中的信息检索的潜在影响</details>
**PDF:** <http://arxiv.org/pdf/2401.13613v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **PLATE: A perception-latency aware estimator,**<br />
**Title_cn:** PLATE：感知延迟感知估计器，<br />
**Authors:** Rodrigo Aldana-López, Rosario Aragüés, Carlos Sagüés<br />
**Abstract:** <details><summary>原文: </summary>Target tracking is a popular problem with many potential applications. There has been a lot of effort on improving the quality of the detection of targets using cameras through different techniques. In general, with higher computational effort applied, i.e., a longer perception-latency, a better detection accuracy is obtained. However, it is not always useful to apply the longest perception-latency allowed, particularly when the environment doesn't require to and when the computational resources are shared between other tasks. In this work, we propose a new Perception-LATency aware Estimator (PLATE), which uses different perception configurations in different moments of time in order to optimize a certain performance measure. This measure takes into account a perception-latency and accuracy trade-off aiming for a good compromise between quality and resource usage. Compared to other heuristic frame-skipping techniques, PLATE comes with a formal complexity and optimality analysis. The advantages of PLATE are verified by several experiments including an evaluation over a standard benchmark with real data and using state of the art deep learning object detection methods for the perception stage.</details>
**Abstract_cn:** <details><summary>译文: </summary>目标跟踪是许多潜在应用中的一个普遍问题。人们在通过不同技术提高相机目标检测质量方面做出了很多努力。一般来说，应用更高的计算量，即更长的感知延迟，可以获得更好的检测精度。然而，应用允许的最长感知延迟并不总是有用的，特别是当环境不需要并且计算资源在其他任务之间共享时。在这项工作中，我们提出了一种新的感知延迟感知估计器（PLATE），它在不同时刻使用不同的感知配置来优化特定的性能指标。该措施考虑了感知延迟和准确性的权衡，旨在在质量和资源使用之间取得良好的折衷。与其他启发式跳帧技术相比，PLATE 具有形式复杂性和最优性分析。 PLATE 的优势已通过多项实验得到验证，包括使用真实数据对标准基准进行评估，以及在感知阶段使用最先进的深度学习对象检测方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13596v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation**<br />
**Title_cn:** SegMamba：用于 3D 医学图像分割的远程顺序建模 Mamba<br />
**Authors:** Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu<br />
**Abstract:** <details><summary>原文: </summary>The Transformer architecture has shown a remarkable ability in modeling global relationships. However, it poses a significant computational challenge when processing high-dimensional medical images. This hinders its development and widespread adoption in this task. Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed. Inspired by its success, we introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation \textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale. Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba. The code for SegMamba is available at: https://github.com/ge-xing/SegMamba</details>
**Abstract_cn:** <details><summary>译文: </summary>Transformer 架构在建模全局关系方面表现出了非凡的能力。然而，在处理高维医学图像时，它带来了巨大的计算挑战。这阻碍了它在这项任务中的发展和广泛采用。 Mamba 作为一种状态空间模型 (SSM)，最近成为顺序建模中远程依赖关系的一种著名方式，以其卓越的内存效率和计算速度在自然语言处理领域表现出色。受其成功的启发，我们推出了 SegMamba，一种新颖的 3D 医学图像 \textbf{Seg}mentation \textbf{Mamba} 模型，旨在有效捕获每个尺度的整个体积特征中的远程依赖性。与基于 Transformer 的方法相比，我们的 SegMamba 从状态空间模型的角度来看，在整个体积特征建模方面表现出色，即使体积特征的分辨率为 {$64\times 64\times 64$}，也能保持卓越的处理速度。 BraTS2023 数据集上的综合实验证明了我们的 SegMamba 的有效性和效率。 SegMamba 的代码位于：https://github.com/ge-xing/SegMamba</details>
**PDF:** <http://arxiv.org/pdf/2401.13560v1><br />
**Code:** <https://github.com/ge-xing/segmamba>**<br />
>>**index:** 7<br />
**Title:** **PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition**<br />
**Title_cn:** PanAf20K：用于野生猿检测和行为识别的大型视频数据集<br />
**Authors:** Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel Angedakin, Katherine Corogenes, Dervla Dowd, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 PanAf20K 数据集，这是自然环境中类人猿最大、最多样化的开放获取带注释视频数据集。它包含超过 700 万帧的约 20,000 个黑猩猩和大猩猩的相机陷阱视频，这些视频是在热带非洲的 18 个野外地点收集的，是泛非计划：养殖黑猩猩的一部分。该镜头附有丰富的注释和基准，使其适合训练和测试各种具有挑战性且对生态重要的计算机视觉任务，包括猿检测和行为识别。鉴于国际自然保护联盟现已将类人猿家族的所有物种列为濒危或极度濒危，进一步对相机陷阱信息进行人工智能分析至关重要。我们希望该数据集能为人工智能社区的参与奠定坚实的基础，以提高性能、效率和结果解释，以支持对类人猿的存在、丰度、分布和行为的评估，从而帮助保护工作。</details>
**PDF:** <http://arxiv.org/pdf/2401.13554v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection**<br />
**Title_cn:** 将一类模型和弱监督模型与自适应阈值交错进行无监督视频异常检测<br />
**Authors:** Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai<br />
**Abstract:** <details><summary>原文: </summary>Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>如果没有人工注释，典型的无监督视频异常检测（UVAD）方法需要训练两个模型，为彼此生成伪标签。在之前的工作中，这两个模型彼此紧密纠缠在一起，并且不知道如何在不显着修改其训练框架的情况下升级其方法。其次，以前的工作通常采用固定阈值来获得伪标签，但用户指定的阈值并不可靠，这不可避免地会在训练过程中引入误差。为了缓解这两个问题，我们提出了一种新颖的交错框架，该框架交替训练 UVAD 的单类分类（OCC）模型和弱监督（WS）模型。我们方法中的OCC或WS模型可以很容易地替换为其他OCC或WS模型，这有助于我们的方法随着这两个领域的最新发展而升级。为了处理固定阈值问题，我们突破了传统的认知边界，提出了一种可以在正常和异常数据上进行训练的加权 OCC 模型。我们还提出了一种自适应机制，用于以宽松到严格的方式自动找到 WS 模型的最佳阈值。实验表明，所提出的 UVAD 方法优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13551v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **QAGait: Revisit Gait Recognition from a Quality Perspective**<br />
**Title_cn:** QGait：从质量角度重新审视步态识别<br />
**Authors:** Zengbin Wang, Saihui Hou, Man Zhang, Xu Liu, Chunshui Cao, Yongzhen Huang, Peipei Li, Shibiao Xu<br />
**Abstract:** <details><summary>原文: </summary>Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait.</details>
**Abstract_cn:** <details><summary>译文: </summary>步态识别是一种很有前途的生物识别方法，旨在根据行人独特的行走模式来识别行人。 Silhouette 模态以其易于获取、结构简单、稀疏表示和方便建模而闻名，已广泛应用于受控实验室研究中。然而，随着步态识别从实验室场景迅速发展到野外场景，各种条件对轮廓模态提出了重大挑战，包括1）无法识别的低质量轮廓（异常分割、严重遮挡，甚至非人类轮廓）形状），2）可识别但具有挑战性的轮廓（背景噪音、非标准姿势、轻微遮挡）。为了应对这些挑战，我们重新审视步态识别流程，并从质量角度处理步态识别，即 QAGait。具体来说，我们提出了一系列具有成本效益的质量评估策略，包括最大连接面积和模板匹配以消除背景噪音和无法识别的轮廓，对齐策略以处理非标准姿势。我们还提出了两个质量感知损失函数，将轮廓质量集成到嵌入空间内的优化中。大量实验证明我们的 QGait 可以保证步态可靠性和性能增强。此外，我们的质量评估策略可以与现有的步态数据集无缝集成，展示了我们的优势。代码可在 https://github.com/wzb-bupt/QAGait 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13531v1><br />
**Code:** <https://github.com/wzb-bupt/qagait>**<br />
>>**index:** 10<br />
**Title:** **Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces**<br />
**Title_cn:** Delocate：具有随机位置的篡改痕迹的 Deepfake 视频的检测和定位<br />
**Authors:** Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou<br />
**Abstract:** <details><summary>原文: </summary>Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames. Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages named recoveringand localization. In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces. Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>Deepfake 视频变得越来越真实，在不同帧之间的面部区域显示出微妙的篡改痕迹。因此，许多现有的 Deepfake 检测方法很难检测未知领域的 Deepfake 视频，同时准确定位被篡改的区域。为了解决这个限制，我们提出了 Delocate，一种新颖的 Deepfake 检测模型，可以识别和定位未知领域的 Deepfake 视频。我们的方法包括两个阶段：恢复和定位。在恢复阶段，模型随机屏蔽感兴趣区域（ROI）并在不篡改痕迹的情况下重建真实人脸，导致真实人脸恢复效果相对较好，而对于假人脸恢复效果较差。在定位阶段，恢复阶段的输出和伪造地面真值掩模作为监督来指导伪造定位过程​​。这个过程策略性地强调了恢复较差的假脸的恢复阶段，有利于被篡改区域的本地化。我们对四个广泛使用的基准数据集进行的广泛实验表明，Delocate 不仅在定位篡改区域方面表现出色，而且还增强了跨域检测性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.13516v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images**<br />
**Title_cn:** 整个幻灯片图像中的组织横截面和笔标记分割<br />
**Authors:** Ruben T. Lucassen, Willeke A. M. Blokx, Mitko Veta<br />
**Abstract:** <details><summary>原文: </summary>Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram. On an independent test set, the model achieved a mean Dice score of 0.981$\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\pm$0.090 for pen marking segmentation. The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\pm$0.350. Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts. The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter.</details>
**Abstract_cn:** <details><summary>译文: </summary>组织分割是一种常规预处理步骤，可通过排除背景区域来降低整个幻灯片图像 (WSI) 分析的计算成本。传统的图像处理技术通常用于组织分割，但通常需要手动调整非典型情况的参数值，无法从背景中排除所有幻灯片和扫描伪影，并且无法分割脂肪组织。如果不消除笔标记伪影，尤其可能成为后续分析的潜在偏差来源。此外，一些应用需要分离各个横截面，由于组织碎片和相邻定位，这可能具有挑战性。为了解决这些问题，我们使用 200 个 H&E 染色 WSI 的数据集开发了一个用于组织和笔标记分割的卷积神经网络。为了分离组织横截面，我们提出了一种基于对二维直方图中横截面的预测质心位置进行聚类的新颖后处理方法。在独立测试集上，该模型在组织分割方面的平均 Dice 得分为 0.981$\pm$0.033，在笔标记分割方面的平均 Dice 得分为 0.912$\pm$0.090。带注释和分离的横截面数量之间的平均绝对差为 0.075$\pm$0.350。我们的结果表明，所提出的模型可以准确地分割 WSI 中的 H&E 染色组织横截面和笔标记，同时对许多常见的载玻片和扫描伪影具有鲁棒性。具有经过训练的模型参数和后处理方法的模型作为名为 SlideSegmenter 的 Python 包公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.13511v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Research about the Ability of LLM in the Tamper-Detection Area**<br />
**Title_cn:** 法学硕士在篡改检测领域的能力研究<br />
**Authors:** Xinyu Yang, Jizhe Zhou<br />
**Abstract:** <details><summary>原文: </summary>In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains. In the field of tamper detection, LLMs are capable of identifying basic tampering activities.To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection. AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated. Manipulation detection, on the other hand, focuses on identifying tampered images. According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye. All of the LLMs can't identify carefully forged images and very realistic images generated by AI. In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，特别是自 2020 年代初以来，大型语言模型 (LLM) 已成为解决各种挑战（从自然语言处理到解决各个领域的复杂问题）的最强大的人工智能工具。在篡改检测领域，LLM能够识别基本的篡改活动。为了评估LLM在更专业领域的能力，我们收集了不同公司开发的五种不同的LLM：GPT-4、LLaMA、Bard、ERNIE Bot 4.0、和同义钱文。这种多样化的模型可以全面评估其在检测复杂篡改实例方面的性能。我们设计了两个检测领域：人工智能生成内容 (AIGC) 检测和操纵检测。 AIGC检测旨在测试区分图像是真实图像还是人工智能生成的能力。另一方面，操纵检测侧重于识别被篡改的图像。根据我们的实验，大多数LLM可以识别与逻辑不一致的合成图片，只有更强大的LLM才能区分逻辑上的、但人眼可见的篡改迹象。所有法学硕士都无法识别精心伪造的图像和人工智能生成的非常逼真的图像。在篡改检测领域，法学硕士还有很长的路要走，特别是在可靠地识别高度复杂的赝品和人工智能生成的非常模仿现实的图像方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.13504v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning**<br />
**Title_cn:** LDCA：具有上下文增强的局部描述符，用于少样本学习<br />
**Authors:** Maofa Wang, Bingchen Yan<br />
**Abstract:** <details><summary>原文: </summary>Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data. Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors. In this work, we introduce a novel approach termed "Local Descriptor with Contextual Augmentation (LDCA)". Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module. This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances. By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative. Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头图像分类已成为计算机视觉领域的一个关键挑战，凸显了以最少的标记数据快速适应新任务的能力。现有方法主要依赖于图像级特征或局部描述符，通常忽略了这些描述符周围的整体上下文。在这项工作中，我们引入了一种称为“具有上下文增强的局部描述符（LDCA）”的新颖方法。具体来说，该方法通过利用自适应全局上下文增强模块，独特地弥合了局部和全局理解之间的差距。该模块包含一个视觉转换器，赋予局部描述符上下文感知能力，范围从广泛的全球视角到复杂的周围细微差别。通过这样做，LDCA 超越了传统的基于描述符的方法，确保每个局部特征在其更大的视觉叙事中得到解释。大量的实验强调了我们方法的有效性，在细粒度分类数据集上显示出比次优方法的最大绝对改进为 20%，从而证明了在少数样本分类任务中的显着进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.13499v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Segmenting Cardiac Muscle Z-disks with Deep Neural Networks**<br />
**Title_cn:** 使用深度神经网络分割心肌 Z 盘<br />
**Authors:** Mihaela Croitor Ibrahim, Nishant Ravikumar, Alistair Curd, Joanna Leng, Oliver Umney, Michelle Peckham<br />
**Abstract:** <details><summary>原文: </summary>Z-disks are complex structures that delineate repeating sarcomeres in striated muscle. They play significant roles in cardiomyocytes such as providing mechanical stability for the contracting sarcomere, cell signalling and autophagy. Changes in Z-disk architecture have been associated with impaired cardiac function. Hence, there is a strong need to create tools to segment Z-disks from microscopy images, that overcome traditional limitations such as variability in image brightness and staining technique. In this study, we apply deep learning based segmentation models to extract Z-disks in images of striated muscle tissue. We leverage a novel Airyscan confocal dataset, which comprises high resolution images of Z-disks of healthy heart tissue, stained with Affimers for specific Z-disk proteins. We employed an interactive labelling tool, Ilastik to obtain ground truth segmentation masks and use the resulting data set to train and evaluate the performance of several state-of-the-art segmentation networks. On the test set, UNet++ achieves best segmentation performance for Z-disks in cardiomyocytes, with an average Dice score of 0.91 and outperforms other established segmentation methods including UNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improved generalisation, when tested on an additional dataset of cardiomyocytes with a titin mutation. This is the first study to demonstrate that automated machine learning-based segmentation approaches may be used effectively to segment Z-disks in confocal microscopy images. Automated segmentation approaches and predicted segmentation masks could be used to derive morphological features of Z-disks (e.g. width and orientation), and subsequently, to quantify disease-related changes to cardiac microstructure.</details>
**Abstract_cn:** <details><summary>译文: </summary>Z 盘是描绘横纹肌中重复肌节的复杂结构。它们在心肌细胞中发挥着重要作用，例如为收缩肌节提供机械稳定性、细胞信号传导和自噬。 Z 盘结构的变化与心脏功能受损有关。因此，迫切需要创建从显微图像中分割 Z 盘的工具，以克服传统的限制，例如图像亮度和染色技术的变化。在本研究中，我们应用基于深度学习的分割模型来提取横纹肌组织图像中的 Z 盘。我们利用新颖的 Airyscan 共聚焦数据集，其中包含健康心脏组织 Z 盘的高分辨率图像，并用 Affimer 染色特定的 Z 盘蛋白质。我们采用交互式标记工具 Ilastik 来获取地面真实分割掩模，并使用生成的数据集来训练和评估几个最先进的分割网络的性能。在测试集上，UNet++ 对心肌细胞中的 Z 盘实现了最佳分割性能，平均 Dice 得分为 0.91，优于其他已建立的分割方法，包括 UNet、FPN、DeepLabv3+ 和 pix2pix。然而，当在具有肌联蛋白突变的心肌细胞的附加数据集上进行测试时，pix2pix 表现出更好的泛化能力。这是第一项证明基于自动化机器学习的分割方法可以有效地用于分割共焦显微镜图像中的 Z 盘的研究。自动分割方法和预测分割掩模可用于导出 Z 盘的形态特征（例如宽度和方向），并随后量化与疾病相关的心脏微观结构变化。</details>
**PDF:** <http://arxiv.org/pdf/2401.13472v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition**<br />
**Title_cn:** GTAutoAct：基于游戏引擎重新开发的动作识别自动数据集生成框架<br />
**Authors:** Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi<br />
**Abstract:** <details><summary>原文: </summary>Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的动作识别任务数据集面临着传统收集和生成方法的限制，包括动作类范围有限、缺乏多视点记录、多样性有限、视频质量差以及劳动密集型手动收集。为了应对这些挑战，我们引入了 GTAutoAct，这是一种创新的数据集生成框架，利用游戏引擎技术来促进动作识别的进步。 GTAutoAct 擅长自动创建大规模、注释良好的数据集，具有广泛的动作类和卓越的视频质量。我们的框架的独特贡献包括：（1）它创新地将现有的基于坐标的 3D 人体运动转换为面向旋转的表示，并增强了多视角的适用性； (2)它采用动态分割和旋转序列插值来创建流畅且逼真的动作动画； (3)它提供了广泛可定制的动画场景； (4) 它实现了自主视频捕获和处理管道，具有随机导航相机，具有自动修剪和标记功能。实验结果强调了该框架的稳健性，并强调了其显着改善动作识别模型训练的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.13414v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter**<br />
**Title_cn:** 合成数据可以实现更快的注释和强大的分割，以实现杂乱中的多对象抓取<br />
**Authors:** Dongmyoung Lee, Wei Chen, Nicolas Rojas<br />
**Abstract:** <details><summary>原文: </summary>Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation. In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset). Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically. RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively. Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人抓取中的对象识别和对象姿态估计仍然是重大挑战，因为构建标记数据集在数据收集和注释方面可能非常耗时且成本高昂。在这项工作中，我们提出了一种合成数据生成方法，通过将生成的合成数据集与较小的真实数据集（混合数据集）相结合，最大限度地减少人为干预并使下游图像分割算法更加稳健。注释实验表明，所提出的合成场景生成可以显着减少标记时间。 RGB 图像分割使用混合数据集进行训练，并与深度信息相结合，以生成各个分割对象的像素到点对应关系。然后由分割算法的置信度分数确定要抓取的对象。拾放实验表明，在我们的混合数据集上训练的分割 (98.9%, 70%) 在标签方面优于真实数据集和公开数据集 (6.7%, 18.8%) 和 (2.8%, 10%)和抓取成功率。补充材料可在 https://sites.google.com/view/synthetic-dataset- Generation 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13405v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **SEDNet: Shallow Encoder-Decoder Network for Brain Tumor Segmentation**<br />
**Title_cn:** SEDNet：用于脑肿瘤分割的浅层编码器-解码器网络<br />
**Authors:** Chollette C. Olisah<br />
**Abstract:** <details><summary>原文: </summary>Despite the advancement in computational modeling towards brain tumor segmentation, of which several models have been developed, it is evident from the computational complexity of existing models which are still at an all-time high, that performance and efficiency under clinical application scenarios are limited. Therefore, this paper proposes a shallow encoder and decoder network named SEDNet for brain tumor segmentation. The proposed network is adapted from the U-Net structure. Though brain tumors do not assume complex structures like the task the traditional U-Net was designed for, their variance in appearance, shape, and ambiguity of boundaries makes it a compelling complex task to solve. SEDNet architecture design is inspired by the localized nature of brain tumors in brain images, thus consists of sufficient hierarchical convolutional blocks in the encoding pathway capable of learning the intrinsic features of brain tumors in brain slices, and a decoding pathway with selective skip path sufficient for capturing miniature local-level spatial features alongside the global-level features of brain tumor. SEDNet with the integration of the proposed preprocessing algorithm and optimization function on the BraTS2020 set reserved for testing achieves impressive dice and Hausdorff scores of 0.9308, 0.9451, 0.9026, and 0.7040, 1.2866, 0.7762 for non-enhancing tumor core (NTC), peritumoral edema (ED), and enhancing tumor (ET), respectively. Furthermore, through transfer learning with initialized SEDNet pre-trained weights, termed SEDNetX, a performance increase is observed. The dice and Hausdorff scores recorded are 0.9336, 0.9478, 0.9061, 0.6983, 1.2691, and 0.7711 for NTC, ED, and ET, respectively. With about 1.3 million parameters and impressive performance in comparison to the state-of-the-art, SEDNet(X) is shown to be computationally efficient for real-time clinical diagnosis.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管脑肿瘤分割的计算模型取得了进展，并开发了多种模型，但从现有模型的计算复杂度仍然处于历史最高水平可以看出，临床应用场景下的性能和效率受到限制。因此，本文提出了一种名为SEDNet的浅层编码器和解码器网络，用于脑肿瘤分割。所提出的网络改编自 U-Net 结构。尽管脑肿瘤不像传统 U-Net 所设计的任务那样具有复杂的结构，但它们在外观、形状和边界模糊性方面的差异使其成为一项引人注目的复杂任务。 SEDNet架构设计的灵感来自于脑图像中脑肿瘤的局部性质，因此由编码路径中足够的分层卷积块组成，能够学习脑切片中脑肿瘤的内在特征，以及具有足以用于学习脑切片中脑肿瘤的内在特征的解码路径。捕获微型局部空间特征以及脑肿瘤的全局特征。 SEDNet 将所提出的预处理算法和优化功能集成在为测试保留的 BraTS2020 集上，对于非增强肿瘤核心 (NTC)、瘤周水肿，获得了令人印象深刻的 dice 和 Hausdorff 分数 0.9308、0.9451、0.9026 和 0.7040、1.2866、0.7762分别是（ED）和增强肿瘤（ET）。此外，通过使用初始化的 SEDNet 预训练权重（称为 SEDNetX）进行迁移学习，可以观察到性能的提高。 NTC、ED 和 ET 记录的骰子分数和 Hausdorff 分数分别为 0.9336、0.9478、0.9061、0.6983、1.2691 和 0.7711。与最先进的技术相比，SEDNet(X) 拥有约 130 万个参数和令人印象深刻的性能，在实时临床诊断方面具有计算效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.13403v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion**<br />
**Title_cn:** UNIMO-G：通过多模态条件扩散生成统一图像<br />
**Authors:** Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao<br />
**Abstract:** <details><summary>原文: </summary>Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents \textbf{UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的文本到图像的扩散模型主要根据文本提示生成图像。然而，文本描述固有的简洁性给忠实合成具有复杂细节（例如特定实体或场景）的图像带来了挑战。本文提出了 \textbf{UNIMO-G}，一个简单的多模态条件扩散框架，它在具有交错文本和视觉输入的多模态提示上运行，展示了文本驱动和主题驱动图像生成的统一能力。 UNIMO-G 包含两个核心组件：用于编码多模态提示的多模态大语言模型 (MLLM)，以及用于基于编码的多模态输入生成图像的条件去噪扩散网络。我们利用两阶段训练策略来有效训练框架：首先对大规模文本图像对进行预训练以开发条件图像生成能力，然后使用多模态提示进行指令调整以实现统一的图像生成能力。采用精心设计的涉及语言基础和图像分割的数据处理管道来构建多模式提示。 UNIMO-G 在文本到图像生成和零样本主题驱动合成方面均表现出色，并且在从涉及多个图像实体的复杂多模态提示生成高保真图像方面尤其有效。</details>
**PDF:** <http://arxiv.org/pdf/2401.13388v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain**<br />
**Title_cn:** 混合频色域中的隐私保护人脸识别<br />
**Authors:** Dong Han, Yong Li, Joachim Denzler<br />
**Abstract:** <details><summary>原文: </summary>Face recognition technology has been deployed in various real-life applications. The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy. It is quite common for clients to upload face images to the service provider in order to access the model inference. However, the face image is a type of sensitive biometric attribute tied to the identity information of each user. Directly exposing the raw face image to the service provider poses a threat to the user's privacy. Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding. The noticeable drop in recognition accuracy is a pitfall for most methods. This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain. Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise. Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities. Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference. The proposed method performs well on multiple widely used verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸识别技术已被部署在各种现实生活中。最复杂的基于深度学习的人脸识别系统依靠通过复杂的深度神经网络训练数百万张人脸图像来实现高精度。客户将人脸图像上传到服务提供商以访问模型推理是很常见的。然而，面部图像是一种与每个用户的身份信息相关的敏感生物特征属性。将原始人脸图像直接暴露给服务提供商会对用户的隐私造成威胁。当前人脸识别的隐私保护方法侧重于隐藏模型输入上的视觉信息或保护模型输出人脸嵌入。识别准确度的显着下降是大多数方法的缺陷。本文提出了一种混合频率-颜色融合方法来降低频域中人脸识别的输入维度。此外，还引入了稀疏颜色信息，以减轻添加差分隐私噪声后精度的显着下降。此外，还应用了特定于身份的嵌入映射方案，通过扩大身份之间的距离来保护原始人脸嵌入。最后，实现了安全多方计算，以在模型推理过程中安全地计算嵌入距离。所提出的方法在多个广泛使用的验证数据集上表现良好。此外，在 1:N 验证场景中，它的准确度比最先进的技术高出约 2.6% 至 4.2%。</details>
**PDF:** <http://arxiv.org/pdf/2401.13386v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks**<br />
**Title_cn:** NACHOS：硬件约束提前退出神经网络的神经架构搜索<br />
**Authors:** Matteo Gambella, Jary Pomponi, Simone Scardapane, Manuel Roveri<br />
**Abstract:** <details><summary>原文: </summary>Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN</details>
**Abstract_cn:** <details><summary>译文: </summary>早期退出神经网络 (EENN) 赋予标准深度神经网络 (DNN) 早期退出分类器 (EEC)，以便在分类获得足够的置信度时在处理的中间点提供预测。这在有效性和效率方面带来了许多好处。目前，EENN 的设计是由专家手动进行的，这是一项复杂且耗时的任务，需要考虑许多方面，包括 EEC 的正确放置、阈值处理和计算开销。为此，该研究正在探索使用神经架构搜索（NAS）来自动化 EENN 的设计。目前，文献中很少提出针对 EENN 的全面 NAS 解决方案，同时考虑骨干网和 EEC 的全自动联合设计策略仍然是一个悬而未决的问题。为此，这项工作提出了硬件约束提前退出神经网络 (NACHOS) 的神经架构搜索，这是第一个用于设计最佳 EENN 的 NAS 框架，满足精度以及乘法和累加 (MAC) 操作执行次数的约束。推理时的 EENN。特别是，这提供了骨干网和 EEC 的联合设计，以在准确性和 MAC 数量之间的最佳权衡方面选择一组可接受的（即尊重约束）帕累托最优解决方案。结果表明，NACHOS 设计的模型与最先进的 EENN 具有竞争力。此外，这项工作还研究了两个新颖的正则化项的有效性，这些正则化项是为优化 EENN 辅助分类器而设计的。</details>
**PDF:** <http://arxiv.org/pdf/2401.13330v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery**<br />
**Title_cn:** 用于广义类别发现的内存一致性引导分而治之学习<br />
**Authors:** Yuanpeng Tu, Zhun Zhong, Yuxi Li, Hengshuang Zhao<br />
**Abstract:** <details><summary>原文: </summary>Generalized category discovery (GCD) aims at addressing a more realistic and challenging setting of semi-supervised learning, where only part of the category labels are assigned to certain training samples. Previous methods generally employ naive contrastive learning or unsupervised clustering scheme for all the samples. Nevertheless, they usually ignore the inherent critical information within the historical predictions of the model being trained. Specifically, we empirically reveal that a significant number of salient unlabeled samples yield consistent historical predictions corresponding to their ground truth category. From this observation, we propose a Memory Consistency guided Divide-and-conquer Learning framework (MCDL). In this framework, we introduce two memory banks to record historical prediction of unlabeled data, which are exploited to measure the credibility of each sample in terms of its prediction consistency. With the guidance of credibility, we can design a divide-and-conquer learning strategy to fully utilize the discriminative information of unlabeled data while alleviating the negative influence of noisy labels. Extensive experimental results on multiple benchmarks demonstrate the generality and superiority of our method, where our method outperforms state-of-the-art models by a large margin on both seen and unseen classes of the generic image recognition and challenging semantic shift settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).</details>
**Abstract_cn:** <details><summary>译文: </summary>广义类别发现（GCD）旨在解决更现实且更具挑战性的半监督学习设置，其中仅将部分类别标签分配给某些训练样本。以前的方法通常对所有样本采用朴素对比学习或无监督聚类方案。然而，他们通常忽略正在训练的模型的历史预测中固有的关键信息。具体来说，我们凭经验揭示，大量显着的未标记样本产生与其真实类别相对应的一致历史预测。根据这一观察，我们提出了一种记忆一致性引导的分而治之的学习框架（MCDL）。在这个框架中，我们引入了两个内存库来记录未标记数据的历史预测，利用它们来衡量每个样本在预测一致性方面的可信度。在可信度的指导下，我们可以设计分而治之的学习策略，充分利用无标签数据的判别信息，同时减轻噪声标签的负面影响。多个基准的广泛实验结果证明了我们方法的通用性和优越性，我们的方法在通用图像识别的可见和不可见类别和具有挑战性的语义转移设置（即， CUB 收益增加 8.4%，Standford Cars 收益增加 8.1%）。</details>
**PDF:** <http://arxiv.org/pdf/2401.13325v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging**<br />
**Title_cn:** 通过深度学习改进合成窄带成像息肉检测<br />
**Authors:** Mathias Ramm Haugland, Hemin Ali Qadir, Ilangko Balasingham<br />
**Abstract:** <details><summary>原文: </summary>To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the original WLI. This is because our WLI-to-SNBI translation model can enhance the observation of polyp surface patterns in the generated SNBI images.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了应对结直肠癌 (CRC) 日益增长的患病率，息肉检测和切除的筛查计划已证明其有用性。结肠镜检查被认为是 CRC 筛查效果最好的程序。为了简化检查，针对传统白光成像 (WLI) 开发了基于深度学习的自动息肉检测方法。与 WLI 相比，窄带成像 (NBI) 可以改善结肠镜检查期间息肉的分类，但需要特殊设备。我们提出了一种基于 CycleGAN 的框架，可将常规 WLI 捕获的图像转换为合成 NBI (SNBI)，作为一种预处理方法，用于在 NBI 不可用时改进 WLI 上的对象检测。本文首先表明，与相对相似的 WLI 数据集相比，NBI 可以实现更好的息肉检测结果。其次，实验结果表明，与原始 WLI 相比，我们提出的模态转换可以在 WLI 生成的 SNBI 图像上实现改进的息肉检测。这是因为我们的 WLI 到 SNBI 转换模型可以增强对生成的 SNBI 图像中息肉表面图案的观察。</details>
**PDF:** <http://arxiv.org/pdf/2401.13315v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region**<br />
**Title_cn:** LiDAR 点云中的小物体跟踪：学习目标感知原型和细粒度搜索区域<br />
**Authors:** Shengjing Tian, Yinan Han, Xiuping Liu, Xiantong Zhao<br />
**Abstract:** <details><summary>原文: </summary>Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>激光雷达点云中的单目标跟踪是环境感知中最重要的部分之一，其中小目标在现实场景中是不可避免的，这会给准确定位带来重大障碍。然而，现有的方法更多地集中于探索通用类别的通用架构，而忽视了小物体由于前景点相对缺乏和对干扰的容忍度较低而长期以来一直是棘手的挑战。为此，我们提出了一种基于连体网络的激光雷达点云小目标跟踪方法，该方法由目标感知原型挖掘（TAPM）模块和区域网格细分（RGS）模块组成。 TAPM模块采用掩模解码器的重建机制来学习特征空间中的原型，旨在突出前景点的存在，以利于后续小物体的定位。通过上述原型能够强调感兴趣的小物体，特征图中的定位偏差仍然导致较高的跟踪误差。为了缓解这个问题，提出了 RGS 模块来基于 ViT 和像素洗牌层恢复搜索区域的细粒度特征。此外，除了正常设置之外，我们还精心设计了缩放实验来评估不同跟踪器在小物体上的鲁棒性。在 KITTI 和 nuScenes 上的大量实验表明，我们的方法可以有效提高小目标的跟踪性能，而不影响正常大小的目标。</details>
**PDF:** <http://arxiv.org/pdf/2401.13285v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In Machine-Assisted Skin Disease Detection**<br />
**Title_cn:** DDI-CoCo：用于了解机器辅助皮肤病检测中颜色对比度效果的数据集<br />
**Authors:** Ming-Chang Chiu, Yingfei Wang, Yen-Ju Kuo, Pin-Yu Chen<br />
**Abstract:** <details><summary>原文: </summary>Skin tone as a demographic bias and inconsistent human labeling poses challenges in dermatology AI. We take another angle to investigate color contrast's impact, beyond skin tones, on malignancy detection in skin disease datasets: We hypothesize that in addition to skin tones, the color difference between the lesion area and skin also plays a role in malignancy detection performance of dermatology AI models. To study this, we first propose a robust labeling method to quantify color contrast scores of each image and validate our method by showing small labeling variations. More importantly, applying our method to \textit{the only} diverse-skin tone and pathologically-confirmed skin disease dataset DDI, yields \textbf{DDI-CoCo Dataset}, and we observe a performance gap between the high and low color difference groups. This disparity remains consistent across various state-of-the-art (SoTA) image classification models, which supports our hypothesis. Furthermore, we study the interaction between skin tone and color difference effects and suggest that color difference can be an additional reason behind model performance bias between skin tones. Our work provides a complementary angle to dermatology AI for improving skin disease detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>肤色作为人口统计偏差和不一致的人类标签给皮肤病学人工智能带来了挑战。我们从另一个角度来研究肤色之外的颜色对比对皮肤病数据集中恶性肿瘤检测的影响：我们假设除了肤色之外，病变区域和皮肤之间的色差也在皮肤科恶性肿瘤检测性能中发挥着作用人工智能模型。为了研究这个问题，我们首先提出了一种鲁棒的标记方法来量化每个图像的颜色对比度分数，并通过显示小的标记变化来验证我们的方法。更重要的是，将我们的方法应用于\textit{唯一}多样化肤色和病理证实的皮肤疾病数据集DDI，产生\textbf{DDI-CoCo数据集}，并且我们观察到高色差组和低色差组之间的性能差距。这种差异在各种最先进的 (SoTA) 图像分类模型中保持一致，这支持了我们的假设。此外，我们研究了肤色和色差效应之间的相互作用，并表明色差可能是肤色之间模型性能偏差背后的另一个原因。我们的工作为皮肤病学人工智能提供了一个补充角度，以改善皮肤疾病的检测。</details>
**PDF:** <http://arxiv.org/pdf/2401.13280v1><br />
**Code:** <https://github.com/charismaticchiu/ddi-coco>**<br />
>>**index:** 25<br />
**Title:** **Enhancing cross-domain detection: adaptive class-aware contrastive transformer**<br />
**Title_cn:** 增强跨域检测：自适应类感知对比变压器<br />
**Authors:** Ziru Zeng, Yue Ding, Hongtao Lu<br />
**Abstract:** <details><summary>原文: </summary>Recently,the detection transformer has gained substantial attention for its inherent minimal post-processing requirement.However,this paradigm relies on abundant training data,yet in the context of the cross-domain adaptation,insufficient labels in the target domain exacerbate issues of class imbalance and model performance degradation.To address these challenges, we propose a novel class-aware cross domain detection transformer based on the adversarial learning and mean-teacher framework.First,considering the inconsistencies between the classification and regression tasks,we introduce an IoU-aware prediction branch and exploit the consistency of classification and location scores to filter and reweight pseudo labels.Second, we devise a dynamic category threshold refinement to adaptively manage model confidence.Third,to alleviate the class imbalance,an instance-level class-aware contrastive learning module is presented to encourage the generation of discriminative features for each class,particularly benefiting minority classes.Experimental results across diverse domain-adaptive scenarios validate our method's effectiveness in improving performance and alleviating class imbalance issues,which outperforms the state-of-the-art transformer based methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，检测变压器因其固有的最小后处理要求而受到广泛关注。然而，这种范式依赖于丰富的训练数据，但在跨域适应的背景下，目标域中的标签不足加剧了类别不平衡的问题为了解决这些挑战，我们提出了一种基于对抗性学习和平均教师框架的新型类感知跨域检测变压器。首先，考虑到分类任务和回归任务之间的不一致，我们引入了一种 IoU 感知模型预测分支并利用分类和位置分数的一致性来过滤和重新加权伪标签。其次，我们设计了动态类别阈值细化来自适应管理模型置信度。第三，为了缓解类不平衡，实例级类感知对比学习模块的提出是为了鼓励为每个类别生成判别性特征，特别是有利于少数类别。跨不同领域自适应场景的实验结果验证了我们的方法在提高性能和缓解类别不平衡问题方面的有效性，其性能优于最先进的方法基于变压器的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13264v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Segment Any Cell: A SAM-based Auto-prompting Fine-tuning Framework for Nuclei Segmentation**<br />
**Title_cn:** 分割任意细胞：基于 SAM 的细胞核分割自动提示微调框架<br />
**Authors:** Saiyang Na, Yuzhi Guo, Feng Jiang, Hehuan Ma, Junzhou Huang<br />
**Abstract:** <details><summary>原文: </summary>In the rapidly evolving field of AI research, foundational models like BERT and GPT have significantly advanced language and vision tasks. The advent of pretrain-prompting models such as ChatGPT and Segmentation Anything Model (SAM) has further revolutionized image segmentation. However, their applications in specialized areas, particularly in nuclei segmentation within medical imaging, reveal a key challenge: the generation of high-quality, informative prompts is as crucial as applying state-of-the-art (SOTA) fine-tuning techniques on foundation models. To address this, we introduce Segment Any Cell (SAC), an innovative framework that enhances SAM specifically for nuclei segmentation. SAC integrates a Low-Rank Adaptation (LoRA) within the attention layer of the Transformer to improve the fine-tuning process, outperforming existing SOTA methods. It also introduces an innovative auto-prompt generator that produces effective prompts to guide segmentation, a critical factor in handling the complexities of nuclei segmentation in biomedical imaging. Our extensive experiments demonstrate the superiority of SAC in nuclei segmentation tasks, proving its effectiveness as a tool for pathologists and researchers. Our contributions include a novel prompt generation strategy, automated adaptability for diverse segmentation tasks, the innovative application of Low-Rank Attention Adaptation in SAM, and a versatile framework for semantic segmentation challenges.</details>
**Abstract_cn:** <details><summary>译文: </summary>在快速发展的人工智能研究领域，BERT 和 GPT 等基础模型具有显着先进的语言和视觉任务。 ChatGPT 和 Segmentation Anything Model (SAM) 等训练前提示模型的出现进一步彻底改变了图像分割。然而，它们在专业领域的应用，特别是在医学成像中的细胞核分割中，揭示了一个关键的挑战：生成高质量、信息丰富的提示与将最先进的 (SOTA) 微调技术应用于基础模型。为了解决这个问题，我们引入了 Segment Any Cell (SAC)，这是一种创新框架，专门针对细胞核分割增强了 SAM。 SAC 在 Transformer 的注意力层中集成了低秩适应 (LoRA)，以改进微调过程，优于现有的 SOTA 方法。它还引入了创新的自动提示生成器，可产生有效的提示来指导分割，这是处理生物医学成像中细胞核分割复杂性的关键因素。我们广泛的实验证明了 SAC 在细胞核分割任务中的优越性，证明了它作为病理学家和研究人员工具的有效性。我们的贡献包括新颖的提示生成策略、对不同分割任务的自动适应性、SAM 中低秩注意力适应的创新应用，以及用于语义分割挑战的多功能框架。</details>
**PDF:** <http://arxiv.org/pdf/2401.13220v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network**<br />
**Title_cn:** AMANet：利用自适应多层次注意力网络推进 SAR 船舶检测<br />
**Authors:** Xiaolin Ma, Junkai Cheng, Aihua Li, Yuhua Zhang, Zhilong Lin<br />
**Abstract:** <details><summary>原文: </summary>Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-hierarchical attention network (AMANet) by embedding the AMAM between the backbone network and the feature pyramid network (FPN). Besides, the AMAM can be readily inserted between different frameworks to improve object detection. Lastly, extensive experiments on two large-scale SAR ship detection datasets demonstrate that our AMANet method is superior to state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于深度学习的方法已成功应用于合成孔径雷达（SAR）图像的船舶检测。尽管开发了多种船舶检测方法，但由于沿海环境的特征有限和杂乱，检测小型沿海船舶仍然是一个重大挑战。为此，提出了一种新颖的自适应多层次注意力模块（AMAM）来学习多尺度特征，并自适应地聚合来自各个特征层的显着特征，即使在复杂的环境中也是如此。具体来说，我们首先融合相邻特征层的信息来增强对较小目标的检测，从而实现多尺度特征增强。然后，为了过滤掉复杂背景的不利影响，我们剖析了通道上先前融合的多级特征，单独挖掘显着区域，并自适应地合并来自不同通道的特征。第三，我们通过在主干网络和特征金字塔网络（FPN）之间嵌入AMAM，提出了一种新颖的自适应多层次注意力网络（AMANet）。此外，AMAM 可以轻松插入不同框架之间，以改进对象检测。最后，对两个大型 SAR 船舶检测数据集的广泛实验表明，我们的 AMANet 方法优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13214v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Common-Sense Bias Discovery and Mitigation for Classification Tasks**<br />
**Title_cn:** 分类任务的常识性偏差发现和缓解<br />
**Authors:** Miao Zhang, Zee fryer, Ben Colman, Ali Shahriyari, Gaurav Bharaj<br />
**Abstract:** <details><summary>原文: </summary>Machine learning model bias can arise from dataset composition: sensitive features correlated to the learning target disturb the model decision rule and lead to performance differences along the features. Existing de-biasing work captures prominent and delicate image features which are traceable in model latent space, like colors of digits or background of animals. However, using the latent space is not sufficient to understand all dataset feature correlations. In this work, we propose a framework to extract feature clusters in a dataset based on image descriptions, allowing us to capture both subtle and coarse features of the images. The feature co-occurrence pattern is formulated and correlation is measured, utilizing a human-in-the-loop for examination. The analyzed features and correlations are human-interpretable, so we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive correlations in a dataset, we demonstrate that downstream model bias can be mitigated by adjusting image sampling weights, without requiring a sensitive group label supervision. Experiments show that our method discovers novel biases on multiple classification tasks for two benchmark image datasets, and the intervention outperforms state-of-the-art unsupervised bias mitigation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习模型偏差可能源于数据集组成：与学习目标相关的敏感特征会干扰模型决策规则，并导致特征上的性能差异。现有的去偏工作捕获了可在模型潜在空间中追踪的突出且精致的图像特征，例如数字的颜色或动物的背景。然而，使用潜在空间不足以理解所有数据集特征相关性。在这项工作中，我们提出了一个基于图像描述提取数据集中的特征簇的框架，使我们能够捕获图像的微妙和粗略特征。制定特征共现模式并测量相关性，利用人在环进行检查。分析的特征和相关性是人类可以解释的，因此我们将该方法命名为常识偏差发现（CSBD）。在暴露数据集中的敏感相关性后，我们证明可以通过调整图像采样权重来减轻下游模型偏差，而不需要敏感组标签监督。实验表明，我们的方法发现了两个基准图像数据集的多个分类任务的新偏差，并且干预优于最先进的无监督偏差缓解方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13213v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation**<br />
**Title_cn:** AdCorDA：通过对抗性校正和域适应进行分类器细化<br />
**Authors:** Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark<br />
**Abstract:** <details><summary>原文: </summary>This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. The adversarial correction technique also results in enhanced robustness to adversarial attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文描述了一种简单而有效的技术，用于改进预训练分类器网络。所提出的 AdCorDA 方法基于训练集的修改并利用网络权重和层输入之间的对偶性。我们称之为输入空间训练。该方法由两个阶段组成：对抗性校正和域适应。对抗性纠正使用对抗性攻击来纠正不正确的训练集分类。训练集中错误分类的样本被去除并用对抗性校正的样本替换以形成新的训练集，然后在第二阶段执行域适应回到原始训练集。广泛的实验验证表明，CIFAR-100 数据集上的准确率显着提高了 5% 以上。该技术可以直接应用于权重量化神经网络的细化，其中实验表明性能比基线有显着提高。对抗性校正技术还可以增强对抗性攻击的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13212v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size**<br />
**Title_cn:** 通过局部混合和自适应步长提高对抗性示例的可迁移性<br />
**Authors:** Junlin Liu, Xinchen Lyu<br />
**Abstract:** <details><summary>原文: </summary>Adversarial examples are one critical security threat to various visual applications, where injected human-imperceptible perturbations can confuse the output.Generating transferable adversarial examples in the black-box setting is crucial but challenging in practice. Existing input-diversity-based methods adopt different image transformations, but may be inefficient due to insufficient input diversity and an identical perturbation step size. Motivated by the fact that different image regions have distinctive weights in classification, this paper proposes a black-box adversarial generative framework by jointly designing enhanced input diversity and adaptive step sizes. We design local mixup to randomly mix a group of transformed adversarial images, strengthening the input diversity. For precise adversarial generation, we project the perturbation into the $tanh$ space to relax the boundary constraint. Moreover, the step sizes of different regions can be dynamically adjusted by integrating a second-order momentum.Extensive experiments on ImageNet validate that our framework can achieve superior transferability compared to state-of-the-art baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性示例是对各种视觉应用程序的一种关键安全威胁，其中注入的人类难以察觉的扰动可能会混淆输出。在黑盒设置中生成可转移的对抗性示例至关重要，但在实践中具有挑战性。现有的基于输入多样性的方法采用不同的图像变换，但由于输入多样性不足和相同的扰动步长可能效率低下。由于不同图像区域在分类中具有不同的权重，本文通过联合设计增强的输入多样性和自适应步长，提出了一种黑盒对抗生成框架。我们设计局部混合来随机混合一组变换后的对抗图像，增强输入多样性。为了精确生成对抗性，我们将扰动投影到 $tanh$ 空间中以放松边界约束。此外，可以通过积分二阶动量来动态调整不同区域的步长。在 ImageNet 上的大量实验验证了我们的框架与最先进的基线相比可以实现卓越的可迁移性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13205v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN**<br />
**Title_cn:** Catch-Up Mix：CNN 中陷入困境的过滤器的 Catch-Up 类<br />
**Authors:** Minsoo Kang, Minkoo Kang, Suhyun Kim<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, our idea is to mitigate the problem of over-reliance on strong filters by substituting highly activated features. To this end, we present a novel method called Catch-up Mix, which provides learning opportunities to a wide range of filters during training, focusing on filters that may lag behind. By mixing activation maps with relatively lower norms, Catch-up Mix promotes the development of more diverse representations and reduces reliance on a small subset of filters. Experimental results demonstrate the superiority of our method in various vision classification datasets, providing enhanced robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习在计算机视觉领域取得了重大进展，特别是在图像分类任务中。尽管深度学习模型在训练数据上具有很高的准确性，但它经常面临与复杂性和过度拟合相关的挑战。一个值得注意的问题是，该模型通常严重依赖有限的过滤器子集来进行预测。这种依赖性可能会导致泛化能力受损，并且更容易受到微小变化的影响。虽然权重衰减、丢失和数据增强等正则化技术通常用于解决这个问题，但它们可能无法直接解决对特定过滤器的依赖。我们的观察表明，当慢速学习过滤器因快速学习过滤器而被剥夺学习机会时，严重依赖问题就会变得严重。从图像增强研究中汲取灵感，通过删除和替换部分图像来对抗对特定图像区域的过度依赖，我们的想法是通过替换高度激活的特征来减轻对强过滤器的过度依赖问题。为此，我们提出了一种称为 Catch-up Mix 的新颖方法，它在训练期间为各种过滤器提供学习机会，重点关注可能落后的过滤器。通过将激活图与相对较低的规范混合，Catch-up Mix 促进了更多样化表示的开发，并减少了对一小部分过滤器的依赖。实验结果证明了我们的方法在各种视觉分类数据集中的优越性，提供了增强的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13193v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **Towards Multi-domain Face Landmark Detection with Synthetic Data from Diffusion model**<br />
**Title_cn:** 利用扩散模型的合成数据进行多域人脸特征点检测<br />
**Authors:** Yuanming Li, Gwantae Kim, Jeong-gi Kwak, Bon-hwa Ku, Hanseok Ko<br />
**Abstract:** <details><summary>原文: </summary>Recently, deep learning-based facial landmark detection for in-the-wild faces has achieved significant improvement. However, there are still challenges in face landmark detection in other domains (e.g. cartoon, caricature, etc). This is due to the scarcity of extensively annotated training data. To tackle this concern, we design a two-stage training approach that effectively leverages limited datasets and the pre-trained diffusion model to obtain aligned pairs of landmarks and face in multiple domains. In the first stage, we train a landmark-conditioned face generation model on a large dataset of real faces. In the second stage, we fine-tune the above model on a small dataset of image-landmark pairs with text prompts for controlling the domain. Our new designs enable our method to generate high-quality synthetic paired datasets from multiple domains while preserving the alignment between landmarks and facial features. Finally, we fine-tuned a pre-trained face landmark detection model on the synthetic dataset to achieve multi-domain face landmark detection. Our qualitative and quantitative results demonstrate that our method outperforms existing methods on multi-domain face landmark detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于深度学习的野外人脸特征点检测取得了显着的进步。然而，其他领域（例如卡通、漫画等）的人脸特征点检测仍然存在挑战。这是由于缺乏广泛注释的训练数据。为了解决这个问题，我们设计了一种两阶段训练方法，该方法有效地利用有限的数据集和预训练的扩散模型来获得多个领域中对齐的地标和人脸对。在第一阶段，我们在大型真实人脸数据集上训练地标条件人脸生成模型。在第二阶段，我们在图像地标对的小型数据集上对上述模型进行微调，并带有用于控制域的文本提示。我们的新设计使我们的方法能够从多个领域生成高质量的合成配对数据集，同时保持地标和面部特征之间的对齐。最后，我们在合成数据集上微调预训练的人脸标志检测模型，以实现多域人脸标志检测。我们的定性和定量结果表明，我们的方法在多域人脸特征点检测方面优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13191v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Boundary and Relation Distillation for Semantic Segmentation**<br />
**Title_cn:** 语义分割的边界和关系蒸馏<br />
**Authors:** Dong Zhang, Pingcheng Dong, Xinting Hu, Long Chen, Kwang-Ting Cheng<br />
**Abstract:** <details><summary>原文: </summary>Recently, it has been revealed that small semantic segmentation (SS) models exhibit a tendency to make errors in maintaining boundary region completeness and preserving target region connectivity, despite their effective segmentation of the main object regions. To address these errors, we propose a targeted boundary and relation distillation (BRD) strategy using knowledge distillation from large teacher models to small student models. Specifically, the boundary distillation extracts explicit object boundaries from the hierarchical feature maps of the backbone network, subsequently enhancing the student model's mask quality in boundary regions. Concurrently, the relation distillation transfers implicit relations from the teacher model to the student model using pixel-level self-relation as a bridge, ensuring that the student's mask has strong target region connectivity. The proposed BRD is designed concretely for SS and is characterized by simplicity and efficiency. Through experimental evaluations on multiple SS datasets, including Pascal VOC 2012, Cityscapes, ADE20K, and COCO-Stuff 10K, we demonstrated that BRD significantly surpasses the current methods without increasing the inference costs, generating crisp region boundaries and smooth connecting regions that are challenging for small models.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，研究表明，小型语义分割（SS）模型尽管能够有效分割主要对象区域，但在保持边界区域完整性和保持目标区域连通性方面容易出错。为了解决这些错误，我们提出了一种有针对性的边界和关系蒸馏（BRD）策略，使用从大型教师模型到小型学生模型的知识蒸馏。具体来说，边界蒸馏从主干网络的分层特征图中提取明确的对象边界，随后增强学生模型在边界区域的掩模质量。同时，关系蒸馏以像素级自关系为桥梁，将隐式关系从教师模型转移到学生模型，确保学生掩模具有较强的目标区域连接性。所提出的BRD是针对SS具体设计的，具有简单、高效的特点。通过对多个 SS 数据集（包括 Pascal VOC 2012、Cityscapes、ADE20K 和 COCO-Stuff 10K）的实验评估，我们证明 BRD 在不增加推理成本的情况下显着超越了当前方法，生成了清晰的区域边界和平滑的连接区域，这对于小型号。</details>
**PDF:** <http://arxiv.org/pdf/2401.13174v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation**<br />
**Title_cn:** 通过动态分组和原型聚合实现高效且有效的深度聚类<br />
**Authors:** Haixin Zhang, Dong Huang<br />
**Abstract:** <details><summary>原文: </summary>Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability. Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner. To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro. Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups. Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance. Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training. Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art. Code is available at https://github.com/Regan-Zhang/DigPro.</details>
**Abstract_cn:** <details><summary>译文: </summary>以前的对比深度聚类方法主要关注实例级信息，而忽略了组/簇内的成员关系，这可能会严重削弱其表示学习和聚类能力。最近，已经开发了一些分组对比方法，然而，这些方法通常依赖于整个数据集的样本来获取伪标签，并且缺乏以批量方式有效更新分组分配的能力。为了解决这些关键问题，我们提出了一种新颖的端到端深度聚类框架，具有动态分组和原型聚合功能，称为 DigPro。具体来说，所提出的动态分组将对比学习从实例级扩展到组级，这对于及时更新组是有效且高效的。同时，我们在球形特征空间中对原型进行对比学习，称为原型聚合，其目的是最大化簇间距离。值得注意的是，通过期望最大化框架，DigPro 在自监督训练过程中同时利用了紧凑的集群内连接、良好分离的集群和高效的组更新。对六个图像基准的广泛实验证明了我们的方法比最先进的方法具有优越的性能。代码可在 https://github.com/Regan-Zhang/DigPro 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13581v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Benchmarking the Fairness of Image Upsampling Methods**<br />
**Title_cn:** 图像上采样方法的公平性基准测试<br />
**Authors:** Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer, Johannes Lederer<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，用于创建图像和视频等合成媒体的深度生成模型迅速发展。虽然这些模型在日常任务中的实际应用很诱人，但评估其公平性的固有风险至关重要。在这项工作中，我们引入了一个用于对条件生成模型的性能和公平性进行基准测试的综合框架。我们开发了一组指标$\unicode{x2013}$，其灵感来自于其监督公平性对应物$\unicode{x2013}$，以评估模型的公平性和多样性。专注于图像上采样的具体应用，我们创建了涵盖各种现代上采样方法的基准。作为基准测试的一部分，我们引入了 UnfairFace，它是 FairFace 的一个子集，它复制了常见的大规模人脸数据集的种族分布。我们的实证研究强调了使用无偏训练集的重要性，并揭示了算法如何响应数据集不平衡的变化。令人震惊的是，我们发现所考虑的方法都没有产生统计上公平且多样化的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.13555v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Generative Human Motion Stylization in Latent Space**<br />
**Title_cn:** 潜在空间中的生成人体运动风格化<br />
**Authors:** Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng<br />
**Abstract:** <details><summary>原文: </summary>Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reeanactment, content preservation, and generalization across various applications and settings. Project Page: https://yxmu.foo/GenMoStyle</details>
**Abstract_cn:** <details><summary>译文: </summary>人体动作风格化旨在修改输入动作的风格，同时保持其内容不变。与直接在姿势空间中操作的现有作品不同，我们利用预训练自动编码器的潜在空间作为运动提取和注入更具表现力和鲁棒性的表示。在此基础上，我们提出了一种新颖的生成模型，该模型可以产生单运动（潜在）代码的多种风格化结果。在训练期间，运动代码被分解为两个编码组件：确定性内容代码和遵循先验分布的概率风格代码；然后，生成器对内容和风格代码的随机组合进行处理，以重建相应的动作代码。我们的方法是通用的，允许从标记或未标记的运动风格中学习概率风格空间，在风格化方面也提供了显着的灵活性。在推理中，用户可以选择使用参考动作或标签中的风格提示来对动作进行风格化。即使没有明确的风格输入，我们的模型也可以通过从无条件风格先验分布中采样来促进新颖的重新风格化。实验结果表明，我们提出的风格化模型尽管采用轻量级设计，但在风格重演、内容保存以及跨各种应用程序和设置的泛化方面优于最先进的模型。项目页面：https://yxmu.foo/GenMoStyle</details>
**PDF:** <http://arxiv.org/pdf/2401.13505v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction**<br />
**Title_cn:** 通过部分信息辨别和跨级交互学习聚类表示<br />
**Authors:** Hai-Xin Zhang, Dong Huang, Hua-Bao Ling, Guang-Yu Zhang, Wei-jun Sun, Zi-hao Wen<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework. In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated. After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces. Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches. The source code is available at https://github.com/Regan-Zhang/PICI.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种称为 PICI 的新型深度图像聚类方法，该方法在联合学习框架中强制执行部分信息区分和跨级别交互。特别是，我们利用 Transformer 编码器作为主干，通过它制定具有两个并行增强视图的蒙版图像建模。通过 Transformer 编码器从蒙版图像中导出类别标记后，进一步合并了三个部分信息学习模块，包括用于通过蒙版图像重建来训练自动编码器的 PISD 模块、用于采用两级对比学习的 PICD 模块以及用于实例级和集群级子空间之间交互的CLI模块。在六个真实图像数据集上进行了大量实验，证明了所提出的 PICI 方法比最先进的深度聚类方法具有更优越的聚类性能。源代码可在 https://github.com/Regan-Zhang/PICI 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13503v1><br />
**Code:** <https://github.com/regan-zhang/pici>**<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks**<br />
**Title_cn:** VisualWebArena：在实际视觉 Web 任务上评估多模式代理<br />
**Authors:** Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried<br />
**Abstract:** <details><summary>原文: </summary>Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.</details>
**Abstract_cn:** <details><summary>译文: </summary>能够在网络上规划、推理和执行操作的自主代理为计算机任务自动化提供了一条有前途的途径。然而，大多数现有基准主要关注基于文本的代理，忽略了许多需要视觉信息才能有效解决的自然任务。鉴于大多数计算机界面迎合人类感知，视觉信息通常会以纯文本模型难以有效利用的方式增强文本数据。为了弥补这一差距，我们引入了 VisualWebArena，这是一个基准测试，旨在评估多模式 Web 代理在现实 \textit{视觉基础任务} 上的性能。 VisualWebArena 包含一组多样化且复杂的基于 Web 的任务，用于评估自主多模式代理的各种功能。为了达到这个基准，代理需要准确地处理图像文本输入，解释自然语言指令，并在网站上执行操作以实现用户定义的目标。我们对最先进的基于 LLM 的自主代理进行了广泛的评估，包括几个多模式模型。通过广泛的定量和定性分析，我们确定了纯文本 LLM 代理的一些局限性，并揭示了最先进的多模式语言代理的能力差距。 VisualWebArena 提供了一个用于评估多模式自主语言代理的框架，并提供了构建更强大的网络自主代理的见解。我们的代码、基线模型和数据可在 https://jykoh.com/vwa 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13649v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild**<br />
**Title_cn:** 扩展至卓越：实践模型扩展以在野外恢复照片般真实的图像<br />
**Authors:** Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, Chao Dong<br />
**Abstract:** <details><summary>原文: </summary>We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 SUPIR（缩放图像恢复），这是一种突破性的图像恢复方法，利用生成先验和模型缩放的力量。利用多模态技术和先进的生成先验，SUPIR 标志着智能和真实图像恢复领域的重大进步。作为 SUPIR 中的关键催化剂，模型缩放极大地增强了其功能，并展示了图像恢复的新潜力。我们收集了一个包含 2000 万张高分辨率、高质量图像的数据集用于模型训练，每张图像都富含描述性文本注释。 SUPIR 提供了通过文本提示恢复图像的功能，拓宽了其应用范围和潜力。此外，我们引入负面质量提示以进一步提高感知质量。我们还开发了一种恢复引导采样方法来抑制基于生成的恢复中遇到的保真度问题。实验证明了 SUPIR 卓越的恢复效果及其通过文本提示操作恢复的新颖能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.13627v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval**<br />
**Title_cn:** SciMMIR：科学多模态信息检索基准测试<br />
**Authors:** Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态信息检索（MMIR）是一个快速发展的领域，通过高级表示学习和跨模态对齐研究，已经取得了重大进展，特别是在图像文本配对方面。然而，当前评估科学领域内图文配对 MMIR 性能的基准显示出显着的差距，其中用学术语言描述的图表和表格图像通常不起重要作用。为了弥补这一差距，我们通过利用开放获取论文集来提取与科学领域相关的数据，开发了专门的科学 MMIR (SciMMIR) 基准。该基准测试由 53 万个精心策划的图像文本对组成，这些图像文本对是从科学文档中带有详细说明的图表中提取的。我们进一步用两级子集-子类别层次结构注释来注释图像-文本对，以促进对基线进行更全面的评估。我们对著名的多模态图像描述和视觉语言模型（例如 CLIP 和 BLIP）进行了零样本和微调评估。我们的分析为 MMIR 在科学领域提供了重要的见解，包括预训练和微调设置的影响以及视觉和文本编码器的影响。我们所有的数据和检查点均可在 https://github.com/Wusiwei0410/SciMMIR 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13478v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Serial fusion of multi-modal biometric systems**<br />
**Title_cn:** 多模态生物识别系统的串行融合<br />
**Authors:** Gian Luca Marcialis, Paolo Mastinu, Fabio Roli<br />
**Abstract:** <details><summary>原文: </summary>Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far. However, this approach exhibits some advantages with respect to the widely adopted parallel approaches. In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors. Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation. Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1.</details>
**Abstract_cn:** <details><summary>译文: </summary>迄今为止，多个生物识别匹配器的串行或连续融合尚未得到彻底研究。然而，相对于广泛采用的并行方法，这种方法表现出一些优势。在本文中，我们基于作者之前的工作，提出了一种用于评估此类系统性能的新颖理论框架。从理论上评估了性能方面的优势以及模型参数计算中的估计误差。通过在 NIST 生物识别评分集 1 上进行的初步实验，从其优缺点的角度对模型进行了分析。</details>
**PDF:** <http://arxiv.org/pdf/2401.13418v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval**<br />
**Title_cn:** 用于看不见的跨域视频时刻检索的生成视频扩散<br />
**Authors:** Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>Video Moment Retrieval (VMR) requires precise modelling of fine-grained moment-text associations to capture intricate visual-language relationships. Due to the lack of a diverse and generalisable VMR dataset to facilitate learning scalable moment-text associations, existing methods resort to joint training on both source and target domain videos for cross-domain applications. Meanwhile, recent developments in vision-language multimodal models pre-trained on large-scale image-text and/or video-text pairs are only based on coarse associations (weakly labelled). They are inadequate to provide fine-grained moment-text correlations required for cross-domain VMR. In this work, we solve the problem of unseen cross-domain VMR, where certain visual and textual concepts do not overlap across domains, by only utilising target domain sentences (text prompts) without accessing their videos. To that end, we explore generative video diffusion for fine-grained editing of source videos controlled by the target sentences, enabling us to simulate target domain videos. We address two problems in video editing for optimising unseen domain VMR: (1) generation of high-quality simulation videos of different moments with subtle distinctions, (2) selection of simulation videos that complement existing source training videos without introducing harmful noise or unnecessary repetitions. On the first problem, we formulate a two-stage video diffusion generation controlled simultaneously by (1) the original video structure of a source video, (2) subject specifics, and (3) a target sentence prompt. This ensures fine-grained variations between video moments. On the second problem, we introduce a hybrid selection mechanism that combines two quantitative metrics for noise filtering and one qualitative metric for leveraging VMR prediction on simulation video selection.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频时刻检索 (VMR) 需要对细粒度时刻-文本关联进行精确建模，以捕获复杂的视觉-语言关系。由于缺乏多样化且通用的VMR数据集来促进学习可扩展的时刻-文本关联，现有方法诉诸于对源域视频和目标域视频进行联合训练以实现跨域应用。与此同时，在大规模图像文本和/或视频文本对上预训练的视觉语言多模态模型的最新发展仅基于粗略关联（弱标记）。它们不足以提供跨域 VMR 所需的细粒度时刻文本关联。在这项工作中，我们通过仅利用目标域句子（文本提示）而不访问其视频，解决了看不见的跨域VMR问题，其中某些视觉和文本概念不跨域重叠。为此，我们探索生成视频扩散，以对由目标句子控制的源视频进行细粒度编辑，使我们能够模拟目标域视频。我们解决了视频编辑中的两个问题，以优化看不见的域VMR：（1）生成具有细微差别的不同时刻的高质量模拟视频，（2）选择补充现有源训练视频的模拟视频，而不会引入有害噪声或不必要的重复。关于第一个问题，我们制定了一个两阶段视频扩散生成，由（1）源视频的原始视频结构，（2）主题细节和（3）目标句子提示同时控制。这确保了视频时刻之间的细粒度变化。关于第二个问题，我们引入了一种混合选择机制，该机制结合了两个用于噪声过滤的定量指标和一个用于在模拟视频选择中利用 VMR 预测的定性指标。</details>
**PDF:** <http://arxiv.org/pdf/2401.13329v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions**<br />
**Title_cn:** InstructDoc：带有指令的视觉文档理解零样本泛化数据集<br />
**Authors:** Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko Saito, Jun Suzuki<br />
**Abstract:** <details><summary>原文: </summary>We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究通过人工编写的指令在现实世界文档上完成各种视觉文档理解（VDU）任务的问题，例如问答和信息提取。为此，我们提出了 InstructDoc，这是第一个包含 30 个公开可用的 VDU 数据集的大规模集合，每个数据集都有统一格式的不同指令，涵盖了 12 种任务，并包括开放文档类型/格式。此外，为了增强 VDU 任务的泛化性能，我们设计了一种新的基于指令的文档阅读和理解模型 InstructDr，它通过可训练的桥接模块连接文档图像、图像编码器和大语言模型 (LLM)。实验表明，InstructDr 可以通过给定的指令有效地适应新的 VDU 数据集、任务和领域，并且在无需特定训练的情况下优于现有的多模态 LLM 和 ChatGPT。</details>
**PDF:** <http://arxiv.org/pdf/2401.13313v1><br />
**Code:** <https://github.com/nttmdlab-nlp/instructdoc>**<br />
>>**index:** 7<br />
**Title:** **ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models**<br />
**Title_cn:** ConTextual：评估大型多模态模型中的上下文敏感文本丰富的视觉推理<br />
**Authors:** Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能的最新进展导致了大型多模态模型 (LMM) 的发展，该模型能够处理涉及对图像中的文本和视觉内容进行联合推理的复杂任务（例如，在公共场所导航地图）。本文介绍了 ConTextual，这是一种新颖的基准测试，由明确设计的指令组成，用于评估 LMM 执行上下文敏感的文本丰富的视觉推理的能力。 ConTextual 强调多样化的现实世界场景（例如，时间阅读、导航、购物等），需要更深入地理解文本和视觉元素之间的交互。我们的研究结果表明，使用人类评估，表现最好的 LMM、GPT-4V(ision) 和人类能力之间存在 30.8% 的显着性能差距，这表明上下文敏感的文本丰富的视觉推理还有很大的改进空间。值得注意的是，虽然 GPT-4V 在模因和引文解释等抽象类别方面表现出色，但其整体表现仍然落后于人类。除了人工评估之外，我们还采用了使用 GPT-4 的自动评估指标，发现了性能差异的类似趋势。我们还对不同的视觉环境进行细粒度的评估，并提供定性分析，为 LMM 设计的未来进步提供了一个强大的框架。 https://con-textual.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2401.13311v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **ChatterBox: Multi-round Multimodal Referring and Grounding**<br />
**Title_cn:** ChatterBox：多轮多模态参考和接地<br />
**Authors:** Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, Qixiang Ye<br />
**Abstract:** <details><summary>原文: </summary>In this study, we establish a baseline for a new task named multimodal multi-round referring and grounding (MRG), opening up a promising direction for instance-level multimodal dialogues. We present a new benchmark and an efficient vision-language model for this purpose. The new benchmark, named CB-300K, spans challenges including multi-round dialogue, complex spatial relationships among multiple instances, and consistent reasoning, which are beyond those shown in existing benchmarks. The proposed model, named ChatterBox, utilizes a two-branch architecture to collaboratively handle vision and language tasks. By tokenizing instance regions, the language branch acquires the ability to perceive referential information. Meanwhile, ChatterBox feeds a query embedding in the vision branch to a token receiver for visual grounding. A two-stage optimization strategy is devised, making use of both CB-300K and auxiliary external data to improve the model's stability and capacity for instance-level understanding. Experiments show that ChatterBox outperforms existing models in MRG both quantitatively and qualitatively, paving a new path towards multimodal dialogue scenarios with complicated and precise interactions. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们为名为多模态多轮参考和接地（MRG）的新任务建立了基线，为实例级多模态对话开辟了一个有希望的方向。为此，我们提出了一个新的基准和有效的视觉语言模型。新的基准测试名为 CB-300K，涵盖了多轮对话、多个实例之间复杂的空间关系以及一致推理等挑战，这些挑战超出了现有基准测试的范围。所提出的模型名为 ChatterBox，利用两分支架构来协作处理视觉和语言任务。通过对实例区域进行标记，语言分支获得了感知参考信息的能力。同时，ChatterBox 将嵌入视觉分支中的查询提供给令牌接收器以进行视觉基础。设计了两阶段优化策略，利用 CB-300K 和辅助外部数据来提高模型的稳定性和实例级理解的能力。实验表明，ChatterBox 在数量和质量上都优于 MRG 中的现有模型，为复杂而精确交互的多模态对话场景开辟了新的道路。代码、数据和模型可在以下位置获取：https://github.com/sunsmarterjie/ChatterBox。</details>
**PDF:** <http://arxiv.org/pdf/2401.13307v1><br />
**Code:** <https://github.com/sunsmarterjie/chatterbox>**<br />
>>**index:** 9<br />
**Title:** **MLLMReID: Multimodal Large Language Model-based Person Re-identification**<br />
**Title_cn:** MLLMReID：基于多模态大语言模型的行人重新识别<br />
**Authors:** Shan Yang, Yongfei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instruction, a simple approach that leverages the essence ability of LLMs to continue writing, avoiding complex and diverse instruction design. Secondly, we proposed DirectReID, which effectively employs the latent image feature vectors of images outputted by LLMs in ReID tasks. The experimental results demonstrate the superiority of our method. We will open-source the code on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在许多任务中取得了令人满意的结果。然而，迄今为止，它们在行人重新识别（ReID）任务中的表现尚未得到探索。本文将研究如何使它们适应 ReID 任务。一个直观的想法是使用 ReID 图像文本数据集微调 MLLM，然后使用其视觉编码器作为 ReID 的骨干。然而，仍然存在两个明显的问题：（1）为ReID设计指令，MLLM可能会过度拟合特定指令，并且设计多种指令会导致更高的成本。 (2) 来自 LLM 的潜在图像特征向量不参与损失计算。教学学习，对齐图像文本特征，导致间接优化和未充分利用特征的学习目标，限制了人物特征学习的有效性。为了解决这些问题，本文提出了MLLMReID：基于多模态大语言模型的ReID。首先，我们提出了Commonstruction，一种简单的方法，利用LLM继续写作的本质能力，避免复杂多样的指令设计。其次，我们提出了 DirectReID，它在 ReID 任务中有效地利用了 LLM 输出图像的潜在图像特征向量。实验结果证明了我们方法的优越性。我们将在 GitHub 上开源代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.13201v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Unified-Width Adaptive Dynamic Network for All-In-One Image Restoration**<br />
**Title_cn:** 用于一体化图像恢复的统一宽度自适应动态网络<br />
**Authors:** Yimin Xu, Nanxi Gao, Zhongyun Shan, Fei Chao, Rongrong Ji<br />
**Abstract:** <details><summary>原文: </summary>In contrast to traditional image restoration methods, all-in-one image restoration techniques are gaining increased attention for their ability to restore images affected by diverse and unknown corruption types and levels. However, contemporary all-in-one image restoration methods omit task-wise difficulties and employ the same networks to reconstruct images afflicted by diverse degradations. This practice leads to an underestimation of the task correlations and suboptimal allocation of computational resources. To elucidate task-wise complexities, we introduce a novel concept positing that intricate image degradation can be represented in terms of elementary degradation. Building upon this foundation, we propose an innovative approach, termed the Unified-Width Adaptive Dynamic Network (U-WADN), consisting of two pivotal components: a Width Adaptive Backbone (WAB) and a Width Selector (WS). The WAB incorporates several nested sub-networks with varying widths, which facilitates the selection of the most apt computations tailored to each task, thereby striking a balance between accuracy and computational efficiency during runtime. For different inputs, the WS automatically selects the most appropriate sub-network width, taking into account both task-specific and sample-specific complexities. Extensive experiments across a variety of image restoration tasks demonstrate that the proposed U-WADN achieves better performance while simultaneously reducing up to 32.3\% of FLOPs and providing approximately 15.7\% real-time acceleration. The code has been made available at \url{https://github.com/xuyimin0926/U-WADN}.</details>
**Abstract_cn:** <details><summary>译文: </summary>与传统的图像恢复方法相比，一体化图像恢复技术因其能够恢复受多种和未知损坏类型和级别影响的图像而受到越来越多的关注。然而，当代的一体化图像恢复方法忽略了任务方面的困难，并采用相同的网络来重建受到不同退化影响的图像。这种做法会导致任务相关性的低估和计算资源的次优分配。为了阐明任务方面的复杂性，我们引入了一个新颖的概念，认为复杂的图像退化可以用基本退化来表示。在此基础上，我们提出了一种创新方法，称为统一宽度自适应动态网络（U-WADN），由两个关键组件组成：宽度自适应骨干网（WAB）和宽度选择器（WS）。 WAB 包含多个宽度不同的嵌套子网络，这有助于选择适合每个任务的最合适的计算，从而在运行时在准确性和计算效率之间取得平衡。对于不同的输入，WS 会自动选择最合适的子网络宽度，同时考虑特定于任务和特定于样本的复杂性。跨各种图像恢复任务的大量实验表明，所提出的 U-WADN 实现了更好的性能，同时减少了高达 32.3% 的 FLOP，并提供了大约 15.7% 的实时加速。该代码已在 \url{https://github.com/xuyimin0926/U-WADN} 中提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.13221v1><br />
**Code:** <https://github.com/xuyimin0926/u-wadn>**<br />
>>**index:** 2<br />
**Title:** **ADMap: Anti-disturbance framework for reconstructing online vectorized HD map**<br />
**Title_cn:** ADMap：重建在线矢量化高精地图的抗干扰框架<br />
**Authors:** Haotian Hu, Fanyi Wang, Yaonong Wang, Laifeng Hu, Jingwei Xu, Zhiwang Zhang<br />
**Abstract:** <details><summary>原文: </summary>In the field of autonomous driving, online high-definition (HD) map reconstruction is crucial for planning tasks. Recent research has developed several high-performance HD map reconstruction models to meet this necessity. However, the point sequences within the instance vectors may be jittery or jagged due to prediction bias, which can impact subsequent tasks. Therefore, this paper proposes the Anti-disturbance Map reconstruction framework (ADMap). To mitigate point-order jitter, the framework consists of three modules: Multi-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point-order relationships between and within instances in a cascading manner, the model can monitor the point-order prediction process more effectively. ADMap achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios. Code and more demos are available at https://github.com/hht1996ok/ADMap.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶领域，在线高清地图重建对于规划任务至关重要。最近的研究开发了几种高性能的高清地图重建模型来满足这种需要。然而，由于预测偏差，实例向量内的点序列可能会出现抖动或锯齿状，这可能会影响后续任务。因此，本文提出抗干扰地图重建框架（ADMap）。为了减轻点阶抖动，该框架由三个模块组成：多尺度感知颈、实例交互注意（IIA）和矢量方向差异损失（VDDL）。通过以级联方式探索实例之间和实例内的点序关系，该模型可以更有效地监控点序预测过程。 ADMap 在 nuScenes 和 Argoverse2 数据集上实现了最先进的性能。大量结果证明了其在复杂多变的驾驶场景中生成稳定可靠的地图元素的能力。代码和更多演示可在 https://github.com/hht1996ok/ADMap 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13172v1><br />
**Code:** <https://github.com/hht1996ok/admap>**<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction**<br />
**Title_cn:** EndoGaussians：用于变形内窥镜组织重建的单视图动态高斯溅射<br />
**Authors:** Yangsen Chen, Hao Wang<br />
**Abstract:** <details><summary>原文: </summary>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.</details>
**Abstract_cn:** <details><summary>译文: </summary>根据内窥镜视频对可变形软体组织进行精确的 3D 重建是 VR 手术和医学图像分析等医疗应用中的关键挑战。现有的方法常常难以准确地理解幻觉组织部分的模糊性，从而限制了它们的实际用途。在这项工作中，我们介绍了 EndoGaussians，这是一种采用高斯分布进行动态内窥镜 3D 重建的新颖方法。该方法标志着高斯分布在这种情况下的首次使用，克服了以前基于 NeRF 的技术的局限性。正如对各种内窥镜数据集的定量评估所证明的那样，我们的方法设定了新的最先进标准。这些进步使我们的方法成为医疗专业人员有前途的工具，为医疗领域的实际应用提供更可靠、更高效的 3D 重建。</details>
**PDF:** <http://arxiv.org/pdf/2401.13352v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects**<br />
**Title_cn:** 具有解耦对象的风格一致的 3D 室内场景合成<br />
**Authors:** Yunfan Zhang, Hong Huang, Zhiwei Xiong, Zhiqi Shen, Guosheng Lin, Hao Wang, Nicholas Vun<br />
**Abstract:** <details><summary>原文: </summary>Controllable 3D indoor scene synthesis stands at the forefront of technological progress, offering various applications like gaming, film, and augmented/virtual reality. The capability to stylize and de-couple objects within these scenarios is a crucial factor, providing an advanced level of control throughout the editing process. This control extends not just to manipulating geometric attributes like translation and scaling but also includes managing appearances, such as stylization. Current methods for scene stylization are limited to applying styles to the entire scene, without the ability to separate and customize individual objects. Addressing the intricacies of this challenge, we introduce a unique pipeline designed for synthesis 3D indoor scenes. Our approach involves strategically placing objects within the scene, utilizing information from professionally designed bounding boxes. Significantly, our pipeline prioritizes maintaining style consistency across multiple objects within the scene, ensuring a cohesive and visually appealing result aligned with the desired aesthetic. The core strength of our pipeline lies in its ability to generate 3D scenes that are not only visually impressive but also exhibit features like photorealism, multi-view consistency, and diversity. These scenes are crafted in response to various natural language prompts, demonstrating the versatility and adaptability of our model.</details>
**Abstract_cn:** <details><summary>译文: </summary>可控 3D 室内场景合成处于技术进步的前沿，提供游戏、电影和增强/虚拟现实等各种应用。在这些场景中对对象进行风格化和解耦的能力是一个关键因素，它可以在整个编辑过程中提供高级的控制。此控件不仅扩展到操纵几何属性（例如平移和缩放），还包括管理外观（例如样式化）。当前的场景风格化方法仅限于将样式应用于整个场景，无法分离和自定义单个对象。为了解决这一挑战的复杂性，我们引入了专为合成 3D 室内场景而设计的独特管道。我们的方法包括利用专业设计的边界框的信息，策略性地将对象放置在场景中。值得注意的是，我们的流程优先考虑保持场景中多个对象的风格一致性，确保具有凝聚力和视觉吸引力的结果与所需的美感保持一致。我们管道的核心优势在于它能够生成 3D 场景，这些场景不仅在视觉上令人印象深刻，而且还表现出照片级真实感、多视图一致性和多样性等特征。这些场景是根据各种自然语言提示精心设计的，展示了我们模型的多功能性和适应性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13203v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond**<br />
**Title_cn:** 用于旋转校正及其他的半监督耦合薄板样条模型<br />
**Authors:** Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br />
**Abstract:** <details><summary>原文: </summary>Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data. It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint. Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction. Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond. The code and data will be available at https://github.com/nie-lang/CoupledTPS.</details>
**Abstract_cn:** <details><summary>译文: </summary>薄板样条 (TPS) 是一种主要扭曲，允许通过控制点运动来表示弹性非线性变换。随着控制点的增加，扭曲变得越来越灵活，但通常会遇到由不良问题（例如内容扭曲）引起的瓶颈。在本文中，我们探讨了 TPS 在基于单图像的变形任务中的通用应用，例如旋转校正、矩形校正和肖像校正。为了打破这一瓶颈，我们提出了耦合薄板样条模型（CoupledTPS），该模型将具有有限控制点的多个TPS迭代耦合成更灵活、更强大的变换。具体来说，我们首先设计一个迭代搜索来根据当前的潜在条件预测新的控制点。然后，我们提出了扭曲流作为不同TPS变换耦合的桥梁，有效消除了多个扭曲引起的插值误差。此外，鉴于繁琐的标注成本，我们开发了一种半监督学习方案，通过利用未标记的数据来提高变形质量。它是通过未标记数据的搜索控制点与其图形增强之间的双重变换来制定的，产生隐式校正一致性约束。最后，我们收集了大量未标记的数据，以展示我们的半监督方案在旋转校正方面的优势。大量实验证明了 CoupledTPS 相对于现有最先进 (SoTA) 旋转校正及其他解决方案的优越性和通用性。代码和数据可在 https://github.com/nie-lang/CoupledTPS 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13432v1><br />
**Code:** <https://github.com/nie-lang/coupledtps>**<br />
>>**index:** 2<br />
**Title:** **Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons**<br />
**Title_cn:** 你们想跳舞吗：多人零镜头组合人类舞蹈生成<br />
**Authors:** Zhe Xu, Kun Wei, Xu Yang, Cheng Deng<br />
**Abstract:** <details><summary>原文: </summary>Human dance generation (HDG) aims to synthesize realistic videos from images and sequences of driving poses. Despite great success, existing methods are limited to generating videos of a single person with specific backgrounds, while the generalizability for real-world scenarios with multiple persons and complex backgrounds remains unclear. To systematically measure the generalizability of HDG models, we introduce a new task, dataset, and evaluation protocol of compositional human dance generation (cHDG). Evaluating the state-of-the-art methods on cHDG, we empirically find that they fail to generalize to real-world scenarios. To tackle the issue, we propose a novel zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos consistent with arbitrary multiple persons and background while precisely following the driving poses. Specifically, in contrast to straightforward DDIM or null-text inversion, we first present a pose-aware inversion method to obtain the noisy latent code and initialization text embeddings, which can accurately reconstruct the composed reference image. Since directly generating videos from them will lead to severe appearance inconsistency, we propose a compositional augmentation strategy to generate augmented images and utilize them to optimize a set of generalizable text embeddings. In addition, consistency-guided sampling is elaborated to encourage the background and keypoints of the estimated clean image at each reverse step to be close to those of the reference image, further improving the temporal consistency of generated videos. Extensive qualitative and quantitative results demonstrate the effectiveness and superiority of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类舞蹈生成（HDG）旨在根据图像和驾驶姿势序列合成逼真的视频。尽管取得了巨大成功，但现有方法仅限于生成具有特定背景的单个人的视频，而对于具有多人和复杂背景的现实场景的通用性仍不清楚。为了系统地衡量 HDG 模型的通用性，我们引入了人类舞蹈生成（cHDG）的新任务、数据集和评估协议。在评估 cHDG 上最先进的方法时，我们凭经验发现它们无法推广到现实世界的场景。为了解决这个问题，我们提出了一种新颖的零镜头框架，称为 MultiDance-Zero，它可以合成与任意多人和背景一致的视频，同时精确遵循驾驶姿势。具体来说，与直接的 DDIM 或空文本反演相比，我们首先提出一种姿势感知反演方法来获取噪声潜在代码和初始化文本嵌入，它可以准确地重建合成的参考图像。由于直接从它们生成视频将导致严重的外观不一致，因此我们提出了一种合成增强策略来生成增强图像并利用它们来优化一组可概括的文本嵌入。此外，还精心设计了一致性引导采样，以鼓励每个反向步骤中估计的干净图像的背景和关键点接近参考图像的背景和关键点，进一步提高生成视频的时间一致性。广泛的定性和定量结果证明了我们方法的有效性和优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13363v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **FLLIC: Functionally Lossless Image Compression**<br />
**Title_cn:** FLLIC：功能无损图像压缩<br />
**Authors:** Xi Zhang, Xiaolin Wu<br />
**Abstract:** <details><summary>原文: </summary>Recently, DNN models for lossless image coding have surpassed their traditional counterparts in compression performance, reducing the bit rate by about ten percent for natural color images. But even with these advances, mathematically lossless image compression (MLLIC) ratios for natural images still fall short of the bandwidth and cost-effectiveness requirements of most practical imaging and vision systems at present and beyond. To break the bottleneck of MLLIC in compression performance, we question the necessity of MLLIC, as almost all digital sensors inherently introduce acquisition noises, making mathematically lossless compression counterproductive. Therefore, in contrast to MLLIC, we propose a new paradigm of joint denoising and compression called functionally lossless image compression (FLLIC), which performs lossless compression of optimally denoised images (the optimality may be task-specific). Although not literally lossless with respect to the noisy input, FLLIC aims to achieve the best possible reconstruction of the latent noise-free original image. Extensive experiments show that FLLIC achieves state-of-the-art performance in joint denoising and compression of noisy images and does so at a lower computational cost.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，用于无损图像编码的 DNN 模型在压缩性能方面超越了传统模型，将自然彩色图像的比特率降低了约 10%。但即使有了这些进步，自然图像的数学无损图像压缩 (MLLIC) 比率仍然达不到当前及未来大多数实用成像和视觉系统的带宽和成本效益要求。为了打破 MLLIC 在压缩性能方面的瓶颈，我们质疑 MLLIC 的必要性，因为几乎所有数字传感器都会固有地引入采集噪声，使得数学上的无损压缩适得其反。因此，与 MLLIC 相比，我们提出了一种称为功能无损图像压缩（FLLIC）的联合去噪和压缩的新范例，它对最佳去噪图像执行无损压缩（最佳性可能是特定于任务的）。尽管相对于噪声输入来说并不是真正意义上的无损，但 FLLIC 的目标是实现潜在无噪声原始图像的最佳重建。大量实验表明，FLLIC 在噪声图像的联合去噪和压缩方面实现了最先进的性能，并且计算成本较低。</details>
**PDF:** <http://arxiv.org/pdf/2401.13616v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry**<br />
**Title_cn:** 基于仅位姿成像几何的线性相对位姿估计<br />
**Authors:** Qi Cai, Xinrui Li, Yuanxin Wu<br />
**Abstract:** <details><summary>原文: </summary>How to efficiently and accurately handle image matching outliers is a critical issue in two-view relative estimation. The prevailing RANSAC method necessitates that the minimal point pairs be inliers. This paper introduces a linear relative pose estimation algorithm for n $( n \geq 6$) point pairs, which is founded on the recent pose-only imaging geometry to filter out outliers by proper reweighting. The proposed algorithm is able to handle planar degenerate scenes, and enhance robustness and accuracy in the presence of a substantial ratio of outliers. Specifically, we embed the linear global translation (LiGT) constraint into the strategies of iteratively reweighted least-squares (IRLS) and RANSAC so as to realize robust outlier removal. Simulations and real tests of the Strecha dataset show that the proposed algorithm achieves relative rotation accuracy improvement of 2 $\sim$ 10 times in face of as large as 80% outliers.</details>
**Abstract_cn:** <details><summary>译文: </summary>如何高效、准确地处理图像匹配异常值是二视图相对估计中的关键问题。流行的 RANSAC 方法要求最小点对必须是内点。本文介绍了一种针对 n $( n \geq 6$) 点对的线性相对位姿估计算法，该算法基于最近的仅位姿成像几何结构，通过适当的重新加权来过滤异常值。所提出的算法能够处理平面退化场景，并在存在大量异常值的情况下增强鲁棒性和准确性。具体来说，我们将线性全局平移（LiGT）约束嵌入到迭代重加权最小二乘（IRLS）和RANSAC策略中，以实现鲁棒的异常值去除。 Strecha数据集的仿真和真实测试表明，该算法在面对高达80%的异常值时，相对旋转精度提高了2 $\sim$ 10倍。</details>
**PDF:** <http://arxiv.org/pdf/2401.13357v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Visual Objectification in Films: Towards a New AI Task for Video Interpretation**<br />
**Title_cn:** 电影中的视觉对象化：迈向视频解读的新人工智能任务<br />
**Authors:** Julie Tores, Lucile Sassatelli, Hui-Yin Wu, Clement Bergman, Lea Andolfi, Victor Ecrement, Frederic Precioso, Thierry Devars, Magali Guaresi, Virginie Julliard, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.</details>
**Abstract_cn:** <details><summary>译文: </summary>在电影性别研究中，“男性凝视”的概念是指角色在银幕上被描绘成欲望的对象而不是主体的方式。在本文中，我们介绍了一种新颖的视频解释任务，用于检测电影中的角色客观化。目的是揭示和量化电影中复杂时间模式的使用，以产生客观化的认知感知。我们介绍 ObyGaze12 数据集，该数据集由 1914 年的电影剪辑组成，由专家针对电影研究和心理学中确定的客观化概念进行了密集注释。我们评估最近的视觉模型，展示任务的可行性以及概念瓶颈模型仍然存在的挑战。我们的新数据集和代码已向社区开放。</details>
**PDF:** <http://arxiv.org/pdf/2401.13296v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics**<br />
**Title_cn:** 利用音频场景语义注入音频的自动图像着色<br />
**Authors:** Pengcheng Zhao, Yanxiang Chen, Yang Zhao, Wei Jia, Zhao Zhang, Ronggang Wang, Richang Hong<br />
**Abstract:** <details><summary>原文: </summary>Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representation is fed into the pretrained network to finally realize the audio-guided colorization. The whole process is trained in a self-supervised manner without human annotation. In addition, an audiovisual colorization dataset is established for training and testing. Experiments demonstrate that audio guidance can effectively improve the performance of automatic colorization, especially for some scenes that are difficult to understand only from visual modality.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动图像着色本质上是一个具有不确定性的病态问题，需要对场景进行准确的语义理解来估计灰度图像的合理颜色。尽管最近基于交互的方法已经取得了令人印象深刻的性能，但为自动着色推断真实且准确的颜色仍然是一项非常困难的任务。为了降低灰度场景语义理解的难度，本文尝试利用相应的音频，它自然包含同一场景的额外语义信息。具体来说，提出了一种新颖的音频注入自动图像着色（AIAIC）网络，它由三个阶段组成。首先，我们以彩色图像语义为桥梁，预训练由彩色图像语义引导的彩色化网络。其次，利用音频和视频的自然共现来学习音频和视觉场景之间的颜色语义相关性。第三，将隐式音频语义表示输入预训练网络，最终实现音频引导着色。整个过程以自我监督的方式进行训练，无需人工注释。此外，还建立了视听着色数据集用于训练和测试。实验表明，音频引导可以有效提高自动着色的性能，特别是对于一些仅从视觉模态难以理解的场景。</details>
**PDF:** <http://arxiv.org/pdf/2401.13270v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Dual-modal Dynamic Traceback Learning for Medical Report Generation**<br />
**Title_cn:** 用于生成医疗报告的双模态动态回溯学习<br />
**Authors:** Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Jinman Kim<br />
**Abstract:** <details><summary>原文: </summary>With increasing reliance on medical imaging in clinical practices, automated report generation from medical images is in great demand. Existing report generation methods typically adopt an encoder-decoder deep learning framework to build a uni-directional image-to-report mapping. However, such a framework ignores the bi-directional mutual associations between images and reports, thus incurring difficulties in associating the intrinsic medical meanings between them. Recent generative representation learning methods have demonstrated the benefits of dual-modal learning from both image and text modalities. However, these methods exhibit two major drawbacks for medical report generation: 1) they tend to capture morphological information and have difficulties in capturing subtle pathological semantic information, and 2) they predict masked text rely on both unmasked images and text, inevitably degrading performance when inference is based solely on images. In this study, we propose a new report generation framework with dual-modal dynamic traceback learning (DTrace) to overcome the two identified drawbacks and enable dual-modal learning for medical report generation. To achieve this, our DTrace introduces a traceback mechanism to control the semantic validity of generated content via self-assessment. Further, our DTrace introduces a dynamic learning strategy to adapt to various proportions of image and text input, enabling report generation without reliance on textual input during inference. Extensive experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that our DTrace outperforms state-of-the-art medical report generation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着临床实践中对医学成像的依赖日益增加，对从医学图像自动生成报告的需求很大。现有的报告生成方法通常采用编码器-解码器深度学习框架来构建单向图像到报告的映射。然而，这样的框架忽略了图像和报告之间的双向相互关联，因此在关联它们之间的内在医学含义时遇到了困难。最近的生成表示学习方法已经证明了图像和文本模态双模态学习的好处。然而，这些方法在医学报告生成方面存在两个主要缺点：1）它们倾向于捕获形态信息，但难以捕获微妙的病理语义信息；2）它们预测屏蔽文本依赖于未屏蔽的图像和文本，不可避免地会降低性能。推理仅基于图像。在本研究中，我们提出了一种具有双模态动态回溯学习（DTrace）的新报告生成框架，以克服两个已发现的缺点，并实现用于医疗报告生成的双模态学习。为了实现这一目标，我们的 DTrace 引入了追溯机制，通过自我评估来控制生成内容的语义有效性。此外，我们的 DTrace 引入了动态学习策略来适应各种比例的图像和文本输入，从而在推理过程中无需依赖文本输入即可生成报告。对两个基准良好的数据集（IU-Xray 和 MIMIC-CXR）进行的大量实验表明，我们的 DTrace 优于最先进的医疗报告生成方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.13267v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Predicting Mitral Valve mTEER Surgery Outcomes Using Machine Learning and Deep Learning Techniques**<br />
**Title_cn:** 使用机器学习和深度学习技术预测二尖瓣 mTEER 手术结果<br />
**Authors:** Tejas Vyas, Mohsena Chowdhury, Xiaojiao Xiao, Mathias Claeys, Géraldine Ong, Guanghui Wang<br />
**Abstract:** <details><summary>原文: </summary>Mitral Transcatheter Edge-to-Edge Repair (mTEER) is a medical procedure utilized for the treatment of mitral valve disorders. However, predicting the outcome of the procedure poses a significant challenge. This paper makes the first attempt to harness classical machine learning (ML) and deep learning (DL) techniques for predicting mitral valve mTEER surgery outcomes. To achieve this, we compiled a dataset from 467 patients, encompassing labeled echocardiogram videos and patient reports containing Transesophageal Echocardiography (TEE) measurements detailing Mitral Valve Repair (MVR) treatment outcomes. Leveraging this dataset, we conducted a benchmark evaluation of six ML algorithms and two DL models. The results underscore the potential of ML and DL in predicting mTEER surgery outcomes, providing insight for future investigation and advancements in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>二尖瓣经导管边缘对边缘修复术 (mTEER) 是一种用于治疗二尖瓣疾病的医疗手术。然而，预测手术结果提出了重大挑战。本文首次尝试利用经典机器学习（ML）和深度学习（DL）技术来预测二尖瓣 mTEER 手术结果。为了实现这一目标，我们编制了 467 名患者的数据集，其中包括标记的超声心动图视频和患者报告，其中包含详细说明二尖瓣修复 (MVR) 治疗结果的经食管超声心动图 (TEE) 测量结果。利用该数据集，我们对六种机器学习算法和两种深度学习模型进行了基准评估。结果强调了 ML 和 DL 在预测 mTEER 手术结果方面的潜力，为该领域的未来研究和进展提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2401.13197v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing Algorithm**<br />
**Title_cn:** 一种广义的基于多尺度束的高光谱稀疏解混算法<br />
**Authors:** Luciano Carvalho Ayres, Ricardo Augusto Borsoi, José Carlos Moreira Bermudez, Sérgio José Melo de Almeida<br />
**Abstract:** <details><summary>原文: </summary>In hyperspectral sparse unmixing, a successful approach employs spectral bundles to address the variability of the endmembers in the spatial domain. However, the regularization penalties usually employed aggregate substantial computational complexity, and the solutions are very noise-sensitive. We generalize a multiscale spatial regularization approach to solve the unmixing problem by incorporating group sparsity-inducing mixed norms. Then, we propose a noise-robust method that can take advantage of the bundle structure to deal with endmember variability while ensuring inter- and intra-class sparsity in abundance estimation with reasonable computational cost. We also present a general heuristic to select the \emph{most representative} abundance estimation over multiple runs of the unmixing process, yielding a solution that is robust and highly reproducible. Experiments illustrate the robustness and consistency of the results when compared to related methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>在高光谱稀疏分解中，一种成功的方法采用光谱束来解决空间域中端元的可变性。然而，通常采用的正则化惩罚会聚集大量的计算复杂性，并且解决方案对噪声非常敏感。我们推广了一种多尺度空间正则化方法，通过结合群体稀疏性混合范数来解决混合问题。然后，我们提出了一种抗噪声方法，该方法可以利用束结构来处理端元变异性，同时以合理的计算成本确保丰度估计中的类间和类内稀疏性。我们还提出了一种通用启发式方法，用于在多次运行的分离过程中选择 \emph{最具代表性} 丰度估计，从而产生稳健且高度可重复的解决方案。实验证明了与相关方法相比结果的稳健性和一致性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13161v1><br />
**Code:** <https://github.com/lucayress/gmbua>**<br />

