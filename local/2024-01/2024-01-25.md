## [UPDATED!] **2024-01-25** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities**<br />
**Title_cn:** 多模态路径：利用其他模态的不相关数据改进 Transformer<br />
**Authors:** Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue<br />
**Abstract:** <details><summary>原文: </summary>We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们建议使用来自其他模态的不相关数据来改进特定模态的转换器，例如，使用音频或点云数据集改进 ImageNet 模型。我们想强调的是，目标模态的数据样本与其他模态无关，这将我们的方法与利用不同模态的配对（例如 CLIP）或交错数据的其他工作区分开来。我们提出了一种名为多模态路径的方法 - 给定目标模态和为其设计的变压器，我们使用用另一种模态的数据训练的辅助变压器并构建路径来连接两个模型的组件，以便可以处理目标模态的数据两种型号。通过这种方式，我们利用从两种模态获得的转换器的通用序列到序列建模能力。作为具体实现，我们像往常一样使用特定于模态的分词器和特定于任务的头，但通过提出的名为“跨模态重新参数化”的方法利用辅助模型的转换器块，该方法在没有任何推理成本的情况下利用辅助权重。在图像、点云、视频和音频识别任务中，我们观察到来自其他模式的不相关数据的显着且一致的性能改进。代码和模型可在 https://github.com/AILab-CVC/M2PT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.14405v1><br />
**Code:** <https://github.com/ailab-cvc/m2pt>**<br />
>>**index:** 2<br />
**Title:** **pix2gestalt: Amodal Segmentation by Synthesizing Wholes**<br />
**Title_cn:** pix2gestalt：通过综合整体进行无模态分割<br />
**Authors:** Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick<br />
**Abstract:** <details><summary>原文: </summary>We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 pix2gestalt，这是一种零样本非模态分割框架，它学习估计在遮挡后仅部分可见的整个对象的形状和外观。通过利用大规模扩散模型并将其表示转移到此任务中，我们学习了一种条件扩散模型，用于在具有挑战性的零样本情况下重建整个对象，包括打破自然和物理先验的示例，例如艺术。作为训练数据，我们使用综合整理的数据集，其中包含与其整个对应对象配对的遮挡对象。实验表明，我们的方法在既定基准上优于监督基线。此外，我们的模型还可用于在存在遮挡的情况下显着提高现有对象识别和 3D 重建方法的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14398v1><br />
**Code:** <https://github.com/cvlab-columbia/pix2gestalt>**<br />
>>**index:** 3<br />
**Title:** **Rethinking Patch Dependence for Masked Autoencoders**<br />
**Title_cn:** 重新思考屏蔽自动编码器的补丁依赖性<br />
**Authors:** Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei A. Efros, Ken Goldberg<br />
**Abstract:** <details><summary>原文: </summary>In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们重新检查了掩码自动编码器（MAE）解码机制中的补丁间依赖性。我们将 MAE 中掩模补丁重建的解码机制分解为自注意力和交叉注意力。我们的调查表明，掩模补丁之间的自注意力对于学习良好的表示并不是必需的。为此，我们提出了一种新颖的预训练框架：交叉注意力屏蔽自动编码器（CrossMAE）。 CrossMAE 的解码器仅利用屏蔽标记和可见标记之间的交叉注意力，不会降低下游性能。这种设计还可以仅解码掩码令牌的一小部分，从而提高效率。此外，每个解码器块现在可以利用不同的编码器功能，从而改进表示学习。 CrossMAE 的性能与 MAE 相当，但解码计算量减少了 2.5 至 3.7 美元\倍$。在相同计算下，它在 ImageNet 分类和 COCO 实例分割方面也超过了 MAE。代码和模型：https://crossmae.github.io</details>
**PDF:** <http://arxiv.org/pdf/2401.14391v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label Pairs**<br />
**Title_cn:** 不一致掩码：消除输入伪标签对的不确定性<br />
**Authors:** Michael R. H. Vorndran, Bernhard F. Roeck<br />
**Abstract:** <details><summary>原文: </summary>Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks. Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models. We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques. By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data. Notably, three of our hybrid models outperform those trained on the fully labeled dataset. Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques. For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions. The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks</details>
**Abstract_cn:** <details><summary>译文: </summary>生成足够的标记数据是有效执行深度学习项目的一个重大障碍，特别是在图像分割的未知领域，与分类任务不同，标记需要大量时间。我们的研究面临着这一挑战，在受硬件资源有限且缺乏广泛数据集或预训练模型限制的环境中运行。我们引入了不一致掩模（IM）的新颖用途，可以有效过滤图像伪标签对中的不确定性，从而大大提高分割质量，超越传统的半监督学习技术。通过将 IM 与其他方法集成，我们在 ISIC 2018 数据集上展示了出色的二元分割性能，从仅 10% 的标记数据开始。值得注意的是，我们的三个混合模型优于在完全标记数据集上训练的模型。我们的方法在另外三个数据集上始终取得了优异的结果，并且与其他技术相结合时显示出进一步的改进。为了进行全面和稳健的评估，本文对流行的半监督学习策略进行了广泛分析，所有这些策略都在相同的起始条件下进行训练。完整代码位于：https://github.com/MichaelVorndran/InconsistencyMasks</details>
**PDF:** <http://arxiv.org/pdf/2401.14387v1><br />
**Code:** <https://github.com/michaelvorndran/inconsistencymasks>**<br />
>>**index:** 5<br />
**Title:** **UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models**<br />
**Title_cn:** UrbanGenAI：使用全景分割和扩散模型重建城市景观<br />
**Authors:** Timo Kapsalis<br />
**Abstract:** <details><summary>原文: </summary>In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design</details>
**Abstract_cn:** <details><summary>译文: </summary>在当代设计实践中，计算机视觉和生成人工智能（genAI）的集成代表了向更具交互性和包容性流程的变革性转变。这些技术提供了图像分析和生成的新维度，这在城市景观重建的背景下尤其重要。本文提出了一种封装在原型应用程序中的新颖工作流程，旨在利用先进图像分割和扩散模型之间的协同作用来实现城市设计的综合方法。我们的方法包括用于详细图像分割的 OneFormer 模型和通过 ControlNet 实现的稳定扩散 XL (SDXL) 扩散模型，用于从文本描述生成图像。验证结果表明原型应用程序具有很高的性能，在对象检测和文本到图像生成方面都显示出极高的准确性。对各类城市景观特征的迭代评估中优异的交集比并集 (IoU) 和 CLIP 分数证明了这一点。初步测试包括利用 UrbanGenAI 作为一种教育工具，增强设计教学法的学习体验，并作为一种参与性工具，促进社区驱动的城市规划。早期结果表明，UrbanGenAI 不仅推进了城市景观重建的技术前沿，而且还提供了显着的教学和参与性规划效益。 UrbanGenAI 的持续开发旨在进一步验证其在更广泛的背景下的有效性，并集成实时反馈机制和 3D 建模功能等附加功能。关键词：生成式人工智能；全景图像分割；扩散模型；城市景观设计；设计教学法；协同设计</details>
**PDF:** <http://arxiv.org/pdf/2401.14379v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition**<br />
**Title_cn:** 用于细粒度车辆识别的渐进式多任务抗噪声学习和蒸馏框架<br />
**Authors:** Dichao Liu<br />
**Abstract:** <details><summary>原文: </summary>Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network. Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks. The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR</details>
**Abstract_cn:** <details><summary>译文: </summary>细粒度车辆识别（FGVR）是智能交通系统必不可少的基础技术，但由于其固有的类内变异性，其难度非常大。以往的FGVR研究大多只关注不同拍摄角度、位置等引起的类内变异，而图像噪声引起的类内变异很少受到关注。本文提出了渐进式多任务抗噪声学习（PMAL）框架和渐进式多任务蒸馏（PMD）框架来解决FGVR中由于图像噪声而导致的类内变异问题。 PMAL 框架通过将图像去噪视为图像识别中的附加任务并逐步迫使模型学习噪声不变性来实现高识别精度。 PMD 框架将 PMAL 训练模型的知识转移到原始骨干网络中，生成的模型与 PMAL 训练模型的识别精度大致相同，但与原始骨干网络相比没有任何额外开销。结合这两个框架，我们获得的模型在两个广泛使用的标准 FGVR 数据集（即斯坦福汽车和 CompCars）以及另外三个基于监控图像的车辆上的识别精度显着超过了以前最先进的方法类型分类数据集，即北京理工大学 (BIT)-车辆、车辆类型图像数据 2 (VTID2) 和用于模型识别的车辆图像数据集 (VIDMMR)，而不会对原始骨干网络产生任何额外开销。源代码位于 https://github.com/Dichao-Liu/Anti-noise_FGVR</details>
**PDF:** <http://arxiv.org/pdf/2401.14336v1><br />
**Code:** <https://github.com/dichao-liu/anti-noise_fgvr>**<br />
>>**index:** 7<br />
**Title:** **Unlocking Past Information: Temporal Embeddings in Cooperative Bird's Eye View Prediction**<br />
**Title_cn:** 解锁过去的信息：合作鸟瞰预测中的时间嵌入<br />
**Authors:** Dominik Rößle, Jeremias Gerner, Klaus Bogenberger, Daniel Cremers, Stefanie Schmidtner, Torsten Schön<br />
**Abstract:** <details><summary>原文: </summary>Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确、全面的鸟瞰图 (BEV) 语义分割对于确保自动驾驶中的安全和主动导航至关重要。尽管协作感知已经超过了单智能体系统的检测能力，但协作感知中流行的基于相机的算法忽略了从历史观察中获得的有价值的信息。在传感器故障或通信问题期间，随着协作感知恢复为单代理感知，这种限制变得至关重要，导致性能下降和 BEV 分割图不完整。本文介绍了 TempCoBEV，这是一个时间模块，旨在将历史线索融入当前观测中，从而提高 BEV 地图分割的质量和可靠性。我们提出了一种重要性引导的注意力架构来有效地整合时间信息，优先考虑 BEV 地图分割的相关属性。 TempCoBEV 是一个独立的时间模块，可无缝集成到最先进的基于相机的协作感知模型中。我们通过对 OPV2V 数据集进行大量实验证明，TempCoBEV 在预测当前和未来的 BEV 地图分割方面比非时间模型表现更好，特别是在涉及通信故障的场景中。我们展示了 TempCoBEV 的功效及其将历史线索集成到当前 BEV 地图中的能力，将最佳通信条件下的预测提高了高达 2%，将通信故障情况下的预测提高了高达 19%。代码将发布在 GitHub 上。</details>
**PDF:** <http://arxiv.org/pdf/2401.14325v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Producing Plankton Classifiers that are Robust to Dataset Shift**<br />
**Title_cn:** 生成对数据集转换具有鲁棒性的浮游生物分类器<br />
**Authors:** Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi<br />
**Abstract:** <details><summary>原文: </summary>Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD degradation compared to nominal test performance, (ii) conducting a diagnostic analysis of degradation causes, and (iii) providing solutions. We find that ensembles of BEiT vision transformers, with targeted augmentations addressing OOD robustness, geometric ensembling, and rotation-based test-time augmentation, constitute the most robust model, which we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated on container classes. Moreover, it exhibits lower sensitivity to dataset shift, and reproduces well the plankton abundances. Our proposed pipeline is applicable to generic plankton classifiers, contingent on the availability of suitable test cells. By identifying critical shortcomings and offering practical procedures to fortify models against dataset shift, our study contributes to the development of more reliable plankton classification technologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>现代浮游生物高通量监测依赖于深度学习分类器来识别水生态系统中的物种。尽管标称性能令人满意，但数据集移位带来了重大挑战，这会导致部署期间性能下降。在我们的研究中，我们将 ZooLake 数据集与 10 个独立部署日的手动注释图像集成在一起，作为测试数据集外 (OOD) 性能的测试单元。我们的分析揭示了分类器最初在数据集中条件下表现良好，但在实际场景中遇到显着失败的情况。例如，标称测试准确度为 92% 的 MobileNet 显示出 77% OOD 准确度。我们系统地调查导致 OOD 性能下降的条件，并提出一种先发制人的评估方法，以识别对新数据进行分类时的潜在陷阱，并查明 OOD 图像中对分类产生不利影响的特征。我们提出了一个三步流程：(i) 与标称测试性能相比识别 OOD 退化，(ii) 对退化原因进行诊断分析，以及 (iii) 提供解决方案。我们发现 BEiT 视觉变换器的集合，具有针对 OOD 鲁棒性的有针对性的增强、几何集成和基于旋转的测试时间增强，构成了最鲁棒的模型，我们将其称为 BEsT 模型。它实现了 83% 的 OOD 准确率，错误集中在容器类上。此外，它对数据集变化的敏感性较低，并且可以很好地再现浮游生物丰度。我们提出的管道适用于通用浮游生物分类器，具体取决于合适的测试单元的可用性。通过识别关键缺陷并提供实用程序来强化模型以应对数据集变化，我们的研究有助于开发更可靠的浮游生物分类技术。</details>
**PDF:** <http://arxiv.org/pdf/2401.14256v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **On generalisability of segment anything model for nuclear instance segmentation in histology images**<br />
**Title_cn:** 组织学图像中核实例分割的分段任意模型的通用性<br />
**Authors:** Kesi Xu, Lea Goetz, Nasir Rajpoot<br />
**Abstract:** <details><summary>原文: </summary>Pre-trained on a large and diverse dataset, the segment anything model (SAM) is the first promptable foundation model in computer vision aiming at object segmentation tasks. In this work, we evaluate SAM for the task of nuclear instance segmentation performance with zero-shot learning and finetuning. We compare SAM with other representative methods in nuclear instance segmentation, especially in the context of model generalisability. To achieve automatic nuclear instance segmentation, we propose using a nuclei detection model to provide bounding boxes or central points of nu-clei as visual prompts for SAM in generating nuclear instance masks from histology images.</details>
**Abstract_cn:** <details><summary>译文: </summary>分段任何模型（SAM）在大型且多样化的数据集上进行了预训练，是计算机视觉中第一个针对对象分割任务的可提示基础模型。在这项工作中，我们通过零样本学习和微调来评估 SAM 在核实例分割任务中的性能。我们将 SAM 与核实例分割中的其他代表性方法进行比较，特别是在模型通用性方面。为了实现自动核实例分割，我们建议使用核检测模型来提供核的边界框或中心点，作为 SAM 从组织学图像生成核实例掩模时的视觉提示。</details>
**PDF:** <http://arxiv.org/pdf/2401.14248v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification**<br />
**Title_cn:** 探索未探索的事物：了解图层调整对图像分类的影响<br />
**Authors:** Haixia Liu, Tim Brailsford, James Goulding, Gavin Smith, Larry Bull<br />
**Abstract:** <details><summary>原文: </summary>This paper investigates how adjustments to deep learning architectures impact model performance in image classification. Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset. Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results. The choice and order of layers as well as filter placement significantly impact model performance. This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了深度学习架构的调整如何影响图像分类中的模型性能。尽管观察到的趋势与整个数据集不一致，但小规模实验产生了初步见解。图像处理管道中的过滤操作至关重要，在预处理之前进行图像过滤可以产生更好的结果。层的选择和顺序以及过滤器的放置会显着影响模型的性能。这项研究为优化深度学习模型提供了宝贵的见解，并为未来研究提供了潜在的途径，包括协作平台。</details>
**PDF:** <http://arxiv.org/pdf/2401.14236v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study**<br />
**Title_cn:** 人工智能临床黑色素瘤诊断：前瞻性多中心研究的见解<br />
**Authors:** Lukas Heinlein, Roman C. Maron, Achim Hekler, Sarah Haggenmüller, Christoph Wies, Jochen S. Utikal, Friedegund Meier, Sarah Hobelsberger, Frank F. Gellrich, Mildred Sergon, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Early detection of melanoma, a potentially lethal type of skin cancer with high prevalence worldwide, improves patient prognosis. In retrospective studies, artificial intelligence (AI) has proven to be helpful for enhancing melanoma detection. However, there are few prospective studies confirming these promising results. Existing studies are limited by low sample sizes, too homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing a fair and thorough evaluation of AI and its generalizability, a crucial aspect for its application in the clinical setting. Therefore, we assessed 'All Data are Ext' (ADAE), an established open-source ensemble algorithm for detecting melanomas, by comparing its diagnostic accuracy to that of dermatologists on a prospectively collected, external, heterogeneous test set comprising eight distinct hospitals, four different camera setups, rare melanoma subtypes, and special anatomical sites. We advanced the algorithm with real test-time augmentation (R-TTA, i.e. providing real photographs of lesions taken from multiple angles and averaging the predictions), and evaluated its generalization capabilities. Overall, the AI showed higher balanced accuracy than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781, 95% CI 0.760-0.802; p<0.001), obtaining a higher sensitivity (0.921, 95% CI 0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p<0.001) at the cost of a lower specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p<0.001). As the algorithm exhibited a significant performance advantage on our heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI may offer the potential to support dermatologists particularly in diagnosing challenging cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>黑色素瘤是一种潜在致命的皮肤癌，在全球范围内发病率很高，早期发现可以改善患者的预后。回顾性研究表明，人工智能 (AI) 已被证明有助于增强黑色素瘤检测。然而，很少有前瞻性研究证实这些有希望的结果。现有的研究受到样本量小、数据集过于同质或缺乏罕见黑色素瘤亚型的限制，阻碍了对人工智能及其普遍性的公平和彻底的评估，而这是人工智能在临床环境中应用的一个关键方面。因此，我们评估了“All Data are Ext”（ADAE），这是一种用于检测黑色素瘤的已建立的开源集成算法，通过将其诊断准确性与皮肤科医生在前瞻性收集的外部异质测试集上的诊断准确性进行比较，该测试集包括八家不同的医院、四家医院不同的相机设置、罕见的黑色素瘤亚型和特殊的解剖部位。我们通过实时测试时间增强（R-TTA，即提供从多个角度拍摄的病变的真实照片并对预测进行平均）来改进该算法，并评估其泛化能力。总体而言，AI 显示出比皮肤科医生更高的平衡准确性（0.798，95% CI 0.779-0.814 vs. 0.781，95% CI 0.760-0.802；p<0.001），获得更高的灵敏度（0.921，95% CI 0.900） - 0.942 vs. 0.734，95% CI 0.701-0.770；p<0.001），但特异性较低（0.673，95% CI 0.641-0.702 vs. 0.828，95% CI 0.804-0.852；p<0.001）。由于该算法在仅包含黑色素瘤可疑病变的异构数据集上表现出显着的性能优势，因此人工智能可能为皮肤科医生提供支持，特别是在诊断具有挑战性的病例方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.14193v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Vivim: a Video Vision Mamba for Medical Video Object Segmentation**<br />
**Title_cn:** Vivim：用于医疗视频对象分割的视频视觉 Mamba<br />
**Authors:** Yijun Yang, Zhaohu Xing, Lei Zhu<br />
**Abstract:** <details><summary>原文: </summary>Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks. Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks. To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block. Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance. Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim. The code for Vivim is available at: https://github.com/scott-yjyang/Vivim.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的卷积神经网络的感受野有限，而从计算复杂度的角度来看，基于变压器的网络在构建长期依赖关系方面表现平平。在视频分析任务中处理长视频序列时，这种瓶颈提出了重大挑战。最近，以 Mamba 着称的具有高效硬件感知设计的状态空间模型（SSM）在长序列建模方面取得了令人瞩目的成就，这促进了深度神经网络在许多视觉任务上的发展。为了更好地捕获视频帧中的可用线索，本文提出了一种基于视频视觉 Mamba 的通用框架，用于医疗视频对象分割任务，名为 Vivim。我们的 Vivim 可以通过我们设计的时空曼巴块有效地将长期时空表示压缩为不同尺度的序列。与现有的基于 Transformer 的视频级方法相比，我们的模型保持了出色的分割结果和更好的速度性能。对美国乳房数据集的广泛实验证明了我们 Vivim 的有效性和效率。 Vivim 的代码位于：https://github.com/scott-yjyang/Vivim。</details>
**PDF:** <http://arxiv.org/pdf/2401.14168v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks**<br />
**Title_cn:** 扎根 SAM：为各种视觉任务组装开放世界模型<br />
**Authors:** Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 Grounded SAM，它使用 Grounding DINO 作为开放集目标检测器，与分段任何模型 (SAM) 相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了一扇门。如图 1 所示，通过使用多功能接地 SAM 管道可以实现广泛的视觉任务。例如，仅基于输入图像的自动注释管道可以通过合并 BLIP 和 Recognize Anything 等模型来实现。此外，结合稳定扩散可以实现可控的图像编辑，而 OSX 的集成则有助于快速进行 3D 人体运动分析。 Grounded SAM 在开放词汇基准测试中也表现出了卓越的性能，结合 Grounding DINO-Base 和 SAM-Huge 模型，在 SegInW（野外分割）零样本基准测试中实现了 48.7 的平均 AP。</details>
**PDF:** <http://arxiv.org/pdf/2401.14159v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Expression-aware video inpainting for HMD removal in XR applications**<br />
**Title_cn:** 用于在 XR 应用程序中移除 HMD 的表情感知视频修复<br />
**Authors:** Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr<br />
**Abstract:** <details><summary>原文: </summary>Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content. However, HMDs present an obstacle to external recording techniques as they block the upper face of the user. This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience. In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs). Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user. The framework and its components ensure the preservation of the user's identity across frames using the reference frame. To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation. Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity. Moreover, the outputs exhibit temporal consistency along the inpainted frames. This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware.</details>
**Abstract_cn:** <details><summary>译文: </summary>头戴式显示器 (HMD) 是观察扩展现实 (XR) 环境和虚拟内容不可或缺的设备。然而，头戴式显示器给外部记录技术带来了障碍，因为它们挡住了用户的上脸。这种限制极大地影响了社交 XR 应用程序，特别是电话会议，其中面部特征和眼睛注视信息在创建沉浸式用户体验方面发挥着至关重要的作用。在这项研究中，我们提出了一种基于生成对抗网络（GAN）的用于去除 HMD 的表达感知视频修复的新网络（EVI-HRnet）。我们的模型有效地填充了有关面部标志和用户的单个无遮挡参考图像的缺失信息。该框架及其组件确保使用参考框架跨框架保存用户的身份。为了进一步提高修复输出的真实感水平，我们引入了一种新颖的面部表情识别（FER）损失函数来保存情感。我们的结果证明了所提出的框架具有从面部视频中去除头戴式显示器的卓越能力，同时保持主体的面部表情和身份。此外，输出沿着修复帧表现出时间一致性。这个轻量级框架提供了一种去除 HMD 遮挡的实用方法，具有增强各种协作 XR 应用程序的潜力，而无需额外的硬件。</details>
**PDF:** <http://arxiv.org/pdf/2401.14136v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease**<br />
**Title_cn:** 基于注意力的阿尔茨海默病 3D MRI 图像高效分类<br />
**Authors:** Yihao Lin, Ximeng Li, Yan Zhang, Jinshan Tang<br />
**Abstract:** <details><summary>原文: </summary>Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>阿尔茨海默病诊断（AD）的早期诊断因其微妙而复杂的临床症状而成为一项具有挑战性的任务。利用图像识别技术的深度学习辅助医学诊断已成为该领域的重要研究课题。这些特征必须准确捕捉大脑解剖结构的主要变化。然而，通过深度学习训练进行特征提取非常耗时且昂贵。本研究提出了一种基于卷积神经网络的新型阿尔茨海默病检测模型。该模型利用预先训练的 ResNet 网络作为主干，结合了 3D 医学图像的后融合算法和注意力机制。实验结果表明，所采用的二维融合算法有效地提高了模型的训练开销。并且引入的注意力机制准确地对图像中的重要区域进行加权，进一步提高了模型的诊断准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14130v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver Activity Recognition**<br />
**Title_cn:** MIFI：用于 Roust 3D 分心驾驶员活动识别的多摄像头功能集成<br />
**Authors:** Jian Kuang, Wenjing Li, Fang Li, Jun Zhang, Zhongcheng Wu<br />
**Abstract:** <details><summary>原文: </summary>Distracted driver activity recognition plays a critical role in risk aversion-particularly beneficial in intelligent transportation systems. However, most existing methods make use of only the video from a single view and the difficulty-inconsistent issue is neglected. Different from them, in this work, we propose a novel MultI-camera Feature Integration (MIFI) approach for 3D distracted driver activity recognition by jointly modeling the data from different camera views and explicitly re-weighting examples based on their degree of difficulty. Our contributions are two-fold: (1) We propose a simple but effective multi-camera feature integration framework and provide three types of feature fusion techniques. (2) To address the difficulty-inconsistent problem in distracted driver activity recognition, a periodic learning method, named example re-weighting that can jointly learn the easy and hard samples, is presented. The experimental results on the 3MDAD dataset demonstrate that the proposed MIFI can consistently boost performance compared to single-view models.</details>
**Abstract_cn:** <details><summary>译文: </summary>分心驾驶员活动识别在规避风险方面发挥着至关重要的作用，这在智能交通系统中尤其有益。然而，大多数现有方法仅使用来自单个视图的视频，并且忽略了难度不一致的问题。与它们不同的是，在这项工作中，我们提出了一种新颖的多摄像头特征集成（MIFI）方法，通过对来自不同摄像头视图的数据进行联合建模，并根据示例的难度程度显式地重新加权，来识别 3D 分心驾驶员活动。我们的贡献有两个：（1）我们提出了一个简单但有效的多相机特征集成框架，并提供了三种类型的特征融合技术。 (2)针对分心驾驶员活动识别中难度不一致的问题，提出了一种可联合学习易样本和难样本的周期性学习方法，称为实例重加权。 3MDAD 数据集上的实验结果表明，与单视图模型相比，所提出的 MIFI 可以持续提高性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14115v1><br />
**Code:** <https://github.com/john828/mifi>**<br />
>>**index:** 17<br />
**Title:** **Double Trouble? Impact and Detection of Duplicates in Face Image Datasets**<br />
**Title_cn:** 双重麻烦？人脸图像数据集中重复项的影响和检测<br />
**Authors:** Torsten Schlett, Christian Rathgeb, Juan Tapia, Christoph Busch<br />
**Abstract:** <details><summary>原文: </summary>Various face image datasets intended for facial biometrics research were created via web-scraping, i.e. the collection of images publicly available on the internet. This work presents an approach to detect both exactly and nearly identical face image duplicates, using file and image hashes. The approach is extended through the use of face image preprocessing. Additional steps based on face recognition and face image quality assessment models reduce false positives, and facilitate the deduplication of the face images both for intra- and inter-subject duplicate sets. The presented approach is applied to five datasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a cleaned MS-Celeb-1M variant). Duplicates are detected within every dataset, with hundreds to hundreds of thousands of duplicates for all except LFW. Face recognition and quality assessment experiments indicate a minor impact on the results through the duplicate removal. The final deduplication data is publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于面部生物识别研究的各种面部图像数据集是通过网络抓取创建的，即互联网上公开提供的图像集合。这项工作提出了一种使用文件和图像哈希来检测完全相同和几乎相同的面部图像副本的方法。该方法通过使用人脸图像预处理进行了扩展。基于人脸识别和人脸图像质量评估模型的附加步骤减少了误报，并促进了对象内和对象间重复集的人脸图像的重复数据删除。所提出的方法适用于五个数据集，即 LFW、TinyFace、Adience、CASIA-WebFace 和 C-MS-Celeb（经过清理的 MS-Celeb-1M 变体）。每个数据集中都会检测到重复项，除了 LFW 之外，所有数据集中都有数百到数十万个重复项。人脸识别和质量评估实验表明，重复去除对结果影响较小。最终的重复数据删除数据是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2401.14088v1><br />
**Code:** <https://github.com/dasec/dataset-duplicates>**<br />
>>**index:** 18<br />
**Title:** **ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation**<br />
**Title_cn:** ProCNS：弱监督医学图像分割的渐进式原型校准和噪声抑制<br />
**Authors:** Y. Liu, L. Lin, K. K. Y. Wong, X. Tang<br />
**Abstract:** <details><summary>原文: </summary>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on three medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods</details>
**Abstract_cn:** <details><summary>译文: </summary>弱监督分割（WSS）已经成为一种解决方案，通过采用稀疏注释格式（例如点、涂鸦、块等）来缓解注释成本和模型性能之间的冲突。典型的方法尝试利用解剖学和拓扑先验将稀疏注释直接扩展为伪标签。然而，由于缺乏对医学图像中模糊边缘的关注以及对稀疏监督的探索不足，现有方法往往会在噪声区域中生成错误且过度自信的伪建议，导致累积模型错误和性能下降。在这项工作中，我们提出了一种新颖的 WSS 方法，名为 ProCNS，包含根据渐进式原型校准和噪声抑制原理设计的两个协同模块。具体来说，我们设计了一种基于原型的区域空间亲和力（PRSA）损失，以最大化空间和语义元素之间的成对亲和力，为我们感兴趣的模型提供更可靠的指导。相似度是从输入图像和原型改进的预测中得出的。同时，我们提出了自适应噪声感知和掩蔽（ANPM）模块来获得更丰富和更具代表性的原型表示，它自适应地识别和掩蔽伪提案中的噪声区域，减少原型计算期间潜在的错误干扰。此外，我们为 ANPM 识别的噪声区域生成专门的软伪标签，提供补充监督。对涉及不同模态的三种医学图像分割任务的广泛实验表明，所提出的框架显着优于代表性的最先进方法</details>
**PDF:** <http://arxiv.org/pdf/2401.14074v1><br />
**Code:** <https://github.com/lyxdlii/procns>**<br />
>>**index:** 19<br />
**Title:** **Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition**<br />
**Title_cn:** 用于基于骨架的动作识别的无监督时空特征丰富和保真度网络<br />
**Authors:** Chuankun Li, Shuai Li, Yanbo Gao, Ping Chen, Jian Li, Wanqing Li<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods suffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability. To address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is first investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the same manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to produce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve this problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-FEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature transformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent unit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to generate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton sequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations. Experimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning methods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based action recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于无监督骨架的动作识别最近取得了显着的进展。现有的无监督学习方法存在严重的过拟合问题，因此使用小型网络，大大降低了表示能力。为了解决这个问题，首先研究了基于骨架的动作识别的无监督学习背后的过拟合机制。可以看出，骨架已经是一个相对高级和低维的特征，但与动作识别的特征不在同一流形中。简单地应用现有的无监督学习方法可能会产生区分不同样本而不是动作类别的特征，从而导致过拟合问题。为了解决这个问题，本文提出了一种无监督时空特征丰富和保真度框架（U-FEFP）来生成包含骨架序列所有信息的丰富分布式特征。使用时空图卷积网络和图卷积门循环单元网络作为基本特征提取网络，开发了时空特征转换子网络。基于无监督的 Bootstrap Your Own Latent 学习用于生成丰富的分布式特征，基于无监督借口任务的学习用于保留骨架序列的信息。这两种无监督学习方法作为 U-FEFP 进行协作，以产生稳健且有区别的表示。在三个广泛使用的基准（即 NTU-RGB+D-60、NTU-RGB+D-120 和 PKU-MMD 数据集）上的实验结果表明，与现有技术相比，所提出的 U-FEFP 实现了最佳性能。艺术无监督学习方法。 t-SNE 插图进一步验证了 U-FEFP 可以为基于无监督骨架的动作识别学习更多判别性特征。</details>
**PDF:** <http://arxiv.org/pdf/2401.14034v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction in High-definition Maps**<br />
**Title_cn:** PLCNet：用于高清地图中自动车道校正的分片车道校正网络<br />
**Authors:** Haiyang Peng, Yi Zhan, Benkang Wang, Hongtao Zhang<br />
**Abstract:** <details><summary>原文: </summary>In High-definition (HD) maps, lane elements constitute the majority of components and demand stringent localization requirements to ensure safe vehicle navigation. Vision lane detection with LiDAR position assignment is a prevalent method to acquire initial lanes for HD maps. However, due to incorrect vision detection and coarse camera-LiDAR calibration, initial lanes may deviate from their true positions within an uncertain range. To mitigate the need for manual lane correction, we propose a patch-wise lane correction network (PLCNet) to automatically correct the positions of initial lane points in local LiDAR images that are transformed from point clouds. PLCNet first extracts multi-scale image features and crops patch (ROI) features centered at each initial lane point. By applying ROIAlign, the fix-sized ROI features are flattened into 1D features. Then, a 1D lane attention module is devised to compute instance-level lane features with adaptive weights. Finally, lane correction offsets are inferred by a multi-layer perceptron and used to correct the initial lane positions. Considering practical applications, our automatic method supports merging local corrected lanes into global corrected lanes. Through extensive experiments on a self-built dataset, we demonstrate that PLCNet achieves fast and effective initial lane correction.</details>
**Abstract_cn:** <details><summary>译文: </summary>在高清（HD）地图中，车道元素构成了大部分组成部分，需要严格的定位要求以确保车辆导航安全。使用 LiDAR 位置分配的视觉车道检测是获取高清地图初始车道的常用方法。然而，由于不正确的视觉检测和粗略的相机激光雷达校准，初始车道可能会在不确定的范围内偏离其真实位置。为了减轻手动车道校正的需要，我们提出了一种分块车道校正网络（PLCNet）来自动校正从点云转换的本地 LiDAR 图像中初始车道点的位置。 PLCNet 首先提取以每个初始车道点为中心的多尺度图像特征和裁剪块 (ROI) 特征。通过应用 ROIAlign，固定大小的 ROI 特征被展平为一维特征。然后，设计一维车道注意模块来计算具有自适应权重的实例级车道特征。最后，由多层感知器推断车道校正偏移并用于校正初始车道位置。考虑到实际应用，我们的自动方法支持将局部校正车道合并到全局校正车道中。通过对自建数据集的大量实验，我们证明了 PLCNet 实现了快速有效的初始车道校正。</details>
**PDF:** <http://arxiv.org/pdf/2401.14024v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification**<br />
**Title_cn:** WAL-Net：用于颈动脉斑块分类的弱监督辅助任务学习网络<br />
**Authors:** Haitao Gan, Lingchao Fu, Ran Zhou, Weiyan Gan, Furong Wang, Xiaoyan Wu, Zhi Yang, Zhongwei Huang<br />
**Abstract:** <details><summary>原文: </summary>The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. This paper proposes a novel weakly supervised auxiliary task learning network model (WAL-Net) to explore the interdependence between carotid plaque classification and segmentation tasks. The plaque classification task is primary task, while the plaque segmentation task serves as an auxiliary task, providing valuable information to enhance the performance of the primary task. Weakly supervised learning is adopted in the auxiliary task to completely break away from the dependence on segmentation annotations. Experiments and evaluations are conducted on a dataset comprising 1270 carotid plaque ultrasound images from Wuhan University Zhongnan Hospital. Results indicate that the proposed method achieved an approximately 1.3% improvement in carotid plaque classification accuracy compared to the baseline network. Specifically, the accuracy of mixed-echoic plaques classification increased by approximately 3.3%, demonstrating the effectiveness of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>颈动脉超声图像的分类是诊断颈动脉斑块的重要手段，对于预测中风风险具有重要的临床意义。最近的研究表明，利用斑块分割作为分类的辅助任务可以通过利用分割和分类任务之间的相关性来提高性能。然而，这种方法依赖于获取大量难以获取的分割注释。本文提出了一种新颖的弱监督辅助任务学习网络模型（WAL-Net）来探索颈动脉斑块分类和分割任务之间的相互依赖性。斑块分类任务是主要任务，而斑块分割任务作为辅助任务，为提高主要任务的性能提供有价值的信息。辅助任务中采用弱监督学习，彻底摆脱对分割标注的依赖。在武汉大学中南医院包含 1270 张颈动脉斑块超声图像的数据集上进行实验和评估。结果表明，与基线网络相比，所提出的方法在颈动脉斑块分类准确度上提高了约 1.3%。具体来说，混合回声斑块分类的准确性提高了约 3.3%，证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13998v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Deep Learning Innovations in Diagnosing Diabetic Retinopathy: The Potential of Transfer Learning and the DiaCNN Model**<br />
**Title_cn:** 诊断糖尿病视网膜病变的深度学习创新：迁移学习和 DiaCNN 模型的潜力<br />
**Authors:** Mohamed R. Shoaib, Heba M. Emara, Jun Zhao, Walid El-Shafai, Naglaa F. Soliman, Ahmed S. Mubarak, Osama A. Omer, Fathi E. Abd El-Samie, Hamada Esmaiel<br />
**Abstract:** <details><summary>原文: </summary>Diabetic retinopathy (DR) is a significant cause of vision impairment, emphasizing the critical need for early detection and timely intervention to avert visual deterioration. Diagnosing DR is inherently complex, as it necessitates the meticulous examination of intricate retinal images by experienced specialists. This makes the early diagnosis of DR essential for effective treatment and the prevention of eventual blindness. Traditional diagnostic methods, relying on human interpretation of these medical images, face challenges in terms of accuracy and efficiency. In the present research, we introduce a novel method that offers superior precision in DR diagnosis, compared to these traditional methods, by employing advanced deep learning techniques. Central to this approach is the concept of transfer learning. This entails using pre-existing, well-established models, specifically InceptionResNetv2 and Inceptionv3, to extract features and fine-tune select layers to cater to the unique requirements of this specific diagnostic task. Concurrently, we also present a newly devised model, DiaCNN, which is tailored for the classification of eye diseases. To validate the efficacy of the proposed methodology, we leveraged the Ocular Disease Intelligent Recognition (ODIR) dataset, which comprises eight different eye disease categories. The results were promising. The InceptionResNetv2 model, incorporating transfer learning, registered an impressive 97.5% accuracy in both the training and testing phases. Its counterpart, the Inceptionv3 model, achieved an even more commendable 99.7% accuracy during training, and 97.5% during testing. Remarkably, the DiaCNN model showcased unparalleled precision, achieving 100% accuracy in training and 98.3\% in testing.</details>
**Abstract_cn:** <details><summary>译文: </summary>糖尿病视网膜病变（DR）是视力损害的一个重要原因，强调早期发现和及时干预以避免视力恶化的迫切需要。诊断 DR 本质上是复杂的，因为它需要经验丰富的专家对复杂的视网膜图像进行细致的检查。这使得 DR 的早期诊断对于有效治疗和预防最终失明至关重要。传统的诊断方法依赖于人类对这些医学图像的解释，在准确性和效率方面面临挑战。在本研究中，我们引入了一种新方法，通过采用先进的深度学习技术，与这些传统方法相比，该方法在 DR 诊断中提供了更高的精度。这种方法的核心是迁移学习的概念。这需要使用预先存在的、完善的模型，特别是 InceptionResNetv2 和 Inceptionv3，来提取特征并微调选择的层，以满足此特定诊断任务的独特要求。同时，我们还提出了一个新设计的模型DiaCNN，它是为眼部疾病的分类量身定制的。为了验证所提出方法的有效性，我们利用了眼部疾病智能识别（ODIR）数据集，该数据集包含八种不同的眼部疾病类别。结果是有希望的。 InceptionResNetv2 模型结合了迁移学习，在训练和测试阶段的准确率达到了令人印象深刻的 97.5%。与之对应的 Inceptionv3 模型在训练期间达到了 99.7% 的准确率，在测试期间达到了 97.5% 的准确率。值得注意的是，DiaCNN 模型展现了无与伦比的精度，训练准确率达到 100%，测试准确率达到 98.3%。</details>
**PDF:** <http://arxiv.org/pdf/2401.13990v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models**<br />
**Title_cn:** BootPIG：在预训练扩散模型中引导零样本个性化图像生成功能<br />
**Authors:** Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik<br />
**Abstract:** <details><summary>原文: </summary>Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的文本到图像生成模型在生成忠实遵循输入提示的图像方面取得了令人难以置信的成功。然而，使用文字来描述所需概念的要求提供了对所生成概念的外观的有限控制。在这项工作中，我们通过提出一种在现有文本到图像扩散模型中启用个性化功能的方法来解决这一缺点。我们提出了一种新颖的架构（BootPIG），它允许用户提供对象的参考图像，以指导生成图像中概念的出现。所提出的 BootPIG 架构对预训练的文本到图像扩散模型进行了最小的修改，并利用单独的 UNet 模型来引导各代人获得所需的外观。我们引入了一个训练过程，允许我们使用预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据来引导 BootPIG 架构中的个性化功能。与需要几天预训练的现有方法相比，BootPIG 架构可以在大约 1 小时内完成训练。 DreamBooth 数据集上的实验表明，BootPIG 的性能优于现有的零样本方法，同时与测试时微调方法相当。通过用户研究，我们验证了 BootPIG 生成相对于现有方法的偏好，无论是在保持参考对象外观的保真度还是与文本提示对齐方面。</details>
**PDF:** <http://arxiv.org/pdf/2401.13974v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization**<br />
**Title_cn:** 改进伪标签并增强半监督域泛化的鲁棒性<br />
**Authors:** Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan<br />
**Abstract:** <details><summary>原文: </summary>Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: https://github.com/Adnan-Khan7/UPLM</details>
**Abstract_cn:** <details><summary>译文: </summary>除了实现领域泛化（DG）之外，视觉识别模型还应该通过利用有限的标签在学习过程中实现数据高效。我们研究半监督域泛化（SSDG）问题，这对于自动化医疗保健等现实世界的应用至关重要。当给定的训练数据仅部分标记时，SSDG 需要学习跨域可推广模型。实证研究表明，DG 方法在 SSDG 设置中往往表现不佳，可能是因为它们无法利用未标记的数据。与完全监督学习相比，半监督学习（SSL）显示出改进但仍然较差的结果。性能最佳的基于 SSL 的 SSDG 方法面临的一个关键挑战是在多个域转换下选择准确的伪标签，并减少在有限标签下对源域的过度拟合。在这项工作中，我们提出了新的 SSDG 方法，该方法利用了一种新颖的不确定性引导伪标签模型平均（UPLM）。我们的不确定性引导伪标记（UPL）使用模型不确定性来改进伪标记选择，解决多源未标记数据下的不良模型校准问题。 UPL 技术通过我们新颖的模型平均 (MA) 策略得到增强，可以减轻对标签有限的源域的过度拟合。对关键代表性 DG 数据集的广泛实验表明，我们的方法相对于现有方法表现出有效性。我们的代码和选择的标记数据种子可在 GitHub 上找到：https://github.com/Adnan-Khan7/UPLM</details>
**PDF:** <http://arxiv.org/pdf/2401.13965v1><br />
**Code:** <https://github.com/adnan-khan7/uplm>**<br />
>>**index:** 25<br />
**Title:** **TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images**<br />
**Title_cn:** TriSAM：用于 VEM 图像中零次皮质血管分割的三平面 SAM<br />
**Authors:** Jia Wan, Wanhua Li, Atmadeep Banerjee, Jason Ken Adhinarta, Evelina Sjostedt, Jingpeng Wu, Jeff Lichtman, Hanspeter Pfister, Donglai Wei<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we address a significant gap in the field of neuroimaging by introducing the largest-to-date public benchmark, BvEM, designed specifically for cortical blood vessel segmentation in Volume Electron Microscopy (VEM) images. The intricate relationship between cerebral blood vessels and neural function underscores the vital role of vascular analysis in understanding brain health. While imaging techniques at macro and mesoscales have garnered substantial attention and resources, the microscale VEM imaging, capable of revealing intricate vascular details, has lacked the necessary benchmarking infrastructure. As researchers delve deeper into the microscale intricacies of cerebral vasculature, our BvEM benchmark represents a critical step toward unraveling the mysteries of neurovascular coupling and its impact on brain function and pathology. The BvEM dataset is based on VEM image volumes from three mammal species: adult mouse, macaque, and human. We standardized the resolution, addressed imaging variations, and meticulously annotated blood vessels through semi-automatic, manual, and quality control processes, ensuring high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical blood vessel segmentation method named TriSAM, which leverages the powerful segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to 3D volume segmentation, TriSAM employs a multi-seed tracking framework, leveraging the reliability of certain image planes for tracking while using others to identify potential turning points. This approach, consisting of Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively achieves long-term 3D blood vessel segmentation without model training or fine-tuning. Experimental results show that TriSAM achieved superior performances on the BvEM benchmark across three species.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们通过引入迄今为止最大的公共基准 BvEM 来解决神经影像领域的重大差距，BvEM 专为体积电子显微镜 (VEM) 图像中的皮质血管分割而设计。脑血管和神经功能之间的复杂关系强调了血管分析在了解大脑健康方面的重要作用。虽然宏观和中尺度成像技术已经获得了大量关注和资源，但能够揭示复杂血管细节的微尺度 VEM 成像却缺乏必要的基准基础设施。随着研究人员深入研究脑血管系统的微观复杂性，我们的 BvEM 基准代表了揭开神经血管耦合之谜及其对大脑功能和病理学影响的关键一步。 BvEM 数据集基于来自三种哺乳动物的 VEM 图像卷：成年小鼠、猕猴和人类。我们通过半自动、手动和质量控制流程标准化了分辨率，解决了成像变化，并仔细注释了血管，确保了高质量的 3D 分割。此外，我们开发了一种名为 TriSAM 的零样本皮质血管分割方法，该方法利用强大的分割模型 SAM 进行 3D 分割。为了将 SAM 从 2D 分割提升到 3D 体积分割，TriSAM 采用了多种子跟踪框架，利用某些图像平面的可靠性进行跟踪，同时使用其他图像平面来识别潜在的转折点。该方法由三平面选择、基于 SAM 的跟踪和递归重定向组成，可有效实现长期 3D 血管分割，无需模型训练或微调。实验结果表明，TriSAM 在三个物种的 BvEM 基准上取得了优异的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.13961v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **A New Image Quality Database for Multiple Industrial Processes**<br />
**Title_cn:** 适用于多种工业流程的新图像质量数据库<br />
**Authors:** Xuanchao Ma, Zehan Wu, Hongyan Liu, Chengxu Zhou, Ke Gu<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed a broader range of applications of image processing technologies in multiple industrial processes, such as smoke detection, security monitoring, and workpiece inspection. Different kinds of distortion types and levels must be introduced into an image during the processes of acquisition, compression, transmission, storage, and display, which might heavily degrade the image quality and thus strongly reduce the final display effect and clarity. To verify the reliability of existing image quality assessment methods, we establish a new industrial process image database (IPID), which contains 3000 distorted images generated by applying different levels of distortion types to each of the 50 source images. We conduct the subjective test on the aforementioned 3000 images to collect their subjective quality ratings in a well-suited laboratory environment. Finally, we perform comparison experiments on IPID database to investigate the performance of some objective image quality assessment algorithms. The experimental results show that the state-of-the-art image quality assessment methods have difficulty in predicting the quality of images that contain multiple distortion types.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，图像处理技术在烟雾检测、安全监控、工件检测等多个工业过程中得到了更广泛的应用。图像在采集、压缩、传输、存储、显示等过程中必然会引入不同类型和程度的畸变，这可能会严重降低图像质量，从而严重降低最终的显示效果和清晰度。为了验证现有图像质量评估方法的可靠性，我们建立了一个新的工业过程图像数据库（IPID），其中包含通过对 50 个源图像中的每一个应用不同级别的失真类型而生成的 3000 个失真图像。我们在合适的实验室环境中对上述 3000 张图像进行主观测试，收集其主观质量评级。最后，我们在IPID数据库上进行了对比实验，以研究一些客观图像质量评估算法的性能。实验结果表明，最先进的图像质量评估方法难以预测包含多种失真类型的图像的质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.13956v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking**<br />
**Title_cn:** AM-SORT：具有历史轨迹嵌入的自适应运动预测器，用于多对象跟踪<br />
**Authors:** Vitaliy Kim, Gunho Jung, Seong-Whan Lee<br />
**Abstract:** <details><summary>原文: </summary>Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多多目标跟踪 (MOT) 方法采用卡尔曼滤波器作为运动预测器，假设速度恒定且滤波噪声呈高斯分布。这些假设使得基于卡尔曼滤波器的跟踪器在线性运动场景中有效。然而，在涉及非线性运动和遮挡的场景中估计未来对象位置时，这些线性假设是一个关键限制。为了解决这个问题，我们提出了一种基于运动的 MOT 方法，具有自适应运动预测器，称为 AM-SORT，它适合估计非线性不确定性。 AM-SORT 是 SORT 系列跟踪器的新颖扩展，它以变压器架构取代卡尔曼滤波器作为运动预测器。我们引入了历史轨迹嵌入，使转换器能够从一系列边界框中提取时空特征。与 DanceTrack 上最先进的跟踪器相比，AM-SORT 的性能具有竞争力，IDF1 为 56.3，HOTA 为 55.6。我们进行了大量的实验来证明我们的方法在预测遮挡下非线性运动方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13950v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention**<br />
**Title_cn:** 利用可变形注意力的蒸馏学习进行自监督视频对象分割<br />
**Authors:** Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung<br />
**Abstract:** <details><summary>原文: </summary>Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频对象分割是计算机视觉中的一个基本研究问题。最近的技术经常将注意力机制应用于从视频序列中学习对象表示。然而，由于视频数据的时间变化，注意力图可能无法与视频帧中的感兴趣对象很好地对齐，从而导致长期视频处理中的累积错误。此外，现有技术利用了复杂的架构，需要高度的计算复杂性，因此限制了将视频对象分割集成到低功率设备中的能力。为了解决这些问题，我们提出了一种基于可变形注意力蒸馏学习的自监督视频对象分割新方法。具体来说，我们设计了一种用于视频对象分割的轻量级架构，可以有效地适应时间变化。这是通过可变形注意力机制实现的，其中捕获注意力模块中视频序列内存的键和值具有跨帧更新的灵活位置。因此，学习到的对象表示适应空间和时间维度。我们通过新的知识蒸馏范式以自我监督的方式训练所提出的架构，其中可变形注意力图被集成到蒸馏损失中。我们定性和定量评估我们的方法，并将其与基准数据集（包括 DAVIS 2016/2017 和 YouTube-VOS 2018/2019）上的现有方法进行比较。实验结果通过其实现的最先进的性能和最佳的内存使用验证了我们的方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13937v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **AscDAMs: Advanced SLAM-based channel detection and mapping system**<br />
**Title_cn:** AscDAMs：基于 SLAM 的高级通道检测和映射系统<br />
**Authors:** Tengfei Wang, Fucheng Lu, Jintao Qin, Taosheng Huang, Hui Kong, Ping Shen<br />
**Abstract:** <details><summary>原文: </summary>Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.</details>
**Abstract_cn:** <details><summary>译文: </summary>获得高分辨率、准确的河道地形和沉积条件是渠道化泥石流研究的首要挑战。目前，广泛使用的卫星成像和无人机摄影测量等测绘技术难以精确观测山地长深沟渠的河道内部状况，特别是在汶川地震地区。 SLAM 是一种新兴的 3D 地图技术；然而，即使对于最先进的SLAM来说，长而深的沟壑中极其恶劣的环境也带来了两个主要挑战：（1）非典型特征； (2)传感器剧烈晃动、振荡。这些问题导致 SLAM 结果存在较大偏差和大量噪声。为了改善此类环境中的 SLAM 映射，我们提出了一种先进的基于 SLAM 的通道检测和映射系统，即 AscDAM。它对SLAM结果的后处理主要有三个增强：（1）数字正射影像地图辅助偏差校正算法大大消除了系统误差； （2）点云平滑算法大幅降低噪声； (3)断面提取算法能够定量评估河道沉积物及其变化。 2023年2月和11月在中国汶川县楚头沟进行了两次野外试验，代表了雨季前后的观测结果。我们展示了 AscDAM 极大改善 SLAM 结果的能力，促进 SLAM 用于绘制特别具有挑战性的环境。该方法弥补了现有技术在检测泥石流河道内部的不足，包括详细的河道形态、侵蚀模式、沉积物区分、体积估计和变化检测。它有助于加强对全面泥石流机制、长期震后演化和灾害评估的研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.13877v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Deep Clustering with Diffused Sampling and Hardness-aware Self-distillation**<br />
**Title_cn:** 具有扩散采样和硬度感知自蒸馏的深度聚类<br />
**Authors:** Hai-Xin Zhang, Dong Huang<br />
**Abstract:** <details><summary>原文: </summary>Deep clustering has gained significant attention due to its capability in learning clustering-friendly representations without labeled data. However, previous deep clustering methods tend to treat all samples equally, which neglect the variance in the latent distribution and the varying difficulty in classifying or clustering different samples. To address this, this paper proposes a novel end-to-end deep clustering method with diffused sampling and hardness-aware self-distillation (HaDis). Specifically, we first align one view of instances with another view via diffused sampling alignment (DSA), which helps improve the intra-cluster compactness. To alleviate the sampling bias, we present the hardness-aware self-distillation (HSD) mechanism to mine the hardest positive and negative samples and adaptively adjust their weights in a self-distillation fashion, which is able to deal with the potential imbalance in sample contributions during optimization. Further, the prototypical contrastive learning is incorporated to simultaneously enhance the inter-cluster separability and intra-cluster compactness. Experimental results on five challenging image datasets demonstrate the superior clustering performance of our HaDis method over the state-of-the-art. Source code is available at https://github.com/Regan-Zhang/HaDis.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度聚类由于其无需标记数据即可学习聚类友好表示的能力而受到广泛关注。然而，以前的深度聚类方法倾向于平等地对待所有样本，忽略了潜在分布的方差以及对不同样本进行分类或聚类的不同难度。为了解决这个问题，本文提出了一种新颖的端到端深度聚类方法，具有扩散采样和硬度感知自蒸馏（HaDis）。具体来说，我们首先通过扩散采样对齐（DSA）将实例的一个视图与另一个视图对齐，这有助于提高集群内的紧凑性。为了减轻采样偏差，我们提出了硬度感知自蒸馏（HSD）机制来挖掘最难的正样本和负样本，并以自蒸馏的方式自适应地调整它们的权重，这能够处理样本中潜在的不平衡问题优化期间的贡献。此外，结合了原型对比学习，以同时增强簇间可分离性和簇内紧凑性。五个具有挑战性的图像数据集的实验结果证明了我们的 HaDis 方法比最先进的方法具有优越的聚类性能。源代码可在 https://github.com/Regan-Zhang/HaDis 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.14038v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models**<br />
**Title_cn:** StyleInject：文本到图像扩散模型的参数高效调整<br />
**Authors:** Yalong Bai, Mohan Zhou, Qing Yang<br />
**Abstract:** <details><summary>原文: </summary>The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>微调文本到图像生成任务的生成模型的能力至关重要，特别是面对准确解释和可视化文本输入所涉及的复杂性。虽然 LoRA 在语言模型适应方面非常高效，但由于图像生成的复杂要求（例如适应广泛的风格和细微差别），它在文本到图像任务中常常表现不佳。为了弥补这一差距，我们引入了 StyleInject，这是一种专为文本到图像模型量身定制的微调方法。 StyleInject包含多个并行的低秩参数矩阵，保持视觉特征的多样性。它通过根据输入信号的特征调整视觉特征的方差来动态适应不同的风格。这种方法极大地减少了对原始模型文本图像对齐能力的影响，同时巧妙地适应迁移学习中的各种风格。事实证明，StyleInject 在学习和增强一系列先进的、经过社区微调的生成模型方面特别有效。我们的综合实验，包括小样本和大规模数据微调以及基础模型蒸馏，表明StyleInject在文本图像语义一致性和人类偏好评估方面都超越了传统LoRA，同时确保了更高的参数效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.13942v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Deconstructing Denoising Diffusion Models for Self-Supervised Learning**<br />
**Title_cn:** 解构自监督学习的去噪扩散模型<br />
**Authors:** Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He<br />
**Abstract:** <details><summary>原文: </summary>In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们检查了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的理念是解构 DDM，逐渐将其转变为经典的去噪自动编码器 (DAE)。这种解构过程使我们能够探索现代 DDM 的各个组成部分如何影响自监督表示学习。我们观察到，只有极少数现代组件对于学习良好的表示至关重要，而许多其他组件则不是必需的。我们的研究最终得出了一种高度简化的方法，并且在很大程度上类似于经典的 DAE。我们希望我们的研究能够重新激发人们对现代自我监督学习领域中一系列经典方法的兴趣。</details>
**PDF:** <http://arxiv.org/pdf/2401.14404v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation**<br />
**Title_cn:** Sketch2NeRF：多视图草图引导的文本到 3D 生成<br />
**Authors:** Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo<br />
**Abstract:** <details><summary>原文: </summary>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，文本转 3D 方法已经使用文本描述实现了高保真 3D 内容生成。然而，生成的对象是随机的并且缺乏细粒度的控制。草图提供了一种廉价的方法来引入这种细粒度的控制。然而，由于这些草图的抽象性和模糊性，实现灵活的控制具有挑战性。在本文中，我们提出了一种多视图草图引导的文本到 3D 生成框架（即 Sketch2NeRF），以将草图控制添加到 3D 生成中。具体来说，我们的方法利用预训练的 2D 扩散模型（例如，Stable Diffusion 和 ControlNet）来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。我们提出了一种新颖的同步生成和重建方法来有效优化 NeRF。在实验中，我们收集了两种多视图草图数据集来评估所提出的方法。我们证明我们的方法可以通过细粒度草图控制合成 3D 一致的内容，同时对文本提示保持高保真度。大量结果表明，我们的方法在草图相似性和文本对齐方面实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14257v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models**<br />
**Title_cn:** 场景图到图像合成：将 CLIP 指导与扩散模型中的图调节相集成<br />
**Authors:** Rameshwar Mishra, A V Subramanyam<br />
**Abstract:** <details><summary>原文: </summary>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型的进步引发了人们对在遵守特定结构准则的同时生成图像的浓厚兴趣。场景图到图像生成就是生成与给定场景图一致的图像的任务之一。然而，视觉场景的复杂性对根据场景图中指定的关系准确对齐对象提出了挑战。现有方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来完成此任务。在这项工作中，我们引入了一种从场景图生成图像的新颖方法，该方法消除了预测中间布局的需要。我们利用预先训练的文本到图像扩散模型和 CLIP 指导将图形知识转化为图像。为此，我们首先使用基于 GAN 的训练对图形编码器进行预训练，以将图形特征与相应图像的 CLIP 特征对齐。此外，我们将图特征与给定场景图中存在的对象标签的 CLIP 嵌入相融合，以创建图一致的 CLIP 引导调节信号。在条件输入中，对象嵌入提供图像的粗略结构，图形特征提供基于对象之间关系的结构对齐。最后，我们使用具有重建和 CLIP 对齐损失的图一致条件信号微调预训练的扩散模型。详细的实验表明，我们的方法在 COCO-stuff 和 Visual Genome 数据集的标准基准上优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.14111v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion**<br />
**Title_cn:** CreativeSynth：基于多模态扩散的视觉艺术创意融合与合成<br />
**Authors:** Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu<br />
**Abstract:** <details><summary>原文: </summary>Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. This allows for the precise manipulation of image style and content while maintaining the integrity of the original model parameters. Rigorous qualitative and quantitative evaluations underscore that CreativeSynth excels in enhancing artistic images' fidelity and preserves their innate aesthetic essence. By bridging the gap between generative models and artistic finesse, CreativeSynth becomes a custom digital palette.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模文本到图像生成模型取得了令人印象深刻的进步，展示了它们合成大量高质量图像的能力。然而，将这些模型应用于艺术图像编辑面临着两个重大挑战。首先，用户很难精心制作文本提示来详细描述输入图像的视觉元素。其次，流行的模式在对特定区域进行修改时，经常会破坏整体的艺术风格，使实现有凝聚力和审美统一的艺术品变得复杂化。为了克服这些障碍，我们构建了创新的统一框架 CreativeSynth，它基于扩散模型，能够协调艺术图像生成领域的多模式输入和多任务。通过将多模态特征与定制的注意力机制相结合，CreativeSynth 通过反转和实时风格转换，促进将现实世界的语义内容输入到艺术领域。这允许精确操作图像样式和内容，同时保持原始模型参数的完整性。严格的定性和定量评估强调了 CreativeSynth 在增强艺术图像的保真度并保留其固有的审美本质方面表现出色。通过弥合生成模型和艺术技巧之间的差距，CreativeSynth 成为一个定制的数字调色板。</details>
**PDF:** <http://arxiv.org/pdf/2401.14066v1><br />
**Code:** <https://github.com/haha-lisa/creativesynth>**<br />
>>**index:** 5<br />
**Title:** **Diffusion-based Data Augmentation for Object Counting Problems**<br />
**Title_cn:** 针对对象计数问题的基于扩散的数据增强<br />
**Authors:** Zhen Wang, Yuelei Li, Jia Wan, Nuno Vasconcelos<br />
**Abstract:** <details><summary>原文: </summary>Crowd counting is an important problem in computer vision due to its wide range of applications in image understanding. Currently, this problem is typically addressed using deep learning approaches, such as Convolutional Neural Networks (CNNs) and Transformers. However, deep networks are data-driven and are prone to overfitting, especially when the available labeled crowd dataset is limited. To overcome this limitation, we have designed a pipeline that utilizes a diffusion model to generate extensive training data. We are the first to generate images conditioned on a location dot map (a binary dot map that specifies the location of human heads) with a diffusion model. We are also the first to use these diverse synthetic data to augment the crowd counting models. Our proposed smoothed density map input for ControlNet significantly improves ControlNet's performance in generating crowds in the correct locations. Also, Our proposed counting loss for the diffusion model effectively minimizes the discrepancies between the location dot map and the crowd images generated. Additionally, our innovative guidance sampling further directs the diffusion process toward regions where the generated crowd images align most accurately with the location dot map. Collectively, we have enhanced ControlNet's ability to generate specified objects from a location dot map, which can be used for data augmentation in various counting problems. Moreover, our framework is versatile and can be easily adapted to all kinds of counting problems. Extensive experiments demonstrate that our framework improves the counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS datasets, showcasing its effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>人群计数由于其在图像理解中的广泛应用而成为计算机视觉中的一个重要问题。目前，这个问题通常使用深度学习方法来解决，例如卷积神经网络（CNN）和 Transformer。然而，深度网络是数据驱动的，很容易过度拟合，特别是当可用的标记人群数据集有限时。为了克服这个限制，我们设计了一个利用扩散模型来生成大量训练数据的管道。我们是第一个使用扩散模型生成以位置点图（指定人体头部位置的二进制点图）为条件的图像。我们也是第一个使用这些多样化的合成数据来增强人群计数模型的人。我们提出的 ControlNet 平滑密度图输入显着提高了 ControlNet 在正确位置生成人群的性能。此外，我们提出的扩散模型的计数损失有效地最小化了位置点图和生成的人群图像之间的差异。此外，我们的创新引导采样进一步将扩散过程引导至生成的人群图像与位置点图最准确对齐的区域。总的来说，我们增强了 ControlNet 从位置点图生成指定对象的能力，可用于各种计数问题中的数据增强。此外，我们的框架是通用的，可以轻松适应各种计数问题。大量实验表明，我们的框架提高了 ShanghaiTech、NWPU-Crowd、UCF-QNRF 和 TRANCOS 数据集的计数性能，展示了其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.13992v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning**<br />
**Title_cn:** 通过随机主题对抗性学习进行外观去偏注视估计<br />
**Authors:** Suneung Kim, Woo-Jeoung Nam, Seong-Whan Lee<br />
**Abstract:** <details><summary>原文: </summary>Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于外观的注视估计已经引起了计算机视觉领域的关注，并且使用各种深度学习技术已经取得了显着的进步。尽管取得了这些进展，但大多数方法的目标是直接从图像中推断注视向量，这会导致对特定于人的外观因素的过度拟合。在本文中，我们解决了这些挑战并提出了一个新颖的框架：随机主题对抗性凝视学习（SAZE），它训练一个网络来概括主题的外观。我们使用面部到凝视编码器和面部身份分类器以及提出的对抗性损失来设计面部泛化网络（Fgen-Net）。所提出的损失概括了面部外观因素，以便身份分类器推断出均匀的概率分布。此外，Fgen-Net 通过学习机制进行训练，该机制通过在每个训练步骤重新选择主题子集来优化网络，以避免过度拟合。我们的实验结果验证了该方法的稳健性，因为它产生了最先进的性能，在 MPIIGaze 和 EyeDiap 数据集上分别达到 3.89 和 4.42。此外，我们通过使用生成模型生成的涉及不同风格的人脸图像进行进一步的实验，证明了积极的泛化效果。</details>
**PDF:** <http://arxiv.org/pdf/2401.13865v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **JUMP: A joint multimodal registration pipeline for neuroimaging with minimal preprocessing**<br />
**Title_cn:** JUMP：用于神经成像的联合多模式配准管道，只需最少的预处理<br />
**Authors:** Adria Casamitjana, Juan Eugenio Iglesias, Raul Tudela, Aida Ninerola-Baizan, Roser Sala-Llonch<br />
**Abstract:** <details><summary>原文: </summary>We present a pipeline for unbiased and robust multimodal registration of neuroimaging modalities with minimal pre-processing. While typical multimodal studies need to use multiple independent processing pipelines, with diverse options and hyperparameters, we propose a single and structured framework to jointly process different image modalities. The use of state-of-the-art learning-based techniques enables fast inferences, which makes the presented method suitable for large-scale and/or multi-cohort datasets with a diverse number of modalities per session. The pipeline currently works with structural MRI, resting state fMRI and amyloid PET images. We show the predictive power of the derived biomarkers using in a case-control study and study the cross-modal relationship between different image modalities. The code can be found in https: //github.com/acasamitjana/JUMP.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种以最少的预处理对神经影像模式进行无偏且稳健的多模式注册的管道。虽然典型的多模态研究需要使用多个独立的处理管道，具有多种选项和超参数，但我们提出了一个单一的结构化框架来联合处理不同的图像模态。使用最先进的基于学习的技术可以实现快速推理，这使得所提出的方法适用于每个会话具有多种模式的大规模和/或多队列数据集。该流程目前适用于结构 MRI、静息态 fMRI 和淀粉样蛋白 PET 图像。我们展示了在病例对照研究中使用的衍生生物标志物的预测能力，并研究了不同图像模式之间的跨模式关系。代码可以在https://github.com/acasamitjana/JUMP找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.14250v1><br />
**Code:** <https://github.com/acasamitjana/jump>**<br />
>>**index:** 2<br />
**Title:** **LanDA: Language-Guided Multi-Source Domain Adaptation**<br />
**Title_cn:** LanDA：语言引导的多源域适应<br />
**Authors:** Zhenbin Wang, Lei Zhang, Lituan Wang, Minjuan Zhu<br />
**Abstract:** <details><summary>原文: </summary>Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data distribution when transferring knowledge from multiple labeled source domains to an unlabeled target domain. However, existing MSDA techniques assume target domain images are available, yet overlook image-rich semantic information. Consequently, an open question is whether MSDA can be guided solely by textual cues in the absence of target domain images. By employing a multimodal model with a joint image and language embedding space, we propose a novel language-guided MSDA approach, termed LanDA, based on optimal transfer theory, which facilitates the transfer of multiple source domains to a new target domain, requiring only a textual description of the target domain without needing even a single target domain image, while retaining task-relevant information. We present extensive experiments across different transfer scenarios using a suite of relevant benchmarks, demonstrating that LanDA outperforms standard fine-tuning and ensemble approaches in both target and source domains.</details>
**Abstract_cn:** <details><summary>译文: </summary>多源域适应 (MSDA) 旨在将知识从多个标记源域转移到未标记目标域时减轻数据分布的变化。然而，现有的 MSDA 技术假设目标域图像可用，但却忽略了图像丰富的语义信息。因此，一个悬而未决的问题是，在没有目标域图像的情况下，MSDA 是否可以仅通过文本线索进行指导。通过采用具有联合图像和语言嵌入空间的多模态模型，我们提出了一种基于最优迁移理论的新型语言引导 MSDA 方法，称为 LanDA，该方法有助于将多个源域迁移到新的目标域，仅需要目标域的文本描述，甚至不需要单个目标域图像，同时保留任务相关信息。我们使用一套相关基准在不同的传输场景中进行了广泛的实验，证明 LanDA 在目标域和源域中均优于标准微调和集成方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.14148v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting**<br />
**Title_cn:** GauU-Scene：使用高斯泼溅的大规模 3D 重建数据集的场景重建基准<br />
**Authors:** Butian Xiong, Zhuo Li, Zhen Li<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information</details>
**Abstract_cn:** <details><summary>译文: </summary>我们在庞大的 U-Scene 数据集上使用新开发的 3D 表示方法 Gaussian Splatting 引入了一种新颖的大规模场景重建基准。 U-Scene 占地超过一平方公里，具有全面的 RGB 数据集和 LiDAR 地面实况。在数据采集方面，我们使用了配备高精度Zenmuse L1激光雷达的Matrix 300无人机，实现了精确的屋顶数据采集。该数据集提供了城市和学术环境的独特融合，用于高级空间分析，转换超过 1.5 公里$^2$。我们对 U 场景与高斯泼溅的评估包括对各种新颖观点的详细分析。我们还将这些结果与从我们精确的点云数据集得出的结果并列，突出了显着差异，强调了组合多模态信息的重要性</details>
**PDF:** <http://arxiv.org/pdf/2401.14032v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration**<br />
**Title_cn:** MambaMorph：基于 Mamba 的骨干网，具有用于可变形 MR-CT 配准的对比特征学习<br />
**Authors:** Tao Guo, Yinuo Wang, Cai Meng<br />
**Abstract:** <details><summary>原文: </summary>Deformable image registration is an essential approach for medical image analysis.This paper introduces MambaMorph, an innovative multi-modality deformable registration network, specifically designed for Magnetic Resonance (MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with its Mamba-based registration module and a contrastive feature learning approach, addressing the prevalent challenges in multi-modality registration. The network leverages Mamba blocks for efficient long-range modeling and high-dimensional data processing, coupled with a feature extractor that learns fine-grained features for enhanced registration accuracy. Experimental results showcase MambaMorph's superior performance over existing methods in MR-CT registration, underlining its potential in clinical applications. This work underscores the significance of feature learning in multi-modality registration and positions MambaMorph as a trailblazing solution in this field. The code for MambaMorph is available at: https://github.com/Guo-Stone/MambaMorph.</details>
**Abstract_cn:** <details><summary>译文: </summary>可变形图像配准是医学图像分析的重要方法。本文介绍了MambaMorph，一种创新的多模态可变形配准网络，专为磁共振（MR）和计算机断层扫描（CT）图像对准而设计。 MambaMorph 以其基于 Mamba 的注册模块和对比特征学习方法脱颖而出，解决了多模态注册中普遍存在的挑战。该网络利用 Mamba 模块进行高效的远程建模和高维数据处理，并结合特征提取器来学习细粒度特征以提高配准精度。实验结果展示了 MambaMorph 在 MR-CT 配准方面优于现有方法的性能，凸显了其在临床应用中的潜力。这项工作强调了特征学习在多模态注册中的重要性，并将 MambaMorph 定位为该领域的开拓性解决方案。 MambaMorph 的代码位于：https://github.com/Guo-Stone/MambaMorph。</details>
**PDF:** <http://arxiv.org/pdf/2401.13934v1><br />
**Code:** <https://github.com/guo-stone/mambamorph>**<br />
>>**index:** 5<br />
**Title:** **Knowledge Graph Supported Benchmark and Video Captioning for Basketball**<br />
**Title_cn:** 知识图谱支持的篮球基准和视频字幕<br />
**Authors:** Zeyu Xi, Ge Shi, Lifang Wu, Xuefen Li, Junchi Yan, Liang Wang, Zilin Liu<br />
**Abstract:** <details><summary>原文: </summary>Despite the recent emergence of video captioning models, how to generate the text description with specific entity names and fine-grained actions is far from being solved, which however has great applications such as basketball live text broadcast. In this paper, a new multimodal knowledge supported basketball benchmark for video captioning is proposed. Specifically, we construct a Multimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond videos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset that contains 9 types of fine-grained shooting events and 286 players' knowledge (i.e., images and names) is constructed based on MbgKG. We develop a novel framework in the encoder-decoder form named Entity-Aware Captioner (EAC) for basketball live text broadcast. The temporal information in video is encoded by introducing the bi-directional GRU (Bi-GRU) module. And the multi-head self-attention module is utilized to model the relationships among the players and select the key players. Besides, we propose a new performance evaluation metric named Game Description Score (GDS), which measures not only the linguistic performance but also the accuracy of the names prediction. Extensive experiments on MbgVC dataset demonstrate that EAC effectively leverages external knowledge and outperforms advanced video captioning models. The proposed benchmark and corresponding codes will be publicly available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管最近出现了视频字幕模型，但如何生成具有特定实体名称和细粒度动作的文本描述还远未解决，但这在篮球直播文本转播等领域具有很大的应用。本文提出了一种新的多模态知识支持的视频字幕篮球基准。具体来说，我们构建了一个多模态篮球比赛知识图（MbgKG）来提供视频之外的知识。然后，基于MbgKG构建了包含9种细粒度投篮事件和286名球员知识（即图像和姓名）的多模态篮球比赛视频字幕（MbgVC）数据集。我们开发了一种编码器-解码器形式的新颖框架，名为实体感知字幕器（EAC），用于篮球直播文本广播。通过引入双向 GRU（Bi-GRU）模块对视频中的时间信息进行编码。利用多头自注意力模块对参与者之间的关系进行建模并选择关键参与者。此外，我们提出了一种新的性能评估指标，称为游戏描述得分（GDS），它不仅衡量语言性能，还衡量名称预测的准确性。 MbgVC 数据集上的大量实验表明，EAC 有效地利用了外部知识，并且优于先进的视频字幕模型。拟议的基准和相应的代码将很快公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.13888v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural Image Compression**<br />
**Title_cn:** 高保真神经图像压缩的语义集成损失和潜在细化<br />
**Authors:** Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in neural compression have surpassed traditional codecs in PSNR and MS-SSIM measurements. However, at low bit-rates, these methods can introduce visually displeasing artifacts, such as blurring, color shifting, and texture loss, thereby compromising perceptual quality of images. To address these issues, this study presents an enhanced neural compression method designed for optimal visual fidelity. We have trained our model with a sophisticated semantic ensemble loss, integrating Charbonnier loss, perceptual loss, style loss, and a non-binary adversarial loss, to enhance the perceptual quality of image reconstructions. Additionally, we have implemented a latent refinement process to generate content-aware latent codes. These codes adhere to bit-rate constraints, balance the trade-off between distortion and fidelity, and prioritize bit allocation to regions of greater importance. Our empirical findings demonstrate that this approach significantly improves the statistical fidelity of neural image compression. On CLIC2024 validation set, our approach achieves a 62% bitrate saving compared to MS-ILLM under FID metric.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经压缩领域的最新进展在 PSNR 和 MS-SSIM 测量方面已经超越了传统编解码器。然而，在低比特率下，这些方法可能会引入视觉上令人不快的伪像，例如模糊、色彩偏移和纹理丢失，从而损害图像的感知质量。为了解决这些问题，本研究提出了一种增强的神经压缩方法，旨在实现最佳视觉保真度。我们使用复杂的语义集成损失来训练我们的模型，整合 Charbonnier 损失、感知损失、风格损失和非二元对抗性损失，以提高图像重建的感知质量。此外，我们还实现了一个潜在的细化过程来生成内容感知的潜在代码。这些代码遵守比特率限制，平衡失真和保真度之间的权衡，并将比特分配优先到更重要的区域。我们的实证研究结果表明，这种方法显着提高了神经图像压缩的统计保真度。在 CLIC2024 验证集上，与 FID 指标下的 MS-ILLM 相比，我们的方法实现了 62% 的比特率节省。</details>
**PDF:** <http://arxiv.org/pdf/2401.14007v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **A real-time rendering method for high albedo anisotropic materials with multiple scattering**<br />
**Title_cn:** 一种多重散射高反照率各向异性材料的实时渲染方法<br />
**Authors:** Shun Fang, Xing Feng, Ming Cui<br />
**Abstract:** <details><summary>原文: </summary>We propose a neural network-based real-time volume rendering method for realistic and efficient rendering of volumetric media. The traditional volume rendering method uses path tracing to solve the radiation transfer equation, which requires a huge amount of calculation and cannot achieve real-time rendering. Therefore, this paper uses neural networks to simulate the iterative integration process of solving the radiative transfer equation to speed up the volume rendering of volume media. Specifically, the paper first performs data processing on the volume medium to generate a variety of sampling features, including density features, transmittance features and phase features. The hierarchical transmittance fields are fed into a 3D-CNN network to compute more important transmittance features. Secondly, the diffuse reflection sampling template and the highlight sampling template are used to layer the three types of sampling features into the network. This method can pay more attention to light scattering, highlights and shadows, and then select important channel features through the attention module. Finally, the scattering distribution of the center points of all sampling templates is predicted through the backbone neural network. This method can achieve realistic volumetric media rendering effects and greatly increase the rendering speed while maintaining rendering quality, which is of great significance for real-time rendering applications. Experimental results indicate that our method outperforms previous methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种基于神经网络的实时体积渲染方法，用于逼真且高效地渲染体积媒体。传统的体绘制方法采用路径追踪来求解辐射传递方程，计算量巨大且无法实现实时绘制。因此，本文利用神经网络来模拟求解辐射传递方程的迭代积分过程，以加速体媒体的体渲染。具体来说，论文首先对体介质进行数据处理，生成多种采样特征，包括密度特征、透过率特征和相位特征。分层透射率场被输入 3D-CNN 网络以计算更重要的透射率特征。其次，利用漫反射采样模板和高光采样模板将三类采样特征分层到网络中。该方法可以更多地关注光散射、高光和阴影，然后通过注意力模块选择重要的通道特征。最后，通过主干神经网络预测所有采样模板中心点的散射分布。该方法可以实现逼真的体媒体渲染效果，在保持渲染质量的同时大幅提高渲染速度，对于实时渲染应用具有重要意义。实验结果表明我们的方法优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.14051v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Diverse and Lifespan Facial Age Transformation Synthesis with Identity Variation Rationality Metric**<br />
**Title_cn:** 具有身份变异理性度量的多样化和寿命面部年龄变换综合<br />
**Authors:** Jiu-Cheng Xie, Jun Yang, Wenqing Wang, Feng Xu, Hao Gao<br />
**Abstract:** <details><summary>原文: </summary>Face aging has received continuous research attention over the past two decades. Although previous works on this topic have achieved impressive success, two longstanding problems remain unsettled: 1) generating diverse and plausible facial aging patterns at the target age stage; 2) measuring the rationality of identity variation between the original portrait and its syntheses with age progression or regression. In this paper, we introduce DLAT + , the first algorithm that can realize Diverse and Lifespan Age Transformation on human faces, where the diversity jointly manifests in the transformation of facial textures and shapes. Apart from the diversity mechanism embedded in the model, multiple consistency restrictions are leveraged to keep it away from counterfactual aging syntheses. Moreover, we propose a new metric to assess the rationality of Identity Deviation under Age Gaps (IDAG) between the input face and its series of age-transformed generations, which is based on statistical laws summarized from plenty of genuine face-aging data. Extensive experimental results demonstrate the uniqueness and effectiveness of our method in synthesizing diverse and perceptually reasonable faces across the whole lifetime.</details>
**Abstract_cn:** <details><summary>译文: </summary>过去二十年来，面部衰老一直受到研究关注。尽管之前关于该主题的工作取得了令人瞩目的成功，但两个长期存在的问题仍未解决：1）在目标年龄阶段生成多样化且合理的面部衰老模式； 2）衡量原始肖像及其合成物之间身份随年龄增长或回归而变化的合理性。在本文中，我们介绍了DLAT+，这是第一个可以实现人脸多样性和寿命年龄变换的算法，其中多样性共同体现在面部纹理和形状的变换上。除了模型中嵌入的多样性机制之外，还利用多重一致性限制来使其远离反事实的老化合成。此外，我们提出了一种新的指标来评估输入人脸及其一系列年龄转换代之间年龄差距下的身份偏差（IDAG）的合理性，该指标基于从大量真实的人脸老化数据中总结出的统计规律。大量的实验结果证明了我们的方法在整个一生中合成多样化且感知合理的面孔方面的独特性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14036v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Learning to Manipulate Artistic Images**<br />
**Title_cn:** 学习操纵艺术图像<br />
**Authors:** Wei Guo, Yuqi Zhang, De Ma, Qian Zheng<br />
**Abstract:** <details><summary>原文: </summary>Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art methods.Code is available at https://github.com/SnailForce/SIM-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉的最新进展显着降低了艺术创作的障碍。基于样本的图像翻译方法由于灵活性和可控性而备受关注。然而，这些方法持有关于语义的假设或需要语义信息作为输入，而准确的语义在艺术图像中并不容易获得。此外，这些方法由于训练数据先验而受到跨域伪影的影响，并且由于空间域中的特征压缩而产生不精确的结构。在本文中，我们提出了一种任意风格图像处理网络（SIM-Net），它利用无语义信息作为指导，并以自监督的方式使用区域传输策略来生成图像。我们的方法在一定程度上平衡了计算效率和高分辨率。此外，我们的方法有利于零样本图像处理。定性和定量实验都证明了我们的方法相对于最先进方法的优越性。代码可在 https://github.com/SnailForce/SIM-Net 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13976v1><br />
**Code:** <https://github.com/snailforce/sim-net>**<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation**<br />
**Title_cn:** 学习具有可见性和特征增强点表示的鲁棒可泛化辐射场<br />
**Authors:** Jiaxu Wang, Ziyi Zhang, Renjing Xu<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF). Previous generic NeRF methods combine multiview stereo techniques with image-based neural rendering for generalization, yielding impressive results, while suffering from three issues. First, occlusions often result in inconsistent feature matching. Then, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation. Third, their image-based representations experience severe degradations when source views are not near enough to the target view. To address challenges, we propose the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field (GPF). Our approach explicitly models visibilities by geometric priors and augments them with neural features. We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality. Moreover, we present a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Besides, our representation can be easily manipulated. Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings, preliminarily proving the potential of the new paradigm for generalizable NeRF.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了可泛化神经辐射场（NeRF）的一种新范式。以前的通用 NeRF 方法将多视图立体技术与基于图像的神经渲染相结合以进行泛化，产生了令人印象深刻的结果，但同时遇到了三个问题。首先，遮挡通常会导致特征匹配不一致。然后，由于采样点和粗糙特征聚合的单独处理，它们会在几何不连续性和局部锐利形状中产生扭曲和伪影。第三，当源视图与目标视图不够接近时，它们基于图像的表示会经历严重的退化。为了应对挑战，我们提出了第一个基于点渲染而不是基于图像渲染构建可泛化神经场的范式，我们将其称为可泛化神经点场（GPF）。我们的方法通过几何先验显式地对可见性进行建模，并通过神经特征对其进行增强。我们提出了一种新颖的非均匀对数采样策略来提高渲染速度和重建质量。此外，我们提出了一个可学习的内核，在空间上增强了特征聚合的特征，减轻了几何形状急剧变化的地方的扭曲。此外，我们的表示很容易被操纵。实验表明，我们的模型在泛化和微调设置上都可以比三个数据集上的所有同行和基准提供更好的几何形状、视图一致性和渲染质量，初步证明了可泛化 NeRF 新范式的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.14354v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Range-Agnostic Multi-View Depth Estimation With Keyframe Selection**<br />
**Title_cn:** 通过关键帧选择进行与范围无关的多视图深度估计<br />
**Authors:** Andrea Conti, Matteo Poggi, Valerio Cambareri, Stefano Mattoccia<br />
**Abstract:** <details><summary>原文: </summary>Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing RAMDepth, an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. Additional material can be found on our project page https://andreaconti.github.io/projects/range_agnostic_multi_view_depth.</details>
**Abstract_cn:** <details><summary>译文: </summary>从姿势帧进行 3D 重建的方法需要有关场景度量范围的先验知识，通常是为了恢复沿极线的匹配线索并缩小搜索范围。然而，这种先验可能无法直接获得，或者在实际场景中估计不准确（例如，根据视频序列进行室外 3D 重建），因此严重影响了性能。在本文中，我们通过提出 RAMDepth（一种高效且纯 2D 的框架，可反转深度估计和匹配步骤顺序），专注于多视图深度估计，而无需了解场景的度量范围。此外，我们还展示了我们的框架能够提供有关用于预测的视图质量的丰富见解的能力。其他材料可以在我们的项目页面 https://andreaconti.github.io/projects/range_agnostic_multi_view_depth 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.14401v1><br />
**Code:** <https://github.com/andreaconti/ramdepth>**<br />
>>**index:** 2<br />
**Title:** **Learning to navigate efficiently and precisely in real environments**<br />
**Title_cn:** 学习在真实环境中高效、精确地导航<br />
**Authors:** Guillaume Bono, Hervé Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf<br />
**Abstract:** <details><summary>原文: </summary>In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.</details>
**Abstract_cn:** <details><summary>译文: </summary>在陆地机器人自主导航的背景下，创建代理动力学和传感的真实模型是机器人文献和商业应用中的普遍习惯，它们用于基于模型的控制和/或定位和绘图。另一方面，最近的 Embodied AI 文献侧重于在 Habitat 或 AI-Thor 等模拟器中训练的模块化或端到端代理，其中重点放在照片真实感渲染和场景多样性上，但高保真度机器人运动被分配了一个不太特权的角色。由此产生的 sim2real 差距极大地影响了训练模型到真实机器人平台的转移。在这项工作中，我们探索在模拟环境中对代理进行端到端训练，以最大限度地减少传感和驱动方面的 sim2real 差距。我们的代理直接预测（离散化）速度命令，这些命令通过真实机器人中的闭环控制来维护。真实机器人（包括底层低级控制器）的行为在经过修改的栖息地模拟器中进行识别和模拟。用于里程计和定位的噪声模型进一步有助于降低 sim2real 差距。我们评估真实的导航场景，探索不同的定位和点目标计算方法，并报告与之前的工作相比在性能和鲁棒性方面取得了显着的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.14349v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Adaptive Mobile Manipulation for Articulated Objects In the Open World**<br />
**Title_cn:** 开放世界中铰接物体的自适应移动操纵<br />
**Authors:** Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak<br />
**Abstract:** <details><summary>原文: </summary>Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation. Video results at https://open-world-mobilemanip.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>在家庭等开放式非结构化环境中部署机器人一直是一个长期存在的研究问题。然而，机器人通常仅在封闭的实验室环境中进行研究，并且先前的移动操纵工作仅限于拾取移动放置，这可以说只是该领域的冰山一角。在本文中，我们介绍了开放世界移动操纵系统，这是一种解决现实关节对象操作的全栈方法，例如开放式非结构化环境中的真实门、橱柜、抽屉和冰箱。该机器人利用自适应学习框架，首先通过行为克隆从一小组数据中学习，然后从训练分布之外的新物体的在线实践中学习。我们还开发了一个低成本的移动操控硬件平台，能够在非结构化环境中安全、自主地在线适应，成本约为 20,000 美元。在我们的实验中，我们在 CMU 校园的 4 栋建筑中使用了 20 个铰接物体。每个对象的在线学习时间不到一个小时，系统能够利用在线适应将 BC 预训练的成功率从 50% 提高到 95%。视频结果位于 https://open-world-mobilemanip.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2401.14403v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Generalized People Diversity: Learning a Human Perception-Aligned Diversity Representation for People Images**<br />
**Title_cn:** 广义的人物多样性：学习人物图像的与人类感知一致的多样性表示<br />
**Authors:** Hansa Srinivasan, Candice Schumann, Aradhana Sinha, David Madras, Gbolahan Oluwafemi Olanubi, Alex Beutel, Susanna Ricco, Jilin Chen<br />
**Abstract:** <details><summary>原文: </summary>Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm, is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators.</details>
**Abstract_cn:** <details><summary>译文: </summary>捕捉图像中人物的多样性具有挑战性：最近的文献往往侧重于使一两个属性多样化，需要昂贵的属性标签或构建分类器。我们引入了一种多样化的人物图像排名方法，该方法以一种不那么规范、无标签的方式更灵活地符合人类对人物多样性的观念。感知对齐文本衍生的人类表示空间（PATHS）旨在捕获与人相关的多样性的所有或许多相关特征，并且当用作标准最大边际相关性（MMR）排序算法中的表示空间时，能够更好地展现一系列与人相关的多样性（例如残疾、文化服装）。 PATHS 分两个阶段创建。首先，使用文本引导方法从预先训练的图像文本模型中提取人物多样性表示。然后，根据人类注释者的感知判断对这种表示进行微调，以捕获人类认为最显着的与人相关的相似性方面。根据人类注释者的并排评分，经验结果表明 PATHS 方法比基线方法更好地实现了多样性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14322v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation**<br />
**Title_cn:** POUR-Net：用于生成低计数 PET 衰减图的群体优先辅助过度表示网络<br />
**Authors:** Bo Zhou, Jun Hou, Tianqi Chen, Yinchi Zhou, Xiongchao Chen, Huidong Xie, Qiong Liu, Xueqi Guo, Yu-Jung Tsai, Vladimir Y. Panin, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within a cascade framework enables iterative refinement of $\mu$-map generation, resulting in the production of high-quality $\mu$-maps. Experimental results underscore the effectiveness of POUR-Net, showing it as a promising solution for accurate CT-free low-count PET attenuation correction, which also surpasses the performance of previous baseline methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>低剂量 PET 提供了一种最大限度减少 PET 成像中辐射暴露的宝贵方法。然而，采用额外的 CT 扫描来生成衰减图 (u-map) 以进行 PET 衰减校正的普遍做法会显着提高辐射剂量。为了解决这一问题并进一步减轻低剂量 PET 检查中的辐射暴露，我们提出了 POUR-Net - 一种创新的人口优先辅助过度代表性网络，旨在从低剂量 PET 生成高质量的衰减图。首先，POUR-Net 结合了过度欠表示网络 (OUR-Net)，以促进高效的特征提取，涵盖低分辨率抽象和精细细节特征，以协助全分辨率级别的深度生成。其次，对 OUR-Net 进行补充，这是一种利用综合 CT 衍生的 u-map 数据集的群体先验生成机 (PPGM)，提供了额外的先验信息来帮助 OUR-Net 生成。 OUR-Net 和 PPGM 在级联框架内的集成可以迭代细化 $\mu$-map 生成，从而生成高质量的 $\mu$-map。实验结果强调了 POUR-Net 的有效性，表明它是一种有前途的精确无 CT 低计数 PET 衰减校正解决方案，这也超越了以前基线方法的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14285v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations**<br />
**Title_cn:** 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释<br />
**Authors:** Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li<br />
**Abstract:** <details><summary>原文: </summary>Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>概念瓶颈模型 (CBM) 等现有方法已成功为黑盒深度学习模型提供基于概念的解释。它们通常通过给定输入预测概念，然后根据预测概念预测最终类别标签来工作。然而，（1）它们常常无法捕捉概念之间的高阶、非线性相互作用，例如，纠正预测的概念（例如“黄胸”）无助于纠正高度相关的概念（例如“黄肚皮”），导致最终精度不佳； （2）它们无法自然地量化不同概念和类标签之间复杂的条件依赖关系（例如，对于具有类标签“Kentucky Warbler”和概念“black bill”的图像，模型正确预测另一个概念的概率是多少“黑皇冠”），因此未能更深入地了解黑盒模型的工作原理。针对这些限制，我们提出了基于能源的概念瓶颈模型（ECBM）。我们的 ECBM 使用一组神经网络来定义候选（输入、概念、类）元组的联合能量。有了这样一个统一的接口，预测、概念校正和条件依赖量化就可以表示为条件概率，这些概率是通过组合不同的能量函数生成的。我们的 ECBM 解决了现有 CBM 的局限性，提供了更高的准确性和更丰富的概念解释。实证结果表明，我们的方法优于现实世界数据集上最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.14142v1><br />
**Code:** <https://github.com/xmed-lab/ecbm>**<br />
>>**index:** 5<br />
**Title:** **Enabling Cross-Camera Collaboration for Video Analytics on Distributed Smart Cameras**<br />
**Title_cn:** 在分布式智能摄像机上实现视频分析的跨摄像机协作<br />
**Authors:** Chulhong Min, Juheon Yi, Utku Gunay Acer, Fahim Kawsar<br />
**Abstract:** <details><summary>原文: </summary>Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing visual analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to the state-of-the-art), while achieving comparable tracking quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>重叠摄像机提供了从不同角度查看场景的令人兴奋的机会，从而可以进行更高级、更全面和更强大的分析。然而，现有的多摄像机流视觉分析系统大多局限于（i）每个摄像机的处理和聚合以及（ii）与工作负载无关的集中处理架构。在本文中，我们介绍了 Argus，这是一种在智能相机上实现跨相机协作的分布式视频分析系统。我们将多摄像机、多目标跟踪确定为多摄像机视频分析的主要任务，并开发了一种新技术，通过在重叠视场中利用对象方式的时空关联来避免冗余、处理繁重的识别任务。多个相机。我们进一步开发了一套技术，可以在没有云支持的情况下以低延迟跨分布式摄像机执行这些操作，方法是：（i）动态排序摄像机和对象检查序列；（ii）在智能摄像机之间灵活分配工作负载，同时考虑到网络传输和异构计算能力。使用两个 Nvidia Jetson 设备对三个真实世界重叠相机数据集进行评估表明，Argus 将对象识别数量和端到端延迟分别减少了 7.13 倍和 2.19 倍（与状态相比分别为 4.86 倍和 1.60 倍） -最先进的），同时实现可比的跟踪质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.14132v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Incorporating Exemplar Optimization into Training with Dual Networks for Human Mesh Recovery**<br />
**Title_cn:** 将示例优化纳入双网络训练中以实现人体网格恢复<br />
**Authors:** Yongwei Nie, Mingxian Fan, Chengjiang Long, Qing Zhang, Jian Zhu, Xuemiao Xu<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel optimization-based human mesh recovery method from a single image. Given a test exemplar, previous approaches optimize the pre-trained regression network to minimize the 2D re-projection loss, which however suffer from over-/under-fitting problems. This is because the ``exemplar optimization'' at testing time has too weak relation to the pre-training process, and the exemplar optimization loss function is different from the training loss function. (1) We incorporate exemplar optimization into the training stage. During training, our method first executes exemplar optimization and subsequently proceeds with training-time optimization. The exemplar optimization may run into a wrong direction, while the subsequent training optimization serves to correct the deviation. Involved in training, the exemplar optimization learns to adapt its behavior to training data, thereby acquires generalibility to test exemplars. (2) We devise a dual-network architecture to convey the novel training paradigm, which is composed of a main regression network and an auxiliary network, in which we can formulate the exemplar optimization loss function in the same form as the training loss function. This further enhances the compatibility between the exemplar and training optimizations. Experiments demonstrate that our exemplar optimization after the novel training scheme significantly outperforms state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的基于优化的单图像人体网格恢复方法。给定一个测试样本，以前的方法优化了预训练的回归网络以最小化二维重投影损失，然而这会遇到过拟合/欠拟合问题。这是因为测试时的“样本优化”与预训练过程的关系太弱，并且样本优化损失函数与训练损失函数不同。 (1)我们将样本优化纳入训练阶段。在训练期间，我们的方法首先执行样本优化，然后进行训练时优化。样本优化可能会走错方向，而后续的训练优化则用于纠正偏差。在训练过程中，样本优化学习使其行为适应训练数据，从而获得测试样本的通用性。 （2）我们设计了一种双网络架构来传达新颖的训练范式，该架构由主回归网络和辅助网络组成，其中我们可以以与训练损失函数相同的形式制定样本优化损失函数。这进一步增强了示例和训练优化之间的兼容性。实验表明，我们在新颖的训练方案之后的示例优化明显优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.14121v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Sparse and Transferable Universal Singular Vectors Attack**<br />
**Title_cn:** 稀疏且可转移的通用奇异向量攻击<br />
**Authors:** Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets<br />
**Abstract:** <details><summary>原文: </summary>The research in the field of adversarial attacks and models' vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly transferable among different models without significantly decreasing the fooling rate. Our findings demonstrate the vulnerability of state-of-the-art models to sparse attacks and highlight the importance of developing robust machine learning systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性攻击和模型脆弱性领域的研究是现代机器学习的基本方向之一。最近的研究揭示了脆弱性现象，了解其背后的机制对于改善神经网络特性和可解释性至关重要。在本文中，我们提出了一种新颖的稀疏通用白盒对抗攻击。我们的方法基于截断幂迭代，为雅可比矩阵隐藏层的 $(p,q)$-奇异向量提供稀疏性。使用 ImageNet 基准验证子集，我们在各种设置下分析了所提出的方法，获得了与密集基线相当的结果，愚弄率超过 50%，同时仅损坏 5% 的像素，并利用 256 个样本进行扰动拟合。我们还表明，我们的算法允许更高的攻击强度，而不影响人类解决任务的能力。此外，我们研究发现，所构建的扰动在不同模型之间具有高度可转移性，而不会显着降低愚弄率。我们的研究结果证明了最先进的模型对稀疏攻击的脆弱性，并强调了开发强大的机器学习系统的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14031v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **An Extensible Framework for Open Heterogeneous Collaborative Perception**<br />
**Title_cn:** 开放异构协作感知的可扩展框架<br />
**Authors:** Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen, Yanfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. It also protects new agents' model details from disclosure since the training can be conducted by the agent owner locally. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. Code and data are available at: https://github.com/yifanlu0227/HEAL.</details>
**Abstract_cn:** <details><summary>译文: </summary>协作感知旨在通过促进多个智能体之间的数据交换来减轻单智能体感知的局限性，例如遮挡。然而，当前的大多数工作都考虑了一种同质场景，其中所有代理都使用身份传感器和感知模型。实际上，异构代理类型可能会不断出现，并且在与现有代理协作时不可避免地面临领域差距。在本文中，我们引入了一个新的开放异构问题：如何将不断出现的新异构代理类型容纳到协作感知中，同时保证高感知性能和低集成成本？为了解决这个问题，我们提出了 HEterogeneous ALLiance (HEAL)，一种新颖的可扩展协作感知框架。 HEAL 首先通过新颖的多尺度前景感知金字塔融合网络与初始代理建立统一的特征空间。当异构新代理以以前未见过的方式或模型出现时，我们通过创新的向后对齐将它们与已建立的统一空间对齐。此步骤只需要对新的Agent类型进行单独训练，因此训练成本极低，可扩展性高。它还可以保护新代理的模型详细信息不被泄露，因为培训可以由代理所有者在本地进行。为了丰富智能体的数据异构性，我们带来了 OPV2V-H，一个具有更多样化传感器类型的新大规模数据集。在 OPV2V-H 和 DAIR-V2X 数据集上的大量实验表明，HEAL 在性能上超越了 SOTA 方法，同时在集成 3 种新代理类型时将训练参数减少了 91.5%。代码和数据可在：https://github.com/yifanlu0227/HEAL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.13964v1><br />
**Code:** <https://github.com/yifanlu0227/heal>**<br />
>>**index:** 9<br />
**Title:** **Conditional Neural Video Coding with Spatial-Temporal Super-Resolution**<br />
**Title_cn:** 具有时空超分辨率的条件神经视频编码<br />
**Authors:** Henan Wang, Xiaohan Pan, Runsen Feng, Zongyu Guo, Zhibo Chen<br />
**Abstract:** <details><summary>原文: </summary>This document is an expanded version of a one-page abstract originally presented at the 2024 Data Compression Conference. It describes our proposed method for the video track of the Challenge on Learned Image Compression (CLIC) 2024. Our scheme follows the typical hybrid coding framework with some novel techniques. Firstly, we adopt Spynet network to produce accurate motion vectors for motion estimation. Secondly, we introduce the context mining scheme with conditional frame coding to fully exploit the spatial-temporal information. As for the low target bitrates given by CLIC, we integrate spatial-temporal super-resolution modules to improve rate-distortion performance. Our team name is IMCLVC.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文档是最初在 2024 年数据压缩会议上提出的一页摘要的扩展版本。它描述了我们为 2024 年学习图像压缩挑战赛 (CLIC) 的视频轨道提出的方法。我们的方案遵循典型的混合编码框架和一些新技术。首先，我们采用Spynet网络来产生精确的运动矢量用于运动估计。其次，我们引入了带有条件帧编码的上下文挖掘方案，以充分利用时空信息。针对CLIC给出的低目标码率，我们集成了时空超分辨率模块来提高码率失真性能。我们的团队名称是 IMCLVC。</details>
**PDF:** <http://arxiv.org/pdf/2401.13959v1><br />
**Code:** null<br />

