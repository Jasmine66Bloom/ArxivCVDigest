## [UPDATED!] **2024-01-26** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Deep learning-based approach for tomato classification in complex scenes**<br />
**Title_cn:** 基于深度学习的复杂场景番茄分类方法<br />
**Authors:** Mikael A. Mousse, Bethel C. A. R. K. Atohoun, Cina Motamed<br />
**Abstract:** <details><summary>原文: </summary>Tracking ripening tomatoes is time consuming and labor intensive. Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants. To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes. The objective is to detect mature tomatoes and harvest them in a timely manner. The proposed approach is declined in two parts. Firstly, the images of the scene are transmitted to the pre-processing layer. This process allows the detection of areas of interest (area of the image containing tomatoes). Then, these images are used as input to the maturity detection layer. This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red. The experiments are based on images collected from the internet gathered through searches using tomato state across diverse languages including English, German, French, and Spanish. The experimental results of the maturity detection layer on a dataset composed of images of tomatoes taken under the extreme conditions, gave a good classification rate.</details>
**Abstract_cn:** <details><summary>译文: </summary>追踪成熟的西红柿既耗时又费力。人工智能技术与计算机视觉技术相结合，可以帮助用户优化监测植物成熟状态的过程。为此，我们提出了一种基于复杂场景下深度学习的番茄成熟监测方法。目的是检测成熟的西红柿并及时收获。提议的方法分两部分被拒绝。首先，场景图像被传输到预处理层。此过程允许检测感兴趣的区域（包含西红柿的图像区域）。然后，这些图像用作成熟度检测层的输入。该层基于深度神经网络学习算法，将提供给它的番茄缩略图分为以下五类之一：绿色、脆性、粉红色、淡红色、成熟红色。这些实验基于从互联网上收集的图像，这些图像是通过使用番茄状态在英语、德语、法语和西班牙语等多种语言中进行搜索而收集的。成熟度检测层在由极端条件下拍摄的西红柿图像组成的数据集上的实验结果给出了良好的分类率。</details>
**PDF:** <http://arxiv.org/pdf/2401.15055v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings**<br />
**Title_cn:** 无法识别但可识别：保留嵌入的图像失真<br />
**Authors:** Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni<br />
**Abstract:** <details><summary>原文: </summary>In the realm of security applications, biometric authentication systems play a crucial role, yet one often encounters challenges concerning privacy and security while developing one. One of the most fundamental challenges lies in avoiding storing biometrics directly in the storage but still achieving decently high accuracy. Addressing this issue, we contribute to both artificial intelligence and engineering fields. We introduce an innovative image distortion technique that effectively renders facial images unrecognizable to the eye while maintaining their identifiability by neural network models. From the theoretical perspective, we explore how reliable state-of-the-art biometrics recognition neural networks are by checking the maximal degree of image distortion, which leaves the predicted identity unchanged. On the other hand, applying this technique demonstrates a practical solution to the engineering challenge of balancing security, precision, and performance in biometric authentication systems. Through experimenting on the widely used datasets, we assess the effectiveness of our method in preserving AI feature representation and distorting relative to conventional metrics. We also compare our method with previously used approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>在安全应用领域，生物识别系统发挥着至关重要的作用，但在开发生物识别系统时经常遇到隐私和安全方面的挑战。最基本的挑战之一在于避免将生物识别数据直接存储在存储器中，但仍能实现相当高的准确性。为了解决这个问题，我们在人工智能和工程领域做出了贡献。我们引入了一种创新的图像失真技术，可以有效地使人眼无法识别面部图像，同时通过神经网络模型保持其可识别性。从理论角度来看，我们通过检查图像失真的最大程度来探索最先进的生物识别神经网络的可靠性，这使得预测的身份保持不变。另一方面，应用该技术展示了平衡生物识别系统的安全性、精度和性能的工程挑战的实用解决方案。通过对广泛使用的数据集进行实验，我们评估了我们的方法在保留人工智能特征表示和相对于传统指标的扭曲方面的有效性。我们还将我们的方法与以前使用的方法进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.15048v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus**<br />
**Title_cn:** PARSAC：通过并行样本共识加速稳健的多模型拟合<br />
**Authors:** Florian Kluger, Bodo Rosenhahn<br />
**Abstract:** <details><summary>原文: </summary>We present a real-time method for robust estimation of multiple instances of geometric models from noisy data. Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis. Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation. In contrast, our method detects all model instances independently and in parallel. A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights. Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion. We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data. As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets. We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种实时方法，用于从噪声数据中鲁棒地估计几何模型的多个实例。消失点、平面单应性或基本矩阵等几何模型对于 3D 场景分析至关重要。以前的方法以迭代方式发现不同的模型实例，从而限制了它们通过并行计算加速的潜力。相比之下，我们的方法独立且并行地检测所有模型实例。神经网络通过预测多组样本和内点权重，将输入数据分割成代表潜在模型实例的簇。使用预测的权重，我们以类似 RANSAC 的方式分别确定每个潜在实例的模型参数。我们通过特定于任务的损失函数来训练神经网络，即我们不需要输入数据的真实分割。由于单应性和基本矩阵拟合的合适训练数据很少，我们另外提供了两个新的合成数据集。我们在这些以及多个已建立的数据集上展示了最先进的性能，每个图像的推理时间短至五毫秒。</details>
**PDF:** <http://arxiv.org/pdf/2401.14919v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Memory-Inspired Temporal Prompt Interaction for Text-Image Classification**<br />
**Title_cn:** 用于文本图像分类的受记忆启发的时间提示交互<br />
**Authors:** Xinyao Yu, Hao Sun, Ziwei Niu, Rui Qin, Zhenjia Bai, Yen-Wei Chen, Lanfen Lin<br />
**Abstract:** <details><summary>原文: </summary>In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage. We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation. The main strength of our paper is that we interact the prompt vectors on intermediate layers to leverage sufficient information exchange between modalities, with compressed trainable parameters and memory usage. We achieve competitive results on several datasets with relatively small memory usage and 2.0M of trainable parameters (about 1% of the pre-trained foundation model).</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，大规模预训练多模态模型（LMM）普遍出现，以整合视觉和语言模态，在各种自然语言处理和计算机视觉任务中取得了相当大的成功。然而，LMM 规模的不断增长导致了为下游任务微调这些模型的巨大计算成本。因此，研究了基于提示的交互策略，以更有效地调整模式。在这种背景下，我们受到人类记忆策略的启发，提出了一种新颖的基于提示的多模态交互策略，即记忆启发的时间提示交互（MITP）。我们提出的方法涉及人类记忆策略的两个阶段：获取阶段以及巩固和激活阶段。我们利用中间层上的时间提示来模拟获取阶段，利用基于相似性的提示交互来模拟记忆巩固，并采用提示生成策略来模拟记忆激活。我们论文的主要优点是我们在中间层上交互提示向量，以利用模态之间充分的信息交换，并压缩可训练参数和内存使用。我们在内存使用相对较小和 2.0M 可训练参数（约占预训练基础模型的 1%）的多个数据集上取得了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.14856v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Adaptive Point Transformer**<br />
**Title_cn:** 自适应点变压器<br />
**Authors:** Alessandro Baiocchi, Indro Spinelli, Alessandro Nicolosi, Simone Scardapane<br />
**Abstract:** <details><summary>原文: </summary>The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces computational complexity while maintaining competitive accuracy compared to standard PTs. The code for AdaPT is made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近 3D 数据采集的激增刺激了点云处理的几何深度学习模型的发展，而 Transformer 在自然语言处理方面取得的巨大成功也推动了这一发展。虽然点云变换器 (PT) 最近取得了令人印象深刻的成果，但它们相对于点云大小的二次缩放对实际应用程序提出了重大的可扩展性挑战。为了解决这个问题，我们提出了自适应点云变换器（AdaPT），这是一种通过自适应令牌选择机制增强的标准 PT 模型。 AdaPT 在推理过程中动态减少标记数量，从而实现大型点云的高效处理。此外，我们引入了一种预算机制，可以在推理时灵活调整模型的计算成本，而无需重新训练或微调单独的模型。我们对点云分类任务的广泛实验评估表明，与标准 PT 相比，AdaPT 显着降低了计算复杂性，同时保持了有竞争力的准确性。 AdaPT 的代码已公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.14845v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring**<br />
**Title_cn:** 车厢监控中基于双特征转换的多模态动作识别<br />
**Authors:** Dan Lin, Philip Hann Yung Lee, Yiming Li, Ruoyu Wang, Kim-Hui Yap, Bingbing Li, You Shing Ngim<br />
**Abstract:** <details><summary>原文: </summary>Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring systems. In real-world applications, it is common for vehicle cabins to be equipped with cameras featuring different modalities. However, multi-modality fusion strategies for the DAR task within car cabins have rarely been studied. In this paper, we propose a novel yet efficient multi-modality driver action recognition method based on dual feature shift, named DFS. DFS first integrates complementary features across modalities by performing modality feature interaction. Meanwhile, DFS achieves the neighbour feature propagation within single modalities, by feature shifting among temporal frames. To learn common patterns and improve model efficiency, DFS shares feature extracting stages among multiple modalities. Extensive experiments have been carried out to verify the effectiveness of the proposed DFS model on the Drive\&Act dataset. The results demonstrate that DFS achieves good performance and improves the efficiency of multi-modality driver action recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>驾驶员动作识别 (DAR) 在车厢监控系统中至关重要。在实际应用中，车厢内通常配备不同模式的摄像头。然而，车舱内 DAR 任务的多模态融合策略却很少被研究。在本文中，我们提出了一种基于双特征转换的新颖而有效的多模态驾驶员动作识别方法，称为DFS。 DFS 首先通过执行模态特征交互来集成跨模态的互补特征。同时，DFS 通过时间帧之间的特征移位来实现单模态内的邻近特征传播。为了学习常见模式并提高模型效率，DFS 在多种模态之间共享特征提取阶段。已经进行了大量的实验来验证所提出的 DFS 模型在 Drive\&Act 数据集上的有效性。结果表明，DFS取得了良好的性能，提高了多模态驾驶员动作识别的效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.14838v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Text Image Inpainting via Global Structure-Guided Diffusion Models**<br />
**Title_cn:** 通过全局结构引导扩散模型进行文本图像修复<br />
**Authors:** Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue<br />
**Abstract:** <details><summary>原文: </summary>Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.</details>
**Abstract_cn:** <details><summary>译文: </summary>现实世界的文本可能会因环境或人为因素引起的腐蚀问题而受到损坏，这阻碍了文本完整样式（例如纹理和结构）的保存。这些腐蚀问题，例如涂鸦标志和不完整的签名，给理解文本带来了困难，从而给下游应用（例如场景文本识别和签名识别）带来了重大挑战。值得注意的是，当前的修复技术通常无法充分解决这个问题，并且难以恢复准确的文本图像以及合理且一致的样式。本文将其表述为文本图像修复的开放问题，旨在建立一个基准来促进其研究。在此过程中，我们建立了两个特定的文本修复数据集，分别包含场景文本图像和手写文本图像。它们中的每一个都包含根据现实生活和合成数据集修改的图像，具有成对的原始图像、损坏的图像和其他辅助信息。在数据集之上，我们进一步开发了一种新颖的神经框架，即全局结构引导扩散模型（GSDM），作为潜在的解决方案。利用文本的全局结构作为先验，所提出的 GSDM 开发了一种有效的扩散模型来恢复干净的文本。我们的方法的有效性通过彻底的实证研究得到了证明，包括识别精度和图像质量的大幅提高。这些发现不仅凸显了我们方法的有效性，而且强调了其增强更广泛的文本图像理解和处理领域的潜力。代码和数据集可在以下位置获取：https://github.com/blackprotoss/GSDM。</details>
**PDF:** <http://arxiv.org/pdf/2401.14832v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition**<br />
**Title_cn:** 深度变分隐私漏斗：人脸识别应用的通用建模<br />
**Authors:** Behrooz Razeghi, Parsa Rahimi, Sébastien Marcel<br />
**Abstract:** <details><summary>原文: </summary>In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本研究中，我们利用信息论隐私漏斗（PF）模型来开发一种使用端到端训练框架来保护隐私的表示学习方法。我们严格解决混淆和实用性之间的权衡。两者都是通过对数损失来量化的，这种度量也被认为是自我信息损失。这一探索加深了信息论隐私和表征学习之间的相互作用，为判别模型和生成模型的数据保护机制提供了实质性见解。重要的是，我们将我们的模型应用于最先进的人脸识别系统。该模型展示了从原始面部图像到派生或精炼嵌入的各种输入的适应性，并且能够胜任分类、重建和生成等任务。</details>
**PDF:** <http://arxiv.org/pdf/2401.14792v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Sketch and Refine: Towards Fast and Accurate Lane Detection**<br />
**Title_cn:** 草图和细化：实现快速准确的车道检测<br />
**Authors:** Chao Chen, Jie Liu, Chang Zhou, Jie Tang, Gangshan Wu<br />
**Abstract:** <details><summary>原文: </summary>Lane detection is to determine the precise location and shape of lanes on the road. Despite efforts made by current methods, it remains a challenging task due to the complexity of real-world scenarios. Existing approaches, whether proposal-based or keypoint-based, suffer from depicting lanes effectively and efficiently. Proposal-based methods detect lanes by distinguishing and regressing a collection of proposals in a streamlined top-down way, yet lack sufficient flexibility in lane representation. Keypoint-based methods, on the other hand, construct lanes flexibly from local descriptors, which typically entail complicated post-processing. In this paper, we present a "Sketch-and-Refine" paradigm that utilizes the merits of both keypoint-based and proposal-based methods. The motivation is that local directions of lanes are semantically simple and clear. At the "Sketch" stage, local directions of keypoints can be easily estimated by fast convolutional layers. Then we can build a set of lane proposals accordingly with moderate accuracy. At the "Refine" stage, we further optimize these proposals via a novel Lane Segment Association Module (LSAM), which allows adaptive lane segment adjustment. Last but not least, we propose multi-level feature integration to enrich lane feature representations more efficiently. Based on the proposed "Sketch and Refine" paradigm, we propose a fast yet effective lane detector dubbed "SRLane". Experiments show that our SRLane can run at a fast speed (i.e., 278 FPS) while yielding an F1 score of 78.9\%. The source code is available at: https://github.com/passerer/SRLane.</details>
**Abstract_cn:** <details><summary>译文: </summary>车道检测是确定道路上车道的精确位置和形状。尽管当前方法做出了努力，但由于现实场景的复杂性，这仍然是一项具有挑战性的任务。现有的方法，无论是基于提案的还是基于关键点的，都无法有效且高效地描绘车道。基于提案的方法通过以简化的自上而下的方式区分和回归提案集合来检测车道，但在车道表示方面缺乏足够的灵活性。另一方面，基于关键点的方法可以根据局部描述符灵活地构造车道，这通常需要复杂的后处理。在本文中，我们提出了一种“草图和细化”范例，该范例利用了基于关键点和基于提案的方法的优点。其动机是车道的局部方向在语义上简单且清晰。在“草图”阶段，可以通过快速卷积层轻松估计关键点的局部方向。然后我们可以相应地建立一组具有中等精度的车道建议。在“优化”阶段，我们通过新颖的车道段关联模块（LSAM）进一步优化这些建议，该模块允许自适应车道段调整。最后但并非最不重要的一点是，我们提出多级特征集成，以更有效地丰富车道特征表示。基于所提出的“草图和细化”范例，我们提出了一种快速而有效的车道检测器，称为“SRLane”。实验表明，我们的 SRLane 可以快速运行（即 278 FPS），同时产生 78.9% 的 F1 分数。源代码位于：https://github.com/passerer/SRLane。</details>
**PDF:** <http://arxiv.org/pdf/2401.14729v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **pLitterStreet: Street Level Plastic Litter Detection and Mapping**<br />
**Title_cn:** pLitterStreet：街道塑料垃圾检测和绘图<br />
**Authors:** Sriram Reddy Mandhati, N. Lakmal Deshapriya, Chatura Lavanga Mendis, Kavinda Gunasekara, Frank Yrle, Angsana Chaksan, Sujit Sanjeev<br />
**Abstract:** <details><summary>原文: </summary>Plastic pollution is a critical environmental issue, and detecting and monitoring plastic litter is crucial to mitigate its impact. This paper presents the methodology of mapping street-level litter, focusing primarily on plastic waste and the location of trash bins. Our methodology involves employing a deep learning technique to identify litter and trash bins from street-level imagery taken by a camera mounted on a vehicle. Subsequently, we utilized heat maps to visually represent the distribution of litter and trash bins throughout cities. Additionally, we provide details about the creation of an open-source dataset ("pLitterStreet") which was developed and utilized in our approach. The dataset contains more than 13,000 fully annotated images collected from vehicle-mounted cameras and includes bounding box labels. To evaluate the effectiveness of our dataset, we tested four well known state-of-the-art object detection algorithms (Faster R-CNN, RetinaNet, YOLOv3, and YOLOv5), achieving an average precision (AP) above 40%. While the results show average metrics, our experiments demonstrated the reliability of using vehicle-mounted cameras for plastic litter mapping. The "pLitterStreet" can also be a valuable resource for researchers and practitioners to develop and further improve existing machine learning models for detecting and mapping plastic litter in an urban environment. The dataset is open-source and more details about the dataset and trained models can be found at https://github.com/gicait/pLitter.</details>
**Abstract_cn:** <details><summary>译文: </summary>塑料污染是一个严重的环境问题，检测和监测塑料垃圾对于减轻其影响至关重要。本文介绍了绘制街道垃圾图的方法，主要关注塑料垃圾和垃圾桶的位置。我们的方法涉及采用深度学习技术，从安装在车辆上的摄像头拍摄的街道图像中识别垃圾和垃圾桶。随后，我们利用热图直观地表示整个城市垃圾和垃圾桶的分布。此外，我们还提供了有关在我们的方法中开发和使用的开源数据集（“pLitterStreet”）创建的详细信息。该数据集包含从车载摄像头收集的 13,000 多张完全注释的图像，并包括边界框标签。为了评估我们数据集的有效性，我们测试了四种众所周知的最先进的目标检测算法（Faster R-CNN、RetinaNet、YOLOv3 和 YOLOv5），实现了 40% 以上的平均精度 (AP)。虽然结果显示的是平均指标，但我们的实验证明了使用车载摄像头进行塑料垃圾测绘的可靠性。 “pLitterStreet”也可以成为研究人员和从业者开发和进一步改进现有机器学习模型的宝贵资源，用于检测和绘制城市环境中的塑料垃圾。该数据集是开源的，有关该数据集和训练模型的更多详细信息可以在 https://github.com/gicait/pLitter 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.14719v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Additional Look into GAN-based Augmentation for Deep Learning COVID-19 Image Classification**<br />
**Title_cn:** 对基于 GAN 的深度学习 COVID-19 图像分类增强的进一步研究<br />
**Authors:** Oleksandr Fedoruk, Konrad Klimaszewski, Aleksander Ogonowski, Michał Kruk<br />
**Abstract:** <details><summary>原文: </summary>The availability of training data is one of the main limitations in deep learning applications for medical imaging. Data augmentation is a popular approach to overcome this problem. A new approach is a Machine Learning based augmentation, in particular usage of Generative Adversarial Networks (GAN). In this case, GANs generate images similar to the original dataset so that the overall training data amount is bigger, which leads to better performance of trained networks. A GAN model consists of two networks, a generator and a discriminator interconnected in a feedback loop which creates a competitive environment. This work is a continuation of the previous research where we trained StyleGAN2-ADA by Nvidia on the limited COVID-19 chest X-ray image dataset. In this paper, we study the dependence of the GAN-based augmentation performance on dataset size with a focus on small samples. Two datasets are considered, one with 1000 images per class (4000 images in total) and the second with 500 images per class (2000 images in total). We train StyleGAN2-ADA with both sets and then, after validating the quality of generated images, we use trained GANs as one of the augmentations approaches in multi-class classification problems. We compare the quality of the GAN-based augmentation approach to two different approaches (classical augmentation and no augmentation at all) by employing transfer learning-based classification of COVID-19 chest X-ray images. The results are quantified using different classification quality metrics and compared to the results from the literature. The GAN-based augmentation approach is found to be comparable with classical augmentation in the case of medium and large datasets but underperforms in the case of smaller datasets. The correlation between the size of the original dataset and the quality of classification is visible independently from the augmentation approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>训练数据的可用性是医学成像深度学习应用的主要限制之一。数据增强是解决这个问题的流行方法。一种新方法是基于机器学习的增强，特别是生成对抗网络（GAN）的使用。在这种情况下，GAN 会生成与原始数据集相似的图像，从而使整体训练数据量更大，从而导致训练网络的性能更好。 GAN 模型由两个网络组成，即生成器和鉴别器，它们在反馈环路中互连，从而创建竞争环境。这项工作是之前研究的延续，我们在有限的 COVID-19 胸部 X 射线图像数据集上训练了 Nvidia 的 StyleGAN2-ADA。在本文中，我们研究了基于 GAN 的增强性能对数据集大小的依赖性，重点关注小样本。考虑两个数据集，一个数据集每类 1000 个图像（总共 4000 个图像），第二个数据集每类 500 个图像（总共 2000 个图像）。我们使用这两个集合训练 StyleGAN2-ADA，然后在验证生成图像的质量后，我们使用经过训练的 GAN 作为多类分类问题的增强方法之一。我们通过采用基于迁移学习的 COVID-19 胸部 X 射线图像分类，将基于 GAN 的增强方法与两种不同方法（经典增强和根本不增强）的质量进行比较。使用不同的分类质量指标对结果进行量化，并与文献结果进行比较。研究发现，在中型和大型数据集的情况下，基于 GAN 的增强方法与经典增强方法相当，但在较小的数据集的情况下表现不佳。原始数据集的大小和分类质量之间的相关性独立于增强方法是可见的。</details>
**PDF:** <http://arxiv.org/pdf/2401.14705v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation**<br />
**Title_cn:** SSR：SAM 是用于域自适应语义分割的强正则化器<br />
**Authors:** Yanqi Ge, Ye Huang, Wen Li, Lixin Duan<br />
**Abstract:** <details><summary>原文: </summary>We introduced SSR, which utilizes SAM (segment-anything) as a strong regularizer during training, to greatly enhance the robustness of the image encoder for handling various domains. Specifically, given the fact that SAM is pre-trained with a large number of images over the internet, which cover a diverse variety of domains, the feature encoding extracted by the SAM is obviously less dependent on specific domains when compared to the traditional ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image encoder is still a mature choice of backbone for the semantic segmentation task, especially when the SAM is category-irrelevant. As a result, our SSR provides a simple yet highly effective design. It uses the ImageNet pre-trained image encoder as the backbone, and the intermediate feature of each stage (ie there are 4 stages in MiT-B5) is regularized by SAM during training. After extensive experimentation on GTA5$\rightarrow$Cityscapes, our SSR significantly improved performance over the baseline without introducing any extra inference overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 SSR，它在训练过程中利用 SAM（segment-anything）作为强大的正则化器，以大大增强图像编码器处理各种域的鲁棒性。具体来说，考虑到 SAM 是通过互联网上的大量图像进行预训练的，这些图像涵盖了不同的领域，与传统的 ImageNet 预训练相比，SAM 提取的特征编码对特定领域的依赖性明显较小。 -经过训练的图像编码器。同时，ImageNet 预训练图像编码器仍然是语义分割任务的成熟骨干选择，特别是当 SAM 与类别无关时。因此，我们的 SSR 提供​​了简单而高效的设计。它使用ImageNet预训练的图像编码器作为主干，每个阶段的中间特征（即MiT-B5中有4个阶段）在训练期间通过SAM进行正则化。经过对 GTA5$\rightarrow$Cityscapes 的广泛实验后，我们的 SSR 显着提高了基准性能，而没有引入任何额外的推理开销。</details>
**PDF:** <http://arxiv.org/pdf/2401.14686v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Multi-model learning by sequential reading of untrimmed videos for action recognition**<br />
**Title_cn:** 通过顺序读取未修剪的视频进行多模型学习以进行动作识别<br />
**Authors:** Kodai Kamiya, Toru Tamaki<br />
**Abstract:** <details><summary>原文: </summary>We propose a new method for learning videos by aggregating multiple models by sequentially extracting video clips from untrimmed video. The proposed method reduces the correlation between clips by feeding clips to multiple models in turn and synchronizes these models through federated learning. Experimental results show that the proposed method improves the performance compared to the no synchronization.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种通过从未修剪的视频中顺序提取视频剪辑来聚合多个模型来学习视频的新方法。该方法通过将剪辑依次馈送到多个模型来减少剪辑之间的相关性，并通过联邦学习同步这些模型。实验结果表明，与不同步相比，该方法提高了性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14675v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution**<br />
**Title_cn:** 从模糊到出色的检测：基于 YOLOv5 的超分辨率空中物体检测<br />
**Authors:** Ragib Amin Nihal, Benjamin Yen, Katsutoshi Itoyama, Kazuhiro Nakadai<br />
**Abstract:** <details><summary>原文: </summary>The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental results demonstrate the model's superior performance in detecting small and densely clustered objects, underlining the significance of dataset choice and architectural adaptation for this specific task. In particular, the method achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach promises to significantly advance object detection in aerial imagery, contributing to more accurate and reliable results in a variety of real-world applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着无人机和卫星技术的广泛使用，对航空图像中精确物体检测的需求激增。传统的物体检测模型是在偏向于大型物体的数据集上进行训练的，很难在小型、密集集群物体普遍存在的空中场景中发挥最佳性能。为了应对这一挑战，我们提出了一种创新方法，将超分辨率和经过调整的轻量级 YOLOv5 架构相结合。我们使用一系列数据集（包括 VisDrone-2023、SeaDroneSee、VEDAI 和 NWPU VHR-10）来评估我们模型的性能。我们的 Super Resolved YOLOv5 架构具有 Transformer 编码器块，允许模型捕获全局上下文和上下文信息，从而改善检测结果，特别是在高密度、遮挡条件下。这种轻量级模型不仅提高了准确性，而且确保了资源的高效利用，使其非常适合实时应用程序。我们的实验结果证明了该模型在检测小型且密集集群的对象方面具有卓越的性能，强调了数据集选择和架构适应对于这一特定任务的重要性。特别是，该方法在 VisDrone 上实现了 52.5% 的 mAP，超过了之前的顶级工作。这种方法有望显着推进航空图像中的目标检测，有助于在各种现实应用中获得更准确、更可靠的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.14661v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Super Efficient Neural Network for Compression Artifacts Reduction and Super Resolution**<br />
**Title_cn:** 用于减少压缩伪影和超分辨率的超高效神经网络<br />
**Authors:** Wen Ma, Qiuwen Lou, Arman Kazemi, Julian Faraone, Tariq Afzal<br />
**Abstract:** <details><summary>原文: </summary>Video quality can suffer from limited internet speed while being streamed by users. Compression artifacts start to appear when the bitrate decreases to match the available bandwidth. Existing algorithms either focus on removing the compression artifacts at the same video resolution, or on upscaling the video resolution but not removing the artifacts. Super resolution-only approaches will amplify the artifacts along with the details by default. We propose a lightweight convolutional neural network (CNN)-based algorithm which simultaneously performs artifacts reduction and super resolution (ARSR) by enhancing the feature extraction layers and designing a custom training dataset. The output of this neural network is evaluated for test streams compressed at low bitrates using variable bitrate (VBR) encoding. The output video quality shows a 4-6 increase in video multi-method assessment fusion (VMAF) score compared to traditional interpolation upscaling approaches such as Lanczos or Bicubic.</details>
**Abstract_cn:** <details><summary>译文: </summary>用户传输视频时，视频质量可能会因互联网速度有限而受到影响。当比特率降低以匹配可用带宽时，压缩伪影开始出现。现有算法要么专注于消除相同视频分辨率下的压缩伪影，要么专注于提高视频分辨率但不消除伪影。默认情况下，仅限超分辨率的方法会放大伪影以及细节。我们提出了一种基于轻量级卷积神经网络（CNN）的算法，通过增强特征提取层和设计自定义训练数据集，同时执行伪影减少和超分辨率（ARSR）。该神经网络的输出针对使用可变比特率 (VBR) 编码以低比特率压缩的测试流进行评估。与 Lanczos 或 Bicubic 等传统插值升级方法相比，输出视频质量显示视频多方法评估融合 (VMAF) 分数提高了 4-6。</details>
**PDF:** <http://arxiv.org/pdf/2401.14641v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Recognizing Multiple Ingredients in Food Images Using a Single-Ingredient Classification Model**<br />
**Title_cn:** 使用单一成分分类模型识别食品图像中的多种成分<br />
**Authors:** Kun Fu, Ying Dai<br />
**Abstract:** <details><summary>原文: </summary>Recognizing food images presents unique challenges due to the variable spatial layout and shape changes of ingredients with different cooking and cutting methods. This study introduces an advanced approach for recognizing ingredients segmented from food images. The method localizes the candidate regions of the ingredients using the locating and sliding window techniques. Then, these regions are assigned into ingredient classes using a CNN (Convolutional Neural Network)-based single-ingredient classification model trained on a dataset of single-ingredient images. To address the challenge of processing speed in multi-ingredient recognition, a novel model pruning method is proposed that enhances the efficiency of the classification model. Subsequently, the multi-ingredient identification is achieved through a decision-making scheme, incorporating two novel algorithms. The single-ingredient image dataset, designed in accordance with the book entitled "New Food Ingredients List FOODS 2021", encompasses 9982 images across 110 diverse categories, emphasizing variety in ingredient shapes. In addition, a multi-ingredient image dataset is developed to rigorously evaluate the performance of our approach. Experimental results validate the effectiveness of our method, particularly highlighting its improved capability in recognizing multiple ingredients. This marks a significant advancement in the field of food image analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于不同烹饪和切割方法下食材的空间布局和形状变化各异，识别食物图像面临着独特的挑战。这项研究引入了一种先进的方法来识别从食物图像中分割的成分。该方法使用定位和滑动窗口技术来定位成分的候选区域。然后，使用在单一成分图像数据集上训练的基于 CNN（卷积神经网络）的单一成分分类模型将这些区域分配到成分类别中。为了解决多成分识别中处理速度的挑战，提出了一种新的模型剪枝方法，可以提高分类模型的效率。随后，通过结合两种新颖算法的决策方案实现了多成分识别。该单一成分图像数据集是根据《New Food Ingredients List FOODS 2021》一书设计的，包含 110 个不同类别的 9982 张图像，强调成分形状的多样性。此外，还开发了多成分图像数据集来严格评估我们方法的性能。实验结果验证了我们方法的有效性，特别强调了其识别多种成分的能力的提高。这标志着食品图像分析领域的重大进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.14579v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **MPTQ-ViT:Mixed-PrecisionPost-TrainingQuantizationforVisionTransformer**<br />
**Title_cn:** MPTQ-ViT：VisionTransformer 的混合精度训练后量化<br />
**Authors:** Yu-Shan Tai, An-Yeu, Wu<br />
**Abstract:** <details><summary>原文: </summary>While vision transformers (ViTs) have shown great potential in computer vision tasks, their intense computation and memory requirements pose challenges for practical applications. Existing post-training quantization methods leverage value redistribution or specialized quantizers to address the non-normal distribution in ViTs. However, without considering the asymmetry in activations and relying on hand-crafted settings, these methods often struggle to maintain performance under low-bit quantization. To overcome these challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the asymmetry issue and reduce the clamping loss. We also introduce optimal scaling factor ratio search (OPT-m) to determine quantization parameters by a data-dependent mechanism automatically. To further enhance the compressibility, we incorporate the above-mentioned techniques and propose a mixed-precision post-training quantization framework for vision transformers (MPTQ-ViT). We develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise bit-width considering both model performance and compressibility. Our experiments on ViT, DeiT, and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset. Specifically, our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然视觉变换器（ViT）在计算机视觉任务中显示出巨大的潜力，但其大量的计算和内存需求给实际应用带来了挑战。现有的训练后量化方法利用值重新分配或专门的量化器来解决 ViT 中的非正态分布。然而，如果不考虑激活的不对称性并依赖于手工设置，这些方法通常很难在低位量化下保持性能。为了克服这些挑战，我们引入了带有偏置项 (SQ-b) 的 SmoothQuant，以缓解不对称问题并减少钳位损耗。我们还引入了最佳缩放因子比率搜索（OPT-m），以通过数据依赖机制自动确定量化参数。为了进一步增强可压缩性，我们结合了上述技术，提出了一种用于视觉变换器的混合精度训练后量化框架（MPTQ-ViT）。我们开发贪婪混合精度量化（Greedy MP）来分配分层位宽，同时考虑模型性能和可压缩性。我们在 ViT、DeiT 和 Swin 上的实验表明，与 ImageNet 数据集上的 SOTA 相比，准确性有了显着提高。具体来说，我们提出的方法在单精度的 4 位 ViT 上实现了 0.90% 到 23.35% 的精度改进，在混合精度的 5 位完全量化 ViT 上实现了 3.82% 到 78.14% 的精度改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.14895v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Annotated Hands for Generative Models**<br />
**Title_cn:** 生成模型的带注释的手<br />
**Authors:** Yue Yang, Atith N Gandhi, Greg Turk<br />
**Abstract:** <details><summary>原文: </summary>Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities. Despite these successes, these systems are surprisingly poor at creating images with hands. We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images. Our approach is to augment the training images with three additional channels that provide annotations to hands in the image. These annotations provide additional structure that coax the generative model to produce higher quality hand images. We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands. We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector.</details>
**Abstract_cn:** <details><summary>译文: </summary>GAN 和扩散模型等生成模型已经展示了令人印象深刻的图像生成能力。尽管取得了这些成功，但这些系统在用手创建图像方面却表现得令人惊讶。我们提出了一种新颖的生成模型训练框架，可以大大提高此类系统创建手部图像的能力。我们的方法是通过三个附加通道来增强训练图像，这些通道为图像中的手提供注释。这些注释提供了额外的结构，可以引导生成模型生成更高质量的手部图像。我们在两种不同的生成模型上演示了这种方法：生成对抗网络和扩散模型。我们在新的手部图像合成数据集和包含手部的真实照片上展示了我们的方法。我们通过使用现成的手部检测器对指关节识别进行更高的置信度来测量生成的手部质量的提高。</details>
**PDF:** <http://arxiv.org/pdf/2401.15075v1><br />
**Code:** <https://github.com/YY-GX/Annotated-Hands-Dataset>**<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities**<br />
**Title_cn:** 从 GPT-4 到 Gemini 及其他：通过四种模式评估 MLLM 的普遍性、可信度和因果关系<br />
**Authors:** Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents. However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs. We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications. To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties). In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在针对多模态内容生成合理响应方面表现出了令人印象深刻的能力。然而，尽管已经部署了最强大的OpenAI的GPT-4和Google的Gemini，但最近基于MLLM的应用程序的性能与广大公众的期望之间仍然存在很大差距。本文致力于通过对最新专有和开源 MLLM 跨四种模式（即文本、代码、图像和视频）的普遍性、可信度和因果推理能力的定性研究来增强对差距的理解。旨在提高 MLLM 的透明度。我们相信这些特性是定义 MLLM 在支持各种下游应用方面的可靠性的几个代表性因素。具体来说，我们评估了闭源 GPT-4 和 Gemini 以及 6 个开源 LLM 和 MLLM。总的来说，我们评估了 230 个手动设计的案例，然后将定性结果总结为 12 个分数（即 4 种模式乘以 3 个属性）。总的来说，我们发现了 14 项实证研究结果，有助于了解专有和开源 MLLM 的功能和局限性，从而实现更可靠的下游多模式应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.15071v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Spatial Transcriptomics Analysis of Zero-shot Gene Expression Prediction**<br />
**Title_cn:** 零样本基因表达预测的空间转录组学分析<br />
**Authors:** Yan Yang, Md Zakir Hossain, Xuesong Li, Shafin Rahman, Eric Stone<br />
**Abstract:** <details><summary>原文: </summary>Spatial transcriptomics (ST) captures gene expression within distinct regions (i.e., windows) of a tissue slide. Traditional supervised learning frameworks applied to model ST are constrained to predicting expression from slide image windows for gene types seen during training, failing to generalize to unseen gene types. To overcome this limitation, we propose a semantic guided network (SGN), a pioneering zero-shot framework for predicting gene expression from slide image windows. Considering a gene type can be described by functionality and phenotype, we dynamically embed a gene type to a vector per its functionality and phenotype, and employ this vector to project slide image windows to gene expression in feature space, unleashing zero-shot expression prediction for unseen gene types. The gene type functionality and phenotype are queried with a carefully designed prompt from a pre-trained large language model (LLM). On standard benchmark datasets, we demonstrate competitive zero-shot performance compared to past state-of-the-art supervised learning approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>空间转录组学 (ST) 捕获组织玻片不同区域（即窗口）内的基因表达。应用于模型 ST 的传统监督学习框架仅限于从幻灯片图像窗口预测训练期间看到的基因类型的表达，无法泛化到看不见的基因类型。为了克服这一限制，我们提出了一种语义引导网络（SGN），这是一种开创性的零样本框架，用于从幻灯片图像窗口预测基因表达。考虑到基因类型可以通过功能和表型来描述，我们根据其功能和表型动态地将基因类型嵌入到向量中，并利用该向量将幻灯片图像窗口投影到特征空间中的基因表达，从而实现零样本表达预测看不见的基因类型。通过预先训练的大语言模型 (LLM) 精心设计的提示来查询基因类型功能和表型。在标准基准数据集上，与过去最先进的监督学习方法相比，我们展示了具有竞争力的零样本性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.14772v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **DAM: Diffusion Activation Maximization for 3D Global Explanations**<br />
**Title_cn:** DAM：3D 全局解释的扩散激活最大化<br />
**Authors:** Hanxiao Tan<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the performance of point cloud models has been rapidly improved. However, due to the limited amount of relevant explainability studies, the unreliability and opacity of these black-box models may lead to potential risks in applications where human lives are at stake, e.g. autonomous driving or healthcare. This work proposes a DDPM-based point cloud global explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a novel point-wise symmetric model, with dual-classifier guidance to generate high-quality global explanations. In addition, an adapted path gradient integration method for DAM is proposed, which not only provides a global overview of the saliency maps for point cloud categories, but also sheds light on how the attributions of the explanations vary during the generation process. Extensive experiments indicate that our method outperforms existing ones in terms of perceptibility, representativeness, and diversity, with a significant reduction in generation time. Our code is available at: https://github.com/Explain3D/DAM</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，点云模型的性能得到了快速提升。然而，由于相关可解释性研究的数量有限，这些黑盒模型的不可靠性和不透明性可能会在危及人类生命的应用中带来潜在风险，例如：自动驾驶或医疗保健。这项工作提出了一种基于 DDPM 的点云全局可解释性方法 (DAM)，该方法利用点扩散变换器 (PDT)（一种新颖的逐点对称模型）和双分类器指导来生成高质量的全局解释。此外，还提出了一种适用于 DAM 的路径梯度积分方法，该方法不仅提供了点云类别显着性图的全局概述，而且还揭示了解释的属性在生成过程中如何变化。大量的实验表明，我们的方法在可感知性、代表性和多样性方面优于现有方法，并且生成时间显着减少。我们的代码位于：https://github.com/Explain3D/DAM</details>
**PDF:** <http://arxiv.org/pdf/2401.14938v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts**<br />
**Title_cn:** TIP-Editor：遵循文本提示和图像提示的精确 3D 编辑器<br />
**Authors:** Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan<br />
**Abstract:** <details><summary>原文: </summary>Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本驱动的 3D 场景编辑因其方便性和用户友好性而受到广泛关注。然而，由于文本描述的固有局限性，现有方法仍然缺乏对编辑结果的指定外观和位置的精确控制。为此，我们提出了一个 3D 场景编辑框架 TIPEditor，它接受文本和图像提示以及用于指定编辑区域的 3D 边界框。通过图片提示，用户可以方便地指定目标内容的详细外观/风格，与文字描述相辅相成，实现外观的精确控制。具体来说，TIP-Editor 采用逐步 2D 个性化策略来更好地学习现有场景和参考图像的表示，其中提出定位损失以鼓励边界框指定的正确对象放置。此外，TIPEditor 使用明确且灵活的 3D 高斯泼溅作为 3D 表示，以方便本地编辑，同时保持背景不变。大量的实验表明，TIP-Editor能够在指定的边界框区域内按照文本和图像提示进行准确的编辑，在编辑质量以及与提示的定性和定量对齐方面始终优于基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.14828v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning**<br />
**Title_cn:** PL-FSCIL：利用提示的力量进行少样本类增量学习<br />
**Authors:** Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Li Li, Xin Ning<br />
**Abstract:** <details><summary>原文: </summary>Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural networks to learn new tasks incrementally from a small number of labeled samples without forgetting previously learned tasks, closely mimicking human learning patterns. In this paper, we propose a novel approach called Prompt Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in conjunction with a pre-trained Vision Transformer (ViT) model to address the challenges of FSCIL effectively. Our work pioneers the use of visual prompts in FSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of two distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors that augment the model by embedding themselves into the attention layer of the ViT model. Specifically, the Domain Prompt assists the ViT model in adapting to new data domains. The task-specific FSCIL Prompt, coupled with a prototype classifier, amplifies the model's ability to effectively handle FSCIL tasks. We validate the efficacy of PL-FSCIL on widely used benchmark datasets such as CIFAR-100 and CUB-200. The results showcase competitive performance, underscoring its promising potential for real-world applications where high-quality data is often scarce. The source code is available at: https://github.com/TianSongS/PL-FSCIL.</details>
**Abstract_cn:** <details><summary>译文: </summary>Few-Shot Class-Incremental Learning (FSCIL) 旨在使深度神经网络能够从少量标记样本中增量学习新任务，而不会忘记以前学过的任务，从而密切模仿人类的学习模式。在本文中，我们提出了一种称为 FSCIL 提示学习 (PL-FSCIL) 的新颖方法，该方法利用提示的力量与预训练的 Vision Transformer (ViT) 模型相结合，有效应对 FSCIL 的挑战。我们的工作开创了在 FSCIL 中使用视觉提示的先河，其特点是非常简单。 PL-FSCIL 包含两个不同的提示：域提示和 FSCIL 提示。两者都是通过将自身嵌入到 ViT 模型的注意力层来增强模型的向量。具体来说，Domain Prompt 帮助 ViT 模型适应新的数据域。特定于任务的 FSCIL 提示与原型分类器相结合，增强了模型有效处理 FSCIL 任务的能力。我们在 CIFAR-100 和 CUB-200 等广泛使用的基准数据集上验证了 PL-FSCIL 的有效性。结果展示了具有竞争力的性能，突显了其在高质量数据往往稀缺的实际应用中的巨大潜力。源代码位于：https://github.com/TianSongS/PL-FSCIL。</details>
**PDF:** <http://arxiv.org/pdf/2401.14807v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising**<br />
**Title_cn:** VJT：一种用于去模糊、低光增强和去噪联合任务的视频转换器<br />
**Authors:** Yuxiang Hui, Yang Liu, Yaofang Liu, Fan Jia, Jinshan Pan, Raymond Chan, Tieyong Zeng<br />
**Abstract:** <details><summary>原文: </summary>Video restoration task aims to recover high-quality videos from low-quality observations. This contains various important sub-tasks, such as video denoising, deblurring and low-light enhancement, since video often faces different types of degradation, such as blur, low light, and noise. Even worse, these kinds of degradation could happen simultaneously when taking videos in extreme environments. This poses significant challenges if one wants to remove these artifacts at the same time. In this paper, to the best of our knowledge, we are the first to propose an efficient end-to-end video transformer approach for the joint task of video deblurring, low-light enhancement, and denoising. This work builds a novel multi-tier transformer where each tier uses a different level of degraded video as a target to learn the features of video effectively. Moreover, we carefully design a new tier-to-tier feature fusion scheme to learn video features incrementally and accelerate the training process with a suitable adaptive weighting scheme. We also provide a new Multiscene-Lowlight-Blur-Noise (MLBN) dataset, which is generated according to the characteristics of the joint task based on the RealBlur dataset and YouTube videos to simulate realistic scenes as far as possible. We have conducted extensive experiments, compared with many previous state-of-the-art methods, to show the effectiveness of our approach clearly.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频恢复任务旨在从低质量的观察中恢复高质量的视频。这包含各种重要的子任务，例如视频去噪、去模糊和低光增强，因为视频经常面临不同类型的退化，例如模糊、低光和噪声。更糟糕的是，在极端环境中拍摄视频时，这些退化可能会同时发生。如果想要同时删除这些伪影，这会带来重大挑战。在本文中，据我们所知，我们首次提出了一种高效的端到端视频变换器方法，用于视频去模糊、低光增强和去噪的联合任务。这项工作构建了一种新颖的多层变压器，其中每一层使用不同级别的降级视频作为目标来有效地学习视频的特征。此外，我们精心设计了一种新的分层特征融合方案，以增量学习视频特征，并通过合适的自适应加权方案加速训练过程。我们还提供了新的Multiscene-Lowlight-Blur-Noise（MLBN）数据集，该数据集是根据基于RealBlur数据集和YouTube视频的联合任务的特点生成的，以尽可能模拟真实的场景。我们进行了广泛的实验，与许多以前最先进的方法进行了比较，以清楚地表明我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14754v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes**<br />
**Title_cn:** 基于能量的模型平衡的拓扑感知探索：Toric QC-LDPC 码和双曲 MET QC-LDPC 码<br />
**Authors:** Vasiliy Usatyuk, Denis Sapozhnikov, Sergey Egorov<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyperboloid, paving the way for the application of our methodology. Practical applications of our approach are investigated in Finite Geometry QC-LDPC Codes, specifically in Material Science. The paper further explores its effectiveness in the realm of Natural Language Processing Transformer Deep Neural Networks, examining Generalized Repeat Accumulate Codes, Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium method is showcased across diverse scientific domains without the use of specific section delineations.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种在 ISING 哈密顿量遇到不规则网格上分布不均匀的电荷时实现平衡的方法。我们的方法采用（多边）QC-LDPC 码和玻尔兹曼机，包括对系统进行维度扩展、用循环代替电荷以及通过循环移位表示距离。这导致电荷系统到空间的系统映射，将不规则网格转变为均匀配置，适用于环面和圆形双曲面拓扑。本文涵盖了与 QC-LDPC 码、多边 QC-LDPC 码和玻尔兹曼机相关的基本定义和符号。它探讨了用于评估配分函数的图概率模型代码中的边缘化问题，包括精确和近似估计技术。为玻尔兹曼机在环双曲面和圆双曲面下达到平衡态提供了严格的证明，为我们的方法的应用铺平了道路。我们的方法的实际应用在有限几何 QC-LDPC 代码中进行了研究，特别是在材料科学中。本文进一步探讨了其在自然语言处理 Transformer 深度神经网络领域的有效性，研究了广义重复累积码、空间耦合和笼图 QC-LDPC 码。我们的拓扑感知硬件高效准循环代码平衡方法的多功能性和影响力在不同的科学领域得到了展示，而无需使用特定的部分描述。</details>
**PDF:** <http://arxiv.org/pdf/2401.14749v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring**<br />
**Title_cn:** 学习森林结构的神经辐射场以进行可扩展和精细监控<br />
**Authors:** Juan Castorena<br />
**Abstract:** <details><summary>原文: </summary>This work leverages neural radiance fields and remote sensing for forestry applications. Here, we show neural radiance fields offer a wide range of possibilities to improve upon existing remote sensing methods in forest monitoring. We present experiments that demonstrate their potential to: (1) express fine features of forest 3D structure, (2) fuse available remote sensing modalities and (3), improve upon 3D structure derived forest metrics. Altogether, these properties make neural fields an attractive computational tool with great potential to further advance the scalability and accuracy of forest monitoring programs.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作利用神经辐射场和遥感进行林业应用。在这里，我们展示了神经辐射场为改进森林监测中现有遥感方法提供了广泛的可能性。我们提出的实验证明了它们的潜力：(1) 表达森林 3D 结构的精细特征，(2) 融合可用的遥感模式，以及 (3) 改进 3D 结构衍生的森林指标。总而言之，这些特性使神经场成为一种有吸引力的计算工具，具有进一步提高森林监测项目的可扩展性和准确性的巨大潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.15029v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field**<br />
**Title_cn:** 基于双神经辐射场的室内环境3D重建与新视图合成<br />
**Authors:** Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu<br />
**Abstract:** <details><summary>原文: </summary>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency.</details>
**Abstract_cn:** <details><summary>译文: </summary>同时实现室内环境的 3D 重建和新视图合成具有广泛的应用，但在技术上非常具有挑战性。基于隐式神经函数的最先进方法可以实现出色的 3D 重建结果，但它们在新视图合成上的性能可能无法令人满意。神经辐射场 (NeRF) 令人兴奋的发展彻底改变了新的视图合成，然而，基于 NeRF 的模型可能无法重建干净的几何表面。我们开发了双神经辐射场（Du-NeRF）来同时实现高质量的几何重建和视图渲染。 Du-NeRF 包含两个几何场，一个源自 SDF 场以促进几何重建，另一个源自密度场以促进新视图合成。 Du-NeRF的创新特点之一是它将与视图无关的分量从密度场中解耦出来，并将其作为标签来监督SDF场的学习过程。这减少了形状-辐射度的模糊性，并使几何和颜色在学习过程中相互受益。大量实验表明，Du-NeRF 可以显着提高室内环境的新颖视图合成和 3D 重建的性能，并且在构建包含不遵守多视图颜色一致性的精细几何形状的区域时特别有效。</details>
**PDF:** <http://arxiv.org/pdf/2401.14726v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Implicit Neural Representation for Physics-driven Actuated Soft Bodies**<br />
**Title_cn:** 物理驱动驱动软体的隐式神经表示<br />
**Authors:** Lingchen Yang, Byungsoo Kim, Gaspard Zoss, Baran Gözcü, Markus Gross, Barbara Solenthaler<br />
**Abstract:** <details><summary>原文: </summary>Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.</details>
**Abstract_cn:** <details><summary>译文: </summary>主动软体可以通过引起变形的内部驱动机制影响其形状。与最近的工作类似，本文利用可微分、准静态和基于物理的模拟层来优化由神经网络参数化的驱动信号。我们的主要贡献是通过定义一个函数来控制主动软体的通用隐式公式，该函数能够实现从材料空间中的空间点到驱动值的连续映射。这一特性使我们能够捕获信号的主频率，从而使该方法与离散化无关并具有广泛的适用性。我们将隐式模型扩展到下颌运动学的特定情况下的面部动画，并表明我们可以可靠地再现用高质量捕获系统捕获的面部表情。我们将该方法应用于体积软体、人体姿势和面部表情，展示了艺术家友好的特性，例如对潜在空间的简单控制和测试时的分辨率不变性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14861v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras**<br />
**Title_cn:** SimpleEgo：从以自我为中心的相机预测概率身体姿势<br />
**Authors:** Hanz Cuevas-Velasquez, Charlie Hewitt, Sadegh Aliakbarian, Tadas Baltrušaitis<br />
**Abstract:** <details><summary>原文: </summary>Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD). This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded. Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these can present hardware design issues. They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs. We predict pose from images captured with conventional rectilinear camera lenses. This resolves hardware design issues, but means body parts are often out of frame. As such, we directly regress probabilistic joint rotations represented as matrix Fisher distributions for a parameterized body model. This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints. This also removes the need to compute 2D heat-maps and allows for simplified DNN architectures which require less compute. Given the lack of egocentric datasets using rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic dataset with 60K stereo images containing high diversity of pose, shape, clothing and skin tone. Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23% overall and 58% for the lower body. Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art. Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们的工作解决了头戴式设备 (HMD) 上的朝下摄像头以自我为中心的人体姿势估计问题。这提出了一个具有挑战性的场景，因为身体的某些部分经常落在图像之外或被遮挡。以前的解决方案通过使用鱼眼相机镜头捕捉更广阔的视野来最大限度地减少这个问题，但这些可能会带来硬件设计问题。他们还预测每个关节的 2D 热图并将其提升到 3D 空间以处理自遮挡，但这需要大型网络架构，而这在资源有限的 HMD 上部署是不切实际的。我们根据传统直线相机镜头拍摄的图像来预测姿势。这解决了硬件设计问题，但意味着身体部位经常脱离框架。因此，我们直接对参数化身体模型的表示为矩阵费舍尔分布的概率关节旋转进行回归。这使我们能够量化姿势的不确定性并解释框架外或闭塞的关节。这也消除了计算 2D 热图的需要，并允许需要更少计算的简化 DNN 架构。鉴于缺乏使用直线相机镜头的以自我为中心的数据集，我们引入了 SynthEgo 数据集，这是一个包含 60K 立体图像的合成数据集，其中包含高度多样性的姿势、形状、服装和肤色。我们的方法在这种具有挑战性的配置中取得了最先进的结果，将每个关节的平均位置误差总体降低了 23%，下半身降低了 58%。我们的架构的参数数量减少了八倍，运行速度是当前最先进技术的两倍。实验表明，对我们的合成数据集进行训练可以很好地概括现实世界的图像，而无需进行微调。</details>
**PDF:** <http://arxiv.org/pdf/2401.14785v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Personality Perception in Human Videos Altered by Motion Transfer Networks**<br />
**Title_cn:** 运动传输网络改变人类视频中的个性感知<br />
**Authors:** Ayda Yurtoğlu, Sinan Sonlu, Yalım Doğan, Uğur Güdükbay<br />
**Abstract:** <details><summary>原文: </summary>The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字角色的成功个性刻画可以改善沟通和沉浸感。目前的研究重点是通过使用启发式规则或数据驱动模型修改动画来表达个性。虽然研究表明动作风格对表面个性有很大影响，但外表的作用也同样重要。这项工作分析了运动和外观对运动传输网络改变的短视频感知个性的影响。我们通过用户研究来标记会议视频剪辑中的人物，以确定最能代表五因素模型的高、中和低特征的样本。我们使用薄板样条运动模型修改这些视频，利用选定的样本作为源和驱动输入。我们按照五个不同的案例来研究运动和外表对人格感知的影响。我们的比较研究表明，运动和外观影响不同的因素：运动强烈影响感知的外向性，而外观有助于传达宜人性和神经质。</details>
**PDF:** <http://arxiv.org/pdf/2401.14733v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Masked Pre-trained Model Enables Universal Zero-shot Denoiser**<br />
**Title_cn:** 掩蔽预训练模型可实现通用零样本降噪器<br />
**Authors:** Xiaoxiao Ma, Zhixiang Wei, Yi Jin, Pengyang Ling, Tianle Liu, Ben Wang, Junkang Dai, Huaian Chen, Enhong Chen<br />
**Abstract:** <details><summary>原文: </summary>In this work, we observe that the model, which is trained on vast general images using masking strategy, has been naturally embedded with the distribution knowledge regarding natural images, and thus spontaneously attains the underlying potential for strong image denoising. Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for denoising of a single image with unseen noise degradation. Concretely, the proposed MPI comprises two key procedures: 1) Masked Pre-training involves training a model on multiple natural images with random masks to gather generalizable representations, allowing for practical applications in varying noise degradation and even in distinct image types. 2) Iterative filling is devised to efficiently fuse pre-trained knowledge for denoising. Similar to but distinct from pre-training, random masking is retained to bridge the gap, but only the predicted parts covered by masks are assembled for efficiency, which enables high-quality denoising within a limited number of iterations. Comprehensive experiments across various noisy scenarios underscore the notable advances of proposed MPI over previous approaches with a marked reduction in inference time. Code is available at https://github.com/krennic999/MPI.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们观察到该模型使用掩蔽策略在大量普通图像上进行训练，自然地嵌入了有关自然图像的分布知识，从而自发地获得了强图像去噪的潜在潜力。基于这一观察，我们提出了一种新颖的零样本去噪范例，即掩模预训练然后迭代填充（MPI）。 MPI 通过掩蔽来预训练模型，并对其进行微调，以对单个图像进行去噪，从而实现看不见的噪声退化。具体来说，所提出的 MPI 包括两个关键过程：1）掩码预训练涉及使用随机掩码在多个自然图像上训练模型以收集可概括的表示，从而允许在不同的噪声退化甚至不同的图像类型中进行实际应用。 2）迭代填充旨在有效融合预先训练的知识以进行去噪。与预训练类似但不同的是，保留了随机掩蔽以弥补差距，但为了提高效率，仅组装掩蔽覆盖的预测部分，从而在有限的迭代次数内实现高质量的去噪。跨各种噪声场景的综合实验强调了所提出的 MPI 相对于以前的方法的显着进步，并且推理时间显着减少。代码可在 https://github.com/krennic999/MPI.git 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.14966v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Machine learning-based analysis of glioma tissue sections: a review**<br />
**Title_cn:** 基于机器学习的神经胶质瘤组织切片分析：综述<br />
**Authors:** Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S. Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea Eberle, Stefan Nikolin, Arno Appenzeller, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in isolation (10/70) or in addition to the TCGA datasets (11/70). Current approaches mostly rely on convolutional neural networks (53/70) for analyzing tissue at 20x magnification (30/70). A new field of research is the integration of clinical data, omics data, or magnetic resonance imaging (27/70). So far, machine learning-based methods have achieved promising results, but are not yet used in real clinical settings. Future work should focus on the independent validation of methods on larger, multi-site datasets with high-quality and up-to-date clinical and molecular pathology annotations to demonstrate routine applicability.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，神经胶质瘤的诊断变得越来越复杂。使用现代机器学习技术对神经胶质瘤组织进行组织学评估为支持诊断和结果预测提供了新的机会。为了概述当前的研究状况，本综述审查了 70 项公开的研究，这些研究涉及基于机器学习的染色人类神经胶质瘤组织切片的分析，涵盖亚型分型 (16/70)、分级 (23/70) 的诊断任务、分子标记预测（13/70）和生存预测（27/70）。所有研究均在方法学方面以及临床适用性方面进行了审查。研究发现，目前研究的重点是评估成人型弥漫性胶质瘤的苏木精和伊红染色组织切片。大多数研究 (49/70) 基于癌症基因组图谱 (TCGA) 中公开的胶质母细胞瘤和低级别胶质瘤数据集，只有少数研究单独使用其他数据集 (10/70) 或除TCGA 数据集 (11/70)。当前的方法主要依靠卷积神经网络 (53/70) 以 20 倍放大倍率 (30/70) 分析组织。一个新的研究领域是临床数据、组学数据或磁共振成像的整合 (27/70)。到目前为止，基于机器学习的方法已经取得了有希望的结果，但尚未应用于实际的临床环境。未来的工作应侧重于在更大的多站点数据集上对方法进行独立验证，并提供高质量和最新的临床和分子病理学注释，以证明常规适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15022v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning**<br />
**Title_cn:** BackdoorBench：后门学习的综合基准和分析<br />
**Authors:** Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen<br />
**Abstract:** <details><summary>原文: </summary>As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为研究深度神经网络（DNN）漏洞的新兴重要课题，后门学习近年来引起了越来越多的关注，许多开创性的后门攻击和防御算法正在相继或同时开发，处于快速军备竞赛的状态。然而，主要由于设置的多样性，以及现有作品的实施和再现性的困难，缺乏统一和规范的后门学习基准，导致不公平的比较和不可靠的结论（例如误导、有偏见甚至错误的结论）结论）。因此，很难评估该文献当前的进展并设计未来的发展路线。为了缓解这种困境，我们建立了一个名为 BackdoorBench 的综合后门学习基准。我们的基准为研究界做出了三项宝贵的贡献。 1）我们基于可扩展的模块化代码库，提供最先进（SOTA）后门学习算法（目前包括 16 种攻击算法和 27 种防御算法）的集成实现。 2）基于4个模型和4个数据集，我们对12种攻击对16种防御、5种中毒率进行综合评估，总共11,492对评估。 3）基于上述评估，我们通过18个有用的分析工具从8个角度进行了丰富的分析，并提供了一些关于后门学习的启发性见解。我们希望我们的努力能够为后门学习奠定坚实的基础，方便研究人员研究现有算法，开发更多创新算法，探索后门学习的内在机制。最后，我们创建了一个用户友好的网站http://backdoorbench.com，该网站收集了BackdoorBench的所有重要信息，包括代码库、文档、排行榜和模型Zoo。</details>
**PDF:** <http://arxiv.org/pdf/2401.15002v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training**<br />
**Title_cn:** 保存-更新-修订以解决对抗性训练中的泛化性和鲁棒性权衡<br />
**Authors:** Shruthi Gowda, Bahram Zonooz, Elahe Arani<br />
**Abstract:** <details><summary>原文: </summary>Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating "robust overfitting". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性训练提高了神经网络对抗对抗性攻击的鲁棒性，尽管是以牺牲标准泛化和鲁棒泛化之间的权衡为代价的。为了揭示驱动这一现象的潜在因素，我们研究了神经网络在从标准环境过渡到对抗环境期间的分层学习能力。我们的实证研究结果表明，有选择地更新特定层同时保留其他层可以大大增强网络的学习能力。因此，我们提出了 CURE，一种新颖的训练框架，它利用梯度突出标准来执行权重的选择性保存、更新和修订。重要的是，CURE 的设计与数据集和架构无关，确保其在各种场景中的适用性。它有效地解决了记忆和过度拟合问题，从而增强了鲁棒性和泛化性之间的权衡，此外，这种训练方法还有助于减轻“鲁棒过度拟合”。此外，我们的研究为选择性对抗训练的机制提供了宝贵的见解，并为未来的研究提供了一条有希望的途径。</details>
**PDF:** <http://arxiv.org/pdf/2401.14948v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Understanding Domain Generalization: A Noise Robustness Perspective**<br />
**Title_cn:** 理解域泛化：噪声鲁棒性视角<br />
**Authors:** Rui Qiao, Bryan Kian Hsiang Low<br />
**Abstract:** <details><summary>原文: </summary>Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管领域泛化（DG）的机器学习算法发展迅速，但没有明确的经验证据表明现有的 DG 算法在标准基准上优于经典的经验风险最小化（ERM）。为了更好地理解这一现象，我们从标签噪声的角度研究了 DG 算法相对于 ERM 是否有优势。具体来说，我们的有限样本分析表明，标签噪声加剧了 ERM 的虚假相关性的影响，从而破坏了泛化能力。相反，我们说明即使存在虚假相关性，DG 算法在有限样本训练期间也表现出隐式标签噪声鲁棒性。这种理想的特性有助于减轻虚假相关性并提高合成实验中的泛化能力。然而，对现实世界基准数据集的其他综合实验表明，与 ERM 相比，标签噪声鲁棒性并不一定会转化为更好的性能。我们推测，由虚假相关性引起的 ERM 失效模式在实践中可能不太明显。</details>
**PDF:** <http://arxiv.org/pdf/2401.14846v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances**<br />
**Title_cn:** 机器视觉冰山解释：通过考虑整体环境情况推进动态测试<br />
**Authors:** Hubert Padusinski, Thilo Braun, Christian Steinhauser, Lennart Ries, Eric Sax<br />
**Abstract:** <details><summary>原文: </summary>Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of our model delivers a structured exploration of entities in a specific domain, their relationships and assigning results of a MV-under-test to construct an entity-relationship graph. Through clustering patterns of relations in the graph general MV deficits are arguable. In Summary, our work contributes to a more nuanced and systematized identification of deficits of a MV test object in correlation to holistic circumstances in HAD operating domains.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的机器视觉测试是否正在走向冰山？这项工作深入研究了机器视觉 (MV) 测试的领域，这在高度自动驾驶 (HAD) 系统中是非常需要的。利用走向冰山的隐喻概念，我们讨论了当前测试策略中隐藏的潜在缺陷。我们强调迫切需要更深入地了解如何处理开发过程中 MV 的不透明功能。忽视这些因素可能会导致生命损失。我们的主要贡献是层次模型，我们称之为粒度等级。该模型鼓励对 MV 预期运行环境的多尺度深度进行精细探索。该模型旨在提供可能影响 MV 功能的所有实体的整体概述，范围从对象属性等单个实体的关系到整个环境场景。我们的模型的应用提供了对特定领域中的实体及其关系的结构化探索，并分配被测 MV 的结果以构建实体关系图。通过图中关系的聚类模式，一般 MV 缺陷是有争议的。总之，我们的工作有助于更细致、更系统地识别 MV 测试对象与 HAD 操作域整体环境相关的缺陷。</details>
**PDF:** <http://arxiv.org/pdf/2401.14831v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Study of the gOMP Algorithm for Recovery of Compressed Sensed Hyperspectral Images**<br />
**Title_cn:** 压缩感知高光谱图像恢复的gOMP算法研究<br />
**Authors:** Jon Alvarez Justo, Milica Orlandic<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral Imaging (HSI) is used in a wide range of applications such as remote sensing, yet the transmission of the HS images by communication data links becomes challenging due to the large number of spectral bands that the HS images contain together with the limited data bandwidth available in real applications. Compressive Sensing reduces the images by randomly subsampling the spectral bands of each spatial pixel and then it performs the image reconstruction of all the bands using recovery algorithms which impose sparsity in a certain transform domain. Since the image pixels are not strictly sparse, this work studies a data sparsification pre-processing stage prior to compression to ensure the sparsity of the pixels. The sparsified images are compressed $2.5\times$ and then recovered using the Generalized Orthogonal Matching Pursuit algorithm (gOMP) characterized by high accuracy, low computational requirements and fast convergence. The experiments are performed in five conventional hyperspectral images where the effect of different sparsification levels in the quality of the uncompressed as well as the recovered images is studied. It is concluded that the gOMP algorithm reconstructs the hyperspectral images with higher accuracy as well as faster convergence when the pixels are highly sparsified and hence at the expense of reducing the quality of the recovered images with respect to the original images.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱成像 (HSI) 广泛应用于遥感等领域，但由于 HS 图像包含大量光谱带以及有限的数据带宽，因此通过通信数据链路传输 HS 图像变得具有挑战性在实际应用中可用。压缩感知通过对每个空间像素的光谱带进行随机二次采样来减少图像，然后使用在特定变换域中施加稀疏性的恢复算法对所有频带进行图像重建。由于图像像素并不是严格稀疏的，因此这项工作研究了压缩之前的数据稀疏预处理阶段，以确保像素的稀疏性。稀疏图像被压缩 $2.5\times$，然后使用广义正交匹配追踪算法 (gOMP) 进行恢复，该算法具有精度高、计算要求低和收敛速度快的特点。实验在五幅传统高光谱图像中进行，研究了不同稀疏化级别对未压缩图像和恢复图像质量的影响。结论是，当像素高度稀疏时，gOMP 算法能够以更高的精度和更快的收敛速度重建高光谱图像，因此会降低恢复图像相对于原始图像的质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.14786v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Comparative Study of Compressive Sensing Algorithms for Hyperspectral Imaging Reconstruction**<br />
**Title_cn:** 高光谱成像重建压缩感知算法的比较研究<br />
**Authors:** Jon Alvarez Justo, Daniela Lupu, Milica Orlandic, Ion Necoara, Tor Arne Johansen<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral Imaging comprises excessive data consequently leading to significant challenges for data processing, storage and transmission. Compressive Sensing has been used in the field of Hyperspectral Imaging as a technique to compress the large amount of data. This work addresses the recovery of hyperspectral images 2.5x compressed. A comparative study in terms of the accuracy and the performance of the convex FISTA/ADMM in addition to the greedy gOMP/BIHT/CoSaMP recovery algorithms is presented. The results indicate that the algorithms recover successfully the compressed data, yet the gOMP algorithm achieves superior accuracy and faster recovery in comparison to the other algorithms at the expense of high dependence on unknown sparsity level of the data to recover.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱成像包含过多的数据，因此给数据处理、存储和传输带来了重大挑战。压缩感知作为一种压缩大量数据的技术已被用于高光谱成像领域。这项工作致力于恢复 2.5 倍压缩的高光谱图像。除了贪婪的 gOMP/BIHT/CoSaMP 恢复算法外，还对凸 FISTA/ADMM 的准确性和性能进行了比较研究。结果表明，该算法成功地恢复了压缩数据，但与其他算法相比，gOMP 算法实现了更高的准确性和更快的恢复速度，但代价是高度依赖于要恢复的数据的未知稀疏程度。</details>
**PDF:** <http://arxiv.org/pdf/2401.14762v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **A Survey on Video Prediction: From Deterministic to Generative Approaches**<br />
**Title_cn:** 视频预测调查：从确定性方法到生成方法<br />
**Authors:** Ruibo Ming, Zhewei Huang, Zhuoxuan Ju, Jianming Hu, Lihui Peng, Shuchang Zhou<br />
**Abstract:** <details><summary>原文: </summary>Video prediction, a fundamental task in computer vision, aims to enable models to generate sequences of future frames based on existing video content. This task has garnered widespread application across various domains. In this paper, we comprehensively survey both historical and contemporary works in this field, encompassing the most widely used datasets and algorithms. Our survey scrutinizes the challenges and evolving landscape of video prediction within the realm of computer vision. We propose a novel taxonomy centered on the stochastic nature of video prediction algorithms. This taxonomy accentuates the gradual transition from deterministic to generative prediction methodologies, underlining significant advancements and shifts in approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频预测是计算机视觉中的一项基本任务，旨在使模型能够根据现有视频内容生成未来帧的序列。这项任务已在各个领域获得了广泛的应用。在本文中，我们全面调查了该领域的历史和当代作品，涵盖最广泛使用的数据集和算法。我们的调查仔细审视了计算机视觉领域内视频预测的挑战和不断发展的前景。我们提出了一种以视频预测算法的随机性质为中心的新颖分类法。这种分类强调了从确定性预测方法到生成性预测方法的逐渐过渡，强调了方法的重大进步和转变。</details>
**PDF:** <http://arxiv.org/pdf/2401.14718v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement**<br />
**Title_cn:** 通过特征解缠来缩小对抗鲁棒性的特征差距<br />
**Authors:** Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical evaluations on three benchmark datasets demonstrate that our approach surpasses existing adversarial fine-tuning methods and adversarial training baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络很容易受到对抗性样本的影响。对抗性微调方法旨在通过以对抗性训练的方式微调自然预训练的模型来增强对抗性的鲁棒性。然而，我们发现对抗样本的一些潜在特征被对抗扰动所混淆，并导致自然样本和对抗样本的最后一个隐藏层的特征之间的差距意外增大。为了解决这个问题，我们提出了一种基于解缠结的方法来显式建模并进一步消除导致特征差距的潜在特征。具体来说，我们引入了一个特征解缠器，将潜在特征从对抗样本的特征中分离出来，从而通过消除潜在特征来提高鲁棒性。此外，我们将预训练模型中的特征与微调模型中对抗样本的特征进行对齐，以进一步受益于自然样本的特征而不会混淆。对三个基准数据集的实证评估表明，我们的方法超越了现有的对抗性微调方法和对抗性训练基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.14707v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning**<br />
**Title_cn:** 通过知识件上下文即时学习实现终身场景图生成<br />
**Authors:** Tao He, Tongtong Wu, Dongyang Zhang, Guiduo Duan, Ke Qin, Yuan-Fang Li<br />
**Abstract:** <details><summary>原文: </summary>Scene graph generation (SGG) endeavors to predict visual relationships between pairs of objects within an image. Prevailing SGG methods traditionally assume a one-off learning process for SGG. This conventional paradigm may necessitate repetitive training on all previously observed samples whenever new relationships emerge, mitigating the risk of forgetting previously acquired knowledge. This work seeks to address this pitfall inherent in a suite of prior relationship predictions. Motivated by the achievements of in-context learning in pretrained language models, our approach imbues the model with the capability to predict relationships and continuously acquire novel knowledge without succumbing to catastrophic forgetting. To achieve this goal, we introduce a novel and pragmatic framework for scene graph generation, namely Lifelong Scene Graph Generation (LSGG), where tasks, such as predicates, unfold in a streaming fashion. In this framework, the model is constrained to exclusive training on the present task, devoid of access to previously encountered training data, except for a limited number of exemplars, but the model is tasked with inferring all predicates it has encountered thus far. Rigorous experiments demonstrate the superiority of our proposed method over state-of-the-art SGG models in the context of LSGG across a diverse array of metrics. Besides, extensive experiments on the two mainstream benchmark datasets, VG and Open-Image(v6), show the superiority of our proposed model to a number of competitive SGG models in terms of continuous learning and conventional settings. Moreover, comprehensive ablation experiments demonstrate the effectiveness of each component in our model.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景图生成（SGG）致力于预测图像中对象对之间的视觉关系。传统上，流行的 SGG 方法假设 SGG 是一次性学习过程。每当出现新关系时，这种传统范式可能需要对所有先前观察到的样本进行重复训练，从而减轻忘记先前获得的知识的风险。这项工作旨在解决一系列先验关系预测中固有的陷阱。受预训练语言模型中上下文学习取得的成就的激励，我们的方法使模型具有预测关系并不断获取新知识而不会屈服于灾难性遗忘的能力。为了实现这一目标，我们引入了一种新颖且实用的场景图生成框架，即终身场景图生成（LSGG），其中谓词等任务以流式方式展开。在此框架中，模型仅限于对当前任务进行排他性训练，除了有限数量的样本外，无法访问以前遇到的训练数据，但模型的任务是推断迄今为止遇到的所有谓词。严格的实验证明了我们提出的方法在 LSGG 的背景下在各种指标上优于最先进的 SGG 模型。此外，对两个主流基准数据集 VG 和 Open-Image(v6) 的大量实验表明，我们提出的模型在持续学习和常规设置方面优于许多竞争性 SGG 模型。此外，全面的消融实验证明了我们模型中每个组件的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.14626v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **CNA-TTA: Clean and Noisy Region Aware Feature Learning within Clusters for Online-Offline Test-Time Adaptation**<br />
**Title_cn:** CNA-TTA：集群内的干净和嘈杂区域感知特征学习，用于在线离线测试时间适应<br />
**Authors:** Hyeonwoo Cho, Chanmin Park, Jinyoung Kim, Won Hwa Kim<br />
**Abstract:** <details><summary>原文: </summary>A domain shift occurs when training (source) and test (target) data diverge in their distribution. Test-time adaptation (TTA) addresses the domain shift problem, aiming to adopt a trained model on the source domain to the target domain in a scenario where only a well-trained source model and unlabeled target data are available. In this scenario, handling false labels in the target domain is crucial because they negatively impact the model performance. To deal with this problem, we propose to utilize cluster structure (i.e., {`Clean'} and {`Noisy'} regions within each cluster) in the target domain formulated by the source model. Given an initial clustering of target samples, we first partition clusters into {`Clean'} and {`Noisy'} regions defined based on cluster prototype (i.e., centroid of each cluster). As these regions have totally different distributions of the true pseudo-labels, we adopt distinct training strategies for the clean and noisy regions: we selectively train the target with clean pseudo-labels in the clean region, whereas we introduce mixup inputs representing intermediate features between clean and noisy regions to increase the compactness of the cluster. We conducted extensive experiments on multiple datasets in online/offline TTA settings, whose results demonstrate that our method, {CNA-TTA}, achieves state-of-the-art for most cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>当训练（源）和测试（目标）数据的分布出现分歧时，就会发生域转移。测试时适应（TTA）解决了域转移问题，旨在在只有经过良好训练的源模型和未标记的目标数据可用的情况下，将源域上经过训练的模型采用到目标域。在这种情况下，处理目标域中的错误标签至关重要，因为它们会对模型性能产生负面影响。为了解决这个问题，我们建议在源模型制定的目标域中利用集群结构（即每个集群内的{`Clean'}和{`Noisy'}区域）。给定目标样本的初始聚类，我们首先将聚类划分为基于聚类原型（即每个聚类的质心）定义的“干净”和“嘈杂”区域。由于这些区域具有完全不同的真实伪标签分布，因此我们对干净区域和噪声区域采用不同的训练策略：我们在干净区域中选择性地使用干净伪标签训练目标，而我们引入代表中间特征的混合输入干净和嘈杂的区域，以增加集群的紧凑性。我们在在线/离线 TTA 设置中对多个数据集进行了广泛的实验，其结果表明我们的方法 {CNA-TTA} 在大多数情况下都达到了最先进的水平。</details>
**PDF:** <http://arxiv.org/pdf/2401.14587v1><br />
**Code:** null<br />

