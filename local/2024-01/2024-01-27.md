## [UPDATED!] **2024-01-27** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Exploring the Transferability of a Foundation Model for Fundus Images: Application to Hypertensive Retinopathy**<br />
**Title_cn:** 探索眼底图像基础模型的可移植性：在高血压性视网膜病变中的应用<br />
**Authors:** Julio Silva-Rodriguez, Jihed Chelbi, Waziha Kabir, Hadi Chakor, Jose Dolz, Ismail Ben Ayed, Riadh Kobbi<br />
**Abstract:** <details><summary>原文: </summary>Using deep learning models pre-trained on Imagenet is the traditional solution for medical image classification to deal with data scarcity. Nevertheless, relevant literature supports that this strategy may offer limited gains due to the high dissimilarity between domains. Currently, the paradigm of adapting domain-specialized foundation models is proving to be a promising alternative. However, how to perform such knowledge transfer, and the benefits and limitations it presents, are under study. The CGI-HRDC challenge for Hypertensive Retinopathy diagnosis on fundus images introduces an appealing opportunity to evaluate the transferability of a recently released vision-language foundation model of the retina, FLAIR. In this work, we explore the potential of using FLAIR features as starting point for fundus image classification, and we compare its performance with regard to Imagenet initialization on two popular transfer learning methods: Linear Probing (LP) and Fine-Tuning (FP). Our empirical observations suggest that, in any case, the use of the traditional strategy provides performance gains. In contrast, direct transferability from FLAIR model allows gains of 2.5%. When fine-tuning the whole network, the performance gap increases up to 4%. In this case, we show that avoiding feature deterioration via LP initialization of the classifier allows the best re-use of the rich pre-trained features. Although direct transferability using LP still offers limited performance, we believe that foundation models such as FLAIR will drive the evolution of deep-learning-based fundus image analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用在 Imagenet 上预训练的深度学习模型是医学图像分类处理数据稀缺问题的传统解决方案。然而，相关文献表明，由于领域之间的高度差异，该策略可能提供有限的收益。目前，采用领域专业基础模型的范式被证明是一种有前途的替代方案。然而，如何进行这种知识转移及其带来的好处和局限性仍在研究中。针对眼底图像高血压视网膜病变诊断的 CGI-HRDC 挑战赛引入了一个有吸引力的机会来评估最近发布的视网膜视觉语言基础模型 FLAIR 的可转移性。在这项工作中，我们探索了使用 FLAIR 特征作为眼底图像分类起点的潜力，并比较了两种流行的迁移学习方法（线性探测（LP）和微调（FP））在 Imagenet 初始化方面的性能。我们的经验观察表明，无论如何，使用传统策略都能带来性能提升。相比之下，从 FLAIR 模型直接转移可带来 2.5% 的收益。当对整个网络进行微调时，性能差距最多增加 4%。在这种情况下，我们表明，通过分类器的 LP 初始化避免特征恶化可以最好地重用丰富的预训练特征。尽管使用 LP 的直接可迁移性仍然提供有限的性能，但我们相信 FLAIR 等基础模型将推动基于深度学习的眼底图像分析的发展。</details>
**PDF:** <http://arxiv.org/pdf/2401.15526v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MiTU-Net: A fine-tuned U-Net with SegFormer backbone for segmenting pubic symphysis-fetal head**<br />
**Title_cn:** MiTU-Net：带有 SegFormer 主干的微调 U-Net，用于分割耻骨联合胎头<br />
**Authors:** Fangyijie Wang, Guenole Silvestre, Kathleen Curran<br />
**Abstract:** <details><summary>原文: </summary>Ultrasound measurements have been examined as potential tools for predicting the likelihood of successful vaginal delivery. The angle of progression (AoP) is a measurable parameter that can be obtained during the initial stage of labor. The AoP is defined as the angle between a straight line along the longitudinal axis of the pubic symphysis (PS) and a line from the inferior edge of the PS to the leading edge of the fetal head (FH). However, the process of measuring AoP on ultrasound images is time consuming and prone to errors. To address this challenge, we propose the Mix Transformer U-Net (MiTU-Net) network, for automatic fetal head-pubic symphysis segmentation and AoP measurement. The MiTU-Net model is based on an encoder-decoder framework, utilizing a pre-trained efficient transformer to enhance feature representation. Within the efficient transformer encoder, the model significantly reduces the trainable parameters of the encoder-decoder model. The effectiveness of the proposed method is demonstrated through experiments conducted on a recent transperineal ultrasound dataset. Our model achieves competitive performance, ranking 5th compared to existing approaches. The MiTU-Net presents an efficient method for automatic segmentation and AoP measurement, reducing errors and assisting sonographers in clinical practice. Reproducibility: Framework implementation and models available on https://github.com/13204942/MiTU-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>超声测量已被视为预测阴道分娩成功可能性的潜在工具。进展角 (AoP) 是可在分娩初始阶段获得的可测量参数。 AoP 定义为沿耻骨联合 (PS) 纵轴的直线与从 PS 下缘到胎头前缘 (FH) 的线之间的角度。然而，在超声图像上测量 AoP 的过程非常耗时且容易出错。为了应对这一挑战，我们提出了 Mix Transformer U-Net (MiTU-Net) 网络，用于自动胎儿头部-耻骨联合分割和 AoP 测量。 MiTU-Net模型基于编码器-解码器框架，利用预先训练的高效变压器来增强特征表示。在高效的 Transformer 编码器中，该模型显着减少了编码器-解码器模型的可训练参数。通过对最近的经会阴超声数据集进行的实验证明了所提出方法的有效性。我们的模型取得了有竞争力的性能，与现有方法相比排名第五。 MiTU-Net 提供了一种有效的自动分割和 AoP 测量方法，可减少错误并协助超声检查人员进行临床实践。可重复性：https://github.com/13204942/MiTU-Net 上提供了框架实现和模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.15513v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **FloodLense: A Framework for ChatGPT-based Real-time Flood Detection**<br />
**Title_cn:** FloodLense：基于 ChatGPT 的实时洪水检测框架<br />
**Authors:** Pranath Reddy Kumbam, Kshitij Maruti Vejre<br />
**Abstract:** <details><summary>原文: </summary>This study addresses the vital issue of real-time flood detection and management. It innovatively combines advanced deep learning models with Large language models (LLM), enhancing flood monitoring and response capabilities. This approach addresses the limitations of current methods by offering a more accurate, versatile, user-friendly and accessible solution. The integration of UNet, RDN, and ViT models with natural language processing significantly improves flood area detection in diverse environments, including using aerial and satellite imagery. The experimental evaluation demonstrates the models' efficacy in accurately identifying and mapping flood zones, showcasing the project's potential in transforming environmental monitoring and disaster management fields.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究解决了实时洪水检测和管理的重要问题。它创新地将先进的深度学习模型与大语言模型（LLM）相结合，增强了洪水监测和响应能力。这种方法通过提供更准确、通用、用户友好且易于访问的解决方案来解决当前方法的局限性。 UNet、RDN 和 ViT 模型与自然语言处理的集成显着改善了不同环境中的洪水区域检测，包括使用航空和卫星图像。实验评估证明了该模型在准确识别和绘制洪泛区地图方面的功效，展示了该项目在改变环境监测和灾害管理领域的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.15501v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Distilling Privileged Multimodal Information for Expression Recognition using Optimal Transport**<br />
**Title_cn:** 使用最佳传输提取特权多模态信息以进行表达识别<br />
**Authors:** Muhammad Haseeb Aslam, Muhammad Osama Zeeshan, Soufiane Belharbi, Marco Pedersoli, Alessandro Koerich, Simon Bacon, Eric Granger<br />
**Abstract:** <details><summary>原文: </summary>Multimodal affect recognition models have reached remarkable performance in the lab environment due to their ability to model complementary and redundant semantic information. However, these models struggle in the wild, mainly because of the unavailability or quality of modalities used for training. In practice, only a subset of the training-time modalities may be available at test time. Learning with privileged information (PI) enables deep learning models (DL) to exploit data from additional modalities only available during training. State-of-the-art knowledge distillation (KD) methods have been proposed to distill multiple teacher models (each trained on a modality) to a common student model. These privileged KD methods typically utilize point-to-point matching and have no explicit mechanism to capture the structural information in the teacher representation space formed by introducing the privileged modality. We argue that encoding this same structure in the student space may lead to enhanced student performance. This paper introduces a new structural KD mechanism based on optimal transport (OT), where entropy-regularized OT distills the structural dark knowledge. Privileged KD with OT (PKDOT) method captures the local structures in the multimodal teacher representation by calculating a cosine similarity matrix and selects the top-k anchors to allow for sparse OT solutions, resulting in a more stable distillation process. Experiments were performed on two different problems: pain estimation on the Biovid dataset (ordinal classification) and arousal-valance prediction on the Affwild2 dataset (regression). Results show that the proposed method can outperform state-of-the-art privileged KD methods on these problems. The diversity of different modalities and fusion architectures indicates that the proposed PKDOT method is modality and model-agnostic.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态情感识别模型由于能够对互补和冗余语义信息进行建模，因此在实验室环境中取得了显着的性能。然而，这些模型在野外举步维艰，主要是因为用于训练的模式不可用或质量不高。在实践中，只有训练时模式的子集在测试时可用。利用特权信息 (PI) 进行学习使深度学习模型 (DL) 能够利用仅在训练期间可用的其他模式中的数据。人们提出了最先进的知识蒸馏（KD）方法，将多个教师模型（每个模型都经过某种模式的训练）蒸馏为通用的学生模型。这些特权KD方法通常利用点对点匹配，并且没有明确的机制来捕获通过引入特权模态形成的教师表示空间中的结构信息。我们认为，在学生空间中编码相同的结构可能会提高学生的表现。本文介绍了一种基于最优传输（OT）的新结构 KD 机制，其中熵正则化 OT 提炼了结构暗知识。特权 KD 与 OT（PKDOT）方法通过计算余弦相似度矩阵来捕获多模态教师表示中的局部结构，并选择 top-k 锚点以允许稀疏 OT 解决方案，从而产生更稳定的蒸馏过程。针对两个不同的问题进行了实验：Biovid 数据集上的疼痛估计（序数分类）和 Affwild2 数据集上的唤醒效价预测（回归）。结果表明，所提出的方法在这些问题上可以优于最先进的特权 KD 方法。不同模态和融合架构的多样性表明所提出的 PKDOT 方法与模态和模型无关。</details>
**PDF:** <http://arxiv.org/pdf/2401.15489v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **A New Method for Vehicle Logo Recognition Based on Swin Transformer**<br />
**Title_cn:** 基于Swin Transformer的车标识别新方法<br />
**Authors:** Yang Li, Doudou Zhang, Jianli Xiao<br />
**Abstract:** <details><summary>原文: </summary>Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.</details>
**Abstract_cn:** <details><summary>译文: </summary>智能交通系统（ITS）利用传感器、摄像头和大数据分析来监控实时交通状况，旨在提高交通效率和安全性。在此过程中，准确的车辆识别至关重要，其中车辆标志识别（VLR）是关键方法。 VLR通过区分道路上的车辆来实现有效的管理和监控。卷积神经网络 (CNN) 在 VLR 研究方面取得了令人印象深刻的进步。然而，实现更高的性能需要大量的时间和计算资源来进行训练。近期，Transformer车型的兴起给VLR带来了新的机遇。 Swin Transformer 凭借其高效计算和全局特征建模能力，在具有挑战性的条件下优于 CNN。在本文中，我们使用 Swin Transformer 实现实时 VLR 并对其进行微调以获得最佳性能。对三个公共车辆标志数据集（HFUT-VL1、XMU、CTGU-VLD）进行的广泛实验表明，最高准确度结果分别为 99.28%、100% 和 99.17%。此外，迁移学习策略的使用使我们的方法能够与最先进的 VLR 方法相媲美。这些发现证实了我们的方法相对于现有方法的优越性。未来的研究可以探索和优化 Swin Transformer 在其他车辆视觉识别任务中的应用，以推动智能交通系统的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.15458v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **New Foggy Object Detecting Model**<br />
**Title_cn:** 新的雾物体检测模型<br />
**Authors:** Rahul Banavathu, Modem Veda Sree, Bollina Kavya Sri, Suddhasil De<br />
**Abstract:** <details><summary>原文: </summary>Object detection in reduced visibility has become a prominent research area. The existing techniques are not accurate enough in recognizing objects under such circumstances. This paper introduces a new foggy object detection method through a two-staged architecture of region identification from input images and detecting objects in such regions. The paper confirms notable improvements of the proposed method's accuracy and detection time over existing techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>能见度较低时的物体检测已成为一个重要的研究领域。现有技术在这种情况下识别物体不够准确。本文介绍了一种新的雾对象检测方法，通过从输入图像识别区域并检测这些区域中的对象的两阶段架构。该论文证实了所提出的方法相对于现有技术的准确性和检测时间有显着的改进。</details>
**PDF:** <http://arxiv.org/pdf/2401.15455v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Systematic Review of Available Datasets in Additive Manufacturing**<br />
**Title_cn:** 对增材制造中可用数据集的系统回顾<br />
**Authors:** Xiao Liu, Alessandra Mileo, Alan F. Smeaton<br />
**Abstract:** <details><summary>原文: </summary>In-situ monitoring incorporating data from visual and other sensor technologies, allows the collection of extensive datasets during the Additive Manufacturing (AM) process. These datasets have potential for determining the quality of the manufactured output and the detection of defects through the use of Machine Learning during the manufacturing process. Open and annotated datasets derived from AM processes are necessary for the machine learning community to address this opportunity, which creates difficulties in the application of computer vision-related machine learning in AM. This systematic review investigates the availability of open image-based datasets originating from AM processes that align with a number of pre-defined selection criteria. The review identifies existing gaps among the current image-based datasets in the domain of AM, and points to the need for greater availability of open datasets in order to allow quality assessment and defect detection during additive manufacturing, to develop.</details>
**Abstract_cn:** <details><summary>译文: </summary>现场监控结合了来自视觉和其他传感器技术的数据，可以在增材制造 (AM) 过程中收集大量数据集。这些数据集有潜力通过在制造过程中使用机器学习来确定制造输出的质量和缺陷检测。机器学习社区需要从增材制造过程中派生的开放且带注释的数据集来抓住这一机会，这给计算机视觉相关的机器学习在增材制造中的应用带来了困难。这项系统审查调查了源自增材制造工艺的开放式基于图像的数据集的可用性，这些数据集符合许多预定义的选择标准。该审查确定了增材制造领域当前基于图像的数据集之间存在的差距，并指出需要提高开放数据集的可用性，以便在增材制造过程中进行质量评估和缺陷检测。</details>
**PDF:** <http://arxiv.org/pdf/2401.15448v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI**<br />
**Title_cn:** 用于多参数 MRI 脑肿瘤分割的去中心化 Gossip 相互学习 (GML)<br />
**Authors:** Jingyun Chen, Yading Yuan<br />
**Abstract:** <details><summary>原文: </summary>Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>联邦学习 (FL) 支持医疗中心之间的协作模型训练，而无需共享私人数据。然而，由于集中式模型聚合的性质，传统的 FL 存在服务器故障和本地数据性能不佳的风险。为了解决这些问题，我们提出了 Gossip Mutual Learning (GML)，这是一个使用 Gossip 协议进行直接点对点通信的去中心化框架。此外，GML鼓励每个站点通过相互学习来优化其本地模型，以考虑不同站点之间的数据差异。对于使用 BraTS 2021 数据集中来自四个临床站点的 146 个病例的肿瘤分割任务，我们证明了 GML 优于本地模型，并实现了与 FedAvg 相似的性能，而通信开销仅为 25%。</details>
**PDF:** <http://arxiv.org/pdf/2401.15434v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Open-RadVLAD: Fast and Robust Radar Place Recognition**<br />
**Title_cn:** Open-RadVLAD：快速、鲁棒的雷达位置识别<br />
**Authors:** Matthew Gadd, Paul Newman<br />
**Abstract:** <details><summary>原文: </summary>Radar place recognition often involves encoding a live scan as a vector and matching this vector to a database in order to recognise that the vehicle is in a location that it has visited before. Radar is inherently robust to lighting or weather conditions, but place recognition with this sensor is still affected by: (1) viewpoint variation, i.e. translation and rotation, (2) sensor artefacts or "noises". For 360-degree scanning radar, rotation is readily dealt with by in some way aggregating across azimuths. Also, we argue in this work that it is more critical to deal with the richness of representation and sensor noises than it is to deal with translational invariance - particularly in urban driving where vehicles predominantly follow the same lane when repeating a route. In our method, for computational efficiency, we use only the polar representation. For partial translation invariance and robustness to signal noise, we use only a one-dimensional Fourier Transform along radial returns. We also achieve rotational invariance and a very discriminative descriptor space by building a vector of locally aggregated descriptors. Our method is more comprehensively tested than all prior radar place recognition work - over an exhaustive combination of all 870 pairs of trajectories from 30 Oxford Radar RobotCar Dataset sequences (each approximately 10 km). Code and detailed results are provided at github.com/mttgdd/open-radvlad, as an open implementation and benchmark for future work in this area. We achieve a median of 91.52% in Recall@1, outstripping the 69.55% for the only other open implementation, RaPlace, and at a fraction of its computational cost (relying on fewer integral transforms e.g. Radon, Fourier, and inverse Fourier).</details>
**Abstract_cn:** <details><summary>译文: </summary>雷达位置识别通常涉及将实时扫描编码为矢量并将该矢量与数据库进行匹配，以便识别车辆位于其之前访问过的位置。雷达本质上对照明或天气条件具有鲁棒性，但该传感器的位置识别仍然受到以下因素的影响：(1) 视点变化，即平移和旋转，(2) 传感器伪影或“噪声”。对于 360 度扫描雷达，可以通过某种方式聚合方位角来轻松处理旋转。此外，我们在这项工作中认为，处理丰富的表示和传感器噪声比处理平移不变性更重要——特别是在城市驾驶中，车辆在重复路线时主要遵循同一车道。在我们的方法中，为了计算效率，我们仅使用极坐标表示。为了实现部分平移不变性和对信号噪声的鲁棒性，我们仅使用沿径向返回的一维傅立叶变换。我们还通过构建局部聚合描述符的向量来实现旋转不变性和非常有辨别力的描述符空间。我们的方法比所有先前的雷达位置识别工作经过了更全面的测试 - 对来自 30 个牛津雷达 RobotCar 数据集序列（每个大约 10 公里）的所有 870 对轨迹进行了详尽的组合。代码和详细结果在 github.com/mttgdd/open-radvlad 上提供，作为该领域未来工作的开放实现和基准。我们在 Recall@1 中实现了 91.52% 的中位数，超过了唯一的其他开放实现 RaPlace 的 69.55%，并且计算成本仅为其一小部分（依赖于较少的积分变换，例如 Radon、傅里叶和逆傅里叶）。</details>
**PDF:** <http://arxiv.org/pdf/2401.15380v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **An open dataset for oracle bone script recognition and decipherment**<br />
**Title_cn:** 甲骨文识别破译开放数据集<br />
**Authors:** Pengjie Wang, Kaile Zhang, Yuliang Liu, Jinpeng Wan, Haisu Guan, Zhebin Kuang, Xinyu Wang, Lianwen Jin, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Oracle Bone Script (OBS), one of the earliest known forms of ancient Chinese writing, holds invaluable insights into the humanities and geography of the Shang Dynasty, dating back 3,000 years. The immense historical and cultural significance of these writings cannot be overstated. However, the passage of time has obscured much of their meaning, presenting a significant challenge in deciphering these ancient texts. With the advent of Artificial Intelligence (AI), employing AI to assist in interpreting OBS has become a feasible option. Yet, progress in this area has been hindered by a lack of high-quality datasets. To address this issue, this paper details the creation of the HUST-OBS dataset. This dataset encompasses 77,064 images of 1,588 individual deciphered scripts and 62,989 images of 9,411 undeciphered characters, with a total of 140,053 images, compiled from diverse sources. Additionally, all images and labels have been reviewed and corrected by experts in oracle bone studies. The hope is that this dataset could inspire and assist future research in deciphering those unknown OBS.</details>
**Abstract_cn:** <details><summary>译文: </summary>甲骨文 (OBS) 是已知最早的古代中国文字形式之一，对 3000 年前的商代人文和地理有着宝贵的见解。这些著作的巨大历史和文化意义怎么强调都不为过。然而，时间的流逝已经模糊了它们的大部分含义，给破译这些古代文本带来了重大挑战。随着人工智能（AI）的出现，利用AI辅助解读OBS已成为一种可行的选择。然而，由于缺乏高质量数据集，这一领域的进展受到阻碍。为了解决这个问题，本文详细介绍了 HUST-OBS 数据集的创建。该数据集包含 1,588 个已破译文字的 77,064 张图像和 9,411 个未破译字符的 62,989 张图像，总共 140,053 张图像，来自不同来源。此外，所有图像和标签都经过甲骨研究专家的审查和纠正。希望这个数据集能够启发和帮助未来的研究破译那些未知的 OBS。</details>
**PDF:** <http://arxiv.org/pdf/2401.15365v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **DeepGI: An Automated Approach for Gastrointestinal Tract Segmentation in MRI Scans**<br />
**Title_cn:** DeepGI：MRI 扫描中胃肠道分割的自动化方法<br />
**Authors:** Ye Zhang, Yulu Gong, Dongji Cui, Xinrui Li, Xinyu Shen<br />
**Abstract:** <details><summary>原文: </summary>Gastrointestinal (GI) tract cancers pose a global health challenge, demanding precise radiotherapy planning for optimal treatment outcomes. This paper introduces a cutting-edge approach to automate the segmentation of GI tract regions in magnetic resonance imaging (MRI) scans. Leveraging advanced deep learning architectures, the proposed model integrates Inception-V4 for initial classification, UNet++ with a VGG19 encoder for 2.5D data, and Edge UNet for grayscale data segmentation. Meticulous data preprocessing, including innovative 2.5D processing, is employed to enhance adaptability, robustness, and accuracy.   This work addresses the manual and time-consuming segmentation process in current radiotherapy planning, presenting a unified model that captures intricate anatomical details. The integration of diverse architectures, each specializing in unique aspects of the segmentation task, signifies a novel and comprehensive solution. This model emerges as an efficient and accurate tool for clinicians, marking a significant advancement in the field of GI tract image segmentation for radiotherapy planning.</details>
**Abstract_cn:** <details><summary>译文: </summary>胃肠道 (GI) 道癌症构成了全球健康挑战，需要精确的放射治疗计划才能获得最佳治疗结果。本文介绍了一种在磁共振成像 (MRI) 扫描中自动分割胃肠道区域的尖端方法。利用先进的深度学习架构，所提出的模型集成了用于初始分类的 Inception-V4、用于 2.5D 数据的带有 VGG19 编码器的 UNet++ 以及用于灰度数据分割的 Edge UNet。采用细致的数据预处理（包括创新的 2.5D 处理）来增强适应性、稳健性和准确性。这项工作解决了当前放射治疗计划中手动且耗时的分割过程，提出了一个捕获复杂解剖细节的统一模型。不同架构的集成，每个架构都专注于分割任务的独特方面，意味着一种新颖且全面的解决方案。该模型成为临床医生高效、准确的工具，标志着放疗规划胃肠道图像分割领域的重大进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.15354v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **You Only Look Bottom-Up for Monocular 3D Object Detection**<br />
**Title_cn:** 对于单目 3D 物体检测，您只能自下而上地查看<br />
**Authors:** Kaixin Xiong, Dingyuan Zhang, Dingkang Liang, Zhe Liu, Hongcheng Yang, Wondimu Dikubab, Jianwei Cheng, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Monocular 3D Object Detection is an essential task for autonomous driving. Meanwhile, accurate 3D object detection from pure images is very challenging due to the loss of depth information. Most existing image-based methods infer objects' location in 3D space based on their 2D sizes on the image plane, which usually ignores the intrinsic position clues from images, leading to unsatisfactory performances. Motivated by the fact that humans could leverage the bottom-up positional clues to locate objects in 3D space from a single image, in this paper, we explore the position modeling from the image feature column and propose a new method named You Only Look Bottum-Up (YOLOBU). Specifically, our YOLOBU leverages Column-based Cross Attention to determine how much a pixel contributes to pixels above it. Next, the Row-based Reverse Cumulative Sum (RRCS) is introduced to build the connections of pixels in the bottom-up direction. Our YOLOBU fully explores the position clues for monocular 3D detection via building the relationship of pixels from the bottom-up way. Extensive experiments on the KITTI dataset demonstrate the effectiveness and superiority of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目 3D 物体检测是自动驾驶的一项重要任务。同时，由于深度信息的丢失，从纯图像中进行准确的 3D 物体检测非常具有挑战性。大多数现有的基于图像的方法都是根据物体在图像平面上的 2D 尺寸来推断物体在 3D 空间中的位置，这通常会忽略图像的内在位置线索，导致性能不理想。受人类可以利用自下而上的位置线索从单个图像中定位 3D 空间中的物体这一事实的启发，在本文中，我们探索了图像特征列中的位置建模，并提出了一种名为 You Only Look Bottum 的新方法 -向上（YOLOBU）。具体来说，我们的 YOLOBU 利用基于列的交叉注意力来确定一个像素对其上方像素的贡献有多大。接下来，引入基于行的反向累积和（RRCS）来建立自下而上方向的像素连接。我们的YOLOBU通过自下而上的方式构建像素关系，充分探索了单目3D检测的位置线索。在 KITTI 数据集上进行的大量实验证明了我们方法的有效性和优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15319v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **ParaTransCNN: Parallelized TransCNN Encoder for Medical Image Segmentation**<br />
**Title_cn:** ParaTransCNN：用于医学图像分割的并行 TransCNN 编码器<br />
**Authors:** Hongkun Sun, Jing Xu, Yuping Duan<br />
**Abstract:** <details><summary>原文: </summary>The convolutional neural network-based methods have become more and more popular for medical image segmentation due to their outstanding performance. However, they struggle with capturing long-range dependencies, which are essential for accurately modeling global contextual correlations. Thanks to the ability to model long-range dependencies by expanding the receptive field, the transformer-based methods have gained prominence. Inspired by this, we propose an advanced 2D feature extraction method by combining the convolutional neural network and Transformer architectures. More specifically, we introduce a parallelized encoder structure, where one branch uses ResNet to extract local information from images, while the other branch uses Transformer to extract global information. Furthermore, we integrate pyramid structures into the Transformer to extract global information at varying resolutions, especially in intensive prediction tasks. To efficiently utilize the different information in the parallelized encoder at the decoder stage, we use a channel attention module to merge the features of the encoder and propagate them through skip connections and bottlenecks. Intensive numerical experiments are performed on both aortic vessel tree, cardiac, and multi-organ datasets. By comparing with state-of-the-art medical image segmentation methods, our method is shown with better segmentation accuracy, especially on small organs. The code is publicly available on https://github.com/HongkunSun/ParaTransCNN.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于卷积神经网络的方法由于其出色的性能而在医学图像分割中变得越来越流行。然而，他们在捕获远程依赖性方面遇到了困难，这对于准确建模全局上下文相关性至关重要。由于能够通过扩展感受野来建模远程依赖关系，基于变压器的方法已经获得了重视。受此启发，我们结合卷积神经网络和 Transformer 架构提出了一种先进的 2D 特征提取方法。更具体地说，我们引入了一种并行编码器结构，其中一个分支使用 ResNet 从图像中提取局部信息，而另一个分支使用 Transformer 提取全局信息。此外，我们将金字塔结构集成到 Transformer 中，以不同分辨率提取全局信息，特别是在密集的预测任务中。为了在解码器阶段有效地利用并行编码器中的不同信息，我们使用通道注意模块来合并编码器的特征并通过跳过连接和瓶颈传播它们。对主动脉血管树、心脏和多器官数据集进行了密集的数值实验。通过与最先进的医学图像分割方法进行比较，我们的方法具有更好的分割精度，特别是在小器官上。该代码可在 https://github.com/HongkunSun/ParaTransCNN 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.15307v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions**<br />
**Title_cn:** 基于 3D 骨骼的行人重新识别调查：方法、设计、挑战和未来方向<br />
**Authors:** Haocong Rao, Chunyan Miao<br />
**Abstract:** <details><summary>原文: </summary>Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models from multiple aspects to offer key insights for model improvement. Finally, we identify critical challenges confronting current studies and discuss several promising directions for future research of SRID.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过 3D 骨骼进行人员重新识别是一个重要的新兴研究领域，引起了模式识别界的极大兴趣。近年来，基于3D骨架的行人重识别（SRID）方法被提出，具有针对多种应用场景的独特优势，有效解决了骨架建模和特征学习中的突出问题。尽管最近取得了进展，但据我们所知，人们很少努力全面总结这些研究及其挑战。在本文中，我们试图通过对当前 SRID 方法、模型设计、挑战和未来方向进行系统调查来填补这一空白。具体来说，我们首先提出了 SRID 问题，并提出了 SRID 研究的分类法，总结了基准数据集、常用模型架构以及对不同方法特征的分析回顾。然后，我们从多个方面详细阐述了SRID模型的设计原理，为模型改进提供关键见解。最后，我们确定了当前研究面临的关键挑战，并讨论了 SRID 未来研究的几个有希望的方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.15296v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection**<br />
**Title_cn:** SkipViT：通过令牌级 Skip 连接加速 Vision Transformer<br />
**Authors:** Foozhan Ataiefard, Walid Ahmed, Habib Hajimolahoseini, Saina Asani, Farnoosh Javadi, Mohammad Hassanpour, Omar Mohamed Awad, Austin Wen, Kangling Liu, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dropping 55% of the tokens while gaining more than 13% training throughput and maintaining classification accuracy at the level of the baseline model on Huawei Ascend910A.</details>
**Abstract_cn:** <details><summary>译文: </summary>众所周知，视觉 Transformer 比 CNN 模型的计算量和数据密集度更高。这些 Transformer 模型（例如 ViT）需要所有输入图像标记来学习它们之间的关系。然而，许多这些标记并不提供信息，并且可能包含不相关的信息，例如不相关的背景或不重要的风景。这些标记被多头自注意力（MHSA）忽略，导致 MHSA 和前馈网络（FFN）中出现许多冗余和不必要的计算。在这项工作中，我们提出了一种方法，通过将不重要的令牌分离并通过不同的低成本计算路径发送来优化它们之间不必要的交互量。我们的方法没有向 ViT 模型添加任何参数，旨在找到训练吞吐量和最终模型 Top-1 精度损失 0% 之间的最佳权衡。我们从头开始训练 ViT-small 的实验结果表明，SkipViT 能够有效丢弃 55% 的 token，同时获得超过 13% 的训练吞吐量，并在华为 Ascend910A 上保持基线模型水平的分类精度。</details>
**PDF:** <http://arxiv.org/pdf/2401.15293v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics**<br />
**Title_cn:** STAC：利用时空数据关联进行高效的跨摄像机流传输和分析<br />
**Authors:** Volodymyr Vakhniuk, Ayush Sarkar, Ragini Gupta<br />
**Abstract:** <details><summary>原文: </summary>We propose an efficient cross-cameras surveillance system called,STAC, that leverages spatio-temporal associations between multiple cameras to provide real-time analytics and inference under constrained network environments. STAC is built using the proposed omni-scale feature learning people reidentification (reid) algorithm that allows accurate detection, tracking and re-identification of people across cameras using the spatio-temporal characteristics of video frames. We integrate STAC with frame filtering and state-of-the-art compression for streaming technique (that is, ffmpeg libx264 codec) to remove redundant information from cross-camera frames. This helps in optimizing the cost of video transmission as well as compute/processing, while maintaining high accuracy for real-time query inference. The introduction of AICity Challenge 2023 Data [1] by NVIDIA has allowed exploration of systems utilizing multi-camera people tracking algorithms. We evaluate the performance of STAC using this dataset to measure the accuracy metrics and inference rate for reid. Additionally, we quantify the reduction in video streams achieved through frame filtering and compression using FFmpeg compared to the raw camera streams. For completeness, we make available our repository to reproduce the results, available at https://github.com/VolodymyrVakhniuk/CS444_Final_Project.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种名为 STAC 的高效跨摄像头监控系统，该系统利用多个摄像头之间的时空关联来在受限网络环境下提供实时分析和推理。 STAC 使用所提出的全尺度特征学习人员重新识别 (reid) 算法构建，该算法允许使用视频帧的时空特征跨摄像机准确检测、跟踪和重新识别人员。我们将 STAC 与帧过滤和最先进的流媒体压缩技术（即 ffmpeg libx264 编解码器）集成，以消除跨相机帧中的冗余信息。这有助于优化视频传输以及计算/处理的成本，同时保持实时查询推理的高精度。 NVIDIA 推出的 AICity Challenge 2023 数据 [1] 允许探索利用多摄像头人员跟踪算法的系统。我们使用该数据集来评估 STAC 的性能，以衡量 reid 的准确性指标和推理率。此外，与原始相机流相比，我们还量化了通过使用 FFmpeg 进行帧过滤和压缩所实现的视频流减少量。为了完整起见，我们提供了我们的存储库来重现结果，可从 https://github.com/VolodymyrVakhniuk/CS444_Final_Project 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.15288v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Applications of Tao General Difference in Discrete Domain**<br />
**Title_cn:** 道广义差分在离散域中的应用<br />
**Authors:** Linmi Tao, Ruiyang Liu, Donglai Tao, Wu Xia, Feilong Ma, Yu Cheng, Jingmao Cui<br />
**Abstract:** <details><summary>原文: </summary>Numerical difference computation is one of the cores and indispensable in the modern digital era. Tao general difference (TGD) is a novel theory and approach to difference computation for discrete sequences and arrays in multidimensional space. Built on the solid theoretical foundation of the general difference in a finite interval, the TGD operators demonstrate exceptional signal processing capabilities in real-world applications. A novel smoothness property of a sequence is defined on the first- and second TGD. This property is used to denoise one-dimensional signals, where the noise is the non-smooth points in the sequence. Meanwhile, the center of the gradient in a finite interval can be accurately location via TGD calculation. This solves a traditional challenge in computer vision, which is the precise localization of image edges with noise robustness. Furthermore, the power of TGD operators extends to spatio-temporal edge detection in three-dimensional arrays, enabling the identification of kinetic edges in video data. These diverse applications highlight the properties of TGD in discrete domain and the significant promise of TGD for the computation across signal processing, image analysis, and video analytic.</details>
**Abstract_cn:** <details><summary>译文: </summary>数值差分计算是现代数字时代不可或缺的核心之一。道广义差分（TGD）是一种新颖的多维空间离散序列和数组差分计算理论和方法。 TGD 算子建立在有限区间一般差分的坚实理论基础之上，在实际应用中展示了卓越的信号处理能力。在第一和第二 TGD 上定义了序列的新颖平滑属性。该属性用于对一维信号进行去噪，其中噪声是序列中的非平滑点。同时，通过TGD计算可以准确定位有限区间内的梯度中心。这解决了计算机视觉中的一个传统挑战，即具有噪声鲁棒性的图像边缘的精确定位。此外，TGD 算子的功能还扩展到三维阵列中的时空边缘检测，从而能够识别视频数据中的动态边缘。这些不同的应用凸显了 TGD 在离散域中的特性以及 TGD 在信号处理、图像分析和视频分析计算方面的重大前景。</details>
**PDF:** <http://arxiv.org/pdf/2401.15287v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis**<br />
**Title_cn:** GEM：通过分段任意模型和数据合成增强玻璃表面分割的简单网络<br />
**Authors:** Jing Hao, Moyun Liu, Kuo Feng Hung<br />
**Abstract:** <details><summary>原文: </summary>Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于玻璃区域的透明度和反射特性不明确，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享传输的任意背景场景和反射物体的视觉外观，因此没有固定的模式。最近的视觉基础模型在大量数据上进行训练，在图像感知和图像生成方面表现出了惊人的性能。为了以更高的精度分割玻璃表面，我们充分利用了两种视觉基础模型：Segment Anything (SAM) 和 Stable Diffusion。具体来说，我们设计了一个简单的玻璃表面分割器，名为 GEM，它仅由 SAM 主干组成，这是一个简单的特征金字塔、辨别查询选择模块和掩码解码器。敏锐的查询选择可以自适应地识别玻璃表面特征，将它们分配为掩模解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为 S-GSD，通过具有四种不同尺度的扩散模型，其中包含原始真实数据大小的 1 倍、5 倍、10 倍和 20 倍。该数据集是迁移学习的可行来源。合成数据的规模对迁移学习有积极影响，但随着数据量的增加，改进将逐渐饱和。大量实验表明，GEM 在 GSD-S 验证集上达到了新的最先进水平（IoU +2.1%）。代码和数据集可在以下网址获取：https://github.com/isbrycee/GEM-Glass-Segmentor。</details>
**PDF:** <http://arxiv.org/pdf/2401.15282v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **SAM-based instance segmentation models for the automation of masonry crack detection**<br />
**Title_cn:** 基于 SAM 的实例分割模型，用于砖石裂缝自动检测<br />
**Authors:** Zehao Ye, Lucy Lovell, Asaad Faramarzi, Jelena Ninic<br />
**Abstract:** <details><summary>原文: </summary>Automating visual inspection for capturing defects based on civil structures appearance is crucial due to its currently labour-intensive and time-consuming nature. An important aspect of automated inspection is image acquisition, which is rapid and cost-effective considering the pervasive developments in both software and hardware computing in recent years. Previous studies largely focused on concrete and asphalt, with less attention to masonry cracks. The latter also lacks publicly available datasets. In this paper, we first present a corresponding data set for instance segmentation with 1,300 annotated images (640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks, and cracks. We then test several leading algorithms for benchmarking, including the latest large-scale model, the prompt-based Segment Anything Model (SAM). We fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel methods for automation of SAM execution. The first method involves abandoning the prompt encoder and connecting the SAM encoder to other decoders, while the second method introduces a learnable self-generating prompter. In order to ensure the seamless integration of the two proposed methods with SAM encoder section, we redesign the feature extractor. Both proposed methods exceed state-of-the-art performance, surpassing the best benchmark by approximately 3% for all classes and around 6% for cracks specifically. Based on successful detection, we propose a method based on a monocular camera and the Hough Line Transform to automatically transform images into orthographic projection maps. By incorporating known real sizes of brick units, we accurately estimate crack dimensions, with the results differing by less than 10% from those obtained by laser scanning. Overall, we address important research gaps in automated masonry crack detection and size estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于目前的劳动密集型和耗时性，自动目视检查以根据土木结构外观捕获缺陷至关重要。自动化检测的一个重要方面是图像采集，考虑到近年来软件和硬件计算的普遍发展，图像采集是快速且经济高效的。以前的研究主要集中在混凝土和沥青上，很少关注砖石裂缝。后者也缺乏公开可用的数据集。在本文中，我们首先提出了一个相应的数据集，用于实例分割，包含 1,300 张带注释的图像（640 像素 x 640 像素），命名为 MCrack1300，涵盖砖块、碎砖块和裂缝。然后，我们测试了几种领先的基准测试算法，包括最新的大型模型、基于提示的分段任意模型 (SAM)。我们使用低秩适应 (LoRA) 微调编码器，并提出了两种新的 SAM 执行自动化方法。第一种方法是放弃提示编码器并将SAM编码器连接到其他解码器，而第二种方法则引入了可学习的自生成提示器。为了确保两种提出的方​​法与 SAM 编码器部分的无缝集成，我们重新设计了特征提取器。两种提出的方​​法都超过了最先进的性能，所有类别均超过最佳基准约 3%，特别是裂缝超过最佳基准约 6%。基于成功的检测，我们提出了一种基于单目相机和霍夫线变换的方法，将图像自动转换为正交投影图。通过结合已知的砖单元实际尺寸，我们可以准确估计裂缝尺寸，结果与激光扫描获得的结果相差不到 10%。总的来说，我们解决了自动砌体裂缝检测和尺寸估计方面的重要研究空白。</details>
**PDF:** <http://arxiv.org/pdf/2401.15266v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes**<br />
**Title_cn:** 消失点引导的驾驶场景视频语义分割<br />
**Authors:** Diandian Guo, Deng-Ping Fan, Tongyu Lu, Christos Sakaridis, Luc Van Gool<br />
**Abstract:** <details><summary>原文: </summary>The estimation of implicit cross-frame correspondences and the high computational cost have long been major challenges in video semantic segmentation (VSS) for driving scenes. Prior works utilize keyframes, feature propagation, or cross-frame attention to address these issues. By contrast, we are the first to harness vanishing point (VP) priors for more effective segmentation. Intuitively, objects near VPs (i.e., away from the vehicle) are less discernible. Moreover, they tend to move radially away from the VP over time in the usual case of a forward-facing camera, a straight road, and linear forward motion of the vehicle. Our novel, efficient network for VSS, named VPSeg, incorporates two modules that utilize exactly this pair of static and dynamic VP priors: sparse-to-dense feature mining (DenseVP) and VP-guided motion fusion (MotionVP). MotionVP employs VP-guided motion estimation to establish explicit correspondences across frames and help attend to the most relevant features from neighboring frames, while DenseVP enhances weak dynamic features in distant regions around VPs. These modules operate within a context-detail framework, which separates contextual features from high-resolution local features at different input resolutions to reduce computational costs. Contextual and local features are integrated through contextualized motion attention (CMA) for the final prediction. Extensive experiments on two popular driving segmentation benchmarks, Cityscapes and ACDC, demonstrate that VPSeg outperforms previous SOTA methods, with only modest computational overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐式跨帧对应的估计和高计算成本长期以来一直是驾驶场景视频语义分割（VSS）的主要挑战。先前的工作利用关键帧、特征传播或跨帧注意力来解决这些问题。相比之下，我们是第一个利用消失点（VP）先验进行更有效分割的人。直观上，VP 附近（即远离车辆）的物体不太容易辨别。此外，在前向摄像头、笔直道路和车辆线性向前运动的通常情况下，随着时间的推移，它们往往会径向远离 VP。我们新颖、高效的 VSS 网络名为 VPSeg，包含两个模块，它们恰好利用了这对静态和动态 VP 先验：稀疏到密集特征挖掘 (DenseVP) 和 VP 引导运动融合 (MotionVP)。 MotionVP 采用 VP 引导的运动估计来建立跨帧的明确对应关系，并帮助关注相邻帧中最相关的特征，而 DenseVP 则增强 VP 周围远处区域的弱动态特征。这些模块在上下文细节框架内运行，该框架将不同输入分辨率下的上下文特征与高分辨率局部特征分开，以降低计算成本。通过上下文运动注意（CMA）集成上下文和局部特征以进行最终预测。对两个流行的驾驶分割基准（Cityscapes 和 ACDC）进行的大量实验表明，VPSeg 优于以前的 SOTA 方法，且计算开销适中。</details>
**PDF:** <http://arxiv.org/pdf/2401.15261v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Face to Cartoon Incremental Super-Resolution using Knowledge Distillation**<br />
**Title_cn:** 使用知识蒸馏面对卡通增量超分辨率<br />
**Authors:** Trinetra Devkatte, Shiv Ram Dubey, Satish Kumar Singh, Abdenour Hadid<br />
**Abstract:** <details><summary>原文: </summary>Facial super-resolution/hallucination is an important area of research that seeks to enhance low-resolution facial images for a variety of applications. While Generative Adversarial Networks (GANs) have shown promise in this area, their ability to adapt to new, unseen data remains a challenge. This paper addresses this problem by proposing an incremental super-resolution using GANs with knowledge distillation (ISR-KD) for face to cartoon. Previous research in this area has not investigated incremental learning, which is critical for real-world applications where new data is continually being generated. The proposed ISR-KD aims to develop a novel unified framework for facial super-resolution that can handle different settings, including different types of faces such as cartoon face and various levels of detail. To achieve this, a GAN-based super-resolution network was pre-trained on the CelebA dataset and then incrementally trained on the iCartoonFace dataset, using knowledge distillation to retain performance on the CelebA test set while improving the performance on iCartoonFace test set. Our experiments demonstrate the effectiveness of knowledge distillation in incrementally adding capability to the model for cartoon face super-resolution while retaining the learned knowledge for facial hallucination tasks in GANs.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部超分辨率/幻觉是一个重要的研究领域，旨在增强各种应用的低分辨率面部图像。虽然生成对抗网络（GAN）在这一领域显示出前景，但它们适应新的、看不见的数据的能力仍然是一个挑战。本文通过提出使用 GAN 和知识蒸馏 (ISR-KD) 进行面对面卡通的增量超分辨率来解决这个问题。该领域之前的研究并未调查增量学习，而增量学习对于不断生成新数据的现实应用程序至关重要。拟议的 ISR-KD 旨在开发一种新颖的面部超分辨率统一框架，可以处理不同的设置，包括不同类型的面部（例如卡通面部）和各种细节级别。为了实现这一目标，基于 GAN 的超分辨率网络在 CelebA 数据集上进行预训练，然后在 iCartoonFace 数据集上进行增量训练，使用知识蒸馏来保留 CelebA 测试集上的性能，同时提高 iCartoonFace 测试集上的性能。我们的实验证明了知识蒸馏在逐步增加卡通人脸超分辨率模型能力方面的有效性，同时保留了 GAN 中面部幻觉任务所学到的知识。</details>
**PDF:** <http://arxiv.org/pdf/2401.15366v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval**<br />
**Title_cn:** 用于无监督图像检索的基于 Transformer 的截断对比量化学习<br />
**Authors:** Ayush Dubey, Shiv Ram Dubey, Satish Kumar Singh, Wei-Ta Chu<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However, the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this paper, we propose a TransClippedCLR model by encoding the global context of an image using Transformer having local context through patch based processing, by generating the hash codes through product quantization and by avoiding the potential false negative pairs through clipped contrastive learning. The proposed model is tested with superior performance for unsupervised image retrieval on benchmark datasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent state-of-the-art deep models. The results using the proposed clipped contrastive learning are greatly improved on all datasets as compared to same backbone network with vanilla contrastive learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督图像检索旨在在没有任何给定级别的情况下学习重要的视觉特征，以检索给定查询图像的相似图像。基于卷积神经网络 (CNN) 的方法已被广泛用于图像哈希的自监督对比学习。然而，由于 CNN 缺乏对全局特征的有效利用以及对比学习中假阴性对产生的偏差，现有方法受到了影响。在本文中，我们提出了一种 TransClippedCLR 模型，通过使用具有局部上下文的 Transformer 通过基于补丁的处理对图像的全局上下文进行编码，通过乘积量化生成哈希码，并通过裁剪对比学习避免潜在的假阴性对。与最近最先进的深度模型相比，所提出的模型在基准数据集（包括 CIFAR10、NUS-Wide 和 Flickr25K）上进行了无监督图像检索的卓越性能测试。与具有普通对比学习的相同骨干网络相比，使用所提出的剪辑对比学习的结果在所有数据集上都得到了极大的改善。</details>
**PDF:** <http://arxiv.org/pdf/2401.15362v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks**<br />
**Title_cn:** 用于持续学习多模态任务的动态 Transformer 架构<br />
**Authors:** Yuliang Cai, Mohammad Rostami<br />
**Abstract:** <details><summary>原文: </summary>Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. Due to the success of transformers in other modalities, our architecture has the potential to be used in multimodal learning settings. In our framework, we benefit from introducing extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory and time overhead. TAM-CL achieves state-of-the-art (SOTA) performance on challenging multimodal tasks</details>
**Abstract_cn:** <details><summary>译文: </summary>Transformer 神经网络在不同数据模式的广泛应用中越来越多地取代现有架构。微调大型预训练变压器神经网络的规模和计算需求不断增加，对这些模型在需要边缘计算的应用中的广泛采用提出了重大挑战。为了应对这一挑战，持续学习（CL）作为一种解决方案出现，通过促进自主学习代理顺序到达的任务之间的知识转移。然而，当前的 CL 方法主要关注完全基于视觉或基于语言的学习任务。我们提出了一个基于 Transformer 的 CL 框架，专注于涉及视觉和语言的学习任务，称为视觉和语言（VaL）任务。由于 Transformer 在其他模式中的成功，我们的架构有潜力用于多模式学习环境。在我们的框架中，我们受益于向基本变压器引入额外的参数，以使网络专门用于每个任务。因此，我们启用动态模型扩展来学习序列中的多个任务。我们还使用知识蒸馏来从过去的相关经验中受益，从而更有效地学习当前的任务。我们提出的方法，任务注意力多模式持续学习（TAM-CL），允许任务之间交换信息，同时减轻灾难性遗忘问题。值得注意的是，我们的方法是可扩展的，产生的内存和时间开销最小。 TAM-CL 在具有挑战性的多模式任务上实现了最先进的 (SOTA) 性能</details>
**PDF:** <http://arxiv.org/pdf/2401.15275v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks**<br />
**Title_cn:** L-AutoDA：利用大型语言模型进行基于自动化决策的对抗攻击<br />
**Authors:** Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang<br />
**Abstract:** <details><summary>原文: </summary>In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highlight new avenues for the development of robust AI systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>在快速发展的机器学习领域，对抗性攻击对模型的鲁棒性和安全性提出了重大挑战。基于决策的攻击只需要对模型决策的反馈，而不需要详细的概率或分数，这种攻击特别阴险且难以防御。这项工作介绍了 L-AutoDA（基于大语言模型的自动决策对抗攻击），这是一种利用大语言模型 (LLM) 的生成功能来自动设计这些攻击的新颖方法。通过在进化框架中与 LLM 迭代交互，L-AutoDA 无需太多人力即可自动有效地设计竞争性攻击算法。我们展示了 L-AutoDA 在 CIFAR-10 数据集上的功效，显示出在成功率和计算效率方面比基线方法有显着改进。我们的研究结果强调了语言模型作为对抗性攻击生成工具的潜力，并强调了开发强大的人工智能系统的新途径。</details>
**PDF:** <http://arxiv.org/pdf/2401.15335v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting**<br />
**Title_cn:** 高斯飞溅：使用高斯飞溅进行动态流体合成<br />
**Authors:** Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \url{https://amysteriouscat.github.io/GaussianSplashing/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们演示了将基于物理的固体和流体动画与 3D 高斯溅射 (3DGS) 集成的可行性，以在使用 3DGS 重建的虚拟场景中创建新颖的效果。利用底层表示中高斯泼溅和基于位置的动力学 (PBD) 的一致性，我们以一种内聚的方式管理渲染、视图合成以及固体和流体的动力学。与高斯着色器类似，我们通过添加法线来增强每个高斯内核，将内核的方向与表面法线对齐以细化 PBD 模拟。这种方法有效地消除了固体旋转变形产生的尖峰噪声。它还允许我们集成基于物理的渲染来增强流体的动态表面反射。因此，我们的框架能够真实地再现动态流体的表面高光，并促进新视图中场景对象和流体之间的交互。欲了解更多信息，请访问我们的项目页面：\url{https://amysteriouscat.github.io/GaussianSplashing/}。</details>
**PDF:** <http://arxiv.org/pdf/2401.15318v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **An Implicit Physical Face Model Driven by Expression and Style**<br />
**Title_cn:** 由表情和风格驱动的隐式物理面部模型<br />
**Authors:** Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Paulo Gotardo, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley<br />
**Abstract:** <details><summary>原文: </summary>3D facial animation is often produced by manipulating facial deformation models (or rigs), that are traditionally parameterized by expression controls. A key component that is usually overlooked is expression 'style', as in, how a particular expression is performed. Although it is common to define a semantic basis of expressions that characters can perform, most characters perform each expression in their own style. To date, style is usually entangled with the expression, and it is not possible to transfer the style of one character to another when considering facial animation. We present a new face model, based on a data-driven implicit neural physics model, that can be driven by both expression and style separately. At the core, we present a framework for learning implicit physics-based actuations for multiple subjects simultaneously, trained on a few arbitrary performance capture sequences from a small set of identities. Once trained, our method allows generalized physics-based facial animation for any of the trained identities, extending to unseen performances. Furthermore, it grants control over the animation style, enabling style transfer from one character to another or blending styles of different characters. Lastly, as a physics-based model, it is capable of synthesizing physical effects, such as collision handling, setting our method apart from conventional approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 面部动画通常是通过操纵面部变形模型（或装备）来生成的，传统上这些模型是通过表情控件进行参数化的。通常被忽视的一个关键组成部分是表达式“风格”，例如特定表达式的执行方式。尽管定义角色可以执行的表达式的语义基础很常见，但大多数角色都以自己的风格执行每个表达式。迄今为止，风格通常与表情纠缠在一起，在考虑面部动画时不可能将一个角色的风格转移到另一个角色。我们提出了一种新的面部模型，基于数据驱动的隐式神经物理模型，可以分别由表情和风格驱动。其核心是，我们提出了一个框架，用于同时学习多个受试者基于隐式物理的驱动，并根据来自一小组身份的一些任意性能捕获序列进行训练。一旦经过训练，我们的方法就可以为任何受过训练的身份提供基于物理的广义面部动画，并扩展到看不见的表演。此外，它还可以控制动画风格，实现从一个角色到另一个角色的风格转移或混合不同角色的风格。最后，作为基于物理的模型，它能够合成物理效果，例如碰撞处理，使我们的方法有别于传统方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.15414v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**<br />
**Title_cn:** AniDress：使用服装索具模型从稀疏视图中制作可动画的宽松服装头像<br />
**Authors:** Beijia Chen, Yuefan Shen, Qing Shuai, Xiaowei Zhou, Kun Zhou, Youyi Zheng<br />
**Abstract:** <details><summary>原文: </summary>Recent communities have seen significant progress in building photo-realistic animatable avatars from sparse multi-view videos. However, current workflows struggle to render realistic garment dynamics for loose-fitting characters as they predominantly rely on naked body models for human modeling while leaving the garment part un-modeled. This is mainly due to that the deformations yielded by loose garments are highly non-rigid, and capturing such deformations often requires dense views as supervision. In this paper, we introduce AniDress, a novel method for generating animatable human avatars in loose clothes using very sparse multi-view videos (4-8 in our setting). To allow the capturing and appearance learning of loose garments in such a situation, we employ a virtual bone-based garment rigging model obtained from physics-based simulation data. Such a model allows us to capture and render complex garment dynamics through a set of low-dimensional bone transformations. Technically, we develop a novel method for estimating temporal coherent garment dynamics from a sparse multi-view video. To build a realistic rendering for unseen garment status using coarse estimations, a pose-driven deformable neural radiance field conditioned on both body and garment motions is introduced, providing explicit control of both parts. At test time, the new garment poses can be captured from unseen situations, derived from a physics-based or neural network-based simulator to drive unseen garment dynamics. To evaluate our approach, we create a multi-view dataset that captures loose-dressed performers with diverse motions. Experiments show that our method is able to render natural garment dynamics that deviate highly from the body and generalize well to both unseen views and poses, surpassing the performance of existing methods. The code and data will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的社区在从稀疏的多视图视频构建逼真的动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色渲染真实的服装动态，因为它们主要依赖裸体模型进行人体建模，而服装部分未建模。这主要是因为宽松的衣服产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了 AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为 4-8）生成穿着宽松衣服的可动画人类化身的新方法。为了在这种情况下捕获和学习宽松服装，我们采用了从基于物理的模拟数据获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕获和渲染复杂的服装动态。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动态的新方法。为了使用粗略估计构建看不见的服装状态的真实渲染，引入了以身体和服装运动为条件的姿势驱动的可变形神经辐射场，从而提供对这两个部分的显式控制。在测试时，可以从看不见的情况中捕获新的服装姿势，这些姿势源自基于物理或基于神经网络的模拟器，以驱动看不见的服装动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松的表演者的不同动作。实验表明，我们的方法能够渲染与身体高度偏离的自然服装动态，并很好地推广到看不见的视图和姿势，超越了现有方法的性能。代码和数据将公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.15348v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **iDeLog: Iterative Dual Spatial and Kinematic Extraction of Sigma-Lognormal Parameters**<br />
**Title_cn:** iDeLog：西格玛对数正态参数的迭代双空间和运动学提取<br />
**Authors:** Miguel A. Ferrer, Moises Diaz, Cristina Carmona-Duarte, Rejean Plamondon<br />
**Abstract:** <details><summary>原文: </summary>The Kinematic Theory of rapid movements and its associated Sigma-Lognormal model have been extensively used in a large variety of applications. While the physical and biological meaning of the model have been widely tested and validated for rapid movements, some shortcomings have been detected when it is used with continuous long and complex movements. To alleviate such drawbacks, and inspired by the motor equivalence theory and a conceivable visual feedback, this paper proposes a novel framework to extract the Sigma-Lognormal parameters, namely iDeLog. Specifically, iDeLog consists of two steps. The first one, influenced by the motor equivalence model, separately derives an initial action plan defined by a set of virtual points and angles from the trajectory and a sequence of lognormals from the velocity. In the second step, based on a hypothetical visual feedback compatible with an open-loop motor control, the virtual target points of the action plan are iteratively moved to improve the matching between the observed and reconstructed trajectory and velocity. During experiments conducted with handwritten signatures, iDeLog obtained promising results as compared to the previous development of the Sigma-Lognormal.</details>
**Abstract_cn:** <details><summary>译文: </summary>快速运动的运动学理论及其相关的西格玛-对数正态模型已广泛应用于各种应用中。虽然该模型的物理和生物学意义已针对快速运动进行了广泛的测试和验证，但在连续长时间且复杂的运动中使用时发现了一些缺点。为了缓解这些缺点，并受到运动等效理论和可想象的视觉反馈的启发，本文提出了一种提取 Sigma-Lognormal 参数的新颖框架，即 iDeLog。具体来说，iDeLog由两个步骤组成。第一个受电机等效模型的影响，分别导出由轨迹的一组虚拟点和角度以及速度的对数正态序列定义的初始动作计划。第二步，基于与开环电机控制兼容的假设视觉反馈，迭代移动行动计划的虚拟目标点，以改善观察到的和重建的轨迹和速度之间的匹配。在使用手写签名进行的实验中，与之前开发的 Sigma-Lognormal 相比，iDeLog 获得了有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.15473v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Temporal evolution in synthetic handwriting**<br />
**Title_cn:** 合成笔迹的时间演化<br />
**Authors:** Cristina Carmona-Duarte, Miguel A. Ferrer, Antonio Parziale, Angelo Marcelli<br />
**Abstract:** <details><summary>原文: </summary>New methods for generating synthetic handwriting images for biometric applications have recently been developed. The temporal evolution of handwriting from childhood to adulthood is usually left unexplored in these works. This paper proposes a novel methodology for including temporal evolution in a handwriting synthesizer by means of simplifying the text trajectory plan and handwriting dynamics. This is achieved through a tailored version of the kinematic theory of rapid human movements and the neuromotor inspired handwriting synthesizer. The realism of the proposed method has been evaluated by comparing the temporal evolution of real and synthetic samples both quantitatively and subjectively. The quantitative test is based on a visual perception algorithm that compares the letter variability and the number of strokes in the real and synthetic handwriting produced at different ages. In the subjective test, 30 people are asked to evaluate the perceived realism of the evolution of the synthetic handwriting.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近开发出了用于生物识别应用生成合成手写图像的新方法。这些作品通常没有探讨从童年到成年的笔迹的时间演变。本文提出了一种新颖的方法，通过简化文本轨迹计划和手写动态，将时间演化纳入手写合成器中。这是通过人体快速运动的运动学理论的定制版本和受神经运动启发的手写合成器来实现的。通过定量和主观地比较真实样本和合成样本的时间演化来评估所提出方法的真实性。定量测试基于视觉感知算法，该算法比较不同年龄的真实笔迹和合成笔迹的字母变异性和笔划数。在主观测试中，30 人被要求评估合成笔迹进化的感知真实性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15472v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-Robot Relative Pose Estimation in SE(2) with Observability Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph Optimization**<br />
**Title_cn:** SE(2) 中多机器人相对位姿估计与可观测性分析：扩展卡尔曼滤波和鲁棒位姿图优化的比较<br />
**Authors:** Kihoon Shin, Hyunjae Sim, Seungwon Nam, Yonghee Kim, Jae Hu, Kwang-Ki K. Kim<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we consider multi-robot localization problems with focus on cooperative localization and observability analysis of relative pose estimation. For cooperative localization, there is extra information available to each robot via communication network and message passing. If odometry data of a target robot can be transmitted to the ego-robot then the observability of their relative pose estimation can be achieved by range-only or bearing-only measurements provided both of their linear velocities are non-zero. If odometry data of a target robot is not directly transmitted but estimated by the ego-robot then there must be both range and bearing measurements to guarantee the observability of relative pose estimation. For ROS/Gazebo simulations, we consider four different sensing and communication structures in which extended Kalman filtering (EKF) and pose graph optimization (PGO) estimation with different robust loss functions (filtering and smoothing with different batch sizes of sliding window) are compared in terms of estimation accuracy. For hardware experiments, two Turtlebot3 equipped with UWB modules are used for real-world inter-robot relative pose estimation, in which both EKF and PGO are applied and compared.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们考虑多机器人定位问题，重点是协作定位和相对位姿估计的可观察性分析。对于协作定位，每个机器人可以通过通信网络和消息传递获得额外的信息。如果目标机器人的里程计数据可以传输到自我机器人，那么它们的相对位姿估计的可观测性可以通过仅距离或仅方位测量来实现，前提是它们的线速度都非零。如果目标机器人的里程计数据不是直接传输的，而是由自我机器人估计的，那么必须有距离和方位测量，以保证相对位姿估计的可观察性。对于 ROS/Gazebo 模拟，我们考虑四种不同的传感和通信结构，其中扩展卡尔曼滤波（EKF）和位姿图优化（PGO）估计与不同的鲁棒损失函数（使用不同批量大小的滑动窗口进行过滤和平滑）进行比较估计精度方面。在硬件实验中，使用两个配备UWB模块的Turtlebot3进行真实世界的机器人间相对位姿估计，其中EKF和PGO都被应用和比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.15313v1><br />
**Code:** null<br />

