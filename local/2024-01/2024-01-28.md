## [UPDATED!] **2024-01-28** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Prediction of Breast Cancer Recurrence Risk Using a Multi-Model Approach Integrating Whole Slide Imaging and Clinicopathologic Features**<br />
**Title_cn:** 使用整合全玻片成像和临床病理特征的多模型方法预测乳腺癌复发风险<br />
**Authors:** Manu Goyal, Jonathan D. Marotti, Adrienne A. Workman, Elaine P. Kuhn, Graham M. Tooker, Seth K. Ramin, Mary D. Chamberlin, Roberta M. diFlorio-Alexander, Saeed Hassanpour<br />
**Abstract:** <details><summary>原文: </summary>Breast cancer is the most common malignancy affecting women worldwide and is notable for its morphologic and biologic diversity, with varying risks of recurrence following treatment. The Oncotype DX Breast Recurrence Score test is an important predictive and prognostic genomic assay for estrogen receptor-positive breast cancer that guides therapeutic strategies; however, such tests can be expensive, delay care, and are not widely available. The aim of this study was to develop a multi-model approach integrating the analysis of whole slide images and clinicopathologic data to predict their associated breast cancer recurrence risks and categorize these patients into two risk groups according to the predicted score: low and high risk. The proposed novel methodology uses convolutional neural networks for feature extraction and vision transformers for contextual aggregation, complemented by a logistic regression model that analyzes clinicopathologic data for classification into two risk categories. This method was trained and tested on 993 hematoxylin and eosin-stained whole-slide images of breast cancers with corresponding clinicopathological features that had prior Oncotype DX testing. The model's performance was evaluated using an internal test set of 198 patients from Dartmouth Health and an external test set of 418 patients from the University of Chicago. The multi-model approach achieved an AUC of 0.92 (95 percent CI: 0.88-0.96) on the internal set and an AUC of 0.85 (95 percent CI: 0.79-0.90) on the external cohort. These results suggest that with further validation, the proposed methodology could provide an alternative to assist clinicians in personalizing treatment for breast cancer patients and potentially improving their outcomes.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳腺癌是影响全世界女性的最常见恶性肿瘤，以其形态和生物学多样性而闻名，治疗后复发的风险也各不相同。 Oncotype DX 乳腺癌复发评分测试是雌激素受体阳性乳腺癌的重要预测和预后基因组检测，可指导治疗策略；然而，此类测试可能价格昂贵、延误护理，并且无法广泛使用。本研究的目的是开发一种多模型方法，整合整个幻灯片图像和临床病理数据的分析，以预测其相关的乳腺癌复发风险，并根据预测评分将这些患者分为两个风险组：低风险和高风险。所提出的新颖方法使用卷积神经网络进行特征提取，使用视觉转换器进行上下文聚合，并辅以逻辑回归模型，分析临床病理数据以将其分为两个风险类别。该方法在 993 张苏木精和伊红染色的乳腺癌全玻片图像上进行了训练和测试，这些图像具有先前经过 Oncotype DX 测试的相应临床病理特征。该模型的性能使用达特茅斯健康中心 198 名患者的内部测试集和芝加哥大学 418 名患者的外部测试集进行评估。多模型方法在内部组上的 AUC 为 0.92（95% CI：0.88-0.96），在外部组上的 AUC 为 0.85（95% CI：0.79-0.90）。这些结果表明，通过进一步验证，所提出的方法可以提供一种替代方案，帮助临床医生对乳腺癌患者进行个性化治疗，并有可能改善其治疗结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.15805v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Real-time object detection and robotic manipulation for agriculture using a YOLO-based learning approach**<br />
**Title_cn:** 使用基于 YOLO 的学习方法进行农业实时物体检测和机器人操作<br />
**Authors:** Hongyu Zhao, Zezhi Tang, Zhenhong Li, Yi Dong, Yuancheng Si, Mingyang Lu, George Panoutsos<br />
**Abstract:** <details><summary>原文: </summary>The optimisation of crop harvesting processes for commonly cultivated crops is of great importance in the aim of agricultural industrialisation. Nowadays, the utilisation of machine vision has enabled the automated identification of crops, leading to the enhancement of harvesting efficiency, but challenges still exist. This study presents a new framework that combines two separate architectures of convolutional neural networks (CNNs) in order to simultaneously accomplish the tasks of crop detection and harvesting (robotic manipulation) inside a simulated environment. Crop images in the simulated environment are subjected to random rotations, cropping, brightness, and contrast adjustments to create augmented images for dataset generation. The you only look once algorithmic framework is employed with traditional rectangular bounding boxes for crop localization. The proposed method subsequently utilises the acquired image data via a visual geometry group model in order to reveal the grasping positions for the robotic manipulation.</details>
**Abstract_cn:** <details><summary>译文: </summary>优化常见作物的收获过程对于实现农业产业化具有重要意义。如今，机器视觉的利用已经实现了农作物的自动识别，从而提高了收获效率，但挑战仍然存在。这项研究提出了一个新的框架，它结合了两个独立的卷积神经网络（CNN）架构，以便在模拟环境中同时完成作物检测和收获（机器人操作）的任务。模拟环境中的裁剪图像经过随机旋转、裁剪、亮度和对比度调整，以创建用于数据集生成的增强图像。您只看一次的算法框架与传统的矩形边界框一起用于作物定位。所提出的方法随后通过视觉几何组模型利用获取的图像数据，以揭示机器人操纵的抓取位置。</details>
**PDF:** <http://arxiv.org/pdf/2401.15785v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion**<br />
**Title_cn:** 腹腔镜肝切除术术前至术中图像融合增强现实方法的客观比较<br />
**Authors:** Sharib Ali, Yamid Espinel, Yueming Jin, Peng Liu, Bianca Güttner, Xukun Zhang, Lihua Zhang, Tom Dowrick, Matthew J. Clarkson, Shiting Xiao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Augmented reality for laparoscopic liver resection is a visualisation mode that allows a surgeon to localise tumours and vessels embedded within the liver by projecting them on top of a laparoscopic image. Preoperative 3D models extracted from CT or MRI data are registered to the intraoperative laparoscopic images during this process. In terms of 3D-2D fusion, most of the algorithms make use of anatomical landmarks to guide registration. These landmarks include the liver's inferior ridge, the falciform ligament, and the occluding contours. They are usually marked by hand in both the laparoscopic image and the 3D model, which is time-consuming and may contain errors if done by a non-experienced user. Therefore, there is a need to automate this process so that augmented reality can be used effectively in the operating room. We present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge (P2ILF), held during the Medical Imaging and Computer Assisted Interventions (MICCAI 2022) conference, which investigates the possibilities of detecting these landmarks automatically and using them in registration. The challenge was divided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D registration task. The teams were provided with training data consisting of 167 laparoscopic images and 9 preoperative 3D models from 9 patients, with the corresponding 2D and 3D landmark annotations. A total of 6 teams from 4 countries participated, whose proposed methods were evaluated on 16 images and two preoperative 3D models from two patients. All the teams proposed deep learning-based methods for the 2D and 3D landmark segmentation tasks and differentiable rendering-based methods for the registration task. Based on the experimental outcomes, we propose three key hypotheses that determine current limitations and future directions for research in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>腹腔镜肝脏切除术的增强现实是一种可视化模式，允许外科医生通过将嵌入肝脏内的肿瘤和血管投影到腹腔镜图像上来定位它们。在此过程中，从 CT 或 MRI 数据提取的术前 3D 模型将被注册到术中腹腔镜图像中。在3D-2D融合方面，大多数算法利用解剖标志来指导配准。这些标志包括肝脏的下脊、镰状韧带和闭塞轮廓。它们通常在腹腔镜图像和 3D 模型中都是手工标记的，这非常耗时，而且如果由无经验的用户完成，可能会出现错误。因此，需要使该过程自动化，以便增强现实可以在手术室中有效使用。我们提出了在医学影像和计算机辅助干预 (MICCAI 2022) 会议期间举行的术前至术中腹腔镜融合挑战 (P2ILF)，该挑战研究了自动检测这些标志并在配准中使用它们的可能性。该挑战分为两个任务：1）2D 和 3D 地标检测任务和 2）3D-2D 配准任务。团队获得的训练数据包括来自 9 名患者的 167 张腹腔镜图像和 9 个术前 3D 模型，以及相应的 2D 和 3D 标志注释。共有来自 4 个国家的 6 个团队参加，他们提出的方法在来自两名患者的 16 张图像和两个术前 3D 模型上进行了评估。所有团队都提出了用于 2D 和 3D 地标分割任务的基于深度学习的方法，以及用于配准任务的基于可微渲染的方法。根据实验结果，我们提出了三个关键假设，确定了该领域研究的当前局限性和未来方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.15753v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks**<br />
**Title_cn:** SERNet-Former：通过具有注意力增强门和注意力融合网络的高效残差网络进行语义分割<br />
**Authors:** Serdar Erisen<br />
**Abstract:** <details><summary>原文: </summary>Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes datasets, and the proposed methods reveal significant improvements on the existing baselines, such as ResNet-50. To the best of our knowledge, the developed network, SERNet-Former, achieves state-of-the-art results (84.62 % mean IoU) on CamVid dataset and challenging results (87.35 % mean IoU) on Cityscapes validation dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>提高语义分割中最先进方法的效率需要克服不断增加的计算成本以及融合来自全局和局部上下文的语义信息等问题。基于最近卷积神经网络（CNN）在语义分割方面的成功和遇到的问题，本研究提出了一种具有独特高效残差网络的编码器-解码器架构。部署注意力增强门（AbG）和注意力增强模块（AbM）的目的是将基于特征的语义信息与编码器中高效残差网络的全局上下文融合。解码器网络分别是通过受 AbM 启发的附加注意力融合网络（AfNs）开发的。 AfN 旨在通过在解码器部分部署额外的卷积层来提高语义信息一对一转换的效率。我们的网络在具有挑战性的 CamVid 和 Cityscapes 数据集上进行了测试，所提出的方法揭示了对现有基线（例如 ResNet-50）的显着改进。据我们所知，开发的网络 SERNet-Former 在 CamVid 数据集上取得了最先进的结果（平均 IoU 为 84.62%），在 Cityscapes 验证数据集上取得了具有挑战性的结果（平均 IoU 为 87.35%）。</details>
**PDF:** <http://arxiv.org/pdf/2401.15741v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **SegmentAnyTree: A sensor and platform agnostic deep learning model for tree segmentation using laser scanning data**<br />
**Title_cn:** SegmentAnyTree：使用激光扫描数据进行树木分割的与传感器和平台无关的深度学习模型<br />
**Authors:** Maciej Wielgosz, Stefano Puliti, Binbin Xiang, Konrad Schindler, Rasmus Astrup<br />
**Abstract:** <details><summary>原文: </summary>This research advances individual tree crown (ITC) segmentation in lidar data, using a deep learning model applicable to various laser scanning types: airborne (ULS), terrestrial (TLS), and mobile (MLS). It addresses the challenge of transferability across different data characteristics in 3D forest scene analysis. The study evaluates the model's performance based on platform (ULS, MLS) and data density, testing five scenarios with varying input data, including sparse versions, to gauge adaptability and canopy layer efficacy. The model, based on PointGroup architecture, is a 3D CNN with separate heads for semantic and instance segmentation, validated on diverse point cloud datasets. Results show point cloud sparsification enhances performance, aiding sparse data handling and improving detection in dense forests. The model performs well with >50 points per sq. m densities but less so at 10 points per sq. m due to higher omission rates. It outperforms existing methods (e.g., Point2Tree, TLS2trees) in detection, omission, commission rates, and F1 score, setting new benchmarks on LAUTx, Wytham Woods, and TreeLearn datasets. In conclusion, this study shows the feasibility of a sensor-agnostic model for diverse lidar data, surpassing sensor-specific approaches and setting new standards in tree segmentation, particularly in complex forests. This contributes to future ecological modeling and forest management advancements.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究使用适用于各种激光扫描类型的深度学习模型，推进了激光雷达数据中的个体树冠 (ITC) 分割：机载 (ULS)、地面 (TLS) 和移动 (MLS)。它解决了 3D 森林场景分析中不同数据特征的可转移性挑战。该研究根据平台（ULS、MLS）和数据密度评估模型的性能，使用不同的输入数据（包括稀疏版本）测试五种场景，以衡量适应性和冠层功效。该模型基于 PointGroup 架构，是一个 3D CNN，具有用于语义和实例分割的独立头，并在不同的点云数据集上进行了验证。结果表明，点云稀疏化可以提高性能，有助于稀疏数据处理并改善茂密森林中的检测。该模型在每平方米 > 50 个点的密度下表现良好，但由于遗漏率较高，在每平方米 10 个点的密度下表现较差。它在检测、遗漏、佣金率和 F1 分数方面优于现有方法（例如 Point2Tree、TLS2trees），为 LAUTx、Wytham Woods 和 TreeLearn 数据集设定了新基准。总之，这项研究表明了针对不同激光雷达数据的传感器不可知模型的可行性，超越了特定于传感器的方法，并为树木分割（特别是在复杂的森林中）设定了新标准。这有助于未来的生态建模和森林管理的进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.15739v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Study of Acquisition Functions for Medical Imaging Deep Active Learning**<br />
**Title_cn:** 医学影像深度主动学习采集函数研究<br />
**Authors:** Bonaventure F. P. Dossou<br />
**Abstract:** <details><summary>原文: </summary>The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where obtaining labeled data (or annotation budget is very limited). We compare several selection criteria (BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the effect of acquired pool size on the model's performance. Our results suggest that uncertainty is useful to the Melanoma detection task, and confirms the hypotheses of the author of the paper of interest, that \textit{bald} performs on average better than other acquisition functions. Our extended analyses however revealed that all acquisition functions perform badly on the positive (cancerous) samples, suggesting exploitation of class unbalance, which could be crucial in real-world settings. We finish by suggesting future work directions that would be useful to improve this current work. The code of our implementation is open-sourced at \url{https://github.com/bonaventuredossou/ece526_course_project}</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度学习革命取得了突破性的成就。从乳腺癌检测到蛋白质折叠，深度学习算法一直是非常重要进步的核心。然而，这些现代进步变得越来越需要数据，尤其是可用性稀缺的标记数据：这在医学背景下更为普遍。在这项工作中，我们展示了主动学习如何在数据稀缺的情况下非常有效，在这种情况下获取标记数据（或注释预算非常有限）。我们在 ISIC 2016 数据集上比较了几个选择标准（BALD、MeanSTD 和 MaxEntropy）。我们还探讨了获取的池大小对模型性能的影响。我们的结果表明，不确定性对于黑色素瘤检测任务很有用，并证实了相关论文作者的假设，即 \textit{bald} 的平均性能优于其他采集函数。然而，我们的扩展分析表明，所有采集函数在阳性（癌性）样本上都表现不佳，这表明利用了类别不平衡，这在现实世界中可能至关重要。最后，我们建议未来的工作方向，这将有助于改进当前的工作。我们的实现代码是开源的，位于 \url{https://github.com/bonaventuredossou/ece526_course_project}</details>
**PDF:** <http://arxiv.org/pdf/2401.15721v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Detection of a facemask in real-time using deep learning methods: Prevention of Covid 19**<br />
**Title_cn:** 使用深度学习方法实时检测口罩：预防 Covid 19<br />
**Authors:** Gautam Siddharth Kashyap, Jatin Sohlot, Ayesha Siddiqui, Ramsha Siddiqui, Karan Malik, Samar Wazir, Alexander E. I. Brownlee<br />
**Abstract:** <details><summary>原文: </summary>A health crisis is raging all over the world with the rapid transmission of the novel-coronavirus disease (Covid-19). Out of the guidelines issued by the World Health Organisation (WHO) to protect us against Covid-19, wearing a facemask is the most effective. Many countries have necessitated the wearing of face masks, but monitoring a large number of people to ensure that they are wearing masks in a crowded place is a challenging task in itself. The novel-coronavirus disease (Covid-19) has already affected our day-to-day life as well as world trade movements. By the end of April 2021, the world has recorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19) including 3,066,113 deaths according to the world health organization (WHO). These increasing numbers motivate automated techniques for the detection of a facemask in real-time scenarios for the prevention of Covid-19. We propose a technique using deep learning that works for single and multiple people in a frame recorded via webcam in still or in motion. We have also experimented with our approach in night light. The accuracy of our model is good compared to the other approaches in the literature; ranging from 74% for multiple people in a nightlight to 99% for a single person in daylight.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着新型冠状病毒（Covid-19）的迅速传播，健康危机正在全世界肆虐。在世界卫生组织 (WHO) 发布的保护我们免受 Covid-19 感染的指南中，戴口罩是最有效的。许多国家都要求佩戴口罩，但监控大量人员以确保他们在拥挤的地方佩戴口罩本身就是一项具有挑战性的任务。新型冠状病毒疾病（Covid-19）已经影响了我们的日常生活以及世界贸易运动。据世界卫生组织 (WHO) 统计，截至 2021 年 4 月，全球已确诊 144,358,956 例新型冠状病毒疾病 (Covid-19) 病例，其中 3,066,113 例死亡。这些不断增加的数字激发了在实时场景中检测口罩的自动化技术，以预防 Covid-19。我们提出了一种使用深度学习的技术，适用于通过网络摄像头记录的静态或动态帧中的单个人和多人。我们还在夜光下尝试了我们的方法。与文献中的其他方法相比，我们的模型的准确性很好；范围从夜光下多人的 74% 到日光下单人的 99%。</details>
**PDF:** <http://arxiv.org/pdf/2401.15675v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes**<br />
**Title_cn:** 嘴唇在说谎：发现口型同步 DeepFakes 中音频和视觉之间的时间不一致<br />
**Authors:** Weifeng Liu, Tianyi She, Jiawei Liu, Run Wang, Dongyu Yao, Ziyou Liang<br />
**Abstract:** <details><summary>原文: </summary>In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, whereas these methods also pose potential and severe security threats to humanity. DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud. However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods. Our preliminary experiments have shown that the effectiveness of the existing methods often drastically decreases or even fails when tackling lip-syncing videos.   In this paper, for the first time, we propose a novel approach dedicated to lip-forgery identification that exploits the inconsistency between lip movements and audio signals. We also mimic human natural cognition by capturing subtle biological links between lips and head regions to boost accuracy. To better illustrate the effectiveness and advances of our proposed method, we curate a high-quality LipSync dataset by employing the SOTA lip generator. We hope this high-quality and diverse dataset could be well served the further research on this challenging and interesting field. Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines. Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations. Our method achieves an accuracy of up to 90.2% in real-world scenarios (e.g., WeChat video call) and shows its powerful capabilities in real scenario deployment. To facilitate the progress of this research community, we release all resources at https://github.com/AaronComo/LipFD.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，DeepFake技术在高质量视频合成方面取得了前所未有的成功，但这些方法也给人类带来了潜在的、严重的安全威胁。 DeepFake 可以分为娱乐应用（例如面部交换）和非法用途（例如口型同步欺诈）。然而，口型伪造视频既不改变身份，也不具有可辨别的视觉伪影，对现有的 DeepFake 检测方法提出了巨大的挑战。我们的初步实验表明，在处理口型同步视频时，现有方法的有效性往往会急剧下降甚至失败。在本文中，我们首次提出了一种致力于嘴唇伪造识别的新颖方法，该方法利用嘴唇运动和音频信号之间的不一致。我们还通过捕捉嘴唇和头部区域之间微妙的生物联系来模仿人类的自然认知，以提高准确性。为了更好地说明我们提出的方法的有效性和进步，我们通过采用 SOTA 唇形生成器来策划高质量的 LipSync 数据集。我们希望这个高质量和多样化的数据集能够很好地服务于这个具有挑战性和有趣的领域的进一步研究。实验结果表明，我们的方法在识别口型同步视频方面的平均准确率超过 95.3%，明显优于基线。大量的实验证明了处理深度伪造的能力以及在不同输入转换中的鲁棒性。我们的方法在现实场景（例如微信视频通话）中实现了高达 90.2% 的准确率，并在实际场景部署中展现了其强大的能力。为了促进该研究社区的进步，我们在 https://github.com/AaronComo/LipFD 上发布了所有资源。</details>
**PDF:** <http://arxiv.org/pdf/2401.15668v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Data-Free Generalized Zero-Shot Learning**<br />
**Title_cn:** 无数据广义零样本学习<br />
**Authors:** Bowen Tang, Long Yan, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu<br />
**Abstract:** <details><summary>原文: </summary>Deep learning models have the ability to extract rich knowledge from large-scale datasets. However, the sharing of data has become increasingly challenging due to concerns regarding data copyright and privacy. Consequently, this hampers the effective transfer of knowledge from existing data to novel downstream tasks and concepts. Zero-shot learning (ZSL) approaches aim to recognize new classes by transferring semantic knowledge learned from base classes. However, traditional generative ZSL methods often require access to real images from base classes and rely on manually annotated attributes, which presents challenges in terms of data restrictions and model scalability. To this end, this paper tackles a challenging and practical problem dubbed as data-free zero-shot learning (DFZSL), where only the CLIP-based base classes data pre-trained classifier is available for zero-shot classification. Specifically, we propose a generic framework for DFZSL, which consists of three main components. Firstly, to recover the virtual features of the base data, we model the CLIP features of base class images as samples from a von Mises-Fisher (vMF) distribution based on the pre-trained classifier. Secondly, we leverage the text features of CLIP as low-cost semantic information and propose a feature-language prompt tuning (FLPT) method to further align the virtual image features and textual features. Thirdly, we train a conditional generative model using the well-aligned virtual image features and corresponding semantic text features, enabling the generation of new classes features and achieve better zero-shot generalization. Our framework has been evaluated on five commonly used benchmarks for generalized ZSL, as well as 11 benchmarks for the base-to-new ZSL. The results demonstrate the superiority and effectiveness of our approach. Our code is available in https://github.com/ylong4/DFZSL</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型能够从大规模数据集中提取丰富的知识。然而，由于对数据版权和隐私的担忧，数据共享变得越来越具有挑战性。因此，这阻碍了知识从现有数据到新的下游任务和概念的有效转移。零样本学习（ZSL）方法旨在通过转移从基类学到的语义知识来识别新类。然而，传统的生成式 ZSL 方法通常需要从基类访问真实图像，并依赖于手动注释的属性，这在数据限制和模型可扩展性方面提出了挑战。为此，本文解决了一个具有挑战性且实际的问题，称为无数据零样本学习（DFZSL），其中只有基于 CLIP 的基类数据预训练分类器可用于零样本分类。具体来说，我们提出了 DFZSL 的通用框架，它由三个主要组件组成。首先，为了恢复基础数据的虚拟特征，我们将基类图像的 CLIP 特征建模为基于预训练分类器的 von Mises-Fisher (vMF) 分布的样本。其次，我们利用 CLIP 的文本特征作为低成本语义信息，并提出一种特征语言提示调整（FLPT）方法来进一步对齐虚拟图像特征和文本特征。第三，我们使用对齐良好的虚拟图像特征和相应的语义文本特征训练条件生成模型，从而能够生成新的类特征并实现更好的零样本泛化。我们的框架已根据通用 ZSL 的 5 个常用基准以及基础到新 ZSL 的 11 个基准进行了评估。结果证明了我们方法的优越性和有效性。我们的代码可以在 https://github.com/ylong4/DFZSL 中找到</details>
**PDF:** <http://arxiv.org/pdf/2401.15657v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **UP-CrackNet: Unsupervised Pixel-Wise Road Crack Detection via Adversarial Image Restoration**<br />
**Title_cn:** UP-CrackNet：通过对抗性图像恢复进行无监督逐像素道路裂缝检测<br />
**Authors:** Nachuan Ma, Rui Fan, Lihua Xie<br />
**Abstract:** <details><summary>原文: </summary>Over the past decade, automated methods have been developed to detect cracks more efficiently, accurately, and objectively, with the ultimate goal of replacing conventional manual visual inspection techniques. Among these methods, semantic segmentation algorithms have demonstrated promising results in pixel-wise crack detection tasks. However, training such data-driven algorithms requires a large amount of human-annotated datasets with pixel-level annotations, which is a highly labor-intensive and time-consuming process. Moreover, supervised learning-based methods often struggle with poor generalization ability in unseen datasets. Therefore, we propose an unsupervised pixel-wise road crack detection network, known as UP-CrackNet. Our approach first generates multi-scale square masks and randomly selects them to corrupt undamaged road images by removing certain regions. Subsequently, a generative adversarial network is trained to restore the corrupted regions by leveraging the semantic context learned from surrounding uncorrupted regions. During the testing phase, an error map is generated by calculating the difference between the input and restored images, which allows for pixel-wise crack detection. Our comprehensive experimental results demonstrate that UP-CrackNet outperforms other general-purpose unsupervised anomaly detection algorithms, and exhibits comparable performance and superior generalizability when compared with state-of-the-art supervised crack segmentation algorithms. Our source code is publicly available at mias.group/UP-CrackNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年中，自动化方法已经被开发出来，可以更有效、更准确、更客观地检测裂缝，最终目标是取代传统的手动目视检查技术。在这些方法中，语义分割算法在像素级裂纹检测任务中表现出了有希望的结果。然而，训练这种数据驱动的算法需要大量具有像素级注释的人工注释数据集，这是一个高度劳动密集型且耗时的过程。此外，基于监督学习的方法通常在未见过的数据集中存在泛化能力差的问题。因此，我们提出了一种无监督的逐像素道路裂缝检测网络，称为 UP-CrackNet。我们的方法首先生成多尺度方形掩模，并随机选择它们通过删除某些区域来破坏未损坏的道路图像。随后，训练生成对抗网络，利用从周围未损坏区域学习到的语义上下文来恢复损坏区域。在测试阶段，通过计算输入图像和恢复图像之间的差异来生成误差图，从而可以进行像素级裂纹检测。我们全面的实验结果表明，UP-CrackNet 优于其他通用无监督异常检测算法，并且与最先进的有监督裂纹分割算法相比，表现出可比的性能和卓越的通用性。我们的源代码可在 mias.group/UP-CrackNet 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.15647v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Cyto R-CNN and CytoNuke Dataset: Towards reliable whole-cell segmentation in bright-field histological images**<br />
**Title_cn:** Cyto R-CNN 和 CytoNuke 数据集：在明场组织学图像中实现可靠的全细胞分割<br />
**Authors:** Johannes Raufeisen, Kunpeng Xie, Fabian Hörst, Till Braunschweig, Jianning Li, Jens Kleesiek, Rainer Röhrig, Jan Egger, Bastian Leibe, Frank Hölzle, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Background: Cell segmentation in bright-field histological slides is a crucial topic in medical image analysis. Having access to accurate segmentation allows researchers to examine the relationship between cellular morphology and clinical observations. Unfortunately, most segmentation methods known today are limited to nuclei and cannot segmentate the cytoplasm.   Material & Methods: We present a new network architecture Cyto R-CNN that is able to accurately segment whole cells (with both the nucleus and the cytoplasm) in bright-field images. We also present a new dataset CytoNuke, consisting of multiple thousand manual annotations of head and neck squamous cell carcinoma cells. Utilizing this dataset, we compared the performance of Cyto R-CNN to other popular cell segmentation algorithms, including QuPath's built-in algorithm, StarDist and Cellpose. To evaluate segmentation performance, we calculated AP50, AP75 and measured 17 morphological and staining-related features for all detected cells. We compared these measurements to the gold standard of manual segmentation using the Kolmogorov-Smirnov test.   Results: Cyto R-CNN achieved an AP50 of 58.65\% and an AP75 of 11.56\% in whole-cell segmentation, outperforming all other methods (QuPath $19.46/0.91\%$; StarDist $45.33/2.32\%$; Cellpose $31.85/5.61\%$). Cell features derived from Cyto R-CNN showed the best agreement to the gold standard ($\bar{D} = 0.15$) outperforming QuPath ($\bar{D} = 0.22$), StarDist ($\bar{D} = 0.25$) and Cellpose ($\bar{D} = 0.23$).   Conclusion: Our newly proposed Cyto R-CNN architecture outperforms current algorithms in whole-cell segmentation while providing more reliable cell measurements than any other model. This could improve digital pathology workflows, potentially leading to improved diagnosis. Moreover, our published dataset can be used to develop further models in the future.</details>
**Abstract_cn:** <details><summary>译文: </summary>背景：明场组织学切片中的细胞分割是医学图像分析中的一个关键主题。获得准确的分割使研究人员能够检查细胞形态与临床观察之间的关系。不幸的是，当今已知的大多数分割方法仅限于细胞核，无法分割细胞质。材料和方法：我们提出了一种新的网络架构 Cyto R-CNN，它能够在明场图像中准确分割整个细胞（包括细胞核和细胞质）。我们还提出了一个新的数据集 CytoNuke，其中包含数千个头颈鳞状细胞癌细胞的手动注释。利用该数据集，我们将 Cyto R-CNN 的性能与其他流行的细胞分割算法（包括 QuPath 的内置算法 StarDist 和 Cellpose）进行了比较。为了评估分割性能，我们计算了 AP50、AP75，并测量了所有检测到的细胞的 17 个形态和染色相关特征。我们使用 Kolmogorov-Smirnov 测试将这些测量结果与手动分割的黄金标准进行了比较。结果：Cyto R-CNN 在全细胞分割中取得了 58.65\% 的 AP50 和 11.56\% 的 AP75，优于所有其他方法（QuPath $19.46/0.91\%$；StarDist $45.33/2.32\%$；Cellpose $31.85/ 5.61\%$）。来自 Cyto R-CNN 的细胞特征显示出与黄金标准 ($\bar{D} = 0.15$) 的最佳一致性，优于 QuPath ($\bar{D} = 0.22$)、StarDist ($\bar{D} = 0.25$) 和 Cellpose ($\bar{D} = 0.23$)。结论：我们新提出的 Cyto R-CNN 架构在全细胞分割方面优于当前算法，同时提供比任何其他模型更可靠的细胞测量。这可以改善数字病理学工作流程，从而有可能改善诊断。此外，我们发布的数据集可用于将来开发进一步的模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.15638v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **SCTransNet: Spatial-channel Cross Transformer Network for Infrared Small Target Detection**<br />
**Title_cn:** SCTransNet：用于红外小目标检测的空间通道交叉变压器网络<br />
**Authors:** Shuai Yuan, Hanlin Qin, Xiang Yan, Naveed AKhtar, Ajmal Mian<br />
**Abstract:** <details><summary>原文: </summary>Infrared small target detection (IRSTD) has recently benefitted greatly from U-shaped neural models. However, largely overlooking effective global information modeling, existing techniques struggle when the target has high similarities with the background. We present a Spatial-channel Cross Transformer Network (SCTransNet) that leverages spatial-channel cross transformer blocks (SCTBs) on top of long-range skip connections to address the aforementioned challenge. In the proposed SCTBs, the outputs of all encoders are interacted with cross transformer to generate mixed features, which are redistributed to all decoders to effectively reinforce semantic differences between the target and clutter at full scales. Specifically, SCTB contains the following two key elements: (a) spatial-embedded single-head channel-cross attention (SSCA) for exchanging local spatial features and full-level global channel information to eliminate ambiguity among the encoders and facilitate high-level semantic associations of the images, and (b) a complementary feed-forward network (CFN) for enhancing the feature discriminability via a multi-scale strategy and cross-spatial-channel information interaction to promote beneficial information transfer. Our SCTransNet effectively encodes the semantic differences between targets and backgrounds to boost its internal representation for detecting small infrared targets accurately. Extensive experiments on three public datasets, NUDT-SIRST, NUAA-SIRST, and IRSTD-1k, demonstrate that the proposed SCTransNet outperforms existing IRSTD methods. Our code will be made public at https://github.com/xdFai.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外小目标检测（IRSTD）最近从 U 形神经模型中受益匪浅。然而，在很大程度上忽视了有效的全局信息建模，当目标与背景高度相似时，现有技术就会陷入困境。我们提出了一种空间通道交叉变压器网络（SCTransNet），它利用远程跳跃连接之上的空间通道交叉变压器块（SCTB）来解决上述挑战。在所提出的 SCTB 中，所有编码器的输出与交叉变换器交互以生成混合特征，这些特征被重新分配给所有解码器，以有效地增强全尺寸目标和杂波之间的语义差异。具体来说，SCTB包含以下两个关键要素：（a）空间嵌入式单头通道交叉注意（SSCA），用于交换局部空间特征和全级全局通道信息，以消除编码器之间的歧义并促进高级语义图像的关联，以及（b）补充前馈网络（CFN），用于通过多尺度策略和跨空间通道信息交互来增强特征可辨别性，以促进有益的信息传递。我们的 SCTransNet 有效地编码了目标和背景之间的语义差异，以增强其内部表示，从而准确地检测小型红外目标。对三个公共数据集 NUDT-SIRST、NUAA-SIRST 和 IRSTD-1k 的广泛实验表明，所提出的 SCTransNet 优于现有的 IRSTD 方法。我们的代码将在 https://github.com/xdFai 公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.15583v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **ARCNet: An Asymmetric Residual Wavelet Column Correction Network for Infrared Image Destriping**<br />
**Title_cn:** ARCNet：用于红外图像去条纹的非对称残余小波列校正网络<br />
**Authors:** Shuai Yuan, Hanlin Qin, Xiang Yan, Naveed Akhtar, Shiqi Yang, Shuowen Yang<br />
**Abstract:** <details><summary>原文: </summary>Infrared image destriping seeks to restore high-quality content from degraded images. Recent works mainly address this task by leveraging prior knowledge to separate stripe noise from the degraded image. However, constructing a robust decoupling model for that purpose remains challenging, especially when significant similarities exist between the stripe noise and vertical background structure. Addressing that, we introduce Asymmetric Residual wavelet Column correction Network (ARCNet) for image destriping, aiming to consistently preserve spatially precise high-resolution representations. Our neural model leverages a novel downsampler, residual haar discrete wavelet transform (RHDWT), stripe directional prior knowledge and data-driven learning to induce a model with enriched feature representation of stripe noise and background. In our technique, the inverse wavelet transform is replaced by transposed convolution for feature upsampling, which can suppress noise crosstalk and encourage the network to focus on robust image reconstruction. After each sampling, a proposed column non-uniformity correction module (CNCM) is leveraged by our method to enhance column uniformity, spatial correlation, and global self-dependence between each layer component. CNCM can establish structural characteristics of stripe noise and utilize contextual information at long-range dependencies to distinguish stripes with varying intensities and distributions. Extensive experiments on synthetic data, real data, and infrared small target detection tasks show that the proposed method outperforms state-of-the-art single-image destriping methods both visually and quantitatively by a considerable margin. Our code will be made publicly available at \url{https://github.com/xdFai}.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外图像去条纹旨在从降级图像中恢复高质量内容。最近的工作主要通过利用先验知识从退化图像中分离条纹噪声来解决此任务。然而，为此目的构建鲁棒的解耦模型仍然具有挑战性，特别是当条纹噪声和垂直背景结构之间存在显着相似性时。为了解决这个问题，我们引入了用于图像去条纹的非对称残差小波列校正网络（ARCNet），旨在一致地保留空间精确的高分辨率表示。我们的神经模型利用新颖的下采样器、残差哈尔离散小波变换（RHDWT）、条纹方向先验知识和数据驱动的学习来诱导模型具有条纹噪声和背景的丰富特征表示。在我们的技术中，逆小波变换被转置卷积取代以进行特征上采样，这可以抑制噪声串扰并鼓励网络专注于鲁棒的图像重建。每次采样后，我们的方法都会利用所提出的列非均匀性校正模块（CNCM）来增强每个层组件之间的列均匀性、空间相关性和全局自相关性。 CNCM 可以建立条纹噪声的结构特征，并利用长距离依赖性的上下文信息来区分具有不同强度和分布的条纹。对合成数据、真实数据和红外小目标检测任务的大量实验表明，所提出的方法在视觉和定量上都远远优于最先进的单图像去条纹方法。我们的代码将在 \url{https://github.com/xdFai} 上公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.15578v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance**<br />
**Title_cn:** Media2Face：多模态指导下的共同语音面部动画生成<br />
**Authors:** Qingcheng Zhao, Pengyu Long, Qixuan Zhang, Dafei Qin, Han Liang, Longwen Zhang, Yingliang Zhang, Jingyi Yu, Lan Xu<br />
**Abstract:** <details><summary>原文: </summary>The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of lexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotional and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.</details>
**Abstract_cn:** <details><summary>译文: </summary>从语音合成 3D 面部动画已经引起了相当多的关注。由于缺乏高质量的 4D 面部数据和注释丰富的多模态标签，以前的方法常常受到现实性有限和缺乏灵活调节的困扰。我们通过三部曲来应对这一挑战。我们首先介绍广义神经参数面部资产（GNPFA），这是一种高效的变分自动编码器，可将面部几何形状和图像映射到高度广义的表情潜在空间，从而解耦表情和身份。然后，我们利用 GNPFA 从大量视频中提取高质量的表情和准确的头部姿势。这展示了 M2F-D 数据集，这是一个大型、多样化的扫描级协同语音 3D 面部动画数据集，具有注释良好的情感和风格标签。最后，我们提出了 Media2Face，一种 GNPFA 潜在空间中的扩散模型，用于生成共同语音面部动画，接受来自音频、文本和图像的丰富的多模态指导。大量实验表明，我们的模型不仅在面部动画合成中实现了高保真度，而且拓宽了3D面部动画的表现力和风格适应性的范围。</details>
**PDF:** <http://arxiv.org/pdf/2401.15687v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach**<br />
**Title_cn:** 通过位置查询和基于扩散的方法一步连续多图像绘制<br />
**Authors:** Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, Junchi Yan<br />
**Abstract:** <details><summary>原文: </summary>Image outpainting aims to generate the content of an input sub-image beyond its original boundaries. It is an important task in content generation yet remains an open problem for generative models. This paper pushes the technical frontier of image outpainting in two directions that have not been resolved in literature: 1) outpainting with arbitrary and continuous multiples (without restriction), and 2) outpainting in a single step (even for large expansion multiples). Moreover, we develop a method that does not depend on a pre-trained backbone network, which is in contrast commonly required by the previous SOTA outpainting methods. The arbitrary multiple outpainting is achieved by utilizing randomly cropped views from the same image during training to capture arbitrary relative positional information. Specifically, by feeding one view and positional embeddings as queries, we can reconstruct another view. At inference, we generate images with arbitrary expansion multiples by inputting an anchor image and its corresponding positional embeddings. The one-step outpainting ability here is particularly noteworthy in contrast to previous methods that need to be performed for $N$ times to obtain a final multiple which is $N$ times of its basic and fixed multiple. We evaluate the proposed approach (called PQDiff as we adopt a diffusion-based generator as our embodiment, under our proposed \textbf{P}ositional \textbf{Q}uery scheme) on public benchmarks, demonstrating its superior performance over state-of-the-art approaches. Specifically, PQDiff achieves state-of-the-art FID scores on the Scenery (\textbf{21.512}), Building Facades (\textbf{25.310}), and WikiArts (\textbf{36.212}) datasets. Furthermore, under the 2.25x, 5x and 11.7x outpainting settings, PQDiff only takes \textbf{40.6\%}, \textbf{20.3\%} and \textbf{10.2\%} of the time of the benchmark state-of-the-art (SOTA) method.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像绘制的目的是生成输入子图像超出其原始边界的内容。这是内容生成中的一项重要任务，但对于生成模型来说仍然是一个悬而未决的问题。本文将图像绘制的技术前沿推向了文献中尚未解决的两个方向：1）任意连续倍数的绘制（无限制），2）一步绘制（即使是大的扩展倍数）。此外，我们开发了一种不依赖于预先训练的骨干网络的方法，这与之前的 SOTA 外画方法通常需要的不同。任意多重外画是通过在训练期间利用来自同一图像的随机裁剪视图来捕获任意相对位置信息来实现的。具体来说，通过提供一个视图和位置嵌入作为查询，我们可以重建另一个视图。在推理时，我们通过输入锚图像及其相应的位置嵌入来生成具有任意扩展倍数的图像。与之前需要执行 $N$ 次才能获得最终倍数（其基本倍数和固定倍数的 $N$ 倍）相比，这里的一步绘制能力尤其值得注意。我们在公共基准上评估了所提出的方法（称为 PQDiff，因为我们采用基于扩散的生成器作为我们的实施例，根据我们提出的 \textbf{P}ositional \textbf{Q}uery 方案），证明了其优于状态的性能最先进的方法。具体来说，PQDiff 在风景 (\textbf{21.512})、建筑立面 (\textbf{25.310}) 和 WikiArts (\textbf{36.212}) 数据集上实现了最先进的 FID 分数。此外，在 2.25x、5x 和 11.7x 绘制设置下，PQDiff 仅采用基准状态时间的 \textbf{40.6\%}、\textbf{20.3\%} 和 \textbf{10.2\%}最先进的（SOTA）方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.15652v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CPDM: Content-Preserving Diffusion Model for Underwater Image Enhancement**<br />
**Title_cn:** CPDM：用于水下图像增强的内容保留扩散模型<br />
**Authors:** Xiaowen Shi, Yuan-Gen Wang<br />
**Abstract:** <details><summary>原文: </summary>Underwater image enhancement (UIE) is challenging since image degradation in aquatic environments is complicated and changing over time. Existing mainstream methods rely on either physical-model or data-driven, suffering from performance bottlenecks due to changes in imaging conditions or training instability. In this article, we make the first attempt to adapt the diffusion model to the UIE task and propose a Content-Preserving Diffusion Model (CPDM) to address the above challenges. CPDM first leverages a diffusion model as its fundamental model for stable training and then designs a content-preserving framework to deal with changes in imaging conditions. Specifically, we construct a conditional input module by adopting both the raw image and the difference between the raw and noisy images as the input, which can enhance the model's adaptability by considering the changes involving the raw images in underwater environments. To preserve the essential content of the raw images, we construct a content compensation module for content-aware training by extracting low-level features from the raw images. Extensive experimental results validate the effectiveness of our CPDM, surpassing the state-of-the-art methods in terms of both subjective and objective metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>水下图像增强 (UIE) 具有挑战性，因为水生环境中的图像退化非常复杂并且会随着时间的推移而变化。现有的主流方法要么依赖物理模型，要么依赖数据驱动，由于成像条件的变化或训练的不稳定而遭受性能瓶颈。在本文中，我们首次尝试将扩散模型应用于 UIE 任务，并提出了内容保留扩散模型（CPDM）来解决上述挑战。 CPDM首先利用扩散模型作为稳定训练的基本模型，然后设计一个内容保留框架来应对成像条件的变化。具体来说，我们构建了一个条件输入模块，采用原始图像以及原始图像与噪声图像之间的差异作为输入，通过考虑原始图像在水下环境中的变化来增强模型的适应性。为了保留原始图像的基本内容，我们通过从原始图像中提取低级特征来构建用于内容感知训练的内容补偿模块。大量的实验结果验证了我们的 CPDM 的有效性，在主观和客观指标方面都超越了最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.15649v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion Models**<br />
**Title_cn:** FreeStyle：使用扩散模型进行文本引导风格迁移的免费午餐<br />
**Authors:** Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li<br />
**Abstract:** <details><summary>原文: </summary>The rapid development of generative diffusion models has significantly advanced the field of style transfer. However, most current style transfer methods based on diffusion models typically involve a slow iterative optimization process, e.g., model fine-tuning and textual inversion of style concept. In this paper, we introduce FreeStyle, an innovative style transfer method built upon a pre-trained large diffusion model, requiring no further optimization. Besides, our method enables style transfer only through a text description of the desired style, eliminating the necessity of style images. Specifically, we propose a dual-stream encoder and single-stream decoder architecture, replacing the conventional U-Net in diffusion models. In the dual-stream encoder, two distinct branches take the content image and style text prompt as inputs, achieving content and style decoupling. In the decoder, we further modulate features from the dual streams based on a given content image and the corresponding style text prompt for precise style transfer. Our experimental results demonstrate high-quality synthesis and fidelity of our method across various content images and style text prompts. The code and more results are available at our project website:https://freestylefreelunch.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成扩散模型的快速发展极大地推进了风格迁移领域的发展。然而，当前大多数基于扩散模型的风格转移方法通常涉及缓慢的迭代优化过程，例如模型微调和风格概念的文本反转。在本文中，我们介绍了 FreeStyle，这是一种基于预先训练的大型扩散模型构建的创新风格转移方法，无需进一步优化。此外，我们的方法仅通过所需样式的文本描述即可实现样式迁移，从而消除了样式图像的必要性。具体来说，我们提出了一种双流编码器和单流解码器架构，取代扩散模型中的传统 U-Net。在双流编码器中，两个不同的分支将内容图像和风格文本提示作为输入，实现内容和风格解耦。在解码器中，我们根据给定的内容图像和相应的风格文本提示进一步调制双流的特征，以实现精确的风格转移。我们的实验结果证明了我们的方法在各种内容图像和样式文本提示中的高质量合成和保真度。代码和更多结果可以在我们的项目网站上找到：https://freestylefreelunch.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2401.15636v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry**<br />
**Title_cn:** BrepGen：具有结构化潜在几何的 B-rep 生成扩散模型<br />
**Authors:** Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D. D. Willis, Yasutaka Furukawa<br />
**Abstract:** <details><summary>原文: </summary>This paper presents BrepGen, a diffusion-based generative approach that directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD) model. BrepGen represents a B-rep model as a novel structured latent geometry in a hierarchical tree. With the root node representing a whole CAD solid, each element of a B-rep model (i.e., a face, an edge, or a vertex) progressively turns into a child-node from top to bottom. B-rep geometry information goes into the nodes as the global bounding box of each primitive along with a latent code describing the local geometric shape. The B-rep topology information is implicitly represented by node duplication. When two faces share an edge, the edge curve will appear twice in the tree, and a T-junction vertex with three incident edges appears six times in the tree with identical node features. Starting from the root and progressing to the leaf, BrepGen employs Transformer-based diffusion models to sequentially denoise node features while duplicated nodes are detected and merged, recovering the B-Rep topology information. Extensive experiments show that BrepGen sets a new milestone in CAD B-rep generation, surpassing existing methods on various benchmarks. Results on our newly collected furniture dataset further showcase its exceptional capability in generating complicated geometry. While previous methods were limited to generating simple prismatic shapes, BrepGen incorporates free-form and doubly-curved surfaces for the first time. Additional applications of BrepGen include CAD autocomplete and design interpolation. The code, pretrained models, and dataset will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了 BrepGen，这是一种基于扩散的生成方法，可直接输出边界表示 (B-rep) 计算机辅助设计 (CAD) 模型。 BrepGen 将 B-rep 模型表示为分层树中的新型结构化潜在几何结构。由于根节点代表整个 CAD 实体，B-rep 模型的每个元素（即面、边或顶点）从上到下逐渐变成子节点。 B-rep 几何信息作为每个基元的全局边界框以及描述局部几何形状的潜在代码进入节点。 B-rep拓扑信息通过节点复制隐式表示。当两个面共享一条边时，边曲线将在树中出现两次，并且具有三个关联边的 T 形连接顶点在具有相同节点特征的树中出现六次。从根开始一直到叶子，BrepGen 采用基于 Transformer 的扩散模型顺序对节点特征进行去噪，同时检测并合并重复节点，恢复 B-Rep 拓扑信息。大量实验表明，BrepGen 在 CAD B-rep 生成方面树立了新的里程碑，在各种基准上超越了现有方法。我们新收集的家具数据集的结果进一步展示了其生成复杂几何形状的卓越能力。虽然以前的方法仅限于生成简单的棱柱形状，但 BrepGen 首次结合了自由形状和双曲面。 BrepGen 的其他应用包括 CAD 自动完成和设计插值。代码、预训练模型和数据集将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.15563v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Improving Data Augmentation for Robust Visual Question Answering with Effective Curriculum Learning**<br />
**Title_cn:** 通过有效的课程学习改进数据增强以实现稳健的视觉问答<br />
**Authors:** Yuhang Zheng, Zhen Wang, Long Chen<br />
**Abstract:** <details><summary>原文: </summary>Being widely used in learning unbiased visual question answering (VQA) models, Data Augmentation (DA) helps mitigate language biases by generating extra training samples beyond the original samples. While today's DA methods can generate robust samples, the augmented training set, significantly larger than the original dataset, often exhibits redundancy in terms of difficulty or content repetition, leading to inefficient model training and even compromising the model performance. To this end, we design an Effective Curriculum Learning strategy ECL to enhance DA-based VQA methods. Intuitively, ECL trains VQA models on relatively ``easy'' samples first, and then gradually changes to ``harder'' samples, and less-valuable samples are dynamically removed. Compared to training on the entire augmented dataset, our ECL strategy can further enhance VQA models' performance with fewer training samples. Extensive ablations have demonstrated the effectiveness of ECL on various methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据增强 (DA) 广泛用于学习无偏差视觉问答 (VQA) 模型，通过生成原始样本之外的额外训练样本来帮助减轻语言偏差。虽然当今的 DA 方法可以生成稳健的样本，但增强的训练集明显大于原始数据集，通常在难度或内容重复方面表现出冗余，导致模型训练效率低下，甚至损害模型性能。为此，我们设计了有效的课程学习策略 ECL 来增强基于 DA 的 VQA 方法。直观上，ECL 首先在相对“简单”的样本上训练 VQA 模型，然后逐渐改为“较难”的样本，动态去除价值较低的样本。与在整个增强数据集上进行训练相比，我们的 ECL 策略可以通过更少的训练样本进一步增强 VQA 模型的性能。广泛的消融已经证明了 ECL 在各种方法上的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15646v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation**<br />
**Title_cn:** 分而治之：语言模型可以规划和自我纠正组合文本到图像的生成<br />
**Authors:** Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, Zhenguo Li<br />
**Abstract:** <details><summary>原文: </summary>Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose CompAgent, a training-free approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10\% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管在生成高质量图像的文本到图像模型方面取得了重大进展，但这些方法仍然难以确保在复杂文本提示的上下文中文本提示对图像的可控性，特别是在保留对象属性和关系时。在本文中，我们提出了 CompAgent，一种用于组合文本到图像生成的免训练方法，以大型语言模型（LLM）代理为核心。 CompAgent 的基本思想以分而治之的方法论为前提。给定一个包含多个概念（包括对象、属性和关系）的复杂文本提示，LLM 代理首先对其进行分解，这需要提取单个对象及其相关属性，并预测连贯的场景布局。然后可以独立地征服这些单独的物体。随后，代理通过分析文本进行推理，计划并使用工具来组合这些孤立的对象。验证和人类反馈机制最终被纳入我们的代理中，以进一步纠正潜在的属性错误并细化生成的图像。在LLM代理的指导下，我们提出了一种免调整的多概念定制模型和布局到图像生成模型作为概念组合的工具，以及本地图像编辑方法作为与代理交互进行验证的工具。场景布局控制这些工具之间的图像生成过程，以防止多个对象之间的混淆。大量实验证明了我们的合成文本到图像生成方法的优越性：CompAgent 在 T2I-CompBench 上实现了超过 10% 的改进，T2I-CompBench 是开放世界合成 T2I 生成的综合基准。对各种相关任务的扩展也说明了我们的 CompAgent 对于潜在应用的灵活性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15688v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Assessment of Autism and ADHD: A Comparative Analysis of Drawing Velocity Profiles and the NEPSY Test**<br />
**Title_cn:** 自闭症和多动症的评估：绘图速度曲线和 NEPSY 测试的比较分析<br />
**Authors:** S. Fortea-Sevilla, A. Garcia-Sosa., P. Morales-Almeida, C. Carmona-Duarte<br />
**Abstract:** <details><summary>原文: </summary>The increasing prevalence of Autism Spectrum Disorder and Attention-Deficit/ Hyperactivity Disorder among students highlights the need to improve evaluation and diagnostic techniques, as well as effective tools to mitigate the negative consequences associated with these disorders. With the widespread use of touchscreen mobile devices, there is an opportunity to gather comprehensive data beyond visual cues. These devices enable the collection and visualization of information on velocity profiles and the time taken to complete drawing and handwriting tasks. These data can be leveraged to develop new neuropsychological tests based on the velocity profile that assists in distinguishing between challenging cases of ASD and ADHD that are difficult to differentiate in clinical practice. In this paper, we present a proof of concept that compares and combines the results obtained from standardized tasks in the NEPSY-II assessment with a proposed observational scale based on the visual analysis of the velocity profile collected using digital tablets.</details>
**Abstract_cn:** <details><summary>译文: </summary>学生中自闭症谱系障碍和注意力缺陷/多动障碍的患病率日益增加，凸显了改进评估和诊断技术以及减轻与这些疾病相关的负面后果的有效工具的必要性。随着触摸屏移动设备的广泛使用，人们有机会收集视觉线索之外的全面数据。这些设备能够收集和可视化有关速度曲线以及完成绘图和手写任务所需时间的信息。这些数据可用于开发基于速度分布的新神经心理学测试，有助于区分在临床实践中难以区分的 ASD 和 ADHD 的挑战性病例。在本文中，我们提出了一个概念验证，它将 NEPSY-II 评估中标准化任务获得的结果与基于使用数字平板电脑收集的速度剖面的视觉分析提出的观测量表进行比较和结合。</details>
**PDF:** <http://arxiv.org/pdf/2401.15685v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Intriguing Equivalence Structures of the Embedding Space of Vision Transformers**<br />
**Title_cn:** 视觉变压器嵌入空间的有趣等价结构<br />
**Authors:** Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu<br />
**Abstract:** <details><summary>原文: </summary>Pre-trained large foundation models play a central role in the recent surge of artificial intelligence, resulting in fine-tuned models with remarkable abilities when measured on benchmark datasets, standard exams, and applications. Due to their inherent complexity, these models are not well understood. While small adversarial inputs to such models are well known, the structures of the representation space are not well characterized despite their fundamental importance. In this paper, using the vision transformers as an example due to the continuous nature of their input space, we show via analyses and systematic experiments that the representation space consists of large piecewise linear subspaces where there exist very different inputs sharing the same representations, and at the same time, local normal spaces where there are visually indistinguishable inputs having very different representations. The empirical results are further verified using the local directional estimations of the Lipschitz constants of the underlying models. Consequently, the resulting representations change the results of downstream models, and such models are subject to overgeneralization and with limited semantically meaningful generalization capability.</details>
**Abstract_cn:** <details><summary>译文: </summary>预训练的大型基础模型在最近的人工智能浪潮中发挥着核心作用，在基准数据集、标准考试和应用程序上进行测量时，经过微调的模型具有卓越的能力。由于其固有的复杂性，这些模型还没有被很好地理解。虽然此类模型的小对抗性输入是众所周知的，但表示空间的结构尽管具有根本重要性，但尚未得到很好的表征。在本文中，由于其输入空间的连续性，我们以视觉变换器为例，通过分析和系统实验表明，表示空间由大的分段线性子空间组成，其中存在共享相同表示的非常不同的输入，并且同时，局部正常空间中存在视觉上无法区分的输入，具有非常不同的表示形式。使用基础模型的 Lipschitz 常数的局部方向估计进一步验证了经验结果。因此，所得的表示会改变下游模型的结果，并且此类模型容易过度泛化，并且语义上有意义的泛化能力有限。</details>
**PDF:** <http://arxiv.org/pdf/2401.15568v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Multi-Person 3D Pose Estimation from Multi-View Uncalibrated Depth Cameras**<br />
**Title_cn:** 来自多视图未校准深度相机的多人 3D 姿态估计<br />
**Authors:** Yu-Jhe Li, Yan Xu, Rawal Khirodkar, Jinhyung Park, Kris Kitani<br />
**Abstract:** <details><summary>原文: </summary>We tackle the task of multi-view, multi-person 3D human pose estimation from a limited number of uncalibrated depth cameras. Recently, many approaches have been proposed for 3D human pose estimation from multi-view RGB cameras. However, these works (1) assume the number of RGB camera views is large enough for 3D reconstruction, (2) the cameras are calibrated, and (3) rely on ground truth 3D poses for training their regression model. In this work, we propose to leverage sparse, uncalibrated depth cameras providing RGBD video streams for 3D human pose estimation. We present a simple pipeline for Multi-View Depth Human Pose Estimation (MVD-HPE) for jointly predicting the camera poses and 3D human poses without training a deep 3D human pose regression model. This framework utilizes 3D Re-ID appearance features from RGBD images to formulate more accurate correspondences (for deriving camera positions) compared to using RGB-only features. We further propose (1) depth-guided camera-pose estimation by leveraging 3D rigid transformations as guidance and (2) depth-constrained 3D human pose estimation by utilizing depth-projected 3D points as an alternative objective for optimization. In order to evaluate our proposed pipeline, we collect three video sets of RGBD videos recorded from multiple sparse-view depth cameras and ground truth 3D poses are manually annotated. Experiments show that our proposed method outperforms the current 3D human pose regression-free pipelines in terms of both camera pose estimation and 3D human pose estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们解决从有限数量的未校准深度相机进行多视图、多人 3D 人体姿势估计的任务。最近，人们提出了许多通过多视角 RGB 相机进行 3D 人体姿态估计的方法。然而，这些工作 (1) 假设 RGB 相机视图的数量足以进行 3D 重建，(2) 相机经过校准，(3) 依靠地面实况 3D 姿势来训练回归模型。在这项工作中，我们建议利用稀疏、未校准的深度相机提供 RGBD 视频流来进行 3D 人体姿势估计。我们提出了一个简单的多视图深度人体姿势估计 (MVD-HPE) 管道，用于联合预测相机姿势和 3D 人体姿势，而无需训练深度 3D 人体姿势回归模型。与仅使用 RGB 特征相比，该框架利用 RGBD 图像的 3D Re-ID 外观特征来制定更准确的对应关系（用于导出相机位置）。我们进一步提出（1）通过利用 3D 刚性变换作为指导进行深度引导的相机姿势估计，以及（2）通过利用深度投影的 3D 点作为优化的替代目标进行深度约束的 3D 人体姿势估计。为了评估我们提出的流程，我们收集了从多个稀疏视图深度相机记录的三个 RGBD 视频视频集，并手动注释了地面实况 3D 姿势。实验表明，我们提出的方法在相机姿势估计和 3D 人体姿势估计方面均优于当前的 3D 人体姿势无回归管道。</details>
**PDF:** <http://arxiv.org/pdf/2401.15616v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes and Minimalist Workflow**<br />
**Title_cn:** GarchingSim：具有逼真场景和极简工作流程的自动驾驶模拟器<br />
**Authors:** Liguo Zhou, Yinglei Song, Yichao Gao, Zhou Yu, Michael Sodamin, Hongshen Liu, Liang Ma, Lian Liu, Hao Liu, Yang Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Conducting real road testing for autonomous driving algorithms can be expensive and sometimes impractical, particularly for small startups and research institutes. Thus, simulation becomes an important method for evaluating these algorithms. However, the availability of free and open-source simulators is limited, and the installation and configuration process can be daunting for beginners and interdisciplinary researchers. We introduce an autonomous driving simulator with photorealistic scenes, meanwhile keeping a user-friendly workflow. The simulator is able to communicate with external algorithms through ROS2 or Socket.IO, making it compatible with existing software stacks. Furthermore, we implement a highly accurate vehicle dynamics model within the simulator to enhance the realism of the vehicle's physical effects. The simulator is able to serve various functions, including generating synthetic data and driving with machine learning-based algorithms. Moreover, we prioritize simplicity in the deployment process, ensuring that beginners find it approachable and user-friendly.</details>
**Abstract_cn:** <details><summary>译文: </summary>对自动驾驶算法进行真实的道路测试可能成本高昂，有时甚至不切实际，特别是对于小型初创公司和研究机构而言。因此，仿真成为评估这些算法的重要方法。然而，免费和开源模拟器的可用性是有限的，安装和配置过程对于初学者和跨学科研究人员来说可能令人望而生畏。我们引入了具有逼真场景的自动驾驶模拟器，同时保持用户友好的工作流程。该模拟器能够通过ROS2或Socket.IO与外部算法进行通信，从而与现有的软件堆栈兼容。此外，我们在模拟器中实现了高精度的车辆动力学模型，以增强车辆物理效果的真实感。该模拟器能够提供各种功能，包括生成合成数据和使用基于机器学习的算法进行驾驶。此外，我们优先考虑部署过程的简单性，确保初学者觉得它易于使用且用户友好。</details>
**PDF:** <http://arxiv.org/pdf/2401.15803v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data**<br />
**Title_cn:** 长期台风轨迹预测：无需再分析数据的物理条件方法<br />
**Authors:** Young-Jae Park, Minseok Seo, Doyi Kim, Hyeri Kim, Sanghoon Choi, Beomkyu Choi, Jeongwon Ryu, Sohee Son, Hae-Gon Jeon, Yeji Choi<br />
**Abstract:** <details><summary>原文: </summary>In the face of escalating climate changes, typhoon intensities and their ensuing damage have surged. Accurate trajectory prediction is crucial for effective damage control. Traditional physics-based models, while comprehensive, are computationally intensive and rely heavily on the expertise of forecasters. Contemporary data-driven methods often rely on reanalysis data, which can be considered to be the closest to the true representation of weather conditions. However, reanalysis data is not produced in real-time and requires time for adjustment because prediction models are calibrated with observational data. This reanalysis data, such as ERA5, falls short in challenging real-world situations. Optimal preparedness necessitates predictions at least 72 hours in advance, beyond the capabilities of standard physics models. In response to these constraints, we present an approach that harnesses real-time Unified Model (UM) data, sidestepping the limitations of reanalysis data. Our model provides predictions at 6-hour intervals for up to 72 hours in advance and outperforms both state-of-the-art data-driven methods and numerical weather prediction models. In line with our efforts to mitigate adversities inflicted by \rthree{typhoons}, we release our preprocessed \textit{PHYSICS TRACK} dataset, which includes ERA5 reanalysis data, typhoon best-track, and UM forecast data.</details>
**Abstract_cn:** <details><summary>译文: </summary>面对不断升级的气候变化，台风强度及其造成的损失激增。准确的轨迹预测对于有效的损害控制至关重要。传统的基于物理的模型虽然全面，但计算量大，并且严重依赖预报员的专业知识。当代数据驱动的方法通常依赖于再分析数据，这可以被认为是最接近真实天气状况的表示。然而，再分析数据不是实时产生的，需要时间进行调整，因为预测模型是用观测数据校准的。这种再分析数据（例如 ERA5）无法应对具有挑战性的现实情况。最佳的准备工作需要至少提前 72 小时进行预测，这超出了标准物理模型的能力。为了应对这些限制，我们提出了一种利用实时统一模型（UM）数据的方法，避开了重新分析数据的局限性。我们的模型可以提前 72 小时以 6 小时为间隔提供预测，其性能优于最先进的数据驱动方法和数值天气预报模型。为了配合我们减轻\r三{台风}造成的逆境的努力，我们发布了经过预处理的\textit{PHYSICS TRACK}数据集，其中包括ERA5再分析数据、台风最佳轨迹和UM预报数据。</details>
**PDF:** <http://arxiv.org/pdf/2401.15726v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Low-resolution Prior Equilibrium Network for CT Reconstruction**<br />
**Title_cn:** 用于 CT 重建的低分辨率先验平衡网络<br />
**Authors:** Yijie Yang, Qifeng Gao, Yuping Duan<br />
**Abstract:** <details><summary>原文: </summary>The unrolling method has been investigated for learning variational models in X-ray computed tomography. However, it has been observed that directly unrolling the regularization model through gradient descent does not produce satisfactory results. In this paper, we present a novel deep learning-based CT reconstruction model, where the low-resolution image is introduced to obtain an effective regularization term for improving the network`s robustness. Our approach involves constructing the backbone network architecture by algorithm unrolling that is realized using the deep equilibrium architecture. We theoretically discuss the convergence of the proposed low-resolution prior equilibrium model and provide the conditions to guarantee convergence. Experimental results on both sparse-view and limited-angle reconstruction problems are provided, demonstrating that our end-to-end low-resolution prior equilibrium model outperforms other state-of-the-art methods in terms of noise reduction, contrast-to-noise ratio, and preservation of edge details.</details>
**Abstract_cn:** <details><summary>译文: </summary>展开方法已被研究用于学习 X 射线计算机断层扫描中的变分模型。然而，据观察，直接通过梯度下降展开正则化模型并不能产生令人满意的结果。在本文中，我们提出了一种基于深度学习的新型 CT 重建模型，其中引入低分辨率图像以获得有效的正则化项，从而提高网络的鲁棒性。我们的方法涉及通过使用深度均衡架构实现的算法展开来构建主干网络架构。我们从理论上讨论了所提出的低分辨率先验平衡模型的收敛性，并提供了保证收敛的条件。提供了稀疏视图和有限角度重建问题的实验结果，证明我们的端到端低分辨率先验平衡模型在降噪、对比方面优于其他最先进的方法噪声比和边缘细节的保留。</details>
**PDF:** <http://arxiv.org/pdf/2401.15663v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Addressing Noise and Efficiency Issues in Graph-Based Machine Learning Models From the Perspective of Adversarial Attack**<br />
**Title_cn:** 从对抗攻击的角度解决基于图的机器学习模型中的噪声和效率问题<br />
**Authors:** Yongyu Wang<br />
**Abstract:** <details><summary>原文: </summary>Given that no existing graph construction method can generate a perfect graph for a given dataset, graph-based algorithms are invariably affected by the plethora of redundant and erroneous edges present within the constructed graphs. In this paper, we propose treating these noisy edges as adversarial attack and use a spectral adversarial robustness evaluation method to diminish the impact of noisy edges on the performance of graph algorithms. Our method identifies those points that are less vulnerable to noisy edges and leverages only these robust points to perform graph-based algorithms. Our experiments with spectral clustering, one of the most representative and widely utilized graph algorithms, reveal that our methodology not only substantially elevates the precision of the algorithm but also greatly accelerates its computational efficiency by leveraging only a select number of robust data points.</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于现有的图构造方法无法为给定的数据集生成完美的图，基于图的算法总是受到构造图中存在的过多冗余和错误边的影响。在本文中，我们建议将这些噪声边缘视为对抗性攻击，并使用谱对抗鲁棒性评估方法来减少噪声边缘对图算法性能的影响。我们的方法识别那些不易受噪声边缘影响的点，并仅利用这些稳健的点来执行基于图的算法。我们对谱聚类（最具代表性和广泛使用的图算法之一）的实验表明，我们的方法不仅大大提高了算法的精度，而且通过仅利用选定数量的稳健数据点，大大提高了算法的计算效率。</details>
**PDF:** <http://arxiv.org/pdf/2401.15615v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Towards Arbitrary-Scale Histopathology Image Super-resolution: An Efficient Dual-branch Framework via Implicit Self-texture Enhancement**<br />
**Title_cn:** 迈向任意尺度的组织病理学图像超分辨率：通过隐式自纹理增强的高效双分支框架<br />
**Authors:** Minghong Duan, Linhao Qu, Zhiwei Yang, Manning Wang, Chenxi Zhang, Zhijian Song<br />
**Abstract:** <details><summary>原文: </summary>High-quality whole-slide scanners are expensive, complex, and time-consuming, thus limiting the acquisition and utilization of high-resolution pathology whole-slide images in daily clinical work. Deep learning-based single-image super-resolution techniques are an effective way to solve this problem by synthesizing high-resolution images from low-resolution ones. However, the existing super-resolution models applied in pathology images can only work in fixed integer magnifications, significantly decreasing their applicability. Though methods based on implicit neural representation have shown promising results in arbitrary-scale super-resolution of natural images, applying them directly to pathology images is inadequate because they have unique fine-grained image textures different from natural images. Thus, we propose an Implicit Self-Texture Enhancement-based dual-branch framework (ISTE) for arbitrary-scale super-resolution of pathology images to address this challenge. ISTE contains a pixel learning branch and a texture learning branch, which first learn pixel features and texture features, respectively. Then, we design a two-stage texture enhancement strategy to fuse the features from the two branches to obtain the super-resolution results, where the first stage is feature-based texture enhancement, and the second stage is spatial-domain-based texture enhancement. Extensive experiments on three public datasets show that ISTE outperforms existing fixed-scale and arbitrary-scale algorithms at multiple magnifications and helps to improve downstream task performance. To the best of our knowledge, this is the first work to achieve arbitrary-scale super-resolution in pathology images. Codes will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>高质量的全玻片扫描仪价格昂贵、复杂且耗时，从而限制了日常临床工作中高分辨率病理全玻片图像的采集和利用。基于深度学习的单图像超分辨率技术通过低分辨率图像合成高分辨率图像是解决这一问题的有效方法。然而，现有应用于病理图像的超分辨率模型只能在固定整数放大倍数下工作，大大降低了其适用性。尽管基于隐式神经表示的方法在自然图像的任意尺度超分辨率方面显示出了良好的结果，但将它们直接应用于病理图像是不够的，因为它们具有不同于自然图像的独特的细粒度图像纹理。因此，我们提出了一种基于隐式自纹理增强的双分支框架（ISTE），用于病理图像的任意尺度超分辨率来应对这一挑战。 ISTE包含像素学习分支和纹理学习分支，首先分别学习像素特征和纹理特征。然后，我们设计了一种两阶段纹理增强策略，融合两个分支的特征以获得超分辨率结果，其中第一阶段是基于特征的纹理增强，第二阶段是基于空间域的纹理增强。对三个公共数据集的广泛实验表明，ISTE 在多个放大倍数下优于现有的固定尺度和任意尺度算法，有助于提高下游任务性能。据我们所知，这是第一个在病理图像中实现任意尺度超分辨率的工作。代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2401.15613v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Pericoronary adipose tissue feature analysis in CT calcium score images with comparison to coronary CTA**<br />
**Title_cn:** CT钙评分图像冠周脂肪组织特征分析与冠状动脉CTA比较<br />
**Authors:** Yingnan Song, Hao Wu, Juhwan Lee, Justin Kim, Ammar Hoori, Tao Hu, Vladislav Zimin, Mohamed Makhlouf, Sadeer Al-Kindi, Sanjay Rajagopalan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We investigated the feasibility and advantages of using non-contrast CT calcium score (CTCS) images to assess pericoronary adipose tissue (PCAT) and its association with major adverse cardiovascular events (MACE). PCAT features from coronary CTA (CCTA) have been shown to be associated with cardiovascular risk but are potentially confounded by iodine. If PCAT in CTCS images can be similarly analyzed, it would avoid this issue and enable its inclusion in formal risk assessment from readily available, low-cost CTCS images. To identify coronaries in CTCS images that have subtle visual evidence of vessels, we registered CTCS with paired CCTA images having coronary labels. We developed a novel axial-disk method giving regions for analyzing PCAT features in three main coronary arteries. We analyzed novel hand-crafted and radiomic features using univariate and multivariate logistic regression prediction of MACE and compared results against those from CCTA. Registration accuracy was sufficient to enable the identification of PCAT regions in CTCS images. Motion or beam hardening artifacts were often present in high-contrast CCTA but not CTCS. Mean HU and volume were increased in both CTCS and CCTA for MACE group. There were significant positive correlations between some CTCS and CCTA features, suggesting that similar characteristics were obtained. Using hand-crafted/radiomics from CTCS and CCTA, AUCs were 0.82/0.79 and 0.83/0.77 respectively, while Agatston gave AUC=0.73. Preliminarily, PCAT features can be assessed from three main coronary arteries in non-contrast CTCS images with performance characteristics that are at the very least comparable to CCTA.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究了使用非对比 CT 钙评分 (CTCS) 图像评估冠状动脉周围脂肪组织 (PCAT) 及其与主要不良心血管事件 (MACE) 的关联的可行性和优势。冠状动脉 CTA (CCTA) 的 PCAT 特征已被证明与心血管风险相关，但可能会被碘混淆。如果 CTCS 图像中的 PCAT 可以进行类似的分析，则可以避免这个问题，并能够将其纳入来自现成的低成本 CTCS 图像的正式风险评估中。为了识别 CTCS 图像中具有细微血管视觉证据的冠状动脉，我们将 CTCS 与具有冠状动脉标签的配对 CCTA 图像进行配准。我们开发了一种新颖的轴盘方法，给出了用于分析三个主要冠状动脉中 PCAT 特征的区域。我们使用 MACE 的单变量和多变量逻辑回归预测分析了新颖的手工制作和放射组学特征，并将结果与​​ CCTA 的结果进行了比较。配准精度足以识别 CTCS 图像中的 PCAT 区域。运动或光束硬化伪影通常出现在高对比度 CCTA 中，但不出现在 CTCS 中。 MACE 组 CTCS 和 CCTA 的平均 HU 和体积均增加。一些 CTCS 和 CCTA 特征之间存在显着的正相关性，表明获得了相似的特征。使用 CTCS 和 CCTA 的手工制作/放射组学，AUC 分别为 0.82/0.79 和 0.83/0.77，而 Agatston 给出的 AUC=0.73。初步，可以从非对比 CTCS 图像中的三个主要冠状动脉评估 PCAT 特征，其性能特征至少与 CCTA 相当。</details>
**PDF:** <http://arxiv.org/pdf/2401.15554v1><br />
**Code:** null<br />

