## [UPDATED!] **2024-01-29** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Computer Vision for Primate Behavior Analysis in the Wild**<br />
**Title_cn:** 用于野外灵长类动物行为分析的计算机视觉<br />
**Authors:** Richard Vogg, Timo Lüddecke, Jonathan Henrich, Sharmita Dey, Matthias Nuske, Valentin Hassler, Derek Murphy, Julia Fischer, Julia Ostner, Oliver Schülke, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉的进步以及日益广泛的基于视频的行为监测对于改变我们研究动物认知和行为的方式具有巨大的潜力。然而，令人兴奋的前景与今天在实践中实际可以实现的目标之间仍然存在相当大的差距，特别是在来自野外的视频中。通过这篇前瞻性论文，我们希望通过指导行为科学家了解当前方法的预期，并引导计算机视觉研究人员解决与动物行为高级研究相关的问题，从而为缩小这一差距做出贡献。我们首先调查与基于视频的动物行为研究直接相关的计算机视觉问题的最先进方法，包括对象检测、多个体跟踪、（间）动作识别和个体识别。然后，我们回顾了高效学习的方法，从实践的角度来看，这是最大的挑战之一。最后，我们对新兴的动物行为计算机视觉领域的未来进行了展望，我们认为该领域应该快速超越常见的逐帧处理，并将视频视为一等公民。</details>
**PDF:** <http://arxiv.org/pdf/2401.16424v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Synchformer: Efficient Synchronization from Sparse Cues**<br />
**Title_cn:** Synchformer：稀疏线索的高效同步<br />
**Authors:** Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman<br />
**Abstract:** <details><summary>原文: </summary>Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse. Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training. This approach achieves state-of-the-art performance in both dense and sparse settings. We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们的目标是视听同步，重点关注“野外”视频，例如 YouTube 上的视频，其中同步线索可能很少。我们的贡献包括一种新颖的视听同步模型，以及通过多模态分段级对比预训练将特征提取与同步建模分离的训练。这种方法在密集和稀疏设置中都实现了最先进的性能。我们还将同步模型训练扩展到百万级“野外”数据集 AudioSet，研究可解释性的证据归因技术，并探索同步模型的新功能：视听同步性。</details>
**PDF:** <http://arxiv.org/pdf/2401.16423v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect**<br />
**Title_cn:** 视觉异常检测调查：挑战、方法和前景<br />
**Authors:** Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, Guansong Pang, Weiming Shen<br />
**Abstract:** <details><summary>原文: </summary>Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection. This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies. Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy. Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉异常检测（VAD）致力于查明视觉数据中与正常性概念的偏差，广泛应用于不同领域，例如工业缺陷检查和医学病变检测。这项调查通过确定三个主要挑战，全面审查了 VAD 的最新进展：1）训练数据的稀缺，2）视觉模式的多样性，以及 3）层次异常的复杂性。我们从简要概述VAD背景及其通用概念定义开始，从样本数量、数据模态和异常层次的角度逐步对VAD的最新进展进行分类、强调和讨论。通过对VAD领域的深入分析，我们最终总结了VAD的未来发展，并总结了本次调查的主要发现和贡献。</details>
**PDF:** <http://arxiv.org/pdf/2401.16402v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Evaluation of pseudo-healthy image reconstruction for anomaly detection with deep generative models: Application to brain FDG PET**<br />
**Title_cn:** 使用深度生成模型评估用于异常检测的伪健康图像重建：在大脑 FDG PET 中的应用<br />
**Authors:** Ravi Hassanaly, Camille Brianceau, Maëlys Solal, Olivier Colliot, Ninon Burgos<br />
**Abstract:** <details><summary>原文: </summary>Over the past years, pseudo-healthy reconstruction for unsupervised anomaly detection has gained in popularity. This approach has the great advantage of not requiring tedious pixel-wise data annotation and offers possibility to generalize to any kind of anomalies, including that corresponding to rare diseases. By training a deep generative model with only images from healthy subjects, the model will learn to reconstruct pseudo-healthy images. This pseudo-healthy reconstruction is then compared to the input to detect and localize anomalies. The evaluation of such methods often relies on a ground truth lesion mask that is available for test data, which may not exist depending on the application.   We propose an evaluation procedure based on the simulation of realistic abnormal images to validate pseudo-healthy reconstruction methods when no ground truth is available. This allows us to extensively test generative models on different kinds of anomalies and measuring their performance using the pair of normal and abnormal images corresponding to the same subject. It can be used as a preliminary automatic step to validate the capacity of a generative model to reconstruct pseudo-healthy images, before a more advanced validation step that would require clinician's expertise. We apply this framework to the reconstruction of 3D brain FDG PET using a convolutional variational autoencoder with the aim to detect as early as possible the neurodegeneration markers that are specific to dementia such as Alzheimer's disease.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的几年里，用于无监督异常检测的伪健康重建越来越受欢迎。这种方法具有不需要繁琐的逐像素数据注释的巨大优势，并且提供了推广到任何类型异常的可能性，包括与罕见疾病相对应的异常。通过仅使用健康受试者的图像训练深度生成模型，该模型将学习重建伪健康图像。然后将这种伪健康重建与输入进行比较，以检测和定位异常。此类方法的评估通常依赖于可用于测试数据的地面真实病变掩模，根据应用情况，该掩模可能不存在。我们提出了一种基于真实异常图像模拟的评估程序，以在没有真实数据可用时验证伪健康重建方法。这使我们能够广泛测试不同类型异常的生成模型，并使用与同一对象相对应的一对正常和异常图像来测量其性能。它可以用作初步的自动步骤，以验证生成模型重建伪健康图像的能力，然后再进行需要临床医生专业知识的更高级的验证步骤。我们将该框架应用于使用卷积变分自动编码器重建 3D 大脑 FDG PET，目的是尽早检测阿尔茨海默病等痴呆症特有的神经退行性标记物。</details>
**PDF:** <http://arxiv.org/pdf/2401.16363v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection**<br />
**Title_cn:** MixSup：基于标签高效 LiDAR 的 3D 物体检测的混合粒度监督<br />
**Authors:** Yuxue Yang, Lue Fan, Zhaoxiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality. We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors. MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations. Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden. The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于标签高效 LiDAR 的 3D 物体检测目前以弱/半监督方法为主。我们提出了 MixSup，而不是仅仅遵循其中一个，这是一种更实用的范例，同时利用大量廉价的粗标签和有限数量的精确标签进行混合粒度监督。我们首先观察到点云通常是无纹理的，这使得学习语义变得困难。然而，点云具有丰富的几何形状，并且对于距传感器的距离具有尺度不变性，使得学习物体的几何形状（例如姿势和形状）相对容易。因此，MixSup 利用大量粗略的集群级标签来学习语义，并利用一些昂贵的框级标签来学习准确的姿势和形状。我们重新设计了主流检测器中的标签分配，使它们能够无缝集成到 MixSup 中，从而实现实用性和通用性。我们使用各种检测器在 nuScenes、Waymo 开放数据集和 KITTI 中验证其有效性。 MixSup 使用廉价的集群注释和仅 10% 的框注释，实现了高达 97.31% 的完全监督性能。此外，我们提出基于 Segment Anything Model 的 PointSAM 进行自动粗标记，进一步减轻注释负担。代码可在 https://github.com/BraveGroup/PointSAM-for-MixSup 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16305v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Regressing Transformers for Data-efficient Visual Place Recognition**<br />
**Title_cn:** 回归变压器以实现数据高效的视觉位置识别<br />
**Authors:** María Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov<br />
**Abstract:** <details><summary>原文: </summary>Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required. This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning. By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉地点识别是计算机视觉中的一项关键任务，特别是对于定位和导航系统。现有方法通常依赖于对比学习：图像描述符被训练为在潜在空间中对于相似图像具有较小的距离，对于不相似的图像具有较大的距离。然而，这种方法很难确保准确的基于距离的图像相似性表示，特别是在使用二进制成对标签进行训练时，并且需要复杂的重新排序策略。这项工作通过将地点识别视为回归问题，使用相机视场重叠作为学习的相似性基础事实，引入了一种全新的视角。通过优化图像描述符以直接与分级相似性标签对齐，这种方法增强了排名能力，无需昂贵的重新排名，提供数据高效的训练和跨多个基准数据集的强大泛化。</details>
**PDF:** <http://arxiv.org/pdf/2401.16304v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Breaking the Barrier: Selective Uncertainty-based Active Learning for Medical Image Segmentation**<br />
**Title_cn:** 打破障碍：基于选择性不确定性的主动学习用于医学图像分割<br />
**Authors:** Siteng Ma, Haochang Wu, Aonghus Lawlor, Ruihai Dong<br />
**Abstract:** <details><summary>原文: </summary>Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance. Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics. However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors. Moreover, uncertainty-based selection introduces redundancy. These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling. To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels. Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries. This resolves the aforementioned disregard for target areas and redundancy. Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance. Our code is available at https://github.com/HelenMa9998/Selective\_Uncertainty\_AL.</details>
**Abstract_cn:** <details><summary>译文: </summary>主动学习（AL）在医学图像分割中得到了广泛的应用，旨在减轻注释工作量并提高性能。传统的基于不确定性的 AL 方法（例如熵和贝叶斯）通常依赖于所有像素级指标的聚合。然而，在不平衡的环境中，这些方法往往忽略目标区域的重要性，例如病变和肿瘤。此外，基于不确定性的选择引入了冗余。这些因素导致性能不理想，在许多情况下甚至不如随机采样。为了解决这个问题，我们引入了一种称为基于选择性不确定性的 AL 的新方法，避免了对所有像素的度量求和的传统做法。通过过滤过程，我们的策略优先考虑目标区域内和决策边界附近的像素。这解决了上述对目标区域和冗余的忽视。我们的方法在五种不同的基于不确定性的方法和两个不同的数据集上显示出显着的改进，利用更少的标记数据达到监督基线并持续实现最高的整体性能。我们的代码位于 https://github.com/HelenMa9998/Selective\_Uncertainty\_AL。</details>
**PDF:** <http://arxiv.org/pdf/2401.16298v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model**<br />
**Title_cn:** 剪切和检测：使用大型基础视频理解模型对剪切未修剪视频进行人体跌倒检测<br />
**Authors:** Till Grutschus, Ola Karrar, Emir Esenov, Ekta Vats<br />
**Abstract:** <details><summary>原文: </summary>This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: "Fall", "Lying" and "Other/Activities of daily living (ADL)". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作探索了大型视频理解基础模型在未修剪视频上人体跌倒检测下游任务中的性能，并利用预训练的视觉转换器进行多类动作检测，类别包括：“跌倒”、“说谎”和“其他/日常生活活动（ADL）”。演示了一种依赖于未修剪视频的简单剪切的时间动作定位方法。该方法包括一个预处理管道，该管道将带有时间戳动作注释的数据集转换为短动作剪辑的标记数据集。介绍了简单有效的剪辑采样策略。该方法的有效性已在公开的高质量跌倒模拟数据集（HQFSD）上进行了实证评估。实验结果验证了所提出的管道的性能。结果对于实时应用来说是有希望的，并且在给定的实验设置下，在 HQFSD 数据集上以最先进的 0.96 F1 分数在视频级别上检测到跌倒。源代码将在 GitHub 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.16280v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **MosquIoT: A System Based on IoT and Machine Learning for the Monitoring of Aedes aegypti (Diptera: Culicidae)**<br />
**Title_cn:** MosquIoT：基于物联网和机器学习的埃及伊蚊监测系统（双翅目：蚊科）<br />
**Authors:** Javier Aira, Teresa Olivares Montes, Francisco M. Delicado, Darìo Vezzani<br />
**Abstract:** <details><summary>原文: </summary>Millions of people around the world are infected with mosquito-borne diseases each year. One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others. Mosquito prevention and eradication campaigns are essential to avoid major public health consequences. In this respect, entomological surveillance is an important tool. At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources. Therefore, new technological tools based on proven techniques need to be designed and developed. However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications. This paper presents the design, development, and testing of an innovative system named MosquIoT. It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs. This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one.</details>
**Abstract_cn:** <details><summary>译文: </summary>全世界每年有数百万人感染蚊媒疾病。最危险的物种之一是埃及伊蚊，它是登革热、黄热病、基孔肯雅热和寨卡病毒等病毒的主要传播媒介。预防和消灭蚊子运动对于避免重大公共卫生后果至关重要。在这方面，昆虫学监测是一个重要的工具。目前，这种传统的监控工具是手动执行的，需要进行数字化转型，以帮助当局做出更好的决策、改进规划工作、加快执行速度并更好地管理可用资源。因此，需要设计和开发基于成熟技术的新技术工具。然而，此类工具还应该具有成本效益、自主、可靠且易于实施，并且应通过连接和多平台软件应用程序来实现。本文介绍了名为 MosquIoT 的创新系统的设计、开发和测试。它基于传统的诱产卵器，配备嵌入式物联网 (IoT) 和微型机器学习 (TinyML) 技术，可实现AE的检测和量化。埃及伊蚊蛋。这种创新且有前景的解决方案可能有助于动态了解 Ae 的行为。城市中的埃及蚊种群，从当前的反应性昆虫学监测模式转变为主动和预测性数字监测模式。</details>
**PDF:** <http://arxiv.org/pdf/2401.16258v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis**<br />
**Title_cn:** 跨数据库活体检测：比较生物识别分析的见解<br />
**Authors:** Oleksandr Kuznetsov, Dmytro Zakharov, Emanuele Frontoni, Andrea Maranesi, Serhii Bohucharskyi<br />
**Abstract:** <details><summary>原文: </summary>In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount. Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge. This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance. Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics. Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses. Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another. Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape. The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain. In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection. In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security.</details>
**Abstract_cn:** <details><summary>译文: </summary>在生物识别安全成为现代身份验证系统基石的时代，确保这些生物识别样本的真实性至关重要。活体检测是区分真实和伪造生物识别样本的能力，处于这一挑战的最前沿。这项研究对活体检测模型进行了全面评估，特别关注其在跨数据库场景中的性能，这是一种因其复杂性和现实世界相关性而臭名昭著的测试范式。我们的研究首先仔细评估各个数据集的模型，揭示其性能指标的细微差别。通过深入研究半总错误率、错误接受率和错误拒绝率等指标，我们发现了有关模型优缺点的宝贵见解。至关重要的是，我们对跨数据库测试的探索提供了一个独特的视角，突出了一个数据集上的训练与另一个数据集上的部署之间的鸿沟。与现有方法（从卷积网络到更复杂的策略）的比较分析丰富了我们对当前形势的理解。即使是最先进的模型之间的性能差异也凸显了该领域固有的挑战。从本质上讲，本文既是研究结果的存储库，也是对生物特征活体检测中更细致、数据多样化和适应性更强的方法的号角号召。在真实性与欺骗性之间的动态舞蹈中，我们的工作为驾驭生物识别安全不断发展的节奏提供了蓝图。</details>
**PDF:** <http://arxiv.org/pdf/2401.16232v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Reconstructing Close Human Interactions from Multiple Views**<br />
**Title_cn:** 从多个视角重建亲密的人际互动<br />
**Authors:** Qing Shuai, Zhiyuan Yu, Zhize Zhou, Lixin Fan, Haijun Yang, Can Yang, Xiaowei Zhou<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了重建由多个校准相机捕获的进行密切交互的多个个体的姿势的挑战性任务。困难来自于由于人与人之间的遮挡而导致的噪声或错误的 2D 关键点检测、由于密切交互而将关键点与个人关联起来的严重模糊性以及训练数据的稀缺性，因为在拥挤的场景中收集和注释运动数据是资源。密集的。我们引入了一种新颖的系统来应对这些挑战。我们的系统集成了基于学习的姿态估计组件及其相应的训练和推理策略。姿势估计组件以多视图 2D 关键点热图作为输入，并使用 3D 条件体积网络重建每个个体的姿势。由于网络不需要图像作为输入，我们可以利用测试场景中已知的相机参数和大量现有的运动捕捉数据来合成大量训练数据，模拟测试场景中的真实数据分布。大量的实验表明，我们的方法在姿势准确性方面显着超越了以前的方法，并且可以推广到各种相机设置和人群规模。该代码可在我们的项目页面上找到：https://github.com/zju3dv/CloseMoCap。</details>
**PDF:** <http://arxiv.org/pdf/2401.16173v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **CIMIL-CRC: a clinically-informed multiple instance learning framework for patient-level colorectal cancer molecular subtypes classification from H\&E stained images**<br />
**Title_cn:** CIMIL-CRC：一种临床知情的多实例学习框架，用于根据 H\&E 染色图像对患者级别的结直肠癌分子亚型进行分类<br />
**Authors:** Hadar Hezi, Matan Gelber, Alexander Balabanov, Yosef E. Maruvka, Moti Freiman<br />
**Abstract:** <details><summary>原文: </summary>Treatment approaches for colorectal cancer (CRC) are highly dependent on the molecular subtype, as immunotherapy has shown efficacy in cases with microsatellite instability (MSI) but is ineffective for the microsatellite stable (MSS) subtype. There is promising potential in utilizing deep neural networks (DNNs) to automate the differentiation of CRC subtypes by analyzing Hematoxylin and Eosin (H\&E) stained whole-slide images (WSIs). Due to the extensive size of WSIs, Multiple Instance Learning (MIL) techniques are typically explored. However, existing MIL methods focus on identifying the most representative image patches for classification, which may result in the loss of critical information. Additionally, these methods often overlook clinically relevant information, like the tendency for MSI class tumors to predominantly occur on the proximal (right side) colon. We introduce `CIMIL-CRC', a DNN framework that: 1) solves the MSI/MSS MIL problem by efficiently combining a pre-trained feature extraction model with principal component analysis (PCA) to aggregate information from all patches, and 2) integrates clinical priors, particularly the tumor location within the colon, into the model to enhance patient-level classification accuracy. We assessed our CIMIL-CRC method using the average area under the curve (AUC) from a 5-fold cross-validation experimental setup for model development on the TCGA-CRC-DX cohort, contrasting it with a baseline patch-level classification, MIL-only approach, and Clinically-informed patch-level classification approach. Our CIMIL-CRC outperformed all methods (AUROC: $0.92\pm0.002$ (95\% CI 0.91-0.92), vs. $0.79\pm0.02$ (95\% CI 0.76-0.82), $0.86\pm0.01$ (95\% CI 0.85-0.88), and $0.87\pm0.01$ (95\% CI 0.86-0.88), respectively). The improvement was statistically significant.</details>
**Abstract_cn:** <details><summary>译文: </summary>结直肠癌 (CRC) 的治疗方法高度依赖于分子亚型，因为免疫疗法对微卫星不稳定 (MSI) 亚型有效，但对微卫星稳定 (MSS) 亚型无效。利用深度神经网络 (DNN) 通过分析苏木精和曙红 (H\&E) 染色的全玻片图像 (WSI) 来自动区分 CRC 亚型，具有广阔的前景。由于 WSI 规模庞大，通常会探索多实例学习 (MIL) 技术。然而，现有的 MIL 方法侧重于识别最具代表性的图像块进行分类，这可能会导致关键信息的丢失。此外，这些方法常常忽视临床相关信息，例如 MSI 类肿瘤主要发生在近端（右侧）结肠的趋势。我们引入“CIMIL-CRC”，一个 DNN 框架，它：1）通过有效地将预训练的特征提取模型与主成分分析（PCA）相结合来聚合来自所有补丁的信息来解决 MSI/MSS MIL 问题，2）集成将临床先验，特别是结肠内的肿瘤位置纳入模型中，以提高患者级别的分类准确性。我们使用来自 TCGA-CRC-DX 队列模型开发的 5 倍交叉验证实验设置的平均曲线下面积 (AUC) 来评估我们的 CIMIL-CRC 方法，并将其与基线补丁级分类 MIL 进行对比仅方法和临床知情的斑块级分类方法。我们的 CIMIL-CRC 优于所有方法（AUROC：$0.92\pm0.002$ (95\% CI 0.91-0.92)，对比 $0.79\pm0.02$ (95\% CI 0.76-0.82)、$0.86\pm0.01$ (95\% CI 0.85-0.88) 和 $0.87\pm0.01$ (95\% CI 0.86-0.88)。改善具有统计学意义。</details>
**PDF:** <http://arxiv.org/pdf/2401.16131v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Towards Scenario Generalization for Vision-based Roadside 3D Object Detection**<br />
**Title_cn:** 迈向基于视觉的路边 3D 物体检测的场景泛化<br />
**Authors:** Lei Yang, Xinyu Zhang, Jun Li, Li Wang, Chuang Zhang, Li Ju, Zhiwei Li, Yang Shen<br />
**Abstract:** <details><summary>原文: </summary>Roadside perception can greatly increase the safety of autonomous vehicles by extending their perception ability beyond the visual range and addressing blind spots. However, current state-of-the-art vision-based roadside detection methods possess high accuracy on labeled scenes but have inferior performance on new scenes. This is because roadside cameras remain stationary after installation and can only collect data from a single scene, resulting in the algorithm overfitting these roadside backgrounds and camera poses. To address this issue, in this paper, we propose an innovative Scenario Generalization Framework for Vision-based Roadside 3D Object Detection, dubbed SGV3D. Specifically, we employ a Background-suppressed Module (BSM) to mitigate background overfitting in vision-centric pipelines by attenuating background features during the 2D to bird's-eye-view projection. Furthermore, by introducing the Semi-supervised Data Generation Pipeline (SSDG) using unlabeled images from new scenes, diverse instance foregrounds with varying camera poses are generated, addressing the risk of overfitting specific camera poses. We evaluate our method on two large-scale roadside benchmarks. Our method surpasses all previous methods by a significant margin in new scenes, including +42.57% for vehicle, +5.87% for pedestrian, and +14.89% for cyclist compared to BEVHeight on the DAIR-V2X-I heterologous benchmark. On the larger-scale Rope3D heterologous benchmark, we achieve notable gains of 14.48% for car and 12.41% for large vehicle. We aspire to contribute insights on the exploration of roadside perception techniques, emphasizing their capability for scenario generalization. The code will be available at {\url{ https://github.com/yanglei18/SGV3D}}</details>
**Abstract_cn:** <details><summary>译文: </summary>路边感知可以将感知能力扩展到视觉范围之外并解决盲点，从而大大提高自动驾驶汽车的安全性。然而，当前最先进的基于视觉的路边检测方法在标记场景上具有高精度，但在新场景上表现较差。这是因为路边摄像头安装后保持静止，只能收集单个场景的数据，导致算法过度拟合这些路边背景和摄像头姿势。为了解决这个问题，在本文中，我们提出了一种基于视觉的路边 3D 物体检测的创新场景泛化框架，称为 SGV3D。具体来说，我们采用背景抑制模块（BSM），通过在 2D 到鸟瞰图投影期间减弱背景特征来减轻以视觉为中心的管道中的背景过度拟合。此外，通过引入使用来自新场景的未标记图像的半监督数据生成管道（SSDG），生成具有不同相机姿势的多样化实例前景，解决了过度拟合特定相机姿势的风险。我们在两个大型路边基准测试中评估了我们的方法。与 DAIR-V2X-I 异源基准上的 BEVHeight 相比，我们的方法在新场景中显着超越了之前的所有方法，包括车辆 +42.57%，行人 +5.87%，骑车人 +14.89%。在更大规模的 Rope3D 异源基准上，我们在汽车方面取得了 14.48% 的显着收益，在大型车辆方面取得了 12.41% 的显着收益。我们渴望为路边感知技术的探索提供见解，强调其场景泛化的能力。代码可在 {\url{ https://github.com/yanglei18/SGV3D}} 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.16110v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A 2D Sinogram-Based Approach to Defect Localization in Computed Tomography**<br />
**Title_cn:** 基于 2D 正弦图的计算机断层扫描缺陷定位方法<br />
**Authors:** Yuzhong Zhou, Linda-Sophie Schneider, Fuxin Fan, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>The rise of deep learning has introduced a transformative era in the field of image processing, particularly in the context of computed tomography. Deep learning has made a significant contribution to the field of industrial Computed Tomography. However, many defect detection algorithms are applied directly to the reconstructed domain, often disregarding the raw sensor data. This paper shifts the focus to the use of sinograms. Within this framework, we present a comprehensive three-step deep learning algorithm, designed to identify and analyze defects within objects without resorting to image reconstruction. These three steps are defect segmentation, mask isolation, and defect analysis. We use a U-Net-based architecture for defect segmentation. Our method achieves the Intersection over Union of 92.02% on our simulated data, with an average position error of 1.3 pixels for defect detection on a 512-pixel-wide detector.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习的兴起为图像处理领域带来了一个变革时代，特别是在计算机断层扫描领域。深度学习为工业计算机断层扫描领域做出了重大贡献。然而，许多缺陷检测算法直接应用于重建域，通常忽略原始传感器数据。本文将重点转移到正弦图的使用上。在此框架内，我们提出了一种全面的三步深度学习算法，旨在识别和分析物体内的缺陷，而无需求助于图像重建。这三个步骤是缺陷分割、掩模隔离和缺陷分析。我们使用基于 U-Net 的架构进行缺陷分割。我们的方法在模拟数据上实现了 92.02% 的交集比并集，在 512 像素宽的检测器上进行缺陷检测的平均位置误差为 1.3 像素。</details>
**PDF:** <http://arxiv.org/pdf/2401.16104v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Neuromorphic Valence and Arousal Estimation**<br />
**Title_cn:** 神经形态效价和唤醒估计<br />
**Authors:** Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Alberto Del Bimbo<br />
**Abstract:** <details><summary>原文: </summary>Recognizing faces and their underlying emotions is an important aspect of biometrics. In fact, estimating emotional states from faces has been tackled from several angles in the literature. In this paper, we follow the novel route of using neuromorphic data to predict valence and arousal values from faces. Due to the difficulty of gathering event-based annotated videos, we leverage an event camera simulator to create the neuromorphic counterpart of an existing RGB dataset. We demonstrate that not only training models on simulated data can still yield state-of-the-art results in valence-arousal estimation, but also that our trained models can be directly applied to real data without further training to address the downstream task of emotion recognition. In the paper we propose several alternative models to solve the task, both frame-based and video-based.</details>
**Abstract_cn:** <details><summary>译文: </summary>识别面孔及其潜在情绪是生物识别技术的一个重要方面。事实上，文献中已经从多个角度解决了从面部估计情绪状态的问题。在本文中，我们遵循使用神经形态数据来预测面部的效价和唤醒值的新途径。由于收集基于事件的带注释视频很困难，我们利用事件摄像机模拟器来创建现有 RGB 数据集的神经形态对应物。我们证明，不仅模拟数据上的训练模型仍然可以在价唤醒估计中产生最先进的结果，而且我们训练的模型可以直接应用于真实数据，无需进一步训练来解决情感的下游任务认出。在本文中，我们提出了几种替代模型来解决该任务，包括基于帧的模型和基于视频的模型。</details>
**PDF:** <http://arxiv.org/pdf/2401.16058v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation**<br />
**Title_cn:** 动态原型适应与蒸馏用于少样本点云分割<br />
**Authors:** Jie Liu, Wenzhe Yin, Haochen Wang, Yunlu CHen, Jan-Jakob Sonke, Efstratios Gavves<br />
**Abstract:** <details><summary>原文: </summary>Few-shot point cloud segmentation seeks to generate per-point masks for previously unseen categories, using only a minimal set of annotated point clouds as reference. Existing prototype-based methods rely on support prototypes to guide the segmentation of query point clouds, but they encounter challenges when significant object variations exist between the support prototypes and query features. In this work, we present dynamic prototype adaptation (DPA), which explicitly learns task-specific prototypes for each query point cloud to tackle the object variation problem. DPA achieves the adaptation through prototype rectification, aligning vanilla prototypes from support with the query feature distribution, and prototype-to-query attention, extracting task-specific context from query point clouds. Furthermore, we introduce a prototype distillation regularization term, enabling knowledge transfer between early-stage prototypes and their deeper counterparts during adaption. By iteratively applying these adaptations, we generate task-specific prototypes for accurate mask predictions on query point clouds. Extensive experiments on two popular benchmarks show that DPA surpasses state-of-the-art methods by a significant margin, e.g., 7.43\% and 6.39\% under the 2-way 1-shot setting on S3DIS and ScanNet, respectively. Code is available at https://github.com/jliu4ai/DPA.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头点云分割旨在仅使用最少的注释点云集作为参考，为以前未见过的类别生成每点掩模。现有的基于原型的方法依靠支持原型来指导查询点云的分割，但是当支持原型和查询特征之间存在显着的对象变化时，它们会遇到挑战。在这项工作中，我们提出了动态原型适应（DPA），它显式地学习每个查询点云的特定于任务的原型来解决对象变化问题。 DPA 通过原型校正、将原始原型与查询特征分布的支持对齐、原型到查询的注意力、从查询点云中提取特定于任务的上下文来实现适应。此外，我们引入了原型蒸馏正则化项，使得在适应过程中早期原型与其更深层原型之间的知识转移成为可能。通过迭代应用这些调整，我们生成特定于任务的原型，以在查询点云上进行准确的掩模预测。对两个流行基准的大量实验表明，DPA 显着超越了最先进的方法，例如，在 S3DIS 和 ScanNet 上的 2 路 1-shot 设置下分别为 7.43\% 和 6.39\%。代码可在 https://github.com/jliu4ai/DPA 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16051v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Second Order Kinematic Surface Fitting in Anatomical Structures**<br />
**Title_cn:** 解剖结构中的二阶运动曲面拟合<br />
**Authors:** Wilhelm Wimmer, Hervé Delingette<br />
**Abstract:** <details><summary>原文: </summary>Symmetry detection and morphological classification of anatomical structures play pivotal roles in medical image analysis. The application of kinematic surface fitting, a method for characterizing shapes through parametric stationary velocity fields, has shown promising results in computer vision and computer-aided design. However, existing research has predominantly focused on first order rotational velocity fields, which may not adequately capture the intricate curved and twisted nature of anatomical structures. To address this limitation, we propose an innovative approach utilizing a second order velocity field for kinematic surface fitting. This advancement accommodates higher rotational shape complexity and improves the accuracy of symmetry detection in anatomical structures. We introduce a robust fitting technique and validate its performance through testing on synthetic shapes and real anatomical structures. Our method not only enables the detection of curved rotational symmetries (core lines) but also facilitates morphological classification by deriving intrinsic shape parameters related to curvature and torsion. We illustrate the usefulness of our technique by categorizing the shape of human cochleae in terms of the intrinsic velocity field parameters. The results showcase the potential of our method as a valuable tool for medical image analysis, contributing to the assessment of complex anatomical shapes.</details>
**Abstract_cn:** <details><summary>译文: </summary>解剖结构的对称性检测和形态分类在医学图像分析中发挥着关键作用。运动曲面拟合是一种通过参数静止速度场表征形状的方法，它的应用在计算机视觉和计算机辅助设计中显示出了有希望的结果。然而，现有的研究主要集中在一阶旋转速度场，这可能无法充分捕捉解剖结构复杂的弯曲和扭曲性质。为了解决这个限制，我们提出了一种利用二阶速度场进行运动表面拟合的创新方法。这一进步可适应更高的旋转形状复杂性，并提高解剖结构中对称检测的准确性。我们引入了一种强大的拟合技术，并通过对合成形状和真实解剖结构的测试来验证其性能。我们的方法不仅能够检测弯曲旋转对称性（核心线），而且还可以通过推导与曲率和扭转相关的内在形状参数来促进形态分类。我们通过根据固有速度场参数对人类耳蜗的形状进行分类来说明我们的技术的有用性。结果展示了我们的方法作为医学图像分析的宝贵工具的潜力，有助于评估复杂的解剖形状。</details>
**PDF:** <http://arxiv.org/pdf/2401.16035v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Gland segmentation via dual encoders and boundary-enhanced attention**<br />
**Title_cn:** 通过双编码器和边界增强注意力进行腺体分割<br />
**Authors:** Huadeng Wang, Jiejiang Yu, Bingbing Li, Xipeng Pan, Zhenbing Liu, Rushi Lan, Xiaonan Luo<br />
**Abstract:** <details><summary>原文: </summary>Accurate and automated gland segmentation on pathological images can assist pathologists in diagnosing the malignancy of colorectal adenocarcinoma. However, due to various gland shapes, severe deformation of malignant glands, and overlapping adhesions between glands. Gland segmentation has always been very challenging. To address these problems, we propose a DEA model. This model consists of two branches: the backbone encoding and decoding network and the local semantic extraction network. The backbone encoding and decoding network extracts advanced Semantic features, uses the proposed feature decoder to restore feature space information, and then enhances the boundary features of the gland through boundary enhancement attention. The local semantic extraction network uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder to realize the extraction of edge features. Experimental results on two public datasets, GlaS and CRAG, confirm that the performance of our method is better than other gland segmentation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>病理图像上准确、自动化的腺体分割可以帮助病理学家诊断结直肠腺癌的恶性程度。但由于腺体形状多样，恶性腺体变形严重，腺体之间重叠粘连。腺体分割一直非常具有挑战性。为了解决这些问题，我们提出了 DEA 模型。该模型由两个分支组成：主干编解码网络和局部语义提取网络。主干编码和解码网络提取高级语义特征，使用所提出的特征解码器来恢复特征空间信息，然后通过边界增强注意来增强腺体的边界特征。局部语义提取网络使用预训练的DeepLabv3+作为局部语义引导编码器，实现边缘特征的提取。在两个公共数据集 GlaS 和 CRAG 上的实验结果证实，我们的方法的性能优于其他腺体分割方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.15990v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Generating Multi-Center Classifier via Conditional Gaussian Distribution**<br />
**Title_cn:** 通过条件高斯分布生成多中心分类器<br />
**Authors:** Zhemin Zhang, Xun Gong<br />
**Abstract:** <details><summary>原文: </summary>The linear classifier is widely used in various image classification tasks. It works by optimizing the distance between a sample and its corresponding class center. However, in real-world data, one class can contain several local clusters, e.g., birds of different poses. To address this complexity, we propose a novel multi-center classifier. Different from the vanilla linear classifier, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. Specifically, we create a conditional Gaussian distribution for each class and then sample multiple sub-centers from that distribution to extend the linear classifier. This approach allows the model to capture intra-class local structures more efficiently. In addition, at test time we set the mean of the conditional Gaussian distribution as the class center of the linear classifier and follow the vanilla linear classifier outputs, thus requiring no additional parameters or computational overhead. Extensive experiments on image classification show that the proposed multi-center classifier is a powerful alternative to widely used linear classifiers. Code available at https://github.com/ZheminZhang1/MultiCenter-Classifier.</details>
**Abstract_cn:** <details><summary>译文: </summary>线性分类器广泛应用于各种图像分类任务。它的工作原理是优化样本与其相应的类中心之间的距离。然而，在现实世界的数据中，一类可以包含多个局部集群，例如不同姿势的鸟类。为了解决这种复杂性，我们提出了一种新颖的多中心分类器。与普通线性分类器不同，我们的建议是建立在训练集的深层特征遵循高斯混合分布的假设之上的。具体来说，我们为每个类创建一个条件高斯分布，然后从该分布中采样多个子中心以扩展线性分类器。这种方法允许模型更有效地捕获类内局部结构。此外，在测试时，我们将条件高斯分布的均值设置为线性分类器的类中心，并遵循普通线性分类器的输出，因此不需要额外的参数或计算开销。大量的图像分类实验表明，所提出的多中心分类器是广泛使用的线性分类器的强大替代方案。代码可在 https://github.com/ZheminZhang1/MultiCenter-Classifier 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.15942v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **HICH Image/Text (HICH-IT): Comprehensive Text and Image Datasets for Hypertensive Intracerebral Hemorrhage Research**<br />
**Title_cn:** HICH 图像/文本 (HICH-IT)：用于高血压脑出血研究的综合文本和图像数据集<br />
**Authors:** Jie Li, Yulong Xia, Tongxin Yang, Fenglin Cai, Miao Wei, Zhiwei Zhang, Li Jiang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a new multimodal dataset in the medical field of hypertensive intracerebral hemorrhage(HICH), called as HICH-IT, which includes both textual information and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of HICH. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the text data, extracting key content from the text information, and categorizes the annotation content of imaging data into four types: brain midline, hematoma, left cerebral ventricle, and right cerebral ventricle. HICH-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition. To further understand the dataset, we have trained deep learning algorithms to observe the performance. The pretrained models have been released at both www.daip.club and github.com/Deep-AI-Application-DAIP. The dataset has been uploaded to https://github.com/CYBUS123456/HICH-IT-Datasets.   Index Terms-HICH, Deep learning, Intraparenchymal hemorrhage, named entity recognition, novel dataset</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了高血压脑出血（HICH）医学领域的一种新的多模态数据集，称为HICH-IT，其中包括文本信息和头部CT图像。该数据集旨在提高人工智能在 HICH 诊断和治疗中的准确性。该数据集建立在标准文本和图像数据的基础上，在文本数据中融入特定的注释，从文本信息中提取关键内容，并将影像数据的注释内容分为四种类型：脑中线、血肿、左脑室和右脑室。 HICH-IT 旨在成为图像分割任务和命名实体识别中特征学习的基础数据集。为了进一步了解数据集，我们训练了深度学习算法来观察性能。预训练模型已在 www.daip.club 和 github.com/Deep-AI-Application-DAIP 上发布。数据集已上传至https://github.com/CYBUS123456/HICH-IT-Datasets。关键词-HICH，深度学习，脑实质出血，命名实体识别，新颖数据集</details>
**PDF:** <http://arxiv.org/pdf/2401.15934v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization**<br />
**Title_cn:** 克服 OOD 泛化的视觉语言模型微调的陷阱<br />
**Authors:** Yuhang Zang, Hanlin Goh, Josh Susskind, Chen Huang<br />
**Abstract:** <details><summary>原文: </summary>Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution (ID) and out-of-distribution (OOD) samples, but also show some improvements in both ID and OOD accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach OGEN to address this pitfall, with the main focus on improving the OOD GENeralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize OOD features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between ID and OOD data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, i.e., adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in OOD generalization performance in different settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的视觉语言模型在各种视觉领域和任务上表现出很强的泛化能力。然而，此类模型主要以闭集方式执行零样本识别，因此很难通过设计来处理开放域视觉概念。最近有一些微调方法，例如即时学习，不仅研究分布内（ID）和分布外（OOD）样本之间的区别，而且还显示了 ID 和 OOD 准确性的一些改进。在本文中，我们首先证明视觉语言模型在经过足够长的微调但没有适当的正则化后，往往会过度拟合给定数据集中的已知类，从而导致未知类的性能下降。然后我们提出了一种新方法 OGEN 来解决这个陷阱，主要关注于改进微调模型的 OOD 泛化。具体来说，引入了类条件特征生成器，仅使用任何未知类的类名来合成 OOD 特征。这种综合特征将提供有关未知数的有用知识，并在联合优化时帮助规范 ID 和 OOD 数据之间的决策边界。同样重要的是我们的自适应自蒸馏机制，可以在联合优化期间规范我们的特征生成模型，即在模型状态之间自适应地传递知识以进一步防止过度拟合。实验验证了我们的方法在不同设置下的 OOD 泛化性能方面取得了令人信服的收益。</details>
**PDF:** <http://arxiv.org/pdf/2401.15914v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **MV2MAE: Multi-View Video Masked Autoencoders**<br />
**Title_cn:** MV2MAE：多视图视频屏蔽自动编码器<br />
**Authors:** Ketul Shah, Robert Crandall, Jie Xu, Peng Zhou, Marian George, Mayank Bansal, Rama Chellappa<br />
**Abstract:** <details><summary>原文: </summary>Videos captured from multiple viewpoints can help in perceiving the 3D structure of the world and benefit computer vision tasks such as action recognition, tracking, etc. In this paper, we present a method for self-supervised learning from synchronized multi-view videos. We use a cross-view reconstruction task to inject geometry information in the model. Our approach is based on the masked autoencoder (MAE) framework. In addition to the same-view decoder, we introduce a separate cross-view decoder which leverages cross-attention mechanism to reconstruct a target viewpoint video using a video from source viewpoint, to help representations robust to viewpoint changes. For videos, static regions can be reconstructed trivially which hinders learning meaningful representations. To tackle this, we introduce a motion-weighted reconstruction loss which improves temporal modeling. We report state-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well as in the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets, demonstrating the robustness of our approach. Code will be made available.</details>
**Abstract_cn:** <details><summary>译文: </summary>从多个视点捕获的视频有助于感知世界的 3D 结构，并有利于动作识别、跟踪等计算机视觉任务。在本文中，我们提出了一种从同步多视点视频进行自监督学习的方法。我们使用跨视图重建任务在模型中注入几何信息。我们的方法基于屏蔽自动编码器（MAE）框架。除了同视图解码器之外，我们还引入了一个单独的跨视图解码器，它利用交叉注意机制使用来自源视点的视频重建目标视点视频，以帮助表示对视点变化具有鲁棒性。对于视频，静态区域可以被简单地重建，这阻碍了学习有意义的表示。为了解决这个问题，我们引入了运动加权重建损失，它改进了时间建模。我们报告了 NTU-60、NTU-120 和 ETRI 数据集以及 NUCLA、PKU-MMD-II 和 ROCOG-v2 数据集上的迁移学习设置的最新结果，证明了我们的稳健性方法。代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2401.15900v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **$\boldsymbol{M^2}$-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining**<br />
**Title_cn:** $\boldsymbol{M^2}$-Encoder：通过大规模高效预训练推进双语图像文本理解<br />
**Authors:** Qingpei Guo, Furong Xu, Hanxiao Zhang, Wang Ren, Ziping Ma, Lin Ju, Jian Wang, Jingdong Chen, Ming Yang<br />
**Abstract:** <details><summary>原文: </summary>Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new benchmarks in both languages for multimodal retrieval and classification tasks. Notably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of 88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification setting, surpassing previously reported SoTA methods by 2.2% and 21.1%, respectively. The $M^2$-Encoder series represents one of the most comprehensive bilingual image-text foundation models to date, so we are making it available to the research community for further exploration and development.</details>
**Abstract_cn:** <details><summary>译文: </summary>像 CLIP 这样的视觉语言基础模型已经彻底改变了人工智能领域。然而，由于大规模预训练数据集的相对稀缺，支持中文和英文等多语言的 VLM 模型已经滞后。为此，我们引入了一个包含超过 60 亿个图像-文本对的综合双语（中英）数据集 BM-6B，旨在增强多模态基础模型以更好地理解两种语言的图像。为了处理如此规模的数据集，我们提出了一种新颖的图像文本对比损失计算分组聚合方法，该方法显着降低了通信开销和 GPU 内存需求，从而使训练速度提高了 60%。我们在 BM-6B 上预训练了一系列具有增强的细粒度理解能力的双语图像文本基础模型，所得模型被称为 $M^2$-Encoders（发音为“M-Square”），在两种语言都用于多模式检索和分类任务。值得注意的是，我们最大的 $M^2$-Encoder-10B 模型在零样本分类设置下在 ImageNet 上实现了 88.5% 的 top-1 准确率，在 ImageNet-CN 上实现了 80.7% 的 top-1 准确率，比之前报道的 SoTA 方法高出 2.2% 和 21.1% ％， 分别。 $M^2$-Encoder 系列代表了迄今为止最全面的双语图像文本基础模型之一，因此我们将其提供给研究社区进行进一步的探索和开发。</details>
**PDF:** <http://arxiv.org/pdf/2401.15896v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Grey Level Texture Features for Segmentation of Chromogenic Dye RNAscope From Breast Cancer Tissue**<br />
**Title_cn:** 用于乳腺癌组织显色染料 RNAscope 分割的灰度纹理特征<br />
**Authors:** Andrew Davidson, Arthur Morley-Bunker, George Wiggins, Logan Walker, Gavin Harris, Ramakrishnan Mukundan, kConFab Investigators<br />
**Abstract:** <details><summary>原文: </summary>Chromogenic RNAscope dye and haematoxylin staining of cancer tissue facilitates diagnosis of the cancer type and subsequent treatment, and fits well into existing pathology workflows. However, manual quantification of the RNAscope transcripts (dots), which signify gene expression, is prohibitively time consuming. In addition, there is a lack of verified supporting methods for quantification and analysis. This paper investigates the usefulness of gray level texture features for automatically segmenting and classifying the positions of RNAscope transcripts from breast cancer tissue. Feature analysis showed that a small set of gray level features, including Gray Level Dependence Matrix and Neighbouring Gray Tone Difference Matrix features, were well suited for the task. The automated method performed similarly to expert annotators at identifying the positions of RNAscope transcripts, with an F1-score of 0.571 compared to the expert inter-rater F1-score of 0.596. These results demonstrate the potential of gray level texture features for automated quantification of RNAscope in the pathology workflow.</details>
**Abstract_cn:** <details><summary>译文: </summary>癌症组织的显色 RNAscope 染料和苏木精染色有助于癌症类型的诊断和后续治疗，并且非常适合现有的病理学工作流程。然而，对代表基因表达的 RNAscope 转录本（点）进行手动定量非常耗时。此外，缺乏经过验证的量化和分析支持方法。本文研究了灰度纹理特征对于自动分割和分类乳腺癌组织中 RNAscope 转录本位置的有用性。特征分析表明，一小组灰度特征（包括灰度依赖矩阵和相邻灰度色调差异矩阵特征）非常适合该任务。自动化方法在识别 RNAscope 转录本位置方面与专家注释者类似，F1 得分为 0.571，而专家评分者间 F1 得分为 0.596。这些结果证明了灰度纹理特征在病理学工作流程中自动定量 RNAscope 的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.15886v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Rectify the Regression Bias in Long-Tailed Object Detection**<br />
**Title_cn:** 纠正长尾目标检测中的回归偏差<br />
**Authors:** Ke Zhu, Minghao Fu, Jie Shao, Tianyu Liu, Jianxin Wu<br />
**Abstract:** <details><summary>原文: </summary>Long-tailed object detection faces great challenges because of its extremely imbalanced class distribution. Recent methods mainly focus on the classification bias and its loss function design, while ignoring the subtle influence of the regression branch. This paper shows that the regression bias exists and does adversely and seriously impact the detection accuracy. While existing methods fail to handle the regression bias, the class-specific regression head for rare classes is hypothesized to be the main cause of it in this paper. As a result, three kinds of viable solutions to cater for the rare categories are proposed, including adding a class-agnostic branch, clustering heads and merging heads. The proposed methods brings in consistent and significant improvements over existing long-tailed detection methods, especially in rare and common classes. The proposed method achieves state-of-the-art performance in the large vocabulary LVIS dataset with different backbones and architectures. It generalizes well to more difficult evaluation metrics, relatively balanced datasets, and the mask branch. This is the first attempt to reveal and explore rectifying of the regression bias in long-tailed object detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>长尾目标检测因其类别分布极不平衡而面临巨大挑战。最近的方法主要关注分类偏差及其损失函数设计，而忽略了回归分支的微妙影响。本文表明回归偏差的存在并且对检测精度产生不利且严重的影响。虽然现有方法无法处理回归偏差，但本文假设稀有类别的特定类别回归头是其主要原因。因此，提出了三种针对稀有类别的可行解决方案，包括添加与类别无关的分支、聚类头和合并头。所提出的方法比现有的长尾检测方法带来了一致且显着的改进，特别是在稀有和常见类别中。所提出的方法在具有不同骨干和架构的大词汇量 LVIS 数据集中实现了最先进的性能。它可以很好地推广到更困难的评估指标、相对平衡的数据集和掩模分支。这是揭示和探索纠正长尾目标检测中回归偏差的首次尝试。</details>
**PDF:** <http://arxiv.org/pdf/2401.15885v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Combining Satellite and Weather Data for Crop Type Mapping: An Inverse Modelling Approach**<br />
**Title_cn:** 结合卫星和天气数据进行作物类型绘图：逆向建模方法<br />
**Authors:** Praveen Ravirathinam, Rahul Ghosh, Ankush Khandelwal, Xiaowei Jia, David Mulla, Vipin Kumar<br />
**Abstract:** <details><summary>原文: </summary>Accurate and timely crop mapping is essential for yield estimation, insurance claims, and conservation efforts. Over the years, many successful machine learning models for crop mapping have been developed that use just the multi-spectral imagery from satellites to predict crop type over the area of interest. However, these traditional methods do not account for the physical processes that govern crop growth. At a high level, crop growth can be envisioned as physical parameters, such as weather and soil type, acting upon the plant leading to crop growth which can be observed via satellites. In this paper, we propose Weather-based Spatio-Temporal segmentation network with ATTention (WSTATT), a deep learning model that leverages this understanding of crop growth by formulating it as an inverse model that combines weather (Daymet) and satellite imagery (Sentinel-2) to generate accurate crop maps. We show that our approach provides significant improvements over existing algorithms that solely rely on spectral imagery by comparing segmentation maps and F1 classification scores. Furthermore, effective use of attention in WSTATT architecture enables detection of crop types earlier in the season (up to 5 months in advance), which is very useful for improving food supply projections. We finally discuss the impact of weather by correlating our results with crop phenology to show that WSTATT is able to capture physical properties of crop growth.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确及时的作物测绘对于产量估算、保险索赔和保护工作至关重要。多年来，已经开发了许多成功的作物测绘机器学习模型，这些模型仅使用卫星的多光谱图像来预测感兴趣区域的作物类型。然而，这些传统方法没有考虑控制作物生长的物理过程。在较高层面上，作物生长可以被设想为物理参数，例如天气和土壤类型，作用于植物导致作物生长，可以通过卫星观察到。在本文中，我们提出了基于天气的时空分割网络（WSTATT），这是一种深度学习模型，通过将其制定为结合天气（Daymet）和卫星图像（Sentinel-）的逆模型来利用对作物生长的理解。 2）生成准确的作物图。通过比较分割图和 F1 分类分数，我们表明我们的方法比仅依赖光谱图像的现有算法提供了显着改进。此外，在 WSTATT 架构中有效利用注意力机制可以在季节早期（最多提前 5 个月）检测作物类型，这对于改善粮食供应预测非常有用。最后，我们通过将我们的结果与作物物候学相关联来讨论天气的影响，以表明 WSTATT 能够捕获作物生长的物理特性。</details>
**PDF:** <http://arxiv.org/pdf/2401.15875v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection**<br />
**Title_cn:** LiDAR-PTQ：点云 3D 物体检测的训练后量化<br />
**Authors:** Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu Zhao, Xiaobo Lu, Xiangxiang Chu<br />
**Abstract:** <details><summary>原文: </summary>Due to highly constrained computing power and memory, deploying 3D lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization (PTQ) has been widely adopted in 2D vision tasks. However, applying it directly to 3D lidar-based tasks inevitably leads to performance degradation. As a remedy, we propose an effective PTQ method called LiDAR-PTQ, which is particularly curated for 3D lidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ features three main components, \textbf{(1)} a sparsity-based calibration method to determine the initialization of quantization parameters, \textbf{(2)} a Task-guided Global Positive Loss (TGPL) to reduce the disparity between the final predictions before and after quantization, \textbf{(3)} an adaptive rounding-to-nearest operation to minimize the layerwise reconstruction error. Extensive experiments demonstrate that our LiDAR-PTQ can achieve state-of-the-art quantization performance when applied to CenterPoint (both Pillar-based and Voxel-based). To our knowledge, for the very first time in lidar-based 3D detection tasks, the PTQ INT8 model's accuracy is almost the same as the FP32 model while enjoying $3\times$ inference speedup. Moreover, our LiDAR-PTQ is cost-effective being $30\times$ faster than the quantization-aware training method. Code will be released at \url{https://github.com/StiphyJay/LiDAR-PTQ}.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于计算能力和内存高度有限，在自动驾驶车辆和机器人配备的边缘设备上部署基于 3D 激光雷达的探测器面临着严峻的挑战。作为一种方便、直接的模型压缩方法，训练后量化（PTQ）已在 2D 视觉任务中广泛采用。然而，将其直接应用于基于 3D 激光雷达的任务不可避免地会导致性能下降。作为补救措施，我们提出了一种名为 LiDAR-PTQ 的有效 PTQ 方法，该方法专门用于 3D 激光雷达检测（基于 SPConv 和无 SPConv）。我们的 LiDAR-PTQ 具有三个主要组成部分，\textbf{(1)} 是一种基于稀疏性的校准方法，用于确定量化参数的初始化，\textbf{(2)} 是一种任务引导的全局正损失 (TGPL)，用于减少量化前后最终预测之间的差异， \textbf{(3)} 是一种自适应舍入到最近的操作，以最小化分层重建误差。大量实验表明，我们的 LiDAR-PTQ 在应用于 CenterPoint（基于 Pillar 和基于体素）时可以实现最先进的量化性能。据我们所知，在基于激光雷达的 3D 检测任务中，PTQ INT8 模型的准确性几乎与 FP32 模型相同，同时享受 3 倍的推理加速。此外，我们的 LiDAR-PTQ 具有成本效益，比量化感知训练方法快 30 美元。代码将在 \url{https://github.com/StiphyJay/LiDAR-PTQ} 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.15865v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Importance-Aware Adaptive Dataset Distillation**<br />
**Title_cn:** 重要性感知自适应数据集蒸馏<br />
**Authors:** Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama<br />
**Abstract:** <details><summary>原文: </summary>Herein, we propose a novel dataset distillation method for constructing small informative datasets that preserve the information of the large original datasets. The development of deep learning models is enabled by the availability of large-scale datasets. Despite unprecedented success, large-scale datasets considerably increase the storage and transmission costs, resulting in a cumbersome model training process. Moreover, using raw data for training raises privacy and copyright concerns. To address these issues, a new task named dataset distillation has been introduced, aiming to synthesize a compact dataset that retains the essential information from the large original dataset. State-of-the-art (SOTA) dataset distillation methods have been proposed by matching gradients or network parameters obtained during training on real and synthetic datasets. The contribution of different network parameters to the distillation process varies, and uniformly treating them leads to degraded distillation performance. Based on this observation, we propose an importance-aware adaptive dataset distillation (IADD) method that can improve distillation performance by automatically assigning importance weights to different network parameters during distillation, thereby synthesizing more robust distilled datasets. IADD demonstrates superior performance over other SOTA dataset distillation methods based on parameter matching on multiple benchmark datasets and outperforms them in terms of cross-architecture generalization. In addition, the analysis of self-adaptive weights demonstrates the effectiveness of IADD. Furthermore, the effectiveness of IADD is validated in a real-world medical application such as COVID-19 detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这里，我们提出了一种新颖的数据集蒸馏方法，用于构建保留大型原始数据集信息的小型信息数据集。深度学习模型的开发是通过大规模数据集的可用性来实现的。尽管取得了前所未有的成功，但大规模数据集大大增加了存储和传输成本，导致模型训练过程变得繁琐。此外，使用原始数据进行训练会引发隐私和版权问题。为了解决这些问题，引入了一项名为数据集蒸馏的新任务，旨在合成一个紧凑的数据集，保留大型原始数据集中的基本信息。通过匹配真实数据集和合成数据集训练期间获得的梯度或网络参数，提出了最先进的（SOTA）数据集蒸馏方法。不同的网络参数对蒸馏过程的贡献各不相同，统一处理它们会导致蒸馏性能下降。基于这一观察，我们提出了一种重要性感知自适应数据集蒸馏（IADD）方法，该方法可以通过在蒸馏过程中自动为不同网络参数分配重要性权重来提高蒸馏性能，从而合成更稳健的蒸馏数据集。 IADD 表现出优于其他基于多个基准数据集参数匹配的 SOTA 数据集蒸馏方法的性能，并在跨架构泛化方面优于它们。此外，自适应权重的分析证明了IADD的有效性。此外，IADD 的有效性在现实世界的医疗应用（例如 COVID-19 检测）中得到了验证。</details>
**PDF:** <http://arxiv.org/pdf/2401.15863v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **Diffusion Facial Forgery Detection**<br />
**Title_cn:** 扩散面部伪造检测<br />
**Authors:** Harry Cheng, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli<br />
**Abstract:** <details><summary>原文: </summary>Detecting diffusion-generated images has recently grown into an emerging research area. Existing diffusion-based datasets predominantly focus on general image generation. However, facial forgeries, which pose a more severe social risk, have remained less explored thus far. To address this gap, this paper introduces DiFF, a comprehensive dataset dedicated to face-focused diffusion-generated images. DiFF comprises over 500,000 images that are synthesized using thirteen distinct generation methods under four conditions. In particular, this dataset leverages 30,000 carefully collected textual and visual prompts, ensuring the synthesis of images with both high fidelity and semantic consistency. We conduct extensive experiments on the DiFF dataset via a human test and several representative forgery detection methods. The results demonstrate that the binary detection accuracy of both human observers and automated detectors often falls below 30%, shedding light on the challenges in detecting diffusion-generated facial forgeries. Furthermore, we propose an edge graph regularization approach to effectively enhance the generalization capability of existing detectors.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测扩散生成的图像最近已发展成为一个新兴的研究领域。现有的基于扩散的数据集主要关注一般图像生成。然而，迄今为止，对构成更严重社会风险的面部伪造的研究仍然较少。为了解决这一差距，本文引入了 DiFF，这是一个专用于以人脸为中心的扩散生成图像的综合数据集。 DiFF 包含超过 500,000 张图像，这些图像是在四种条件下使用 13 种不同的生成方法合成的。特别是，该数据集利用了 30,000 个精心收集的文本和视觉提示，确保图像的合成具有高保真度和语义一致性。我们通过人体测试和几种代表性的伪造检测方法对 DiFF 数据集进行了广泛的实验。结果表明，人类观察者和自动检测器的二进制检测精度通常低于 30%，揭示了检测扩散生成的面部伪造所面临的挑战。此外，我们提出了一种边缘图正则化方法来有效增强现有检测器的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.15859v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **LCVO: An Efficient Pretraining-Free Framework for Visual Question Answering Grounding**<br />
**Title_cn:** LCVO：一种高效的免预训练视觉问答基础框架<br />
**Authors:** Yuhan Chen, Lumei Su, Lihua Chen, Zhiwei Lin<br />
**Abstract:** <details><summary>原文: </summary>In this paper, the LCVO modular method is proposed for the Visual Question Answering (VQA) Grounding task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf Open-Vocabulary Object Detection (OVD) model, where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCVO establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models, exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources, evaluating the proposed method's performance on benchmark datasets including GQA, CLEVR, and VizWiz-VQA-Grounding. Comparative analyses with baseline methods demonstrate the robust competitiveness of LCVO.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了 LCVO 模块化方法，用于视觉语言多模态领域的视觉问答（VQA）基础任务。这种方法依赖于冻结的大语言模型 (LLM) 作为现成的 VQA 模型和现成的开放词汇对象检测 (OVD) 模型之间的中间中介，其中 LLM 在 VQA 模型和开放词汇对象检测 (OVD) 模型之间转换和传递文本信息。这两个模块基于设计的提示。 LCVO建立了一个集成的即插即用框架，无需任何预训练过程。该框架可以在低计算资源下部署用于 VQA Grounding 任务。框架内的模块化模型允许应用各种最先进的预训练模型，展现出与时俱进的巨大潜力。实验实现是在计算和内存资源受限的情况下进行的，评估了所提出的方法在 GQA、CLEVR 和 VizWiz-VQA-Grounding 等基准数据集上的性能。与基线方法的比较分析证明了 LCVO 强大的竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2401.15842v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes**<br />
**Title_cn:** 越来越少：使用更少的基类从很少的示例中更好地学习<br />
**Authors:** Raphael Lafargue, Yassir Bendou, Bastien Pasdeloup, Jean-Philippe Diguet, Ian Reid, Vincent Gripon, Jack Valmadre<br />
**Abstract:** <details><summary>原文: </summary>When training data is scarce, it is common to make use of a feature extractor that has been pre-trained on a large base dataset, either by fine-tuning its parameters on the ``target'' dataset or by directly adopting its representation as features for a simple classifier. Fine-tuning is ineffective for few-shot learning, since the target dataset contains only a handful of examples. However, directly adopting the features without fine-tuning relies on the base and target distributions being similar enough that these features achieve separability and generalization. This paper investigates whether better features for the target dataset can be obtained by training on fewer base classes, seeking to identify a more useful base dataset for a given task.We consider cross-domain few-shot image classification in eight different domains from Meta-Dataset and entertain multiple real-world settings (domain-informed, task-informed and uninformed) where progressively less detail is known about the target task. To our knowledge, this is the first demonstration that fine-tuning on a subset of carefully selected base classes can significantly improve few-shot learning. Our contributions are simple and intuitive methods that can be implemented in any few-shot solution. We also give insights into the conditions in which these solutions are likely to provide a boost in accuracy. We release the code to reproduce all experiments from this paper on GitHub. https://github.com/RafLaf/Few-and-Fewer.git</details>
**Abstract_cn:** <details><summary>译文: </summary>当训练数据稀缺时，通常使用在大型基础数据集上预先训练的特征提取器，或者通过在“目标”数据集上微调其参数，或者直接采用其表示形式简单分类器的特征。微调对于少样本学习是无效的，因为目标数据集仅包含少数示例。然而，直接采用特征而不进行微调依赖于基础分布和目标分布足够相似以使这些特征实现可分离性和泛化性。本文研究了是否可以通过在更少的基类上进行训练来获得更好的目标数据集特征，寻求为给定任务识别更有用的基数据集。我们考虑来自元的八个不同领域的跨域少样本图像分类。数据集和娱乐多个现实世界设置（领域通知、任务通知和非通知），其中有关目标任务的详细信息逐渐减少。据我们所知，这是第一次证明对精心选择的基类子集进行微调可以显着改善小样本学习。我们的贡献是简单直观的方法，可以在任何少数解决方案中实现。我们还深入了解这些解决方案可能提高准确性的条件。我们在 GitHub 上发布了重现本文所有实验的代码。 https://github.com/RafLaf/Few-and-Fewer.git</details>
**PDF:** <http://arxiv.org/pdf/2401.15834v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **Knowledge-Aware Neuron Interpretation for Scene Classification**<br />
**Title_cn:** 用于场景分类的知识感知神经元解释<br />
**Authors:** Yong Guan, Freddy Lecue, Jiaoyan Chen, Ru Li, Jeff Z. Pan<br />
**Abstract:** <details><summary>原文: </summary>Although neural models have achieved remarkable performance, they still encounter doubts due to the intransparency. To this end, model prediction explanation is attracting more and more attentions. However, current methods rarely incorporate external knowledge and still suffer from three limitations: (1) Neglecting concept completeness. Merely selecting concepts may not sufficient for prediction. (2) Lacking concept fusion. Failure to merge semantically-equivalent concepts. (3) Difficult in manipulating model behavior. Lack of verification for explanation on original model. To address these issues, we propose a novel knowledge-aware neuron interpretation framework to explain model predictions for image scene classification. Specifically, for concept completeness, we present core concepts of a scene based on knowledge graph, ConceptNet, to gauge the completeness of concepts. Our method, incorporating complete concepts, effectively provides better prediction explanations compared to baselines. Furthermore, for concept fusion, we introduce a knowledge graph-based method known as Concept Filtering, which produces over 23% point gain on neuron behaviors for neuron interpretation. At last, we propose Model Manipulation, which aims to study whether the core concepts based on ConceptNet could be employed to manipulate model behavior. The results show that core concepts can effectively improve the performance of original model by over 26%.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管神经模型已经取得了令人瞩目的性能，但由于不透明，它们仍然受到质疑。为此，模型预测解释越来越受到人们的关注。然而，当前的方法很少结合外部知识，并且仍然存在三个局限性：（1）忽视概念完整性。仅仅选择概念可能不足以进行预测。 (2)缺乏概念融合。未能合并语义等效的概念。 (3)难以操纵模型行为。缺乏对原始模型解释的验证。为了解决这些问题，我们提出了一种新颖的知识感知神经元解释框架来解释图像场景分类的模型预测。具体来说，为了概念完整性，我们基于知识图谱、ConceptNet 提出场景的核心概念，以衡量概念的完整性。与基线相比，我们的方法结合了完整的概念，有效地提供了更好的预测解释。此外，对于概念融合，我们引入了一种基于知识图的方法，称为概念过滤，它可以使神经元解释的神经元行为获得超过 23% 的增益。最后，我们提出了模型操纵，旨在研究基于ConceptNet的核心概念是否可以用于操纵模型行为。结果表明，核心概念可有效将原模型性能提升26%以上。</details>
**PDF:** <http://arxiv.org/pdf/2401.15820v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Transparency Attacks: How Imperceptible Image Layers Can Fool AI Perception**<br />
**Title_cn:** 透明攻击：难以察觉的图像层如何欺骗人工智能感知<br />
**Authors:** Forrest McKee, David Noever<br />
**Abstract:** <details><summary>原文: </summary>This paper investigates a novel algorithmic vulnerability when imperceptible image layers confound multiple vision models into arbitrary label assignments and captions. We explore image preprocessing methods to introduce stealth transparency, which triggers AI misinterpretation of what the human eye perceives. The research compiles a broad attack surface to investigate the consequences ranging from traditional watermarking, steganography, and background-foreground miscues. We demonstrate dataset poisoning using the attack to mislabel a collection of grayscale landscapes and logos using either a single attack layer or randomly selected poisoning classes. For example, a military tank to the human eye is a mislabeled bridge to object classifiers based on convolutional networks (YOLO, etc.) and vision transformers (ViT, GPT-Vision, etc.). A notable attack limitation stems from its dependency on the background (hidden) layer in grayscale as a rough match to the transparent foreground image that the human eye perceives. This dependency limits the practical success rate without manual tuning and exposes the hidden layers when placed on the opposite display theme (e.g., light background, light transparent foreground visible, works best against a light theme image viewer or browser). The stealth transparency confounds established vision systems, including evading facial recognition and surveillance, digital watermarking, content filtering, dataset curating, automotive and drone autonomy, forensic evidence tampering, and retail product misclassifying. This method stands in contrast to traditional adversarial attacks that typically focus on modifying pixel values in ways that are either slightly perceptible or entirely imperceptible for both humans and machines.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了当难以察觉的图像层将多个视觉模型混淆为任意标签分配和标题时的一种新算法漏洞。我们探索图像预处理方法来引入隐形透明度，这会引发人工智能对人眼感知的误解。该研究编制了广泛的攻击面，以调查传统水印、隐写术和背景-前景错误等后果。我们通过使用单个攻击层或随机选择的中毒类别来错误标记灰度景观和徽标集合的攻击来演示数据集中毒。例如，对于人眼来说，军用坦克是通往基于卷积网络（YOLO 等）和视觉转换器（ViT、GPT-Vision 等）的对象分类器的错误标签桥梁。一个显着的攻击限制源于它对灰度背景（隐藏）层的依赖，作为与人眼感知的透明前景图像的粗略匹配。这种依赖性限制了无需手动调整的实际成功率，并且当放置在相反的显示主题上时会暴露隐藏层（例如，浅色背景、浅色透明前景可见，最适合浅色主题图像查看器或浏览器）。隐形透明度混淆了现有的视觉系统，包括逃避面部识别和监视、数字水印、内容过滤、数据集管理、汽车和无人机自主、法医证据篡改以及零售产品错误分类。这种方法与传统的对抗性攻击形成鲜明对比，传统的对抗性攻击通常侧重于以人类和机器稍微可感知或完全不可感知的方式修改像素值。</details>
**PDF:** <http://arxiv.org/pdf/2401.15817v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields**<br />
**Title_cn:** 分而治之：重新思考神经辐射场的训练范式<br />
**Authors:** Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado<br />
**Abstract:** <details><summary>原文: </summary>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 在合成 3D 场景的高保真视图方面表现出了潜力，但 NeRF 的标准训练范例预设了训练集中的每个图像具有同等重要性。这种假设对渲染呈现复杂几何形状的特定视图提出了重大挑战，从而导致性能不佳。在本文中，我们仔细研究了当前训练范例的含义，并重新设计它，以通过 NeRF 获得更出色的渲染质量。根据输入视图的视觉相似性将输入视图分为多个组，并在每个组上训练单独的模型，使每个模型能够专门研究特定区域，而不会牺牲速度或效率。随后，这些专门模型的知识通过师生蒸馏范例聚合成单个实体，从而实现在线渲染的空间效率。根据经验，我们在两个公开可用的数据集（即 NeRF 合成数据集和 Tanks&Temples）上评估了我们的新颖训练框架。我们的评估表明，我们的 DaC 训练管道提高了最先进的基线模型的渲染质量，同时表现出卓越的最低收敛性。</details>
**PDF:** <http://arxiv.org/pdf/2401.16144v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models**<br />
**Title_cn:** Diffutoon：通过扩散模型进行高分辨率可编辑卡通着色<br />
**Authors:** Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang<br />
**Abstract:** <details><summary>原文: </summary>Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/).</details>
**Abstract_cn:** <details><summary>译文: </summary>卡通着色是一种非真实感动画渲染任务。其主要目的是渲染具有平坦且风格化外观的对象。随着扩散模型已经上升到图像合成方法的最前沿，本文深入研究了一种基于扩散模型的卡通着色的创新形式，旨在将逼真的视频直接渲染为动漫风格。在视频风格化中，现有的方法遇到了持续的挑战，特别是在保持一致性和实现高视觉质量方面。在本文中，我们将卡通着色问题建模为四个子问题：风格化、一致性增强、结构指导和着色。为了解决视频风格化的挑战，我们提出了一种有效的卡通着色方法，称为 \textit{Diffutoon}。 Diffutoon 能够渲染非常详细、高分辨率且持续时间较长的动漫风格视频。它还可以通过附加分支根据提示编辑内容。 Diffutoon 的功效是通过定量指标和人工评估来评估的。值得注意的是，在我们的实验中，Diffutoon 超越了开源和闭源基线方法。我们的工作伴随着 Github 上源代码和示例视频的发布（项目页面：https://ecnu-cilab.github.io/DiffutoonProjectPage/）。</details>
**PDF:** <http://arxiv.org/pdf/2401.16224v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Spatial-Aware Latent Initialization for Controllable Image Generation**<br />
**Title_cn:** 用于可控图像生成的空间感知潜在初始化<br />
**Authors:** Wenqiang Sun, Teng Li, Zehong Lin, Jun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recently, text-to-image diffusion models have demonstrated impressive ability to generate high-quality images conditioned on the textual input. However, these models struggle to accurately adhere to textual instructions regarding spatial layout information. While previous research has primarily focused on aligning cross-attention maps with layout conditions, they overlook the impact of the initialization noise on the layout guidance. To achieve better layout control, we propose leveraging a spatial-aware initialization noise during the denoising process. Specifically, we find that the inverted reference image with finite inversion steps contains valuable spatial awareness regarding the object's position, resulting in similar layouts in the generated images. Based on this observation, we develop an open-vocabulary framework to customize a spatial-aware initialization noise for each layout condition. Without modifying other modules except the initialization noise, our approach can be seamlessly integrated as a plug-and-play module within other training-free layout guidance frameworks. We evaluate our approach quantitatively and qualitatively on the available Stable Diffusion model and COCO dataset. Equipped with the spatial-aware latent initialization, our method significantly improves the effectiveness of layout guidance while preserving high-quality content.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，文本到图像的扩散模型已经展示了根据文本输入生成高质量图像的令人印象深刻的能力。然而，这些模型很难准确地遵守有关空间布局信息的文本指令。虽然之前的研究主要集中在将交叉注意力图与布局条件对齐，但他们忽略了初始化噪声对布局指导的影响。为了实现更好的布局控制，我们建议在去噪过程中利用空间感知初始化噪声。具体来说，我们发现具有有限反转步骤的反转参考图像包含有关对象位置的有价值的空间意识，从而在生成的图像中产生相似的布局。基于这一观察，我们开发了一个开放词汇框架，为每个布局条件定制空间感知的初始化噪声。除了初始化噪声之外，无需修改其他模块，我们的方法可以作为即插即用模块无缝集成到其他免培训布局指导框架中。我们在可用的稳定扩散模型和 COCO 数据集上定量和定性地评估我们的方法。配备空间感知潜在初始化，我们的方法显着提高了布局指导的有效性，同时保留了高质量的内容。</details>
**PDF:** <http://arxiv.org/pdf/2401.16157v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling**<br />
**Title_cn:** Motion-I2V：通过显式运动建模实现一致且可控的图像到视频生成<br />
**Authors:** Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image's feature to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even at the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region annotations. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出 Motion-I2V，这是一种用于一致且可控的图像到视频生成 (I2V) 的新颖框架。与之前直接学习复杂的图像到视频映射的方法相比，Motion-I2V 通过显式运动建模将 I2V 分解为两个阶段。对于第一阶段，我们提出了一种基于扩散的运动场预测器，其重点是推导参考图像像素的轨迹。对于第二阶段，我们提出运动增强时间注意力来增强视频潜在扩散模型中有限的一维时间注意力。该模块可以在第一阶段预测轨迹的指导下，有效地将参考图像的特征传播到合成帧。与现有方法相比，即使存在较大运动和视点变化，Motion-I2V 也可以生成更一致的视频。通过第一阶段训练稀疏轨迹ControlNet，Motion-I2V可以支持用户通过稀疏轨迹和区域注释精确控制运动轨迹和运动区域。与仅依赖文本指令相比，这为 I2V 过程提供了更多的可控性。此外，Motion-I2V 的第二阶段自然支持零镜头视频到视频的转换。定性和定量比较都证明了 Motion-I2V 相对于先前方法在一致且可控的图像到视频生成方面的优势。</details>
**PDF:** <http://arxiv.org/pdf/2401.15977v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Concise but Effective Network for Image Guided Depth Completion in Autonomous Driving**<br />
**Title_cn:** 自动驾驶中图像引导深度补全的简洁而有效的网络<br />
**Authors:** Moyun Liu, Youping Chen, Jingming Xie, Lei Yao, Yang Zhang, Joey Tianyi Zhou<br />
**Abstract:** <details><summary>原文: </summary>Depth completion is a crucial task in autonomous driving, aiming to convert a sparse depth map into a dense depth prediction. Due to its potentially rich semantic information, RGB image is commonly fused to enhance the completion effect. Image-guided depth completion involves three key challenges: 1) how to effectively fuse the two modalities; 2) how to better recover depth information; and 3) how to achieve real-time prediction for practical autonomous driving. To solve the above problems, we propose a concise but effective network, named CENet, to achieve high-performance depth completion with a simple and elegant structure. Firstly, we use a fast guidance module to fuse the two sensor features, utilizing abundant auxiliary features extracted from the color space. Unlike other commonly used complicated guidance modules, our approach is intuitive and low-cost. In addition, we find and analyze the optimization inconsistency problem for observed and unobserved positions, and a decoupled depth prediction head is proposed to alleviate the issue. The proposed decoupled head can better output the depth of valid and invalid positions with very few extra inference time. Based on the simple structure of dual-encoder and single-decoder, our CENet can achieve superior balance between accuracy and efficiency. In the KITTI depth completion benchmark, our CENet attains competitive performance and inference speed compared with the state-of-the-art methods. To validate the generalization of our method, we also evaluate on indoor NYUv2 dataset, and our CENet still achieve impressive results. The code of this work will be available at https://github.com/lmomoy/CENet.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度补全是自动驾驶中的一项关键任务，旨在将稀疏深度图转换为密集深度预测。由于其潜在丰富的语义信息，RGB图像通常被融合以增强补全效果。图像引导深度补全涉及三个关键挑战：1）如何有效融合两种模式； 2）如何更好的恢复深度信息； 3）如何实现实际自动驾驶的实时预测。为了解决上述问题，我们提出了一种简洁但有效的网络，命名为CENet，以简单而优雅的结构实现高性能深度完成。首先，我们使用快速引导模块来融合两个传感器特征，利用从颜色空间提取的丰富辅助特征。与其他常用的复杂引导模块不同，我们的方法直观且成本低廉。此外，我们发现并分析了观测位置和未观测位置的优化不一致问题，并提出了解耦深度预测头来缓解该问题。所提出的解耦头可以更好地输出有效和无效位置的深度，并且只需很少的额外推理时间。基于双编码器和单解码器的简单结构，我们的 CENet 可以在精度和效率之间实现卓越的平衡。在 KITTI 深度完成基准中，与最先进的方法相比，我们的 CENet 获得了有竞争力的性能和推理速度。为了验证我们方法的泛化性，我们还对室内 NYUv2 数据集进行了评估，并且我们的 CENet 仍然取得了令人印象深刻的结果。这项工作的代码将在 https://github.com/lmomoy/CENet 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.15902v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Sliced Wasserstein with Random-Path Projecting Directions**<br />
**Title_cn:** 具有随机路径投影方向的切片 Wasserstein<br />
**Authors:** Khai Nguyen, Shujian Zhang, Tam Le, Nhat Ho<br />
**Abstract:** <details><summary>原文: </summary>Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational properties of RPSW and IWRPSW. Finally, we showcase the favorable performance of RPSW and IWRPSW in gradient flow and the training of denoising diffusion generative models on images.</details>
**Abstract_cn:** <details><summary>译文: </summary>切片分布选择已被用作一种有效的技术，基于最小化应用中的切片 Wasserstein 距离来提高参数估计器的性能。以前的工作要么利用昂贵的优化来选择切片分布，要么使用需要昂贵的采样方法的切片分布。在这项工作中，我们提出了一种免优化的切片分布，为蒙特卡罗期望估计提供快速采样。特别是，我们引入了随机路径投影方向（RPD），它是通过利用两个输入测量后的两个随机向量之间的归一化差来构造的。从 RPD 中，我们推导出随机路径切片分布 (RPSD) 和切片 Wasserstein 的两个变体，即随机路径投影切片 Wasserstein (RPSW) 和重要性加权随机路径投影切片 Wasserstein (IWRPSW)。然后我们讨论 RPSW 和 IWRPSW 的拓扑、统计和计算特性。最后，我们展示了 RPSW 和 IWRPSW 在梯度流中的良好性能以及图像上的去噪扩散生成模型的训练。</details>
**PDF:** <http://arxiv.org/pdf/2401.15889v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**<br />
**Title_cn:** InternLM-XComposer2：掌握视觉语言大模型中的自由形式文本图像合成和理解<br />
**Authors:** Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出 InternLM-XComposer2，这是一种尖端的视觉语言模型，在自由形式的文本图像合成和理解方面表现出色。该模型超越了传统的视觉语言理解，可以根据轮廓、详细文本规范和参考图像等不同输入巧妙地制作交错的文本图像内容，从而实现高度可定制的内容创建。 InternLM-XComposer2提出了一种部分LoRA（PLoRA）方法，该方法将额外的LoRA参数专门应用于图像标记，以保持预先训练的语言知识的完整性，在精确的视觉理解和具有文学天赋的文本写作之间取得平衡。实验结果表明，基于 InternLM2-7B 的 InternLM-XComposer2 在生成高质量长文本多模态内容方面具有优越性，并且在各种基准测试中具有出色的视觉语言理解性能，不仅显着优于现有的多模态模型，而且还匹配甚至在某些评测中超过了GPT-4V和Gemini Pro。这凸显了其在多模态理解领域的卓越熟练程度。具有 7B 参数的 InternLM-XComposer2 模型系列可在 https://github.com/InternLM/InternLM-XComposer 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16420v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology**<br />
**Title_cn:** PathMMU：用于病理学理解和推理的大规模多模式专家级基准<br />
**Authors:** Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Xiaoxiao Lan, Mengyue Zheng, Jingxiong Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology. However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation. To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question. The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\&As. Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU. We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption. We also fine-tune representative LMMs to assess their adaptability to PathMMU. The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\% zero-shot performance, significantly lower than the 71.4\% demonstrated by human pathologists. After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\%, but still fall short of the expertise shown by pathologists. We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型多模态模型的出现释放了人工智能领域的巨大潜力，特别是在病理学领域。然而，缺乏专业化、高质量的基准，阻碍了其发展和精确评估。为了解决这个问题，我们引入了 PathMMU，这是最大、质量最高、经过专家验证的 LMM 病理学基准。它包含 33,573 个多模态多项选择问题和来自不同来源的 21,599 张图像，每个问题都附有正确答案的解释。 PathMMU 的构建利用了 GPT-4V 的强大功能，利用大约 30,000 个收集的图像标题对来生成问答。值得注意的是，为了最大限度地发挥PathMMU的权威，我们邀请了六位病理学家在PathMMU的验证和测试集中严格标准下仔细审查每个问题，同时为PathMMU设定了专家级的性能基准。我们进行了广泛的评估，包括对 14 个开源和三个闭源 LMM 及其对图像损坏的鲁棒性进行零样本评估。我们还对代表性 LMM 进行微调，以评估它们对 PathMMU 的适应性。实证结果表明，先进的 LMM 很难应对具有挑战性的 PathMMU 基准，其中表现最好的 LMM GPT-4V 仅实现了 51.7% 的零样本性能，显着低于人类病理学家所证明的 71.4%。经过微调，即使是开源的 LMM 也能以超过 60% 的性能超越 GPT-4V，但仍达不到病理学家所表现出的专业水平。我们希望 PathMMU 能够提供宝贵的见解，并促进更专业的下一代病理学法学硕士的发展。</details>
**PDF:** <http://arxiv.org/pdf/2401.16355v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs**<br />
**Title_cn:** LLaVA-MoLE：LoRA 专家的稀疏组合，用于缓解指令微调 MLLM 中的数据冲突<br />
**Authors:** Shaoxiang Chen, Zequn Jie, Lin Ma<br />
**Abstract:** <details><summary>原文: </summary>Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply a sparse mixture of LoRA experts for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA finetuing of LLaVA-1.5, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.</details>
**Abstract_cn:** <details><summary>译文: </summary>对各种图像文本指令数据进行指令微调是获得通用的多模态大语言模型（MLLM）的关键，指令数据的不同配置可以导致具有不同能力的微调模型。然而，我们发现，当混合来自不同域的指令数据时，数据冲突是不可避免的，这可能导致特定域的任务的性能下降。为了解决这个问题，我们建议应用 LoRA 专家的稀疏混合来进行指令微调 MLLM。在 Transformer 层中，我们通过专门为 MLP 层创建一组 LoRA 专家来扩展流行的低秩自适应 (LoRA) 方法，并根据路由函数将每个令牌路由到 top-1 专家，从而允许自适应选择来自不同域的令牌。由于 LoRA 专家是稀疏激活的，因此与原始 LoRA 方法相比，训练和推理成本大致保持不变。通过替换 LLaVA-1.5 的普通 LoRA 微调，我们的最终模型被命名为 LLaVA-MoLE。大量实验证明，LLaVA-MoLE 有效缓解了将多个不同的指令数据集与各种配置混合时的数据冲突问题，并在强大的 plain-LoRA 基线上实现了一致的性能增益。最重要的是，在混合数据集上，LLaVA-MoLE 甚至可以优于使用两倍样本训练的普通 LoRA 基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.16160v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception**<br />
**Title_cn:** Mobile-Agent：具有视觉感知的自主多模式移动设备代理<br />
**Authors:** Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang<br />
**Abstract:** <details><summary>原文: </summary>Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于多模式大语言模型（MLLM）的移动设备代理正在成为流行的应用程序。在本文中，我们介绍了 Mobile-Agent，一种自主的多模式移动设备代理。 Mobile-Agent 首先利用视觉感知工具来准确识别和定位应用程序前端界面中的视觉和文本元素。基于感知到的视觉上下文，它会自主规划和分解复杂的操作任务，并逐步导航移动应用程序进行操作。与以前依赖应用程序的 XML 文件或移动系统元数据的解决方案不同，Mobile-Agent 允许以视觉为中心的方式在不同的移动操作环境中提供更大的适应性，从而消除了特定于系统的定制的必要性。为了评估 Mobile-Agent 的性能，我们引入了 Mobile-Eval，这是评估移动设备操作的基准。基于Mobile-Eval，我们对Mobile-Agent进行了全面的评估。实验结果表明Mobile-Agent取得了显着的准确率和完成率。即使有挑战性的指令，例如多应用程序操作，Mobile-Agent 仍然可以完成要求。代码和模型将在 https://github.com/X-PLUG/MobileAgent 开源。</details>
**PDF:** <http://arxiv.org/pdf/2401.16158v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers**<br />
**Title_cn:** 正在寻找更合适的选择？适应个体驾驶员的增量学习多模态对象引用框架<br />
**Authors:** Amr Gomaa, Guillermo Reyes, Michael Feld, Antonio Krüger<br />
**Abstract:** <details><summary>原文: </summary>The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we propose \textit{IcRegress}, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at \url{https://github.com/amrgomaaelhady/IcRegress}.</details>
**Abstract_cn:** <details><summary>译文: </summary>汽车行业向自动和半自动车辆的快速发展使得传统的车辆交互方法（例如基于触摸和语音命令系统）不足以满足越来越多的非驾驶相关任务，例如引用外部物体。机动车。因此，研究转向手势输入（例如手、凝视和头部姿势手势）作为驾驶过程中更合适的交互模式。然而，由于驾驶的动态性和个体差异，驾驶员的手势输入表现存在显着差异。虽然从理论上讲，这种固有的可变性可以通过大量数据驱动的机器学习模型来调节，但普遍的方法倾向于用于对象引用的受约束的、单实例训练的模型。这些模型持续适应个体驾驶员的不同行为和各种驾驶场景的能力有限。为了解决这个问题，我们提出了 \textit{IcRegress}，一种新颖的基于回归的增量学习方法，它适应不断变化的行为以及从事驾驶和参考物体双重任务的驾驶员的独特特征。我们建议为多模式手势界面提供更加个性化和适应性更强的解决方案，采用持续的终身学习来增强驾驶员体验、安全性和便利性。我们的方法使用车外对象引用用例进行了评估，突显了增量学习模型相对于单个训练模型在各种驾驶员特征（例如惯用手、驾驶经验和多种驾驶条件）方面的优越性。最后，为了促进可重复性、简化部署并促进进一步研究，我们在 \url{https://github.com/amrgomaaelhady/IcRegress} 提供我们的方法作为开源框架。</details>
**PDF:** <http://arxiv.org/pdf/2401.16123v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Find the Cliffhanger: Multi-Modal Trailerness in Soap Operas**<br />
**Title_cn:** 寻找悬念：肥皂剧中的多模式预告片<br />
**Authors:** Carlo Bretti, Pascal Mettes, Hendrik Vincent Koops, Daan Odijk, Nanne van Noord<br />
**Abstract:** <details><summary>原文: </summary>Creating a trailer requires carefully picking out and piecing together brief enticing moments out of a longer video, making it a chal- lenging and time-consuming task. This requires selecting moments based on both visual and dialogue information. We introduce a multi-modal method for predicting the trailerness to assist editors in selecting trailer- worthy moments from long-form videos. We present results on a newly introduced soap opera dataset, demonstrating that predicting trailerness is a challenging task that benefits from multi-modal information. Code is available at https://github.com/carlobretti/cliffhanger</details>
**Abstract_cn:** <details><summary>译文: </summary>制作预告片需要从较长的视频中仔细挑选并拼凑出简短而诱人的时刻，这使其成为一项具有挑战性且耗时的任务。这需要根据视觉和对话信息来选择时刻。我们引入了一种多模态方法来预测预告片，以帮助编辑从长视频中选择值得预告片的时刻。我们在新引入的肥皂剧数据集上展示了结果，表明预测预告片是一项具有挑战性的任务，受益于多模态信息。代码可在 https://github.com/carlobretti/cliffhanger 获取</details>
**PDF:** <http://arxiv.org/pdf/2401.16076v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MoE-LLaVA: Mixture of Experts for Large Vision-Language Models**<br />
**Title_cn:** MoE-LLaVA：大型视觉语言模型的专家组合<br />
**Authors:** Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, Li Yuan<br />
**Abstract:** <details><summary>原文: </summary>For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation. In this work, we propose a novel training strategy MoE-tuning for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity. Furthermore, we present the MoE-LLaVA framework, a MoE-based sparse LVLM architecture. This framework uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Our extensive experiments highlight the excellent capabilities of MoE-LLaVA in visual understanding and its potential to reduce hallucinations in model outputs. Remarkably, with just 3 billion sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmarks. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at \url{https://github.com/PKU-YuanGroup/MoE-LLaVA}.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于大型视觉语言模型（LVLM），缩放模型可以有效提高性能。然而，扩展模型参数会显着增加训练和推断成本，因为计算中的每个标记都会激活所有模型参数。在这项工作中，我们提出了一种针对 LVLM 的新型训练策略 MoE-tuning，它可以构建参数数量惊人但计算成本恒定的稀疏模型，并有效解决通常与多模态学习和模型稀疏性相关的性能下降问题。此外，我们还提出了 MoE-LLaVA 框架，这是一种基于 MoE 的稀疏 LVLM 架构。该框架独特地在部署过程中通过路由器仅激活前 k 个专家，使其余专家保持不活动状态。我们广泛的实验强调了 MoE-LLaVA 在视觉理解方面的出色能力及其减少模型输出中的幻觉的潜力。值得注意的是，MoE-LLaVA 仅具有 30 亿个稀疏激活参数，在各种视觉理解数据集上表现出与 LLaVA-1.5-7B 相当的性能，甚至在物体幻觉基准测试中超过了 LLaVA-1.5-13B。通过 MoE-LLaVA，我们的目标是为稀疏 LVLM 建立基线，并为未来开发更高效、更有效的多模态学习系统的研究提供有价值的见解。代码发布于\url{https://github.com/PKU-YuanGroup/MoE-LLaVA}。</details>
**PDF:** <http://arxiv.org/pdf/2401.15947v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA**<br />
**Title_cn:** 松饼还是吉娃娃？使用多面板 VQA 挑战大型视觉语言模型<br />
**Authors:** Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang<br />
**Abstract:** <details><summary>原文: </summary>Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, our paper introduces Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark that specifically challenges models in comprehending multipanel images. The benchmark comprises 6,600 questions and answers related to multipanel images. While these questions are straightforward for average humans, achieving nearly perfect correctness, they pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) we tested. In our study, we utilized synthetically curated multipanel images specifically designed to isolate and evaluate the impact of diverse factors on model performance, revealing the sensitivity of LVLMs to various interferences in multipanel images, such as adjacent subfigures and layout complexity. As a result, MultipanelVQA highlights the need and direction for improving LVLMs' ability to understand complex visual-language contexts. Code and data are released at https://sites.google.com/view/multipanelvqa/home.</details>
**Abstract_cn:** <details><summary>译文: </summary>多面板图像（通常为网页截图、海报等）遍布我们的日常生活。这些图像的特点是由不同布局的多个子图组成，可以有效地向人们传达信息。为了构建先进的多模态人工智能应用程序，例如理解复杂场景和浏览网页的代理，多面板视觉推理技能至关重要，并且对这方面的模型进行全面评估也很重要。因此，我们的论文介绍了多面板视觉问答（MultipanelVQA），这是一种新颖的基准，专门挑战模型理解多面板图像的能力。该基准包含 6,600 个与多面板图像相关的问题和答案。虽然这些问题对于普通人来说很简单，几乎可以实现完美的正确性，但它们对我们测试的最先进的大视觉语言模型（LVLM）提出了重大挑战。在我们的研究中，我们利用专门设计的综合多面板图像来隔离和评估不同因素对模型性能的影响，揭示 LVLM 对多面板图像中各种干扰的敏感性，例如相邻子图和布局复杂性。因此，MultipanelVQA 强调了提高 LVLM 理解复杂视觉语言上下文的能力的需求和方向。代码和数据发布于 https://sites.google.com/view/multipanelvqa/home。</details>
**PDF:** <http://arxiv.org/pdf/2401.15847v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting**<br />
**Title_cn:** Endo-4DGS：使用 4D 高斯溅射进行内窥镜单眼场景重建的蒸馏深度排序<br />
**Authors:** Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Hongliang Ren<br />
**Abstract:** <details><summary>原文: </summary>In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes. Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands. Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras. Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate. To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data. This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations. This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions. We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process. Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy. These results underline the vast potential of Endo-4DGS to improve surgical assistance.</details>
**Abstract_cn:** <details><summary>译文: </summary>在机器人辅助微创手术领域，动态场景重建可以显着增强下游任务并改善手术结果。基于神经辐射场 (NeRF) 的方法最近因其重建场景的卓越能力而受到关注。尽管如此，这些方法受到缓慢的推理、长时间的训练和大量的计算需求的阻碍。此外，有些依赖于立体深度估计，但由于与立体相机相关的高成本和后勤挑战，这通常是不可行的。此外，目前可变形场景的单目重建质量还不够。为了克服这些障碍，我们推出了 Endo-4DGS，这是一种创新的实时内窥镜动态重建方法，它利用 4D 高斯分布 (GS)，不需要地面真实深度数据。该方法通过合并时间组件来扩展 3D GS，并利用轻量级 MLP 来捕获时间高斯变形。这有效地促进了多条件下动态手术场景的重建。我们还集成 Depth-Anything 从单目视图生成伪深度图，增强深度引导重建过程。我们的方法已经在两个手术数据集上得到了验证，事实证明它可以实时渲染、高效计算并以极高的准确性进行重建。这些结果凸显了 Endo-4DGS 在改善手术辅助方面的巨大潜力。</details>
**PDF:** <http://arxiv.org/pdf/2401.16416v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator**<br />
**Title_cn:** 发现错误：使用线框定位器生成非自回归图形布局<br />
**Authors:** Jieru Lin, Danqing Huang, Tiejun Zhao, Dechen Zhan, Chin-Yew Lin<br />
**Abstract:** <details><summary>原文: </summary>Layout generation is a critical step in graphic design to achieve meaningful compositions of elements. Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position). So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens. Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding. However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate. This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework. Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input. We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance. Experiments on two public datasets show that our approach outperforms both AR and NAR baselines. Extensive studies further prove the effectiveness of different modules with interesting findings. Our code will be available at https://github.com/ffffatgoose/SpotError.</details>
**Abstract_cn:** <details><summary>译文: </summary>布局生成是图形设计中实现有意义的元素组合的关键步骤。大多数以前的作品将其视为通过连接元素属性标记（即类别、大小、位置）的序列生成问题。到目前为止，自回归方法（AR）已经取得了有希望的结果，但在全局上下文建模中仍然受到限制，并且由于它只能处理先前生成的标记而受到错误传播的影响。最近的非自回归尝试（NAR）已经显示出有竞争力的结果，它提供了更广泛的上下文范围和通过迭代解码进行细化的灵活性。然而，当前的工作仅使用简单的启发式方法来识别错误的标记以进行细化，这是不准确的。本文首先进行深入分析，以更好地理解AR和NAR框架之间的区别。此外，根据我们的观察，像素空间在捕获图形布局的空间模式（例如重叠、对齐）时更加敏感，我们提出了一种基于学习的定位器来检测错误标记，该定位器将从生成的布局序列渲染的线框图像作为输入。我们证明它可以作为对象空间中元素序列的补充模式，并对整体性能做出巨大贡献。对两个公共数据集的实验表明，我们的方法优于 AR 和 NAR 基线。广泛的研究进一步证明了不同模块的有效性，并得出了有趣的发现。我们的代码将在 https://github.com/ffffatgoose/SpotError 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.16375v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Synthesis of 3D on-air signatures with the Sigma-Lognormal model**<br />
**Title_cn:** 使用 Sigma-Lognormal 模型合成 3D 直播签名<br />
**Authors:** Miguel A. Ferrer, Moises Diaz, Cristina Carmona-Duarte, Jose J. Quintana Hernandez, Rejean Plamondon<br />
**Abstract:** <details><summary>原文: </summary>Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification. A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas. This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves. Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures. Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases. We also observed that training 3D automatic signature verifiers with duplicates can reduce errors. We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures. Finally, a perception test confirmed the human likeness of the generated specimens. The databases generated are publicly available, only for research purposes, at .</details>
**Abstract_cn:** <details><summary>译文: </summary>签名合成是一种生成人工样本的计算技术，可以支持自动签名验证中的决策。许多工作致力于这个主题，其重点是在画布上合成动态和静态二维手写体。本文提出了一个利用对数正态性原理生成合成 3D 直播签名的框架，该框架模仿指尖移动时起作用的复杂神经运动控制过程。针对涉及人工个体和重复样本开发的常见情况，本文有助于综合：（1）完全 3D 新签名的轨迹和速度； (2) 仅知道签名的 3D 轨迹时的运动信息，以及 (3) 3D 真实签名的重复样本。验证是通过生成模仿真实签名的合成 3D 签名数据库来进行的，并显示对真实和熟练伪造品的自动签名验证报​​告的性能与真实和合成数据库的性能相似。我们还观察到，使用重复项训练 3D 自动签名验证器可以减少错误。我们进一步证明，我们的建议对于合成 3D 空中书写和手势也有效。最后，感知测试证实了生成的样本与人类的相似性。生成的数据库可公开获取，仅用于研究目的，网址为 。</details>
**PDF:** <http://arxiv.org/pdf/2401.16329v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation**<br />
**Title_cn:** 利用位置编码进行鲁棒的基于多参考的对象 6D 姿态估计<br />
**Authors:** Jaewoo Park, Jaeguk Kim, Nam Ik Cho<br />
**Abstract:** <details><summary>原文: </summary>Accurately estimating the pose of an object is a crucial task in computer vision and robotics. There are two main deep learning approaches for this: geometric representation regression and iterative refinement. However, these methods have some limitations that reduce their effectiveness. In this paper, we analyze these limitations and propose new strategies to overcome them. To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates. To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints. Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object. Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods. We will soon release the code.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确估计物体的姿态是计算机视觉和机器人技术中的一项关键任务。为此有两种主要的深度学习方法：几何表示回归和迭代细化。然而，这些方法有一些限制，降低了它们的有效性。在本文中，我们分析了这些局限性并提出了克服它们的新策略。为了解决几何表示模糊的问题，我们对对象的 3D 坐标使用具有高频分量的位置编码。为了解决细化方法中的局部最小值问题，我们引入了一种独立于内在矩阵约束的基于归一化图像平面的多参考细化策略。最后，我们利用自适应实例归一化和简单的遮挡增强方法来帮助我们的模型专注于目标对象。我们在 Linemod、Linemod-Occlusion 和 YCB-Video 数据集上的实验表明，我们的方法优于现有方法。我们将很快发布代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.16284v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction**<br />
**Title_cn:** FIMP：多智能体运动预测的未来交互建模<br />
**Authors:** Sungmin Woo, Minjung Kim, Donghyeong Kim, Sungjun Jang, Sangyoun Lee<br />
**Abstract:** <details><summary>原文: </summary>Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>多智能体运动预测是自动驾驶中的一个关键问题，但由于动态智能体的模糊意图及其复杂的交互，它仍然是一个挑战。现有研究试图通过使用历史时间步长中的确定数据来捕获道路实体之间的相互作用，因为未来信息不可用并且涉及高度不确定性。然而，如果没有足够的指导来捕获交互代理的未来状态，它们经常会产生不切实际的轨迹重叠。在这项工作中，我们提出了运动预测的未来交互建模（FIMP），它以端到端的方式捕获潜在的未来交互。 FIMP采用未来解码器，隐式提取中间特征级别中潜在的未来信息，并通过未来亲和力学习和top-k过滤策略识别交互实体对。实验表明，我们未来的交互建模显着提高了性能，从而在 Argoverse 运动预测基准上获得卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.16189v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **DeFlow: Decoder of Scene Flow Network in Autonomous Driving**<br />
**Title_cn:** DeFlow：自动驾驶中场景流网络的解码器<br />
**Authors:** Qingwen Zhang, Yi Yang, Heng Fang, Ruoyu Geng, Patric Jensfelt<br />
**Abstract:** <details><summary>原文: </summary>Scene flow estimation determines a scene's 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is open-sourced at https://github.com/KTH-RPL/deflow.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景流估计通过预测场景中点的运动来确定场景的 3D 运动场，尤其适用于辅助自动驾驶任务。许多以大规模点云作为输入的网络使用体素化来创建用于实时运行的伪图像。然而，体素化过程通常会导致特定点特征的丢失。这给场景流任务恢复这些特征带来了挑战。我们的论文介绍了 DeFlow，它可以使用门控循环单元 (GRU) 细化从基于体素的特征过渡到点特征。为了进一步增强场景流估计性能，我们制定了一种新颖的损失函数，该函数考虑了静态点和动态点之间的数据不平衡。对 Argoverse 2 场景流任务的评估表明，DeFlow 在大规模点云数据上取得了最先进的结果，表明我们的网络与其他网络相比具有更好的性能和效率。该代码在 https://github.com/KTH-RPL/deflow 上开源。</details>
**PDF:** <http://arxiv.org/pdf/2401.16122v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Domain adaptation strategies for 3D reconstruction of the lumbar spine using real fluoroscopy data**<br />
**Title_cn:** 使用真实透视数据进行腰椎 3D 重建的域适应策略<br />
**Authors:** Sascha Jecklin, Youyang Shen, Amandine Gout, Daniel Suter, Lilian Calvet, Lukas Zingg, Jennifer Straub, Nicola Alessandro Cavalcanti, Mazda Farshad, Philipp Fürnstahl, et.al.<br />
**Abstract:** <details><summary>原文: </summary>This study tackles key obstacles in adopting surgical navigation in orthopedic surgeries, including time, cost, radiation, and workflow integration challenges. Recently, our work X23D showed an approach for generating 3D anatomical models of the spine from only a few intraoperative fluoroscopic images. This negates the need for conventional registration-based surgical navigation by creating a direct intraoperative 3D reconstruction of the anatomy. Despite these strides, the practical application of X23D has been limited by a domain gap between synthetic training data and real intraoperative images.   In response, we devised a novel data collection protocol for a paired dataset consisting of synthetic and real fluoroscopic images from the same perspectives. Utilizing this dataset, we refined our deep learning model via transfer learning, effectively bridging the domain gap between synthetic and real X-ray data. A novel style transfer mechanism also allows us to convert real X-rays to mirror the synthetic domain, enabling our in-silico-trained X23D model to achieve high accuracy in real-world settings.   Our results demonstrated that the refined model can rapidly generate accurate 3D reconstructions of the entire lumbar spine from as few as three intraoperative fluoroscopic shots. It achieved an 84% F1 score, matching the accuracy of our previous synthetic data-based research. Additionally, with a computational time of only 81.1 ms, our approach provides real-time capabilities essential for surgery integration.   Through examining ideal imaging setups and view angle dependencies, we've further confirmed our system's practicality and dependability in clinical settings. Our research marks a significant step forward in intraoperative 3D reconstruction, offering enhancements to surgical planning, navigation, and robotics.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究解决了在骨科手术中采用手术导航的关键障碍，包括时间、成本、辐射和工作流程集成挑战。最近，我们的 X23D 工作展示了一种仅通过少量术中透视图像即可生成脊柱 3D 解剖模型的方法。通过创建直接的术中解剖结构 3D 重建，无需传统的基于配准的手术导航。尽管取得了这些进步，X23D 的实际应用仍然受到合成训练数据和真实术中图像之间的域差距的限制。作为回应，我们为配对数据集设计了一种新颖的数据收集协议，该数据集由来自相同角度的合成和真实荧光透视图像组成。利用该数据集，我们通过迁移学习完善了深度学习模型，有效地弥合了合成 X 射线数据与真实 X 射线数据之间的领域差距。新颖的传输机制还允许我们将真实的 X 射线转换为镜像合成域，使我们的计算机模拟训练的 X23D 模型能够在现实环境中实现高精度。我们的结果表明，经过改进的模型只需进行 3 次术中透视拍摄即可快速生成整个腰椎的准确 3D 重建。它获得了 84% 的 F1 分数，与我们之前基于综合数据的研究的准确性相匹配。此外，我们的方法的计算时间仅为 81.1 毫秒，提供了手术集成所必需的实时功能。通过检查理想的成像设置和视角依赖性，我们进一步证实了我们的系统在临床环境中的实用性和可靠性。我们的研究标志着术中 3D 重建领域向前迈出了重要一步，为手术规划、导航和机器人技术提供了增强。</details>
**PDF:** <http://arxiv.org/pdf/2401.16027v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **AccessLens: Auto-detecting Inaccessibility of Everyday Objects**<br />
**Title_cn:** AccessLens：自动检测日常对象的不可访问性<br />
**Authors:** Nahyun Kwon, Qian Lu, Muhammad Hasham Qazi, Joanne Liu, Changhoon Oh, Shu Kong, Jeeeun Kim<br />
**Abstract:** <details><summary>原文: </summary>In our increasingly diverse society, everyday physical interfaces often present barriers, impacting individuals across various contexts. This oversight, from small cabinet knobs to identical wall switches that can pose different contextual challenges, highlights an imperative need for solutions. Leveraging low-cost 3D-printed augmentations such as knob magnifiers and tactile labels seems promising, yet the process of discovering unrecognized barriers remains challenging because disability is context-dependent. We introduce AccessLens, an end-to-end system designed to identify inaccessible interfaces in daily objects, and recommend 3D-printable augmentations for accessibility enhancement. Our approach involves training a detector using the novel AccessDB dataset designed to automatically recognize 21 distinct Inaccessibility Classes (e.g., bar-small and round-rotate) within 6 common object categories (e.g., handle and knob). AccessMeta serves as a robust way to build a comprehensive dictionary linking these accessibility classes to open-source 3D augmentation designs. Experiments demonstrate our detector's performance in detecting inaccessible objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>在我们日益多元化的社会中，日常物理界面常常会带来障碍，影响不同环境下的个人。这种疏忽，从小型橱柜旋钮到相同的墙壁开关，可能会带来不同的环境挑战，凸显了对解决方案的迫切需求。利用旋钮放大镜和触觉标签等低成本 3D 打印增强功能似乎很有希望，但发现未被识别的障碍的过程仍然具有挑战性，因为残疾是与环境相关的。我们推出了 AccessLens，这是一种端到端系统，旨在识别日常物品中不可访问的界面，并推荐可 3D 打印的增强功能来增强可访问性。我们的方法包括使用新颖的 AccessDB 数据集来训练检测器，该数据集旨在自动识别 6 个常见对象类别（例如手柄和旋钮）内的 21 个不同的不可访问性类别（例如，条形小和圆形旋转）。 AccessMeta 是构建综合字典的强大方法，将这些辅助功能类与开源 3D 增强设计联系起来。实验证明了我们的探测器在探测难以接近的物体方面的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.15996v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Hand-Centric Motion Refinement for 3D Hand-Object Interaction via Hierarchical Spatial-Temporal Modeling**<br />
**Title_cn:** 通过分层时空建模实现以手为中心的 3D 手与物体交互运动细化<br />
**Authors:** Yuze Hao, Jianrong Zhang, Tao Zhuo, Fuan Wen, Hehe Fan<br />
**Abstract:** <details><summary>原文: </summary>Hands are the main medium when people interact with the world. Generating proper 3D motion for hand-object interaction is vital for applications such as virtual reality and robotics. Although grasp tracking or object manipulation synthesis can produce coarse hand motion, this kind of motion is inevitably noisy and full of jitter. To address this problem, we propose a data-driven method for coarse motion refinement. First, we design a hand-centric representation to describe the dynamic spatial-temporal relation between hands and objects. Compared to the object-centric representation, our hand-centric representation is straightforward and does not require an ambiguous projection process that converts object-based prediction into hand motion. Second, to capture the dynamic clues of hand-object interaction, we propose a new architecture that models the spatial and temporal structure in a hierarchical manner. Extensive experiments demonstrate that our method outperforms previous methods by a noticeable margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>手是人们与世界互动的主要媒介。为手部物体交互生成适当的 3D 运动对于虚拟现实和机器人等应用至关重要。尽管抓取跟踪或对象操纵合成可以产生粗糙的手部运动，但这种运动不可避免地充满噪声且充满抖动。为了解决这个问题，我们提出了一种数据驱动的粗运动细化方法。首先，我们设计了一种以手为中心的表示来描述手和物体之间的动态时空关系。与以对象为中心的表示相比，我们以手为中心的表示很简单，不需要将基于对象的预测转换为手部运动的模糊投影过程。其次，为了捕捉手与物体交互的动态线索，我们提出了一种新的架构，以分层方式对空间和时间结构进行建模。大量的实验表明，我们的方法明显优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.15987v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **StableIdentity: Inserting Anybody into Anywhere at First Sight**<br />
**Title_cn:** StableIdentity：第一眼就把任何人插入到任何地方<br />
**Authors:** Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, Huchuan Lu<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in large pretrained text-to-image models have shown unprecedented capabilities for high-quality human-centric generation, however, customizing face identity is still an intractable problem. Existing methods cannot ensure stable identity preservation and flexible editability, even with several images for each subject during training. In this work, we propose StableIdentity, which allows identity-consistent recontextualization with just one face image. More specifically, we employ a face encoder with an identity prior to encode the input face, and then land the face representation into a space with an editable prior, which is constructed from celeb names. By incorporating identity prior and editability prior, the learned identity can be injected anywhere with various contexts. In addition, we design a masked two-phase diffusion loss to boost the pixel-level perception of the input face and maintain the diversity of generation. Extensive experiments demonstrate our method outperforms previous customization methods. In addition, the learned identity can be flexibly combined with the off-the-shelf modules such as ControlNet. Notably, to the best knowledge, we are the first to directly inject the identity learned from a single image into video/3D generation without finetuning. We believe that the proposed StableIdentity is an important step to unify image, video, and 3D customized generation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型预训练文本到图像模型的最新进展显示出前所未有的以人为中心的高质量生成能力，然而，定制面部身份仍然是一个棘手的问题。现有方法无法确保稳定的身份保存和灵活的可编辑性，即使在训练期间每个受试者有多个图像。在这项工作中，我们提出了 StableIdentity，它允许仅使用一张面部图像进行身份一致的重新上下文化。更具体地说，我们采用具有身份先验的面部编码器对输入面部进行编码，然后将面部表示放入具有可编辑先验的空间中，该先验是由名人姓名构造的。通过合并身份先验和可编辑性先验，学习到的身份可以注入到具有各种上下文的任何地方。此外，我们设计了一个掩码两相扩散损失来增强输入人脸的像素级感知并保持生成的多样性。大量的实验证明我们的方法优于以前的定制方法。此外，学习到的身份可以与ControlNet等现成模块灵活结合。值得注意的是，据了解，我们是第一个将从单个图像中学习到的身份直接注入到视频/3D 生成中而无需进行微调的人。我们相信，所提出的 StableIdentity 是统一图像、视频和 3D 定制生成模型的重要一步。</details>
**PDF:** <http://arxiv.org/pdf/2401.15975v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Routers in Vision Mixture of Experts: An Empirical Study**<br />
**Title_cn:** 专家视觉混合中的路由器：实证研究<br />
**Authors:** Tianlin Liu, Mathieu Blondel, Carlos Riquelme, Joan Puigcerver<br />
**Abstract:** <details><summary>原文: </summary>Mixture-of-Experts (MoE) models are a promising way to scale up model capacity without significantly increasing computational cost. A key component of MoEs is the router, which decides which subset of parameters (experts) process which feature embeddings (tokens). In this paper, we present a comprehensive study of routers in MoEs for computer vision tasks. We introduce a unified MoE formulation that subsumes different MoEs with two parametric routing tensors. This formulation covers both sparse MoE, which uses a binary or hard assignment between experts and tokens, and soft MoE, which uses a soft assignment between experts and weighted combinations of tokens. Routers for sparse MoEs can be further grouped into two variants: Token Choice, which matches experts to each token, and Expert Choice, which matches tokens to each expert. We conduct head-to-head experiments with 6 different routers, including existing routers from prior work and new ones we introduce. We show that (i) many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers generally outperform Token Choice routers, and (iii) soft MoEs generally outperform sparse MoEs with a fixed compute budget. These results provide new insights regarding the crucial role of routers in vision MoE models.</details>
**Abstract_cn:** <details><summary>译文: </summary>专家混合 (MoE) 模型是一种在不显着增加计算成本的情况下扩大模型容量的有前途的方法。 MoE 的一个关键组件是路由器，它决定哪个参数子集（专家）处理哪些特征嵌入（令牌）。在本文中，我们对 MoE 中用于计算机视觉任务的路由器进行了全面的研究。我们引入了一个统一的 MoE 公式，其中包含具有两个参数路由张量的不同 MoE。该公式涵盖了稀疏 MoE（在专家和令牌之间使用二进制或硬分配）和软 MoE（在专家和令牌的加权组合之间使用软分配）。稀疏 MoE 的路由器可以进一步分为两种变体：令牌选择（将专家与每个令牌匹配）和专家选择（将令牌与每个专家匹配）。我们使用 6 个不同的路由器进行了头对头的实验，包括之前工作中的现有路由器和我们引入的新路由器。我们表明，（i）许多最初为语言建模而开发的路由器可以适应在视觉任务中表现出色，（ii）在稀疏 MoE 中，专家选择路由器通常优于令牌选择路由器，以及（iii）软 MoE 通常优于稀疏 MoE固定的计算预算。这些结果提供了关于路由器在视觉 MoE 模型中的关键作用的新见解。</details>
**PDF:** <http://arxiv.org/pdf/2401.15969v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Motion-induced error reduction for high-speed dynamic digital fringe projection system**<br />
**Title_cn:** 高速动态数字条纹投影系统的运动引起的误差减少<br />
**Authors:** Sanghoon Jeon, Hyo-Geon Lee, Jae-Sung Lee, Bo-Min Kang, Byung-Wook Jeon, Jun Young Yoon, Jae-Sang Hyun<br />
**Abstract:** <details><summary>原文: </summary>In phase-shifting profilometry (PSP), any motion during the acquisition of fringe patterns can introduce errors because it assumes both the object and measurement system are stationary. Therefore, we propose a method to pixel-wise reduce the errors when the measurement system is in motion due to a motorized linear stage. The proposed method introduces motion-induced error reduction algorithm, which leverages the motor's encoder and pinhole model of the camera and projector. 3D shape measurement is possible with only three fringe patterns by applying geometric constraints of the digital fringe projection system. We address the mismatch problem due to the motion-induced camera pixel disparities and reduce phase-shift errors. These processes are easy to implement and require low computational cost. Experimental results demonstrate that the presented method effectively reduces the errors even in non-uniform motion.</details>
**Abstract_cn:** <details><summary>译文: </summary>在相移轮廓测量 (PSP) 中，采集条纹图案期间的任何运动都可能引入误差，因为它假设物体和测量系统都是静止的。因此，我们提出了一种在测量系统由于电动线性平台而运动时逐像素减少误差的方法。该方法引入了运动引起的误差减少算法，该算法利用电机的编码器以及相机和投影仪的针孔模型。通过应用数字条纹投影系统的几何约束，仅使用三个条纹图案即可进行 3D 形状测量。我们解决了由于运动引起的相机像素差异而导致的失配问题，并减少了相移误差。这些过程易于实现并且需要较低的计算成本。实验结果表明，即使在非匀速运动中，该方法也能有效地减少误差。</details>
**PDF:** <http://arxiv.org/pdf/2401.15938v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Vision-Informed Flow Image Super-Resolution with Quaternion Spatial Modeling and Dynamic Flow Convolution**<br />
**Title_cn:** 基于四元数空间建模和动态流卷积的视觉信息流图像超分辨率<br />
**Authors:** Qinglong Cao, Zhengqin Xu, Chao Ma, Xiaokang Yang, Yuntian Chen<br />
**Abstract:** <details><summary>原文: </summary>Flow image super-resolution (FISR) aims at recovering high-resolution turbulent velocity fields from low-resolution flow images. Existing FISR methods mainly process the flow images in natural image patterns, while the critical and distinct flow visual properties are rarely considered. This negligence would cause the significant domain gap between flow and natural images to severely hamper the accurate perception of flow turbulence, thereby undermining super-resolution performance. To tackle this dilemma, we comprehensively consider the flow visual properties, including the unique flow imaging principle and morphological information, and propose the first flow visual property-informed FISR algorithm. Particularly, different from natural images that are constructed by independent RGB channels in the light field, flow images build on the orthogonal UVW velocities in the flow field. To empower the FISR network with an awareness of the flow imaging principle, we propose quaternion spatial modeling to model this orthogonal spatial relationship for improved FISR. Moreover, due to viscosity and surface tension characteristics, fluids often exhibit a droplet-like morphology in flow images. Inspired by this morphological property, we design the dynamic flow convolution to effectively mine the morphological information to enhance FISR. Extensive experiments on the newly acquired flow image datasets demonstrate the state-of-the-art performance of our method. Code and data will be made available.</details>
**Abstract_cn:** <details><summary>译文: </summary>流图像超分辨率（FISR）旨在从低分辨率流图像中恢复高分辨率湍流速度场。现有的 FISR 方法主要处理自然图像模式中的流图像，而很少考虑关键和独特的流视觉属性。这种疏忽会导致流动和自然图像之间存在显着的域差距，严重妨碍流动湍流的准确感知，从而破坏超分辨率性能。为了解决这个困境，我们综合考虑流视觉特性，包括独特的流成像原理和形态信息，并提出了第一个基于流视觉特性的FISR算法。特别是，与光场中由独立的 RGB 通道构建的自然图像不同，流图像建立在流场中的正交 UVW 速度之上。为了使 FISR 网络能够了解流成像原理，我们提出了四元数空间建模来对这种正交空间关系进行建模，以改进 FISR。此外，由于粘度和表面张力特性，流体通常在流图像中表现出类似液滴的形态。受这种形态学特性的启发，我们设计了动态流卷积来有效挖掘形态学信息以增强 FISR。对新获取的流图像数据集进行的大量实验证明了我们方法的最先进的性能。将提供代码和数据。</details>
**PDF:** <http://arxiv.org/pdf/2401.15913v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **3DPFIX: Improving Remote Novices' 3D Printing Troubleshooting through Human-AI Collaboration**<br />
**Title_cn:** 3DPFIX：通过人机协作改善远程新手的 3D 打印故障排除<br />
**Authors:** Nahyun Kwon, Tong Sun, Yuyang Gao, Liang Zhao, Xu Wang, Jeeeun Kim, Sungsoo Ray Hong<br />
**Abstract:** <details><summary>原文: </summary>The widespread consumer-grade 3D printers and learning resources online enable novices to self-train in remote settings. While troubleshooting plays an essential part of 3D printing, the process remains challenging for many remote novices even with the help of well-developed online sources, such as online troubleshooting archives and online community help. We conducted a formative study with 76 active 3D printing users to learn how remote novices leverage online resources in troubleshooting and their challenges. We found that remote novices cannot fully utilize online resources. For example, the online archives statically provide general information, making it hard to search and relate their unique cases with existing descriptions. Online communities can potentially ease their struggles by providing more targeted suggestions, but a helper who can provide custom help is rather scarce, making it hard to obtain timely assistance. We propose 3DPFIX, an interactive 3D troubleshooting system powered by the pipeline to facilitate Human-AI Collaboration, designed to improve novices' 3D printing experiences and thus help them easily accumulate their domain knowledge. We built 3DPFIX that supports automated diagnosis and solution-seeking. 3DPFIX was built upon shared dialogues about failure cases from Q\&A discourses accumulated in online communities. We leverage social annotations (i.e., comments) to build an annotated failure image dataset for AI classifiers and extract a solution pool. Our summative study revealed that using 3DPFIX helped participants spend significantly less effort in diagnosing failures and finding a more accurate solution than relying on their common practice. We also found that 3DPFIX users learn about 3D printing domain-specific knowledge. We discuss the implications of leveraging community-driven data in developing future Human-AI Collaboration designs.</details>
**Abstract_cn:** <details><summary>译文: </summary>广泛使用的消费级 3D 打印机和在线学习资源使新手能够在远程环境中进行自我训练。虽然故障排除在 3D 打印中发挥着重要作用，但即使有完善的在线资源（例如在线故障排除档案和在线社区帮助）的帮助，该过程对于许多远程新手来说仍然具有挑战性。我们对 76 名活跃 3D 打印用户进行了一项形成性研究，以了解远程新手如何利用在线资源进行故障排除及其面临的挑战。我们发现远程新手无法充分利用在线资源。例如，在线档案静态地提供一般信息，使得很难搜索其独特案例并将其与现有描述联系起来。在线社区可以通过提供更有针对性的建议来缓解他们的困境，但能够提供定制帮助的帮助者相当稀缺，因此很难获得及时的帮助。我们提出了 3DPFIX，这是一种由管道提供支持的交互式 3D 故障排除系统，旨在促进人机协作，旨在改善新手的 3D 打印体验，从而帮助他们轻松积累领域知识。我们构建了支持自动诊断和解决方案寻求的 3DPFIX。 3DPFIX 是基于在线社区中积累的问答讨论中有关失败案例的共享对话而构建的。我们利用社交注释（即评论）为人工智能分类器构建带注释的故障图像数据集并提取解决方案池。我们的总结性研究表明，与依赖常规实践相比，使用 3DPFIX 帮助参与者在诊断故障和找到更准确的解决方案方面花费的精力显着减少。我们还发现 3DPFIX 用户学习了 3D 打印领域的特定知识。我们讨论了利用社区驱动的数据来开发未来的人机协作设计的影响。</details>
**PDF:** <http://arxiv.org/pdf/2401.15877v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **2L3: Lifting Imperfect Generated 2D Images into Accurate 3D**<br />
**Title_cn:** 2L3：将不完美的生成 2D 图像提升为精确的 3D<br />
**Authors:** Yizheng Chen, Rengan Xie, Qi Ye, Sen Yang, Zixuan Xie, Tianxiao Chen, Rong Li, Yuchi Huo<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing 3D objects from a single image is an intriguing but challenging problem. One promising solution is to utilize multi-view (MV) 3D reconstruction to fuse generated MV images into consistent 3D objects. However, the generated images usually suffer from inconsistent lighting, misaligned geometry, and sparse views, leading to poor reconstruction quality. To cope with these problems, we present a novel 3D reconstruction framework that leverages intrinsic decomposition guidance, transient-mono prior guidance, and view augmentation to cope with the three issues, respectively. Specifically, we first leverage to decouple the shading information from the generated images to reduce the impact of inconsistent lighting; then, we introduce mono prior with view-dependent transient encoding to enhance the reconstructed normal; and finally, we design a view augmentation fusion strategy that minimizes pixel-level loss in generated sparse views and semantic loss in augmented random views, resulting in view-consistent geometry and detailed textures. Our approach, therefore, enables the integration of a pre-trained MV image generator and a neural network-based volumetric signed distance function (SDF) representation for a single image to 3D object reconstruction. We evaluate our framework on various datasets and demonstrate its superior performance in both quantitative and qualitative assessments, signifying a significant advancement in 3D object reconstruction. Compared with the latest state-of-the-art method Syncdreamer~\cite{liu2023syncdreamer}, we reduce the Chamfer Distance error by about 36\% and improve PSNR by about 30\% .</details>
**Abstract_cn:** <details><summary>译文: </summary>从单个图像重建 3D 对象是一个有趣但具有挑战性的问题。一种有前景的解决方案是利用多视图 (MV) 3D 重建将生成的 MV 图像融合为一致的 3D 对象。然而，生成的图像通常会受到光照不一致、几何形状不对齐和视图稀疏的影响，导致重建质量较差。为了解决这些问题，我们提出了一种新颖的 3D 重建框架，该框架利用内在分解指导、瞬态单先验指导和视图增强来分别解决这三个问题。具体来说，我们首先利用从生成的图像中解耦阴影信息来减少不一致照明的影响；然后，我们引入单先验和与视图相关的瞬态编码，以增强重建的法线；最后，我们设计了一种视图增强融合策略，最大限度地减少生成的稀疏视图中的像素级损失和增强随机视图中的语义损失，从而产生视图一致的几何形状和详细纹理。因此，我们的方法能够集成预训练的 MV 图像生成器和基于神经网络的体积符号距离函数 (SDF) 表示，以实现单个图像到 3D 对象的重建。我们在各种数据集上评估我们的框架，并展示其在定量和定性评估方面的卓越性能，这标志着 3D 对象重建方面的重大进步。与最新的最先进方法Syncdreamer~\cite{liu2023syncdreamer}相比，我们将Chamfer Distance误差减少了约36%，PSNR提高了约30%。</details>
**PDF:** <http://arxiv.org/pdf/2401.15841v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Continual Learning with Pre-Trained Models: A Survey**<br />
**Title_cn:** 使用预先训练的模型进行持续学习：一项调查<br />
**Authors:** Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan<br />
**Abstract:** <details><summary>原文: </summary>Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT</details>
**Abstract_cn:** <details><summary>译文: </summary>如今，现实世界的应用程序经常面对流数据，这需要学习系统随着数据的演变吸收新的知识。持续学习（CL）旨在实现这一目标，同时克服学习新知识时对旧知识的灾难性遗忘。典型的 CL 方法从头开始构建模型，并随着传入数据而增长。然而，预训练模型 (PTM) 时代的到来引发了巨大的研究兴趣，特别是在利用 PTM 强大的表征能力方面。本文对基于 PTM 的 CL 的最新进展进行了全面的调查。我们将现有方法分为三个不同的组，对它们的相似点、差异以及各自的优缺点进行比较分析。此外，我们还提供了一项对比各种最先进方法的实证研究，以强调对比较公平性的担忧。重现这些评估的源代码位于：https://github.com/sun-hailong/LAMDA-PILOT</details>
**PDF:** <http://arxiv.org/pdf/2401.16386v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Class-aware Optimal Transport Approach with Higher-Order Moment Matching for Unsupervised Domain Adaptation**<br />
**Title_cn:** 一种具有高阶矩匹配的类感知最优传输方法，用于无监督域适应<br />
**Authors:** Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung Tran, Dinh Phung<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we introduce a novel approach called class-aware optimal transport (OT), which measures the OT distance between a distribution over the source class-conditional distributions and a mixture of source and target data distribution. Our class-aware OT leverages a cost function that determines the matching extent between a given data example and a source class-conditional distribution. By optimizing this cost function, we find the optimal matching between target examples and source class-conditional distributions, effectively addressing the data and label shifts that occur between the two domains. To handle the class-aware OT efficiently, we propose an amortization solution that employs deep neural networks to formulate the transportation probabilities and the cost function. Additionally, we propose minimizing class-aware Higher-order Moment Matching (HMM) to align the corresponding class regions on the source and target domains. The class-aware HMM component offers an economical computational approach for accurately evaluating the HMM distance between the two distributions. Extensive experiments on benchmark datasets demonstrate that our proposed method significantly outperforms existing state-of-the-art baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督域适应（UDA）旨在将知识从标记的源域转移到未标记的目标域。在本文中，我们介绍了一种称为类感知最优传输（OT）的新方法，该方法测量源类条件分布上的分布与源和目标数据分布的混合之间的 OT 距离。我们的类感知 OT 利用成本函数来确定给定数据示例和源类条件分布之间的匹配程度。通过优化这个成本函数，我们找到了目标示例和源类条件分布之间的最佳匹配，有效地解决了两个域之间发生的数据和标签移位问题。为了有效地处理类感知 OT，我们提出了一种摊销解决方案，采用深度神经网络来制定运输概率和成本函数。此外，我们建议最小化类感知高阶矩匹配（HMM），以对齐源域和目标域上的相应类区域。类感知 HMM 组件提供了一种经济的计算方法，用于准确评估两个分布之间的 HMM 距离。对基准数据集的大量实验表明，我们提出的方法显着优于现有的最先进的基线。</details>
**PDF:** <http://arxiv.org/pdf/2401.15952v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River Contraction**<br />
**Title_cn:** 亚马逊 2023 年干旱：Sentinel-1 揭示里奥内格罗河极端收缩<br />
**Authors:** Fabien H Wagner, Samuel Favrichon, Ricardo Dalagnol, Mayumi CM Hirye, Adugna Mullissa, Sassan Saatchi<br />
**Abstract:** <details><summary>原文: </summary>The Amazon, the world's largest rainforest, faces a severe historic drought. The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023. Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. The accuracy of the water surface model was high with an F1-score of 0.93. The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction. The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686). The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds. Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$). Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions.</details>
**Abstract_cn:** <details><summary>译文: </summary>世界上最大的雨林亚马逊雨林正面临历史性的严重干旱。亚马逊河主要支流之一的内格罗河在 2023 年 10 月达到一个世纪以来的最低水位。这里，我们使用 U-net 深度学习模型绘制了 2022 年每 12 天的内格罗河流域水面图2023 年使用 10 m 空间分辨率的 Sentinel-1 卫星雷达图像。水面模型精度较高，F1分数为0.93。 12 天的水面马赛克时间序列是根据 Sentinel-1 预测生成的。水面掩模与联合研究中心的全球地表水 (GSW) 产品（F1 分数：0.708）和巴西 Mapbiomas 水计划（F1 分数：0.686）表现出相对一致的一致性。地图的主要错误是被淹没的林地、被淹没的灌木丛和云层的遗漏错误。里奥内格罗水面于 2023 年 11 月 25 日左右达到最低水平，并减少至 2022-2023 年期间观测到的最大水面（14,036.3 km$^2$）的 68.1%（9,559.9 km$^2$）。合成孔径雷达（SAR）数据与深度学习技术相结合，可以显着改善热带地区水面的近实时测绘。</details>
**PDF:** <http://arxiv.org/pdf/2401.16393v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization**<br />
**Title_cn:** 纯化对抗训练（AToP）：提高鲁棒性和泛化性<br />
**Authors:** Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao<br />
**Abstract:** <details><summary>原文: </summary>The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>众所周知，深度神经网络很容易受到精心设计的对抗性攻击。基于对抗性训练（AT）的最成功的防御技术可以针对特定攻击实现最佳鲁棒性，但不能很好地泛化到未见过的攻击。另一种基于对抗性净化（AP）的有效防御技术可以增强泛化能力，但无法实现最佳的鲁棒性。同时，这两种方法都有一个共同的限制，即标准精度下降。为了缓解这些问题，我们提出了一种称为净化对抗训练（AToP）的新颖框架，它包括两个组成部分：通过随机变换（RT）进行的扰动破坏和通过对抗性损失进行的净化器模型微调（FT）。 RT 对于避免对已知攻击的过度学习至关重要，从而导致对未见过的攻击的鲁棒性泛化，而 FT 对于提高鲁棒性至关重要。为了以高效且可扩展的方式评估我们的方法，我们在 CIFAR-10、CIFAR-100 和 ImageNette 上进行了广泛的实验，以证明我们的方法实现了最先进的结果，并表现出针对未见过的攻击的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.16352v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Cross-Modal Coordination Across a Diverse Set of Input Modalities**<br />
**Title_cn:** 跨多种输入模式的跨模式协调<br />
**Authors:** Jorge Sánchez, Rodrigo Laguna<br />
**Abstract:** <details><summary>原文: </summary>Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>跨模态检索是通过使用不同模态的查询来检索给定模态的样本的任务。由于实际应用范围广泛，问题主要集中在视觉和语言案例上，例如文本到图像检索，像 CLIP 这样的模型已被证明可以有效解决此类任务。学习这种协调表示的主要方法包括将它们投影到一个公共空间上，在该空间中，匹配的视图保持靠近，而来自非匹配对的视图彼此远离。尽管这种跨模式协调也已应用于其他成对组合，但将其扩展到任意数量的不同模式是文献中尚未充分探讨的问题。在本文中，我们提出了两种不同的方法来解决该问题。第一个基于将 CLIP 对比目标扩展到任意数量的输入模态，而第二个则偏离对比公式，通过将跨模态相似性回归到反映两个简单直观约束的目标来解决协调问题跨模态检索任务。我们在两个不同的数据集、不同的输入模式组合上进行了实验，结果表明该方法不仅简单有效，而且还允许以新颖的方式解决检索问题。除了捕获更多样化的成对交互之外，我们还表明，我们可以通过组合两种或多种此类模式的嵌入，使用学习到的表示来提高检索性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.16347v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Defining and Extracting generalizable interaction primitives from DNNs**<br />
**Title_cn:** 从 DNN 中定义和提取可推广的交互原语<br />
**Authors:** Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang<br />
**Abstract:** <details><summary>原文: </summary>Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.</details>
**Abstract_cn:** <details><summary>译文: </summary>将深度神经网络 (DNN) 编码的知识忠实地总结为一些符号原始模式而不丢失太多信息，这是可解释人工智能的核心挑战。为此，任等人。 (2023c) 导出了一系列定理来证明 DNN 的推理分数可以解释为输入变量之间的一小组相互作用。然而，由于缺乏泛化能力，仍然很难将此类交互视为由 DNN 编码的忠实原始模式。因此，考虑到针对同一任务训练的不同 DNN，我们开发了一种新方法来提取这些 DNN 共享的交互。实验表明，提取的交互可以更好地反映不同 DNN 共享的共同知识。</details>
**PDF:** <http://arxiv.org/pdf/2401.16318v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **High Resolution Image Quality Database**<br />
**Title_cn:** 高分辨率图像质量数据库<br />
**Authors:** Huang Huang, Qiang Wan, Jari Korhonen<br />
**Abstract:** <details><summary>原文: </summary>With technology for digital photography and high resolution displays rapidly evolving and gaining popularity, there is a growing demand for blind image quality assessment (BIQA) models for high resolution images. Unfortunately, the publicly available large scale image quality databases used for training BIQA models contain mostly low or general resolution images. Since image resizing affects image quality, we assume that the accuracy of BIQA models trained on low resolution images would not be optimal for high resolution images. Therefore, we created a new high resolution image quality database (HRIQ), consisting of 1120 images with resolution of 2880x2160 pixels. We conducted a subjective study to collect the subjective quality ratings for HRIQ in a controlled laboratory setting, resulting in accurate MOS at high resolution. To demonstrate the importance of a high resolution image quality database for training BIQA models to predict mean opinion scores (MOS) of high resolution images accurately, we trained and tested several traditional and deep learning based BIQA methods on different resolution versions of our database. The database is publicly available in https://github.com/jarikorhonen/hriq.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着数字摄影和高分辨率显示技术的快速发展和普及，对高分辨率图像的盲图像质量评估 (BIQA) 模型的需求不断增长。不幸的是，用于训练 BIQA 模型的公开可用的大规模图像质量数据库大多包含低分辨率或一般分辨率的图像。由于图像大小调整会影响图像质量，因此我们假设在低分辨率图像上训练的 BIQA 模型的准确性对于高分辨率图像来说并不是最佳的。因此，我们创建了一个新的高分辨率图像质量数据库（HRIQ），由1120张分辨率为2880x2160像素的图像组成。我们进行了一项主观研究，在受控实验室环境中收集 HRIQ 的主观质量评级，从而获得高分辨率的准确 MOS。为了证明高分辨率图像质量数据库对于训练 BIQA 模型以准确预测高分辨率图像的平均意见得分 (MOS) 的重要性，我们在数据库的不同分辨率版本上训练和测试了几种传统和基于深度学习的 BIQA 方法。该数据库可在 https://github.com/jarikorhonen/hriq 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16087v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Data-Driven Filter Design in FBP: Transforming CT Reconstruction with Trainable Fourier Series**<br />
**Title_cn:** FBP 中的数据驱动滤波器设计：利用可训练的傅里叶级数改变 CT 重建<br />
**Authors:** Yipeng Sun, Linda-Sophie Schneider, Fuxin Fan, Mareike Thies, Mingxuan Gu, Siyuan Mei, Yuzhong Zhou, Siming Bayer, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>In this study, we introduce a Fourier series-based trainable filter for computed tomography (CT) reconstruction within the filtered backprojection (FBP) framework. This method overcomes the limitation in noise reduction, inherent in conventional FBP methods, by optimizing Fourier series coefficients to construct the filter. This method enables robust performance across different resolution scales and maintains computational efficiency with minimal increment for the trainable parameters compared to other deep learning frameworks. Additionally, we propose Gaussian edge-enhanced (GEE) loss function that prioritizes the $L_1$ norm of high-frequency magnitudes, effectively countering the blurring problems prevalent in mean squared error (MSE) approaches. The model's foundation in the FBP algorithm ensures excellent interpretability, as it relies on a data-driven filter with all other parameters derived through rigorous mathematical procedures. Designed as a plug-and-play solution, our Fourier series-based filter can be easily integrated into existing CT reconstruction models, making it a versatile tool for a wide range of practical applications. Our research presents a robust and scalable method that expands the utility of FBP in both medical and scientific imaging.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本研究中，我们引入了一种基于傅立叶级数的可训练滤波器，用于滤波反投影（FBP）框架内的计算机断层扫描（CT）重建。该方法通过优化傅里叶级数系数​​来构造滤波器，克服了传统FBP方法固有的降噪限制。与其他深度学习框架相比，该方法能够在不同分辨率尺度上实现稳健的性能，并以最小的可训练参数增量保持计算效率。此外，我们提出了高斯边缘增强（GEE）损失函数，该函数优先考虑高频幅度的 $L_1$ 范数，有效解决均方误差（MSE）方法中普遍存在的模糊问题。该模型以 FBP 算法为基础，确保了出色的可解释性，因为它依赖于数据驱动的过滤器，所有其他参数均通过严格的数学程序得出。我们的基于傅立叶级数的滤波器设计为即插即用解决方案，可以轻松集成到现有的 CT 重建模型中，使其成为适用于各种实际应用的多功能工具。我们的研究提出了一种稳健且可扩展的方法，可扩展 FBP 在医学和科学成像中的实用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.16039v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Bridging the Domain Gap: A Simple Domain Matching Method for Reference-based Image Super-Resolution in Remote Sensing**<br />
**Title_cn:** 弥合域差距：遥感中基于参考的图像超分辨率的简单域匹配方法<br />
**Authors:** Jeongho Min, Yejun Lee, Dongyoung Kim, Jaejun Yoo<br />
**Abstract:** <details><summary>原文: </summary>Recently, reference-based image super-resolution (RefSR) has shown excellent performance in image super-resolution (SR) tasks. The main idea of RefSR is to utilize additional information from the reference (Ref) image to recover the high-frequency components in low-resolution (LR) images. By transferring relevant textures through feature matching, RefSR models outperform existing single image super-resolution (SISR) models. However, their performance significantly declines when a domain gap between Ref and LR images exists, which often occurs in real-world scenarios, such as satellite imaging. In this letter, we introduce a Domain Matching (DM) module that can be seamlessly integrated with existing RefSR models to enhance their performance in a plug-and-play manner. To the best of our knowledge, we are the first to explore Domain Matching-based RefSR in remote sensing image processing. Our analysis reveals that their domain gaps often occur in different satellites, and our model effectively addresses these challenges, whereas existing models struggle. Our experiments demonstrate that the proposed DM module improves SR performance both qualitatively and quantitatively for remote sensing super-resolution tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于参考的图像超分辨率（RefSR）在图像超分辨率（SR）任务中表现出了优异的性能。 RefSR 的主要思想是利用参考 (Ref) 图像中的附加信息来恢复低分辨率 (LR) 图像中的高频分量。通过特征匹配传输相关​​纹理，RefSR 模型的性能优于现有的单图像超分辨率 (SISR) 模型。然而，当参考图像和 LR 图像之间存在域差距时，它们的性能会显着下降，这种情况经常发生在卫星成像等现实场景中。在这封信中，我们介绍了一个域匹配（DM）模块，该模块可以与现有的 RefSR 模型无缝集成，以即插即用的方式增强其性能。据我们所知，我们是第一个在遥感图像处理中探索基于域匹配的 RefSR 的人。我们的分析表明，它们的域差距经常出现在不同的卫星中，我们的模型有效地解决了这些挑战，而现有模型则陷入困境。我们的实验表明，所提出的 DM 模块在定性和定量上提高了遥感超分辨率任务的 SR 性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.15944v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation**<br />
**Title_cn:** 使用隐式连续表示任意尺度缩小潮汐流数据<br />
**Authors:** Dongheon Lee, Seungmyong Jeong, Youngmin Ro<br />
**Abstract:** <details><summary>原文: </summary>Numerical models have long been used to understand geoscientific phenomena, including tidal currents, crucial for renewable energy production and coastal engineering. However, their computational cost hinders generating data of varying resolutions. As an alternative, deep learning-based downscaling methods have gained traction due to their faster inference speeds. But most of them are limited to only inference fixed scale and overlook important characteristics of target geoscientific data. In this paper, we propose a novel downscaling framework for tidal current data, addressing its unique characteristics, which are dissimilar to images: heterogeneity and local dependency. Moreover, our framework can generate any arbitrary-scale output utilizing a continuous representation model. Our proposed framework demonstrates significantly improved flow velocity predictions by 93.21% (MSE) and 63.85% (MAE) compared to the Baseline model while achieving a remarkable 33.2% reduction in FLOPs.</details>
**Abstract_cn:** <details><summary>译文: </summary>数值模型长期以来被用来理解地球科学现象，包括对可再生能源生产和沿海工程至关重要的潮汐流。然而，它们的计算成本阻碍了生成不同分辨率的数据。作为替代方案，基于深度学习的缩减方法因其更快的推理速度而受到关注。但大多数仅限于推断固定尺度，忽视了目标地球科学数据的重要特征。在本文中，我们提出了一种新颖的潮流数据降尺度框架，解决了其与图像不同的独特特征：异质性和局部依赖性。此外，我们的框架可以利用连续表示模型生成任何任意规模的输出。我们提出的框架表明，与基线模型相比，流速预测显着提高了 93.21% (MSE) 和 63.85% (MAE)，同时 FLOP 显着降低了 33.2%。</details>
**PDF:** <http://arxiv.org/pdf/2401.15893v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability**<br />
**Title_cn:** TransTroj：通过嵌入不可区分性将后门攻击转移到预训练模型<br />
**Authors:** Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Pre-trained models (PTMs) are extensively utilized in various downstream tasks. Adopting untrusted PTMs may suffer from backdoor attacks, where the adversary can compromise the downstream models by injecting backdoors into the PTM. However, existing backdoor attacks to PTMs can only achieve partially task-agnostic and the embedded backdoors are easily erased during the fine-tuning process. In this paper, we propose a novel transferable backdoor attack, TransTroj, to simultaneously meet functionality-preserving, durable, and task-agnostic. In particular, we first formalize transferable backdoor attacks as the indistinguishability problem between poisoned and clean samples in the embedding space. We decompose the embedding indistinguishability into pre- and post-indistinguishability, representing the similarity of the poisoned and reference embeddings before and after the attack. Then, we propose a two-stage optimization that separately optimizes triggers and victim PTMs to achieve embedding indistinguishability. We evaluate TransTroj on four PTMs and six downstream tasks. Experimental results show that TransTroj significantly outperforms SOTA task-agnostic backdoor attacks (18%$\sim$99%, 68% on average) and exhibits superior performance under various system settings. The code is available at https://github.com/haowang-cqu/TransTroj .</details>
**Abstract_cn:** <details><summary>译文: </summary>预训练模型（PTM）广泛应用于各种下游任务。采用不受信任的 PTM 可能会遭受后门攻击，攻击者可以通过向 PTM 注入后门来破坏下游模型。然而，现有的针对 PTM 的后门攻击只能实现部分任务无关，并且嵌入的后门在微调过程中很容易被删除。在本文中，我们提出了一种新颖的可转移后门攻击 TransTroj，以同时满足功能保留、持久和任务无关的要求。特别是，我们首先将可转移后门攻击形式化为嵌入空间中有毒样本和干净样本之间的不可区分问题。我们将嵌入不可区分性分解为前后不可区分性，表示攻击前后中毒嵌入和参考嵌入的相似性。然后，我们提出了一种两阶段优化，分别优化触发器和受害者 PTM，以实现嵌入的不可区分性。我们在四个 PTM 和六个下游任务上评估 TransTroj。实验结果表明，TransTroj 显着优于 SOTA 任务无关后门攻击（18%$\sim$99%，平均 68%），并在各种系统设置下表现出优越的性能。代码可在 https://github.com/haowang-cqu/TransTroj 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.15883v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Spatial Decomposition and Temporal Fusion based Inter Prediction for Learned Video Compression**<br />
**Title_cn:** 基于空间分解和时间融合的学习视频压缩帧间预测<br />
**Authors:** Xihua Sheng, Li Li, Dong Liu, Houqiang Li<br />
**Abstract:** <details><summary>原文: </summary>Video compression performance is closely related to the accuracy of inter prediction. It tends to be difficult to obtain accurate inter prediction for the local video regions with inconsistent motion and occlusion. Traditional video coding standards propose various technologies to handle motion inconsistency and occlusion, such as recursive partitions, geometric partitions, and long-term references. However, existing learned video compression schemes focus on obtaining an overall minimized prediction error averaged over all regions while ignoring the motion inconsistency and occlusion in local regions. In this paper, we propose a spatial decomposition and temporal fusion based inter prediction for learned video compression. To handle motion inconsistency, we propose to decompose the video into structure and detail (SDD) components first. Then we perform SDD-based motion estimation and SDD-based temporal context mining for the structure and detail components to generate short-term temporal contexts. To handle occlusion, we propose to propagate long-term temporal contexts by recurrently accumulating the temporal information of each historical reference feature and fuse them with short-term temporal contexts. With the SDD-based motion model and long short-term temporal contexts fusion, our proposed learned video codec can obtain more accurate inter prediction. Comprehensive experimental results demonstrate that our codec outperforms the reference software of H.266/VVC on all common test datasets for both PSNR and MS-SSIM.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频压缩性能与帧间预测的准确性密切相关。对于运动和遮挡不一致的局部视频区域往往很难获得准确的帧间预测。传统视频编码标准提出了各种技术来处理运动不一致和遮挡，例如递归分区、几何分区和长期参考。然而，现有的学习视频压缩方案侧重于获得所有区域平均的总体最小化预测误差，而忽略了局部区域的运动不一致和遮挡。在本文中，我们提出了一种基于空间分解和时间融合的学习视频压缩帧间预测。为了处理运动不一致，我们建议首先将视频分解为结构和细节（SDD）组件。然后，我们对结构和细节组件执行基于 SDD 的运动估计和基于 SDD 的时间上下文挖掘，以生成短期时间上下文。为了处理遮挡，我们建议通过反复累积每个历史参考特征的时间信息并将其与短期时间上下文融合来传播长期时间上下文。通过基于 SDD 的运动模型和长短期时间上下文融合，我们提出的学习视频编解码器可以获得更准确的帧间预测。综合实验结果表明，我们的编解码器在 PSNR 和 MS-SSIM 的所有常见测试数据集上均优于 H.266/VVC 参考软件。</details>
**PDF:** <http://arxiv.org/pdf/2401.15864v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing**<br />
**Title_cn:** 跨尺度 MAE：遥感多尺度开发的故事<br />
**Authors:** Maofeng Tang, Andrei Cozma, Konstantinos Georgiou, Hairong Qi<br />
**Abstract:** <details><summary>原文: </summary>Remote sensing images present unique challenges to image analysis due to the extensive geographic coverage, hardware limitations, and misaligned multi-scale images. This paper revisits the classical multi-scale representation learning problem but under the general framework of self-supervised learning for remote sensing image understanding. We present Cross-Scale MAE, a self-supervised model built upon the Masked Auto-Encoder (MAE).During pre-training, Cross-Scale MAE employs scale augmentation techniques and enforces cross-scale consistency constraints through both contrastive and generative losses to ensure consistent and meaningful representations well-suited for a wide range of downstream tasks. Further, our implementation leverages the xFormers library to accelerate network pre-training on a single GPU while maintaining the quality of learned representations. Experimental evaluations demonstrate that Cross-Scale MAE exhibits superior performance compared to standard MAE and other state-of-the-art remote sensing MAE methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于广泛的地理覆盖范围、硬件限制和未对齐的多尺度图像，遥感图像给图像分析带来了独特的挑战。本文在遥感图像理解的自监督学习的总体框架下重新审视了经典的多尺度表示学习问题。我们提出了 Cross-Scale MAE，这是一种基于 Masked Auto-Encoder (MAE) 构建的自监督模型。在预训练期间，Cross-Scale MAE 采用尺度增强技术，并通过对比损失和生成损失来强制跨尺度一致性约束确保一致且有意义的表示非常适合各种下游任务。此外，我们的实现利用 xFormers 库来加速单个 GPU 上的网络预训练，同时保持学习表示的质量。实验评估表明，与标准 MAE 和其他最先进的遥感 MAE 方法相比，跨尺度 MAE 表现出卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.15855v1><br />
**Code:** null<br />

