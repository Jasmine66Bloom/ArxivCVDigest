## [UPDATED!] **2024-01-30** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation**<br />
**Title_cn:** 您只需一步：通过刻度蒸馏实现快速超分辨率和稳定扩散<br />
**Authors:** Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 YONOS-SR，这是一种基于稳定扩散的新型图像超分辨率方法，仅使用单个 DDIM 步骤即可产生最先进的结果。我们提出了一种新颖的规模蒸馏方法来训练我们的 SR 模型。我们不是直接在感兴趣的比例因子上训练我们的 SR 模型，而是首先在较小的放大比例上训练教师模型，从而使教师的 SR 问题变得更简单。然后，我们在训练期间使用教师的预测作为目标，训练更高放大倍数的学生模型。迭代重复此过程，直到达到最终模型的目标比例因子。我们的尺度蒸馏背后的基本原理是，教师通过以下方式帮助学生扩散模型训练：i）提供适合当前噪声水平的目标，而不是使用来自所有噪声水平的地面实况数据的相同目标；ii）提供准确的目标因为老师有一个更简单的任务要解决。我们凭经验表明，蒸馏模型显着优于直接针对高尺度训练的模型，特别是在推理过程中只需要很少的步骤。拥有一个只需要一步的强大扩散模型，我们就可以冻结 U-Net 并在其之上微调解码器。我们证明，空间蒸馏 U-Net 和微调解码器的结合优于仅一步需要 200 个步骤的最先进方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.17258v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ContactGen: Contact-Guided Interactive 3D Human Generation for Partners**<br />
**Title_cn:** ContactGen：为合作伙伴提供接触引导的交互式 3D 人类生成<br />
**Authors:** Dongjun Gu, Jaehyeok Shim, Jaehoon Jang, Changwoo Kang, Kyungdon Joo<br />
**Abstract:** <details><summary>原文: </summary>Among various interactions between humans, such as eye contact and gestures, physical interactions by contact can act as an essential moment in understanding human behaviors. Inspired by this fact, given a 3D partner human with the desired interaction label, we introduce a new task of 3D human generation in terms of physical contact. Unlike previous works of interacting with static objects or scenes, a given partner human can have diverse poses and different contact regions according to the type of interaction. To handle this challenge, we propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework. Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label. Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model. We demonstrate ContactGen on the CHI3D dataset, where our method generates physically plausible and diverse poses compared to comparison methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>在人类之间的各种互动中，例如眼神交流和手势，通过接触进行的身体互动可以成为理解人类行为的重要时刻。受这一事实的启发，给定一个具有所需交互标签的 3D 人类伙伴，我们在物理接触方面引入了 3D 人类生成的新任务。与之前与静态物体或场景交互的作品不同，给定的人类伙伴可以根据交互类型具有不同的姿势和不同的接触区域。为了应对这一挑战，我们提出了一种基于引导扩散框架为给定伙伴人类生成交互式 3D 人类的新方法。具体来说，我们新提出了一个接触预测模块，该模块根据交互标签自适应地估计两个输入人类之间的潜在接触区域。使用估计的潜在接触区域作为补充指导，我们动态地强制 ContactGen 在引导扩散模型中为给定的伙伴人类生成交互式 3D 人类。我们在 CHI3D 数据集上演示了 ContactGen，与比较方法相比，我们的方法生成了物理上合理且多样化的姿势。</details>
**PDF:** <http://arxiv.org/pdf/2401.17212v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI**<br />
**Title_cn:** 3D-PLI 中神经纤维分布模式的自监督表示学习<br />
**Authors:** Alexander Oberstrass, Sascha E. A. Muenzing, Meiqi Niu, Nicola Palomero-Gallagher, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid<br />
**Abstract:** <details><summary>原文: </summary>A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution. Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping. However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available. To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning. We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps. The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain. We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing. We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture and performing data mining for interactively selected templates of specific components of fiber architecture such as U-fibers.</details>
**Abstract_cn:** <details><summary>译文: </summary>除其他因素外，对人脑组织原理的全面理解还需要对神经纤维结构进行良好量化的描述符。三维偏振光成像 (3D-PLI) 是一种显微成像技术，能够以高分辨率洞察有髓神经纤维的细粒度组织。表征 3D-PLI 中观察到的光纤架构的描述符将支持下游分析任务，例如多模态相关性研究、聚类和映射。然而，3D-PLI 中独立于观察者的光纤架构表征的最佳实践尚不可用。为此，我们建议应用完全数据驱动的方法，使用自监督表示学习来表征 3D-PLI 图像中的神经纤维结构。我们引入了 3D 上下文对比学习 (CL-3D) 目标，该目标利用 3D 重建体积的组织学大脑部分的纹理示例的空间邻域来采样正对以进行对比学习。我们将此采样策略与专门设计的图像增强相结合，以获得对 3D-PLI 参数图典型变化的鲁棒性。该方法在黑长尾猴大脑的 3D 重建枕叶中进行了演示。我们表明，提取的特征对神经纤维的不同配置高度敏感，但对组织学处理引起的连续脑切片之间的变化具有鲁棒性。我们展示了它们在检索同质光纤架构集群以及对光纤架构特定组件（例如 U 光纤）​​的交互式选择模板进行数据挖掘方面的实际适用性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17207v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **An Open Software Suite for Event-Based Video**<br />
**Title_cn:** 用于基于事件的视频的开放软件套件<br />
**Authors:** Andrew C. Freeman<br />
**Abstract:** <details><summary>原文: </summary>While traditional video representations are organized around discrete image frames, event-based video is a new paradigm that forgoes image frames altogether. Rather, pixel samples are temporally asynchronous and independent of one another. Until now, researchers have lacked a cohesive software framework for exploring the representation, compression, and applications of event-based video. I present the AD$\Delta$ER software suite to fill this gap. This framework includes utilities for transcoding framed and multimodal event-based video sources to a common representation, rate control mechanisms, lossy compression, application support, and an interactive GUI for transcoding and playback. In this paper, I describe these various software components and their usage.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然传统的视频表示是围绕离散图像帧组织的，但基于事件的视频是完全放弃图像帧的新范例。相反，像素样本在时间上是异步的并且彼此独立。到目前为止，研究人员还缺乏一个有凝聚力的软件框架来探索基于事件的视频的表示、压缩和应用。我推出 AD$\Delta$ER 软件套件来填补这一空白。该框架包括用于将帧视频源和基于事件的多模式视频源转码为通用表示的实用程序、速率控制机制、有损压缩、应用程序支持以及用于转码和播放的交互式 GUI。在本文中，我描述了这些不同的软件组件及其用法。</details>
**PDF:** <http://arxiv.org/pdf/2401.17151v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Repositioning the Subject within Image**<br />
**Title_cn:** 重新定位图像中的主体<br />
**Authors:** Yikai Wang, Chenjie Cao, Qiaole Dong, Yifan Li, Yanwei Fu<br />
**Abstract:** <details><summary>原文: </summary>Current image manipulation primarily centers on static manipulation, such as replacing specific regions within an image or altering its overall style. In this paper, we introduce an innovative dynamic manipulation task, subject repositioning. This task involves relocating a user-specified subject to a desired position while preserving the image's fidelity. Our research reveals that the fundamental sub-tasks of subject repositioning, which include filling the void left by the repositioned subject, reconstructing obscured portions of the subject and blending the subject to be consistent with surrounding areas, can be effectively reformulated as a unified, prompt-guided inpainting task. Consequently, we can employ a single diffusion generative model to address these sub-tasks using various task prompts learned through our proposed task inversion technique. Additionally, we integrate pre-processing and post-processing techniques to further enhance the quality of subject repositioning. These elements together form our SEgment-gEnerate-and-bLEnd (SEELE) framework. To assess SEELE's effectiveness in subject repositioning, we assemble a real-world subject repositioning dataset called ReS. Our results on ReS demonstrate the quality of repositioned image generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的图像处理主要集中在静态处理上，例如替换图像中的特定区域或改变其整体风格。在本文中，我们介绍了一种创新的动态操纵任务，即主题重新定位。此任务涉及将用户指定的主题重新定位到所需位置，同时保持图像的保真度。我们的研究表明，主题重新定位的基本子任务，包括填充重新定位的主题留下的空白、重建主题的模糊部分以及将主题融合到与周围区域一致，可以有效地重新表述为统一的、即时的-指导修复任务。因此，我们可以采用单个扩散生成模型来使用通过我们提出的任务反转技术学习的各种任务提示来解决这些子任务。此外，我们集成了预处理和后处理技术，以进一步提高主体重新定位的质量。这些元素共同构成了我们的SEgment-gEnerate-and-bLEnd (SEELE) 框架。为了评估 SEELE 在受试者重新定位方面的有效性，我们组装了一个名为 ReS 的真实世界受试者重新定位数据集。我们在 ReS 上的结果证明了重新定位图像生成的质量。</details>
**PDF:** <http://arxiv.org/pdf/2401.16861v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion**<br />
**Title_cn:** BoostDream：通过多视图扩散高效细化高质量文本到 3D 生成<br />
**Authors:** Yonghao Yu, Shunan Zhu, Huai Qin, Haorui Li<br />
**Abstract:** <details><summary>原文: </summary>Witnessing the evolution of text-to-image diffusion models, significant strides have been made in text-to-3D generation. Currently, two primary paradigms dominate the field of text-to-3D: the feed-forward generation solutions, capable of swiftly producing 3D assets but often yielding coarse results, and the Score Distillation Sampling (SDS) based solutions, known for generating high-fidelity 3D assets albeit at a slower pace. The synergistic integration of these methods holds substantial promise for advancing 3D generation techniques. In this paper, we present BoostDream, a highly efficient plug-and-play 3D refining method designed to transform coarse 3D assets into high-quality. The BoostDream framework comprises three distinct processes: (1) We introduce 3D model distillation that fits differentiable representations from the 3D assets obtained through feed-forward generation. (2) A novel multi-view SDS loss is designed, which utilizes a multi-view aware 2D diffusion model to refine the 3D assets. (3) We propose to use prompt and multi-view consistent normal maps as guidance in refinement.Our extensive experiment is conducted on different differentiable 3D representations, revealing that BoostDream excels in generating high-quality 3D assets rapidly, overcoming the Janus problem compared to conventional SDS-based methods. This breakthrough signifies a substantial advancement in both the efficiency and quality of 3D generation processes.</details>
**Abstract_cn:** <details><summary>译文: </summary>见证文本到图像扩散模型的演变，文本到 3D 生成已经取得了重大进展。目前，两种主要范式在文本转 3D 领域占据主导地位：前馈生成解决方案，能够快速生成 3D 资产，但通常会产生粗糙的结果，以及基于分数蒸馏采样 (SDS) 的解决方案，以生成高效率而闻名。保真 3D 资产尽管速度较慢。这些方法的协同集成为推进 3D 生成技术带来了巨大的希望。在本文中，我们提出了 BoostDream，这是一种高效的即插即用 3D 精炼方法，旨在将粗略的 3D 资产转化为高质量的资产。 BoostDream 框架包含三个不同的过程：(1) 我们引入了 3D 模型蒸馏，该模型蒸馏适合通过前馈生成获得的 3D 资产的可微表示。 (2) 设计了一种新颖的多视图 SDS 损失，它利用多视图感知 2D 扩散模型来细化 3D 资产。 (3) 我们建议使用即时和多视图一致的法线贴图作为细化的指导。我们对不同的可微 3D 表示进行了广泛的实验，表明 BoostDream 擅长快速生成高质量的 3D 资产，与传统的 BoostDream 相比克服了 Janus 问题传统的基于 SDS 的方法。这一突破标志着 3D 生成过程的效率和质量的显着进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.16764v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Pick-and-Draw: Training-free Semantic Guidance for Text-to-Image Personalization**<br />
**Title_cn:** Pick-and-Draw：用于文本到图像个性化的免训练语义指导<br />
**Authors:** Henglei Lv, Jiayu Xiao, Liang Li, Qingming Huang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion-based text-to-image personalization have achieved great success in generating subjects specified by users among various contexts. Even though, existing finetuning-based methods still suffer from model overfitting, which greatly harms the generative diversity, especially when given subject images are few. To this end, we propose Pick-and-Draw, a training-free semantic guidance approach to boost identity consistency and generative diversity for personalization methods. Our approach consists of two components: appearance picking guidance and layout drawing guidance. As for the former, we construct an appearance palette with visual features from the reference image, where we pick local patterns for generating the specified subject with consistent identity. As for layout drawing, we outline the subject's contour by referring to a generative template from the vanilla diffusion model, and inherit the strong image prior to synthesize diverse contexts according to different text conditions. The proposed approach can be applied to any personalized diffusion models and requires as few as a single reference image. Qualitative and quantitative experiments show that Pick-and-Draw consistently improves identity consistency and generative diversity, pushing the trade-off between subject fidelity and image-text fidelity to a new Pareto frontier.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散的文本到图像个性化在生成用户在各种上下文中指定的主题方面取得了巨大成功。尽管如此，现有的基于微调的方法仍然存在模型过度拟合的问题，这极大地损害了生成多样性，特别是当给定的主题图像很少时。为此，我们提出了 Pick-and-Draw，一种免训练的语义指导方法，以提高个性化方法的身份一致性和生成多样性。我们的方法由两个部分组成：外观挑选指导和布局图指导。对于前者，我们构建了一个具有参考图像视觉特征的外观调色板，其中我们选择局部模式来生成具有一致身份的指定主题。在布局绘制方面，我们参考香草扩散模型的生成模板勾勒出主体的轮廓，并继承之前的强图像，根据不同的文本条件合成不同的上下文。所提出的方法可以应用于任何个性化扩散模型，并且只需要单个参考图像。定性和定量实验表明，Pick-and-Draw 持续提高了身份一致性和生成多样性，将主题保真度和图像文本保真度之间的权衡推向了新的帕累托前沿。</details>
**PDF:** <http://arxiv.org/pdf/2401.16762v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear**<br />
**Title_cn:** GazeGPT：使用智能眼镜的注视相关情境 AI 增强人类能力<br />
**Authors:** Robert Konrad, Nitish Padmanaban, J. Gabriel Buckmaster, Kevin C. Boyle, Gordon Wetzstein<br />
**Abstract:** <details><summary>原文: </summary>Multimodal large language models (LMMs) excel in world knowledge and problem-solving abilities. Through the use of a world-facing camera and contextual AI, emerging smart accessories aim to provide a seamless interface between humans and LMMs. Yet, these wearable computing systems lack an understanding of the user's attention. We introduce GazeGPT as a new user interaction paradigm for contextual AI. GazeGPT uses eye tracking to help the LMM understand which object in the world-facing camera view a user is paying attention to. Using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual AI. Moreover, we prototype a variety of application scenarios that suggest GazeGPT could be of significant value to users as part of future AI-driven personal assistants.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（LMM）在世界知识和解决问题的能力方面表现出色。通过使用面向世界的摄像头和情境人工智能，新兴的智能配件旨在提供人类和 LMM 之间的无缝界面。然而，这些可穿戴计算系统缺乏对用户注意力的理解。我们引入 GazeGPT 作为上下文 AI 的新用户交互范例。 GazeGPT 使用眼球追踪来帮助 LMM 了解用户正在关注面向世界的相机视图中的哪个对象。通过广泛的用户评估，我们表明这种注视条件机制是比其他替代方案更快、更准确的指向机制；它通过显着提高狗品种分类任务的准确性来增强人类的能力；并且它一直被认为比情境人工智能的头部或身体驱动的选择机制更自然。此外，我们对各种应用场景进行了原型设计，这些场景表明 GazeGPT 作为未来人工智能驱动的个人助理的一部分，可能对用户具有重大价值。</details>
**PDF:** <http://arxiv.org/pdf/2401.17217v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning**<br />
**Title_cn:** 通过持续语言学习，拥抱 CLIP 中的语言包容性和多样性<br />
**Authors:** Bang Yang, Yong Dai, Xuxin Cheng, Yaowei Li, Asif Raza, Yuexian Zou<br />
**Abstract:** <details><summary>原文: </summary>While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at \url{https://github.com/yangbang18/CLFM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然视觉语言预训练模型（VL-PTM）近年来在多模态研究方面取得了进展，但它们对英语等几种语言的掌握限制了它们在更广泛社区中的适用性。为此，人们越来越有兴趣通过联合学习设置开发多语言 VL 模型，然而，由于昂贵的成本和数据可用性，这可能不切实际。在这项工作中，我们建议通过持续语言学习（CLL）来扩展 VL-PTM 的语言能力，其中模型需要增量更新其语言知识，而不会遭受灾难性遗忘（CF）。我们通过引入一个名为 CLL-CLIP 的模型开始我们的研究，该模型建立在 CLIP 的基础上，CLIP 是一种流行的 VL-PTM，已获得图像-英语文本对齐。具体来说，CLL-CLIP 包含一个可扩展的标记嵌入层来处理语言差异。它仅训练标记嵌入以提高记忆稳定性，并在跨模式和跨语言目标下进行优化，以学习图像和多语言文本之间的对齐。为了减轻协变量移位和词汇重叠引起的 CF，我们进一步提出了一种新方法，确保初始化期间所有令牌嵌入的相同分布，并在训练期间规范令牌嵌入学习。我们基于 MSCOCO 和 XM3600 数据集构建了涵盖 36 种语言的 CLL 基准，然后评估多语言图文检索性能。大量实验验证了 CLL-CLIP 的有效性，并表明我们的方法可以提高 CLL-CLIP，例如，XM3600 上的文本到图像平均 Recall@1 提高 6.7%，并持续改进各种最先进的方法。我们的代码和数据可以在 \url{https://github.com/yangbang18/CLFM} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17186v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation**<br />
**Title_cn:** M2CURL：通过用于机器人操作的自监督表示学习实现样本高效的多模态强化学习<br />
**Authors:** Fotios Lygerakis, Vedant Dave, Elmar Rueckert<br />
**Abstract:** <details><summary>原文: </summary>One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks. This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模式强化学习（RL）最关键的方面之一是不同观察模式的有效整合。从这些模态中获得稳健且准确的表示是增强强化学习算法的鲁棒性和样本效率的关键。然而，在强化学习环境中学习视觉触觉数据的表示提出了重大挑战，特别是由于数据的高维性以及将视觉和触觉输入与动态环境和任务目标相关联所涉及的复杂性。为了应对这些挑战，我们提出了多模态对比无监督强化学习（M2CURL）。我们的方法采用了一种新颖的多模态自监督学习技术，可以学习有效的表示并有助于 RL 算法更快地收敛。我们的方法与 RL 算法无关，因此能够与任何可用的 RL 算法集成。我们在 Tactile Gym 2 模拟器上评估 M2CURL，结果表明它显着提高了不同操作任务中的学习效率。与没有我们的表示学习方法的标准强化学习算法相比，更快的收敛速度和更高的每集累积奖励证明了这一点。</details>
**PDF:** <http://arxiv.org/pdf/2401.17032v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data**<br />
**Title_cn:** 基于离散低维数据的连续天气模式跨模态预测的多模态表示学习<br />
**Authors:** Alif Bin Abdul Qayyum, Xihaier Luo, Nathan M. Urban, Xiaoning Qian, Byung-Jun Yoon<br />
**Abstract:** <details><summary>原文: </summary>World is looking for clean and renewable energy sources that do not pollute the environment, in an attempt to reduce greenhouse gas emissions that contribute to global warming. Wind energy has significant potential to not only reduce greenhouse emission, but also meet the ever increasing demand for energy. To enable the effective utilization of wind energy, addressing the following three challenges in wind data analysis is crucial. Firstly, improving data resolution in various climate conditions to ensure an ample supply of information for assessing potential energy resources. Secondly, implementing dimensionality reduction techniques for data collected from sensors/simulations to efficiently manage and store large datasets. Thirdly, extrapolating wind data from one spatial specification to another, particularly in cases where data acquisition may be impractical or costly. We propose a deep learning based approach to achieve multi-modal continuous resolution wind data prediction from discontinuous wind data, along with data dimensionality reduction.</details>
**Abstract_cn:** <details><summary>译文: </summary>世界正在寻找不污染环境的清洁和可再生能源，以减少导致全球变暖的温室气体排放。风能不仅具有减少温室气体排放的巨大潜力，而且还可以满足不断增长的能源需求。为了实现风能的有效利用，解决风数据分析中的以下三个挑战至关重要。首先，提高各种气候条件下的数据分辨率，确保为评估潜在能源资源提供充足的信息。其次，对从传感器/模拟收集的数据实施降维技术，以有效管理和存储大型数据集。第三，将风数据从一种空间规范外推到另一种空间规范，特别是在数据采集可能不切实际或成本高昂的情况下。我们提出了一种基于深度学习的方法，从不连续的风数据中实现多模态连续分辨率风数据预测，并降低数据维数。</details>
**PDF:** <http://arxiv.org/pdf/2401.16936v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation**<br />
**Title_cn:** 模态不完整场景分割的傅立叶快速调整<br />
**Authors:** Ruiping Liu, Jiaming Zhang, Kunyu Peng, Yufan Chen, Ke Cao, Junwei Zheng, M. Saquib Sarfraz, Kailun Yang, Rainer Stiefelhagen<br />
**Abstract:** <details><summary>原文: </summary>Integrating information from multiple modalities enhances the robustness of scene perception systems in autonomous vehicles, providing a more comprehensive and reliable sensory framework. However, the modality incompleteness in multi-modal segmentation remains under-explored. In this work, we establish a task called Modality-Incomplete Scene Segmentation (MISS), which encompasses both system-level modality absence and sensor-level modality errors. To avoid the predominant modality reliance in multi-modal fusion, we introduce a Missing-aware Modal Switch (MMS) strategy to proactively manage missing modalities during training. Utilizing bit-level batch-wise sampling enhances the model's performance in both complete and incomplete testing scenarios. Furthermore, we introduce the Fourier Prompt Tuning (FPT) method to incorporate representative spectral information into a limited number of learnable prompts that maintain robustness against all MISS scenarios. Akin to fine-tuning effects but with fewer tunable parameters (1.1%). Extensive experiments prove the efficacy of our proposed approach, showcasing an improvement of 5.84% mIoU over the prior state-of-the-art parameter-efficient methods in modality missing. The source code will be publicly available at https://github.com/RuipingL/MISS.</details>
**Abstract_cn:** <details><summary>译文: </summary>集成来自多种模式的信息增强了自动驾驶车辆场景感知系统的稳健性，提供了更全面、更可靠的感知框架。然而，多模态分割中的模态不完整性仍未得到充分探索。在这项工作中，我们建立了一项名为模态不完整场景分割（MISS）的任务，其中包括系统级模态缺失和传感器级模态错误。为了避免多模态融合中对模态的主要依赖，我们引入了一种缺失感知模态切换（MMS）策略，以在训练期间主动管理缺失的模态。利用位级批量采样可以增强模型在完整和不完整测试场景中的性能。此外，我们引入了傅立叶提示调谐（FPT）方法，将代表性光谱信息合并到有限数量的可学习提示中，从而保持针对所有 MISS 场景的鲁棒性。类似于微调效果，但可调参数较少 (1.1%)。大量的实验证明了我们提出的方法的有效性，在模态缺失方面比先前最先进的参数有效方法提高了 5.84% mIoU。源代码将在 https://github.com/RuipingL/MISS 公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2401.16923v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain**<br />
**Title_cn:** EarthGPT：遥感领域多传感器图像理解的通用多模态大语言模型<br />
**Authors:** Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal large language models (MLLMs) have demonstrated remarkable success in vision and visual-language tasks within the natural image domain. Owing to the significant diversities between the natural image and RS image hinder the development of MLLMs in the remote sensing (RS) domain. Currently, the unified and powerful MLLM capable of various RS visual tasks is still under-explored. To fill the gap, a pioneer MLLM called EarthGPT is proposed for universal RS image comprehension, which integrates various multi-sensor RS interpretation tasks uniformly. More importantly, a large-scale multi-sensor multi-modal RS instruction-following dataset named MMRS is carefully constructed, which comprises 1005.842k image-text pairs based on 34 existing diverse RS datasets and includes multi-sensor images such as optical, synthetic aperture radar (SAR), and infrared. The MMRS addresses the issue of MLLMs lacking RS expert knowledge and stimulates the development of MMLMs in the RS domain. Extensive experiments demonstrate the EarthGPT's superior performance in various RS visual interpretation tasks compared with the other specialist models and MLLMs, which proves the effectiveness of the proposed EarthGPT and provides a versatile paradigm for open-set reasoning tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在自然图像领域的视觉和视觉语言任务中取得了显着的成功。由于自然图像和RS图像之间的显着差异阻碍了MLLMs在遥感（RS）领域的发展。目前，能够胜任各种RS视觉任务的统一且强大的MLLM仍处于探索之中。为了填补这一空白，一种名为 EarthGPT 的先驱 MLLM 被提出用于通用 RS 图像理解，它统一集成了各种多传感器 RS 解释任务。更重要的是，精心构建了一个名为 MMRS 的大规模多传感器多模态 RS 指令跟踪数据集，该数据集包含基于 34 个现有不同 RS 数据集的 1005.842k 个图像文本对，并包含光学、合成等多传感器图像。孔径雷达（SAR）和红外。 MMRS解决了MLLM缺乏RS专家知识的问题，并刺激了MMLM在RS领域的发展。大量实验表明，与其他专业模型和 MLLM 相比，EarthGPT 在各种 RS 视觉解释任务中具有优越的性能，这证明了所提出的 EarthGPT 的有效性，并为开放集推理任务提供了通用范例。</details>
**PDF:** <http://arxiv.org/pdf/2401.16822v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **MouSi: Poly-Visual-Expert Vision-Language Models**<br />
**Title_cn:** MouSi：多视觉专家视觉语言模型<br />
**Authors:** Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的大型视觉语言模型（VLM）经常遇到单个视觉组件能力不足、视觉标记过长等挑战。这些问题可能会限制模型准确解释复杂视觉信息和过长上下文信息的有效性。解决这些挑战对于提高 VLM 的性能和适用性至关重要。本文提出使用集成专家技术来协同各个视觉编码器的能力，包括图像文本匹配、OCR、图像分割等方面的技术。该技术引入了融合网络来统一不同视觉专家的输出处理，同时弥合图像编码器和预训练的法学硕士之间的差距。此外，我们探索了不同的位置编码方案，以减轻冗长的图像特征序列造成的位置编码的浪费，有效解决位置溢出和长度限制的问题。例如，在我们的实现中，该技术显着降低了 SAM 等模型中的位置占用率，从大量的 4096 减少到更高效且易于管理的 64，甚至降至 1。实验结果表明，具有多个专家的 VLM 始终表现出优于隔离的性能。随着更多专家的集成，视觉编码器的性能显着提升。我们已经开源了本报告中使用的培训代码。所有这些资源都可以在我们的项目网站上找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.17221v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis**<br />
**Title_cn:** StrokeNUWA：用于矢量图形合成的笔画标记化<br />
**Authors:** Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation ''stroke tokens'' on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a 94x speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了利用 LLM 进行视觉合成，传统方法通过专门的视觉模块将光栅图像信息转换为离散网格标记，同时破坏模型捕获视觉场景的真实语义表示的能力。本文认为，图像的另一种表示形式——矢量图形，可以通过对图像信息进行更自然和语义连贯的分割来有效地克服这一限制。因此，我们介绍了 StrokeNUWA，这是一项探索矢量图形上更好的视觉表示“笔划标记”的开创性工作，它本质上具有丰富的视觉语义，与法学硕士自然兼容，并且高度压缩。配备笔画令牌后，StrokeNUWA 在矢量图形生成任务中的各种指标上可以显着超越传统的基于 LLM 和基于优化的方法。此外，StrokeNUWA 的推理速度比现有方法提高了 94 倍，SVG 代码压缩率为 6.9%。</details>
**PDF:** <http://arxiv.org/pdf/2401.17093v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **CPR++: Object Localization via Single Coarse Point Supervision**<br />
**Title_cn:** CPR++：通过单粗点监督进行对象定位<br />
**Authors:** Xuehui Yu, Pengfei Chen, Kuiran Wang, Xumeng Han, Guorong Li, Zhenjun Han, Qixiang Ye, Jianbin Jiao<br />
**Abstract:** <details><summary>原文: </summary>Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention. However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points. Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem. In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective. CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point. Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization. We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++. We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization. Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++. We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL. The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于点的对象定位（POL）在低成本数据标注下追求高性能对象感知，引起了越来越多的关注。然而，由于标注点的不一致，点标注模式不可避免地引入语义方差。现有的 POL 严重依赖严格的注释规则来处理该问题，而这些规则很难定义和应用。在本研究中，我们提出了粗点细化（CPR），据我们所知，这是从算法角度减轻语义方差的首次尝试。 CPR通过选择邻域区域中的语义中心点来替换初始标注点来减少语义方差。此外，我们设计了一个采样区域估计模块来动态计算每个对象的采样区域，并使用级联结构来实现端到端优化。我们进一步将方差正则化集成到结构中以集中预测分数，产生 CPR++。我们观察到CPR++可以获得尺度信息并进一步减少全局区域的语义方差，从而保证高性能的目标定位。对四个具有挑战性的数据集进行的广泛实验验证了 CPR 和 CPR++ 的有效性。我们希望我们的工作能够激发更多关于设计算法而不是注释规则的研究来解决 POL 中的语义方差问题。数据集和代码将在 github.com/ucas-vg/PointTinyBenchmark 上公开。</details>
**PDF:** <http://arxiv.org/pdf/2401.17203v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision**<br />
**Title_cn:** OmniSCV：用于计算机视觉的全方位合成图像生成器<br />
**Authors:** Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>Omnidirectional and 360{\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.</details>
**Abstract_cn:** <details><summary>译文: </summary>全向和 360{\deg} 图像在工业和消费社会中变得越来越普遍，导致全向计算机视觉受到关注。它们的宽视野可以仅从图像中收集有关环境的大量信息。然而，这些图像的失真需要开发特定的算法来对其进行处理和解释。此外，大量图像对于基于学习的计算机视觉算法的正确训练至关重要。在本文中，我们提出了一种用于生成具有语义和深度信息的全向图像数据集的工具。这些图像是通过接口插件在虚幻引擎 4 的真实虚拟环境中获取的一组捕获合成的。我们收集了各种著名的投影模型，例如等距柱面和柱面全景图、不同的鱼眼镜头、折反射系统和经验模型。此外，我们在我们的工具中包含了逼真的非中心投影系统，如非中心全景图和非中心折反射系统。据我们所知，这是文献中第一个报道的用于生成逼真非中心图像的工具。此外，由于全向图像是虚拟制作的，因此我们提供有关语义和深度的像素信息以及对相机校准参数的完美了解。这允许创建具有像素精度的地面实况信息，用于训练学习算法和测试 3D 视觉方法。为了验证所提出的工具，测试了不同的计算机视觉算法，例如从屈光和折反射中心图像提取线、使用等距柱状全景图的 3D 布局恢复和 SLAM 以及从非中心全景图进行 3D 重建。</details>
**PDF:** <http://arxiv.org/pdf/2401.17061v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization**<br />
**Title_cn:** ViTree：用于逐步可解释的细粒度视觉分类的单路径神经树<br />
**Authors:** Danning Lao, Qi Liu, Jiazi Bu, Junchi Yan, Wei Shen<br />
**Abstract:** <details><summary>原文: </summary>As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount. Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration. In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees. By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner. Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process. This patch and path selectivity enhances model interpretability of ViTree, enabling better insights into the model's inner workings. Remarkably, extensive experimentation validates that this streamlined approach surpasses various strong competitors and achieves state-of-the-art performance while maintaining exceptional interpretability which is proved by multi-perspective methods. Code can be found at https://github.com/SJTU-DeepVisionLab/ViTree.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着计算机视觉的不断发展并在各个领域得到广泛应用，深度学习模型的可解释性需求变得至关重要。现有的方法通常采用事后技术或原型来解释决策过程，这可能是间接的并且缺乏内在的说明。在这项研究中，我们介绍了 ViTree，这是一种用于细粒度视觉分类的新颖方法，它将流行的视觉转换器作为特征提取主干与神经决策树相结合。通过遍历树路径，ViTree 有效地从经过 Transformer 处理的特征中选择补丁，以突出显示信息丰富的局部区域，从而以逐步的方式细化表示。与之前依赖软分布或路径集合的基于树的模型不同，ViTree 选择单个树路径，提供更清晰、更简单的决策过程。这种补丁和路径选择性增强了 ViTree 的模型可解释性，从而可以更好地了解模型的内部工作原理。值得注意的是，大量的实验验证了这种简化的方法超越了各种强大的竞争对手，实现了最先进的性能，同时保持了多视角方法证明的卓越的可解释性。代码可以在 https://github.com/SJTU-DeepVisionLab/ViTree 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.17050v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Deep 3D World Models for Multi-Image Super-Resolution Beyond Optical Flow**<br />
**Title_cn:** 超越光流的多图像超分辨率深度 3D 世界模型<br />
**Authors:** Luca Savant Aira, Diego Valsesia, Andrea Bordone Molini, Giulia Fracastoro, Enrico Magli, Andrea Mirabile<br />
**Abstract:** <details><summary>原文: </summary>Multi-image super-resolution (MISR) allows to increase the spatial resolution of a low-resolution (LR) acquisition by combining multiple images carrying complementary information in the form of sub-pixel offsets in the scene sampling, and can be significantly more effective than its single-image counterpart. Its main difficulty lies in accurately registering and fusing the multi-image information. Currently studied settings, such as burst photography, typically involve assumptions of small geometric disparity between the LR images and rely on optical flow for image registration. We study a MISR method that can increase the resolution of sets of images acquired with arbitrary, and potentially wildly different, camera positions and orientations, generalizing the currently studied MISR settings. Our proposed model, called EpiMISR, moves away from optical flow and explicitly uses the epipolar geometry of the acquisition process, together with transformer-based processing of radiance feature fields to substantially improve over state-of-the-art MISR methods in presence of large disparities in the LR images.</details>
**Abstract_cn:** <details><summary>译文: </summary>多图像超分辨率 (MISR) 允许通过组合在场景采样中以子像素偏移形式携带互补信息的多个图像来提高低分辨率 (LR) 采集的空间分辨率，并且可以显着提高效率比它的单图像对应物。其主要难点在于多图像信息的准确配准和融合。目前研究的设置（例如连拍摄影）通常涉及 LR 图像之间较小几何差异的假设，并依赖光流进行图像配准。我们研究了一种 MISR 方法，该方法可以提高使用任意且可能截然不同的相机位置和方向获取的图像集的分辨率，从而概括了当前研究的 MISR 设置。我们提出的模型称为 EpiMISR，它摆脱了光流并明确使用采集过程的对极几何结构，以及基于变压器的辐射特征场处理，以在存在大量数据的情况下大幅改进最先进的 MISR 方法。 LR 图像中的差异。</details>
**PDF:** <http://arxiv.org/pdf/2401.16972v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CAFCT: Contextual and Attentional Feature Fusions of Convolutional Neural Networks and Transformer for Liver Tumor Segmentation**<br />
**Title_cn:** CAFCT：用于肝脏肿瘤分割的卷积神经网络和 Transformer 的上下文和注意力特征融合<br />
**Authors:** Ming Kang, Chee-Ming Ting, Fung Fung Ting, Raphaël Phan<br />
**Abstract:** <details><summary>原文: </summary>Medical image semantic segmentation techniques can help identify tumors automatically from computed tomography (CT) scans. In this paper, we propose a Contextual and Attentional feature Fusions enhanced Convolutional Neural Network (CNN) and Transformer hybrid network (CAFCT) model for liver tumor segmentation. In the proposed model, three other modules are introduced in the network architecture: Attentional Feature Fusion (AFF), Atrous Spatial Pyramid Pooling (ASPP) of DeepLabv3, and Attention Gates (AGs) to improve contextual information related to tumor boundaries for accurate segmentation. Experimental results show that the proposed CAFCT achieves a mean Intersection over Union (IoU) of 90.38% and Dice score of 86.78%, respectively, on the Liver Tumor Segmentation Benchmark (LiTS) dataset, outperforming pure CNN or Transformer methods, e.g., Attention U-Net, and PVTFormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像语义分割技术可以帮助从计算机断层扫描（CT）扫描中自动识别肿瘤。在本文中，我们提出了一种用于肝脏肿瘤分割的上下文和注意力特征融合增强的卷积神经网络（CNN）和变压器混合网络（CAFCT）模型。在所提出的模型中，网络架构中引入了其他三个模块：注意力特征融合（AFF）、DeepLabv3的Atrous Spatial Pyramid Pooling（ASPP）和注意力门（AG），以改善与肿瘤边界相关的上下文信息以实现精确分割。实验结果表明，所提出的 CAFCT 在肝脏肿瘤分割基准（LiTS）数据集上的平均交集（IoU）和 Dice 得分分别为 90.38% 和 86.78%，优于纯 CNN 或 Transformer 方法，例如 Attention U -Net 和 PVTFormer。</details>
**PDF:** <http://arxiv.org/pdf/2401.16886v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing**<br />
**Title_cn:** SmartFRZ：使用基于注意力的层冻结的高效训练框架<br />
**Authors:** Sheng Li, Geng Yuan, Yue Dai, Youtao Zhang, Yanzhi Wang, Xulong Tang<br />
**Abstract:** <details><summary>原文: </summary>There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer freezing for different networks during training processes. To this end, we propose a generic and efficient training framework (SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer freezing, which can automatically select the appropriate layers to freeze without compromising accuracy. Experimental results show that SmartFRZ effectively reduces the amount of computation in training and achieves significant training acceleration, and outperforms the state-of-the-art layer freezing approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能应用程序不断涌现，其中模型训练是为这些应用程序提供高质量服务的关键。然而，模型训练过程既耗时又耗能，不可避免地影响了用户对应用效率的需求。层冻结是一种有效的模型训练技术，为了提高训练效率而被提出。尽管现有的层冻结方法显示出降低模型训练成本的巨大潜力，但它们仍然存在缺乏通用性和准确性受损等缺点。例如，现有的层冻结方法要么需要在训练之前手动定义冻结配置，这不适用于不同的网络，要么使用启发式冻结标准，很难保证在不同场景下的良好准确性。因此，缺乏一种通用且智能的层冻结方法，可以在训练过程中自动对不同网络进行“原位”层冻结。为此，我们提出了一个通用且高效的培训框架（SmartFRZ）。 SmartFRZ提出的核心技术是注意力引导层冻结，它可以在不影响准确性的情况下自动选择合适的层进行冻结。实验结果表明，SmartFRZ有效减少了训练计算量，实现了显着的训练加速，并且优于最先进的层冻结方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.16720v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Towards Precise 3D Human Pose Estimation with Multi-Perspective Spatial-Temporal Relational Transformers**<br />
**Title_cn:** 利用多视角时空关系变换器实现精确的 3D 人体姿势估计<br />
**Authors:** Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang<br />
**Abstract:** <details><summary>原文: </summary>3D human pose estimation captures the human joint points in three-dimensional space while keeping the depth information and physical structure. That is essential for applications that require precise pose information, such as human-computer interaction, scene understanding, and rehabilitation training. Due to the challenges in data collection, mainstream datasets of 3D human pose estimation are primarily composed of multi-view video data collected in laboratory environments, which contains rich spatial-temporal correlation information besides the image frame content. Given the remarkable self-attention mechanism of transformers, capable of capturing the spatial-temporal correlation from multi-view video datasets, we propose a multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose detection. Firstly, the spatial module represents the human pose feature by intra-image content, while the frame-image relation module extracts temporal relationships and 3D spatial positional relationship features between the multi-perspective images. Secondly, the self-attention mechanism is adopted to eliminate the interference from non-human body parts and reduce computing resources. Our method is evaluated on Human3.6M, a popular 3D human pose detection dataset. Experimental results demonstrate that our approach achieves state-of-the-art performance on this dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D人体姿态估计捕获三维空间中的人体关节点，同时保留深度信息和物理结构。这对于需要精确姿态信息的应用至关重要，例如人机交互、场景理解和康复训练。由于数据采集方面的挑战，3D人体姿态估计的主流数据集主要由在实验室环境中采集的多视角视频数据组成，这些数据除了图像帧内容之外还包含丰富的时空相关信息。鉴于 Transformer 卓越的自注意力机制，能够从多视图视频数据集中捕获时空相关性，我们提出了一种用于 3D 序列到序列 (seq2seq) 人体姿势检测的多阶段框架。首先，空间模块通过图像内内容表示人体姿态特征，而帧图像关系模块提取多视角图像之间的时间关系和3D空间位置关系特征。其次，采用自注意力机制，消除非人体部位的干扰，减少计算资源。我们的方法在 Human3.6M（一种流行的 3D 人体姿势检测数据集）上进行评估。实验结果表明，我们的方法在此数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.16700v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality**<br />
**Title_cn:** VR-GS：虚拟现实中的物理动力学感知交互式高斯溅射系统<br />
**Authors:** Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain momentum, there's a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for content creation, editing, and interaction within these virtual spaces are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting in a Virtual Reality setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our Virtual Reality system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.Our project page is available at: https://yingjiang96.github.io/VR-GS/.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着消费者虚拟现实 (VR) 和混合现实 (MR) 技术的发展势头，人们越来越关注 3D 虚拟内容互动的开发。不幸的是，在这些虚拟空间内进行内容创建、编辑和交互的传统技术充满了困难。它们往往不仅是工程密集型的，而且还需要广泛的专业知识，这增加了虚拟对象操作的挫败感和低效率。我们提出的 VR-GS 系统代表了以人为本的 3D 内容交互的飞跃，提供了无缝且直观的用户体验。通过在虚拟现实环境中开发物理动力学感知的交互式高斯泼溅，并在变形体模拟的同时构建高效的两级嵌入策略，VR-GS 确保了实时执行和高度逼真的动态响应。我们的虚拟现实系统的组件旨在实现高效率和有效性，从详细的场景重建和对象分割开始，通过多视图图像修复前进，并扩展到基于物理的交互式编辑。该系统还结合了实时变形嵌入和动态阴影投射，确保了全面且引人入胜的虚拟体验。我们的项目页面位于：https://yingjian96.github.io/VR-GS/。</details>
**PDF:** <http://arxiv.org/pdf/2401.16663v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **YOLO-World: Real-Time Open-Vocabulary Object Detection**<br />
**Title_cn:** YOLO-World：实时开放词汇目标检测<br />
**Authors:** Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan<br />
**Abstract:** <details><summary>原文: </summary>The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>You Only Look Once (YOLO) 系列检测器已成为高效实用的工具。然而，它们对预定义和训练的对象类别的依赖限制了它们在开放场景中的适用性。为了解决这一限制，我们引入了 YOLO-World，这是一种创新方法，通过视觉语言建模和大规模数据集的预训练来增强 YOLO 的开放词汇检测功能。具体来说，我们提出了一种新的可重新参数化的视觉语言路径聚合网络（RepVL-PAN）和区域文本对比损失，以促进视觉和语言信息之间的交互。我们的方法擅长以零样本的方式高效地检测各种物体。在具有挑战性的 LVIS 数据集上，YOLO-World 在 V100 上实现了 35.4 AP 和 52.0 FPS，在准确性和速度方面优于许多最先进的方法。此外，经过微调的 YOLO-World 在多个下游任务上实现了卓越的性能，包括对象检测和开放词汇实例分割。</details>
**PDF:** <http://arxiv.org/pdf/2401.17270v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses**<br />
**Title_cn:** 使用因子图和人体姿势进行多摄像机异步球定位和轨迹预测<br />
**Authors:** Qingyu Xiao, Zulfiqar Zaidi, Matthew Gombolay<br />
**Abstract:** <details><summary>原文: </summary>The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our result shows the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter.</details>
**Abstract_cn:** <details><summary>译文: </summary>快速、精确的球定位和预测对于开发球类运动中的敏捷机器人至关重要，特别是在网球等以高速球运动和强力旋转为特征的运动中。由旋转引起的马格努斯效应增加了飞行过程中的轨迹预测以及与地面接触时的弹跳动力学的复杂性。在这项研究中，我们引入了一种创新方法，将多摄像头系统与因子图相结合，以实现实时和异步 3D 网球定位。此外，我们还估计速度和自旋等隐藏状态以进行轨迹预测。此外，为了增强球飞行早期的旋转推断（可用的观察有限），我们使用时间卷积网络（TCN）集成人体姿势数据，以计算因子图中的旋转先验。这种细化在因子图的开头提供了更准确的自旋先验，从而改进了用于预测的早期隐藏状态推断。我们的结果表明，经过训练的 TCN 可以预测 RMSE 为 5.27 Hz 的自旋先验。与利用自适应扩展卡尔曼滤波器的基线方法相比，将 TCN 集成到因子图中可将着陆位置的预测误差降低超过 63.6%。</details>
**PDF:** <http://arxiv.org/pdf/2401.17185v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Non-central panorama indoor dataset**<br />
**Title_cn:** 非中心全景室内数据集<br />
**Authors:** Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>Omnidirectional images are one of the main sources of information for learning based scene understanding algorithms. However, annotated datasets of omnidirectional images cannot keep the pace of these learning based algorithms development. Among the different panoramas and in contrast to standard central ones, non-central panoramas provide geometrical information in the distortion of the image from which we can retrieve 3D information of the environment [2]. However, due to the lack of commercial non-central devices, up until now there was no dataset of these kinds of panoramas. In this data paper, we present the first dataset of non-central panoramas for indoor scene understanding. The dataset is composed by {\bf 2574} RGB non-central panoramas taken in around 650 different rooms. Each panorama has associated a depth map and annotations to obtain the layout of the room from the image as a structural edge map, list of corners in the image, the 3D corners of the room and the camera pose. The images are taken from photorealistic virtual environments and pixel-wise automatically annotated.</details>
**Abstract_cn:** <details><summary>译文: </summary>全向图像是基于学习的场景理解算法的主要信息来源之一。然而，带注释的全向图像数据集无法跟上这些基于学习的算法开发的步伐。在不同的全景图中，与标准的中心全景相比，非中心全景提供了图像失真的几何信息，我们可以从中检索环境的 3D 信息 [2]。然而，由于缺乏商业化的非中心设备，迄今为止还没有此类全景图的数据集。在这篇数据论文中，我们提出了第一个用于室内场景理解的非中心全景数据集。该数据集由在大约 650 个不同房间拍摄的 {\bf 2574} RGB 非中心全景图组成。每个全景图都关联了深度图和注释，以从图像中获取房间的布局（作为结构边缘图）、图像中的角点列表、房间的 3D 角点和相机姿势。这些图像取自逼真的虚拟环境，并按像素自动注释。</details>
**PDF:** <http://arxiv.org/pdf/2401.17075v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Atlanta Scaled layouts from non-central panoramas**<br />
**Title_cn:** 亚特兰大 非中心全景的比例布局<br />
**Authors:** Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without geometric nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover an scaled 3D layout. The results of our experiments show that we improve state-of-the-art methods for layout reconstruction and line extraction in non-central projection systems. We completely solve the problem in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了一种使用非中央采集系统恢复室内环境 3D 布局的新颖方法。从非中心全景图中，可以通过几何推理独立恢复完整的和按比例缩放的 3D 线，而无需几何或比例假设。然而，它们对噪声和复杂几何模型的敏感性导致这些全景图很少被研究。我们的新流程旨在通过神经网络提取室内环境结构线的边界，并在新的几何处理中利用非中心投影系统的特性来恢复缩放的 3D 布局。我们的实验结果表明，我们改进了非中心投影系统中布局重建和线条提取的最先进方法。我们完全解决了曼哈顿和亚特兰大环境中的问题，处理遮挡并检索房间的公制比例，而无需额外的测量。据作者所知，我们的方法是第一个在非中心全景图上使用深度学习并从单个全景图恢复缩放布局的工作。</details>
**PDF:** <http://arxiv.org/pdf/2401.17058v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation**<br />
**Title_cn:** BlockFusion：使用潜在三平面外推法生成可扩展的 3D 场景<br />
**Authors:** Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 BlockFusion，一种基于扩散的模型，它生成 3D 场景作为单位块，并无缝合并新块来扩展场景。 BlockFusion 使用从完整 3D 场景网格中随机裁剪的 3D 块数据集进行训练。通过每块拟合，所有训练块都转换为混合神经场：具有包含几何特征的三平面，后面是用于解码有符号距离值的多层感知器（MLP）。采用变分自动编码器将三平面压缩到潜在三平面空间中，并在该空间上执行去噪扩散过程。应用于潜在表示的扩散可以生成高质量且多样化的 3D 场景。要在生成过程中扩展场景，只需附加空块以与当前场景重叠，并推断现有的潜在三平面以填充新块。外推是通过在去噪迭代期间使用来自重叠三平面的特征样本调节生成过程来完成的。潜在三平面外推产生语义和几何上有意义的过渡，与现有场景和谐地融合在一起。 2D 布局调节机制用于控制场景元素的放置和排列。实验结果表明，BlockFusion 能够在室内和室外场景中生成多样化、几何一致且无边界的大型 3D 场景，且具有前所未有的高质量形状。</details>
**PDF:** <http://arxiv.org/pdf/2401.17053v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **An Embeddable Implicit IUVD Representation for Part-based 3D Human Surface Reconstruction**<br />
**Title_cn:** 基于部件的 3D 人体表面重建的可嵌入隐式 IUVD 表示<br />
**Authors:** Baoxing Li, Yong Deng, Yehui Yang, Xu Zhao<br />
**Abstract:** <details><summary>原文: </summary>To reconstruct a 3D human surface from a single image, it is important to consider human pose, shape and clothing details simultaneously. In recent years, a combination of parametric body models (such as SMPL) that capture body pose and shape prior, and neural implicit functions that learn flexible clothing details, has been used to integrate the advantages of both approaches. However, the combined representation introduces additional computation, e.g. signed distance calculation, in 3D body feature extraction, which exacerbates the redundancy of the implicit query-and-infer process and fails to preserve the underlying body shape prior. To address these issues, we propose a novel IUVD-Feedback representation, which consists of an IUVD occupancy function and a feedback query algorithm. With this representation, the time-consuming signed distance calculation is replaced by a simple linear transformation in the IUVD space, leveraging the SMPL UV maps. Additionally, the redundant query points in the query-and-infer process are reduced through a feedback mechanism. This leads to more reasonable 3D body features and more effective query points, successfully preserving the parametric body prior. Moreover, the IUVD-Feedback representation can be embedded into any existing implicit human reconstruction pipelines without modifying the trained neural networks. Experiments on THuman2.0 dataset demonstrate that the proposed IUVD-Feedback representation improves result robustness and achieves three times faster acceleration in the query-and-infer process. Furthermore, this representation has the potential to be used in generative applications by leveraging its inherited semantic information from the parametric body model.</details>
**Abstract_cn:** <details><summary>译文: </summary>要从单个图像重建 3D 人体表面，同时考虑人体姿势、体形和服装细节非常重要。近年来，先验捕捉身体姿势和形状的参数化身体模型（例如 SMPL）与学习灵活服装细节的神经隐式函数相结合，已被用来整合这两种方法的优点。然而，组合表示引入了额外的计算，例如3D 身体特征提取中的有符号距离计算，加剧了隐式查询和推断过程的冗余，并且无法保留底层的身体形状先验。为了解决这些问题，我们提出了一种新颖的 IUVD 反馈表示，它由 IUVD 占用函数和反馈查询算法组成。通过这种表示，耗时的符号距离计算被 IUVD 空间中的简单线性变换所取代，利用 SMPL UV 贴图。此外，通过反馈机制减少了查询和推断过程中的冗余查询点。这导致更合理的3D身体特征和更有效的查询点，成功地保留了参数化身体先验。此外，IUVD 反馈表示可以嵌入到任何现有的隐式人体重建管道中，而无需修改训练的神经网络。 THuman2.0 数据集上的实验表明，所提出的 IUVD 反馈表示提高了结果的鲁棒性，并在查询和推断过程中实现了三倍的加速。此外，通过利用从参数化身体模型继承的语义信息，这种表示有可能在生成应用程序中使用。</details>
**PDF:** <http://arxiv.org/pdf/2401.16810v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **All-optical complex field imaging using diffractive processors**<br />
**Title_cn:** 使用衍射处理器的全光学复杂场成像<br />
**Authors:** Jingxi Li, Yuhang Li, Tianyi Gan, Che-Yung Shen, Mona Jarrahi, Aydogan Ozcan<br />
**Abstract:** <details><summary>原文: </summary>Complex field imaging, which captures both the amplitude and phase information of input optical fields or objects, can offer rich structural insights into samples, such as their absorption and refractive index distributions. However, conventional image sensors are intensity-based and inherently lack the capability to directly measure the phase distribution of a field. This limitation can be overcome using interferometric or holographic methods, often supplemented by iterative phase retrieval algorithms, leading to a considerable increase in hardware complexity and computational demand. Here, we present a complex field imager design that enables snapshot imaging of both the amplitude and quantitative phase information of input fields using an intensity-based sensor array without any digital processing. Our design utilizes successive deep learning-optimized diffractive surfaces that are structured to collectively modulate the input complex field, forming two independent imaging channels that perform amplitude-to-amplitude and phase-to-intensity transformations between the input and output planes within a compact optical design, axially spanning ~100 wavelengths. The intensity distributions of the output fields at these two channels on the sensor plane directly correspond to the amplitude and quantitative phase profiles of the input complex field, eliminating the need for any digital image reconstruction algorithms. We experimentally validated the efficacy of our complex field diffractive imager designs through 3D-printed prototypes operating at the terahertz spectrum, with the output amplitude and phase channel images closely aligning with our numerical simulations. We envision that this complex field imager will have various applications in security, biomedical imaging, sensing and material science, among others.</details>
**Abstract_cn:** <details><summary>译文: </summary>复杂场成像可捕获输入光场或物体的振幅和相位信息，可以提供对样品的丰富结构洞察，例如其吸收和折射率分布。然而，传统的图像传感器是基于强度的，本质上缺乏直接测量场的相位分布的能力。这种限制可以使用干涉或全息方法来克服，通常辅以迭代相位检索算法，从而导致硬件复杂性和计算需求的显着增加。在这里，我们提出了一种复杂的场成像仪设计，可以使用基于强度的传感器阵列对输入场的振幅和定量相位信息进行快照成像，而无需任何数字处理。我们的设计利用连续的深度学习优化的衍射表面，这些表面的结构可以共同调制输入复杂场，形成两个独立的成像通道，在紧凑的光学器件内的输入和输出平面之间执行幅度到幅度和相位到强度的转换。设计，轴向跨越约 100 个波长。传感器平面上这两个通道的输出场的强度分布直接对应于输入复杂场的幅度和定量相位分布，从而无需任何数字图像重建算法。我们通过在太赫兹光谱下运行的 3D 打印原型，通过实验验证了复杂场衍射成像仪设计的功效，输出幅度和相位通道图像与我们的数值模拟密切相关。我们预计这种复杂的场成像仪将在安全、生物医学成像、传感和材料科学等领域有多种应用。</details>
**PDF:** <http://arxiv.org/pdf/2401.16779v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Multi-granularity Correspondence Learning from Long-term Noisy Videos**<br />
**Title_cn:** 从长期噪声视频中进行多粒度对应学习<br />
**Authors:** Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng<br />
**Abstract:** <details><summary>原文: </summary>Existing video-language studies mainly focus on learning short video clips, leaving long-term temporal dependencies rarely explored due to over-high computational cost of modeling long videos. To address this issue, one feasible solution is learning the correspondence between video clips and captions, which however inevitably encounters the multi-granularity noisy correspondence (MNC) problem. To be specific, MNC refers to the clip-caption misalignment (coarse-grained) and frame-word misalignment (fine-grained), hindering temporal learning and video understanding. In this paper, we propose NOise Robust Temporal Optimal traNsport (Norton) that addresses MNC in a unified optimal transport (OT) framework. In brief, Norton employs video-paragraph and clip-caption contrastive losses to capture long-term dependencies based on OT. To address coarse-grained misalignment in video-paragraph contrast, Norton filters out the irrelevant clips and captions through an alignable prompt bucket and realigns asynchronous clip-caption pairs based on transport distance. To address the fine-grained misalignment, Norton incorporates a soft-maximum operator to identify crucial words and key frames. Additionally, Norton exploits the potential faulty negative samples in clip-caption contrast by rectifying the alignment target with OT assignment to ensure precise temporal modeling. Extensive experiments on video retrieval, videoQA, and action segmentation verify the effectiveness of our method. Code is available at https://lin-yijie.github.io/projects/Norton.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的视频语言研究主要集中在学习短视频剪辑，由于建模长视频的计算成本过高，很少探索长期时间依赖性。为了解决这个问题，一种可行的解决方案是学习视频剪辑和字幕之间的对应关系，但这不可避免地会遇到多粒度噪声对应（MNC）问题。具体来说，MNC 指的是剪辑-标题错位（粗粒度）和帧-字错位（细粒度），阻碍了时间学习和视频理解。在本文中，我们提出了噪声鲁棒时间最优传输（Norton），它在统一的最优传输（OT）框架中解决了 MNC 问题。简而言之，诺顿利用视频段落和剪辑字幕对比损失来捕获基于 OT 的长期依赖性。为了解决视频段落对比度中的粗粒度错位问题，诺顿通过可对齐的提示桶过滤掉不相关的剪辑和字幕，并根据传输距离重新对齐异步剪辑字幕对。为了解决细粒度的错位问题，诺顿采用了软极大值运算符来识别关键单词和关键帧。此外，Norton 通过 OT 分配纠正对齐目标，利用剪辑字幕对比度中潜在的错误负样本，以确保精确的时间建模。视频检索、视频质量保证和动作分割的大量实验验证了我们方法的有效性。代码可从 https://lin-yijie.github.io/projects/Norton 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16702v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration**<br />
**Title_cn:** 为什么、何时以及如何在大数据驱动的 3D 物体检测中使用主动学习来实现安全自动驾驶：实证探索<br />
**Authors:** Ross Greer, Bjørk Antoniussen, Mathias V. Andersen, Andreas Møgelmose, Mohan M. Trivedi<br />
**Abstract:** <details><summary>原文: </summary>Active learning strategies for 3D object detection in autonomous driving datasets may help to address challenges of data imbalance, redundancy, and high-dimensional data. We demonstrate the effectiveness of entropy querying to select informative samples, aiming to reduce annotation costs and improve model performance. We experiment using the BEVFusion model for 3D object detection on the nuScenes dataset, comparing active learning to random sampling and demonstrating that entropy querying outperforms in most cases. The method is particularly effective in reducing the performance gap between majority and minority classes. Class-specific analysis reveals efficient allocation of annotated resources for limited data budgets, emphasizing the importance of selecting diverse and informative data for model training. Our findings suggest that entropy querying is a promising strategy for selecting data that enhances model learning in resource-constrained environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶数据集中 3D 物体检测的主动学习策略可能有助于解决数据不平衡、冗余和高维数据的挑战。我们证明了熵查询选择信息样本的有效性，旨在降低注释成本并提高模型性能。我们在 nuScenes 数据集上使用 BEVFusion 模型进行 3D 对象检测进行实验，将主动学习与随机采样进行比较，并证明熵查询在大多数情况下表现优于。该方法对于缩小多数班级和少数班级之间的表现差距特别有效。特定于类的分析揭示了有限数据预算的注释资源的有效分配，强调了为模型训练选择多样化和信息丰富的数据的重要性。我们的研究结果表明，熵查询是一种有前途的数据选择策略，可以增强资源受限环境中的模型学习。</details>
**PDF:** <http://arxiv.org/pdf/2401.16634v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Zero-shot Classification using Hyperdimensional Computing**<br />
**Title_cn:** 使用超维计算的零样本分类<br />
**Authors:** Samuele Ruffino, Geethan Karunaratne, Michael Hersche, Luca Benini, Abu Sebastian, Abbas Rahimi<br />
**Abstract:** <details><summary>原文: </summary>Classification based on Zero-shot Learning (ZSL) is the ability of a model to classify inputs into novel classes on which the model has not previously seen any training examples. Providing an auxiliary descriptor in the form of a set of attributes describing the new classes involved in the ZSL-based classification is one of the favored approaches to solving this challenging task. In this work, inspired by Hyperdimensional Computing (HDC), we propose the use of stationary binary codebooks of symbol-like distributed representations inside an attribute encoder to compactly represent a computationally simple end-to-end trainable model, which we name Hyperdimensional Computing Zero-shot Classifier~(HDC-ZSC). It consists of a trainable image encoder, an attribute encoder based on HDC, and a similarity kernel. We show that HDC-ZSC can be used to first perform zero-shot attribute extraction tasks and, can later be repurposed for Zero-shot Classification tasks with minimal architectural changes and minimal model retraining. HDC-ZSC achieves Pareto optimal results with a 63.8% top-1 classification accuracy on the CUB-200 dataset by having only 26.6 million trainable parameters. Compared to two other state-of-the-art non-generative approaches, HDC-ZSC achieves 4.3% and 9.9% better accuracy, while they require more than 1.85x and 1.72x parameters compared to HDC-ZSC, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于零样本学习 (ZSL) 的分类是模型将输入分类为模型之前未见过任何训练示例的新类的能力。以一组属性的形式提供辅助描述符，描述基于 ZSL 的分类中涉及的新类，是解决这一具有挑战性的任务的首选方法之一。在这项工作中，受超维计算（HDC）的启发，我们建议在属性编码器内使用类似符号的分布式表示的固定二进制码本来紧凑地表示计算上简单的端到端可训练模型，我们将其命名为超维计算零-射击分类器〜（HDC-ZSC）。它由可训练的图像编码器、基于HDC的属性编码器和相似度内核组成。我们证明 HDC-ZSC 首先可以用于执行零样本属性提取任务，然后可以重新用于零样本分类任务，只需最少的架构更改和最少的模型重新训练。 HDC-ZSC 仅具有 2660 万个可训练参数，在 CUB-200 数据集上实现了帕累托最优结果，具有 63.8% 的 top-1 分类精度。与其他两种最先进的非生成方法相比，HDC-ZSC 的准确度提高了 4.3% 和 9.9%，而与 HDC-ZSC 相比，它们分别需要超过 1.85 倍和 1.72 倍的参数。</details>
**PDF:** <http://arxiv.org/pdf/2401.16876v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Reviving Undersampling for Long-Tailed Learning**<br />
**Title_cn:** 恢复欠采样以实现长尾学习<br />
**Authors:** Hao Yu, Yingxiao Du, Jianxin Wu<br />
**Abstract:** <details><summary>原文: </summary>The training datasets used in long-tailed recognition are extremely unbalanced, resulting in significant variation in per-class accuracy across categories. Prior works mostly used average accuracy to evaluate their algorithms, which easily ignores those worst-performing categories. In this paper, we aim to enhance the accuracy of the worst-performing categories and utilize the harmonic mean and geometric mean to assess the model's performance. We revive the balanced undersampling idea to achieve this goal. In few-shot learning, balanced subsets are few-shot and will surely under-fit, hence it is not used in modern long-tailed learning. But, we find that it produces a more equitable distribution of accuracy across categories with much higher harmonic and geometric mean accuracy, and, but lower average accuracy. Moreover, we devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy almost intact when compared to state-of-the-art long-tailed learning methods. We validate the effectiveness of our approach on widely utilized benchmark datasets for long-tailed learning. Our code is at \href{https://github.com/yuhao318/BTM/}{https://github.com/yuhao318/BTM/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>长尾识别中使用的训练数据集极不平衡，导致跨类别的每类准确率存在显着差异。之前的工作大多使用平均准确度来评估他们的算法，这很容易忽略那些表现最差的类别。在本文中，我们的目标是提高表现最差类别的准确性，并利用调和平均值和几何平均值来评估模型的性能。我们复兴平衡欠采样的想法来实现这一目标。在少样本学习中，平衡子集是少样本的，肯定会欠拟合，因此它不用于现代长尾学习。但是，我们发现它在各个类别之间产生了更公平的准确度分布，具有更高的调和和几何平均准确度，但平均准确度较低。此外，我们设计了一种简单的模型集成策略，与最先进的长尾学习方法相比，它不会导致任何额外的开销，并实现了改进的调和和几何平均值，同时保持平均精度几乎完好无损。我们在广泛使用的长尾学习基准数据集上验证了我们的方法的有效性。我们的代码位于 \href{https://github.com/yuhao318/BTM/}{https://github.com/yuhao318/BTM/}。</details>
**PDF:** <http://arxiv.org/pdf/2401.16811v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning**<br />
**Title_cn:** 基于对比学习的深度神经网络故障注入攻击检测与恢复<br />
**Authors:** Chenan Wang, Pu Zhao, Siyue Wang, Xue Lin<br />
**Abstract:** <details><summary>原文: </summary>Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance. This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs. Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>当在执行设备上实现深度神经网络 (DNN) 模型作为推理引擎时，很容易受到故障注入攻击 (FIA) 的影响，错误注入攻击会操纵模型参数来破坏推理执行，从而导致灾难性的性能。这项工作将视觉表示的对比学习 (CL) 引入深度学习训练和推理管道中，即一种自监督学习方法，以在 FIA 下实现具有自我弹性的 DNN 推理引擎。我们提出的基于 CL 的 FIA 检测和恢复 (CFDR) 框架具有 (i) 仅使用单批测试数据进行实时检测和 (ii) 即使仅使用少量未标记的测试数据也能有效快速恢复。使用 CIFAR-10 数据集对多种类型的 FIA 进行评估，我们的 CFDR 显示出良好的检测和恢复效果。</details>
**PDF:** <http://arxiv.org/pdf/2401.16766v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images**<br />
**Title_cn:** MuSc：零样本工业异常分类和分割以及未标记图像的相互评分<br />
**Authors:** Xurui Li, Ziming Huang, Feng Xue, Yu Zhou<br />
**Abstract:** <details><summary>原文: </summary>This paper studies zero-shot anomaly classification (AC) and segmentation (AS) in industrial vision. We reveal that the abundant normal and abnormal cues implicit in unlabeled test images can be exploited for anomaly determination, which is ignored by prior methods. Our key observation is that for the industrial product images, the normal image patches could find a relatively large number of similar patches in other unlabeled images, while the abnormal ones only have a few similar patches. We leverage such a discriminative characteristic to design a novel zero-shot AC/AS method by Mutual Scoring (MuSc) of the unlabeled images, which does not need any training or prompts. Specifically, we perform Local Neighborhood Aggregation with Multiple Degrees (LNAMD) to obtain the patch features that are capable of representing anomalies in varying sizes. Then we propose the Mutual Scoring Mechanism (MSM) to leverage the unlabeled test images to assign the anomaly score to each other. Furthermore, we present an optimization approach named Re-scoring with Constrained Image-level Neighborhood (RsCIN) for image-level anomaly classification to suppress the false positives caused by noises in normal images. The superior performance on the challenging MVTec AD and VisA datasets demonstrates the effectiveness of our approach. Compared with the state-of-the-art zero-shot approaches, MuSc achieves a $\textbf{21.1%}$ PRO absolute gain (from 72.7% to 93.8%) on MVTec AD, a $\textbf{19.4%}$ pixel-AP gain and a $\textbf{14.7%}$ pixel-AUROC gain on VisA. In addition, our zero-shot approach outperforms most of the few-shot approaches and is comparable to some one-class methods. Code is available at https://github.com/xrli-U/MuSc.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究工业视觉中的零样本异常分类（AC）和分割（AS）。我们发现，未标记的测试图像中隐含的丰富的正常和异常线索可用于异常确定，这是现有方法所忽略的。我们的主要观察结果是，对于工业产品图像，正常图像块可以在其他未标记图像中找到相对大量的相似块，而异常图像仅具有少量相似块。我们利用这种判别特征，通过未标记图像的相互评分（MuSc）来设计一种新颖的零样本 AC/AS 方法，该方法不需要任何训练或提示。具体来说，我们执行多度局部邻域聚合（LNAMD）以获得能够表示不同大小异常的补丁特征。然后，我们提出相互评分机制（MSM），利用未标记的测试图像为彼此分配异常分数。此外，我们提出了一种名为受约束图像级邻域重新评分（RsCIN）的优化方法，用于图像级异常分类，以抑制正常图像中噪声引起的误报。在具有挑战性的 MVTec AD 和 VisA 数据集上的卓越性能证明了我们方法的有效性。与最先进的零样本方法相比，MuSc 在 MVTec AD 上实现了 $\textbf{21.1%}$ PRO 绝对增益（从 72.7% 到 93.8%），$\textbf{19.4%}$ VisA 上的像素 AP 增益和 $\textbf{14.7%}$ 像素 AUROC 增益。此外，我们的零样本方法优于大多数少样本方法，并且与一些一类方法相当。代码可在 https://github.com/xrli-U/MuSc 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.16753v1><br />
**Code:** <https://github.com/xrli-U/MuSc>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **A simple, strong baseline for building damage detection on the xBD dataset**<br />
**Title_cn:** 用于在 xBD 数据集上构建损伤检测的简单而强大的基线<br />
**Authors:** Sebastian Gerard, Paul Borne-Pons, Josephine Sullivan<br />
**Abstract:** <details><summary>原文: </summary>We construct a strong baseline method for building damage detection by starting with the highly-engineered winning solution of the xView2 competition, and gradually stripping away components. This way, we obtain a much simpler method, while retaining adequate performance. We expect the simplified solution to be more widely and easily applicable. This expectation is based on the reduced complexity, as well as the fact that we choose hyperparameters based on simple heuristics, that transfer to other datasets. We then re-arrange the xView2 dataset splits such that the test locations are not seen during training, contrary to the competition setup. In this setting, we find that both the complex and the simplified model fail to generalize to unseen locations. Analyzing the dataset indicates that this failure to generalize is not only a model-based problem, but that the difficulty might also be influenced by the unequal class distributions between events.   Code, including the baseline model, is available under https://github.com/PaulBorneP/Xview2_Strong_Baseline</details>
**Abstract_cn:** <details><summary>译文: </summary>我们从 xView2 竞赛中精心设计的获胜解决方案开始，逐步剥离组件，构建了用于构建损坏检测的强大基线方法。这样，我们获得了一种更简单的方法，同时保留了足够的性能。我们期望简化的解决方案能够更广泛、更容易应用。这种期望是基于降低的复杂性，以及我们基于简单启发式选择超参数这一事实，并将其转移到其他数据集。然后，我们重新排列 xView2 数据集分割，以便在训练期间看不到测试位置，这与竞赛设置相反。在这种情况下，我们发现复杂模型和简化模型都无法泛化到看不见的位置。分析数据集表明，这种泛化失败不仅是一个基于模型的问题，而且难度也可能受到事件之间不平等的类别分布的影响。代码（包括基线模型）可在 https://github.com/PaulBorneP/Xview2_Strong_Baseline 下获取</details>
**PDF:** <http://arxiv.org/pdf/2401.17271v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks**<br />
**Title_cn:** 防御语言模型免受越狱攻击的稳健提示优化<br />
**Authors:** Andy Zhou, Bo Li, Haohan Wang<br />
**Abstract:** <details><summary>原文: </summary>Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管人工智能对齐取得了进步，但语言模型（LM）仍然容易受到对抗性攻击或越狱，其中对手会修改输入提示以引发有害行为。虽然已经提出了一些防御措施，但它们侧重于狭隘的威胁模型，缺乏强大的防御能力，我们认为这种防御措施应该是有效的、普遍的和实用的。为了实现这一目标，我们提出了第一个对抗性目标来保护 LM 免受越狱攻击，并提出了一种算法，即鲁棒提示优化（RPO），该算法使用基于梯度的令牌优化来强制执行无害的输出。这产生了一个易于访问的后缀，显着提高了对优化过程中出现的越狱和未知的、持续越狱的鲁棒性，将 Starling-7B 在 20 次越狱中的攻击成功率从 84% 降低到 8.66%。此外，我们发现RPO对正常LM使用影响较小，在自适应攻击下成功，并且可以转移到黑盒模型，将GPT-4最强攻击的成功率从92%降低到6%。</details>
**PDF:** <http://arxiv.org/pdf/2401.17263v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **SLIC: A Learned Image Codec Using Structure and Color**<br />
**Title_cn:** SLIC：使用结构和颜色的学习图像编解码器<br />
**Authors:** Srivatsa Prativadibhayankaram, Mahadev Prasad Panda, Thomas Richter, Heiko Sparenberg, Siegfried Fößel, André Kaup<br />
**Abstract:** <details><summary>原文: </summary>We propose the structure and color based learned image codec (SLIC) in which the task of compression is split into that of luminance and chrominance. The deep learning model is built with a novel multi-scale architecture for Y and UV channels in the encoder, where the features from various stages are combined to obtain the latent representation. An autoregressive context model is employed for backward adaptation and a hyperprior block for forward adaptation. Various experiments are carried out to study and analyze the performance of the proposed model, and to compare it with other image codecs. We also illustrate the advantages of our method through the visualization of channel impulse responses, latent channels and various ablation studies. The model achieves Bj{\o}ntegaard delta bitrate gains of 7.5% and 4.66% in terms of MS-SSIM and CIEDE2000 metrics with respect to other state-of-the-art reference codecs.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了基于结构和颜色的学习图像编解码器（SLIC），其中压缩任务分为亮度和色度任务。深度学习模型是在编码器中针对 Y 和 UV 通道采用新颖的多尺度架构构建的，其中来自各个阶段的特征被组合以获得潜在表示。自回归上下文模型用于后向适应，超先验块用于前向适应。进行了各种实验来研究和分析所提出模型的性能，并将其与其他图像编解码器进行比较。我们还通过通道脉冲响应、潜在通道和各种消融研究的可视化来说明我们方法的优点。相对于其他最先进的参考编解码器，该模型在 MS-SSIM 和 CIEDE2000 指标方面实现了 7.5% 和 4.66% 的 Bj{\o}ntegaard 增量比特率增益。</details>
**PDF:** <http://arxiv.org/pdf/2401.17246v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment**<br />
**Title_cn:** ReAlnet：通过人类神经表征对齐实现更像人脑的视觉<br />
**Authors:** Zitong Lu, Yile Wang, Julie D. Golomb<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, "Re(presentational)Al(ignment)net", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visual representational patterns across object categories and different neural data modalities. Furthermore, we discover that alignment with human brain representations improves the model's adversarial robustness. Our findings suggest that ReAlnet sets a new precedent in the field, bridging the gap between artificial and human vision, and paving the way for more brain-like artificial intelligence systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管人工智能取得了显着的进步，但当前的物体识别模型在模拟人脑视觉信息处理机制方面仍然落后。最近的研究强调了使用神经数据来模拟大脑处理的潜力。然而，这些通常依赖于非人类受试者的侵入性神经记录，这在我们对人类视觉感知的理解和更多类似人脑的视觉模型的开发方面留下了重大差距。为了解决这一差距，我们首次提出“Re(presentational)Al(ignment)net”，这是一种基于非侵入性脑电图记录的与人脑活动一致的视觉模型，与人脑表征具有显着更高的相似性。我们创新的图像到大脑的多层编码对齐框架不仅优化了模型的多层，标志着神经对齐的实质性飞跃，而且使模型能够有效地学习和模仿人脑跨对象类别和不同类别的视觉表征模式。神经数据模式。此外，我们发现与人脑表征的一致性提高了模型的对抗鲁棒性。我们的研究结果表明，ReAlnet 在该领域树立了新的先例，弥合了人工视觉和人类视觉之间的差距，并为更多类脑人工智能系统铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2401.17231v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques**<br />
**Title_cn:** NormEnsembleXAI：揭示 XAI 集成技术的优点和缺点<br />
**Authors:** Weronika Hryniewska-Guzik, Bartosz Sawicki, Przemysław Biecek<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文对可解释人工智能（XAI）集成方法进行了全面的比较分析。我们的研究带来了三项重大贡献。首先，我们介绍了一种新颖的集成方法 NormEnsembleXAI，该方法利用最小、最大和平均函数以及归一化技术来增强可解释性。其次，我们深入了解 XAI 集成方法的优点和缺点。最后，我们提供了一个库，促进 XAI 集成的实际实施，从而促进透明且可解释的深度学习模型的采用。</details>
**PDF:** <http://arxiv.org/pdf/2401.17200v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Evaluation in Neural Style Transfer: A Review**<br />
**Title_cn:** 神经风格迁移评估：回顾<br />
**Authors:** Eleftherios Ioannou, Steve Maddock<br />
**Abstract:** <details><summary>原文: </summary>The field of Neural Style Transfer (NST) has witnessed remarkable progress in the past few years, with approaches being able to synthesize artistic and photorealistic images and videos of exceptional quality. To evaluate such results, a diverse landscape of evaluation methods and metrics is used, including authors' opinions based on side-by-side comparisons, human evaluation studies that quantify the subjective judgements of participants, and a multitude of quantitative computational metrics which objectively assess the different aspects of an algorithm's performance. However, there is no consensus regarding the most suitable and effective evaluation procedure that can guarantee the reliability of the results. In this review, we provide an in-depth analysis of existing evaluation techniques, identify the inconsistencies and limitations of current evaluation methods, and give recommendations for standardized evaluation practices. We believe that the development of a robust evaluation framework will not only enable more meaningful and fairer comparisons among NST methods but will also enhance the comprehension and interpretation of research findings in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经风格迁移（NST）领域在过去几年中取得了显着进展，其方法能够合成具有卓越品质的艺术和真实感图像和视频。为了评估这些结果，使用了多种评估方法和指标，包括基于并排比较的作者意见、量化参与者主观判断的人类评估研究以及客观评估的多种定量计算指标算法性能的不同方面。然而，对于最合适、最有效的、能够保证结果可靠性的评估程序，目前还没有达成共识。在本次综述中，我们对现有评估技术进行了深入分析，找出了当前评估方法的不一致和局限性，并提出了标准化评估实践的建议。我们相信，开发一个强大的评估框架不仅可以使 NST 方法之间进行更有意义和更公平的比较，而且还可以增强对该领域研究结果的理解和解释。</details>
**PDF:** <http://arxiv.org/pdf/2401.17109v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **H-SynEx: Using synthetic images and ultra-high resolution ex vivo MRI for hypothalamus subregion segmentation**<br />
**Title_cn:** H-SynEx：使用合成图像和超高分辨率离体 MRI 进行下丘脑分区分割<br />
**Authors:** Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes França, Simone Appenzeller, Juan Eugenio Iglesias, Leticia Rittner<br />
**Abstract:** <details><summary>原文: </summary>Purpose: To develop a method for automated segmentation of hypothalamus subregions informed by ultra-high resolution ex vivo magnetic resonance images (MRI), which generalizes across MRI sequences and resolutions without retraining.   Materials and Methods: We trained our deep learning method, H-synEx, with synthetic images derived from label maps built from ultra-high resolution ex vivo MRI scans, which enables finer-grained manual segmentation when compared with 1mm isometric in vivo images. We validated this retrospective study using 1535 in vivo images from six datasets and six MRI sequences. The quantitative evaluation used the Dice Coefficient (DC) and Average Hausdorff distance (AVD). Statistical analysis compared hypothalamic subregion volumes in controls, Alzheimer's disease (AD), and behavioral variant frontotemporal dementia (bvFTD) subjects using the area under the curve (AUC) and Wilcoxon rank sum test.   Results: H-SynEx can segment the hypothalamus across various MRI sequences, encompassing FLAIR sequences with significant slice spacing (5mm). Using hypothalamic volumes on T1w images to distinguish control from AD and bvFTD patients, we observed AUC values of 0.74 and 0.79 respectively. Additionally, AUC=0.66 was found for volume variation on FLAIR scans when comparing control and non-patients.   Conclusion: Our results show that H-SynEx successfully leverages information from ultra-high resolution scans to segment in vivo from different MRI sequences such as T1w, T2w, PD, qT1, FA, and FLAIR. We also found that our automated segmentation was able to discriminate controls versus patients on FLAIR images with 5mm spacing. H-SynEx is openly available at https://github.com/liviamarodrigues/hsynex.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：开发一种通过超高分辨率离体磁共振图像（MRI）自动分割下丘脑子区域的方法，该方法可以概括整个 MRI 序列和分辨率，而无需重新训练。材料和方法：我们使用从超高分辨率离体 MRI 扫描构建的标签图衍生的合成图像来训练我们的深度学习方法 H-synEx，与 1mm 等距体内图像相比，它可以实现更细粒度的手动分割。我们使用来自 6 个数据集和 6 个 MRI 序列的 1535 张体内图像验证了这项回顾性研究。定量评价使用骰子系数（DC）和平均豪斯多夫距离（AVD）。统计分析使用曲线下面积 (AUC) 和 Wilcoxon 秩和检验比较了对照组、阿尔茨海默病 (AD) 和行为变异型额颞叶痴呆 (bvFTD) 受试者的下丘脑亚区域体积。结果：H-SynEx 可以跨各种 MRI 序列对下丘脑进行分割，包括具有显着切片间距 (5mm) 的 FLAIR 序列。使用 T1w 图像上的下丘脑体积来区分对照组、AD 和 bvFTD 患者，我们观察到 AUC 值分别为 0.74 和 0.79。此外，在比较对照和非患者时，发现 FLAIR 扫描的体积变化 AUC=0.66。结论：我们的结果表明，H-SynEx 成功地利用超高分辨率扫描的信息，从不同的 MRI 序列（如 T1w、T2w、PD、qT1、FA 和 FLAIR）中进行体内分割。我们还发现，我们的自动分割能够在 5 毫米间距的 FLAIR 图像上区分对照组和患者。 H-SynEx 可在 https://github.com/liviamarodrigues/hsynex 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17104v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **CharNet: Generalized Approach for High-Complexity Character Classification**<br />
**Title_cn:** CharNet：高复杂性字符分类的通用方法<br />
**Authors:** Boris Kriuk<br />
**Abstract:** <details><summary>原文: </summary>Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a straightforward, generalizable, and highly effective approach (CharNet) for detailed character image classification and compares its performance to that of existing approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>手写字符识别（HCR）对于机器学习研究人员来说是一个具有挑战性的问题。与印刷文本数据不同，手写字符数据集由于人为引入的偏差而具有更多变化。由于存在大量独特的字符类别，某些数据（例如语标文字或中韩字符序列）给 HCR 问题带来了新的复杂性。此类数据集的分类任务要求模型学习具有相似特征的图像的高复杂性细节。随着计算资源可用性的最新进展和计算机视觉理论的进一步发展，一些研究团队已经有效地解决了所出现的挑战。尽管以实现高效率而闻名，但许多常见方法仍然不可推广，需要使用特定于数据集的解决方案来获得更好的结果。由于结构复杂和计算要求高，现有方法经常阻碍解决方案的普及。本文提出了一种简单、可推广且高​​效的方法（CharNet）来进行详细的字符图像分类，并将其性能与现有方法进行了比较。</details>
**PDF:** <http://arxiv.org/pdf/2401.17098v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Active Generation Network of Human Skeleton for Action Recognition**<br />
**Title_cn:** 用于动作识别的人体骨骼主动生成网络<br />
**Authors:** Long Liu, Xin Wang, Fangming Li, Jiayu Chen<br />
**Abstract:** <details><summary>原文: </summary>Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition. Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action. In addition, the data generated by these methods lack diversity when only a few training samples are available. To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples. The AGN consists of an action generation network and an uncertainty metric network. The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action. The latter guides generating actions. Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric. Finally, UMN provides the uncertainty sampling basis for the generated actions.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据生成是一种数据增强技术，用于增强基于骨架的人体动作识别的泛化能力。大多数现有的数据生成方法都面临着确保行动动态信息的时间一致性的挑战。此外，当只有少数训练样本可用时，这些方法生成的数据缺乏多样性。为了解决这些问题，我们提出了一种新颖的主动生成网络（AGN），当特定动作的数据只有单个样本或几个样本时，它可以通过运动风格迁移自适应地学习各种动作类别，以生成新的动作。 AGN 由动作生成网络和不确定性度量网络组成。前者以ST-GCN为Backbone，可以隐式学习目标动作的形态特征，同时保留源动作的类别特征。后者指导生成动作。具体来说，动作识别模型为每个动作生成预测向量，然后使用不确定性度量对其进行评分。最后，UMN为生成的动作提供不确定性采样基础。</details>
**PDF:** <http://arxiv.org/pdf/2401.17086v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Efficient Gesture Recognition on Spiking Convolutional Networks Through Sensor Fusion of Event-Based and Depth Data**<br />
**Title_cn:** 通过基于事件和深度数据的传感器融合在尖峰卷积网络上进行高效手势识别<br />
**Authors:** Lea Steffen, Thomas Trapp, Arne Roennau, Rüdiger Dillmann<br />
**Abstract:** <details><summary>原文: </summary>As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着智能系统在我们的日常生活中变得越来越重要，需要新的交互方式。经典的用户界面给身体受损的人带来了问题，并且部分不实用或不方便。手势识别是一种替代方案，但在使用传统摄像头时通常反应不够灵敏。这项工作提出了一种尖峰卷积神经网络，用于处理事件和深度数据以进行手势识别。该网络使用开源神经拟态计算框架 LAVA 进行模拟，用于在嵌入式系统上进行离线训练和评估。为了进行评估，使用了三个开源数据集。由于这些并不代表所应用的双模态，因此记录了具有同步事件和深度数据的新数据集。结果表明，即使在不同编码的数据上，时间编码对深度信息和模态融合的可行性也有利于网络性能和泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.17064v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Floor extraction and door detection for visually impaired guidance**<br />
**Title_cn:** 楼层提取和门检测，为视障人士提供引导<br />
**Authors:** Bruno Berenguel-Baeta, Manuel Guerrero-Viu, Alejandro de Nova, Jesus Bermudez-Cameo, Alejandro Perez-Yus, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>Finding obstacle-free paths in unknown environments is a big navigation issue for visually impaired people and autonomous robots. Previous works focus on obstacle avoidance, however they do not have a general view of the environment they are moving in. New devices based on computer vision systems can help impaired people to overcome the difficulties of navigating in unknown environments in safe conditions. In this work it is proposed a combination of sensors and algorithms that can lead to the building of a navigation system for visually impaired people. Based on traditional systems that use RGB-D cameras for obstacle avoidance, it is included and combined the information of a fish-eye camera, which will give a better understanding of the user's surroundings. The combination gives robustness and reliability to the system as well as a wide field of view that allows to obtain many information from the environment. This combination of sensors is inspired by human vision where the center of the retina (fovea) provides more accurate information than the periphery, where humans have a wider field of view. The proposed system is mounted on a wearable device that provides the obstacle-free zones of the scene, allowing the planning of trajectories for people guidance.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于视障人士和自主机器人来说，在未知环境中寻找无障碍路径是一个很大的导航问题。以前的工作主要集中在避障上，但是他们没有对他们所移动的环境有一个总体的了解。基于计算机视觉系统的新设备可以帮助受损的人克服在安全条件下未知环境中导航的困难。在这项工作中，提出了传感器和算法的结合，可以为视障人士构建导航系统。在使用RGB-D摄像头进行避障的传统系统的基础上，它加入并结合了鱼眼摄像头的信息，这将让用户更好地了解周围的环境。这种组合为系统提供了稳健性和可靠性，以及广阔的视野，可以从环境中获取许多信息。这种传感器组合的灵感来自人类视觉，其中视网膜中心（中央凹）提供比外围更准确的信息，而外围人类拥有更广阔的视野。所提出的系统安装在可穿戴设备上，该设备提供场景的无障碍区域，从而可以规划引导人员的轨迹。</details>
**PDF:** <http://arxiv.org/pdf/2401.17056v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of SAR ATR**<br />
**Title_cn:** 评估 SAR ATR 的综合测量对抗漏洞<br />
**Authors:** Bowen Peng, Bo Peng, Jingyuan Xia, Tianpeng Liu, Yongxiang Liu, Li Liu<br />
**Abstract:** <details><summary>原文: </summary>Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations. This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with measured data. Compared with the current measured-to-measured (M2M) transfer setting, our approach does not need direct access to the victim model or the measured SAR data. We also propose the transferability estimation attack (TEA) to uncover the adversarial risks in this more challenging and practical scenario. The TEA makes full use of the limited similarity between the synthetic and measured data pairs for blind estimation and optimization of S2M transferability, leading to feasible surrogate model enhancement without mastering the victim model and data. Comprehensive evaluations based on the publicly available synthetic and measured paired labeled experiment (SAMPLE) dataset demonstrate that the TEA outperforms state-of-the-art methods and can significantly enhance various attack algorithms in computer vision and remote sensing applications. Codes and data are available at https://github.com/scenarri/S2M-TEA.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，人们越来越关注基于深度神经网络 (DNN) 的合成孔径雷达 (SAR) 自动目标识别 (ATR) 面对对抗性攻击的脆弱性，其中 DNN 很容易被具有难以察觉但具有攻击性的扰动的干净输入所欺骗。本文研究了合成到测量（S2M）传输设置，其中攻击者仅基于合成数据生成对抗性扰动，并将其传输到使用测量数据训练的受害者模型。与当前的测量到测量（M2M）传输设置相比，我们的方法不需要直接访问受害者模型或测量的 SAR 数据。我们还提出了可转移性估计攻击（TEA），以揭示这种更具挑战性和实际场景中的对抗性风险。 TEA充分利用合成数据和测量数据对之间有限的相似性来进行S2M可转移性的盲估计和优化，从而在不掌握受害者模型和数据的情况下实现可行的代理模型增强。基于公开的合成和测量配对标记实验 (SAMPLE) 数据集的综合评估表明，TEA 的性能优于最先进的方法，并且可以显着增强计算机视觉和遥感应用中的各种攻击算法。代码和数据可在 https://github.com/scenarri/S2M-TEA 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17038v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Multilayer Graph Approach to Deep Subspace Clustering**<br />
**Title_cn:** 深层子空间聚类的多层图方法<br />
**Authors:** Lovro Sindičić, Ivica Kopriva<br />
**Abstract:** <details><summary>原文: </summary>Deep subspace clustering (DSC) networks based on self-expressive model learn representation matrix, often implemented in terms of fully connected network, in the embedded space. After the learning is finished, representation matrix is used by spectral clustering module to assign labels to clusters. However, such approach ignores complementary information that exist in other layers of the encoder (including the input data themselves). Herein, we apply selected linear subspace clustering algorithm to learn representation matrices from representations learned by all layers of encoder network including the input data. Afterward, we learn a multilayer graph that in a multi-view like manner integrates information from graph Laplacians of all used layers. That improves further performance of selected DSC network. Furthermore, we also provide formulation of our approach to cluster out-of-sample/test data points. We validate proposed approach on four well-known datasets with two DSC networks as baseline models. In almost all the cases, proposed approach achieved statistically significant improvement in three performance metrics. MATLAB code of proposed algorithm is posted on https://github.com/lovro-sinda/MLG-DSC.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于自表达模型的深度子空间聚类（DSC）网络学习表示矩阵，通常在嵌入空间中以全连接网络的形式实现。学习完成后，谱聚类模块使用表示矩阵为聚类分配标签。然而，这种方法忽略了编码器其他层中存在的补充信息（包括输入数据本身）。在这里，我们应用选定的线性子空间聚类算法从编码器网络所有层（包括输入数据）学习的表示中学习表示矩阵。之后，我们学习一个多层图，它以类似多视图的方式集成来自所有使用层的图拉普拉斯算子的信息。这进一步提高了所选 DSC 网络的性能。此外，我们还提供了对样本外/测试数据点进行聚类的方法的公式。我们以两个 DSC 网络作为基线模型，在四个著名数据集上验证了所提出的方法。在几乎所有情况下，所提出的方法在三个性能指标上都实现了统计上的显着改进。所提出算法的 MATLAB 代码发布在 https://github.com/lovro-sinda/MLG-DSC 上。</details>
**PDF:** <http://arxiv.org/pdf/2401.17033v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Static and Dynamic Synthesis of Bengali and Devanagari Signatures**<br />
**Title_cn:** 孟加拉语和梵文签名的静态和动态合成<br />
**Authors:** Miguel A. Ferrer, Sukalpa Chanda, Moises Diaz, Chayan Kr. Banerjee, Anirban Majumdar, Cristina Carmona-Duarte, Parikshit Acharya, Umapada Pal<br />
**Abstract:** <details><summary>原文: </summary>Developing an automatic signature verification system is challenging and demands a large number of training samples. This is why synthetic handwriting generation is an emerging topic in document image analysis. Some handwriting synthesizers use the motor equivalence model, the well-established hypothesis from neuroscience, which analyses how a human being accomplishes movement. Specifically, a motor equivalence model divides human actions into two steps: 1) the effector independent step at cognitive level and 2) the effector dependent step at motor level. In fact, recent work reports the successful application to Western scripts of a handwriting synthesizer, based on this theory. This paper aims to adapt this scheme for the generation of synthetic signatures in two Indic scripts, Bengali (Bangla), and Devanagari (Hindi). For this purpose, we use two different online and offline databases for both Bengali and Devanagari signatures. This paper reports an effective synthesizer for static and dynamic signatures written in Devanagari or Bengali scripts. We obtain promising results with artificially generated signatures in terms of appearance and performance when we compare the results with those for real signatures.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发自动签名验证系统具有挑战性，需要大量的训练样本。这就是为什么合成手写生成是文档图像分析中的一个新兴主题。一些手写合成器使用运动等效模型，这是神经科学中公认的假设，它分析人类如何完成运动。具体来说，运动等效模型将人类行为分为两个步骤：1）认知层面的效应器独立步骤和2）运动层面的效应器依赖步骤。事实上，最近的工作报告了基于这一理论的手写合成器在西方文字中的成功应用。本文旨在采用该方案来生成两种印度文字：孟加拉语（孟加拉语）和梵文（印地语）的合成签名。为此，我们对孟加拉语和梵文签名使用两个不同的在线和离线数据库。本文报告了一种有效的合成器，用于用梵文或孟加拉语脚本编写的静态和动态签名。当我们将人工生成的签名的结果与真实签名的结果进行比较时，我们在外观和性能方面获得了有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2401.17026v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **MF-MOS: A Motion-Focused Model for Moving Object Segmentation**<br />
**Title_cn:** MF-MOS：用于运动物体分割的运动聚焦模型<br />
**Authors:** Jintao Cheng, Kang Zeng, Zhuoxu Huang, Xiaoyu Tang, Jin Wu, Chengxi Zhang, Xieyuanli Chen, Rui Fan<br />
**Abstract:** <details><summary>原文: </summary>Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field. Dynamic capture is always critical in the MOS problem. Previous methods capture motion features from the range images directly. Differently, we argue that the residual maps provide greater potential for motion information, while range images contain rich semantic guidance. Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for LiDAR moving object segmentation. Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch. Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the LiDAR-based MOS task. Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance. The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS.</details>
**Abstract_cn:** <details><summary>译文: </summary>移动对象分割（MOS）为检测交通参与者提供了可靠的解决方案，因此在自动驾驶领域引起了极大的兴趣。动态捕获在 MOS 问题中始终至关重要。以前的方法直接从距离图像中捕获运动特征。不同的是，我们认为残差图为运动信息提供了更大的潜力，而范围图像包含丰富的语义指导。基于这种直觉，我们提出了 MF-MOS，一种新颖的运动聚焦模型，具有双分支结构，用于 LiDAR 运动物体分割。新颖的是，我们通过从残差图中捕获运动并从距离图像生成语义特征来解耦时空信息，这些特征用作运动分支的可移动对象引导。我们简单而独特的解决方案可以充分利用距离图像和残差图，从而大大提高基于 LiDAR 的 MOS 任务的性能。值得注意的是，我们的 MF-MOS 在提交后在 SemanticKITTI 数据集的 MOS 排行榜上取得了领先的 IoU 76.7%，展示了当前最先进的性能。我们的 MF-MOS 的实现已在 https://github.com/SCNU-RISLAB/MF-MOS 上发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.17023v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets**<br />
**Title_cn:** 自动驾驶数据集上的分布外检测性能评估<br />
**Authors:** Jens Henriksson, Christian Berger, Stig Ursing, Markus Borg<br />
**Abstract:** <details><summary>原文: </summary>Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.   This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.</details>
**Abstract_cn:** <details><summary>译文: </summary>需要系统地研究安全措施，以评估关键应用的深度神经网络 (DNN) 的预期性能。由于缺乏高维 DNN 的验证方法，需要在可接受的性能和分布外 (OOD) 样本的处理之间进行权衡。这项工作通过应用基于预测类最可能的类条件高斯分布的马哈拉诺比斯距离 (MD) 作为 OOD 分数来评估语义分割 DNN 的拒绝输出。该评估遵循在 Cityscapes 数据集上训练的三个 DNN 并在四个汽车数据集上进行测试，发现即使应用于看不见的数据集，也可以以像素覆盖为代价大幅降低分类风险。我们的研究结果的适用性将支持安全措施的合法化，并在争论汽车感知中安全使用 DNN 时激励其使用。</details>
**PDF:** <http://arxiv.org/pdf/2401.17013v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Category-wise Fine-Tuning: Resisting Incorrect Pseudo-Labels in Multi-Label Image Classification with Partial Labels**<br />
**Title_cn:** 按类别微调：在具有部分标签的多标签图像分类中抵抗不正确的伪标签<br />
**Authors:** Chak Fong Chong, Xinyi Fang, Jielong Guo, Yapeng Wang, Wei Ke, Chan-Tong Lam, Sio-Kei Im<br />
**Abstract:** <details><summary>原文: </summary>Large-scale image datasets are often partially labeled, where only a few categories' labels are known for each image. Assigning pseudo-labels to unknown labels to gain additional training signals has become prevalent for training deep classification models. However, some pseudo-labels are inevitably incorrect, leading to a notable decline in the model classification performance. In this paper, we propose a novel method called Category-wise Fine-Tuning (CFT), aiming to reduce model inaccuracies caused by the wrong pseudo-labels. In particular, CFT employs known labels without pseudo-labels to fine-tune the logistic regressions of trained models individually to calibrate each category's model predictions. Genetic Algorithm, seldom used for training deep models, is also utilized in CFT to maximize the classification performance directly. CFT is applied to well-trained models, unlike most existing methods that train models from scratch. Hence, CFT is general and compatible with models trained with different methods and schemes, as demonstrated through extensive experiments. CFT requires only a few seconds for each category for calibration with consumer-grade GPUs. We achieve state-of-the-art results on three benchmarking datasets, including the CheXpert chest X-ray competition dataset (ensemble mAUC 93.33%, single model 91.82%), partially labeled MS-COCO (average mAP 83.69%), and Open Image V3 (mAP 85.31%), outperforming the previous bests by 0.28%, 2.21%, 2.50%, and 0.91%, respectively. The single model on CheXpert has been officially evaluated by the competition server, endorsing the correctness of the result. The outstanding results and generalizability indicate that CFT could be substantial and prevalent for classification model development. Code is available at: https://github.com/maxium0526/category-wise-fine-tuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型图像数据集通常是部分标记的，其中每个图像只知道几个类别的标签。将伪标签分配给未知标签以获得额外的训练信号已成为训练深度分类模型的普遍做法。然而，一些伪标签不可避免地是错误的，导致模型分类性能显着下降。在本文中，我们提出了一种称为类别微调（CFT）的新方法，旨在减少由错误伪标签引起的模型不准确。特别是，CFT 使用没有伪标签的已知标签来单独微调训练模型的逻辑回归，以校准每个类别的模型预测。很少用于训练深度模型的遗传算法也被用于 CFT 中，以直接最大化分类性能。 CFT 适用于训练有素的模型，这与大多数从头开始训练模型的现有方法不同。因此，正如大量实验所证明的那样，CFT 是通用的，并且与使用不同方法和方案训练的模型兼容。对于每个类别，CFT 只需几秒钟即可使用消费级 GPU 进行校准。我们在三个基准数据集上取得了最先进的结果，包括 CheXpert 胸部 X 射线竞赛数据集（整体 mAUC 93.33%，单一模型 91.82%）、部分标记的 MS-COCO（平均 mAP 83.69%）和 Open图像 V3 (mAP 85.31%)，分别比之前的最佳值高出 0.28%、2.21%、2.50% 和 0.91%。 CheXpert上的单一模型已经通过竞赛服务器的正式评估，认可了结果的正确性。出色的结果和普遍性表明 CFT 对于分类模型的开发来说可能是重要且普遍的。代码位于：https://github.com/maxium0526/category-wise-fine-tuning。</details>
**PDF:** <http://arxiv.org/pdf/2401.16991v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning**<br />
**Title_cn:** 使用深度学习对浸渍纤维和血管进行分割和表征<br />
**Authors:** Saqib Qamar, Abu Imran Baba, Stéphane Verger, Magnus Andersson<br />
**Abstract:** <details><summary>原文: </summary>Purpose: Wood comprises different cell types, such as fibers and vessels, defining its properties. Studying their shape, size, and arrangement in microscopic images is crucial for understanding wood samples. Typically, this involves macerating (soaking) samples in a solution to separate cells, then spreading them on slides for imaging with a microscope that covers a wide area, capturing thousands of cells. However, these cells often cluster and overlap in images, making the segmentation difficult and time-consuming using standard image-processing methods. Results: In this work, we develop an automatic deep learning segmentation approach that utilizes the one-stage YOLOv8 model for fast and accurate fiber and vessel segmentation and characterization in microscopy images. The model can analyze 32640 x 25920 pixels images and demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95 of 78 %. To assess the model's robustness, we examined fibers from a genetically modified tree line known for longer fibers. The outcomes were comparable to previous manual measurements. Additionally, we created a user-friendly web application for image analysis and provided the code for use on Google Colab. Conclusion: By leveraging YOLOv8's advances, this work provides a deep learning solution to enable efficient quantification and analysis of wood cells suitable for practical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：木材由不同的细胞类型组成，例如纤维和导管，从而决定了其特性。研究它们在显微图像中的形状、大小和排列对于理解木材样本至关重要。通常，这涉及将样品浸渍（浸泡）在溶液中以分离细胞，然后将它们铺在载玻片上，用覆盖大面积的显微镜成像，捕获数千个细胞。然而，这些细胞经常在图像中聚集和重叠，使得使用标准图像处理方法进行分割变得困难且耗时。结果：在这项工作中，我们开发了一种自动深度学习分割方法，该方法利用单阶段 YOLOv8 模型在显微镜图像中快速准确地进行纤维和血管分割和表征。该模型可以分析 32640 x 25920 像素图像并展示有效的细胞检测和分割，实现 78% 的 mAP_0.5-0.95。为了评估模型的稳健性，我们检查了来自以较长纤维闻名的转基因树系的纤维。结果与之前的手动测量相当。此外，我们还创建了一个用户友好的网络应用程序用于图像分析，并提供了在 Google Colab 上使用的代码。结论：通过利用 YOLOv8 的进步，这项工作提供了一种深度学习解决方案，可以有效地量化和分析适合实际应用的木材细胞。</details>
**PDF:** <http://arxiv.org/pdf/2401.16937v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Dynamic MRI reconstruction using low-rank plus sparse decomposition with smoothness regularization**<br />
**Title_cn:** 使用低秩加稀疏分解和平滑正则化进行动态 MRI 重建<br />
**Authors:** Chee-Ming Ting, Fuad Noman, Raphaël C. -W. Phan, Hernando Ombao<br />
**Abstract:** <details><summary>原文: </summary>The low-rank plus sparse (L+S) decomposition model has enabled better reconstruction of dynamic magnetic resonance imaging (dMRI) with separation into background (L) and dynamic (S) component. However, use of low-rank prior alone may not fully explain the slow variations or smoothness of the background part at the local scale. In this paper, we propose a smoothness-regularized L+S (SR-L+S) model for dMRI reconstruction from highly undersampled k-t-space data. We exploit joint low-rank and smooth priors on the background component of dMRI to better capture both its global and local temporal correlated structures. Extending the L+S formulation, the low-rank property is encoded by the nuclear norm, while the smoothness by a general \ell_{p}-norm penalty on the local differences of the columns of L. The additional smoothness regularizer can promote piecewise local consistency between neighboring frames. By smoothing out the noise and dynamic activities, it allows accurate recovery of the background part, and subsequently more robust dMRI reconstruction. Extensive experiments on multi-coil cardiac and synthetic data shows that the SR-L+S model outp</details>
**Abstract_cn:** <details><summary>译文: </summary>低秩加稀疏 (L+S) 分解模型能够更好地重建动态磁共振成像 (dMRI)，并将其分离为背景 (L) 和动态 (S) 分量。然而，单独使用低秩先验可能无法完全解释背景部分在局部尺度上的缓慢变化或平滑度。在本文中，我们提出了一种平滑正则化 L+S (SR-L+S) 模型，用于从高度欠采样的 k-t 空间数据进行 dMRI 重建。我们利用 dMRI 背景成分的联合低秩和平滑先验来更好地捕获其全局和局部时间相关结构。扩展 L+S 公式，低秩属性由核范数编码，而平滑度则通过对 L 列的局部差异进行一般 \ell_{p}-范数惩罚。附加的平滑度正则化器可以分段提升相邻帧之间的局部一致性。通过平滑噪声和动态活动，它可以准确恢复背景部分，并随后进行更稳健的 dMRI 重建。对多线圈心脏和合成数据的大量实验表明，SR-L+S 模型优于</details>
**PDF:** <http://arxiv.org/pdf/2401.16928v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **A Tournament of Transformation Models: B-Spline-based vs. Mesh-based Multi-Objective Deformable Image Registration**<br />
**Title_cn:** 变换模型锦标赛：基于 B 样条与基于网格的多目标可变形图像配准<br />
**Authors:** Georgios Andreadis, Joas I. Mulder, Anton Bouter, Peter A. N. Bosman, Tanja Alderliesten<br />
**Abstract:** <details><summary>原文: </summary>The transformation model is an essential component of any deformable image registration approach. It provides a representation of physical deformations between images, thereby defining the range and realism of registrations that can be found. Two types of transformation models have emerged as popular choices: B-spline models and mesh models. Although both models have been investigated in detail, a direct comparison has not yet been made, since the models are optimized using very different optimization methods in practice. B-spline models are predominantly optimized using gradient-descent methods, while mesh models are typically optimized using finite-element method solvers or evolutionary algorithms. Multi-objective optimization methods, which aim to find a diverse set of high-quality trade-off registrations, are increasingly acknowledged to be important in deformable image registration. Since these methods search for a diverse set of registrations, they can provide a more complete picture of the capabilities of different transformation models, making them suitable for a comparison of models. In this work, we conduct the first direct comparison between B-spline and mesh transformation models, by optimizing both models with the same state-of-the-art multi-objective optimization method, the Multi-Objective Real-Valued Gene-pool Optimal Mixing Evolutionary Algorithm (MO-RV-GOMEA). The combination with B-spline transformation models, moreover, is novel. We experimentally compare both models on two different registration problems that are both based on pelvic CT scans of cervical cancer patients, featuring large deformations. Our results, on three cervical cancer patients, indicate that the choice of transformation model can have a profound impact on the diversity and quality of achieved registration outcomes.</details>
**Abstract_cn:** <details><summary>译文: </summary>变换模型是任何可变形图像配准方法的重要组成部分。它提供了图像之间物理变形的表示，从而定义了可以找到的配准的范围和真实度。两种类型的变换模型已成为流行的选择：B 样条模型和网格模型。尽管对这两个模型进行了详细研究，但尚未进行直接比较，因为这两个模型在实践中使用非常不同的优化方法进行优化。 B 样条模型主要使用梯度下降方法进行优化，而网格模型通常使用有限元法求解器或进化算法进行优化。多目标优化方法旨在寻找多种高质量的权衡配准，人们越来越认识到它在变形图像配准中的重要性。由于这些方法搜索不同的注册集，因此它们可以更全面地了解不同转换模型的功能，从而适合模型比较。在这项工作中，我们通过使用相同的最先进的多目标优化方法（多目标实值基因库最优）来优化这两个模型，对 B 样条模型和网格变换模型进行了首次直接比较混合进化算法（MO-RV-GOMEA）。此外，与 B 样条变换模型的结合也是新颖的。我们在两个不同的配准问题上对这两种模型进行了实验比较，这两个问题都基于宫颈癌患者的盆腔 CT 扫描，具有较大的变形特征。我们对三名宫颈癌患者的结果表明，转化模式的选择可以对所实现的注册结果的多样性和质量产生深远的影响。</details>
**PDF:** <http://arxiv.org/pdf/2401.16867v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **MESA: Matching Everything by Segmenting Anything**<br />
**Title_cn:** MESA：通过分割任何内容来匹配所有内容<br />
**Authors:** Yesheng Zhang, Xu Zhao<br />
**Abstract:** <details><summary>原文: </summary>Feature matching is a crucial task in the field of computer vision, which involves finding correspondences between images. Previous studies achieve remarkable performance using learning-based feature comparison. However, the pervasive presence of matching redundancy between images gives rise to unnecessary and error-prone computations in these methods, imposing limitations on their accuracy. To address this issue, we propose MESA, a novel approach to establish precise area (or region) matches for efficient matching redundancy reduction. MESA first leverages the advanced image understanding capability of SAM, a state-of-the-art foundation model for image segmentation, to obtain image areas with implicit semantic. Then, a multi-relational graph is proposed to model the spatial structure of these areas and construct their scale hierarchy. Based on graphical models derived from the graph, the area matching is reformulated as an energy minimization task and effectively resolved. Extensive experiments demonstrate that MESA yields substantial precision improvement for multiple point matchers in indoor and outdoor downstream tasks, e.g. +13.61% for DKM in indoor pose estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>特征匹配是计算机视觉领域的一项关键任务，涉及寻找图像之间的对应关系。先前的研究使用基于学习的特征比较取得了显着的性能。然而，图像之间普遍存在的匹配冗余导致这些方法中出现不必要且容易出错的计算，从而限制了其准确性。为了解决这个问题，我们提出了 MESA，一种建立精确区域（或区域）匹配以有效减少匹配冗余的新方法。 MESA首先利用SAM（一种最先进的图像分割基础模型）的高级图像理解能力来获取具有隐式语义的图像区域。然后，提出了一个多关系图来对这些区域的空间结构进行建模并构建它们的尺度层次结构。基于从图导出的图模型，区域匹配被重新表述为能量最小化任务并得到有效解决。大量实验表明，MESA 可以显着提高室内和室外下游任务（例如，多点匹配器）的精度。 DKM 在室内姿态估计中+13.61%。</details>
**PDF:** <http://arxiv.org/pdf/2401.16741v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Optimal-Landmark-Guided Image Blending for Face Morphing Attacks**<br />
**Title_cn:** 用于面部变形攻击的最佳地标引导图像混合<br />
**Authors:** Qiaoyun He, Zongyong Deng, Zuyuan He, Qijun Zhao<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose a novel approach for conducting face morphing attacks, which utilizes optimal-landmark-guided image blending. Current face morphing attacks can be categorized into landmark-based and generation-based approaches. Landmark-based methods use geometric transformations to warp facial regions according to averaged landmarks but often produce morphed images with poor visual quality. Generation-based methods, which employ generation models to blend multiple face images, can achieve better visual quality but are often unsuccessful in generating morphed images that can effectively evade state-of-the-art face recognition systems~(FRSs). Our proposed method overcomes the limitations of previous approaches by optimizing the morphing landmarks and using Graph Convolutional Networks (GCNs) to combine landmark and appearance features. We model facial landmarks as nodes in a bipartite graph that is fully connected and utilize GCNs to simulate their spatial and structural relationships. The aim is to capture variations in facial shape and enable accurate manipulation of facial appearance features during the warping process, resulting in morphed facial images that are highly realistic and visually faithful. Experiments on two public datasets prove that our method inherits the advantages of previous landmark-based and generation-based methods and generates morphed images with higher quality, posing a more significant threat to state-of-the-art FRSs.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种进行面部变形攻击的新方法，该方法利用最佳地标引导图像混合。当前的面部变形攻击可以分为基于地标和基于生成的方法。基于地标的方法使用几何变换根据平均地标来扭曲面部区域，但通常会产生视觉质量较差的变形图像。基于生成的方法采用生成模型来混合多个人脸图像，可以实现更好的视觉质量，但通常无法成功生成可以有效躲避最先进的人脸识别系统（FRS）的变形图像。我们提出的方法通过优化变形地标并使用图卷积网络（GCN）结合地标和外观特征来克服以前方法的局限性。我们将面部标志建模为完全连接的二分图中的节点，并利用 GCN 来模拟它们的空间和结构关系。其目的是捕捉面部形状的变化，并在变形过程中准确操纵面部外观特征，从而产生高度逼真和视觉忠实的变形面部图像。在两个公共数据集上的实验证明，我们的方法继承了之前基于地标和基于生成的方法的优点，并生成了更高质量的变形图像，对最先进的 FRS 构成了更重大的威胁。</details>
**PDF:** <http://arxiv.org/pdf/2401.16722v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **LF Tracy: A Unified Single-Pipeline Approach for Salient Object Detection in Light Field Cameras**<br />
**Title_cn:** LF Tracy：用于光场相机中显着物体检测的统一单管道方法<br />
**Authors:** Fei Teng, Jiaming Zhang, Jiawei Liu, Kunyu Peng, Xina Cheng, Zhiyong Li, Kailun Yang<br />
**Abstract:** <details><summary>原文: </summary>Leveraging the rich information extracted from light field (LF) cameras is instrumental for dense prediction tasks. However, adapting light field data to enhance Salient Object Detection (SOD) still follows the traditional RGB methods and remains under-explored in the community. Previous approaches predominantly employ a custom two-stream design to discover the implicit angular feature within light field cameras, leading to significant information isolation between different LF representations. In this study, we propose an efficient paradigm (LF Tracy) to address this limitation. We eschew the conventional specialized fusion and decoder architecture for a dual-stream backbone in favor of a unified, single-pipeline approach. This comprises firstly a simple yet effective data augmentation strategy called MixLD to bridge the connection of spatial, depth, and implicit angular information under different LF representations. A highly efficient information aggregation (IA) module is then introduced to boost asymmetric feature-wise information fusion. Owing to this innovative approach, our model surpasses the existing state-of-the-art methods, particularly demonstrating a 23% improvement over previous results on the latest large-scale PKU dataset. By utilizing only 28.9M parameters, the model achieves a 10% increase in accuracy with 3M additional parameters compared to its backbone using RGB images and an 86% rise to its backbone using LF images. The source code will be made publicly available at https://github.com/FeiBryantkit/LF-Tracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用从光场 (LF) 相机提取的丰富信息有助于执行密集的预测任务。然而，采用光场数据来增强显着目标检测 (SOD) 仍然遵循传统的 RGB 方法，并且在社区中仍未得到充分探索。以前的方法主要采用定制的双流设计来发现光场相机内隐式的角度特征，从而导致不同的 LF 表示之间存在显着的信息隔离。在本研究中，我们提出了一种有效的范式（LF Tracy）来解决这一限制。我们避开了双流主干的传统专用融合和解码器架构，转而采用统一的单管道方法。这首先包括一个简单而有效的数据增强策略，称为 MixLD，以桥接不同 LF 表示下的空间、深度和隐式角度信息的连接。然后引入高效的信息聚合（IA）模块来促进非对称特征信息融合。由于这种创新方法，我们的模型超越了现有的最先进的方法，特别是在最新的大规模 PKU 数据集上比之前的结果提高了 23%。通过仅使用 2890 万个参数，与使用 RGB 图像的主干相比，该模型在增加 300 万个额外参数的情况下，准确率提高了 10%，而使用 LF 图像的主干则提高了 86%。源代码将在 https://github.com/FeiBryantkit/LF-Tracy 公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.16712v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **EdgeOL: Efficient in-situ Online Learning on Edge Devices**<br />
**Title_cn:** EdgeOL：边缘设备上的高效原位在线学习<br />
**Authors:** Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang<br />
**Abstract:** <details><summary>原文: </summary>Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人辅助老年护理和物体识别等新兴应用通常采用深度学习神经网络 (DNN) 模型，自然需要：i) 处理流式推理请求，ii) 适应可能的部署场景变化。在线模型微调被广泛采用来满足这些需求。然而，微调涉及大量能源消耗，使得在边缘设备上部署具有挑战性。在本文中，我们提出了 EdgeOL，这是一种边缘在线学习框架，可通过互调和内调优化来优化推理精度、微调执行时间和能源效率。实验结果表明，与即时在线学习策略相比，EdgeOL 平均降低整体微调执行时间 82%，能耗降低 74%，平均推理精度提高 1.70%。</details>
**PDF:** <http://arxiv.org/pdf/2401.16694v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Characterization of Magnetic Labyrinthine Structures through Junctions and Terminals Detection using Template Matching and CNN**<br />
**Title_cn:** 使用模板匹配和 CNN 通过连接和终端检测来表征磁性迷宫结构<br />
**Authors:** Vinícius Yu Okubo, Kotaro Shimizu, B. S. Shivaram, Hae Yong Kim<br />
**Abstract:** <details><summary>原文: </summary>In material sciences, characterizing faults in periodic structures is vital for understanding material properties. To characterize magnetic labyrinthine patterns, it is necessary to accurately identify junctions and terminals, often featuring over a thousand closely packed defects per image. This study introduces a new technique called TM-CNN (Template Matching - Convolutional Neural Network) designed to detect a multitude of small objects in images, such as defects in magnetic labyrinthine patterns. TM-CNN was used to identify these structures in 444 experimental images, and the results were explored to deepen the understanding of magnetic materials. It employs a two-stage detection approach combining template matching, used in initial detection, with a convolutional neural network, used to eliminate incorrect identifications. To train a CNN classifier, it is necessary to create a large number of training images. This difficulty prevents the use of CNN in many practical applications. TM-CNN significantly reduces the manual workload for creating training images by automatically making most of the annotations and leaving only a small number of corrections to human reviewers. In testing, TM-CNN achieved an impressive F1 score of 0.988, far outperforming traditional template matching and CNN-based object detection algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>在材料科学中，表征周期性结构中的断层对于理解材料特性至关重要。为了表征磁性迷宫图案，必须准确识别结点和端子，每张图像通常具有一千多个紧密堆积的缺陷。这项研究引入了一种名为 TM-CNN（模板匹配 - 卷积神经网络）的新技术，旨在检测图像中的大量小物体，例如磁性迷宫图案中的缺陷。使用TM-CNN在444张实验图像中识别这些结构，并探索结果以加深对磁性材料的理解。它采用两阶段检测方法，将初始检测中使用的模板匹配与用于消除错误识别的卷积神经网络相结合。为了训练 CNN 分类器，需要创建大量训练图像。这一困难阻碍了 CNN 在许多实际应用中的使用。 TM-CNN 通过自动进行大部分注释并仅将少量修正留给人工审阅者，显着减少了创建训练图像的手动工作量。在测试中，TM-CNN 取得了令人印象深刻的 F1 分数 0.988，远远优于传统的模板匹配和基于 CNN 的目标检测算法。</details>
**PDF:** <http://arxiv.org/pdf/2401.16688v1><br />
**Code:** null<br />

