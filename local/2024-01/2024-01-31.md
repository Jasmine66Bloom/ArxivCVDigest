## [UPDATED!] **2024-01-31** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **Improved Scene Landmark Detection for Camera Localization**<br />
**Title_cn:** 改进了相机定位的场景地标检测<br />
**Authors:** Tien Do, Sudipta N. Sinha<br />
**Abstract:** <details><summary>原文: </summary>Camera localization methods based on retrieval, local feature matching, and 3D structure-based pose estimation are accurate but require high storage, are slow, and are not privacy-preserving. A method based on scene landmark detection (SLD) was recently proposed to address these limitations. It involves training a convolutional neural network (CNN) to detect a few predetermined, salient, scene-specific 3D points or landmarks and computing camera pose from the associated 2D-3D correspondences. Although SLD outperformed existing learning-based approaches, it was notably less accurate than 3D structure-based methods. In this paper, we show that the accuracy gap was due to insufficient model capacity and noisy labels during training. To mitigate the capacity issue, we propose to split the landmarks into subgroups and train a separate network for each subgroup. To generate better training labels, we propose using dense reconstructions to estimate visibility of scene landmarks. Finally, we present a compact architecture to improve memory efficiency. Accuracy wise, our approach is on par with state of the art structure based methods on the INDOOR-6 dataset but runs significantly faster and uses less storage. Code and models can be found at https://github.com/microsoft/SceneLandmarkLocalization.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于检索、局部特征匹配和基于 3D 结构的姿态估计的相机定位方法虽然准确，但需要高存储空间、速度慢且不保护隐私。最近提出了一种基于场景地标检测（SLD）的方法来解决这些限制。它涉及训练卷积神经网络 (CNN) 来检测一些预先确定的、显着的、特定于场景的 3D 点或地标，并根据相关的 2D-3D 对应关系计算相机姿势。尽管 SLD 的性能优于现有的基于学习的方法，但它的准确度明显低于基于 3D 结构的方法。在本文中，我们表明准确性差距是由于训练期间模型容量不足和噪声标签造成的。为了缓解容量问题，我们建议将地标分成子组，并为每个子组训练一个单独的网络。为了生成更好的训练标签，我们建议使用密集重建来估计场景地标的可见性。最后，我们提出了一种紧凑的架构来提高内存效率。在准确性方面，我们的方法与 INDOOR-6 数据集上最先进的基于结构的方法相当，但运行速度明显更快，并且使用的存储空间更少。代码和模型可以在 https://github.com/microsoft/SceneLandmarkLocalization 找到。</details>
**PDF:** <http://arxiv.org/pdf/2401.18083v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition**<br />
**Title_cn:** 基于骨架的动作识别的连续图学习的基准敏感性<br />
**Authors:** Wei Wei, Tom De Schepper, Kevin Mets<br />
**Abstract:** <details><summary>原文: </summary>Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task-order robust methods can still be class-order sensitive and observe results that contradict previous empirical observations on architectural sensitivity in CL.</details>
**Abstract_cn:** <details><summary>译文: </summary>持续学习（CL）是一个旨在构建机器学习模型的研究领域，该模型可以在不同的任务上不断积累知识，而无需从头开始重新训练。之前的研究表明，预训练图神经网络（GNN）在微调后可能会导致负迁移（Hu et al., 2020），这一设置与 CL 密切相关。因此，我们专注于在连续图学习（CGL）环境中研究 GNN。我们提出了第一个时空图的连续图学习基准，并用它在这种新颖的环境中对著名的 CGL 方法进行基准测试。该基准测试基于 N-UCLA 和 NTU-RGB+D 数据集，用于基于骨架的动作识别。除了标准性能指标的基准测试之外，我们还研究了 CGL 方法的类和任务顺序敏感性，即学习顺序对每个类/任务性能的影响，以及具有不同宽度和深度的骨干 GNN 的 CGL 方法的架构敏感性。我们发现，任务顺序鲁棒方法仍然可以对类顺序敏感，并且观察到的结果与之前对 CL 架构敏感性的经验观察相矛盾。</details>
**PDF:** <http://arxiv.org/pdf/2401.18054v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multilinear Operator Networks**<br />
**Title_cn:** 多线性算子网络<br />
**Authors:** Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管深度神经网络在图像识别方面具有非凡的能力，但对激活函数的依赖仍然是一个很大程度上未被探索的领域，并且尚未消除。另一方面，多项式网络是一类不需要激活函数的模型，但尚未达到与现代架构相当的性能。在这项工作中，我们的目标是缩小这一差距并提出 MONet，它仅依赖于多线性算子。 MONet 的核心层称为 Mu-Layer，捕获输入令牌元素的乘法交互。 MONet 捕获输入元素的高度交互，我们在一系列图像识别和科学计算基准测试中展示了我们的方法的有效性。所提出的模型优于先前的多项式网络，并且与现代架构的性能相当。我们相信 MONet 可以激发对完全使用多线性运算的模型的进一步研究。</details>
**PDF:** <http://arxiv.org/pdf/2401.17992v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Shrub of a thousand faces: an individual segmentation from satellite images using deep learning**<br />
**Title_cn:** 千面灌木：使用深度学习对卫星图像进行单独分割<br />
**Authors:** Rohaifa Khaldi, Siham Tabik, Sergio Puertas-Ruiz, Julio Peñas de Giles, José Antonio Hódar Correa, Regino Zamora, Domingo Alcaraz Segura<br />
**Abstract:** <details><summary>原文: </summary>Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model. We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance. Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods. They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty. Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>监测长寿灌木（如杜松）的分布和大小结构，可用于估计气候变化对高山和高纬度生态系统的长期影响。历史航空超高分辨率图像提供了一种回顾性工具，可以高精度监测灌木生长和分布。目前，深度学习模型在检测和描绘具有定义形状的物体轮廓方面提供了令人印象深刻的结果。然而，调整这些模型来检测表达复杂生长模式的自然物体（例如杜松）仍然是一项具有挑战性的任务。这项研究提出了一种新颖的方法，利用遥感 RGB 图像与基于 Mask R-CNN 的实例分割模型相结合，单独描绘内华达山脉（西班牙）树线上方的杜松灌木。在本研究中，我们提出了一种新的数据构建设计，其中包括使用照片解释（PI）和现场工作（FW）数据分别开发和外部验证模型。我们还提出了一种新的灌木定制评估算法，该算法基于称为地面真实区域多重交叉点（MIoGTA）的新指标，以评估和优化模型灌木描绘性能。最后，我们首次部署开发的模型来生成杜松个体的全面地图。实验结果证明了我们的双数据构建方法在克服与传统现场调查方法相关的局限性方面的效率。他们还强调了 MIoGTA 指标在评估具有复杂生长模式的物种的实例分割模型时的稳健性，显示出针对数据注释不确定性的更强弹性。此外，他们还展示了使用带有 ResNet101-C4 主干的 Mask R-CNN 来描绘 PI 和 FW 灌木的有效性，分别实现了 87.87% 和 76.86% 的 F1 分数。</details>
**PDF:** <http://arxiv.org/pdf/2401.17985v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study**<br />
**Title_cn:** 使用视觉检测模型增强多模态大语言模型：实证研究<br />
**Authors:** Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen<br />
**Abstract:** <details><summary>原文: </summary>Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管多模态大语言模型（MLLM）在集成文本和图像模态方面具有令人印象深刻的能力，但在准确解释详细的视觉元素方面仍然存在挑战。本文提出了一项关于通过最先进的 (SOTA) 对象检测和光学字符识别模型增强 MLLM 的实证研究，以提高细粒度图像理解并减少响应中的幻觉。我们的研究调查了基于嵌入的检测信息注入、这种注入对 MLLM 原始能力的影响以及检测模型的互换性。我们对 LLaVA-1.5、DINO 和 PaddleOCRv2 ​​等模型进行了系统实验，结果表明我们的方法不仅提高了 MLLM 在特定视觉任务中的性能，而且保持了其原有的优势。由此产生的增强型 MLLM 在 10 个基准测试中的 9 个上优于 SOTA 模型，在标准化平均分数上实现了高达 12.99% 的改进，标志着多模态理解方面的显着进步。我们发布代码以促进进一步探索 MLLM 的细粒度多模态对话功能。</details>
**PDF:** <http://arxiv.org/pdf/2401.17981v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **MelNet: A Real-Time Deep Learning Algorithm for Object Detection**<br />
**Title_cn:** MelNet：一种用于目标检测的实时深度学习算法<br />
**Authors:** Yashar Azadvatan, Murat Kurt<br />
**Abstract:** <details><summary>原文: </summary>In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that of other pre-trained models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，引入了一种新颖的目标检测深度学习算法，称为 MelNet。 MelNet 使用 KITTI 数据集进行了目标检测训练。经过 300 个训练周期后，MelNet 的 mAP（平均精度）得分为 0.732。此外，三个替代模型 - YOLOv5、EfficientDet 和 Faster-RCNN-MobileNetv3 - 在 KITTI 数据集上进行训练，并与 MelNet 并列进行对象检测。结果强调了在某些情况下采用迁移学习的有效性。值得注意的是，在著名数据集（例如 ImageNet、COCO 和 Pascal VOC）上训练的现有模型产生了优异的结果。另一项发现强调了创建针对特定场景的新模型并在特定数据集上对其进行训练的可行性。这项调查表明，仅在 KITTI 数据集上训练 MelNet 在 150 个 epoch 后也超过了 EfficientDet。因此，训练后，MelNet 的性能与其他预训练模型的性能非常接近。</details>
**PDF:** <http://arxiv.org/pdf/2401.17972v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **HyperZ$\cdot$Z$\cdot$W Operator Connects Slow-Fast Networks for Full Context Interaction**<br />
**Title_cn:** HyperZ$\cdot$Z$\cdot$W 运算符连接慢速网络以实现全上下文交互<br />
**Authors:** Harvie Zhang<br />
**Abstract:** <details><summary>原文: </summary>The self-attention mechanism utilizes large implicit weight matrices, programmed through dot product-based activations with very few trainable parameters, to enable long sequence modeling. In this paper, we investigate the possibility of discarding residual learning by employing large implicit kernels to achieve full context interaction at each layer of the network. To accomplish it, we introduce coordinate-based implicit MLPs as a slow network to generate hyper-kernels for another fast convolutional network. To get context-varying weights for fast dynamic encoding, we propose a $\mathrm{Hyper}\mathcal{Z{\cdot}Z{\cdot}W}$ operator that connects hyper-kernels ($\mathcal{W}$) and hidden activations ($\mathcal{Z}$) through simple elementwise multiplication, followed by convolution of $\mathcal{Z}$ using the context-dependent $\mathcal{W}$. Based on this design, we present a novel Terminator architecture that integrates hyper-kernels of different sizes to produce multi-branch hidden representations for enhancing the feature extraction capability of each layer. Additionally, a bottleneck layer is employed to compress the concatenated channels, allowing only valuable information to propagate to the subsequent layers. Notably, our model incorporates several innovative components and exhibits excellent properties, such as introducing local feedback error for updating the slow network, stable zero-mean features, faster training convergence, and fewer model parameters. Extensive experimental results on pixel-level 1D and 2D image classification benchmarks demonstrate the superior performance of our architecture.</details>
**Abstract_cn:** <details><summary>译文: </summary>自注意力机制利用大型隐式权重矩阵，通过基于点积的激活和很少的可训练参数进行编程，以实现长序列建模。在本文中，我们研究了通过采用大型隐式内核来放弃残差学习的可能性，以在网络的每一层实现完整的上下文交互。为了实现这一目标，我们引入基于坐标的隐式 MLP 作为慢速网络，为另一个快速卷积网络生成超内核。为了获得快速动态编码的上下文变化权重，我们提出了一个连接超内核的 $\mathrm{Hyper}\mathcal{Z{\cdot}Z{\cdot}W}$ 运算符 ($\mathcal{W}$ ）和隐藏激活（$\mathcal{Z}$）通过简单的元素乘法，然后使用上下文相关的$\mathcal{W}$进行$\mathcal{Z}$的卷积。基于此设计，我们提出了一种新颖的终结者架构，该架构集成了不同大小的超内核以产生多分支隐藏表示，以增强每层的特征提取能力。此外，还采用瓶颈层来压缩级联通道，只允许有价值的信息传播到后续层。值得注意的是，我们的模型结合了多个创新组件，并表现出优异的特性，例如引入局部反馈误差来更新慢速网络、稳定的零均值特征、更快的训练收敛和更少的模型参数。像素级一维和二维图像分类基准的大量实验结果证明了我们的架构的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.17948v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Source-free Domain Adaptive Object Detection in Remote Sensing Images**<br />
**Title_cn:** 遥感图像中的无源域自适应目标检测<br />
**Authors:** Weixing Liu, Jun Liu, Xin Su, Han Nie, Bin Luo<br />
**Abstract:** <details><summary>原文: </summary>Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images. However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process. This setting is often impractical in the real world due to RS data privacy and transmission difficulty. To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model. We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment. The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias. The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels. By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features. Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images. Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well. Our code will be available at: https://weixliu.github.io/ .</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究使用无监督域自适应目标检测（UDAOD）方法来弥合遥感（RS）图像中的域差距。然而，UDAOD方法通常假设源域数据可以在域适应过程中被访问。由于RS数据隐私和传输困难，这种设置在现实世界中通常不切实际。为了应对这一挑战，我们提出了一种实用的 RS 图像无源对象检测（SFOD）设置，其目的是仅使用源预训练模型来执行目标域自适应。我们提出了一种新的 RS 图像 SFOD 方法，由两部分组成：扰动域生成和对齐。所提出的多级扰动通过根据颜色和风格偏差在图像级别和特征级别扰动域变体特征，以简单而有效的形式构造扰动域。所提出的多级对齐计算师生网络中扰动域和目标域之间的特征和标签一致性，并引入特征原型的蒸馏来减轻伪标签的噪声。通过要求检测器在扰动域和目标域中保持一致，检测器被迫关注域不变特征。三个合成真实实验和三个跨传感器实验的广泛结果验证了我们的方法的有效性，该方法不需要访问源域遥感图像。此外，计算机视觉数据集上的实验表明我们的方法也可以扩展到其他领域。我们的代码将在以下位置提供：https://weixliu.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2401.17916v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation**<br />
**Title_cn:** Hi-SAM：结合 Segment Anything 模型进行分层文本分割<br />
**Authors:** Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu, Bo Du, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>The Segment Anything Model (SAM), a profound vision foundation model pre-trained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in text segmentation across four hierarchies, including stroke, word, text-line, and paragraph, while realizing layout analysis as well. Specifically, we first turn SAM into a high-quality text stroke segmentation (TSS) model through a parameter-efficient fine-tuning approach. We use this TSS model to iteratively generate the text stroke labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TSS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the promptable mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for text stroke segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring 20x fewer training epochs. The code is available at https://github.com/ymy-k/Hi-SAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>Segment Anything Model (SAM) 是一种在大规模数据集上预训练的深度视觉基础模型，打破了一般分割的界限，并激发了各种下游应用。本文介绍了 Hi-SAM，这是一种利用 SAM 进行分层文本分割的统一模型。 Hi-SAM 擅长跨四个层次的文本分割，包括笔画、单词、文本行和段落，同时还实现布局分析。具体来说，我们首先通过参数高效的微调方法将 SAM 转变为高质量的文本笔划分割（TSS）模型。我们使用此 TSS 模型以半自动方式迭代生成文本笔划标签，统一 HierText 数据集中四个文本层次结构的标签。随后，有了这些完整的标签，我们推出了基于 TSS 架构和定制分层掩模解码器的端到端可训练 Hi-SAM。在推理过程中，Hi-SAM 提供自动掩模生成（AMG）模式和提示分割模式。就AMG模式而言，Hi-SAM首先对文本笔画前景掩模进行分割，然后对前景点进行采样以进行分层文本掩模生成，并顺便实现布局分析。至于提示模式，Hi-SAM 通过单击即可提供单词、文本行和段落掩码。实验结果显示了我们的 TSS 模型的最先进性能：对于文本笔划分割，Total-Text 上的 fgIOU 为 84.86%，TextSeg 上的 fgIOU 为 88.96%。此外，与之前在 HierText 上进行联合分层检测和布局分析的专家相比，Hi-SAM 取得了显着的改进：在文本行级别上提高了 4.73% PQ 和 5.39% F1，在段落级别布局上提高了 5.49% PQ 和 7.39% F1分析，需要的训练次数减少了 20 倍。代码可在 https://github.com/ymy-k/Hi-SAM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17904v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition**<br />
**Title_cn:** PVLR：用于多标签图像识别的提示驱动的视觉语言表示学习<br />
**Authors:** Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei<br />
**Abstract:** <details><summary>原文: </summary>Multi-label image recognition is a fundamental task in computer vision. Recently, vision-language models have made notable advancements in this area. However, previous methods often failed to effectively leverage the rich knowledge within language models and instead incorporated label semantics into visual features in a unidirectional manner. In this paper, we propose a Prompt-driven Visual-Linguistic Representation Learning (PVLR) framework to better leverage the capabilities of the linguistic modality. In PVLR, we first introduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP) and Context-Aware Prompting (CAP). KAP utilizes fixed prompts to capture the intrinsic semantic knowledge and relationships across all labels, while CAP employs learnable prompts to capture context-aware label semantics and relationships. Later, we propose an Interaction and Fusion Module (IFM) to interact and fuse the representations obtained from KAP and CAP. In contrast to the unidirectional fusion in previous works, we introduce a Dual-Modal Attention (DMA) that enables bidirectional interaction between textual and visual features, yielding context-aware label representations and semantic-related visual representations, which are subsequently used to calculate similarities and generate final predictions for all labels. Extensive experiments on three popular datasets including MS-COCO, Pascal VOC 2007, and NUS-WIDE demonstrate the superiority of PVLR.</details>
**Abstract_cn:** <details><summary>译文: </summary>多标签图像识别是计算机视觉中的一项基本任务。最近，视觉语言模型在这一领域取得了显着的进步。然而，以前的方法往往无法有效利用语言模型中丰富的知识，而是以单向的方式将标签语义合并到视觉特征中。在本文中，我们提出了一种提示驱动的视觉语言表征学习（PVLR）框架，以更好地利用语言模态的功能。在PVLR中，我们首先引入了一种双重提示策略，包括知识感知提示（KAP）和上下文感知提示（CAP）。 KAP 利用固定提示来捕获所有标签的内在语义知识和关系，而 CAP 利用可学习的提示来捕获上下文感知的标签语义和关系。后来，我们提出了一个交互和融合模块（IFM）来交互和融合从 KAP 和 CAP 获得的表示。与之前作品中的单向融合相比，我们引入了双模态注意力（DMA），它能够实现文本和视觉特征之间的双向交互，产生上下文感知的标签表示和语义相关的视觉表示，随后用于计算相似性并生成所有标签的最终预测。在 MS-COCO、Pascal VOC 2007 和 NUS-WIDE 等三个流行数据集上进行的大量实验证明了 PVLR 的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17881v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error**<br />
**Title_cn:** AEROBLADE：使用自动编码器重建误差对潜在扩散图像进行免训练检测<br />
**Authors:** Jonas Ricker, Denis Lukovnikov, Asja Fischer<br />
**Abstract:** <details><summary>原文: </summary>With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用最新的文本到图像模型，任何人都可以生成具有任意内容的具有欺骗性的真实图像，从而加剧了视觉虚假信息日益增长的威胁。以低计算成本生成高分辨率图像的关键推动因素是潜在扩散模型（LDM）的开发。与传统的扩散模型相比，LDM 在预训练的自动编码器 (AE) 的低维潜在空间而不是高维图像空间中执行去噪过程。尽管具有相关性，LDM 的法证分析仍处于起步阶段。在这项工作中，我们提出了 AEROBLADE，这是一种利用 LDM 固有组件的新颖检测方法：用于在图像和潜在空间之间转换图像的 AE。我们发现，AE 可以比真实图像更准确地重建生成的图像，从而允许基于重建误差的简单检测方法。最重要的是，我们的方法易于实现，不需要任何训练，但几乎与依赖大量训练的检测器的性能相匹配。我们凭经验证明 AEROBLADE 可有效对抗最先进的 LDM，包括稳定扩散和中程。除了检测之外，我们的方法还可以对图像进行定性分析，这可用于识别修复区域。</details>
**PDF:** <http://arxiv.org/pdf/2401.17879v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **VR-based generation of photorealistic synthetic data for training hand-object tracking models**<br />
**Title_cn:** 基于 VR 生成逼真的合成数据，用于训练手部物体跟踪模型<br />
**Authors:** Chengyan Zhang, Rahul Chaudhari<br />
**Abstract:** <details><summary>原文: </summary>Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present "blender-hoisynth", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于精确跟踪 3D 手部物体交互 (HOI) 的监督学习模型需要大量带注释的数据进行训练。此外，对于非专家来说，在 2D 图像上标记 3D 地面实况（例如 6DoF 对象姿势）并不直观。为了解决这些问题，我们推出了“blender-hoisynth”，一个基于 Blender 软件的交互式合成数据生成器。 Blender-hoisynth 可以可扩展地生成并自动注释可视化 HOI 训练数据。其他竞争方法通常完全无需人工​​输入即可生成合成 HOI 数据。虽然这在某些情况下可能是有益的，但 HOI 应用程序本质上需要直接控制 HOI 作为人类意图的表达。借助 Blender-hoisynth，用户可以使用标准虚拟现实硬件通过虚拟手与对象进行交互。合成生成的数据具有高度真实感的特点，包含视觉上合理且物理上真实的手部抓握物体并以 3D 方式移动物体的视频。为了证明我们数据生成的有效性，我们用 hoisynth 数据替换了著名的 DexYCB 数据集中的大部分训练数据，并用它训练了最先进的 HOI 重建模型。我们表明，尽管进行了数据替换，模型性能并没有显着下降。</details>
**PDF:** <http://arxiv.org/pdf/2401.17874v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model**<br />
**Title_cn:** 卷积遇见 LoRA：分段任意模型的参数高效微调<br />
**Authors:** Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan<br />
**Abstract:** <details><summary>原文: </summary>The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>Segment Anything Model (SAM) 是图像分割的基础框架。虽然它在典型场景中表现出显着的零样本泛化能力，但当应用于医学图像和遥感等专业领域时，其优势就会减弱。为了解决这一限制，本文引入了 Conv-LoRA，这是一种简单而有效的参数高效微调方法。通过将超轻量级卷积参数集成到低秩适应 (LoRA) 中，Conv-LoRA 可以将与图像相关的归纳偏差注入到普通 ViT 编码器中，进一步强化 SAM 的局部先验假设。值得注意的是，Conv-LoRA 不仅保留了 SAM 广泛的分割知识，而且还恢复了其学习高级图像语义的能力，而这种能力受到 SAM 前景-背景分割预训练的限制。跨多个领域的不同基准的综合实验强调了 Conv-LoRA 在使 SAM 适应现实世界语义分割任务方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17868v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Semantic Anything in 3D Gaussians**<br />
**Title_cn:** 3D 高斯中的任何语义<br />
**Authors:** Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed. Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain. Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc. In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters. We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians. Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods. We also propose a cross-view label-voting approach to assign labels from different views. In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks. Codes will be released soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>受益于其高质量的渲染结果和实时渲染速度，3D 高斯分布已成为神经辐射场 (NeRF) 的替代 3D 表示形式。考虑到 3D 高斯表示仍未解析，有必要首先在该域内执行对象分割。随后，可以执行场景编辑和碰撞检测，这对于虚拟现实（VR）、增强现实（AR）、游戏/电影制作等多种应用至关重要。在本文中，我们提出了一种新颖的方法通过交互式程序实现 3D 高斯对象分割，无需任何训练过程和学习参数。我们将所提出的方法称为 SA-GS，即 3D 高斯中的任意分段。给定单个输入视图中的一组点击点，SA-GS 可以推广 SAM，通过所提出的多视图掩模生成和视图方式标签分配方法来实现 3D 一致分割。我们还提出了一种跨视图标签投票方法来分配来自不同视图的标签。此外，为了解决由于边界处的3D高斯空间尺寸不可忽略而导致的分割对象的边界粗糙度问题，SA-GS采用了简单但有效的高斯分解方案。大量实验表明，SA-GS 实现了高质量的 3D 分割结果，也可以轻松应用于场景编辑和碰撞检测任务。代码即将发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.17857v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Instruction-Guided Scene Text Recognition**<br />
**Title_cn:** 指令引导的场景文本识别<br />
**Authors:** Yongkun Du, Zhineng Chen, Yuchen Su, Caiyan Jia, Yu-Gang Jiang<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal models have shown appealing performance in visual tasks recently, as instruction-guided training has evoked the ability to understand fine-grained visual content. However, current methods cannot be trivially applied to scene text recognition (STR) due to the gap between natural and text images. In this paper, we introduce a novel paradigm that formulates STR as an instruction learning problem, and propose instruction-guided scene text recognition (IGTR) to achieve effective cross-modal learning. IGTR first generates rich and diverse instruction triplets of <condition,question,answer>, serving as guidance for nuanced text image understanding. Then, we devise an architecture with dedicated cross-modal feature fusion module, and multi-task answer head to effectively fuse the required instruction and image features for answering questions. Built upon these designs, IGTR facilitates accurate text recognition by comprehending character attributes. Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins. Furthermore, by adjusting the instructions, IGTR enables various recognition schemes. These include zero-shot prediction, where the model is trained based on instructions not explicitly targeting character recognition, and the recognition of rarely appearing and morphologically similar characters, which were previous challenges for existing models.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态模型最近在视觉任务中表现出了引人注目的性能，因为指令引导的训练激发了理解细粒度视觉内容的能力。然而，由于自然图像和文本图像之间的差距，当前的方法不能轻易应用于场景文本识别（STR）。在本文中，我们介绍了一种新颖的范式，将 STR 表述为指令学习问题，并提出指令引导场景文本识别（IGTR）以实现有效的跨模态学习。 IGTR 首先生成丰富多样的指令三元组<条件、问题、答案>，作为细致入微的文本图像理解的指导。然后，我们设计了一种具有专用跨模态特征融合模块和多任务答案头的架构，以有效融合回答问题所需的指令和图像特征。基于这些设计，IGTR 通过理解字符属性来促进准确的文本识别。在英语和中文基准上的实验表明，IGTR 明显优于现有模型。此外，通过调整指令，IGTR可以实现各种识别方案。其中包括零样本预测，其中模型是根据未明确针对字符识别的指令进行训练的，以及很少出现和形态相似的字符的识别，这是现有模型之前面临的挑战。</details>
**PDF:** <http://arxiv.org/pdf/2401.17851v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** 利用 Swin Transformer 进行本地到全局弱监督语义分割<br />
**Authors:** Rozhan Ahmadi, Shohreh Kasaei<br />
**Abstract:** <details><summary>原文: </summary>In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the initial seed CAMs by bringing local and global views together. SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features. SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination. Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models. It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network. SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，使用图像级标签作为监督的弱监督语义分割在计算机视觉领域受到了广泛关注。大多数现有方法通过关注通过类激活图（CAM）生成伪标签来促进监督学习，从而解决了这些标签中缺乏空间信息所带来的挑战。由于卷积神经网络 (CNN) 的局部模式检测，CAM 通常只强调对象中最具辨别力的部分，这使得准确地区分前景对象与背景对象变得困难。最近的研究表明，Vision Transformer (ViT) 特征由于其全局视图，在捕获场景布局方面比 CNN 更有效。然而，分层 ViT 的使用尚未在该领域得到广泛探索。这项工作通过提出“SWTformer”来探索 Swin Transformer 的使用，通过将本地和全局视图结合在一起来提高初始种子 CAM 的准确性。 SWTformer-V1 仅使用补丁标记作为特征来生成类概率和 CAM。 SWTformer-V2 采用多尺度特征融合机制来提取附加信息，并利用背景感知机制来生成更准确的定位图，并改进跨对象辨别能力。基于 PascalVOC 2012 数据集的实验，SWTformer-V1 实现了 0.98% mAP 的更高定位精度，优于最先进的模型。在生成初始定位图方面，它的性能平均比其他方法高 0.82% mIoU，仅取决于分类网络。 SWTformer-V2 将生成的种子 CAM 的精度进一步提高了 5.32% mIoU，进一步证明了 Swin 变压器提供的局部到全局视图的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17828v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Do Object Detection Localization Errors Affect Human Performance and Trust?**<br />
**Title_cn:** 对象检测定位错误会影响人类表现和信任吗？<br />
**Authors:** Sven de Witte, Ombretta Strafforello, Jan van Gemert<br />
**Abstract:** <details><summary>原文: </summary>Bounding boxes are often used to communicate automatic object detection results to humans, aiding humans in a multitude of tasks. We investigate the relationship between bounding box localization errors and human task performance. We use observer performance studies on a visual multi-object counting task to measure both human trust and performance with different levels of bounding box accuracy. The results show that localization errors have no significant impact on human accuracy or trust in the system. Recall and precision errors impact both human performance and trust, suggesting that optimizing algorithms based on the F1 score is more beneficial in human-computer tasks. Lastly, the paper offers an improvement on bounding boxes in multi-object counting tasks with center dots, showing improved performance and better resilience to localization inaccuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>边界框通常用于向人类传达自动对象检测结果，帮助人类完成多种任务。我们研究了边界框定位错误与人工任务性能之间的关系。我们使用视觉多对象计数任务的观察者性能研究来衡量不同级别的边界框准确度下的人类信任和性能。结果表明，定位错误对人类准确性或对系统的信任没有重大影响。召回率和精度错误都会影响人类的表现和信任，这表明基于 F1 分数的优化算法在人机任务中更有利。最后，本文对带有中心点的多对象计数任务中的边界框进行了改进，显示出改进的性能和对定位不准确的更好的恢复能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.17821v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes**<br />
**Title_cn:** SimAda：一个简单的统一框架，用于在表现不佳的场景中调整分段任意模型<br />
**Authors:** Yiran Song, Qianyu Zhou, Xuequan Lu, Zhiwen Shao, Lizhuang Ma<br />
**Abstract:** <details><summary>原文: </summary>Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data. Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks. In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks. We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes. Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework. SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models. We conduct extensive experiments on nine datasets of six downstream tasks. The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs. Code is available at: https://github.com/zongzi13545329/SimAda</details>
**Abstract_cn:** <details><summary>译文: </summary>分段任意模型（SAM）在常见视觉场景中表现出了出色的泛化能力，但缺乏对专业数据的理解。尽管许多工作都专注于针对下游任务优化 SAM，但这些特定于任务的方法通常限制了对其他下游任务的通用性。在本文中，我们旨在研究通用视觉模块对 SAM 微调的影响，并使它们能够泛化到所有下游任务。我们提出了一个名为 SimAda 的简单统一框架，用于在表现不佳的场景中调整 SAM。具体来说，我们的框架将不同方法的通用模块抽象为基本设计元素，并基于共享的理论框架设计了四种变体。 SimAda 简单而有效，它消除了所有特定于数据集的设计，只专注于一般优化，确保 SimAda 可以应用于所有基于 SAM 甚至基于 Transformer 的模型。我们对六个下游任务的九个数据集进行了广泛的实验。结果表明，SimAda 显着提高了 SAM 在多个下游任务上的性能，并在大多数任务上实现了最先进的性能，而无需针对特定任务进行设计。代码位于：https://github.com/zongzi13545329/SimAda</details>
**PDF:** <http://arxiv.org/pdf/2401.17803v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Tiered approach for rapid damage characterisation of infrastructure enabled by remote sensing and deep learning technologies**<br />
**Title_cn:** 利用遥感和深度学习技术快速表征基础设施损坏的分层方法<br />
**Authors:** Nadiia Kopiika, Andreas Karavias, Pavlos Krassakis, Zehao Ye, Jelena Ninic, Nataliya Shakhovska, Nikolaos Koukouzas, Sotirios Argyroudis, Stergios-Aristoteles Mitoulis<br />
**Abstract:** <details><summary>原文: </summary>Critical infrastructure such as bridges are systematically targeted during wars and conflicts. This is because critical infrastructure is vital for enabling connectivity and transportation of people and goods, and hence, underpinning the national and international defence planning and economic growth. Mass destruction of bridges, along with minimal or no accessibility to these assets during natural and anthropogenic disasters, prevents us from delivering rapid recovery. As a result, systemic resilience is drastically reduced. A solution to this challenge is to use technology for stand-off observations. Yet, no method exists to characterise damage at different scales, i.e. regional, asset, and structural (component), and more so there is little or no systematic correlation between assessments at scale. We propose an integrated three-level tiered approach to fill this capability gap, and we demonstrate the methods for damage characterisation enabled by fit-for-purpose digital technologies. Next, this method is applied and validated to a case study in Ukraine that includes 17 bridges. From macro to micro, we deploy technology at scale, from Sentinel-1 SAR images, crowdsourced information, and high-resolution images to deep learning for damaged infrastructure. For the first time, the interferometric coherence difference and semantic segmentation of images were deployed to improve the reliability of damage characterisations from regional to infrastructure component level, when enhanced assessment accuracy is required. This integrated method improves the speed of decision-making, and thus, enhances resilience. Keywords: critical infrastructure, damage characterisation, targeted attacks, restoration</details>
**Abstract_cn:** <details><summary>译文: </summary>桥梁等关键基础设施在战争和冲突期间成为系统性攻击目标。这是因为关键基础设施对于实现人员和货物的互联互通和运输至关重要，从而支撑国家和国际国防规划和经济增长。桥梁的大规模破坏，以及在自然和人为灾害期间这些资产的访问很少或根本无法访问，使我们无法实现快速恢复。结果，系统弹性大大降低。应对这一挑战的一个解决方案是使用远距离观测技术。然而，没有方法可以描述不同规模（即区域、资产和结构（组成部分））的损害特征，更重要的是，规模评估之间很少或没有系统相关性。我们提出了一种集成的三级分层方法来填补这一能力差距，并演示了由适合用途的数字技术实现的损伤表征方法。接下来，该方法在乌克兰的一个包括 17 座桥梁的案例研究中得到应用和验证。从宏观到微观，我们大规模部署技术，从 Sentinel-1 SAR 图像、众包信息、高分辨率图像到受损基础设施的深度学习。当需要提高评估精度时，首次采用干涉相干差异和图像语义分割来提高从区域到基础设施组件级别的损伤表征的可靠性。这种集成方法提高了决策速度，从而增强了弹性。关键词：关键基础设施、损害特征、针对性攻击、恢复</details>
**PDF:** <http://arxiv.org/pdf/2401.17759v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement**<br />
**Title_cn:** 利用人机交互提高计算机视觉数据集质量<br />
**Authors:** Esla Timothy Anzaku, Hyesoo Hong, Jin-Woo Park, Wonjun Yang, Kangmin Kim, JongBum Won, Deshika Vinoshani Kumari Herath, Arnout Van Messem, Wesley De Neve<br />
**Abstract:** <details><summary>原文: </summary>Large-scale datasets for single-label multi-class classification, such as \emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision. However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors. In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement. We term this novel framework \emph{Multilabelfy}. Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets. Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection. Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions. This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation. Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development. The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.   \keywords{Computer Vision \and Dataset Quality Enhancement \and Dataset Validation \and Human-Computer Interaction \and Multi-label Annotation.}</details>
**Abstract_cn:** <details><summary>译文: </summary>用于单标签多类分类的大规模数据集，例如 \emph{ImageNet-1k}，在推进深度学习和计算机视觉方面发挥了重要作用。然而，一个关键且经常未被充分研究的方面是这些数据集的综合质量评估，特别是关于潜在的多标签注释错误。在本文中，我们介绍了一个轻量级、用户友好且可扩展的框架，该框架可以协同人类和机器智能，以实现高效的数据集验证和质量增强。我们将这个新颖的框架称为\emph{Multilabelfy}。 Multilabelfy 的核心是一个适应性强的基于网络的平台，它系统地指导注释者完成重新评估过程，有效地利用人机交互来提高数据集质量。通过在 ImageNetV2 数据集上使用 Multilabelfy，我们发现大约 $47.88\%$ 的图像至少包含两个标签，这强调了对此类有影响力的数据集进行更严格评估的必要性。此外，我们的分析显示每张图像的潜在标签数量与模型 top-1 准确性之间存在负相关，这阐明了模型评估和选择的关键因素。我们的开源框架 Multilabelfy 为数据集增强提供了一种方便、轻量级的解决方案，强调多标签比例。这项研究解决了数据集完整性方面的主要挑战，并为模型性能评估提供了重要见解。此外，它还强调了将人类专业知识与机器能力相结合以产生更强大的模型和值得信赖的数据开发的优势。 Multilabelfy 的源代码可在 https://github.com/esla/Multilabelfy 上获取。 \keywords{计算机视觉\和数据集质量增强\和数据集验证\和人机交互\和多标签注释。}</details>
**PDF:** <http://arxiv.org/pdf/2401.17736v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Unified Physical-Digital Face Attack Detection**<br />
**Title_cn:** 统一的物理-数字人脸攻击检测<br />
**Authors:** Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Face Recognition (FR) systems can suffer from physical (i.e., print photo) and digital (i.e., DeepFake) attacks. However, previous related work rarely considers both situations at the same time. This implies the deployment of multiple models and thus more computational burden. The main reasons for this lack of an integrated model are caused by two factors: (1) The lack of a dataset including both physical and digital attacks with ID consistency which means the same ID covers the real face and all attack types; (2) Given the large intra-class variance between these two attacks, it is difficult to learn a compact feature space to detect both attacks simultaneously. To address these issues, we collect a Unified physical-digital Attack dataset, called UniAttackData. The dataset consists of $1,800$ participations of 2 and 12 physical and digital attacks, respectively, resulting in a total of 29,706 videos. Then, we propose a Unified Attack Detection framework based on Vision-Language Models (VLMs), namely UniAttackDetection, which includes three main modules: the Teacher-Student Prompts (TSP) module, focused on acquiring unified and specific knowledge respectively; the Unified Knowledge Mining (UKM) module, designed to capture a comprehensive feature space; and the Sample-Level Prompt Interaction (SLPI) module, aimed at grasping sample-level semantics. These three modules seamlessly form a robust unified attack detection framework. Extensive experiments on UniAttackData and three other datasets demonstrate the superiority of our approach for unified face attack detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸识别 (FR) 系统可能会遭受物理（即打印照片）和数字（即 DeepFake）攻击。然而，以往的相关工作很少同时考虑这两种情况。这意味着部署多个模型，从而带来更多的计算负担。缺乏集成模型的主要原因有两个因素：（1）缺乏包含物理攻击和数字攻击且具有ID一致性的数据集，即相同的ID涵盖真实人脸和所有攻击类型； （2）考虑到这两种攻击之间存在较大的类内方差，很难学习紧凑的特征空间来同时检测这两种攻击。为了解决这些问题，我们收集了一个统一的物理数字攻击数据集，称为 UniAttackData。该数据集分别包含 2 次和 12 次物理攻击和数字攻击，价值 1,800 美元，总共 29,706 个视频。然后，我们提出了一种基于视觉语言模型（VLM）的统一攻击检测框架，即UniAttackDetection，它包括三个主要模块：教师-学生提示（TSP）模块，分别侧重于获取统一和特定知识；统一知识挖掘（UKM）模块，旨在捕获综合特征空间；样本级提示交互（SLPI）模块，旨在掌握样本级语义。这三个模块无缝地构成了一个强大的统一攻击检测框架。在 UniAttackData 和其他三个数据集上进行的大量实验证明了我们的统一人脸攻击检测方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17699v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Datacube segmentation via Deep Spectral Clustering**<br />
**Title_cn:** 通过深度谱聚类进行数据立方分割<br />
**Authors:** Alessandro Bombini, Fernando García-Avello Bofías, Caterina Bracci, Michele Ginolfi, Chiara Ruberto<br />
**Abstract:** <details><summary>原文: </summary>Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-Means clustering algorithm.   We apply this technique to two different use cases, of different physical origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩展视觉技术在物理学中无处不在。然而，由于从构成数据立方体的光谱中辨别相关信息的内在困难，从这种分析中产生的数据立方体常常对它们的解释提出挑战。此外，数据立方体光谱的巨大维数在其统计解释中提出了复杂的任务；然而，这种复杂性包含大量的统计信息，可以以无监督的方式利用这些信息来概述当前案例研究的一些基本属性，例如，可以通过数据立方体的（深度）聚类来获得图像分割光谱，在适当定义的低维嵌入空间中执行。为了解决这个主题，我们探索在编码空间中应用无监督聚类方法的可能性，即对数据立方体像素的光谱属性执行深度聚类。统计降维由专门训练的（变分）自动编码器执行，负责将频谱映射到较低维的度量空间，而聚类过程由（可学习的）迭代 K 均值聚类算法执行。我们将该技术应用于具有不同物理起源的两个不同用例：一组关于绘画艺术品的宏观映射 X 射线荧光 (MA-XRF) 合成数据，以及一组模拟天体物理观测数据。</details>
**PDF:** <http://arxiv.org/pdf/2401.17695v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **All Beings Are Equal in Open Set Recognition**<br />
**Title_cn:** 开集认识中众生平等<br />
**Authors:** Chaohua Li, Enhao Zhang, Chuanxing Geng, SongCan Chen<br />
**Abstract:** <details><summary>原文: </summary>In open-set recognition (OSR), a promising strategy is exploiting pseudo-unknown data outside given $K$ known classes as an additional $K$+$1$-th class to explicitly model potential open space. However, treating unknown classes without distinction is unequal for them relative to known classes due to the category-agnostic and scale-agnostic of the unknowns. This inevitably not only disrupts the inherent distributions of unknown classes but also incurs both class-wise and instance-wise imbalances between known and unknown classes. Ideally, the OSR problem should model the whole class space as $K$+$\infty$, but enumerating all unknowns is impractical. Since the core of OSR is to effectively model the boundaries of known classes, this means just focusing on the unknowns nearing the boundaries of targeted known classes seems sufficient. Thus, as a compromise, we convert the open classes from infinite to $K$, with a novel concept Target-Aware Universum (TAU) and propose a simple yet effective framework Dual Contrastive Learning with Target-Aware Universum (DCTAU). In details, guided by the targeted known classes, TAU automatically expands the unknown classes from the previous $1$ to $K$, effectively alleviating the distribution disruption and the imbalance issues mentioned above. Then, a novel Dual Contrastive (DC) loss is designed, where all instances irrespective of known or TAU are considered as positives to contrast with their respective negatives. Experimental results indicate DCTAU sets a new state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>在开放集识别 (OSR) 中，一种有前途的策略是利用给定 $K$ 已知类之外的伪未知数据作为附加的 $K$+$1$ 类来显式建模潜在的开放空间。然而，由于未知类与类别无关和规模无关，相对于已知类，不加区别地对待未知类是不平等的。这不可避免地不仅会破坏未知类的固有分布，而且还会导致已知类和未知类之间的类和实例方面的不平衡。理想情况下，OSR 问题应该将整个类空间建模为 $K$+$\infty$，但枚举所有未知数是不切实际的。由于 OSR 的核心是有效地对已知类的边界进行建模，这意味着仅关注目标已知类边界附近的未知数似乎就足够了。因此，作为妥协，我们将开放类从无限转换为$K$，采用新颖的概念目标感知宇宙（TAU），并提出一个简单而有效的框架双重对比学习与目标感知宇宙（DCTAU）。具体来说，在目标已知类别的引导下，TAU 自动将未知类别从之前的 $1$ 扩展到 $K$，有效缓解了上述分配中断和不平衡问题。然后，设计了一种新颖的双重对比（DC）损失，其中所有实例（无论已知还是 TAU）都被视为正例，以与各自的负例进行对比。实验结果表明 DCTAU 创下了新的最先进水平。</details>
**PDF:** <http://arxiv.org/pdf/2401.17654v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition**<br />
**Title_cn:** 用于提示语音识别的计算和参数高效的多模态融合变压器<br />
**Authors:** Lei Liu, Li Liu, Haizhou Li<br />
**Abstract:** <details><summary>原文: </summary>Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively. The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR. However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data. As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion. Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both poor recognition accuracy and inefficient computation for the ACSR task. To address these problems, we develop a novel computation and parameter efficient multi-modal fusion transformer by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams. More precisely, TIAA firstly models the modality-specific fine-grained temporal dependencies over all tokens of each modality, and then learns the efficient cross-modal interaction for the modality-shared coarse-grained temporal dependencies over the important tokens of different modalities. Besides, a light-weight gated hidden projection is designed to control the feature flows of TIAA. The resulting model, named Economical Cued Speech Fusion Transformer (EcoCued), achieves state-of-the-art performance on all existing CS datasets, compared with existing transformer-based fusion methods and ACSR fusion methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>提示语音（CS）是听障人士使用的一种纯视觉编码方法，它将唇读与几种特定的手形相结合，使口语变得可见。自动 CS 识别 (ACSR) 旨在将语音的视觉线索转录为文本，这可以帮助听力障碍人士有效地进行交流。 CS的视觉信息包含唇读和手势，两者的融合在ACSR中发挥着重要作用。然而，大多数先前的融合方法都难以捕获多模态 CS 数据的长序列输入中存在的全局依赖性。因此，这些方法通常无法学习有助于融合的有效跨模态关系。最近，基于注意力的变压器已成为捕获多模态融合中长序列的全局依赖性的流行想法，但现有的多模态融合变压器在 ACSR 任务中存在识别精度差和计算效率低的问题。为了解决这些问题，我们通过提出一种新颖的令牌重要性感知注意机制（TIAA）来开发一种新颖的计算和参数高效的多模态融合变压器，其中制定了令牌利用率（TUR）以从多模式流。更准确地说，TIAA 首先对每种模态的所有标记上特定模态的细粒度时间依赖性进行建模，然后学习不同模态的重要标记上模态共享的粗粒度时间依赖性的有效跨模态交互。此外，还设计了轻量级门控隐藏投影来控制 TIAA 的特征流。与现有的基于 Transformer 的融合方法和 ACSR 融合方法相比，所得模型名为经济 Cued 语音融合变压器 (EcoCued)，在所有现有 CS 数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.17604v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data**<br />
**Title_cn:** 擅长字幕，不擅长计数：在地球观测数据上对 GPT-4V 进行基准测试<br />
**Authors:** Chenhui Zhang, Sherrie Wang<br />
**Abstract:** <details><summary>原文: </summary>Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks. Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型视觉语言模型 (VLM) 在涉及自然语言指令视觉输入的复杂任务中表现出了令人印象深刻的性能。然而，目前尚不清楚自然图像在多大程度上能够转移到地球观测（EO）数据，这些数据主要是卫星和航空图像，在 VLM 训练数据中不太常见。在这项工作中，我们提出了一个全面的基准，通过评估 VLM 在场景理解、定位和计数以及变化检测任务方面的能力，来衡量 VLM 成为 EO 数据有用工具的进展。在现实世界应用的推动下，我们的基准包括城市监控、救灾、土地利用和保护等场景。我们发现，尽管像 GPT-4V 这样最先进的 VLM 拥有广泛的世界知识，可以在位置理解和图像字幕等开放式任务上表现出色，但它们糟糕的空间推理能力限制了对象定位和计数任务的实用性。我们的基准测试将在 https://vleo.danielz.ch/ 和 Hugging Face 上公开发布：https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70，以便于模型评估。</details>
**PDF:** <http://arxiv.org/pdf/2401.17600v1><br />
**Code:** <https://github.com/Earth-Intelligence-Lab/vleo-bench>**<br />
>>**index:** 26<br />
**Title:** **Head and Neck Tumor Segmentation from [18F]F-FDG PET/CT Images Based on 3D Diffusion Model**<br />
**Title_cn:** 基于 3D 扩散模型的 [18F]F-FDG PET/CT 图像的头颈肿瘤分割<br />
**Authors:** Yafei Dong, Kuang Gong<br />
**Abstract:** <details><summary>原文: </summary>Head and neck (H&N) cancers are among the most prevalent types of cancer worldwide, and [18F]F-FDG PET/CT is widely used for H&N cancer management. Recently, the diffusion model has demonstrated remarkable performance in various image-generation tasks. In this work, we proposed a 3D diffusion model to accurately perform H&N tumor segmentation from 3D PET and CT volumes. The 3D diffusion model was developed considering the 3D nature of PET and CT images acquired. During the reverse process, the model utilized a 3D UNet structure and took the concatenation of PET, CT, and Gaussian noise volumes as the network input to generate the tumor mask. Experiments based on the HECKTOR challenge dataset were conducted to evaluate the effectiveness of the proposed diffusion model. Several state-of-the-art techniques based on U-Net and Transformer structures were adopted as the reference methods. Benefits of employing both PET and CT as the network input as well as further extending the diffusion model from 2D to 3D were investigated based on various quantitative metrics and the uncertainty maps generated. Results showed that the proposed 3D diffusion model could generate more accurate segmentation results compared with other methods. Compared to the diffusion model in 2D format, the proposed 3D model yielded superior results. Our experiments also highlighted the advantage of utilizing dual-modality PET and CT data over only single-modality data for H&N tumor segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>头颈 (H&N) 癌症是全球最常见的癌症类型之一，[18F]F-FDG PET/CT 广泛用于 H&N 癌症管理。最近，扩散模型在各种图像生成任务中表现出了卓越的性能。在这项工作中，我们提出了一种 3D 扩散模型，可以根据 3D PET 和 CT 体积准确地执行 H&N 肿瘤分割。 3D 扩散模型的开发考虑了所获取的 PET 和 CT 图像的 3D 性质。在逆向过程中，模型利用3D UNet结构，并将PET、CT和高斯噪声量的串联作为网络输入来生成肿瘤掩模。基于 HECKTOR 挑战数据集的实验评估了所提出的扩散模型的有效性。采用基于 U-Net 和 Transformer 结构的几种最先进的技术作为参​​考方法。基于各种定量指标和生成的不确定性图，研究了采用 PET 和 CT 作为网络输入以及进一步将扩散模型从 ​​2D 扩展到 3D 的好处。结果表明，与其他方法相比，所提出的 3D 扩散模型可以生成更准确的分割结果。与 2D 格式的扩散模型相比，所提出的 3D 模型产生了更好的结果。我们的实验还强调了利用双模态 PET 和 CT 数据相对于仅使用单模态数据进行 H&N 肿瘤分割的优势。</details>
**PDF:** <http://arxiv.org/pdf/2401.17593v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Local Feature Matching Using Deep Learning: A Survey**<br />
**Title_cn:** 使用深度学习进行局部特征匹配：调查<br />
**Authors:** Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo<br />
**Abstract:** <details><summary>原文: </summary>Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prevalent datasets and metrics to facilitate a quantitative comparison of state-of-the-art techniques. The paper also explores the practical application of local feature matching in diverse domains such as Structure from Motion, Remote Sensing Image Registration, and Medical Image Registration, underscoring its versatility and significance across various fields. Ultimately, we endeavor to outline the current challenges faced in this domain and furnish future research directions, thereby serving as a reference for researchers involved in local feature matching and its interconnected domains.</details>
**Abstract_cn:** <details><summary>译文: </summary>局部特征匹配在计算机视觉领域有着广泛的应用，涵盖图像检索、3D 重建和对象识别等领域。然而，由于视点和照明变化等因素，提高匹配的准确性和鲁棒性仍然存在挑战。近年来，深度学习模型的引入引发了对局部特征匹配技术的广泛探索。这项工作的目标是提供局部特征匹配方法的全面概述。根据检测器的存在，这些方法分为两个关键部分。基于检测器的类别涵盖的模型包括检测然后描述、联合检测和描述、描述然后检测以及基于图的技术。相比之下，无检测器类别包括基于 CNN、基于变换器和基于补丁的方法。我们的研究超越了方法论分析，结合了对流行数据集和指标的评估，以促进最先进技术的定量比较。本文还探讨了局部特征匹配在运动结构、遥感图像配准和医学图像配准等不同领域的实际应用，强调了其在各个领域的多功能性和重要性。最终，我们努力概述该领域当前面临的挑战并提供未来的研究方向，从而为涉及局部特征匹配及其互连领域的研究人员提供参考。</details>
**PDF:** <http://arxiv.org/pdf/2401.17592v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Towards Image Semantics and Syntax Sequence Learning**<br />
**Title_cn:** 迈向图像语义和句法序列学习<br />
**Authors:** Chun Tao, Timur Ibrayev, Kaushik Roy<br />
**Abstract:** <details><summary>原文: </summary>Convolutional neural networks and vision transformers have achieved outstanding performance in machine perception, particularly for image classification. Although these image classifiers excel at predicting image-level class labels, they may not discriminate missing or shifted parts within an object. As a result, they may fail to detect corrupted images that involve missing or disarrayed semantic information in the object composition. On the contrary, human perception easily distinguishes such corruptions. To mitigate this gap, we introduce the concept of "image grammar", consisting of "image semantics" and "image syntax", to denote the semantics of parts or patches of an image and the order in which these parts are arranged to create a meaningful object. To learn the image grammar relative to a class of visual objects/scenes, we propose a weakly supervised two-stage approach. In the first stage, we use a deep clustering framework that relies on iterative clustering and feature refinement to produce part-semantic segmentation. In the second stage, we incorporate a recurrent bi-LSTM module to process a sequence of semantic segmentation patches to capture the image syntax. Our framework is trained to reason over patch semantics and detect faulty syntax. We benchmark the performance of several grammar learning models in detecting patch corruptions. Finally, we verify the capabilities of our framework in Celeb and SUNRGBD datasets and demonstrate that it can achieve a grammar validation accuracy of 70 to 90% in a wide variety of semantic and syntactical corruption scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络和视觉转换器在机器感知方面取得了出色的性能，特别是在图像分类方面。尽管这些图像分类器擅长预测图像级类别标签，但它们可能无法区分对象内丢失或移位的部分。因此，他们可能无法检测到对象组合中涉及丢失或混乱的语义信息的损坏图像。相反，人类的感知很容易区分这种腐败。为了弥补这一差距，我们引入了“图像语法”的概念，由“图像语义”和“图像语法”组成，来表示图像的部分或块的语义以及这些部分排列以创建图像的顺序。有意义的对象。为了学习与一类视觉对象/场景相关的图像语法，我们提出了一种弱监督的两阶段方法。在第一阶段，我们使用深度聚类框架，该框架依赖于迭代聚类和特征细化来产生部分语义分割。在第二阶段，我们结合了一个循环双 LSTM 模块来处理一系列语义分割补丁以捕获图像语法。我们的框架经过训练可以推理补丁语义并检测错误的语法。我们对几种语法学习模型在检测补丁损坏方面的性能进行了基准测试。最后，我们在 Celeb 和 SUNRGBD 数据集中验证了我们的框架的功能，并证明它可以在各种语义和句法损坏场景中实现 70% 到 90% 的语法验证准确性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17515v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs**<br />
**Title_cn:** 用于 FPGA 上深度学习加速的可训练定点量化<br />
**Authors:** Dingyi Dai, Yichi Zhang, Jiahao Zhang, Zhanqiu Hu, Yaohui Cai, Qi Sun, Zhiru Zhang<br />
**Abstract:** <details><summary>原文: </summary>Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as their software counterparts. Our evaluation shows that compared to post-training quantization, QFX can quantize models trained with element-wise layers quantized to fewer bits and achieve higher accuracy on both CIFAR-10 and ImageNet datasets. We further demonstrate the efficacy of multiplier-free quantization using a state-of-the-art binarized neural network accelerator designed for an embedded FPGA (AMD Xilinx Ultra96 v2). We plan to release QFX in open-source format.</details>
**Abstract_cn:** <details><summary>译文: </summary>量化是在资源受限设备（例如嵌入式 FPGA）上部署深度学习模型的关键技术。之前的工作主要集中在量化矩阵乘法，而将 BatchNorm 或快捷方式等其他层保留为浮点形式，尽管定点算术在 FPGA 上效率更高。常见的做法是将预训练模型微调到定点以进行 FPGA 部署，但这可能会降低准确性。这项工作提出了 QFX，一种新颖的可训练定点量化方法，可在模型训练期间自动学习二进制点位置。此外，我们在 QFX 中引入了无乘法器量化策略，以最大限度地减少 DSP 使用。 QFX 被实现为基于 PyTorch 的库，在 FPGA HLS 支持下，在反向传播过程中以可微分的方式高效地模拟定点算法。只需最少的努力，使用 QFX 训练的模型就可以通过 HLS 轻松部署，产生与软件对应物相同的数值结果。我们的评估表明，与训练后量化相比，QFX 可以量化使用量化为更少位数的逐元素层训练的模型，并在 CIFAR-10 和 ImageNet 数据集上实现更高的准确性。我们使用专为嵌入式 FPGA (AMD Xilinx Ultra96 v2) 设计的最先进的二值化神经网络加速器进一步证明了无乘法器量化的功效。我们计划以开源格式发布 QFX。</details>
**PDF:** <http://arxiv.org/pdf/2401.17544v1><br />
**Code:** null<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators**<br />
**Title_cn:** 运动引导：使用可微运动估计器进行基于扩散的图像编辑<br />
**Authors:** Daniel Geng, Andrew Owens<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型能够根据文本描述生成令人印象深刻的图像，并且这些模型的扩展允许用户以相对粗糙的比例编辑图像。然而，利用扩散模型精确编辑图像中对象的布局、位置、姿势和形状的能力仍然很困难。为此，我们提出了运动引导，这是一种零样本技术，允许用户指定密集、复杂的运动场，指示图像中每个像素应该移动的位置。运动引导的工作原理是通过现成的光流网络利用梯度来控制扩散采样过程。具体来说，我们设计了一种引导损失，鼓励样本进行由流网络估计的所需运动，同时在视觉上与源图像相似。通过同时从扩散模型中采样并引导样本具有较低的引导损失，我们可以获得运动编辑的图像。我们证明我们的技术适用于复杂的运动，并对真实和生成的图像进行高质量的编辑。</details>
**PDF:** <http://arxiv.org/pdf/2401.18085v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting**<br />
**Title_cn:** CARFF：用于 3D 场景预测的条件自动编码辐射场<br />
**Authors:** Jiezhi Yang, Khushi Desai, Charles Packer, Harshil Bhatia, Nicholas Rhinehart, Rowan McAllister, Joseph Gonzalez<br />
**Abstract:** <details><summary>原文: </summary>We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting, a method for predicting future 3D scenes given past observations, such as 2D ego-centric images. Our method maps an image to a distribution over plausible 3D latent scene configurations using a probabilistic encoder, and predicts the evolution of the hypothesized scenes through time. Our latent scene representation conditions a global Neural Radiance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward downstream applications. This approach extends beyond previous neural rendering work by considering complex scenarios of uncertainty in environmental states and dynamics. We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network. We demonstrate the utility of our method in realistic scenarios using the CARLA driving simulator, where CARFF can be used to enable efficient trajectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 CARFF：用于 3D 场景预测的条件自动编码辐射场，这是一种根据过去的观察（例如以 2D 自我为中心的图像）预测未来 3D 场景的方法。我们的方法使用概率编码器将图像映射到合理的 3D 潜在场景配置上的分布，并预测假设场景随时间的演变。我们的潜在场景表示以全局神经辐射场 (NeRF) 为条件来表示 3D 场景模型，从而实现可解释的预测和直接的下游应用。这种方法通过考虑环境状态和动态的不确定性的复杂场景，超越了之前的神经渲染工作。我们采用 Pose-Conditional-VAE 和 NeRF 的两阶段训练来学习 3D 表示。此外，我们利用混合密度网络自动回归预测潜在场景表示作为部分可观察的马尔可夫决策过程。我们使用 CARLA 驾驶模拟器展示了我们的方法在现实场景中的实用性，其中 CARFF 可用于在涉及视觉遮挡的复杂多智能体自动驾驶场景中实现高效的轨迹和应急计划。</details>
**PDF:** <http://arxiv.org/pdf/2401.18075v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Advances in 3D Generation: A Survey**<br />
**Title_cn:** 3D 生成的进展：调查<br />
**Authors:** Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, Ying Shan<br />
**Abstract:** <details><summary>原文: </summary>Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成 3D 模型是计算机图形学的核心，也是数十年来研究的焦点。随着高级神经表示和生成模型的出现，3D 内容生成领域正在快速发展，使得能够创建越来越高质量和多样化的 3D 模型。该领域的快速增长使得很难跟上所有最新发展。在本次调查中，我们的目标是介绍 3D 生成方法的基本方法，并建立一个结构化的路线图，包括 3D 表示、生成方法、数据集和相应的应用程序。具体来说，我们介绍了作为 3D 生成支柱的 3D 表示。此外，我们还全面概述了有关生成方法的快速增长的文献，按算法范式的类型进行分类，包括前馈生成、基于优化的生成、程序生成和生成新颖视图合成。最后，我们讨论可用的数据集、应用程序和开放挑战。我们希望这项调查能够帮助读者探索这个令人兴奋的主题，并促进 3D 内容生成领域的进一步进步。</details>
**PDF:** <http://arxiv.org/pdf/2401.17807v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Double InfoGAN for Contrastive Analysis**<br />
**Title_cn:** 双InfoGAN进行对比分析<br />
**Authors:** Florence Carton, Robin Louiset, Pietro Gori<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Analysis (CA) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA-VAEs). However, they all either ignore important constraints or they don't enforce fundamental assumptions. This may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Furthermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we propose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms SOTA CA-VAEs in terms of latent separation and image quality. Datasets and code are available online.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比分析 (CA) 旨在发现目标域与背景域相比的共同点和独特性。这在许多应用中引起了极大的兴趣，例如医学成像。当前最先进的 (SOTA) 方法是基于 VAE (CA-VAE) 的潜变量模型。然而，它们要么忽略了重要的约束，要么不执行基本假设。这可能会导致次优解决方案，其中独特因素被误认为是常见因素（反之亦然）。此外，生成的图像质量相当差，这是 VAE 的典型特征，降低了它们的可解释性和有用性。在这里，我们提出了 Double InfoGAN，这是第一个基于 GAN 的 CA 方法，它利用了 GAN 的高质量合成和 InfoGAN 的分离能力。在四个视觉数据集（从简单的合成示例到复杂的医学图像）上的实验结果表明，所提出的方法在潜在分离和图像质量方面优于 SOTA CA-VAE。数据集和代码可在线获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17776v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **3D-Plotting Algorithm for Insects using YOLOv5**<br />
**Title_cn:** 使用 YOLOv5 的昆虫 3D 绘图算法<br />
**Authors:** Daisuke Mori, Hiroki Hayami, Yasufumi Fujimoto, Isao Goto<br />
**Abstract:** <details><summary>原文: </summary>In ecological research, accurately collecting spatiotemporal position data is a fundamental task for understanding the behavior and ecology of insects and other organisms. In recent years, advancements in computer vision techniques have reached a stage of maturity where they can support, and in some cases, replace manual observation. In this study, a simple and inexpensive method for monitoring insects in three dimensions (3D) was developed so that their behavior could be observed automatically in experimental environments. The main achievements of this study have been to create a 3D monitoring algorithm using inexpensive cameras and other equipment to design an adjusting algorithm for depth error, and to validate how our plotting algorithm is quantitatively precise, all of which had not been realized in conventional studies. By offering detailed 3D visualizations of insects, the plotting algorithm aids researchers in more effectively comprehending how insects interact within their environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>在生态研究中，准确收集时空位置数据是了解昆虫和其他生物的行为和生态的基本任务。近年来，计算机视觉技术的进步已经达到成熟阶段，可以支持甚至在某些情况下取代手动观察。在这项研究中，开发了一种简单且廉价的三维（3D）昆虫监测方法，以便可以在实验环境中自动观察它们的行为。这项研究的主要成果是使用廉价相机和其他设备创建了 3D 监测算法，设计了深度误差的调整算法，并验证了我们的绘图算法在定量上的精确性，所有这些在传统研​​究中都没有实现。通过提供昆虫的详细 3D 可视化，绘图算法可以帮助研究人员更有效地理解昆虫如何在其环境中相互作用。</details>
**PDF:** <http://arxiv.org/pdf/2401.17714v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation**<br />
**Title_cn:** Image Anything：迈向推理连贯且免训练的多模态图像生成<br />
**Authors:** Yuanhuiyi Lyu, Xu Zheng, Lin Wang<br />
**Abstract:** <details><summary>原文: </summary>The multifaceted nature of human perception and comprehension indicates that, when we think, our body can naturally take any combination of senses, a.k.a., modalities and form a beautiful picture in our brain. For example, when we see a cattery and simultaneously perceive the cat's purring sound, our brain can construct a picture of a cat in the cattery. Intuitively, generative AI models should hold the versatility of humans and be capable of generating images from any combination of modalities efficiently and collaboratively. This paper presents ImgAny, a novel end-to-end multi-modal generative model that can mimic human reasoning and generate high-quality images. Our method serves as the first attempt in its capacity of efficiently and flexibly taking any combination of seven modalities, ranging from language, audio to vision modalities, including image, point cloud, thermal, depth, and event data. Our key idea is inspired by human-level cognitive processes and involves the integration and harmonization of multiple input modalities at both the entity and attribute levels without specific tuning across modalities. Accordingly, our method brings two novel training-free technical branches: 1) Entity Fusion Branch ensures the coherence between inputs and outputs. It extracts entity features from the multi-modal representations powered by our specially constructed entity knowledge graph; 2) Attribute Fusion Branch adeptly preserves and processes the attributes. It efficiently amalgamates distinct attributes from diverse input modalities via our proposed attribute knowledge graph. Lastly, the entity and attribute features are adaptively fused as the conditional inputs to the pre-trained Stable Diffusion model for image generation. Extensive experiments under diverse modality combinations demonstrate its exceptional capability for visual content creation.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类感知和理解的多面性表明，当我们思考时，我们的身体可以自然地采取任何感官组合，也就是模态，并在我们的大脑中形成一幅美丽的图画。例如，当我们看到猫舍并同时感知到猫的咕噜声时，我们的大脑可以构建出猫舍里有一只猫的图片。直观地说，生成式人工智能模型应该保留人类的多功能性，并且能够高效、协作地从任意模式组合生成图像。本文提出了 ImgAny，一种新颖的端到端多模态生成模型，可以模仿人类推理并生成高质量图像。我们的方法是首次尝试，能够高效、灵活地任意组合七种模态，从语言、音频到视觉模态，包括图像、点云、热、深度和事件数据。我们的关键思想受到人类认知过程的启发，涉及实体和属性级别上多种输入模式的集成和协调，而无需跨模式进行特定调整。因此，我们的方法带来了两个新颖的免训练技术分支：1）实体融合分支确保输入和输出之间的一致性。它从由我们专门构建的实体知识图支持的多模态表示中提取实体特征； 2）属性融合分支巧妙地保存和处理属性。它通过我们提出的属性知识图有效地合并了来自不同输入模式的不同属性。最后，实体和属性特征被自适应地融合作为用于图像生成的预训练稳定扩散模型的条件输入。不同模态组合下的大量实验证明了其卓越的视觉内容创建能力。</details>
**PDF:** <http://arxiv.org/pdf/2401.17664v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models**<br />
**Title_cn:** 基于扩散模型的图像空间和频率感知恢复方法<br />
**Authors:** Kyungsung Lee, Donggyu Lee, Myungjoo Kang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型最近已成为图像恢复（IR）的一个有前途的框架，因为它们能够产生高质量的重建并且与现有方法兼容。解决红外噪声逆问题的现有方法考虑了像素级数据保真度。在本文中，我们提出了 SaFaRI，一种具有高斯噪声的 IR 空间和频率感知扩散模型。我们的模型鼓励图像在空间和频率域上保持数据保真度，从而提高重建质量。我们全面评估了我们的模型在各种噪声逆问题上的性能，包括修复、去噪和超分辨率。我们的全面评估表明，SaFaRI 在 ImageNet 数据集和 FFHQ 数据集上均实现了最先进的性能，在 LPIPS 和 FID 指标方面优于现有的零样本 IR 方法。</details>
**PDF:** <http://arxiv.org/pdf/2401.17629v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Topology-Aware Latent Diffusion for 3D Shape Generation**<br />
**Title_cn:** 用于生成 3D 形状的拓扑感知潜在扩散<br />
**Authors:** Jiangbei Hu, Ben Fei, Baixin Xu, Fei Hou, Weidong Yang, Shengfa Wang, Na Lei, Chen Qian, Ying He<br />
**Abstract:** <details><summary>原文: </summary>We introduce a new generative model that combines latent diffusion with persistent homology to create 3D shapes with high diversity, with a special emphasis on their topological characteristics. Our method involves representing 3D shapes as implicit fields, then employing persistent homology to extract topological features, including Betti numbers and persistence diagrams. The shape generation process consists of two steps. Initially, we employ a transformer-based autoencoding module to embed the implicit representation of each 3D shape into a set of latent vectors. Subsequently, we navigate through the learned latent space via a diffusion model. By strategically incorporating topological features into the diffusion process, our generative module is able to produce a richer variety of 3D shapes with different topological structures. Furthermore, our framework is flexible, supporting generation tasks constrained by a variety of inputs, including sparse and partial point clouds, as well as sketches. By modifying the persistence diagrams, we can alter the topology of the shapes generated from these input modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种新的生成模型，该模型将潜在扩散与持久同源性相结合，以创建具有高度多样性的 3D 形状，并特别强调其拓扑特征。我们的方法涉及将 3D 形状表示为隐式场，然后采用持久同源性来提取拓扑特征，包括贝蒂数和持久性图。形状生成过程由两个步骤组成。最初，我们采用基于 Transformer 的自动编码模块将每个 3D 形状的隐式表示嵌入到一组潜在向量中。随后，我们通过扩散模型浏览学习到的潜在空间。通过策略性地将拓扑特征融入到扩散过程中，我们的生成模块能够生成更丰富的具有不同拓扑结构的 3D 形状。此外，我们的框架非常灵活，支持受各种输入约束的生成任务，包括稀疏和部分点云以及草图。通过修改持久性图，我们可以改变从这些输入模式生成的形状的拓扑。</details>
**PDF:** <http://arxiv.org/pdf/2401.17603v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Task-Oriented Diffusion Model Compression**<br />
**Title_cn:** 面向任务的扩散模型压缩<br />
**Authors:** Geonung Kim, Beomsu Kim, Eunhyeok Park, Sunghyun Cho<br />
**Abstract:** <details><summary>原文: </summary>As recent advancements in large-scale Text-to-Image (T2I) diffusion models have yielded remarkable high-quality image generation, diverse downstream Image-to-Image (I2I) applications have emerged. Despite the impressive results achieved by these I2I models, their practical utility is hampered by their large model size and the computational burden of the iterative denoising process. In this paper, we explore the compression potential of these I2I models in a task-oriented manner and introduce a novel method for reducing both model size and the number of timesteps. Through extensive experiments, we observe key insights and use our empirical knowledge to develop practical solutions that aim for near-optimal results with minimal exploration costs. We validate the effectiveness of our method by applying it to InstructPix2Pix for image editing and StableSR for image restoration. Our approach achieves satisfactory output quality with 39.2% and 56.4% reduction in model footprint and 81.4% and 68.7% decrease in latency to InstructPix2Pix and StableSR, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着大规模文本到图像（T2I）扩散模型的最新进展产生了显着的高质量图像，各种下游图像到图像（I2I）应用程序也随之出现。尽管这些 I2I 模型取得了令人印象深刻的结果，但它们的实用性却受到模型尺寸大和迭代去噪过程的计算负担的阻碍。在本文中，我们以面向任务的方式探索这些 I2I 模型的压缩潜力，并介绍了一种减少模型大小和时间步数的新方法。通过大量的实验，我们观察到关键的见解，并利用我们的经验知识来开发实用的解决方案，旨在以最小的勘探成本获得接近最佳的结果。我们通过将其应用于 InstructPix2Pix 进行图像编辑和 StableSR 进行图像恢复来验证该方法的有效性。我们的方法实现了令人满意的输出质量，模型占用空间减少了 39.2% 和 56.4%，InstructPix2Pix 和 StableSR 的延迟分别减少了 81.4% 和 68.7%。</details>
**PDF:** <http://arxiv.org/pdf/2401.17547v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Binding Touch to Everything: Learning Unified Multimodal Tactile Representations**<br />
**Title_cn:** 将触摸与一切结合起来：学习统一的多模态触觉表征<br />
**Authors:** Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/</details>
**Abstract_cn:** <details><summary>译文: </summary>将触摸与其他方式联系起来的能力对人类和计算系统具有巨大的影响。然而，由于昂贵的数据收集过程和非标准化的传感器输出，触摸多模态学习仍然具有挑战性。我们推出 UniTouch，这是一种基于视觉的触摸传感器的统一触觉模型，可连接到多种模式，包括视觉、语言和声音。我们通过将 UniTouch 嵌入与已经与各种其他模式相关联的预训练图像嵌入对齐来实现这一目标。我们进一步提出可学习的传感器特定标记，允许模型同时从一组异构触觉传感器中学习。 UniTouch 能够在零样本设置下执行各种触摸传感任务，从机器人抓取预测到触摸图像问答。据我们所知，UniTouch 是第一个展示此类功能的公司。项目页面：https://cfeng16.github.io/UniTouch/</details>
**PDF:** <http://arxiv.org/pdf/2401.18084v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Controllable Dense Captioner with Multimodal Embedding Bridging**<br />
**Title_cn:** 具有多模式嵌入桥接的可控密集字幕器<br />
**Authors:** Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, Fang Wan<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance. ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module. While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance. BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions. Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP). Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning. Code is available at https://github.com/callsys/ControlCap.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种可控的密集字幕器（ControlCap），它通过引入语言指导来适应用户对密集字幕的意图。 ControlCap被定义为多模态嵌入桥接架构，包括多模态嵌入生成（MEG）模块和双向嵌入桥接（BEB）模块。虽然 MEG 模块通过将详细信息的嵌入与上下文感知信息相结合来表示对象/区域，但它还通过利用它们作为语言指导，赋予 ControlCap 对专门控件的适应性。 BEB 模块通过从视觉领域借用/返回特征并收集这些特征来预测文本描述，从而使语言指导与视觉嵌入保持一致。在 Visual Genome 和 VG-COCO 数据集上的实验表明，ControlCap 分别比最先进的方法高出 1.5% 和 3.7% (mAP)。最后但并非最不重要的一点是，凭借将区域类别对转换为区域文本对的能力，ControlCap 能够充当密集字幕的强大数据引擎。代码可在 https://github.com/callsys/ControlCap 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17910v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis**<br />
**Title_cn:** 邻近 QA：释放多模态大型语言模型的力量进行空间邻近分析<br />
**Authors:** Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal large language models (MLLMs) have demonstrated remarkable vision-language capabilities, primarily due to the exceptional in-context understanding and multi-task learning strengths of large language models (LLMs). The advent of visual instruction tuning has further enhanced MLLMs' performance in vision-language understanding. However, while existing MLLMs adeptly recognize \textit{what} objects are in an image, they still face challenges in effectively discerning \textit{where} these objects are, particularly along the distance (scene depth) axis. To overcome this limitation in MLLMs, we introduce Proximity Question Answering (Proximity QA), a novel framework designed to enable MLLMs to infer the proximity relationship between objects in images. The framework operates in two phases: the first phase focuses on guiding the models to understand the relative depth of objects, and the second phase further encourages the models to infer the proximity relationships between objects based on their depth perceptions. We also propose a VQA dataset called Proximity-110K, containing additional instructions that incorporate depth information and the proximity relationships of objects. We have conducted extensive experiments to validate Proximity QA's superior ability in depth perception and proximity analysis, outperforming other state-of-the-art MLLMs. Code and dataset will be released at \textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）已经表现出了卓越的视觉语言能力，这主要归功于大语言模型（LLM）卓越的上下文理解和多任务学习优势。视觉指令调优的出现进一步增强了 MLLM 在视觉语言理解方面的性能。然而，虽然现有的 MLLM 能够熟练地识别图像中的 \textit{what} 对象，但它们在有效识别 \textit{where} 这些对象方面仍然面临挑战，特别是沿着距离（场景深度）轴。为了克服 MLLM 中的这一限制，我们引入了邻近问答（Proximity QA），这是一种新颖的框架，旨在使 MLLM 能够推断图像中对象之间的邻近关系。该框架分两个阶段运行：第一阶段侧重于引导模型理解对象的相对深度，第二阶段进一步鼓励模型根据深度感知推断对象之间的邻近关系。我们还提出了一个名为 Proximity-110K 的 VQA 数据集，其中包含包含深度信息和对象邻近关系的附加指令。我们进行了大量的实验来验证 Proximity QA 在深度感知和邻近分析方面的卓越能力，优于其他最先进的 MLLM。代码和数据集将在 \textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git} 发布。</details>
**PDF:** <http://arxiv.org/pdf/2401.17862v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval**<br />
**Title_cn:** M2-RAAP：一种多模式方法，用于推进基于适应的预训练，实现有效且高效的零样本视频文本检索<br />
**Authors:** Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, Qingpei Guo<br />
**Abstract:** <details><summary>原文: </summary>We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种多模式方法，用于推进基于适应的预训练，以实现有效且高效的零镜头视频文本检索，称为 M2-RAAP。在流行的图像文本模型（如CLIP）上，当前大多数基于自适应的视频文本预训练方法都面临三个主要问题，即噪声数据语料、耗时的预训练和有限的性能增益。为此，我们进行了一项全面的研究，包括视频文本预训练的四个关键步骤。具体来说，我们研究 1) 数据过滤和细化，2) 视频输入类型选择，3) 时间建模，以及 4) 视频特征增强。然后，我们将这项实证研究总结为 M2-RAAP 配方，其中我们的技术贡献在于 1) 数据过滤和文本重写管道，产生 1M 高质量双语视频文本对，2) 将视频输入替换为关键帧来加速预训练，3）辅助字幕引导（ACG）策略来增强视频功能。我们通过在来自不同语言的两个精炼视频文本数据集上采用三个图像文本基础模型进行了广泛的实验，验证了 M2-RAAP 用于基于适应的预训练的鲁棒性和可重复性。结果表明，M2-RAAP 具有卓越的性能，显着减少了数据量 (-90%) 和时间消耗 (-95%)，在四个英文零样本检索数据集和两个中文零样本检索数据集上建立了新的 SOTA。我们正在准备完善的双语数据注释和代码库，可在 https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP 获取。</details>
**PDF:** <http://arxiv.org/pdf/2401.17797v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks**<br />
**Title_cn:** SNP-S3：各种视频文本任务的共享网络预训练和显着语义强化<br />
**Authors:** Xingning Dong, Qingpei Guo, Tian Gan, Qing Wang, Jianlong Wu, Xiangyuan Ren, Yuan Cheng, Wei Chu<br />
**Abstract:** <details><summary>原文: </summary>We present a framework for learning cross-modal video representations by directly pre-training on raw data to facilitate various downstream video-text tasks. Our main contributions lie in the pre-training framework and proxy tasks. First, based on the shortcomings of two mainstream pixel-level pre-training architectures (limited applications or less efficient), we propose Shared Network Pre-training (SNP). By employing one shared BERT-type network to refine textual and cross-modal features simultaneously, SNP is lightweight and could support various downstream applications. Second, based on the intuition that people always pay attention to several "significant words" when understanding a sentence, we propose the Significant Semantic Strengthening (S3) strategy, which includes a novel masking and matching proxy task to promote the pre-training performance. Experiments conducted on three downstream video-text tasks and six datasets demonstrate that, we establish a new state-of-the-art in pixel-level video-text pre-training; we also achieve a satisfactory balance between the pre-training efficiency and the fine-tuning performance. The codebase are available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个通过直接对原始数据进行预训练来学习跨模式视频表示的框架，以促进各种下游视频文本任务。我们的主要贡献在于预训练框架和代理任务。首先，基于两种主流像素级预训练架构的缺点（应用有限或效率较低），我们提出共享网络预训练（SNP）。通过采用一个共享的 BERT 型网络同时细化文本和跨模态特征，SNP 是轻量级的，可以支持各种下游应用。其次，基于人们在理解句子时总是关注几个“重要单词”的直觉，我们提出了重要语义强化（S3）策略，其中包括一种新颖的掩蔽和匹配代理任务来提升预训练性能。对三个下游视频文本任务和六个数据集进行的实验表明，我们在像素级视频文本预训练中建立了新的最先进技术；我们还在预训练效率和微调性能之间取得了令人满意的平衡。代码库位于 https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp。</details>
**PDF:** <http://arxiv.org/pdf/2401.17773v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **DROP: Decouple Re-Identification and Human Parsing with Task-specific Features for Occluded Person Re-identification**<br />
**Title_cn:** DROP：将重新识别和人体解析与特定于任务的特征分离以进行被遮挡人员重新识别<br />
**Authors:** Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu, Qingsong Zhao, Cairong Zhao<br />
**Abstract:** <details><summary>原文: </summary>The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP) method for occluded person re-identification (ReID). Unlike mainstream approaches using global features for simultaneous multi-task learning of ReID and human parsing, or relying on semantic information for attention guidance, DROP argues that the inferior performance of the former is due to distinct granularity requirements for ReID and human parsing features. ReID focuses on instance part-level differences between pedestrian parts, while human parsing centers on semantic spatial context, reflecting the internal structure of the human body. To address this, DROP decouples features for ReID and human parsing, proposing detail-preserving upsampling to combine varying resolution feature maps. Parsing-specific features for human parsing are decoupled, and human position information is exclusively added to the human parsing branch. In the ReID branch, a part-aware compactness loss is introduced to enhance instance-level part differences. Experimental results highlight the efficacy of DROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke, surpassing two mainstream methods. The codebase is accessible at https://github.com/shuguang-52/DROP.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了用于遮挡行人重新识别（ReID）的解耦重新识别和人体解析（DROP）方法。与使用全局特征进行 ReID 和人类解析同时多任务学习，或依赖语义信息进行注意力引导的主流方法不同，DROP 认为，前者的性能较差是由于 ReID 和人类解析特征的粒度要求不同。 ReID侧重于行人部位之间的实例部位级别差异，而人体解析则侧重于语义空间上下文，反映人体的内部结构。为了解决这个问题，DROP 将 ReID 和人体解析的特征解耦，提出保留细节的上采样以组合不同分辨率的特征图。人类解析的解析特定特征是解耦的，并且人类位置信息专门添加到人类解析分支中。在 ReID 分支中，引入了零件感知紧凑性损失以增强实例级零件差异。实验结果凸显了 DROP 的有效性，特别是在 Occlusion-Duke 上达到 76.8% 的 Rank-1 准确率，超越了两种主流方法。代码库可从 https://github.com/shuguang-52/DROP 访问。</details>
**PDF:** <http://arxiv.org/pdf/2401.18032v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement**<br />
**Title_cn:** LaneGraph2Seq：通过点边编码和连接增强使用语言模型提取车道拓扑<br />
**Authors:** Renyuan Peng, Xinyue Cai, Hang Xu, Jiachen Lu, Feng Wen, Wei Zhang, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>Understanding road structures is crucial for autonomous driving. Intricate road structures are often depicted using lane graphs, which include centerline curves and connections forming a Directed Acyclic Graph (DAG). Accurate extraction of lane graphs relies on precisely estimating vertex and edge information within the DAG. Recent research highlights Transformer-based language models' impressive sequence prediction abilities, making them effective for learning graph representations when graph data are encoded as sequences. However, existing studies focus mainly on modeling vertices explicitly, leaving edge information simply embedded in the network. Consequently, these approaches fall short in the task of lane graph extraction. To address this, we introduce LaneGraph2Seq, a novel approach for lane graph extraction. It leverages a language model with vertex-edge encoding and connectivity enhancement. Our serialization strategy includes a vertex-centric depth-first traversal and a concise edge-based partition sequence. Additionally, we use classifier-free guidance combined with nucleus sampling to improve lane connectivity. We validate our method on prominent datasets, nuScenes and Argoverse 2, showcasing consistent and compelling results. Our LaneGraph2Seq approach demonstrates superior performance compared to state-of-the-art techniques in lane graph extraction.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解道路结构对于自动驾驶至关重要。复杂的道路结构通常使用车道图来描述，其中包括形成有向无环图 (DAG) 的中心线曲线和连接。车道图的准确提取依赖于精确估计 DAG 内的顶点和边缘信息。最近的研究强调了基于 Transformer 的语言模型令人印象深刻的序列预测能力，使它们在图数据编码为序列时能够有效地学习图表示。然而，现有的研究主要集中在显式地对顶点进行建模，而将边缘信息简单地嵌入到网络中。因此，这些方法在车道图提取任务中存在不足。为了解决这个问题，我们引入了 LaneGraph2Seq，一种用于车道图提取的新方法。它利用具有点边编码和连接增强功能的语言模型。我们的序列化策略包括以顶点为中心的深度优先遍历和简洁的基于边缘的分区序列。此外，我们使用无分类器指导与核采样相结合来改善车道连接性。我们在著名数据集 nuScenes 和 Argoverse 2 上验证了我们的方法，展示了一致且令人信服的结果。与泳道图提取中最先进的技术相比，我们的 LaneGraph2Seq 方法表现出卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.17609v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields**<br />
**Title_cn:** ReplaceAnything3D：使用组合神经辐射场进行文本引导的 3D 场景编辑<br />
**Authors:** Edward Bartrum, Thu Nguyen-Phuoc, Chris Xie, Zhengqin Li, Numair Khan, Armen Avetisyan, Douglas Lanman, Lei Xiao<br />
**Abstract:** <details><summary>原文: </summary>We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入 ReplaceAnything3D 模型 (RAM3D)，这是一种新颖的文本引导 3D 场景编辑方法，可以替换场景中的特定对象。给定场景的多视图图像、描述要替换的对象的文本提示以及描述新对象的文本提示，我们的擦除和替换方法可以有效地将场景中的对象与新生成的内容交换，同时保持跨对象的 3D 一致性多种观点。我们通过将 ReplaceAnything3D 应用于各种逼真的 3D 场景来展示 ReplaceAnything3D 的多功能性，展示修改后的前景对象的结果，这些对象与场景的其余部分很好地集成，而不影响其整体完整性。</details>
**PDF:** <http://arxiv.org/pdf/2401.17895v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Fine-Grained Zero-Shot Learning: Advances, Challenges, and Prospects**<br />
**Title_cn:** 细粒度零样本学习：进展、挑战和前景<br />
**Authors:** Jingcai Guo, Zhijie Rao, Song Guo, Jingren Zhou, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Recent zero-shot learning (ZSL) approaches have integrated fine-grained analysis, i.e., fine-grained ZSL, to mitigate the commonly known seen/unseen domain bias and misaligned visual-semantics mapping problems, and have made profound progress. Notably, this paradigm differs from existing close-set fine-grained methods and, therefore, can pose unique and nontrivial challenges. However, to the best of our knowledge, there remains a lack of systematic summaries of this topic. To enrich the literature of this domain and provide a sound basis for its future development, in this paper, we present a broad review of recent advances for fine-grained analysis in ZSL. Concretely, we first provide a taxonomy of existing methods and techniques with a thorough analysis of each category. Then, we summarize the benchmark, covering publicly available datasets, models, implementations, and some more details as a library. Last, we sketch out some related applications. In addition, we discuss vital challenges and suggest potential future directions.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的零样本学习（ZSL）方法集成了细粒度分析，即细粒度 ZSL，以减轻众所周知的可见/不可见领域偏差和错位的视觉语义映射问题，并取得了深远的进展。值得注意的是，这种范式不同于现有的紧密集细粒度方法，因此可能会带来独特且重要的挑战。然而，据我们所知，仍然缺乏对该主题的系统总结。为了丰富该领域的文献并为其未来的发展提供坚实的基础，在本文中，我们对 ZSL 细粒度分析的最新进展进行了广泛的回顾。具体来说，我们首先提供现有方法和技术的分类，并对每个类别进行彻底分析。然后，我们总结基准，涵盖公开可用的数据集、模型、实现以及作为库的更多细节。最后，我们勾勒出一些相关的应用。此外，我们还讨论了重大挑战并提出了潜在的未来方向。</details>
**PDF:** <http://arxiv.org/pdf/2401.17766v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow**<br />
**Title_cn:** 探索夜间光流的常见外观边界适应<br />
**Authors:** Hanyu Zhou, Yi Chang, Haoyue Liu, Wending Yan, Yuxing Duan, Zhiwei Shi, Luxin Yan<br />
**Abstract:** <details><summary>原文: </summary>We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge from auxiliary domain to nighttime domain in either input visual space or output motion space. However, this direct adaptation is ineffective, since there exists a large domain gap due to the intrinsic heterogeneous nature of the feature representations between auxiliary and nighttime domains. To overcome this issue, we explore a common-latent space as the intermediate bridge to reinforce the feature alignment between auxiliary and nighttime domains. In this work, we exploit two auxiliary daytime and event domains, and propose a novel common appearance-boundary adaptation framework for nighttime optical flow. In appearance adaptation, we employ the intrinsic image decomposition to embed the auxiliary daytime image and the nighttime image into a reflectance-aligned common space. We discover that motion distributions of the two reflectance maps are very similar, benefiting us to consistently transfer motion appearance knowledge from daytime to nighttime domain. In boundary adaptation, we theoretically derive the motion correlation formula between nighttime image and accumulated events within a spatiotemporal gradient-aligned common space. We figure out that the correlation of the two spatiotemporal gradient maps shares significant discrepancy, benefitting us to contrastively transfer boundary knowledge from event to nighttime domain. Moreover, appearance adaptation and boundary adaptation are complementary to each other, since they could jointly transfer global motion and local boundary knowledge to the nighttime domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究了夜间光流的一项具有挑战性的任务，该任务受到纹理减弱和噪声放大的影响。这些退化削弱了辨别性视觉特征，从而导致无效的运动特征匹配。通常，现有方法采用域适应将知识从输入视觉空间或输出运动空间中的辅助域转移到夜间域。然而，这种直接适应是无效的，因为由于辅助域和夜间域之间的特征表示的内在异构性，存在很大的域差距。为了克服这个问题，我们探索了一个公共潜在空间作为中间桥梁，以加强辅助域和夜间域之间的特征对齐。在这项工作中，我们利用两个辅助的白天和事件域，并提出了一种新颖的夜间光流通用外观边界适应框架。在外观适应中，我们采用本征图像分解将辅助白天图像和夜间图像嵌入到反射率对齐的公共空间中。我们发现两个反射图的运动分布非常相似，这有利于我们一致地将运动外观知识从白天转移到夜间领域。在边界适应中，我们从理论上推导了夜间图像和时空梯度对齐的公共空间内累积事件之间的运动相关公式。我们发现两个时空梯度图的相关性存在显着差异，这有利于我们将边界知识从事件域转移到夜间域。此外，外观适应和边界适应是相互补充的，因为它们可以共同将全局运动和局部边界知识转移到夜间领域。</details>
**PDF:** <http://arxiv.org/pdf/2401.17642v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Reimagining Reality: A Comprehensive Survey of Video Inpainting Techniques**<br />
**Title_cn:** 重新想象现实：视频修复技术的综合调查<br />
**Authors:** Shreyank N Gowda, Yash Thakre, Shashank Narayana Gowda, Xiaobo Jin<br />
**Abstract:** <details><summary>原文: </summary>This paper offers a comprehensive analysis of recent advancements in video inpainting techniques, a critical subset of computer vision and artificial intelligence. As a process that restores or fills in missing or corrupted portions of video sequences with plausible content, video inpainting has evolved significantly with the advent of deep learning methodologies. Despite the plethora of existing methods and their swift development, the landscape remains complex, posing challenges to both novices and established researchers. Our study deconstructs major techniques, their underpinning theories, and their effective applications. Moreover, we conduct an exhaustive comparative study, centering on two often-overlooked dimensions: visual quality and computational efficiency. We adopt a human-centric approach to assess visual quality, enlisting a panel of annotators to evaluate the output of different video inpainting techniques. This provides a nuanced qualitative understanding that complements traditional quantitative metrics. Concurrently, we delve into the computational aspects, comparing inference times and memory demands across a standardized hardware setup. This analysis underscores the balance between quality and efficiency: a critical consideration for practical applications where resources may be constrained. By integrating human validation and computational resource comparison, this survey not only clarifies the present landscape of video inpainting techniques but also charts a course for future explorations in this vibrant and evolving field.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文对视频修复技术的最新进展进行了全面分析，视频修复技术是计算机视觉和人工智能的一个重要子集。作为一种用可信内容恢复或填充视频序列中丢失或损坏部分的过程，视频修复随着深度学习方法的出现而发生了显着发展。尽管现有方法众多且发展迅速，但情况仍然复杂，对新手和成熟的研究人员都提出了挑战。我们的研究解构了主要技术、其基础理论及其有效应用。此外，我们进行了详尽的比较研究，重点关注两个经常被忽视的维度：视觉质量和计算效率。我们采用以人为本的方法来评估视觉质量，招募一组注释者来评估不同视频修复技术的输出。这提供了细致入微的定性理解，补充了传统的定量指标。同时，我们深入研究计算方面，比较标准化硬件设置的推理时间和内存需求。该分析强调了质量和效率之间的平衡：对于资源可能受到限制的实际应用来说，这是一个关键的考虑因素。通过整合人类验证和计算资源比较，这项调查不仅阐明了视频修复技术的现状，还为这个充满活力和不断发展的领域的未来探索制定了路线。</details>
**PDF:** <http://arxiv.org/pdf/2401.17883v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **RADIN: Souping on a Budget**<br />
**Title_cn:** RADIN：预算中的汤<br />
**Authors:** Thibaut Menes, Olivier Risser-Maroix<br />
**Abstract:** <details><summary>原文: </summary>Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).</details>
**Abstract_cn:** <details><summary>译文: </summary>Model Soups 扩展了随机权重平均 (SWA)，结合了使用不同超参数进行微调的模型。然而，由于子集选择问题，它们的采用受到计算挑战的阻碍。在本文中，我们建议通过使用平均集成 Logits 性能来近似 soups 性能来加速模型 soups。理论见解验证了任何混合比下的整体逻辑与重量平均汤之间的一致性。我们的资源调整汤制作 (RADIN) 程序因允许灵活的评估预算而脱颖而出，使用户能够调整适合其资源的探索预算，同时与之前的贪婪方法相比（在 ImageNet 上高达 4%）以较低的预算提高性能。</details>
**PDF:** <http://arxiv.org/pdf/2401.17790v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Robustly overfitting latents for flexible neural image compression**<br />
**Title_cn:** 鲁棒地过度拟合潜在的灵活神经图像压缩<br />
**Authors:** Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai<br />
**Abstract:** <details><summary>原文: </summary>Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two-class rounding. Finally, we show how refinement of the latents with our best-performing method improves the compression performance on the Tecnick dataset and how it can be deployed to partly move along the rate-distortion curve.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经图像压缩已经取得了很大的进步。最先进的模型基于变分自动编码器，并且性能优于经典模型。神经压缩模型学习将图像编码为量化的潜在表示，该表示可以有效地发送到解码器，解码器将量化的潜在表示解码为重建图像。虽然这些模型在实践中被证明是成功的，但由于不完善的优化以及编码器和解码器容量的限制，它们导致了次优结果。最近的工作展示了如何使用随机 Gumbel 退火 (SGA) 来细化预训练神经图像压缩模型的潜力。我们通过引入 SGA+ 来扩展这个想法，它包含三种基于 SGA 的不同方法。此外，我们对我们提出的方法进行了详细分析，展示了它们如何提高性能，并表明它们对超参数选择不太敏感。此外，我们还展示了如何将每种方法扩展到三级舍入而不是两级舍入。最后，我们展示了如何使用性能最佳的方法对潜在特征进行细化，从而提高 Tecnick 数据集的压缩性能，以及如何部署它以部分沿着率失真曲线移动。</details>
**PDF:** <http://arxiv.org/pdf/2401.17789v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **COMET: Contrastive Mean Teacher for Online Source-Free Universal Domain Adaptation**<br />
**Title_cn:** COMET：在线无源通用域适应的对比平均老师<br />
**Authors:** Pascal Schlachter, Bin Yang<br />
**Abstract:** <details><summary>原文: </summary>In real-world applications, there is often a domain shift from training to test data. This observation resulted in the development of test-time adaptation (TTA). It aims to adapt a pre-trained source model to the test data without requiring access to the source data. Thereby, most existing works are limited to the closed-set assumption, i.e. there is no category shift between source and target domain. We argue that in a realistic open-world setting a category shift can appear in addition to a domain shift. This means, individual source classes may not appear in the target domain anymore, samples of new classes may be part of the target domain or even both at the same time. Moreover, in many real-world scenarios the test data is not accessible all at once but arrives sequentially as a stream of batches demanding an immediate prediction. Hence, TTA must be applied in an online manner. To the best of our knowledge, the combination of these aspects, i.e. online source-free universal domain adaptation (online SF-UniDA), has not been studied yet. In this paper, we introduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario. It applies a contrastive loss to rebuild a feature space where the samples of known classes build distinct clusters and the samples of new classes separate well from them. It is complemented by an entropy loss which ensures that the classifier output has a small entropy for samples of known classes and a large entropy for samples of new classes to be easily detected and rejected as unknown. To provide the losses with reliable pseudo labels, they are embedded into a mean teacher (MT) framework. We evaluate our method across two datasets and all category shifts to set an initial benchmark for online SF-UniDA. Thereby, COMET yields state-of-the-art performance and proves to be consistent and robust across a variety of different scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>在实际应用中，通常会出现从训练数据到测试数据的领域转变。这一观察导致了测试时间适应（TTA）的发展。它的目的是使预先训练的源模型适应测试数据，而不需要访问源数据。因此，大多数现有工作仅限于闭集假设，即源域和目标域之间没有类别转换。我们认为，在现实的开放世界环境中，除了领域转移之外，还可能出现类别转移。这意味着，单个源类可能不再出现在目标域中，新类的样本可能是目标域的一部分，甚至同时是两者。此外，在许多现实场景中，测试数据无法一次性全部访问，而是作为需要立即预测的批次流按顺序到达。因此，TTA必须以在线方式申请。据我们所知，这些方面的组合，即在线无源通用域适应（online SF-UniDA），尚未被研究。在本文中，我们介绍了针对这种新颖场景量身定制的对比平均教师（COMET）。它应用对比损失来重建特征空间，其中已知类的样本构建不同的聚类，而新类的样本与它们很好地分离。它由熵损失来补充，熵损失确保分类器输出对于已知类别的样本具有较小的熵，对于新类别的样本具有较大的熵，以便易于检测并拒绝为未知。为了给损失提供可靠的伪标签，它们被嵌入到平均教师（MT）框架中。我们在两个数据集和所有类别变化中评估我们的方法，为在线 SF-UniDA 设置初始基准。因此，COMET 产生了最先进的性能，并证明在各种不同的场景中都保持一致和稳健。</details>
**PDF:** <http://arxiv.org/pdf/2401.17728v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking**<br />
**Title_cn:** 揭示多视角多人关联和跟踪的自我监督力量<br />
**Authors:** Wei Feng, Feifan Wang, Ruize Han, Zekun Qian, Song Wang<br />
**Abstract:** <details><summary>原文: </summary>Multi-view multi-human association and tracking (MvMHAT), is a new but important problem for multi-person scene video surveillance, aiming to track a group of people over time in each view, as well as to identify the same person across different views at the same time, which is different from previous MOT and multi-camera MOT tasks only considering the over-time human tracking. This way, the videos for MvMHAT require more complex annotations while containing more information for self learning. In this work, we tackle this problem with a self-supervised learning aware end-to-end network. Specifically, we propose to take advantage of the spatial-temporal self-consistency rationale by considering three properties of reflexivity, symmetry and transitivity. Besides the reflexivity property that naturally holds, we design the self-supervised learning losses based on the properties of symmetry and transitivity, for both appearance feature learning and assignment matrix optimization, to associate the multiple humans over time and across views. Furthermore, to promote the research on MvMHAT, we build two new large-scale benchmarks for the network training and testing of different algorithms. Extensive experiments on the proposed benchmarks verify the effectiveness of our method. We have released the benchmark and code to the public.</details>
**Abstract_cn:** <details><summary>译文: </summary>多视图多人关联与跟踪（MvMHAT）是多人场景视频监控的一个新但重要的问题，旨在随着时间的推移在每个视图中跟踪一群人，以及在不同的视图中识别同一个人与之前的 MOT 和多摄像头 MOT 任务只考虑超时的人体跟踪不同。这样，MvMHAT 的视频需要更复杂的注释，同时包含更多用于自学习的信息。在这项工作中，我们通过自监督学习感知端到端网络来解决这个问题。具体来说，我们建议通过考虑自反性、对称性和传递性三个属性来利用时空自洽原理。除了自然存在的自反性属性之外，我们还基于对称性和传递性的属性设计了自监督学习损失，用于外观特征学习和分配矩阵优化，以随时间和跨视图关联多个人。此外，为了促进 MvMHAT 的研究，我们建立了两个新的大规模基准用于不同算法的网络训练和测试。对所提出的基准进行大量实验验证了我们方法的有效性。我们已向公众发布了基准测试和代码。</details>
**PDF:** <http://arxiv.org/pdf/2401.17617v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion**<br />
**Title_cn:** 敏捷但安全：学习无碰撞高速腿式运动<br />
**Authors:** Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya Shi<br />
**Abstract:** <details><summary>原文: </summary>Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.</details>
**Abstract_cn:** <details><summary>译文: </summary>在杂乱环境中导航的腿式机器人必须同时敏捷，以高效执行任务，并安全地避免与障碍物或人类发生碰撞。现有研究要么开发保守的控制器（< 1.0 m/s）以确保安全，要么专注于敏捷性而不考虑潜在的致命碰撞。本文介绍了敏捷但安全（ABS），这是一种基于学习的控制框架，可实现四足机器人敏捷且无碰撞的运动。 ABS 涉及在障碍物中执行敏捷运动技能的敏捷策略和防止故障的恢复策略，协同实现高速和无碰撞导航。 ABS 中的策略切换由学习控制理论的到达避免价值网络控制，该网络还将恢复策略作为目标函数进行指导，从而在闭环中保护机器人。训练过程涉及学习敏捷策略、避免触及价值网络、恢复策略和外部感知表示网络，所有这些都是在模拟中进行的。这些训练有素的模块可以通过机载传感和计算直接部署在现实世界中，从而在具有静态和动态障碍物的有限室内和室外空间中实现高速、无碰撞导航。</details>
**PDF:** <http://arxiv.org/pdf/2401.17583v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Is Registering Raw Tagged-MR Enough for Strain Estimation in the Era of Deep Learning?**<br />
**Title_cn:** 注册Raw Tagged-MR足以用于深度学习时代的应变估计吗？<br />
**Authors:** Zhangxing Bian, Ahmed Alshareef, Shuwen Wei, Junyu Chen, Yuli Wang, Jonghye Woo, Dzung L. Pham, Jiachen Zhuo, Aaron Carass, Jerry L. Prince<br />
**Abstract:** <details><summary>原文: </summary>Magnetic Resonance Imaging with tagging (tMRI) has long been utilized for quantifying tissue motion and strain during deformation. However, a phenomenon known as tag fading, a gradual decrease in tag visibility over time, often complicates post-processing. The first contribution of this study is to model tag fading by considering the interplay between $T_1$ relaxation and the repeated application of radio frequency (RF) pulses during serial imaging sequences. This is a factor that has been overlooked in prior research on tMRI post-processing. Further, we have observed an emerging trend of utilizing raw tagged MRI within a deep learning-based (DL) registration framework for motion estimation. In this work, we evaluate and analyze the impact of commonly used image similarity objectives in training DL registrations on raw tMRI. This is then compared with the Harmonic Phase-based approach, a traditional approach which is claimed to be robust to tag fading. Our findings, derived from both simulated images and an actual phantom scan, reveal the limitations of various similarity losses in raw tMRI and emphasize caution in registration tasks where image intensity changes over time.</details>
**Abstract_cn:** <details><summary>译文: </summary>带标记的磁共振成像 (tMRI) 长期以来一直用于量化变形过程中的组织运动和应变。然而，一种称为标签褪色的现象，即标签可见性随着时间的推移逐渐降低，通常会使后处理变得复杂。这项研究的第一个贡献是通过考虑串行成像序列期间 $T_1$ 弛豫和射频 (RF) 脉冲重复应用之间的相互作用来对标签衰落进行建模。这是先前 tMRI 后处理研究中被忽视的一个因素。此外，我们还观察到在基于深度学习 (DL) 的配准框架中利用原始标记 MRI 进行运动估计的新兴趋势。在这项工作中，我们评估和分析了在原始 tMRI 上训练 DL 配准时常用的图像相似性目标的影响。然后将其与基于谐波相位的方法进行比较，这是一种据称对标签衰落具有鲁棒性的传统方法。我们的研究结果来自模拟图像和实际的模型扫描，揭示了原始 tMRI 中各种相似性损失的局限性，并强调在图像强度随时间变化的配准任务中要小心。</details>
**PDF:** <http://arxiv.org/pdf/2401.17571v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Data-Effective Learning: A Comprehensive Medical Benchmark**<br />
**Title_cn:** 数据有效学习：综合医学基准<br />
**Authors:** Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan<br />
**Abstract:** <details><summary>原文: </summary>Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive experimental results show the baseline MedDEL can achieve performance comparable to the original large dataset with only 5% of the data. Establishing such an open data-effective learning benchmark is crucial for the medical AI research community because it facilitates efficient data use, promotes collaborative breakthroughs, and fosters the development of cost-effective, scalable, and impactful healthcare solutions. The project can be accessed at https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据有效学习旨在以最有影响力的方式使用数据来训练人工智能模型，其中涉及注重数据质量而不是数量的策略，确保用于训练的数据具有较高的信息价值。数据有效学习在加速人工智能训练、降低计算成本和节省数据存储方面发挥着深远的作用，这在近年来医疗数据量的增长超出了许多人的预期的情况下非常重要。然而，由于缺乏标准和综合基准，医学数据有效学习的研究还很少。为了解决这一差距，我们的论文引入了一个专门用于评估医学领域数据有效学习的综合基准。该基准包括来自 31 个医疗中心的数百万数据样本的数据集 (DataDEL)、比较基线方法 (MedDEL) 和新的评估指标 (NormDEL)，以客观地衡量数据有效的学习绩效。我们广泛的实验结果表明，基线 MedDEL 仅需 5% 的数据即可实现与原始大型数据集相当的性能。建立这样一个开放的数据有效的学习基准对于医学人工智能研究社区至关重要，因为它有助于有效的数据使用，促进协作突破，并促进开发具有成本效益、可扩展和有影响力的医疗保健解决方案。该项目可以通过 https://github.com/shadow2469/Data-Effective-Learning-A-Compressive-Medical-Benchmark.git 访问。</details>
**PDF:** <http://arxiv.org/pdf/2401.17542v1><br />
**Code:** null<br />

