## [UPDATED!] **2024-02-01** (Publish Time)

>## **分类/检测/识别/分割**
>---
>>**index:** 1<br />
**Title:** **We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline**<br />
**Title_cn:** 我们没有有效地使用视频：更新的域自适应视频分割基准<br />
**Authors:** Simar Kareer, Vivek Vijaykumar, Harsh Maheshwari, Prithvijit Chattopadhyay, Judy Hoffman, Viraj Prabhu<br />
**Abstract:** <details><summary>原文: </summary>There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA</details>
**Abstract_cn:** <details><summary>译文: </summary>在用于语义分割（DAS）的无监督域适应方面已经进行了大量的工作，试图将在图像上训练的模型从标记的源域适应到未标记的目标域。虽然绝大多数先前的工作都将其作为帧级图像 DAS 问题进行研究，但一些视频 DAS 工作试图额外利用相邻帧中存在的时间信号。然而，Video-DAS 工作历来研究了一组与 Image-DAS 不同的基准，并且交叉基准极少。在这项工作中，我们解决了这一差距。令人惊讶的是，我们发现 (1) 即使在仔细控制数据和模型架构之后，最先进的 Image-DAS 方法（HRDA 和 HRDA+MIC）} 在已建立的 Video-DAS 基准上仍优于 Video-DAS 方法（+ Viper$\rightarrow$CityscapesSeq 上的 mIoU 为 14.5 mIoU，Synthia$\rightarrow$CityscapesSeq 上的 +19.0 mIoU），以及 (2) Image-DAS 和 Video-DAS 技术的简单组合只会带来跨数据集的边际改进。为了避免 Image-DAS 和 Video-DAS 之间的孤立进展，我们开源了我们的代码库，并在通用基准上支持一套全面的 Video-DAS 和 Image-DAS 方法。代码可在 https://github.com/SimarKareer/UnifiedVideoDA 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.00868v1><br />
**Code:** <https://github.com/simarkareer/unifiedvideoda>**<br />
>>**index:** 2<br />
**Title:** **Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection**<br />
**Title_cn:** 面向分布外检测的最佳特征整形方法<br />
**Authors:** Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould<br />
**Abstract:** <details><summary>原文: </summary>Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>特征整形是指在分布外 (OOD) 检测方面表现出最先进性能的一系列方法。这些方法通常来自预训练深度学习模型的倒数第二层来操纵特征表示，以便更好地区分分布内 (ID) 和 OOD 样本。然而，现有的特征塑造方法通常采用针对特定模型架构和 OOD 数据集手动设计的规则，从而限制了其泛化能力。为了解决这一差距，我们首先制定一个用于研究特征塑造方法的抽象优化框架。然后，我们提出了使用简单的分段常数整形函数对框架进行具体简化，并表明现有的特征整形方法近似于具体优化问题的最优解。此外，假设 OOD 数据不可访问，我们提出了一种公式，仅利用 ID 数据即可生成分段常数整形函数的封闭式解。通过大量的实验，我们表明，通过我们的方法优化的特征整形函数提高了 OOD 检测在各种数据集和模型架构中的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.00865v1><br />
**Code:** <https://github.com/qinyu-allen-zhao/optfsood>**<br />
>>**index:** 3<br />
**Title:** **Automatic Segmentation of the Spinal Cord Nerve Rootlets**<br />
**Title_cn:** 脊髓神经根的自动分割<br />
**Authors:** Jan Valosek, Theo Mathieu, Raphaelle Schlienger, Olivia S. Kowalczyk, Julien Cohen-Adad<br />
**Abstract:** <details><summary>原文: </summary>Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions. The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.</details>
**Abstract_cn:** <details><summary>译文: </summary>脊神经根的精确识别与描绘脊髓水平有关，以研究脊髓的功能活动。本研究的目标是开发一种自动方法，用于从 T2 加权磁共振成像 (MRI) 扫描中对脊神经根进行语义分割。来自两个开放获取 MRI 数据集的图像用于训练 3D 多类卷积神经网络，使用主动学习方法来分割 C2-C8 背神经根。每个输出类别对应于一个脊柱水平。该方法在训练期间未见过的数据集的 3T T2 加权图像上进行了测试，以评估站点间、会话间和分辨率间的变异性。测试 Dice 得分为 0.67 ± 0.16（根系水平的平均值 ± 标准差），表明性能良好。该方法还表现出较低的供应商间和站点间变异性（变异系数 <= 1.41 %），以及较低的会话间变异性（变异系数 <= 1.30 %），表明不同 MRI 供应商、站点之间的稳定预测，和会议。所提出的方法是开源的，可在脊髓工具箱 (SCT) v6.2 及更高版本中随时使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.00724v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Vehicle Perception from Satellite**<br />
**Title_cn:** 卫星车辆感知<br />
**Authors:** Bin Zhao, Pengfei Han, Xuelong Li<br />
**Abstract:** <details><summary>原文: </summary>Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.</details>
**Abstract_cn:** <details><summary>译文: </summary>卫星能够捕捉高分辨率视频。它使得卫星对车辆的感知成为可能。与街道监控、行车记录仪等设备相比，卫星视频提供了更广阔的城市尺度视野，从而捕捉并显示全球的交通动态场景。卫星交通监控是一项具有巨大应用潜力的新任务，包括交通拥堵预测、路径规划、车辆调度等。实际上，受分辨率和视野的限制，捕获的车辆非常小（几个像素）并且移动缓慢。更糟糕的是，这些卫星位于近地轨道（LEO）来捕捉如此高分辨率的视频，因此背景也在移动。在这种情况下，卫星视角的交通监控是一项极具挑战性的任务。为了吸引更多研究人员进入这一领域，我们建立了一个大规模的卫星交通监控基准。它支持多种任务，包括微小物体检测、计数和密度估计。该数据集基于 GTA-V 录制的 12 个卫星视频和 14 个合成视频构建。它们被分为 408 个视频片段，其中包含 7,336 个真实卫星图像和 1,960 个合成图像。总共标注了 128,801 辆车辆，每张图像中的车辆数量从 0 到 101 不等。在数据集上评估了传统计算机视觉中的几种经典和最先进的方法，以比较不同方法的性能，分析这项任务中的挑战，并讨论未来的前景。该数据集位于：https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos。</details>
**PDF:** <http://arxiv.org/pdf/2402.00703v1><br />
**Code:** <https://github.com/chenxi1510/vehicle-perception-from-satellite-videos>**<br />
>>**index:** 5<br />
**Title:** **Approximating Optimal Morphing Attacks using Template Inversion**<br />
**Title_cn:** 使用模板反转近似最佳变形攻击<br />
**Authors:** Laurent Colbois, Hatef Otroshi Shahreza, Sébastien Marcel<br />
**Abstract:** <details><summary>原文: </summary>Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run. We hope this might facilitate the development of large scale deep morph datasets for training detection models.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的工作证明了反转人脸识别系统的可行性，能够仅使用嵌入来恢复令人信服的人脸图像。我们利用这种模板反转模型来开发一种新型的深度变形攻击，该攻击基于反转理论上的最佳变形嵌入，该变形嵌入是作为源图像的人脸嵌入的平均值而获得的。我们尝试了这种方法的两种变体：第一种利用完全独立的嵌入到图像反转模型，而第二种利用预训练 StyleGAN 网络的合成网络来提高变形真实性。我们从多个源数据集生成变形攻击，并研究这些攻击针对多个人脸识别网络的有效性。我们展示了我们的方法在白盒和黑盒攻击场景中的有效性方面可以与基于深度学习的变形生成的先前最先进技术竞争并定期击败，并且运行速度更快。我们希望这可能有助于开发用于训练检测模型的大规模深度变形数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.00695v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation**<br />
**Title_cn:** 构建点云清理、平面检测和语义分割的框架<br />
**Authors:** Ilyass Abouelaziz, Youssef Mourchid<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一个框架来解决建筑点云清理、平面检测和语义分割所涉及的挑战，最终目标是增强建筑建模。我们在清理阶段的重点是通过采用基于 z 分数测量的自适应阈值技术从获取的点云数据中删除异常值。在清洁过程之后，我们使用强大的 RANSAC 范式执行平面检测。目标是进行多个平面分割，并将片段分为不同的类别，例如地板、天花板和墙壁。生成的片段可以生成代表建筑物建筑元素的准确且详细的点云。此外，我们还解决了语义分割问题，该问题在建筑物内不同组件（例如墙壁、窗户、门、屋顶和物体）的识别和分类中起着至关重要的作用。受 PointNet 架构的启发，我们提出了一种深度学习架构，用于建筑物中的高效语义分割。结果证明了所提出的框架在处理建筑建模任务方面的有效性，为提高建筑建模领域的准确性和效率铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.00692v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Coronary Artery Disease Classification with Different Lesion Degree Ranges based on Deep Learning**<br />
**Title_cn:** 基于深度学习的不同病变程度范围的冠状动脉疾病分类<br />
**Authors:** Ariadna Jiménez-Partinen, Karl Thurnhofer-Hemsi, Esteban J. Palomo, Jorge Rodríguez-Capitán, Ana I. Molina-Ramos<br />
**Abstract:** <details><summary>原文: </summary>Invasive Coronary Angiography (ICA) images are considered the gold standard for assessing the state of the coronary arteries. Deep learning classification methods are widely used and well-developed in different areas where medical imaging evaluation has an essential impact due to the development of computer-aided diagnosis systems that can support physicians in their clinical procedures. In this paper, a new performance analysis of deep learning methods for binary ICA classification with different lesion degrees is reported. To reach this goal, an annotated dataset of ICA images that contains the ground truth, the location of lesions and seven possible severity degrees ranging between 0% and 100% was employed. The ICA images were divided into 'lesion' or 'non-lesion' patches. We aim to study how binary classification performance is affected by the different lesion degrees considered in the positive class. Therefore, five known convolutional neural network architectures were trained with different input images where different lesion degree ranges were gradually incorporated until considering the seven lesion degrees. Besides, four types of experiments with and without data augmentation were designed, whose F-measure and Area Under Curve (AUC) were computed. Reported results achieved an F-measure and AUC of 92.7% and 98.1%, respectively. However, lesion classification is highly affected by the degree of the lesion intended to classify, with 15% less accuracy when <99% lesion patches are present.</details>
**Abstract_cn:** <details><summary>译文: </summary>侵入性冠状动脉造影 (ICA) 图像被认为是评估冠状动脉状态的金标准。由于计算机辅助诊断系统的发展可以支持医生的临床操作，深度学习分类方法在医学影像评估具有重要影响的不同领域得到了广泛的应用和发展。本文报告了一种新的深度学习方法对不同病变程度的二元 ICA 分类的性能分析。为了实现这一目标，使用了一个带注释的 ICA 图像数据集，其中包含真实情况、病变位置以及 0% 到 100% 之间的七种可能的严重程度。 ICA 图像被分为“病变”或“非病变”斑块。我们的目的是研究二元分类性能如何受到正类中考虑的不同病变程度的影响。因此，使用不同的输入图像来训练五种已知的卷积神经网络架构，其中逐渐合并不同的病变程度范围，直到考虑七个病变程度。此外，设计了四种类型的有和没有数据增强的实验，计算了其F-measure和曲线下面积（AUC）。报告结果的 F 测量值和 AUC 分别为 92.7% 和 98.1%。然而，病变分类很大程度上受到要分类的病变程度的影响，当存在 <99% 的病变斑块时，准确度会降低 15%。</details>
**PDF:** <http://arxiv.org/pdf/2402.00593v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **CADICA: a new dataset for coronary artery disease detection by using invasive coronary angiography**<br />
**Title_cn:** CADICA：使用侵入性冠状动脉造影检测冠状动脉疾病的新数据集<br />
**Authors:** Ariadna Jiménez-Partinen, Miguel A. Molina-Cabello, Karl Thurnhofer-Hemsi, Esteban J. Palomo, Jorge Rodríguez-Capitán, Ana I. Molina-Ramos, Manuel Jiménez-Navarro<br />
**Abstract:** <details><summary>原文: </summary>Coronary artery disease (CAD) remains the leading cause of death globally and invasive coronary angiography (ICA) is considered the gold standard of anatomical imaging evaluation when CAD is suspected. However, risk evaluation based on ICA has several limitations, such as visual assessment of stenosis severity, which has significant interobserver variability. This motivates to development of a lesion classification system that can support specialists in their clinical procedures. Although deep learning classification methods are well-developed in other areas of medical imaging, ICA image classification is still at an early stage. One of the most important reasons is the lack of available and high-quality open-access datasets. In this paper, we reported a new annotated ICA images dataset, CADICA, to provide the research community with a comprehensive and rigorous dataset of coronary angiography consisting of a set of acquired patient videos and associated disease-related metadata. This dataset can be used by clinicians to train their skills in angiographic assessment of CAD severity and by computer scientists to create computer-aided diagnostic systems to help in such assessment. In addition, baseline classification methods are proposed and analyzed, validating the functionality of CADICA and giving the scientific community a starting point to improve CAD detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>冠状动脉疾病 (CAD) 仍然是全球死亡的主要原因，当怀疑患有 CAD 时，侵入性冠状动脉造影 (ICA) 被认为是解剖成像评估的金标准。然而，基于 ICA 的风险评估有一些局限性，例如狭窄严重程度的视觉评估，观察者之间存在显着的差异。这推动了病变分类系统的开发，该系统可以支持专家的临床操作。尽管深度学习分类方法在医学成像的其他领域已经很成熟，但 ICA 图像分类仍处于早期阶段。最重要的原因之一是缺乏可用且高质量的开放获取数据集。在本文中，我们报告了一个新的带注释的 ICA 图像数据集 CADICA，为研究界提供全面且严格的冠状动脉造影数据集，其中包含一组采集的患者视频和相关的疾病相关元数据。临床医生可以使用该数据集来培训他们对 CAD 严重程度进行血管造影评估的技能，计算机科学家可以使用该数据集来创建计算机辅助诊断系统来帮助进行此类评估。此外，还提出并分析了基线分类方法，验证了 CADICA 的功能，并为科学界提供了改进 CAD 检测的起点。</details>
**PDF:** <http://arxiv.org/pdf/2402.00570v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification**<br />
**Title_cn:** 您只需要一个图卷积即可：高效的灰度图像分类<br />
**Authors:** Jacob Fein-Ashley, Tian Ye, Sachini Wickramasinghe, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna<br />
**Abstract:** <details><summary>原文: </summary>Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance. Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像分类器通常依赖卷积神经网络 (CNN) 来完成任务，而卷积神经网络本质上比多层感知器 (MLP) 更重量级，这在实时应用中可能会出现问题。此外，许多图像分类模型同时适用于 RGB 和灰度数据集。仅对灰度图像进行操作的分类器并不常见。灰度图像分类具有多种应用，包括但不限于医学图像分类和合成孔径雷达（SAR）自动目标识别（ATR）。因此，我们提出了一种使用图像矢量化视图的新颖的灰度（单通道）图像分类方法。我们通过将图像视为向量并将问题设置减少为灰度图像分类设置来利用 MLP 的轻量性。我们发现批量使用单个图卷积层可以提高模型的准确性并减少模型性能的方差。此外，我们在 FPGA 上为所提出的模型开发了一个定制加速器，并进行了多项优化以提高其性能。我们在基准灰度图像数据集上的实验结果证明了所提出模型的有效性，与其他最先进的图像分类模型相比，在各种不同的数据集上实现了大大降低的延迟（最多减少 16 美元\倍$）和具有竞争力或领先的性能。特定领域的灰度图像分类数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.00564v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Masked Conditional Diffusion Model for Enhancing Deepfake Detection**<br />
**Title_cn:** 用于增强 Deepfake 检测的屏蔽条件扩散模型<br />
**Authors:** Tiewen Chen, Shanmin Yang, Shu Hu, Zhenghan Fang, Ying Fu, Xi Wu, Xin Wang<br />
**Abstract:** <details><summary>原文: </summary>Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset. However, their results severely degrade when confronted with forged samples that the model has not yet seen during training. In this paper, deepfake data to help detect deepfakes. this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection. It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts. Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models.</details>
**Abstract_cn:** <details><summary>译文: </summary>当训练和测试人脸来自同一数据集时，最近关于深度伪造检测的研究取得了有希望的结果。然而，当遇到模型在训练过程中尚未见过的伪造样本时，他们的结果会严重下降。在本文中，deepfake 数据可以帮助检测deepfakes。本文提出了我们对基于扩散模型的数据增强的新见解，并提出了一种用于增强深度伪造检测的掩模条件扩散模型（MCDM）。它从蒙面的原始人脸中生成各种伪造的人脸，从而鼓励深度伪造检测模型学习通用且稳健的表示，而不会过度拟合特殊的伪影。大量实验表明，用我们的方法生成的伪造图像质量很高，有助于提高深度伪造检测模型的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.00541v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **A Manifold Representation of the Key in Vision Transformers**<br />
**Title_cn:** 视觉变形金刚关键的多种表示<br />
**Authors:** Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad<br />
**Abstract:** <details><summary>原文: </summary>Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the budget of such representations and aim for further performance improvements based on our findings.</details>
**Abstract_cn:** <details><summary>译文: </summary>Vision Transformers 通过堆叠多个注意力块来实现多头自注意力（MSA）。查询、键和值通常交织在一起，并通过单个共享线性转换在这些块中生成。本文探讨了将键与查询和值分开的概念，并采用键的流形表示。我们的实验表明，解耦并赋予密钥流形结构可以提高模型性能。具体来说，ViT-B 在 ImageNet-1K 数据集上的 top-1 准确率提高了 0.87%，而 Swin-T 在流形键中有 8 个图表的 top-1 准确率提高了 0.52%。我们的方法还在 COCO 数据集上的对象检测和实例分割任务中产生了积极的结果。通过详细的消融研究，我们确定这些性能提升不仅仅是由于添加更多参数和计算的简单性。未来的研究可能会调查削减此类代表预算的策略，并根据我们的研究结果进一步提高绩效。</details>
**PDF:** <http://arxiv.org/pdf/2402.00534v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Bias Mitigating Few-Shot Class-Incremental Learning**<br />
**Title_cn:** 减少少样本类增量学习的偏差<br />
**Authors:** Li-Jun Zhao, Zhen-Duo Chen, Zi-Chao Zhang, Xin Luo, Xin-Shun Xu<br />
**Abstract:** <details><summary>原文: </summary>Few-shot class-incremental learning (FSCIL) aims at recognizing novel classes continually with limited novel class samples. A mainstream baseline for FSCIL is first to train the whole model in the base session, then freeze the feature extractor in the incremental sessions. Despite achieving high overall accuracy, most methods exhibit notably low accuracy for incremental classes. Some recent methods somewhat alleviate the accuracy imbalance between base and incremental classes by fine-tuning the feature extractor in the incremental sessions, but they further cause the accuracy imbalance between past and current incremental classes. In this paper, we study the causes of such classification accuracy imbalance for FSCIL, and abstract them into a unified model bias problem. Based on the analyses, we propose a novel method to mitigate model bias of the FSCIL problem during training and inference processes, which includes mapping ability stimulation, separately dual-feature classification, and self-optimizing classifiers. Extensive experiments on three widely-used FSCIL benchmark datasets show that our method significantly mitigates the model bias problem and achieves state-of-the-art performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>小样本类增量学习（FSCIL）旨在利用有限的新类样本不断识别新类。 FSCIL 的主流基线是首先在基本会话中训练整个模型，然后在增量会话中冻结特征提取器。尽管总体准确率很高，但大多数方法对于增量类的准确率明显较低。最近的一些方法通过微调增量会话中的特征提取器在一定程度上缓解了基类和增量类之间的准确性不平衡，但它们进一步导致了过去和当前增量类之间的准确性不平衡。在本文中，我们研究了 FSCIL 这种分类精度不平衡的原因，并将其抽象为统一的模型偏差问题。基于分析，我们提出了一种新方法来减轻训练和推理过程中 FSCIL 问题的模型偏差，其中包括映射能力刺激、单独的双特征分类和自优化分类器。对三个广泛使用的 FSCIL 基准数据集进行的大量实验表明，我们的方法显着减轻了模型偏差问题并实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.00481v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Can you see me now? Blind spot estimation for autonomous vehicles using scenario-based simulation with random reference sensors**<br />
**Title_cn:** 你现在能看见我吗？使用基于场景的模拟和随机参考传感器来估计自动驾驶汽车的盲点<br />
**Authors:** Marc Uecker, J. Marius Zöllner<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a method for estimating blind spots for sensor setups of autonomous or automated vehicles and/or robotics applications. In comparison to previous methods that rely on geometric approximations, our presented approach provides more realistic coverage estimates by utilizing accurate and detailed 3D simulation environments. Our method leverages point clouds from LiDAR sensors or camera depth images from high-fidelity simulations of target scenarios to provide accurate and actionable visibility estimates. A Monte Carlo-based reference sensor simulation enables us to accurately estimate blind spot size as a metric of coverage, as well as detection probabilities of objects at arbitrary positions.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了一种估计自主或自动化车辆和/或机器人应用的传感器设置盲点的方法。与以前依赖几何近似的方法相比，我们提出的方法通过利用准确且详细的 3D 模拟环境提供更真实的覆盖范围估计。我们的方法利用来自 LiDAR 传感器的点云或来自目标场景高保真模拟的相机深度图像来提供准确且可操作的能见度估计。基于蒙特卡罗的参考传感器模拟使我们能够准确估计盲点大小作为覆盖范围的度量，以及任意位置物体的检测概率。</details>
**PDF:** <http://arxiv.org/pdf/2402.00467v1><br />
**Code:** <https://github.com/pyrestone/numpy_cukd>**<br />
>>**index:** 14<br />
**Title:** **Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection**<br />
**Title_cn:** 用于无监督异常检测的双学生知识蒸馏网络<br />
**Authors:** Liyi Yao, Shaobing Gao<br />
**Abstract:** <details><summary>原文: </summary>Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于数据不平衡和缺陷的多样性，学生-教师网络（S-T）在无监督异常检测中受到青睐，它探索从知识蒸馏过程中得出的特征表示的差异来识别异常。然而，普通的 S-T 网络并不稳定。采用相同的结构构建S-T网络可以减弱异常的代表性差异。但使用不同的结构会增加正常数据上出现不同性能的可能性。为了解决这个问题，我们提出了一种新颖的双学生知识蒸馏（DSKD）架构。与其他 S-T 网络不同，我们使用两个学生网络和一个预训练的教师网络，其中学生具有相同的规模但结构相反。该框架可以增强蒸馏效果，提高正常数据识别的一致性，同时引入异常表示的多样性。为了探索高维语义信息以捕获异常线索，我们采用了两种策略。首先，采用金字塔匹配模式对网络中间层的多尺度特征图进行知识蒸馏。其次，通过深度特征嵌入模块促进两个学生网络之间的交互，该模块的灵感来自于现实世界的小组讨论。在分类方面，我们通过测量教师和学生网络的输出特征图之间的差异来获得像素级异常分割图，从中计算异常分数以进行样本判定。我们在三个基准数据集上评估 DSKD，并通过消融实验探讨内部模块的影响。结果表明，DSKD 可以在 ResNet18 等小型模型上实现出色的性能，并有效改进普通 S-T 网络。</details>
**PDF:** <http://arxiv.org/pdf/2402.00448v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Lightweight Pixel Difference Networks for Efficient Visual Representation Learning**<br />
**Title_cn:** 用于高效视觉表示学习的轻量级像素差分网络<br />
**Authors:** Zhuo Su, Jiehua Zhang, Longguang Wang, Hua Zhang, Zhen Liu, Matti Pietikäinen, Li Liu<br />
**Abstract:** <details><summary>原文: </summary>Recently, there have been tremendous efforts in developing lightweight Deep Neural Networks (DNNs) with satisfactory accuracy, which can enable the ubiquitous deployment of DNNs in edge devices. The core challenge of developing compact and efficient DNNs lies in how to balance the competing goals of achieving high accuracy and high efficiency. In this paper we propose two novel types of convolutions, dubbed \emph{Pixel Difference Convolution (PDC) and Binary PDC (Bi-PDC)} which enjoy the following benefits: capturing higher-order local differential information, computationally efficient, and able to be integrated with existing DNNs. With PDC and Bi-PDC, we further present two lightweight deep networks named \emph{Pixel Difference Networks (PiDiNet)} and \emph{Binary PiDiNet (Bi-PiDiNet)} respectively to learn highly efficient yet more accurate representations for visual tasks including edge detection and object recognition. Extensive experiments on popular datasets (BSDS500, ImageNet, LFW, YTF, \emph{etc.}) show that PiDiNet and Bi-PiDiNet achieve the best accuracy-efficiency trade-off. For edge detection, PiDiNet is the first network that can be trained without ImageNet, and can achieve the human-level performance on BSDS500 at 100 FPS and with $<$1M parameters. For object recognition, among existing Binary DNNs, Bi-PiDiNet achieves the best accuracy and a nearly $2\times$ reduction of computational cost on ResNet18. Code available at \href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet}.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，人们在开发具有令人满意的精度的轻量级深度神经网络（DNN）方面付出了巨大的努力，这使得 DNN 能够在边缘设备中普遍部署。开发紧凑高效的 DNN 的核心挑战在于如何平衡实现高精度和高效率的竞争目标。在本文中，我们提出了两种新颖的卷积类型，称为\emph{像素差分卷积（PDC）和二进制PDC（Bi-PDC）}，它们具有以下优点：捕获高阶局部差分信息，计算效率高，并且能够与现有的 DNN 集成。通过 PDC 和 Bi-PDC，我们进一步提出了两个轻量级深度网络，分别名为 \emph{Pixel Difference Networks (PiDiNet)} 和 \emph{Binary PiDiNet (Bi-PiDiNet)}，以学习高效且更准确的视觉任务表示，包括边缘检测和物体识别。对流行数据集（BSDS500、ImageNet、LFW、YTF、\emph{etc.}）的大量实验表明，PiDiNet 和 Bi-PiDiNet 实现了最佳的精度-效率权衡。对于边缘检测，PiDiNet 是第一个无需 ImageNet 即可训练的网络，并且可以在 BSDS500 上以 100 FPS 和 $<1M 参数实现人类水平的性能。对于目标识别，在现有的二进制 DNN 中，Bi-PiDiNet 实现了最佳精度，并且在 ResNet18 上减少了近 2 倍的计算成本。代码可在 \href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.00422v1><br />
**Code:** <https://github.com/hellozhuo/pidinet>**<br />
>>**index:** 16<br />
**Title:** **Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser**<br />
**Title_cn:** 通过基于 Transformer 的模态注​​入器解开多模态大脑 MR 图像翻译<br />
**Authors:** Jihoon Cho, Xiaofeng Liu, Fangxu Xing, Jinsong Ouyang, Georges El Fakhri, Jinah Park, Jonghye Woo<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease diagnosis due to its ability to provide complementary information by analyzing a relationship between multimodal images on the same subject. Acquiring all MR modalities, however, can be expensive, and, during a scanning session, certain MR images may be missed depending on the study protocol. The typical solution would be to synthesize the missing modalities from the acquired images such as using generative adversarial networks (GANs). Yet, GANs constructed with convolutional neural networks (CNNs) are likely to suffer from a lack of global relationships and mechanisms to condition the desired modality. To address this, in this work, we propose a transformer-based modality infuser designed to synthesize multimodal brain MR images. In our method, we extract modality-agnostic features from the encoder and then transform them into modality-specific features using the modality infuser. Furthermore, the modality infuser captures long-range relationships among all brain structures, leading to the generation of more realistic images. We carried out experiments on the BraTS 2018 dataset, translating between four MR modalities, and our experimental results demonstrate the superiority of our proposed method in terms of synthesis quality. In addition, we conducted experiments on a brain tumor segmentation task and different conditioning methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态磁共振 (MR) 成像在疾病诊断中发挥着至关重要的作用，因为它能够通过分析同一对象的多模态图像之间的关系来提供补充信息。然而，获取所有 MR 模式的成本可能很高，而且在扫描过程中，根据研究方案，可能会错过某些 MR 图像。典型的解决方案是从获取的图像中合成缺失的模式，例如使用生成对抗网络（GAN）。然而，用卷积神经网络 (CNN) 构建的 GAN 可能会缺乏全局关系和机制来调节所需的模态。为了解决这个问题，在这项工作中，我们提出了一种基于变压器的模态注入器，旨在合成多模态大脑 MR 图像。在我们的方法中，我们从编码器中提取与模态无关的特征，然后使用模态注入器将它们转换为特定于模态的特征。此外，模态注入器捕获所有大脑结构之间的远程关系，从而生成更真实的图像。我们在 BraTS 2018 数据集上进行了实验，在四种 MR 模态之间进行转换，我们的实验结果证明了我们提出的方法在合成质量方面的优越性。此外，我们还对脑肿瘤分割任务和不同的调节方法进行了实验。</details>
**PDF:** <http://arxiv.org/pdf/2402.00375v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **High-Quality Medical Image Generation from Free-hand Sketch**<br />
**Title_cn:** 从手绘草图生成高质量的医学图像<br />
**Authors:** Quan Huu Cap, Atsushi Fukuda<br />
**Abstract:** <details><summary>原文: </summary>Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications. Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images). However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results. In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it. Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process. Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations. Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>从手绘草图生成医学图像为各种重要的医学成像应用带来了希望。由于在医学领域收集手绘草图数据极其困难，大多数基于深度学习的方法被提出来从合成草图（例如，来自真实图像的分割掩模的边缘图或轮廓）生成医学图像。然而，这些模型往往无法概括徒手草图，导致结果不令人满意。在本文中，我们提出了一种实用的徒手草图到图像生成模型，称为 Sketch2MedI，它学习在 StyleGAN 的潜在空间中表示草图并从中生成医学图像。由于能够将草图编码到这个有意义的表示空间中，Sketch2Medi 只需要合成草图进行训练，从而实现经济高效的学习过程。我们的 Sketch2Medi 展示了对徒手草图的强大泛化，从而生成高质量且逼真的医学图像。 Sketch2Medi 与 pix2pix、CycleGAN、UNIT 和 U-GAT-IT 模型的比较评估显示，在生成咽部图像方面具有卓越的性能，无论是在各种指标的定量还是定性方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.00353v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Machine Unlearning for Image-to-Image Generative Models**<br />
**Title_cn:** 图像到图像生成模型的机器遗忘<br />
**Authors:** Guihong Li, Hsiang Hsu, Chun-Fu, Chen, Radu Marculescu<br />
**Abstract:** <details><summary>原文: </summary>Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器遗忘已经成为一种新的范式，它故意忘记给定模型中的数据样本，以遵守严格的法规。然而，现有的机器忘记学习方法主要集中在分类模型上，而生成模型的忘记学习领域相对尚未被探索。本文充当了一座桥梁，通过为图像到图像生成模型提供机器取消学习的统一框架来解决这一差距。在此框架内，我们提出了一种计算高效的算法，以严格的理论分析为基础，该算法证明保留样本的性能下降可以忽略不计，同时有效地从遗忘样本中删除信息。对两个大型数据集ImageNet-1K和Places-365的实证研究进一步表明，我们的算法不依赖于保留样本的可用性，这进一步符合数据保留策略。据我们所知，这项工作是第一个代表针对图像到图像生成模型专门定制的机器取消学习的系统性、理论性和实证性探索的工作。我们的代码可在 https://github.com/jpmorganchase/l2l-generator-unlearning 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.00351v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Self-supervised learning of video representations from a child's perspective**<br />
**Title_cn:** 从儿童的角度进行视频表示的自我监督学习<br />
**Authors:** A. Emin Orhan, Wentao Wang, Alex N. Wang, Mengye Ren, Brenden M. Lake<br />
**Abstract:** <details><summary>原文: </summary>Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.</details>
**Abstract_cn:** <details><summary>译文: </summary>孩子们从几年的以自我为中心的视觉体验中学习了他们周围世界的强大的内部模型。这种内部模型可以通过高度通用的学习算法从儿童的视觉体验中学习，还是需要很强的归纳偏差？最近在收集大规模、纵向、发展现实的视频数据集和通用自我监督学习（SSL）算法方面取得的进展使我们能够开始解决这个先天与后天的问题。然而，现有的工作通常侧重于基于图像的 SSL 算法和可以从静态图像中学习的视觉功能（例如对象识别），从而忽略了世界的时间方面。为了缩小这一差距，我们在这里使用从儿童早期发育阶段（6-31 个月）两年内收集的纵向、以自我为中心的头部摄像头记录来训练自监督视频模型。由此产生的模型在促进从少量标记示例中学习动作概念方面非常有效；它们具有良好的数据大小缩放特性；它们还显示出新兴的视频插值功能。与使用完全相同的数据训练的基于图像的模型相比，视频模型还可以学习更强大的对象表示。这些结果表明，孩子的内部世界模型的重要时间方面可以使用高度通用的学习算法从他们的视觉经验中学习，并且没有强烈的归纳偏差。</details>
**PDF:** <http://arxiv.org/pdf/2402.00300v1><br />
**Code:** <https://github.com/eminorhan/video-models>**<br />
>>**index:** 20<br />
**Title:** **Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images**<br />
**Title_cn:** 使用无人机图像进行弃土堆描绘的传统分割方法和基于深度学习的分割方法的比较评估<br />
**Authors:** Sureka Thiruchittampalam, Bikram P. Banerjee, Nancy F. Glenn, Simit Raval<br />
**Abstract:** <details><summary>原文: </summary>The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. Yet, on-site characterisation of individual piles poses a formidable challenge. The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches. This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>矿山堆放场的稳定性取决于弃土堆的精确布置，并考虑到其地质和岩土特性。然而，单个桩的现场表征提出了巨大的挑战。利用基于图像的技术来表征弃土堆，并利用通过无人机系统远程获取的数据，是一种有前途的补充解决方案。图像处理，例如基于对象的分类和特征提取，依赖于有效的分割。这项研究完善并并置了各种分割方法，特别是基于颜色和基于形态的技术。目标是增强和评估采矿环境中基于对象的弃土特征分析的途径。此外，对传统分割方法和基于深度学习方法的分割方法进行了比较分析。在评估的各种分割方法中，基于形态学的深度学习分割方法，分段任意模型（SAM），与其他方法相比表现出优越的性能。这一结果强调了结合先进的形态学和深度学习技术来准确高效地表征弃土堆的有效性。这项研究的结果为分割策略的优化提供了宝贵的见解，从而推进了基于图像的技术在采矿环境中弃土堆特征描述中的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.00295v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **FineBio: A Fine-Grained Video Dataset of Biological Experiments with Hierarchical Annotation**<br />
**Title_cn:** FineBio：具有分层注释的生物实验细粒度视频数据集<br />
**Authors:** Takuma Yagi, Misaki Ohashi, Yifei Huang, Ryosuke Furuta, Shungo Adachi, Toutai Mitsuyama, Yoichi Sato<br />
**Abstract:** <details><summary>原文: </summary>In the development of science, accurate and reproducible documentation of the experimental process is crucial. Automatic recognition of the actions in experiments from videos would help experimenters by complementing the recording of experiments. Towards this goal, we propose FineBio, a new fine-grained video dataset of people performing biological experiments. The dataset consists of multi-view videos of 32 participants performing mock biological experiments with a total duration of 14.5 hours. One experiment forms a hierarchical structure, where a protocol consists of several steps, each further decomposed into a set of atomic operations. The uniqueness of biological experiments is that while they require strict adherence to steps described in each protocol, there is freedom in the order of atomic operations. We provide hierarchical annotation on protocols, steps, atomic operations, object locations, and their manipulation states, providing new challenges for structured activity understanding and hand-object interaction recognition. To find out challenges on activity understanding in biological experiments, we introduce baseline models and results on four different tasks, including (i) step segmentation, (ii) atomic operation detection (iii) object detection, and (iv) manipulated/affected object detection. Dataset and code are available from https://github.com/aistairc/FineBio.</details>
**Abstract_cn:** <details><summary>译文: </summary>在科学的发展中，准确且可重复的实验过程记录至关重要。从视频中自动识别实验中的动作将通过补充实验记录来帮助实验者。为了实现这一目标，我们提出了 FineBio，这是一个新的人们进行生物实验的细粒度视频数据集。该数据集由 32 名参与者进行模拟生物实验的多视图视频组成，总时长为 14.5 小时。一个实验形成了一个层次结构，其中协议由多个步骤组成，每个步骤进一步分解为一组原子操作。生物实验的独特性在于，虽然它们需要严格遵守每个协议中描述的步骤，但原子操作的顺序是自由的。我们提供了协议、步骤、原子操作、对象位置及其操作状态的分层注释，为结构化活动理解和手部对象交互识别提供了新的挑战。为了找出生物实验中活动理解的挑战，我们引入了四种不同任务的基线模型和结果，包括（i）步骤分割，（ii）原子操作检测，（iii）对象检测，以及（iv）操纵/受影响的对象检测。数据集和代码可从 https://github.com/aistairc/FineBio 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.00293v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues**<br />
**Title_cn:** 通过空间动作单元线索引导可解释的面部表情识别<br />
**Authors:** Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger<br />
**Abstract:** <details><summary>原文: </summary>While state-of-the-art facial expression recognition (FER) classifiers achieve a high level of accuracy, they lack interpretability, an important aspect for end-users. To recognize basic facial expressions, experts resort to a codebook associating a set of spatial action units to a facial expression. In this paper, we follow the same expert footsteps, and propose a learning strategy that allows us to explicitly incorporate spatial action units (aus) cues into the classifier's training to build a deep interpretable model. In particular, using this aus codebook, input image expression label, and facial landmarks, a single action units heatmap is built to indicate the most discriminative regions of interest in the image w.r.t the facial expression. We leverage this valuable spatial cue to train a deep interpretable classifier for FER. This is achieved by constraining the spatial layer features of a classifier to be correlated with \aus map. Using a composite loss, the classifier is trained to correctly classify an image while yielding interpretable visual layer-wise attention correlated with aus maps, simulating the experts' decision process. This is achieved using only the image class expression as supervision and without any extra manual annotations. Moreover, our method is generic. It can be applied to any CNN- or transformer-based deep classifier without the need for architectural change or adding significant training time. Our extensive evaluation on two public benchmarks RAFDB, and AFFECTNET datasets shows that our proposed strategy can improve layer-wise interpretability without degrading classification performance. In addition, we explore a common type of interpretable classifiers that rely on Class-Activation Mapping methods (CAMs), and we show that our training technique improves the CAM interpretability.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然最先进的面部表情识别 (FER) 分类器具有很高的准确性，但它们缺乏可解释性，而这对最终用户来说是一个重要方面。为了识别基本的面部表情，专家们求助于将一组空间动作单元与面部表情相关联的密码本。在本文中，我们遵循相同的专家脚步，并提出了一种学习策略，使我们能够明确地将空间动作单元（aus）线索纳入分类器的训练中，以构建深度可解释模型。特别是，使用此 aus 代码本、输入图像表情标签和面部标志，构建单个动作单元热图来指示图像中与面部表情相关的最具辨别力的感兴趣区域。我们利用这一宝贵的空间线索来训练 FER 的深度可解释分类器。这是通过将分类器的空间层特征限制为与\aus图相关来实现的。使用复合损失，训练分类器正确分类图像，同时产生与 aus 地图相关的可解释的视觉分层注意力，模拟专家的决策过程。这是仅使用图像类表达式作为监督来实现的，无需任何额外的手动注释。此外，我们的方法是通用的。它可以应用于任何基于 CNN 或 Transformer 的深度分类器，无需更改架构或增加大量训练时间。我们对两个公共基准 RAFDB 和 AFFECTNET 数据集的广泛评估表明，我们提出的策略可以在不降低分类性能的情况下提高分层可解释性。此外，我们探索了一种依赖于类激活映射方法（CAM）的常见类型的可解释分类器，并且我们表明我们的训练技术提高了 CAM 可解释性。</details>
**PDF:** <http://arxiv.org/pdf/2402.00281v1><br />
**Code:** <https://github.com/sbelharbi/interpretable-fer-aus>**<br />
>>**index:** 23<br />
**Title:** **LRDif: Diffusion Models for Under-Display Camera Emotion Recognition**<br />
**Title_cn:** LRDif：用于屏下摄像头情绪识别的扩散模型<br />
**Authors:** Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana<br />
**Abstract:** <details><summary>原文: </summary>This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC's image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究引入了 LRDif，这是一种新型的基于扩散的框架，专为屏下摄像头 (UDC) 背景下的面部表情识别 (FER) 而设计。为了解决 UDC 图像退化带来的固有挑战，例如清晰度降低和噪声增加，LRDif 采用两阶段训练策略，集成压缩初步提取网络（FPEN）和敏捷变压器网络（UDCformer）来有效识别情感标签来自 UDC 图像。通过利用扩散模型 (DM) 强大的分布映射功能和变压器的空间依赖性建模能力，LRDif 有效克服了 UDC 环境中固有的噪声和失真障碍。对标准 FER 数据集（包括 RAF-DB、KDEF 和 FERPlus、LRDif）的综合实验展示了最先进的性能，强调了其在推进 FER 应用方面的潜力。这项工作不仅通过解决 FER 中的 UDC 挑战来弥补文献中的重大空白，而且还为该领域的未来研究树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2402.00250v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning**<br />
**Title_cn:** AnimateLCM：通过解耦一致性学习加速个性化扩散模型和适配器的动画<br />
**Authors:** Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li<br />
**Abstract:** <details><summary>原文: </summary>Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频扩散模型因其能够生成连贯且高保真度的视频而受到越来越多的关注。然而，迭代去噪过程使其计算量大且耗时，从而限制了其应用。受到一致性模型（CM）的启发，一致性模型（CM）提炼预训练的图像扩散模型，以最少的步骤加速采样，以及其在条件图像生成上的成功扩展潜在一致性模型（LCM），我们提出了AnimateLCM，允许在最少的步骤内生成高保真视频。我们没有直接在原始视频数据集上进行一致性学习，而是提出了一种解耦的一致性学习策略，该策略将图像生成先验和运动生成先验的蒸馏解耦，从而提高了训练效率并增强了生成视觉质量。此外，使稳定扩散社区中的即插即用适配器组合能够实现各种功能（例如用于可控发电的ControlNet）。我们提出了一种有效的策略，使现有适配器适应我们的蒸馏文本条件视频一致性模型，或者从头开始训练适配器，而不损害采样速度。我们在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最佳效果。实验结果验证了我们提出的方法的有效性。代码和权重将被公开。更多详细信息请访问 https://github.com/G-U-N/AnimateLCM。</details>
**PDF:** <http://arxiv.org/pdf/2402.00769v1><br />
**Code:** <https://github.com/g-u-n/animatelcm>**<br />

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields**<br />
**Title_cn:** ViCA-NeRF：神经辐射场的视图一致性感知 3D 编辑<br />
**Authors:** Jiahua Dong, Yu-Xiong Wang<br />
**Abstract:** <details><summary>原文: </summary>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出 ViCA-NeRF，这是第一个使用文本指令进行 3D 编辑的视图一致性感知方法。除了隐式神经辐射场（NeRF）建模之外，我们的主要见解是利用两个正则化源，这些正则化源可以在不同视图之间显式传播编辑信息，从而确保多视图一致性。对于几何正则化，我们利用 NeRF 导出的深度信息来建立不同视图之间的图像对应关系。对于学习正则化，我们在编辑和未编辑图像之间对齐 2D 扩散模型中的潜在代码，使我们能够编辑关键视图并将更新传播到整个场景。结合这两种策略，我们的 ViCA-NeRF 分两个阶段运行。在初始阶段，我们混合来自不同视图的编辑来创建初步的 3D 编辑。接下来是 NeRF 训练的第二阶段，致力于进一步完善场景的外观。实验结果表明，与现有技术相比，ViCA-NeRF 提供更灵活、更高效（快 3 倍）的编辑，具有更高水平的一致性和细节。我们的代码是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2402.00864v1><br />
**Code:** <https://github.com/dongjiahua/vica-nerf>**<br />
>>**index:** 2<br />
**Title:** **Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering**<br />
**Title_cn:** Emo-Avatar：通过纹理渲染高效的单目视频风格头像<br />
**Authors:** Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu<br />
**Abstract:** <details><summary>原文: </summary>Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.</details>
**Abstract_cn:** <details><summary>译文: </summary>艺术视频肖像生成是计算机图形学和视觉领域中一项重要且备受追捧的任务。虽然已经开发了各种方法将 NeRF 或 StyleGAN 与指导编辑模型集成以创建和编辑可驾驶肖像，但这些方法面临着一些挑战。它们通常严重依赖大型数据集，需要大量的定制过程，并且经常导致图像质量下降。为了解决上述问题，我们通过延迟神经渲染提出了高效单调视频风格头像（Emo-Avatar），增强了 StyleGAN 生成动态、可驾驶肖像视频的能力。我们提出了一个两阶段延迟神经渲染管道。在第一阶段，我们利用少镜头 PTI 初始化，通过从视频中采样的几个极端姿势来初始化 StyleGAN 生成器，以捕获目标肖像中对齐面部的一致表示。在第二阶段，我们提出了一个拉普拉斯金字塔，用于从通过动态表达流变形的 UV 图进行高频纹理采样，以进行运动感知纹理先期集成，以提供躯干特征，以增强 StyleGAN 生成用于肖像视频渲染的完整上半身的能力。与现有方法相比，Emo-Avatar 将样式定制时间从数小时缩短至仅 5 分钟。此外，Emo-Avatar 仅需要单个参考图像进行编辑，并采用区域感知对比学习和语义不变 CLIP 指导，确保一致的高分辨率输出和身份保存。通过定量和定性评估，Emo-Avatar 在训练效率、渲染质量以及自我和交叉重演的可编辑性方面表现出了优于现有方法的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.00827v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CapHuman: Capture Your Moments in Parallel Universes**<br />
**Title_cn:** CapHuman：在平行宇宙中捕捉你的瞬间<br />
**Authors:** Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang<br />
**Abstract:** <details><summary>原文: </summary>We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们专注于一种新颖的以人为中心的图像合成任务，即仅给定一张参考面部照片，期望在不同的背景下生成具有不同头部位置、姿势和面部表情的特定个体图像。为了实现这一目标，我们认为我们的生成模型应该具有以下有利特征：（1）对我们的世界和人类社会有很强的视觉和语义理解，以生成基本对象和人类图像。 (2)可概括的身份保存能力。 (3)灵活细粒度的头部控制。最近，大型预训练文本到图像扩散模型已经显示出显着的结果，成为强大的生成基础。作为基础，我们的目标是释放预训练模型的上述两项功能。在这项工作中，我们提出了一个名为 CapHuman 的新框架。我们采用“编码然后学习对齐”范式，这使得新个体能够进行通用的身份保存，而无需在推理时进行繁琐的调整。CapHuman 对身份特征进行编码，然后学习将它们对齐到潜在空间中。此外，我们引入了 3D 面部先验使我们的模型能够以灵活且 3D 一致的方式控制人体头部。广泛的定性和定量分析表明，我们的 CapHuman 可以生成身份保留良好、照片般真实且高保真的肖像，具有内容丰富的表示形式和各种头部再现，优于既定基线。代码和检查点将在 https://github.com/VamosC/CapHuman 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.00627v1><br />
**Code:** <https://github.com/vamosc/caphuman>**<br />
>>**index:** 4<br />
**Title:** **Dynamic Texture Transfer using PatchMatch and Transformers**<br />
**Title_cn:** 使用 PatchMatch 和 Transformer 进行动态纹理传输<br />
**Authors:** Guo Pu, Shiyao Xu, Xixin Cao, Zhouhui Lian<br />
**Abstract:** <details><summary>原文: </summary>How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art.</details>
**Abstract_cn:** <details><summary>译文: </summary>如何将给定视频的动态纹理自动转移到目标静态图像是一个具有挑战性且持续存在的问题。在本文中，我们建议通过一个简单而有效的模型来处理此任务，该模型同时利用 PatchMatch 和 Transformer。关键思想是将动态纹理传输任务分解为两个阶段，其中在第一阶段通过基于 PatchMatch 算法的距离图引导纹理传输模块合成具有所需动态纹理的目标视频的起始帧。然后，在第二阶段，合成图像被分解为结构不可知的补丁，根据这些补丁，可以通过利用配备VQ-VAE的Transformers处理长离散序列的强大能力来预测其相应的后续补丁。获得所有这些补丁后，我们应用高斯加权平均合并策略将它们平滑地组装到目标风格化视频的每一帧中。实验结果证明了与现有技术相比，所提出的方法在动态纹理传输方面的有效性和优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.00606v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction**<br />
**Title_cn:** Image2Points：用于高质量 PET 图像重建的基于 3D 点的上下文聚类 GAN<br />
**Authors:** Jiaqi Cui, Yan Wang, Lu Wen, Pinxian Zeng, Xi Wu, Jiliu Zhou, Dinggang Shen<br />
**Abstract:** <details><summary>原文: </summary>To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images. However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction. In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET. Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details. Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images. Experiments on both clinical and phantom datasets demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction methods qualitatively and quantitatively. Code is available at https://github.com/gluucose/PCCGAN.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了获得高质量的正电子发射断层扫描 (PET) 图像，同时最大限度地减少辐射暴露，人们提出了多种方法来从相应的低剂量 PET (LPET) 图像重建标准剂量 PET (SPET) 图像。然而，这些方法严重依赖基于体素的表示，无法充分考虑精确的结构和细粒度的上下文，从而导致重建受损。在本文中，我们提出了一种基于 3D 点的上下文聚类 GAN，即 PCC-GAN，用于从 LPET 重建高质量的 SPET 图像。具体来说，受到点的几何表示能力的启发，我们采用基于点的表示来增强图像结构的显式表达，从而促进更精细细节的重建。此外，应用上下文聚类策略来探索点之间的上下文关系，这减轻了重建图像中小结构的模糊性。对临床和模型数据集的实验表明，我们的 PCC-GAN 在定性和定量上都优于最先进的重建方法。代码可在 https://github.com/gluucose/PCCGAN 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.00376v1><br />
**Code:** <https://github.com/gluucose/pccgan>**<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **In-Bed Pose Estimation: A Review**<br />
**Title_cn:** 床上姿势估计：回顾<br />
**Authors:** Ziya Ata Yazıcı, Sara Colantonio, Hazım Kemal Ekenel<br />
**Abstract:** <details><summary>原文: </summary>Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare. One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed. This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals. Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses. The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map. Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions. Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation. To expedite advancements in this domain, we conduct a review of existing datasets and approaches. Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field.</details>
**Abstract_cn:** <details><summary>译文: </summary>人体姿势估计是从图像或视频中识别人身体关节位置的过程，代表了包括医疗保健在内的各个领域广泛使用的技术。此类医疗保健应用之一涉及床上姿势估计，其中分析躺在毯子下的个人的身体姿势。例如，这项任务可用于监测一个人的睡眠行为并及早发现症状，以便在家庭和医院中诊断潜在的疾病。一些研究利用单模态和多模态方法来估计人体在床上的姿势。单模态研究通常采用 RGB 图像，而多模态研究则使用 RGB、长波红外、压力图和深度图等模态。多模态研究的优点是除了 RGB 之外还可以使用模态，可以捕获有助于应对遮挡的信息。此外，一些多模态研究排除了 RGB，这样更适合隐私保护。为了加快该领域的进步，我们对现有数据集和方法进行了审查。我们的目标是展示先前研究的局限性、当前的挑战，并为床上人体姿势估计领域的未来工作提供见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.00700v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird's-Eye-View**<br />
**Title_cn:** 鱼眼相机和超声波传感器融合，实现鸟瞰近场障碍物感知<br />
**Authors:** Arindam Das, Sudarshan Paul, Niko Scholz, Akhilesh Kumar Malviya, Ganesh Sistu, Ujjwal Bhattacharya, Ciarán Eising<br />
**Abstract:** <details><summary>原文: </summary>Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving. Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization. However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare. Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions. Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors. Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality. Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV. The fusion of these two modalities is facilitated via concatenation. At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space. Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception. We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors. We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios. When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的障碍物识别是自动驾驶近场感知领域的一项根本挑战。传统上，鱼眼摄像机经常用于全面的环视感知，包括后视障碍物定位。然而，此类相机的性能在低光照条件下、夜间或受到强烈阳光照射时可能会显着恶化。相反，超声波传感器等具有成本效益的传感器在这些条件下基本上不受影响。因此，据我们所知，我们提出了第一个端到端多模态融合模型，该模型利用鱼眼相机和超声波传感器，在鸟瞰（BEV）视角下实现高效障碍物感知。最初，ResNeXt-50 被用作一组单模态编码器来提取特定于每种模态的特征。随后，与可见光谱相关的特征空间转换为 BEV。通过串联促进这两种模式的融合。同时，基于超声波频谱的单峰特征图通过内容感知扩张卷积，用于减轻融合特征空间中两个传感器之间的传感器错位。最后，两级语义占用解码器利用融合的特征来生成网格预测，以实现精确的障碍物感知。我们进行了系统研究，以确定两个传感器多模态融合的最佳策略。我们提供有关数据集创建程序、注释指南的见解，并执行彻底的数据分析，以确保充分覆盖所有场景。当应用于我们的数据集时，实验结果强调了我们提出的多模态融合方法的稳健性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.00637v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Instruction Makes a Difference**<br />
**Title_cn:** 指导有所作为<br />
**Authors:** Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney<br />
**Abstract:** <details><summary>原文: </summary>We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improvement.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了指令文档视觉问答（iDocVQA）数据集和大型语言文档（LLaDoc）模型，分别用于训练用于文档分析和文档图像预测的语言视觉（LV）模型。通常，DocVQA 任务的深度神经网络是在缺乏指令的数据集上进行训练的。我们证明，使用遵循指令的数据集可以提高性能。我们使用最新的 (SotA) 大型语言和视觉助手 (LLaVA)1.5 作为基本模型来比较文档相关数据集的性能。我们还使用基于轮询的对象探测评估（POPE）数据集来评估派生模型的对象幻觉性能。结果表明，指令调优性能是零样本性能的 11 倍到 32 倍，是非指令（传统任务）微调的 0.1% 到 4.2%。尽管取得了一些进展，但仍低于人类表现（94.36%），这意味着还有很大的改进空间。</details>
**PDF:** <http://arxiv.org/pdf/2402.00453v1><br />
**Code:** <https://github.com/ltu-machine-learning/idocvqa>**<br />
>>**index:** 4<br />
**Title:** **Safety of Multimodal Large Language Models on Images and Text**<br />
**Title_cn:** 图像和文本多模态大语言模型的安全性<br />
**Authors:** Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao<br />
**Abstract:** <details><summary>原文: </summary>Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text. We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey. Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs. Next, we comprehensively present attack and defense techniques related to MLLMs' safety. Finally, we analyze several unsolved issues and discuss promising research directions.</details>
**Abstract_cn:** <details><summary>译文: </summary>被多模态大型语言模型 (MLLM) 的强大功能所吸引，公众越来越多地利用它们来提高日常工作的效率。尽管如此，当这些模型部署在现实场景中时，MLLM 对不安全指令的脆弱性带来了巨大的安全风险。在本文中，我们系统地调查了当前在图像和文本上的 MLLM 安全性评估、攻击和防御方面的工作。我们首先介绍 MLLM 在图像和文本方面的概述以及对安全性的理解，这有助于研究人员了解我们调查的详细范围。然后，我们审查用于衡量 MLLM 安全性的评估数据集和指标。接下来，我们全面介绍与MLLM安全相关的攻击和防御技术。最后，我们分析了几个未解决的问题并讨论了有前途的研究方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.00357v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multimodal Embodied Interactive Agent for Cafe Scene**<br />
**Title_cn:** 咖啡馆场景的多模态实体交互代理<br />
**Authors:** Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>With the surge in the development of large language models, embodied intelligence has attracted increasing attention. Nevertheless, prior works on embodied intelligence typically encode scene or historical memory in an unimodal manner, either visual or linguistic, which complicates the alignment of the model's action planning with embodied control. To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions. Specifically, we propose a novel Multimodal Environment Memory (MEM) module, facilitating the integration of embodied control with large models through the visual-language memory of scenes. This capability enables MEIA to generate executable action plans based on diverse requirements and the robot's capabilities. We conduct experiments in a dynamic virtual cafe environment, utilizing multiple large models through zero-shot learning, and carefully design scenarios for various situations. The experimental results showcase the promising performance of our MEIA in various embodied interactive tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着大型语言模型发展的激增，具身智能越来越受到人们的关注。然而，先前关于具身智能的研究通常以视觉或语言的单模态方式对场景或历史记忆进行编码，这使得模型的行动规划与具身控制的协调变得复杂。为了克服这一限制，我们引入了多模式嵌入式交互代理（MEIA），它能够将以自然语言表达的高级任务转换为一系列可执行动作。具体来说，我们提出了一种新颖的多模态环境记忆（MEM）模块，通过场景的视觉语言记忆促进体现控制与大型模型的集成。此功能使 MEIA 能够根据不同的要求和机器人的功能生成可执行的行动计划。我们在动态的虚拟咖啡馆环境中进行实验，通过零样本学习利用多个大型模型，并针对各种情况精心设计场景。实验结果展示了我们的 MEIA 在各种具体交互任务中的良好表现。</details>
**PDF:** <http://arxiv.org/pdf/2402.00290v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks**<br />
**Title_cn:** 视觉法学硕士可以用自己生成的印刷攻击来欺骗自己<br />
**Authors:** Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer<br />
**Abstract:** <details><summary>原文: </summary>Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大视觉语言模型（LVLM）取得了重大进展；一类新的 VL 模型，利用大型预训练语言模型。然而，它们对印刷攻击的脆弱性尚未得到研究，印刷攻击涉及将误导性文本叠加到图像上。此外，先前的工作印刷攻击依赖于从预定义的类集中随机抽取误导性类。然而，随机选择的类别可能不是最有效的攻击。为了解决这些问题，我们首先引入一个新颖的基准测试，专门用于测试 LVLM 对印刷攻击的脆弱性。此外，我们引入了一种新的、更有效的印刷攻击：自生成的印刷攻击。事实上，我们的方法在给定图像的情况下，通过简单地提示它们推荐印刷攻击来利用 GPT-4V 等模型的强大语言功能。使用我们的新颖基准，我们发现印刷攻击对 LVLM 构成重大威胁。此外，我们发现，与之前的工作攻击相比，GPT-4V 使用我们的新方法推荐的印刷攻击不仅对 GPT-4V 本身更有效，而且对许多功能较弱但流行的开源模型（如 LLaVA、InstructBLIP、和 MiniGPT4。</details>
**PDF:** <http://arxiv.org/pdf/2402.00626v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming**<br />
**Title_cn:** 360-GS：用于室内漫游的布局引导的全景高斯分布<br />
**Authors:** Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯溅射 (3D-GS) 最近因其实时和逼真的渲染而引起了极大的关注。该技术通常将透视图像作为输入，并通过将一组 3D 椭圆高斯分布到图像平面上来优化它们，从而产生 2D 高斯分布。然而，将 3D-GS 应用于全景输入在使用 2D 高斯对 ${360^\circ}$ 图像的球面投影进行有效建模方面提出了挑战。在实际应用中，输入全景图通常很稀疏，导致 3D 高斯初始化不可靠，进而导致 3D-GS 质量下降。此外，由于无纹理平面（例如墙壁和地板）的几何形状受到约束，3D-GS 很难用椭圆高斯模型对这些平坦区域进行建模，从而导致新视图中出现明显的漂浮物。为了解决这些问题，我们提出了 360-GS，这是一种针对有限的全景输入集的新型 $360^{\circ}$ 高斯泼溅。 360-GS 不是将 3D 高斯直接喷射到球面上，而是将它们投影到单位球体的切平面上，然后将它们映射到球面投影。这种调整使得能够使用高斯来表示投影。我们通过利用全景图中的布局先验来指导 360-GS 的优化，这些先验易于获取并包含有关室内场景的强大结构信息。我们的实验结果表明，360-GS 可以实现全景渲染，并且优于最先进的方法，新颖的视图合成中的伪影更少，从而在室内场景中提供沉浸式漫游。</details>
**PDF:** <http://arxiv.org/pdf/2402.00763v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GS++: Error Analyzing and Optimal Gaussian Splatting**<br />
**Title_cn:** GS++：误差分析和最佳高斯分布<br />
**Authors:** Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements. However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\phi$. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D Gaussian Splatting 在实时神经渲染中得到了广泛的关注和应用。与此同时，人们对该技术在点云存储、性能和稀疏视点的鲁棒性等方面的局限性提出了担忧，从而导致了各种改进。然而，人们明显缺乏对泼溅本身固有的局部仿射近似引入的投影误差的关注，以及这些误差对照片级真实感渲染质量的影响。本文从投影函数 $\phi$ 的一阶泰勒展开的残差开始，讨论 3D 高斯分布的投影误差函数。该分析建立了误差与高斯平均位置之间的相关性。随后，本文利用函数优化理论，分析函数的极小值，为高斯分布提供一种最优投影策略，称为最优高斯分布。实验验证进一步证实，这种投影方法可以减少伪影，从而产生更令人信服的真实渲染。</details>
**PDF:** <http://arxiv.org/pdf/2402.00752v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video Compression**<br />
**Title_cn:** LVC-LGMC：用于学习视频压缩的联合局部和全局运动补偿<br />
**Authors:** Wei Jiang, Junru Li, Kai Zhang, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>Existing learned video compression models employ flow net or deformable convolutional networks (DCN) to estimate motion information. However, the limited receptive fields of flow net and DCN inherently direct their attentiveness towards the local contexts. Global contexts, such as large-scale motions and global correlations among frames are ignored, presenting a significant bottleneck for capturing accurate motions. To address this issue, we propose a joint local and global motion compensation module (LGMC) for leaned video coding. More specifically, we adopt flow net for local motion compensation. To capture global context, we employ the cross attention in feature domain for motion compensation. In addition, to avoid the quadratic complexity of vanilla cross attention, we divide the softmax operations in attention into two independent softmax operations, leading to linear complexity. To validate the effectiveness of our proposed LGMC, we integrate it with DCVC-TCM and obtain learned video compression with joint local and global motion compensation (LVC-LGMC). Extensive experiments demonstrate that our LVC-LGMC has significant rate-distortion performance improvements over baseline DCVC-TCM.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的学习视频压缩模型采用流网或可变形卷积网络（DCN）来估计运动信息。然而，流网和 DCN 有限的感受野本质上将它们的注意力集中在局部环境上。全局上下文，例如大规模运动和帧之间的全局相关性被忽略，为捕获准确运动带来了重大瓶颈。为了解决这个问题，我们提出了一种用于倾斜视频编码的联合局部和全局运动补偿模块（LGMC）。更具体地说，我们采用流网络进行局部运动补偿。为了捕获全局上下文，我们采用特征域中的交叉注意力进行运动补偿。此外，为了避免普通交叉注意力的二次复杂度，我们将注意力中的softmax操作分为两个独立的softmax操作，从而导致线性复杂度。为了验证我们提出的 LGMC 的有效性，我们将其与 DCVC-TCM 集成，并获得具有联合局部和全局运动补偿的学习视频压缩（LVC-LGMC）。大量实验表明，我们的 LVC-LGMC 比基线 DCVC-TCM 具有显着的率失真性能改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.00680v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Merging Multi-Task Models via Weight-Ensembling Mixture of Experts**<br />
**Title_cn:** 通过专家权重组合合并多任务模型<br />
**Authors:** Anke Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/</details>
**Abstract_cn:** <details><summary>译文: </summary>将在不同任务上训练的各种特定于任务的基于 Transformer 的模型合并到单个统一模型中可以同时执行所有任务。以前的方法（以任务算术为例）已被证明既有效又可扩展。现有方法主要集中于在原始模型参数空间内寻找静态最优解。一个显着的挑战是减轻不同模型参数之间的干扰，这会大大降低性能。在本文中，我们建议合并大部分参数，同时将 Transformer 层的 MLP 升级为权重集成混合专家 (MoE) 模块，该模块可以根据输入动态集成共享知识和特定于任务的知识，从而提供更灵活的解决方案，可以适应每个实例的特定需求。我们的主要见解是，通过识别和分离共享知识和特定于任务的知识，然后动态集成它们，我们可以在很大程度上缓解参数干扰问题。我们进行了传统的多任务模型合并实验，并评估了我们方法的泛化性和鲁棒性。结果证明了我们方法的有效性，并提供了对我们方法的全面理解。代码可在 https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/ 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.00433v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and Judger Mechanism**<br />
**Title_cn:** SmartCooper：具有自适应融合和判断机制的车辆协同感知<br />
**Authors:** Yuang Zhang, Haonan An, Zhengru Fang, Guowen Xu, Yuan Zhou, Xianhao Chen, Yuguang Fang<br />
**Abstract:** <details><summary>原文: </summary>In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10\% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\% compared with state-of-the-art schemes.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，自动驾驶因其通过联网自动驾驶车辆（CAV）之间的协作感知来改善道路安全的潜力而受到了广泛关注。然而，车辆传输环境中随时间变化的信道变化需要通信资源的动态分配。此外，在协作感知的背景下，重要的是要认识到并非所有 CAV 都能贡献有价值的数据，有些 CAV 数据甚至会对协作感知产生有害影响。在本文中，我们介绍了 SmartCooper，这是一种自适应协作感知框架，它结合了通信优化和判断器机制，以促进 CAV 数据融合。我们的方法首先是优化车辆的连接性，同时考虑通信限制。然后，我们训练一个可学习的编码器，以根据信道状态信息（CSI）动态调整压缩比。随后，我们设计了一种判断器机制来过滤自适应解码器重建的有害图像数据。我们在 OpenCOOD 平台上评估了我们提出的算法的有效性。我们的结果表明，与非评判者方案相比，通信成本大幅降低了 23.10%。此外，与最先进的方案相比，我们将 Intersection over Union (AP@IoU) 的平均精度显着提高了 7.15%。</details>
**PDF:** <http://arxiv.org/pdf/2402.00321v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Survey on Hallucination in Large Vision-Language Models**<br />
**Title_cn:** 大视觉语言模型中幻觉的调查<br />
**Authors:** Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng<br />
**Abstract:** <details><summary>原文: </summary>Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大型视觉语言模型（LVLM）的发展因其实际实施潜力而在人工智能领域引起了越来越多的关注。然而，“幻觉”，或者更具体地说，实际视觉内容与相应文本生成之间的不一致，对使用 LVLM 提出了重大挑战。在这项综合调查中，我们剖析了 LVLM 相关的幻觉，试图建立一个概述并促进未来的缓解。我们的审查从澄清 LVLM 幻觉的概念开始，呈现各种幻觉症状并强调 LVLM 幻觉固有的独特挑战。随后，我们概述了专门为评估 LVLM 特有的幻觉而定制的基准和方法。此外，我们还深入研究了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还严格审查现有的减轻幻觉的方法。讨论了与 LVLM 内幻觉相关的悬而未决的问题和未来方向，以结束本次调查。</details>
**PDF:** <http://arxiv.org/pdf/2402.00253v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering**<br />
**Title_cn:** StopThePop：用于视图一致实时渲染的排序高斯泼溅<br />
**Authors:** Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger<br />
**Abstract:** <details><summary>原文: </summary>Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.</details>
**Abstract_cn:** <details><summary>译文: </summary>高斯泼溅已成为从不同领域的图像构建 3D 表示的重要模型。然而，3D 高斯喷射渲染管道的效率依赖于多种简化。值得注意的是，使用单个视图空间深度将高斯图减少为 2D splats 会在视图旋转期间引入弹出和混合伪影。解决这个问题需要精确的每像素深度计算，但与全局排序操作相比，完整的每像素排序被证明成本过高。在本文中，我们提出了一种新颖的分层光栅化方法，该方法以最小的处理开销系统地重新利用和剔除图块。正如定量和定性测量所证明的那样，我们的软件光栅化器有效地消除了弹出伪影和视图不一致。同时，我们的方法减少了通过弹出来欺骗视图相关效果的可能性，确保更真实的表示。尽管消除了作弊行为，我们的方法仍为测试图像实现了可比的定量结果，同时提高了运动中新颖视图合成的一致性。由于其设计，我们的分层方法平均仅比原始高斯泼溅慢 4%。值得注意的是，强制一致性可以将高斯数量减少大约一半，同时质量和视图一致性几乎相同。因此，渲染性能几乎翻倍，使我们的方法比原始高斯泼溅法快 1.6 倍，同时内存需求减少 50%。</details>
**PDF:** <http://arxiv.org/pdf/2402.00525v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **AToM: Amortized Text-to-Mesh using 2D Diffusion**<br />
**Title_cn:** AToM：使用 2D 扩散的摊销文本到网格<br />
**Authors:** Guocheng Qian, Junli Cao, Aliaksandr Siarohin, Yash Kant, Chaoyang Wang, Michael Vasilkovsky, Hsin-Ying Lee, Yuwei Fang, Ivan Skorokhodov, Peiye Zhuang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously. In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts. Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability. Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs. AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 Amortized Text-to-Mesh (AToM)，这是一种同时跨多个文本提示进行优化的前馈文本到网格框架。现有的文本转 3D 方法通常需要耗时的每次提示优化以及通常输出除多边形网格之外的表示形式，而 AToM 可以在不到 1 秒的时间内直接生成高质量的纹理网格，训练时间减少约 10 倍成本，并推广到看不见的提示。我们的关键想法是一种新颖的基于三平面的文本到网格架构，具有两阶段摊销优化策略，可确保稳定的训练并实现可扩展性。通过对各种即时基准进行大量实验，AToM 的准确度显着优于最先进的摊销方法（在 DF415 数据集中），并且可生成更具可区分性和更高质量的 3D 输出。与按提示解决方案不同，AToM 表现出强大的通用性，为看不见的插值提示提供细粒度的 3D 资产，无需在推理过程中进一步优化。</details>
**PDF:** <http://arxiv.org/pdf/2402.00867v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Geometry Transfer for Stylizing Radiance Fields**<br />
**Title_cn:** 用于风格化辐射场的几何传递<br />
**Authors:** Hyunyoung Jung, Seonghyeon Nam, Nikolaos SarafianosSungjoo Yoo, Alexander Sorkine-Hornung, Rakesh Ranjan<br />
**Abstract:** <details><summary>原文: </summary>Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.</details>
**Abstract_cn:** <details><summary>译文: </summary>形状和几何图案对于定义风格特征至关重要。然而，当前的 3D 样式传输方法主要侧重于传输颜色和纹理，常常忽略几何方面。在本文中，我们介绍了几何迁移，这是一种利用几何变形进行 3D 风格迁移的新颖方法。该技术采用深度图来提取风格指南，随后应用于风格化辐射场的几何形状。此外，我们提出了利用 3D 场景中的几何线索的新技术，从而增强审美表现力并更准确地反映预期风格。我们的大量实验表明，几何迁移可以实现更广泛、更具表现力的风格化范围，从而显着扩大 3D 风格迁移的范围。</details>
**PDF:** <http://arxiv.org/pdf/2402.00863v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras**<br />
**Title_cn:** DRSM：用于固定单目相机动态重建的高效神经 4d 分解<br />
**Authors:** Weixing Xie, Xiao Dong, Yong Yang, Qiqin Lin, Jingze Chen, Junfeng Yao, Xiaohu Guo<br />
**Abstract:** <details><summary>原文: </summary>With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology. In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras. Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting. Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction. To overcome these difficulties, we propose deep supervised optimization and ray casting strategies. With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着视频共享和直播应用生成的单目视频的流行，在固定单目摄像机中重建和编辑动态场景已成为一项特殊但令人期待的技术。与利用多视图观察的场景重建相比，从单个视图建模动态场景的问题明显更缺乏约束和不适定。受神经渲染最新进展的启发，我们提出了一种新颖的框架来解决单目相机中动态场景的 4D 分解问题。我们的框架利用分解的静态和动态特征平面来表示 4D 场景，并强调通过密集光线投射来学习动态区域。来自单一视图和遮挡的 3D 线索不足也是场景重建中的特殊挑战。为了克服这些困难，我们提出了深度监督优化和光线投射策略。通过对各种视频的实验，我们的方法比现有的单视图动态场景表示方法产生了更高保真度的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.00740v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Diffusion-based Light Field Synthesis**<br />
**Title_cn:** 基于扩散的光场合成<br />
**Authors:** Ruisheng Gao, Yutong Liu, Zeyu Xiao, Zhiwei Xiong<br />
**Abstract:** <details><summary>原文: </summary>Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing.</details>
**Abstract_cn:** <details><summary>译文: </summary>光场（LF）有利于记录跨角度维度的综合场景辐射，在3D重建、虚拟现实和计算摄影等领域有着广泛的应用。然而，由于主流的采集策略，光场采集不可避免地耗时且资源密集。涉及手动捕获或费力的软件合成。鉴于这样的挑战，我们引入了 LFdiff，这是一种为 LF 合成量身定制的简单而有效的基于扩散的生成框架，它仅采用单个 RGB 图像作为输入。LFdiff 利用单目深度估计估计的视差网络并包含两个独特的组件：一个新颖的条件方案和一个针对 LF 数据定制的噪声估计网络。具体来说，我们设计了一个位置感知的扭曲条件方案，通过鲁棒的条件信号增强视图间几何学习。然后我们提出 DistgUnet，基于解缠结的噪声估计网络，以利用全面的 LF 表示。大量实验表明，LFdiff 擅长合成视觉上令人愉悦且视差可控的光场，并具有增强的泛化能力。此外，综合结果证实了生成的 LF 数据的广泛适用性，涵盖低频超分辨率和重新聚焦等应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.00575v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Recasting Regional Lighting for Shadow Removal**<br />
**Title_cn:** 重铸区域照明以消除阴影<br />
**Authors:** Yuhao Liu, Zhanghan Ke, Ke Xu, Fang Liu, Zhenwei Wang, Rynson W. H. Lau<br />
**Abstract:** <details><summary>原文: </summary>Removing shadows requires an understanding of both lighting conditions and object textures in a scene. Existing methods typically learn pixel-level color mappings between shadow and non-shadow images, in which the joint modeling of lighting and object textures is implicit and inadequate. We observe that in a shadow region, the degradation degree of object textures depends on the local illumination, while simply enhancing the local illumination cannot fully recover the attenuated textures. Based on this observation, we propose to condition the restoration of attenuated textures on the corrected local lighting in the shadow region. Specifically, We first design a shadow-aware decomposition network to estimate the illumination and reflectance layers of shadow regions explicitly. We then propose a novel bilateral correction network to recast the lighting of shadow regions in the illumination layer via a novel local lighting correction module, and to restore the textures conditioned on the corrected illumination layer via a novel illumination-guided texture restoration module. We further annotate pixel-wise shadow masks for the public SRD dataset, which originally contains only image pairs. Experiments on three benchmarks show that our method outperforms existing state-of-the-art shadow removal methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>消除阴影需要了解场景中的照明条件和对象纹理。现有方法通常学习阴影和非阴影图像之间的像素级颜色映射，其中照明和对象纹理的联合建模是隐式且不充分的。我们观察到，在阴影区域，物体纹理的退化程度取决于局部照明，而简单地增强局部照明并不能完全恢复衰减的纹理。基于这一观察，我们建议以阴影区域中校正的局部照明为条件来恢复衰减纹理。具体来说，我们首先设计一个阴影感知分解网络来明确估计阴影区域的照明和反射层。然后，我们提出了一种新颖的双边校正网络，通过新颖的局部照明校正模块来重新投射照明层中阴影区域的照明，并通过新颖的照明引导纹理恢复模块来恢复以校正后的照明层为条件的纹理。我们进一步注释公共 SRD 数据集的像素级阴影掩模，该数据集最初仅包含图像对。三个基准的实验表明，我们的方法优于现有最先进的阴影去除方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.00341v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID**<br />
**Title_cn:** 探索无监督可见红外行人再识别的同质和异质一致标签关联<br />
**Authors:** Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework. Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. The code will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督可见光-红外行人重新识别（USL-VI-ReID）旨在从不同方式检索相同身份的行人图像，而无需注释。虽然先前的工作侧重于建立跨模态伪标签关联来弥合模态差距，但他们忽略了在伪标签空间中维护实例级同质和异构一致性，从而导致粗关联。为此，我们引入了模态统一标签传输（MULT）模块，该模块同时考虑同质和异构细粒度实例级结构，产生高质量的跨模态标签关联。它对同质和异质亲和力进行建模，利用它们来定义伪标签的不一致性，然后将其最小化，从而导致伪标签保持跨模态的对齐和模态内结构的一致性。此外，还提出了一种简单的即插即用在线跨内存标签细化（OCLR）模块，以进一步减轻噪声伪标签的影响，同时对齐不同的模态，并结合模态不变表示学习（MIRL）框架。实验表明，我们提出的方法优于现有的 USL-VI-ReID 方法，凸显了我们的 MULT 与其他跨模态关联方法相比的优越性。该代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.00672v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters**<br />
**Title_cn:** 使用 Soft Silhouette 分数进行深度聚类：实现紧凑且分离良好的聚类<br />
**Authors:** Georgios Vardakas, Ioannis Papakostas, Aristidis Likas<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督学习在大数据时代得到了重视，它提供了一种从未标记的数据集中提取有价值的见解的方法。深度聚类已成为一个重要的无监督类别，旨在利用神经网络的非线性映射能力来增强聚类性能。大多数深度聚类文献侧重于最小化某些嵌入空间中的簇内变异性，同时保持学习的表示与原始高维数据集一致。在这项工作中，我们提出了 soft silhoutte，一种轮廓系数的概率公式。软轮廓奖励紧凑且明显分离的聚类解决方案，如传统的轮廓系数。当在深度聚类框架内进行优化时，软轮廓会引导学习到的表示形成紧凑且分离良好的聚类。此外，我们引入了一种基于自动编码器的深度学习架构，适用于优化软轮廓目标函数。所提出的深度聚类方法已经在各种基准数据集上进行了测试，并与几种经过充分研究的深度聚类方法进行了比较，产生了非常令人满意的聚类结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.00608v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **BootsTAP: Bootstrapped Training for Tracking-Any-Point**<br />
**Title_cn:** BootsTAP：用于跟踪任意点的引导训练<br />
**Authors:** Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, João Carreira, Andrew Zisserman<br />
**Abstract:** <details><summary>原文: </summary>To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了使模型更好地理解物理和运动，让它们感知真实场景中固体表面如何移动和变形是很有用的。这可以形式化为跟踪任意点（TAP），它要求算法能够跟踪视频中与固体表面相对应的任何点，可能在空间和时间上都很密集。 TAP 的大规模地面实况训练数据仅在模拟中可用，目前模拟的对象和运动种类有限。在这项工作中，我们演示了如何使用自我监督的学生-教师设置，以最小的架构更改来改进大规模、未标记、未整理的现实世界数据。我们在 TAP-Vid 基准测试中展示了最先进的性能，大幅超越了之前的结果：例如，TAP-Vid-DAVIS 性能从 61.3% 提高到 66.4%，TAP-Vid-Kinetics 从 57.2% 提高至 61.5%。</details>
**PDF:** <http://arxiv.org/pdf/2402.00847v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction**<br />
**Title_cn:** ChaosBench：基于物理的多通道次季节到季节气候预测基准<br />
**Authors:** Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke, Aditya Grover, Pierre Gentine<br />
**Abstract:** <details><summary>原文: </summary>Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确预测次季节到季节尺度的气候对于防灾、降低经济风险和改进气候变化政策制定至关重要。然而，由于系统的混沌性质，S2S 预测仍然具有挑战性。目前，现有的天气和气候应用基准往往 (1) 预测范围较短，最多 14 天，(2) 不包括广泛的业务基线预测，(3) 缺乏基于物理的约束为了便于解释。因此，我们提出了 ChaosBench，一个大规模、多通道、基于物理的 S2S 预测基准。 ChaosBench 拥有超过 46 万帧的现实世界观测和模拟，每个帧有 60 个可变通道，跨度长达 45 年。除了基于视觉的指标之外，我们还提出了几种基于物理的指标，以实现物理上更加一致的模型。此外，我们还纳入了来自 4 个国家气象机构的一系列基于物理的预测，作为我们的数据驱动对应预测的基线。我们建立了两个复杂程度不同的任务：完整动态预测和稀疏动态预测。我们的基准是第一个对现有模型（包括 PanguWeather、FourCastNetV2、GraphCast 和 ClimaX）进行大规模评估的基准之一，并发现最初为天气规模应用开发的方法在 S2S 任务上失败。我们在 https://leap-stc.github.io/ChaosBench 发布了基准代码和数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.00712v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching**<br />
**Title_cn:** 深度机器人素描：深度 Q 学习网络在类人素描中的应用<br />
**Authors:** Raul Fernandez-Fernandez, Juan G. Victores, Carlos Balaguer<br />
**Abstract:** <details><summary>原文: </summary>The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs</details>
**Abstract_cn:** <details><summary>译文: </summary>目前强化学习算法在复杂环境中的表现所取得的成功启发了许多认知科学的最新理论方法。艺术环境在认知科学界被研究为丰富的、自然的、多感官的、多文化的环​​境。在这项工作中，我们建议引入强化学习来改善艺术机器人应用的控制。深度 Q 学习神经网络 (DQN) 是在机器人领域实施强化学习最成功的算法之一。 DQN 方法生成复杂的控制策略，用于在各种环境中执行复杂的机器人应用程序。当前的艺术绘画机器人应用程序使用简单的控制法则，限制了框架对一组简单环境的适应性。在这项工作中，建议在艺术绘画机器人应用中引入 DQN。目标是研究复杂控制策略的引入如何影响基本艺术绘画机器人应用的性能。这项工作的主要预期贡献是作为未来为复杂艺术绘画机器人框架引入 DQN 方法的工作的第一个基线。实验包括使用 DQN 生成的策略和 TEO（人形机器人）在现实世界中执行人类绘制的草图。比较结果的相似性并获得相对于参考输入的奖励</details>
**PDF:** <http://arxiv.org/pdf/2402.00676v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks**<br />
**Title_cn:** 神经网络的热带决策边界对于对抗性攻击具有鲁棒性<br />
**Authors:** Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, Jefferson Huang<br />
**Abstract:** <details><summary>原文: </summary>We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种简单、易于实现且计算高效的热带卷积神经网络架构，该架构对于对抗性攻击具有鲁棒性。我们通过将热带投影环中的数据嵌入到单个隐藏层中来利用分段线性神经网络的热带性质，该隐藏层可以添加到任何模型中。我们从理论上研究了其决策边界的几何形状，并使用计算实验展示了其针对图像数据集的对抗性攻击的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.00576v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Short: Benchmarking transferable adversarial attacks**<br />
**Title_cn:** 简短：可转移对抗性攻击的基准测试<br />
**Authors:** Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen<br />
**Abstract:** <details><summary>原文: </summary>The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintessential resource for both scholars and practitioners in the field, charting the complex terrain of adversarial transferability and setting a foundation for future explorations in this vital sector. The associated codebase is accessible at: https://github.com/KxPlaug/TAA-Bench</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型对抗对抗性攻击的稳健性仍然是一个关键问题。这项研究首次对对抗性攻击的可转移性方面进行了详尽的回顾。它系统地分类并严格评估了为增强对抗性攻击的可转移性而开发的各种方法。这项研究涵盖了一系列技术，包括生成结构、语义相似性、梯度编辑、目标修改和集成方法。同时，本文介绍了一个基准框架\textit{TAA-Bench}，集成了十种领先的对抗性攻击可转移性方法，从而为跨不同模型架构的比较分析提供了标准化和系统化的平台。通过全面的审查，我们描述了每种方法的功效和局限性，阐明了它们的基本操作原理和实际用途。这篇综述致力于成为该领域学者和从业者的典型资源，描绘出对抗性可转移性的复杂地形，并为这一重要领域的未来探索奠定基础。相关的代码库可在以下位置访问：https://github.com/KxPlaug/TAA-Bench</details>
**PDF:** <http://arxiv.org/pdf/2402.00418v1><br />
**Code:** <https://github.com/kxplaug/taa-bench>**<br />
>>**index:** 6<br />
**Title:** **LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model**<br />
**Title_cn:** LM-HT SNN：通过可学习的多层次阈值模型增强 SNN 与 ANN 对应物的性能<br />
**Authors:** Zecheng Hao, Xinyu Shi, Zhiyu Pan, Yujia Liu, Zhaofei Yu, Tiejun Huang<br />
**Abstract:** <details><summary>原文: </summary>Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion framework. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our LM-HT model can significantly outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs.</details>
**Abstract_cn:** <details><summary>译文: </summary>与传统的人工神经网络（ANN）相比，尖峰神经网络（SNN）因其以更加受生物启发和节能的方式传输信息的内在能力而引起了广泛的学术兴趣。然而，尽管之前人们通过各种方法努力优化 SNN 的学习梯度和模型结构，但 SNN 在性能方面仍然在一定程度上落后于 ANN。最近提出的多阈值模型为进一步增强SNN的学习能力提供了更多可能性。在本文中，我们从数学角度严格分析了多阈值模型、普通尖峰模型和量化人工神经网络之间的关系，然后提出了一种新颖的LM-HT模型，这是一种可以动态调节全局的等距多层次模型。输入电流和膜电位泄漏在时间维度上。此外，我们注意到基于LM-HT模型的直接训练算法可以与传统的ANN-SNN转换框架无缝集成。这种新颖的混合学习框架可以有效改善转换后的 SNN 在低时延下相对较差的性能。大量的实验结果表明，我们的 LM-HT 模型在各种类型的数据集上都可以显着优于以前的最先进的工作，这促使 SNN 达到与量化 ANN 相当的全新性能水平。</details>
**PDF:** <http://arxiv.org/pdf/2402.00411v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **InfMAE: A Foundation Model in Infrared Modality**<br />
**Title_cn:** InfMAE：红外模态的基础模型<br />
**Authors:** Fangcen Liu, Chenqiang Gao, Yaming Zhang, Junjie Guo, Jinhao Wang, Deyu Meng<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities. However, it remains an open question on how to design an infrared foundation model. In this paper, we propose InfMAE, a foundation model in infrared modality. We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community. Besides, we design an information-aware masking strategy, which is suitable for infrared images. This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation. In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks. Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks. Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks. Our code will be made public at https://github.com/liufangcen/InfMAE.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，基础模型席卷了计算机视觉领域，促进了不同模式下各种任务的开发。然而，如何设计红外基础模型仍然是一个悬而未决的问题。在本文中，我们提出了 InfMAE，这是红外模态的基础模型。我们发布了一个名为 Inf30 的红外数据集，以解决红外视觉社区缺乏大规模自监督学习数据的问题。此外，我们设计了一种适用于红外图像的信息感知掩蔽策略。这种掩蔽策略允许在自监督学习过程中更加重视红外图像中信息更丰富的区域，有利于学习广义表示。此外，我们采用多尺度编码器来增强预训练编码器在下游任务中的性能。最后，基于红外图像没有大量细节和纹理信息的事实，我们设计了红外解码器模块，进一步提高了下游任务的性能。大量实验表明，我们提出的方法 InfMAE 在三个下游任务中优于其他监督方法和自监督学习方法。我们的代码将在https://github.com/liufangcen/InfMAE公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.00407v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling**<br />
**Title_cn:** SCO-VIST：基于社交互动常识知识的视觉叙事<br />
**Authors:** Eileen Wang, Soyeon Caren Han, Josiah Poon<br />
**Abstract:** <details><summary>原文: </summary>Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm. Our proposed framework produces stories superior across multiple metrics in terms of visual grounding, coherence, diversity, and humanness, per both automatic and human evaluations.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉叙事旨在根据给定的图像序列自动生成连贯的故事。与图像字幕等任务不同，视觉故事应该包含事实描述、世界观和人类社会常识，将脱节的元素组合在一起，形成一个连贯且引人入胜的人类可写故事。然而，大多数模型在尝试创建故事时主要侧重于应用事实信息和使用分类/词汇外部知识。本文介绍了 SCO-VIST，一个将图像序列表示为具有对象和关系的图的框架，其中包括人类行为动机及其社会互动常识知识。然后，SCO-VIST 采用表示绘图点的图，并使用基于语义和基于出现的边缘权重在绘图点之间创建桥梁。该加权故事图使用 Floyd-Warshall 算法生成一系列事件的故事情节。根据自动和人工评估，我们提出的框架在视觉基础、连贯性、多样性和人性方面产生了跨多个指标的优秀故事。</details>
**PDF:** <http://arxiv.org/pdf/2402.00319v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Invariance-powered Trustworthy Defense via Remove Then Restore**<br />
**Title_cn:** 通过删除然后恢复实现不变性的可信防御<br />
**Authors:** Xiaowei Fu, Yuhang Zhou, Lina Ma, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Adversarial attacks pose a challenge to the deployment of deep neural networks (DNNs), while previous defense models overlook the generalization to various attacks. Inspired by targeted therapies for cancer, we view adversarial samples as local lesions of natural benign samples, because a key finding is that salient attack in an adversarial sample dominates the attacking process, while trivial attack unexpectedly provides trustworthy evidence for obtaining generalizable robustness. Based on this finding, a Pixel Surgery and Semantic Regeneration (PSSR) model following the targeted therapy mechanism is developed, which has three merits: 1) To remove the salient attack, a score-based Pixel Surgery module is proposed, which retains the trivial attack as a kind of invariance information. 2) To restore the discriminative content, a Semantic Regeneration module based on a conditional alignment extrapolator is proposed, which achieves pixel and semantic consistency. 3) To further harmonize robustness and accuracy, an intractable problem, a self-augmentation regularizer with adversarial R-drop is designed. Experiments on numerous benchmarks show the superiority of PSSR.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性攻击对深度神经网络（DNN）的部署提出了挑战，而以前的防御模型忽视了对各种攻击的泛化。受癌症靶向治疗的启发，我们将对抗性样本视为自然良性样本的局部病变，因为一个关键发现是对抗性样本中的显着攻击主导了攻击过程，而琐碎攻击意外地为获得普遍稳健性提供了可靠的证据。基于这一发现，开发了遵循靶向治疗机制的像素手术和语义再生（PSSR）模型，该模型具有三个优点：1）为了消除显着攻击，提出了基于评分的像素手术模块，该模块保留了琐碎的攻击攻击作为一种不变性信息。 2）为了恢复有判别性的内容，提出了一种基于条件对齐外推器的语义再生模块，实现了像素和语义的一致性。 3）为了进一步协调鲁棒性和准确性这一棘手问题，设计了一种具有对抗性 R-drop 的自增强正则化器。众多基准测试的实验显示了 PSSR 的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.00304v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps**<br />
**Title_cn:** 了解使用向量空间和逆映射进行图像分析的神经网络系统<br />
**Authors:** Rebecca Pattichis, Marios S. Pattichis<br />
**Abstract:** <details><summary>原文: </summary>There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.</details>
**Abstract_cn:** <details><summary>译文: </summary>人们对开发可用于理解图像分析中使用的复杂神经网络的数学方法有着浓厚的兴趣。在本文中，我们介绍了线性代数技术，将神经网络层建模为信号空间之间的映射。首先，我们演示如何使用信号空间来可视化权重空间和卷积层内核。我们还演示了如何使用残差向量空间来进一步可视化每层丢失的信息。其次，我们介绍可逆网络的概念以及用于计算产生特定输出的输入图像的算法。我们在两个可逆网络和 ResNet18 上展示了我们的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.00261v1><br />
**Code:** null<br />

