## [UPDATED!] **2024-02-02** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties**<br />
**Title_cn:** NeuroCine：解码人脑活动中的生动视频序列<br />
**Authors:** Jingyuan Sun, Mingxiao Li, Zijiao Chen, Marie-Francine Moens<br />
**Abstract:** <details><summary>原文: </summary>In the pursuit to understand the intricacies of human brain's visual processing, reconstructing dynamic visual experiences from brain activities emerges as a challenging yet fascinating endeavor. While recent advancements have achieved success in reconstructing static images from non-invasive brain recordings, the domain of translating continuous brain activities into video format remains underexplored. In this work, we introduce NeuroCine, a novel dual-phase framework to targeting the inherent challenges of decoding fMRI data, such as noises, spatial redundancy and temporal lags. This framework proposes spatial masking and temporal interpolation-based augmentation for contrastive learning fMRI representations and a diffusion model enhanced by dependent prior noise for video generation. Tested on a publicly available fMRI dataset, our method shows promising results, outperforming the previous state-of-the-art models by a notable margin of ${20.97\%}$, ${31.00\%}$ and ${12.30\%}$ respectively on decoding the brain activities of three subjects in the fMRI dataset, as measured by SSIM. Additionally, our attention analysis suggests that the model aligns with existing brain structures and functions, indicating its biological plausibility and interpretability.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了理解人脑视觉处理的复杂性，从大脑活动中重建动态视觉体验成为一项具有挑战性但令人着迷的努力。虽然最近的进展在从非侵入性大脑记录中重建静态图像方面取得了成功，但将连续大脑活动转换为视频格式的领域仍然尚未得到充分探索。在这项工作中，我们引入了 NeuroCine，这是一种新颖的双相框架，旨在解决解码 fMRI 数据的固有挑战，例如噪声、空间冗余和时间滞后。该框架提出了用于对比学习 fMRI 表示的空间掩蔽和基于时间插值的增强，以及通过视频生成的相关先验噪声增强的扩散模型。在公开可用的 fMRI 数据集上进行测试后，我们的方法显示出令人鼓舞的结果，其性能明显优于之前最先进的模型 ${20.97\%}$、${31.00\%}$ 和 ${12.30\ %}$ 分别解码 fMRI 数据集中三个受试者的大脑活动（通过 SSIM 测量）。此外，我们的注意力分析表明该模型与现有的大脑结构和功能相符，表明其生物学合理性和可解释性。</details>
**PDF:** <http://arxiv.org/pdf/2402.01590v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Boximator: Generating Rich and Controllable Motions for Video Synthesis**<br />
**Title_cn:** Boximator：为视频合成生成丰富且可控的运动<br />
**Authors:** Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, Hang Li<br />
**Abstract:** <details><summary>原文: </summary>Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成丰富且可控的运动是视频合成中的关键挑战。我们提出了 Boximator，一种细粒度运动控制的新方法。 Boximator 引入了两种约束类型：硬盒和软盒。用户使用硬框选择条件帧中的对象，然后使用任一类型的框来粗略或严格地定义对象在未来帧中的位置、形状或运动路径。 Boximator 充当现有视频扩散模型的插件。其训练过程通过冻结原始权重并仅训练控制模块来保留基本模型的知识。为了解决训练挑战，我们引入了一种新颖的自跟踪技术，该技术极大地简化了盒子-对象相关性的学习。根据经验，Boximator 实现了最先进的视频质量 (FVD) 分数，在两个基本模型的基础上进行了改进，并在合并框约束后进一步增强。其强大的运动可控性通过边界框对齐度量的大幅增加得到验证。人工评估还表明，与基本模型相比，用户更喜欢 Boximator 生成结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.01566v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Cross-view Masked Diffusion Transformers for Person Image Synthesis**<br />
**Title_cn:** 用于人物图像合成的交叉视图掩模扩散变压器<br />
**Authors:** Trung X. Pham, Zhang Kang, Chang D. Yoo<br />
**Abstract:** <details><summary>原文: </summary>We present X-MDPT (Cross-view Masked Diffusion Prediction Transformers), a novel diffusion model designed for pose-guided human image generation. X-MDPT distinguishes itself by employing masked diffusion transformers that operate on latent patches, a departure from the commonly-used Unet structures in existing works. The model comprises three key modules: 1) a denoising diffusion Transformer, 2) an aggregation network that consolidates conditions into a single vector for the diffusion process, and 3) a mask cross-prediction module that enhances representation learning with semantic information from the reference image. X-MDPT demonstrates scalability, improving FID, SSIM, and LPIPS with larger models. Despite its simple design, our model outperforms state-of-the-art approaches on the DeepFashion dataset while exhibiting efficiency in terms of training parameters, training time, and inference speed. Our compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent diffusion approach (FID 8.07) using only $11\times$ fewer parameters. Our best model surpasses the pixel-based diffusion with $\frac{2}{3}$ of the parameters and achieves $5.43 \times$ faster inference.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 X-MDPT（交叉视图掩模扩散预测变压器），这是一种专为姿势引导的人体图像生成而设计的新型扩散模型。 X-MDPT 的独特之处在于采用了对潜在补丁进行操作的掩模扩散变压器，这与现有作品中常用的 Unet 结构不同。该模型包含三个关键模块：1) 去噪扩散变压器；2) 将条件合并为扩散过程的单个向量的聚合网络；3) 掩模交叉预测模块，利用来自参考的语义信息增强表示学习图像。 X-MDPT 展示了可扩展性，通过更大的模型改进了 FID、SSIM 和 LPIPS。尽管设计简单，但我们的模型在 DeepFashion 数据集上的性能优于最先进的方法，同时在训练参数、训练时间和推理速度方面表现出效率。我们的紧凑型 33MB 模型实现了 7.42 的 FID，超越了之前的 Unet 潜在扩散方法 (FID 8.07)，仅使用了 11 美元的参数。我们的最佳模型超越了参数 $\frac{2}{3}$ 的基于像素的扩散，并实现了 $5.43 \times$ 的更快推理速度。</details>
**PDF:** <http://arxiv.org/pdf/2402.01516v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Advancing Brain Tumor Inpainting with Generative Models**<br />
**Title_cn:** 利用生成模型推进脑肿瘤修复<br />
**Authors:** Ruizhi Zhu, Xinru Zhang, Haowen Pang, Chundan Xu, Chuyang Ye<br />
**Abstract:** <details><summary>原文: </summary>Synthesizing healthy brain scans from diseased brain scans offers a potential solution to address the limitations of general-purpose algorithms, such as tissue segmentation and brain extraction algorithms, which may not effectively handle diseased images. We consider this a 3D inpainting task and investigate the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic resonance imaging(MRI) data. Our contributions encompass potential modifications tailored to MRI-specific needs, and we conducted evaluations of multiple inpainting techniques using the BraTS2023 Inpainting datasets to assess their efficacy and limitations.</details>
**Abstract_cn:** <details><summary>译文: </summary>从患病的大脑扫描合成健康的大脑扫描提供了一种潜在的解决方案，可以解决通用算法的局限性，例如组织分割和大脑提取算法，这些算法可能无法有效地处理患病图像。我们将此视为 3D 修复任务，并研究 2D 修复方法的适应性以满足 3D 磁共振成像 (MRI) 数据的要求。我们的贡献包括针对 MRI 特定需求进行的潜在修改，并且我们使用 BraTS2023 修复数据集对多种修复技术进行了评估，以评估其功效和局限性。</details>
**PDF:** <http://arxiv.org/pdf/2402.01509v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Synthetic Data for the Mitigation of Demographic Biases in Face Recognition**<br />
**Title_cn:** 用于减轻人脸识别中人口统计偏差的合成数据<br />
**Authors:** Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Dominik Lawatsch, Florian Domin, Maxim Schaubert<br />
**Abstract:** <details><summary>原文: </summary>This study investigates the possibility of mitigating the demographic biases that affect face recognition technologies through the use of synthetic data. Demographic biases have the potential to impact individuals from specific demographic groups, and can be identified by observing disparate performance of face recognition systems across demographic groups. They primarily arise from the unequal representations of demographic groups in the training data. In recent times, synthetic data have emerged as a solution to some problems that affect face recognition systems. In particular, during the generation process it is possible to specify the desired demographic and facial attributes of images, in order to control the demographic distribution of the synthesized dataset, and fairly represent the different demographic groups. We propose to fine-tune with synthetic data existing face recognition systems that present some demographic biases. We use synthetic datasets generated with GANDiffFace, a novel framework able to synthesize datasets for face recognition with controllable demographic distribution and realistic intra-class variations. We consider multiple datasets representing different demographic groups for training and evaluation. Also, we fine-tune different face recognition systems, and evaluate their demographic fairness with different metrics. Our results support the proposed approach and the use of synthetic data to mitigate demographic biases in face recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究探讨了通过使用合成数据来减轻影响人脸识别技术的人口统计学偏见的可能性。人口统计学偏见有可能影响特定人口群体的个人，并且可以通过观察不同人口群体的面部识别系统的不同性能来识别。它们主要源于训练数据中人口群体的不平等代表性。近年来，合成数据的出现可以解决影响人脸识别系统的一些问题。特别是，在生成过程中，可以指定图像所需的人口统计和面部属性，以便控制合成数据集的人口统计分布，并公平地代表不同的人口统计群体。我们建议利用合成数据对存在一些人口统计偏差的现有人脸识别系统进行微调。我们使用 GANDiffFace 生成的合成数据集，GANDiffFace 是一种新颖的框架，能够合成具有可控人口分布和现实类内变化的人脸识别数据集。我们考虑代表不同人口群体的多个数据集进行训练和评估。此外，我们还对不同的人脸识别系统进行微调，并使用不同的指标评估其人口统计公平性。我们的结果支持所提出的方法和使用合成数据来减轻人脸识别中的人口统计偏差。</details>
**PDF:** <http://arxiv.org/pdf/2402.01472v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation**<br />
**Title_cn:** EmoSpeaker：一次性细粒度情感控制说话面部生成<br />
**Authors:** Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun<br />
**Abstract:** <details><summary>原文: </summary>Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: https://peterfanfan.github.io/EmoSpeaker/</details>
**Abstract_cn:** <details><summary>译文: </summary>实现细粒度的情感控制对于情感生成任务至关重要，因为它增强了生成模型的表达能力，使其能够准确、全面地捕捉和表达各种细微的情感状态，从而提高生成内容的情感质量和个性化。仅使用肖像和录音来生成精细的面部动画来准确地描绘情感表达是一项挑战。为了应对这一挑战，我们提出了一种视觉属性引导的音频解耦器。这使得能够获取仅与音频内容相关的内容向量，从而增强后续嘴唇运动系数预测的稳定性。为了实现更精确的情感表达，我们引入了细粒度的情感系数预测模块。此外，我们提出了一种使用细粒度情绪矩阵的情绪强度控制方法。通过这些，可以实现对生成视频中的情绪表达的有效控制以及对情绪强度的更精细的分类。随后，设计了一系列 3DMM 系数生成网络来预测 3D 系数，然后利用渲染网络生成最终视频。我们的实验结果表明，我们提出的方法 EmoSpeaker 在表情变化和唇形同步方面优于现有的情感说话面部生成方法。项目页面：https://peterfanfan.github.io/EmoSpeaker/</details>
**PDF:** <http://arxiv.org/pdf/2402.01422v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors**<br />
**Title_cn:** 作弊后缀：对具有多模态先验的文本到图像扩散模型的针对性攻击<br />
**Authors:** Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models. Our code is publicly available at \url{https://github.com/ydc123/MMP-Attack}.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型已广泛应用于各种图像生成任务中，展示了图像和文本模式之间的非凡联系。然而，它们面临着被恶意利用的挑战，即通过在原始提示中附加特定后缀来生成有害或敏感图像。现有的工作主要集中于利用单模态信息进行攻击，未能利用多模态特征，导致性能不太理想。结合多模态先验（MMP），即文本和图像特征，我们在这项工作中提出了一种名为 MMP-Attack 的有针对性的攻击方法。具体来说，MMP-Attack 的目标是将目标对象添加到图像内容中，同时删除原始对象。 MMP-Attack较现有作品具有显着优势，具有卓越的通用性和可移植性，可以有效攻击DALL-E 3等商业文本到图像（T2I）模型。据我们所知，这标志着首次成功攻击尝试对商业 T2I 模型进行基于转移的攻击。我们的代码可在 \url{https://github.com/ydc123/MMP-Attack} 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.01369v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?**<br />
**Title_cn:** 形状注入的关节嵌入可以改善图像条件 3D 扩散吗？<br />
**Authors:** Cristian Sbrolli, Paolo Cudrano, Matteo Matteucci<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation. The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results. Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images? Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality? This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus. Our comprehensive analysis assesses CISP's guidance performance against CLIP guided models, focusing on generation quality, diversity, and coherence of the produced shapes with the conditioning image. We find that, while matching CLIP in generation quality and diversity, CISP substantially improves coherence with input images, underscoring the value of incorporating 3D knowledge into generative models. These findings suggest a promising direction for advancing the synthesis of 3D visual content by integrating multimodal systems with 3D representations.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度生成模型的最新进展，特别是将 CLIP（对比语言图像预训练）应用于去噪扩散概率模型（DDPM），在文本到图像生成方面表现出了显着的有效性。 CLIP 结构良好的嵌入空间也已扩展到使用 DDPM 进行图像到形状生成，产生了显着的结果。尽管取得了这些成功，但还是出现了一些基本问题：CLIP 能否确保从图像生成形状时获得最佳结果？我们能否利用条件作用将显式 3D 知识带入生成过程并获得更好的质量？本研究引入了 CISP（对比图像形状预训练），旨在增强 2D 图像引导的 3D 形状合成。 CISP 旨在通过在共享嵌入空间中将 2D 图像与 3D 形状对齐来丰富 CLIP 框架，特别是捕获可能被 CLIP 的文本图像焦点忽略的 3D 特征。我们的综合分析根据 CLIP 引导模型评估 CISP 的引导性能，重点关注生成的质量、多样性以及生成的形状与调节图像的一致性。我们发现，虽然在生成质量和多样性方面与 CLIP 相匹配，但 CISP 显着提高了与输入图像的一致性，强调了将 3D 知识纳入生成模型的价值。这些发现为通过将多模态系统与 3D 表示相集成来推进 3D 视觉内容的合成提供了一个有希望的方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.01241v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **PRIME: Protect Your Videos From Malicious Editing**<br />
**Title_cn:** PRIME：保护您的视频免遭恶意编辑<br />
**Authors:** Guanlin Li, Shuai Yang, Jie Zhang, Tianwei Zhang<br />
**Abstract:** <details><summary>原文: </summary>With the development of generative models, the quality of generated content keeps increasing. Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts. While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals. Malicious users can exploit these tools for deceptive or illegal purposes. Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness. Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance. Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics. Our evaluation results indicate that PRIME only costs 8.3% GPU hours of the cost of the previous state-of-the-art method and achieves better protection results on both human evaluation and objective metrics. Code can be found in https://github.com/GuanlinLee/prime.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着生成模型的发展，生成内容的质量不断提高。最近，开源模型使操作和编辑照片和视频变得异常容易，只需几个简单的提示即可。这些尖端技术受到欢迎的同时，也引发了对个人隐私和肖像权的担忧。恶意用户可以利用这些工具进行欺骗或非法目的。尽管之前的一些工作侧重于保护照片免受生成模型的影响，但我们发现保护视频和图像在效率和效果方面仍然存在差距。因此，我们引入了我们的保护方法PRIME，以显着降低时间成本并提高保护性能。此外，为了评估我们提出的保护方法，我们考虑了客观指标和人类主观指标。我们的评估结果表明，PRIME 的 GPU 小时数仅为之前最先进方法成本的 8.3%，并且在人工评估和客观指标上都取得了更好的保护效果。代码可以在 https://github.com/GuanlinLee/prime 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.01239v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Structured World Modeling via Semantic Vector Quantization**<br />
**Title_cn:** 通过语义向量量化进行结构化世界建模<br />
**Authors:** Yi-Fu Wu, Minseung Lee, Sungjin Ahn<br />
**Abstract:** <details><summary>原文: </summary>Neural discrete representations are crucial components of modern neural networks. However, their main limitation is that the primary strategies such as VQ-VAE can only provide representations at the patch level. Therefore, one of the main goals of representation learning, acquiring structured, semantic, and compositional abstractions such as the color and shape of an object, remains elusive. In this paper, we present the first approach to semantic neural discrete representation learning. The proposed model, called Semantic Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in unsupervised object-centric learning to address this limitation. Specifically, we observe that a simple approach quantizing at the object level poses a significant challenge and propose constructing scene representations hierarchically, from low-level discrete concept schemas to object representations. Additionally, we suggest a novel method for structured semantic world modeling by training a prior over these representations, enabling the ability to generate images by sampling the semantic properties of the objects in the scene. In experiments on various 2D and 3D object-centric datasets, we find that our model achieves superior generation performance compared to non-semantic vector quantization methods such as VQ-VAE and previous object-centric generative models. Furthermore, we find that the semantic discrete representations can solve downstream scene understanding tasks that require reasoning about the properties of different objects in the scene.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经离散表示是现代神经网络的重要组成部分。然而，它们的主要限制是 VQ-VAE 等主要策略只能提供补丁级别的表示。因此，表示学习的主要目标之一，即获取结构化、语义和组合抽象，例如物体的颜色和形状，仍然难以实现。在本文中，我们提出了第一种语义神经离散表示学习方法。所提出的模型称为语义向量量化变分自动编码器（SVQ），利用无监督的以对象为中心的学习的最新进展来解决这一限制。具体来说，我们观察到在对象级别量化的简单方法提出了重大挑战，并提出从低级离散概念模式到对象表示分层构建场景表示。此外，我们提出了一种通过训练这些表示的先验来进行结构化语义世界建模的新方法，从而能够通过对场景中对象的语义属性进行采样来生成图像。在各种 2D 和 3D 以对象为中心的数据集上的实验中，我们发现与 VQ-VAE 等非语义矢量量化方法和以前的以对象为中心的生成模型相比，我们的模型实现了卓越的生成性能。此外，我们发现语义离散表示可以解决需要推理场景中不同对象属性的下游场景理解任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.01203v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Unsupervised Generation of Pseudo Normal PET from MRI with Diffusion Model for Epileptic Focus Localization**<br />
**Title_cn:** 利用扩散模型从 ​​MRI 中无监督生成伪正常 PET，用于癫痫病灶定位<br />
**Authors:** Wentao Chen, Jiwei Li, Xichen Xu, Hui Huang, Siyu Yuan, Miao Zhang, Tianming Xu, Jie Luo, Weimin Zhou<br />
**Abstract:** <details><summary>原文: </summary>[$^{18}$F]fluorodeoxyglucose (FDG) positron emission tomography (PET) has emerged as a crucial tool in identifying the epileptic focus, especially in cases where magnetic resonance imaging (MRI) diagnosis yields indeterminate results. FDG PET can provide the metabolic information of glucose and help identify abnormal areas that are not easily found through MRI. However, the effectiveness of FDG PET-based assessment and diagnosis depends on the selection of a healthy control group. The healthy control group typically consists of healthy individuals similar to epilepsy patients in terms of age, gender, and other aspects for providing normal FDG PET data, which will be used as a reference for enhancing the accuracy and reliability of the epilepsy diagnosis. However, significant challenges arise when a healthy PET control group is unattainable. Yaakub \emph{et al.} have previously introduced a Pix2PixGAN-based method for MRI to PET translation. This method used paired MRI and FDG PET scans from healthy individuals for training, and produced pseudo normal FDG PET images from patient MRIs that are subsequently used for lesion detection. However, this approach requires a large amount of high-quality, paired MRI and PET images from healthy control subjects, which may not always be available. In this study, we investigated unsupervised learning methods for unpaired MRI to PET translation for generating pseudo normal FDG PET for epileptic focus localization. Two deep learning methods, CycleGAN and SynDiff, were employed, and we found that diffusion-based method achieved improved performance in accurately localizing the epileptic focus.</details>
**Abstract_cn:** <details><summary>译文: </summary>[$^{18}$F]氟脱氧葡萄糖 (FDG) 正电子发射断层扫描 (PET) 已成为识别癫痫病灶的重要工具，特别是在磁共振成像 (MRI) 诊断产生不确定结果的情况下。 FDG PET可以提供葡萄糖的代谢信息，帮助识别MRI不易发现的异常区域。然而，基于 FDG PET 的评估和诊断的有效性取决于健康对照组的选择。健康对照组通常由年龄、性别等方面与癫痫患者相似的健康个体组成，提供正常的FDG PET数据，作为提高癫痫诊断准确性和可靠性的参考。然而，当无法获得健康的 PET 对照组时，就会出现重大挑战。 Yaakub \emph{等人}之前介绍了一种基于 Pix2PixGAN 的 MRI 到 PET 转换方法。该方法使用健康个体的配对 MRI 和 FDG PET 扫描进行训练，并从患者 MRI 生成伪正常 FDG PET 图像，随后用于病变检测。然而，这种方法需要来自健康对照受试者的大量高质量、配对的 MRI 和 PET 图像，而这些图像可能并不总是可用。在这项研究中，我们研究了用于不配对 MRI 到 PET 转换的无监督学习方法，以生成用于癫痫病灶定位的伪正常 FDG PET。采用了两种深度学习方法 CycleGAN 和 SynDiff，我们发现基于扩散的方法在准确定位癫痫病灶方面取得了改进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.01191v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Ambient-Pix2PixGAN for Translating Medical Images from Noisy Data**<br />
**Title_cn:** Ambient-Pix2PixGAN 用于从噪声数据转换医学图像<br />
**Authors:** Wentao Chen, Xichen Xu, Jie Luo, Weimin Zhou<br />
**Abstract:** <details><summary>原文: </summary>Image-to-image translation is a common task in computer vision and has been rapidly increasing the impact on the field of medical imaging. Deep learning-based methods that employ conditional generative adversarial networks (cGANs), such as Pix2PixGAN, have been extensively explored to perform image-to-image translation tasks. However, when noisy medical image data are considered, such methods cannot be directly applied to produce clean images. Recently, an augmented GAN architecture named AmbientGAN has been proposed that can be trained on noisy measurement data to synthesize high-quality clean medical images. Inspired by AmbientGAN, in this work, we propose a new cGAN architecture, Ambient-Pix2PixGAN, for performing medical image-to-image translation tasks by use of noisy measurement data. Numerical studies that consider MRI-to-PET translation are conducted. Both traditional image quality metrics and task-based image quality metrics are employed to assess the proposed Ambient-Pix2PixGAN. It is demonstrated that our proposed Ambient-Pix2PixGAN can be successfully trained on noisy measurement data to produce high-quality translated images in target imaging modality.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像到图像的转换是计算机视觉中的一项常见任务，并且对医学成像领域的影响正在迅速增加。采用条件生成对抗网络 (cGAN) 的基于深度学习的方法（例如 Pix2PixGAN）已被广泛探索用于执行图像到图像的翻译任务。然而，当考虑噪声医学图像数据时，此类方法不能直接应用来产生干净的图像。最近，提出了一种名为 AmbientGAN 的增强型 GAN 架构，可以对噪声测量数据进行训练，以合成高质量的干净医学图像。受 AmbientGAN 的启发，在这项工作中，我们提出了一种新的 cGAN 架构 Ambient-Pix2PixGAN，用于通过使用噪声测量数据执行医学图像到图像的转换任务。进行了考虑 MRI 到 PET 转换的数值研究。采用传统的图像质量指标和基于任务的图像质量指标来评估所提出的 Ambient-Pix2PixGAN。事实证明，我们提出的 Ambient-Pix2PixGAN 可以成功地对噪声测量数据进行训练，以在目标成像模式中生成高质量的翻译图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.01186v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **AmbientCycleGAN for Establishing Interpretable Stochastic Object Models Based on Mathematical Phantoms and Medical Imaging Measurements**<br />
**Title_cn:** AmbientCycleGAN 用于基于数学模型和医学成像测量建立可解释的随机对象模型<br />
**Authors:** Xichen Xu, Wentao Chen, Weimin Zhou<br />
**Abstract:** <details><summary>原文: </summary>Medical imaging systems that are designed for producing diagnostically informative images should be objectively assessed via task-based measures of image quality (IQ). Ideally, computation of task-based measures of IQ needs to account for all sources of randomness in the measurement data, including the variability in the ensemble of objects to be imaged. To address this need, stochastic object models (SOMs) that can generate an ensemble of synthesized objects or phantoms can be employed. Various mathematical SOMs or phantoms were developed that can interpretably synthesize objects, such as lumpy object models and parameterized torso phantoms. However, such SOMs that are purely mathematically defined may not be able to comprehensively capture realistic object variations. To establish realistic SOMs, it is desirable to use experimental data. An augmented generative adversarial network (GAN), AmbientGAN, was recently proposed for establishing SOMs from medical imaging measurements. However, it remains unclear to which extent the AmbientGAN-produced objects can be interpretably controlled. This work introduces a novel approach called AmbientCycleGAN that translates mathematical SOMs to realistic SOMs by use of noisy measurement data. Numerical studies that consider clustered lumpy background (CLB) models and real mammograms are conducted. It is demonstrated that our proposed method can stably establish SOMs based on mathematical models and noisy measurement data. Moreover, the ability of the proposed AmbientCycleGAN to interpretably control image features in the synthesized objects is investigated.</details>
**Abstract_cn:** <details><summary>译文: </summary>设计用于生成诊断信息图像的医学成像系统应通过基于任务的图像质量 (IQ) 测量进行客观评估。理想情况下，基于任务的智商测量的计算需要考虑测量数据中的所有随机性来源，包括要成像的物体整体的可变性。为了满足这一需求，可以采用可以生成合成对象或幻影集合的随机对象模型 (SOM)。人们开发了各种数学 SOM 或模型，可以解释性地合成物体，例如块状物体模型和参数化躯干模型。然而，这种纯粹数学定义的 SOM 可能无法全面捕捉现实的物体变化。为了建立现实的 SOM，需要使用实验数据。最近提出了一种增强生成对抗网络 (GAN) AmbientGAN，用于根据医学成像测量建立 SOM。然而，目前尚不清楚 AmbientGAN 生成的物体在多大程度上可以被可解释地控制。这项工作引入了一种名为 AmbientCycleGAN 的新颖方法，该方法通过使用噪声测量数据将数学 SOM 转换为现实的 SOM。进行了考虑聚类块状背景 (CLB) 模型和真实乳房 X 光检查的数值研究。事实证明，我们提出的方法可以基于数学模型和噪声测量数据稳定地建立 SOM。此外，还研究了所提出的 AmbientCycleGAN 可解释地控制合成对象中的图像特征的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.01171v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Source-Free Unsupervised Domain Adaptation with Hypothesis Consolidation of Prediction Rationale**<br />
**Title_cn:** 具有预测基本原理假设巩固的无源无监督域适应<br />
**Authors:** Yangyang Shu, Xiaofeng Cao, Qi Chen, Bowen Zhang, Ziqin Zhou, Anton van den Hengel, Lingqiao Liu<br />
**Abstract:** <details><summary>原文: </summary>Source-Free Unsupervised Domain Adaptation (SFUDA) is a challenging task where a model needs to be adapted to a new domain without access to target domain labels or source domain data. The primary difficulty in this task is that the model's predictions may be inaccurate, and using these inaccurate predictions for model adaptation can lead to misleading results. To address this issue, this paper proposes a novel approach that considers multiple prediction hypotheses for each sample and investigates the rationale behind each hypothesis. By consolidating these hypothesis rationales, we identify the most likely correct hypotheses, which we then use as a pseudo-labeled set to support a semi-supervised learning procedure for model adaptation. To achieve the optimal performance, we propose a three-step adaptation process: model pre-adaptation, hypothesis consolidation, and semi-supervised learning. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance in the SFUDA task and can be easily integrated into existing approaches to improve their performance. The codes are available at \url{https://github.com/GANPerf/HCPR}.</details>
**Abstract_cn:** <details><summary>译文: </summary>无源无监督域适应（SFUDA）是一项具有挑战性的任务，其中模型需要适应新域，而无需访问目标域标签或源域数据。这项任务的主要困难是模型的预测可能不准确，并且使用这些不准确的预测进行模型自适应可能会导致误导性的结果。为了解决这个问题，本文提出了一种新颖的方法，该方法考虑每个样本的多个预测假设，并研究每个假设背后的基本原理。通过巩固这些假设的基本原理，我们确定了最有可能的正确假设，然后将其用作伪标记集来支持模型适应的半监督学习过程。为了实现最佳性能，我们提出了一个三步适应过程：模型预适应、假设巩固和半监督学习。大量的实验结果表明，我们的方法在 SFUDA 任务中实现了最先进的性能，并且可以轻松集成到现有方法中以提高其性能。这些代码可在 \url{https://github.com/GANPerf/HCPR} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.01157v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **A Single Simple Patch is All You Need for AI-generated Image Detection**<br />
**Title_cn:** 只需一个简单的补丁即可进行 AI 生成的图像检测<br />
**Authors:** Jiaxuan Chen, Jieteng Yao, Li Niu<br />
**Abstract:** <details><summary>原文: </summary>The recent development of generative models unleashes the potential of generating hyper-realistic fake images. To prevent the malicious usage of fake images, AI-generated image detection aims to distinguish fake images from real images. Nevertheless, existing methods usually suffer from poor generalizability across different generators. In this work, we propose an embarrassingly simple approach named SSP, i.e., feeding the noise pattern of a Single Simple Patch (SSP) to a binary classifier, which could achieve 14.6% relative improvement over the recent method on GenImage dataset. Our SSP method is very robust and generalizable, which could serve as a simple and competitive baseline for the future methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型的最新发展释放了生成超现实假图像的潜力。为了防止恶意使用假图像，人工智能生成的图像检测旨在区分假图像和真实图像。然而，现有方法通常在不同生成器之间的通用性较差。在这项工作中，我们提出了一种名为 SSP 的极其简单的方法，即将单个简单补丁 (SSP) 的噪声模式输入到二元分类器，与 GenImage 数据集上的最新方法相比，该方法可以实现 14.6% 的相对改进。我们的 SSP 方法非常稳健且具有通用性，可以作为未来方法的简单且有竞争力的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.01123v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Compositional Generative Modeling: A Single Model is Not All You Need**<br />
**Title_cn:** 组合生成建模：单一模型并不是您所需要的全部<br />
**Authors:** Yilun Du, Leslie Kaelbling<br />
**Abstract:** <details><summary>原文: </summary>Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于大量数据训练的大型整体生成模型已成为人工智能研究中日益占主导地位的方法。在本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何使我们能够以更有效的数据方式学习分布，从而能够泛化到训练时看不见的数据分布部分。我们进一步展示了这如何使我们能够为训练中完全看不见的任务编程和构建新的生成模型。最后，我们表明，在许多情况下，我们可以从数据中发现单独的组成成分。</details>
**PDF:** <http://arxiv.org/pdf/2402.01103v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models**<br />
**Title_cn:** Skip $\textbackslash n$：一种减少大视觉语言模型中幻觉的简单方法<br />
**Authors:** Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '$\textbackslash n\textbackslash n$'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '$\textbackslash n\textbackslash n$' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of `\textbackslash n'.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型视觉语言模型（LVLM）的最新进展展示了用人类语言理解视觉信息的令人印象深刻的能力。尽管取得了这些进步，LVLM 仍然面临多模态幻觉的挑战，例如生成视觉信息中不存在的对象的文本描述。然而，多模态幻觉的根本原因仍未得到很好的探索。在本文中，我们提出了一个新的观点，表明 LVLM 的固有偏差可能是幻觉的关键因素。具体来说，我们系统地识别了与段落分隔符（'$\textbackslash n\textbackslash n$'）相关的语义转移偏差，其中训练数据中'$\textbackslash n\textbackslash n$'之前和之后的内容经常表现出显着的语义变化。这种模式导致模型推断“$\textbackslash n\textbackslash n$”后面的内容应该与前面的幻觉描述较少的内容明显不同，从而增加了“$\textbackslash n\”后面出现幻觉描述的概率。文本反斜杠 n$'。我们已经在多个公开可用的 LVLM 上验证了这一假设。此外，我们发现在生成的描述中故意插入“$\textbackslash n\textbackslash n$”会引发更多幻觉。提出了一种简单的方法，通过跳过“\textbackslash n”的输出来有效减轻 LVLM 的幻觉。</details>
**PDF:** <http://arxiv.org/pdf/2402.01345v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A general framework for rotation invariant point cloud analysis**<br />
**Title_cn:** 旋转不变点云分析的通用框架<br />
**Authors:** Shuqing Luo, Wei Gao<br />
**Abstract:** <details><summary>原文: </summary>We propose a general method for deep learning based point cloud analysis, which is invariant to rotation on the inputs. Classical methods are vulnerable to rotation, as they usually take aligned point clouds as input. Principle Component Analysis (PCA) is a practical approach to achieve rotation invariance. However, there are still some gaps between theory and practical algorithms. In this work, we present a thorough study on designing rotation invariant algorithms for point cloud analysis. We first formulate it as a permutation invariant problem, then propose a general framework which can be combined with any backbones. Our method is beneficial for further research such as 3D pre-training and multi-modal learning. Experiments show that our method has considerable or better performance compared to state-of-the-art approaches on common benchmarks. Code is available at https://github.com/luoshuqing2001/RI_framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种基于深度学习的点云分析的通用方法，该方法对于输入的旋转具有不变性。经典方法容易受到旋转的影响，因为它们通常采用对齐的点云作为输入。主成分分析（PCA）是实现旋转不变性的实用方法。然而，理论和实际算法之间仍然存在一些差距。在这项工作中，我们对设计用于点云分析的旋转不变算法进行了深入的研究。我们首先将其表述为排列不变问题，然后提出一个可以与任何主干网络结合的通用框架。我们的方法有利于进一步的研究，例如 3D 预训练和多模态学习。实验表明，与常见基准测试中最先进的方法相比，我们的方法具有相当大或更好的性能。代码可在 https://github.com/luoshuqing2001/RI_framework 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.01331v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep Multimodal Fusion of Data with Heterogeneous Dimensionality via Projective Networks**<br />
**Title_cn:** 通过投影网络实现异构维度数据的深度多模态融合<br />
**Authors:** José Morano, Guilherme Aresta, Christoph Grechenig, Ursula Schmidt-Erfurth, Hrvoje Bogunović<br />
**Abstract:** <details><summary>原文: </summary>The use of multimodal imaging has led to significant improvements in the diagnosis and treatment of many diseases. Similar to clinical practice, some works have demonstrated the benefits of multimodal fusion for automatic segmentation and classification using deep learning-based methods. However, current segmentation methods are limited to fusion of modalities with the same dimensionality (e.g., 3D+3D, 2D+2D), which is not always possible, and the fusion strategies implemented by classification methods are incompatible with localization tasks. In this work, we propose a novel deep learning-based framework for the fusion of multimodal data with heterogeneous dimensionality (e.g., 3D+2D) that is compatible with localization tasks. The proposed framework extracts the features of the different modalities and projects them into the common feature subspace. The projected features are then fused and further processed to obtain the final prediction. The framework was validated on the following tasks: segmentation of geographic atrophy (GA), a late-stage manifestation of age-related macular degeneration, and segmentation of retinal blood vessels (RBV) in multimodal retinal imaging. Our results show that the proposed method outperforms the state-of-the-art monomodal methods on GA and RBV segmentation by up to 3.10% and 4.64% Dice, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态成像的使用显着改善了许多疾病的诊断和治疗。与临床实践类似，一些工作已经证明了多模态融合对于使用基于深度学习的方法进行自动分割和分类的好处。然而，当前的分割方法仅限于具有相同维度的模态融合（例如，3D+3D、2D+2D），这并不总是可能的，并且分类方法实现的融合策略与定位任务不兼容。在这项工作中，我们提出了一种新颖的基于深度学习的框架，用于融合与定位任务兼容的异构维度（例如 3D+2D）的多模态数据。所提出的框架提取不同模态的特征并将它们投影到公共特征子空间中。然后将投影的特征进行融合并进一步处理以获得最终的预测。该框架在以下任务上进行了验证：地理萎缩（GA）（年龄相关性黄斑变性的晚期表现）的分割，以及多模态视网膜成像中视网膜血管（RBV）的分割。我们的结果表明，所提出的方法在 GA 和 RBV 分割方面比最先进的单峰方法分别高出 3.10% 和 4.64% Dice。</details>
**PDF:** <http://arxiv.org/pdf/2402.01311v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **TSJNet: A Multi-modality Target and Semantic Awareness Joint-driven Image Fusion Network**<br />
**Title_cn:** TSJNet：多模态目标和语义感知联合驱动的图像融合网络<br />
**Authors:** Yuchan Jie, Yushen Xu, Xiaosong Li, Haishu Tan<br />
**Abstract:** <details><summary>原文: </summary>Multi-modality image fusion involves integrating complementary information from different modalities into a single image. Current methods primarily focus on enhancing image fusion with a single advanced task such as incorporating semantic or object-related information into the fusion process. This method creates challenges in achieving multiple objectives simultaneously. We introduce a target and semantic awareness joint-driven fusion network called TSJNet. TSJNet comprises fusion, detection, and segmentation subnetworks arranged in a series structure. It leverages object and semantically relevant information derived from dual high-level tasks to guide the fusion network. Additionally, We propose a local significant feature extraction module with a double parallel branch structure to fully capture the fine-grained features of cross-modal images and foster interaction among modalities, targets, and segmentation information. We conducted extensive experiments on four publicly available datasets (MSRS, M3FD, RoadScene, and LLVIP). The results demonstrate that TSJNet can generate visually pleasing fused results, achieving an average increase of 2.84% and 7.47% in object detection and segmentation mAP @0.5 and mIoU, respectively, compared to the state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态图像融合涉及将来自不同模态的互补信息集成到单个图像中。当前的方法主要侧重于通过单个高级任务来增强图像融合，例如将语义或对象相关信息合并到融合过程中。这种方法给同时实现多个目标带来了挑战。我们引入了一种目标和语义感知联合驱动的融合网络，称为 TSJNet。 TSJNet 包括以串联结构排列的融合、检测和分割子网络。它利用源自双重高级任务的对象和语义相关信息来指导融合网络。此外，我们提出了一种具有双并行分支结构的局部显着特征提取模块，以充分捕获跨模态图像的细粒度特征，并促进模态、目标和分割信息之间的交互。我们对四个公开可用的数据集（MSRS、M3FD、RoadScene 和 LLVIP）进行了广泛的实验。结果表明，TSJNet 可以生成视觉上令人愉悦的融合结果，与最先进的方法相比，目标检测和分割 mAP @0.5 和 mIoU 分别平均提高 2.84% 和 7.47%。</details>
**PDF:** <http://arxiv.org/pdf/2402.01212v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **2AFC Prompting of Large Multimodal Models for Image Quality Assessment**<br />
**Title_cn:** 2AFC提示大型多模态模型用于图像质量评估<br />
**Authors:** Hanwei Zhu, Xiangjie Sui, Baoliang Chen, Xuelin Liu, Peilin Chen, Yuming Fang, Shiqi Wang<br />
**Abstract:** <details><summary>原文: </summary>While abundant research has been conducted on improving high-level visual understanding and reasoning capabilities of large multimodal models~(LMMs), their visual quality assessment~(IQA) ability has been relatively under-explored. Here we take initial steps towards this goal by employing the two-alternative forced choice~(2AFC) prompting, as 2AFC is widely regarded as the most reliable way of collecting human opinions of visual quality. Subsequently, the global quality score of each image estimated by a particular LMM can be efficiently aggregated using the maximum a posterior estimation. Meanwhile, we introduce three evaluation criteria: consistency, accuracy, and correlation, to provide comprehensive quantifications and deeper insights into the IQA capability of five LMMs. Extensive experiments show that existing LMMs exhibit remarkable IQA ability on coarse-grained quality comparison, but there is room for improvement on fine-grained quality discrimination. The proposed dataset sheds light on the future development of IQA models based on LMMs. The codes will be made publicly available at https://github.com/h4nwei/2AFC-LMMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然在提高大型多模态模型（LMM）的高级视觉理解和推理能力方面已经进行了大量研究，但其视觉质量评估（IQA）能力的探索相对不足。在这里，我们通过采用两种选择强制选择~（2AFC）提示来实现这一目标，因为 2AFC 被广泛认为是收集人类视觉质量意见的最可靠方式。随后，可以使用最大后验估计来有效地聚合由特定 LMM 估计的每个图像的全局质量得分。同时，我们引入了三个评估标准：一致性、准确性和相关性，以提供对五个 LMM 的 IQA 能力的全面量化和更深入的了解。大量实验表明，现有的 LMM 在粗粒度质量比较方面表现出显着的 IQA 能力，但在细粒度质量辨别方面还有改进的空间。所提出的数据集揭示了基于 LMM 的 IQA 模型的未来发展。这些代码将在 https://github.com/h4nwei/2AFC-LMMs 上公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.01162v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Survey for Foundation Models in Autonomous Driving**<br />
**Title_cn:** 自动驾驶基础模型调查<br />
**Authors:** Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen<br />
**Abstract:** <details><summary>原文: </summary>The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.</details>
**Abstract_cn:** <details><summary>译文: </summary>基础模型的出现彻底改变了自然语言处理和计算机视觉领域，为其在自动驾驶（AD）中的应用铺平了道路。这项调查对 40 多篇研究论文进行了全面回顾，展示了基础模型在增强 AD 方面的作用。大型语言模型有助于 AD 中的规划和模拟，特别是通过它们在推理、代码生成和翻译方面的熟练程度。与此同时，视觉基础模型越来越多地适应关键任务，例如 3D 对象检测和跟踪，以及创建用于模拟和测试的真实驾驶场景。多模态基础模型集成了不同的输入，表现出卓越的视觉理解和空间推理，这对于端到端 AD 至关重要。这项调查不仅提供了结构化的分类法，根据 AD 领域内的模式和功能对基础模型进行分类，而且还深入研究了当前研究中采用的方法。它确定了现有基础模型和尖端 AD 方法之间的差距，从而规划了未来的研究方向并提出了弥合这些差距的路线图。</details>
**PDF:** <http://arxiv.org/pdf/2402.01105v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation**<br />
**Title_cn:** HyperPlanes：快速适应 NeRF 的超网络方法<br />
**Authors:** Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek<br />
**Abstract:** <details><summary>原文: </summary>Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 是一种广泛接受的标准，用于从少量基础图像合成新的 3D 对象视图。然而，NeRF 的泛化属性有限，这意味着我们需要使用大量的计算资源来为我们想要表示的每个项目训练单独的架构。为了解决这个问题，我们提出了一种基于超网络范式的几次学习方法，在推理过程中不需要梯度优化。超网络从训练数据中收集信息并生成通用权重的更新。因此，我们开发了一种有效的方法，只需一步即可从少量图像生成高质量的 3D 对象表示。通过与最先进的解决方案的直接比较和全面的消融研究证实了这一点。</details>
**PDF:** <http://arxiv.org/pdf/2402.01524v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting**<br />
**Title_cn:** GaMeS：基于网格的高斯分布调整和修改<br />
**Authors:** Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek<br />
**Abstract:** <details><summary>原文: </summary>In recent years, a range of neural network-based methods for image rendering have been introduced. For instance, widely-researched neural radiance fields (NeRF) rely on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-theart technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that pin all Gaussians splats on the object surface (mesh). The unique contribution of our methods is defining Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain high-quality renders in the real-time generation of high-quality views. Furthermore, we demonstrate that in the absence of a predefined mesh, it is possible to fine-tune the initial mesh during the learning process.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，引入了一系列基于神经网络的图像渲染方法。例如，广泛研究的神经辐射场 (NeRF) 依靠神经网络来表示 3D 场景，从而可以从少量 2D 图像合成真实的视图。然而，大多数 NeRF 模型都受到较长训练和推理时间的限制。相比之下，高斯泼溅 (GS) 是一种新颖、最先进的技术，用于渲染 3D 场景中的点，通过高斯分布来近似点对图像像素的贡献，从而保证快速训练和快速实时渲染。 GS 的一个缺点是缺乏明确的调节方法，因为需要调节数十万个高斯分量。为了解决这个问题，我们引入了高斯网格分布（GaMeS）模型，它是网格和高斯分布的混合体，它将所有高斯分布固定在对象表面（网格）上。我们方法的独特贡献是仅根据网格上的位置来定义高斯图，从而允许在动画过程中自动调整位置、比例和旋转。因此，我们在实时生成高质量视图时获得了高质量渲染。此外，我们证明，在没有预定义网格的情况下，可以在学习过程中微调初始网格。</details>
**PDF:** <http://arxiv.org/pdf/2402.01459v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate Distortion Optimization**<br />
**Title_cn:** 基于速率失真优化的高效动态 NeRF 体积视频编码<br />
**Authors:** Zhiyu Zhang, Guo Lu, Huanxiong Liang, Anni Tang, Qiang Hu, Li Song<br />
**Abstract:** <details><summary>原文: </summary>Volumetric videos, benefiting from immersive 3D realism and interactivity, hold vast potential for various applications, while the tremendous data volume poses significant challenges for compression. Recently, NeRF has demonstrated remarkable potential in volumetric video compression thanks to its simple representation and powerful 3D modeling capabilities, where a notable work is ReRF. However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency. In contrast, in this paper, we propose a volumetric video compression method based on dynamic NeRF in a more compact manner. Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling. Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency. Extensive experiments demonstrate that our method achieves higher compression efficiency compared to ReRF on various datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>体积视频受益于沉浸式 3D 真实感和交互性，在各种应用中拥有巨大的潜力，而巨大的数据量对压缩提出了巨大的挑战。最近，NeRF凭借其简单的表示和强大的3D建模功能在体积视频压缩方面展现出了巨大的潜力，其中一个值得注意的工作是ReRF。然而，ReRF 将建模与压缩过程分开，导致压缩效率不理想。相比之下，在本文中，我们以更紧凑的方式提出了一种基于动态 NeRF 的体积视频压缩方法。具体来说，我们将 NeRF 表示分解为系数域和基域，在时域中增量更新基域以实现动态建模。此外，我们对建模和压缩过程进行端到端联合优化，进一步提高压缩效率。大量实验表明，与 ReRF 相比，我们的方法在各种数据集上实现了更高的压缩效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.01380v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect Diffusion Guidance**<br />
**Title_cn:** 通过间接扩散指导克服稀疏视图可推广 NeRF 中的不确定性<br />
**Authors:** Yaokun Li, Chao Gou, Guang Tan<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing novel views. However, their reliance on dense inputs and scene-specific optimization has limited their broader applicability. Generalizable NeRFs (Gen-NeRF), while intended to address this, often produce blurring artifacts in unobserved regions with sparse inputs, which are full of uncertainty. In this paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings. We assume that NeRF's inability to effectively mitigate this uncertainty stems from its inherent lack of generative capacity. Therefore, we innovatively propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address this uncertainty from a generative perspective by leveraging a distilled diffusion prior as guidance. Specifically, to avoid model confusion caused by directly regularizing with inconsistent samplings as in previous methods, our approach introduces a strategy to indirectly inject the inherently missing imagination into the learned implicit function through a diffusion-guided latent space. Empirical evaluation across various benchmarks demonstrates the superior performance of our approach in handling uncertainty with sparse inputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 已证明在合成新颖视图方面的有效性。然而，它们对密集输入和特定场景优化的依赖限制了它们更广泛的适用性。可泛化 NeRF（Gen-NeRF）虽然旨在解决这个问题，但通常会在输入稀疏的未观察区域中产生模糊伪影，这充满了不确定性。在本文中，我们的目标是减少 Gen-NeRF 中的不确定性以获得合理的渲染。我们假设 NeRF 无法有效缓解这种不确定性源于其固有的生成能力的缺乏。因此，我们创新性地提出了一种间接扩散引导的 NeRF 框架，称为 ID-NeRF，通过利用蒸馏扩散先验作为指导，从生成角度解决这种不确定性。具体来说，为了避免像以前的方法那样直接对不一致的采样进行正则化而导致模型混乱，我们的方法引入了一种策略，通过扩散引导的潜在空间将固有缺失的想象力间接注入到学习的隐函数中。跨各种基准的实证评估证明了我们的方法在处理稀疏输入的不确定性方面具有卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.01217v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **AutoGCN -- Towards Generic Human Activity Recognition with Neural Architecture Search**<br />
**Title_cn:** AutoGCN——通过神经架构搜索实现通用人类活动识别<br />
**Authors:** Felix Tempel, Inga Strümke, Espen Alexander F. Ihlen<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces AutoGCN, a generic Neural Architecture Search (NAS) algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks (GCNs). HAR has gained attention due to advances in deep learning, increased data availability, and enhanced computational capabilities. At the same time, GCNs have shown promising results in modeling relationships between body key points in a skeletal graph. While domain experts often craft dataset-specific GCN-based methods, their applicability beyond this specific context is severely limited. AutoGCN seeks to address this limitation by simultaneously searching for the ideal hyperparameters and architecture combination within a versatile search space using a reinforcement controller while balancing optimal exploration and exploitation behavior with a knowledge reservoir during the search process. We conduct extensive experiments on two large-scale datasets focused on skeleton-based action recognition to assess the proposed algorithm's performance. Our experimental results underscore the effectiveness of AutoGCN in constructing optimal GCN architectures for HAR, outperforming conventional NAS and GCN methods, as well as random search. These findings highlight the significance of a diverse search space and an expressive input representation to enhance the network performance and generalizability.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 AutoGCN，这是一种使用图卷积网络 (GCN) 进行人类活动识别 (HAR) 的通用神经架构搜索 (NAS) 算法。由于深度学习的进步、数据可用性的增加和计算能力的增强，HAR 受到了关注。与此同时，GCN 在骨骼图中身体关键点之间的关系建模方面显示出了有希望的结果。虽然领域专家经常设计特定于数据集的基于 GCN 的方法，但它们在特定上下文之外的适用性受到严重限制。 AutoGCN 试图通过使用强化控制器在通用搜索空间内同时搜索理想的超参数和架构组合来解决这一限制，同时在搜索过程中平衡最佳探索和利用行为与知识库。我们在两个专注于基于骨架的动作识别的大型数据集上进行了广泛的实验，以评估所提出的算法的性能。我们的实验结果强调了 AutoGCN 在为 HAR 构建最佳 GCN 架构方面的有效性，优于传统 NAS 和 GCN 方法以及随机搜索。这些发现强调了多样化搜索空间和富有表现力的输入表示对于增强网络性能和通用性的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2402.01313v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted Inference**<br />
**Title_cn:** Bi-CryptoNets：利用不同级别的隐私进行加密推理<br />
**Authors:** Man-Jie Yuan, Zheng Zou, Wei Gao<br />
**Abstract:** <details><summary>原文: </summary>Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view. This work takes a different view from the input data and structure of neural networks. We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy. The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations. We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections. We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a well-trained teacher neural network. Empirical studies show the effectiveness and decrease of inference latency for our bi-CryptoNets.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，隐私保护神经网络受到越来越多的关注，并且已经开发了各种算法来从密码学的角度保持准确性、计算复杂性和信息安全之间的平衡。这项工作对神经网络的输入数据和结构采取了不同的观点。我们根据重要性和隐私将输入数据（例如，一些图像）分解为敏感和不敏感的部分。敏感部分包含一些重要的隐私信息，例如人脸，我们采用强同态加密来保证安全，而不敏感部分包含一些背景信息，我们添加扰动。我们提出双加密网络，即明文和密文分支，分别处理两个片段，密文分支可以通过单向连接利用来自明文分支的信息。我们通过从训练有素的教师神经网络转移表示来为我们的双加密网络采用知识蒸馏。实证研究表明我们的双加密网络推理延迟的有效性和减少。</details>
**PDF:** <http://arxiv.org/pdf/2402.01296v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection**<br />
**Title_cn:** Spiking CenterNet：用于物体检测的蒸馏增强尖峰神经网络<br />
**Authors:** Lennard Bodden, Franziska Schwaiger, Duc Bach Ha, Lars Kreuzberg, Sven Behnke<br />
**Abstract:** <details><summary>原文: </summary>In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>在边缘人工智能、自动驾驶汽车和气候变化的时代，对节能、小型、嵌入式人工智能的需求不断增长。尖峰神经网络 (SNN) 凭借其事件驱动的信息流和稀疏激活，是解决这一挑战的一种有前途的方法。我们提出了 Spiking CenterNet 用于事件数据的对象检测。它将 SNN CenterNet 适配与高效的基于 M2U-Net 的解码器相结合。我们的模型显着优于 Prophesee 具有挑战性的 GEN1 汽车检测数据集之前的同类工作，同时使用的能量还不到一半。将非尖峰教师的知识提炼到我们的 SNN 中可以进一步提高性能。据我们所知，我们的工作是第一个利用尖峰目标检测领域知识蒸馏的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.01287v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Cascaded Scaling Classifier: class incremental learning with probability scaling**<br />
**Title_cn:** Cascaded Scaling Classifier：具有概率缩放的类增量学习<br />
**Authors:** Jary Pomponi, Alessio Devoto, Simone Scardapane<br />
**Abstract:** <details><summary>原文: </summary>Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved by modifying the output of the model with auxiliary scaling functions. We empirically show that our approach performs well on multiple benchmarks against well-established baselines, and we also study each component of our proposal and how the combinations of such components affect the final results.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类有能力获取新知识并将学到的知识转移到不同的领域，从而产生轻微的遗忘。同样的能力，称为持续学习，在使用神经网络时很难实现，因为在学习新任务时，遗忘会影响过去学到的任务。这种遗忘可以通过重放过去任务中存储的样本来减轻，但是长序列的任务可能需要大的内存；此外，这可能会导致保存的样本过度拟合。在本文中，我们提出了一种新颖的正则化方法和一种新颖的增量分类器，分别称为“边际阻尼”和“级联缩放分类器”。第一种结合了软约束和知识蒸馏方法，以保留过去学到的知识，同时允许模型有效地学习新模式。后者是一个门控增量分类器，帮助模型修改过去的预测而不直接干扰它们。这是通过使用辅助缩放函数修改模型的输出来实现的。我们的经验表明，我们的方法在多个基准测试中与既定基线相比表现良好，我们还研究了我们提案的每个组成部分以及这些组成部分的组合如何影响最终结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.01262v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Faster Inference of Integer SWIN Transformer by Removing the GELU Activation**<br />
**Title_cn:** 通过删除 GELU 激活来加快整数 SWIN Transformer 的推理<br />
**Authors:** Mohammadreza Tayaranian, Seyyed Hasan Mozafari, James J. Clark, Brett Meyer, Warren Gross<br />
**Abstract:** <details><summary>原文: </summary>SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer. While previous work proposed to replace the non-integer operations with linear approximation functions, we propose to replace GELU with ReLU activation. The advantage of ReLU over previous methods is its low memory and computation complexity. We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN transformer and show that on an RTX 4090 NVIDIA GPU we can improve the inference latency of the quantized SWIN transformer by at least $11\%$ while maintaining an accuracy drop of under $0.5\%$ on the ImageNet evaluation dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>SWIN Transformer 是一种著名的视觉 Transformer 模型，在图像分类任务中具有最先进的准确性。尽管取得了如此成功，但与类似的深度神经网络相比，其独特的架构导致推理速度较慢。模型的整数量化是用于改善其推理延迟的方法之一。然而，最先进的技术还无法完全量化该模型。在这项工作中，我们通过删除与 Swin Transformer 中的 GELU 激活相关的浮点运算来改进最先进方法的推理延迟。虽然之前的工作建议用线性逼近函数代替非整数运算，但我们建议用 ReLU 激活代替 GELU。 ReLU 相对于之前方法的优势在于其内存和计算复杂度较低。我们使用迭代知识蒸馏来补偿由于用 ReLU 替换 GELU 造成的准确性损失。我们量化了 GELU-less SWIN 变压器，并表明在 RTX 4090 NVIDIA GPU 上，我们可以将量化 SWIN 变压器的推理延迟提高至少 11\%$，同时在 ImageNet 评估中保持精度下降低于 0.5\%$数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.01169v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Deep Continuous Networks**<br />
**Title_cn:** 深度连续网络<br />
**Authors:** Nergis Tomen, Silvia L. Pintea, Jan C. van Gemert<br />
**Abstract:** <details><summary>原文: </summary>CNNs and computational models of biological vision share some fundamental principles, which opened new avenues of research. However, fruitful cross-field research is hampered by conventional CNN architectures being based on spatially and depthwise discrete representations, which cannot accommodate certain aspects of biological complexity such as continuously varying receptive field sizes and dynamics of neuronal responses. Here we propose deep continuous networks (DCNs), which combine spatially continuous filters, with the continuous depth framework of neural ODEs. This allows us to learn the spatial support of the filters during training, as well as model the continuous evolution of feature maps, linking DCNs closely to biological models. We show that DCNs are versatile and highly applicable to standard image classification and reconstruction problems, where they improve parameter and data efficiency, and allow for meta-parametrization. We illustrate the biological plausibility of the scale distributions learned by DCNs and explore their performance in a neuroscientifically inspired pattern completion task. Finally, we investigate an efficient implementation of DCNs by changing input contrast.</details>
**Abstract_cn:** <details><summary>译文: </summary>CNN 和生物视觉计算模型共享一些基本原理，这开辟了新的研究途径。然而，基于空间和深度离散表示的传统 CNN 架构阻碍了富有成效的跨领域研究，这些架构无法适应生物复杂性的某些方面，例如不断变化的感受野大小和神经元反应的动态。在这里，我们提出深度连续网络（DCN），它将空间连续滤波器与神经常微分方程的连续深度框架相结合。这使我们能够在训练过程中学习滤波器的空间支持，并对特征图的连续演化进行建模，将 DCN 与生物模型紧密联系起来。我们证明 DCN 是通用的，并且高度适用于标准图像分类和重建问题，它们提高了参数和数据效率，并允许元参数化。我们说明了 DCN 学习到的尺度分布的生物学合理性，并探讨了它们在受神经科学启发的模式完成任务中的表现。最后，我们通过改变输入对比度来研究 DCN 的有效实现。</details>
**PDF:** <http://arxiv.org/pdf/2402.01557v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data**<br />
**Title_cn:** 缩小人类行为分析的差距：合成三峰数据的管道<br />
**Authors:** Christian Stippel, Thomas Heitzinger, Rafael Sterzinger, Martin Kampel<br />
**Abstract:** <details><summary>原文: </summary>In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data utilizing conditional image-to-image translation. By employing this approach, we generate trimodal data that can be leveraged to train models for settings with limited data, bad lightning conditions, or privacy-sensitive areas.</details>
**Abstract_cn:** <details><summary>译文: </summary>在普遍的机器学习中，特别是在人类行为分析 (HBA) 中，RGB 由于其信息的可访问性和丰富性而成为主要模式。然而，与它的好处相关的挑战，包括对照明条件的敏感性和隐私问题。克服这些漏洞的一种可能性是采取不同的方式。例如，热感特别擅长强调人体形态，而深度则增加了关键的背景层次。尽管它们具有已知的优点，但只有少数集成这些模式的 HBA 特定数据集存在。为了解决这一不足，我们的研究引入了一种新颖的生成技术，用于创建三模态（即 RGB、热和深度）、以人为本的数据集。该技术利用源自 RGB 图像的人体分割掩模，并结合自动获取的热背景和深度背景。有了这两种成分，我们利用条件图像到图像的转换，从现有的 RGB 数据中合成深度和热对应物。通过采用这种方法，我们生成三模态数据，可用于针对数据有限、闪电条件恶劣或隐私敏感区域的设置训练模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.01537v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Convolution kernel adaptation to calibrated fisheye**<br />
**Title_cn:** 卷积核适应校准鱼眼<br />
**Authors:** Bruno Berenguel-Baeta, Maria Santos-Villafranca, Jesus Bermudez-Cameo, Alejandro Perez-Yus, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>Convolution kernels are the basic structural component of convolutional neural networks (CNNs). In the last years there has been a growing interest in fisheye cameras for many applications. However, the radially symmetric projection model of these cameras produces high distortions that affect the performance of CNNs, especially when the field of view is very large. In this work, we tackle this problem by proposing a method that leverages the calibration of cameras to deform the convolution kernel accordingly and adapt to the distortion. That way, the receptive field of the convolution is similar to standard convolutions in perspective images, allowing us to take advantage of pre-trained networks in large perspective datasets. We show how, with just a brief fine-tuning stage in a small dataset, we improve the performance of the network for the calibrated fisheye with respect to standard convolutions in depth estimation and semantic segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积核是卷积神经网络（CNN）的基本结构组件。在过去的几年里，人们对鱼眼相机的许多应用越来越感兴趣。然而，这些相机的径向对称投影模型会产生高失真，影响 CNN 的性能，尤其是当视场非常大时。在这项工作中，我们通过提出一种利用相机校准来相应地使卷积核变形并适应失真的方法来解决这个问题。这样，卷积的感受野类似于透视图像中的标准卷积，使我们能够利用大型透视数据集中的预训练网络。我们展示了如何通过在小数据集中进行简短的微调阶段，相对于深度估计和语义分割中的标准卷积来提高校准鱼眼网络的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.01456v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision**<br />
**Title_cn:** XAI 通过原型和非专家监督进行皮肤癌检测<br />
**Authors:** Miguel Correia, Alceu Bissoto, Carlos Santiago, Catarina Barata<br />
**Abstract:** <details><summary>原文: </summary>Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过皮肤镜图像分析检测皮肤癌是一项关键任务。然而，用于此目的的现有模型通常缺乏可解释性和可靠性，由于其黑匣子性质而引起了医生的担忧。在本文中，我们提出了一种使用可解释的原型部分模型来诊断黑色素瘤的新方法。我们通过结合以下内容引入基于非专家反馈的引导监督：1）使用分割网络自动获得的二进制掩码； 2) 用户改进的原型。这两种不同的信息路径旨在确保学习到的原型与皮肤病变内的相关区域相对应，排除其边界之外的混杂因素。实验结果表明，即使没有专家监督，与不可解释的模型相比，我们的方法也能实现卓越的性能和泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.01410v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data**<br />
**Title_cn:** ALERT-Transformer：桥接异步和同步机器学习，以实现基于事件的实时时空数据<br />
**Authors:** Carmen Martin-Turrero, Maxence Bouvier, Manuel Breitenstein, Pietro Zanuttigh, Vincent Parret<br />
**Abstract:** <details><summary>原文: </summary>We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling rate.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们寻求利用密集的机器学习模型对基于事件的传感器生成的连续超稀疏时空数据进行经典处理。我们提出了一种由异步传感和同步处理组成的新型混合管道，它结合了多种想法：（1）基于 PointNet 模型的嵌入——警报模块——由于泄漏机制，它可以不断集成新事件并消除旧事件，（ 2) 嵌入数据的灵活读出，允许以任何采样率向任何下游模型提供始终最新的特征，(3) 在受 Vision Transformer 启发的基于补丁的方法中利用输入稀疏性来优化效率该方法的。然后，这些嵌入由经过对象和手势识别训练的变压器模型进行处理。使用这种方法，我们实现了最先进的性能，并且延迟低于竞争对手。我们还证明我们的异步模型可以以任何所需的采样率运行。</details>
**PDF:** <http://arxiv.org/pdf/2402.01393v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **FindingEmo: An Image Dataset for Emotion Recognition in the Wild**<br />
**Title_cn:** FindEmo：用于野外情绪识别的图像数据集<br />
**Authors:** Laurent Mertens, Elahe' Yargholi, Hans Op de Beeck, Jan Van den Stock, Joost Vennekens<br />
**Abstract:** <details><summary>原文: </summary>We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出FindingEmo，这是一个新的图像数据集，包含 25k 图像的注释，专门针对情绪识别而定制。与现有数据集相反，它专注于描绘各种自然、社会环境中多人的复杂场景，图像被作为一个整体进行注释，从而超越了传统上对面部或单个个体的关注。带注释的维度包括效价、唤醒度和情感标签，并使用 Prolific 收集注释。我们连同注释一起发布了指向原始图像的 URL 列表以及所有相关的源代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.01355v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Phrase Grounding-based Style Transfer for Single-Domain Generalized Object Detection**<br />
**Title_cn:** 用于单域广义目标检测的基于短语基础的风格迁移<br />
**Authors:** Hao Li, Wei Wang, Cong Wang, Zhigang Luo, Xinwang Liu, Kenli Li, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Single-domain generalized object detection aims to enhance a model's generalizability to multiple unseen target domains using only data from a single source domain during training. This is a practical yet challenging task as it requires the model to address domain shift without incorporating target domain data into training. In this paper, we propose a novel phrase grounding-based style transfer (PGST) approach for the task. Specifically, we first define textual prompts to describe potential objects for each unseen target domain. Then, we leverage the grounded language-image pre-training (GLIP) model to learn the style of these target domains and achieve style transfer from the source to the target domain. The style-transferred source visual features are semantically rich and could be close to imaginary counterparts in the target domain. Finally, we employ these style-transferred visual features to fine-tune GLIP. By introducing imaginary counterparts, the detector could be effectively generalized to unseen target domains using only a single source domain for training. Extensive experimental results on five diverse weather driving benchmarks demonstrate our proposed approach achieves state-of-the-art performance, even surpassing some domain adaptive methods that incorporate target domain images into the training process.The source codes and pre-trained models will be made available.</details>
**Abstract_cn:** <details><summary>译文: </summary>单域广义对象检测旨在在训练期间仅使用来自单个源域的数据来增强模型对多个未见过的目标域的泛化能力。这是一项实用但具有挑战性的任务，因为它要求模型在不将目标域数据纳入训练的情况下解决域转移问题。在本文中，我们针对该任务提出了一种新颖的基于短语基础的风格迁移（PGST）方法。具体来说，我们首先定义文本提示来描述每个看不见的目标域的潜在对象。然后，我们利用扎根语言图像预训练（GLIP）模型来学习这些目标域的风格，并实现从源到目标域的风格迁移。风格转移的源视觉特征在语义上丰富，并且可能接近目标域中的假想对应物。最后，我们利用这些风格转移的视觉特征来微调 GLIP。通过引入假想的对应物，检测器可以仅使用单个源域进行训练，有效地推广到看不见的目标域。对五种不同天气驾驶基准的广泛实验结果表明，我们提出的方法实现了最先进的性能，甚至超越了一些将目标域图像纳入训练过程的域自适应方法。源代码和预训练模型将被制作可用的。</details>
**PDF:** <http://arxiv.org/pdf/2402.01304v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **AGILE: Approach-based Grasp Inference Learned from Element Decomposition**<br />
**Title_cn:** AGILE：从元素分解中学习的基于方法的抓取推理<br />
**Authors:** MohammadHossein Koosheshi, Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor, Mohammad Reza Hairi Yazdi<br />
**Abstract:** <details><summary>原文: </summary>Humans, this species expert in grasp detection, can grasp objects by taking into account hand-object positioning information. This work proposes a method to enable a robot manipulator to learn the same, grasping objects in the most optimal way according to how the gripper has approached the object. Built on deep learning, the proposed method consists of two main stages. In order to generalize the network on unseen objects, the proposed Approach-based Grasping Inference involves an element decomposition stage to split an object into its main parts, each with one or more annotated grasps for a particular approach of the gripper. Subsequently, a grasp detection network utilizes the decomposed elements by Mask R-CNN and the information on the approach of the gripper in order to detect the element the gripper has approached and the most optimal grasp. In order to train the networks, the study introduces a robotic grasping dataset collected in the Coppeliasim simulation environment. The dataset involves 10 different objects with annotated element decomposition masks and grasp rectangles. The proposed method acquires a 90% grasp success rate on seen objects and 78% on unseen objects in the Coppeliasim simulation environment. Lastly, simulation-to-reality domain adaptation is performed by applying transformations on the training set collected in simulation and augmenting the dataset, which results in a 70% physical grasp success performance using a Delta parallel robot and a 2 -fingered gripper.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类是抓握检测方面的专家，可以通过考虑手部物体定位信息来抓握物体。这项工作提出了一种方法，使机器人操纵器能够学习相同的内容，根据抓取器接近物体的方式以最佳方式抓取物体。该方法基于深度学习，由两个主要阶段组成。为了将网络推广到看不见的物体上，所提出的基于方法的抓取推理涉及到一个元素分解阶段，将一个物体分成几个主要部分，每个部分都有一个或多个针对抓取器的特定方法的带注释的抓取。随后，抓取检测网络利用 Mask R-CNN 分解的元素和抓取器接近的信息来检测抓取器已接近的元素和最佳抓取。为了训练网络，该研究引入了在 Coppeliasim 模拟环境中收集的机器人抓取数据集。该数据集涉及 10 个不同的对象，带有带注释的元素分解掩模和抓取矩形。该方法在 Coppeliasim 模拟环境中对可见物体的抓取成功率达到 90%，对不可见物体的抓取成功率达到 78%。最后，通过对模拟中收集的训练集应用转换并增强数据集来执行模拟到现实领域的适应，从而使用 Delta 并联机器人和 2 指夹具实现 70% 的物理抓取成功率。</details>
**PDF:** <http://arxiv.org/pdf/2402.01303v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Delving into Decision-based Black-box Attacks on Semantic Segmentation**<br />
**Title_cn:** 深入研究语义分割的基于决策的黑盒攻击<br />
**Authors:** Zhaoyu Chen, Zhengyang Shan, Jingwen Chang, Kaixun Jiang, Dingkang Yang, Yiting Cheng, Wenqiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation is a fundamental visual task that finds extensive deployment in applications with security-sensitive considerations. Nonetheless, recent work illustrates the adversarial vulnerability of semantic segmentation models to white-box attacks. However, its adversarial robustness against black-box attacks has not been fully explored. In this paper, we present the first exploration of black-box decision-based attacks on semantic segmentation. First, we analyze the challenges that semantic segmentation brings to decision-based attacks through the case study. Then, to address these challenges, we first propose a decision-based attack on semantic segmentation, called Discrete Linear Attack (DLA). Based on random search and proxy index, we utilize the discrete linear noises for perturbation exploration and calibration to achieve efficient attack efficiency. We conduct adversarial robustness evaluation on 5 models from Cityscapes and ADE20K under 8 attacks. DLA shows its formidable power on Cityscapes by dramatically reducing PSPNet's mIoU from an impressive 77.83% to a mere 2.14% with just 50 queries.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割是一项基本的视觉任务，在具有安全敏感考虑的应用程序中得到广泛部署。尽管如此，最近的工作说明了语义分割模型对白盒攻击的对抗性脆弱性。然而，其针对黑盒攻击的对抗鲁棒性尚未得到充分探索。在本文中，我们首次探索了基于黑盒决策的语义分割攻击。首先，我们通过案例分析语义分割给基于决策的攻击带来的挑战。然后，为了应对这些挑战，我们首先提出了一种基于决策的语义分割攻击，称为离散线性攻击（DLA）。基于随机搜索和代理索引，我们利用离散线性噪声进行扰动探索和校准，以实现高效的攻击效率。我们对来自 Cityscapes 和 ADE20K 的 5 个模型在 8 次攻击下进行了对抗性鲁棒性评估。 DLA 通过仅 50 个查询将 PSPNet 的 mIoU 从令人印象深刻的 77.83% 大幅降低到仅 2.14%，展示了其在 Cityscapes 上的强大威力。</details>
**PDF:** <http://arxiv.org/pdf/2402.01220v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Segment Any Change**<br />
**Title_cn:** 分段任何更改<br />
**Authors:** Zhuo Zheng, Yanfei Zhong, Liangpei Zhang, Stefano Ermon<br />
**Abstract:** <details><summary>原文: </summary>Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉基础模型在零样本图像分类和分割方面取得了显着的成果，但零样本变化检测仍然是一个悬而未决的问题。在本文中，我们提出了分段任意变化模型（AnyChange），这是一种新型的变化检测模型，支持对未见变化类型和数据分布的零样本预测和泛化。 AnyChange 通过我们的免训练适应方法、双时态潜在匹配建立在分段任何模型 (SAM) 的基础上。通过揭示和利用 SAM 潜在空间中的图像内和图像间语义相似性，双时态潜在匹配以免训练的方式赋予 SAM 零样本变化检测能力。我们还提出了一种点查询机制来启用 AnyChange 的零样本以对象为中心的变化检测功能。我们进行了大量的实验来确认 AnyChange 对于零样本变更检测的有效性。 AnyChange 在无监督变化检测的 SECOND 基准上创造了新记录，比之前的 SOTA 得分高出高达 4.4% F$_1$ 分数，并且在监督变化检测中通过可忽略的手动注释（每张图像 1 个像素）实现了相当的准确度。</details>
**PDF:** <http://arxiv.org/pdf/2402.01188v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **DeepBranchTracer: A Generally-Applicable Approach to Curvilinear Structure Reconstruction Using Multi-Feature Learning**<br />
**Title_cn:** DeepBranchTracer：一种使用多特征学习进行曲线结构重建的通用方法<br />
**Authors:** Chao Liu, Ting Zhao, Nenggan Zheng<br />
**Abstract:** <details><summary>原文: </summary>Curvilinear structures, which include line-like continuous objects, are fundamental geometrical elements in image-based applications. Reconstructing these structures from images constitutes a pivotal research area in computer vision. However, the complex topology and ambiguous image evidence render this process a challenging task. In this paper, we introduce DeepBranchTracer, a novel method that learns both external image features and internal geometric characteristics to reconstruct curvilinear structures. Firstly, we formulate the curvilinear structures extraction as a geometric attribute estimation problem. Then, a curvilinear structure feature learning network is designed to extract essential branch attributes, including the image features of centerline and boundary, and the geometric features of direction and radius. Finally, utilizing a multi-feature fusion tracing strategy, our model iteratively traces the entire branch by integrating the extracted image and geometric features. We extensively evaluated our model on both 2D and 3D datasets, demonstrating its superior performance over existing segmentation and reconstruction methods in terms of accuracy and continuity.</details>
**Abstract_cn:** <details><summary>译文: </summary>曲线结构（包括线状连续对象）是基于图像的应用中的基本几何元素。从图像重建这些结构构成了计算机视觉的关键研究领域。然而，复杂的拓扑结构和模糊的图像证据使这一过程成为一项具有挑战性的任务。在本文中，我们介绍了 DeepBranchTracer，这是一种学习外部图像特征和内部几何特征来重建曲线结构的新方法。首先，我们将曲线结构提取表述为几何属性估计问题。然后，设计曲线结构特征学习网络来提取必要的分支属性，包括中心线和边界的图像特征以及方向和半径的几何特征。最后，利用多特征融合跟踪策略，我们的模型通过集成提取的图像和几何特征来迭代跟踪整个分支。我们在 2D 和 3D 数据集上广泛评估了我们的模型，证明其在准确性和连续性方面优于现有分割和重建方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.01187v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Scale Equalization for Multi-Level Feature Fusion**<br />
**Title_cn:** 多级特征融合的尺度均衡<br />
**Authors:** Bum Jun Kim, Sang Woo Kim<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks have exhibited remarkable performance in a variety of computer vision fields, especially in semantic segmentation tasks. Their success is often attributed to multi-level feature fusion, which enables them to understand both global and local information from an image. However, we found that multi-level features from parallel branches are on different scales. The scale disequilibrium is a universal and unwanted flaw that leads to detrimental gradient descent, thereby degrading performance in semantic segmentation. We discover that scale disequilibrium is caused by bilinear upsampling, which is supported by both theoretical and empirical evidence. Based on this observation, we propose injecting scale equalizers to achieve scale equilibrium across multi-level features after bilinear upsampling. Our proposed scale equalizers are easy to implement, applicable to any architecture, hyperparameter-free, implementable without requiring extra computational cost, and guarantee scale equilibrium for any dataset. Experiments showed that adopting scale equalizers consistently improved the mIoU index across various target datasets, including ADE20K, PASCAL VOC 2012, and Cityscapes, as well as various decoder choices, including UPerHead, PSPHead, ASPPHead, SepASPPHead, and FCNHead.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在各种计算机视觉领域，尤其是语义分割任务中表现出了卓越的性能。他们的成功通常归功于多级特征融合，这使他们能够从图像中理解全局和局部信息。然而，我们发现并行分支的多级特征具有不同的尺度。尺度不平衡是一种普遍且不必要的缺陷，会导致有害的梯度下降，从而降低语义分割的性能。我们发现尺度不平衡是由双线性上采样引起的，这一点得到了理论和经验证据的支持。基于这一观察，我们建议注入尺度均衡器，以在双线性上采样后实现多级特征之间的尺度平衡。我们提出的尺度均衡器易于实现，适用于任何架构，无超参数，无需额外的计算成本即可实现，并保证任何数据集的尺度平衡。实验表明，采用尺度均衡器可以持续改善各种目标数据集（包括 ADE20K、PASCAL VOC 2012 和 Cityscapes）以及各种解码器选择（包括 UPerHead、PSPHead、ASPPPHead、SepASPPHead 和 FCNHead）的 mIoU 指数。</details>
**PDF:** <http://arxiv.org/pdf/2402.01149v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping**<br />
**Title_cn:** DeepAAT：深度自动化空中三角测量，用于基于无人机的快速测绘<br />
**Authors:** Zequan Chen, Jianping Li, Qusheng Li, Bisheng Yang, Zhen Dong<br />
**Abstract:** <details><summary>原文: </summary>Automated Aerial Triangulation (AAT), aiming to restore image pose and reconstruct sparse points simultaneously, plays a pivotal role in earth observation. With its rich research heritage spanning several decades in photogrammetry, AAT has evolved into a fundamental process widely applied in large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its advancements, classic AAT methods still face challenges like low efficiency and limited robustness. This paper introduces DeepAAT, a deep learning network designed specifically for AAT of UAV imagery. DeepAAT considers both spatial and spectral characteristics of imagery, enhancing its capability to resolve erroneous matching pairs and accurately predict image poses. DeepAAT marks a significant leap in AAT's efficiency, ensuring thorough scene coverage and precision. Its processing speed outpaces incremental AAT methods by hundreds of times and global AAT methods by tens of times while maintaining a comparable level of reconstruction accuracy. Additionally, DeepAAT's scene clustering and merging strategy facilitate rapid localization and pose determination for large-scale UAV images, even under constrained computing resources. The experimental results demonstrate DeepAAT's substantial improvements over conventional AAT methods, highlighting its potential in the efficiency and accuracy of UAV-based 3D reconstruction tasks. To benefit the photogrammetry society, the code of DeepAAT will be released at: https://github.com/WHU-USI3DV/DeepAAT.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动空中三角测量（AAT）旨在同时恢复图像位姿和重建稀疏点，在对地观测中发挥着关键作用。凭借其在摄影测量领域数十年的丰富研究成果，AAT 已发展成为广泛应用于大规模无人机 (UAV) 测绘的基本流程。尽管取得了进步，经典的 AAT 方法仍然面临效率低和鲁棒性有限等挑战。本文介绍了 DeepAAT，这是一种专为无人机图像 AAT 设计的深度学习网络。 DeepAAT 考虑图像的空间和光谱特征，增强其解决错误匹配对和准确预测图像姿态的能力。 DeepAAT标志着AAT效率的重大飞跃，确保了场景的全面覆盖和精度。其处理速度比增量 AAT 方法快数百倍，比全局 AAT 方法快数十倍，同时保持相当的重建精度水平。此外，即使在计算资源有限的情况下，DeepAAT 的场景聚类和合并策略也有助于大规模无人机图像的快速定位和姿态确定。实验结果表明，DeepAAT 相对于传统 AAT 方法有了实质性改进，凸显了其在基于无人机的 3D 重建任务的效率和准确性方面的潜力。为了造福摄影测量界，DeepAAT的代码将发布在：https://github.com/WHU-USI3DV/DeepAAT。</details>
**PDF:** <http://arxiv.org/pdf/2402.01134v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes**<br />
**Title_cn:** 通过内核-特征对稀疏变分高斯过程进行自注意力<br />
**Authors:** Yingyi Chen, Qinghua Tao, Francesco Tonin, Johan A. K. Suykens<br />
**Abstract:** <details><summary>原文: </summary>While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity; iii) an evidence lower bound is derived so that variational parameters can be optimized towards this objective. Experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然 Transformers 的强大功能显着提高了预测准确性，但它也可能会产生过于自信的预测，并且需要校准的不确定性估计，这通常可以通过高斯过程 (GP) 来解决。现有的工作将具有对称核的 GP 应用到注意核的变分推理中；然而，忽略了注意力核本质上是不对称的这一事实。此外，对于大规模数据，推导 GP 后验的复杂性仍然很高。在这项工作中，我们提出了内核-特征对稀疏变分高斯过程（KEP-SVGP）来构建不确定性感知的自注意力，其中注意力内核的不对称性由内核 SVD（KSVD）解决，并降低了复杂性。通过 KEP-SVGP，i) 由来自 KSVD w.r.t. 的两组奇异向量导出的 SVGP 对。注意核充分表征了不对称性； ii) 仅使用 KSVD 中的一小组伴随特征函数，SVGP 后验的推导可以基于包含奇异值的对角矩阵的求逆，从而有助于降低时间复杂度； iii) 导出证据下限，以便可以针对该目标优化变分参数。实验验证了我们在分布内、分布转移和分布外基准上的出色性能和效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.01476v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LIR: Efficient Degradation Removal for Lightweight Image Restoration**<br />
**Title_cn:** LIR：高效退化去除以实现轻量级图像恢复<br />
**Authors:** Dongqi Fan, Ting Yue, Xin Zhao, Liang Chang<br />
**Abstract:** <details><summary>原文: </summary>Recently, there have been significant advancements in Image Restoration based on CNN and transformer. However, the inherent characteristics of the Image Restoration task are often overlooked in many works. These works often focus on the basic block design and stack numerous basic blocks to the model, leading to redundant parameters and unnecessary computations and hindering the efficiency of the image restoration. In this paper, we propose a Lightweight Image Restoration network called LIR to efficiently remove degradation (blur, rain, noise, haze, etc.). A key component in LIR is the Efficient Adaptive Attention (EAA) Block, which is mainly composed of Adaptive Filters and Attention Blocks. It is capable of adaptively sharpening contours, removing degradation, and capturing global information in various image restoration scenes in an efficient and computation-friendly manner. In addition, through a simple structural design, LIR addresses the degradations existing in the local and global residual connections that are ignored by modern networks. Extensive experiments demonstrate that our LIR achieves comparable performance to state-of-the-art networks on most benchmarks with fewer parameters and computations. It is worth noting that our LIR produces better visual results than state-of-the-art networks that are more in line with the human aesthetic.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于 CNN 和 Transformer 的图像恢复取得了重大进展。然而，图像恢复任务的固有特征在许多工作中经常被忽视。这些工作往往侧重于基本块设计，将大量基本块堆叠到模型中，导致冗余参数和不必要的计算，阻碍图像恢复的效率。在本文中，我们提出了一种称为 LIR 的轻量级图像恢复网络，以有效消除退化（模糊、雨、噪声、雾霾等）。 LIR 的一个关键组件是高效自适应注意力（EAA）块，它主要由自适应滤波器和注意力块组成。它能够以高效且计算友好的方式自适应锐化轮廓、消除退化并捕获各种图像恢复场景中的全局信息。此外，通过简单的结构设计，LIR 解决了现代网络忽略的局部和全局残差连接中存在的退化问题。大量实验表明，我们的 LIR 在大多数基准测试中以更少的参数和计算量实现了与最先进网络相当的性能。值得注意的是，我们的 LIR 产生的视觉效果比最先进的网络更好，更符合人类审美。</details>
**PDF:** <http://arxiv.org/pdf/2402.01368v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Spectrum-guided Feature Enhancement Network for Event Person Re-Identification**<br />
**Title_cn:** 用于事件人员重新识别的频谱引导特征增强网络<br />
**Authors:** Hongchen Tan, Yi Zhang, Xiuping Liu, Baocai Yin, Nan Ma, Xin Li, Huchuan Lu<br />
**Abstract:** <details><summary>原文: </summary>As a cutting-edge biosensor, the event camera holds significant potential in the field of computer vision, particularly regarding privacy preservation. However, compared to traditional cameras, event streams often contain noise and possess extremely sparse semantics, posing a formidable challenge for event-based person re-identification (event Re-ID). To address this, we introduce a novel event person re-identification network: the Spectrum-guided Feature Enhancement Network (SFE-Net). This network consists of two innovative components: the Multi-grain Spectrum Attention Mechanism (MSAM) and the Consecutive Patch Dropout Module (CPDM). MSAM employs a fourier spectrum transform strategy to filter event noise, while also utilizing an event-guided multi-granularity attention strategy to enhance and capture discriminative person semantics. CPDM employs a consecutive patch dropout strategy to generate multiple incomplete feature maps, encouraging the deep Re-ID model to equally perceive each effective region of the person's body and capture robust person descriptors. Extensive experiments on Event Re-ID datasets demonstrate that our SFE-Net achieves the best performance in this task.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为一种尖端的生物传感器，事件相机在计算机视觉领域具有巨大的潜力，特别是在隐私保护方面。然而，与传统摄像头相比，事件流往往包含噪声且语义极其稀疏，这给基于事件的行人重识别（事件Re-ID）带来了巨大的挑战。为了解决这个问题，我们引入了一种新颖的事件人员重新识别网络：频谱引导特征增强网络（SFE-Net）。该网络由两个创新组件组成：多粒度频谱注意力机制（MSAM）和连续补丁丢失模块（CPDM）。 MSAM 采用傅立叶频谱变换策略来过滤事件噪声，同时还利用事件引导的多粒度注意力策略来增强和捕获有区别的人物语义。 CPDM采用连续的patch dropout策略来生成多个不完整的特征图，鼓励深度Re-ID模型平等地感知人身体的每个有效区域并捕获稳健的人描述符。对事件重识别数据集的大量实验表明，我们的 SFE-Net 在此任务中实现了最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.01269v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Enhanced Urban Region Profiling with Adversarial Self-Supervised Learning**<br />
**Title_cn:** 通过对抗性自我监督学习增强城市区域分析<br />
**Authors:** Weiliang Chan, Qianqian Ren, Jinbao Li<br />
**Abstract:** <details><summary>原文: </summary>Urban region profiling is pivotal for smart cities, but mining fine-grained semantics from noisy and incomplete urban data remains challenging. In response, we propose a novel self-supervised graph collaborative filtering model for urban region embedding called EUPAS. Specifically, region heterogeneous graphs containing human mobility data, point of interests (POIs) information, and geographic neighborhood details for each region are fed into the model, which generates region embeddings that preserve intra-region and inter-region dependencies through GCNs and multi-head attention. Meanwhile, we introduce spatial perturbation augmentation to generate positive samples that are semantically similar and spatially close to the anchor, preparing for subsequent contrastive learning. Furthermore, adversarial training is employed to construct an effective pretext task by generating strong positive pairs and mining hard negative pairs for the region embeddings. Finally, we jointly optimize supervised and self-supervised learning to encourage the model to capture the high-level semantics of region embeddings while ignoring the noisy and unimportant details. Extensive experiments on real-world datasets demonstrate the superiority of our model over state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>城市区域分析对于智慧城市至关重要，但从嘈杂和不完整的城市数据中挖掘细粒度语义仍然具有挑战性。为此，我们提出了一种用于城市区域嵌入的新型自监督图协同过滤模型，称为 EUPAS。具体来说，包含人员流动性数据、兴趣点 (POI) 信息和每个区域的地理邻域详细信息的区域异构图被输入到模型中，该模型生成区域嵌入，通过 GCN 和多区域网络来保留区域内和区域间的依赖关系。头注意。同时，我们引入空间扰动增强来生成语义相似且空间上接近锚点的正样本，为后续的对比学习做好准备。此外，通过生成强正对并挖掘区域嵌入的硬负对，采用对抗性训练来构建有效的借口任务。最后，我们联合优化监督和自监督学习，以鼓励模型捕获区域嵌入的高级语义，同时忽略噪声和不重要的细节。对现实世界数据集的大量实验证明了我们的模型相对于最先进方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.01163v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Seeing Objects in a Cluttered World: Computational Objectness from Motion in Video**<br />
**Title_cn:** 在杂乱的世界中看到对象：视频中运动的计算对象性<br />
**Authors:** Douglas Poland, Amar Saini<br />
**Abstract:** <details><summary>原文: </summary>Perception of the visually disjoint surfaces of our cluttered world as whole objects, physically distinct from those overlapping them, is a cognitive phenomenon called objectness that forms the basis of our visual perception. Shared by all vertebrates and present at birth in humans, it enables object-centric representation and reasoning about the visual world. We present a computational approach to objectness that leverages motion cues and spatio-temporal attention using a pair of supervised spatio-temporal R(2+1)U-Nets. The first network detects motion boundaries and classifies the pixels at those boundaries in terms of their local foreground-background sense. This motion boundary sense (MBS) information is passed, along with a spatio-temporal object attention cue, to an attentional surface perception (ASP) module which infers the form of the attended object over a sequence of frames and classifies its 'pixels' as visible or obscured. The spatial form of the attention cue is flexible, but it must loosely track the attended object which need not be visible. We demonstrate the ability of this simple but novel approach to infer objectness from phenomenology without object models, and show that it delivers robust perception of individual attended objects in cluttered scenes, even with blur and camera shake. We show that our data diversity and augmentation minimizes bias and facilitates transfer to real video. Finally, we describe how this computational objectness capability can grow in sophistication and anchor a robust modular video object perception framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>将我们杂乱的世界中视觉上不相交的表面感知为整个物体，在物理上不同于那些重叠它们的物体，是一种称为客观性的认知现象，它构成了我们视觉感知的基础。它为所有脊椎动物所共有，并在人类出生时就存在，它使得能够以对象为中心的视觉世界表示和推理成为可能。我们提出了一种利用运动线索和时空注意力的对象性计算方法，使用一对监督时空 R(2+1)U-Net。第一个网络检测运动边界，并根据局部前景-背景感对这些边界处的像素进行分类。该运动边界感知 (MBS) 信息与时空对象注意提示一起传递到注意表面感知 (ASP) 模块，该模块通过一系列帧推断所关注对象的形式，并将其“像素”分类为可见或模糊。注意线索的空间形式是灵活的，但它必须松散地跟踪不需要可见的关注对象。我们展示了这种简单但新颖的方法能够在没有对象模型的情况下从现象学推断对象性，并表明即使在模糊和相机抖动的情况下，它也能在杂乱的场景中提供对个体参与对象的强大感知。我们表明，我们的数据多样性和增强可以最大限度地减少偏见并促进转移到真实视频。最后，我们描述了这种计算对象能力如何变得更加复杂，并锚定一个强大的模块化视频对象感知框架。</details>
**PDF:** <http://arxiv.org/pdf/2402.01126v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **How many views does your deep neural network use for prediction?**<br />
**Title_cn:** 您的深度神经网络使用多少个视图进行预测？<br />
**Authors:** Keisuke Kawano, Takuro Kutsuna, Keisuke Sano<br />
**Abstract:** <details><summary>原文: </summary>The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu & Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管进行了大量的理论和实证分析，但深度神经网络（DNN）的泛化能力仍未完全被理解。最近，Allen-Zhu＆Li（2023）引入了多视图的概念来解释DNN的泛化能力，但他们的主要目标是集成或蒸馏模型，并且没有估计用于特定预测的多视图的方法讨论了输入。在本文中，我们提出了最小足够视图（MSV），它类似于多视图，但可以有效地计算真实图像。 MSV 是输入中的一组最小且不同的特征，每个特征都保留模型对输入的预测。我们凭经验表明，跨模型（包括卷积模型和变压器模型）的 MSV 数量和预测精度之间存在明显的关系，这表明多视图视角对于理解（非集成或非集成）的泛化能力也很重要。蒸馏）DNN。</details>
**PDF:** <http://arxiv.org/pdf/2402.01095v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Scaled 360 layouts: Revisiting non-central panoramas**<br />
**Title_cn:** 缩放 360 度布局：重新审视非中心全景图<br />
**Authors:** Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>From a non-central panorama, 3D lines can be recovered by geometric reasoning. However, their sensitivity to noise and the complex geometric modeling required has led these panoramas being very little investigated. In this work we present a novel approach for 3D layout recovery of indoor environments using single non-central panoramas. We obtain the boundaries of the structural lines of the room from a non-central panorama using deep learning and exploit the properties of non-central projection systems in a new geometrical processing to recover the scaled layout. We solve the problem for Manhattan environments, handling occlusions, and also for Atlanta environments in an unified method. The experiments performed improve the state-of-the-art methods for 3D layout recovery from a single panorama. Our approach is the first work using deep learning with non-central panoramas and recovering the scale of single panorama layouts.</details>
**Abstract_cn:** <details><summary>译文: </summary>从非中心全景图中，可以通过几何推理恢复 3D 线。然而，它们对噪声的敏感性和所需的复杂几何模型导致这些全景图很少被研究。在这项工作中，我们提出了一种使用单个非中心全景图恢复室内环境 3D 布局的新颖方法。我们使用深度学习从非中心全景图中获取房间结构线的边界，并在新的几何处理中利用非中心投影系统的特性来恢复缩放布局。我们通过统一的方法解决曼哈顿环境、处理遮挡以及亚特兰大环境的问题。所进行的实验改进了从单个全景图恢复 3D 布局的最先进方法。我们的方法是第一个将深度学习与非中心全景结合使用并恢复单个全景布局的比例的工作。</details>
**PDF:** <http://arxiv.org/pdf/2402.01466v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine Mesh Models Using Local Anatomical Vertebral Axes**<br />
**Title_cn:** 3D 椎骨测量：使用局部解剖椎轴评估人体脊柱网格模型中的椎骨尺寸<br />
**Authors:** Ivanna Kramer, Vinzent Rittel, Lara Blomenkamp, Sabine Bauer, Dietrich Paulus<br />
**Abstract:** <details><summary>原文: </summary>Vertebral morphological measurements are important across various disciplines, including spinal biomechanics and clinical applications, pre- and post-operatively. These measurements also play a crucial role in anthropological longitudinal studies, where spinal metrics are repeatedly documented over extended periods. Traditionally, such measurements have been manually conducted, a process that is time-consuming. In this study, we introduce a novel, fully automated method for measuring vertebral morphology using 3D meshes of lumbar and thoracic spine models.Our experimental results demonstrate the method's capability to accurately measure low-resolution patient-specific vertebral meshes with mean absolute error (MAE) of 1.09 mm and those derived from artificially created lumbar spines, where the average MAE value was 0.7 mm. Our qualitative analysis indicates that measurements obtained using our method on 3D spine models can be accurately reprojected back onto the original medical images if these images are available.</details>
**Abstract_cn:** <details><summary>译文: </summary>椎骨形态测量在各个学科中都很重要，包括脊柱生物力学和术前和术后的临床应用。这些测量在人类学纵向研究中也发挥着至关重要的作用，其中脊柱指标在较长时间内被重复记录。传统上，此类测量是手动进行的，这个过程非常耗时。在这项研究中，我们引入了一种新颖的全自动方法，使用腰椎和胸椎模型的 3D 网格来测量椎体形态。我们的实验结果表明，该方法能够以平均绝对误差 (MAE) 准确测量低分辨率患者特定椎体网格。 ）的 1.09 毫米，以及来自人工制造的腰椎的那些，其中平均 MAE 值为 0.7 毫米。我们的定性分析表明，如果原始医学图像可用，则使用我们的方法在 3D 脊柱模型上获得的测量结果可以准确地重新投影回原始医学图像上。</details>
**PDF:** <http://arxiv.org/pdf/2402.01462v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **SiMA-Hand: Boosting 3D Hand-Mesh Reconstruction by Single-to-Multi-View Adaptation**<br />
**Title_cn:** SiMA-Hand：通过单视图到多视图适应促进 3D 手网格重建<br />
**Authors:** Yinqiao Wang, Hao Xu, Pheng-Ann Heng, Chi-Wing Fu<br />
**Abstract:** <details><summary>原文: </summary>Estimating 3D hand mesh from RGB images is a longstanding track, in which occlusion is one of the most challenging problems. Existing attempts towards this task often fail when the occlusion dominates the image space. In this paper, we propose SiMA-Hand, aiming to boost the mesh reconstruction performance by Single-to-Multi-view Adaptation. First, we design a multi-view hand reconstructor to fuse information across multiple views by holistically adopting feature fusion at image, joint, and vertex levels. Then, we introduce a single-view hand reconstructor equipped with SiMA. Though taking only one view as input at inference, the shape and orientation features in the single-view reconstructor can be enriched by learning non-occluded knowledge from the extra views at training, enhancing the reconstruction precision on the occluded regions. We conduct experiments on the Dex-YCB and HanCo benchmarks with challenging object- and self-caused occlusion cases, manifesting that SiMA-Hand consistently achieves superior performance over the state of the arts. Code will be released on https://github.com/JoyboyWang/SiMA-Hand Pytorch.</details>
**Abstract_cn:** <details><summary>译文: </summary>从 RGB 图像估计 3D 手部网格是一个长期存在的问题，其中遮挡是最具挑战性的问题之一。当遮挡占据图像空间时，现有的针对此任务的尝试通常会失败。在本文中，我们提出了 SiMA-Hand，旨在通过单视图自适应来提高网格重建性能。首先，我们设计了一个多视图手重建器，通过在图像、关节和顶点级别整体采用特征融合来融合多个视图的信息。然后，我们介绍了配备 SiMA 的单视图手重建器。尽管在推理时仅采用一个视图作为输入，但可以通过在训练时从额外视图中学习非遮挡知识来丰富单视图重建器中的形状和方向特征，从而提高遮挡区域的重建精度。我们在 Dex-YCB 和 HanCo 基准上进行了具有挑战性的物体和自身引起的遮挡情况的实验，表明 SiMA-Hand 始终取得优于现有技术的性能。代码将在 https://github.com/JoyboyWang/SiMA-Hand Pytorch 上发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.01389v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Comprehensive Survey on 3D Content Generation**<br />
**Title_cn:** 3D 内容生成的综合调查<br />
**Authors:** Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang Tang, Ziwei Liu, Wanli Ouyang, Wangmeng Zuo, Junjun Jiang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed remarkable advances in artificial intelligence generated content(AIGC), with diverse input modalities, e.g., text, image, video, audio and 3D. The 3D is the most close visual modality to real-world 3D environment and carries enormous knowledge. The 3D content generation shows both academic and practical values while also presenting formidable technical challenges. This review aims to consolidate developments within the burgeoning domain of 3D content generation. Specifically, a new taxonomy is proposed that categorizes existing approaches into three types: 3D native generative methods, 2D prior-based 3D generative methods, and hybrid 3D generative methods. The survey covers approximately 60 papers spanning the major techniques. Besides, we discuss limitations of current 3D content generation techniques, and point out open challenges as well as promising directions for future work. Accompanied with this survey, we have established a project website where the resources on 3D content generation research are provided. The project page is available at https://github.com/hitcslj/Awesome-AIGC-3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，人工智能生成内容（AIGC）取得了显着进步，输入方式多种多样，例如文本、图像、视频、音频和 3D。 3D 是最接近真实 3D 环境的视觉形态，蕴藏着巨大的知识。 3D 内容生成既具有学术价值又具有实用价值，同时也提出了艰巨的技术挑战。本次审查旨在巩固 3D 内容生成这一新兴领域的发展。具体来说，提出了一种新的分类法，将现有方法分为三种类型：3D 原生生成方法、基于 2D 先验的 3D 生成方法和混合 3D 生成方法。该调查涵盖了大约 60 篇涵盖主要技术的论文。此外，我们讨论了当前 3D 内容生成技术的局限性，并指出了开放的挑战以及未来工作的有希望的方向。配合这项调查，我们建立了一个项目网站，提供 3D 内容生成研究的资源。该项目页面位于 https://github.com/hitcslj/Awesome-AIGC-3D。</details>
**PDF:** <http://arxiv.org/pdf/2402.01166v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Simulator-Free Visual Domain Randomization via Video Games**<br />
**Title_cn:** 通过视频游戏实现无模拟器视觉域随机化<br />
**Authors:** Chintan Trivedi, Nemanja Rašajski, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis<br />
**Abstract:** <details><summary>原文: </summary>Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. BehAVE successfully aligns player behavioral patterns and is able to zero-shot transfer them to multiple unseen FPS games when trained on just one FPS game. In a more challenging setting, BehAVE manages to improve the zero-shot transferability of foundation models to unseen FPS games (up to 22%) even when trained on a game of a different genre (Minecraft). Code and dataset can be found at https://github.com/nrasajski/BehAVE.</details>
**Abstract_cn:** <details><summary>译文: </summary>域随机化是一种有效的计算机视觉技术，用于提高视觉模型在显示相似内容的视觉不同域之间的可转移性。然而，现有的方法广泛依赖于调整难以构建的复杂且专门的模拟引擎，从而影响其可行性和可扩展性。本文介绍了 BehAVE，这是一种视频理解框架，它独特地利用大量现有的商业视频游戏进行域随机化，而无需访问其模拟引擎。在 BehAVE 下，(1) 视频游戏固有的丰富视觉多样性充当随机化的来源，(2) 玩家行为（通过动作的文本描述在语义上表示）指导具有相似内容的视频的“对齐”。我们在 25 款第一人称射击 (FPS) 类型的游戏上跨各种视频和文本基础模型测试了 BehAVE，并报告了其对域随机化的鲁棒性。 BehAVE 成功地调整了玩家的行为模式，并且当仅在一款 FPS 游戏上进行训练时，能够将其零射击转移到多个未见过的 FPS 游戏。在更具挑战性的环境中，即使在不同类型的游戏（《我的世界》）上进行训练，BehAVE 也能设法提高基础模型到未见过的 FPS 游戏的零镜头可移植性（高达 22%）。代码和数据集可以在 https://github.com/nrasajski/BehAVE 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.01335v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Immersive Video Compression using Implicit Neural Representations**<br />
**Title_cn:** 使用隐式神经表示的沉浸式视频压缩<br />
**Authors:** Ho Man Kwan, Fan Zhang, Andrew Gower, David Bull<br />
**Abstract:** <details><summary>原文: </summary>Recent work on implicit neural representations (INRs) has evidenced their potential for efficiently representing and encoding conventional video content. In this paper we, for the first time, extend their application to immersive (multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video codec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video codec, HiNeRV, which was developed for single-view video compression. We have modified the model to learn a different group of feature grids for each view, and share the learnt network parameters among all views. This enables the model to effectively exploit the spatio-temporal and the inter-view redundancy that exists within multi-view videos. The proposed codec was used to compress multi-view texture and depth video sequences in the MPEG Immersive Video (MIV) Common Test Conditions, and tested against the MIV Test model (TMIV) that uses the VVenC video codec. The results demonstrate the superior performance of MV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV. The implementation of MV-HiNeRV will be published for further development and evaluation.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近关于隐式神经表示（INR）的工作证明了它们在有效表示和编码传统视频内容方面的潜力。在本文中，我们通过提出 MV-HiNeRV（一种新的基于 INR 的沉浸式视频编解码器），首次将其应用扩展到沉浸式（多视图）视频。 MV-HiNeRV 是最先进的基于 INR 的视频编解码器 HiNeRV 的增强版本，专为单视图视频压缩而开发。我们修改了模型，为每个视图学习不同的特征网格组，并在所有视图之间共享学习的网络参数。这使得模型能够有效地利用多视图视频中存在的时空和视图间冗余。所提出的编解码器用于在 MPEG 沉浸式视频 (MIV) 通用测试条件下压缩多视图纹理和深度视频序列，并针对使用 VVenC 视频编解码器的 MIV 测试模型 (TMIV) 进行测试。结果证明了 MV-HiNeRV 的优越性能，与 TMIV 相比具有显着的编码增益（高达 72.33%）。 MV-HiNeRV 的实施将被发布以供进一步开发和评估。</details>
**PDF:** <http://arxiv.org/pdf/2402.01596v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SLYKLatent, a Learning Framework for Facial Features Estimation**<br />
**Title_cn:** SLYKLatent，面部特征估计的学习框架<br />
**Authors:** Samuel Adebayo, Joost C. Dessing, Seán McLoone<br />
**Abstract:** <details><summary>原文: </summary>In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function. Our evaluation on benchmark datasets achieves an 8.7% improvement on Gaze360, rivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%, surpassing existing methods by significant margins. Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components. This approach has strong potential in human-robot interaction.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们提出了 SLYKLatent，这是一种通过解决由于任意不确定性、协变位移和测试域泛化而导致的数据集中的外观不稳定挑战来增强注视估计的新方法。 SLYKLatent 利用自监督学习对面部表情数据集进行初始训练，然后使用基于补丁的三分支网络和逆解释方差加权训练损失函数进行细化。我们对基准数据集的评估在 Gaze360 上实现了 8.7% 的改进，可与顶级 MPIIFaceGaze 结果相媲美，并领先 ETH-XGaze 的子集 13%，大幅超越现有方法。 RAF-DB 和 Affectnet 上的适应性测试显示准确度分别为 86.4% 和 60.9%。消融研究证实了 SLYKLatent 新颖成分的有效性。这种方法在人机交互方面具有巨大的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.01555v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Visual Gyroscope: Combination of Deep Learning Features and Direct Alignment for Panoramic Stabilization**<br />
**Title_cn:** 视觉陀螺仪：结合深度学习功能和直接对准实现全景稳定<br />
**Authors:** Bruno Berenguel-Baeta, Antoine N. Andre, Guillaume Caron, Jesus Bermudez-Cameo, Jose J. Guerrero<br />
**Abstract:** <details><summary>原文: </summary>In this article we present a visual gyroscope based on equirectangular panoramas. We propose a new pipeline where we take advantage of combining three different methods to obtain a robust and accurate estimation of the attitude of the camera. We quantitatively and qualitatively validate our method on two image sequences taken with a $360^\circ$ dual-fisheye camera mounted on different aerial vehicles.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种基于等距柱状全景图的视觉陀螺仪。我们提出了一种新的流程，我们利用组合三种不同的方法来获得对相机姿态的稳健且准确的估计。我们在使用安装在不同飞行器上的 360 美元双鱼眼相机拍摄的两个图像序列上定量和定性验证了我们的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.01461v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning**<br />
**Title_cn:** 关键任务——卫星数据是机器学习的一种独特模式<br />
**Authors:** Esther Rolf, Konstantin Klemmer, Caleb Robinson, Hannah Kerner<br />
**Abstract:** <details><summary>原文: </summary>Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.</details>
**Abstract_cn:** <details><summary>译文: </summary>卫星数据有可能激发机器学习的巨大转变——在这种转变中，我们重新思考为传统数据模式设计的现有实践。随着卫星数据机器学习 (SatML) 因其对现实世界的影响而受到关注，我们的领域正处于十字路口。我们可以继续应用不合适的方法，也可以启动一个围绕卫星数据的独特特征和挑战的新研究议程。本立场文件认为，卫星数据构成了机器学习研究的一种独特模式，我们必须认识到这一点，以提高 SatML 研究在理论、方法和部署方面的质量和影响。我们概述了关键的讨论问题和可行的建议，将 SatML 从一个有趣的应用领域转变为一个专门的研究学科，帮助解决机器学习和社会的重大挑战。</details>
**PDF:** <http://arxiv.org/pdf/2402.01444v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes**<br />
**Title_cn:** 描述图像$\textit{快和慢}$：量化和预测视觉语言过程中人类信号的变化<br />
**Authors:** Ece Takmaz, Sandro Pezzelle, Raquel Fernández<br />
**Abstract:** <details><summary>原文: </summary>There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for humans and what leads to variations in human outputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像的属性与人类在描述图像时的行为方式之间存在着复杂的关系。这种行为表现出充足的变化，如眼球运动等人类信号以及人类开始描述图像时所体现的那样。尽管这些视觉语言变异信号很有价值，但在当前预训练模型的训练中它们实际上被忽视了，这激发了进一步的研究。使用荷兰语图像描述语料库和同时收集的眼动追踪数据，我们探索了视觉语言信号变化的本质，并发现它们彼此相关。鉴于这个结果，我们假设变化部分源于图像的属性，并探索由预训练的视觉编码器编码的图像表示是否可以捕获这种变化。我们的结果表明，预训练模型在弱到中等程度上做到了这一点，这表明这些模型对于人类刺激的复杂性以及导致人类输出变化的原因缺乏偏见。</details>
**PDF:** <http://arxiv.org/pdf/2402.01352v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **UCVC: A Unified Contextual Video Compression Framework with Joint P-frame and B-frame Coding**<br />
**Title_cn:** UCVC：具有联合 P 帧和 B 帧编码的统一上下文视频压缩框架<br />
**Authors:** Jiayu Yang, Wei Jiang, Yongqi Zhai, Chunhui Yang, Ronggang Wang<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a learned video compression method in response to video compression track of the 6th Challenge on Learned Image Compression (CLIC), at DCC 2024.Specifically, we propose a unified contextual video compression framework (UCVC) for joint P-frame and B-frame coding. Each non-intra frame refers to two neighboring decoded frames, which can be either both from the past for P-frame compression, or one from the past and one from the future for B-frame compression. In training stage, the model parameters are jointly optimized with both P-frames and B-frames. Benefiting from the designs, the framework can support both P-frame and B-frame coding and achieve comparable compression efficiency with that specifically designed for P-frame or B-frame.As for challenge submission, we report the optimal compression efficiency by selecting appropriate frame types for each test sequence. Our team name is PKUSZ-LVC.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文针对 DCC 2024 第六届学习图像压缩挑战赛 (CLIC) 的视频压缩赛道，提出了一种学习视频压缩方法。具体来说，我们提出了一种用于联合 P 帧和 B 帧的统一上下文视频压缩框架 (UCVC) -帧编码。每个非帧内帧指的是两个相邻的解码帧，对于P帧压缩来说，这两个相邻的解码帧可以是来自过去的帧，对于B帧压缩来说，这两个帧可以是过去的帧，也可以是来自未来的帧的帧。在训练阶段，模型参数与P帧和B帧联合优化。受益于这些设计，该框架可以同时支持 P 帧和 B 帧编码，并实现与专门为 P 帧或 B 帧设计的压缩效率相当。至于挑战提交，我们通过选择适当的方法报告最佳压缩效率每个测试序列的帧类型。我们的团队名称是 PKUSZ-LVC。</details>
**PDF:** <http://arxiv.org/pdf/2402.01289v1><br />
**Code:** null<br />

