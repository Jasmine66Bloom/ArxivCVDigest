## [UPDATED!] **2024-02-03** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets**<br />
**Title_cn:** 重新审视不平衡数据集上二进制语义分割的生成对抗网络<br />
**Authors:** Lei Xu, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Anomalous pavement surface conditions detection aims to detect pixels representing anomalous states, such as cracks, on pavement surface images automatically by algorithms. Recently, deep learning models have been intensively applied to related topics with outstanding performance. However, most existing deep learning-related solutions rarely achieve a stable performance on diverse datasets. To address this issue, in this work, we propose a deep learning framework based on conditional Generative Adversarial Networks for anomalous region detection on pavement images at the pixel level. In particular, the proposed framework is developed to enhance the generator's ability to estimate the probability feature map from heterogeneous inputs with two training stages and multiscale feature representation. Moreover, several attention mechanisms are incorporated into the proposed framework to mitigate the performance deterioration of model training on severely imbalanced datasets. We implement experiments on six accessible pavement datasets. Extensive qualitative and quantitative experiments demonstrate that the proposed framework can achieve SOTA results on these datasets efficiently and robustly.</details>
**Abstract_cn:** <details><summary>译文: </summary>异常路面状况检测旨在通过算法自动检测路面图像上代表异常状态的像素，例如裂缝。近年来，深度学习模型在相关主题中得到了广泛的应用，并取得了突出的表现。然而，大多数现有的深度学习相关解决方案很少在不同的数据集上实现稳定的性能。为了解决这个问题，在这项工作中，我们提出了一种基于条件生成对抗网络的深度学习框架，用于像素级路面图像的异常区域检测。特别是，所提出的框架是为了增强生成器从具有两个训练阶段和多尺度特征表示的异构输入估计概率特征图的能力。此外，提出的框架中纳入了几种注意力机制，以减轻严重不平衡数据集上模型训练的性能恶化。我们在六个可访问的路面数据集上进行了实验。广泛的定性和定量实验表明，所提出的框架可以在这些数据集上高效、鲁棒地实现 SOTA 结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.02245v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **On the Exploitation of DCT-Traces in the Generative-AI Domain**<br />
**Title_cn:** 关于 DCT-Trace 在生成人工智能领域的利用<br />
**Authors:** Orazio Pontorno, Luca Guarnera, Sebastiano Battiato<br />
**Abstract:** <details><summary>原文: </summary>Since their appearance, Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics. In recent years, researchers have discovered that generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. To capture this evidence, in this paper we analyzed deepfake images in the frequency domain, examining in detail the beta-AC coefficients of the Discrete Cosine Transform (DCT). Recognizing that not all coefficients contribute equally to image recognition, we hypothesize the existence of a unique "discriminative fingerprint" for each type of image, embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. The integration of the Explainable AI (XAI) LIME algorithm combined with a neural classifier to explore alternative combinations of coefficients provides a deeper insight into the discriminative features of synthetic images. Experimental results reveal the significant potential of using a specific combination of beta-AC coefficients in order to improve the analysis of traces left by generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>自出现以来，Deepfakes 就代表了网络安全和数字取证领域最严峻的挑战之一。近年来，研究人员发现生成模型在合成数据中留下了独特的痕迹，如果详细分析和识别，可以利用这些痕迹来改善现有深度伪造探测器的泛化局限性。为了捕捉这一证据，在本文中，我们分析了频域中的深度伪造图像，详细检查了离散余弦变换 (DCT) 的 beta-AC 系数。认识到并非所有系数对图像识别的贡献相同，我们假设每种类型的图像都存在独特的“判别指纹”，嵌入特定的系数组合中。为了识别它们，机器学习分类器接受了各种系数组合的训练。将可解释 AI (XAI) LIME 算法与神经分类器相结合来探索系数的替代组合，可以更深入地了解合成图像的判别特征。实验结果揭示了使用特定的 beta-AC 系数组合来改进生成模型留下的痕迹的分析的巨大潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02209v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Diabetes detection using deep learning techniques with oversampling and feature augmentation**<br />
**Title_cn:** 使用具有过采样和特征增强的深度学习技术进行糖尿病检测<br />
**Authors:** María Teresa García-Ordás, Carmen Benavides, José Alberto Benítez-Andrades, Héctor Alaiz-Moretón, Isaías García-Rodríguez<br />
**Abstract:** <details><summary>原文: </summary>Background and objective: Diabetes is a chronic pathology which is affecting more and more people over the years. It gives rise to a large number of deaths each year. Furthermore, many people living with the disease do not realize the seriousness of their health status early enough. Late diagnosis brings about numerous health problems and a large number of deaths each year so the development of methods for the early diagnosis of this pathology is essential.   Methods: In this paper, a pipeline based on deep learning techniques is proposed to predict diabetic people. It includes data augmentation using a variational autoencoder (VAE), feature augmentation using an sparse autoencoder (SAE) and a convolutional neural network for classification. Pima Indians Diabetes Database, which takes into account information on the patients such as the number of pregnancies, glucose or insulin level, blood pressure or age, has been evaluated.   Results: A 92.31% of accuracy was obtained when CNN classifier is trained jointly the SAE for featuring augmentation over a well balanced dataset. This means an increment of 3.17% of accuracy with respect the state-of-the-art.   Conclusions: Using a full deep learning pipeline for data preprocessing and classification has demonstrate to be very promising in the diabetes detection field outperforming the state-of-the-art proposals.</details>
**Abstract_cn:** <details><summary>译文: </summary>背景和目的：糖尿病是一种慢性疾病，多年来影响着越来越多的人。它每年都会造成大量人员死亡。此外，许多患有这种疾病的人没有及早意识到其健康状况的严重性。晚期诊断每年会带来许多健康问题和大量死亡，因此开发这种病理的早期诊断方法至关重要。方法：本文提出了一种基于深度学习技术的管道来预测糖尿病患者。它包括使用变分自动编码器 (VAE) 的数据增强、使用稀疏自动编码器 (SAE) 的特征增强以及用于分类的卷积神经网络。对皮马印第安人糖尿病数据库进行了评估，该数据库考虑了患者的信息，例如怀孕次数、血糖或胰岛素水平、血压或年龄。结果：当 CNN 分类器与 SAE 联合训练以在平衡良好的数据集上进行增强时，获得了 92.31% 的准确率。这意味着相对于最先进的技术，准确度提高了 3.17%。结论：使用完整的深度学习管道进行数据预处理和分类已被证明在糖尿病检测领域非常有前途，优于最先进的建议。</details>
**PDF:** <http://arxiv.org/pdf/2402.02188v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance**<br />
**Title_cn:** 使用最优后验协方差改进反问题的扩散模型<br />
**Authors:** Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni Zou, Hongkai Xiong<br />
**Abstract:** <details><summary>原文: </summary>Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimental results demonstrate that the proposed methods significantly enhance the overall performance or robustness to hyperparameters of recent methods. Code is available at https://github.com/xypeng9903/k-diffusion-inverse-problems</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的扩散模型为噪声线性逆问题提供了一种有前途的零样本解决方案，而无需针对特定的逆问题进行重新训练。在本文中，我们从逼近条件采样反向扩散过程的条件后验均值的角度，对现有的零样本方法提出了第一个统一解释。我们发现，最近的方法相当于在给定扩散噪声图像的干净图像上对难以处理的后验分布进行各向同性高斯近似，唯一的区别在于各向同性后验协方差的手工设计。受这一发现的启发，我们提出了一种基于最大似然估计的通用即插即用后验协方差优化，以改进最近的方法。为了在无需重新训练的情况下实现最佳后验协方差，我们提供了基于两种专门设计的通用解决方案，这些方法专门用于利用带有和不带有反向协方差的预训练模型。实验结果表明，所提出的方法显着提高了最新方法的整体性能或对超参数的鲁棒性。代码可在 https://github.com/xypeng9903/k-diffusion-inverse-problems 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.02149v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Generative Visual Compression: A Review**<br />
**Title_cn:** 生成视觉压缩：回顾<br />
**Authors:** Bolin Chen, Shanzhi Yin, Peilin Chen, Shiqi Wang, Yan Ye<br />
**Abstract:** <details><summary>原文: </summary>Artificial Intelligence Generated Content (AIGC) is leading a new technical revolution for the acquisition of digital content and impelling the progress of visual compression towards competitive performance gains and diverse functionalities over traditional codecs. This paper provides a thorough review on the recent advances of generative visual compression, illustrating great potentials and promising applications in ultra-low bitrate communication, user-specified reconstruction/filtering, and intelligent machine analysis. In particular, we review the visual data compression methodologies with deep generative models, and summarize how compact representation and high-fidelity reconstruction could be actualized via generative techniques. In addition, we generalize related generative compression technologies for machine vision and intelligent analytics. Finally, we discuss the fundamental challenges on generative visual compression techniques and envision their future research directions.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能生成内容 (AIGC) 正在引领一场数字内容获取的新技术革命，并推动视觉压缩的进步，实现与传统编解码器相比具有竞争力的性能提升和多样化功能。本文对生成视觉压缩的最新进展进行了全面的回顾，阐述了其在超低比特率通信、用户指定的重建/过滤和智能机器分析方面的巨大潜力和有前景的应用。特别是，我们回顾了具有深度生成模型的视觉数据压缩方法，并总结了如何通过生成技术实现紧凑表示和高保真重建。此外，我们还推广了用于机器视觉和智能分析的相关生成压缩技术。最后，我们讨论了生成视觉压缩技术的基本挑战并展望了它们未来的研究方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.02140v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Enhancing crop classification accuracy by synthetic SAR-Optical data generation using deep learning**<br />
**Title_cn:** 使用深度学习生成合成 SAR 光学数据来提高作物分类精度<br />
**Authors:** Ali Mirzaei, Hossein Bagheri, Iman Khosravi<br />
**Abstract:** <details><summary>原文: </summary>Crop classification using remote sensing data has emerged as a prominent research area in recent decades. Studies have demonstrated that fusing SAR and optical images can significantly enhance the accuracy of classification. However, a major challenge in this field is the limited availability of training data, which adversely affects the performance of classifiers. In agricultural regions, the dominant crops typically consist of one or two specific types, while other crops are scarce. Consequently, when collecting training samples to create a map of agricultural products, there is an abundance of samples from the dominant crops, forming the majority classes. Conversely, samples from other crops are scarce, representing the minority classes. Addressing this issue requires overcoming several challenges and weaknesses associated with traditional data generation methods. These methods have been employed to tackle the imbalanced nature of the training data. Nevertheless, they still face limitations in effectively handling the minority classes. Overall, the issue of inadequate training data, particularly for minority classes, remains a hurdle that traditional methods struggle to overcome. In this research, We explore the effectiveness of conditional tabular generative adversarial network (CTGAN) as a synthetic data generation method based on a deep learning network, in addressing the challenge of limited training data for minority classes in crop classification using the fusion of SAR-optical data. Our findings demonstrate that the proposed method generates synthetic data with higher quality that can significantly increase the number of samples for minority classes leading to better performance of crop classifiers.</details>
**Abstract_cn:** <details><summary>译文: </summary>近几十年来，利用遥感数据进行作物分类已成为一个重要的研究领域。研究表明，融合SAR和光学图像可以显着提高分类的准确性。然而，该领域的一个主要挑战是训练数据的可用性有限，这会对分类器的性能产生不利影响。在农业地区，主要农作物通常由一种或两种特定类型组成，而其他农作物稀缺。因此，在收集训练样本以创建农产品地图时，有大量来自优势作物的样本，构成了大多数类别。相反，其他作物的样本很少，代表了少数群体。解决这个问题需要克服与传统数据生成方法相关的几个挑战和弱点。这些方法已用于解决训练数据的不平衡性质。尽管如此，他们在有效处理少数群体方面仍面临局限性。总体而言，训练数据不足的问题，特别是少数群体的训练数据，仍然是传统方法难以克服的障碍。在这项研究中，我们探索了条件表格生成对抗网络（CTGAN）作为一种基于深度学习网络的合成数据生成方法的有效性，在使用 SAR-融合解决作物分类中少数类训练数据有限的挑战方面，光学数据。我们的研究结果表明，所提出的方法生成了更高质量的合成数据，可以显着增加少数类别的样本数量，从而提高作物分类器的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02121v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **DiffVein: A Unified Diffusion Network for Finger Vein Segmentation and Authentication**<br />
**Title_cn:** DiffVein：用于指静脉分割和身份验证的统一扩散网络<br />
**Authors:** Yanjun Liu, Wenming Yang, Qingmin Liao<br />
**Abstract:** <details><summary>原文: </summary>Finger vein authentication, recognized for its high security and specificity, has become a focal point in biometric research. Traditional methods predominantly concentrate on vein feature extraction for discriminative modeling, with a limited exploration of generative approaches. Suffering from verification failure, existing methods often fail to obtain authentic vein patterns by segmentation. To fill this gap, we introduce DiffVein, a unified diffusion model-based framework which simultaneously addresses vein segmentation and authentication tasks. DiffVein is composed of two dedicated branches: one for segmentation and the other for denoising. For better feature interaction between these two branches, we introduce two specialized modules to improve their collective performance. The first, a mask condition module, incorporates the semantic information of vein patterns from the segmentation branch into the denoising process. Additionally, we also propose a Semantic Difference Transformer (SD-Former), which employs Fourier-space self-attention and cross-attention modules to extract category embedding before feeding it to the segmentation task. In this way, our framework allows for a dynamic interplay between diffusion and segmentation embeddings, thus vein segmentation and authentication tasks can inform and enhance each other in the joint training. To further optimize our model, we introduce a Fourier-space Structural Similarity (FourierSIM) loss function, which is tailored to improve the denoising network's learning efficacy. Extensive experiments on the USM and THU-MVFV3V datasets substantiates DiffVein's superior performance, setting new benchmarks in both vein segmentation and authentication tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>指静脉认证以其高安全性和特异性而受到认可，已成为生物识别研究的焦点。传统方法主要集中于静脉特征提取以进行判别建模，对生成方法的探索有限。由于验证失败，现有方法常常无法通过分割获得真实的静脉图案。为了填补这一空白，我们引入了 DiffVein，这是一种基于统一扩散模型的框架，可同时解决静脉分割和身份验证任务。 DiffVein 由两个专用分支组成：一个用于分割，另一个用于去噪。为了这两个分支之间更好的功能交互，我们引入了两个专门的模块来提高它们的集体性能。第一个是掩模条件模块，将来自分割分支的静脉图案的语义信息合并到去噪过程中。此外，我们还提出了语义差异变换器（SD-Former），它采用傅立叶空间自注意力和交叉注意力模块来提取类别嵌入，然后将其输入到分割任务。通过这种方式，我们的框架允许扩散和分割嵌入之间的动态相互作用，因此静脉分割和身份验证任务可以在联合训练中相互告知和增强。为了进一步优化我们的模型，我们引入了傅里叶空间结构相似性（FourierSIM）损失函数，该函数是为提高去噪网络的学习效率而定制的。 USM 和 THU-MVFV3V 数据集上的大量实验证实了 DiffVein 的卓越性能，为静脉分割和身份验证任务树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2402.02060v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross Appearance-Edge Learning**<br />
**Title_cn:** GenFace：大规模细粒度人脸伪造基准和交叉外观边缘学习<br />
**Authors:** Yaning Zhang, Zitong Yu, Xiaobin Huang, Linlin Shen, Jianfeng Ren<br />
**Abstract:** <details><summary>原文: </summary>The rapid advancement of photorealistic generators has reached a critical juncture where the discrepancy between authentic and manipulated images is increasingly indistinguishable. Thus, benchmarking and advancing techniques detecting digital manipulation become an urgent issue. Although there have been a number of publicly available face forgery datasets, the forgery faces are mostly generated using GAN-based synthesis technology, which does not involve the most recent technologies like diffusion. The diversity and quality of images generated by diffusion models have been significantly improved and thus a much more challenging face forgery dataset shall be used to evaluate SOTA forgery detection literature. In this paper, we propose a large-scale, diverse, and fine-grained high-fidelity dataset, namely GenFace, to facilitate the advancement of deepfake detection, which contains a large number of forgery faces generated by advanced generators such as the diffusion-based model and more detailed labels about the manipulation approaches and adopted generators. In addition to evaluating SOTA approaches on our benchmark, we design an innovative cross appearance-edge learning (CAEL) detector to capture multi-grained appearance and edge global representations, and detect discriminative and general forgery traces. Moreover, we devise an appearance-edge cross-attention (AECA) module to explore the various integrations across two domains. Extensive experiment results and visualizations show that our detection model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Code and datasets will be available at \url{https://github.com/Jenine-321/GenFace</details>
**Abstract_cn:** <details><summary>译文: </summary>真实感生成器的快速发展已经达到了一个关键时刻，真实图像和经过处理的图像之间的差异越来越难以区分。因此，检测数字操纵的基准和先进技术成为一个紧迫的问题。虽然已经有很多公开的人脸伪造数据集，但伪造的人脸大多是使用基于 GAN 的合成技术生成的，不涉及扩散等最新技术。扩散模型生成的图像的多样性和质量得到了显着提高，因此应使用更具挑战性的人脸伪造数据集来评估 SOTA 伪造检测文献。在本文中，我们提出了一个大规模、多样化、细粒度的高保真数据集，即 GenFace，以促进 Deepfake 检测的进步，其中包含由高级生成器（例如扩散生成器）生成的大量伪造人脸。基于模型和有关操作方法和采用的生成器的更详细标签。除了在我们的基准上评估 SOTA 方法之外，我们还设计了一种创新的交叉外观边缘学习 (CAEL) 检测器来捕获多粒度外观和边缘全局表示，并检测有区别的和一般的伪造痕迹。此外，我们设计了一个外观边缘交叉注意（AECA）模块来探索跨两个领域的各种集成。大量的实验结果和可视化结果表明，我们的检测模型在跨生成器、交叉伪造和跨数据集评估等不同设置上均优于现有技术。代码和数据集可在 \url{https://github.com/Jenine-321/GenFace 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.02003v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Physical Perception Network and an All-weather Multi-modality Benchmark for Adverse Weather Image Fusion**<br />
**Title_cn:** 物理感知网络和恶劣天气图像融合的全天候多模态基准<br />
**Authors:** Xilai Li, Wuyang Liu, Xiaosong Li, Haishu Tan<br />
**Abstract:** <details><summary>原文: </summary>Multi-modality image fusion (MMIF) integrates the complementary information from different modal images to provide comprehensive and objective interpretation of a scenes. However, existing MMIF methods lack the ability to resist different weather interferences in real-life scenarios, preventing them from being useful in practical applications such as autonomous driving. To bridge this research gap, we proposed an all-weather MMIF model. Regarding deep learning architectures, their network designs are often viewed as a black box, which limits their multitasking capabilities. For deweathering module, we propose a physically-aware clear feature prediction module based on an atmospheric scattering model that can deduce variations in light transmittance from both scene illumination and depth. For fusion module, We utilize a learnable low-rank representation model to decompose images into low-rank and sparse components. This highly interpretable feature separation allows us to better observe and understand images. Furthermore, we have established a benchmark for MMIF research under extreme weather conditions. It encompasses multiple scenes under three types of weather: rain, haze, and snow, with each weather condition further subdivided into various impact levels. Extensive fusion experiments under adverse weather demonstrate that the proposed algorithm has excellent detail recovery and multi-modality feature extraction capabilities.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态图像融合（MMIF）集成了来自不同模态图像的互补信息，以提供对场景的全面、客观的解释。然而，现有的MMIF方法缺乏抵抗现实场景中不同天气干扰的能力，无法在自动驾驶等实际应用中发挥作用。为了弥补这一研究空白，我们提出了全天候 MMIF 模型。对于深度学习架构，它们的网络设计通常被视为黑匣子，这限制了它们的多任务处理能力。对于去风化模块，我们提出了一种基于大气散射模型的物理感知清晰特征预测模块，该模块可以从场景照明和深度中推断出透光率的变化。对于融合模块，我们利用可学习的低秩表示模型将图像分解为低秩和稀疏组件。这种高度可解释的特征分离使我们能够更好地观察和理解图像。此外，我们还为极端天气条件下的 MMIF 研究建立了基准。它包含雨、雾、雪三种天气下的多个场景，每种天气条件进一步细分为不同的影响级别。恶劣天气下的大量融合实验表明，该算法具有出色的细节恢复和多模态特征提取能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02090v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **RIDERS: Radar-Infrared Depth Estimation for Robust Sensing**<br />
**Title_cn:** RIDERS：用于稳健传感的雷达红外深度估计<br />
**Authors:** Han Li, Yukai Ma, Yuehao Huang, Yaqing Gu, Weihua Xu, Yong Liu, Xingxing Zuo<br />
**Abstract:** <details><summary>原文: </summary>Dense depth recovery is crucial in autonomous driving, serving as a foundational element for obstacle avoidance, 3D object detection, and local path planning. Adverse weather conditions, including haze, dust, rain, snow, and darkness, introduce significant challenges to accurate dense depth estimation, thereby posing substantial safety risks in autonomous driving. These challenges are particularly pronounced for traditional depth estimation methods that rely on short electromagnetic wave sensors, such as visible spectrum cameras and near-infrared LiDAR, due to their susceptibility to diffraction noise and occlusion in such environments. To fundamentally overcome this issue, we present a novel approach for robust metric depth estimation by fusing a millimeter-wave Radar and a monocular infrared thermal camera, which are capable of penetrating atmospheric particles and unaffected by lighting conditions. Our proposed Radar-Infrared fusion method achieves highly accurate and finely detailed dense depth estimation through three stages, including monocular depth prediction with global scale alignment, quasi-dense Radar augmentation by learning Radar-pixels correspondences, and local scale refinement of dense depth using a scale map learner. Our method achieves exceptional visual quality and accurate metric estimation by addressing the challenges of ambiguity and misalignment that arise from directly fusing multi-modal long-wave features. We evaluate the performance of our approach on the NTU4DRadLM dataset and our self-collected challenging ZJU-Multispectrum dataset. Especially noteworthy is the unprecedented robustness demonstrated by our proposed method in smoky scenarios. Our code will be released at \url{https://github.com/MMOCKING/RIDERS}.</details>
**Abstract_cn:** <details><summary>译文: </summary>密集深度恢复对于自动驾驶至关重要，是避障、3D 物体检测和局部路径规划的基础要素。雾霾、灰尘、雨雪、黑暗等恶劣天气条件给精确的密集深度估计带来了重大挑战，从而给自动驾驶带来了巨大的安全风险。对于依赖短电磁波传感器的传统深度估计方法（例如可见光谱相机和近红外激光雷达）来说，这些挑战尤其明显，因为它们在此类环境中容易受到衍射噪声和遮挡的影响。为了从根本上解决这个问题，我们提出了一种通过融合毫米波雷达和单目红外热像仪来进行鲁棒的度量深度估计的新方法，它们能够穿透大气颗粒并且不受照明条件的影响。我们提出的雷达-红外融合方法通过三个阶段实现了高精度和精细的密集深度估计，包括具有全局尺度对齐的单目深度预测、通过学习雷达像素对应关系的准密集雷达增强以及使用密集深度的局部尺度细化。比例尺地图学习者。我们的方法通过解决直接融合多模态长波特征所带来的模糊性和错位的挑战，实现了卓越的视觉质量和准确的度量估计。我们评估了我们的方法在 NTU4DRadLM 数据集和我们自行收集的具有挑战性的 ZJU-Multispectrum 数据集上的性能。特别值得注意的是我们提出的方法在烟雾场景中表现出前所未有的鲁棒性。我们的代码将在 \url{https://github.com/MMOCKING/RIDERS} 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.02067v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning**<br />
**Title_cn:** MLIP：通过发散编码器和知识引导的对比学习增强医学视觉表示<br />
**Authors:** Zhe Li, Laurence T. Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, Stan Z. Li<br />
**Abstract:** <details><summary>原文: </summary>The scarcity of annotated data has sparked significant interest in unsupervised pre-training methods that leverage medical reports as auxiliary signals for medical visual representation learning. However, existing research overlooks the multi-granularity nature of medical visual representation and lacks suitable contrastive learning techniques to improve the models' generalizability across different granularities, leading to the underutilization of image-text information. To address this, we propose MLIP, a novel framework leveraging domain-specific medical knowledge as guiding signals to integrate language information into the visual domain through image-text contrastive learning. Our model includes global contrastive learning with our designed divergence encoder, local token-knowledge-patch alignment contrastive learning, and knowledge-guided category-level contrastive learning with expert knowledge. Experimental evaluations reveal the efficacy of our model in enhancing transfer performance for tasks such as image classification, object detection, and semantic segmentation. Notably, MLIP surpasses state-of-the-art methods even with limited annotated data, highlighting the potential of multimodal pre-training in advancing medical representation learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>带注释数据的稀缺引发了人们对无监督预训练方法的极大兴趣，这些方法利用医疗报告作为医学视觉表示学习的辅助信号。然而，现有研究忽视了医学视觉表示的多粒度性质，并且缺乏合适的对比学习技术来提高模型在不同粒度上的泛化性，导致图像文本信息的利用不足。为了解决这个问题，我们提出了 MLIP，这是一种利用特定领域的医学知识作为指导信号，通过图像文本对比学习将语言信息整合到视觉领域的新颖框架。我们的模型包括使用我们设计的散度编码器进行的全局对比学习、本地令牌知识补丁对齐对比学习以及使用专家知识进行知识引导的类别级对比学习。实验评估揭示了我们的模型在增强图像分类、对象检测和语义分割等任务的传输性能方面的功效。值得注意的是，即使注释数据有限，MLIP 也超越了最先进的方法，凸显了多模态预训练在推进医学表征学习方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02045v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving**<br />
**Title_cn:** 用于自动驾驶中极端情况检测的多模态增强对象学习器<br />
**Authors:** Lixing Xiao, Ruixiao Shi, Xiaoyang Tang, Yi Zhou<br />
**Abstract:** <details><summary>原文: </summary>Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MENOL outperforms the baseline ORE by 71.3% and 60.6%, respectively. The code will be available at https://github.com/tryhiseyyysum/MENOL.</details>
**Abstract_cn:** <details><summary>译文: </summary>先前的目标检测工作在封闭场景中取得了较高的准确率，但在开放场景中的表现却不尽如人意。具有挑战性的开放世界问题之一是自动驾驶中的极端情况检测。现有的检测器在处理这些情况时遇到了困难，严重依赖视觉外观并且泛化能力较差。在本文中，我们提出了一种解决方案，通过减少已知类和未知类之间的差异，并引入多模态增强的对象概念学习器。利用以视觉为中心和图像文本模式，我们的半监督学习框架将客观性知识传授给学生模型，从而实现类感知检测。我们的方法，用于角点案例检测的多模态增强对象学习器（MENOL），可以以较低的训练成本显着提高新类的召回率。通过仅使用 5100 个标记训练图像在 CODA-val 数据集上实现 76.6% 的 mAR 角点和 79.8% 的 mAR 不可知性，MENOL 的性能分别优于基线 ORE 71.3% 和 60.6%。该代码可在 https://github.com/tryhiseyyysum/MENOL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02026v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation**<br />
**Title_cn:** S-NeRF++：通过神经重建和生成进行自动驾驶模拟<br />
**Authors:** Yurui Chen, Junge Zhang, Ziyang Xie, Wenye Li, Feihu Zhang, Jiachen Lu, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>Autonomous driving simulation system plays a crucial role in enhancing self-driving data and simulating complex and rare traffic scenarios, ensuring navigation safety. However, traditional simulation systems, which often heavily rely on manual modeling and 2D image editing, struggled with scaling to extensive scenes and generating realistic simulation data. In this study, we present S-NeRF++, an innovative autonomous driving simulation system based on neural reconstruction. Trained on widely-used self-driving datasets such as nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street scenes and foreground objects with high rendering quality as well as offering considerable flexibility in manipulation and simulation. Specifically, S-NeRF++ is an enhanced neural radiance field for synthesizing large-scale scenes and moving vehicles, with improved scene parameterization and camera pose learning. The system effectively utilizes noisy and sparse LiDAR data to refine training and address depth outliers, ensuring high quality reconstruction and novel-view rendering. It also provides a diverse foreground asset bank through reconstructing and generating different foreground vehicles to support comprehensive scenario creation. Moreover, we have developed an advanced foreground-background fusion pipeline that skillfully integrates illumination and shadow effects, further enhancing the realism of our simulations. With the high-quality simulated data provided by our S-NeRF++, we found the perception methods enjoy performance boost on several autonomous driving downstream tasks, which further demonstrate the effectiveness of our proposed simulator.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶仿真系统对于增强自动驾驶数据、模拟复杂、罕见的交通场景、保障导航安全起着至关重要的作用。然而，传统的仿真系统通常严重依赖手动建模和 2D 图像编辑，难以扩展到广泛的场景并生成真实的仿真数据。在这项研究中，我们提出了 S-NeRF++，一种基于神经重构的创新自动驾驶模拟系统。 S-NeRF++ 在 nuScenes 和 Waymo 等广泛使用的自动驾驶数据集上进行训练，可以生成大量具有高渲染质量的逼真街道场景和前景物体，并在操作和模拟方面提供相当大的灵活性。具体来说，S-NeRF++ 是一种增强的神经辐射场，用于合成大规模场景和移动车辆，并改进了场景参数化和相机姿态学习。该系统有效地利用噪声和稀疏的激光雷达数据来改进训练并解决深度异常值，确保高质量的重建和新颖的视图渲染。并通过重构生成不同的前台载体，提供多元化的前台资产库，支持综合场景打造。此外，我们还开发了先进的前景-背景融合管道，巧妙地集成了照明和阴影效果，进一步增强了模拟的真实感。通过我们的 S-NeRF++ 提供的高质量模拟数据，我们发现感知方法在几个自动驾驶下游任务上的性能得到提升，这进一步证明了我们提出的模拟器的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02112v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **ParZC: Parametric Zero-Cost Proxies for Efficient NAS**<br />
**Title_cn:** ParZC：高效 NAS 的参数化零成本代理<br />
**Authors:** Peijie Dong, Lujun Li, Xinglin Pan, Zimian Wei, Xiang Liu, Qiang Wang, Xiaowen Chu<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in Zero-shot Neural Architecture Search (NAS) highlight the efficacy of zero-cost proxies in various NAS benchmarks. Several studies propose the automated design of zero-cost proxies to achieve SOTA performance but require tedious searching progress. Furthermore, we identify a critical issue with current zero-cost proxies: they aggregate node-wise zero-cost statistics without considering the fact that not all nodes in a neural network equally impact performance estimation. Our observations reveal that node-wise zero-cost statistics significantly vary in their contributions to performance, with each node exhibiting a degree of uncertainty. Based on this insight, we introduce a novel method called Parametric Zero-Cost Proxies (ParZC) framework to enhance the adaptability of zero-cost proxies through parameterization. To address the node indiscrimination, we propose a Mixer Architecture with Bayesian Network (MABN) to explore the node-wise zero-cost statistics and estimate node-specific uncertainty. Moreover, we propose DiffKendall as a loss function to directly optimize Kendall's Tau coefficient in a differentiable manner so that our ParZC can better handle the discrepancies in ranking architectures. Comprehensive experiments on NAS-Bench-101, 201, and NDS demonstrate the superiority of our proposed ParZC compared to existing zero-shot NAS methods. Additionally, we demonstrate the versatility and adaptability of ParZC by transferring it to the Vision Transformer search space.</details>
**Abstract_cn:** <details><summary>译文: </summary>零样本神经架构搜索 (NAS) 的最新进展凸显了零成本代理在各种 NAS 基准测试中的功效。一些研究提出了零成本代理的自动化设计来实现 SOTA 性能，但需要繁琐的搜索过程。此外，我们还发现了当前零成本代理的一个关键问题：它们聚合了节点级的零成本统计数据，而没有考虑到神经网络中并非所有节点都会同等影响性能估计这一事实。我们的观察表明，节点式零成本统计数据对性能的贡献差异很大，每个节点都表现出一定程度的不确定性。基于这种见解，我们引入了一种称为参数化零成本代理（ParZC）框架的新颖方法，通过参数化来增强零成本代理的适应性。为了解决节点不歧视问题，我们提出了一种带有贝叶斯网络（MABN）的混合器架构来探索节点零成本统计并估计特定于节点的不确定性。此外，我们提出 DiffKendall 作为损失函数，以可微的方式直接优化 Kendall 的 Tau 系数，以便我们的 ParZC 可以更好地处理排名架构中的差异。在 NAS-Bench-101、201 和 NDS 上的综合实验证明了我们提出的 ParZC 与现有零样本 NAS 方法相比的优越性。此外，我们通过将 ParZC 转移到 Vision Transformer 搜索空间来展示 ParZC 的多功能性和适应性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02105v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Precise Knowledge Transfer via Flow Matching**<br />
**Title_cn:** 通过流程匹配实现精准知识传递<br />
**Authors:** Shitong Shao, Zhiqiang Shen, Linrui Gong, Huanran Chen, Xu Dai<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose a novel knowledge transfer framework that introduces continuous normalizing flows for progressive knowledge transformation and leverages multi-step sampling strategies to achieve precision knowledge transfer. We name this framework Knowledge Transfer with Flow Matching (FM-KT), which can be integrated with a metric-based distillation method with any form (\textit{e.g.} vanilla KD, DKD, PKD and DIST) and a meta-encoder with any available architecture (\textit{e.g.} CNN, MLP and Transformer). By introducing stochastic interpolants, FM-KD is readily amenable to arbitrary noise schedules (\textit{e.g.}, VP-ODE, VE-ODE, Rectified flow) for normalized flow path estimation. We theoretically demonstrate that the training objective of FM-KT is equivalent to minimizing the upper bound of the teacher feature map or logit negative log-likelihood. Besides, FM-KT can be viewed as a unique implicit ensemble method that leads to performance gains. By slightly modifying the FM-KT framework, FM-KT can also be transformed into an online distillation framework OFM-KT with desirable performance gains. Through extensive experiments on CIFAR-100, ImageNet-1k, and MS-COCO datasets, we empirically validate the scalability and state-of-the-art performance of our proposed methods among relevant comparison approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新颖的知识转移框架，该框架引入了用于渐进式知识转换的连续规范化流，并利用多步采样策略来实现精确的知识转移。我们将这个框架命名为流匹配知识转移（FM-KT），它可以与任何形式的基于度量的蒸馏方法（\textit{例如} vanilla KD、DKD、PKD 和 DIST）以及元编码器集成任何可用的架构（\textit{例如 CNN、MLP 和 Transformer）。通过引入随机插值，FM-KD 很容易适应任意噪声表（\textit{e.g.}、VP-ODE、VE-ODE、整流流），以进行归一化流路径估计。我们从理论上证明，FM-KT 的训练目标相当于最小化教师特征图的上限或 logit 负对数似然。此外，FM-KT 可以被视为一种独特的隐式集成方法，可以带来性能提升。通过稍微修改 FM-KT 框架，FM-KT 还可以转变为在线蒸馏框架 OFM-KT，并具有理想的性能提升。通过对 CIFAR-100、ImageNet-1k 和 MS-COCO 数据集进行广泛的实验，我们凭经验验证了我们提出的方法在相关比较方法中的可扩展性和最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02012v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Polyp-DAM: Polyp segmentation via depth anything model**<br />
**Title_cn:** Polyp-DAM：通过深度任何模型进行息肉分割<br />
**Authors:** Zhuoran Zheng, Chen Wu, Wei Wang, Yeying Jin, Xiuyi Jia<br />
**Abstract:** <details><summary>原文: </summary>Recently, large models (Segment Anything model) came on the scene to provide a new baseline for polyp segmentation tasks. This demonstrates that large models with a sufficient image level prior can achieve promising performance on a given task. In this paper, we unfold a new perspective on polyp segmentation modeling by leveraging the Depth Anything Model (DAM) to provide depth prior to polyp segmentation models. Specifically, the input polyp image is first passed through a frozen DAM to generate a depth map. The depth map and the input polyp images are then concatenated and fed into a convolutional neural network with multiscale to generate segmented images. Extensive experimental results demonstrate the effectiveness of our method, and in addition, we observe that our method still performs well on images of polyps with noise. The URL of our code is \url{https://github.com/zzr-idam/Polyp-DAM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大型模型（Segment Anything 模型）登场，为息肉分割任务提供了新的基线。这表明，具有足够图像级别先验的大型模型可以在给定任务上实现有希望的性能。在本文中，我们利用深度任意模型（DAM）在息肉分割模型之前提供深度，从而展现了息肉分割建模的新视角。具体来说，输入的息肉图像首先通过冻结的 DAM 以生成深度图。然后将深度图和输入息肉图像连接起来并输入到具有多尺度的卷积神经网络中以生成分段图像。大量的实验结果证明了我们的方法的有效性，此外，我们观察到我们的方法在带有噪声的息肉图像上仍然表现良好。我们代码的 URL 是 \url{https://github.com/zzr-idam/Polyp-DAM}。</details>
**PDF:** <http://arxiv.org/pdf/2402.02298v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **$\textit{A Contrario}$ Paradigm for YOLO-based Infrared Small Target Detection**<br />
**Title_cn:** $\textit{A Contrario}$ 基于 YOLO 的红外小目标检测范式<br />
**Authors:** Alina Ciocarlan, Sylvie Le Hégarat-Mascle, Sidonie Lefebvre, Arnaud Woiselle, Clara Barbanson<br />
**Abstract:** <details><summary>原文: </summary>Detecting small to tiny targets in infrared images is a challenging task in computer vision, especially when it comes to differentiating these targets from noisy or textured backgrounds. Traditional object detection methods such as YOLO struggle to detect tiny objects compared to segmentation neural networks, resulting in weaker performance when detecting small targets. To reduce the number of false alarms while maintaining a high detection rate, we introduce an $\textit{a contrario}$ decision criterion into the training of a YOLO detector. The latter takes advantage of the $\textit{unexpectedness}$ of small targets to discriminate them from complex backgrounds. Adding this statistical criterion to a YOLOv7-tiny bridges the performance gap between state-of-the-art segmentation methods for infrared small target detection and object detection networks. It also significantly increases the robustness of YOLO towards few-shot settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测红外图像中的小到微小目标是计算机视觉中的一项具有挑战性的任务，特别是在将这些目标与噪声或纹理背景区分开来时。与分割神经网络相比，YOLO 等传统目标检测方法难以检测微小目标，导致检测小目标时性能较差。为了减少误报的数量，同时保持高检测率，我们在 YOLO 检测器的训练中引入了 $\textit{a contrario}$ 决策标准。后者利用小目标的$\textit{unexpectedness}$来将它们与复杂的背景区分开来。将此统计标准添加到 YOLOv7-tiny 中可以弥补红外小目标检测和物体检测网络的最先进分割方法之间的性能差距。它还显着提高了 YOLO 对少样本设置的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02288v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation**<br />
**Title_cn:** 用于实时语义分割的多级特征聚合和递归对齐网络<br />
**Authors:** Yanhua Zhang, Ke Zhang, Jingyu Wang, Yulin Wu, Wuwei Wang<br />
**Abstract:** <details><summary>原文: </summary>Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignment module with recursive upsampling architecture for accurate and efficient spatial alignment between multi-scale score maps. Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to adaptively fuse multi-scale scores so that the final prediction can favor objects of multiple scales. Comprehensive experiments on three benchmark datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness and efficiency of our method. In particular, we achieve a better balance between speed and accuracy than state-of-the-art real-time methods on Cityscapes and CamVid datasets. Code is available at: https://github.com/Yanhua-Zhang/MFARANet.</details>
**Abstract_cn:** <details><summary>译文: </summary>实时语义分割是现实世界应用的一项重要研究。然而，许多方法特别注重降低计算复杂度和模型大小，同时很大程度上牺牲了准确性。在某些场景中，例如自动导航和驾驶辅助系统，准确性和速度同样重要。为了解决这个问题，我们提出了一种新颖的多级特征聚合和递归对齐网络（MFARANet），旨在以实时推理速度实现高分割精度。我们采用 ResNet-18 作为主干以确保效率，并提出三个核心组件来补偿由于主干较浅而导致的模型容量降低。具体来说，我们首先设计多级特征聚合模块（MFAM），将编码器中的分层特征聚合到每个尺度，以利于后续的空间对齐和多尺度推理。然后，我们通过将基于流的对齐模块与递归上采样架构相结合来构建递归对齐模块（RAM），以实现多尺度得分图之间准确有效的空间对齐。最后，提出了自适应分数融合模块（ASFM）来自适应地融合多尺度分数，以便最终的预测可以有利于多尺度的对象。对 Cityscapes、CamVid 和 PASCAL-Context 等三个基准数据集的综合实验表明了我们方法的有效性和效率。特别是，与 Cityscapes 和 CamVid 数据集上最先进的实时方法相比，我们在速度和准确性之间实现了更好的平衡。代码位于：https://github.com/Yanhua-Zhang/MFARANet。</details>
**PDF:** <http://arxiv.org/pdf/2402.02286v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification**<br />
**Title_cn:** InceptionCapsule：用于医学图像分类的具有自注意力的 Inception-Resnet 和 CapsuleNet<br />
**Authors:** Elham Sadeghnezhad, Sajjad Salem<br />
**Abstract:** <details><summary>原文: </summary>Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are acceptable results compared to other approaches in the literature.</details>
**Abstract_cn:** <details><summary>译文: </summary>初始权重在深度神经网络中非常重要，因为权重的随机选择会产生不同的输出并增加过度拟合和欠拟合的可能性。另一方面，基于向量的方法提取向量特征需要丰富的向量才能进行更准确的分类。 InceptionCapsule 方法的出现就是为了缓解这两个问题。该方法使用迁移学习和 Inception-ResNet 模型来避免随机选择权重，该权重从 ImageNet 中获取初始权重。它还使用 Inception 中间层的输出来生成丰富的向量。提取的向量被赋予胶囊网络进行学习，该网络配备了注意力技术。 Kvasir 数据和带有 GT 数据集的 BUSI 用于评估这种方法。该模型在 Kvasir 上的 5 类分类准确率达到 97.62，8 类分类准确率达到 94.30。在带有 GT 的 BUSI 数据集中，所提出的方法实现了准确度 = 98.88、精度 = 95.34 和 F1-score = 93.74，与文献中的其他方法相比，这是可以接受的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.02274v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers**<br />
**Title_cn:** MixedNUTS：通过非线性混合分类器实现免训练精度-鲁棒性平衡<br />
**Authors:** Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi<br />
**Abstract:** <details><summary>原文: </summary>Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points, sacrificing merely 0.87 points in robust accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性的鲁棒性通常以准确性下降为代价，阻碍了鲁棒分类模型的实际应用。为了更好地权衡而基于训练的解决方案受到与已训练的高性能大型模型的不兼容性的限制，因此需要探索免训练的集成方法。观察到鲁棒模型对干净数据和对抗性数据的正确预测比错误预测更有信心，我们推测放大这种“良性置信属性”可以协调集成环境中的准确性和鲁棒性。为了实现这一目标，我们提出了“MixedNUTS”，这是一种免训练方法，其中鲁棒分类器和标准非鲁棒分类器的输出逻辑通过仅三个参数的非线性变换进行处理，并通过有效的算法进行优化。然后 MixedNUTS 将转换后的 logits 转换为概率并将它们混合作为总体输出。在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上，使用自定义强自适应攻击的实验结果表明，MixedNUTS 大大提高了准确性和接近 SOTA 的鲁棒性——它将 CIFAR-100 的干净精度提高了 7.86 点，而鲁棒精度仅牺牲了 0.87 点。</details>
**PDF:** <http://arxiv.org/pdf/2402.02263v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images**<br />
**Title_cn:** ExTTNet：一种从发票图像中提取表格文本的深度学习算法<br />
**Authors:** Adem Akdoğan, Murat Kurt<br />
**Abstract:** <details><summary>原文: </summary>In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，发票中的产品表是通过深度学习模型自主获取的，该模型被命名为 ExTTNet。首先，使用光学字符识别 (OCR) 技术从发票图像中获取文本。 Tesseract OCR 引擎 [37] 用于此过程。然后，通过使用特征提取方法来增加现有特征的数量以提高准确性。根据OCR结果获得的每个文本是否是表格元素来完成标记过程。在本研究中，使用了多层人工神经网络模型。训练使用 Nvidia RTX 3090 显卡进行，耗时 162 美元分钟。训练的结果是，F1 分数为 0.92 美元。</details>
**PDF:** <http://arxiv.org/pdf/2402.02246v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Image Fusion via Vision-Language Model**<br />
**Title_cn:** 通过视觉语言模型进行图像融合<br />
**Authors:** Zixiang Zhao, Lilun Deng, Haowen Bai, Yukun Cui, Zhipeng Zhang, Yulun Zhang, Haotong Qin, Dongdong Chen, Jiangshe Zhang, Peng Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Image fusion integrates essential information from multiple source images into a single composite, emphasizing the highlighting structure and textures, and refining imperfect areas. Existing methods predominantly focus on pixel-level and semantic visual features for recognition. However, they insufficiently explore the deeper semantic information at a text-level beyond vision. Therefore, we introduce a novel fusion paradigm named image Fusion via vIsion-Language Model (FILM), for the first time, utilizing explicit textual information in different source images to guide image fusion. In FILM, input images are firstly processed to generate semantic prompts, which are then fed into ChatGPT to obtain rich textual descriptions. These descriptions are fused in the textual domain and guide the extraction of crucial visual features from the source images through cross-attention, resulting in a deeper level of contextual understanding directed by textual semantic information. The final fused image is created by vision feature decoder. This paradigm achieves satisfactory results in four image fusion tasks: infrared-visible, medical, multi-exposure, and multi-focus image fusion. We also propose a vision-language dataset containing ChatGPT-based paragraph descriptions for the ten image fusion datasets in four fusion tasks, facilitating future research in vision-language model-based image fusion. Code and dataset will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像融合将多个源图像的基本信息集成到单个合成图像中，强调突出结构和纹理，并细化不完美的区域。现有方法主要关注像素级和语义视觉特征进行识别。然而，他们在视觉之外的文本层面上探索更深层次的语义信息还不够。因此，我们引入了一种新颖的融合范式，称为通过视觉语言模型进行图像融合（FILM），首次利用不同源图像中的显式文本信息来指导图像融合。在FILM中，输入图像首先被处理以生成语义提示，然后将其输入ChatGPT以获得丰富的文本描述。这些描述在文本域中融合，并通过交叉注意力指导从源图像中提取关键视觉特征，从而产生由文本语义信息指导的更深层次的上下文理解。最终的融合图像由视觉特征解码器创建。该范式在红外-可见光、医学、多重曝光和多焦点图像融合四种图像融合任务中取得了令人满意的结果。我们还提出了一个视觉语言数据集，其中包含四个融合任务中十个图像融合数据集的基于 ChatGPT 的段落描述，以促进基于视觉语言模型的图像融合的未来研究。代码和数据集将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.02235v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **CoFiNet: Unveiling Camouflaged Objects with Multi-Scale Finesse**<br />
**Title_cn:** CoFiNet：通过多尺度技巧揭开伪装物体的面纱<br />
**Authors:** Cunhan Guo, Heyan Huang<br />
**Abstract:** <details><summary>原文: </summary>Camouflaged Object Detection (COD) is a critical aspect of computer vision aimed at identifying concealed objects, with applications spanning military, industrial, medical and monitoring domains. To address the problem of poor detail segmentation effect, we introduce a novel method for camouflage object detection, named CoFiNet. Our approach primarily focuses on multi-scale feature fusion and extraction, with special attention to the model's segmentation effectiveness for detailed features, enhancing its ability to effectively detect camouflaged objects. CoFiNet adopts a coarse-to-fine strategy. A multi-scale feature integration module is laveraged to enhance the model's capability of fusing context feature. A multi-activation selective kernel module is leveraged to grant the model the ability to autonomously alter its receptive field, enabling it to selectively choose an appropriate receptive field for camouflaged objects of different sizes. During mask generation, we employ the dual-mask strategy for image segmentation, separating the reconstruction of coarse and fine masks, which significantly enhances the model's learning capacity for details. Comprehensive experiments were conducted on four different datasets, demonstrating that CoFiNet achieves state-of-the-art performance across all datasets. The experiment results of CoFiNet underscore its effectiveness in camouflage object detection and highlight its potential in various practical application scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>伪装物体检测 (COD) 是计算机视觉的一个重要方面，旨在识别隐藏物体，其应用涵盖军事、工业、医疗和监控领域。为了解决细节分割效果差的问题，我们引入了一种新的伪装目标检测方法，称为 CoFiNet。我们的方法主要关注多尺度特征融合和提取，特别关注模型对细节特征的分割有效性，增强其有效检测伪装物体的能力。 CoFiNet采用由粗到细的策略。利用多尺度特征集成模块来增强模型融合上下文特征的能力。利用多激活选择性内核模块赋予模型自主改变其感受野的能力，使其能够为不同大小的伪装物体选择性地选择合适的感受野。在掩模生成过程中，我们采用双掩模策略进行图像分割，将粗掩模和精细掩模的重建分开，这显着增强了模型对细节的学习能力。在四个不同的数据集上进行了全面的实验，证明 CoFiNet 在所有数据集上都实现了最先进的性能。 CoFiNet的实验结果强调了其在伪装目标检测方面的有效性，并凸显了其在各种实际应用场景中的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02217v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained Skeleton-Based Action Recognition**<br />
**Title_cn:** 用于细粒度骨架动作识别的小波解耦对比增强网络<br />
**Authors:** Haochen Chang, Jing Chen, Yilin Li, Jixiang Chen, Xiaofeng Zhang<br />
**Abstract:** <details><summary>原文: </summary>Skeleton-based action recognition has attracted much attention, benefiting from its succinctness and robustness. However, the minimal inter-class variation in similar action sequences often leads to confusion. The inherent spatiotemporal coupling characteristics make it challenging to mine the subtle differences in joint motion trajectories, which is critical for distinguishing confusing fine-grained actions. To alleviate this problem, we propose a Wavelet-Attention Decoupling (WAD) module that utilizes discrete wavelet transform to effectively disentangle salient and subtle motion features in the time-frequency domain. Then, the decoupling attention adaptively recalibrates their temporal responses. To further amplify the discrepancies in these subtle motion features, we propose a Fine-grained Contrastive Enhancement (FCE) module to enhance attention towards trajectory features by contrastive learning. Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D and the fine-grained dataset FineGYM. Our methods perform competitively compared to state-of-the-art methods and can discriminate confusing fine-grained actions well.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于骨架的动作识别因其简洁性和鲁棒性而备受关注。然而，类似动作序列中最小的类间差异常常会导致混乱。固有的时空耦合特征使得挖掘关节运动轨迹的细微差异变得具有挑战性，这对于区分令人困惑的细粒度动作至关重要。为了缓解这个问题，我们提出了一种小波注意力解耦（WAD）模块，它利用离散小波变换来有效地分离时频域中的显着和微妙的运动特征。然后，解耦注意力自适应地重新校准它们的时间响应。为了进一步放大这些微妙运动特征的差异，我们提出了细粒度对比增强（FCE）模块，通过对比学习来增强对轨迹特征的关注。在粗粒度数据集 NTU RGB+D 和细粒度数据集 FineGYM 上进行了大量的实验。与最先进的方法相比，我们的方法具有竞争力，并且可以很好地区分令人困惑的细粒度操作。</details>
**PDF:** <http://arxiv.org/pdf/2402.02210v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events**<br />
**Title_cn:** GPT-4V 作为交通助手：深入研究复杂交通事件的视觉语言模型<br />
**Authors:** Xingcheng Zhou, Alois C. Knoll<br />
**Abstract:** <details><summary>原文: </summary>The recognition and understanding of traffic incidents, particularly traffic accidents, is a topic of paramount importance in the realm of intelligent transportation systems and intelligent vehicles. This area has continually captured the extensive focus of both the academic and industrial sectors. Identifying and comprehending complex traffic events is highly challenging, primarily due to the intricate nature of traffic environments, diverse observational perspectives, and the multifaceted causes of accidents. These factors have persistently impeded the development of effective solutions. The advent of large vision-language models (VLMs) such as GPT-4V, has introduced innovative approaches to addressing this issue. In this paper, we explore the ability of GPT-4V with a set of representative traffic incident videos and delve into the model's capacity of understanding these complex traffic situations. We observe that GPT-4V demonstrates remarkable cognitive, reasoning, and decision-making ability in certain classic traffic events. Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios. These limitations merit further exploration and resolution.</details>
**Abstract_cn:** <details><summary>译文: </summary>对交通事件，特别是交通事故的识别和理解是智能交通系统和智能车辆领域中最重要的课题。该领域不断引起学术界和工业界的广泛关注。识别和理解复杂的交通事件非常具有挑战性，这主要是由于交通环境的复杂性、观察视角的多样化以及事故原因的多方面性。这些因素一直阻碍有效解决方案的制定。 GPT-4V 等大型视觉语言模型 (VLM) 的出现引入了解决这一问题的创新方法。在本文中，我们通过一组代表性交通事件视频探索了 GPT-4V 的能力，并深入研究了该模型理解这些复杂交通情况的能力。我们观察到GPT-4V在某些经典交通事件中表现出了卓越的认知、推理和决策能力。同时，我们还发现了 GPT-4V 的某些局限性，这限制了它在更复杂的场景中的理解。这些局限性值得进一步探索和解决。</details>
**PDF:** <http://arxiv.org/pdf/2402.02205v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **RecNet: An Invertible Point Cloud Encoding through Range Image Embeddings for Multi-Robot Map Sharing and Reconstruction**<br />
**Title_cn:** RecNet：通过范围图像嵌入进行可逆点云编码，用于多机器人地图共享和重建<br />
**Authors:** Nikolaos Stathoulopoulos, Mario A. V. Saucedo, Anton Koval, George Nikolakopoulos<br />
**Abstract:** <details><summary>原文: </summary>In the field of resource-constrained robots and the need for effective place recognition in multi-robotic systems, this article introduces RecNet, a novel approach that concurrently addresses both challenges. The core of RecNet's methodology involves a transformative process: it projects 3D point clouds into depth images, compresses them using an encoder-decoder framework, and subsequently reconstructs the range image, seamlessly restoring the original point cloud. Additionally, RecNet utilizes the latent vector extracted from this process for efficient place recognition tasks. This unique approach not only achieves comparable place recognition results but also maintains a compact representation, suitable for seamless sharing among robots to reconstruct their collective maps. The evaluation of RecNet encompasses an array of metrics, including place recognition performance, structural similarity of the reconstructed point clouds, and the bandwidth transmission advantages, derived from sharing only the latent vectors. This reconstructed map paves a groundbreaking way for exploring its usability in navigation, localization, map-merging, and other relevant missions. Our proposed approach is rigorously assessed using both a publicly available dataset and field experiments, confirming its efficacy and potential for real-world applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>在资源受限的机器人领域以及多机器人系统中有效位置识别的需求中，本文介绍了 RecNet，这是一种同时解决这两个挑战的新颖方法。 RecNet方法的核心涉及一个变革过程：它将3D点云投影为深度图像，使用编码器-解码器框架对其进行压缩，随后重建距离图像，无缝地恢复原始点云。此外，RecNet 利用从该过程中提取的潜在向量来执行高效的地点识别任务。这种独特的方法不仅实现了可比较的地点识别结果，而且保持了紧凑的表示，适合机器人之间的无缝共享以重建其集体地图。 RecNet 的评估涵盖了一系列指标，包括位置识别性能、重建点云的结构相似性以及仅共享潜在向量得出的带宽传输优势。这张重建的地图为探索其在导航、定位、地图合并和其他相关任务中的可用性铺平了道路。我们提出的方法使用公开数据集和现场实验进行了严格评估，证实了其有效性和实际应用的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02192v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Detecting Respiratory Pathologies Using Convolutional Neural Networks and Variational Autoencoders for Unbalancing Data**<br />
**Title_cn:** 使用卷积神经网络和变分自动编码器检测不平衡数据的呼吸病理学<br />
**Authors:** María Teresa García-Ordás, José Alberto Benítez-Andrades, Isaías García-Rodríguez, Carmen Benavides, Héctor Alaiz-Moretón<br />
**Abstract:** <details><summary>原文: </summary>The aim of this paper was the detection of pathologies through respiratory sounds. The ICBHI (International Conference on Biomedical and Health Informatics) Benchmark was used. This dataset is composed of 920 sounds of which 810 are of chronic diseases, 75 of non-chronic diseases and only 35 of healthy individuals. As more than 88% of the samples of the dataset are from the same class (Chronic), the use of a Variational Convolutional Autoencoder was proposed to generate new labeled data and other well known oversampling techniques after determining that the dataset classes are unbalanced. Once the preprocessing step was carried out, a Convolutional Neural Network (CNN) was used to classify the respiratory sounds into healthy, chronic, and non-chronic disease. In addition, we carried out a more challenging classification trying to distinguish between the different types of pathologies or healthy: URTI, COPD, Bronchiectasis, Pneumonia, and Bronchiolitis. We achieved results up to 0.993 F-Score in the three-label classification and 0.990 F-Score in the more challenging six-class classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文的目的是通过呼吸音检测病理。使用 ICBHI（国际生物医学和健康信息学会议）基准。该数据集由 920 个声音组成，其中 810 个是慢性疾病的声音，75 个是非慢性疾病的声音，只有 35 个是健康个体的声音。由于数据集超过 88% 的样本来自同一类别（慢性），因此在确定数据集类别不平衡后，提出使用变分卷积自动编码器来生成新的标记数据和其他众所周知的过采样技术。执行预处理步骤后，使用卷积神经网络（CNN）将呼吸音分类为健康、慢性和非慢性疾病。此外，我们进行了更具挑战性的分类，试图区分不同类型的病理或健康：上呼吸道感染、慢性阻塞性肺病、支气管扩张、肺炎和细支气管炎。我们在三标签分类中取得了高达 0.993 F 分数的结果，在更具挑战性的六类分类中取得了高达 0.990 F 分数的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.02183v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric analysis**<br />
**Title_cn:** 评估越野自动驾驶细分对抗对抗性攻击的鲁棒性：以数据集为中心的分析<br />
**Authors:** Pankaj Deoli, Rohit Kumar, Axel Vierling, Karsten Berns<br />
**Abstract:** <details><summary>原文: </summary>This study investigates the vulnerability of semantic segmentation models to adversarial input perturbations, in the domain of off- road autonomous driving. Despite good performance in generic conditions, the state-of-the-art classifiers are often susceptible to (even) small perturbations, ultimately resulting in inaccurate predic- tions with high confidence. Prior research has directed their focus on making models more robust by modifying the architecture and training with noisy input images, but has not explored the influence of datasets in adversarial attacks. Our study aims to address this gap by examining the impact of non-robust features in off-road datasets and comparing the effects of adversarial attacks on different seg- mentation network architectures. To enable this, a robust dataset is created consisting of only robust features and training the net- works on this robustified dataset. We present both qualitative and quantitative analysis of our findings, which have important impli- cations on improving the robustness of machine learning models in off-road autonomous driving applications. Additionally, this work contributes to the safe navigation of autonomous robot Unimog U5023 in rough off-road unstructured environments by evaluating the robustness of segmentation outputs. The code is publicly avail- able at https:// github.com/ rohtkumar/ adversarial_attacks_ on_segmentation</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究研究了越野自动驾驶领域语义分割模型对对抗性输入扰动的脆弱性。尽管在一般条件下具有良好的性能，但最先进的分类器通常容易受到（甚至）小的扰动，最终导致高置信度的不准确预测。先前的研究重点是通过修改架构和使用噪声输入图像进行训练来使模型更加稳健，但没有探索数据集在对抗性攻击中的影响。我们的研究旨在通过检查越野数据集中非鲁棒特征的影响并比较对抗性攻击对不同分段网络架构的影响来解决这一差距。为了实现这一点，创建了一个仅包含鲁棒特征的鲁棒数据集，并在此鲁棒数据集上训练网络。我们对研究结果进行了定性和定量分析，这对于提高越野自动驾驶应用中机器学习模型的稳健性具有重要意义。此外，这项工作通过评估分割输出的稳健性，有助于自主机器人 Unimog U5023 在恶劣的越野非结构化环境中的安全导航。该代码可在 https://github.com/rohtkumar/adversarial_attacks_on_segmentation 上公开获取</details>
**PDF:** <http://arxiv.org/pdf/2402.02154v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models**<br />
**Title_cn:** 采用混合分类回归模型的数据驱动地震烈度分布预测<br />
**Authors:** Koyu Mizutani, Haruki Mitarai, Kakeru Miyazaki, Soichiro Kumano, Toshihiko Yamasaki<br />
**Abstract:** <details><summary>原文: </summary>Earthquakes are among the most immediate and deadly natural disasters that humans face. Accurately forecasting the extent of earthquake damage and assessing potential risks can be instrumental in saving numerous lives. In this study, we developed linear regression models capable of predicting seismic intensity distributions based on earthquake parameters: location, depth, and magnitude. Because it is completely data-driven, it can predict intensity distributions without geographical information. The dataset comprises seismic intensity data from earthquakes that occurred in the vicinity of Japan between 1997 and 2020, specifically containing 1,857 instances of earthquakes with a magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We trained both regression and classification models and combined them to take advantage of both to create a hybrid model. The proposed model outperformed commonly used Ground Motion Prediction Equations (GMPEs) in terms of the correlation coefficient, F1 score, and MCC. Furthermore, the proposed model can predict even abnormal seismic intensity distributions, a task at conventional GMPEs often struggle.</details>
**Abstract_cn:** <details><summary>译文: </summary>地震是人类面临的最直接、最致命的自然灾害之一。准确预测地震破坏程度并评估潜在风险有助于挽救无数生命。在这项研究中，我们开发了线性回归模型，能够根据地震参数（位置、深度和震级）预测地震强度分布。由于它完全由数据驱动，因此无需地理信息即可预测强度分布。该数据集包含 1997 年至 2020 年间在日本附近发生的地震的地震烈度数据，具体包含来自日本气象厅的 1,857 起 5.0 级或以上的地震。我们训练了回归模型和分类模型，并将它们结合起来，利用两者的优势来创建混合模型。所提出的模型在相关系数、F1 分数和 MCC 方面优于常用的地面运动预测方程 (GMPE)。此外，所提出的模型甚至可以预测异常的地震强度分布，而这是传统 GMPE 中经常面临的一项任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.02150v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Déjà Vu Memorization in Vision-Language Models**<br />
**Title_cn:** 视觉语言模型中的似曾相识记忆<br />
**Authors:** Bargav Jayaraman, Chuan Guo, Kamalika Chaudhuri<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\'ej\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\'ej\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型 (VLM) 已成为最先进的表示学习解决方案，具有无数的下游应用，例如图像分类、检索和生成。一个自然的问题是这些模型是否记住它们的训练数据，这对泛化也有影响。我们提出了一种测量 VLM 记忆的新方法，我们称之为 d\'ej\`a vu 记忆。对于在图像-标题对上训练的 VLM，我们表明该模型确实保留了训练图像中各个对象的信息，超出了从相关性或图像标题中推断出的信息。我们在样本和总体水平上评估了视觉记忆，并表明这对于在多达 5000 万个图像标题对上训练的 OpenCLIP 具有重要意义。最后，我们表明文本随机化极大地减轻了记忆，同时仅适度影响模型的下游任务性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02103v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Decomposition-based and Interference Perception for Infrared and Visible Image Fusion in Complex Scenes**<br />
**Title_cn:** 复杂场景中红外和可见光图像融合的基于分解和干涉感知<br />
**Authors:** Xilai Li, Xiaosong Li, Haishu Tan<br />
**Abstract:** <details><summary>原文: </summary>Infrared and visible image fusion has emerged as a prominent research in computer vision. However, little attention has been paid on complex scenes fusion, causing existing techniques to produce sub-optimal results when suffers from real interferences. To fill this gap, we propose a decomposition-based and interference perception image fusion method. Specifically, we classify the pixels of visible image from the degree of scattering of light transmission, based on which we then separate the detail and energy information of the image. This refined decomposition facilitates the proposed model in identifying more interfering pixels that are in complex scenes. To strike a balance between denoising and detail preservation, we propose an adaptive denoising scheme for fusing detail components. Meanwhile, we propose a new weighted fusion rule by considering the distribution of image energy information from the perspective of multiple directions. Extensive experiments in complex scenes fusions cover adverse weathers, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection and depth estimation, consistently indicate the effectiveness and superiority of the proposed method compared with the recent representative methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外和可见光图像融合已成为计算机视觉领域的一项重要研究。然而，对复杂场景融合的关注很少，导致现有技术在受到真实干扰时产生次优结果。为了填补这一空白，我们提出了一种基于分解和干扰感知的图像融合方法。具体来说，我们根据光透射的散射程度对可见图像的像素进行分类，然后在此基础上分离图像的细节信息和能量信息。这种精细的分解有助于所提出的模型识别复杂场景中更多干扰像素。为了在去噪和细节保留之间取得平衡，我们提出了一种融合细节分量的自适应去噪方案。同时，我们从多个方向的角度考虑图像能量信息的分布，提出了一种新的加权融合规则。复杂场景融合的大量实验涵盖恶劣天气、噪声、模糊、过度曝光、火灾，以及包括语义分割、目标检测、显着目标检测和深度估计在内的下游任务，一致表明了该方法与现有方法相比的有效性和优越性。最近的代表性方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.02096v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification**<br />
**Title_cn:** 用于零样本遥感图像场景分类的深度语义视觉对齐<br />
**Authors:** Wenjia Xu, Jiuniu Wang, Zhiwei Wei, Mugen Peng, Yirong Wu<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. We predict attributes for each class by depicting the semantic-visual similarity between attributes and images. In this way, the attribute annotation process is accomplished by machine instead of human as in other methods. Moreover, we propose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the self-attention mechanism in the transformer to associate local image regions together, integrating the background context information for prediction. The DSVA model further utilizes the attribute attention maps to focus on the informative image regions that are essential for knowledge transfer in ZSL, and maps the visual images into attribute space to perform ZSL classification. With extensive experiments, we show that our model outperforms other state-of-the-art models by a large margin on a challenging large-scale RS scene classification benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在遥感（RS）图像分类方面取得了可喜的进展，其训练过程需要每个类别都有丰富的样本。然而，考虑到RS目标数据库是动态增长的，为每个RS类别标注标签既耗时又不现实。零样本学习（ZSL）允许识别训练期间未见过的新类，这为上述问题提供了一个有前途的解决方案。然而，以前的 ZSL 模型主要依赖于手动标记的属性或从语言模型中提取的词嵌入来将知识从已知的类转移到新的类。此外，先驱的 ZSL 模型使用在 ImageNet 上预训练的卷积神经网络，该网络专注于每幅图像中出现的主要对象，而忽略了在 RS 场景分类中也很重要的背景上下文。为了解决上述问题，我们建议自动收集视觉可检测的属性。我们通过描述属性和图像之间的语义视觉相似性来预测每个类别的属性。这样，属性标注过程就由机器完成，而不是像其他方法那样由人工完成。此外，我们提出了一种深度语义视觉对齐（DSVA），它利用变压器中的自注意力机制将局部图像区域关联在一起，整合背景上下文信息进行预测。 DSVA模型进一步利用属性注意力图来关注ZSL中知识迁移所必需的信息图像区域，并将视觉图像映射到属性空间以执行ZSL分类。通过大量的实验，我们表明我们的模型在具有挑战性的大规模 RS 场景分类基准上大幅优于其他最先进的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.02094v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **DeCoF: Generated Video Detection via Frame Consistency**<br />
**Title_cn:** DeCoF：通过帧一致性生成视频检测<br />
**Authors:** Long Ma, Jiajia Zhang, Hongping Deng, Ningyu Zhang, Yong Liao, Haiyang Yu<br />
**Abstract:** <details><summary>原文: </summary>The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several commercial proprietary models.</details>
**Abstract_cn:** <details><summary>译文: </summary>先进视频生成方法生成的视频质量不断提高，给社会带来了新的安全挑战，这使得生成视频检测成为紧迫的研究重点。为了促进这一领域的合作研究，我们明确构建了第一个用于生成视频检测的开源数据集，为社区提供宝贵的资源来进行基准测试和改进检测方法。通过一系列精心设计的探测实验，我们的研究探索了时间和空间伪影在开发生成视频的通用且鲁棒的检测器中的重要性。基于视频帧一致性的原理，我们引入了一种简单而有效的检测模型（DeCoF），该模型消除了泛化特征学习过程中空间伪影的影响。我们广泛的实验证明了 DeCoF 在检测由看不见的视频生成模型生成的视频方面的功效，并证实了其在多个商业专有模型中强大的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02085v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small Target Detection**<br />
**Title_cn:** TCI-Former：用于红外小目标检测的热传导变压器<br />
**Authors:** Tianxiang Chen, Zhentao Tan, Qi Chu, Yue Wu, Bin Liu, Nenghai Yu<br />
**Abstract:** <details><summary>原文: </summary>Infrared small target detection (ISTD) is critical to national security and has been extensively applied in military areas. ISTD aims to segment small target pixels from background. Most ISTD networks focus on designing feature extraction blocks or feature fusion modules, but rarely describe the ISTD process from the feature map evolution perspective. In the ISTD process, the network attention gradually shifts towards target areas. We abstract this process as the directional movement of feature map pixels to target areas through convolution, pooling and interactions with surrounding pixels, which can be analogous to the movement of thermal particles constrained by surrounding variables and particles. In light of this analogy, we propose Thermal Conduction-Inspired Transformer (TCI-Former) based on the theoretical principles of thermal conduction. According to thermal conduction differential equation in heat dynamics, we derive the pixel movement differential equation (PMDE) in the image domain and further develop two modules: Thermal Conduction-Inspired Attention (TCIA) and Thermal Conduction Boundary Module (TCBM). TCIA incorporates finite difference method with PMDE to reach a numerical approximation so that target body features can be extracted. To further remove errors in boundary areas, TCBM is designed and supervised by boundary masks to refine target body features with fine boundary details. Experiments on IRSTD-1k and NUAA-SIRST demonstrate the superiority of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外小目标探测（ISTD）对国家安全至关重要，已广泛应用于军事领域。 ISTD 旨在从背景中分割小目标像素。大多数ISTD网络专注于设计特征提取块或特征融合模块，但很少从特征图演化的角度描述ISTD过程。在ISTD过程中，网络注意力逐渐向目标领域转移。我们将这个过程抽象为特征图像素通过卷积、池化以及与周围像素的相互作用向目标区域的定向移动，这可以类似于受周围变量和粒子约束的热粒子的移动。鉴于这个类比，我们基于热传导的理论原理提出了热传导启发变压器（TCI-Former）。根据热动力学中的热传导微分方程，我们推导了图像域中的像素运动微分方程（PMDE），并进一步开发了两个模块：热传导启发注意力模块（TCIA）和热传导边界模块（TCBM）。 TCIA将有限差分法与PMDE相结合，达到数值逼近，从而提取目标身体特征。为了进一步消除边界区域中的错误，TCBM 被设计并通过边界掩模进行监督，以通过精细的边界细节来细化目标身体特征。 IRSTD-1k 和 NUAA-SIRST 上的实验证明了我们方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02046v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation**<br />
**Title_cn:** ScribFormer：Transformer 使 CNN 更好地进行基于 Scribble 的医学图像分割<br />
**Authors:** Zihan Li, Yuan Zheng, Dandan Shan, Shuzhou Yang, Qingde Li, Beizhan Wang, Yuanting Zhang, Qingqi Hong, Dinggang Shen<br />
**Abstract:** <details><summary>原文: </summary>Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation methods. Furthermore, the ACAM branch assists in unifying the shallow convolution features and the deep convolution features to improve model's performance further. Extensive experiments on two public datasets and one private dataset show that our ScribFormer has superior performance over the state-of-the-art scribble-supervised segmentation methods, and achieves even better results than the fully-supervised segmentation methods. The code is released at https://github.com/HUANGLIZI/ScribFormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的涂鸦监督分割方法通常采用具有编码器-解码器架构的 CNN 框架。尽管具有多种优点，但该框架通常只能捕获具有局部感受野的卷积层的小范围特征依赖性，这使得很难从涂鸦注释提供的有限信息中学习全局形状信息。为了解决这个问题，本文提出了一种新的 CNN-Transformer 混合解决方案，用于涂鸦监督医学图像分割，称为 ScribFormer。所提出的 ScribFormer 模型具有三分支结构，即 CNN 分支、Transformer 分支和注意力引导类激活图 (ACAM) 分支的混合。具体来说，CNN分支与Transformer分支协作，将从CNN学到的局部特征与从Transformer获得的全局表示融合，这可以有效克服现有涂鸦监督分割方法的局限性。此外，ACAM分支有助于统一浅层卷积特征和深层卷积特征，以进一步提高模型的性能。对两个公共数据集和一个私有数据集的大量实验表明，我们的 ScribFormer 比最先进的涂鸦监督分割方法具有更优越的性能，并且比完全监督分割方法取得了更好的结果。代码发布于https://github.com/HUANGLIZI/ScribFormer。</details>
**PDF:** <http://arxiv.org/pdf/2402.02029v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Transfer Learning in ECG Diagnosis: Is It Effective?**<br />
**Title_cn:** 心电图诊断中的迁移学习：有效吗？<br />
**Authors:** Cuong V. Nguyen, Cuong D. Do<br />
**Abstract:** <details><summary>原文: </summary>The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional neural networks than with recurrent neural networks, which are the two most prevalent architectures for time-series ECG applications. Our results underscore the importance of transfer learning in ECG diagnosis, yet depending on the amount of available data, researchers may opt not to use it, considering the non-negligible cost associated with pre-training.</details>
**Abstract_cn:** <details><summary>译文: </summary>在心电图诊断中采用深度学习通常会受到现实场景中大型、标记良好的数据集的缺乏的阻碍，从而导致使用迁移学习来利用从较大数据集中学到的特征。然而，迁移学习始终优于从头开始训练的普遍假设从未得到系统验证。在本研究中，我们通过研究比较微调性能与从头开始训练的性能，涵盖各种心电图数据集和深度神经网络，对多标签心电图分类中迁移学习的有效性进行了首次广泛的实证研究。我们确认微调是小型下游数据集的首选；然而，当数据集足够大时，从头开始训练可以达到相当的性能，尽管需要更长的训练时间才能赶上。此外，我们发现迁移学习与卷积神经网络比循环神经网络表现出更好的兼容性，循环神经网络是时间序列心电图应用的两种最流行的架构。我们的结果强调了迁移学习在心电图诊断中的重要性，但根据可用数据的数量，考虑到与预训练相关的不可忽略的成本，研究人员可能选择不使用它。</details>
**PDF:** <http://arxiv.org/pdf/2402.02021v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Hypergraph-Transformer (HGT) for Interactive Event Prediction in Laparoscopic and Robotic Surgery**<br />
**Title_cn:** 用于腹腔镜和机器人手术中交互式事件预测的超图变换器 (HGT)<br />
**Authors:** Lianhao Yin, Yutong Ban, Jennifer Eckhoff, Ozanan Meireles, Daniela Rus, Guy Rosman<br />
**Abstract:** <details><summary>原文: </summary>Understanding and anticipating intraoperative events and actions is critical for intraoperative assistance and decision-making during minimally invasive surgery. Automated prediction of events, actions, and the following consequences is addressed through various computational approaches with the objective of augmenting surgeons' perception and decision-making capabilities. We propose a predictive neural network that is capable of understanding and predicting critical interactive aspects of surgical workflow from intra-abdominal video, while flexibly leveraging surgical knowledge graphs. The approach incorporates a hypergraph-transformer (HGT) structure that encodes expert knowledge into the network design and predicts the hidden embedding of the graph. We verify our approach on established surgical datasets and applications, including the detection and prediction of action triplets, and the achievement of the Critical View of Safety (CVS). Moreover, we address specific, safety-related tasks, such as predicting the clipping of cystic duct or artery without prior achievement of the CVS. Our results demonstrate the superiority of our approach compared to unstructured alternatives.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解和预测术中事件和行动对于微创手术期间的术中协助和决策至关重要。通过各种计算方法来自动预测事件、行动和随后的后果，目的是增强外科医生的感知和决策能力。我们提出了一种预测神经网络，能够从腹腔内视频理解和预测手术工作流程的关键交互方面，同时灵活利用手术知识图。该方法采用了超图变换器（HGT）结构，将专家知识编码到网络设计中并预测图的隐藏嵌入。我们在已建立的手术数据集和应用程序上验证了我们的方法，包括动作三元组的检测和预测，以及安全批判性观点（CVS）的实现。此外，我们还解决与安全相关的具体任务，例如在未事先实现 CVS 的情况下预测胆囊管或动脉的夹闭。我们的结果证明了我们的方法与非结构化替代方案相比的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.01974v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **BVI-Lowlight: Fully Registered Benchmark Dataset for Low-Light Video Enhancement**<br />
**Title_cn:** BVI-Lowlight：完全注册的低光视频增强基准数据集<br />
**Authors:** Nantheera Anantrasirichai, Ruirui Lin, Alexandra Malyugina, David Bull<br />
**Abstract:** <details><summary>原文: </summary>Low-light videos often exhibit spatiotemporal incoherent noise, leading to poor visibility and compromised performance across various computer vision applications. One significant challenge in enhancing such content using modern technologies is the scarcity of training data. This paper introduces a novel low-light video dataset, consisting of 40 scenes captured in various motion scenarios under two distinct low-lighting conditions, incorporating genuine noise and temporal artifacts. We provide fully registered ground truth data captured in normal light using a programmable motorized dolly, and subsequently, refine them via image-based post-processing to ensure the pixel-wise alignment of frames in different light levels. This paper also presents an exhaustive analysis of the low-light dataset, and demonstrates the extensive and representative nature of our dataset in the context of supervised learning. Our experimental results demonstrate the significance of fully registered video pairs in the development of low-light video enhancement methods and the need for comprehensive evaluation. Our dataset is available at DOI:10.21227/mzny-8c77.</details>
**Abstract_cn:** <details><summary>译文: </summary>低光视频通常会表现出时空不相干噪声，导致各种计算机视觉应用的可视性较差并影响性能。使用现代技术增强此类内容的一项重大挑战是训练数据的稀缺。本文介绍了一种新颖的低光视频数据集，由在两种不同的低光条件下的各种运动场景中捕获的 40 个场景组成，包含真实的噪声和时间伪影。我们使用可编程电动小车提供在正常光线下捕获的完全配准的地面实况数据，然后通过基于图像的后处理对其进行细化，以确保不同光线水平下帧的像素级对齐。本文还对弱光数据集进行了详尽的分析，并展示了我们的数据集在监督学习背景下的广泛性和代表性。我们的实验结果证明了完全配准的视频对在低光视频增强方法开发中的重要性以及综合评估的必要性。我们的数据集可在 DOI:10.21227/mzny-8c77 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.01970v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Zero-shot sketch-based remote sensing image retrieval based on multi-level and attention-guided tokenization**<br />
**Title_cn:** 基于多层次和注意力引导标记化的零样本草图遥感图像检索<br />
**Authors:** Bo Yang, Chen Wang, Xiaoshuang Ma, Beiping Song, Zhuang Liu<br />
**Abstract:** <details><summary>原文: </summary>Effectively and efficiently retrieving images from remote sensing databases is a critical challenge in the realm of remote sensing big data. Utilizing hand-drawn sketches as retrieval inputs offers intuitive and user-friendly advantages, yet the potential of multi-level feature integration from sketches remains underexplored, leading to suboptimal retrieval performance. To address this gap, our study introduces a novel zero-shot, sketch-based retrieval method for remote sensing images, leveraging multi-level, attention-guided tokenization. This approach starts by employing multi-level self-attention feature extraction to tokenize the query sketches, as well as self-attention feature extraction to tokenize the candidate images. It then employs cross-attention mechanisms to establish token correspondence between these two modalities, facilitating the computation of sketch-to-image similarity. Our method demonstrates superior retrieval accuracy over existing sketch-based remote sensing image retrieval techniques, as evidenced by tests on four datasets. Notably, it also exhibits robust zero-shot learning capabilities and strong generalizability in handling unseen categories and novel remote sensing data. The method's scalability can be further enhanced by the pre-calculation of retrieval tokens for all candidate images in a database. This research underscores the significant potential of multi-level, attention-guided tokenization in cross-modal remote sensing image retrieval. For broader accessibility and research facilitation, we have made the code and dataset used in this study publicly available online. Code and dataset are available at https://github.com/Snowstormfly/Cross-modal-retrieval-MLAGT.</details>
**Abstract_cn:** <details><summary>译文: </summary>从遥感数据库中有效且高效地检索图像是遥感大数据领域的一个关键挑战。利用手绘草图作为检索输入具有直观且用户友好的优势，但草图的多级特征集成的潜力仍未得到充分开发，导致检索性能不佳。为了解决这一差距，我们的研究引入了一种新颖的零样本、基于草图的遥感图像检索方法，利用多层次、注意力引导的标记化。该方法首先采用多级自注意特征提取来标记查询草图，并使用自注意特征提取来标记候选图像。然后，它采用交叉注意机制来建立这两种模态之间的令牌对应关系，从而促进草图到图像相似度的计算。正如对四个数据集的测试所证明的那样，我们的方法表现出优于现有基于草图的遥感图像检索技术的检索精度。值得注意的是，它在处理未见过的类别和新颖的遥感数据方面还表现出强大的零样本学习能力和强大的通用性。通过预先计算数据库中所有候选图像的检索标记，可以进一步增强该方法的可扩展性。这项研究强调了多层次、注意力引导标记化在跨模式遥感图像检索中的巨大潜力。为了更广泛的可访问性和研究便利性，我们已在线公开本研究中使用的代码和数据集。代码和数据集可在 https://github.com/Snowstormfly/Cross-modal-retrieval-MLAGT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02141v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM**<br />
**Title_cn:** NeuV-SLAM：RGBD 密集 SLAM 的快速神经多分辨率体素优化<br />
**Authors:** Wenzhi Guo, Bing Wang, Lijun Chen<br />
**Abstract:** <details><summary>原文: </summary>We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping pipeline based on neural multiresolution voxels, characterized by ultra-fast convergence and incremental expansion capabilities. This pipeline utilizes RGBD images as input to construct multiresolution neural voxels, achieving rapid convergence while maintaining robust incremental scene reconstruction and camera tracking. Central to our methodology is to propose a novel implicit representation, termed VDF that combines the implementation of neural signed distance field (SDF) voxels with an SDF activation strategy. This approach entails the direct optimization of color features and SDF values anchored within the voxels, substantially enhancing the rate of scene convergence. To ensure the acquisition of clear edge delineation, SDF activation is designed, which maintains exemplary scene representation fidelity even under constraints of voxel resolution. Furthermore, in pursuit of advancing rapid incremental expansion with low computational overhead, we developed hashMV, a novel hash-based multiresolution voxel management structure. This architecture is complemented by a strategically designed voxel generation technique that synergizes with a two-dimensional scene prior. Our empirical evaluations, conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's exceptional efficacy in terms of convergence speed, tracking accuracy, scene reconstruction, and rendering quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍了 NeuV-SLAM，这是一种基于神经多分辨率体素的新型密集同步定位和建图管道，其特点是超快速收敛和增量扩展能力。该流程利用 RGBD 图像作为输入来构建多分辨率神经体素，实现快速收敛，同时保持稳健的增量场景重建和相机跟踪。我们方法的核心是提出一种新颖的隐式表示，称为 VDF，它将神经符号距离场 (SDF) 体素的实现与 SDF 激活策略结合起来。这种方法需要直接优化锚定在体素内的颜色特征和 SDF 值，从而大大提高场景收敛速度。为了确保获得清晰的边缘轮廓，设计了 SDF 激活，即使在体素分辨率的限制下也能保持示例性场景表示保真度。此外，为了追求以低计算开销推进快速增量扩展，我们开发了 hashMV，一种新颖的基于哈希的多分辨率体素管理结构。该架构得到了战略性设计的体素生成技术的补充，该技术与二维场景先验相协同。我们对 Replica 和 ScanNet 数据集进行的实证评估证实了 NeuV-SLAM 在收敛速度、跟踪精度、场景重建和渲染质量方面的卓越功效。</details>
**PDF:** <http://arxiv.org/pdf/2402.02020v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Multiple-Crop Human Mesh Recovery with Contrastive Learning and Camera Consistency in A Single Image**<br />
**Title_cn:** 在单个图像中具有对比学习和相机一致性的多裁剪人体网格恢复<br />
**Authors:** Yongwei Nie, Changzhen Liu, Chengjiang Long, Qing Zhang, Guiqing Li, Hongmin Cai<br />
**Abstract:** <details><summary>原文: </summary>We tackle the problem of single-image Human Mesh Recovery (HMR). Previous approaches are mostly based on a single crop. In this paper, we shift the single-crop HMR to a novel multiple-crop HMR paradigm. Cropping a human from image multiple times by shifting and scaling the original bounding box is feasible in practice, easy to implement, and incurs neglectable cost, but immediately enriches available visual details. With multiple crops as input, we manage to leverage the relation among these crops to extract discriminative features and reduce camera ambiguity. Specifically, (1) we incorporate a contrastive learning scheme to enhance the similarity between features extracted from crops of the same human. (2) We also propose a crop-aware fusion scheme to fuse the features of multiple crops for regressing the target mesh. (3) We compute local cameras for all the input crops and build a camera-consistency loss between the local cameras, which reward us with less ambiguous cameras. Based on the above innovations, our proposed method outperforms previous approaches as demonstrated by the extensive experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们解决了单图像人体网格恢复（HMR）的问题。以前的方法大多基于单一作物。在本文中，我们将单作物 HMR 转变为一种新颖的多作物 HMR 范式。通过移动和缩放原始边界框来多次从图像中裁剪人体在实践中是可行的，易于实现，并且产生的成本可以忽略不计，但立即丰富了可用的视觉细节。以多种作物作为输入，我们设法利用这些作物之间的关系来提取判别性特征并减少相机模糊度。具体来说，（1）我们采用对比学习方案来增强从同一个人的农作物中提取的特征之间的相似性。 （2）我们还提出了一种作物感知融合方案，以融合多种作物的特征来回归目标网格。 (3) 我们计算所有输入作物的本地相机，并在本地相机之间建立相机一致性损失，这会奖励我们不太模糊的相机。基于上述创新，我们提出的方法优于以前的方法，如大量实验所证明的。</details>
**PDF:** <http://arxiv.org/pdf/2402.02074v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey**<br />
**Title_cn:** 预训练视觉模型的参数高效微调：调查<br />
**Authors:** Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, Yuntao Du<br />
**Abstract:** <details><summary>原文: </summary>Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模预训练视觉模型（PVM）在各种下游视觉任务中表现出巨大的适应性潜力。然而，随着最先进的 PVM 参数增长到数十亿甚至数万亿，标准的完全微调范例由于高计算和存储需求而变得不可持续。为此，研究人员正在探索参数高效微调（PEFT），其目的是通过最小的参数修改来超越完全微调的性能。这项调查提供了视觉 PEFT 的全面概述和未来方向，对最新进展进行了系统回顾。首先，我们提供 PEFT 的正式定义并讨论模型预训练方法。然后，我们将现有方法分为三类：基于附加的、基于部分的和基于统一的。最后，我们介绍了常用的数据集和应用程序，并提出了未来潜在的研究挑战。 https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning 提供了全面的资源集合。</details>
**PDF:** <http://arxiv.org/pdf/2402.02242v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse, Respiration, and Blood Pressure Estimation**<br />
**Title_cn:** MSPM：用于远程脉搏、呼吸和血压估计的多站点生理监测数据集<br />
**Authors:** Jeremy Speth, Nathan Vance, Benjamin Sporrer, Lu Niu, Patrick Flynn, Adam Czajka<br />
**Abstract:** <details><summary>原文: </summary>Visible-light cameras can capture subtle physiological biomarkers without physical contact with the subject. We present the Multi-Site Physiological Monitoring (MSPM) dataset, which is the first dataset collected to support the study of simultaneous camera-based vital signs estimation from multiple locations on the body. MSPM enables research on remote photoplethysmography (rPPG), respiration rate, and pulse transit time (PTT); it contains ground-truth measurements of pulse oximetry (at multiple body locations) and blood pressure using contacting sensors. We provide thorough experiments demonstrating the suitability of MSPM to support research on rPPG, respiration rate, and PTT. Cross-dataset rPPG experiments reveal that MSPM is a challenging yet high quality dataset, with intra-dataset pulse rate mean absolute error (MAE) below 4 beats per minute (BPM), and cross-dataset pulse rate MAE below 2 BPM in certain cases. Respiration experiments find a MAE of 1.09 breaths per minute by extracting motion features from the chest. PTT experiments find that across the pairs of different body sites, there is high correlation between remote PTT and contact-measured PTT, which facilitates the possibility for future camera-based PTT research.</details>
**Abstract_cn:** <details><summary>译文: </summary>可见光相机可以捕获微妙的生理生物标记，而无需与受试者进行物理接触。我们提出了多站点生理监测（MSPM）数据集，这是第一个收集的数据集，用于支持基于摄像头的身体多个位置同时估计生命体征的研究。 MSPM 支持远程光电容积描记法 (rPPG)、呼吸速率和脉搏传导时间 (PTT) 的研究；它包含使用接触式传感器对脉搏血氧饱和度（在多个身体部位）和血压进行的真实测量。我们提供全面的实验，证明 MSPM 支持 rPPG、呼吸速率和 PTT 研究的适用性。跨数据集 rPPG 实验表明，MSPM 是一个具有挑战性但高质量的数据集，数据集内脉率平均绝对误差 (MAE) 低于 4 次/分钟 (BPM)，并且在某些情况下跨数据集脉率 MAE 低于 2 BPM 。呼吸实验通过从胸部提取运动特征，发现 MAE 为每分钟 1.09 次呼吸。 PTT实验发现，在成对的不同身体部位上，远程PTT和接触测量的PTT之间存在高度相关性，这为未来基于摄像头的PTT研究提供了可能性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02224v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Implicit Neural Representation of Tileable Material Textures**<br />
**Title_cn:** 可平铺材质纹理的隐式神经表示<br />
**Authors:** Hallison Paz, Tiago Novello, Luiz Velho<br />
**Abstract:** <details><summary>原文: </summary>We explore sinusoidal neural networks to represent periodic tileable textures. Our approach leverages the Fourier series by initializing the first layer of a sinusoidal neural network with integer frequencies with a period $P$. We prove that the compositions of sinusoidal layers generate only integer frequencies with period $P$. As a result, our network learns a continuous representation of a periodic pattern, enabling direct evaluation at any spatial coordinate without the need for interpolation. To enforce the resulting pattern to be tileable, we add a regularization term, based on the Poisson equation, to the loss function. Our proposed neural implicit representation is compact and enables efficient reconstruction of high-resolution textures with high visual fidelity and sharpness across multiple levels of detail. We present applications of our approach in the domain of anti-aliased surface.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们探索正弦神经网络来表示周期性的可平铺纹理。我们的方法通过使用周期为 $P$ 的整数频率初始化正弦神经网络的第一层来利用傅立叶级数。我们证明正弦层的组合仅产生周期为 $P$ 的整数频率。因此，我们的网络学习周期性模式的连续表示，从而能够在任何空间坐标上进行直接评估，而无需插值。为了强制生成的图案可平铺，我们在损失函数中添加了一个基于泊松方程的正则化项。我们提出的神经隐式表示结构紧凑，能够有效重建高分辨率纹理，在多个细节级别上具有高视觉保真度和清晰度。我们介绍了我们的方法在抗锯齿表面领域的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.02208v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **From Synthetic to Real: Unveiling the Power of Synthetic Data for Video Person Re-ID**<br />
**Title_cn:** 从合成到真实：揭示视频行人重识别合成数据的力量<br />
**Authors:** Xiangqun Zhang, Ruize Han, Wei Feng<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we study a new problem of cross-domain video based person re-identification (Re-ID). Specifically, we take the synthetic video dataset as the source domain for training and use the real-world videos for testing, which significantly reduces the dependence on real training data collection and annotation. To unveil the power of synthetic data for video person Re-ID, we first propose a self-supervised domain invariant feature learning strategy for both static and temporal features. Then, to further improve the person identification ability in the target domain, we develop a mean-teacher scheme with the self-supervised ID consistency loss. Experimental results on four real datasets verify the rationality of cross-synthetic-real domain adaption and the effectiveness of our method. We are also surprised to find that the synthetic data performs even better than the real data in the cross-domain setting.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们研究了基于跨域视频的行人重新识别（Re-ID）的新问题。具体来说，我们以合成视频数据集作为训练的源域，并使用真实世界的视频进行测试，这显着减少了对真实训练数据收集和注释的依赖。为了揭示视频人物重新识别的合成数据的力量，我们首先提出了一种针对静态和时间特征的自监督域不变特征学习策略。然后，为了进一步提高目标域中的人员识别能力，我们开发了一种具有自监督ID一致性损失的平均教师方案。四个真实数据集的实验结果验证了跨合成真实域适应的合理性和我们方法的有效性。我们还惊讶地发现，在跨域设置中，合成数据的表现甚至比真实数据还要好。</details>
**PDF:** <http://arxiv.org/pdf/2402.02108v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with Global Insights**<br />
**Title_cn:** DCS-Net：具有全球洞察力的开创性无泄漏点云预训练框架<br />
**Authors:** Zhe Li, Zhangyang Gao, Cheng Tan, Stan Z. Li, Laurence T. Yang<br />
**Abstract:** <details><summary>原文: </summary>Masked autoencoding and generative pretraining have achieved remarkable success in computer vision and natural language processing, and more recently, they have been extended to the point cloud domain. Nevertheless, existing point cloud models suffer from the issue of information leakage due to the pre-sampling of center points, which leads to trivial proxy tasks for the models. These approaches primarily focus on local feature reconstruction, limiting their ability to capture global patterns within point clouds. In this paper, we argue that the reduced difficulty of pretext tasks hampers the model's capacity to learn expressive representations. To address these limitations, we introduce a novel solution called the Differentiable Center Sampling Network (DCS-Net). It tackles the information leakage problem by incorporating both global feature reconstruction and local feature reconstruction as non-trivial proxy tasks, enabling simultaneous learning of both the global and local patterns within point cloud. Experimental results demonstrate that our method enhances the expressive capacity of existing point cloud models and effectively addresses the issue of information leakage.</details>
**Abstract_cn:** <details><summary>译文: </summary>掩码自动编码和生成预训练在计算机视觉和自然语言处理方面取得了显着的成功，最近，它们已扩展到点云领域。然而，现有的点云模型由于中心点的预采样而存在信息泄漏的问题，这导致模型的代理任务变得琐碎。这些方法主要关注局部特征重建，限制了它们捕获点云内全局模式的能力。在本文中，我们认为借口任务难度的降低阻碍了模型学习表达表征的能力。为了解决这些限制，我们引入了一种称为可微中心采样网络（DCS-Net）的新颖解决方案。它通过将全局特征重建和局部特征重建合并为重要的代理任务来解决信息泄漏问题，从而能够同时学习点云内的全局和局部模式。实验结果表明，我们的方法增强了现有点云模型的表达能力，有效解决了信息泄漏问题。</details>
**PDF:** <http://arxiv.org/pdf/2402.02088v1><br />
**Code:** null<br />

