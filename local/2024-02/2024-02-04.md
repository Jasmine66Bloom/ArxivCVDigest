## [UPDATED!] **2024-02-04** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing**<br />
**Title_cn:** DiffEditor：提高基于扩散的图像编辑的准确性和灵活性<br />
**Authors:** Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>过去几年，大规模文本到图像 (T2I) 扩散模型彻底改变了图像生成。尽管拥有多样化、高质量的生成能力，但将这些能力转化为细粒度的图像编辑仍然具有挑战性。在本文中，我们提出 DiffEditor 来纠正现有基于扩散的图像编辑中的两个弱点：（1）在复杂场景中，编辑结果通常缺乏编辑准确性并表现出意外的伪影； (2)缺乏协调编辑操作的灵活性，例如想象新内容。在我们的解决方案中，我们在细粒度图像编辑中引入了图像提示，与文本提示配合更好地描述编辑内容。为了在保持内容一致性的同时增加灵活性，我们将随机微分方程（SDE）局部结合到常微分方程（ODE）采样中。此外，我们将基于区域分数的梯度引导和时间旅行策略纳入扩散采样中，进一步提高了编辑质量。大量的实验表明，我们的方法可以在各种细粒度图像编辑任务上有效地实现最先进的性能，包括在单个图像内进行编辑（例如，对象移动、调整大小和内容拖动）和跨图像编辑（例如，外观替换和对象粘贴）。我们的源代码发布于 https://github.com/MC-E/DragonDiffusion。</details>
**PDF:** <http://arxiv.org/pdf/2402.02583v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **AI Art Neural Constellation: Revealing the Collective and Contrastive State of AI-Generated and Human Art**<br />
**Title_cn:** 人工智能艺术神经星座：揭示人工智能生成艺术和人类艺术的集体和对比状态<br />
**Authors:** Faizan Farooq Khan, Diana Kim, Divyansh Jha, Youssef Mohamed, Hanna H Chang, Ahmed Elgammal, Luba Elliott, Mohamed Elhoseiny<br />
**Abstract:** <details><summary>原文: </summary>Discovering the creative potentials of a random signal to various artistic expressions in aesthetic and conceptual richness is a ground for the recent success of generative machine learning as a way of art creation. To understand the new artistic medium better, we conduct a comprehensive analysis to position AI-generated art within the context of human art heritage. Our comparative analysis is based on an extensive dataset, dubbed ``ArtConstellation,'' consisting of annotations about art principles, likability, and emotions for 6,000 WikiArt and 3,200 AI-generated artworks. After training various state-of-the-art generative models, art samples are produced and compared with WikiArt data on the last hidden layer of a deep-CNN trained for style classification. We actively examined the various art principles to interpret the neural representations and used them to drive the comparative knowledge about human and AI-generated art. A key finding in the semantic analysis is that AI-generated artworks are visually related to the principle concepts for modern period art made in 1800-2000. In addition, through Out-Of-Distribution (OOD) and In-Distribution (ID) detection in CLIP space, we find that AI-generated artworks are ID to human art when they depict landscapes and geometric abstract figures, while detected as OOD when the machine art consists of deformed and twisted figures. We observe that machine-generated art is uniquely characterized by incomplete and reduced figuration. Lastly, we conducted a human survey about emotional experience. Color composition and familiar subjects are the key factors of likability and emotions in art appreciation. We propose our whole methodologies and collected dataset as our analytical framework to contrast human and AI-generated art, which we refer to as ``ArtNeuralConstellation''. Code is available at: https://github.com/faixan-khan/ArtNeuralConstellation</details>
**Abstract_cn:** <details><summary>译文: </summary>发现随机信号在审美和概念丰富性方面对各种艺术表达的创造潜力是生成机器学习作为一种艺术创作方式最近取得成功的基础。为了更好地理解新的艺术媒介，我们进行了全面的分析，将人工智能生成的艺术置于人类艺术遗产的背景下。我们的比较分析基于一个名为“ArtConstellation”的广泛数据集，其中包含 6,000 件 WikiArt 和 3,200 件人工智能生成的艺术作品的艺术原理、喜爱度和情感注释。在训练各种最先进的生成模型后，将生成艺术样本，并与经过风格分类训练的深度 CNN 最后隐藏层上的 WikiArt 数据进行比较。我们积极研究各种艺术原理来解释神经表征，并利用它们来推动有关人类和人工智能生成艺术的比较知识。语义分析的一个重要发现是，人工智能生成的艺术品在视觉上与 1800-2000 年现代艺术的基本概念相关。此外，通过CLIP空间中的分布外（OOD）和分布内（ID）检测，我们发现人工智能生成的艺术品在描绘风景和几何抽象图形时对人类艺术来说是ID，而在描绘风景和几何抽象图形时则被检测为OOD机器艺术由变形和扭曲的图形组成。我们观察到机器生成的艺术的独特特征是不完整和简化的形象。最后，我们进行了一项关于情感体验的人类调查。色彩构成和熟悉的题材是艺术欣赏中令人喜爱和情感的关键因素。我们提出了整个方法论和收集的数据集作为我们的分析框架，以对比人类和人工智能生成的艺术，我们将其称为“ArtNeuralConstellation”。代码位于：https://github.com/faixan-khan/ArtNeuralConstellation</details>
**PDF:** <http://arxiv.org/pdf/2402.02453v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PromptRR: Diffusion Models as Prompt Generators for Single Image Reflection Removal**<br />
**Title_cn:** PromptRR：扩散模型作为单图像反射去除的提示生成器<br />
**Authors:** Tao Wang, Wanglong Lu, Kaihao Zhang, Wenhan Luo, Tae-Kyun Kim, Tong Lu, Hongdong Li, Ming-Hsuan Yang<br />
**Abstract:** <details><summary>原文: </summary>Existing single image reflection removal (SIRR) methods using deep learning tend to miss key low-frequency (LF) and high-frequency (HF) differences in images, affecting their effectiveness in removing reflections. To address this problem, this paper proposes a novel prompt-guided reflection removal (PromptRR) framework that uses frequency information as new visual prompts for better reflection performance. Specifically, the proposed framework decouples the reflection removal process into the prompt generation and subsequent prompt-guided restoration. For the prompt generation, we first propose a prompt pre-training strategy to train a frequency prompt encoder that encodes the ground-truth image into LF and HF prompts. Then, we adopt diffusion models (DMs) as prompt generators to generate the LF and HF prompts estimated by the pre-trained frequency prompt encoder. For the prompt-guided restoration, we integrate specially generated prompts into the PromptFormer network, employing a novel Transformer-based prompt block to effectively steer the model toward enhanced reflection removal. The results on commonly used benchmarks show that our method outperforms state-of-the-art approaches. The codes and models are available at https://github.com/TaoWangzj/PromptRR.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的使用深度学习的单图像反射去除（SIRR）方法往往会错过图像中关键的低频（LF）和高频（HF）差异，从而影响其去除反射的有效性。为了解决这个问题，本文提出了一种新颖的提示引导反射去除（PromptRR）框架，该框架使用频率信息作为新的视觉提示，以获得更好的反射性​​能。具体来说，所提出的框架将反射消除过程解耦为提示生成和随后的提示引导恢复。对于提示生成，我们首先提出一种提示预训练策略来训练频率提示编码器，将地面实况图像编码为低频和高频提示。然后，我们采用扩散模型（DM）作为提示生成器来生成由预训练的频率提示编码器估计的低频和高频提示。对于提示引导的恢复，我们将专门生成的提示集成到 PromptFormer 网络中，采用一种新颖的基于 Transformer 的提示块来有效引导模型增强反射去除。常用基准测试的结果表明，我们的方法优于最先进的方法。代码和模型可在 https://github.com/TaoWangzj/PromptRR 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02374v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE Distillation and Diffusion Probabilistic Feedback**<br />
**Title_cn:** 使用 $β$-VAE 蒸馏和扩散概率反馈进行闭环无监督表示解耦<br />
**Authors:** Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng<br />
**Abstract:** <details><summary>原文: </summary>Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>表征解开可能有助于人工智能从根本上理解现实世界，从而有利于区分和生成任务。目前它至少有三个未解决的核心问题：（i）严重依赖标签注释和合成数据——导致对自然场景的泛化能力较差； (ii) 启发式/手工解开约束使得很难自适应地实现最佳训练权衡； （iii）缺乏合理的评估指标，特别是对于真实的无标签数据。为了解决这些挑战，我们提出了一种 \textbf{C}losed-\textbf{L}oop 无监督表示 \textbf{Dis} 纠缠方法，称为 \textbf{CL-Dis}。具体来说，我们使用基于扩散的自动编码器（Diff-AE）作为主干，同时依靠 $\beta$-VAE 作为副驾驶员来提取语义解缠结的表示。扩散模型强大的生成能力和VAE模型良好的解缠能力是互补的。为了加强解缠结，VAE潜蒸馏和扩散反馈在闭环系统中相互关联，以进一步相互促进。然后，引入自监督 \textbf{Navigation} 策略来识别解开的潜在空间中可解释的语义方向。最后，设计了一种基于内容跟踪的新指标来评估解缠效果。实验证明了 CL-Dis 在真实图像处理和视觉分析等应用中的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02346v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Your Diffusion Model is Secretly a Certifiably Robust Classifier**<br />
**Title_cn:** 您的扩散模型实际上是一个经过验证的稳健分类器<br />
**Authors:** Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, Jun Zhu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型最近被用作稳健分类的生成分类器。然而，对扩散分类器的鲁棒性仍然缺乏全面的理论理解，这让我们怀疑它们是否容易受到未来更强的攻击。在这项研究中，我们提出了一个新的扩散分类器系列，称为噪声扩散分类器~（NDC），它具有最先进的经过认证的鲁棒性。具体来说，我们推广扩散分类器，通过推导这些分布的证据下界 (ELBO)、使用 ELBO 近似似然并通过贝叶斯定理计算分类概率，对高斯损坏的数据进行分类。我们将这些广义扩散分类器与随机平滑相结合，构建具有非恒定 Lipschitzness 的平滑分类器。实验结果证明了我们提出的 NDC 具有卓越的经认证稳健性。值得注意的是，我们是第一个使用单一现成扩散模型在 $\ell_2$ 范数分别小于 0.25 和 0.5 的对抗性扰动下在 CIFAR-10 上实现 80\%+ 和 70\%+ 认证稳健性的人无需任何附加数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.02316v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Generalizable Entity Grounding via Assistance of Large Language Model**<br />
**Title_cn:** 通过大语言模型的辅助进行泛化实体基础<br />
**Authors:** Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, Xiangtai Li, Weidong Guo, Yu Xu, Ming-Hsuan Yang<br />
**Abstract:** <details><summary>原文: </summary>In this work, we propose a novel approach to densely ground visual entities from a long caption. We leverage a large multimodal model (LMM) to extract semantic nouns, a class-agnostic segmentation model to generate entity-level segmentation, and the proposed multi-modal feature fusion module to associate each semantic noun with its corresponding segmentation mask. Additionally, we introduce a strategy of encoding entity segmentation masks into a colormap, enabling the preservation of fine-grained predictions from features of high-resolution masks. This approach allows us to extract visual features from low-resolution images using the CLIP vision encoder in the LMM, which is more computationally efficient than existing approaches that use an additional encoder for high-resolution images. Our comprehensive experiments demonstrate the superiority of our method, outperforming state-of-the-art techniques on three tasks, including panoptic narrative grounding, referring expression segmentation, and panoptic segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了一种从长标题中获取密集视觉实体的新颖方法。我们利用大型多模态模型（LMM）来提取语义名词，利用与类无关的分割模型来生成实体级分割，并利用所提出的多模态特征融合模块将每个语义名词与其相应的分割掩模相关联。此外，我们引入了一种将实体分割掩模编码到颜色图中的策略，从而能够保留高分辨率掩模特征的细粒度预测。这种方法允许我们使用 LMM 中的 CLIP 视觉编码器从低分辨率图像中提取视觉特征，这比使用额外的高分辨率图像编码器的现有方法的计算效率更高。我们的综合实验证明了我们的方法的优越性，在三项任务上优于最先进的技术，包括全景叙事基础、指称表达分割和全景分割。</details>
**PDF:** <http://arxiv.org/pdf/2402.02555v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model**<br />
**Title_cn:** LHRS-Bot：利用 VGI 增强型大型多模态语言模型增强遥感能力<br />
**Authors:** Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, Pengfeng Xiao<br />
**Abstract:** <details><summary>原文: </summary>The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>大语言模型 (LLM) 的革命性功能为多模式大语言模型 (MLLM) 铺平了道路，并促进了跨各个专业领域的多样化应用。然而，在遥感 (RS) 领域，最近的 MLLM 工作并未充分考虑遥感图像中不同的地理景观和不同的物体。为了弥补这一差距，我们利用广泛的自愿地理信息 (VGI) 和全球可用的 RS 图像，构建了一个大规模 RS 图像文本数据集 LHRS-Align 和一个信息丰富的 RS 特定指令数据集 LHRS-Instruct。在此基础上，我们引入了 LHRS-Bot，这是一种通过新颖的多级视觉语言对齐策略和课程学习方法为 RS 图像理解量身定制的 MLLM。综合实验表明，LHRS-Bot 表现出对 RS 图像的深刻理解以及在 RS 领域进行细致推理的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02544v1><br />
**Code:** <https://github.com/NJU-LHRS/LHRS-Bot>**<br />
>>**index:** 3<br />
**Title:** **Knowledge Generation for Zero-shot Knowledge-based VQA**<br />
**Title_cn:** 零样本基于知识的 VQA 的知识生成<br />
**Authors:** Rui Cao, Jing Jiang<br />
**Abstract:** <details><summary>原文: </summary>Previous solutions to knowledge-based visual question answering~(K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.</details>
**Abstract_cn:** <details><summary>译文: </summary>之前基于知识的视觉问答~（K-VQA）的解决方案是从外部知识库检索知识，并使用监督学习来训练K-VQA模型。最近预训练的法学硕士已被用作 K-VQA 的知识源和零样本 QA 模型，并展示了有希望的结果。然而，这些最近的方法没有明确显示回答问题所需的知识，因此缺乏可解释性。受最近关于基于文本的 QA 的法学硕士知识生成工作的启发，在这项工作中，我们提出并测试了一种类似的基于知识生成的 K-VQA 方法，该方法首先从法学硕士生成知识，然后将生成的知识合并到 K-以零样本方式进行 VQA。我们在两个 K-VQA 基准上评估我们的方法，发现我们的方法比以前的零样本 K-VQA 方法表现更好，并且我们生成的知识通常是相关且有用的。</details>
**PDF:** <http://arxiv.org/pdf/2402.02541v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering**<br />
**Title_cn:** GeReA：基于知识的视觉问答的问题感知提示字幕<br />
**Authors:** Ziyu Ma, Shutao Li, Bin Sun, Jianfei Cai, Zuxiang Long, Fuyan Ma<br />
**Abstract:** <details><summary>原文: </summary>Knowledge-based visual question answering (VQA) requires world knowledge beyond the image for accurate answer. Recently, instead of extra knowledge bases, a large language model (LLM) like GPT-3 is activated as an implicit knowledge engine to jointly acquire and reason the necessary knowledge for answering by converting images into textual information (e.g., captions and answer candidates). However, such conversion may introduce irrelevant information, which causes the LLM to misinterpret images and ignore visual details crucial for accurate knowledge. We argue that multimodal large language model (MLLM) is a better implicit knowledge engine than the LLM for its superior capability of visual understanding. Despite this, how to activate the capacity of MLLM as the implicit knowledge engine has not been explored yet. Therefore, we propose GeReA, a generate-reason framework that prompts a MLLM like InstructBLIP with question relevant vision and language information to generate knowledge-relevant descriptions and reasons those descriptions for knowledge-based VQA. Specifically, the question-relevant image regions and question-specific manual prompts are encoded in the MLLM to generate the knowledge relevant descriptions, referred to as question-aware prompt captions. After that, the question-aware prompt captions, image-question pair, and similar samples are sent into the multi-modal reasoning model to learn a joint knowledge-image-question representation for answer prediction. GeReA unlocks the use of MLLM as the implicit knowledge engine, surpassing all previous state-of-the-art methods on OK-VQA and A-OKVQA datasets, with test accuracies of 66.5% and 63.3% respectively. Our code will be released at https://github.com/Upper9527/GeReA.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于知识的视觉问答（VQA）需要图像之外的世界知识才能获得准确的答案。最近，像 GPT-3 这样的大型语言模型（LLM）被激活作为隐式知识引擎，而不是额外的知识库，通过将图像转换为文本信息（例如标题和候选答案）来共同获取和推理回答所需的知识。 。然而，这种转换可能会引入不相关的信息，从而导致法学硕士误解图像并忽略对准确知识至关重要的视觉细节。我们认为，多模态大语言模型（MLLM）因其卓越的视觉理解能力而成为比 LLM 更好的隐式知识引擎。尽管如此，如何激活MLLM作为隐性知识引擎的能力尚未被探索。因此，我们提出了 GeReA，一个生成推理框架，它提示像 InstructBLIP 这样的 MLLM 具有问题相关的视觉和语言信息，以生成与知识相关的描述，并为基于知识的 VQA 推理这些描述。具体来说，与问题相关的图像区域和特定于问题的手动提示在 MLLM 中进行编码，以生成与知识相关的描述，称为问题感知提示标题。之后，问题感知提示字幕、图像-问题对和相似样本被发送到多模态推理模型中，以学习用于答案预测的联合知识-图像-问题表示。 GeReA 解锁了 MLLM 作为隐式知识引擎的使用，超越了之前在 OK-VQA 和 A-OKVQA 数据集上所有最先进的方法，测试准确率分别为 66.5% 和 63.3%。我们的代码将在 https://github.com/Upper9527/GeReA 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.02503v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face Generation and Editing**<br />
**Title_cn:** M$^3$Face：用于人脸生成和编辑的统一多模态多语言框架<br />
**Authors:** Mohammadreza Mofayezi, Reza Alipour, Mohammad Ali Kakavand, Ehsaneddin Asgari<br />
**Abstract:** <details><summary>原文: </summary>Human face generation and editing represent an essential task in the era of computer vision and the digital world. Recent studies have shown remarkable progress in multi-modal face generation and editing, for instance, using face segmentation to guide image generation. However, it may be challenging for some users to create these conditioning modalities manually. Thus, we introduce M3Face, a unified multi-modal multilingual framework for controllable face generation and editing. This framework enables users to utilize only text input to generate controlling modalities automatically, for instance, semantic segmentation or facial landmarks, and subsequently generate face images. We conduct extensive qualitative and quantitative experiments to showcase our frameworks face generation and editing capabilities. Additionally, we propose the M3CelebA Dataset, a large-scale multi-modal and multilingual face dataset containing high-quality images, semantic segmentations, facial landmarks, and different captions for each image in multiple languages. The code and the dataset will be released upon publication.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸生成和编辑是计算机视觉和数字世界时代的一项重要任务。最近的研究表明，多模式人脸生成和编辑方面取得了显着进展，例如，使用人脸分割来指导图像生成。然而，对于一些用户来说，手动创建这些调节方式可能具有挑战性。因此，我们引入了M3Face，一个用于可控人脸生成和编辑的统一多模态多语言框架。该框架使用户能够仅利用文本输入自动生成控制模式，例如语义分割或面部标志，并随后生成面部图像。我们进行了广泛的定性和定量实验，以展示我们的框架的面部生成和编辑功能。此外，我们提出了 M3CelebA 数据集，这是一个大规模多模态和多语言人脸数据集，其中包含高质量图像、语义分割、面部标志以及多语言的每张图像的不同标题。代码和数据集将在发布后发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.02369v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Vision Transformer-based Multimodal Feature Fusion Network for Lymphoma Segmentation on PET/CT Images**<br />
**Title_cn:** 基于 Vision Transformer 的多模态特征融合网络，用于 PET/CT 图像上的淋巴瘤分割<br />
**Authors:** Huan Huang, Liheng Qiu, Shenmiao Yang, Longxi Li, Jiaofen Nan, Yanting Li, Chuang Han, Fubao Zhu, Chen Zhao, Weihua Zhou<br />
**Abstract:** <details><summary>原文: </summary>Background: Diffuse large B-cell lymphoma (DLBCL) segmentation is a challenge in medical image analysis. Traditional segmentation methods for lymphoma struggle with the complex patterns and the presence of DLBCL lesions. Objective: We aim to develop an accurate method for lymphoma segmentation with 18F-Fluorodeoxyglucose positron emission tomography (PET) and computed tomography (CT) images. Methods: Our lymphoma segmentation approach combines a vision transformer with dual encoders, adeptly fusing PET and CT data via multimodal cross-attention fusion (MMCAF) module. In this study, PET and CT data from 165 DLBCL patients were analyzed. A 5-fold cross-validation was employed to evaluate the performance and generalization ability of our method. Ground truths were annotated by experienced nuclear medicine experts. We calculated the total metabolic tumor volume (TMTV) and performed a statistical analysis on our results. Results: The proposed method exhibited accurate performance in DLBCL lesion segmentation, achieving a Dice similarity coefficient of 0.9173$\pm$0.0071, a Hausdorff distance of 2.71$\pm$0.25mm, a sensitivity of 0.9462$\pm$0.0223, and a specificity of 0.9986$\pm$0.0008. Additionally, a Pearson correlation coefficient of 0.9030$\pm$0.0179 and an R-square of 0.8586$\pm$0.0173 were observed in TMTV when measured on manual annotation compared to our segmentation results. Conclusion: This study highlights the advantages of MMCAF and vision transformer for lymphoma segmentation using PET and CT, offering great promise for computer-aided lymphoma diagnosis and treatment.</details>
**Abstract_cn:** <details><summary>译文: </summary>背景：弥漫性大 B 细胞淋巴瘤 (DLBCL) 分割是医学图像分析中的一个挑战。传统的淋巴瘤分割方法难以应对复杂的模式和 DLBCL 病变的存在。目的：我们的目标是开发一种利用 18F-氟脱氧葡萄糖正电子发射断层扫描 (PET) 和计算机断层扫描 (CT) 图像进行淋巴瘤分割的准确方法。方法：我们的淋巴瘤分割方法将视觉转换器与双编码器相结合，通过多模态交叉注意融合 (MMCAF) 模块巧妙地融合 PET 和 CT 数据。在这项研究中，分析了 165 名 DLBCL 患者的 PET 和 CT 数据。采用 5 倍交叉验证来评估我们方法的性能和泛化能力。基本事实由经验丰富的核医学专家注释。我们计算了总代谢肿瘤体积（TMTV）并对我们的结果进行了统计分析。结果：该方法在DLBCL病灶分割中表现出准确的性能，Dice相似系数为0.9173$\pm$0.0071，Hausdorff距离为2.71$\pm$0.25mm，灵敏度为0.9462$\pm$0.0223，特异性为0.9986 $\pm$0.0008。此外，与我们的分割结果相比，在手动注释测量时，TMTV 中观察到的 Pearson 相关系数为 0.9030$\pm$0.0179，R 方为 0.8586$\pm$0.0173。结论：本研究凸显了 MMCAF 和视觉转换器使用 PET 和 CT 进行淋巴瘤分割的优势，为计算机辅助淋巴瘤诊断和治疗提供了广阔的前景。</details>
**PDF:** <http://arxiv.org/pdf/2402.02349v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues**<br />
**Title_cn:** 通过加强音频提示引导视听分割<br />
**Authors:** Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu, Le Lu, Jieping Ye, Nenghai Yu<br />
**Abstract:** <details><summary>原文: </summary>How to effectively interact audio with vision has garnered considerable interest within the multi-modality research field. Recently, a novel audio-visual segmentation (AVS) task has been proposed, aiming to segment the sounding objects in video frames under the guidance of audio cues. However, most existing AVS methods are hindered by a modality imbalance where the visual features tend to dominate those of the audio modality, due to a unidirectional and insufficient integration of audio cues. This imbalance skews the feature representation towards the visual aspect, impeding the learning of joint audio-visual representations and potentially causing segmentation inaccuracies. To address this issue, we propose AVSAC. Our approach features a Bidirectional Audio-Visual Decoder (BAVD) with integrated bidirectional bridges, enhancing audio cues and fostering continuous interplay between audio and visual modalities. This bidirectional interaction narrows the modality imbalance, facilitating more effective learning of integrated audio-visual representations. Additionally, we present a strategy for audio-visual frame-wise synchrony as fine-grained guidance of BAVD. This strategy enhances the share of auditory components in visual features, contributing to a more balanced audio-visual representation learning. Extensive experiments show that our method attains new benchmarks in AVS performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>如何有效地实现音频与视觉的交互在多模态研究领域引起了相当大的兴趣。最近，提出了一种新颖的视听分割（AVS）任务，旨在在音频提示的指导下分割视频帧中的发声对象。然而，大多数现有的 AVS 方法都受到模态不平衡的阻碍，由于音频提示的单向和集成不足，视觉特征往往主导音频模态。这种不平衡使特征表示偏向视觉方面，阻碍了联合视听表示的学习，并可能导致分割不准确。为了解决这个问题，我们提出了 AVSAC。我们的方法采用具有集成双向桥的双向视听解码器（BAVD），增强音频提示并促进音频和视觉模式之间的持续相互作用。这种双向交互缩小了模态不平衡，促进更有效地学习集成视听表示。此外，我们提出了一种视听帧同步策略作为 BAVD 的细粒度指导。这种策略增强了视觉特征中听觉成分的份额，有助于更平衡的视听表征学习。大量实验表明，我们的方法在 AVS 性能方面达到了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2402.02327v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Spatio-temporal Prompting Network for Robust Video Feature Extraction**<br />
**Title_cn:** 用于鲁棒视频特征提取的时空提示网络<br />
**Authors:** Guanxiong Sun, Chi Wang, Zhaoyu Zhang, Jiankang Deng, Stefanos Zafeiriou, Yang Hua<br />
**Abstract:** <details><summary>原文: </summary>Frame quality deterioration is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking. Code is available at https://github.com/guanxiongsun/vfe.pytorch.</details>
**Abstract_cn:** <details><summary>译文: </summary>帧质量恶化是视频理解领域的主要挑战之一。为了补偿由恶化的帧引起的信息损失，最近的方法利用基于变压器的集成模块来获取时空信息。然而，这些集成模块笨重且复杂。此外，每个集成模块都是专门针对其目标任务定制的，因此很难推广到多个任务。在本文中，我们提出了一个简洁而统一的框架，称为时空提示网络（STPN）。它可以通过动态调整主干网络中的输入特征来有效地提取鲁棒且准确的视频特征。具体来说，STPN 预测多个包含相邻帧时空信息的视频提示。然后，这些视频提示被添加到当前帧的补丁嵌入之前，作为视频特征提取的更新输入。此外，STPN 很容易推广到各种视频任务，因为它不包含特定于任务的模块。没有花里胡哨的东西，STPN 在用于不同视频理解任务的三个广泛使用的数据集上实现了最先进的性能，即用于视频对象检测的 ImageNetVID、用于视频实例分割的 YouTubeVIS 和用于视觉对象跟踪的 GOT-10k。代码可在 https://github.com/guanxiongsun/vfe.pytorch 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02574v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers**<br />
**Title_cn:** DeSparsify：针对 Vision Transformer 中代币稀疏化机制的对抗性攻击<br />
**Authors:** Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai<br />
**Abstract:** <details><summary>原文: </summary>Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our evaluation demonstrates the attack's effectiveness on three token sparsification techniques and examines the attack's transferability between them and its effect on the GPU resources. To mitigate the impact of the attack, we propose various countermeasures.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉转换器为计算机视觉领域的进步做出了巨大贡献，在各种任务（例如图像分类、对象检测）中展示了最先进的性能。然而，它们的高计算要求随着使用的代币数量呈二次方增长。令牌稀疏化技术已经被提出来解决这个问题。这些技术采用依赖于输入的策略，从计算管道中丢弃无信息的标记，从而提高模型的效率。然而，它们的活力和平均情况假设使它们容易受到新的威胁向量的影响——精心设计的对抗性示例能够欺骗稀疏化机制，从而导致最坏情况的性能。在本文中，我们提出了 DeSparsify，这是一种针对使用令牌稀疏机制的视觉转换器的可用性的攻击。该攻击旨在耗尽操作系统的资源，同时保持其隐蔽性。我们的评估证明了攻击对三种令牌稀疏化技术的有效性，并检查了攻击在它们之间的可转移性及其对 GPU 资源的影响。为了减轻攻击的影响，我们提出了各种对策。</details>
**PDF:** <http://arxiv.org/pdf/2402.02554v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Classification of Tennis Actions Using Deep Learning**<br />
**Title_cn:** 使用深度学习对网球动作进行分类<br />
**Authors:** Emil Hovad, Therese Hougaard-Jensen, Line Katrine Harder Clemmensen<br />
**Abstract:** <details><summary>原文: </summary>Recent advances of deep learning makes it possible to identify specific events in videos with greater precision. This has great relevance in sports like tennis in order to e.g., automatically collect game statistics, or replay actions of specific interest for game strategy or player improvements. In this paper, we investigate the potential and the challenges of using deep learning to classify tennis actions. Three models of different size, all based on the deep learning architecture SlowFast were trained and evaluated on the academic tennis dataset THETIS. The best models achieve a generalization accuracy of 74 %, demonstrating a good performance for tennis action classification. We provide an error analysis for the best model and pinpoint directions for improvement of tennis datasets in general. We discuss the limitations of the data set, general limitations of current publicly available tennis data-sets, and future steps needed to make progress.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习的最新进展使得能够更精确地识别视频中的特定事件。这在网球等运动中具有很大的相关性，例如自动收集比赛统计数据，或重播特定兴趣的动作以实现比赛策略或球员改进。在本文中，我们研究了使用深度学习对网球动作进行分类的潜力和挑战。三个不同大小的模型均基于深度学习架构 SlowFast，在学术网球数据集 THETIS 上进行训练和评估。最好的模型达到了 74% 的泛化准确率，在网球动作分类方面表现出了良好的性能。我们提供最佳模型的误差分析，并确定网球数据集总体改进的方向。我们讨论了数据集的局限性、当前公开的网球数据集的一般局限性以及取得进展所需的未来步骤。</details>
**PDF:** <http://arxiv.org/pdf/2402.02545v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Embedding Non-Distortive Cancelable Face Template Generation**<br />
**Title_cn:** 嵌入非扭曲可取消面部模板生成<br />
**Authors:** Dmytro Zakharov, Oleksandr Kuznetsov, Emanuele Frontoni, Natalia Kryvinska<br />
**Abstract:** <details><summary>原文: </summary>Biometric authentication systems are crucial for security, but developing them involves various complexities, including privacy, security, and achieving high accuracy without directly storing pure biometric data in storage. We introduce an innovative image distortion technique that makes facial images unrecognizable to the eye but still identifiable by any custom embedding neural network model. Using the proposed approach, we test the reliability of biometric recognition networks by determining the maximum image distortion that does not change the predicted identity. Through experiments on MNIST and LFW datasets, we assess its effectiveness and compare it based on the traditional comparison metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>生物识别认证系统对于安全性至关重要，但开发它们涉及各种复杂性，包括隐私、安全性以及在不直接将纯生物识别数据存储在存储中的情况下实现高精度。我们引入了一种创新的图像失真技术，使人眼无法识别面部图像，但仍然可以通过任何自定义嵌入神经网络模型进行识别。使用所提出的方法，我们通过确定不改变预测身份的最大图像失真来测试生物识别网络的可靠性。通过在 MNIST 和 LFW 数据集上的实验，我们评估其有效性并基于传统的比较指标进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.02540v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Deep Supervision by Gaussian Pseudo-label-based Morphological Attention for Abdominal Aorta Segmentation in Non-Contrast CTs**<br />
**Title_cn:** 基于高斯伪标签的形态学关注对非造影 CT 腹主动脉分割的深度监督<br />
**Authors:** Qixiang Ma, Antoine Lucas, Adrien Kaladji, Pascal Haigron<br />
**Abstract:** <details><summary>原文: </summary>The segmentation of the abdominal aorta in non-contrast CT images is a non-trivial task for computer-assisted endovascular navigation, particularly in scenarios where contrast agents are unsuitable. While state-of-the-art deep learning segmentation models have been proposed recently for this task, they are trained on manually annotated strong labels. However, the inherent ambiguity in the boundary of the aorta in non-contrast CT may undermine the reliability of strong labels, leading to potential overfitting risks. This paper introduces a Gaussian-based pseudo label, integrated into conventional deep learning models through deep supervision, to achieve Morphological Attention (MA) enhancement. As the Gaussian pseudo label retains the morphological features of the aorta without explicitly representing its boundary distribution, we suggest that it preserves aortic morphology during training while mitigating the negative impact of ambiguous boundaries, reducing the risk of overfitting. It is introduced in various 2D/3D deep learning models and validated on our local data set of 30 non-contrast CT volumes comprising 5749 CT slices. The results underscore the effectiveness of MA in preserving the morphological characteristics of the aorta and addressing overfitting concerns, thereby enhancing the performance of the models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在非造影 CT 图像中分割腹主动脉对于计算机辅助血管内导航来说是一项艰巨的任务，特别是在造影剂不适合的情况下。虽然最近针对此任务提出了最先进的深度学习分割模型，但它们是在手动注释的强标签上进行训练的。然而，非造影 CT 中主动脉边界固有的模糊性可能会破坏强标签的可靠性，从而导致潜在的过度拟合风险。本文引入了一种基于高斯的伪标签，通过深度监督集成到传统的深度学习模型中，以实现形态注意力（MA）增强。由于高斯伪标签保留了主动脉的形态特征，但没有明确表示其边界分布，因此我们建议它在训练过程中保留主动脉形态，同时减轻边界模糊的负面影响，降低过度拟合的风险。它被引入各种 2D/3D 深度学习模型中，并在我们的 30 个非造影 CT 体积（包括 5749 个 CT 切片）的本地数据集上进行了验证。结果强调了 MA 在保留主动脉形态特征和解决过度拟合问题方面的有效性，从而提高了模型的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02514v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **VM-UNet: Vision Mamba UNet for Medical Image Segmentation**<br />
**Title_cn:** VM-UNet：用于医学图像分割的 Vision Mamba UNet<br />
**Authors:** Jiacheng Ruan, Suncheng Xiang<br />
**Abstract:** <details><summary>原文: </summary>In the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a U-shape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems. Our code is available at https://github.com/JCruan519/VM-UNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学图像分割领域，基于 CNN 和基于 Transformer 的模型都得到了广泛的探索。然而，CNN 在远程建模能力方面表现出局限性，而 Transformer 则受到其二次计算复杂性的阻碍。最近，以 Mamba 为代表的状态空间模型 (SSM) 已成为一种有前途的方法。它们不仅擅长对远程交互进行建模，而且还保持线性计算复杂性。在本文中，利用状态空间模型，我们提出了一种用于医学图像分割的 U 形架构模型，称为 Vision Mamba UNet（VM-UNet）。具体来说，引入视觉状态空间（VSS）块作为基础块来捕获广泛的上下文信息，并构建不对称的编码器-解码器结构。我们在 ISIC17、ISIC18 和 Synapse 数据集上进行了全面的实验，结果表明 VM-UNet 在医学图像分割任务中表现出竞争力。据我们所知，这是第一个基于纯SSM模型构建的医学图像分割模型。我们的目标是建立一个基线，并为未来开发更高效、更有效的基于 SSM 的分割系统提供有价值的见解。我们的代码可在 https://github.com/JCruan519/VM-UNet 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02491v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Deep Spectral Improvement for Unsupervised Image Instance Segmentation**<br />
**Title_cn:** 无监督图像实例分割的深度光谱改进<br />
**Authors:** Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei<br />
**Abstract:** <details><summary>原文: </summary>Deep spectral methods reframe the image decomposition process as a graph partitioning task by extracting features using self-supervised learning and utilizing the Laplacian of the affinity matrix to obtain eigensegments. However, instance segmentation has received less attention compared to other tasks within the context of deep spectral methods. This paper addresses the fact that not all channels of the feature map extracted from a self-supervised backbone contain sufficient information for instance segmentation purposes. In fact, Some channels are noisy and hinder the accuracy of the task. To overcome this issue, this paper proposes two channel reduction modules: Noise Channel Reduction (NCR) and Deviation-based Channel Reduction (DCR). The NCR retains channels with lower entropy, as they are less likely to be noisy, while DCR prunes channels with low standard deviation, as they lack sufficient information for effective instance segmentation. Furthermore, the paper demonstrates that the dot product, commonly used in deep spectral methods, is not suitable for instance segmentation due to its sensitivity to feature map values, potentially leading to incorrect instance segments. A new similarity metric called Bray-Curtis over Chebyshev (BoC) is proposed to address this issue. It takes into account the distribution of features in addition to their values, providing a more robust similarity measure for instance segmentation. Quantitative and qualitative results on the Youtube-VIS2019 dataset highlight the improvements achieved by the proposed channel reduction methods and the use of BoC instead of the conventional dot product for creating the affinity matrix. These improvements are observed in terms of mean Intersection over Union and extracted instance segments, demonstrating enhanced instance segmentation performance. The code is available on: https://github.com/farnooshar/SpecUnIIS</details>
**Abstract_cn:** <details><summary>译文: </summary>深谱方法通过使用自监督学习提取特征并利用亲和矩阵的拉普拉斯算子来获得特征段，将图像分解过程重新构建为图分割任务。然而，与深谱方法背景下的其他任务相比，实例分割受到的关注较少。本文解决了这样一个事实：并非从自监督主干提取的特征图的所有通道都包含用于实例分割目的的足够信息。事实上，有些通道存在噪音，阻碍了任务的准确性。为了克服这个问题，本文提出了两个通道减少模块：噪声通道减少（NCR）和基于偏差的通道减少（DCR）。 NCR 保留熵较低的通道，因为它们不太可能有噪声，而 DCR 修剪具有低标准偏差的通道，因为它们缺乏有效实例分割的足够信息。此外，本文还表明，深谱方法中常用的点积由于对特征图值敏感，不适合实例分割，可能导致不正确的实例分割。提出了一种名为 Bray-Curtis over Chebyshev (BoC) 的新相似性度量来解决这个问题。除了特征值之外，它还考虑特征的分布，为实例分割提供更强大的相似性度量。 Youtube-VIS2019 数据集上的定量和定性结果突出了所提出的通道缩减方法以及使用 BoC 而不是传统点积来创建亲和矩阵所实现的改进。这些改进是在平均交集和提取的实例段方面观察到的，证明了实例分割性能的增强。该代码位于：https://github.com/farnooshar/SpecUnIIS</details>
**PDF:** <http://arxiv.org/pdf/2402.02474v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition**<br />
**Title_cn:** 学习手拉手和人与人交互识别的互激励<br />
**Authors:** Mengyuan Liu, Chen Chen, Songtao Wu, Fanyang Meng, Hong Liu<br />
**Abstract:** <details><summary>原文: </summary>Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>识别交互动作，包括手与手的交互和人与人的交互，在视频分析和人机交互领域的各种应用引起了越来越多的关注。考虑到图卷积在从骨架数据建模拓扑感知特征方面取得的成功，最近的方法通常在单独的实体上操作图卷积，并使用后期融合进行交互动作识别，这几乎无法建模成对实体之间的相互语义关系。为此，我们通过堆叠互激励图卷积（me-GC）层提出了互激励图卷积网络（me-GCN）。具体来说，me-GC使用相互拓扑激励模块首先从各个实体中提取邻接矩阵，然后自适应地对它们之间的相互约束进行建模。此外，me-GC扩展了上述思想，并进一步使用相互特征激励模块从成对实体中提取和合并深层特征。与图卷积相比，我们提出的 me-GC 逐渐学习图卷积操作的每一层和每个阶段的互信息。在具有挑战性的手对手交互数据集（即 Assembely101 数据集）和两个大规模人与人交互数据集（即 NTU60-Interaction 和 NTU120-Interaction）上进行的大量实验一致验证了我们提出的方法的优越性，它优于最先进的基于 GCN 和基于 Transformer 的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.02431v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Exploiting Low-level Representations for Ultra-Fast Road Segmentation**<br />
**Title_cn:** 利用低级表示进行超快速道路分段<br />
**Authors:** Huan Zhou, Feng Xue, Yucong Li, Shi Gong, Yiqun Li, Yu Zhou<br />
**Abstract:** <details><summary>原文: </summary>Achieving real-time and accuracy on embedded platforms has always been the pursuit of road segmentation methods. To this end, they have proposed many lightweight networks. However, they ignore the fact that roads are "stuff" (background or environmental elements) rather than "things" (specific identifiable objects), which inspires us to explore the feasibility of representing roads with low-level instead of high-level features. Surprisingly, we find that the primary stage of mainstream network models is sufficient to represent most pixels of the road for segmentation. Motivated by this, we propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg). Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail branch is firstly designed to extract low-level feature representation for the road by the first stage of ResNet-18. To suppress texture-less regions mistaken as the road in the low-level feature, the context semantic branch is then designed to extract the context feature in a fast manner. To this end, in the second branch, we asymmetrically downsample the input image and design an aggregation module to achieve comparable receptive fields to the third stage of ResNet-18 but with less time consumption. Finally, to segment the road from the low-level feature, a selective fusion module is proposed to calculate pixel-wise attention between the low-level representation and context feature, and suppress the non-road low-level response by this attention. On KITTI-Road, LFD-RoadSeg achieves a maximum F1-measure (MaxF) of 95.21% and an average precision of 93.71%, while reaching 238 FPS on a single TITAN Xp and 54 FPS on a Jetson TX2, all with a compact model size of just 936k parameters. The source code is available at https://github.com/zhouhuan-hust/LFD-RoadSeg.</details>
**Abstract_cn:** <details><summary>译文: </summary>在嵌入式平台上实现实时性和准确性一直是道路分割方法的追求。为此，他们提出了很多轻量级网络。然而，他们忽略了道路是“东西”（背景或环境元素）而不是“事物”（特定的可识别对象）这一事实，这启发我们探索用低级特征而不是高级特征来表示道路的可行性。令人惊讶的是，我们发现主流网络模型的初级阶段足以表示道路的大部分像素进行分割。受此启发，我们提出了一种低级特征主导的道路分割网络（LFD-RoadSeg）。具体来说，LFD-RoadSeg采用双边结构。空间细节分支首先被设计为通过 ResNet-18 的第一阶段提取道路的低级特征表示。为了抑制低级特征中被误认为道路的无纹理区域，然后设计上下文语义分支以快速提取上下文特征。为此，在第二个分支中，我们对输入图像进行非对称下采样，并设计一个聚合模块，以实现与 ResNet-18 第三阶段相当的感受野，但消耗的时间更少。最后，为了从低级特征中分割道路，提出了一种选择性融合模块来计算低级表示和上下文特征之间的像素级注意力，并通过该注意力抑制非道路低级响应。在 KITTI-Road 上，LFD-RoadSeg 的最大 F1 测量 (MaxF) 为 95.21%，平均精度为 93.71%，同时在单个 TITAN Xp 上达到 238 FPS，在 Jetson TX2 上达到 54 FPS，所有这些都采用紧凑模型参数大小仅为 936k。源代码可在 https://github.com/zhouhuan-hust/LFD-RoadSeg 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02430v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **NOAH: Learning Pairwise Object Category Attentions for Image Classification**<br />
**Title_cn:** NOAH：学习图像分类的成对对象类别注意力<br />
**Authors:** Chao Li, Aojun Zhou, Anbang Yao<br />
**Abstract:** <details><summary>原文: </summary>A modern deep neural network (DNN) for image classification tasks typically consists of two parts: a backbone for feature extraction, and a head for feature encoding and class predication. We observe that the head structures of mainstream DNNs adopt a similar feature encoding pipeline, exploiting global feature dependencies while disregarding local ones. In this paper, we revisit the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that relies on a new form of dot-product attention called pairwise object category attention (POCA), efficiently exploiting spatially dense category-specific attentions to augment classification performance. NOAH introduces a neat combination of feature split, transform and merge operations to learn POCAs at local to global scales. As a drop-in design, NOAH can be easily used to replace existing heads of various types of DNNs, improving classification performance while maintaining similar model efficiency. We validate the effectiveness of NOAH on ImageNet classification benchmark with 25 DNN architectures spanning convolutional neural networks, vision transformers and multi-layer perceptrons. In general, NOAH is able to significantly improve the performance of lightweight DNNs, e.g., showing 3.14\%5.3\%1.9\% top-1 accuracy improvement to MobileNetV2 (0.5x)Deit-Tiny (0.5x)gMLP-Tiny (0.5x). NOAH also generalizes well when applied to medium-size and large-size DNNs. We further show that NOAH retains its efficacy on other popular multi-class and multi-label image classification benchmarks as well as in different training regimes, e.g., showing 3.6\%1.1\% mAP improvement to large ResNet101ViT-Large on MS-COCO dataset. Project page: https://github.com/OSVAI/NOAH.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于图像分类任务的现代深度神经网络（DNN）通常由两部分组成：用于特征提取的主干，以及用于特征编码和类别预测的头部。我们观察到主流 DNN 的头部结构采用类似的特征编码管道，利用全局特征依赖性，同时忽略局部特征依赖性。在本文中，我们重新审视特征编码问题，并提出非全局注意力头（NOAH），它依赖于一种新形式的点积注意力，称为成对对象类别注意力（POCA），有效地利用空间密集的类别特定注意力来增强分类性能。 NOAH 引入了特征分割、转换和合并操作的巧妙组合，以在局部到全局范围内学习 POCA。作为一种嵌入式设计，NOAH 可以轻松地用于替换各类 DNN 的现有头，在提高分类性能的同时保持相似的模型效率。我们使用 25 个涵盖卷积神经网络、视觉变换器和多层感知器的 DNN 架构在 ImageNet 分类基准上验证了 NOAH 的有效性。总的来说，NOAH 能够显着提高轻量级 DNN 的性能，例如，MobileNetV2 (0.5x)Deit-Tiny (0.5x)gMLP-Tiny (0.5) 的 top-1 精度提高了 3.14\%5.3\%1.9\% X）。当应用于中型和大型 DNN 时，NOAH 也具有很好的泛化能力。我们进一步表明，NOAH 在其他流行的多类和多标签图像分类基准以及不同的训练方案中保留了其功效，例如，在 MS-COCO 数据集上，大型 ResNet101ViT-Large 的 mAP 提高了 3.6\%1.1\% 。项目页面：https://github.com/OSVAI/NOAH。</details>
**PDF:** <http://arxiv.org/pdf/2402.02377v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation**<br />
**Title_cn:** 探索医学图像的内在属性以进行自监督二元语义分割<br />
**Authors:** Pranav Singh, Jacopo Cirrone<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors. This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce. Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation. In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation. We evaluate MedSASS against existing state- of-the-art methods across four diverse medical datasets, showcasing its superiority. MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to existing state-of-the-art self-supervised strategies.</details>
**Abstract_cn:** <details><summary>译文: </summary>自监督学习的最新进展释放了利用未标记数据进行辅助任务的潜力，促进了有益先​​验的学习。这在医学图像分析等标记数据稀缺的领域尤其有利。尽管对于分类任务有效，但这种方法在更复杂的应用（例如医学图像分割）中显示出局限性。在本文中，我们介绍了动态自适应语义分割增强医学成像（MedSASS），这是一种专为医学图像分割量身定制的专用自监督框架。我们在四个不同的医学数据集中针对现有最先进的方法评估了 MedSASS，展示了其优越性。 MedSASS 的性能比现有的基于 CNN 的自监督方法高出 3.83%，并且与基于 ViT 的方法的性能相当。此外，当 MedSASS 进行端到端训练（涵盖编码器和解码器）时，与现有最先进的自监督策略相比，CNN 显着提高了 14.4%，基于 ViT 的架构显着提高了 6%。</details>
**PDF:** <http://arxiv.org/pdf/2402.02367v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Region-Based Representations Revisited**<br />
**Title_cn:** 重新审视基于区域的表示<br />
**Authors:** Michal Shlapentokh-Rothman, Ansel Blume, Yao Xiao, Yuqun Wu, Sethuraman T V, Heyi Tao, Jae Yong Lee, Wilfredo Torres, Yu-Xiong Wang, Derek Hoiem<br />
**Abstract:** <details><summary>原文: </summary>We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong unsupervised representations like DINOv2 and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The compactness of the representation also makes it well-suited to video analysis and other problems requiring inference across many images.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究基于区域的表示对于识别是否有效。区域曾经是识别方法的支柱，但现在几乎只使用基于像素和块的特征。我们表明，像 SAM 这样的最新类不可知分割器可以有效地与 DINOv2 等强大的无监督表示相结合，并用于各种任务，包括语义分割、基于对象的图像检索和多图像分析。一旦提取了掩码和特征，即使使用线性解码器，这些表示也可以实现具有竞争力的性能，使它们非常适合需要自定义查询的应用程序。表示的紧凑性也使其非常适合视频分析和其他需要跨多个图像进行推理的问题。</details>
**PDF:** <http://arxiv.org/pdf/2402.02352v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning**<br />
**Title_cn:** 点云很重要：重新思考不同观察空间对机器人学习的影响<br />
**Authors:** Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, Tong He<br />
**Abstract:** <details><summary>原文: </summary>In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help design more generalizable and robust robotic models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本研究中，我们探讨了不同观察空间对机器人学习的影响，重点关注三种主要模式：RGB、RGB-D 和点云。通过在两个基准测试和模拟器上进行的超过 17 种不同的接触丰富的操作任务的广泛实验，我们观察到了一个显着的趋势：基于点云的方法，即使是那些设计最简单的方法，在表现。这在两种情况下都保持一致：从头开始训练和利用预训练。此外，我们的研究结果表明，点云观察可以改善与各种几何和视觉线索相关的策略零样本泛化，包括摄像机视点、照明条件、噪声水平和背景外观。结果表明，3D 点云是复杂机器人任务的一种有价值的观察方式。我们将开源所有代码和检查点，希望我们的见解能够帮助设计更通用和更强大的机器人模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.02500v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Key-Graph Transformer for Image Restoration**<br />
**Title_cn:** 用于图像恢复的关键图转换器<br />
**Authors:** Bin Ren, Yawei Li, Jingyun Liang, Rakesh Ranjan, Mengyuan Liu, Rita Cucchiara, Luc Van Gool, Nicu Sebe<br />
**Abstract:** <details><summary>原文: </summary>While it is crucial to capture global information for effective image restoration (IR), integrating such cues into transformer-based methods becomes computationally expensive, especially with high input resolution. Furthermore, the self-attention mechanism in transformers is prone to considering unnecessary global cues from unrelated objects or regions, introducing computational inefficiencies. In response to these challenges, we introduce the Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch features as graph nodes. The proposed Key-Graph Constructor efficiently forms a sparse yet representative Key-Graph by selectively connecting essential nodes instead of all the nodes. Then the proposed Key-Graph Attention is conducted under the guidance of the Key-Graph only among selected nodes with linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed KGT's state-of-the-art performance, showcasing advancements both quantitatively and qualitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然捕获全局信息对于有效的图像恢复 (IR) 至关重要，但将此类线索集成到基于转换器的方法中会导致计算成本高昂，尤其是在高输入分辨率的情况下。此外，变压器中的自注意力机制很容易考虑来自不相关对象或区域的不必要的全局线索，从而导致计算效率低下。为了应对这些挑战，我们在本文中引入了Key-Graph Transformer（KGT）。具体来说，KGT 将补丁特征视为图节点。所提出的密钥图构造函数通过有选择地连接基本节点而不是所有节点，有效地形成稀疏但具有代表性的密钥图。然后，所提出的关键图注意仅在每个窗口内具有线性计算复杂度的选定节点之间在关键图的指导下进行。 6 项 IR 任务的广泛实验证实了所提出的 KGT 的最先进性能，展示了定量和定性方面的进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.02634v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion**<br />
**Title_cn:** 用于 X 射线到 CT 图像融合的完全可微相关驱动的 2D/3D 配准<br />
**Authors:** Minheng Chen, Zhirun Zhang, Shuheng Gu, Zhangyang Ge, Youyong Kong<br />
**Abstract:** <details><summary>原文: </summary>Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approaches and the conventional optimization-based baseline.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于图像的刚性 2D/3D 配准是荧光镜引导手术干预的关键技术。近年来，一些基于学习的完全可微方法已经产生了有益的成果，但特征提取和梯度流传输过程仍然缺乏可控性和可解释性。为了缓解这些问题，在这项工作中，我们提出了一种新型的完全可微相关驱动网络，使用双分支 CNN 变换器编码器，使网络能够从高频局部特征中提取和分离低频全局特征。进一步提出了一种相关驱动损失，用于基于嵌入信息的低频特征和高频特征分解。此外，我们的工作中还应用了学习近似凸形状相似函数的训练策略。我们在内部数据集上测试了我们的方法，并表明它优于现有的完全可微的基于学习的注册方法和传统的基于优化的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.02498v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied Scenarios**<br />
**Title_cn:** GNSS 拒绝场景中的角度鲁棒性无人机导航<br />
**Authors:** Yuxin Wang, Zunlei Feng, Haofei Zhang, Yang Gao, Jie Lei, Li Sun, Mingli Song<br />
**Abstract:** <details><summary>原文: </summary>Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation. However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings. In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks. Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation. To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368. Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing. Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于在极端条件下无法接收全球导航卫星系统（GNSS）的信号，实现无人机（UAV）的精确和鲁棒导航是一项具有挑战性的任务。最近出现的基于视觉的导航已成为基于 GNSS 的导航的一种有前途且可行的替代方案。然而，现有的基于视觉的技术不足以解决实际环境中由环境干扰和不准确的位置预测引起的飞行偏差。在本文中，我们提出了一种新颖的角度鲁棒性导航范例来处理点对点导航任务中的飞行偏差。此外，我们提出了一个模型，包括自适应特征增强模块、跨知识注意力引导模块和鲁棒的面向任务的头部模块，以准确预测高精度导航的方向角。为了评估基于视觉的导航方法，我们收集了一个名为 UAV_AR368 的新数据集。此外，我们还利用Google Earth设计了模拟飞行测试仪（SFTI）来模拟不同的飞行环境，从而减少与真实飞行测试相关的费用。实验结果表明，所提出的模型在理想和干扰情况下到达成功率分别提高了 26.0% 和 45.6%，优于现有技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.02405v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning**<br />
**Title_cn:** 从视觉提示中学习语义代理，以在深度度量学习中进行参数高效的微调<br />
**Authors:** Li Ren, Chen Chen, Liqiang Wang, Kien Hua<br />
**Abstract:** <details><summary>原文: </summary>Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance. We conduct extensive experiments to demonstrate that our proposed framework is effective and efficient by evaluating popular DML benchmarks. In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度度量学习（DML）作为一个关键目标长期以来一直吸引着机器学习社区的关注。现有的解决方案集中于对传统图像数据集上的预训练模型进行微调。由于最近从大规模数据集训练的预训练模型取得了成功，因此在保留先前获得的知识的同时使模型适应本地数据域中的 DML 任务具有挑战性。在本文中，我们研究了用于微调 DML 任务的预训练模型的参数有效方法。特别是，我们提出了一种基于在预训练视觉变压器（ViT）中学习视觉提示（VPT）的新颖且有效的框架。基于传统的基于代理的 DML 范例，我们通过合并来自输入图像和 ViT 的语义信息来增强代理，其中我们优化了每个类别的视觉提示。我们证明了我们对语义信息的新近似优于代表性能力，从而提高了度量学习性能。我们进行了大量的实验，通过评估流行的 DML 基准来证明我们提出的框架是有效且高效的。特别是，我们证明了我们的微调方法可以达到与最近最先进的 DML 完全微调工作相当甚至更好的性能，同时仅调整总参数的一小部分。</details>
**PDF:** <http://arxiv.org/pdf/2402.02340v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Multiplexed all-optical permutation operations using a reconfigurable diffractive optical network**<br />
**Title_cn:** 使用可重构衍射光网络的多路复用全光排列运算<br />
**Authors:** Guangdong Ma, Xilin Yang, Bijie Bai, Jingxi Li, Yuhang Li, Tianyi Gan, Che-Yung Shen, Yijie Zhang, Yuzhu Li, Mona Jarrahi, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Large-scale and high-dimensional permutation operations are important for various applications in e.g., telecommunications and encryption. Here, we demonstrate the use of all-optical diffractive computing to execute a set of high-dimensional permutation operations between an input and output field-of-view through layer rotations in a diffractive optical network. In this reconfigurable multiplexed material designed by deep learning, every diffractive layer has four orientations: 0, 90, 180, and 270 degrees. Each unique combination of these rotatable layers represents a distinct rotation state of the diffractive design tailored for a specific permutation operation. Therefore, a K-layer rotatable diffractive material is capable of all-optically performing up to 4^K independent permutation operations. The original input information can be decrypted by applying the specific inverse permutation matrix to output patterns, while applying other inverse operations will lead to loss of information. We demonstrated the feasibility of this reconfigurable multiplexed diffractive design by approximating 256 randomly selected permutation matrices using K=4 rotatable diffractive layers. We also experimentally validated this reconfigurable diffractive network using terahertz radiation and 3D-printed diffractive layers, providing a decent match to our numerical results. The presented rotation-multiplexed diffractive processor design is particularly useful due to its mechanical reconfigurability, offering multifunctional representation through a single fabrication process.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模和高维置换运算对于电信和加密等各种应用非常重要。在这里，我们演示了使用全光衍射计算通过衍射光学网络中的层旋转在输入和输出视场之间执行一组高维排列操作。在这种通过深度学习设计的可重构多重材料中，每个衍射层都有四个方向：0、90、180 和 270 度。这些可旋转层的每个独特组合代表了专为特定排列操作定制的衍射设计的独特旋转状态。因此，K层可旋转衍射材料能够全光学地执行多达4^K次独立排列操作。通过将特定的逆置换矩阵应用于输出模式可以解密原始输入信息，而应用其他逆运算将导致信息丢失。我们通过使用 K=4 可旋转衍射层近似 256 个随机选择的排列矩阵，证明了这种可重构多重衍射设计的可行性。我们还使用太赫兹辐射和 3D 打印衍射层对这种可重构衍射网络进行了实验验证，与我们的数值结果非常匹配。所提出的旋转复用衍射处理器设计由于其机械可重构性而特别有用，通过单一制造工艺提供多功能表示。</details>
**PDF:** <http://arxiv.org/pdf/2402.02397v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation**<br />
**Title_cn:** 3D 人体姿势估计的不确定性感知测试时间优化<br />
**Authors:** Ti Wang, Mengyuan Liu, Hong Liu, Bin Ren, Yingxuan You, Wenhao Li, Nicu Sebe, Xia Li<br />
**Abstract:** <details><summary>原文: </summary>Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a latent state. Projection loss is then employed to ensure the generated poses are well aligned in 2D space for high-quality optimization. Furthermore, we utilize the uncertainty of each joint to determine how much each joint is allowed for optimization. The effectiveness and superiority of the proposed framework are validated through extensive experiments on two challenging datasets: Human3.6M and MPI-INF-3DHP. Notably, our approach outperforms the previous best result by a large margin of 4.5% on Human3.6M. Our source code will be open-sourced.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管数据驱动的方法在 3D 人体姿态估计方面取得了成功，但它们经常存在领域差距，并且泛化能力有限。相比之下，基于优化的方法在针对特定情况进行微调方面表现出色，但在整体性能方面通常不如数据驱动的方法。我们观察到，以前基于优化的方法通常依赖于投影约束，这只能确保二维空间中的对齐，可能导致过度拟合问题。为了解决这个问题，我们提出了一种不确定性感知测试时间优化（UAO）框架，该框架保留预训练模型的先验信息，并利用关节的不确定性缓解过拟合问题。具体来说，在训练阶段，我们设计了一个有效的 2D 到 3D 网络，用于估计相应的 3D 姿态，同时量化每个 3D 关节的不确定性。对于测试期间的优化，所提出的优化框架冻结预训练模型并仅优化潜在状态。然后利用投影损失来确保生成的姿势在 2D 空间中良好对齐，以实现高质量优化。此外，我们利用每个关节的不确定性来确定每个关节允许优化的程度。通过对两个具有挑战性的数据集 Human3.6M 和 MPI-INF-3DHP 进行大量实验，验证了所提出框架的有效性和优越性。值得注意的是，我们的方法在 Human3.6M 上比之前的最佳结果高出 4.5%。我们的源代码将是开源的。</details>
**PDF:** <http://arxiv.org/pdf/2402.02339v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization**<br />
**Title_cn:** CNS-Edit：通过耦合神经形状优化进行 3D 形状编辑<br />
**Authors:** Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Hao Zhang, Chi-Wing Fu<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a new approach based on a coupled representation and a neural volume optimization to implicitly perform 3D shape editing in latent space. This work has three innovations. First, we design the coupled neural shape (CNS) representation for supporting 3D shape editing. This representation includes a latent code, which captures high-level global semantics of the shape, and a 3D neural feature volume, which provides a spatial context to associate with the local shape changes given by the editing. Second, we formulate the coupled neural shape optimization procedure to co-optimize the two coupled components in the representation subject to the editing operation. Last, we offer various 3D shape editing operators, i.e., copy, resize, delete, and drag, and derive each into an objective for guiding the CNS optimization, such that we can iteratively co-optimize the latent code and neural feature volume to match the editing target. With our approach, we can achieve a rich variety of editing results that are not only aware of the shape semantics but are also not easy to achieve by existing approaches. Both quantitative and qualitative evaluations demonstrate the strong capabilities of our approach over the state-of-the-art solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种基于耦合表示和神经体积优化的新方法，可在潜在空间中隐式执行 3D 形状编辑。本工作有三点创新。首先，我们设计耦合神经形状 (CNS) 表示以支持 3D 形状编辑。该表示包括一个潜在代码，它捕获形状的高级全局语义，以及一个 3D 神经特征体积，它提供一个空间上下文来与编辑给出的局部形状变化相关联。其次，我们制定耦合神经形状优化程序，以共同优化受编辑操作影响的表示中的两个耦合组件。最后，我们提供各种 3D 形状编辑操作符，即复制、调整大小、删除和拖动，并将每个操作符导出为指导 CNS 优化的目标，以便我们可以迭代地共同优化潜在代码和神经特征量以匹配编辑目标。通过我们的方法，我们可以实现丰富多样的编辑结果，这些结果不仅了解形状语义，而且现有方法也不容易实现。定量和定性评估都证明了我们的方法相对于最先进的解决方案的强大能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.02313v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **BECLR: Batch Enhanced Contrastive Few-Shot Learning**<br />
**Title_cn:** BECLR：批量增强对比小样本学习<br />
**Authors:** Stylianos Poulakakis-Daktylidis, Hadi Jamali-Rad<br />
**Abstract:** <details><summary>原文: </summary>Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios where FSL approaches suffer the most from sample bias. We later on discuss that DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we coin as BECLR), constructively magnifying each other's impact. We then present a suite of extensive quantitative and qualitative experimentation to corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL benchmarks (to the best of our knowledge), and significantly outperforms the best of the current baselines (codebase available at: https://github.com/stypoumic/BECLR).</details>
**Abstract_cn:** <details><summary>译文: </summary>从很少的标记样本中快速学习是深度表示学习时代区分机器和人类的基本属性。无监督少样本学习（U-FSL）希望通过在训练时放弃对注释的依赖来弥补这一差距。出于对对比学习方法在 U-FSL 领域的成功的兴趣，我们在预训练和下游推理阶段结构性地解决了它们的缺点。我们提出了一种新颖的动态集群内存（DyCE）模块，以促进高度可分离的潜在表示空间，以增强预训练阶段的正采样，并将隐式类级洞察注入无监督对比学习中。然后，我们在几次推理阶段解决了样本偏差的问题，这个问题在某种程度上被忽视了，但却是至关重要的。我们提出了一种基于迭代最优传输的分布对齐 (OpTA) 策略，并证明它可以有效解决该问题，特别是在 FSL 方法受样本偏差影响最大的低样本场景中。我们稍后讨论 DyCE 和 OpTA 是一种新颖的端到端方法（我们称之为 BECLR）的两个相互交织的部分，建设性地放大了彼此的影响。然后，我们提出了一系列广泛的定量和定性实验，以证实 BECLR 在所有现有 U-FSL 基准中设定了新的最先进水平（据我们所知），并且显着优于当前基准中的最佳基准（代码库位于：https://github.com/stypoumic/BECLR）。</details>
**PDF:** <http://arxiv.org/pdf/2402.02444v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving**<br />
**Title_cn:** SIMPL：用于自动驾驶的简单高效的多智能体运动预测基准<br />
**Authors:** Lu Zhang, Peiliang Li, Sikang Liu, Shaojie Shen<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 & 2 motion forecasting benchmarks compared with other state-of-the-art methods. Furthermore, its lightweight design and low inference latency make SIMPL highly extensible and promising for real-world onboard deployment. We open-source the code at https://github.com/HKUST-Aerial-Robotics/SIMPL.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种用于自动驾驶车辆的简单高效的运动预测基线（SIMPL）。与精度高但重复计算的传统以代理为中心的方法以及精度和通用性受到影响的以场景为中心的方法不同，SIMPL 为所有相关交通参与者提供实时、准确的运动预测。为了提高准确性和推理速度，我们提出了一种紧凑而高效的全局特征融合模块，该模块以对称方式执行定向消息传递，使网络能够在单次前馈传递中预测所有道路使用者的未来运动，并减轻视点移动导致的精度损失。此外，我们在轨迹解码中使用伯恩斯坦基多项式研究连续轨迹参数化，允许在任何所需时间点评估状态及其高阶导数，这对于下游规划任务很有价值。作为强大的基线，与其他最先进的方法相比，SIMPL 在 Argoverse 1 和 2 运动预测基准上表现出极具竞争力的性能。此外，其轻量级设计和低推理延迟使 SIMPL 具有高度可扩展性，并有望用于现实世界的机载部署。我们在 https://github.com/HKUST-Aerial-Robotics/SIMPL 开源代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.02519v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Uncertainty-Aware Perceiver**<br />
**Title_cn:** 不确定性感知感知器<br />
**Authors:** EuiYul Song<br />
**Abstract:** <details><summary>原文: </summary>The Perceiver makes few architectural assumptions about the relationship among its inputs with quadratic scalability on its memory and computation time. Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT in terms of accuracy to some degree. However, the Perceiver does not take predictive uncertainty and calibration into account. The Perceiver also generalizes its performance on three datasets, three models, one evaluation metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative performance improvement against other models is marginal. Furthermore, its reduction of architectural prior is not substantial; is not equivalent to its quality. Thereby, I invented five mutations of the Perceiver, the Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100, the Uncertainty-Aware Perceivers make considerable performance enhancement compared to the Perceiver.</details>
**Abstract_cn:** <details><summary>译文: </summary>感知器对其输入之间的关系及其内存和计算时间的二次可扩展性做出了很少的架构假设。事实上，Perceiver 模型在某种程度上的准确性超过了 ResNet-50 和 ViT，或者与 ResNet-50 和 ViT 具有竞争力。然而，感知器没有考虑预测不确定性和校准。感知器还概括了其在三个数据集、三个模型、一个评估指标和一个超参数设置上的性能。最糟糕的是，感知器相对于其他模型的相对性能改进是微乎其微的。此外，它对架构先验的减少并不显着；不等于它的质量。因此，我发明了感知器的五种突变，即不确定性感知感知器，它们可以获得不确定性估计并根据三个指标测量其性能。通过 CIFAR-10 和 CIFAR-100 的实验，与感知器相比，不确定性感知感知器的性能得到了显着增强。</details>
**PDF:** <http://arxiv.org/pdf/2402.02433v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Physics-Inspired Degradation Models for Hyperspectral Image Fusion**<br />
**Title_cn:** 用于高光谱图像融合的物理启发退化模型<br />
**Authors:** Jie Lian, Lizhi Wang, Lin Zhu, Renwei Dian, Zhiwei Xiong, Hua Huang<br />
**Abstract:** <details><summary>原文: </summary>The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) has garnered increasing research interest. However, most fusion methods solely focus on the fusion algorithm itself and overlook the degradation models, which results in unsatisfactory performance in practical scenarios. To fill this gap, we propose physics-inspired degradation models (PIDM) to model the degradation of LR-HSI and HR-MSI, which comprises a spatial degradation network (SpaDN) and a spectral degradation network (SpeDN). SpaDN and SpeDN are designed based on two insights. First, we employ spatial warping and spectral modulation operations to simulate lens aberrations, thereby introducing non-uniformity into the spatial and spectral degradation processes. Second, we utilize asymmetric downsampling and parallel downsampling operations to separately reduce the spatial and spectral resolutions of the images, thus ensuring the matching of spatial and spectral degradation processes with specific physical characteristics. Once SpaDN and SpeDN are established, we adopt a self-supervised training strategy to optimize the network parameters and provide a plug-and-play solution for fusion methods. Comprehensive experiments demonstrate that our proposed PIDM can boost the fusion performance of existing fusion methods in practical scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>低空间分辨率高光谱图像（LR-HSI）与高空间分辨率多光谱图像（HR-MSI）的融合引起了越来越多的研究兴趣。然而，大多数融合方法仅仅关注融合算法本身，而忽略了退化模型，导致实际场景中的性能不理想。为了填补这一空白，我们提出了物理启发退化模型（PIDM）来模拟 LR-HSI 和 HR-MSI 的退化，其中包括空间退化网络（SpaDN）和频谱退化网络（SpeDN）。 SpaDN 和 SpeDN 的设计基于两个见解。首先，我们采用空间扭曲和光谱调制操作来模拟镜头像差，从而在空间和光谱退化过程中引入不均匀性。其次，我们利用非对称下采样和并行下采样操作分别降低图像的空间和光谱分辨率，从而确保空间和光谱退化过程与特定物理特征的匹配。 SpaDN和SpeDN建立后，我们采用自监督训练策略来优化网络参数，并为融合方法提供即插即用的解决方案。综合实验表明，我们提出的 PIDM 可以在实际场景中提高现有融合方法的融合性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02411v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **AI-Generated Content Enhanced Computer-Aided Diagnosis Model for Thyroid Nodules: A ChatGPT-Style Assistant**<br />
**Title_cn:** AI 生成的内容增强型甲状腺结节计算机辅助诊断模型：ChatGPT 式助手<br />
**Authors:** Jincao Yao, Yunpeng Wang, Zhikai Lei, Kai Wang, Xiaoxian Li, Jianhua Zhou, Xiang Hao, Jiafei Shen, Zhenping Wang, Rongrong Ru, et.al.<br />
**Abstract:** <details><summary>原文: </summary>An artificial intelligence-generated content-enhanced computer-aided diagnosis (AIGC-CAD) model, designated as ThyGPT, has been developed. This model, inspired by the architecture of ChatGPT, could assist radiologists in assessing the risk of thyroid nodules through semantic-level human-machine interaction. A dataset comprising 19,165 thyroid nodule ultrasound cases from Zhejiang Cancer Hospital was assembled to facilitate the training and validation of the model. After training, ThyGPT could automatically evaluate thyroid nodule and engage in effective communication with physicians through human-computer interaction. The performance of ThyGPT was rigorously quantified using established metrics such as the receiver operating characteristic (ROC) curve, area under the curve (AUC), sensitivity, and specificity. The empirical findings revealed that radiologists, when supplemented with ThyGPT, markedly surpassed the diagnostic acumen of their peers utilizing traditional methods as well as the performance of the model in isolation. These findings suggest that AIGC-CAD systems, exemplified by ThyGPT, hold the promise to fundamentally transform the diagnostic workflows of radiologists in forthcoming years.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能生成的内容增强计算机辅助诊断 (AIGC-CAD) 模型（命名为 ThyGPT）已经开发出来。该模型受到 ChatGPT 架构的启发，可以帮助放射科医生通过语义级人机交互评估甲状腺结节的风险。汇集了来自浙江省肿瘤医院的 19,165 例甲状腺结节超声病例数据集，以方便模型的训练和验证。经过训练，ThyGPT可以自动评估甲状腺结节，并通过人机交互与医生进行有效沟通。 ThyGPT 的性能使用既定指标（例如受试者工作特征 (ROC) 曲线、曲线下面积 (AUC)、灵敏度和特异性）进行严格量化。实证结果显示，放射科医生在补充 ThyGPT 后，其诊断敏锐度以及模型的孤立性能明显超过了使用传统方法的同行。这些发现表明，以 ThyGPT 为代表的 AIGC-CAD 系统有望在未来几年从根本上改变放射科医生的诊断工作流程。</details>
**PDF:** <http://arxiv.org/pdf/2402.02401v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Revisiting the Power of Prompt for Visual Tuning**<br />
**Title_cn:** 重新审视视觉调整提示的力量<br />
**Authors:** Yuzhu Wang, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Manni Duan, Meng Wang<br />
**Abstract:** <details><summary>原文: </summary>Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin. For instance, it surpasses full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable parameters on the FGVC and VTAB-1K benchmarks. Notably, our method significantly advances the adaptation for self-supervised pretraining, achieving impressive task performance gains of at least 10% to 30%. Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size. We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉提示调整（VPT）是一种很有前景的解决方案，它结合了可学习的提示标记来为下游任务定制预训练模型。然而，VPT 及其变体经常遇到诸如提示初始化、提示长度和自监督预训练中表现不佳等挑战，阻碍了成功的上下文适应。本研究首先探讨熟练训练期间提示和补丁标记之间的相关性演变。受到提示令牌倾向于与补丁令牌共享高互信息的观察的启发，我们建议使用下游令牌原型初始化提示。策略初始化是之前初始化的替代，大大提高了微调的性能。为了进一步完善，我们通过简化的管道优化了代币构建，与 VPT 相比，该管道保持了出色的性能，几乎没有增加计算费用。详尽的实验表明，我们提出的方法明显优于现有方法。例如，它在 24 项任务中的 19 项中超越了完全微调，在 FGVC 和 VTAB-1K 基准测试中使用了不到 0.4% 的可学习参数。值得注意的是，我们的方法显着提高了自我监督预训练的适应性，实现了至少 10% 至 30% 的令人印象深刻的任务绩效提升。此外，实验结果表明，所提出的 SPT 对于提示长度具有鲁棒性，并且可以很好地适应模型容量和训练数据大小。最后，我们对目标数据量进行了深入的探索，以促进预训练模型适应下游任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.02382v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Stereographic Spherical Sliced Wasserstein Distances**<br />
**Title_cn:** 立体球面切片 Wasserstein 距离<br />
**Authors:** Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu, Rocio Diaz Martin, Soheil Kolouri<br />
**Abstract:** <details><summary>原文: </summary>Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>比较球形概率分布在地质学、医学领域、计算机视觉和深度表示学习等各个领域都引起了极大的兴趣。基于最佳传输的距离（例如 Wasserstein 距离）用于比较概率度量的实用性激发了人们积极研究开发这些距离的计算有效的变化以用于球形概率度量。本文介绍了一种高速且高度可并行化的距离，用于使用立体投影和广义 Radon 变换来比较球形测量，我们将其称为立体球形切片 Wasserstein (S3W) 距离。我们仔细解决了由立体投影引起的距离失真，并对我们提出的度量及其旋转不变变化进行了广泛的理论分析。最后，我们评估所提出的指标的性能，并通过广泛的数值研究（包括梯度流和自监督学习）将其与最近的基线在速度和准确性方面进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.02345v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Video Editing for Video Retrieval**<br />
**Title_cn:** 用于视频检索的视频编辑<br />
**Authors:** Bin Zhu, Kevin Flanagan, Adriano Fragomeni, Michael Wray, Dima Damen<br />
**Abstract:** <details><summary>原文: </summary>Though pre-training vision-language models have demonstrated significant benefits in boosting video-text retrieval performance from large-scale web videos, fine-tuning still plays a critical role with manually annotated clips with start and end times, which requires considerable human effort. To address this issue, we explore an alternative cheaper source of annotations, single timestamps, for video-text retrieval. We initialise clips from timestamps in a heuristic way to warm up a retrieval model. Then a video clip editing method is proposed to refine the initial rough boundaries to improve retrieval performance. A student-teacher network is introduced for video clip editing. The teacher model is employed to edit the clips in the training set whereas the student model trains on the edited clips. The teacher weights are updated from the student's after the student's performance increases. Our method is model agnostic and applicable to any retrieval models. We conduct experiments based on three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip. Experiments conducted on three video retrieval datasets, YouCook2, DiDeMo and ActivityNet-Captions show that our edited clips consistently improve retrieval performance over initial clips across all the three retrieval models.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管预训练视觉语言模型在提高大规模网络视频的视频文本检索性能方面已显示出显着的优势，但对于带有开始和结束时间的手动注释剪辑，微调仍然发挥着关键作用，这需要大量的人力。为了解决这个问题，我们探索了另一种更便宜的注释来源，即单个时间戳，用于视频文本检索。我们以启发式的方式从时间戳初始化剪辑来预热检索模型。然后提出了一种视频剪辑编辑方法来细化初始粗略边界以提高检索性能。引入了学生-教师网络来进行视频剪辑编辑。教师模型用于编辑训练集中的剪辑，而学生模型则对编辑后的剪辑进行训练。学生成绩提高后，教师权重会根据学生的权重进行更新。我们的方法与模型无关，适用于任何检索模型。我们基于三种最先进的检索模型 COOT、VideoCLIP 和 CLIP4Clip 进行实验。在三个视频检索数据集 YouCook2、DiDeMo 和 ActivityNet-Captions 上进行的实验表明，我们编辑的剪辑在所有三个检索模型中始终比初始剪辑提高了检索性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02335v1><br />
**Code:** null<br />

