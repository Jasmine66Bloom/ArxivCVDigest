## [UPDATED!] **2024-02-05** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?**<br />
**Title_cn:** 扩散模型是否能够学习语义上有意义且有效的表示？<br />
**Authors:** Qiyao Liang, Ziming Liu, Ila Fiete<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型能够通过不常见的并置来生成令人印象深刻的图像，例如宇航员在月球上骑马并具有正确放置的阴影。这些输出表明执行组合泛化的能力，但是模型是如何做到这一点的呢？我们对条件 DDPM 进行受控实验，学习生成以指定 $x$- 和 $y$- 位置为中心的 2D 球形高斯凹凸。我们的结果表明，语义上有意义的潜在表示的出现是实现高性能的关键。在通过学习获得成功表现的过程中，该模型经历了潜在表示的三个不同阶段：（A 阶段）无潜在结构，（B 阶段）无序状态的 2D 流形，以及（C 阶段）2D 有序流形。对应于每个阶段，我们识别出不同性质的生成行为：1) 生成多个凹凸，2) 生成一个凹凸，但在不准确的 $x$ 和 $y$ 位置，3) 在正确的 $x 处生成凹凸$ 和 y 位置。此外，我们表明，即使在特征（$x$- 与 $y$-位置）以倾斜频率表示的不平衡数据集下，$x$ 和 $y$ 的学习过程也是耦合的而不是分解的，这证明了简单的香草-风格的扩散模型无法学习有效的表示，其中 $x$ 和 $y$ 中的定位被分解为单独的一维任务。这些发现表明，未来的工作需要找到归纳偏差，从而推动生成模型发现和利用其输入中的可因式分解的独立结构，这将需要将这些模型纳入数据效率更高的体系中。</details>
**PDF:** <http://arxiv.org/pdf/2402.03305v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models**<br />
**Title_cn:** GUARD：通过角色扮演生成自然语言越狱，以测试大型语言模型的准则遵守情况<br />
**Authors:** Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang<br />
**Abstract:** <details><summary>原文: </summary>The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>绕过大型语言模型 (LLM) 安全过滤器的“越狱”和有害响应的发现鼓励社区实施安全措施。一项主要的安全措施是在发布之前通过越狱主动测试法学硕士。因此，此类测试需要一种能够大规模且高效地生成越狱的方法。在本文中，我们遵循一种新颖而直观的策略来生成人类一代风格的越狱。我们提出了一个角色扮演系统，为用户法学硕士分配四个不同的角色，以协作完成新的越狱。此外，我们收集现有的越狱，并使用聚类频率和语义模式逐句将它们分成不同的独立特征。我们将这些特征组织成知识图谱，使它们更易于访问和检索。我们的不同角色系统将利用这个知识图谱来生成新的越狱，事实证明，这可以有效地诱导法学硕士产生不道德或违反指南的反应。此外，我们还在我们的系统中开创了一个设置，该设置将自动遵循政府发布的指导方针来生成越狱，以测试法学硕士是否相应地遵循指导方针。我们将我们的系统称为 GUARD（通过自适应角色扮演诊断维护指南）。我们通过实证验证了 GUARD 在三个尖端开源 LLM（Vicuna-13B、LongChat-7B 和 Llama-2-7B）以及广泛使用的商业 LLM (ChatGPT) 上的有效性。此外，我们的工作扩展到视觉语言模型（MiniGPT-v2 和 Gemini Vision Pro）领域，展示了 GUARD 的多功能性，并为跨不同模式开发更安全、更可靠的基于 LLM 的应用程序提供了宝贵的见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.03299v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Zero-shot Object-Level OOD Detection with Context-Aware Inpainting**<br />
**Title_cn:** 具有上下文感知修复功能的零样本对象级 OOD 检测<br />
**Authors:** Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le<br />
**Abstract:** <details><summary>原文: </summary>Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习算法越来越多地以黑盒云服务或预训练模型的形式提供，而无需访问其训练数据。这引发了零样本分布外（OOD）检测问题。具体来说，我们的目标是检测不属于分类器标签集但被错误分类为分布内（ID）对象的 OOD 对象。我们的方法 RONIN 使用现成的扩散模型来通过修复来替换检测到的对象。 RONIN 使用预测的 ID 标签来调节修复过程，使输入对象更接近分布域。因此，重建的对象在 ID 情况下非常接近原始对象，而在 OOD 情况下则远离原始对象，从而使 RONIN 能够有效地区分 ID 和 OOD 样本。通过大量的实验，我们证明了 RONIN 在多个数据集上（无论是零样本还是非零样本设置）与之前的方法相比都取得了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.03292v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **InstanceDiffusion: Instance-level Control for Image Generation**<br />
**Title_cn:** InstanceDiffusion：图像生成的实例级控制<br />
**Authors:** Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像扩散模型可生成高质量图像，但无法控制图像中的各个实例。我们引入了 InstanceDiffusion，它为文本到图像的扩散模型添加了精确的实例级控制。 InstanceDiffusion 支持每个实例的自由形式语言条件，并允许以灵活的方式指定实例位置，例如简单的单点、涂鸦、边界框或复杂的实例分割掩码及其组合。我们建议对文本到图像模型进行三项重大更改，以实现精确的实例级控制。我们的 UniFusion 模块支持文本到图像模型的实例级条件，ScaleU 模块提高了图像保真度，我们的多实例采样器改进了多个实例的生成。对于每个位置条件，InstanceDiffusion 都显着超越了专门的最先进模型。值得注意的是，在 COCO 数据集上，我们对于框输入的 AP$_{50}^\text{box}$ 优于之前最先进的技术 20.4%，对于掩码输入优于之前的最先进技术 25.4% IoU。</details>
**PDF:** <http://arxiv.org/pdf/2402.03290v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images**<br />
**Title_cn:** IGUANe：用于大脑 MR 图像多中心协调的 3D 通用 CycleGAN<br />
**Authors:** Vincent Roca, Grégory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes<br />
**Abstract:** <details><summary>原文: </summary>In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>在 MRI 研究中，来自多个采集部位的成像数据的聚合增加了样本量，但可能会引入与部位相关的变异性，从而阻碍后续分析的一致性。用于图像翻译的深度学习方法已成为跨站点协调 MR 图像的解决方案。在这项研究中，我们介绍了 IGUANe（统一对抗网络图像生成），这是一种原始的 3D 模型，它利用域翻译的优势和风格转移方法的直接应用来实现多中心脑 MR 图像协调。 IGUANe 通过多对一策略集成任意数量的域进行训练，从而扩展了 CycleGAN 架构。在推理过程中，该模型可以应用于任何图像，甚至来自未知的采集站点，使其成为通用的协调生成器。 IGUANe 在包含来自 11 个不同扫描仪的 T1 加权图像的数据集上进行训练，并根据来自看不见的站点的数据进行评估。评估包括 MR 图像随移动对象的变换、域内 MR 图像之间成对距离的保存、与年龄和阿尔茨海默病 (AD) 相关的体积模式的演变，以及年龄回归的表现和患者分类任务。与其他协调和归一化方法的比较表明，IGUANe 更好地保留了 MR 图像中的个体信息，并且更适合维护和增强与年龄和 AD 相关的变异性。未来的研究可能会在其他多中心环境中进一步评估 IGUANe，使用相同的模型或重新训练它以应用于不同的图像模式。</details>
**PDF:** <http://arxiv.org/pdf/2402.03227v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?**<br />
**Title_cn:** 有机还是扩散：我们可以区分人类艺术和人工智能生成的图像吗？<br />
**Authors:** Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao<br />
**Abstract:** <details><summary>原文: </summary>The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成式人工智能图像的出现彻底颠覆了艺术世界。从人类艺术中识别人工智能生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增长。如果未能解决这个问题，不良行为者就可以欺骗为人类艺术支付高价的个人，以及制定政策禁止人工智能图像的公司。这对于人工智能模型训练者来说也至关重要，他们需要过滤训练数据以避免潜在的模型崩溃。有几种不同的方法可以区分人类艺术和人工智能图像，包括通过监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其艺术技巧知识进行识别。在本文中，我们试图了解这些方法在良性和对抗性环境中对抗当今现代生成模型的表现如何。我们策划 7 种风格的真实人类艺术，从 5 个生成模型生成匹配图像，并应用 8 个检测器（5 个自动检测器和 3 个不同的人类群体，包括 180 名众包工作者、4000 多名专业艺术家和 13 名在检测 AI 方面经验丰富的专家艺术家）。 Hive 和专家艺术家都做得很好，但会以不同的方式犯错误（Hive 在对抗性扰动方面较弱，而专家艺术家会产生更高的误报）。我们相信，随着模型的不断发展，这些弱点仍将存在，并使用我们的数据来证明为什么人类和自动检测器的组合团队可以提供准确性和鲁棒性的最佳组合。</details>
**PDF:** <http://arxiv.org/pdf/2402.03214v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion**<br />
**Title_cn:** Direct-a-Video：通过用户控制的摄像机移动和对象运动生成定制视频<br />
**Authors:** Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao<br />
**Abstract:** <details><summary>原文: </summary>Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: https://direct-a-video.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的文本到视频传播模型取得了令人瞩目的进展。在实践中，用户通常希望能够独立控制对象运动和摄像机移动以进行定制视频创建。然而，当前的方法缺乏以解耦的方式单独控制对象运动和相机运动的重点，这限制了文本到视频模型的可控性和灵活性。在本文中，我们介绍了 Direct-a-Video，这是一种允许用户独立指定一个或多个对象的运动和/或摄像机运动的系统，就像导演视频一样。我们提出了一种简单而有效的策略来解耦控制物体运动和相机运动。对象运动是通过使用模型固有先验的空间交叉注意力调制来控制的，不需要额外的优化。对于相机运动，我们引入了新的时间交叉注意层来解释定量的相机运动参数。我们进一步采用基于增强的方法在小规模数据集上以自我监督的方式训练这些层，从而消除了显式运动注释的需要。两个组件独立运行，允许单独或组合控制，并且可以推广到开放域场景。大量的实验证明了我们方法的优越性和有效性。项目页面：https://direct-a-video.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.03162v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics**<br />
**Title_cn:** 超越对抗性扰动：具有合法语义的多方面辅助对抗性示例<br />
**Authors:** Shuai Li, Xiaoyu Jiang, Xiaoguang Ma<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic adversarial examples. The code and pre-trained models were available at https://github.com/shuaili1027/MAELS.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络非常容易受到恶意微小扰动操纵的对抗性示例的影响。尽管大多数传统的对抗性攻击通过最小化它们的几何距离来确保对抗性示例和相应的原始图像之间的视觉不可感知性，但这些对几何距离的限制导致有限的攻击可转移性、较差的视觉质量和人类不可感知的可解释性。在本文中，我们提出了一种有监督的语义转换生成模型来生成具有真实合法语义的对抗性示例，其中首次构建了包含连续语义变化的无限制对抗性流形，以实现从非对抗性示例到对抗性示例的合法过渡那些。对 MNIST 和工业缺陷数据集的综合实验表明，我们的对抗性示例不仅表现出更好的视觉质量，而且实现了卓越的攻击可转移性和对模型漏洞的更有效解释，表明它们作为通用对抗性示例的巨大潜力。代码和预训练模型可在 https://github.com/shuaili1027/MAELS.git 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03095v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing**<br />
**Title_cn:** 视觉文本满足低级视觉：视觉文本处理的综合调查<br />
**Authors:** Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou<br />
**Abstract:** <details><summary>原文: </summary>Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks. Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets. Finally, we identify principal challenges and potential avenues for future research. Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉文本是文档和场景图像中的关键元素，在计算机视觉领域具有重要意义并引起了广泛关注。除了视觉文本检测和识别之外，在基本生成模型的出现的推动下，视觉文本处理领域的研究也经历了激增。然而，由于文本与一般对象的独特属性和特征，挑战仍然存在。正如我们的研究中所观察到的，有效利用这些独特的文本特征对于视觉文本处理至关重要。在这项调查中，我们对该领域的最新进展进行了全面、多角度的分析。最初，我们引入了一种分层分类法，涵盖从文本图像增强和恢复到文本图像操作的各个领域，然后是不同的学习范例。随后，我们深入讨论了结构、笔画、语义、风格和空间上下文等特定文本特征如何无缝集成到各种任务中。此外，我们探索可用的公共数据集，并在几个广泛使用的数据集上对审查的方法进行基准测试。最后，我们确定了未来研究的主要挑战和潜在途径。我们的目标是将这项调查作为基本资源，促进视觉文本处理动态领域的持续探索和创新。</details>
**PDF:** <http://arxiv.org/pdf/2402.03082v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **PFDM: Parser-Free Virtual Try-on via Diffusion Model**<br />
**Title_cn:** PFDM：通过扩散模型进行无解析器虚拟试戴<br />
**Authors:** Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang<br />
**Abstract:** <details><summary>原文: </summary>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can "wear" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models.</details>
**Abstract_cn:** <details><summary>译文: </summary>虚拟试穿可以显着改善在线和店内场景的服装购物体验，吸引了计算机视觉的广泛兴趣。然而，为了实现高保真试戴性能，大多数最先进的方法仍然依赖于准确的分割掩模，这些掩模通常是由近乎完美的解析器或手动标记生成的。为了克服这一瓶颈，我们提出了一种基于扩散模型（PFDM）的无解析器虚拟试戴方法。给定两张图像，PFDM 可以通过隐式变形在没有任何其他信息的情况下无缝地“穿着”目标人物的衣服。为了有效地学习模型，我们合成了许多伪图像并通过在人身上穿着各种服装来构建样本对。在大规模扩展数据集的监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。实验表明，我们提出的 PFDM 可以成功处理复杂的情况，合成高保真图像，并且优于最先进的无解析器和基于解析器的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.03047v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions**<br />
**Title_cn:** InteractiveVideo：具有协同多模式指令的以用户为中心的可控视频生成<br />
**Authors:** Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue<br />
**Abstract:** <details><summary>原文: </summary>We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at https://github.com/invictus717/InteractiveVideo</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入 $\textit{InteractiveVideo}$，一个以用户为中心的视频生成框架。与基于用户提供的图像或文本操作的传统生成方法不同，我们的框架是为动态交互而设计的，允许用户在整个生成过程中通过各种直观的机制来指导生成模型，例如文本和图像提示、绘画、拖放等。我们提出了一种协同多模态指令机制，旨在将用户的多模态指令无缝集成到生成模型中，从而促进用户输入和生成过程之间的协作和响应交互。这种方法可以通过精确有效的用户指令对生成结果进行迭代和细粒度的细化。通过 $\textit{InteractiveVideo}$，用户可以灵活地精心定制视频的关键方面。他们可以绘制参考图像、编辑语义并调整视频动作，直到完全满足他们的要求。代码、模型和演示可在 https://github.com/invictus717/InteractiveVideo 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.03040v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Retrieval-Augmented Score Distillation for Text-to-3D Generation**<br />
**Title_cn:** 用于文本转 3D 生成的检索增强分数蒸馏<br />
**Authors:** Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim<br />
**Abstract:** <details><summary>原文: </summary>Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到 3D 生成通过结合强大的 2D 扩散模型取得了巨大的成功，但 3D 先验知识不足也导致了 3D 几何的不一致。近年来，随着大规模多视图数据集的发布，在多视图数据集上微调扩散模型成为解决3D不一致问题的主流。然而，与 2D 数据相比，3D 数据的质量和多样性有限，因此面临着根本性的困难。为了避免这些权衡，我们探索了一种专为分数蒸馏而设计的检索增强方法，称为 RetDream。我们假设通过在优化过程中直接使用语义相关资产，可以充分利用 2D 扩散模型的表现力和 3D 资产的几何一致性。为此，我们引入了文本转 3D 生成中基于检索的质量增强的新颖框架。我们利用检索到的资产将其几何先验合并到变分目标中，并调整扩散模型的 2D 先验以实现视图一致性，从而在生成场景的几何和保真度方面实现显着改进。我们进行了大量的实验来证明 RetDream 具有卓越的品质和更高的几何一致性。项目页面位于 https://ku-cvlab.github.io/RetDream/。</details>
**PDF:** <http://arxiv.org/pdf/2402.02972v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Instance Segmentation XXL-CT Challenge of a Historic Airplane**<br />
**Title_cn:** 历史飞机的实例分割 XXL-CT 挑战<br />
**Authors:** Roland Gruber, Johann Christopher Engster, Markus Michen, Nele Blum, Maik Stille, Stefan Gerth, Thomas Wittenberg<br />
**Abstract:** <details><summary>原文: </summary>Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing. This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality. To asses recent advancements in the field of machine learning-based image segmentation, the "Instance Segmentation XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes. We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>XXL-CT 图像中复合对象的实例分割对无损测试提出了独特的挑战。这种复杂性源于缺乏已知的参考分割标签、有限的适用分割工具以及部分降低的图像质量。为了评估基于机器学习的图像分割领域的最新进展，开展了“历史飞机的实例分割 XXL-CT 挑战”。该挑战旨在探索自动或交互式实例分割方法，以有效描绘不同的飞机部件，例如螺钉、铆钉、金属板或压力管。我们报告了这一挑战的组织和结果，并描述了所提交的分割方法的功能和局限性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02928v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis**<br />
**Title_cn:** ViewFusion：学习用于新视图合成的可组合扩散模型<br />
**Authors:** Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin<br />
**Abstract:** <details><summary>原文: </summary>Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习为解决新颖视图合成的老问题提供了丰富的新方法，从基于神经辐射场（NeRF）的方法到端到端风格架构。每种方法都有特定的优势，但在适用性上也有特定的限制。这项工作介绍了 ViewFusion，这是一种最先进的端到端生成方法，用于新颖的视图合成，具有无与伦比的灵活性。 ViewFusion 包括同时对场景的任意数量的输入视图应用扩散去噪步骤，然后将每个视图获得的噪声梯度与（推断的）像素加权掩模相结合，确保对于目标场景的每个区域，只有最考虑到信息丰富的输入视图。我们的方法通过以下方式解决了以前方法的几个局限性：（1）可在多个场景和对象类别中进行训练和泛化，（2）在训练和测试时自适应地采用可变数量的无姿势视图，（3）生成合理的视图即使在严重不确定的条件下（由于其生成性） - 同时生成与最先进的方法相当甚至更好的质量视图。局限性包括不生成场景的 3D 嵌入，导致推理速度相对较慢，并且我们的方法仅在相对较小的数据集 NMR 上进行测试。代码可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.02906v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data**<br />
**Title_cn:** SynthVision - 使用合成图像数据在计算机视觉模型中利用最小输入获得最大输出<br />
**Authors:** Yudara Kularathne, Prathapa Janitha, Sithira Ambepitiya, Thanveer Ahamed, Dinuka Wijesundara, Prarththanan Sothyrajah<br />
**Abstract:** <details><summary>原文: </summary>Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.</details>
**Abstract_cn:** <details><summary>译文: </summary>疾病检测计算机视觉模型的快速发展对于应对流行病或生物恐怖主义事件等紧急医疗危机至关重要。然而，传统的数据收集方法对于这些场景来说太慢，需要创新方法从最少的数据快速生成可靠的模型。我们通过构建一个全面的计算机视觉模型来展示我们的新方法，该模型仅使用合成数据来检测人乳头瘤病毒生殖器疣。在我们的研究中，我们采用了使用扩散模型的两阶段实验设计。在第一阶段，利用扩散模型从 ​​10 个 HPV 引导图像生成大量不同的合成图像，明确专注于准确描绘尖锐湿疣。第二阶段涉及使用该合成数据集训练和测试视觉模型。该方法旨在评估扩散模型在快速生成高质量训练数据方面的有效性以及随后对医学图像识别中视觉模型性能的影响。研究结果揭示了对通过扩散模型生成的合成图像训练的视觉模型的性能的重要见解。视觉模型在准确识别尖锐湿疣病例方面表现出卓越的性能。它达到了 96% 的准确率，突显了其在医学图像分类方面的有效性。对于 HPV 病例，该模型表现出 99% 的高精度和 94% 的召回率。在正常情况下，准确率为 95%，召回率高达 99%。这些指标表明模型能够正确识别真阳性病例并最大限度地减少误报。该模型在 HPV 病例中的 F1 得分为 96%，在正常病例中的 F1 得分为 97%。两个类别的高 F1 分数凸显了模型精度和召回率的平衡性质，确保了预测的可靠性和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02826v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Extreme Two-View Geometry From Object Poses with Diffusion Models**<br />
**Title_cn:** 具有扩散模型的物体姿势的极端二视图几何<br />
**Authors:** Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu<br />
**Abstract:** <details><summary>原文: </summary>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类具有令人难以置信的能力，可以毫不费力地感知包含同一对象的两个图像之间的视点差异，即使视点变化惊人地巨大并且图像中没有共同可见的区域。然而，这种非凡的技能已被证明对现有的相机姿态估计方法是一个挑战，由于缺乏用于匹配的重叠局部特征，这些方法在面临较大视点差异时常常会失败。在本文中，我们的目标是有效利用对象先验的力量，在面对极端视点变化时准确确定双视图几何。在我们的方法中，我们首先在数学上将相对相机姿态估计问题转换为对象姿态估计问题。然后，为了估计物体姿态，我们利用从扩散模型 Zero123 学到的物体先验来合成物体的新颖视图图像。匹配新视图图像以确定对象姿势，从而确定双视图相机姿势。在实验中，我们的方法表现出了对大视点变化的非凡鲁棒性和弹性，能够在合成数据集和真实数据集上一致地估计双视图姿势，并具有出色的泛化能力。代码将在 https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.02800v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models**<br />
**Title_cn:** DisDet：探索扩散模型后门攻击的可检测性<br />
**Authors:** Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan<br />
**Abstract:** <details><summary>原文: </summary>In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在令人兴奋的生成式人工智能时代，扩散模型已成为一种非常强大且广泛采用的各种数据模态的内容生成和编辑工具，使得研究其潜在安全风险变得非常必要和关键。最近，一些开创性的工作揭示了扩散模型针对后门攻击的脆弱性，呼吁对这种流行且基础的人工智能技术的安全挑战进行深入分析和调查。在本文中，我们首次系统地探讨了后门扩散模型的中毒噪声输入的可检测性，这是一个重要的性能指标，但在现有的工作中很少进行探索。我们从防御者的角度出发，首先分析现有扩散后门攻击中触发模式的特性，发现分布差异在木马检测中的重要作用。基于这一发现，我们提出了一种低成本的触发检测机制，可以有效地识别中毒的输入噪声。然后，我们进一步从攻击方研究同样的问题，提出一种后门攻击策略，可以学习不明显的触发因素来逃避我们提出的检测方案。对各种扩散模型和数据集的实证评估证明了所提出的触发检测和检测规避攻击策略的有效性。对于触发器检测，我们基于分布差异的解决方案可以对现有作品中使用的木马触发器实现100％的检测率。为了逃避触发检测，我们提出的隐形触发设计方法执行端到端学习，使有毒噪声输入的分布接近良性噪声的分布，从而使后门的检测通过率接近 100%，具有非常高的攻击和良性性能扩散模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.02739v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data**<br />
**Title_cn:** InVA：用于协调多模态神经影像数据的综合变分自动编码器<br />
**Authors:** Bowen Lei, Rajarshi Guhaniyogi, Krishnendu Chandra, Aaron Scheffler, Bani Mallick<br />
**Abstract:** <details><summary>原文: </summary>There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \texttt{InVA} over VAEs, which typically do not allow borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI).</details>
**Abstract_cn:** <details><summary>译文: </summary>人们对探索源自不同成像模式的多个图像之间的非线性关联非常感兴趣。虽然关于图像对图像回归来描述基于多个图像的图像的预测推理的文献越来越多，但现有方法在图像预测中有效借用多个成像模态之间的信息方面存在局限性。基于变分自动编码器 (VAE) 的文献，本文提出了一种新颖的方法，称为集成变分自动编码器 (\texttt{InVA}) 方法，该方法借用从不同来源获得的多个图像中的信息来得出预测推理图像。所提出的方法捕​​获结果图像和输入图像之间复杂的非线性关联，同时允许快速计算。数值结果证明了 \texttt{InVA} 相对于 VAE 的巨大优势，VAE 通常不允许在输入图像之间借用信息。所提出的框架通过磁共振成像（MRI）轻松获得的人脑扫描中皮层结构的多种测量结果，为昂贵的正电子发射形貌（PET）提供高度准确的预测推论。</details>
**PDF:** <http://arxiv.org/pdf/2402.02734v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN**<br />
**Title_cn:** GAN 支持快速准确的协作无线电地图估计<br />
**Authors:** Zezhong Zhang, Guangxu Zhu, Junting Chen, Shuguang Cui<br />
**Abstract:** <details><summary>原文: </summary>In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI. Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate.</details>
**Abstract_cn:** <details><summary>译文: </summary>6G时代，迫切需要实时无线资源监控和管理，以支持多样化的无线应用。这就需要对无线电资源的分布进行快速、准确的估计，这通常由地理环境中的空间信号功率强度来表示，称为无线电地图。在本文中，我们提出了一种由生成对抗网络（GAN）支持的协作无线电地图估计（CRME）方法，称为 GAN-CRME，其特点是无需发射机信息即可快速准确地估计无线电地图。通过利用移动用户的分布式接收信号强度 (RSS) 测量与使用深度神经网络估计器的地理地图之间的交互来推断无线电地图，从而降低数据采集成本和计算复杂性。此外，还提出了一种基于 GAN 的学习算法，通过利用生成式人工智能的力量来提高深度神经网络估计器的推理能力。仿真结果表明，当地理地图信息不准确时，所提出的 GAN-CRME 甚至能够进行粗略纠错。</details>
**PDF:** <http://arxiv.org/pdf/2402.02729v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion**<br />
**Title_cn:** AONeuS：声光传感器融合的神经渲染框架<br />
**Authors:** Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler<br />
**Abstract:** <details><summary>原文: </summary>Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods. A website visualizing the results of our paper is located at this address: https://aoneus.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>水下感知和 3D 表面重建是具有挑战性的问题，在建筑、安全、海洋考古和环境监测领域有着广泛的应用。危险的操作条件、脆弱的环境和有限的导航控制通常决定潜水器限制其运动范围，从而限制它们捕获测量值的基线。在 3D 场景重建的背景下，众所周知，较小的基线使重建更具挑战性。我们的工作开发了一种基于物理的多模态声光神经表面重建框架（AONEuS），能够有效地将高分辨率 RGB 测量与低分辨率深度分辨成像声纳测量相集成。通过融合这些互补模式，我们的框架可以根据在严格限制的基线上捕获的测量结果重建准确的高分辨率 3D 表面。通过广泛的模拟和实验室内实验，我们证明 AONEuS 的性能显着优于最近的仅 RGB 和仅声纳基于逆微分渲染的表面重建方法。可视化我们论文结果的网站位于以下地址：https://aoneus.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2402.03309v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection**<br />
**Title_cn:** ActiveAnno3D - 用于多模态 3D 对象检测的主动学习框架<br />
**Authors:** Ahmed Ghita, Bjørk Antoniussen, Walter Zimmer, Ross Greer, Christian Creß, Andreas Møgelmose, Mohan M. Trivedi, Alois C. Knoll<br />
**Abstract:** <details><summary>原文: </summary>The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset. We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs. Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模数据集的管理仍然成本高昂，并且需要大量时间和资源。数据通常是手动标记的，创建高质量数据集的挑战仍然存在。在这项工作中，我们利用主动学习进行多模态 3D 对象检测来填补研究空白​​。我们提出了 ActiveAnno3D，这是一种主动学习框架，用于选择为训练提供最大信息量的标记数据样本。我们探索各种连续训练方法，并整合关于计算需求和检测性能的最有效方法。此外，我们使用 BEVFusion 和 PV-RCNN 在 nuScenes 和 TUM Traffic Intersection 数据集上进行了广泛的实验和消融研究。我们表明，当仅使用 TUM 交通交叉口数据集的一半训练数据（77.25 mAP 与 83.50 mAP）时，我们可以使用 PV-RCNN 和基于熵的查询策略实现几乎相同的性能。当使用一半的训练数据时，BEVFusion 的 mAP 为 64.31；当使用完整的 nuScenes 数据集时，BEVFusion 的 mAP 为 75.0。我们将主动学习框架集成到 proAnno 标记工具中，以实现人工智能辅助数据选择和标记，并最大限度地降低标记成本。最后，我们在我们的网站上提供代码、权重和可视化结果：https://active3d-framework.github.io/active3d-framework。</details>
**PDF:** <http://arxiv.org/pdf/2402.03235v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi: Multimodal Understanding Leaderboard with Text and Images**<br />
**Title_cn:** 多：带有文本和图像的多模式理解排行榜<br />
**Authors:** Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）的快速进展凸显了向学术界引入具有挑战性但现实的基准的必要性。现有的基准主要侧重于简单的自然图像理解，但 Multi 成为 MLLM 的前沿基准，提供了一个全面的数据集，用于评估 MLLM 是否理解复杂的图形和表格以及科学问题。该基准反映了当前现实的考试风格，提供了多模式输入，并要求精确或开放式的回答，类似于现实生活中的学校测试。它向 MLLM 提出各种任务的挑战，从公式推导到图像细节分析，再到跨模态推理。 Multi 包含 18,000 多个问题，重点是多种形式的基于科学的 QA。我们还引入了 Multi-Elite（一个包含 500 个问题的子集，用于测试 MLLM 的极限）和 Multi-Extend（它通过 4,500 多个知识片段增强了情境学习研究）。我们的评估表明 MLLM 具有巨大的进步潜力，GPT-4V 在 Multi 上实现了 63.7% 的准确率，而其他 MLLM 的得分在 31.3% 到 53.7% 之间。 Multi不仅是一个强大的评估平台，而且为专家级人工智能的发展铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.03173v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**<br />
**Title_cn:** Video-LaVIT：具有解耦视觉运动标记化的统一视频语言预训练<br />
**Authors:** Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于多模态大语言模型 (LLM) 的最新进展，人们越来越关注将其从图像文本数据扩展到信息更丰富的现实世界视频。与静态图像相比，视频由于其时空动态建模，对有效的大规模预训练提出了独特的挑战。在本文中，我们通过有效的视频分解来解决视频语言预训练中的此类局限性，将每个视频表示为关键帧和时间运动。然后使用精心设计的标记器将它们适应法学硕士，将视觉和时间信息离散化为几个标记，从而实现视频、图像和文本的统一生成预训练。推理时，LLM 生成的令牌被小心地恢复到原始的连续像素空间，以创建各种视频内容。我们提出的框架能够理解和生成图像和视频内容，正如其在图像和视频理解和生成方面在 13 个多模态基准中的竞争表现所证明的那样。我们的代码和模型将在 https://video-lavit.github.io 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.03161v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Text-Guided Image Clustering**<br />
**Title_cn:** 文本引导图像聚类<br />
**Authors:** Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela Gipp, Claudia Plant, Benjamin Roth<br />
**Abstract:** <details><summary>原文: </summary>Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像聚类将图像集合划分为有意义的组，通常通过人类给出的注释进行事后解释。这些通常采用文本形式，这就引出了使用文本作为图像聚类抽象的问题。然而，当前的图像聚类方法忽略了生成的文本描述的使用。因此，我们提出文本引导图像聚类，即使用图像字幕和视觉问答（VQA）模型生成文本，然后对生成的文本进行聚类。此外，我们引入了一种新方法，通过提示 VQA 模型来注入任务或领域知识以进行聚类。在八个不同的图像聚类数据集中，我们的结果表明，获得的文本表示通常优于图像特征。此外，我们提出了一种基于计数的聚类可解释性方法。我们的评估表明，派生的基于关键字的解释比相应的聚类准确度所建议的更好地描述了聚类。总的来说，这项研究挑战了传统方法，并为使用生成文本的图像聚类范式转变铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.02996v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives**<br />
**Title_cn:** 深入研究道路场景理解的多模态多任务基础模型：从学习范式的角度<br />
**Authors:** Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS</details>
**Abstract_cn:** <details><summary>译文: </summary>基础模型确实对各个领域产生了深远的影响，成为显着塑造智能系统功能的关键组件。在智能汽车的背景下，利用基础模型的力量已被证明是具有变革性的，可以在视觉理解方面带来显着的进步。多模态多任务视觉理解基础模型（MM-VUFM）具备多模态、多任务学习能力，能够有效处理和融合多种模态的数据，同时处理各种与驾驶相关的任务，具有强大的适应性，有助于对周围场景有更全面的了解。在本次调查中，我们对专为道路场景设计的 MM-VUFM 进行了系统分析。我们的目标不仅是提供常见实践的全面概述，参考特定任务模型、统一多模态模型、统一多任务模型和基础模型提示技术，而且还强调它们在不同学习范式中的先进能力。这些范例包括开放世界的理解、道路场景的高效传输、持续学习、交互和生成能力。此外，我们还提供对关键挑战和未来趋势的见解，例如闭环驾驶系统、可解释性、具体驾驶代理和世界模型。为了方便研究人员及时了解道路场景 MM-VUFM 的最新进展，我们在 https://github.com/rolsheng/MM-VUFM4DS 建立了一个持续更新的存储库</details>
**PDF:** <http://arxiv.org/pdf/2402.02968v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes**<br />
**Title_cn:** 4D 高斯泼溅：实现动态场景的高效新颖视图合成<br />
**Authors:** Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen<br />
**Abstract:** <details><summary>原文: </summary>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们考虑动态场景的新颖视图合成（NVS）问题。最近的神经方法已经在静态 3D 场景中取得了出色的 NVS 结果，但扩展到 4D 时变场景仍然很重要。先前的努力通常通过学习规范空间加上隐式或显式变形场来编码动态，这在突然运动或捕获高保真渲染等具有挑战性的场景中表现不佳。在本文中，我们介绍了 4D 高斯分布 (4DGS)，这是一种利用各向异性 4D XYZT 高斯表示动态场景的新颖方法，其灵感来自于 3D 高斯分布在静态场景中的成功。我们通过对 4D 高斯进行时间切片来对每个时间戳的动态进行建模，这自然地组成了动态 3D 高斯，并且可以无缝投影到图像中。作为一种明确的时空表示，4DGS 展示了对复杂动态和精细细节进行建模的强大能力，尤其是对于运动突然的场景。我们在高度优化的 CUDA 加速框架中进一步实现了时间切片和泼溅技术，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时推理渲染速度，在 RTX 4090 GPU 上实现了高达 583 FPS 的实时推理渲染速度。对具有不同运动的场景的严格评估展示了 4DGS 的卓越效率和有效性，它在数量和质量上始终优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.03307v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM**<br />
**Title_cn:** SGS-SLAM：神经密集 SLAM 的语义高斯泼溅<br />
**Authors:** Mingrui Li, Shuhong Liu, Heng Zhou<br />
**Abstract:** <details><summary>原文: </summary>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义理解在密集同步定位与建图（SLAM）中发挥着至关重要的作用，有助于全面的场景解释。最近将高斯泼溅集成到 SLAM 系统中的进展已经证明了其通过使用显式 3D 高斯表示来生成高质量渲染的有效性。在此进展的基础上，我们提出了 SGS-SLAM，这是第一个基于 3D 高斯的语义密集视觉 SLAM 系统，它提供精确的 3D 语义分割和高保真重建。具体来说，我们建议在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。大量实验表明，SGS-SLAM 在相机姿态估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保留了实时渲染能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.03246v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition**<br />
**Title_cn:** FROSTER：Frozen CLIP 是开放词汇动作识别的强大老师<br />
**Authors:** Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 FROSTER，一个用于开放词汇动作识别的有效框架。 CLIP 模型在一系列基于图像的任务中取得了显着的成功，这得益于其预保留大量图像文本对所带来的强大泛化能力。然而，由于 CLIP 预训练中缺乏时间信息，将 CLIP 直接应用于开放词汇动作识别任务具有挑战性。此外，在动作识别数据集上微调 CLIP 可能会导致过度拟合并阻碍其泛化性，从而在处理看不见的动作时导致结果不令人满意。为了解决这些问题，FROSTER采用残差特征蒸馏方法来确保CLIP保留其泛化能力，同时有效适应动作识别任务。具体来说，残余特征蒸馏将冻结的 CLIP 模型视为老师，以保持原始 CLIP 所表现出的通用性，并监督特征学习以提取视频特定特征，以弥合图像和视频之间的差距。同时，它使用残差子网络进行特征蒸馏，以在学习可泛化特征和视频特定特征这两个不同目标之间达到平衡。我们在基础到小说和跨数据集设置下，在开放词汇动作识别基准上广泛评估 FROSTER。 FROSTER 在所有数据集上始终保持最先进的性能。项目页面：https://visual-ai.github.io/froster。</details>
**PDF:** <http://arxiv.org/pdf/2402.03241v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Good Teachers Explain: Explanation-Enhanced Knowledge Distillation**<br />
**Title_cn:** 好老师讲解：讲解增强知识蒸馏<br />
**Authors:** Amin Parchami-Araghi, Moritz Böhle, Sukrut Rao, Bernt Schiele<br />
**Abstract:** <details><summary>原文: </summary>Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识蒸馏 (KD) 已被证明可以有效地将大型教师模型压缩为较小的学生模型。众所周知，学生模型可以达到与教师相似的准确度，但也有研究表明，他们通常无法学习相同的功能。然而，通常非常希望学生和教师的函数具有相似的属性，例如基于相同的输入特征进行预测，因为这可以确保学生从教师那里学习“正确的特征”。在这项工作中，我们探索是否可以通过优化经典的 KD 损失以及教师和学生生成的解释的相似性来实现这一点。尽管这个想法简单直观，但我们发现我们提出的“解释增强”KD (e$^2$KD) (1) 在准确性和师生一致性方面始终如一地提供了巨大的收益，(2) 确保了学生从老师那里学到正确的理由并给出类似的解释，并且（3）在模型架构、训练数据量方面是稳健的，甚至可以使用“近似”的预先计算的解释。</details>
**PDF:** <http://arxiv.org/pdf/2402.03119v1><br />
**Code:** <https://github.com/m-parchami/goodteachersexplain>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **HASSOD: Hierarchical Adaptive Self-Supervised Object Detection**<br />
**Title_cn:** HASSOD：分层自适应自监督目标检测<br />
**Authors:** Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang<br />
**Abstract:** <details><summary>原文: </summary>The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类视觉感知系统在没有明确监督的情况下表现出非凡的学习能力和理解物体的部分到整体组成的能力。从这两种能力中汲取灵感，我们提出了分层自适应自监督对象检测（HASSOD），这是一种新颖的方法，可以在没有人类监督的情况下学习检测对象并理解其组成。 HASSOD 采用分层自适应聚类策略，根据自我监督的视觉表示将区域分组为对象掩模，自适应地确定每个图像的对象数量。此外，HASSOD通过分析掩模之间的覆盖关系并构建树结构来识别对象在组成方面的层次级别。这种额外的自我监督学习任务可以提高检测性能并增强可解释性。最后，我们放弃了现有方法中低效的多轮自训练过程，而是采用半监督学习的 Mean Teacher 框架，从而实现更顺畅、更高效的训练过程。通过对流行图像数据集进行广泛的实验，我们证明了 HASSOD 相对于现有方法的优越性，从而推进了自监督目标检测的最先进技术。值得注意的是，我们将 LVIS 上的 Mask AR 从 20.2 提高到 22.5，将 SA-1B 上的 Mask AR 从 17.0 提高到 26.0。项目页面：https://HASSOD-NeurIPS23.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2402.03311v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining**<br />
**Title_cn:** Swin-UMamba：基于 Mamba 的 UNet 和基于 ImageNet 的预训练<br />
**Authors:** Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的医学图像分割需要集成多尺度信息，从局部特征到全局依赖性。然而，现有方法对远程全局信息进行建模具有挑战性，其中卷积神经网络（CNN）受到其局部感受野的限制，而视觉变换器（ViT）则受到其注意力机制的高二次复杂度的影响。最近，基于 Mamba 的模型因其在长序列建模方面令人印象深刻的能力而受到极大关注。多项研究表明，这些模型在各种任务中都可以优于流行的视觉模型，提供更高的准确性、更低的内存消耗和更少的计算负担。然而，现有的基于 Mamba 的模型大多是从头开始训练的，并没有探索预训练的力量，而预训练已被证明对于数据高效的医学图像分析非常有效。本文介绍了一种基于 Mamba 的新型模型 Swin-UMamba，该模型专为医学图像分割任务而设计，利用了基于 ImageNet 的预训练的优势。我们的实验结果揭示了基于 ImageNet 的训练在增强基于 Mamba 的模型性能方面的重要作用。与 CNN、ViT 和最新的基于 Mamba 的模型相比，Swin-UMamba 表现出了巨大的优越性能。值得注意的是，在腹部 MRI、肠镜检查和显微镜检查数据集上，Swin-UMamba 的平均得分比最接近的 U-Mamba 好 3.58%。 Swin-UMamba的代码和模型公开于：https://github.com/JiarunLiu/Swin-UMamba</details>
**PDF:** <http://arxiv.org/pdf/2402.03302v1><br />
**Code:** <https://github.com/jiarunliu/swin-umamba>**<br />
>>**index:** 3<br />
**Title:** **CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models**<br />
**Title_cn:** 基于 CT 的胸部手术规划解剖分割：3D U 形深度学习模型的基准研究<br />
**Authors:** Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, Yiming Xiao<br />
**Abstract:** <details><summary>原文: </summary>Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic surgery. Our study systematically examines the impact of different attention mechanisms, number of resolution stages, and network configurations on segmentation accuracy and computational complexity. To allow cross-reference with other recent benchmarking studies, we also included a performance assessment of the BTCV abdominal structural segmentation. With the STUNet ranking at the top, our study demonstrated the value of CNN-based U-shaped models for the investigated tasks and the benefit of residual blocks in network configuration designs to boost segmentation performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近人们对针对特定患者的胸部手术规划和模拟的兴趣日益浓厚，需要通过自动医学图像分割算法高效、稳健地创建数字解剖模型。深度学习 (DL) 现在在各种放射学任务中都是最先进的，自 2D UNet 诞生以来，U 形 DL 模型在医学图像分割方面尤其出色。迄今为止，通过整合不同的注意力机制和网络配置，已经提出了许多 U 形模型的变体。利用大型多标签数据库的最新发展，这些模型的系统基准研究可以为临床部署和未来模型设计提供有价值的见解，但此类研究仍然很少。我们对 3D U 形模型的变体（3DUNet、STUNet、AttentionUNet、SwinUNETR、FocalSegNet 和具有四种变体的新型 3D SwinUnet）进行了首次基准研究，重点是胸外科手术中基于 CT 的解剖分割。我们的研究系统地研究了不同的注意力机制、分辨率阶段的数量和网络配置对分割准确性和计算复杂性的影响。为了与其他最近的基准研究进行交叉引用，我们还对 BTCV 腹部结构分割进行了性能评估。 STUNet 排名靠前，我们的研究证明了基于 CNN 的 U 形模型对于所研究任务的价值，以及网络配置设计中残差块对提高分割性能的好处。</details>
**PDF:** <http://arxiv.org/pdf/2402.03230v1><br />
**Code:** <https://github.com/healthx-lab/deepsegthoracic>**<br />
>>**index:** 4<br />
**Title:** **Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms**<br />
**Title_cn:** 通过以凝视为中心的损失项来减轻面部交换中的不可思议（眼睛）<br />
**Authors:** Ethan Wilson, Frederick Shic, Sophie Jörg, Eakta Jain<br />
**Abstract:** <details><summary>原文: </summary>Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部交换技术的进步使得自动生成高度真实的面部成为可能。然而，换脸的感觉与观看真实面孔时不同，其中观看者眼睛周围的行为存在重大差异。面部交换算法通常不强调眼睛，而是依靠考虑整个面部的像素或特征匹配损失来指导训练过程。我们进一步调查观众对换脸的感知，将分析重点放在恐怖谷效应的存在上。我们还提出了一种新颖的损失方程，用于训练面部交换模型，利用预训练的注视估计网络来直接改善眼睛的表示。我们确认，观看的脸部互换确实会引起观众的不可思议的反应。我们提出的改进显着减少了面部交换与其源材料之间的视角误差。我们的方法还减少了观看者执行深度换脸检测任务时眼睛作为决定因素的发生率。我们的研究结果对面部交换的特殊效果、数字化身、隐私机制等有影响；用户的负面反应可能会限制所述应用程序的有效性。我们的视线改进是通过有针对性的方法减轻观众负面看法的第一步。</details>
**PDF:** <http://arxiv.org/pdf/2402.03188v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification**<br />
**Title_cn:** RRWNet：用于有效视网膜动脉/静脉分割和分类的递归细化网络<br />
**Authors:** José Morano, Guilherme Aresta, Hrvoje Bogunović<br />
**Abstract:** <details><summary>原文: </summary>The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique. Nonetheless, manually performing these tasks is labor-intensive and prone to human error. Various automated methods have been proposed to address this problem. However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps. This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors. The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps. Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches. In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency. The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet.</details>
**Abstract_cn:** <details><summary>译文: </summary>视网膜血管的口径和结构是各种疾病和医疗状况的重要生物标志物。对视网膜脉管系统的彻底分析需要对血管进行分割并将其分类为动脉和静脉，这通常是在通过视网膜成像（一种广泛使用的成像技术）获得的彩色眼底图像上进行的。尽管如此，手动执行这些任务是劳动密集型的，并且容易出现人为错误。已经提出了各种自动化方法来解决这个问题。然而，由于明显的分类错误影响了分割图的拓扑一致性，当前动脉/静脉分割和分类的技术水平面临着挑战。本研究提出了一种创新的端到端框架 RRWNet，旨在递归地细化语义分割图并纠正明显的分类错误。该框架由一个完全卷积神经网络组成，该网络具有一个从输入图像生成基本分割图的基本子网，以及一个迭代和递归地改进这些图的递归细化子网。对公共数据集的评估证明了所提出的方法的最先进的性能，与现有方法相比，产生了拓扑更加一致的分割图，并且明显的分类错误更少。此外，递归细化模块在对其他方法的分割图进行后处理、自动纠正分类错误并提高拓扑一致性方面被证明是有效的。模型代码、权重和预测可在 https://github.com/j-morano/rrwnet 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03166v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks**<br />
**Title_cn:** 消除梯度反转攻击中的硬标签约束<br />
**Authors:** Yanbo Wang, Jian Liang, Ran He<br />
**Abstract:** <details><summary>原文: </summary>Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>梯度反转攻击旨在从联邦学习框架中暴露的中间梯度重建本地训练数据。尽管攻击成功，但之前的所有方法，从重建单个数据点开始，然后将单图像限制放宽到批量级别，都仅在硬标签约束下进行测试。即使对于单图像重建，我们仍然缺乏基于分析的算法来恢复增强的软标签。在这项工作中，我们将重点从扩大批量大小转移到研究硬标签约束，考虑在训练过程中使用标签平滑和混合技术的更现实的情况。特别是，我们第一个提出了一种新颖的算法，可以从单输入梯度中同时恢复真实增强标签和最后一个全连接层的输入特征，并为任何基于分析的标签恢复提供必要条件方法。大量的实验证明了标签恢复的准确性，以及对后续图像重建的好处。我们认为分类任务中的软标签在梯度反转攻击中值得进一步关注。</details>
**PDF:** <http://arxiv.org/pdf/2402.03124v1><br />
**Code:** <https://github.com/ybwang119/label_recovery>**<br />
>>**index:** 7<br />
**Title:** **Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector**<br />
**Title_cn:** 通过增强型开放集对象检测器进行跨域少样本对象检测<br />
**Authors:** Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Lingjie Kong, Yanwei Fu, Luc Van Gool, et.al.<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting open-set detectors directly for CD-FSOD. Sequentially, to overcome the performance degradation issue and also to answer the second proposed question, we endeavor to enhance the vanilla DE-ViT. With several novel components including finetuning, a learnable prototype module, and a lightweight attention module, we present an improved Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on both out-of-domain and in-domain target datasets, establishing new SOTAs for both CD-FSOD and FSOD. All the datasets, codes, and models will be released to the community.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了跨域少样本目标检测（CD-FSOD）的挑战，旨在为具有最少标记示例的新领域开发准确的目标检测器。虽然基于 Transformer 的开放集检测器（例如 DE-ViT~\cite{zhang2023Detect}）在开放词汇目标检测和传统的少样本目标检测方面都表现出色，检测的类别超出了训练期间看到的类别，因此我们自然地提出了两个关键问题问题：1）这种开集检测方法可以轻松推广到 CD-FSOD 吗？ 2）如果不是，当面临显着的域差距时如何增强开放集方法的结果？为了解决第一个问题，我们引入了几个指标来量化域差异，并建立具有不同域指标值的新 CD-FSOD 基准。一些最先进的（SOTA）开放集对象检测方法在此基准上进行了评估，在域外数据集上观察到明显的性能下降。这表明直接采用开集探测器进行CD-FSOD是失败的。接下来，为了克服性能下降问题并回答第二个提出的问题，我们努力增强 vanilla DE-ViT。通过微调、可学习原型模块和轻量级注意力模块等几个新颖的组件，我们提出了一种改进的 CD-FSOD (CD-ViTO) 跨域视觉转换器。实验表明，我们的 CD-ViTO 在域外和域内目标数据集上都取得了令人印象深刻的结果，为 CD-FSOD 和 FSOD 建立了新的 SOTA。所有数据集、代码和模型都将发布到社区。</details>
**PDF:** <http://arxiv.org/pdf/2402.03094v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Taylor Videos for Action Recognition**<br />
**Title_cn:** 用于动作识别的泰勒视频<br />
**Authors:** Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng<br />
**Abstract:** <details><summary>原文: </summary>Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed. Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D CNNs, 3D CNNs, and transformers. When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow. When fused with RGB or optical flow videos, further accuracy improvement is achieved.</details>
**Abstract_cn:** <details><summary>译文: </summary>从视频中有效地提取动作是动作识别的一个关键且长期存在的问题。这个问题非常具有挑战性，因为运动（i）没有明确的形式，（ii）具有位移、速度和加速度等各种概念，并且（iii）通常包含由不稳定像素引起的噪声。为了解决这些挑战，我们提出了泰勒视频，这是一种新的视频格式，突出显示每个帧中的主导运动（例如，挥手），称为泰勒帧。泰勒视频以泰勒级数命名，泰勒级数使用重要术语在给定点逼近函数。在视频场景中，我们定义了一个隐式运动提取函数，旨在从视频时间块中提取运动。在此块中，使用帧、差异帧和高阶差异帧，我们执行泰勒展开以在起始帧处近似该函数。我们展示了泰勒级数中高阶项的总和为我们提供了主导运动模式，其中静态物体、小且不稳定的运动被移除。通过实验，我们证明 Taylor 视频是流行架构（包括 2D CNN、3D CNN 和 Transformer）的有效输入。单独使用时，与 RGB 视频和光流相比，泰勒视频的动作识别精度具有竞争力。当与 RGB 或光流视频融合时，可以进一步提高精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.03019v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **[Citation needed] Data usage and citation practices in medical imaging conferences**<br />
**Title_cn:** [需要引用]医学影像会议中的数据使用和引用实践<br />
**Authors:** Théo Sourget, Ahmet Akkoç, Stinna Winther, Christine Lyngbye Galsgaard, Amelia Jiménez-Sánchez, Dovile Juodelyte, Caroline Petitjean, Veronika Cheplygina<br />
**Abstract:** <details><summary>原文: </summary>Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学成像论文通常关注方法论，但算法的质量和结论的有效性高度依赖于所使用的数据集。由于创建数据集需要大量的工作，研究人员经常使用公开的数据集，但是没有采用的标准来引用科学论文中使用的数据集，导致难以跟踪数据集的使用情况。在这项工作中，我们介绍了我们创建的两个开源工具，可以帮助检测数据集的使用情况，一个使用 OpenAlex 和全文分析的管道 \url{https://github.com/TheoSourget/Public_Medical_Datasets_References}，以及一个我们的研究中使用 PDF 注释软件 \url{https://github.com/TheoSourget/pdf_annotator} 来手动标记数据集的存在。我们应用这两种工具来研究 MICCAI 和 MIDL 论文中 20 个公开可用的医学数据集的使用情况。我们计算了 2013 年至 2023 年间论文中 3 种存在类型的比例和演变：被引用、全文提及、被引用和提及。我们的研究结果证明了有限数据集的使用集中。我们还强调了不同的引用实践，这使得跟踪的自动化变得困难。</details>
**PDF:** <http://arxiv.org/pdf/2402.03003v1><br />
**Code:** <https://github.com/theosourget/public_medical_datasets_references>**<br />
>>**index:** 10<br />
**Title:** **A Safety-Adapted Loss for Pedestrian Detection in Automated Driving**<br />
**Title_cn:** 自动驾驶中行人检测的安全自适应损失<br />
**Authors:** Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian, Rudolph Triebel<br />
**Abstract:** <details><summary>原文: </summary>In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶 (AD) 等安全关键领域，物体检测器的错误可能会危及行人和其他易受伤害的道路使用者 (VRU)。由于常见的评估指标并不是足够的安全指标，因此最近的工作采用了一些方法来识别安全关键的 VRU 并反向注释对象检测器的风险。然而，这些方法没有考虑深度神经网络（DNN）训练过程中的安全因素。因此，最先进的 DNN 会同等地惩罚所有误检，无论其严重性如何。随后，为了减少关键故障案例（即漏报）的发生，可能需要安全意识训练策略来增强关键行人的检测性能。在本文中，我们提出了一种新颖的安全意识损失变化，该变化利用训练期间估计的每个行人关键性分数。我们利用运动域中基于可达性集的碰撞时间（TTC-RSB）度量以及距离信息来考虑最坏情况的威胁，从而量化关键性。我们在 nuScenes 数据集上使用 RetinaNet 和 FCOS 进行的评估结果表明，使用我们的安全感知损失函数训练模型可以减少对关键行人的误检测，而不会牺牲一般情况（即安全关键区域外的行人）的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.02986v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing**<br />
**Title_cn:** 用于道路场景解析的高分辨率无人机图像的无监督语义分割<br />
**Authors:** Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang<br />
**Abstract:** <details><summary>原文: </summary>Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.</details>
**Abstract_cn:** <details><summary>译文: </summary>解析无人机图像中的道路场景时存在两个挑战。首先，无人机图像的高分辨率导致处理困难。其次，有监督的深度学习方法需要大量的手动注释来训练稳健且准确的模型。本文介绍了一种利用视觉语言模型和基本计算机视觉模型的最新进展的无监督道路解析框架。首先，采用视觉语言模型来有效处理超高分辨率无人机图像，以快速检测感兴趣的道路区域图像。随后，利用视觉基础模型SAM为没有类别信息的道路区域生成掩模。接下来，自监督表示学习网络从所有屏蔽区域中提取特征表示。最后，应用无监督聚类算法对这些特征表示进行聚类，并为每个聚类分配 ID。屏蔽区域与相应的 ID 相结合，生成初始伪标签，从而启动常规语义分割的迭代自训练过程。所提出的方法在开发数据集上实现了令人印象深刻的 89.96% mIoU，而无需依赖任何手动注释。特别值得注意的是该方法具有非凡的灵活性，甚至超越了人类定义类别的限制，能够从数据集本身获取新类别的知识。</details>
**PDF:** <http://arxiv.org/pdf/2402.02985v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **One-class anomaly detection through color-to-thermal AI for building envelope inspection**<br />
**Title_cn:** 通过颜色到热人工智能进行一级异常检测，用于建筑围护结构检查<br />
**Authors:** Polina Kurtser, Kailun Feng, Thomas Olofsson, Aitor De Andres<br />
**Abstract:** <details><summary>原文: </summary>We present a label-free method for detecting anomalies during thermographic inspection of building envelopes. It is based on the AI-driven prediction of thermal distributions from color images. Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions. The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training. We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges. The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种无标签方法，用于在建筑围护结构热成像检查过程中检测异常情况。它基于人工智能驱动的彩色图像热分布预测。该方法有效地充当热图像区域的一类分类器，预测热分布与实际热分布之间存在高度不匹配。该算法可以通过选择用于训练的目标样本来学习将某些特征识别为正常或异常。我们通过使用在不同室外温度下收集的数据训练算法来证明这一原理，从而检测到热桥。该方法可以在日常建筑检查期间协助人类专业人员，或与移动平台结合以自动检查大面积区域。</details>
**PDF:** <http://arxiv.org/pdf/2402.02963v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space**<br />
**Title_cn:** HoughToRadon 变换：用于投影空间特征改进的新神经网络层<br />
**Authors:** Alexandra Zhabitskaya, Alexander Sheshkus, Vladimir L. Arlazarov<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, "inner" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 HoughToRadon 变换层，这是一个新颖的层，旨在提高与霍夫变换结合的神经网络的速度，以解决语义图像分割问题。通过将其放置在霍夫变换层之后，“内部”卷积接收具有新的有益属性的修改后的特征图，例如处理图像的较小区域以及通过角度和移位的参数空间线性。这些属性并没有单独在霍夫变换中呈现。此外，HoughToRadon 变换层允许我们使用两个新参数调整中间特征图的大小，从而使我们能够平衡所得神经网络的速度和质量。我们在开放 MIDV-500 数据集上的实验表明，这种新方法可以节省文档分割任务的时间，并达到最先进的 97.7% 准确率，在计算复杂度更高的情况下优于 HoughEncoder。</details>
**PDF:** <http://arxiv.org/pdf/2402.02946v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Time-, Memory- and Parameter-Efficient Visual Adaptation**<br />
**Title_cn:** 时间、内存和参数高效的视觉适应<br />
**Authors:** Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab<br />
**Abstract:** <details><summary>原文: </summary>As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着基础模型变得越来越流行，越来越需要针对下游任务有效地对其进行微调。尽管已经提出了许多自适应方法，但它们的设计目的只是在训练参数数量方面有效。然而，它们通常仍然需要在整个模型中反向传播梯度，这意味着它们的训练时间和内存成本不会显着减少。我们提出了一种自适应方法，该方法不会通过主干网络反向传播梯度。我们通过设计一个并行的轻量级网络来实现这一目标，该网络对来自冻结的、预训练的主干网的特征进行操作。因此，我们的方法不仅在参数方面高效，而且在训练时间和内存使用方面也高效。我们的方法在流行的 VTAB 基准上实现了最先进的精度参数权衡，并且我们进一步展示了我们如何在训练时间和内存使用方面超越先前的工作。我们通过采用 40 亿个参数的视觉转换器主干来适应计算要求较高的视频分类任务，而不需要任何复杂的模型并行性，进一步证明了我们方法的训练效率和可扩展性。在这里，我们优于之前基于适配器的方法，该方法只能扩展到 10 亿个参数的骨干网，或者在相同的 GPU 和更少的训练时间下完全微调更小的骨干网。</details>
**PDF:** <http://arxiv.org/pdf/2402.02887v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Multi-scale fMRI time series analysis for understanding neurodegeneration in MCI**<br />
**Title_cn:** 多尺度功能磁共振成像时间序列分析用于了解 MCI 中的神经退行性变<br />
**Authors:** Ammu R., Debanjali Bhattacharya, Ameiy Acharya, Ninad Aithal, Neelam Sinha<br />
**Abstract:** <details><summary>原文: </summary>In this study, we present a technique that spans multi-scale views (global scale -- meaning brain network-level and local scale -- examining each individual ROI that constitutes the network) applied to resting-state fMRI volumes. Deep learning based classification is utilized in understanding neurodegeneration. The novelty of the proposed approach lies in utilizing two extreme scales of analysis. One branch considers the entire network within graph-analysis framework. Concurrently, the second branch scrutinizes each ROI within a network independently, focusing on evolution of dynamics. For each subject, graph-based approach employs partial correlation to profile the subject in a single graph where each ROI is a node, providing insights into differences in levels of participation. In contrast, non-linear analysis employs recurrence plots to profile a subject as a multichannel 2D image, revealing distinctions in underlying dynamics. The proposed approach is employed for classification of a cohort of 50 healthy control (HC) and 50 Mild Cognitive Impairment (MCI), sourced from ADNI dataset. Results point to: (1) reduced activity in ROIs such as PCC in MCI (2) greater activity in occipital in MCI, which is not seen in HC (3) when analysed for dynamics, all ROIs in MCI show greater predictability in time-series.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们提出了一种跨越多尺度视图（全局尺度——意味着大脑网络水平和局部尺度——检查构成网络的每个单独的 ROI）的技术，应用于静息态 fMRI 体积。基于深度学习的分类用于理解神经退行性疾病。该方法的新颖之处在于利用了两种极端的分析尺度。一个分支在图分析框架内考虑整个网络。同时，第二个分支独立审查网络内的每个投资回报率，重点关注动态的演变。对于每个主题，基于图表的方法采用部分相关性在单个图表中描述主题，其中每个 ROI 都是一个节点，从而提供对参与水平差异的洞察。相比之下，非线性分析采用递归图将对象描绘为多通道 2D 图像，揭示潜在动态的差异。所提出的方法用于对源自 ADNI 数据集的 50 名健康对照 (HC) 和 50 名轻度认知障碍 (MCI) 人群进行分类。结果表明：(1) ROI 中的活动减少，例如 MCI 中的 PCC (2) MCI 中枕骨的活动增加，这在 HC 中未见 (3) 当进行动态分析时，MCI 中的所有 ROI 在时间上都显示出更大的可预测性 -系列。</details>
**PDF:** <http://arxiv.org/pdf/2402.02811v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects**<br />
**Title_cn:** 用于表面缺陷显着性检测的联合注意力引导特征融合网络<br />
**Authors:** Xiaoheng Jiang, Feng Yan, Yang Lu, Ke Wang, Shuai Guo, Tianzhu Zhang, Yanwei Pang, Jianwei Niu, Mingliang Xu<br />
**Abstract:** <details><summary>原文: </summary>Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales. The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion. The attention map makes the model pay more attention to defect features. The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input. The obtained DRF features capture rich context information with a large range of receptive fields. Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods. Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.</details>
**Abstract_cn:** <details><summary>译文: </summary>表面缺陷检测在工业制造和生产过程中发挥着重要作用。尽管基于卷积神经网络（CNN）的缺陷检测方法取得了巨大的飞跃，但仍然面临着缺陷尺度变化、背景复杂、对比度低等诸多挑战。为了解决这些问题，我们提出了一种基于编码器-解码器网络的联合注意力引导特征融合网络（JAFFNet），用于表面缺陷的显着性检测。 JAFFNet 主要将联合注意力引导特征融合（JAFF）模块纳入解码阶段，以自适应地融合低级和高级特征。 JAFF模块在特征融合过程中学习强调缺陷特征并抑制背景噪声，这有利于检测低对比度缺陷。此外，JAFFNet在编码器后面引入了密集感受野（DRF）模块来捕获具有丰富上下文信息的特征，这有助于检测不同尺度的缺陷。 JAFF模块主要利用高级语义特征提供的学习联合通道空间注意力图来指导特征融合。注意力图使得模型更加关注缺陷特征。 DRF 模块利用一系列多感受野 (MRF) 单元，每个单元将所有前面的 MRF 特征图和原始输入作为输入。获得的 DRF 特征捕获了具有大范围感受野的丰富上下文信息。在 SD-saliency-900、Magnetictile 和 DAGM 2007 上进行的大量实验表明，与其他最先进的方法相比，我们的方法取得了令人鼓舞的性能。同时，我们的方法达到了 66 FPS 的实时缺陷检测速度。</details>
**PDF:** <http://arxiv.org/pdf/2402.02797v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Transmission Line Detection Based on Improved Hough Transform**<br />
**Title_cn:** 基于改进Hough变换的输电线路检测<br />
**Authors:** Wei Song, Pei Li, Man Wang<br />
**Abstract:** <details><summary>原文: </summary>To address the challenges of low detection accuracy and high false positive rates of transmission lines in UAV (Unmanned Aerial Vehicle) images, we explore the linear features and spatial distribution. We introduce an enhanced stochastic Hough transform technique tailored for detecting transmission lines in complex backgrounds. By employing the Hessian matrix for initial preprocessing of transmission lines, and utilizing boundary search and pixel row segmentation, our approach distinguishes transmission line areas from the background. We significantly reduce both false positives and missed detections, thereby improving the accuracy of transmission line identification. Experiments demonstrate that our method not only processes images more rapidly, but also yields superior detection results compared to conventional and random Hough transform methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决无人机图像中传输线检测精度低和误报率高的挑战，我们探索了线性特征和空间分布。我们引入了一种增强的随机霍夫变换技术，专门用于检测复杂背景中的传输线。通过采用 Hessian 矩阵对传输线进行初始预处理，并利用边界搜索和像素行分割，我们的方法将传输线区域与背景区分开来。我们显着减少了误报和漏检，从而提高了传输线路识别的准确性。实验表明，与传统和随机霍夫变换方法相比，我们的方法不仅可以更快地处理图像，而且可以产生更好的检测结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.02761v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective**<br />
**Title_cn:** 从融合策略的角度提高激光雷达-相机融合模型对抗天气腐蚀的鲁棒性<br />
**Authors:** Yihao Huang, Kaiyuan Yu, Qing Guo, Felix Juefei-Xu, Xiaojun Jia, Tianlin Li, Geguang Pu, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，LiDAR-相机融合模型显着推进了自动驾驶中的 3D 物体检测任务。然而，它们对复杂物理世界中雾、雨、雪和阳光等常见天气破坏的稳健性仍有待探索。在本文中，我们从损坏数据集的融合策略的角度评估融合模型的鲁棒性。基于评估，我们进一步提出了一种简洁而实用的融合策略，以增强融合模型的鲁棒性，即灵活加权融合激光雷达和相机源的特征，以适应不同的天气场景。对四种类型的融合模型进行的实验，每种模型都有两种不同的轻量级实现，证实了该方法的广泛适用性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02738v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells**<br />
**Title_cn:** FDNet：用于诱导多能干细胞衍生的星形胶质细胞分割的频域去噪网络<br />
**Authors:** Haoran Li, Jiahua Shi, Huaming Chen, Bo Du, Simon Maksour, Gabrielle Phillips, Mirella Dottori, Jun Shen<br />
**Abstract:** <details><summary>原文: </summary>Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their corresponding pixel-level annotation masks. Moreover, a novel frequency domain denoising network, named FDNet, is proposed for astrocyte segmentation. In detail, our FDNet consists of a contextual information fusion module (CIF), an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse multi-scale feature embeddings to localize the astrocytes. FTB transforms feature embeddings into the frequency domain and conducts a high-pass filter to eliminate interference information. Experimental results demonstrate the superiority of our proposed FDNet over the state-of-the-art substitutes in astrocyte segmentation, shedding insights for iPSC differentiation progress prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>从体细胞人工产生的诱导多能干细胞（iPSC）在神经退行性疾病的疾病建模和药物筛选中发挥着重要作用。由 iPSC 分化而来的星形胶质细胞是研究神经元代谢的重要靶标。星形胶质细胞的分化进程可以通过在不同分化阶段的显微镜图像中观察到的形态变化来监测，然后在成熟时通过分子生物学技术来确定。然而，星形胶质细胞通常“完美”地融入背景，并且其中一些被干扰信息（即死细胞、培养基沉积物和细胞碎片）覆盖，这使得星形胶质细胞难以观察。由于缺乏带注释的数据集，现有最先进的深度学习方法无法用来解决这个问题。在本文中，我们引入了一项名为星形胶质细胞分割的新任务，该任务使用一个名为 IAI704 的新颖数据集，其中包含 704 张图像及其相应的像素级注释掩模。此外，提出了一种新颖的频域去噪网络，称为 FDNet，用于星形胶质细胞分割。具体来说，我们的 FDNet 由上下文信息融合模块（CIF）、注意力块（AB）和傅里叶变换块（FTB）组成。 CIF 和 AB 融合多尺度特征嵌入来定位星形胶质细胞。 FTB将特征嵌入变换到频域并进行高通滤波器以消除干扰信息。实验结果证明，我们提出的 FDNet 在星形胶质细胞分割方面优于最先进的替代品，为 iPSC 分化进展预测提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.02724v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Image-Caption Encoding for Improving Zero-Shot Generalization**<br />
**Title_cn:** 用于改进零样本泛化的图像标题编码<br />
**Authors:** Eric Yang Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis, Brian Kulis<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets. Our code: https://github.com/Chris210634/ice</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型的最新进展将对比方法与生成方法相结合，以在零样本图像分类等下游推理任务上实现最先进的 (SOTA)。然而，这些图像分类模型的一个持续存在的问题是它们的分布外（OOD）泛化能力。我们首先表明，当 OOD 数据点被错误分类时，通常可以在 Top-K 预测类中找到正确的类。为了将模型预测引导到顶级预测类别中的正确类别，我们提出了图像标题编码（ICE）方法，这是一种直接强制仅在评估时图像条件预测和标题条件预测之间的一致性的简单方法。直观上，我们利用生成的标题的独特属性来指导我们在 Top-K 预测类中本地搜索正确的类标签。我们表明，我们的方法可以轻松地与其他 SOTA 方法结合，将 Top-1 OOD 准确率平均提高 0.5%，在具有挑战性的数据集上提高高达 3%。我们的代码：https://github.com/Chris210634/ice</details>
**PDF:** <http://arxiv.org/pdf/2402.02662v1><br />
**Code:** <https://github.com/chris210634/ice>**<br />
>>**index:** 21<br />
**Title:** **Learning with Mixture of Prototypes for Out-of-Distribution Detection**<br />
**Title_cn:** 混合原型学习以进行分布外检测<br />
**Authors:** Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore<br />
**Abstract:** <details><summary>原文: </summary>Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.</details>
**Abstract_cn:** <details><summary>译文: </summary>分布外（OOD）检测旨在检测远离分布内（ID）训练数据的测试样本，这对于机器学习模型在现实世界中的安全部署至关重要。随着深度表示学习的增强，基于距离的 OOD 检测方法应运而生。他们通过测量与 ID 类质心或原型的距离来识别看不见的 OOD 样本。然而，现有的方法依赖于过于简化的数据假设来学习表示，例如，使用一个质心类原型对每一类的 ID 数据进行建模，或者使用并非为 OOD 检测而设计的损失函数，这忽略了数据内的自然多样性。天真地强制每一类的数据样本仅围绕一个原型紧凑，导致对实际数据的建模不充分并且性能有限。为了解决这些问题，我们提出了原型混合学习（PALM），它用多个原型对每个类进行建模以捕获样本多样性，并学习更忠实和紧凑的样本嵌入以增强 OOD 检测。我们的方法自动识别并动态更新原型，通过互邻软分配权重将每个样本分配给原型的子集。 PALM 优化了最大似然估计 (MLE) 损失，以鼓励样本嵌入在相关原型周围紧凑，并优化所有原型上的对比损失，以增强原型级别的类内紧凑性和类间区分度。此外，原型的自动估计使我们的方法能够扩展到具有未标记 ID 数据的具有挑战性的 OOD 检测任务。大量实验证明了 PALM 的优越性，在具有挑战性的 CIFAR-100 基准上实现了最先进的平均 AUROC 性能 93.82。代码可在 https://github.com/jeff024/PALM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.02653v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Densely Decoded Networks with Adaptive Deep Supervision for Medical Image Segmentation**<br />
**Title_cn:** 用于医学图像分割的具有自适应深度监督的密集解码网络<br />
**Authors:** Suraj Mishra<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation using deep neural networks has been highly successful. However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features. To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections. Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow. Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction. In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns. Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training. Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用深度神经网络的医学图像分割非常成功。然而，这些网络的有效性往往受到密集预测不足和无法提取鲁棒特征的限制。为了实现精细的密集预测，我们通过选择性地引入“拐杖”网络连接来提出密集解码网络（ddn）。网络解码器的每个上采样阶段中的这种“拐杖”连接（1）通过合并来自编码器的高分辨率特征来增强目标定位，以及（2）通过促进多级上下文信息流来改进分割。此外，我们提出了一种基于自适应深度监督（ads）的训练策略，该策略利用和适应输入数据集的特定属性，以进行稳健的特征提取。特别是，广告通过将平均输入对象大小与网络的分层有效感受野（lerf）相匹配来战略性地定位和部署辅助监督，从而产生一类 ddns。这种包含来自特定隐藏层的“伴随目标”有助于模型密切关注一些不同的依赖于输入的特征，否则网络可能会在训练过程中“忽略”这些特征。我们的新网络和训练策略在 4 个不同模式的不同数据集上进行了验证，证明了其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02649v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **CLIP Can Understand Depth**<br />
**Title_cn:** CLIP 可以理解深度<br />
**Authors:** Dunam Kim, Seokju Lee<br />
**Abstract:** <details><summary>原文: </summary>Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近关于将 CLIP 推广到单目深度估计的研究表明，在网络爬行数据上进行预训练的 CLIP 对于推导图像块和深度相关提示之间的适当相似性效率很低。在本文中，我们采用 CLIP 来实现具有密集预测的有意义的单目深度估计质量，而无需微调其原始视觉语言对齐。通过联合训练一个紧凑的反卷积解码器和一个名为mirror的微小可学习嵌入矩阵，作为其文本编码器的静态提示，CLIP能够理解深度。通过这种方法，我们的模型在 NYU Depth v2 和 KITTI 数据集上表现出令人印象深刻的性能，与之前几个最先进的仅视觉模型相匹配，大大优于每个基于 CLIP 的深度估计模型。时间深度一致性和空间连续性的实验表明，我们提出的框架可以有效地细化 CLIP 的先验知识。此外，对镜子的消融研究证明，尽管没有给出任何以人类方式编写的提示，但所得模型不仅利用图像编码器的知识，而且还利用文本编码器的知识来估计深度。这项研究表明，通过最小的调整，视觉语言基础模型（例如 CLIP）的先验知识甚至可以推广到预训练期间学习具有挑战性的领域。我们促进未来的工作重点关注使用非人类语言提示调整视觉语言模型的次优先验知识的方法，从而实现与特定任务的最先进方法相媲美的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.03251v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Perceptual Learned Image Compression via End-to-End JND-Based Optimization**<br />
**Title_cn:** 通过基于 JND 的端到端优化进行感知学习图像压缩<br />
**Authors:** Farhad Pakdaman, Sanaz Nami, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Emerging Learned image Compression (LC) achieves significant improvements in coding efficiency by end-to-end training of neural networks for compression. An important benefit of this approach over traditional codecs is that any optimization criteria can be directly applied to the encoder-decoder networks during training. Perceptual optimization of LC to comply with the Human Visual System (HVS) is among such criteria, which has not been fully explored yet. This paper addresses this gap by proposing a novel framework to integrate Just Noticeable Distortion (JND) principles into LC. Leveraging existing JND datasets, three perceptual optimization methods are proposed to integrate JND into the LC training process: (1) Pixel-Wise JND Loss (PWL) prioritizes pixel-by-pixel fidelity in reproducing JND characteristics, (2) Image-Wise JND Loss (IWL) emphasizes on overall imperceptible degradation levels, and (3) Feature-Wise JND Loss (FWL) aligns the reconstructed image features with perceptually significant features. Experimental evaluations demonstrate the effectiveness of JND integration, highlighting improvements in rate-distortion performance and visual quality, compared to baseline methods. The proposed methods add no extra complexity after training.</details>
**Abstract_cn:** <details><summary>译文: </summary>新兴的学习图像压缩（LC）通过对压缩神经网络进行端到端训练，显着提高了编码效率。与传统编解码器相比，这种方法的一个重要好处是，任何优化标准都可以在训练期间直接应用于编码器-解码器网络。 LC 的感知优化以符合人类视觉系统 (HVS) 是此类标准之一，但尚未得到充分探索。本文通过提出一种将可察觉失真 (JND) 原理集成到 LC 中的新颖框架来解决这一差距。利用现有的 JND 数据集，提出了三种感知优化方法，将 JND 集成到 LC 训练过程中：（1）逐像素 JND 损失（PWL）在再现 JND 特征时优先考虑逐像素保真度，（2）逐图像 JND损失（IWL）强调整体不可察觉的退化水平，（3）特征级JND损失（FWL）将重建的图像特征与感知显着特征对齐。实验评估证明了 JND 集成的有效性，突出了与基线方法相比在率失真性能和视觉质量方面的改进。所提出的方法在训练后不会增加额外的复杂性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02836v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Training-Free Consistent Text-to-Image Generation**<br />
**Title_cn:** 免训练一致的文本到图像生成<br />
**Authors:** Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像模型允许用户通过自然语言指导图像生成过程，从而提供了新水平的创意灵活性。然而，使用这些模型在不同的提示下一致地描绘同一主题仍然具有挑战性。现有方法对模型进行微调，以教其描述特定用户提供的主题的新单词或向模型添加图像调节。这些方法需要针对每个对象进行长时间的优化或大规模的预训练。此外，他们很难将生成的图像与文本提示对齐，并且在描绘多个主题时面临困难。在这里，我们提出了 ConsiStory，这是一种免训练方法，通过共享预训练模型的内部激活来实现一致的主题生成。我们引入了主题驱动的共享注意力块和基于对应的特征注入，以促进图像之间的主题一致性。此外，我们制定策略来鼓励布局多样性，同时保持主题一致性。我们将 ConsiStory 与一系列基线进行比较，并展示了在主题一致性和文本对齐方面最先进的性能，而无需任何优化步骤。最后，ConsiStory 可以自然地扩展到多主体场景，甚至可以实现对常见对象的免训练个性化。</details>
**PDF:** <http://arxiv.org/pdf/2402.03286v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image**<br />
**Title_cn:** AdaTreeFormer：从单个高分辨率图像进行树木计数的少量镜头域适应<br />
**Authors:** Hamed Amini Amirkolaee, Miaojing Shi, Lianghua He, Mark Mulligan<br />
**Abstract:** <details><summary>原文: </summary>The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains. We also adopt adversarial learning into the framework to further reduce the gap between source and target domains. Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly.</details>
**Abstract_cn:** <details><summary>译文: </summary>在摄影测量和遥感领域，仅使用单个航空或卫星图像来估计和计算树木密度的过程是一项艰巨的任务。然而，它在森林管理中发挥着至关重要的作用。地形各异的树木种类繁多，严重阻碍了树木计数模型的良好表现。本文的目的是提出一个框架，该框架从具有足够标记树的源域中学习，并适应仅具有有限数量标记树的目标域。我们的方法称为 AdaTreeFormer，包含一个具有分层特征提取方案的共享编码器，用于从源域和目标域中提取稳健的特征。它还由三个子网组成：两个子网分别用于从源域和目标域中提取自域注意力图，一个用于提取跨域注意力图。对于后者，引入注意适应机制，在生成树密度图的同时从不同领域提取相关信息；提出了一种分层跨域特征对齐方案，逐步对齐源域和目标域的特征。我们还在框架中采用对抗性学习，以进一步缩小源域和目标域之间的差距。我们的 AdaTreeFormer 使用三个树木计数数据集（即江苏、优胜美地和伦敦）对六个设计的域适应任务进行了评估；并且显着优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.02956v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey**<br />
**Title_cn:** 探索计算机视觉混合 CNN 和 ViT 架构的协同作用：一项调查<br />
**Authors:** Haruna Yunusa, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Abdulganiyu Abdu Yusuf, Isah Bello, Adamu Lawan<br />
**Abstract:** <details><summary>原文: </summary>The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and ViTs and their collective impact on shaping the future of CV architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络 (CNN) 和视觉变换器 (ViT) 架构的混合已经成为一种突破性的方法，突破了计算机视觉 (CV) 的界限。这篇全面的综述对最先进的混合 CNN-ViT 架构的文献进行了全面的审查，探索了这两种方法之间的协同作用。本次调查的主要内容包括：（1）普通CNN和ViT的背景，（2）系统回顾各种分类混合设计，探索通过合并CNN和ViT模型所实现的协同作用，（3）比较分析和应用任务-不同混合架构之间的具体协同作用，(4)混合模型的挑战和未来方向，(5)最后，调查总结了主要发现和建议。通过对混合 CV 架构的探索，该调查旨在作为指导资源，促进人们更深入地了解 CNN 和 ViT 之间的复杂动态及其对塑造 CV 架构未来的集体影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.02941v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **GPU-Accelerated 3D Polygon Visibility Volumes for Synergistic Perception and Navigation**<br />
**Title_cn:** GPU 加速的 3D 多边形可见体积可实现协同感知和导航<br />
**Authors:** Andrew Willis, Collin Hague, Artur Wolek, Kevin Brink<br />
**Abstract:** <details><summary>原文: </summary>UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location. Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment. This problem is further complicated when the ground location is generalized to a convex 2D polygonal region. This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume. The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations. Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces. Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints. Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission. This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs.</details>
**Abstract_cn:** <details><summary>译文: </summary>无人机任务通常需要地面位置和车辆位置之间满足特定的几何约束。此类要求对于必须在车辆位置和地面控制位置之间保持视线的环境来说是典型的，并且在无人机希望能够（例如使用摄像头传感器）感知特定的监视应用中也很重要。复杂几何环境中的区域。当地面位置推广到凸二维多边形区域时，这个问题会变得更加复杂。本文描述了一个系统的理论和实现，该系统可以快速计算包含所有 3D 坐标的 3D 体积，从中可以完整地查看 2D 凸平面区域；称为可见体积。所提出的方法结合使用 GPU 加速和几何布尔运算的深度图计算来计算可见性体积。此问题的解决方案需要复杂的 3D 几何分析技术，这些技术必须在一组不连续和非分析曲面上使用任意精度算术来执行。后处理步骤合并导航约束，以进一步限制封闭的坐标以包括可见性和导航约束。传感可见度约束与导航约束的集成产生了一系列可导航空间，车辆将在其中满足任务的感知传感和导航需求。然后，该算法提供协同感知和导航敏感解决方案，产生大量 3D 坐标，满足任务路径和传感需求。</details>
**PDF:** <http://arxiv.org/pdf/2402.03135v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey**<br />
**Title_cn:** 人工智能增强虚拟现实在医学中的应用：综合调查<br />
**Authors:** Yixuan Wu, Kaiyuan Hu, Danny Z. Chen, Jian Wu<br />
**Abstract:** <details><summary>原文: </summary>With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift. Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences. Among those applications, medicine is considered one of the most promising areas. In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention. This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies. To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着计算机图形学和人工智能技术的快速发展，我们与世界互动的方式发生了变革。虚拟现实（VR）技术在人工智能（AI）的辅助下，凭借其为用户提供沉浸式体验的优势，已成为多个应用领域的主导交互媒体。在这些应用中，医学被认为是最有前途的领域之一。在本文中，我们对医疗保健和服务中人工智能增强型 VR 应用的新兴领域进行了全面研究。通过引入系统的分类法，我们根据医疗诊断和治疗的不同阶段，将相关技术和应用细致地分为三个明确的类别：可视化增强、VR相关的医疗数据处理和VR辅助干预。这种分类可以对人工智能驱动的虚拟现实在医疗领域发挥的不同作用进行结构化探索，为更全面地理解和评估这些技术提供一个框架。据我们所知，这是对医疗环境中人工智能驱动的 VR 系统的首次系统调查，为这一跨学科领域的未来研究奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.03093v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Motion-Aware Video Frame Interpolation**<br />
**Title_cn:** 运动感知视频帧插值<br />
**Authors:** Pengfei Han, Fuhua Zhang, Bin Zhao, Xuelong Li<br />
**Abstract:** <details><summary>原文: </summary>Video frame interpolation methodologies endeavor to create novel frames betwixt extant ones, with the intent of augmenting the video's frame frequency. However, current methods are prone to image blurring and spurious artifacts in challenging scenarios involving occlusions and discontinuous motion. Moreover, they typically rely on optical flow estimation, which adds complexity to modeling and computational costs. To address these issues, we introduce a Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly estimates intermediate optical flow from consecutive frames by introducing a novel hierarchical pyramid module. It not only extracts global semantic relationships and spatial details from input frames with different receptive fields, enabling the model to capture intricate motion patterns, but also effectively reduces the required computational cost and complexity. Subsequently, a cross-scale motion structure is presented to estimate and refine intermediate flow maps by the extracted features. This approach facilitates the interplay between input frame features and flow maps during the frame interpolation process and markedly heightens the precision of the intervening flow delineations. Finally, a discerningly fashioned loss centered around an intermediate flow is meticulously contrived, serving as a deft rudder to skillfully guide the prognostication of said intermediate flow, thereby substantially refining the precision of the intervening flow mappings. Experiments illustrate that MA-VFI surpasses several representative VFI methods across various datasets, and can enhance efficiency while maintaining commendable efficacy.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频帧插值方法致力于在现有帧之间创建新颖的帧，目的是增强视频的帧频率。然而，当前的方法在涉及遮挡和不连续运动的挑战性场景中容易出现图像模糊和虚假伪影。此外，它们通常依赖于光流估计，这增加了建模的复杂性和计算成本。为了解决这些问题，我们引入了运动感知视频帧插值（MA-VFI）网络，该网络通过引入新颖的分层金字塔模块直接估计连续帧的中间光流。它不仅从具有不同感受野的输入帧中提取全局语义关系和空间细节，使模型能够捕获复杂的运动模式，而且还有效降低了所需的计算成本和复杂性。随后，提出了跨尺度运动结构，以通过提取的特征来估计和细化中间流图。这种方法促进了帧插值过程中输入帧特征和流图之间的相互作用，并显着提高了介入流描绘的精度。最后，精心设计了一个以中间流为中心的独特损失，作为灵巧的舵来巧妙地指导所述中间流的预测，从而大大提高了中间流映射的精度。实验表明，MA-VFI 在各种数据集上超越了几种代表性的 VFI 方法，并且可以在保持令人称赞的功效的同时提高效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.02892v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer**<br />
**Title_cn:** ToonAging：艺术肖像风格迁移下的面部再老化<br />
**Authors:** Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo<br />
**Abstract:** <details><summary>原文: </summary>Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸再老化是计算机视觉和图形领域的一个突出领域，在电影、广告和直播等真实感领域有着重要的应用。最近，将面部重新老化应用于漫画、插图和动画等非真实感图像的需求已成为各个娱乐领域的延伸。然而，由于缺乏能够无缝编辑 NPR 图像上的表观年龄的网络，这意味着这些任务仅限于一种简单的方法，即按顺序应用每个任务。由于域差异，这通常会导致令人不快的伪影和面部属性丢失。在本文中，我们介绍了一种新颖的单阶段方法，用于面部重新老化与肖像风格迁移相结合，并在单个生成步骤中执行。我们利用现有的面部再老化和风格转移网络，两者都在同一公关领域内进行训练。我们的方法独特地融合了不同的潜在向量，每个潜在向量负责管理与衰老相关的属性和 NPR 外观。采用基于示例的方法，我们的方法比域级微调方法提供了更大的灵活性，域级微调方法通常需要针对每个域进行单独的训练或微调。这有效地解决了需要配对数据集进行重新老化和域级数据驱动的风格化方法的限制。我们的实验表明，我们的模型可以毫不费力地生成重新老化的图像，同时转移示例的风格，保持自然的外观和可控性。</details>
**PDF:** <http://arxiv.org/pdf/2402.02733v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Representation Surgery for Multi-Task Model Merging**<br />
**Title_cn:** 多任务模型合并的表示手术<br />
**Authors:** Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xiaojun Chen, Xingwei Wang, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes.</details>
**Abstract_cn:** <details><summary>译文: </summary>多任务学习（MTL）将多个任务的信息压缩到统一的主干中，以提高计算效率和泛化能力。最近的工作直接合并多个独立训练的模型来执行MTL，而不是收集它们的原始数据进行联合训练，极大地扩展了MTL的应用场景。然而，通过可视化现有模型合并方案的表示分布，我们发现合并后的模型经常遭受表示偏差的困境。也就是说，合并模型和单独模型之间的表示分布存在显着差异，导致合并 MTL 的性能较差。在本文中，我们提出了一种称为“Surgery”的表征手术解决方案，以减少合并模型中的表征偏差。具体来说，Surgery 是一个轻量级的特定于任务的模块，它将合并模型的表示作为输入，并尝试输出合并模型的表示中包含的偏差。然后，我们设计了一个无监督优化目标，通过最小化合并模型表示与单个模型表示之间的距离来更新手术模块。大量实验表明，当我们的外科手术模块应用于最先进的 (SOTA) 模型合并方案时，MTL 性能得到显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.02705v1><br />
**Code:** <https://github.com/ennengyang/representationsurgery>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Test-Time Adaptation for Depth Completion**<br />
**Title_cn:** 深度完成的测试时间调整<br />
**Authors:** Hyoungseob Park, Anjali Gupta, Alex Wong<br />
**Abstract:** <details><summary>原文: </summary>It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.</details>
**Abstract_cn:** <details><summary>译文: </summary>当将在某些（源）数据集上训练的模型转移到目标测试数据时，由于它们之间的域差距，通常会观察到性能下降。弥补这一差距的现有方法，例如域适应（DA），可能需要训练模型所依据的源数据（通常不可用），而其他方法，即无源 DA，则需要多次通过测试数据。我们提出了一种用于深度完成的在线测试时自适应方法，该方法从单个图像和相关的稀疏深度图推断密集深度图，从而缩小单次传递中的性能差距。我们首先研究了每种数据模态的域转移如何影响模型性能。根据我们的观察，稀疏深度模态表现出比图像小得多的协变量偏移，我们设计了一个在源域中训练的嵌入模块，该模块保留从仅编码稀疏深度的特征到编码图像和稀疏深度的特征的映射。在测试期间，使用该图作为源域特征的代理来投影稀疏深度特征，并用作训练一组辅助参数（即适应层）的指导，以将目标测试域中的图像和稀疏深度特征对齐到源域的那个。我们在室内和室外场景中评估了我们的方法，结果表明它比基线平均提高了 21.1%。</details>
**PDF:** <http://arxiv.org/pdf/2402.03312v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **V-IRL: Grounding Virtual Intelligence in Real Life**<br />
**Title_cn:** V-IRL：将虚拟智能融入现实生活<br />
**Authors:** Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie<br />
**Abstract:** <details><summary>原文: </summary>There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类居住的地球与创建现代人工智能代理的数字领域之间存在着感官鸿沟。为了开发能够在现实世界中像人类一样灵活感知、思考和行动的人工智能代理，必须弥合数字世界和物理世界之间的现实差距。我们如何才能在像我们所居住的环境一样丰富多样的环境中体现代理，而不受真实硬件和控制的限制？为此，我们引入了 V-IRL：一个平台，使代理能够在虚拟但现实的环境中与现实世界进行可扩展的交互。我们的平台是开发能够完成各种实际任务的代理的游乐场，也是衡量感知、决策以及与全球真实数据交互等能力进展的巨大测试平台。</details>
**PDF:** <http://arxiv.org/pdf/2402.03310v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Towards a Flexible Scale-out Framework for Efficient Visual Data Query Processing**<br />
**Title_cn:** 面向高效可视数据查询处理的灵活横向扩展框架<br />
**Authors:** Rohit Verma, Arun Raghunath<br />
**Abstract:** <details><summary>原文: </summary>There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models. With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query. Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system. Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution. However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server. VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query. Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems. Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server. We demonstrate a 64X reduction in query execution time when adding 64 remote servers.</details>
**Abstract_cn:** <details><summary>译文: </summary>人们对可视化数据管理系统越来越感兴趣，这些系统支持从调整图像大小到运行复杂的机器学习模型等专门操作的查询。由于此类操作过多，在最短的时间内接收查询响应的基本需求就会受到影响，特别是当客户端希望在单个查询中运行多个此类操作时。现有系统提供了一种临时方法，将不同的解决方案组合在一起以提供端到端的可视化数据管理系统。与此类解决方案不同，可视化数据管理系统 (VDMS) 本机执行具有多个操作的查询，从而提供端到端解决方案。然而，本机操作的固定子集和同步线程架构限制了其通用性和可扩展性。在本文中，我们开发了 VDMS-Async，它添加了使用 VDMS 运行用户定义的操作以及在远程服务器上的查询中执行操作的功能。 VDMS-Async 利用事件驱动架构创建高效的管道来执行查询中的操作。我们的实验表明，与现有最先进的系统相比，VDMS-Async 将查询执行时间缩短了 2-3 倍。此外，远程操作与事件驱动架构相结合，使 VDMS-Async 能够随着每个新远程服务器的添加而线性扩展查询执行时间。我们证明，添加 64 个远程服务器后，查询执行时间减少了 64 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.03283v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss**<br />
**Title_cn:** 使用门控卷积和上下文重建损失的全景图像修复<br />
**Authors:** Li Yu, Yanjun Gao, Farhad Pakdaman, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习的方法在解决全景图像修复任务方面取得了令人鼓舞的成果。然而，现有方法很难区分有效像素和无效像素并为损坏区域找到合适的参考，从而导致修复结果中出现伪影。为了应对这些挑战，我们提出了一个全景图像修复框架，该框架由一个人脸生成器、一个立方体生成器、一个侧分支和两个鉴别器组成。我们使用立方体贴图投影（CMP）格式作为网络输入。生成器采用门控卷积来区分有效像素和无效像素，而侧分支的设计利用上下文重建（CR）损失来引导生成器找到最合适的参考补丁来修复缺失区域。在 PSNR 和 SSIM 方面，将所提出的方法与 SUN360 街景数据集上最先进的 (SOTA) 方法进行了比较。实验结果和消融研究表明，所提出的方法在数量和质量上都优于 SOTA。</details>
**PDF:** <http://arxiv.org/pdf/2402.02936v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes**<br />
**Title_cn:** 通过多光源场景中的平滑技术实现逐像素颜色恒定<br />
**Authors:** Umut Cem Entok, Firas Laakom, Farhad Pakdaman, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid. This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image. Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images. Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources. The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss. Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges. Additionally, we propose a label-smoothing technique that enables the model to generalize well despite the uncertainties in ground truth. Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数场景由多个光源照明，传统的均匀照明假设是无效的。大多数颜色恒常性方法都忽略了这个问题，主要是由于多个光源对图像的复杂空间影响。此外，大多数现有的多光源方法无法保持照明的平滑变化，这源于自然图像中的空间依赖性。受此启发，我们提出了一种新颖的多光源颜色恒常性方法，通过学习由多个光源引起的像素级照明图。所提出的方法通过用总变化损失来规范训练来增强相邻像素内的平滑度。此外，进一步提供双边滤波器以增强估计图像的自然外观，同时保留边缘。此外，我们提出了一种标签平滑技术，使模型能够很好地概括，尽管地面事实存在不确定性。定量和定性实验表明，所提出的方法优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.02922v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding**<br />
**Title_cn:** 探索通用音频理解的联合自监督学习<br />
**Authors:** Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Lan Ma, Jiajun Shen<br />
**Abstract:** <details><summary>原文: </summary>The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task. Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>联邦学习 (FL) 和自监督学习 (SSL) 的集成提供了独特的协同组合，可利用音频数据进行通用音频理解，而不会损害用户数据隐私。然而，人们很少努力研究 FL 体系中的 SSL 模型以实现通用音频理解，特别是当训练数据由大规模异构音频源生成时。在本文中，我们评估了特征匹配和预测音频 SSL 技术集成到使用非独立同分布（非独立同分布）数据模拟的大规模 FL 设置中时的性能。我们提出了一种新颖的联合 SSL (F-SSL) 框架，称为 FASSL，它能够从大规模去中心化异构客户端学习中间特征表示，并保存未标记的音频数据。我们的研究发现，在音频检索任务中，音频 F-SSL 方法的表现与集中式音频 SSL 方法相当。大量实验证明了 FASSL 的有效性和重要性，因为它有助于获得最先进的 FL 聚合方法的最佳全局模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.02889v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Time-Distributed Backdoor Attacks on Federated Spiking Learning**<br />
**Title_cn:** 对联邦尖峰学习的时间分布式后门攻击<br />
**Authors:** Gorka Abad, Stjepan Picek, Aitor Urbieta<br />
**Abstract:** <details><summary>原文: </summary>This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了尖峰神经网络 (SNN) 和联邦学习 (FL) 对于使用神经形态数据的后门攻击的脆弱性。尽管 SNN 具有高效性并且 FL 具有隐私优势（尤其是在低功耗设备中），但我们证明这些系统很容易受到此类攻击。我们首先使用神经形态数据评估将 FL 与 SNN 结合使用的可行性，展示其潜在用途。然后，我们评估了已知 FL 攻击方法到 SNN 的可迁移性，发现这些方法会导致攻击性能不佳。因此，我们探索涉及单个和多个攻击者的后门攻击，以提高攻击性能。我们的主要贡献是开发一种针对 SNN 和 FL 的新型攻击策略，该策略可以在恶意设备之间临时分发后门触发器，从而增强攻击的有效性和隐蔽性。在最好的情况下，我们实现了 100 的攻击成功率、0.13 MSE 和 98.9 SSIM。此外，我们调整并评估了现有的后门攻击防御措施，揭示了其在保护 SNN 方面的不足。这项研究强调了部署 SNN 和 FL 时需要采取强有力的安全措施，特别是在后门攻击的情况下。</details>
**PDF:** <http://arxiv.org/pdf/2402.02886v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Enhancing Compositional Generalization via Compositional Feature Alignment**<br />
**Title_cn:** 通过组合特征对齐增强组合泛化<br />
**Authors:** Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao<br />
**Abstract:** <details><summary>原文: </summary>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习模型的实际应用经常面临数据分布变化，其中训练数据分布和测试数据分布之间存在差异。在常见的多域多类设置中，随着类和域数量的增加，收集每个域类组合的训练数据变得不可行。这一挑战自然导致了对具有组合泛化（CG）能力的模型的追求，其中模型可以泛化到看不见的领域类组合。为了深入研究 CG 挑战，我们开发了 CG-Bench，这是一套源自现有现实世界图像数据集的 CG 基准，并观察到基础模型（例如 CLIP 和 DINOv2）上流行的预训练微调范式难以应对这一挑战。为了应对这一挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，i）在预训练编码器上学习关于类和域标签的两个正交线性头，ii）使用新学的头僵住了。我们从理论上和经验上证明 CFA 鼓励预训练模型的组合特征学习。我们进一步在 CG-Bench 上针对 CLIP 和 DINOv2 这两个强大的预训练视觉基础模型进行了广泛的实验。实验结果表明，CFA 在成分泛化方面优于常见的微调技术，证实了 CFA 在成分特征学习方面的功效。</details>
**PDF:** <http://arxiv.org/pdf/2402.02851v1><br />
**Code:** <https://github.com/haoxiang-wang/compositional-feature-alignment>**<br />
>>**index:** 9<br />
**Title:** **Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes**<br />
**Title_cn:** 使用运动提示来监督低数据状态下的单帧身体姿势和形状估计<br />
**Authors:** Andrey Davydov, Alexey Sidnev, Artsiom Sanakoyeu, Yuhua Chen, Mathieu Salzmann, Pascal Fua<br />
**Abstract:** <details><summary>原文: </summary>When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.</details>
**Abstract_cn:** <details><summary>译文: </summary>当有足够的带注释的训练数据可用时，有监督的深度学习算法擅长使用单个相机来估计人体姿势和形状。通过使用其他信息源（例如身体形状数据库）来了解先验知识，可以减轻可用数据太少的影响。不幸的是，此类来源并不总是可用。我们表明，在这种情况下，可以使用易于获取的未注释视频来提供所需的监督信号。给定使用太少注释数据的训练模型，我们计算连续帧中的姿势以及它们之间的光流。然后，我们强制图像光流与从一帧到下一帧的姿势变化推断出的光流之间的一致性。这提供了足够的额外监督，以有效地细化网络权重，并与使用更多注释数据训练的方法相媲美。</details>
**PDF:** <http://arxiv.org/pdf/2402.02736v1><br />
**Code:** null<br />

