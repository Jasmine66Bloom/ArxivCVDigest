## [UPDATED!] **2024-02-06** (Publish Time)

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters**<br />
**Title_cn:** EVA-CLIP-18B：将 CLIP 扩展到 180 亿个参数<br />
**Authors:** Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang<br />
**Abstract:** <details><summary>原文: </summary>Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩大对比语言图像预训练（CLIP）对于增强视觉和多模态模型至关重要。我们推出了 EVA-CLIP-18B，这是迄今为止最大、最强大的开源 CLIP 模型，拥有 180 亿个参数。仅使用 60 亿个训练样本，EVA-CLIP-18B 在 27 个广泛认可的图像分类基准上平均实现了 80.7% 的出色零样本 top-1 准确率，优于其先驱 EVA-CLIP（50 亿个参数）和其他开放模型-来源 CLIP 模型有很大优势。值得注意的是，尽管保持了来自 LAION-2B 和 COYO-700M 的 20 亿图像文本对的恒定训练数据集，但我们观察到 EVA-CLIP 的模型大小缩放带来了一致的性能改进。该数据集是公开可用的，并且比其他最先进的 CLIP 模型中使用的内部数据集（例如 DFN-5B、WebLI-10B）小得多。 EVA-CLIP-18B 展示了 EVA 式从弱到强的视觉模型缩放的潜力。通过公开我们的模型权重，我们希望促进视觉和多模态基础模型的未来研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.04252v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models**<br />
**Title_cn:** SHIELD：使用多模态大语言模型进行人脸欺骗和伪造检测的评估基准<br />
**Authors:** Yichen Shi, Yuhao Gao, Yingxin Lai, Hongyang Wang, Jun Feng, Lei He, Jun Wan, Changsheng Chen, Zitong Yu, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD</details>
**Abstract_cn:** <details><summary>译文: </summary>基于强大的视觉语义表示和语言推理能力，多模态大语言模型（MLLM）在各种视觉领域（例如通用对象识别和基础）表现出了卓越的问题解决能力。然而，MLLM 是否对微妙的视觉欺骗/伪造线索敏感，以及它们在人脸攻击检测（例如，人脸欺骗和伪造检测）领域的表现如何，仍有待探索。在本文中，我们引入了一个新的基准，即 SHIELD，来评估 MLLM 在人脸欺骗和伪造检测方面的能力。具体来说，我们设计了真/假和多项选择题来评估这两个人脸安全任务中的多模态人脸数据。对于面部反欺骗任务，我们在四种类型的演示攻击（即打印攻击、重放攻击、刚性掩模、纸掩模）下评估三种不同的模式（即 RGB、红外、深度）。对于人脸伪造检测任务，我们使用视觉和声学方式评估基于 GAN 和基于扩散的数据。每个问题都在标准和思维链 (COT) 设置下进行零样本和少样本测试。结果表明，MLLM 在人脸安全领域具有巨大的潜力，在可解释性、多模态灵活推理以及联合人脸欺骗和伪造检测方面比传统特定模型具有优势。此外，我们开发了一种新颖的多属性思想链（MA-COT）范式，用于描述和判断人脸图像的各种特定于任务和与任务无关的属性，为微妙的欺骗/伪造线索挖掘提供丰富的任务相关知识。在单独的人脸反欺骗、单独的人脸伪造检测和联合检测任务中进行的大量实验证明了所提出的 MA-COT 的有效性。该项目位于https$:$//github.com/laiyingxin2/SHIELD</details>
**PDF:** <http://arxiv.org/pdf/2402.04178v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation**<br />
**Title_cn:** 无需培训、基于 CLIP 的适应的难以超越的基线<br />
**Authors:** Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \url{https://github.com/mrflogs/ICLR24}.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比语言图像预训练（CLIP）因其卓越的零样本能力而受到欢迎。最近的研究重点是开发高效的微调方法，例如即时学习和适配器，以增强 CLIP 在下游任务中的性能。然而，这些方法仍然需要额外的训练时间和计算资源，这对于资源有限的设备来说是不可取的。在本文中，我们回顾了经典算法高斯判别分析（GDA），并将其应用于 CLIP 的下游分类。通常，GDA 假设每个类别的特征遵循具有相同协方差的高斯分布。通过利用贝叶斯公式，分类器可以用类均值和协方差来表示，可以根据数据进行估计，而无需训练。为了整合来自视觉和文本模式的知识，我们将其与 CLIP 中原始的零样本分类器集成。 17 个数据集的广泛结果验证了我们的方法在少数样本分类、不平衡学习和分布外泛化方面超越或达到了与最先进方法相当的结果。此外，我们将我们的方法扩展到新的泛化和无监督学习，再次证明了其相对于竞争方法的优越性。我们的代码可在 \url{https://github.com/mrflogs/ICLR24} 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04087v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing**<br />
**Title_cn:** 使用空间和通道注意进行多类道路缺陷检测和分割以进行自主道路修复<br />
**Authors:** Jongmin Yu, Chen Bene Chi, Sebastiano Fichera, Paolo Paoletti, Devansh Mehta, Shan Luo<br />
**Abstract:** <details><summary>原文: </summary>Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>道路路面检测和分割对于开发自动道路修复系统至关重要。然而，由于道路路面图像的纹理简单性、缺陷几何形状的多样性以及类之间的形态模糊性，开发同时执行多类缺陷检测和分割的实例分割方法具有挑战性。我们提出了一种用于多类道路缺陷检测和分割的新颖的端到端方法。所提出的方法包括多个空间和通道方向的注意力块，可用于学习跨空间和通道方向维度的全局表示。通过这些注意力块，可以学习道路缺陷的形态信息（空间特征）以及图像的颜色和深度信息的更全局通用的表示。为了证明我们框架的有效性，我们对新收集的带有九个道路缺陷类别注释的数据集进行了各种消融研究并与先前的方法进行了比较。实验表明，我们提出的方法优于现有的多类道路缺陷检测和分割方法的最先进方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.04064v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models**<br />
**Title_cn:** 连接点：黑盒视觉语言模型的协作微调<br />
**Authors:** Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan<br />
**Abstract:** <details><summary>原文: </summary>With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\% compared to the white-box method.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着预训练视觉语言模型（VLM）的出现，人们投入了大量的精力来针对下游任务对其进行微调。尽管在设计有效的微调方法方面取得了进展，但此类方法需要访问模型的参数，这可能具有挑战性，因为模型所有者通常选择将其模型作为黑匣子提供以保护模型所有权。本文提出了一种 \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) 方法，用于微调黑盒 VLM 到下游任务，其中人们只能访问模型的输入提示和输出预测。 CraFT 包括两个模块，一个用于学习文本提示的提示生成模块和一个用于增强残差样式输出预测的预测细化模块。此外，我们引入了辅助预测一致损失来促进这些模块之间的一致优化。这些模块通过新颖的协作训练算法进行了优化。对超过 15 个数据集的少量样本分类进行的大量实验证明了 CraFT 的优越性。结果表明，CraFT 在 16 个镜头的数据集和仅 8,000 个查询的情况下实现了约 12% 的不错的增益。此外，CraFT 训练速度更快，部署时仅使用约 1/80 的内存占用，与白盒方法相比仅牺牲 1.62%。</details>
**PDF:** <http://arxiv.org/pdf/2402.04050v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation**<br />
**Title_cn:** Polyp-DDPM：基于扩散的语义息肉合成以增强分割<br />
**Authors:** Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao<br />
**Abstract:** <details><summary>原文: </summary>This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究介绍了 Polyp-DDPM，这是一种基于扩散的方法，用于生成戴在面罩上的息肉的真实图像，旨在增强胃肠道息肉的分割。我们的方法解决了与医学图像相关的数据限制、高注释成本和隐私问题的挑战。通过在分割掩模（代表异常区域的二进制掩模）上调节扩散模型，Polyp-DDPM 在图像质量方面优于最先进的方法（与上述分数相比，Frechet 起始距离 (FID) 分数达到 78.47） 83.79）和分割性能（实现 0.7156 的交集（IoU），而来自基线模型的合成图像小于 0.6694，真实数据小于 0.7067）。我们的方法生成高质量、多样化的合成数据集用于训练，从而增强息肉分割模型以与真实图像进行比较，并提供更强大的数据增强功能来改进分割模型。 Polyp-DDPM 的源代码和预训练权重可在 https://github.com/mobaidoctor/polyp-ddpm 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04031v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **YOLOPoint Joint Keypoint and Object Detection**<br />
**Title_cn:** YOLOPoint 联合关键点和物体检测<br />
**Authors:** Anton Backhaus, Thorsten Luettel, Hans-Joachim Wuensche<br />
**Abstract:** <details><summary>原文: </summary>Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>未来的智能车辆必须能够理解周围环境并安全导航。基于摄像头的车辆系统可以使用关键点以及物体作为独立于 GNSS 的 SLAM 和视觉里程计的低级和高级地标。为此，我们提出了 YOLOPoint，这是一种卷积神经网络模型，通过结合 YOLOv5 和 SuperPoint 创建一个既实时又准确的单一前向网络，同时检测图像中的关键点和对象。通过使用共享主干和轻量级网络结构，YOLOPoint 能够在 HPatches 和 KITTI 基准测试上具有竞争力的表现。</details>
**PDF:** <http://arxiv.org/pdf/2402.03989v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time**<br />
**Title_cn:** 只要有足够的时间，人类就能在识别异常姿势的物体方面击败深度网络<br />
**Authors:** Netta Ollikka, Amro Abbas, Andrea Perin, Markku Kilpeläinen, Stéphane Deny<br />
**Abstract:** <details><summary>原文: </summary>Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extra viewing time may be key to attain such robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习正在缩小在多个物体识别基准上与人类的差距。在这里，我们在具有挑战性的图像背景下研究这一差距，其中物体是从不寻常的视角看到的。我们发现，人类擅长识别异常姿势的物体，而最先进的预训练网络（EfficientNet、SWAG、ViT、SWIN、BEiT、ConvNext）在这种情况下系统性很脆弱。值得注意的是，当我们限制图像曝光时间时，人类的表现会下降到深层网络的水平，这表明当人类识别不寻常姿势的物体时，会发生额外的心理过程（需要额外的时间）。最后，我们对人类与网络的错误模式的分析表明，即使是有时间限制的人类也与前馈深度网络不同。我们的结论是，需要做更多的工作才能使计算机视觉系统达到人类视觉系统的鲁棒性水平。了解额外观看时间内发生的心理过程的本质可能是实现这种鲁棒性的关键。</details>
**PDF:** <http://arxiv.org/pdf/2402.03973v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping**<br />
**Title_cn:** 通过变形约束翘曲提高跨模型属的对抗性可迁移性<br />
**Authors:** Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo Hou, Linlin Shen, Siyang Song<br />
**Abstract:** <details><summary>原文: </summary>Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.</details>
**Abstract_cn:** <details><summary>译文: </summary>由代理模型生成的对抗性示例通常表现出对未知目标系统的有限可转移性。为了解决这个问题，人们提出了许多可迁移性增强方法（例如输入变换和模型增强）。然而，它们在攻击具有与代理模型不同的模型属的系统时表现不佳。在本文中，我们提出了一种新颖且通用的攻击策略，称为变形约束翘曲攻击（DeCoWA），可以有效地应用于跨模型属攻击。具体来说，DeCoWA 首先通过弹性变形，即变形约束扭曲（DeCoW）来增强输入示例，以获得增强输入的丰富局部细节。为了避免随机变形导致的全局语义的严重扭曲，DeCoW 通过一种新颖的自适应控制策略进一步限制扭曲变换的强度和方向。大量实验表明，我们的 DeCoWA 在 CNN 代理上制作的可转移示例可能会严重阻碍 Transformer 在各种任务上的性能（反之亦然），包括图像分类、视频动作识别和音频识别。代码可在 https://github.com/LinQinLiang/DeCoWA 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03951v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **A new method for optical steel rope non-destructive damage detection**<br />
**Title_cn:** 一种光学钢丝绳无损损伤检测新方法<br />
**Authors:** Yunqing Bao, Bin Hu<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种高海拔环境（空中索道）钢丝绳无损损伤检测的新算法。该算法包括两个关键部分：首先，设计了一个名为 RGBD-UNet 的分割模型，用于从复杂背景中准确提取钢丝绳。该模型具有通过所提出的 CMA 模块处理和组合颜色和深度信息的能力。其次，开发了名为 VovNetV3.5 的检测模型来区分正常和异常钢丝绳。它将 VovNet 架构与 DBB 模块集成以增强性能。此外，提出了一种新的背景增强方法来增强分割模型的泛化能力。创建包含不同场景下钢丝绳图像的数据集，用于分割和检测模型的训练和测试。实验证明比基线模型有显着改进。在所提出的数据集上，检测模型实现的最高精度达到0.975，分割模型实现的最大F-measure达到0.948。</details>
**PDF:** <http://arxiv.org/pdf/2402.03843v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **An SVD-free Approach to Nonlinear Dictionary Learning based on RVFL**<br />
**Title_cn:** 基于RVFL的无SVD非线性字典学习方法<br />
**Authors:** G. Madhuri, Atul Negi<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a novel nonlinear dictionary learning algorithm leveraging the theory of a feed-forward neural network called Random Vector Functional Link (RVFL). The proposed RVFL-based nonlinear Dictionary Learning (RVFLDL) learns a dictionary as a sparse-to-dense feature map from nonlinear sparse coefficients to the dense input features. Kernel-based nonlinear dictionary learning methods operate in a feature space obtained by an implicit feature map, and they are not independent of computationally expensive operations like Singular Value Decomposition (SVD). Training the RVFL-based dictionary is free from SVD computation as RVFL generates weights from the input to the output layer analytically. Sparsity-inducing Horse-shoe prior is assumed on the coefficients to generate a sparse coefficient matrix w.r.t an initial random dictionary. Higher-order dependencies between the input sparse coefficients and the dictionary atoms are incorporated into the training process by nonlinearly transforming the sparse coefficients and adding them as enhanced features. Thus the method projects sparse coefficients to a higher dimensional space while inducing nonlinearities into the dictionary. For classification using RVFL-net, a classifier matrix is learned as a transform that maps nonlinear sparse coefficients to the labels. The performance of the method illustrated in image classification and reconstruction applications is comparable to that of other nonlinear dictionary learning methods. Experiments show that RVFLDL is scalable and provides a solution better than those obtained using other nonlinear dictionary learning methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种新颖的非线性字典学习算法，利用称为随机向量函数链接（RVFL）的前馈神经网络理论。所提出的基于 RVFL 的非线性字典学习 (RVFLDL) 将字典学习为从非线性稀疏系数到稠密输入特征的稀疏到稠密特征映射。基于内核的非线性字典学习方法在由隐式特征图获得的特征空间中运行，并且它们不独立于奇异值分解（SVD）等计算量大的操作。训练基于 RVFL 的字典无需进行 SVD 计算，因为 RVFL 通过分析方式生成从输入到输出层的权重。在系数上假设稀疏性诱导马蹄先验，以生成关于初始随机字典的稀疏系数矩阵。通过非线性变换稀疏系数并将其添加为增强特征，将输入稀疏系数和字典原子之间的高阶依赖性纳入训练过程。因此，该方法将稀疏系数投影到更高维空间，同时将非线性引入字典中。对于使用 RVFL-net 进行分类，分类器矩阵被学习为将非线性稀疏系数映射到标签的变换。该方法在图像分类和重建应用中的性能可与其他非线性字典学习方法相媲美。实验表明，RVFLDL 是可扩展的，并且提供了比使用其他非线性字典学习方法获得的解决方案更好的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.03833v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Face Detection: Present State and Research Directions**<br />
**Title_cn:** 人脸检测：现状和研究方向<br />
**Authors:** Purnendu Prabhat, Himanshu Gupta, Ajeet Kumar Vishwakarma<br />
**Abstract:** <details><summary>原文: </summary>The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数处理人类图像的计算机视觉应用程序都使用人脸检测作为核心组件。尽管有关该主题的研究很多，但人脸检测仍然存在问题。人脸检测的准确性和速度可能还会提高。这篇综述文件展示了该领域取得的进展以及仍需要解决的实质性问题。该论文提供了可以作为人脸检测领域研究项目的研究方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.03796v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Energy-based Domain-Adaptive Segmentation with Depth Guidance**<br />
**Title_cn:** 具有深度引导的基于能量的域自适应分割<br />
**Authors:** Jinjing Zhu, Zhedong Hu, Tae-Kyun Kim, Lin Wang<br />
**Abstract:** <details><summary>原文: </summary>Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近人们努力利用自监督深度估计作为语义分割的无监督域适应（UDA）的指导。然而，现有技术忽视了语义特征和深度特征之间的差异以及特征融合的可靠性，从而导致分割性能不佳。为了解决这个问题，我们提出了一种名为 SMART（基于能量估计的跨域语义分割）的新型 UDA 框架，该框架利用基于能量的模型（EBM）来获取任务自适应特征，并通过自监督实现语义分割的可靠特征融合深度估计。我们的框架包含两个新颖的组件：基于能量的特征融合（EB2F）和基于能量的可靠融合评估（RFA）模块。 EB2F 模块通过使用 Hopfield 能量显式测量和减少其差异来生成任务自适应语义和深度特征，以实现更好的特征融合。 RFA模块使用能量评分来评估特征融合的可靠性，以提高深度引导的有效性。对两个数据集的广泛实验表明，我们的方法比之前的工作取得了显着的性能提升，验证了我们基于能量的学习方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.03795v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning**<br />
**Title_cn:** 通过弱监督即时学习探索低资源医学图像分类<br />
**Authors:** Fudan Zheng, Jindong Cao, Weijiang Yu, Zhiguang Chen, Nong Xiao, Yutong Lu<br />
**Abstract:** <details><summary>原文: </summary>Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design. Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification. The proposed prompt generator is lightweight and therefore can be embedded into any network architecture.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于医学领域资源匮乏，注释成本高昂且专业，支持临床辅助诊断的医学图像识别的大多数进展都面临着挑战。通过相关的医学文本提示，利用大规模预训练视觉语言模型的可转移表示可以缓解资源不足的问题。然而，现有的预训练视觉语言模型需要领域专家精心设计医疗提示，这大大增加了临床医生的负担。为了解决这个问题，我们提出了一种弱监督提示学习方法MedPrompt来自动生成医疗提示，其中包括无监督预训练视觉语言模型和弱监督提示学习模型。无监督预训练视觉语言模型利用医学图像和相应医学文本之间的自然相关性进行预训练，无需任何手动注释。弱监督提示学习模型仅利用数据集中的图像类别来指导提示中特定类别向量的学习，而提示中其他上下文向量的学习不需要手动注释来指导。据我们所知，这是第一个自动生成医疗提示的模型。通过这些提示，预训练的视觉语言模型可以摆脱手动注释和手动提示设计的强烈专家依赖性。实验结果表明，使用我们自动生成的提示的模型优于其全镜头学习手工制作的提示，而仅使用最少数量的标记样本进行少镜头学习，并且在零镜头图像分类上达到了优异或相当的精度。所提出的提示生成器是轻量级的，因此可以嵌入到任何网络架构中。</details>
**PDF:** <http://arxiv.org/pdf/2402.03783v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection**<br />
**Title_cn:** AttackNet：通过定制的活体检测卷积神经网络架构增强生物识别安全性<br />
**Authors:** Oleksandr Kuznetsov, Dmytro Zakharov, Emanuele Frontoni, Andrea Maranesi<br />
**Abstract:** <details><summary>原文: </summary>Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance. This paper introduces AttackNet, a bespoke Convolutional Neural Network architecture, meticulously designed to combat spoofing threats in biometric systems. Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level pattern discernment. Three distinctive architectural phases form the crux of the model, each underpinned by judiciously chosen activation functions, normalization techniques, and dropout layers to ensure robustness and resilience against adversarial attacks. Benchmarking our model across diverse datasets affirms its prowess, showcasing superior performance metrics in comparison to contemporary models. Furthermore, a detailed comparative analysis accentuates the model's efficacy, drawing parallels with prevailing state-of-the-art methodologies. Through iterative refinement and an informed architectural strategy, AttackNet underscores the potential of deep learning in safeguarding the future of biometric security.</details>
**Abstract_cn:** <details><summary>译文: </summary>生物识别安全是现代身份验证和认证系统的基石，其中生物识别样本的完整性和可靠性至关重要。本文介绍 AttackNet，这是一种定制的卷积神经网络架构，经过精心设计，旨在对抗生物识别系统中的欺骗威胁。该模型植根于深度学习方法，提供分层防御机制，从低级特征提取无缝过渡到高级模式识别。三个独特的架构阶段构成了模型的关键，每个阶段都以明智选择的激活函数、标准化技术和丢失层为基础，以确保针对对抗性攻击的鲁棒性和弹性。在不同的数据集上对我们的模型进行基准测试证实了它的能力，与当代模型相比，展示了卓越的性能指标。此外，详细的比较分析强调了模型的有效性，与流行的最先进的方法进行了比较。通过迭代改进和明智的架构策略，AttackNet 强调了深度学习在保障生物识别安全的未来方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.03769v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction**<br />
**Title_cn:** MoD-SLAM：用于无界 3D 场景重建的单目密集建图<br />
**Authors:** Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li<br />
**Abstract:** <details><summary>原文: </summary>Neural implicit representations have recently been demonstrated in many fields including Simultaneous Localization And Mapping (SLAM). Current neural SLAM can achieve ideal results in reconstructing bounded scenes, but this relies on the input of RGB-D images. Neural-based SLAM based only on RGB images is unable to reconstruct the scale of the scene accurately, and it also suffers from scale drift due to errors accumulated during tracking. To overcome these limitations, we present MoD-SLAM, a monocular dense mapping method that allows global pose optimization and 3D reconstruction in real-time in unbounded scenes. Optimizing scene reconstruction by monocular depth estimation and using loop closure detection to update camera pose enable detailed and precise reconstruction on large scenes. Compared to previous work, our approach is more robust, scalable and versatile. Our experiments demonstrate that MoD-SLAM has more excellent mapping performance than prior neural SLAM methods, especially in large borderless scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经隐式表示最近已在许多领域得到证明，包括同步定位和建图（SLAM）。目前的神经SLAM在重建有界场景方面可以取得理想的结果，但这依赖于RGB-D图像的输入。仅基于RGB图像的神经SLAM无法准确重建场景的尺度，而且还会因跟踪过程中累积的误差而出现尺度漂移。为了克服这些限制，我们提出了 MoD-SLAM，这是一种单目密集建图方法，允许在无界场景中实时进行全局姿态优化和 3D 重建。通过单目深度估计优化场景重建并使用闭环检测来更新相机姿态，从而能够对大场景进行详细而精确的重建。与之前的工作相比，我们的方法更加稳健、可扩展且通用。我们的实验表明，MoD-SLAM 比现有的神经 SLAM 方法具有更出色的建图性能，尤其是在大型无边界场景中。</details>
**PDF:** <http://arxiv.org/pdf/2402.03762v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Virtual Classification: Modulating Domain-Specific Knowledge for Multidomain Crowd Counting**<br />
**Title_cn:** 虚拟分类：调整多域人群计数的特定领域知识<br />
**Authors:** Mingyue Guo, Binghui Chen, Zhaoyi Yan, Yaowei Wang, Qixiang Ye<br />
**Abstract:** <details><summary>原文: </summary>Multidomain crowd counting aims to learn a general model for multiple diverse datasets. However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias. In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting. MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias. Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions. To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space. This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated. Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning. Code is available at \url{https://github.com/csguomy/MDKNet}.</details>
**Abstract_cn:** <details><summary>译文: </summary>多域人群计数旨在学习多个不同数据集的通用模型。然而，深度网络更喜欢对主导域而不是所有域的分布进行建模，这称为域偏差。在本研究中，我们提出了一种简单而有效的调制领域特定知识网络（MDKNet）来处理多领域人群计数中的领域偏差问题。 MDKNet 是通过采用“调制”的思想来实现的，能够实现深度网络平衡并以很小的偏差对不同数据集的不同分布进行建模。具体来说，我们提出了一个特定于实例的批量归一化（IsBN）模块，它充当基础调制器来细化信息流以适应域分布。为了精确调制特定于域的信息，引入域引导虚拟分类器（DVC）来学习域可分离的潜在空间。该空间被用作 IsBN 调制器的输入指导，以便可以很好地处理多个数据集的混合分布。在上海科技 A/B、QNRF 和 NWPU 等流行基准上进行的大量实验验证了 MDKNet 在处理多域人群计数方面的优越性以及多域学习的有效性。代码可在 \url{https://github.com/csguomy/MDKNet} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03758v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in Panchromatic Satellite Images**<br />
**Title_cn:** SISP：全色卫星图像中细粒度船舶实例分割的基准数据集<br />
**Authors:** Pengming Feng, Mingjie Xie, Hongning Liu, Xuanjia Zhao, Guangjun He, Xueliang Zhang, Jian Guan<br />
**Abstract:** <details><summary>原文: </summary>Fine-grained ship instance segmentation in satellite images holds considerable significance for monitoring maritime activities at sea. However, existing datasets often suffer from the scarcity of fine-grained information or pixel-wise localization annotations, as well as the insufficient image diversity and variations, thus limiting the research of this task. To this end, we propose a benchmark dataset for fine-grained Ship Instance Segmentation in Panchromatic satellite images, namely SISP, which contains 56,693 well-annotated ship instances with four fine-grained categories across 10,000 sliced images, and all the images are collected from SuperView-1 satellite with the resolution of 0.5m. Targets in the proposed SISP dataset have characteristics that are consistent with real satellite scenes, such as high class imbalance, various scenes, large variations in target densities and scales, and high inter-class similarity and intra-class diversity, all of which make the SISP dataset more suitable for real-world applications. In addition, we introduce a Dynamic Feature Refinement-assist Instance segmentation network, namely DFRInst, as the benchmark method for ship instance segmentation in satellite images, which can fortify the explicit representation of crucial features, thus improving the performance of ship instance segmentation. Experiments and analysis are performed on the proposed SISP dataset to evaluate the benchmark method and several state-of-the-art methods to establish baselines for facilitating future research. The proposed dataset and source codes will be available at: https://github.com/Justlovesmile/SISP.</details>
**Abstract_cn:** <details><summary>译文: </summary>卫星图像中的细粒度船舶实例分割对于监测海上活动具有重要意义。然而，现有的数据集往往缺乏细粒度信息或像素级定位注释，以及图像多样性和变化不足，从而限制了该任务的研究。为此，我们提出了一个用于全色卫星图像中细粒度船舶实例分割的基准数据集，即 SISP，它包含 10,000 个切片图像中的 56,693 个经过良好注释的船舶实例，其中有四个细粒度类别，并且所有图像都收集自SuperView-1卫星，分辨率0.5m。所提出的SISP数据集中的目标具有与真实卫星场景一致的特征，例如类不平衡度高、场景多样、目标密度和尺度变化大、类间相似度和类内多样性高，所有这些使得SISP数据集更适合实际应用。此外，我们引入了动态特征细化辅助实​​例分割网络，即DFRInst，作为卫星图像中船舶实例分割的基准方法，它可以强化关键特征的显式表示，从而提高船舶实例分割的性能。在所提出的 SISP 数据集上进行实验和分析，以评估基准方法和几种最先进的方法，以建立促进未来研究的基线。建议的数据集和源代码将在以下网址提供：https://github.com/Justlovesmile/SISP。</details>
**PDF:** <http://arxiv.org/pdf/2402.03708v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats**<br />
**Title_cn:** MMAUD：针对现代微型无人​​机威胁的综合多模式反无人机数据集<br />
**Authors:** Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen, Jianfei Yang, Fen Liu, Jianping Li, Han Wang, Lihua Xie<br />
**Abstract:** <details><summary>原文: </summary>In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了应对小型无人机 (UAV) 所带来的不断变化的挑战，这些无人机具有运输有害有效载荷或独立造成损害的潜力，我们推出了 MMAUD：综合性多模态反无人机数据集。 MMAUD 通过专注于无人机检测、无人机类型分类和轨迹估计，解决了当代威胁检测方法中的一个关键差距。 MMAUD 因结合了多种感官输入而脱颖而出，包括立体视觉、各种激光雷达、雷达和音频阵列。它提供了独特的空中探测，对于解决现实场景至关重要，其保真度高于使用热成像和 RGB 在特定有利位置捕获的数据集。此外，MMAUD 提供准确的徕卡生成的地面实况数据，增强可信度并能够自信地改进算法和模型，这在其他数据集中从未见过。大多数现有作品都没有公开其数据集，这使得 MMAUD 成为开发准确有效的解决方案的宝贵资源。我们提出的模式具有成本效益且适应性强，允许用户试验和实施新的无人机威胁检测工具。我们的数据集通过结合周围的重型机械声音来密切模拟现实世界的场景。这种方法增强了数据集的适用性，捕获了邻近车辆操作期间面临的确切挑战。预计 MMAUD 可以在推进无人机威胁检测、分类、轨迹估计能力等方面发挥关键作用。我们的数据集、代码和设计将在 https://github.com/ntu-aris/MMAUD 中提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.03706v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **SHMC-Net: A Mask-guided Feature Fusion Network for Sperm Head Morphology Classification**<br />
**Title_cn:** SHMC-Net：用于精子头部形态分类的掩模引导特征融合网络<br />
**Authors:** Nishchal Sapkota, Yejia Zhang, Sirui Li, Peixian Liang, Zhuo Zhao, Danny Z Chen<br />
**Abstract:** <details><summary>原文: </summary>Male infertility accounts for about one-third of global infertility cases. Manual assessment of sperm abnormalities through head morphology analysis encounters issues of observer variability and diagnostic discrepancies among experts. Its alternative, Computer-Assisted Semen Analysis (CASA), suffers from low-quality sperm images, small datasets, and noisy class labels. We propose a new approach for sperm head morphology classification, called SHMC-Net, which uses segmentation masks of sperm heads to guide the morphology classification of sperm images. SHMC-Net generates reliable segmentation masks using image priors, refines object boundaries with an efficient graph-based method, and trains an image network with sperm head crops and a mask network with the corresponding masks. In the intermediate stages of the networks, image and mask features are fused with a fusion scheme to better learn morphological features. To handle noisy class labels and regularize training on small datasets, SHMC-Net applies Soft Mixup to combine mixup augmentation and a loss function. We achieve state-of-the-art results on SCIAN and HuSHeM datasets, outperforming methods that use additional pre-training or costly ensembling techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>男性不育症约占全球不育症病例的三分之一。通过头部形态分析对精子异常进行手动评估会遇到观察者变异性和专家之间诊断差异的问题。它的替代方案是计算机辅助精液分析 (CASA)，但存在精子图像质量低、数据集小和类别标签嘈杂的问题。我们提出了一种精子头部形态分类的新方法，称为 SHMC-Net，它使用精子头部的分割掩模来指导精子图像的形态分类。 SHMC-Net 使用图像先验生成可靠的分割掩模，使用高效的基于图的方法细化对象边界，并使用精子头部作物训练图像网络和使用相应掩模的掩模网络。在网络的中间阶段，图像和掩模特征通过融合方案进行融合，以更好地学习形态特征。为了处理噪声类标签并规范小数据集的训练，SHMC-Net 应用 Soft Mixup 将混合增强和损失函数结合起来。我们在 SCIAN 和 HuSHeM 数据集上取得了最先进的结果，优于使用额外预训练或昂贵的集成技术的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.03697v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation**<br />
**Title_cn:** ConUNETR：用于 3D Micro-CT 胚胎软骨分割的条件变压器网络<br />
**Authors:** Nishchal Sapkota, Yejia Zhang, Susan M. Motch Perrine, Yuhan Hsi, Sirui Li, Meng Wu, Greg Holmes, Abdul R. Abdulai, Ethylin W. Jabs, Joan T. Richtsmeier, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Studying the morphological development of cartilaginous and osseous structures is critical to the early detection of life-threatening skeletal dysmorphology. Embryonic cartilage undergoes rapid structural changes within hours, introducing biological variations and morphological shifts that limit the generalization of deep learning-based segmentation models that infer across multiple embryonic age groups. Obtaining individual models for each age group is expensive and less effective, while direct transfer (predicting an age unseen during training) suffers a potential performance drop due to morphological shifts. We propose a novel Transformer-based segmentation model with improved biological priors that better distills morphologically diverse information through conditional mechanisms. This enables a single model to accurately predict cartilage across multiple age groups. Experiments on the mice cartilage dataset show the superiority of our new model compared to other competitive segmentation models. Additional studies on a separate mice cartilage dataset with a distinct mutation show that our model generalizes well and effectively captures age-based cartilage morphology patterns.</details>
**Abstract_cn:** <details><summary>译文: </summary>研究软骨和骨结构的形态发育对于早期发现危及生命的骨骼畸形至关重要。胚胎软骨在数小时内经历快速的结构变化，引入生物变异和形态变化，限制了基于深度学习的分割模型的泛化，这些模型可以推断多个胚胎年龄组。为每个年龄组获取单独的模型成本高昂且效率较低，而直接迁移（预测训练期间未见的年龄）会因形态变化而导致潜在的性能下降。我们提出了一种基于 Transformer 的新型分割模型，该模型具有改进的生物学先验，可以通过条件机制更好地提取形态多样化的信息。这使得单一模型能够准确预测多个年龄组的软骨。对小鼠软骨数据集的实验表明，与其他竞争性分割模型相比，我们的新模型具有优越性。对具有明显突变的单独小鼠软骨数据集的其他研究表明，我们的模型可以很好地概括并有效地捕获基于年龄的软骨形态模式。</details>
**PDF:** <http://arxiv.org/pdf/2402.03695v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection**<br />
**Title_cn:** BEAM：用于多视图 3D 物体检测的 Beta 分布射线去噪<br />
**Authors:** Feng Liu, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang Wan, Qixiang Ye, Yanzhao Zhou<br />
**Abstract:** <details><summary>原文: </summary>Multi-view 3D object detectors struggle with duplicate predictions due to the lack of depth information, resulting in false positive detections. In this study, we introduce BEAM, a novel Beta Distribution Ray Denoising approach that can be applied to any DETR-style multi-view 3D detector to explicitly incorporate structure prior knowledge of the scene. By generating rays from cameras to objects and sampling spatial denoising queries from the Beta distribution family along these rays, BEAM enhances the model's ability to distinguish spatial hard negative samples arising from ambiguous depths. BEAM is a plug-and-play technique that adds only marginal computational costs during training, while impressively preserving the inference speed. Extensive experiments and ablation studies on the NuScenes dataset demonstrate significant improvements over strong baselines, outperforming the state-of-the-art method StreamPETR by 1.9% mAP. The code will be available at https://github.com/LiewFeng/BEAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于缺乏深度信息，多视图 3D 物体检测器难以应对重复预测，从而导致误报检测。在本研究中，我们介绍了 BEAM，这是一种新颖的 Beta 分布射线去噪方法，可应用于任何 DETR 式多视图 3D 探测器，以明确地结合场景的结构先验知识。通过生成从摄像机到物体的射线并沿这些射线对 Beta 分布族的空间去噪查询进行采样，BEAM 增强了模型区分由模糊深度产生的空间硬负样本的能力。 BEAM 是一种即插即用技术，在训练期间仅增加边际计算成本，同时令人印象深刻地保持了推理速度。对 NuScenes 数据集的大量实验和消融研究表明，相对于强基线有显着改进，比最先进的方法 StreamPETR 高 1.9% mAP。该代码可在 https://github.com/LiewFeng/BEAM 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03634v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model**<br />
**Title_cn:** CAT-SAM：用于分段任意模型的少样本自适应的条件调整网络<br />
**Authors:** Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Shijian Lu<br />
**Abstract:** <details><summary>原文: </summary>The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation. However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters. The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder. The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits. We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks. Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup. Project page: \url{https://xiaoaoran.github.io/projects/CAT-SAM}</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的Segment Anything Model（SAM）在一般图像分割中表现出了卓越的零样本能力和灵活的几何提示。然而，SAM 在处理各种非常规图像（例如航空图像、医学图像和非 RGB 图像）时常常遇到困难。本文提出了 CAT-SAM，这是一种 ConditionAl Tuning 网络，只需少量目标样本即可使 SAM 适应各种非常规目标任务。 CAT-SAM 冻结整个 SAM，并使用少量可学习参数同时调整其掩模解码器和图像编码器。核心设计是一个即时桥结构，可以实现重量级图像编码器和轻量级掩模解码器的解码器条件联合调整。桥接将掩模解码器的提示令牌映射到图像编码器，促进编码器和解码器的协同适应，互惠互利。我们为图像编码器开发了两种代表性的调整策略，这导致了两种 CAT-SAM 变体：一种在输入空间中注入可学习的提示标记，另一种插入轻量级适配器网络。超过 11 项非常规任务的大量实验表明，即使在非常具有挑战性的一次性适应设置下，两种 CAT-SAM 变体也能始终如一地实现卓越的目标分割性能。项目页面：\url{https://xiaoaoran.github.io/projects/CAT-SAM}</details>
**PDF:** <http://arxiv.org/pdf/2402.03631v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning**<br />
**Title_cn:** 利用注入知识的学习提高跨模式的上下文一致性，以实现有效的多模式营销<br />
**Authors:** Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek<br />
**Abstract:** <details><summary>原文: </summary>The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.</details>
**Abstract_cn:** <details><summary>译文: </summary>能够捕捉多种模式瞬间的智能设备的普及使用户能够在线体验多模式信息。然而，大型语言（LLM）和视觉模型（LVM）在捕获具有跨模态语义关系的整体意义方面仍然受到限制。如果没有明确的常识知识（例如，作为知识图），视觉语言模型（VLM）只能通过捕获大量语料库中的高级模式来学习隐式表示，从而缺少必要的上下文跨模式线索。在这项工作中，我们设计了一个框架，将知识图形式的明确常识知识与大型 VLM 结合起来，以提高下游任务的性能，预测多模式营销活动的有效性。虽然营销应用程序为评估我们的方法提供了令人信服的指标，但我们的方法能够及早发现可能有说服力的多模式活动以及评估和增强营销理论。</details>
**PDF:** <http://arxiv.org/pdf/2402.03607v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **VRMM: A Volumetric Relightable Morphable Head Model**<br />
**Title_cn:** VRMM：体积可重复照明可变形头部模型<br />
**Authors:** Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了体积可重光照变形模型 (VRMM)，这是一种用于 3D 面部建模的新型体积和参数化面部先验。虽然最近的体积先验模型比 3D 可变形模型 (3DMM) 等传统方法有所改进，但它们在模型学习和个性化重建方面面临挑战。我们的 VRMM 通过采用一种新颖的训练框架来克服这些问题，该框架可以有效地将身份、表达和照明的潜在空间解开并编码为低维表示。该框架采用自监督学习设计，大大减少了训练数据的约束，使其在实践中更加可行。学习后的 VRMM 提供重新照明功能并包含全面的表达式。我们通过头像生成、面部重建和动画等各种应用展示了 VRMM 的多功能性和有效性。此外，我们通过基于 VRMM 的新颖的保留先验个性化框架解决了生成体积模型中过度拟合的常见问题。这种方法甚至可以根据单个肖像输入进行准确的 3D 面部重建。我们的实验展示了 VRMM 显着增强 3D 面部建模领域的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.04101v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal**<br />
**Title_cn:** HarmBench：自动化红队和强力拒绝的标准化评估框架<br />
**Authors:** Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动化红队对于发现和减轻与恶意使用大型语言模型 (LLM) 相关的风险有着巨大的希望，但该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了 HarmBench，这是一个用于自动化红队的标准化评估框架。我们确定了以前在红队评估中未考虑到的几个理想属性，并系统地设计 HarmBench 以满足这些标准。使用 HarmBench，我们对 18 种红队方法和 33 种目标法学硕士和防御进行了大规模比较，得出了新颖的见解。我们还引入了一种高效的对抗训练方法，该方法极大地增强了 LLM 在各种攻击中的鲁棒性，展示了 HarmBench 如何实现攻击和防御的协同开发。我们在 https://github.com/centerforaisafety/HarmBench 开源 HarmBench。</details>
**PDF:** <http://arxiv.org/pdf/2402.04249v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs**<br />
**Title_cn:** 本能偏见：虚假图像导致 MLLM 产生幻觉<br />
**Authors:** Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, Tong Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The resource is available in https://github.com/MasaiahHan/CorrelationQA.</details>
**Abstract_cn:** <details><summary>译文: </summary>大语言模型（LLM）最近取得了显着的进展，其中多模态大语言模型（MLLM）的出现赋予了LLM视觉能力，在各种多模态任务中取得了令人印象深刻的表现。然而，当出现某些图像和文本输入时，那些强大的 MLLM（例如 GPT-4V）仍然会严重失败。在本文中，我们确定了困扰 MLLM 的一类典型输入，这些输入由高度相关但与答案不一致的图像组成，导致 MLLM 产生幻觉。为了量化效果，我们提出了 CorrelationQA，这是第一个评估给定虚假图像的幻觉水平的基准。该基准测试包含 13 个类别的 7,308 个文本-图像对。基于所提出的 CorrelationQA，我们对 9 个主流 MLLM 进行了全面分析，表明它们普遍不同程度地遭受这种本能偏见。我们希望我们策划的基准和评估结果有助于更好地评估 MLLM 在存在误导性图像的情况下的稳健性。该资源位于 https://github.com/MasaiahHan/CorrelationQA。</details>
**PDF:** <http://arxiv.org/pdf/2402.03757v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback**<br />
**Title_cn:** 使用 AI 反馈的强化学习来调整视频的大型多模态模型<br />
**Authors:** Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型的最新进展影响了视频大型多模态模型（VLMM）的发展。之前的 VLMM 方法涉及使用指令调整数据集进行监督微调 (SFT)、将 LLM 与视觉编码器集成以及添加额外的可学习模块。视频和文本多模式对齐仍然具有挑战性，主要是由于与纯文本数据相比，多模式指令调整数据的数量和质量不足。我们提出了一种新颖的对齐策略，采用多模态人工智能系统来监督自身，称为人工智能反馈强化学习（RLAIF），提供自我偏好反馈来完善自​​身并促进视频和文本模态的对齐。具体来说，我们提出了上下文感知奖励建模，通过在生成偏好反馈期间提供详细的视频描述作为上下文，以丰富对视频内容的理解。我们的多模态 RLAIF 方法 VLM-RLAIF 在不同的视频基准测试中展示了增强的性能，其性能优于包括 SFT 模型在内的现有方法。我们致力于开源我们的代码、模型和数据集，以促进该领域的进一步研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.03746v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Automatic Robotic Development through Collaborative Framework by Large Language Models**<br />
**Title_cn:** 通过大型语言模型的协作框架进行自动机器人开发<br />
**Authors:** Zhirong Luan, Yujun Lai<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable code generation abilities of large language models LLMs, they still face challenges in complex task handling. Robot development, a highly intricate field, inherently demands human involvement in task allocation and collaborative teamwork . To enhance robot development, we propose an innovative automated collaboration framework inspired by real-world robot developers. This framework employs multiple LLMs in distinct roles analysts, programmers, and testers. Analysts delve deep into user requirements, enabling programmers to produce precise code, while testers fine-tune the parameters based on user feedback for practical robot application. Each LLM tackles diverse, critical tasks within the development process. Clear collaboration rules emulate real world teamwork among LLMs. Analysts, programmers, and testers form a cohesive team overseeing strategy, code, and parameter adjustments . Through this framework, we achieve complex robot development without requiring specialized knowledge, relying solely on non experts participation.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管大型语言模型法学硕士具有出色的代码生成能力，但它们在复杂任务处理方面仍然面临挑战。机器人开发是一个高度复杂的领域，本质上需要人类参与任务分配和协作团队合作。为了加强机器人开发，我们提出了一个受现实世界机器人开发人员启发的创新自动化协作框架。该框架采用了多个法学硕士担任不同角色的分析师、程序员和测试人员。分析师深入研究用户需求，使程序员能够编写出精确的代码，而测试人员则根据用户反馈微调参数，以适应实际的机器人应用。每个法学硕士都处理开发过程中的各种关键任务。清晰的协作规则模拟了法学硕士之间现实世界的团队合作。分析师、程序员和测试人员组成了一个有凝聚力的团队，负责监督策略、代码和参数调整。通过这个框架，我们无需专业知识，仅依靠非专家参与即可实现复杂的机器人开发。</details>
**PDF:** <http://arxiv.org/pdf/2402.03699v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **U-shaped Vision Mamba for Single Image Dehazing**<br />
**Title_cn:** 用于单图像去雾的 U 形 Vision Mamba<br />
**Authors:** Zhuoran Zheng, Chen Wu<br />
**Abstract:** <details><summary>原文: </summary>Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \url{https://github.com/zzr-idam}.</details>
**Abstract_cn:** <details><summary>译文: </summary>目前，Transformer 是最流行的图像去雾架构，但由于其计算复杂度较高，其处理远程依赖的能力在资源受限的设备上受到限制。为了应对这一挑战，我们引入了 U 形 Vision Mamba (UVM-Net)，这是一种高效的单图像去雾网络。受状态空间序列模型（SSM）（一种以其处理长序列的能力而闻名的新型深度序列模型）的启发，我们设计了一个 Bi-SSM 模块，它将卷积层的局部特征提取能力与 SSM 的能力集成在一起。捕获远程依赖关系。大量的实验结果证明了我们方法的有效性。我们的方法为图像去雾以及其他图像恢复任务提供了更高效的远程依赖建模思想。代码的 URL 是 \url{https://github.com/zzr-idam}。</details>
**PDF:** <http://arxiv.org/pdf/2402.04139v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning**<br />
**Title_cn:** 用于参数高效微调的低阶注意力侧调<br />
**Authors:** Ningyuan Tang, Minghao Fu, Ke Zhu, Jianxin Wu<br />
**Abstract:** <details><summary>原文: </summary>In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters. LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\% of GPU memory footprint and 60\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>在将大型预训练模型微调到下游任务时，参数高效微调（PEFT）方法可以有效地微调可训练参数较少的预训练模型，但会遇到 GPU 内存消耗高和训练速度慢的问题。由于这些方法中的可学习参数与预训练模型纠缠在一起，因此在微调期间必须计算并存储与冻结预训练模型参数相关的梯度。我们提出了低秩注意力侧调（LAST），它不仅冻结预训练网络的参数，还冻结预训练网络的输出，从而将可训练模块与预训练模型分开。 LAST 训练仅由低秩自注意力模块组成的侧网络。通过将预训练模型视为冻结的特征提取器，侧网络从预训练模型中获取中间输出，并专注于学习特定于任务的知识。我们还表明，LAST 可以在多个优化目标上高度并行，使其在下游任务适应方面非常高效，例如在寻找最佳超参数方面。与现有 PEFT 方法相比，LAST 在 VTAB-1K 和其他视觉适应任务上优于以前最先进的方法，大约仅占用 30% 的 GPU 内存占用和 60% 的训练时间，但精度却显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.04009v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting**<br />
**Title_cn:** 基于扩散的运动行为预测的可控多样化采样<br />
**Authors:** Yiming Xu, Hao Cheng, Monika Sester<br />
**Abstract:** <details><summary>原文: </summary>In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶任务中，复杂交通环境中的轨迹预测需要遵守现实世界的上下文条件和行为多模态。现有方法主要依赖于先验假设或基于精选数据训练的生成模型来学习道路代理受场景约束的随机行为。然而，由于数据不平衡和简单化先验，它们经常面临模式平均问题，甚至可能由于不稳定的训练和单一的地面实况监督而遭受模式崩溃。这些问题导致现有方法失去预测多样性和遵守场景约束。为了解决这些挑战，我们引入了一种名为可控扩散轨迹（CDT）的新型轨迹生成器，它将地图信息和社交互动集成到基于 Transformer 的条件去噪扩散模型中，以指导未来轨迹的预测。为了确保多模态，我们结合了行为标记来指导轨迹的模式，例如直行、右转或左转。此外，我们将预测的端点作为替代行为标记纳入 CDT 模型中，以促进准确轨迹的预测。对 Argoverse 2 基准的大量实验表明，CDT 擅长在复杂的城市环境中生成多样化且符合场景的轨迹。</details>
**PDF:** <http://arxiv.org/pdf/2402.03981v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Intensive Vision-guided Network for Radiology Report Generation**<br />
**Title_cn:** 用于生成放射学报告的强化视觉引导网络<br />
**Authors:** Fudan Zheng, Mengfei Li, Ying Wang, Weijiang Yu, Ruixuan Wang, Zhiguang Chen, Nong Xiao, Yutong Lu<br />
**Abstract:** <details><summary>原文: </summary>Automatic radiology report generation is booming due to its huge application potential for the healthcare industry. However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects. First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view. However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis. Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods. We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports. Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception. GIA aims to learn three types of vision perception: depth view, space view, and pixel view. On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction. Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD. Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动放射学报告生成由于其在医疗保健行业的巨大应用潜力而蓬勃发展。然而，现有的计算机视觉和自然语言处理方法来解决这个问题在两个方面受到限制。首先，在提取图像特征时，大多数人忽略了视觉中的多视图推理，而对医学图像的单视图结构进行建模，例如空间视图或通道视图。然而，临床医生在日常临床诊断中依靠多视角影像信息进行综合判断。其次，在生成报告时，他们忽略了多模态信息的上下文推理，而专注于利用基于检索的方法进行纯文本优化。我们的目标是通过提出一种模型来解决这两个问题，该模型可以更好地模拟临床医生的观点并生成更准确的报告。鉴于特征提取中的上述限制，我们在医学图像编码器中提出了全局密集注意力（GIA）模块来模拟和集成多视图视觉感知。 GIA 旨在学习三种类型的视觉感知：深度视图、空间视图和像素视图。另一方面，为了解决报告生成中的上述问题，我们探索如何涉及多模态信号来生成精确匹配的报告，即如何在下一个单词预测中将先前预测的单词与区域感知的视觉内容集成。具体来说，我们设计了一个视觉知识引导解码器（VKGD），它可以自适应地考虑模型需要在多大程度上依赖视觉信息和先前预测的文本来辅助下一个单词预测。因此，我们最终的强化视觉引导网络 (IVGN) 框架包括 GIA 引导的视觉编码器和 VKGD。对两个常用数据集 IU X-Ray 和 MIMIC-CXR 的实验证明了我们的方法与其他最先进的方法相比具有卓越的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.03754v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images**<br />
**Title_cn:** 在具有最小缩放图像的小数据集上预训练轻量级视觉变压器<br />
**Authors:** Jen Hong Tan<br />
**Abstract:** <details><summary>原文: </summary>Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.</details>
**Abstract_cn:** <details><summary>译文: </summary>在小图像分辨率的小数据集上，轻量级 Vision Transformer (ViT) 能否匹配或超过 ResNet 等卷积神经网络 (CNN) 的性能？该报告表明，纯 ViT 确实可以通过预训练实现卓越的性能，使用带有最小图像缩放的掩模自动编码器技术。我们在 CIFAR-10 和 CIFAR-100 数据集上进行的实验涉及 ViT 模型，参数少于 365 万个，乘法累加 (MAC) 计数低于 0.27G，使它们符合“轻量级”模型的资格。与以前的方法不同，我们的方法在类似的基于 Transformer 的轻量级架构中获得了最先进的性能，而无需显着放大 CIFAR-10 和 CIFAR-100 的图像。这一成就强调了我们模型的效率，不仅在处理小数据集方面，而且在有效处理接近原始比例的图像方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.03752v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification**<br />
**Title_cn:** 基于注意力的形状和步态表示学习，用于基于视频的换衣人员重新识别<br />
**Authors:** Vuong D. Nguyen, Samiha Mirza, Pranav Mantini, Shishir K. Shah<br />
**Abstract:** <details><summary>原文: </summary>Current state-of-the-art Video-based Person Re-Identification (Re-ID) primarily relies on appearance features extracted by deep learning models. These methods are not applicable for long-term analysis in real-world scenarios where persons have changed clothes, making appearance information unreliable. In this work, we deal with the practical problem of Video-based Cloth-Changing Person Re-ID (VCCRe-ID) by proposing "Attention-based Shape and Gait Representations Learning" (ASGL) for VCCRe-ID. Our ASGL framework improves Re-ID performance under clothing variations by learning clothing-invariant gait cues using a Spatial-Temporal Graph Attention Network (ST-GAT). Given the 3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises multi-head attention modules, which are able to enhance the robustness of gait embeddings under viewpoint changes and occlusions. The ST-GAT amplifies the important motion ranges and reduces the influence of noisy poses. Then, the multi-head learning module effectively reserves beneficial local temporal dynamics of movement. We also boost discriminative power of person representations by learning body shape cues using a GAT. Experiments on two large-scale VCCRe-ID datasets demonstrate that our proposed framework outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in mAP.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前最先进的基于视频的行人重新识别（Re-ID）主要依赖于深度学习模型提取的外观特征。这些方法不适用于人们换衣服的现实场景中的长期分析，使得外观信息不可靠。在这项工作中，我们通过为 VCCRe-ID 提出“基于注意力的形状和步态表示学习”（ASGL）来解决基于视频的换衣人重识别（VCCRe-ID）的实际问题。我们的 ASGL 框架通过使用时空图注意网络 (ST-GAT) 学习服装不变的步态线索，提高了服装变化下的 Re-ID 性能。考虑到基于 3D 骨架的时空图，我们提出的 ST-GAT 包含多头注意力模块，能够增强步态嵌入在视点变化和遮挡下的鲁棒性。 ST-GAT 放大了重要的运动范围并减少了噪声姿势的影响。然后，多头学习模块有效地保留有益的局部运动动态。我们还通过使用 GAT 学习体形线索来增强人物表征的辨别能力。在两个大型 VCCRe-ID 数据集上进行的实验表明，我们提出的框架在 1 级精度方面优于最先进的方法 12.2%，在 mAP 方面优于最先进的方法 7.0%。</details>
**PDF:** <http://arxiv.org/pdf/2402.03716v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Instance by Instance: An Iterative Framework for Multi-instance 3D Registration**<br />
**Title_cn:** 逐个实例：多实例 3D 配准的迭代框架<br />
**Authors:** Xinyue Cao, Xiyu Zhang, Yuxin Cheng, Zhaoshuai Qi, Yanning Zhang, Jiaqi Yang<br />
**Abstract:** <details><summary>原文: </summary>Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system. In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg). It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones. Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances. Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg. Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>多实例注册是计算机视觉和机器人技术中的一个具有挑战性的问题，其中一个对象的多个实例需要在标准坐标系中注册。在这项工作中，我们提出了第一个名为逐实例（IBI）的迭代框架，用于多实例 3D 配准（MI-3DReg）。它连续注册给定场景中的所有实例，从最简单的开始，逐渐到更具挑战性的实例。在整个迭代过程中，异常值不断被消除，导致剩余的和更具挑战性的实例的内部值率不断增加。在IBI框架下，我们进一步提出了一种基于稀疏到密集对应的多实例注册方法（IBI-S2DC）来实现鲁棒的MI-3DReg。对合成数据集和真实数据集的实验证明了 IBI 的有效性，并表明了 IBI-S2DC 的新的最先进性能，例如，我们的 MHF1 比现有的最先进性能高 12.02%/12.35%合成/真实数据集上的艺术方法 ECC。</details>
**PDF:** <http://arxiv.org/pdf/2402.04195v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN**<br />
**Title_cn:** 使用 3D RRDB-GAN 实现放射学中的 3D 体积超分辨率<br />
**Authors:** Juhyung Ha, Nian Wang, Surendra Maharjan, Xuhong Zhang<br />
**Abstract:** <details><summary>原文: </summary>This study introduces the 3D Residual-in-Residual Dense Block GAN (3D RRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D RRDB-GAN is the integration of a 2.5D perceptual loss function, which contributes to improved volumetric image quality and realism. The effectiveness of our model was evaluated through 4x super-resolution experiments across diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6. These evaluations, encompassing both quantitative metrics like LPIPS and FID and qualitative assessments through sample visualizations, demonstrate the models effectiveness in detailed image analysis. The 3D RRDB-GAN offers a significant contribution to medical imaging, particularly by enriching the depth, clarity, and volumetric detail of medical images. Its application shows promise in enhancing the interpretation and analysis of complex medical imagery from a comprehensive 3D perspective.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究介绍了用于放射学图像 3D 超分辨率的 3D 残差密集块 GAN (3D RRDB-GAN)。 3D RRDB-GAN 的一个关键方面是集成了 2.5D 感知损失函数，这有助于提高体积图像质量和真实感。我们的模型的有效性通过不同数据集（包括 Mice Brain MRH、OASIS、HCP1200 和 MSD-Task-6）的 4 倍超分辨率实验进行了评估。这些评估包括 LPIPS 和 FID 等定量指标以及通过样本可视化进行的定性评估，证明了模型在详细图像分析中的有效性。 3D RRDB-GAN 为医学成像做出了重大贡献，特别是丰富了医学图像的深度、清晰度和体积细节。它的应用有望从全面的 3D 角度增强复杂医学图像的解释和分析。</details>
**PDF:** <http://arxiv.org/pdf/2402.04171v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EscherNet: A Generative Model for Scalable View Synthesis**<br />
**Title_cn:** EscherNet：可扩展视图合成的生成模型<br />
**Authors:** Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison<br />
**Abstract:** <details><summary>原文: </summary>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{https://kxhit.github.io/EscherNet}.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 EscherNet，一种用于视图合成的多视图条件扩散模型。 EscherNet 学习隐式和生成 3D 表示，并结合专门的相机位置编码，从而允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。 EscherNet 在视图合成方面提供了卓越的通用性、灵活性和可扩展性——尽管使用固定数量的 3 个参考视图到 3 个目标视图进行训练，但它可以在单个消费级 GPU 上同时生成 100 多个一致的目标视图。因此，EscherNet 不仅解决了零样本新颖视图合成的问题，而且还自然地统一了单图像和多图像 3D 重建，将这些不同的任务组合到一个单一的、有凝聚力的框架中。我们广泛的实验表明，即使与专门针对每个问题量身定制的方法相比，EscherNet 在多个基准测试中也实现了最先进的性能。这种非凡的多功能性为设计 3D 视觉的可扩展神经架构开辟了新的方向。项目页面：\url{https://kxhit.github.io/EscherNet}。</details>
**PDF:** <http://arxiv.org/pdf/2402.03908v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation**<br />
**Title_cn:** 信念场景图：通过期望计算​​用对象扩展部分场景<br />
**Authors:** Mario A. V. Saucedo, Akash Patel, Akshit Saradagi, Christoforos Kanellakis, George Nikolakopoulos<br />
**Abstract:** <details><summary>原文: </summary>In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of \textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了信念场景图的新概念，它是部分 3D 场景图的实用驱动扩展，可以利用部分信息进行高效的高级任务规划。我们提出了一种基于图的学​​习方法，用于计算任何给定 3D 场景图上的置信度（也称为期望），然后用于策略性地添加与机器人任务相关的新节点（称为盲节点） 。我们提出了基于相关信息（CECI）的期望计算方法，通过从可用的训练数据中学习直方图来合理地近似真实的信念/期望。开发了一种新颖的图卷积神经网络 (GCN) 模型，用于从 3D 场景图存储库中学习 CECI。由于不存在用于训练新颖的 CECI 模型的 3D 场景图数据库，因此我们提出了一种基于语义注释的现实生活 3D 空间生成 3D 场景图数据集的新颖方法。然后，生成的数据集用于训练所提出的 CECI 模型并对所提出的方法进行广泛验证。我们建立了 \textit{Belief Scene Graphs} (BSG) 的新概念，作为将期望集成到抽象表示中的核心组件。这个新概念是经典 3D 场景图概念的演变，旨在为各种机器人任务的任务规划和优化提供高级推理。整个框架的功效已经在对象搜索场景中进行了评估，并且还在现实生活中进行了测试，以模拟人类对看不见的对象的常识。</details>
**PDF:** <http://arxiv.org/pdf/2402.03840v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving**<br />
**Title_cn:** OASim：基于神经渲染的自动驾驶开放自适应模拟器<br />
**Authors:** Guohang Yan, Jiahao Pi, Jianfei Guo, Zhaotong Luo, Min Dou, Nianchen Deng, Qiusheng Huang, Daocheng Fu, Licheng Wen, Pinlong Cai, et.al.<br />
**Abstract:** <details><summary>原文: </summary>With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real-world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering. It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted into the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition. Code is available at https://github.com/PJLab-ADG/OASim.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着深度学习和计算机视觉技术的发展，自动驾驶为提高交通安全和效率提供了新的解决方案。构建高质量数据集的重要性不言而喻，尤其是随着近年来端到端自动驾驶算法的兴起。数据在算法闭环系统中起着核心作用。然而，收集真实世界的数据既昂贵、耗时又不安全。随着隐式渲染技术的发展以及使用生成模型大规模生成数据的深入研究，我们提出了OASim，一种基于隐式神经渲染的开放式自适应模拟器和自动驾驶数据生成器。它具有以下特点：（1）通过神经隐式表面重建技术进行高质量的场景重建。 (2)自我车辆和参与车辆的轨迹编辑。 (3)丰富的车辆模型库，可自由选择插入场景。 (4)丰富的传感器模型库，可以选择指定的传感器来生成数据。 (5)高度可定制的数据生成系统，可以根据用户需求生成数据。我们通过 Carla 模拟器的感知性能评估和现实世界的数据采集来证明所生成数据的高质量和保真度。代码可在 https://github.com/PJLab-ADG/OASim 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03830v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos**<br />
**Title_cn:** Rig3DGS：从休闲单目视频创建可控肖像<br />
**Authors:** Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras<br />
**Abstract:** <details><summary>原文: </summary>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html</details>
**Abstract_cn:** <details><summary>译文: </summary>从休闲智能手机视频中创建可控的 3D 人物肖像是非常理想的，因为它们在 AR/VR 应用中具有巨大的价值。 3D Gaussian Splatting (3DGS) 的最新发展显示出渲染质量和训练效率的提高。然而，从单视图捕获中准确建模和分离头部运动和面部表情以实现高质量渲染仍然是一个挑战。在本文中，我们引入 Rig3DGS 来应对这一挑战。我们在规范空间中使用一组 3D 高斯函数来表示整个场景，包括动态主题。使用一组控制信号（例如头部姿势和表情），我们将它们转换到具有学习变形的 3D 空间，以生成所需的渲染。我们的关键创新是精心设计的变形方法，该方法以源自 3D 可变形模型的可学习先验为指导。这种方法在训练中非常高效，并且在控制各种捕获的面部表情、头部位置和视图合成方面非常有效。我们通过广泛的定量和定性实验证明了所学变形的有效性。项目页面可以在http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html找到</details>
**PDF:** <http://arxiv.org/pdf/2402.03723v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **3Doodle: Compact Abstraction of Objects with 3D Strokes**<br />
**Title_cn:** 3Doodle：使用 3D 笔画对对象进行紧凑抽象<br />
**Authors:** Changwoon Choi, Jaeah Lee, Jaesik Park, Young Min Kim<br />
**Abstract:** <details><summary>原文: </summary>While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic B ezier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然手绘草图长期以来一直是传达物体特征的有效表示，但它们通常是主观的，与现实的表示有很大偏差。此外，草图对于任意视点来说并不一致，使得捕捉 3D 形状变得困难。我们提出 3Dooole，在给定目标对象的多视图图像的情况下生成描述性且视图一致的草图图像。我们的方法基于这样的想法：一组 3D 笔划可以有效地表示 3D 结构信息并渲染视图一致的 2D 草图。我们将 2D 草图表示为视图无关组件和视图相关组件的联合。 3D 三次 B ezier 曲线表示与视图无关的 3D 特征线，而超二次曲面的轮廓表示不同视点体积的平滑轮廓。我们的流程直接优化 3D 笔画基元的参数，以完全可微分的方式最大限度地减少感知损失。由此产生的稀疏 3D 笔画集可以渲染为包含各种对象的基本 3D 特征形状的抽象草图。我们证明，与最近的草图生成方法相比，3Doodle 可以忠实地表达原始图像的概念。</details>
**PDF:** <http://arxiv.org/pdf/2402.03690v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning**<br />
**Title_cn:** OVOR：OnePrompt 具有虚拟离群值正则化功能，可实现免排练的课堂增量学习<br />
**Authors:** Wei-Cheng Huang, Chun-Fu Chen, Hsiang Hsu<br />
**Abstract:** <details><summary>原文: </summary>Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at https://github.com/jpmorganchase/ovor.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究表明，通过使用大型预训练模型和可学习的提示，用于课堂增量学习（CIL）设置的免排练方法可以实现优于基于排练的方法的性能。无需排练的 CIL 方法很难区分不同任务中的类，因为它们没有一起训练。在这项工作中，我们提出了一种基于虚拟异常值的正则化方法，以收紧分类器的决策边界，从而减轻不同任务之间的类混淆。最近的基于提示的方法通常需要一组特定于任务的提示，以防止用新任务的知识覆盖先前任务的知识，从而导致在从池中查询和编写适当的提示时进行额外的计算。正如我们在论文中所揭示的，这种额外成本可以在不牺牲准确性的情况下消除。我们证明，基于简化提示的方法可以使用更少的可学习参数和更低的推理成本，获得与之前配备提示池的最先进（SOTA）方法相当的结果。我们的正则化方法已经证明了它与不同的基于提示的方法的兼容性，提高了以前的 SOTA 免演练 CIL 方法在 ImageNet-R 和 CIFAR-100 基准上的准确性。我们的源代码可在 https://github.com/jpmorganchase/ovor 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04129v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning**<br />
**Title_cn:** 用于冷启动无范例增量学习的弹性特征整合<br />
**Authors:** Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de Weijer, Andrew D. Bagdanov<br />
**Abstract:** <details><summary>原文: </summary>Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>无范例类增量学习 (EFCIL) 旨在从一系列任务中学习，而无需访问先前的任务数据。在本文中，我们考虑了具有挑战性的冷启动场景，其中第一个任务中没有足够的数据来学习高质量的骨干网。这对于 EFCIL 来说尤其具有挑战性，因为它需要高可塑性，这会导致特征漂移，而在无样本设置中很难补偿。为了解决这个问题，我们提出了一种简单而有效的方法，通过规范与先前任务高度相关的方向上的漂移来巩固特征表示，并采用原型来减少任务新近度偏差。我们的方法称为弹性特征合并（EFC），利用基于经验特征矩阵（EFM）的特征漂移的易于处理的二阶近似。 EFM 在特征空间中引入了伪度量，我们用它来规范重要方向上的特征漂移，并更新用于新型非对称交叉熵损失的高斯原型，该原型有效地平衡了原型排练与新任务的数据。 CIFAR-100、Tiny-ImageNet、ImageNet-Subset 和 ImageNet-1K 上的实验结果表明，弹性特征合并能够通过保持模型可塑性更好地学习新任务，并且显着优于最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.03917v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching**<br />
**Title_cn:** Deep MSFOP：在深度函数图中保留多个光谱滤波器算子以实现无监督形状匹配<br />
**Authors:** Feifan Luo, Qingsong Li, Ling Hu, Xinru Liu, Haojun Xu, Haibo Wang, Ting Li, Shengjun Liu<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel constraint called Multiple Spectral filter Operators Preservation (MSFOR) to compute functional maps and based on it, develop an efficient deep functional map architecture called Deep MSFOP for shape matching. The core idea is that, instead of using the general descriptor preservation constraint, we require our maps to preserve multiple spectral filter operators. This allows us to incorporate more informative geometrical information, contained in different frequency bands of functions, into the functional map computing. This can be confirmed by that some previous techniques like wavelet preservation and LBO commutativity are actually our special cases. Moreover, we also develop a very efficient way to compute the maps with MSFOP constraint, which can be conveniently embedded into the deep learning, especially having learnable filter operators. Utilizing the above results, we finally design our Deep MSFOP pipeline, equipped with a suitable unsupervised loss jointly penalizing the functional map and the underlying pointwise map. Our deep functional map has notable advantages, including that the functional map is more geometrically informative and guaranteed to be proper, and the computing is numerically stable. Extensive experimental results on different datasets demonstrate that our approach outperforms the existing state-of-the-art methods, especially in challenging settings like non-isometric and inconsistent topology datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种称为多光谱滤波器算子保留（MSFOR）的新颖约束来计算功能图，并基于它开发了一种称为深度 MSFOP 的高效深度功能图架构，用于形状匹配。核心思想是，我们不使用通用描述符保留约束，而是要求我们的映射保留多个光谱滤波器算子。这使我们能够将包含在函数不同频带中的更多几何信息合并到函数图计算中。这可以通过之前的一些技术（例如小波保存和 LBO 交换性）实际上是我们的特例来证实。此外，我们还开发了一种非常有效的方法来计算具有 MSFOP 约束的映射，它可以方便地嵌入到深度学习中，特别是具有可学习的过滤器算子。利用上述结果，我们最终设计了 Deep MSFOP 管道，配备了合适的无监督损失，共同惩罚功能图和底层的逐点图。我们的深度函数图具有显着的优势，包括函数图的几何信息更丰富且保证正确，并且计算在数值上稳定。对不同数据集的广泛实验结果表明，我们的方法优于现有的最先进方法，特别是在非等距和不一致的拓扑数据集等具有挑战性的设置中。</details>
**PDF:** <http://arxiv.org/pdf/2402.03904v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Convincing Rationales for Visual Question Answering Reasoning**<br />
**Title_cn:** 视觉问答推理的令人信服的理由<br />
**Authors:** Kun Li, George Vosselman, Michael Ying Yang<br />
**Abstract:** <details><summary>原文: </summary>Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image. It requires deep understanding of both the textual question and visual image. Prior works directly evaluate the answering models by simply calculating the accuracy of the predicted answers. However, the inner reasoning behind the prediction is disregarded in such a "black box" system, and we do not even know if one can trust the predictions. In some cases, the models still get the correct answers even when they focus on irrelevant visual regions or textual tokens, which makes the models unreliable and illogical. To generate both visual and textual rationales next to the predicted answer to the given image/question pair, we propose Convincing Rationales for VQA, CRVQA. Considering the extra annotations brought by the new outputs, {CRVQA} is trained and evaluated by samples converted from some existing VQA datasets and their visual labels. The extensive experiments demonstrate that the visual and textual rationales support the prediction of the answers, and further improve the accuracy. Furthermore, {CRVQA} achieves competitive performance on generic VQA datatsets in the zero-shot evaluation setting. The dataset and source code will be released under https://github.com/lik1996/CRVQA2024.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉问答 (VQA) 是一项具有挑战性的任务，用于预测有关图像内容的问题的答案。它需要对文本问题和视觉图像都有深刻的理解。先前的工作通过简单地计算预测答案的准确性来直接评估答案模型。然而，在这样一个“黑匣子”系统中，预测背后的内在推理被忽视，我们甚至不知道预测是否可信。在某些情况下，即使模型关注不相关的视觉区域或文本标记，模型仍然可以获得正确的答案，这使得模型不可靠且不合逻辑。为了在给定图像/问题对的预测答案旁边生成视觉和文本基本原理，我们提出了 VQA、CRVQA 的令人信服的基本原理。考虑到新输出带来的额外注释，{CRVQA} 通过从一些现有 VQA 数据集及其视觉标签转换而来的样本进行训练和评估。大量的实验表明，视觉和文本的基本原理支持对答案的预测，并进一步提高了准确性。此外，{CRVQA} 在零样本评估设置中在通用 VQA 数据集上实现了具有竞争力的性能。数据集和源代码将在https://github.com/lik1996/CRVQA2024下发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.03896v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models**<br />
**Title_cn:** 视觉超对齐：视觉基础模型的弱到强泛化<br />
**Authors:** Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, Yunhe Wang<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型的最新进展引发了人们对其非凡且近乎超人的能力的兴趣，促使研究人员探索评估和优化这些能力的方法，这被称为超对齐。在此背景下，我们的论文深入研究了视觉基础模型领域，重点关注弱到强泛化的概念，即使用较弱的模型来监督较强的模型，旨在增强后者的能力，使其超越前者的限制。我们引入了一种新颖的、自适应可调的损失函数，用于弱到强的监督。我们的综合实验涵盖各种场景，包括小样本学习、迁移学习、噪声标签学习和公共知识蒸馏设置。结果是惊人的：我们的方法不仅超过了强对强泛化设定的性能基准，而且超过了使用整个数据集微调强模型的结果。这一令人信服的证据强调了从弱到强泛化的巨大潜力，展示了其大幅提升视觉基础模型性能的能力。该代码可在 https://github.com/ggjy/vision_weak_to_strong 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03749v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations**<br />
**Title_cn:** CogCoM：训练大型视觉语言模型，通过操作链深入细节<br />
**Authors:** Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at https://github.com/THUDM/CogCoM.</details>
**Abstract_cn:** <details><summary>译文: </summary>得益于将视觉指令与答案相结合的大量训练，视觉语言模型 (VLM) 已经证明了其广泛的可行性。然而，这种决定性的一致性导致模型忽略关键的视觉推理，并进一步导致细致的视觉问题和不忠实的反应的失败。在本文中，我们提出了操作链（Chain of Manipulations），这是一种使 VLM 能够通过一系列操作来解决问题的机制，其中每个操作是指对视觉输入的操作，或者来自通过先前训练获得的内在能力（例如，基础），或者来自模仿类人行为（例如放大）。这种机制鼓励 VLM 通过证据视觉推理生成忠实的响应，并允许用户在可解释的路径中追踪错误原因。因此，我们训练了 CogCoM，这是一种通用的 17B VLM，具有基于内存的兼容架构，赋予了这种推理机制。实验表明，我们的模型在 3 个类别的 8 个基准测试中实现了最先进的性能，并且使用有限数量的数据训练步骤即可快速获得有竞争力的性能。代码和数据可在 https://github.com/THUDM/CogCoM 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04236v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions**<br />
**Title_cn:** 针对情境感知交通规则异常的知情强化学习<br />
**Authors:** Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph, J. Marius Zöllner<br />
**Abstract:** <details><summary>原文: </summary>Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.</details>
**Abstract_cn:** <details><summary>译文: </summary>强化学习是一个高度活跃的研究领域，具有广阔的前景。然而，在自动驾驶领域，通常正在检查非常简单的场景。常见的方法使用不可解释的控制命令作为动作空间和缺乏结构的非结构化奖励设计。在这项工作中，我们引入了知情强化学习，其中结构化规则手册被集成为知识源。我们学习轨迹并通过情境感知奖励设计对其进行评估，从而产生动态奖励，使代理能够了解需要受控交通规则例外的情况。我们的方法适用于任意 RL 模型。我们使用最新的基于模型的代理成功地展示了复杂场景的高完成率。</details>
**PDF:** <http://arxiv.org/pdf/2402.04168v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction**<br />
**Title_cn:** 深度图像先验分析和利用自引导进行图像重建<br />
**Authors:** Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar<br />
**Abstract:** <details><summary>原文: </summary>The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度图像先验 (DIP) 从不完整或损坏的测量中恢复高质量图像的能力使其在图像恢复和医学成像（包括磁共振成像 (MRI)）的逆问题中很受欢迎。然而，传统的 DIP 遭受严重的过拟合和谱偏差效应。在这项工作中，我们首先通过分析不同架构的内核机制中底层网络的训练动态，分析 DIP 如何从欠采样成像测量中恢复信息。揭示了基于 DIP 的恢复的重要基本特性。当前的研究表明，与使用随机输入相比，将参考图像合并为网络输入可以增强 DIP 在图像重建中的性能。然而，获得合适的参考图像需要监督，并且带来了实际困难。为了克服这个障碍，我们进一步引入了一种自驱动的重建过程，该过程可以同时优化网络权重和输入，同时消除对训练数据的需求。我们的方法采用了一种新颖的降噪正则化项，可以对网络输入和重建图像进行鲁棒且稳定的联合估计。我们证明，我们的自引导方法在 MR 图像重建性能方面超越了原始 DIP 和现代监督方法，并且表现优于原始 DIP 和现代监督方法。之前基于 DIP 的图像修复方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.04097v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses**<br />
**Title_cn:** DNN 上的隐私泄露：模型反转攻击和防御的调查<br />
**Authors:** Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia<br />
**Abstract:** <details><summary>原文: </summary>Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models. These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this field and presents a holistic survey. Firstly, our work briefly reviews the traditional MI on machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on \textbf{D}eep \textbf{N}eural \textbf{N}etworks (DNNs) across multiple modalities and learning tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>模型反转 (MI) 攻击旨在通过滥用对预训练模型的访问来泄露有关训练数据的私人信息。这些攻击使对手能够重建与私人训练数据密切相关的高保真数据，这引起了严重的隐私问题。尽管该领域取得了快速进展，但我们缺乏对现有 MI 攻击和防御的全面概述。为了填补这一空白，本文深入研究了这一领域并提出了全面的调查。首先，我们的工作简要回顾了机器学习场景中的传统 MI。然后，我们精心分析和比较了跨多种模式和学习任务的对 \textbf{D}eep \textbf{N}eural \textbf{N} 网络（DNN）的大量最新攻击和防御。</details>
**PDF:** <http://arxiv.org/pdf/2402.04013v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MobileVLM V2: Faster and Stronger Baseline for Vision Language Model**<br />
**Title_cn:** MobileVLM V2：更快更强的视觉语言模型基线<br />
**Authors:** Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出了 MobileVLM V2，这是在 MobileVLM 上进行显着改进的一系列视觉语言模型，这证明新颖的架构设计的精心编排、针对移动 VLM 量身定制的改进训练方案以及丰富的高质量数据集管理可以极大地提高 VLM 的性能。具体而言，与 3B 规模的更大 VLM 相比，MobileVLM V2 1.7B 在标准 VLM 基准测试中实现了更好或同等的性能。值得注意的是，我们的 3B 模型在 7B+ 规模上优于多种 VLM。我们的模型将在 https://github.com/Meituan-AutoML/MobileVLM 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.03766v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **AoSRNet: All-in-One Scene Recovery Networks via Multi-knowledge Integration**<br />
**Title_cn:** AoSRNet：通过多知识集成的多合一场景恢复网络<br />
**Authors:** Yuxu Lu, Dong Yang, Yuan Gao, Ryan Wen Liu, Jun Liu, Yu Guo<br />
**Abstract:** <details><summary>原文: </summary>Scattering and attenuation of light in no-homogeneous imaging media or inconsistent light intensity will cause insufficient contrast and color distortion in the collected images, which limits the developments such as vision-driven smart urban, autonomous vehicles, and intelligent robots. In this paper, we propose an all-in-one scene recovery network via multi-knowledge integration (termed AoSRNet) to improve the visibility of imaging devices in typical low-visibility imaging scenes (e.g., haze, sand dust, and low light). It combines gamma correction (GC) and optimized linear stretching (OLS) to create the detail enhancement module (DEM) and color restoration module (CRM). Additionally, we suggest a multi-receptive field extraction module (MEM) to attenuate the loss of image texture details caused by GC nonlinear and OLS linear transformations. Finally, we refine the coarse features generated by DEM, CRM, and MEM through Encoder-Decoder to generate the final restored image. Comprehensive experimental results demonstrate the effectiveness and stability of AoSRNet compared to other state-of-the-art methods. The source code is available at \url{https://github.com/LouisYuxuLu/AoSRNet}.</details>
**Abstract_cn:** <details><summary>译文: </summary>非均匀成像介质中的光散射和衰减或光强度不一致会导致收集到的图像对比度不足和颜色失真，从而限制了视觉驱动的智慧城市、自动驾驶汽车和智能机器人等领域的发展。在本文中，我们提出了一种通过多知识集成的一体化场景恢复网络（称为 AoSRNet），以提高成像设备在典型低可见度成像场景（例如雾霾、沙尘和低光）中的可见度。它结合了伽玛校正（GC）和优化的线性拉伸（OLS）来创建细节增强模块（DEM）和色彩恢复模块（CRM）。此外，我们建议使用多感受野提取模块（MEM）来减少由 GC 非线性和 OLS 线性变换引起的图像纹理细节的损失。最后，我们通过Encoder-Decoder对DEM、CRM和MEM生成的粗略特征进行细化，生成最终的恢复图像。综合实验结果证明了 AoSRNet 与其他最先进方法相比的有效性和稳定性。源代码可在 \url{https://github.com/LouisYuxuLu/AoSRNet} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.03738v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution**<br />
**Title_cn:** FoolSDEdit：欺骗性地将您的编辑引向有针对性的属性感知分发<br />
**Authors:** Qi Zhou, Dongxia Wang, Tianlin Li, Zhihong Xu, Yang Liu, Kui Ren, Wenhai Wang, Qing Guo<br />
**Abstract:** <details><summary>原文: </summary>Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings. However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images. This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns. For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit. To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics. We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting. Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes. To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together. We optimize the execution strategy of various perturbations, framing it as a network architecture search problem. We create the SuperPert, a graph representing diverse execution strategies for different perturbations. After training, we obtain the optimized execution strategy for effective TAGA against SDEdit. Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>引导图像合成方法（例如基于扩散模型的 SDEdit）擅长根据用户输入（例如笔画）创建逼真的图像。然而，现有的努力主要集中在图像质量上，往往忽略了一个关键点：扩散模型代表数据分布，而不是单个图像。这导致生成与用户意图相矛盾的图像的可能性很低，但很关键，从而引起道德问题。例如，用户输入具有女性特征的笔画可能有一定概率从 SDEdit 中获得男性面孔。为了暴露这个潜在的漏洞，我们的目标是构建一个对抗性攻击，迫使 SDEdit 生成与指定属性（例如女性）一致的特定数据分布，而不改变输入的属性特征。我们提出了目标属性生成攻击（TAGA），使用属性感知目标函数并优化添加到输入笔划绘画中的对抗性噪声。实证研究表明，传统的对抗性噪声与 TAGA 相抗衡，而曝光和运动模糊等自然扰动很容易改变生成图像的属性。为了执行有效的攻击，我们引入了 FoolSDEdit：我们设计了联合对抗性曝光和模糊攻击，将曝光和运动模糊添加到笔画中并一起优化它们。我们优化各种扰动的执行策略，将其视为网络架构搜索问题。我们创建了 SuperPert，这是一个代表不同扰动的不同执行策略的图表。经过训练，我们获得了针对 SDEdit 有效 TAGA 的优化执行策略。对两个数据集的综合实验表明，我们的方法迫使 SDEdit 生成目标属性感知数据分布，其性能显着优于基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.03705v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning**<br />
**Title_cn:** QuEST：通过高效选择性微调进行低位扩散模型量化<br />
**Authors:** Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在图像生成任务中取得了显着的成功，但其实际部署受到高内存和时间消耗的限制。虽然量化为扩散模型压缩和加速铺平了道路，但当模型量化到低位时，现有方法完全失败。在本文中，我们揭示了量化扩散模型中损害当前方法有效性的三个属性：不平衡的激活分布、不精确的时间信息以及易受特定模块扰动的影响。为了缓解由于分布不平衡而加剧的低位量化难度，我们建议对量化模型进行微调以更好地适应激活分布。基于这个想法，我们确定了两种关键类型的量化层：保存重要时间信息的层和对减少位宽敏感的层，并对它们进行微调以有效减轻性能下降。我们凭经验验证我们的方法可以修改激活分布并提供有意义的时间信息，从而促进更容易、更准确的量化。我们的方法通过三个高分辨率图像生成任务进行了评估，并在各种位宽设置下实现了最先进的性能，并且是第一个在全 4 位（即 W4A4）上生成可读图像的方法（稳定）扩散。</details>
**PDF:** <http://arxiv.org/pdf/2402.03666v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Reviewing FID and SID Metrics on Generative Adversarial Networks**<br />
**Title_cn:** 审查生成对抗网络的 FID 和 SID 指标<br />
**Authors:** Ricardo de Deijn, Aishwarya Batra, Brandon Koch, Naseef Mansoor, Hema Makkena<br />
**Abstract:** <details><summary>原文: </summary>The growth of generative adversarial network (GAN) models has increased the ability of image processing and provides numerous industries with the technology to produce realistic image transformations. However, with the field being recently established there are new evaluation metrics that can further this research. Previous research has shown the Fr\'echet Inception Distance (FID) to be an effective metric when testing these image-to-image GANs in real-world applications. Signed Inception Distance (SID), a founded metric in 2023, expands on FID by allowing unsigned distances. This paper uses public datasets that consist of fa\c{c}ades, cityscapes, and maps within Pix2Pix and CycleGAN models. After training these models are evaluated on both inception distance metrics which measure the generating performance of the trained models. Our findings indicate that usage of the metric SID incorporates an efficient and effective metric to complement, or even exceed the ability shown using the FID for the image-to-image GANs</details>
**Abstract_cn:** <details><summary>译文: </summary>生成对抗网络（GAN）模型的发展提高了图像处理的能力，并为众多行业提供了产生逼真图像转换的技术。然而，随着该领域最近的建立，新的评估指标可以进一步推进这项研究。先前的研究表明，Fr'echet 起始距离 (FID) 在实际应用中测试这些图像到图像 GAN 时是一个有效的指标。有符号起始距离 (SID) 是 2023 年建立的指标，通过允许无符号距离来扩展 FID。本文使用的公共数据集包括 Pix2Pix 和 CycleGAN 模型中的立面、城市景观和地图。训练后，根据两个初始距离指标对这些模型进行评估，这些指标衡量训练模型的生成性能。我们的研究结果表明，度量 SID 的使用包含了一种高效且有效的度量，可以补充甚至超过使用 FID 进行图像到图像 GAN 所显示的能力</details>
**PDF:** <http://arxiv.org/pdf/2402.03654v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation**<br />
**Title_cn:** GRASP：图结构金字塔整体幻灯片图像表示<br />
**Authors:** Ali Khajegili Mirabadi, Graham Archibald, Amirali Darbandsari, Alberto Contreras-Sanz, Ramin Ebrahim Nakhli, Maryam Asadi, Allen Zhang, C. Blake Gilks, Peter Black, Gang Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Cancer subtyping is one of the most challenging tasks in digital pathology, where Multiple Instance Learning (MIL) by processing gigapixel whole slide images (WSIs) has been in the spotlight of recent research. However, MIL approaches do not take advantage of inter- and intra-magnification information contained in WSIs. In this work, we present GRASP, a novel graph-structured multi-magnification framework for processing WSIs in digital pathology. Our approach is designed to dynamically emulate the pathologist's behavior in handling WSIs and benefits from the hierarchical structure of WSIs. GRASP, which introduces a convergence-based node aggregation instead of traditional pooling mechanisms, outperforms state-of-the-art methods over two distinct cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times smaller than the closest-performing state-of-the-art model in terms of the number of parameters. Our results show that GRASP is dynamic in finding and consulting with different magnifications for subtyping cancers and is reliable and stable across different hyperparameters. The model's behavior has been evaluated by two expert pathologists confirming the interpretability of the model's dynamic. We also provide a theoretical foundation, along with empirical evidence, for our work, explaining how GRASP interacts with different magnifications and nodes in the graph to make predictions. We believe that the strong characteristics yet simple structure of GRASP will encourage the development of interpretable, structure-based designs for WSI representation in digital pathology. Furthermore, we publish two large graph datasets of rare Ovarian and Bladder cancers to contribute to the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>癌症亚型分型是数字病理学中最具挑战性的任务之一，其中通过处理十亿像素全幻灯片图像 (WSI) 的多实例学习 (MIL) 一直是最近研究的焦点。然而，MIL 方法没有利用 WSI 中包含的内部和内部放大信息。在这项工作中，我们提出了 GRASP，一种新颖的图结构多放大框架，用于处理数字病理学中的 WSI。我们的方法旨在动态模拟病理学家在处理 WSI 时的行为，并从 WSI 的层次结构中受益。 GRASP 引入了基于收敛的节点聚合，而不是传统的池化机制，在两个不同的癌症数据集上的性能优于最先进的方法，平衡精度高达 10%，同时比最接近的方法小 7 倍。在参数数量方面执行最先进的模型。我们的结果表明，GRASP 在寻找和咨询不同放大倍数的癌症亚型方面是动态的，并且在不同的超参数下都是可靠和稳定的。该模型的行为已由两位病理学家专家进行了评估，确认了模型动态的可解释性。我们还为我们的工作提供了理论基础和经验证据，解释了 GRASP 如何与图中的不同放大倍数和节点交互来做出预测。我们相信，GRASP 强大的特性和简单的结构将鼓励开发可解释的、基于结构的设计，用于数字病理学中的 WSI 表示。此外，我们还发布了两个罕见卵巢癌和膀胱癌的大型图形数据集，为该领域做出贡献。</details>
**PDF:** <http://arxiv.org/pdf/2402.03592v1><br />
**Code:** null<br />

