## [UPDATED!] **2024-02-07** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **SPAD : Spatially Aware Multiview Diffusers**<br />
**Title_cn:** SPAD：空间感知多视图扩散器<br />
**Authors:** Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin<br />
**Abstract:** <details><summary>原文: </summary>We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: https://yashkant.github.io/spad</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 SPAD，这是一种从文本提示或单个图像创建一致的多视图图像的新颖方法。为了实现多视图生成，我们通过跨视图交互扩展其自注意力层来重新利用预训练的 2D 扩散模型，并在 Objaverse 的高质量子集上对其进行微调。我们发现，先前工作（例如 MVDream）中提出的自注意力的天真扩展会导致视图之间的内容复制。因此，我们明确地约束基于极几何的交叉视图注意力。为了进一步增强 3D 一致性，我们利用从相机光线导出的 Plucker 坐标并将它们作为位置编码注入。这使得 SPAD 能够很好地推理 3D 空间邻近度。与最近只能在固定方位角和仰角生成视图的作品相比，SPAD 提供了完整的相机控制，并在 Objaverse 和 Google Scanned Objects 数据集中未见过的物体上实现了最先进的视图合成结果。最后，我们证明使用 SPAD 生成文本到 3D 可以防止多面 Janus 问题。请访问我们的网页查看更多详细信息：https://yashkant.github.io/spad</details>
**PDF:** <http://arxiv.org/pdf/2402.05235v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models**<br />
**Title_cn:** 利用分割引导扩散模型生成解剖学可控的医学图像<br />
**Authors:** Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over state-of-the-art models. We also offer an accessible codebase and release a dataset of generated paired breast MRIs. Our approach facilitates diverse applications, including pre-registered image generation, counterfactual scenarios, and others.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型已经实现了非常高质量的医学图像生成，这可以通过补充小型或不平衡的数据集以及其他应用程序来帮助减少获取和注释新图像的费用。然而，这些都受到在生成的图像中执行全局解剖真实性的挑战的阻碍。为此，我们提出了一种用于解剖控制医学图像生成的扩散模型。我们的模型在每个采样步骤都遵循多类解剖分割掩模，并结合了 \textit{随机掩模消融} 训练算法，以能够对选定的解剖约束组合进行调节，同时允许其他解剖区域的灵活性。这也提高了网络对完全无条件（无约束生成）情况的解剖真实性的学习。对乳房 MRI 和腹部/颈部到骨盆 CT 数据集的比较评估表明，与最先进的模型相比，具有卓越的解剖真实性和输入掩模忠实度。我们还提供了一个可访问的代码库，并发布了生成的配对乳房 MRI 数据集。我们的方法促进了多种应用，包括预注册图像生成、反事实场景等。</details>
**PDF:** <http://arxiv.org/pdf/2402.05210v1><br />
**Code:** <https://github.com/mazurowski-lab/segmentation-guided-diffusion>**<br />
>>**index:** 3<br />
**Title:** **$λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space**<br />
**Title_cn:** $λ$-ECLIPSE：利用 CLIP 潜在空间的多概念个性化文本到图像扩散模型<br />
**Authors:** Maitreya Patel, Sangmin Jung, Chitta Baral, Yezhou Yang<br />
**Abstract:** <details><summary>原文: </summary>Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffusion text-to-image priors. Building on this, we introduce $\lambda$-ECLIPSE. Our method illustrates that effective P-T2I does not necessarily depend on the latent space of diffusion models. $\lambda$-ECLIPSE achieves single, multi-subject, and edge-guided T2I personalization with just 34M parameters and is trained on a mere 74 GPU hours using 1.6M image-text interleaved data. Through extensive experiments, we also establish that $\lambda$-ECLIPSE surpasses existing baselines in composition alignment while preserving concept alignment performance, even with significantly lower resource utilization.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管个性化文本到图像 (P-T2I) 生成模型最近取得了进展，但主题驱动的 T2I 仍然具有挑战性。主要瓶颈包括 1) 密集的培训资源需求，2) 超参数敏感性导致输出不一致，以及 3) 平衡新颖的视觉概念和构图对齐的复杂性。我们首先重申 T2I 扩散模型的核心理念，以解决上述局限性。当代主题驱动的 T2I 方法主要依赖于潜在扩散模型 (LDM)，该模型通过交叉注意层促进 T2I 映射。虽然 LDM 具有明显的优势，但 P-T2I 方法对这些扩散模型的潜在空间的依赖显着增加了资源需求，导致结果不一致，并且需要对单个所需图像进行多次迭代。最近，ECLIPSE 展示了一种更资源有效的途径来训练基于 UnCLIP 的 T2I 模型，避免了扩散文本到图像先验的需要。在此基础上，我们引入了 $\lambda$-ECLIPSE。我们的方法说明有效的 P-T2I 不一定依赖于扩散模型的潜在空间。 $\lambda$-ECLIPSE 只需 34M 个参数即可实现单主体、多主体和边缘引导的 T2I 个性化，并使用 160 万图像文本交错数据在仅 74 个 GPU 小时上进行训练。通过大量的实验，我们还确定 $\lambda$-ECLIPSE 在组合对齐方面超越了现有基线，同时保留了概念对齐性能，即使资源利用率显着降低。</details>
**PDF:** <http://arxiv.org/pdf/2402.05195v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation**<br />
**Title_cn:** LGM：用于高分辨率 3D 内容创建的大型多视图高斯模型<br />
**Authors:** Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu<br />
**Abstract:** <details><summary>原文: </summary>3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D内容创作在质量和速度方面都取得了显着进步。尽管当前的前馈模型可以在几秒钟内生成 3D 对象，但其分辨率受到训练期间所需的密集计算的限制。在本文中，我们介绍了大型多视图高斯模型 (LGM)，这是一种新颖的框架，旨在从文本提示或单视图图像生成高分辨率 3D 模型。我们的主要见解有两个：1）3D 表示：我们提出多视图高斯特征作为一种高效而强大的表示，然后可以将其融合在一起以进行可微渲染。 2）3D Backbone：我们提出了一个非对称 U-Net 作为在多视图图像上运行的高吞吐量主干，它可以通过利用多视图扩散模型从文本或单视图图像输入生成。大量的实验证明了我们的方法的高保真度和效率。值得注意的是，我们保持了 5 秒内生成 3D 对象的快速速度，同时将训练分辨率提高到 512，从而实现了高分辨率 3D 内容生成。</details>
**PDF:** <http://arxiv.org/pdf/2402.05054v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Blue noise for diffusion models**<br />
**Title_cn:** 扩散模型的蓝噪声<br />
**Authors:** Xingchang Huang, Corentin Salaün, Cristina Vasconcelos, Christian Theobalt, Cengiz Öztireli, Gurprit Singh<br />
**Abstract:** <details><summary>原文: </summary>Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数现有的扩散模型使用高斯噪声在所有时间步长上进行训练和采样，这可能无法最佳地解释去噪网络重建的频率内容。尽管相关噪声在计算机图形学中的应用多种多样，但其改进训练过程的潜力尚未得到充分开发。在本文中，我们介绍了一类新颖且通用的扩散模型，考虑了图像内部和图像之间的相关噪声。更具体地说，我们提出了一种时变噪声模型，将相关噪声纳入训练过程，以及一种快速生成相关噪声掩模的方法。我们的模型建立在确定性扩散模型的基础上，与仅使用高斯白（随机）噪声相比，利用蓝噪声来帮助提高生成质量。此外，我们的框架允许在单个小批量内引入跨图像的相关性，以改善梯度流。我们使用我们的方法对各种数据集进行定性和定量评估，在 FID 指标方面实现了对现有确定性扩散模型的不同任务的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.04930v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation**<br />
**Title_cn:** 通过扩散引导源数据生成进行无源域适应<br />
**Authors:** Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种利用无源域适应扩散模型 (DM-SFDA) 的泛化能力的新方法。我们提出的 DM-SFDA 方法涉及微调预训练的文本到图像扩散模型，以使用目标图像的特征生成源域图像来指导扩散过程。具体来说，对预训练的扩散模型进行微调，以生成源样本，从而最小化熵并最大化预训练源模型的置信度。然后，我们应用已建立的无监督域适应技术将生成的源图像与目标域数据对齐。我们通过一系列数据集（包括 Office-31、Office-Home 和 VisDA）的综合实验来验证我们的方法。结果突显了 SFDA 性能的显着改进，展示了扩散模型在生成上下文相关的特定领域图像方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.04929v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints**<br />
**Title_cn:** 通过具有审美约束的扩散模型实现对齐布局生成<br />
**Authors:** Jian Chen, Ruiyi Zhang, Yufan Zhou, Changyou Chen<br />
**Abstract:** <details><summary>原文: </summary>Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>可控布局生成是指在图形设计（例如文档和网页设计）中创建合理的元素视觉排列的过程，并具有代表设计意图的约束。尽管最近基于扩散的模型已经取得了最先进的 FID 分数，但与早期基于变压器的模型相比，它们往往表现出更明显的错位。在这项工作中，我们提出了 $\textbf{LA}$yout $\textbf{C}$onstraint 扩散 mod$\textbf{E}$l (LACE)，一个处理广泛布局生成任务的统一模型，例如排列具有指定属性的元素并细化或完成粗略的布局设计。该模型基于连续扩散模型。与使用离散扩散模型的现有方法相比，连续状态空间设计可以在训练中结合可微的美学约束函数。对于条件生成，我们通过屏蔽输入引入条件。大量的实验结果表明，LACE 可以生成高质量的布局，并且性能优于现有的最先进的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.04754v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Cortical Surface Diffusion Generative Models**<br />
**Title_cn:** 皮质表面扩散生成模型<br />
**Authors:** Zhenshan Xie, Simon Dahan, Logan Z. J. Williams, M. Jorge Cardoso, Emma C. Robinson<br />
**Abstract:** <details><summary>原文: </summary>Cortical surface analysis has gained increased prominence, given its potential implications for neurological and developmental disorders. Traditional vision diffusion models, while effective in generating natural images, present limitations in capturing intricate development patterns in neuroimaging due to limited datasets. This is particularly true for generating cortical surfaces where individual variability in cortical morphology is high, leading to an urgent need for better methods to model brain development and diverse variability inherent across different individuals. In this work, we proposed a novel diffusion model for the generation of cortical surface metrics, using modified surface vision transformers as the principal architecture. We validate our method in the developing Human Connectome Project (dHCP), the results suggest our model demonstrates superior performance in capturing the intricate details of evolving cortical surfaces. Furthermore, our model can generate high-quality realistic samples of cortical surfaces conditioned on postmenstrual age(PMA) at scan.</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于其对神经系统和发育障碍的潜在影响，皮质表面分析越来越受到重视。传统的视觉扩散模型虽然可以有效地生成自然图像，但由于数据集有限，在捕获神经成像中复杂的发育模式方面存在局限性。对于生成皮质表面尤其如此，其中皮质形态的个体变异性很高，导致迫切需要更好的方法来模拟大脑发育和不同个体固有的多样性变异性。在这项工作中，我们提出了一种新颖的扩散模型，用于生成皮质表面指标，使用改进的表面视觉变换器作为主要架构。我们在正在开发的人类连接组项目（dHCP）中验证了我们的方法，结果表明我们的模型在捕获不断变化的皮质表面的复杂细节方面表现出卓越的性能。此外，我们的模型可以在扫描时根据经后年龄（PMA）生成高质量的真实皮质表面样本。</details>
**PDF:** <http://arxiv.org/pdf/2402.04753v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions**<br />
**Title_cn:** EvoSeed：用现实世界的幻觉揭示深度神经网络的威胁<br />
**Authors:** Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \url{https://shashankkotyan.github.io/EvoSeed}.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络是使用自然对抗样本来开发的，这些样本对人类感知没有影响，但会被错误分类。当前的方法通常依赖于深度神经网络的白盒性质来生成这些对抗性样本或与训练分布相比改变对抗性样本的分布。为了减轻当前方法的局限性，我们提出了 EvoSeed，一种新颖的基于进化策略的搜索算法框架，用于生成自然对抗样本。我们的 EvoSeed 框架使用辅助扩散和分类器模型在与模型无关的黑盒设置中运行。我们使用 CMA-ES 来优化对抗性种子向量的搜索，当通过条件扩散模型处理时，会导致分类器模型错误分类的不受限制的自然对抗性样本。实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法展示了使用进化算法提高对抗样本质量的前景。我们希望我们的研究能够开辟新的途径，以增强深度神经网络在现实场景中的鲁棒性。项目网站可以通过 \url{https://shashankkotyan.github.io/EvoSeed} 访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.04699v1><br />
**Code:** <https://github.com/shashankkotyan/EvoSeed>**<br />
>>**index:** 10<br />
**Title:** **Noise Map Guidance: Inversion with Spatial Context for Real Image Editing**<br />
**Title_cn:** 噪声图指导：使用空间上下文进行反演以进行真实图像编辑<br />
**Authors:** Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Tae-Hyun Oh, Yonghyun Jeong<br />
**Abstract:** <details><summary>原文: </summary>Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity. Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization. Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing. Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality. Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本引导的扩散模型已成为图像合成中的流行工具，以生成高质量和多样化的图像而闻名。然而，它们在编辑真实图像时经常遇到障碍，主要是由于文本条件恶化了重建质量并随后影响了编辑保真度。空文本反演（NTI）在这一领域取得了长足的进步，但它无法捕获空间上下文，并且需要计算密集型的每个时间步优化。为了解决这些挑战，我们提出了噪声图引导（NMG），这是一种富含空间背景的反演方法，专为真实图像编辑而定制。值得注意的是，NMG 无需优化即可实现这一目标，同时保留了编辑质量。我们的实证研究强调了 NMG 对各种编辑技术的适应性及其对 DDIM 反转变体的稳健性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04625v1><br />
**Code:** <https://github.com/hansam95/nmg>**<br />
>>**index:** 11<br />
**Title:** **Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy**<br />
**Title_cn:** 用于放射治疗剂量预测的多尺度细化三重态约束变压器<br />
**Authors:** Lu Wen, Qihun Zhang, Zhenghao Feng, Yuanyuan Xu, Xiao Chen, Jiliu Zhou, Yan Wang<br />
**Abstract:** <details><summary>原文: </summary>Radiotherapy is a primary treatment for cancers with the aim of applying sufficient radiation dose to the planning target volume (PTV) while minimizing dose hazards to the organs at risk (OARs). Convolutional neural networks (CNNs) have automated the radiotherapy plan-making by predicting the dose maps. However, current CNN-based methods ignore the remarkable dose difference in the dose map, i.e., high dose value in the interior PTV while low value in the exterior PTV, leading to a suboptimal prediction. In this paper, we propose a triplet-constraint transformer (TCtrans) with multi-scale refinement to predict the high-quality dose distribution. Concretely, a novel PTV-guided triplet constraint is designed to refine dose feature representations in the interior and exterior PTV by utilizing the explicit geometry of PTV. Furthermore, we introduce a multi-scale refinement (MSR) module to effectively fulfill the triplet constraint in different decoding layers with multiple scales. Besides, a transformer encoder is devised to learn the important global dosimetric knowledge. Experiments on a clinical cervical cancer dataset demonstrate the superiority of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>放射治疗是癌症的主要治疗方法，目的是对计划靶区 (PTV) 施加足够的放射剂量，同时最大限度地减少对危及器官 (OAR) 的剂量危害。卷积神经网络 (CNN) 通过预测剂量图来自动化放射治疗计划的制定。然而，当前基于 CNN 的方法忽略了剂量图中显着的剂量差异，即内部 PTV 的剂量值较高，而外部 PTV 的剂量值较低，从而导致预测不理想。在本文中，我们提出了一种具有多尺度细化的三重态约束变压器（TCtrans）来预测高质量的剂量分布。具体而言，设计了一种新颖的 PTV 引导三重态约束，通过利用 PTV 的显式几何结构来细化内部和外部 PTV 中的剂量特征表示。此外，我们引入了多尺度细化（MSR）模块，以有效地满足具有多个尺度的不同解码层中的三元组约束。此外，设计了变压器编码器来学习重要的全局剂量学知识。对临床宫颈癌数据集的实验证明了我们方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04566v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **BRI3L: A Brightness Illusion Image Dataset for Identification and Localization of Regions of Illusory Perception**<br />
**Title_cn:** BRI3L：用于识别和定位幻觉感知区域的亮度幻觉图像数据集<br />
**Authors:** Aniket Roy, Anirban Roy, Soma Mitra, Kuntal Ghosh<br />
**Abstract:** <details><summary>原文: </summary>Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: https://github.com/aniket004/BRI3L</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉错觉在理解视觉感知方面发挥着重要作用。目前理解和评估视错觉的方法大多是基于确定性过滤的方法，它们对少数视错觉进行评估，因此得出的结论并不通用。为此，我们生成了包含五种亮度错觉的 22,366 张图像的大型数据集（BRI3L：用于识别和定位错觉感知的亮度错觉图像数据集），并使用基于数据驱动的神经网络的方法对数据集进行基准测试。数据集包含标签信息 - (1) 特定图像是否是虚幻的/非虚幻的，(2) 图像虚幻区域的分割掩模。因此，可以使用该数据集来评估分类和分割任务。我们遵循涉及人类受试者的标准心理物理学实验来验证数据集。据我们所知，这是首次尝试使用数据驱动的方法来开发视觉错觉数据集和基准来进行错觉分类和定位。我们考虑五种经过充分研究的亮度错觉类型：1）赫尔曼网格，2）同时亮度对比度，3）白色错觉，4）网格错觉和5）诱导光栅错觉。对数据集进行基准测试，错觉识别准确率达到 99.56%，错觉定位像素准确率达到 84.37%。研究表明，深度学习模型的应用也概括了看不见的亮度错觉，例如亮度同化到对比度过渡。我们还测试了最先进的扩散模型产生亮度错觉的能力。我们在 github 存储库中提供了所有代码、数据集、说明等：https://github.com/aniket004/BRI3L</details>
**PDF:** <http://arxiv.org/pdf/2402.04541v1><br />
**Code:** <https://github.com/aniket004/bri3l>**<br />
>>**index:** 13<br />
**Title:** **Text2Street: Controllable Text-to-image Generation for Street Views**<br />
**Title_cn:** Text2Street：街景的可控文本到图像生成<br />
**Authors:** Jinming Su, Songen Gu, Yiting Duan, Xingyue Chen, Junfeng Luo<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image generation has made remarkable progress with the emergence of diffusion models. However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional text-to-image models difficult to deal with. To address these challenges, we propose a novel controllable text-to-image framework, named \textbf{Text2Street}. In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation. Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation. Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation. Extensive experiments show that the proposed approach achieves controllable street-view text-to-image generation and validates the effectiveness of the Text2Street framework for street views.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着扩散模型的出现，文本到图像的生成取得了显着的进步。然而，基于文本生成街景图像仍然是一项艰巨的任务，主要是因为街景的道路拓扑复杂，交通状况多样，天气条件多样，​​这使得传统的文本到图像模型很难对付。为了应对这些挑战，我们提出了一种新颖的可控文本到图像框架，名为 \textbf{Text2Street}。在该框架中，我们首先引入了车道感知的道路拓扑生成器，它通过配备计数适配器的准确的道路结构和车道线实现文本到地图的生成，实现可控的道路拓扑生成。然后，提出基于位置的对象布局生成器，通过对象级边界框扩散策略获得文本到布局的生成，实现可控交通对象布局生成。最后，设计了多控制图像生成器，集成道路拓扑、物体布局和天气描述，实现可控街景图像生成。大量实验表明，所提出的方法实现了可控街景文本到图像的生成，并验证了 Text2Street 街景框架的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04504v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs**<br />
**Title_cn:** BIKED++：包含 140 万张自行车图像和参数化 CAD 设计的多模式数据集<br />
**Authors:** Lyle Regenwetter, Yazan Abu Obaideh, Amin Heyrani Nobari, Faez Ahmed<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family which includes thousands of mixed-representation human-designed bicycle models and several datasets quantifying design performance. The code and dataset can be found at: https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一个包含 140 万个程序生成的自行车设计的公共数据集，这些自行车设计以参数化方式表示为 JSON 文件和光栅化图像。该数据集是通过使用渲染引擎创建的，该渲染引擎利用 BikeCAD 软件从参数化设计生成矢量图形。该渲染引擎在论文中进行了讨论，并与数据集一起公开发布。尽管该数据集有许多应用，但主要动机是需要在参数化和基于图像的设计表示之间训练跨模式预测模型。例如，我们证明可以训练预测模型来直接根据参数表示准确估计对比语言图像预训练（CLIP）嵌入。这允许在参数自行车设计和文本字符串或参考图像之间建立相似关系。经过训练的预测模型也被公开。该数据集加入了 BIKED 数据集系列，其中包括数千个混合表示的人类设计的自行车模型和多个量化设计性能的数据集。代码和数据集可以在以下位置找到：https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main</details>
**PDF:** <http://arxiv.org/pdf/2402.05301v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection**<br />
**Title_cn:** 检查医学视觉和基于语言的疾病检测的多模态联合学习中的模态不一致性<br />
**Authors:** Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly, we assess the capability of client-level and server-level regularization techniques towards mitigating modality incongruity effects. Experiments are conducted under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology reports.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态联邦学习 (MMFL) 在每个客户端中利用多种模式来构建比单模态更强大的联邦学习 (FL) 模型。然而，不同客户缺失模态的影响，也称为模态不一致，却被极大地忽视了。本文首次分析了模态不一致的影响，并揭示了其与参与客户之间数据异构性的联系。我们特别检查与单模态和多模态客户不一致的 MMFL 是否比单模态 FL 更有利。此外，我们研究了解决该问题的三种潜在途径。首先，我们研究了 MMFL 中各种自注意力机制对不一致性不可知信息融合的有效性。其次，我们引入了在多模态客户端中预先训练的模态插补网络（MIN），用于单模态客户端中的模态翻译，并研究其缓解模态缺失问题的潜力。第三，我们评估客户端级和服务器级正则化技术减轻模态不一致影响的能力。实验是在多种 MMFL 设置下对两个公开可用的真实世界数据集 MIMIC-CXR 和 Open-I 进行的，并附有胸部 X 光和放射学报告。</details>
**PDF:** <http://arxiv.org/pdf/2402.05294v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation**<br />
**Title_cn:** 基于语言的增强解决对象目标导航中的快捷学习问题<br />
**Authors:** Dennis Hoftijzer, Gertjan Burghouts, Luuk Spreeuwers<br />
**Abstract:** <details><summary>原文: </summary>Deep Reinforcement Learning (DRL) has shown great potential in enabling robots to find certain objects (e.g., `find a fridge') in environments like homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL methods are predominantly trained and evaluated using environment simulators. Although DRL has shown impressive results, the simulators may be biased or limited. This creates a risk of shortcut learning, i.e., learning a policy tailored to specific visual details of training environments. We aim to deepen our understanding of shortcut learning in ObjectNav, its implications and propose a solution. We design an experiment for inserting a shortcut bias in the appearance of training environments. As a proof-of-concept, we associate room types to specific wall colors (e.g., bedrooms with green walls), and observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to environments where this is not the case (e.g., bedrooms with blue walls). We find that shortcut learning is the root cause: the agent learns to navigate to target objects, by simply searching for the associated wall color of the target object's room. To solve this, we propose Language-Based (L-B) augmentation. Our key insight is that we can leverage the multimodal feature space of a Vision-Language Model (VLM) to augment visual representations directly at the feature-level, requiring no changes to the simulator, and only an addition of one layer to the model. Where the SOTA ObjectNav method's success rate drops 69%, our proposal has only a drop of 23%.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度强化学习 (DRL) 在使机器人能够在家庭或学校等环境中找到某些物体（例如“找到冰箱”）方面显示出巨大的潜力。此任务称为对象目标导航 (ObjectNav)。 DRL 方法主要使用环境模拟器进行训练和评估。尽管 DRL 已显示出令人印象深刻的结果，但模拟器可能存在偏差或有限。这产生了捷径学习的风险，即学习针对训练环境的特定视觉细节量身定制的策略。我们的目标是加深对 ObjectNav 快捷学习及其含义的理解，并提出解决方案。我们设计了一个实验，在训练环境的外观中插入捷径偏差。作为概念验证，我们将房间类型与特定的墙壁颜色（例如，带有绿色墙壁的卧室）相关联，并观察到最先进的 (SOTA) ObjectNav 方法在不适合的环境中的泛化能力较差。案例（例如，蓝色墙壁的卧室）。我们发现快捷学习是根本原因：代理通过简单地搜索目标对象房间的相关墙壁颜色来学习导航到目标对象。为了解决这个问题，我们提出了基于语言（L-B）的增强。我们的主要见解是，我们可以利用视觉语言模型 (VLM) 的多模态特征空间直接在特征级别增强视觉表示，无需对模拟器进行任何更改，只需向模型添加一层。 SOTA ObjectNav 方法的成功率下降了 69%，而我们的提案仅下降了 23%。</details>
**PDF:** <http://arxiv.org/pdf/2402.05090v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty**<br />
**Title_cn:** 具有标签不确定性的遥感数据的高效多分辨率融合<br />
**Authors:** Hersh Vakharia, Xiaoxiao Du<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection. This paper presents a new method for fusing multi-modal and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources. We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework. We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态传感器数据融合利用每个传感器的补充或增强信息，可以提高场景分类和目标检测等应用的整体性能。本文提出了一种融合多模态和多分辨率遥感器数据的新方法，无需像素级训练标签，而像素级训练标签很难获得。之前，我们开发了一个多实例多分辨率融合（MIMRF）框架，用于解决融合的标签不确定性，但由于用于集成传感器数据源的模糊度量的搜索空间很大，因此训练速度可能很慢。我们提出了一种基于二元模糊测量的新方法，它减少了搜索空间并显着提高了 MIMRF 框架的效率。我们展示了合成数据和真实世界遥感检测任务的实验结果，并表明所提出的 MIMRF-BFM 算法可以在给定不确定性的遥感数据的情况下有效且高效地执行多分辨率融合。</details>
**PDF:** <http://arxiv.org/pdf/2402.05045v1><br />
**Code:** <https://github.com/hvak/mimrf-bfm>**<br />
>>**index:** 5<br />
**Title:** **Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?**<br />
**Title_cn:** 文字还是图像？仇恨模因检测模型的跨域泛化能力更重要的是什么？<br />
**Authors:** Piush Aggarwal, Jawar Mehrabanian, Weigang Huang, Özge Alacam, Torsten Zesch<br />
**Abstract:** <details><summary>原文: </summary>This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\Delta$F1 of 0.18.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文深入研究了多模式仇恨模因检测中跨领域泛化的艰巨挑战，提出了令人信服的发现。我们提供了足够的证据来支持这样的假设：只有可恶模因的文本成分才能使现有的多模态分类器能够跨不同领域进行泛化，而图像成分被证明对特定的训练数据集高度敏感。证据包括演示，表明仇恨文本分类器在零样本设置中的表现与仇恨模因分类器类似。同时，将模因图像生成的字幕引入仇恨模因分类器会使性能恶化，平均 F1 为 0.02。通过黑盒解释，我们确定了文本模态的显着贡献（平均为 83%），随着模因图像标题的引入（52%），文本模态的贡献逐渐减弱。此外，我们对新创建的混杂因素数据集的评估显示，与平均 $\Delta$F1 为 0.18 的图像混杂因素相比，文本混杂因素的性能更高。</details>
**PDF:** <http://arxiv.org/pdf/2402.04967v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark**<br />
**Title_cn:** MLLM 作为法官：使用视觉语言基准评估多模式 LLM 作为法官<br />
**Authors:** Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts regarding MLLMs as fully reliable evaluators. Code and dataset are available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）最近受到了广泛关注，在通用人工智能领域显示出巨大的潜力。然而，评估 MLLM 的效用面临着相当大的挑战，这主要是由于缺乏符合人类偏好的多模式基准。受法学硕士中的“LLM-as-a-Judge”的启发，本文引入了一种新颖的基准，称为“MLLM-as-a-Judge”，用于评估 MLLM 协助法官的能力，包括三个不同的任务：评分评估、配对比较和批量排行。我们的研究表明，虽然 MLLM 在配对比较中表现出显着的类似人类的辨别能力，但在评分评估和批次排名任务中与人类偏好存在显着差异。此外，即使对于 GPT-4V 等先进模型，MLLM 仍然面临判断挑战，包括多样化的偏见、幻觉反应和不一致。这些发现强调了将 MLLM 作为完全可靠的评估器进行增强和进一步研究工作的迫切需要。代码和数据集可在 https://github.com/Dongping-Chen/MLLM-as-a-Judge 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04788v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior**<br />
**Title_cn:** InstructScene：具有语义图先验的指令驱动 3D 室内场景合成<br />
**Authors:** Chenguo Lin, Yadong Mu<br />
**Abstract:** <details><summary>原文: </summary>Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: https://chenguolin.github.io/projects/InstructScene.</details>
**Abstract_cn:** <details><summary>译文: </summary>理解自然语言指令是 3D 室内场景合成系统的一个迷人特性。现有方法直接对对象关节分布进行建模，并在场景内隐式表达对象关系，从而阻碍了生成的可控性。我们引入了 InstructScene，这是一种新颖的生成框架，它集成了语义图先验和布局解码器，以提高 3D 场景合成的可控性和保真度。所提出的语义图先验联合学习场景外观和布局分布，以零样本的方式展示了跨各种下游任务的多功能性。为了促进文本驱动的 3D 场景合成的基准测试，我们利用大型语言和多模态模型构建了高质量的场景指令对数据集。大量的实验结果表明，所提出的方法大大超越了现有的最先进的方法。彻底的消融研究证实了关键设计组件的功效。项目页面：https://chenuol​​in.github.io/projects/InstructScene。</details>
**PDF:** <http://arxiv.org/pdf/2402.04717v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **ScreenAI: A Vision-Language Model for UI and Infographics Understanding**<br />
**Title_cn:** ScreenAI：用于 UI 和信息图表理解的视觉语言模型<br />
**Authors:** Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cărbune, Jason Lin, Jindong Chen, Abhanshu Sharma<br />
**Abstract:** <details><summary>原文: </summary>Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.</details>
**Abstract_cn:** <details><summary>译文: </summary>屏幕用户界面（UI）和信息图表共享相似的视觉语言和设计原则，在人类交流和人机交互中发挥着重要作用。我们介绍 ScreenAI，这是一种专门研究 UI 和信息图形理解的视觉语言模型。我们的模型通过 pix2struct 的灵活修补策略改进了 PaLI 架构，并在独特的数据集混合上进行了训练。这种混合的核心是一个新颖的屏幕注释任务，其中模型必须识别 UI 元素的类型和位置。我们使用这些文本注释来描述大型语言模型的屏幕，并自动大规模生成问答 (QA)、UI 导航和摘要训练数据集。我们进行消融研究来证明这些设计选择的影响。仅用 5B 参数，ScreenAI 在基于 UI 和信息图表的任务（多页 DocVQA、WebSRC、MoTIF 和小部件字幕）上实现了新的最先进的结果，并在其他任务（图表 QA）上实现了新的一流性能、DocVQA 和 InfographicVQA）与相似尺寸的模型进行比较。最后，我们发布了三个新数据集：一个专注于屏幕注释任务，另外两个专注于问答。</details>
**PDF:** <http://arxiv.org/pdf/2402.04615v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation**<br />
**Title_cn:** ColorSwap：用于多模式评估的颜色和词序数据集<br />
**Authors:** Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning on fewer than 2,000 examples yields significant performance gains on this out-of-distribution word-order understanding task. The dataset is here: https://github.com/Top34051/colorswap.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 ColorSwap 数据集，旨在评估和提高多模态模型将对象与其颜色匹配的熟练程度。该数据集由 2,000 个独特的图像标题对组成，分为 1,000 个示例。每个示例都包含一个标题-图像对，以及一个“颜色交换”对。我们遵循 Winoground 模式：示例中的两个标题具有相同的单词，但颜色单词已重新排列以修改不同的对象。该数据集是通过自动字幕和图像生成与人类参与循环的新颖结合而创建的。我们评估了图像文本匹配（ITM）和视觉语言模型（VLM），发现即使是最新的模型在这项任务上仍然不够稳健。 GPT-4V 和 LLaVA 在我们的主要 VLM 指标上得分为 72% 和 42%，尽管它们可能会通过更先进的提示技术而有所改善。在主要 ITM 指标上，CLIP 和 SigLIP 等对比模型的表现接近机会（分别为 12% 和 30%），尽管非对比 BLIP ITM 模型更强（87%）。我们还发现，对少于 2,000 个示例进行微调可以在这种非分布词序理解任务中带来显着的性能提升。数据集位于：https://github.com/Top34051/colorswap。</details>
**PDF:** <http://arxiv.org/pdf/2402.04492v1><br />
**Code:** <https://github.com/top34051/colorswap>**<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering**<br />
**Title_cn:** NeRF 作为基于物理的逆渲染中的非远程环境发射器<br />
**Authors:** Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao<br />
**Abstract:** <details><summary>原文: </summary>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: https://nerfemitterpbir.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于物理的逆渲染旨在联合优化捕获的 2D 图像的形状、材质和照明。这里，照明是实现忠实光传输模拟的重要组成部分。虽然环境贴图通常用作逆向渲染中的照明模型，但我们表明，其远距离照明假设会导致空间不变照明，这在现实世界逆向渲染中可能是不准确的近似。我们建议使用 NeRF 作为空间变化的环境光照模型，并使用 NeRF 作为非远距离环境发射器构建逆渲染管道。通过将我们的方法与真实和合成数据集上的环境贴图进行比较，我们表明基于 NeRF 的发射器可以更准确地对场景照明进行建模，并导致更准确的逆渲染。项目页面和视频：https://nerfemitterpbir.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.04829v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Mesh-based Gaussian Splatting for Real-time Large-scale Deformation**<br />
**Title_cn:** 用于实时大范围变形的基于网格的高斯分布<br />
**Authors:** Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai<br />
**Abstract:** <details><summary>原文: </summary>Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).</details>
**Abstract_cn:** <details><summary>译文: </summary>神经隐式表示，包括神经距离场和神经辐射场，已经展示了重建具有复杂几何和拓扑的表面以及生成场景的新颖视图的强大功能。然而，对于用户来说，实时地直接变形或操纵这些具有大变形的隐式表示是具有挑战性的。高斯溅射（GS）最近成为一种有前途的具有显式几何的方法，用于表示静态场景并促进新颖视图的高质量和实时合成。然而，由于使用离散高斯且缺乏显式拓扑，它不易变形。为了解决这个问题，我们开发了一种基于 GS 的新颖方法，可以实现交互式变形。我们的关键想法是设计一种创新的基于网格的 GS 表示，并将其集成到高斯学习和操作中。 3D 高斯是在显式网格上定义的，并且它们彼此绑定：3D 高斯的渲染指导网格面分割以进行自适应细化，网格面分割指导 3D 高斯的分割。此外，显式网格约束有助于规范高斯分布，抑制质量差的高斯分布（例如未对齐的高斯分布、狭长形状的高斯分布），从而提高视觉质量并避免变形过程中的伪影。基于这种表示，我们进一步引入了大规模高斯变形技术来实现可变形 GS，它根据相关网格的操作来改变 3D 高斯的参数。我们的方法受益于现有的网格变形数据集，可实现更真实的数据驱动的高斯变形。大量的实验表明，我们的方法实现了高质量的重建和有效的变形，同时在高帧率（平均 65 FPS）下保持了有希望的渲染结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.04796v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding**<br />
**Title_cn:** OV-NeRF：具有视觉和语言基础模型的开放词汇神经辐射场，用于 3D 语义理解<br />
**Authors:** Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li<br />
**Abstract:** <details><summary>原文: </summary>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 的发展为封装 3D 场景的几何和外观特征提供了有效的表示。增强 NeRF 在开放词汇 3D 语义感知任务中的能力一直是最近的焦点。然而，目前直接从对比语言图像预训练（CLIP）中提取语义进行语义场学习的方法由于 CLIP 提供的噪声和视图不一致的语义而遇到困难。为了解决这些限制，我们提出了 OV-NeRF，它利用预训练视觉和语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。首先，从单视图角度，我们引入区域语义排名（RSR）正则化，利用 SAM 派生的 2D 掩模提案来纠正每个训练视图的噪声语义，从而促进准确的语义场学习。其次，从跨视图的角度来看，我们提出了跨视图自我增强（CSE）策略来解决视图不一致语义带来的挑战。 CSE 不是总是利用 CLIP 的 2D 不一致语义，而是利用训练有素的语义场本身生成的 3D 一致语义进行语义场训练，旨在减少歧义并增强不同视图之间的整体语义一致性。大量实验验证了我们的 OV-NeRF 优于当前最先进的方法，在 Replica 和 Scannet 上的 mIoU 指标上分别实现了 20.31% 和 18.42% 的显着改进。此外，我们的方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其稳健性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04648v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial Imagery**<br />
**Title_cn:** BirdNeRF：从航空图像中快速神经重建大规模场景<br />
**Authors:** Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao<br />
**Abstract:** <details><summary>原文: </summary>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们介绍了 BirdNeRF，它是神经辐射场 (NeRF) 的一种改进，专为使用航空图像重建大规模场景而设计。与之前专注于小规模和以对象为中心的 NeRF 重建的研究不同，我们的方法解决了多个挑战，包括（1）解决与大型模型相关的缓慢训练和渲染的问题。 (2) 满足大量图像建模所需的计算需求，需要大量资源，例如高性能 GPU。 （3）克服由于模型容量有限而在大规模重建任务中常见的明显伪影和低视觉保真度。具体来说，我们提出了一种新颖的基于鸟瞰姿势的空间分解算法，该算法将大型航空图像集分解为多个具有适当大小重叠的小集，使我们能够训练子场景的单独 NeRF。这种分解方法不仅将渲染时间与场景大小解耦，而且使渲染能够无缝缩放到任意大的环境。此外，它允许对环境进行逐块更新，增强了重建过程的灵活性和适应性。此外，我们提出了一种投影引导的新颖视图重新渲染策略，有助于有效利用独立训练的子场景来生成出色的渲染结果。我们在现有数据集以及我们自己的无人机镜头上评估了我们的方法，在具有相似渲染质量的单个 GPU 上，将重建速度比经典摄影测量软件提高了 10 倍，比最先进的大规模 NeRF 解决方案提高了 50 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.04554v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning**<br />
**Title_cn:** 基于跨模型半监督学习的道路检测知识蒸馏<br />
**Authors:** Wanli Ma, Oktay Karakus, Paul L. Rosin<br />
**Abstract:** <details><summary>原文: </summary>The advancement of knowledge distillation has played a crucial role in enabling the transfer of knowledge from larger teacher models to smaller and more efficient student models, and is particularly beneficial for online and resource-constrained applications. The effectiveness of the student model heavily relies on the quality of the distilled knowledge received from the teacher. Given the accessibility of unlabelled remote sensing data, semi-supervised learning has become a prevalent strategy for enhancing model performance. However, relying solely on semi-supervised learning with smaller models may be insufficient due to their limited capacity for feature extraction. This limitation restricts their ability to exploit training data. To address this issue, we propose an integrated approach that combines knowledge distillation and semi-supervised learning methods. This hybrid approach leverages the robust capabilities of large models to effectively utilise large unlabelled data whilst subsequently providing the small student model with rich and informative features for enhancement. The proposed semi-supervised learning-based knowledge distillation (SSLKD) approach demonstrates a notable improvement in the performance of the student model, in the application of road segmentation, surpassing the effectiveness of traditional semi-supervised learning methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识蒸馏的进步在实现知识从较大的教师模型转移到更小、更高效的学生模型方面发挥了至关重要的作用，并且对于在线和资源受限的应用程序尤其有利。学生模型的有效性在很大程度上取决于从教师那里获得的知识的质量。鉴于未标记遥感数据的可访问性，半监督学习已成为增强模型性能的流行策略。然而，由于特征提取能力有限，仅依靠较小模型的半监督学习可能是不够的。这种限制限制了他们利用训练数据的能力。为了解决这个问题，我们提出了一种结合知识蒸馏和半监督学习方法的综合方法。这种混合方法利用大型模型的强大功能来有效利用大量未标记数据，同时随后为小型学生模型提供丰富且信息丰富的功能以进行增强。所提出的基于半监督学习的知识蒸馏（SSLKD）方法在道路分割的应用中证明了学生模型的性能显着提高，超越了传统半监督学习方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.05305v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss**<br />
**Title_cn:** EfficientViT-SAM：加速分段任何模型而不会造成性能损失<br />
**Authors:** Zhuoyang Zhang, Han Cai, Song Han<br />
**Abstract:** <details><summary>原文: </summary>We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出 EfficientViT-SAM，这是一个新的加速段任何模型系列。我们保留了 SAM 的轻量级提示编码器和掩模解码器，同时用 EfficientViT 替换了笨重的图像编码器。对于训练，我们从 SAM-ViT-H 图像编码器到 EfficientViT 的知识蒸馏开始。随后，我们对 SA-1B 数据集进行端到端训练。受益于 EfficientViT 的效率和容量，EfficientViT-SAM 在 A100 GPU 上测量的 TensorRT 加速比 SAM-ViT-H 提高了 48.9 倍，且不牺牲性能。我们的代码和预训练模型发布于 https://github.com/mit-han-lab/efficientvit。</details>
**PDF:** <http://arxiv.org/pdf/2402.05008v1><br />
**Code:** <https://github.com/mit-han-lab/efficientvit>**<br />
>>**index:** 3<br />
**Title:** **ConvLoRA and AdaBN based Domain Adaptation via Self-Training**<br />
**Title_cn:** 通过自训练进行基于 ConvLoRA 和 AdaBN 的域适应<br />
**Authors:** Sidra Aleem, Julia Dietlmeier, Eric Arazo, Suzanne Little<br />
**Abstract:** <details><summary>原文: </summary>Existing domain adaptation (DA) methods often involve pre-training on the source domain and fine-tuning on the target domain. For multi-target domain adaptation, having a dedicated/separate fine-tuned network for each target domain, that retain all the pre-trained model parameters, is prohibitively expensive. To address this limitation, we propose Convolutional Low-Rank Adaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to convolutional layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters. To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA. Our method has fewer trainable parameters and performs better or on-par with large independent fine-tuned networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images. Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses convolutional and batch normalization layers. Code is available at: https://github.com/aleemsidra/ConvLoRA.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的域适应（DA）方法通常涉及源域的预训练和目标域的微调。对于多目标域适应，为每个目标域拥有一个专用/单独的微调网络来保留所有预训练的模型参数，其成本非常昂贵。为了解决这个限制，我们提出了卷积低秩适应（ConvLoRA）。 ConvLoRA 冻结预先训练的模型权重，将可训练的低秩分解矩阵添加到卷积层，并通过这些矩阵反向传播梯度，从而大大减少可训练参数的数量。为了进一步提高适应性，我们利用自适应批量归一化（AdaBN）来计算特定于目标的运行统计数据并将其与 ConvLoRA 一起使用。当在包含大脑 MRI 图像的卡尔加里-坎皮纳斯数据集的分割上进行测试时，我们的方法具有较少的可训练参数，并且与大型独立微调网络（可训练参数少于总基础模型的 0.9％）表现更好或相当。我们的方法简单而有效，可以应用于任何使用卷积和批量归一化层的基于深度学习的架构。代码位于：https://github.com/aleemsidra/ConvLoRA。</details>
**PDF:** <http://arxiv.org/pdf/2402.04964v1><br />
**Code:** <https://github.com/aleemsidra/convlora>**<br />
>>**index:** 4<br />
**Title:** **Group Distributionally Robust Dataset Distillation with Risk Minimization**<br />
**Title_cn:** 具有风险最小化的分组分布稳健数据集蒸馏<br />
**Authors:** Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen<br />
**Abstract:** <details><summary>原文: </summary>Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density? Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference. Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD. We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据集蒸馏（DD）已成为一种广泛采用的技术，用于制作合成数据集，捕获训练数据集的基本信息，促进准确神经模型的训练。其应用跨越各个领域，包括迁移学习、联邦学习和神经架构搜索。构建合成数据的最流行的方法依赖于将训练模型与合成数据集和训练数据集的收敛特性相匹配。然而，以训练数据集为目标必须被视为辅助，就像训练集是总体分布的近似替代品一样，后者是感兴趣的数据。然而，尽管 DD 很受欢迎，但仍有待探索的一个方面是 DD 与其泛化的关系，特别是在不常见的亚组中。也就是说，我们如何确保在合成数据集上训练的模型在面对来自人口密度低的地区的样本时表现良好？在这里，数据集的代表性和覆盖范围比推理时保证的训练误差变得更加突出。从分布式鲁棒优化中汲取灵感，我们引入了一种将聚类与损失风险度量最小化相结合的算法，以进行 DD。我们为我们的方法提供了理论依据，并通过数值实验证明了其跨子组的有效泛化和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04676v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection**<br />
**Title_cn:** G-NAS：用于单域泛化对象检测的泛化神经架构搜索<br />
**Authors:** Fan Wu, Jinling Gao, Lanqing Hong, Xinbing Wang, Chenghu Zhou, Nanyang Ye<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们关注一项现实但具有挑战性的任务，即单域泛化对象检测（S-DGOD），其中只有一个源域的数据可用于训练对象检测器，但必须泛化多个不同的目标域。在S-DGOD中，由于任务的复杂性，需要高容量的拟合和泛化能力。可微分神经架构搜索 (NAS) 以其复杂数据拟合的高容量而闻名，我们建议利用可微分 NAS 来解决 S-DGOD。然而，由于特征不平衡现象，它可能会面临严重的过度拟合问题，其中通过梯度下降优化的参数偏向于从易于学习的特征中学习，这些特征通常是非因果的并且与地面真实标签虚假相关，例如物体检测数据中的背景特征。因此，这会导致严重的性能下降，特别是在推广到源域和目标域之间存在巨大域差距的看不见的目标域时。为了解决这个问题，我们提出了泛化损失（G-loss），它是一个 OoD 感知目标，通过使用梯度下降来优化参数，不仅在易于学习的特征子集上，而且在还有剩余的预测特征用于泛化，整体框架被命名为G-NAS。 S-DGOD 城市场景数据集上的实验结果表明，与基线方法相比，所提出的 G-NAS 实现了 SOTA 性能。代码可在 https://github.com/wufan-cse/G-NAS 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04672v1><br />
**Code:** <https://github.com/wufan-cse/g-nas>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Combining shape and contour features to improve tool wear monitoring in milling processes**<br />
**Title_cn:** 结合形状和轮廓特征来改进铣削过程中的刀具磨损监控<br />
**Authors:** M. T. García-Ordás, E. Alegre-Gutiérrez, V. González-Castro, R. Alaiz-Rodríguez<br />
**Abstract:** <details><summary>原文: </summary>In this paper, a new system based on combinations of a shape descriptor and a contour descriptor has been proposed for classifying inserts in milling processes according to their wear level following a computer vision based approach. To describe the wear region shape we have proposed a new descriptor called ShapeFeat and its contour has been characterized using the method BORCHIZ that, to the best of our knowledge, achieves the best performance for tool wear monitoring following a computer vision-based approach. Results show that the combination of BORCHIZ with ShapeFeat using a late fusion method improves the classification performance significantly, obtaining an accuracy of 91.44% in the binary classification (i.e. the classification of the wear as high or low) and 82.90% using three target classes (i.e. classification of the wear as high, medium or low). These results outperform the ones obtained by both descriptors used on their own, which achieve accuracies of 88.70 and 80.67% for two and three classes, respectively, using ShapeFeat and 87.06 and 80.24% with B-ORCHIZ. This study yielded encouraging results for the manufacturing community in order to classify automatically the inserts in terms of their wear for milling processes.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，提出了一种基于形状描述符和轮廓描述符组合的新系统，用于按照基于计算机视觉的方法根据磨损程度对铣削过程中的刀片进行分类。为了描述磨损区域形状，我们提出了一种名为 ShapeFeat 的新描述符，并使用 BORCHIZ 方法对其轮廓进行了表征，据我们所知，采用基于计算机视觉的方法实现了刀具磨损监测的最佳性能。结果表明，采用后期融合方法的 BORCHIZ 与 ShapeFeat 的结合显着提高了分类性能，在二元分类（即磨损程度高或低的分类）中获得了 91.44% 的准确率，在三个目标类别（即磨损的高低分类）中获得了 82.90% 的准确率（即磨损分为高、中或低）。这些结果优于单独使用两个描述符获得的结果，使用 ShapeFeat 时，两个和三个类别的准确率分别为 88.70 和 80.67%，使用 B-ORCHIZ 时分别达到 87.06 和 80.24%。这项研究为制造界取得了令人鼓舞的结果，以便根据铣削过程中的磨损情况自动对刀片进行分类。</details>
**PDF:** <http://arxiv.org/pdf/2402.05978v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Tool wear monitoring using an online, automatic and low cost system based on local texture**<br />
**Title_cn:** 使用基于局部纹理的在线、自动和低成本系统进行刀具磨损监测<br />
**Authors:** M. T. García-Ordás, E. Alegre-Gutiérrez, R. Alaiz-Rodríguez, V. González-Castro<br />
**Abstract:** <details><summary>原文: </summary>In this work we propose a new online, low cost and fast approach based on computer vision and machine learning to determine whether cutting tools used in edge profile milling processes are serviceable or disposable based on their wear level. We created a new dataset of 254 images of edge profile cutting heads which is, to the best of our knowledge, the first publicly available dataset with enough quality for this purpose. All the inserts were segmented and their cutting edges were cropped, obtaining 577 images of cutting edges: 301 functional and 276 disposable. The proposed method is based on (1) dividing the cutting edge image in different regions, called Wear Patches (WP), (2) characterising each one as worn or serviceable using texture descriptors based on different variants of Local Binary Patterns (LBP) and (3) determine, based on the state of these WP, if the cutting edge (and, therefore, the tool) is serviceable or disposable. We proposed and assessed five different patch division configurations. The individual WP were classified by a Support Vector Machine (SVM) with an intersection kernel. The best patch division configuration and texture descriptor for the WP achieves an accuracy of 90.26% in the detection of the disposable cutting edges. These results show a very promising opportunity for automatic wear monitoring in edge profile milling processes.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了一种基于计算机视觉和机器学习的在线、低成本、快速的新方法，以根据磨损程度确定边缘仿形铣削工艺中使用的切削刀具是否可维修或一次性。我们创建了一个包含 254 张边缘轮廓切割头图像的新数据集，据我们所知，这是第一个具有足够质量用于此目的的公开可用数据集。所有刀片均被分段并裁剪其切削刃，获得 577 个切削刃图像：301 个功能性刀片和 276 个一次性刀片。所提出的方法基于（1）将尖端图像划分为不同区域，称为磨损补丁（WP），（2）使用基于局部二进制模式（LBP）不同变体的纹理描述符将每个区域描述为磨损或可用(3) 根据这些 WP 的状态，确定切削刃（以及刀具）是可使用的还是一次性的。我们提出并评估了五种不同的补丁划分配置。各个 WP 通过具有交集内核的支持向量机 (SVM) 进行分类。 WP 的最佳补丁分割配置和纹理描述符在一次性切割边缘的检测中实现了 90.26% 的准确率。这些结果显示了边缘仿形铣削过程中自动磨损监测的非常有前途的机会。</details>
**PDF:** <http://arxiv.org/pdf/2402.05977v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Self-calibrated convolution towards glioma segmentation**<br />
**Title_cn:** 用于神经胶质瘤分割的自校准卷积<br />
**Authors:** Felipe C. R. Salvagnini, Gerson O. Barbosa, Alexandre X. Falcao, Cid A. N. Santos<br />
**Abstract:** <details><summary>原文: </summary>Accurate brain tumor segmentation in the early stages of the disease is crucial for the treatment's effectiveness, avoiding exhaustive visual inspection of a qualified specialist on 3D MR brain images of multiple protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma segmentation, being nnU-Net one of the best. In this work, we evaluate self-calibrated convolutions in different parts of the nnU-Net network to demonstrate that self-calibrated modules in skip connections can significantly improve the enhanced-tumor and tumor-core segmentation accuracy while preserving the wholetumor segmentation accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>在疾病早期阶段准确的脑肿瘤分割对于治疗的有效性至关重要，避免合格专家对多种方案（例如 T1、T2、T2-FLAIR、T1-Gd）的 3D MR 脑图像进行详尽的目视检查。存在多种用于神经胶质瘤分割的网络，其中 nnU-Net 是最好的网络之一。在这项工作中，我们评估了 nnU-Net 网络不同部分的自校准卷积，以​​证明跳跃连接中的自校准模块可以显着提高增强型肿瘤和肿瘤核心分割精度，同时保持整个肿瘤分割精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.05218v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types**<br />
**Title_cn:** 通过针对不同文档类型的专业模型和先进技术增强孟加拉语 OCR<br />
**Authors:** AKM Shahariar Azad Rabby, Hasmot Ali, Md. Majedul Islam, Sheikh Abujar, Fuad Rahman<br />
**Abstract:** <details><summary>原文: </summary>This research paper presents a unique Bengali OCR system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extraction and analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究论文提出了一种具有某些功能的独特孟加拉语 OCR 系统。该系统擅长重建文档布局，同时保留结构、对齐方式和图像。它结合了先进的图像和签名检测功能，可实现准确提取。专门的分词模型可满足不同的文档类型，包括计算机撰写、凸版印刷、打字机和手写文档。该系统处理静态和动态手写输入，识别各种书写风格。此外，它还能够识别孟加拉语的复合字符。广泛的数据收集工作提供了多样化的语料库，而先进的技术组件则优化了字符和单词识别。其他贡献包括图像、徽标、签名和表格识别、透视校正、布局重建以及用于高效和可扩展处理的排队模块。该系统在高效、准确的文本提取和分析方面表现出了出色的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.05158v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation**<br />
**Title_cn:** Mamba-UNet：用于医学图像分割的类似 UNet 的纯视觉 Mamba<br />
**Authors:** Ziyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, Lei Li<br />
**Abstract:** <details><summary>原文: </summary>In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学图像分析的最新进展中，卷积神经网络 (CNN) 和视觉变换器 (ViT) 设定了重要的基准。前者擅长通过卷积运算捕获局部特征，而后者则通过利用自注意力机制实现了卓越的全局上下文理解。然而，这两种架构在有效建模医学图像中的远程依赖性方面都存在局限性，而这是精确分割的关键方面。 Mamba 架构以其在处理长序列和全局上下文信息方面的能力以及作为状态空间模型 (SSM) 的计算效率的提高而闻名，受到 Mamba 架构的启发，我们提出了 Mamba-UNet，这是一种在医学图像分割中协同 U-Net 的新颖架构凭借曼巴的能力。 Mamba-UNet 采用基于纯 Visual Mamba (VMamba) 的编码器-解码器结构，并注入跳跃连接以保留不同规模网络的空间信息。这种设计促进了全面的特征学习过程，捕获医学图像中复杂的细节和更广泛的语义上下文。我们在 VMamba 块内引入了一种新颖的集成机制，以确保编码器和解码器路径之间的无缝连接和信息流，从而增强分段性能。我们对公开的 MRI 心脏多结构分割数据集进行了实验。结果表明，在相同超参数设置下，Mamba-UNet 在医学图像分割方面优于 UNet、Swin-UNet。源代码和基线实现可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.05079v1><br />
**Code:** <https://github.com/ziyangwang007/mamba-unet>**<br />
>>**index:** 6<br />
**Title:** **Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training**<br />
**Title_cn:** 使用合成训练在 HoloLens 上检测平面、无纹理的工业对象并进行姿态估计<br />
**Authors:** Thomas Pöllabauer, Fabian Rücker, Andreas Franek, Felix Gorschlüter<br />
**Abstract:** <details><summary>原文: </summary>Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability.</details>
**Abstract_cn:** <details><summary>译文: </summary>目前最先进的 6d 姿态估计计算量太大，无法部署在边缘设备上，例如 Microsoft HoloLens (2) 或 Apple iPad，这两种设备都用于越来越多的增强现实应用。 AR 的质量在很大程度上取决于其检测和叠加场景内几何图形的能力。我们提出了一种经过综合训练的基于客户端-服务器的增强现实应用程序，展示了边缘设备上金属和无纹理工业对象的最先进的对象姿势估计。合成数据可以在没有真实照片的情况下进行训练，即针对尚未制造的物体。我们对 AR 辅助分类任务的定性评估、对两个渲染的定量评估以及 HoloLens 2 上记录的真实世界数据，揭示了其现实世界的适用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04979v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation**<br />
**Title_cn:** 用于标签移位鲁棒测试时间适应的通道选择性归一化<br />
**Authors:** Pedro Vianna, Muawiz Chaudhary, Paria Mehrbod, An Tang, Guy Cloutier, Guy Wolf, Michael Eickenberg, Eugene Belilovsky<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution. For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets. To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference. Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks. It is implemented by recalculating batch normalization statistics on test batches. Prior work has focused on analysis with test data that has the same label distribution as the training data. However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. This presents a risk in applying test time adaptation methods in deployment. We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts. Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes. We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts. We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在许多不同的任务中都有有用的应用，但是它们的性能可能会受到数据分布变化的严重影响。例如，在生物医学领域，它们的性能可能会受到训练和测试数据集之间数据（不同机器、人群）变化的影响。为了确保现实世界场景的鲁棒性和泛化性，最近研究了测试时适应，作为在推理过程中将模型调整为新数据分布的方法。测试时批量归一化是一种简单且流行的方法，它在域转换基准上实现了引人注目的性能。它是通过重新计算测试批次的批次标准化统计来实现的。之前的工作重点是使用与训练数据具有相同标签分布的测试数据进行分析。然而，在许多实际应用中，该技术很容易受到标签分布变化的影响，有时会产生灾难性的失败。这带来了在部署中应用测试时间适应方法的风险。我们建议通过仅选择性地适应深层网络中的通道来应对这一挑战，最大限度地减少对标签变化敏感的剧烈适应。我们的选择方案基于我们凭经验激发的两个原则：（1）后面的网络层对标签移位更敏感（2）单个特征可以对特定类别敏感。我们将所提出的技术应用于三个分类任务，包括 CIFAR10-C、Imagenet-C 和脂肪肝诊断，其中我们探索了协变量和标签分布变化。我们发现我们的方法可以带来 TTA 的好处，同时显着降低其他方法中常见的失败风险，同时对超参数的选择具有鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04958v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Is Two-shot All You Need? A Label-efficient Approach for Video Segmentation in Breast Ultrasound**<br />
**Title_cn:** 您只需要两次拍摄吗？乳腺超声视频分割的标签高效方法<br />
**Authors:** Jiajun Zeng, Ruobing Huang, Dong Ni<br />
**Abstract:** <details><summary>原文: </summary>Breast lesion segmentation from breast ultrasound (BUS) videos could assist in early diagnosis and treatment. Existing video object segmentation (VOS) methods usually require dense annotation, which is often inaccessible for medical datasets. Furthermore, they suffer from accumulative errors and a lack of explicit space-time awareness. In this work, we propose a novel two-shot training paradigm for BUS video segmentation. It not only is able to capture free-range space-time consistency but also utilizes a source-dependent augmentation scheme. This label-efficient learning framework is validated on a challenging in-house BUS video dataset. Results showed that it gained comparable performance to the fully annotated ones given only 1.9% training labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳腺超声 (BUS) 视频中的乳腺病灶分割有助于早期诊断和治疗。现有的视频对象分割（VOS）方法通常需要密集的注释，这对于医疗数据集来说通常是无法访问的。此外，它们还存在累积误差和缺乏明确的时空意识。在这项工作中，我们提出了一种用于 BUS 视频分割的新颖的双镜头训练范例。它不仅能够捕获自由范围的时空一致性，而且还利用依赖于源的增强方案。这种标签高效的学习框架在具有挑战性的内部 BUS 视频数据集上得到了验证。结果表明，它获得了与仅给出 1.9% 训练标签的完全注释的性能相当的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.04921v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration**<br />
**Title_cn:** 通过级联深度估计和校准实现基于相机的精确 3D 物体检测<br />
**Authors:** Chaoqun Wang, Yiran Qin, Zijian Kang, Ningning Ma, Ruimao Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近基于相机的 3D 对象检测受到从图像到 3D 特征空间转换的精度以及 3D 空间内对象定位的准确性的限制。本文旨在解决基于相机的 3D 对象检测的一个基本问题：如何有效学习深度信息以实现准确的特征提升和对象定位。与之前使用监督估计模型直接预测深度分布的方法不同，我们提出了一个由两种深度感知学习范式组成的级联框架。首先，深度估计（DE）方案利用相对深度信息来实现从 2D 空间到 3D 空间的有效特征提升。此外，深度校准 (DC) 方案引入了深度重建，以进一步调整沿深度轴的 3D 对象定位扰动。在实践中，DE是通过使用绝对和相对深度优化损失来显式实现的，以提高深度预测的精度，而DC的能力在训练阶段通过深度去噪机制隐式嵌入到检测Transformer中。整个模型训练是通过端到端的方式完成的。我们提出了一个基线检测器，并评估了我们建议的有效性，在 NuScenes 基准上提高了 +2.2%/+2.7% NDS/mAP，并获得了 55.9%/45.7% NDS/mAP 的可比性能。此外，我们基于各种探测器进行了大量实验，以证明其通用性，NDS 提高了约 +2%。</details>
**PDF:** <http://arxiv.org/pdf/2402.04883v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation**<br />
**Title_cn:** STAR：用于改进对象检测和 6D 姿势估计的形状聚焦纹理不可知表示<br />
**Authors:** Peter Hönig, Stefan Thalhammer, Jean-Baptiste Weibel, Matthias Hirschmanner, Markus Vincze<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in machine learning have greatly benefited object detection and 6D pose estimation for robotic grasping. However, textureless and metallic objects still pose a significant challenge due to fewer visual cues and the texture bias of CNNs. To address this issue, we propose a texture-agnostic approach that focuses on learning from CAD models and emphasizes object shape features. To achieve a focus on learning shape features, the textures are randomized during the rendering of the training data. By treating the texture as noise, the need for real-world object instances or their final appearance during training data generation is eliminated. The TLESS and ITODD datasets, specifically created for industrial settings in robotics and featuring textureless and metallic objects, were used for evaluation. Texture agnosticity also increases the robustness against image perturbations such as imaging noise, motion blur, and brightness changes, which are common in robotics applications. Code and datasets are publicly available at github.com/hoenigpeter/randomized_texturing.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习的最新进展极大地促进了机器人抓取的对象检测和 6D 姿态估计。然而，由于视觉线索较少和 CNN 的纹理偏差，无纹理和金属物体仍然构成重大挑战。为了解决这个问题，我们提出了一种与纹理无关的方法，该方法侧重于从 CAD 模型中学习并强调对象形状特征。为了专注于学习形状特征，在训练数据的渲染过程中纹理被随机化。通过将纹理视为噪声，消除了在训练数据生成期间对真实世界对象实例或其最终外观的需求。 TLESS 和 ITODD 数据集是专门为机器人工业环境创建的，具有无纹理和金属物体，用于评估。纹理不可知性还提高了针对图像扰动的鲁棒性，例如成像噪声、运动模糊和亮度变化，这些在机器人应用中很常见。代码和数据集可在 github.com/hoenigpeter/randomized_texturing 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04878v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Advancing Anomaly Detection: An Adaptation Model and a New Dataset**<br />
**Title_cn:** 推进异常检测：适应模型和新数据集<br />
**Authors:** Liyun Zhu, Arjun Raj, Lei Wang<br />
**Abstract:** <details><summary>原文: </summary>Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection. However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges. Extending these models to entirely new scenarios necessitates retraining or fine-tuning, a process that can be time consuming. To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot learning framework for faster adaptation of pre-trained models to new concepts. Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views. In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views. This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models. MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions. Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset. Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios. This highlights our method's potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>行业监控广泛应用于零售、制造、教育和智慧城市等行业，每个行业都呈现出独特的异常情况，需要专门的检测。然而，使异常检测模型适应同一场景中的新观点提出了挑战。将这些模型扩展到全新的场景需要重新训练或微调，这个过程可能非常耗时。为了应对这些挑战，我们提出了场景自适应异常检测（SA2D）方法，利用小样本学习框架使预训练模型更快地适应新概念。尽管采用了这种方法，但由于缺乏具有不同场景和摄像机视图的综合数据集，出现了重大挑战。为此，我们引入了多场景异常检测 (MSAD) 数据集，其中包含从各种摄像机视图捕获的 14 个不同场景。这个现实世界的数据集是第一个高分辨率异常检测数据集，为训练高级模型提供了坚实的基础。 MSAD 包括多种正常运动模式，并结合了具有挑战性的变化，例如不同的照明和天气条件。通过实验，我们验证了 SA2D 的有效性，特别是在 MSAD 数据集上进行训练时。我们的结果表明，SA2D 不仅在同一场景中的新观点下表现出色，而且在面对全新场景时也表现出有竞争力的性能。这凸显了我们的方法在解决在不同和不断变化的监视场景中检测异常的挑战方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.04857v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning**<br />
**Title_cn:** SARI：基于噪声部分标签学习的简单平均和鲁棒识别<br />
**Authors:** Darshana Saravanan, Naresh Manwani, Vineet Gandhi<br />
**Abstract:** <details><summary>原文: </summary>Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on seven datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in almost all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>部分标签学习（PLL）是一种弱监督学习范例，其中每个训练实例都与一组候选标签（部分标签）配对，其中一个是真实标签。噪声锁相环（NPLL）通过允许一些部分标签不包含真实标签来放松这一约束，从而增强了问题的实用性。我们的工作以 NPLL 为中心，并提出了一个名为 SARI 的简约框架，该框架最初通过加权最近邻算法利用噪声部分标签来为图像分配伪标签。然后，使用这些伪标签和图像对来训练具有标签平滑和标准正则化技术的深度神经网络分类器。随后使用分类器的特征和预测来细化和提高伪标签的准确性。 SARI 结合了文献中基于平均的策略（在伪标签中）和基于识别的策略（在分类器训练中）的优势。我们对七个数据集进行了彻底的实验，并将 SARI 与现有技术中的九个 NPLL 和 PLL 方法进行了比较。 SARI 在几乎所有研究设置中都取得了最先进的结果，在细粒度分类和极端噪声设置中获得了显着的收益。</details>
**PDF:** <http://arxiv.org/pdf/2402.04835v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Color Recognition in Challenging Lighting Environments: CNN Approach**<br />
**Title_cn:** 具有挑战性的照明环境中的颜色识别：CNN 方法<br />
**Authors:** Nizamuddin Maitlo, Nooruddin Noonari, Sajid Ahmed Ghanghro, Sathishkumar Duraisamy, Fayaz Ahmed<br />
**Abstract:** <details><summary>原文: </summary>Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings. Researchers are working to enhance the color detection techniques for the application of computer vision. They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled. To address this issue, a color detection method, which is based on a Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the Convolutional Neural Network trained to detect the color of an object in different lighting conditions. It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>光在人类或机器视觉的视觉中起着至关重要的作用，感知的颜色始终基于周围环境的照明条件。研究人员正在努力增强计算机视觉应用的颜色检测技术。他们已经使用不同的颜色检测方法实现了提出的几种方法，但仍然存在可以填补的空白。为了解决这个问题，提出了一种基于卷积神经网络（CNN）的颜色检测方法。首先，使用边缘检测分割技术执行图像分割来指定对象，然后将分割后的对象输入到经过训练的卷积神经网络中，以检测不同照明条件下对象的颜色。实验验证，我们的方法可以显着增强不同光照条件下颜色检测的鲁棒性，并且我们的方法比现有方法表现出更好的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.04762v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation**<br />
**Title_cn:** 半监督核实例分割的边界感知对比学习<br />
**Authors:** Ye Zhang, Ziyue Wang, Yifeng Wang, Hao Bian, Linghan Cai, Hengrui Li, Lingbo Zhang, Yongbing Zhang<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation. However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei. Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries. To address the above problem, this paper proposes a boundary-aware contrastive learning network to denoise the boundary noise in a semi-supervised nuclei segmentation task. The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive learning (CRC) module. The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature contrastive learning. We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督分割方法在自然场景中表现出了良好的结果，提供了一种减少对手动注释依赖的解决方案。然而，由于细胞核和组织之间细微的颜色差异以及细胞核之间显着的形态变化，这些方法在直接应用于病理图像时面临重大挑战。因此，生成的伪标签通常包含很多噪声，尤其是在原子核边界处。为了解决上述问题，本文提出了一种边界感知对比学习网络，用于对半监督细胞核分割任务中的边界噪声进行去噪。该模型有两个关键设计：低分辨率去噪（LRD）模块和跨 RoI 对比学习（CRC）模块。 LRD通过伪标签去噪提高了核边界的平滑度，CRC通过边界特征对比学习增强了前景和背景的区分度。我们进行了大量的实验来证明我们提出的方法相对于现有的半监督实例分割方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04756v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Adversarial Robustness Through Artifact Design**<br />
**Title_cn:** 通过工件设计实现对抗鲁棒性<br />
**Authors:** Tsufit Shua, Mahmood Sharif<br />
**Abstract:** <details><summary>原文: </summary>Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors. We found that, combined with adversarial training, our approach led to up to 25.18\% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性例子的出现是对机器学习的挑战。为了阻碍它们，大多数防御措施都会改变模型的训练方式（例如对抗性训练）或推理方式（例如随机平滑）。尽管如此，虽然这些方法显着提高了模型的对抗鲁棒性，但模型仍然非常容易受到对抗性例子的影响。认识到，在交通标志识别等某些领域，对象是按照指定如何设计工件（例如标志）的标准来实现的，因此我们提出了一种提高对抗鲁棒性的新方法。具体来说，我们提供了一种重新定义标准的方法，对现有标准进行微小的更改，以防御对抗性示例。我们将工件设计问题表述为鲁棒优化问题，并提出基于梯度和贪婪搜索方法来解决它。我们评估了交通标志识别领域的方法，使其能够改变交通标志象形图（即标志内的符号）及其颜色。我们发现，与对抗训练相结合，与针对两种对手类型的最先进方法相比，我们的方法的鲁棒精度提高了 25.18%，同时进一步提高了良性输入的精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.04660v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **GSN: Generalisable Segmentation in Neural Radiance Field**<br />
**Title_cn:** GSN：神经辐射领域的通用分割<br />
**Authors:** Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan<br />
**Abstract:** <details><summary>原文: </summary>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: https://vinayak-vg.github.io/GSN/</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的辐射场（RF）表示捕获特定场景的细节，并且必须针对每个场景重新训练。 RF 中添加了语义特征字段，以促进多项分割任务。广义 RF 表示学习视图插值的原理。给定一些视图，广义 RF 可以呈现未知且未经训练的场景的新视图。我们提出了一种将特征字段提炼为广义 GNT 表示的方法。我们的 GSN 表示动态生成未见过的场景的新视图以及一致的每像素语义特征。这使得能够对任意新场景进行多视图分割。我们展示了不同的语义特征被提炼成广义的 RF。我们的多视图分割结果与使用传统 RF 的方法相当。 GSN 显着缩小了标准和通用 RF 方法之间的差距。项目页面：https://vinayak-vg.github.io/GSN/</details>
**PDF:** <http://arxiv.org/pdf/2402.04632v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors**<br />
**Title_cn:** LLM 与 VLM 相遇：利用细粒度描述符增强开放词汇对象检测<br />
**Authors:** Sheng Jin, Xueying Jiang, Jiaxing Huang, Lewei Lu, Shijian Lu<br />
**Abstract:** <details><summary>原文: </summary>Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.</details>
**Abstract_cn:** <details><summary>译文: </summary>受视觉语言模型 (VLM) 在图像分类任务中出色的零样本能力的启发，开放词汇目标检测通过将广泛的 VLM 知识提炼到检测器训练中，引起了越来越多的兴趣。然而，大多数现有的开放词汇检测器仅通过将区域嵌入与分类标签（例如自行车）对齐来进行学习，而忽略了 VLM 将视觉嵌入与对象部分的细粒度文本描述（例如踏板和铃）对齐的能力。本文介绍了 DVDet，一种描述符增强型开放词汇检测器，它引入了条件上下文提示和分层文本描述符，可实现精确的区域文本对齐以及一般的开放词汇检测训练。具体来说，条件上下文提示将区域嵌入转换为类似图像的表示，可以直接集成到通用开放词汇检测训练中。此外，我们引入大型语言模型作为交互式和隐式知识库，它能够迭代挖掘和细化面向视觉的文本描述符，以实现精确的区域文本对齐。对多个大规模基准测试的大量实验表明，DVDet 的性能始终大幅优于最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.04630v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Multi-Scale Semantic Segmentation with Modified MBConv Blocks**<br />
**Title_cn:** 使用修改的 MBConv 块进行多尺度语义分割<br />
**Authors:** Xi Chen, Yang Cai, Yuan Wu, Bo Xiong, Taesung Park<br />
**Abstract:** <details><summary>原文: </summary>Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks. Despite their success, their application in semantic segmentation has remained relatively unexplored. This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation. Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification. We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities. By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，MBConv 模块最初是为了在资源有限的情况下提高效率而设计的，后来适应了尖端的图像分类性能，已经在图像分类任务中展示了巨大的潜力。尽管取得了成功，但它们在语义分割中的应用仍然相对未经探索。本文介绍了专门为语义分割量身定制的 MBConv 块的新颖改编。我们的修改源于这样的见解：语义分割需要提取比图像分类更详细的空间信息。我们认为，为了有效地执行多尺度语义分割，U-Net 架构的每个分支，无论其分辨率如何，都应该具有等效的分割能力。通过实施这些更改，我们的方法在 Cityscapes 测试和验证数据集上分别获得了令人印象深刻的平均交集 (IoU) 分数，分别为 84.5% 和 84.0%，这证明了我们提出的修改在增强语义分割性能方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04618v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment**<br />
**Title_cn:** 认识 JEANIE：通过时间视点对齐进行 3D 骨架序列的相似性测量<br />
**Authors:** Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz<br />
**Abstract:** <details><summary>原文: </summary>Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be matched to the query temporal block with the same or adjacent (next) temporal index, and adjacent camera views to achieve joint local temporal-viewpoint warping. JEANIE selects the smallest distance among matching paths with different temporal-viewpoint warping patterns, an advantage over DTW which only performs temporal alignment. We also propose an unsupervised FSAR akin to clustering of sequences with JEANIE as a distance measure. JEANIE achieves state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II on supervised and unsupervised FSAR, and their meta-learning inspired fusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频序列表现出动作速度、时间位置和主体姿势的显着干扰变化（不良影响），导致在比较两组帧或评估两个序列的相似性时出现时间视点错位。因此，我们提出序列对的联合时间和相机视点对齐（JEANIE）。我们特别关注 3D 骨架序列，其相机和拍摄对象的姿势可以在 3D 中轻松操纵。我们在骨架少镜头动作识别（FSAR）上评估 JEANIE，其中由于新类别的样本有限，匹配支持查询序列对的时间块（构成序列的时间块）（通过剔除令人讨厌的变化）至关重要。给定一个查询序列，我们通过模拟多个摄像机位置来创建其多个视图。对于支持序列，我们将其与视图模拟的查询序列相匹配，如流行的动态时间规整（DTW）中那样。具体地，每个支持时间块可以与具有相同或相邻（下一个）时间索引以及相邻相机视图的查询时间块匹配，以实现联合局部时间视点扭曲。 JEANIE 在具有不同时间视点扭曲模式的匹配路径中选择最小距离，这比仅执行时间对齐的 DTW 具有优势。我们还提出了一种类似于序列聚类的无监督 FSAR，并使用 JEANIE 作为距离度量。 JEANIE 在 NTU-60、NTU-120、Kinetics-sculpture 和 UWA3D Multiview Activity II 的监督和无监督 FSAR 及其元学习启发融合上取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.04599v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)**<br />
**Title_cn:** 利用双输出尖峰架构 (DOSA) 提高持续多标签学习中的不平衡鲁棒性<br />
**Authors:** Sourav Mishra, Shirin Dora, Suresh Sundaram<br />
**Abstract:** <details><summary>原文: </summary>Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance of the model by making it more robust to data imbalance. A modified F1 score is presented to evaluate the effectiveness of the proposed loss function in handling imbalance. Experiments on several benchmark multi-label datasets show that DOSA trained with the proposed loss function shows improved robustness to data imbalance and obtains better continual multi-label learning performance than CIFDM, a previous state-of-the-art algorithm.</details>
**Abstract_cn:** <details><summary>译文: </summary>为解决典型的监督分类问题而设计的算法只能从一组固定的样本和标签中学习，这使得它们不适合现实世界，在现实世界中，数据作为样本流到达，通常随着时间的推移与多个标签相关联。这激发了对任务无关的持续多标签学习问题的研究。虽然最近的文献中已经提出了使用深度学习方法进行持续多标签学习的算法，但它们的计算量往往很大。尽管尖峰神经网络 (SNN) 提供了人工神经网络的计算高效替代方案，但现有文献尚未使用 SNN 进行持续多标签学习。此外，使用 SNN 准确确定多个标签仍然是一个开放的研究问题。这项工作提出了一种双输出尖峰架构（DOSA）来弥补这些研究空白。还提出了一种新颖的不平衡感知损失函数，通过使其对数据不平衡更加鲁棒来提高模型的多标签分类性能。提出了修改后的 F1 分数来评估所提出的损失函数在处理不平衡方面的有效性。在多个基准多标签数据集上的实验表明，使用所提出的损失函数训练的 DOSA 表现出对数据不平衡的鲁棒性提高，并且比之前最先进的算法 CIFDM 获得了更好的连续多标签学习性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.04596v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation**<br />
**Title_cn:** 用于 CBCT 牙齿分割的带有掩模图像建模的稀疏解剖提示半监督学习<br />
**Authors:** Pengyu Dai, Yafei Ou, Yang Liu, Yue Zhao<br />
**Abstract:** <details><summary>原文: </summary>Accurate tooth identification and segmentation in Cone Beam Computed Tomography (CBCT) dental images can significantly enhance the efficiency and precision of manual diagnoses performed by dentists. However, existing segmentation methods are mainly developed based on large data volumes training, on which their annotations are extremely time-consuming. Meanwhile, the teeth of each class in CBCT dental images being closely positioned, coupled with subtle inter-class differences, gives rise to the challenge of indistinct boundaries when training model with limited data. To address these challenges, this study aims to propose a tasked-oriented Masked Auto-Encoder paradigm to effectively utilize large amounts of unlabeled data to achieve accurate tooth segmentation with limited labeled data. Specifically, we first construct a self-supervised pre-training framework of masked auto encoder to efficiently utilize unlabeled data to enhance the network performance. Subsequently, we introduce a sparse masked prompt mechanism based on graph attention to incorporate boundary information of the teeth, aiding the network in learning the anatomical structural features of teeth. To the best of our knowledge, we are pioneering the integration of the mask pre-training paradigm into the CBCT tooth segmentation task. Extensive experiments demonstrate both the feasibility of our proposed method and the potential of the boundary prompt mechanism.</details>
**Abstract_cn:** <details><summary>译文: </summary>锥形束计算机断层扫描 (CBCT) 牙科图像中准确的牙齿识别和分割可以显着提高牙医手动诊断的效率和精度。然而，现有的分割方法主要是基于大数据量训练开发的，其标注非常耗时。同时，CBCT牙齿图像中各类别的牙齿位置紧密，加上类别间的细微差异，在有限数据的训练模型中带来了边界不清晰的挑战。为了应对这些挑战，本研究旨在提出一种面向任务的掩模自动编码器范式，以有效利用大量未标记数据，以有限的标记数据实现准确的牙齿分割。具体来说，我们首先构建了一个掩码自动编码器的自监督预训练框架，以有效地利用未标记数据来增强网络性能。随后，我们引入了一种基于图注意力的稀疏屏蔽提示机制来合并牙齿的边界信息，帮助网络学习牙齿的解剖结构特征。据我们所知，我们正在率先将掩模预训练范例集成到 CBCT 牙齿分割任务中。大量的实验证明了我们提出的方法的可行性和边界提示机制的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.04587v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention**<br />
**Title_cn:** Attention Guided CAM：自注意力引导的 Vision Transformer 的视觉解释<br />
**Authors:** Saebom Leem, Hyunseok Seo<br />
**Abstract:** <details><summary>原文: </summary>Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.</details>
**Abstract_cn:** <details><summary>译文: </summary>Vision Transformer（ViT）是计算机视觉领域使用最广泛的模型之一，在各种任务上都具有出色的性能。为了在各种应用中充分利用基于 ViT 的架构，需要具有良好定位性能的适当可视化方法，但由于其独特的结构，这些基于 CNN 的模型中采用的方法在 ViT 中仍然不可用。在这项工作中，我们提出了一种应用于 ViT 的注意力引导可视化方法，为其决策提供高级语义解释。我们的方法有选择地聚合从分类输出直接传播到每个自注意力的梯度，收集从输入图像的每个位置提取的图像特征的贡献。这些梯度还受到归一化自注意力分数的指导，即成对补丁相关分数。它们用于补充由自注意力机制有效检测到的补丁级上下文信息的梯度。我们的方法提供了详尽的高级语义解释，并且仅使用类标签就具有出色的本地化性能。因此，我们的方法在弱监督定位任务中优于 ViT 之前领先的可解释性方法，并且在捕获目标类对象的完整实例方面表现出强大的能力。同时，我们的方法提供了忠实解释模型的可视化，这在扰动比较测试中得到了证明。</details>
**PDF:** <http://arxiv.org/pdf/2402.04563v1><br />
**Code:** <https://github.com/leemsaebom/attention-guided-cam-visual-explanations-of-vision-transformer-guided-by-self-attention>**<br />
>>**index:** 23<br />
**Title:** **FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models**<br />
**Title_cn:** FM-Fusion：视觉语言基础模型推动的实例感知语义映射<br />
**Authors:** Chuhao Liu, Ke Wang, Jieqi Shi, Zhijian Qiao, Shaojie Shen<br />
**Abstract:** <details><summary>原文: </summary>Semantic mapping based on the supervised object detectors is sensitive to image distribution. In real-world environments, the object detection and segmentation performance can lead to a major drop, preventing the use of semantic mapping in a wider domain. On the other hand, the development of vision-language foundation models demonstrates a strong zero-shot transferability across data distribution. It provides an opportunity to construct generalizable instance-aware semantic maps. Hence, this work explores how to boost instance-aware semantic mapping from object detection generated from foundation models. We propose a probabilistic label fusion method to predict close-set semantic classes from open-set label measurements. An instance refinement module merges the over-segmented instances caused by inconsistent segmentation. We integrate all the modules into a unified semantic mapping system. Reading a sequence of RGB-D input, our work incrementally reconstructs an instance-aware semantic map. We evaluate the zero-shot performance of our method in ScanNet and SceneNN datasets. Our method achieves 40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation task. It outperforms the traditional semantic mapping method significantly.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于监督对象检测器的语义映射对图像分布敏感。在现实环境中，对象检测和分割性能可能会导致大幅下降，从而阻碍了语义映射在更广泛的领域中的使用。另一方面，视觉语言基础模型的开发展示了跨数据分布的强大的零样本可迁移性。它提供了构建可概括的实例感知语义图的机会。因此，这项工作探讨了如何通过基础模型生成的对象检测来增强实例感知语义映射。我们提出了一种概率标签融合方法来从开放集标签测量中预测封闭集语义类。实例细化模块合并由于分割不一致而导致的过度分割实例。我们将所有模块集成到一个统一的语义映射系统中。读取 RGB-D 输入序列，我们的工作逐步重建实例感知语义图。我们评估了我们的方法在 ScanNet 和 SceneNN 数据集中的零样本性能。我们的方法在 ScanNet 语义实例分割任务上实现了 40.3 的平均精度 (mAP)。它显着优于传统的语义映射方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.04555v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision**<br />
**Title_cn:** BioDrone：基于仿生无人机的单目标跟踪基准，用于鲁棒视觉<br />
**Authors:** Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu, Rongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation. The robustness of SOT faces two main challenges: tiny target and fast motion. These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the first bionic drone-based visual benchmark for SOT. Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics. BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT. To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses. Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and insights for robust SOT. Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision. The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目标跟踪（SOT）是计算机视觉中的一个基本问题，具有广泛的应用，包括自动驾驶、增强现实和机器人导航。 SOT 的鲁棒性面临两个主要挑战：微小目标和快速运动。这些挑战在无人机 (UAV) 拍摄的视频中尤其明显，其中目标通常距离摄像机很远，并且通常相对于摄像机有显着的运动。为了评估 SOT 方法的稳健性，我们提出了 BioDrone——第一个基于仿生无人机的 SOT 视觉基准。与现有的无人机数据集不同，BioDrone 具有从扑翼无人机系统捕获的视频，由于其空气动力学特性，相机抖动较大。因此，BioDrone 强调跟踪连续帧之间发生剧烈变化的微小目标，为 SOT 提供新的稳健视觉基准。迄今为止，BioDrone 提供了最大的基于无人机的 SOT 基准，具有高质量的细粒度手动注释，并自动生成帧级标签，专为稳健的视觉分析而设计。利用我们提出的 BioDrone，我们对现有 SOT 方法进行了系统评估，比较了 20 个代表性模型的性能，并研究了优化 SOTA 方法（KeepTrack KeepTrack）以实现稳健 SOT 的新方法。我们的评估为稳健的 SOT 提供了新的基线和见解。展望未来，我们希望 BioDrone 不仅能够成为稳健 SOT 的高质量基准，而且还能引发未来对稳健计算机视觉的研究。数据库、工具包、评估服务器和基线结果可在 http://biodrone.aitestunion.com 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04519v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **A Psychological Study: Importance of Contrast and Luminance in Color to Grayscale Mapping**<br />
**Title_cn:** 心理学研究：颜色对比度和亮度对灰度映射的重要性<br />
**Authors:** Prasoon Ambalathankandy, Yafei Ou, Sae Kaneko, Masayuki Ikebe<br />
**Abstract:** <details><summary>原文: </summary>Grayscale images are essential in image processing and computer vision tasks. They effectively emphasize luminance and contrast, highlighting important visual features, while also being easily compatible with other algorithms. Moreover, their simplified representation makes them efficient for storage and transmission purposes. While preserving contrast is important for maintaining visual quality, other factors such as preserving information relevant to the specific application or task at hand may be more critical for achieving optimal performance. To evaluate and compare different decolorization algorithms, we designed a psychological experiment. During the experiment, participants were instructed to imagine color images in a hypothetical "colorless world" and select the grayscale image that best resembled their mental visualization. We conducted a comparison between two types of algorithms: (i) perceptual-based simple color space conversion algorithms, and (ii) spatial contrast-based algorithms, including iteration-based methods. Our experimental findings indicate that CIELAB exhibited superior performance on average, providing further evidence for the effectiveness of perception-based decolorization algorithms. On the other hand, the spatial contrast-based algorithms showed relatively poorer performance, possibly due to factors such as DC-offset and artificial contrast generation. However, these algorithms demonstrated shorter selection times. Notably, no single algorithm consistently outperformed the others across all test images. In this paper, we will delve into a comprehensive discussion on the significance of contrast and luminance in color-to-grayscale mapping based on our experimental results and analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>灰度图像在图像处理和计算机视觉任务中至关重要。它们有效地强调亮度和对比度，突出重要的视觉特征，同时也很容易与其他算法兼容。此外，它们的简化表示使它们能够有效地用于存储和传输目的。虽然保留对比度对于保持视觉质量很重要，但其他因素（例如保留与特定应用程序或手头任务相关的信息）对于实现最佳性能可能更为重要。为了评估和比较不同的脱色算法，我们设计了一个心理实验。在实验过程中，参与者被要求在假设的“无色世界”中想象彩色图像，并选择最接近他们心理想象的灰度图像。我们对两种类型的算法进行了比较：（i）基于感知的简单颜色空间转换算法，以及（ii）基于空间对比度的算法，包括基于迭代的方法。我们的实验结果表明，CIELAB 平均表现出优越的性能，为基于感知的脱色算法的有效性提供了进一步的证据。另一方面，基于空间对比度的算法表现出相对较差的性能，这可能是由于直流偏移和人工对比度生成等因素造成的。然而，这些算法表现出更短的选择时间。值得注意的是，没有任何一种算法在所有测试图像中始终优于其他算法。在本文中，我们将根据我们的实验结果和分析，深入探讨对比度和亮度在颜色到灰度映射中的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04583v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Dual-disentangled Deep Multiple Clustering**<br />
**Title_cn:** 双解纠缠深度多重聚类<br />
**Authors:** Jiawei Yao, Juhua Hu<br />
**Abstract:** <details><summary>原文: </summary>Multiple clustering has gathered significant attention in recent years due to its potential to reveal multiple hidden structures of the data from different perspectives. Most of multiple clustering methods first derive feature representations by controlling the dissimilarity among them, subsequently employing traditional clustering methods (e.g., k-means) to achieve the final multiple clustering outcomes. However, the learned feature representations can exhibit a weak relevance to the ultimate goal of distinct clustering. Moreover, these features are often not explicitly learned for the purpose of clustering. Therefore, in this paper, we propose a novel Dual-Disentangled deep Multiple Clustering method named DDMC by learning disentangled representations. Specifically, DDMC is achieved by a variational Expectation-Maximization (EM) framework. In the E-step, the disentanglement learning module employs coarse-grained and fine-grained disentangled representations to obtain a more diverse set of latent factors from the data. In the M-step, the cluster assignment module utilizes a cluster objective function to augment the effectiveness of the cluster output. Our extensive experiments demonstrate that DDMC consistently outperforms state-of-the-art methods across seven commonly used tasks. Our code is available at https://github.com/Alexander-Yao/DDMC.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，多重聚类因其从不同角度揭示数据的多个隐藏结构的潜力而引起了广泛关注。大多数多重聚类方法首先通过控制特征之间的不相似性来导出特征表示，然后采用传统的聚类方法（例如k-means）来实现最终的多重聚类结果。然而，学习到的特征表示与不同聚类的最终目标的相关性较弱。此外，这些特征通常不是为了聚类的目的而明确学习的。因此，在本文中，我们通过学习解缠结表示提出了一种新颖的双解缠结深度多重聚类方法，称为 DDMC。具体来说，DDMC 是通过变分期望最大化 (EM) 框架实现的。在E步骤中，解缠学习模块采用粗粒度和细粒度的解缠表示来从数据中获取更多样化的潜在因子集。在 M 步骤中，聚类分配模块利用聚类目标函数来增强聚类输出的有效性。我们的大量实验表明，DDMC 在七个常用任务中始终优于最先进的方法。我们的代码可在 https://github.com/Alexander-Yao/DDMC 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.05310v1><br />
**Code:** <https://github.com/alexander-yao/ddmc>**<br />
>>**index:** 2<br />
**Title:** **Image captioning for Brazilian Portuguese using GRIT model**<br />
**Title_cn:** 使用 GRIT 模型为巴西葡萄牙语制作图像字幕<br />
**Authors:** Rafael Silva de Alencar, William Alberto Cruz Castañeda, Marcellus Amadeus<br />
**Abstract:** <details><summary>原文: </summary>This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作展示了巴西葡萄牙语图像字幕模型的早期开发。我们使用 GRIT（基于网格和区域的图像字幕转换器）模型来完成这项工作。 GRIT 是一种仅限 Transformer 的神经架构，可有效利用两种视觉特征来生成更好的字幕。 GRIT 方法是作为一种更有效的图像字幕生成方法而出现的。在这项工作中，我们调整了 GRIT 模型，使其在巴西葡萄牙语数据集中进行训练，以获得巴西葡萄牙语的图像字幕方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05106v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction**<br />
**Title_cn:** 通过空间频率交互的双路径耦合图像去雨网络<br />
**Authors:** Yuhong He, Aiwen Jiang, Lingfang Jiang, Zhifeng Wang, Lu Wang<br />
**Abstract:** <details><summary>原文: </summary>Transformers have recently emerged as a significant force in the field of image deraining. Existing image deraining methods utilize extensive research on self-attention. Though showcasing impressive results, they tend to neglect critical frequency information, as self-attention is generally less adept at capturing high-frequency details. To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock). We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation. Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>变形金刚最近已成为图像去雨领域的重要力量。现有的图像去雨方法利用了对自注意力的广泛研究。尽管展示了令人印象深刻的结果，但它们往往忽略关键频率信息，因为自注意力通常不太擅长捕获高频细节。为了克服这个缺点，我们开发了一种创新的双路径耦合去雨网络（DPCNet），它通过空间特征提取块（SFEBlock）和频率特征提取块（FFEBlock）集成来自空间和频率域的信息。我们进一步引入了用于双路径特征聚合的有效自适应融合模块（AFM）。对六个公共除雨基准和下游视觉任务的广泛实验表明，我们提出的方法不仅优于现有最先进的除雨方法，而且在下游视觉任务上具有出色的鲁棒性，实现了视觉上令人愉悦的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.04855v1><br />
**Code:** <https://github.com/madeline-hyh/dpcnet>**<br />
>>**index:** 4<br />
**Title:** **Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer**<br />
**Title_cn:** Spiking-PhysFormer：基于相机的远程光电体积描记法，具有并行尖峰驱动变压器<br />
**Authors:** Mingxuan Liu, Jiankai Tang, Haoxiang Li, Jiahao Qi, Siwei Li, Kegang Wang, Yuntao Wang, Hong Chen<br />
**Abstract:** <details><summary>原文: </summary>Artificial neural networks (ANNs) can help camera-based remote photoplethysmography (rPPG) in measuring cardiac activity and physiological signals from facial videos, such as pulse wave, heart rate and respiration rate with better accuracy. However, most existing ANN-based methods require substantial computing resources, which poses challenges for effective deployment on mobile devices. Spiking neural networks (SNNs), on the other hand, hold immense potential for energy-efficient deep learning owing to their binary and event-driven architecture. To the best of our knowledge, we are the first to introduce SNNs into the realm of rPPG, proposing a hybrid neural network (HNN) model, the Spiking-PhysFormer, aimed at reducing power consumption. Specifically, the proposed Spiking-PhyFormer consists of an ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based predictor head. First, to simplify the transformer block while preserving its capacity to aggregate local and global spatio-temporal features, we design a parallel spike transformer block to replace sequential sub-blocks. Additionally, we propose a simplified spiking self-attention mechanism that omits the value parameter without compromising the model's performance. Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD demonstrate that the proposed model achieves a 12.4\% reduction in power consumption compared to PhysFormer. Additionally, the power consumption of the transformer block is reduced by a factor of 12.2, while maintaining decent performance as PhysFormer and other ANN-based models.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工神经网络 (ANN) 可以帮助基于摄像头的远程光电容积描记术 (rPPG) 更准确地测量心脏活动和面部视频中的生理信号，例如脉搏波、心率和呼吸率。然而，大多数现有的基于 ANN 的方法需要大量的计算资源，这给移动设备上的有效部署带来了挑战。另一方面，尖峰神经网络（SNN）由于其二进制和事件驱动的架构，在节能深度学习方面拥有巨大的潜力。据我们所知，我们是第一个将 SNN 引入 rPPG 领域的人，提出了一种混合神经网络 (HNN) 模型 Spiking-PhysFormer，旨在降低功耗。具体来说，所提出的 Spiking-PhyFormer 由基于 ANN 的补丁嵌入块、基于 SNN 的变换器块和基于 ANN 的预测器头组成。首先，为了简化变压器块，同时保留其聚合局部和全局时空特征的能力，我们设计了一个并行尖峰变压器块来代替顺序子块。此外，我们提出了一种简化的尖峰自注意力机制，该机制省略了值参数，而不会影响模型的性能。在 PURE、UBFC-rPPG、UBFC-Phys 和 MMPD 四个数据集上进行的实验表明，与 PhysFormer 相比，所提出的模型实现了 12.4% 的功耗降低。此外，变压器模块的功耗降低了 12.2 倍，同时保持了 PhysFormer 和其他基于 ANN 模型的良好性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.04798v2><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction**<br />
**Title_cn:** 基于人机交互社会运动预测的机器人交互行为生成<br />
**Authors:** Esteve Valls Mascaro, Yashuai Yan, Dongheui Lee<br />
**Abstract:** <details><summary>原文: </summary>Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands.</details>
**Abstract_cn:** <details><summary>译文: </summary>将机器人集成到人口稠密的环境中是一项复杂的挑战，需要了解人类的社会动态。在这项工作中，我们建议在共享的人机表示空间中对社交运动预测进行建模，这有助于我们合成在社交场景中与人类交互的机器人运动，尽管在运动训练中没有观察到任何机器人。我们开发了一种名为 ECHO 的基于 Transformer 的架构，它在上述共享空间中运行，以预测在社交场景中遇到的代理的未来运动。与之前的工作相反，我们将社会运动问题重新表述为基于周围代理对预测的个体运动进行细化，这有助于训练，同时允许在场景中只有一个人时进行单运动预测。我们在多人和人机运动预测任务中评估我们的模型，并在高效和实时执行的同时大幅获得最先进的性能。此外，我们的定性结果展示了我们的方法在生成可通过文本命令控制的人机交互行为方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04768v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Troublemaker Learning for Low-Light Image Enhancement**<br />
**Title_cn:** 低光图像增强的麻烦制造者学习<br />
**Authors:** Yinghao Song, Zhiyuan Cao, Wanhong Xiang, Sifan Long, Bo Yang, Hongwei Ge, Yanchun Liang, Chunguo Wu<br />
**Abstract:** <details><summary>原文: </summary>Low-light image enhancement (LLIE) restores the color and brightness of underexposed images. Supervised methods suffer from high costs in collecting low/normal-light image pairs. Unsupervised methods invest substantial effort in crafting complex loss functions. We address these two challenges through the proposed TroubleMaker Learning (TML) strategy, which employs normal-light images as inputs for training. TML is simple: we first dim the input and then increase its brightness. TML is based on two core components. First, the troublemaker model (TM) constructs pseudo low-light images from normal images to relieve the cost of pairwise data. Second, the predicting model (PM) enhances the brightness of pseudo low-light images. Additionally, we incorporate an enhancing model (EM) to further improve the visual performance of PM outputs. Moreover, in LLIE tasks, characterizing global element correlations is important because more information on the same object can be captured. CNN cannot achieve this well, and self-attention has high time complexity. Accordingly, we propose Global Dynamic Convolution (GDC) with O(n) time complexity, which essentially imitates the partial calculation process of self-attention to formulate elementwise correlations. Based on the GDC module, we build the UGDC model. Extensive quantitative and qualitative experiments demonstrate that UGDC trained with TML can achieve competitive performance against state-of-the-art approaches on public datasets. The code is available at https://github.com/Rainbowman0/TML_LLIE.</details>
**Abstract_cn:** <details><summary>译文: </summary>低光图像增强 (LLIE) 可恢复曝光不足图像的颜色和亮度。监督方法在收集低光/正常光图像对时成本很高。无监督方法投入了大量精力来构建复杂的损失函数。我们通过提出的 TroubleMaker Learning (TML) 策略来解决这两个挑战，该策略采用正常光图像作为训练的输入。 TML 很简单：我们首先调暗输入，然后增加其亮度。 TML 基于两个核心组件。首先，麻烦制造者模型（TM）从正常图像构建伪低光图像，以减轻成对数据的成本。其次，预测模型（PM）增强了伪低光图像的亮度。此外，我们还引入了增强模型 (EM)，以进一步提高 PM 输出的视觉性能。此外，在 LLIE 任务中，表征全局元素相关性很重要，因为可以捕获同一对象的更多信息。 CNN不能很好地实现这一点，而self-attention的时间复杂度很高。因此，我们提出了时间复杂度为 O(n) 的全局动态卷积（GDC），它本质上模仿了自注意力的部分计算过程来制定元素相关性。基于GDC模块，我们构建了UGDC模型。广泛的定量和定性实验表明，使用 TML 训练的 UGDC 可以在公共数据集上实现与最先进方法相比的竞争性能。代码可在 https://github.com/Rainbowman0/TML_LLIE 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04584v1><br />
**Code:** <https://github.com/rainbowman0/tml_llie>**<br />
>>**index:** 7<br />
**Title:** **Progressive Conservative Adaptation for Evolving Target Domains**<br />
**Title_cn:** 针对不断变化的目标领域的渐进保守适应<br />
**Authors:** Gangming Zhao, Chaoqi Chen, Wenhao He, Chengwei Pan, Chaowei Fang, Jinpeng Li, Xilin Chen, Yizhou Yu<br />
**Abstract:** <details><summary>原文: </summary>Conventional domain adaptation typically transfers knowledge from a source domain to a stationary target domain. However, in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions. Restoring and adapting to such target data results in escalating computational and resource consumption over time. Hence, it is vital to devise algorithms to address the evolving domain adaptation (EDA) problem, \emph{i.e.,} adapting models to evolving target domains without access to historic target domains. To achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda). To manage new target data that diverges from previous distributions, we fine-tune the classifier head based on the progressively updated class prototypes. Moreover, as adjusting to the most recent target domain can interfere with the features learned from previous target domains, we develop a conservative sparse attention mechanism. This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge. The proposed PCAda is implemented with a meta-learning framework, which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop. Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的域适应通常将知识从源域转移到固定目标域。然而，在许多现实世界的情况下，目标数据通常按顺序出现并具有不断变化的分布。随着时间的推移，恢复和适应此类目标数据会导致计算和资源消耗不断增加。因此，设计算法来解决不断发展的领域适应（EDA）问题至关重要，即在不访问历史目标领域的情况下使模型适应不断发展的目标领域。为了实现这一目标，我们提出了一种简单而有效的方法，称为渐进保守适应（PCAda）。为了管理与以前的分布不同的新目标数据，我们根据逐步更新的类原型对分类器头进行微调。此外，由于调整到最新的目标域可能会干扰从以前的目标域中学到的特征，因此我们开发了一种保守的稀疏注意力机制。这种机制将特征适应限制在基本维度内，从而简化了与历史知识相关的推理。所提出的PCAda采用元学习框架实现，在内循环中逐步更新类原型的帮助下实现分类器的快速适应，并通过保守的稀疏注意力在不严重干扰历史知识的情况下学习通用特征在外循环中。在旋转 MNIST、Caltran 和 Portraits 数据集上的实验证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04573v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion**<br />
**Title_cn:** DMAT：用于人体去遮挡的动态掩模感知变压器<br />
**Authors:** Guoqiang Liang, Jiahao Hu, Qingyue Wang, Shizhou Zhang<br />
**Abstract:** <details><summary>原文: </summary>Human de-occlusion, which aims to infer the appearance of invisible human parts from an occluded image, has great value in many human-related tasks, such as person re-id, and intention inference. To address this task, this paper proposes a dynamic mask-aware transformer (DMAT), which dynamically augments information from human regions and weakens that from occlusion. First, to enhance token representation, we design an expanded convolution head with enlarged kernels, which captures more local valid context and mitigates the influence of surrounding occlusion. To concentrate on the visible human parts, we propose a novel dynamic multi-head human-mask guided attention mechanism through integrating multiple masks, which can prevent the de-occluded regions from assimilating to the background. Besides, a region upsampling strategy is utilized to alleviate the impact of occlusion on interpolated images. During model learning, an amodal loss is developed to further emphasize the recovery effect of human regions, which also refines the model's convergence. Extensive experiments on the AHP dataset demonstrate its superior performance compared to recent state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>人体去遮挡旨在从遮挡图像中推断出不可见人体部位的外观，在许多与人类相关的任务中具有巨大价值，例如行人重新识别和意图推断。为了解决这一任务，本文提出了一种动态掩模感知变换器（DMAT），它动态地增强来自人体区域的信息并削弱来自遮挡的信息。首先，为了增强标记表示，我们设计了一个具有放大内核的扩展卷积头，它可以捕获更多的局部有效上下文并减轻周围遮挡的影响。为了专注于可见的人体部分，我们通过集成多个掩模提出了一种新颖的动态多头人类掩模引导注意机制，可以防止去遮挡区域同化到背景。此外，利用区域上采样策略来减轻遮挡对插值图像的影响。在模型学习过程中，开发了一种非模态损失以进一步强调人体区域的恢复效果，这也细化了模型的收敛性。对 AHP 数据集的大量实验表明，与最近最先进的方法相比，它具有优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.04558v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **V2VSSC: A 3D Semantic Scene Completion Benchmark for Perception with Vehicle to Vehicle Communication**<br />
**Title_cn:** V2VSSC：车对车通信感知的 3D 语义场景完成基准<br />
**Authors:** Yuanfang Zhang, Junxuan Li, Kaiqing Luo, Yiying Yang, Jiayi Han, Nian Liu, Denghui Qin, Peng Han, Chengpei Xu<br />
**Abstract:** <details><summary>原文: </summary>Semantic scene completion (SSC) has recently gained popularity because it can provide both semantic and geometric information that can be used directly for autonomous vehicle navigation. However, there are still challenges to overcome. SSC is often hampered by occlusion and short-range perception due to sensor limitations, which can pose safety risks. This paper proposes a fundamental solution to this problem by leveraging vehicle-to-vehicle (V2V) communication. We propose the first generalized collaborative SSC framework that allows autonomous vehicles to share sensing information from different sensor views to jointly perform SSC tasks. To validate the proposed framework, we further build V2VSSC, the first V2V SSC benchmark, on top of the large-scale V2V perception dataset OPV2V. Extensive experiments demonstrate that by leveraging V2V communication, the SSC performance can be increased by 8.3% on geometric metric IoU and 6.0% mIOU.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义场景完成（SSC）最近受到欢迎，因为它可以提供可直接用于自动车辆导航的语义和几何信息。然而，仍有一些挑战需要克服。由于传感器的限制，SSC 经常受到遮挡和短距离感知的阻碍，这可能会带来安全风险。本文通过利用车对车（V2V）通信提出了解决该问题的根本方案。我们提出了第一个通用协作 SSC 框架，允许自动驾驶车辆共享来自不同传感器视图的传感信息，以共同执行 SSC 任务。为了验证所提出的框架，我们在大规模 V2V 感知数据集 OPV2V 的基础上进一步构建了 V2VSSC，这是第一个 V2V SSC 基准。大量实验表明，通过利用 V2V 通信，SSC 性能可以在几何指标 IoU 和 6.0% mIOU 上提高 8.3%。</details>
**PDF:** <http://arxiv.org/pdf/2402.04671v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Review on Digital Pixel Sensors**<br />
**Title_cn:** 数字像素传感器综述<br />
**Authors:** Md Rahatul Islam Udoy, Shamiul Alam, Md Mazharul Islam, Akhilesh Jaiswal, Ahmedullah Aziz<br />
**Abstract:** <details><summary>原文: </summary>Digital pixel sensor (DPS) has evolved as a pivotal component in modern imaging systems and has the potential to revolutionize various fields such as medical imaging, astronomy, surveillance, IoT devices, etc. Compared to analog pixel sensors, the DPS offers high speed and good image quality. However, the introduced intrinsic complexity within each pixel, primarily attributed to the accommodation of the ADC circuit, engenders a substantial increase in the pixel pitch. Unfortunately, such a pronounced escalation in pixel pitch drastically undermines the feasibility of achieving high-density integration, which is an obstacle that significantly narrows down the field of potential applications. Nonetheless, designing compact conversion circuits along with strategic integration of 3D architectural paradigms can be a potential remedy to the prevailing situation. This review article presents a comprehensive overview of the vast area of DPS technology. The operating principles, advantages, and challenges of different types of DPS circuits have been analyzed. We categorize the schemes into several categories based on ADC operation. A comparative study based on different performance metrics has also been showcased for a well-rounded understanding.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字像素传感器 (DPS) 已发展成为现代成像系统的关键组件，具有彻底改变医学成像、天文学、监控、物联网设备等各个领域的潜力。与模拟像素传感器相比，DPS 提供高速和良好的图像质量。然而，每个像素内引入的固有复杂性（主要归因于 ADC 电路的调节）导致像素间距大幅增加。不幸的是，像素间距的显着增加极大地破坏了实现高密度集成的可行性，这是显着缩小潜在应用领域的障碍。尽管如此，设计紧凑的转换电路以及 3D 架构范例的战略集成可能是解决当前情况的潜在补救措施。这篇评论文章全面概述了 DPS 技术的广阔领域。分析了不同类型DPS电路的工作原理、优点和挑战。我们根据 ADC 操作将方案分为几类。还展示了基于不同性能指标的比较研究，以实现全面的理解。</details>
**PDF:** <http://arxiv.org/pdf/2402.04507v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT)**<br />
**Title_cn:** MIRT：四维计算机断层扫描 (4DCT) 的同时重建和仿射运动补偿技术<br />
**Authors:** Anh-Tuan Nguyen, Jens Renders, Domenico Iuso, Yves Maris, Jeroen Soete, Martine Wevers, Jan Sijbers, Jan De Beenhouwer<br />
**Abstract:** <details><summary>原文: </summary>In four-dimensional computed tomography (4DCT), 3D images of moving or deforming samples are reconstructed from a set of 2D projection images. Recent techniques for iterative motion-compensated reconstruction either necessitate a reference acquisition or alternate image reconstruction and motion estimation steps. In these methods, the motion estimation step involves the estimation of either complete deformation vector fields (DVFs) or a limited set of parameters corresponding to the affine motion, including rigid motion or scaling. The majority of these approaches rely on nested iterations, incurring significant computational expenses. Notably, despite the direct benefits of an analytical formulation and a substantial reduction in computational complexity, there has been no exploration into parameterizing DVFs for general affine motion in CT imaging. In this work, we propose the Motion-compensated Iterative Reconstruction Technique (MIRT)- an efficient iterative reconstruction scheme that combines image reconstruction and affine motion estimation in a single update step, based on the analytical gradients of the motion towards both the reconstruction and the affine motion parameters. When most of the state-of-the-art 4DCT methods have not attempted to be tested on real data, results from simulation and real experiments show that our method outperforms the state-of-the-art CT reconstruction with affine motion correction methods in computational feasibility and projection distance. In particular, this allows accurate reconstruction for a proper microscale diamond in the appearance of motion from the practically acquired projection radiographs, which leads to a novel application of 4DCT.</details>
**Abstract_cn:** <details><summary>译文: </summary>在四维计算机断层扫描 (4DCT) 中，移动或变形样本的 3D 图像是根据一组 2D 投影图像重建的。最近的迭代运动补偿重建技术要么需要参考采集，要么需要交替图像重建和运动估计步骤。在这些方法中，运动估计步骤涉及对完整变形矢量场（DVF）或与仿射运动相对应的有限参数集（包括刚性运动或缩放）的估计。这些方法大多数依赖于嵌套迭代，从而产生大量的计算费用。值得注意的是，尽管分析公式具有直接的好处并且计算复杂性大大降低，但尚未对 CT 成像中一般仿射运动的 DVF 参数化进行探索。在这项工作中，我们提出了运动补偿迭代重建技术（MIRT） - 一种有效的迭代重建方案，基于运动对重建和重建的解析梯度，将图像重建和仿射运动估计结合在单个更新步骤中。仿射运动参数。当大多数最先进的 4DCT 方法尚未尝试在实际数据上进行测试时，仿真和实际实验的结果表明，我们的方法在以下方面优于采用仿射运动校正方法的最先进的 CT 重建：计算可行性和投影距离。特别是，这使得可以从实际获取的投影射线照片中准确地重建适当的微型钻石的运动外观，这导致了 4DCT 的新颖应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.04480v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **RAGE for the Machine: Image Compression with Low-Cost Random Access for Embedded Applications**<br />
**Title_cn:** RAGE for the Machine：针对嵌入式应用的低成本随机访问图像压缩<br />
**Authors:** Christian D. Rask, Daniel E. Lucani<br />
**Abstract:** <details><summary>原文: </summary>We introduce RAGE, an image compression framework that achieves four generally conflicting objectives: 1) good compression for a wide variety of color images, 2) computationally efficient, fast decompression, 3) fast random access of images with pixel-level granularity without the need to decompress the entire image, 4) support for both lossless and lossy compression. To achieve these, we rely on the recent concept of generalized deduplication (GD), which is known to provide efficient lossless (de)compression and fast random access in time-series data, and deliver key expansions suitable for image compression, both lossless and lossy. Using nine different datasets, incl. graphics, logos, natural images, we show that RAGE has similar or better compression ratios to state-of-the-art lossless image compressors, while delivering pixel-level random access capabilities. Tests in an ARM Cortex-M33 platform show seek times between 9.9 and 40.6~ns and average decoding time per pixel between 274 and 1226~ns. Our measurements also show that RAGE's lossy variant, RAGE-Q, outperforms JPEG by several fold in terms of distortion in embedded graphics and has reasonable compression and distortion for natural images.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 RAGE，一个图像压缩框架，它实现了四个通常相互冲突的目标：1）对各种彩色图像进行良好的压缩，2）计算效率高，快速解压缩，3）快速随机访问具有像素级粒度的图像，而无需解压缩整个图像，4）支持无损和有损压缩。为了实现这些目标，我们依靠最新的广义重复数据删除（GD）概念，众所周知，它可以在时间序列数据中提供高效的无损（解）压缩和快速随机访问，并提供适合图像压缩（无损和压缩）的密钥扩展。有损。使用九个不同的数据集，包括。图形、徽标、自然图像，我们表明 RAGE 具有与最先进的无损图像压缩器相似或更好的压缩比，同时提供像素级随机访问功能。 ARM Cortex-M33 平台中的测试显示寻道时间在 9.9 到 40.6~ns 之间，每个像素的平均解码时间在 274 到 1226~ns 之间。我们的测量还表明，RAGE 的有损变体 RAGE-Q 在嵌入图形的失真方面比 JPEG 好几倍，并且对自然图像具有合理的压缩和失真。</details>
**PDF:** <http://arxiv.org/pdf/2402.05974v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning**<br />
**Title_cn:** 通过残差学习对水下图像进行物理知情和数据驱动的模拟<br />
**Authors:** Tanmoy Mondal, Ricardo Mendoza, Lucas Drumetz<br />
**Abstract:** <details><summary>原文: </summary>In general, underwater images suffer from color distortion and low contrast, because light is attenuated and backscattered as it propagates through water (differently depending on wavelength and on the properties of the water body). An existing simple degradation model (similar to atmospheric image "hazing" effects), though helpful, is not sufficient to properly represent the underwater image degradation because there are unaccounted for and non-measurable factors e.g. scattering of light due to turbidity of water, reflective characteristics of turbid medium etc. We propose a deep learning-based architecture to automatically simulate the underwater effects where only a dehazing-like image formation equation is known to the network, and the additional degradation due to the other unknown factors if inferred in a data-driven way. We only use RGB images (because in real-time scenario depth image is not available) to estimate the depth image. For testing, we have proposed (due to the lack of real underwater image datasets) a complex image formation model/equation to manually generate images that resemble real underwater images (used as ground truth). However, only the classical image formation equation (the one used for image dehazing) is informed to the network. This mimics the fact that in a real scenario, the physics are never completely known and only simplified models are known. Thanks to the ground truth, generated by a complex image formation equation, we could successfully perform a qualitative and quantitative evaluation of proposed technique, compared to other purely data driven approaches</details>
**Abstract_cn:** <details><summary>译文: </summary>一般来说，水下图像会出现颜色失真和对比度低的问题，因为光在水中传播时会衰减和反向散射（具体取决于波长和水体的特性）。现有的简单退化模型（类似于大气图像“雾化”效应）虽然有帮助，但不足以正确表示水下图像退化，因为存在无法解释和不可测量的因素，例如由于水的浑浊度、浑浊介质的反射特性等引起的光散射。我们提出了一种基于深度学习的架构来自动模拟水下效果，其中网络只知道类似去雾的图像形成方程，并且由于如果以数据驱动的方式推断出其他未知因素。我们只使用RGB图像（因为在实时场景中深度图像不可用）来估计深度图像。为了进行测试，我们提出了（由于缺乏真实的水下图像数据集）一个复杂的图像形成模型/方程来手动生成类似于真实水下图像（用作地面实况）的图像。然而，只有经典的图像形成方程（用于图像去雾的方程）被告知网络。这模仿了这样一个事实：在真实场景中，物理学永远不会完全已知，只有简化的模型是已知的。与其他纯数据驱动的方法相比，由于由复杂的图像形成方程生成的基本事实，我们可以成功地对所提出的技术进行定性和定量评估</details>
**PDF:** <http://arxiv.org/pdf/2402.05281v1><br />
**Code:** <https://github.com/anoynymreview/underwater_simulation>**<br />
>>**index:** 3<br />
**Title:** **A Survey on Domain Generalization for Medical Image Analysis**<br />
**Title_cn:** 医学图像分析领域泛化综述<br />
**Authors:** Ziwei Niu, Shuyi Ouyang, Shiao Xie, Yen-wei Chen, Lanfen Lin<br />
**Abstract:** <details><summary>原文: </summary>Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years. However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue. In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions. This paper presents the a comprehensive review of substantial developments in this area. First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings. Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints. Furthermore, we introduce the commonly used datasets. Finally, we summarize existing literature and present some potential research topics for the future. For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分析（MediA）已成为计算机辅助诊断系统中的重要工具，特别是随着近年来深度学习（DL）的进步。然而，训练有素的深度模型在部署到不同的医疗场所、模式和序列时通常会出现显着的性能下降，称为域转移问题。有鉴于此，MediA 的域泛化 (DG) 旨在通过在未知数据分布中有效泛化和稳健执行来解决域转移挑战。本文对该领域的重大进展进行了全面回顾。首先，我们提供了医学领域领域转移和领域泛化的正式定义，并讨论了几个相关的设置。随后，我们从数据操作层面、特征表示层面和模型训练层面三个角度总结了最新的方法，并针对每个观点详细介绍了一些算法。此外，我们还介绍了常用的数据集。最后，我们总结现有文献并提出未来一些潜在的研究主题。对于本次调查，我们还通过收集支持资源创建了一个 GitHub 项目，链接为：https://github.com/Ziwei-Niu/DG_for_MedIA</details>
**PDF:** <http://arxiv.org/pdf/2402.05035v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **4-Dimensional deformation part model for pose estimation using Kalman filter constraints**<br />
**Title_cn:** 使用卡尔曼滤波器约束进行位姿估计的 4 维变形零件模型<br />
**Authors:** Enrique Martinez-Berti, Antonio-Jose Sanchez-Salmeron, Carlos Ricolfe-Viala<br />
**Abstract:** <details><summary>原文: </summary>The main goal of this article is to analyze the effect on pose estimation accuracy when using a Kalman filter added to 4-dimensional deformation part model partial solutions. The experiments run with two data sets showing that this method improves pose estimation accuracy compared with state-of-the-art methods and that a Kalman filter helps to increase this accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文的主要目标是分析在 4 维变形部分模型部分解中添加卡尔曼滤波器时对位姿估计精度的影响。使用两个数据集进行的实验表明，与最先进的方法相比，该方法提高了姿态估计的准确性，并且卡尔曼滤波器有助于提高这种准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.04953v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Data-efficient Large Vision Models through Sequential Autoregression**<br />
**Title_cn:** 通过顺序自回归实现数据高效的大视觉模型<br />
**Authors:** Jianyuan Guo, Zhiwei Hao, Chengcheng Wang, Yehui Tang, Han Wu, Han Hu, Kai Han, Chang Xu<br />
**Abstract:** <details><summary>原文: </summary>Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding. These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks. However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens. In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset. We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase. Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models. The code is available at https://github.com/ggjy/DeLVM.</details>
**Abstract_cn:** <details><summary>译文: </summary>在纯粹的顺序视觉数据上训练通用视觉模型，避开语言输入，预示着视觉理解的新领域。这些模型不仅旨在理解而且无缝过渡到域外任务。然而，当前的努力因过度依赖庞大模型（例如具有 3B 以上参数的模型）以及需要大量视觉数据（通常包含惊人的 400B 标记）而受到阻碍。在本文中，我们深入研究了一种高效的、基于自回归的视觉模型的开发，该模型的创新性架构是在有限的数据集上运行。我们精心演示了该模型如何在测试阶段熟练地完成一系列涵盖高级和低级语义理解的视觉任务。我们的实证评估强调了该模型适应各种任务的敏捷性，预示着参数足迹的显着减少，以及训练数据需求的显着减少，从而为通用视觉模型领域更可持续和更容易的进步铺平了道路。代码可在 https://github.com/ggjy/DeLVM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.04841v1><br />
**Code:** <https://github.com/ggjy/delvm>**<br />
>>**index:** 6<br />
**Title:** **AINS: Affordable Indoor Navigation Solution via Line Color Identification Using Mono-Camera for Autonomous Vehicles**<br />
**Title_cn:** AINS：通过使用单摄像头进行线条颜色识别的经济实惠的室内导航解决方案，适用于自动驾驶汽车<br />
**Authors:** Nizamuddin Maitlo, Nooruddin Noonari, Kaleem Arshid, Naveed Ahmed, Sathishkumar Duraisamy<br />
**Abstract:** <details><summary>原文: </summary>Recently, researchers have been exploring various ways to improve the effectiveness and efficiency of autonomous vehicles by researching new methods, especially for indoor scenarios. Autonomous Vehicles in indoor navigation systems possess many challenges especially the limited accuracy of GPS in indoor scenarios. Several, robust methods have been explored for autonomous vehicles in indoor scenarios to solve this problem, but the ineffectiveness of the proposed methods is the high deployment cost. To address the above-mentioned problems we have presented A low-cost indoor navigation method for autonomous vehicles called Affordable Indoor Navigation Solution (AINS) which is based on based on Monocular Camera. Our proposed solution is mainly based on a mono camera without relying on various huge or power-inefficient sensors to find the path, such as range finders and other navigation sensors. Our proposed method shows that we can deploy autonomous vehicles indoor navigation systems while taking into consideration the cost. We can observe that the results shown by our solution are better than existing solutions and we can reduce the estimated error and time consumption.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，研究人员一直在通过研究新方法来探索各种方法来提高自动驾驶汽车的有效性和效率，特别是针对室内场景。室内导航系统中的自动驾驶车辆面临许多挑战，特别是室内场景中 GPS 的精度有限。人们已经针对室内场景中的自动驾驶汽车探索了几种鲁棒的方法来解决这个问题，但所提出的方法的无效性在于部署成本较高。为了解决上述问题，我们提出了一种基于单目摄像头的低成本自动驾驶汽车室内导航方法，称为经济型室内导航解决方案（AINS）。我们提出的解决方案主要基于单摄像头，而不依赖于各种巨大或低功耗的传感器来寻找路径，例如测距仪和其他导航传感器。我们提出的方法表明，我们可以在考虑成本的同时部署自动驾驶汽车室内导航系统。我们可以观察到，我们的解决方案显示的结果比现有解决方案更好，并且我们可以减少估计误差和时间消耗。</details>
**PDF:** <http://arxiv.org/pdf/2402.04750v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **The Influence of Autofocus Lenses in the Camera Calibration Process**<br />
**Title_cn:** 自动对焦镜头对相机标定过程的影响<br />
**Authors:** Carlos Ricolfe-Viala, Alicia Esparza<br />
**Abstract:** <details><summary>原文: </summary>Camera calibration is a crucial step in robotics and computer vision. Accurate camera parameters are necessary to achieve robust applications. Nowadays, camera calibration process consists of adjusting a set of data to a pin-hole model, assuming that with a reprojection error close to cero, camera parameters are correct. Since all camera parameters are unknown, computed results are considered true. However, the pin-hole model does not represent the camera behavior accurately if the focus is considered. Real cameras change the focal length slightly to obtain sharp objects in the image and this feature skews the calibration result if a unique pin-hole model is computed with a constant focal length. In this paper, a deep analysis of the camera calibration process is done to detect and strengthen its weaknesses. The camera is mounted in a robot arm to known extrinsic camera parameters with accuracy and to be able to compare computed results with the true ones. Based on the bias that exist between computed results and the true ones, a modification of the widely accepted camera calibration method using images of a planar template is presented. A pin-hole model with distance dependent focal length is proposed to improve the calibration process substantially</details>
**Abstract_cn:** <details><summary>译文: </summary>相机校准是机器人和计算机视觉的关键步骤。准确的相机参数对于实现稳健的应用是必要的。如今，相机标定过程包括将一组数据调整为针孔模型，假设重投影误差接近 cero，相机参数是正确的。由于所有相机参数都是未知的，因此计算结果被认为是真实的。然而，如果考虑焦点，针孔模型并不能准确地表示相机行为。真实相机会稍微改变焦距以获得图像中的清晰物体，如果使用恒定焦距计算独特的针孔模型，则此功能会扭曲校准结果。本文对相机标定过程进行了深入分析，以发现并加强其弱点。相机安装在机器人手臂上，准确已知相机的外部参数，并能够将计算结果与真实结果进行比较。基于计算结果与真实结果之间存在的偏差，提出了一种对广泛接受的使用平面模板图像的相机标定方法的修改。提出了一种具有距离依赖焦距的针孔模型，以显着改进校准过程</details>
**PDF:** <http://arxiv.org/pdf/2402.04686v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **An Over Complete Deep Learning Method for Inverse Problems**<br />
**Title_cn:** 一种逆问题的超完备深度学习方法<br />
**Authors:** Moshe Eliasof, Eldad Haber, Eran Treister<br />
**Abstract:** <details><summary>原文: </summary>Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems.</details>
**Abstract_cn:** <details><summary>译文: </summary>获得反问题的有意义的解决方案一直是科学和工程中许多应用的重大挑战。最近基于近端和扩散方法的机器学习技术已经显示出有希望的结果。然而，正如我们在这项工作中所展示的，当它们应用于一些典型问题时也可能面临挑战。我们表明，与之前关于超完备字典的工作类似，可以通过将解决方案嵌入到更高的维度来克服这些缺点。这项工作的新颖之处在于我们共同设计和学习嵌入向量的嵌入和正则化器。我们在几个典型和常见的逆问题上证明了这种方法的优点。</details>
**PDF:** <http://arxiv.org/pdf/2402.04653v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **BEBLID: Boosted efficient binary local image descriptor**<br />
**Title_cn:** BEBLID：提升高效的二进制局部图像描述符<br />
**Authors:** Iago Suárez, Ghesn Sfeir, José M. Buenaposada, Luis Baumela<br />
**Abstract:** <details><summary>原文: </summary>Efficient matching of local image features is a fundamental task in many computer vision applications. However, the real-time performance of top matching algorithms is compromised in computationally limited devices, such as mobile phones or drones, due to the simplicity of their hardware and their finite energy supply. In this paper we introduce BEBLID, an efficient learned binary image descriptor. It improves our previous real-valued descriptor, BELID, making it both more efficient for matching and more accurate. To this end we use AdaBoost with an improved weak-learner training scheme that produces better local descriptions. Further, we binarize our descriptor by forcing all weak-learners to have the same weight in the strong learner combination and train it in an unbalanced data set to address the asymmetries arising in matching and retrieval tasks. In our experiments BEBLID achieves an accuracy close to SIFT and better computational efficiency than ORB, the fastest algorithm in the literature.</details>
**Abstract_cn:** <details><summary>译文: </summary>局部图像特征的有效匹配是许多计算机视觉应用中的一项基本任务。然而，由于硬件的简单性和有限的能源供应，顶级匹配算法的实时性能在计算能力有限的设备（例如手机或无人机）中受到影响。在本文中，我们介绍了 BEBLID，一种高效的学习二值图像描述符。它改进了我们之前的实值描述符 BELID，使其匹配更高效、更准确。为此，我们使用 AdaBoost 和改进的弱学习器训练方案，以产生更好的局部描述。此外，我们通过强制所有弱学习器在强学习器组合中具有相同的权重来对描述符进行二值化，并在不平衡的数据集中对其进行训练，以解决匹配和​​检索任务中出现的不对称问题。在我们的实验中，BEBLID 实现了接近 SIFT 的精度，并且比文献中最快的算法 ORB 更好的计算效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.04482v1><br />
**Code:** <https://github.com/iago-suarez/beblid>**<br />

