## [UPDATED!] **2024-02-08** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models**<br />
**Title_cn:** CLR-Face：使用基于分数的扩散模型进行盲脸恢复的条件潜在细化<br />
**Authors:** Maitreya Suin, Rama Chellappa<br />
**Abstract:** <details><summary>原文: </summary>Recent generative-prior-based methods have shown promising blind face restoration performance. They usually project the degraded images to the latent space and then decode high-quality faces either by single-stage latent optimization or directly from the encoding. Generating fine-grained facial details faithful to inputs remains a challenging problem. Most existing methods produce either overly smooth outputs or alter the identity as they attempt to balance between generation and reconstruction. This may be attributed to the typical trade-off between quality and resolution in the latent space. If the latent space is highly compressed, the decoded output is more robust to degradations but shows worse fidelity. On the other hand, a more flexible latent space can capture intricate facial details better, but is extremely difficult to optimize for highly degraded faces using existing techniques. To address these issues, we introduce a diffusion-based-prior inside a VQGAN architecture that focuses on learning the distribution over uncorrupted latent embeddings. With such knowledge, we iteratively recover the clean embedding conditioning on the degraded counterpart. Furthermore, to ensure the reverse diffusion trajectory does not deviate from the underlying identity, we train a separate Identity Recovery Network and use its output to constrain the reverse diffusion process. Specifically, using a learnable latent mask, we add gradients from a face-recognition network to a subset of latent features that correlates with the finer identity-related details in the pixel space, leaving the other features untouched. Disentanglement between perception and fidelity in the latent space allows us to achieve the best of both worlds. We perform extensive evaluations on multiple real and synthetic datasets to validate the superiority of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的基于生成先验的方法已经显示出有希望的盲脸恢复性能。他们通常将降级图像投影到潜在空间，然后通过单阶段潜在优化或直接从编码解码高质量的面部。生成忠实于输入的细粒度面部细节仍然是一个具有挑战性的问题。大多数现有方法在尝试平衡生成和重建时要么产生​​过于平滑的输出，要么改变身份。这可能归因于潜在空间中质量和分辨率之间的典型权衡。如果潜在空间被高度压缩，则解码输出对降级更稳健，但保真度较差。另一方面，更灵活的潜在空间可以更好地捕捉复杂的面部细节，但使用现有技术很难针对高度退化的面部进行优化。为了解决这些问题，我们在 VQGAN 架构中引入了基于扩散的先验，该架构专注于学习未损坏的潜在嵌入的分布。有了这些知识，我们就可以迭代地在退化的对应物上恢复干净的嵌入条件。此外，为了确保反向扩散轨迹不偏离底层身份，我们训练一个单独的身份恢复网络并使用其输出来约束反向扩散过程。具体来说，使用可学习的潜在掩模，我们将来自面部识别网络的梯度添加到与像素空间中更精细的身份相关细节相关的潜在特征子集，而其他特征保持不变。潜在空间中感知和保真度之间的解开使我们能够实现两全其美。我们对多个真实和合成数据集进行了广泛的评估，以验证我们方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06106v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Animated Stickers: Bringing Stickers to Life with Video Diffusion**<br />
**Title_cn:** 动画贴纸：通过视频扩散让贴纸变得栩栩如生<br />
**Authors:** David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入动画贴纸，这是一种视频扩散模型，可根据文本提示和静态贴纸图像生成动画。我们的模型建立在最先进的 Emu 文本到图像模型之上，并添加了时间层来建模运动。由于域差距，即视觉和运动风格的差异，在生成自然视频方面表现良好的模型在应用于贴纸时无法再生成生动的视频。为了弥补这一差距，我们采用了两阶段的微调流程：首先使用弱域内数据，然后采用人机交互（HITL）策略，我们将其称为教师集合。它将多名教师的最佳品质提炼成更小的学生模型。我们表明，这种策略允许我们专门针对运动质量的改进，同时保持静态图像的风格。通过推理优化，我们的模型能够在一秒内生成具有高质量、有趣且相关运动的八帧视频。</details>
**PDF:** <http://arxiv.org/pdf/2402.06088v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **InstaGen: Enhancing Object Detection by Training on Synthetic Dataset**<br />
**Title_cn:** InstaGen：通过合成数据集训练增强目标检测<br />
**Authors:** Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们引入了一种新颖的范例来增强对象检测器的能力，例如，通过对扩散模型生成的合成数据集进行训练来扩展类别或提高检测性能。具体来说，我们将实例级接地头集成到预先训练的生成扩散模型中，以增强其在生成图像中定位任意实例的能力。接地头经过训练，将类别名称的文本嵌入与扩散模型的区域视觉特征对齐，使用现成的对象检测器的监督，以及针对未涵盖的（新颖）类别的新颖的自我训练方案探测器。这种扩散模型的增强版本被称为 InstaGen，可以用作目标检测的数据合成器。我们进行了彻底的实验来表明，在对 InstaGen 的合成数据集进行训练时可以增强对象检测器，在开放词汇 (+4.5 AP) 和数据稀疏 (+ 1.2 至 5.2 AP）场景。</details>
**PDF:** <http://arxiv.org/pdf/2402.05937v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Collaborative Control for Geometry-Conditioned PBR Image Generation**<br />
**Title_cn:** 几何条件 PBR 图像生成的协作控制<br />
**Authors:** Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, Simon Donné<br />
**Abstract:** <details><summary>原文: </summary>Current 3D content generation builds on generative models that output RGB images. Modern graphics pipelines, however, require physically-based rendering (PBR) material properties. We propose to model the PBR image distribution directly to avoid photometric inaccuracies in RGB generation and the inherent ambiguity in extracting PBR from RGB. Existing paradigms for cross-modal finetuning are not suited for PBR generation due to a lack of data and the high dimensionality of the output modalities: we overcome both challenges by retaining a frozen RGB model and tightly linking a newly trained PBR model using a novel cross-network communication paradigm. As the base RGB model is fully frozen, the proposed method does not risk catastrophic forgetting during finetuning and remains compatible with techniques such as IPAdapter pretrained for the base RGB model. We validate our design choices, robustness to data sparsity, and compare against existing paradigms with an extensive experimental section.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的 3D 内容生成基于输出 RGB 图像的生成模型。然而，现代图形管道需要基于物理的渲染 (PBR) 材质属性。我们建议直接对 PBR 图像分布进行建模，以避免 RGB 生成中的光度误差以及从 RGB 中提取 PBR 时固有的模糊性。由于缺乏数据和输出模态的高维度，现有的跨模态微调范例不适合 PBR 生成：我们通过保留冻结的 RGB 模型并使用新颖的交叉紧密链接新训练的 PBR 模型来克服这两个挑战-网络通​​信范式。由于基本 RGB 模型已完全冻结，因此所提出的方法不会在微调过程中面临灾难性遗忘的风险，并且与为基本 RGB 模型预训练的 IPAdapter 等技术保持兼容。我们验证我们的设计选择、对数据稀疏性的鲁棒性，并通过广泛的实验部分与现有范例进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.05919v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning**<br />
**Title_cn:** AvatarMMC：使用多模态调节生成和编辑 3D 头部头像<br />
**Authors:** Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka<br />
**Abstract:** <details><summary>原文: </summary>We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN. Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing. \\href{avatarmmc-sig24.github.io}{Project Page}</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍了一种基于 3D 生成对抗网络 (GAN) 和潜在扩散模型 (LDM) 的多模态调节的 3D 头部头像生成和编辑方法。 3D GAN 可以在单一条件或无条件的情况下生成高质量的头部头像。然而，生成符合不同模式的多种条件的样本具有挑战性。另一方面，LDM 擅长学习复杂的条件分布。为此，我们建议利用 LDM 的调节能力来实现对预训练 3D GAN 潜在空间的多模态控制。我们的方法可以在给定混合控制信号（例如 RGB 输入、分割蒙版和全局属性）的情况下生成和编辑 3D 头部头像。这可以更好地控制全局和本地合成化身的生成和编辑。实验表明，我们提出的方法在生成和编辑任务上在定性和定量上都优于仅基于 GAN 的方法。据我们所知，我们的方法是第一个将多模式调节引入 3D 头像生成和编辑的方法。 \\href{avatarmmc-sig24.github.io}{项目页面}</details>
**PDF:** <http://arxiv.org/pdf/2402.05803v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **CTGAN: Semantic-guided Conditional Texture Generator for 3D Shapes**<br />
**Title_cn:** CTGAN：语义引导的 3D 形状条件纹理生成器<br />
**Authors:** Yi-Ting Pan, Chai-Rong Lee, Shu-Ho Fan, Jheng-Wei Su, Jia-Bin Huang, Yung-Yu Chuang, Hung-Kuo Chu<br />
**Abstract:** <details><summary>原文: </summary>The entertainment industry relies on 3D visual content to create immersive experiences, but traditional methods for creating textured 3D models can be time-consuming and subjective. Generative networks such as StyleGAN have advanced image synthesis, but generating 3D objects with high-fidelity textures is still not well explored, and existing methods have limitations. We propose the Semantic-guided Conditional Texture Generator (CTGAN), producing high-quality textures for 3D shapes that are consistent with the viewing angle while respecting shape semantics. CTGAN utilizes the disentangled nature of StyleGAN to finely manipulate the input latent codes, enabling explicit control over both the style and structure of the generated textures. A coarse-to-fine encoder architecture is introduced to enhance control over the structure of the resulting textures via input segmentation. Experimental results show that CTGAN outperforms existing methods on multiple quality metrics and achieves state-of-the-art performance on texture generation in both conditional and unconditional settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>娱乐行业依靠 3D 视觉内容来创造身临其境的体验，但创建纹理 3D 模型的传统方法可能非常耗时且主观。 StyleGAN 等生成网络具有先进的图像合成功能，但生成具有高保真纹理的 3D 对象尚未得到很好的探索，并且现有方法存在局限性。我们提出了语义引导的条件纹理生成器 (CTGAN)，为 3D 形状生成与视角一致的高质量纹理，同时尊重形状语义。 CTGAN 利用 StyleGAN 的解缠结性质来精细地操纵输入潜在代码，从而能够显式控制生成的纹理的样式和结构。引入了从粗到精的编码器架构，以通过输入分割增强对所得纹理结构的控制。实验结果表明，CTGAN 在多个质量指标上均优于现有方法，并且在条件和无条件设置下的纹理生成方面均实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.05728v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer**<br />
**Title_cn:** DiffSpeaker：带有扩散变压器的语音驱动 3D 面部动画<br />
**Authors:** Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei<br />
**Abstract:** <details><summary>原文: </summary>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel.</details>
**Abstract_cn:** <details><summary>译文: </summary>语音驱动的 3D 面部动画对于许多多媒体应用都很重要。最近的工作显示出使用扩散模型或 Transformer 架构来完成此任务的前景。然而，它们的单纯聚合并不能提高性能。我们怀疑这是由于缺乏配对的音频 4D 数据，这对于 Transformer 在 Diffusion 框架内有效地充当降噪器至关重要。为了解决这个问题，我们提出了 DiffSpeaker，这是一个基于 Transformer 的网络，配备了新颖的有偏差条件注意模块。这些模块可以替代标准 Transformer 中的传统自注意力/交叉注意力，并结合精心设计的偏差，引导注意力机制集中于相关的特定任务和扩散相关条件。我们还探索了扩散范式中准确的嘴唇同步和非语言面部表情之间的权衡。实验表明，我们的模型不仅在现有基准上实现了最先进的性能，而且由于其并行生成面部运动的能力，推理速度也很快。</details>
**PDF:** <http://arxiv.org/pdf/2402.05712v1><br />
**Code:** <https://github.com/theericma/diffspeaker>**<br />
>>**index:** 8<br />
**Title:** **Scalable Diffusion Models with State Space Backbone**<br />
**Title_cn:** 具有状态空间主干的可扩展扩散模型<br />
**Authors:** Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: https://github.com/feizc/DiS.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文对基于状态空间架构的扩散模型类别进行了新的探索。我们致力于训练图像数据的扩散模型，其中传统的 U-Net 主干被状态空间主干取代，在原始补丁或潜在空间上运行。鉴于其在适应远程依赖性方面的显着功效，扩散状态空间模型（DiS）的特点是将所有输入（包括时间、条件和噪声图像块）视为标记。我们对 DiS 的评估涵盖无条件和类条件图像生成场景，表明 DiS 表现出与同等大小的基于 CNN 或基于 Transformer 的 U-Net 架构相当的性能（即使不是更优）。此外，我们分析了 DiS 的可扩展性，通过以 Gflops 量化的前向传递复杂度来衡量。通过增加深度/宽度或增加输入标记来实现具有更高 Gflops 的 DiS 模型始终表现出较低的 FID。除了表现出值得称赞的可扩展性特征之外，潜在空间中的 DiS-H/2 模型在类条件 ImageNet 基准测试上以 256$\times$256 和 512$\times$512 的分辨率实现了类似于先前扩散模型的性能水平，同时显着降低了计算负担。代码和模型可在以下网址获取：https://github.com/feizc/DiS。</details>
**PDF:** <http://arxiv.org/pdf/2402.05608v1><br />
**Code:** <https://github.com/feizc/dis>**<br />
>>**index:** 9<br />
**Title:** **Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs**<br />
**Title_cn:** 联合端到端图像压缩和去噪：利用对比学习和多尺度自 ONN<br />
**Authors:** Yuxin Xie, Li Yu, Farhad Pakdaman, Moncef Gabbouj<br />
**Abstract:** <details><summary>原文: </summary>Noisy images are a challenge to image compression algorithms due to the inherent difficulty of compressing noise. As noise cannot easily be discerned from image details, such as high-frequency signals, its presence leads to extra bits needed for compression. Since the emerging learned image compression paradigm enables end-to-end optimization of codecs, recent efforts were made to integrate denoising into the compression model, relying on clean image features to guide denoising. However, these methods exhibit suboptimal performance under high noise levels, lacking the capability to generalize across diverse noise types. In this paper, we propose a novel method integrating a multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint image compression and denoising. We employ contrastive learning to boost the network ability to differentiate noise from high frequency signal components, by emphasizing the correlation between noisy and clean counterparts. Experimental results demonstrate the effectiveness of the proposed method both in rate-distortion performance, and codec speed, outperforming the current state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于压缩噪声固有的困难，噪声图像对图像压缩算法来说是一个挑战。由于噪声无法轻易从图像细节（例如高频信号）中辨别出来，因此它的存在会导致压缩所需的额外比特。由于新兴的学习图像压缩范例能够实现编解码器的端到端优化，因此最近人们努力将去噪集成到压缩模型中，依靠干净的图像特征来指导去噪。然而，这些方法在高噪声水平下表现出次优性能，缺乏泛化不同噪声类型的能力。在本文中，我们提出了一种集成由自组织运算神经网络组成的多尺度降噪器的新方法，用于联合图像压缩和降噪。我们采用对比学习，通过强调噪声和干净对应部分之间的相关性，来增强网络区分噪声和高频信号分量的能力。实验结果证明了所提出的方法在率失真性能和编解码器速度方面的有效性，优于当前最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.05582v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application**<br />
**Title_cn:** Minecraft-ify：用于游戏内应用程序的 Minecraft 风格图像生成和文本引导图像编辑<br />
**Authors:** Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI Amin, Sanghyun Seo<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们首先提出了角色纹理生成系统\textit{Minecraft-ify}，专门用于Minecraft视频游戏的游戏内应用。我们的可以生成针对具有立方流形的 3D 虚拟角色的纹理映射的面部聚焦图像。虽然现有的项目或作品仅生成纹理，但所提出的系统可以反转用户提供的真实图像，或从学习的分布生成平均/随机外观。此外，还可以使用 StyleGAN 和 StyleCLIP 通过文本引导对其进行操作。作为用户友好的人工智能工具，这些功能提供了更广泛的用户体验和更大的自由度。项目页面可以在 https://gh-bumsookim.github.io/Minecraft-ify/ 找到</details>
**PDF:** <http://arxiv.org/pdf/2402.05448v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport**<br />
**Title_cn:** 通过不平衡最优传输进行生成建模的可扩展 Wasserstein 梯度流<br />
**Authors:** Jaemoo Choi, Jaewoong Choi, Myungjoo Kang<br />
**Abstract:** <details><summary>原文: </summary>Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>Wasserstein 梯度流 (WGF) 描述了 Wasserstein 空间内概率密度的梯度动态。 WGF 提供了一种有前途的方法来对概率分布进行优化。对连续 WGF 进行数值逼近需要时间离散化方法。最著名的方法是 JKO 方案。在这方面，以前的 WGF 模型采用 JKO 方案并对每个 JKO 步骤的传输图进行参数化。然而，这种方法会导致训练复杂度 $O(K^2)$ 与 JKO 步骤数 $K$ 呈二次方关系。这严重限制了 WGF 模型的可扩展性。在本文中，我们介绍了一种可扩展的基于 WGF 的生成模型，称为半对偶 JKO (S-JKO)。我们的模型基于 JKO 步骤的半对偶形式，源自 JKO 步骤和不平衡最优传输之间的等价性。我们的方法将训练复杂度降低到 $O(K)$。我们证明，我们的模型显着优于现有的基于 WGF 的生成模型，在 CIFAR-10 上达到 2.62 的 FID 分数，在 CelebA-HQ-256 上达到 6.19 的 FID 分数，这与最先进的图像生成模型相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.05443v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models**<br />
**Title_cn:** 得到你想要的，而不是你不想要的：文本到图像扩散模型的图像内容抑制<br />
**Authors:** Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang<br />
**Abstract:** <details><summary>原文: </summary>The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the prompt. In this paper, we analyze how to manipulate the text embeddings and remove unwanted content from them. We introduce two contributions, which we refer to as $\textit{soft-weighted regularization}$ and $\textit{inference-time text embedding optimization}$. The first regularizes the text embedding matrix and effectively suppresses the undesired content. The second method aims to further suppress the unwanted content generation of the prompt, and encourages the generation of desired content. We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness. Furthermore, our method is generalizability to both the pixel-space diffusion models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable Diffusion).</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的文本到图像扩散模型的成功很大程度上归功于它们能够由复杂的文本提示引导，这使得用户能够精确地描述所需的内容。然而，这些模型很难有效地抑制不需要的内容的生成，这些内容被明确要求从提示中生成的图像中省略。在本文中，我们分析了如何操作文本嵌入并从中删除不需要的内容。我们引入两个贡献，我们将其称为 $\textit{软加权正则化}$ 和 $\textit{推理时文本嵌入优化}$。第一个规则化文本嵌入矩阵并有效抑制不需要的内容。第二种方法旨在进一步抑制提示中不想要的内容的产生，并鼓励想要的内容的产生。我们通过大量实验对我们的方法进行定量和定性评估，验证其有效性。此外，我们的方法可推广到像素空间扩散模型（即 DeepFloyd-IF）和潜在空间扩散模型（即稳定扩散）。</details>
**PDF:** <http://arxiv.org/pdf/2402.05375v1><br />
**Code:** <https://github.com/sen-mao/suppresseot>**<br />
>>**index:** 13<br />
**Title:** **Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model**<br />
**Title_cn:** 反扫描：使用色彩校正扩散模型从扫描图像到原始图像<br />
**Authors:** Junghun Cha, Ali Haider, Seoyun Yang, Hoeyeong Jin, Subin Yang, A. F. M. Shahab Uddin, Jaehyoung Kim, Soo Ye Kim, Sung-Ho Bae<br />
**Abstract:** <details><summary>原文: </summary>A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world. However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world. Although restoring high-quality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available. In this paper, we define this problem as Descanning and introduce a new high-quality and large-scale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations. In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion probabilistic model (DDPM) that removes local degradations. To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images. We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses.</details>
**Abstract_cn:** <details><summary>译文: </summary>大量的模拟信息（即文档和图像）已以扫描副本的形式数字化，以便在数字世界中存储、共享和/或分析。然而，由于物理世界中的打印、存储和扫描过程造成的各种失真，这些内容的质量严重下降。尽管从扫描副本中恢复高质量内容已成为许多产品不可或缺的任务，但尚未对其进行系统性探索，并且据我们所知，没有可用的公共数据集。在本文中，我们将这个问题定义为反扫描，并引入一个新的高质量、大规模数据集，名为 DESCAN-18K。它包含 18K 对在野外收集的原始图像和扫描图像，其中包含多种复杂的退化。为了消除这种复杂的退化，我们提出了一种称为 DescanDiffusion 的新图像恢复模型，由校正全局颜色退化的颜色编码器和消除局部退化的条件去噪扩散概率模型（DDPM）组成。为了进一步提高 DescanDiffusion 的泛化能力，我们还通过再现扫描图像中的显着退化来设计合成数据生成方案。通过全面的实验和分析，我们证明我们的 DescanDiffusion 在客观和主观上优于其他基准，包括商业修复产品。</details>
**PDF:** <http://arxiv.org/pdf/2402.05350v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps**<br />
**Title_cn:** CLIP-Loc：基于对象的地图中全球定位的多模式地标协会<br />
**Authors:** Shigemichi Matsuzaki, Takuma Sugino, Kazuhito Tanaka, Zijun Sha, Shintaro Nakaoka, Shintaro Yoshizawa, Kazuhiro Shintani<br />
**Abstract:** <details><summary>原文: </summary>This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文描述了一种使用基于对象的地图和相机图像进行全局定位的多模态数据关联方法。在使用基于对象的地图的全局定位或重新定位中，现有方法通常诉诸于将检测到的对象和具有相同对象类别的地标的所有可能组合进行匹配，然后使用 RANSAC 或强力搜索进行内点提取。由于对应候选的指数增长而导致地标数量增加，这种方法变得不可行。在本文中，我们建议用自然语言描述来标记地标，并使用视觉语言模型（VLM）根据图像观察的概念相似性来提取对应关系。与仅使用对象类别的方法相比，通过利用详细的文本信息，我们的方法可以有效地提取对应关系。通过实验，我们证明所提出的方法与基线方法相比能够以更少的迭代次数实现更准确的全局定位，展示了其效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.06092v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models**<br />
**Title_cn:** SPHINX-X：缩放一系列多模态大型语言模型的数据和参数<br />
**Authors:** Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出 SPHINX-X，这是一个在 SPHINX 上开发的广泛的多模态大语言模型 (MLLM) 系列。为了提高架构和训练效率，我们修改了 SPHINX 框架，删除了冗余的视觉编码器，绕过带有跳过标记的完全填充的子图像，并将多阶段训练简化为单阶段一体化范例。为了充分释放 MLLM 的潜力，我们构建了一个全面的多领域和多模式数据集，涵盖语言、视觉和视觉语言任务方面的公开可用资源。我们通过精心策划的 OCR 密集型和标记集数据集进一步丰富了这个集合，扩展了多样性和通用性。通过对不同的基础 LLM（包括 TinyLlama1.1B、InternLM2-7B、LLaMA2-13B 和 Mixtral8x7B）进行训练，我们获得了参数大小和多语言功能各不相同的一系列 MLLM。全面的基准测试揭示了多模态性能与数据和参数尺度之间的强相关性。代码和模型发布于 https://github.com/Alpha-VLLM/LLaMA2-Accessory</details>
**PDF:** <http://arxiv.org/pdf/2402.05935v1><br />
**Code:** <https://github.com/alpha-vllm/llama2-accessory>**<br />
>>**index:** 3<br />
**Title:** **WebLINX: Real-World Website Navigation with Multi-Turn Dialogue**<br />
**Title_cn:** WebLINX：具有多轮对话的真实世界网站导航<br />
**Authors:** Xing Han Lù, Zdeněk Kasner, Siva Reddy<br />
**Abstract:** <details><summary>原文: </summary>We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了会话式网络导航问题，其中数字代理控制网络浏览器并遵循用户指令以多轮对话方式解决现实世界的任务。为了解决这个问题，我们引入了 WEBLINX - 一个跨 2300 个对话式 Web 导航专家演示的 100K 交互的大规模基准。我们的基准测试涵盖了 150 多个现实世界网站上的广泛模式，可用于在不同场景中训练和评估代理。由于存在的信息量很大，大型语言模型 (LLM) 无法实时处理整个网页。为了解决这个瓶颈，我们设计了一个受检索启发的模型，通过对相关元素进行排名来有效地修剪 HTML 页面。我们使用选定的元素以及屏幕截图和操作历史记录来评估各种模型在浏览网络时复制人类行为的能力。我们的实验涵盖小型纯文本到专有的多模式法学硕士。我们发现较小的微调解码器超越了最好的零样本 LLM（包括 GPT-4V），但也超越了在屏幕截图上明确预训练的较大微调多模态模型。然而，所有经过微调的模型都很难推广到看不见的网站。我们的研究结果强调了对可以推广到新环境的大型多模式模型的需求。我们的代码、数据和模型可供研究：https://mcgill-nlp.github.io/weblinx</details>
**PDF:** <http://arxiv.org/pdf/2402.05930v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion**<br />
**Title_cn:** CREMA：通过高效模块化适应和融合进行多模态合成视频推理<br />
**Authors:** Shoubin Yu, Jaehong Yoon, Mohit Bansal<br />
**Abstract:** <details><summary>原文: </summary>Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additional modalities. We validate our method on video-3D, video-audio, and video-language reasoning tasks and achieve better/equivalent performance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and SeViLA while using 96% fewer trainable parameters. We provide extensive analyses of CREMA, including the impact of each modality on reasoning domains, the design of the fusion module, and example visualizations.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管多模态组合推理方法取得了令人印象深刻的进步，但由于在更新大量模型参数的同时处理固定模态输入，它们的灵活性和效率仍然受到限制。本文解决了这些关键挑战，并提出了 CREMA，这是一种高效的模块化模态融合框架，用于将任何新模态注入视频推理中。我们首先利用现有的预训练模型，从给定视频中增强多种信息模式（例如光流、3D 点云、音频），而无需额外的人工注释。接下来，我们引入一个查询转换器，它具有与每个可访问模态关联的多个参数高效模块。它将不同的模态特征投射到 LLM 令牌嵌入空间，允许模型集成不同的数据类型以生成响应。此外，我们提出了一个融合模块，旨在压缩多模态查询，在结合其他模态的同时保持法学硕士的计算效率。我们在视频 3D、视频音频和视频语言推理任务上验证了我们的方法，并针对强大的多模态 LLM（包括 BLIP-2、3D-LLM 和 SeViLA）实现了更好/同等的性能，同时使用的可训练参数减少了 96%。我们提供对 CREMA 的广泛分析，包括每种模态对推理领域的影响、融合模块的设计和示例可视化。</details>
**PDF:** <http://arxiv.org/pdf/2402.05889v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting**<br />
**Title_cn:** FusionSF：在矢量量化框架中融合异构模式以实现稳健的太阳能预测<br />
**Authors:** Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin<br />
**Abstract:** <details><summary>原文: </summary>Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的太阳能功率预测对于光伏电站并网、调度和保障电网安全至关重要。对于那些缺乏足够数据的新安装太阳能电站来说，这个问题变得更加困难。目前的研究主要依赖于单一模态格式的历史太阳能数据或数值天气预报，忽略了不同模态提供的补充信息。在本文中，我们提出了一种多模态融合框架，将历史电力数据、数值天气预报和卫星图像集成在一起，显着提高了预报性能。我们引入了一个矢量量化框架，该框架将模态与不同的信息密度对齐，在集成足够的信息和避免模型过度拟合之间取得平衡。我们的框架展示了强大的零样本预测能力，这对于那些新安装的工厂特别有用。此外，我们收集并发布了来自现实世界植物的多模态太阳能（MMSP）数据集，以进一步推动多模态太阳能预测算法的研究。我们广泛的实验表明，我们的模型不仅具有鲁棒性，而且还提高了零样本预测和训练数据丰富的场景的准确性，超越了领先的模型。我们已将其纳入 eForecaster 平台，并将其部署到 300 多个太阳能发电厂，容量超过 15GW。</details>
**PDF:** <http://arxiv.org/pdf/2402.05823v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Question Aware Vision Transformer for Multimodal Reasoning**<br />
**Title_cn:** 用于多模态推理的问题感知视觉转换器<br />
**Authors:** Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言（VL）模型已经获得了重要的研究焦点，使得多模态推理取得了显着的进步。这些架构通常包含视觉编码器、大型语言模型 (LLM) 以及将视觉特征与 LLM 表示空间对齐的投影模块。尽管取得了成功，但一个关键的限制仍然存在：视觉编码过程仍然与用户查询脱钩，通常以图像相关问题的形式出现。因此，所得到的视觉特征可能无法最佳地适应图像的特定于查询的元素。为了解决这个问题，我们引入了 QA-ViT，一种用于多模态推理的问题感知视觉变换器方法，它将问题感知直接嵌入到视觉编码器中。这种整合导致动态视觉特征集中于所提出问题的相关图像方面。 QA-ViT 与模型无关，可以有效地合并到任何 VL 架构中。大量的实验证明了将我们的方法应用于各种多模式架构的有效性，从而导致不同任务的持续改进，并展示了其增强视觉和场景文本理解的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.05472v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network**<br />
**Title_cn:** MTSA-SNN：基于尖峰神经网络的多模态时间序列分析模型<br />
**Authors:** Chengzhi Liu, Chong Zhong, Mingyu Jin, Zheng Tao, Zihong Luo, Chenghao Liu, Shuliang Zhao<br />
**Abstract:** <details><summary>原文: </summary>Time series analysis and modelling constitute a crucial research area. Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data. To address these challenges, we propose a Multi-modal Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation. The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from multi-modal pulse signals complementary. Additionally, we incorporate wavelet transform operations to enhance the model's ability to analyze and evaluate temporal information. Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks. This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information. Access to the source code is available at https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN</details>
**Abstract_cn:** <details><summary>译文: </summary>时间序列分析和建模构成了一个重要的研究领域。由于计算复杂度高、捕获时间信息的能力有限以及处理事件驱动数据的困难，传统的人工神经网络难以处理复杂的非平稳时间序列数据。为了应对这些挑战，我们提出了基于尖峰神经网络的多模态时间序列分析模型（MTSA-SNN）。脉冲编码器将时间图像和顺序信息的编码统一在基于脉冲的通用表示中。联合学习模块采用联合学习功能和权重分配机制来融合来自互补的多模态脉冲信号的信息。此外，我们还结合了小波变换运算来增强模型分析和评估时间信息的能力。实验结果表明，我们的方法在三个复杂的时间序列任务上取得了优异的性能。这项工作提供了一种有效的事件驱动方法来克服与分析复杂的时间信息相关的挑战。可以通过以下网址访问源代码：https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN</details>
**PDF:** <http://arxiv.org/pdf/2402.05423v1><br />
**Code:** <https://github.com/chenngzz/mtsa-snn>**<br />
>>**index:** 8<br />
**Title:** **Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey**<br />
**Title_cn:** 知识图满足多模态学习：综合调查<br />
**Authors:** Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识图谱 (KG) 在推进各种人工智能应用方面发挥着关键作用，语义网络社区对多模态维度的探索开启了新的创新途径。在本次调查中，我们仔细回顾了 300 多篇文章，重点关注两个主要方面的知识图谱感知研究：知识图谱驱动的多模态（KG4MM）学习，其中知识图谱支持多模态任务，以及多模态知识图谱（MM4KG） ，它将 KG 研究扩展到 MMKG 领域。我们首先定义 KG 和 MMKG，然后探讨它们的构建进度。我们的综述包括两个主要任务类别：KG 感知的多模态学习任务，例如图像分类和视觉问答，以及内在的 MMKG 任务，例如多模态知识图补全和实体对齐，突出了具体的研究轨迹。对于大多数此类任务，我们提供了定义、评估基准，并另外概述了进行相关研究的基本见解。最后，我们讨论当前的挑战并确定新兴趋势，例如大语言建模和多模式预训练策略的进展。本调查旨在为已经参与或考虑深入研究 KG 和多模态学习研究的研究人员提供全面的参考，为 MMKG 研究不断发展的前景提供见解并支持未来的工作。</details>
**PDF:** <http://arxiv.org/pdf/2402.05391v2><br />
**Code:** <https://github.com/zjukg/kg-mm-survey>**<br />
>>**index:** 9<br />
**Title:** **CIC: A framework for Culturally-aware Image Captioning**<br />
**Title_cn:** CIC：具有文化意识的图像字幕框架<br />
**Authors:** Youngsik Yun, Jihie Kim<br />
**Abstract:** <details><summary>原文: </summary>Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 different cultural groups with a high understanding of the corresponding culture shows that our proposed framework generates more culturally descriptive captions when compared to the image captioning baseline based on VLPs. Our code and dataset will be made publicly available upon acceptance.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像字幕使用 BLIP 等视觉语言预训练模型 (VLP) 从图像生成描述性句子，该模型已得到很大改进。然而，当前的方法缺乏为图像中描绘的文化元素生成详细的描述性说明，例如亚洲文化群体的人们所穿的传统服装。在本文中，我们提出了一个新的框架，\textbf{文化感知图像字幕（CIC）}，它生成字幕并描述从代表文化的图像中的文化视觉元素中提取的文化元素。受到通过适当的提示将视觉模态和大语言模型（LLM）相结合的方法的启发，我们的框架（1）根据图像的文化类别生成问题，（2）使用生成的问题从视觉问答（VQA）中提取文化视觉元素，以及(3) 使用法学硕士和提示生成具有文化意识的字幕。我们对来自 4 个不同文化群体的 45 名参与者进行了人类评估，这些参与者对相应文化有高度的了解，结果表明，与基于 VLP 的图像字幕基线相比，我们提出的框架生成了更具文化描述性的字幕。我们的代码和数据集将在接受后公开发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.05374v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery**<br />
**Title_cn:** 用于机器人手术的隐私保护综合连续语义分割<br />
**Authors:** Mengya Xu, Mobarakol Islam, Long Bai, Hongliang Ren<br />
**Abstract:** <details><summary>原文: </summary>Deep Neural Networks (DNNs) based semantic segmentation of the robotic instruments and tissues can enhance the precision of surgical activities in robot-assisted surgery. However, in biological learning, DNNs cannot learn incremental tasks over time and exhibit catastrophic forgetting, which refers to the sharp decline in performance on previously learned tasks after learning a new one. Specifically, when data scarcity is the issue, the model shows a rapid drop in performance on previously learned instruments after learning new data with new instruments. The problem becomes worse when it limits releasing the dataset of the old instruments for the old model due to privacy concerns and the unavailability of the data for the new or updated version of the instruments for the continual learning model. For this purpose, we develop a privacy-preserving synthetic continual semantic segmentation framework by blending and harmonizing (i) open-source old instruments foreground to the synthesized background without revealing real patient data in public and (ii) new instruments foreground to extensively augmented real background. To boost the balanced logit distillation from the old model to the continual learning model, we design overlapping class-aware temperature normalization (CAT) by controlling model learning utility. We also introduce multi-scale shifted-feature distillation (SD) to maintain long and short-range spatial relationships among the semantic objects where conventional short-range spatial features with limited information reduce the power of feature distillation. We demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018 instrument segmentation dataset with a generalized continual learning setting. Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度神经网络（DNN）的机器人器械和组织语义分割可以提高机器人辅助手术中手术活动的精度。然而，在生物学习中，DNN 无法随着时间的推移学习增量任务，并表现出灾难性遗忘，这是指在学习新任务后，先前学习的任务的性能急剧下降。具体来说，当数据稀缺成为问题时，该模型在使用新仪器学习新数据后，在之前学习的仪器上表现出性能迅速下降。当由于隐私问题以及持续学习模型的新版本或更新版本的工具的数据不可用而限制发布旧模型的旧工具的数据集时，问题会变得更糟。为此，我们开发了一个保护隐私的合成连续语义分割框架，通过混合和协调（i）开源旧仪器前景到合成背景，而不在公共场合透露真实的患者数据，以及（ii）新仪器前景到广泛增强的真实数据背景。为了促进从旧模型到持续学习模型的平衡逻辑蒸馏，我们通过控制模型学习效用来设计重叠的类感知温度归一化（CAT）。我们还引入了多尺度移位特征蒸馏（SD）来维持语义对象之间的长程和短程空间关系，其中信息有限的传统短程空间特征降低了特征蒸馏的能力。我们通过广义持续学习设置在 EndoVis 2017 和 2018 仪器分割数据集上展示了我们的框架的有效性。代码可在~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.05860v1><br />
**Code:** <https://github.com/xumengyaamy/synthetic_cat_sd>**<br />
>>**index:** 2<br />
**Title:** **Flashback: Understanding and Mitigating Forgetting in Federated Learning**<br />
**Title_cn:** 闪回：理解和减轻联邦学习中的遗忘<br />
**Authors:** Mohammed Aljahdali, Ahmed M. Abdelmoniem, Marco Canini, Samuel Horváth<br />
**Abstract:** <details><summary>原文: </summary>In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.</details>
**Abstract_cn:** <details><summary>译文: </summary>在联邦学习（FL）中，遗忘或跨轮知识丢失会阻碍算法收敛，特别是在客户端之间存在严重数据异质性的情况下。本研究探讨了这个问题的细微差别，强调了遗忘在异构数据环境下 FL 低效学习中的关键作用。知识丢失发生在客户端本地更新和服务器端聚合步骤中；只顾其中之一并不能减少遗忘。我们引入了一种衡量遗忘的指标，以确保在新知识获取过程中的清晰识别。利用这些见解，我们提出了 Flashback，这是一种采用动态蒸馏方法的 FL 算法，用于规范局部模型，并有效地聚合它们的知识。在不同的基准测试中，Flashback 优于其他方法，通过 6 到 16 轮收敛，减少了遗忘，并实现了更快的轮到目标准确度。</details>
**PDF:** <http://arxiv.org/pdf/2402.05558v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Early Fusion of Features for Semantic Segmentation**<br />
**Title_cn:** 语义分割的早期特征融合<br />
**Authors:** Anupam Gupta, Ashok Krishnamurthy, Lisa Singh<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a novel segmentation framework that integrates a classifier network with a reverse HRNet architecture for efficient image segmentation. Our approach utilizes a ResNet-50 backbone, pretrained in a semi-supervised manner, to generate feature maps at various scales. These maps are then processed by a reverse HRNet, which is adapted to handle varying channel dimensions through 1x1 convolutions, to produce the final segmentation output. We strategically avoid fine-tuning the backbone network to minimize memory consumption during training. Our methodology is rigorously tested across several benchmark datasets including Mapillary Vistas, Cityscapes, CamVid, COCO, and PASCAL-VOC2012, employing metrics such as pixel accuracy and mean Intersection over Union (mIoU) to evaluate segmentation performance. The results demonstrate the effectiveness of our proposed model in achieving high segmentation accuracy, indicating its potential for various applications in image analysis. By leveraging the strengths of both the ResNet-50 and reverse HRNet within a unified framework, we present a robust solution to the challenges of image segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种新颖的分割框架，该框架将分类器网络与反向 HRNet 架构集成在一起，以实现高效的图像分割。我们的方法利用以半监督方式预训练的 ResNet-50 主干网络来生成各种尺度的特征图。然后，这些图由反向 HRNet 进行处理，该 HRNet 适合通过 1x1 卷积处理不同的通道维度，以产生最终的分割输出。我们策略性地避免微调主干网络，以最大限度地减少训练期间的内存消耗。我们的方法在多个基准数据集（包括 Mapillary Vistas、Cityscapes、CamVid、COCO 和 PASCAL-VOC2012）上进行了严格测试，采用像素精度和平均交集 (mIoU) 等指标来评估分割性能。结果证明了我们提出的模型在实现高分割精度方面的有效性，表明其在图像分析中的各种应用的潜力。通过在统一框架内利用 ResNet-50 和反向 HRNet 的优势，我们为图像分割的挑战提供了一个强大的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.06091v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing**<br />
**Title_cn:** 探索 GPT-4V 中的视觉文化意识：全面的探索<br />
**Authors:** Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan, Daniel Hershcovich<br />
**Abstract:** <details><summary>原文: </summary>Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original MaRVL human annotations, suggesting a promising solution for future visual cultural benchmark construction.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，预训练的大型视觉语言模型因其卓越的性能而引起了人们的极大兴趣。尽管从不同角度评估这些模型付出了相当大的努力，但最先进的 GPT-4V 模型中视觉文化意识的程度仍未得到探索。为了解决这一差距，我们使用 MaRVL 基准数据集广泛探讨了 GPT-4V，旨在研究其在视觉理解方面的能力和局限性，重点关注文化方面。具体来说，我们引入了三个视觉相关任务，即字幕分类、成对字幕和文化标签选择，以系统地深入研究细粒度的视觉文化评估。实验结果表明，GPT-4V 在识别文化概念方面表现出色，但在泰米尔语和斯瓦希里语等低资源语言中表现仍然较弱。值得注意的是，通过人类评估，GPT-4V 被证明在图像字幕任务中比原始 MaRVL 人类注释更具文化相关性，这为未来视觉文化基准构建提供了一个有前途的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.06015v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Point-VOS: Pointing Up Video Object Segmentation**<br />
**Title_cn:** Point-VOS：指向上方视频对象分割<br />
**Authors:** Idil Esen Zulfikar, Sabarinath Mahadevan, Paul Voigtlaender, Bastian Leibe<br />
**Abstract:** <details><summary>原文: </summary>Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task. We will make our code and annotations available at https://pointvos.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前最先进的视频对象分割（VOS）方法在训练和测试期间都依赖于密集的每个对象掩模注释。这需要耗时且昂贵的视频注释机制。我们提出了一种新颖的 Point-VOS 任务，具有时空稀疏逐点注释方案，可大大减少注释工作。我们将标注方案应用于两个带有文本描述的大型视频数据集，并在 32K 视频中的 133K 对象上标注了超过 1900 万个点。根据我们的注释，我们提出了一个新的 Point-VOS 基准，以及相应的基于点的训练机制，我们用它来建立强大的基线结果。我们表明，现有的 VOS 方法可以轻松地适应在训练期间利用我们的点注释，并且在对这些点生成的伪掩模进行训练时可以实现接近完全监督性能的结果。此外，我们还表明，通过在视频叙事基础（VNG）任务上对其进行评估，我们的数据可用于改进连接视觉和语言的模型。我们将在 https://pointvos.github.io 上提供我们的代码和注释。</details>
**PDF:** <http://arxiv.org/pdf/2402.05917v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation**<br />
**Title_cn:** ClickSAM：使用点击提示微调 Segment Anything Model 以进行超声图像分割<br />
**Authors:** Aimee Guo, Gace Fei, Hemanth Pasupuletic, Jing Wang<br />
**Abstract:** <details><summary>原文: </summary>The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true positive and false negative segments, and negative clicks are generated using the false positive segments. The Centroidal Voronoi Tessellation algorithm is then employed to collect positive and negative click prompts in each segment that are used to enhance the model performance during the second stage of training. With click-train methods, ClickSAM exhibits superior performance compared to other existing models for ultrasound image segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>新发布的Segment Anything Model（SAM）因其卓越的分割精度、多种输入提示、训练能力和高效的模型设计而成为图像处理中流行的工具。然而，其当前模型是在不适合医学图像（尤其是超声图像）的多样化数据集上进行训练的。超声图像往往含有大量噪声，因此很难分割出重要的结构。在这个项目中，我们开发了 ClickSAM，它使用超声图像的点击提示来微调分段任意模型。 ClickSAM 有两个训练阶段：第一阶段针对以真实轮廓为中心的单击提示进行训练，第二阶段侧重于通过额外的正面和负面点击提示来提高模型性能。通过将第一阶段的预测与真实掩模进行比较，计算真阳性、假阳性和假阴性片段。使用真阳性和假阴性片段生成阳性点击，使用假阳性片段生成阴性点击。然后采用质心 Voronoi Tessellation 算法收集每个片段中的正向和负向点击提示，用于在训练的第二阶段增强模型性能。通过点击训练方法，ClickSAM 与其他现有的超声图像分割模型相比表现出了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.05902v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data**<br />
**Title_cn:** Mamba-ND：多维数据的选择性状态空间建模<br />
**Authors:** Shufan Li, Harkanwar Singh, Aditya Grover<br />
**Abstract:** <details><summary>原文: </summary>In recent years, Transformers have become the de-facto architecture for sequence modeling on text and a variety of multi-dimensional data, such as images and video. However, the use of self-attention layers in a Transformer incurs prohibitive compute and memory complexity that scales quadratically w.r.t. the sequence length. A recent architecture, Mamba, based on state space models has been shown to achieve comparable performance for modeling text sequences, while scaling linearly with the sequence length. In this work, we present Mamba-ND, a generalized design extending the Mamba architecture to arbitrary multi-dimensional data. Our design alternatively unravels the input data across different dimensions following row-major orderings. We provide a systematic comparison of Mamba-ND with several other alternatives, based on prior multi-dimensional extensions such as Bi-directional LSTMs and S4ND. Empirically, we show that Mamba-ND demonstrates performance competitive with the state-of-the-art on a variety of multi-dimensional benchmarks, including ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather forecasting.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，Transformers 已成为文本和各种多维数据（例如图像和视频）序列建模的事实上的架构。然而，在 Transformer 中使用自注意力层会带来令人望而却步的计算和内存复杂性，其规模会呈二次方扩展。序列长度。最近的基于状态空间模型的架构 Mamba 已被证明可以在文本序列建模方面实现可比的性能，同时随序列长度线性扩展。在这项工作中，我们提出了 Mamba-ND，这是一种将 Mamba 架构扩展到任意多维数据的通用设计。我们的设计交替地按照行优先排序在不同维度上解析输入数据。我们基于先前的多维扩展（例如双向 LSTM 和 S4ND），对 Mamba-ND 与其他几种替代方案进行了系统比较。根据经验，我们表明 Mamba-ND 在各种多维基准上表现出与最先进的性能竞争，包括 ImageNet-1K 分类、HMDB-51 动作识别和 ERA5 天气​​预报。</details>
**PDF:** <http://arxiv.org/pdf/2402.05892v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning**<br />
**Title_cn:** 使用 YOLO v7 在磁共振成像中检测肾脏：监督对比学习<br />
**Authors:** Pouria Yazdian Anari, Fiona Obiezu, Nathan Lay, Fatemeh Dehghani Firouzabadi, Aditi Chaurasia, Mahshid Golagha, Shiva Singh, Fatemeh Homayounieh, Aryan Zahergivar, Stephanie Harmon, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predictive value (PPV), sensitivity, and mean average precision (mAP). Results The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of 0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV of 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95 +/- 0.01. Conclusion Using a semi-supervised approach with a medical image library, we developed a high-performing model for kidney detection. Further external validation is required to assess the model's generalizability.</details>
**Abstract_cn:** <details><summary>译文: </summary>简介 本研究通过在医学图像格式上训练和测试修改后的 YOLO V7，探索使用最新的 You Only Look Once (YOLO V7) 对象检测方法来增强医学成像中的肾脏检测。方法 研究包括 878 名不同亚型肾细胞癌 (RCC) 患者和 206 名正常肾脏患者。总共检索了 1084 名患者的 5657 幅 MRI 扫描图像。从回顾性维护的数据库中招募了 326 名患有 1034 个肿瘤的患者，并在他们的肿瘤周围绘制了边界框。主要模型在 80% 的带注释案例上进行训练，其中 20% 保存用于测试（主要测试集）。然后使用最好的主要模型来识别其余 861 名患者的肿瘤，并使用该模型在他们的扫描中生成边界框坐标。使用未分段患者生成的坐标创建了十个基准训练集。最终模型用于预测主要测试集中的肾脏。我们报告了阳性预测值 (PPV)、灵敏度和平均精确度 (mAP)。结果 主要训练集显示平均 PPV 为 0.94 +/- 0.01，灵敏度为 0.87 +/- 0.04，mAP 为 0.91 +/- 0.02。最佳初级模型的 PPV 为 0.97，灵敏度为 0.92，mAP 为 0.95。最终模型的平均 PPV 为 0.95 +/- 0.03，灵敏度为 0.98 +/- 0.004，mAP 为 0.95 +/- 0.01。结论 使用半监督方法和医学图像库，我们开发了一种高性能的肾脏检测模型。需要进一步的外部验证来评估模型的普遍性。</details>
**PDF:** <http://arxiv.org/pdf/2402.05817v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method**<br />
**Title_cn:** Jacquard V2：使用“人在环”数据校正方法细化数据集<br />
**Authors:** Qiuhao Li, Shenghai Yuan<br />
**Abstract:** <details><summary>原文: </summary>In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在工业自动化快速发展的背景下，基于视觉的机器人抓取发挥着越来越重要的作用。为了提高视觉识别的准确性，必须利用大规模数据集来训练模型以获得与处理各种物体相关的隐性知识。从头开始创建数据集是一个时间和劳动力密集型的过程。此外，由于出于权宜之计的自动注释，现有数据集经常包含错误，这使得这些数据集的改进成为一项重大的研究挑战。因此，在流行的 Jacquard Grasp 中的抓取边界框注释中发现了几个问题。我们建议利用人机交互（HIL）方法来提高数据集质量。这种方法依赖于骨干深度学习网络来预测机器人抓取的物体位置和方向。交并集 (IOU) 值低于 0.2 的预测由操作人员进行评估。经过评估后，数据被分为假阴性（FN）和真阴性（TN）。然后 FN 被细分为缺失注释或灾难性标签错误。缺少标签的图像会通过有效的抓取边界框信息进行增强，而受灾难性标签错误影响的图像将被完全删除。使用开源工具 Labelbee 对 HIL 数据集增强进行了 53,026 次迭代，从而删除了 2,884 张图像，并合并了 30,292 张图像的地面真实信息。增强的数据集名为 Jacquard V2 Grasping Dataset，用作一系列神经网络的训练数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.05747v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **An Ordinal Regression Framework for a Deep Learning Based Severity Assessment for Chest Radiographs**<br />
**Title_cn:** 基于深度学习的胸部 X 线照片严重性评估的序数回归框架<br />
**Authors:** Patrick Wienholt, Alexander Hermans, Firas Khader, Behrus Puladi, Bastian Leibe, Christiane Kuhl, Sven Nebelung, Daniel Truhn<br />
**Abstract:** <details><summary>原文: </summary>This study investigates the application of ordinal regression methods for categorizing disease severity in chest radiographs. We propose a framework that divides the ordinal regression problem into three parts: a model, a target function, and a classification function. Different encoding methods, including one-hot, Gaussian, progress-bar, and our soft-progress-bar, are applied using ResNet50 and ViT-B-16 deep learning models. We show that the choice of encoding has a strong impact on performance and that the best encoding depends on the chosen weighting of Cohen's kappa and also on the model architecture used. We make our code publicly available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究探讨了序数回归方法在胸片中疾病严重程度分类中的应用。我们提出了一个框架，将序数回归问题分为三个部分：模型、目标函数和分类函数。 ResNet50 和 ViT-B-16 深度学习模型应用了不同的编码方法，包括 one-hot、Gaussian、progress-bar 和我们的 soft-progress-bar。我们表明，编码的选择对性能有很大影响，并且最佳编码取决于所选的 Cohen kappa 权重以及所使用的模型架构。我们在 GitHub 上公开提供我们的代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.05685v1><br />
**Code:** <https://github.com/paddyongithub/ordinal_regression>**<br />
>>**index:** 9<br />
**Title:** **DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation**<br />
**Title_cn:** DAPlankton：通过细粒度域适应进行多仪器浮游生物识别的基准数据集<br />
**Authors:** Daniel Batrakhanov, Tuomas Eerola, Kaisa Kraft, Lumi Haraguchi, Lasse Lensu, Sanna Suikkanen, María Teresa Camarena-Gómez, Jukka Seppälä, Heikki Kälviäinen<br />
**Abstract:** <details><summary>原文: </summary>Plankton recognition provides novel possibilities to study various environmental aspects and an interesting real-world context to develop domain adaptation (DA) methods. Different imaging instruments cause domain shift between datasets hampering the development of general plankton recognition methods. A promising remedy for this is DA allowing to adapt a model trained on one instrument to other instruments. In this paper, we present a new DA dataset called DAPlankton which consists of phytoplankton images obtained with different instruments. Phytoplankton provides a challenging DA problem due to the fine-grained nature of the task and high class imbalance in real-world datasets. DAPlankton consists of two subsets. DAPlankton_LAB contains images of cultured phytoplankton providing a balanced dataset with minimal label uncertainty. DAPlankton_SEA consists of images collected from the Baltic Sea providing challenging real-world data with large intra-class variance and class imbalance. We further present a benchmark comparison of three widely used DA methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>浮游生物识别为研究各种环境因素和有趣的现实世界环境提供了新的可能性，以开发领域适应（DA）方法。不同的成像仪器导致数据集之间的域转移阻碍了通用浮游生物识别方法的发展。一个有前途的补救措施是 DA，允许将在一台仪器上训练的模型应用于其他仪器。在本文中，我们提出了一个名为 DAPlankton 的新 DA 数据集，它由使用不同仪器获得的浮游植物图像组成。由于任务的细粒度性质和现实数据集中的高度不平衡，浮游植物提供了一个具有挑战性的 DA 问题。 DAPlankton 由两个子集组成。 DAPlankton_LAB 包含培养浮游植物的图像，提供标签不确定性最小的平衡数据集。 DAPlankton_SEA 由从波罗的海收集的图像组成，提供具有较大类内方差和类不平衡的具有挑战性的现实世界数据。我们进一步对三种广泛使用的 DA 方法进行了基准比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.05615v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner**<br />
**Title_cn:** RESMatch：半监督方式的引用表达分割<br />
**Authors:** Ying Zang, Chenglong Fu, Runlong Cao, Didi Zhu, Min Zhang, Wenjun Hu, Lanyun Zhu, Tianrun Chen<br />
**Abstract:** <details><summary>原文: </summary>Referring expression segmentation (RES), a task that involves localizing specific instance-level objects based on free-form linguistic descriptions, has emerged as a crucial frontier in human-AI interaction. It demands an intricate understanding of both visual and textual contexts and often requires extensive training data. This paper introduces RESMatch, the first semi-supervised learning (SSL) approach for RES, aimed at reducing reliance on exhaustive data annotation. Extensive validation on multiple RES datasets demonstrates that RESMatch significantly outperforms baseline approaches, establishing a new state-of-the-art. Although existing SSL techniques are effective in image segmentation, we find that they fall short in RES. Facing the challenges including the comprehension of free-form linguistic descriptions and the variability in object attributes, RESMatch introduces a trifecta of adaptations: revised strong perturbation, text augmentation, and adjustments for pseudo-label quality and strong-weak supervision. This pioneering work lays the groundwork for future research in semi-supervised learning for referring expression segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>引用表达分割（RES）是一项涉及基于自由形式语言描述本地化特定实例级对象的任务，已成为人类与人工智能交互的关键前沿。它需要对视觉和文本上下文有复杂的理解，并且通常需要大量的训练数据。本文介绍了RESMatch，这是RES的第一个半监督学习（SSL）方法，旨在减少对详尽数据注释的依赖。对多个 RES 数据集的广泛验证表明，RESMatch 的性能显着优于基线方法，建立了新的最先进技术。尽管现有的 SSL 技术在图像分割方面很有效，但我们发现它们在 RES 方面存在不足。面对自由形式语言描述的理解和对象属性的可变性等挑战，RESMatch 引入了三重适应：修订的强扰动、文本增强以及伪标签质量和强弱监督的调整。这项开创性的工作为未来的指代表达分割半监督学习研究奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.05589v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning**<br />
**Title_cn:** 使用深度学习的超声图像腕管综合症一站式自动诊断系统<br />
**Authors:** Jiayu Peng, Jiajun Zeng, Manlin Lai, Ruobing Huang, Dong Ni, Zhenzhou Li<br />
**Abstract:** <details><summary>原文: </summary>Objective: Ultrasound (US) examination has unique advantages in diagnosing carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and diagnosing CTS depends heavily on the expertise of examiners. To alleviate this problem, we aimed to develop a one-stop automated CTS diagnosis system (OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool. Methods: We combined real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis into a unified framework, called OSA-CTSD. We collected a total of 32,301 static images from US videos of 90 normal wrists and 40 CTS wrists for evaluation using a simplified scanning protocol. Results: The proposed model showed better segmentation and measurement performance than competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px, Dice score of 85.78%, and IoU score of 76.00%, respectively. In the reader study, it demonstrated comparable performance with the average performance of the experienced in classifying the CTS, while outperformed that of the inexperienced radiologists in terms of classification metrics (e.g., accuracy score of 3.59% higher and F1 score of 5.85% higher). Conclusion: The OSA-CTSD demonstrated promising diagnostic performance with the advantages of real-time, automation, and clinical interpretability. The application of such a tool can not only reduce reliance on the expertise of examiners, but also can help to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：超声（US）检查在诊断腕管综合征（CTS）方面具有独特的优势，而识别正中神经（MN）和诊断CTS在很大程度上取决于检查者的专业知识。为了缓解这个问题，我们的目标是开发一站式自动化 CTS 诊断系统（OSA-CTSD）并评估其作为计算机辅助诊断工具的有效性。方法：我们将实时 MN 描绘、准确的生物特征测量和可解释的 CTS 诊断结合到一个统一的框架中，称为 OSA-CTSD。我们从美国视频中收集了 90 个正常手腕和 40 个 CTS 手腕的总共 32,301 张静态图像，使用简化的扫描协议进行评估。结果：所提出的模型显示出比竞争方法更好的分割和测量性能，报告的 HD95 分数为 7.21px，ASSD 分数为 2.64px，Dice 分数为 85.78%，IoU 分数为 76.00%。在读者研究中，它在 CTS 分类方面表现出与经验丰富的放射科医生的平均表现相当的性能，而在分类指标方面优于没有经验的放射科医生（例如，准确度得分高出 3.59%，F1 得分高出 5.85%） 。结论：OSA-CTSD 表现出良好的诊断性能，具有实时、自动化和临床可解释性等优点。这样的工具的应用不仅可以减少对检查人员专业知识的依赖，还可以帮助推动未来CTS诊断流程的标准化，使患者和放射科医生都受益。</details>
**PDF:** <http://arxiv.org/pdf/2402.05554v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction**<br />
**Title_cn:** 高效的表情中性估计及其在人脸识别效用预测中的应用<br />
**Authors:** Marcel Grimmer, Raymond N. J. Veldhuis, Christoph Busch<br />
**Abstract:** <details><summary>原文: </summary>The recognition performance of biometric systems strongly depends on the quality of the compared biometric samples. Motivated by the goal of establishing a common understanding of face image quality and enabling system interoperability, the committee draft of ISO/IEC 29794-5 introduces expression neutrality as one of many component quality elements affecting recognition performance. In this study, we train classifiers to assess facial expression neutrality using seven datasets. We conduct extensive performance benchmarking to evaluate their classification and face recognition utility prediction abilities. Our experiments reveal significant differences in how each classifier distinguishes "neutral" from "non-neutral" expressions. While Random Forests and AdaBoost classifiers are most suitable for distinguishing neutral from non-neutral facial expressions with high accuracy, they underperform compared to Support Vector Machines in predicting face recognition utility.</details>
**Abstract_cn:** <details><summary>译文: </summary>生物识别系统的识别性能很大程度上取决于比较生物识别样本的质量。出于建立对人脸图像质量的共同理解和实现系统互操作性的目标，ISO/IEC 29794-5 委员会草案将表情中立性引入为影响识别性能的众多组成质量元素之一。在这项研究中，我们使用七个数据集训练分类器来评估面部表情中立性。我们进行了广泛的性能基准测试，以评估它们的分类和人脸识别效用预测能力。我们的实验揭示了每个分类器如何区分“中性”和“非中性”表达的显着差异。虽然随机森林和 AdaBoost 分类器最适合以高精度区分中性和非中性面部表情，但与支持向量机相比，它们在预测面部识别效用方面表现不佳。</details>
**PDF:** <http://arxiv.org/pdf/2402.05548v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array**<br />
**Title_cn:** 使用低成本单光子雪崩二极管阵列的尖峰神经网络增强手势识别<br />
**Authors:** Zhenya Zang, Xingda Li, David Day Uei Li<br />
**Abstract:** <details><summary>原文: </summary>We present a compact spiking convolutional neural network (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array. In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network. A vanilla convolutional neural network (CNN) was also implemented to compare the performance of SCNN with the same network topologies and training strategies. Our SCNN was trained from scratch instead of being converted from the CNN. We tested the three models in dark and ambient light (AL)-corrupted environments. The results indicate that SCNN achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating operations with only 8 timesteps. SMLP also presents a trade-off between computational workload and accuracy. The code and collected datasets of this work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种紧凑的尖峰卷积神经网络 (SCNN) 和尖峰多层感知器 (SMLP)，使用 9.6 美元的单光子雪崩二极管 (SPAD) 阵列来识别黑暗和明亮环境中的十种不同手势。在我们的手势识别（HGR）系统中，利用光子强度数据来训练和测试网络。还实现了普通卷积神经网络 (CNN)，以比较具有相同网络拓扑和训练策略的 SCNN 的性能。我们的 SCNN 是从头开始训练的，而不是从 CNN 转换而来。我们在黑暗和环境光 (AL) 损坏的环境中测试了这三个型号。结果表明，SCNN 的准确率 (90.8%) 与 CNN (92.9%) 相当，并且仅需要 8 个时间步即可表现出较低的浮动操作。 SMLP 还提出了计算工作量和准确性之间的权衡。这项工作的代码和收集的数据集可在 https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.05441v1><br />
**Code:** <https://github.com/zzy666666zzy/tinylidar_net_snn>**<br />
>>**index:** 14<br />
**Title:** **Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification**<br />
**Title_cn:** 基于无分割联结时间分类损失的文本验证码分类 OCR 模型<br />
**Authors:** Vaibhav Khatavkar, Makarand Velankar, Sneha Petkar<br />
**Abstract:** <details><summary>原文: </summary>Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha. Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80\% character level accuracy, while 95\% word level accuracy. The accuracy of the proposed model is compared with the state-of-the-art models and proves to be effective. The variable length complex captcha can be thus processed with the segmentation-free connectionist temporal classification loss technique with dependencies which will be massively used in securing the software systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>验证码广泛用于通过区分计算机响应和人类响应来保护系统免受自动响应的影响。文本、音频、视频、图片基于图像的光学字符识别 (OCR) 用于创建验证码。基于文本的 OCR 验证码是最常用的验证码，其面临的问题是内容复杂且扭曲。人们尝试使用机器学习和神经网络构建基于验证码检测和分类的系统，但需要对其准确性进行调整。现有系统在识别扭曲字符、处理可变长度验证码以及查找验证码中的顺序依赖性方面面临挑战。在这项工作中，我们提出了一种基于联结主义时间分类损失技术的用于文本验证码分类的无分割 OCR 模型。所提出的模型在公开可用的验证码数据集上进行训练和测试。所提出的模型给出了 99.80% 的字符级准确率，而 95% 的单词级准确率。所提出模型的准确性与最先进的模型进行了比较，并证明是有效的。因此，可以使用具有依赖性的无分段连接主义时间分类损失技术来处理可变长度的复杂验证码，该技术将大量用于保护软件系统。</details>
**PDF:** <http://arxiv.org/pdf/2402.05417v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **SpirDet: Towards Efficient, Accurate and Lightweight Infrared Small Target Detector**<br />
**Title_cn:** SpirDet：迈向高效、准确、轻便的红外小目标探测器<br />
**Authors:** Qianchen Mao, Qiang Li, Bingshu Wang, Yongjun Zhang, Tao Dai, C. L. Philip Chen<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the detection of infrared small targets using deep learning methods has garnered substantial attention due to notable advancements. To improve the detection capability of small targets, these methods commonly maintain a pathway that preserves high-resolution features of sparse and tiny targets. However, it can result in redundant and expensive computations. To tackle this challenge, we propose SpirDet, a novel approach for efficient detection of infrared small targets. Specifically, to cope with the computational redundancy issue, we employ a new dual-branch sparse decoder to restore the feature map. Firstly, the fast branch directly predicts a sparse map indicating potential small target locations (occupying only 0.5\% area of the map). Secondly, the slow branch conducts fine-grained adjustments at the positions indicated by the sparse map. Additionally, we design an lightweight DO-RepEncoder based on reparameterization with the Downsampling Orthogonality, which can effectively reduce memory consumption and inference latency. Extensive experiments show that the proposed SpirDet significantly outperforms state-of-the-art models while achieving faster inference speed and fewer parameters. For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by 4.7 and has a $7\times$ $FPS$ acceleration compared to the previous state-of-the-art model. The code will be open to the public.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，利用深度学习方法检测红外小目标由于取得了显着进展而引起了广泛关注。为了提高小目标的检测能力，这些方法通常维护一条保留稀疏和微小目标的高分辨率特征的路径。然而，它可能会导致冗余且昂贵的计算。为了应对这一挑战，我们提出了 SpirDet，这是一种有效检测红外小目标的新方法。具体来说，为了解决计算冗余问题，我们采用新的双分支稀疏解码器来恢复特征图。首先，快速分支直接预测稀疏地图，指示潜在的小目标位置（仅占据地图的 0.5\% 区域）。其次，慢分支在稀疏图指示的位置进行细粒度的调整。此外，我们设计了一种基于下采样正交性重参数化的轻量级 DO-RepEncoder，可以有效减少内存消耗和推理延迟。大量实验表明，所提出的 SpirDet 显着优于最先进的模型，同时实现更快的推理速度和更少的参数。例如，在 IRSTD-1K 数据集上，与之前最先进的模型相比，SpirDet 将 $MIoU$ 提高了 4.7，并具有 $7\times$ $FPS$ 加速。该代码将向公众开放。</details>
**PDF:** <http://arxiv.org/pdf/2402.05410v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions**<br />
**Title_cn:** 通过对一系列损失函数进行训练来优化类不平衡数据上的 ROC 曲线<br />
**Authors:** Kelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran, Carlo Tomasi<br />
**Abstract:** <details><summary>原文: </summary>Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code will be made available at: https://github.com/klieberman/roc_lct.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管二元分类是计算机视觉中一个经过充分研究的问题，但在严重的类别不平衡下训练可靠的分类器仍然是一个具有挑战性的问题。最近的工作提出了通过修改损失函数或优化方法来减轻不平衡情况下训练的影响的技术。虽然这项工作显着提高了多类情况下的整体准确性，但我们观察到，这些方法的超参数值的微小变化可能会导致二进制问题的接收器操作特征（ROC）曲线方面的性能高度可变。严重失衡。为了降低对超参数选择的敏感性并训练更通用的模型，我们建议对一系列损失函数进行训练，而不是单个损失函数。我们开发了一种将损失条件训练（LCT）应用于不平衡分类问题的方法。在 CIFAR 和 Kaggle 竞赛数据集上的大量实验结果表明，我们的方法提高了模型性能，并且对超参数选择更加鲁棒。代码将在以下位置提供：https://github.com/klieberman/roc_lct。</details>
**PDF:** <http://arxiv.org/pdf/2402.05400v1><br />
**Code:** <https://github.com/klieberman/roc_lct>**<br />
>>**index:** 17<br />
**Title:** **On the Effect of Image Resolution on Semantic Segmentation**<br />
**Title_cn:** 图像分辨率对语义分割的影响<br />
**Authors:** Ritambhara Singh, Abhishek Jain, Pietro Perona, Shivani Agarwal, Junfeng Yang<br />
**Abstract:** <details><summary>原文: </summary>High-resolution semantic segmentation requires substantial computational resources. Traditional approaches in the field typically downscale the input images before processing and then upscale the low-resolution outputs back to their original dimensions. While this strategy effectively identifies broad regions, it often misses finer details. In this study, we demonstrate that a streamlined model capable of directly producing high-resolution segmentations can match the performance of more complex systems that generate lower-resolution results. By simplifying the network architecture, we enable the processing of images at their native resolution. Our approach leverages a bottom-up information propagation technique across various scales, which we have empirically shown to enhance segmentation accuracy. We have rigorously tested our method using leading-edge semantic segmentation datasets. Specifically, for the Cityscapes dataset, we further boost accuracy by applying the Noisy Student Training technique.</details>
**Abstract_cn:** <details><summary>译文: </summary>高分辨率语义分割需要大量的计算资源。该领域的传统方法通常在处理之前缩小输入图像的尺寸，然后将低分辨率输出放大回其原始尺寸。虽然这种策略有效地识别了广泛的区域，但它常常会错过更精细的细节。在这项研究中，我们证明了能够直接生成高分辨率分割的简化模型可以与生成较低分辨率结果的更复杂系统的性能相匹配。通过简化网络架构，我们能够以原始分辨率处理图像。我们的方法利用跨不同尺度的自下而上的信息传播技术，我们的经验表明该技术可以提高分割准确性。我们使用领先的语义分割数据集严格测试了我们的方法。具体来说，对于 Cityscapes 数据集，我们通过应用嘈杂的学生训练技术进一步提高准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.05398v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts**<br />
**Title_cn:** 通过混合集群条件专家的任务定制屏蔽自动编码器<br />
**Authors:** Zhili Liu, Kai Chen, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, James T. Kwok<br />
**Abstract:** <details><summary>原文: </summary>Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>Masked Autoencoder~（MAE）是一种流行的自监督学习方法，在模型预训练中取得了可喜的结果。然而，当各种下游任务的数据分布与预训练数据不同时，语义上不相关的预训练信息可能会导致负迁移，从而阻碍 MAE 的可扩展性。为了解决这个问题，我们提出了一种新的基于 MAE 的预训练范例，即集群条件专家混合 (MoCE)，它可以训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合 (MoE) 不同，我们的 MoCE 通过使用聚类条件门仅使用语义相关的图像来训练每个专家。因此，每个下游任务可以分配给使用与下游数据最相似的数据预先训练的定制模型。对 11 个下游任务集合的实验表明，MoCE 的性能平均优于普通 MAE 2.45%。它还在检测和分割方面获得了最先进的自监督学习结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.05382v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Scrapping The Web For Early Wildfire Detection**<br />
**Title_cn:** 废弃网络以进行早期野火检测<br />
**Authors:** Mateo Lostanlen, Felix Veith, Cristian Buc, Valentin Barriere<br />
**Abstract:** <details><summary>原文: </summary>Early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads. To this end, we present \Pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations. Our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images. We ran experiments using a state-of-the-art object detection model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall. We will make our code and data publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>早期发现野火对于快速响应工作至关重要，从而最大限度地减少野火蔓延的负面影响。为此，我们提出了 \Pyro，一个基于网络抓取的数据集，由来自摄像机网络的野火视频组成，并通过手动边界框级注释进行了增强。我们的数据集根据提高数据质量和多样性的策略进行过滤，将最终数据减少到一组 10,000 张图像。我们使用最先进的对象检测模型进行了实验，发现所提出的数据集具有挑战性，并且将其与其他公共数据集结合使用有助于总体上获得更高的结果。我们将公开我们的代码和数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.05349v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images**<br />
**Title_cn:** 单目图像几何估计的自适应表面法线约束<br />
**Authors:** Xiaoxiao Long, Yuhang Zheng, Yupeng Zheng, Beiwen Tian, Cheng Lin, Lingjie Liu, Hao Zhao, Guyue Zhou, Wenping Wang<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel approach to learn geometries such as depth and surface normal from images while incorporating geometric context. The difficulty of reliably capturing geometric context in existing methods impedes their ability to accurately enforce the consistency between the different geometric properties, thereby leading to a bottleneck of geometric estimation quality. We therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet efficient method. Our approach extracts geometric context that encodes the geometric variations present in the input image and correlates depth estimation with geometric constraints. By dynamically determining reliable local geometry from randomly sampled candidates, we establish a surface normal constraint, where the validity of these candidates is evaluated using the geometric context. Furthermore, our normal estimation leverages the geometric context to prioritize regions that exhibit significant geometric variations, which makes the predicted normals accurately capture intricate and detailed geometric information. Through the integration of geometric context, our method unifies depth and surface normal estimations within a cohesive framework, which enables the generation of high-quality 3D geometry from images. We validate the superiority of our approach over state-of-the-art methods through extensive evaluations and comparisons on diverse indoor and outdoor datasets, showcasing its efficiency and robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种新颖的方法来学习几何形状，例如从图像中学习深度和表面法线，同时结合几何上下文。现有方法难以可靠地捕获几何上下文，这阻碍了它们准确执行不同几何属性之间的一致性的能力，从而导致几何估计质量的瓶颈。因此，我们提出了自适应表面法线（ASN）约束，这是一种简单而有效的方法。我们的方法提取几何上下文，对输入图像中存在的几何变化进行编码，并将深度估计与几何约束相关联。通过从随机采样的候选对象中动态确定可靠的局部几何形状，我们建立了表面法线约束，其中使用几何上下文评估这些候选对象的有效性。此外，我们的法线估计利用几何上下文来优先考虑表现出显着几何变化的区域，这使得预测法线能够准确地捕获复杂而详细的几何信息。通过几何上下文的集成，我们的方法将深度和表面法线估计统一在一个内聚框架内，从而能够从图像生成高质量的 3D 几何图形。我们通过对各种室内和室外数据集的广泛评估和比较，验证了我们的方法相对于最先进方法的优越性，展示了其效率和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.05869v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents**<br />
**Title_cn:** 通过协作法学硕士代理进行自动驾驶的可编辑场景模拟<br />
**Authors:** Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, Yanfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶中的场景模拟因其生成定制数据的巨大潜力而​​受到广泛关注。然而，现有的可编辑场景模拟方法在用户交互效率、多摄像头真实感渲染和外部数字资产集成方面面临局限性。为了应对这些挑战，本文介绍了 ChatSim，这是第一个通过自然语言命令和外部数字资产实现可编辑的逼真 3D 驾驶场景模拟的系统。为了实现具有高度命令灵活性的编辑，~ChatSim 利用大型语言模型 (LLM) 代理协作框架。为了生成逼真的结果，ChatSim 采用了一种新颖的多摄像头神经辐射场方法。此外，为了释放广泛的高质量数字资产的潜力，ChatSim采用了一种新颖的多摄像机照明估计方法来实现场景一致的资产渲染。我们在 Waymo 开放数据集上的实验表明，ChatSim 可以处理复杂的语言命令并生成相应的逼真场景视频。</details>
**PDF:** <http://arxiv.org/pdf/2402.05746v1><br />
**Code:** <https://github.com/yifanlu0227/chatsim>**<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images**<br />
**Title_cn:** 使用新颖的并行图像数据集检查大型视觉语言模型中的性别和种族偏见<br />
**Authors:** Kathleen C. Fraser, Svetlana Kiritchenko<br />
**Abstract:** <details><summary>原文: </summary>Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着大型语言模型 (LLM) 和后续聊天模型的最新进展，新一波大型视觉语言模型 (LVLM) 已经出现。这些模型除了文本之外还可以将图像作为输入，并执行诸如视觉问答、图像字幕、故事生成等任务。在这里，我们根据人们的感知特征来检查此类系统中潜在的性别和种族偏见在输入图像中。为了实现这一目标，我们提出了一个新的数据集 PAIRS（每天场景的并行图像）。 PAIRS 数据集包含人工智能生成的人物图像集，这些图像在背景和视觉内容方面高度相似，但在性别（男性、女性）和种族（黑人、白人）维度上有所不同。通过使用此类图像查询 LVLM，我们观察到根据所描绘的人的感知性别或种族，响应存在显着差异。</details>
**PDF:** <http://arxiv.org/pdf/2402.05779v1><br />
**Code:** <https://github.com/katiefraser/pairs>**<br />
>>**index:** 2<br />
**Title:** **Real-World Robot Applications of Foundation Models: A Review**<br />
**Title_cn:** 基础模型的现实世界机器人应用：回顾<br />
**Authors:** Kento Kawaharazuka, Tatsuya Matsushima, Andrew Gambardella, Jiaxian Guo, Chris Paxton, Andy Zeng<br />
**Abstract:** <details><summary>原文: </summary>Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>基础模型的最新发展，例如大型语言模型（LLM）和视觉语言模型（VLM），经过大量数据的训练，促进了跨不同任务和模式的灵活应用。他们的影响涵盖各个领域，包括医疗保健、教育和机器人技术。本文概述了基础模型在现实机器人技术中的实际应用，重点是替换现有机器人系统中的特定组件。总结涵盖了基础模型中输入输出关系的视角，以及它们在机器人领域的感知、运动规划和控制中的作用。本文最后讨论了未来的挑战以及对实际机器人应用的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.05741v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Enhancing Zero-shot Counting via Language-guided Exemplar Learning**<br />
**Title_cn:** 通过语言引导的示例学习增强零样本计数<br />
**Authors:** Mingjie Wang, Jun Zhou, Yong Dai, Eric Buys, Minglun Gong<br />
**Abstract:** <details><summary>原文: </summary>Recently, Class-Agnostic Counting (CAC) problem has garnered increasing attention owing to its intriguing generality and superior efficiency compared to Category-Specific Counting (CSC). This paper proposes a novel ExpressCount to enhance zero-shot object counting by delving deeply into language-guided exemplar learning. Specifically, the ExpressCount is comprised of an innovative Language-oriented Exemplar Perceptron and a downstream visual Zero-shot Counting pipeline. Thereinto, the perceptron hammers at exploiting accurate exemplar cues from collaborative language-vision signals by inheriting rich semantic priors from the prevailing pre-trained Large Language Models (LLMs), whereas the counting pipeline excels in mining fine-grained features through dual-branch and cross-attention schemes, contributing to the high-quality similarity learning. Apart from building a bridge between the LLM in vogue and the visual counting tasks, expression-guided exemplar estimation significantly advances zero-shot learning capabilities for counting instances with arbitrary classes. Moreover, devising a FSC-147-Express with annotations of meticulous linguistic expressions pioneers a new venue for developing and validating language-based counting models. Extensive experiments demonstrate the state-of-the-art performance of our ExpressCount, even showcasing the accuracy on par with partial CSC models.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，与类别特定计数（CSC）相比，类别不可知计数（CAC）问题由于其有趣的通用性和卓越的效率而受到越来越多的关注。本文提出了一种新颖的 ExpressCount，通过深入研究语言引导的样本学习来增强零样本对象计数。具体来说，ExpressCount 由创新的面向语言的范例感知器和下游视觉零样本计数管道组成。其中，感知器通过从流行的预训练大型语言模型（LLM）继承丰富的语义先验，致力于从协作语言视觉信号中挖掘准确的样本线索，而计数管道擅长通过双分支和交叉注意方案，有助于高质量的相似性学习。除了在流行的法学硕士和视觉计数任务之间架起一座桥梁之外，表达引导的样本估计还显着提高了用于计算任意类实例的零样本学习能力。此外，设计带有细致语言表达注释的 FSC-147-Express 开创了开发和验证基于语言的计数模型的新场所。大量的实验证明了我们的 ExpressCount 的最先进的性能，甚至展示了与部分 CSC 模型相当的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.05394v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy**<br />
**Title_cn:** 内存高效的视觉变压器：激活感知的混合等级压缩策略<br />
**Authors:** Seyedarmin Azizi, Mahdi Nazemi, Massoud Pedram<br />
**Abstract:** <details><summary>原文: </summary>As Vision Transformers (ViTs) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minimum early in the optimization process and strikes a good balance between the model compression and output accuracy. Notably, the presented method significantly reduces the parameter count of DeiT-B by 60% with less than 1% accuracy drop on the ImageNet dataset, overcoming the usual accuracy degradation seen in low-rank approximations. In addition to this, the presented compression technique can compress large DeiT/ViT models to have about the same model size as smaller DeiT/ViT variants while yielding up to 1.8% accuracy gain. These results highlight the efficacy of our approach, presenting a viable solution for embedding ViTs in memory-constrained environments without compromising their performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着视觉变压器 (ViT) 越来越多地在计算机视觉领域树立新的基准，它们在推理引擎上的实际部署往往因其巨大的内存带宽和（片上）内存占用要求而受到阻碍。本文通过引入一种激活感知模型压缩方法来解决这一内存限制，该方法使用不同层的选择性低秩权重张量近似来减少 ViT 的参数数量。关键思想是将权重张量分解为两个参数有效张量的和，同时最小化输入激活与原始权重张量的乘积与输入激活与近似张量和的乘积之间的误差。通过采用有效的逐层误差补偿技术（使用层输出损失的梯度），可以进一步细化该近似值。这些技术的结合取得了优异的结果，同时避免了在优化过程的早期陷入浅局部最小值，并在模型压缩和输出精度之间取得了良好的平衡。值得注意的是，所提出的方法将 DeiT-B 的参数数量显着减少了 60%，并且 ImageNet 数据集上的准确率下降了不到 1%，克服了低秩近似中常见的准确率下降问题。除此之外，所提出的压缩技术可以压缩大型 DeiT/ViT 模型，使其具有与较小的 DeiT/ViT 变体大致相同的模型大小，同时产生高达 1.8% 的精度增益。这些结果凸显了我们方法的有效性，为在内存受限的环境中嵌入 ViT 提供了一种可行的解决方案，同时又不影响其性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.06004v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Memory Consolidation Enables Long-Context Video Understanding**<br />
**Title_cn:** 内存整合可实现长上下文视频理解<br />
**Authors:** Ivana Balažević, Yuge Shi, Pinelopi Papalampidi, Rahma Chaabouni, Skanda Koppula, Olivier J. Hénaff<br />
**Abstract:** <details><summary>原文: </summary>Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数基于变换器的视频编码器由于其二次复杂度而仅限于短时间上下文。尽管已经做出了各种尝试来扩展此上下文，但这通常是以概念和计算复杂性为代价的。我们建议通过简单地微调现有的预训练视频转换器来重新利用它们，以处理从过去的激活中非参数导出的记忆。通过利用冗余减少，我们的记忆整合视觉转换器（MC-ViT）毫不费力地将其上下文扩展到过去，并在从较长的视频中学习时表现出出色的缩放行为。在此过程中，MC-ViT 在 EgoSchema、Perception Test 和 Diving48 上的长上下文视频理解方面树立了新的最先进水平，其性能优于受益于更多数量级参数的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05861v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement**<br />
**Title_cn:** 您只需要一种色彩空间：用于低光图像增强的高效网络<br />
**Authors:** Yixu Feng, Cheng Zhang, Pei Wang, Peng Wu, Qingsen Yan, Yanning Zhang<br />
**Abstract:** <details><summary>原文: </summary>Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two branches dedicated to processing the decoupled image brightness and color in the HVI space. Within CIDNet, we introduce the Lightweight Cross-Attention (LCA) module to facilitate interaction between image structure and content information in both branches, while also suppressing noise in low-light images. Finally, we conducted 22 quantitative and qualitative experiments to show that the proposed CIDNet outperforms the state-of-the-art methods on 11 datasets. The code will be available at https://github.com/Fediory/HVI-CIDNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>低光图像增强（LLIE）任务倾向于从损坏的低光图像中恢复细节和视觉信息。大多数现有方法通过深度神经网络 (DNN) 在 sRGB 和 HSV 颜色空间上学习低/正常光图像之间的映射函数。然而，增强涉及放大图像信号，并将这些色彩空间应用于具有低信噪比的低光图像，可能会给增强过程带来敏感性和不稳定性。因此，这导致增强图像中出现颜色伪影和亮度伪影。为了缓解这个问题，我们提出了一种新颖的可训练色彩空间，称为水平/垂直强度（HVI）。它不仅将亮度和颜色与 RGB 通道解耦以减轻增强过程中的不稳定性，而且由于可训练参数，它还可以适应不同照明范围内的低光图像。此外，我们设计了一种新颖的颜色和强度解耦网络（CIDNet），它有两个分支，专门用于处理 HVI 空间中解耦的图像亮度和颜色。在 CIDNet 中，我们引入了轻量级交叉注意（LCA）模块，以促进两个分支中图像结构和内容信息之间的交互，同时还抑制低光图像中的噪声。最后，我们进行了 22 项定量和定性实验，表明所提出的 CIDNet 在 11 个数据集上优于最先进的方法。该代码可在 https://github.com/Fediory/HVI-CIDNet 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.05809v1><br />
**Code:** <https://github.com/fediory/hvi-cidnet>**<br />
>>**index:** 4<br />
**Title:** **Binding Dynamics in Rotating Features**<br />
**Title_cn:** 旋转特征中的绑定动力学<br />
**Authors:** Sindy Löwe, Francesco Locatello, Max Welling<br />
**Abstract:** <details><summary>原文: </summary>In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The "$\chi$-binding" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative "cosine binding" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to self-attention and biological neural processes, and to shed light on the fundamental dynamics for object-centric representations to emerge in Rotating Features.</details>
**Abstract_cn:** <details><summary>译文: </summary>在人类认知中，绑定问题描述了大脑如何灵活地将不同信息整合到有凝聚力的对象表征中的悬而未决的问题。类似地，在机器学习中，人们追求通过以无监督的方式学习以对象为中心的表示来实现强泛化和推理的模型。旋转特征借鉴神经科学理论，通过引入向量值特征来学习这种表示，这些向量值特征封装了对象特征的大小和对象从属关系的方向。嵌入在架构每一层中的“$\chi$-绑定”机制已被证明是至关重要的，但仍然知之甚少。在本文中，我们提出了一种替代的“余弦绑定”机制，该机制显式计算特征之间的对齐并相应地调整权重，并且我们证明它实现了等效的性能。这使我们能够与自注意力和生物神经过程建立直接联系，并阐明旋转特征中出现的以对象为中心的表示的基本动力学。</details>
**PDF:** <http://arxiv.org/pdf/2402.05627v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers**<br />
**Title_cn:** AttnLRP：Transformers 的注意力感知分层相关性传播<br />
**Authors:** Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek<br />
**Abstract:** <details><summary>原文: </summary>Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an open-source implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型很容易出现有偏差的预测和幻觉，这凸显了理解其模型内部推理过程的至关重要性。然而，实现整个黑盒变压器模型的忠实归因并保持计算效率是一个尚未解决的挑战。通过扩展分层相关性传播归因方法来处理注意力层，我们有效地解决了这些挑战。虽然存在部分解决方案，但我们的方法是第一个不仅忠实且整体地归因于变压器模型的输入和潜在表示的方法，其计算效率类似于奇异向后传递。通过对 Llama 2、Flan-T5 和 Vision Transformer 架构的现有方法进行广泛评估，我们证明我们提出的方法在忠实度方面超越了替代方法，并且能够理解潜在表示，为基于概念的解释打开了大门。我们在 GitHub https://github.com/rachtibat/LRP-for-Transformers 上提供了一个开源实现。</details>
**PDF:** <http://arxiv.org/pdf/2402.05602v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **On Convolutional Vision Transformers for Yield Prediction**<br />
**Title_cn:** 用于产量预测的卷积视觉变压器<br />
**Authors:** Alvin Inderka, Florian Huber, Volker Steinhage<br />
**Abstract:** <details><summary>原文: </summary>While a variety of methods offer good yield prediction on histogrammed remote sensing data, vision Transformers are only sparsely represented in the literature. The Convolution vision Transformer (CvT) is being tested to evaluate vision Transformers that are currently achieving state-of-the-art results in many other vision tasks. CvT combines some of the advantages of convolution with the advantages of dynamic attention and global context fusion of Transformers. It performs worse than widely tested methods such as XGBoost and CNNs, but shows that Transformers have potential to improve yield prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然多种方法可以对直方图遥感数据提供良好的产量预测，但视觉 Transformer 在文献中只很少出现。卷积视觉 Transformer (CvT) 正在接受测试，以评估目前在许多其他视觉任务中取得最先进结果的视觉 Transformer。 CvT结合了卷积的一些优点和Transformers的动态注意力和全局上下文融合的优点。它的性能比经过广泛测试的方法（例如 XGBoost 和 CNN）差，但表明 Transformer 有潜力提高良率预测。</details>
**PDF:** <http://arxiv.org/pdf/2402.05557v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis**<br />
**Title_cn:** MIGC：用于文本到图像合成的多实例生成控制器<br />
**Authors:** Dewei Zhou, You Li, Fan Ma, Zongxin Yang, Yi Yang<br />
**Abstract:** <details><summary>原文: </summary>We present a Multi-Instance Generation (MIG) task, simultaneously generating multiple instances with diverse controls in one image. Given a set of predefined coordinates and their corresponding descriptions, the task is to ensure that generated instances are accurately at the designated locations and that all instances' attributes adhere to their corresponding description. This broadens the scope of current research on Single-instance generation, elevating it to a more versatile and practical dimension. Inspired by the idea of divide and conquer, we introduce an innovative approach named Multi-Instance Generation Controller (MIGC) to address the challenges of the MIG task. Initially, we break down the MIG task into several subtasks, each involving the shading of a single instance. To ensure precise shading for each instance, we introduce an instance enhancement attention mechanism. Lastly, we aggregate all the shaded instances to provide the necessary information for accurately generating multiple instances in stable diffusion (SD). To evaluate how well generation models perform on the MIG task, we provide a COCO-MIG benchmark along with an evaluation pipeline. Extensive experiments were conducted on the proposed COCO-MIG benchmark, as well as on various commonly used benchmarks. The evaluation results illustrate the exceptional control capabilities of our model in terms of quantity, position, attribute, and interaction.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种多实例生成（MIG）任务，在一张图像中同时生成具有不同控件的多个实例。给定一组预定义的坐标及其相应的描述，任务是确保生成的实例准确地位于指定位置，并且所有实例的属性都遵循其相应的描述。这拓宽了当前单实例生成的研究范围，将其提升到更加通用和实用的维度。受分而治之思想的启发，我们引入了一种名为多实例生成控制器（MIGC）的创新方法来解决 MIG 任务的挑战。最初，我们将 MIG 任务分解为多个子任务，每个子任务都涉及单个实例的着色。为了确保每个实例的精确着色，我们引入了实例增强注意机制。最后，我们聚合所有着色实例，以提供在稳定扩散（SD）中准确生成多个实例所需的信息。为了评估生成模型在 MIG 任务上的表现，我们提供了 COCO-MIG 基准以及评估流程。对提议的 COCO-MIG 基准以及各种常用基准进行了广泛的实验。评估结果说明了我们的模型在数量、位置、属性和交互方面卓越的控制能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.05408v1><br />
**Code:** <https://github.com/limuloo/migc>**<br />
>>**index:** 8<br />
**Title:** **Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis**<br />
**Title_cn:** 释放几何的无限力量：用于全玻片组织病理学图像分析的新型几何感知转换器 (GOAT)<br />
**Authors:** Mingxin Liu, Yunzan Liu, Pengbo Xu, Jiquan Ma<br />
**Abstract:** <details><summary>原文: </summary>The histopathology analysis is of great significance for the diagnosis and prognosis of cancers, however, it has great challenges due to the enormous heterogeneity of gigapixel whole slide images (WSIs) and the intricate representation of pathological features. However, recent methods have not adequately exploited geometrical representation in WSIs which is significant in disease diagnosis. Therefore, we proposed a novel weakly-supervised framework, Geometry-Aware Transformer (GOAT), in which we urge the model to pay attention to the geometric characteristics within the tumor microenvironment which often serve as potent indicators. In addition, a context-aware attention mechanism is designed to extract and enhance the morphological features within WSIs.</details>
**Abstract_cn:** <details><summary>译文: </summary>组织病理学分析对于癌症的诊断和预后具有重要意义，但由于十亿像素全切片图像（WSI）的巨大异质性和病理特征的复杂表示，它面临着巨大的挑战。然而，最近的方法尚未充分利用 WSI 中的几何表示，这对于疾病诊断具有重要意义。因此，我们提出了一种新的弱监督框架，即几何感知变压器（GOAT），我们敦促模型关注肿瘤微环境内的几何特征，这些特征通常作为有效的指标。此外，还设计了上下文感知注意机制来提取和增强 WSI 内的形态特征。</details>
**PDF:** <http://arxiv.org/pdf/2402.05373v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging**<br />
**Title_cn:** 用于噪声干涉成像中相位检索的 3D-2D 神经网络<br />
**Authors:** Andrew H. Proppe, Guillaume Thekkadath, Duncan England, Philip J. Bustard, Frédéric Bouchard, Jeff S. Lundeen, Benjamin J. Sussman<br />
**Abstract:** <details><summary>原文: </summary>In recent years, neural networks have been used to solve phase retrieval problems in imaging with superior accuracy and speed than traditional techniques, especially in the presence of noise. However, in the context of interferometric imaging, phase noise has been largely unaddressed by existing neural network architectures. Such noise arises naturally in an interferometer due to mechanical instabilities or atmospheric turbulence, limiting measurement acquisition times and posing a challenge in scenarios with limited light intensity, such as remote sensing. Here, we introduce a 3D-2D Phase Retrieval U-Net (PRUNe) that takes noisy and randomly phase-shifted interferograms as inputs, and outputs a single 2D phase image. A 3D downsampling convolutional encoder captures correlations within and between frames to produce a 2D latent space, which is upsampled by a 2D decoder into a phase image. We test our model against a state-of-the-art singular value decomposition algorithm and find PRUNe reconstructions consistently show more accurate and smooth reconstructions, with a x2.5 - 4 lower mean squared error at multiple signal-to-noise ratios for interferograms with low (< 1 photon/pixel) and high (~100 photons/pixel) signal intensity. Our model presents a faster and more accurate approach to perform phase retrieval in extremely low light intensity interferometry in presence of phase noise, and will find application in other multi-frame noisy imaging techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，神经网络已被用来解决成像中的相位恢复问题，其精度和速度比传统技术更高，尤其是在存在噪声的情况下。然而，在干涉成像的背景下，现有的神经网络架构基本上没有解决相位噪声问题。由于机械不稳定性或大气湍流，这种噪声自然会在干涉仪中产生，限制了测量采集时间，并对光强度有限的场景（例如遥感）构成了挑战。在这里，我们引入了 3D-2D 相位检索 U-Net (PRUNe)，它将噪声和随机相移干涉图作为输入，并输出单个 2D 相位图像。 3D 下采样卷积编码器捕获帧内和帧之间的相关性以生成 2D 潜在空间，该空间由 2D 解码器上采样为相位图像。我们针对最先进的奇异值分解算法测试我们的模型，发现 PRUNe 重建始终显示出更准确和平滑的重建，在干涉图的多个信噪比下均方误差降低了 2.5 - 4 倍具有低（< 1 光子/像素）和高（~100 光子/像素）信号强度。我们的模型提出了一种更快、更准确的方法，可以在存在相位噪声的情况下在极低光强度干涉测量中执行​​相位检索，并将在其他多帧噪声成像技术中找到应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.06063v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write**<br />
**Title_cn:** InkSight：通过学习读写实现离线到在线手写转换<br />
**Authors:** Blagoj Mitrevski, Arina Rak, Julian Schnitzler, Chengkun Li, Andrii Maksai, Jesse Berent, Claudiu Musat<br />
**Abstract:** <details><summary>原文: </summary>Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore, it generalizes beyond its training domain into simple sketches. Our human evaluation reveals that 87% of the samples produced by our model on the challenging HierText dataset are considered as a valid tracing of the input image and 67% look like a pen trajectory traced by a human.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字笔记越来越受欢迎，它提供了一种持久、可编辑且易于索引的矢量化形式（称为数字墨水）存储笔记的方式。然而，这种笔记方式与传统的纸笔笔记方式之间仍然存在很大差距，而传统的纸笔笔记方式仍然受到绝大多数人的青睐。我们的工作 InkSight 旨在弥合这一差距，让实体笔记记录者能够轻松地将他们的工作（离线手写）转换为数字墨水（在线手写），我们将这一过程称为“去渲染”。先前关于该主题的研究主要集中在图像的几何属性上，导致其训练领域之外的泛化能力有限。我们的方法结合了阅读和写作先验，允许在缺乏大量难以获得的配对样本的情况下训练模型。据我们所知，这是第一个能够有效地渲染具有不同视觉特征和背景的任意照片中的手写文本的作品。此外，它超越了其训练领域，概括为简单的草图。我们的人工评估表明，我们的模型在具有挑战性的 HierText 数据集上生成的样本中有 87% 被认为是对输入图像的有效跟踪，而 67% 看起来像人类跟踪的笔轨迹。</details>
**PDF:** <http://arxiv.org/pdf/2402.05804v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery**<br />
**Title_cn:** UAV-Rain1k：无人机航拍图像去除雨滴的基准<br />
**Authors:** Wenhui Chang, Hongming Chen, Xin He, Xiang Chen, Liangduo Shen<br />
**Abstract:** <details><summary>原文: </summary>Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality. Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight. To fill the gap in this research, we first construct a new benchmark dataset for removing raindrops from UAV images, called UAV-Rain1k. In this letter, we provide a dataset generation pipeline, which includes modeling raindrop shapes using Blender, collecting background images from various UAV angles, random sampling of rain masks and etc. Based on the proposed benchmark, we further present a comprehensive evaluation of existing representative image deraining algorithms, and reveal future research opportunities worth exploring. The proposed dataset will be publicly available at https://github.com/cschenxiang/UAV-Rain1k.</details>
**Abstract_cn:** <details><summary>译文: </summary>粘附在无人机镜头上的雨滴会阻碍背景场景的可见度并降低图像质量。尽管最近在图像除雨方法和数据集方面取得了进展，但由于无人机飞行过程中角度变化和快速移动带来的独特挑战，人们对无人机航空图像中的雨滴去除缺乏关注。为了填补这项研究的空白，我们首先构建了一个新的用于从无人机图像中去除雨滴的基准数据集，称为 UAV-Rain1k。在这封信中，我们提供了一个数据集生成流程，其中包括使用 Blender 建模雨滴形状、从不同无人机角度收集背景图像、雨罩的随机采样等。基于所提出的基准，我们进一步对现有代表性模型进行了综合评估图像去雨算法，并揭示未来值得探索的研究机会。拟议的数据集将在 https://github.com/cschenfang/UAV-Rain1k 上公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.05773v1><br />
**Code:** <https://github.com/cschenxiang/uav-rain1k>**<br />
>>**index:** 4<br />
**Title:** **An Optimization-based Baseline for Rigid 2D/3D Registration Applied to Spine Surgical Navigation Using CMA-ES**<br />
**Title_cn:** 基于优化的刚性 2D/3D 配准基线应用于使用 CMA-ES 的脊柱手术导航<br />
**Authors:** Minheng Chen, Tonglong Li, Zhirun Zhang, Youyong Kong<br />
**Abstract:** <details><summary>原文: </summary>A robust and efficient optimization-based 2D/3D registration framework is crucial for the navigation system of orthopedic surgical robots. It can provide precise position information of surgical instruments and implants during surgery. While artificial intelligence technology has advanced rapidly in recent years, traditional optimization-based registration methods remain indispensable in the field of 2D/3D registration.he exceptional precision of this method enables it to be considered as a post-processing step of the learning-based methods, thereby offering a reliable assurance for registration. In this paper, we present a coarse-to-fine registration framework based on the CMA-ES algorithm. We conducted intensive testing of our method using data from different parts of the spine. The results shows the effectiveness of the proposed framework on real orthopedic spine surgery clinical data. This work can be viewed as an additional extension that complements the optimization-based methods employed in our previous studies.</details>
**Abstract_cn:** <details><summary>译文: </summary>强大而高效的基于优化的 2D/3D 配准框架对于骨科手术机器人的导航系统至关重要。它可以在手术过程中提供手术器械和植入物的精确位置信息。尽管人工智能技术近年来发展迅速，但传统的基于优化的配准方法在2D/3D配准领域仍然不可或缺。该方法卓越的精度使其可以被视为基于学习的配准方法的后处理步骤。方式，为注册提供可靠保证。在本文中，我们提出了一种基于 CMA-ES 算法的从粗到细的配准框架。我们使用来自脊柱不同部位的数据对我们的方法进行了深入的测试。结果显示了所提出的框架在真实骨科脊柱手术临床数据上的有效性。这项工作可以被视为一个额外的扩展，补充了我们之前研究中采用的基于优化的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05642v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction**<br />
**Title_cn:** NCRF：用于手-物体交互的自由视点渲染的神经接触辐射场<br />
**Authors:** Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis<br />
**Abstract:** <details><summary>原文: </summary>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>在 3D 计算机视觉中，对手与物体交互进行建模是一项极具挑战性的任务。尽管在这一领域取得了显着的进展，但现有的方法仍然无法真实地合成手部物体交互，由于手部和物体之间的严重相互遮挡而导致渲染质量下降，并且手部物体姿势不准确估计。为了应对这些挑战，我们提出了一种新颖的自由视点渲染框架，即神经接触辐射场（NCRF），以从一组稀疏视频中重建手部对象交互。特别是，所提出的 NCRF 框架由两个关键组件组成：（a）接触优化字段，用于根据 3D 查询点预测准确的接触字段，以实现手与物体之间的理想接触。 (b) 手部物体神经辐射场，用于学习静态规范空间中的隐式手部物体表示，与专门设计的手部物体运动场相配合，以产生观察到规范的对应关系。我们共同学习这些关键组件，它们在视觉和几何约束下相互帮助和规范，产生高质量的手部物体重建，从而实现照片般逼真的新颖视图合成。对 HO3D 和 DexYCB 数据集的大量实验表明，我们的方法在渲染质量和姿态估计精度方面均优于当前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05532v2><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems**<br />
**Title_cn:** 用于反问题的内存高效深度端到端后验网络（DEEPEN）<br />
**Authors:** Jyothi Rikhab Chand, Mathews Jacob<br />
**Abstract:** <details><summary>原文: </summary>End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic Resonance (MR) image recovery, but suffer from high memory usage during training. In addition, these deterministic approaches do not offer opportunities for sampling from the posterior distribution. In this paper, we introduce a memory-efficient approach for E2E learning of the posterior distribution. We represent this distribution as the combination of a data-consistency-induced likelihood term and an energy model for the prior, parameterized by a Convolutional Neural Network (CNN). The CNN weights are learned from training data in an E2E fashion using maximum likelihood optimization. The learned model enables the recovery of images from undersampled measurements using the Maximum A Posteriori (MAP) optimization. In addition, the posterior model can be sampled to derive uncertainty maps about the reconstruction. Experiments on parallel MR image reconstruction show that our approach performs comparable to the memory-intensive E2E unrolled algorithm, performs better than its memory-efficient counterpart, and can provide uncertainty maps. Our framework paves the way towards MR image reconstruction in 3D and higher dimensions</details>
**Abstract_cn:** <details><summary>译文: </summary>端到端 (E2E) 展开优化框架显示出磁共振 (MR) 图像恢复的前景，但在训练期间会遇到高内存使用率的问题。此外，这些确定性方法不提供从后验分布中采样的机会。在本文中，我们介绍了一种用于后验分布的 E2E 学习的内存有效方法。我们将此分布表示为数据一致性引起的似然项和先验能量模型的组合，由卷积神经网络 (CNN) 参数化。 CNN 权重是使用最大似然优化以 E2E 方式从训练数据中学习的。学习模型能够使用最大后验概率 (MAP) 优化从欠采样测量中恢复图像。此外，可以对后验模型进行采样以导出有关重建的不确定性图。并行 MR 图像重建实验表明，我们的方法的性能与内存密集型 E2E 展开算法相当，比内存高效的对应算法性能更好，并且可以提供不确定性图。我们的框架为 3D 和更高维度的 MR 图像重建铺平了道路</details>
**PDF:** <http://arxiv.org/pdf/2402.05422v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning**<br />
**Title_cn:** TaE：长尾类增量学习的任务感知可扩展表示<br />
**Authors:** Linjie Li, S. Liu, Zhenyu Wu, JI yang<br />
**Abstract:** <details><summary>原文: </summary>Class-incremental learning (CIL) aims to train classifiers that learn new classes without forgetting old ones. Most CIL methods focus on balanced data distribution for each task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced, which trains on data where head classes have more samples than tail classes. Existing methods mainly focus on preserving representative samples from previous classes to combat catastrophic forgetting. Recently, dynamic network algorithms frozen old network structures and expanded new ones, achieving significant performance. However, with the introduction of the long-tail problem, merely extending task-specific parameters can lead to miscalibrated predictions, while expanding the entire model results in an explosion of memory size. To address these issues, we introduce a novel Task-aware Expandable (TaE) framework, dynamically allocating and updating task-specific trainable parameters to learn diverse representations from each incremental task, while resisting forgetting through the majority of frozen model parameters. To further encourage the class-specific feature representation, we develop a Centroid-Enhanced (CEd) method to guide the update of these task-aware parameters. This approach is designed to adaptively minimize the distances between intra-class features while simultaneously maximizing the distances between inter-class features across all seen classes. The utility of this centroid-enhanced method extends to all "training from scratch" CIL algorithms. Extensive experiments were conducted on CIFAR-100 and ImageNet100 under different settings, which demonstrates that TaE achieves state-of-the-art performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>类增量学习（CIL）旨在训练分类器学习新类而不忘记旧类。大多数 CIL 方法侧重于每个任务的平衡数据分布，而忽略了现实世界的长尾分布。因此，引入了长尾类增量学习（LT-CIL），它在头类比尾类拥有更多样本的数据上进行训练。现有的方法主要集中于保留先前类别的代表性样本以对抗灾难性遗忘。最近，动态网络算法冻结了旧的网络结构并扩展了新的网络结构，取得了显着的性能。然而，随着长尾问题的引入，仅仅扩展特定于任务的参数可能会导致错误校准的预测，而扩展整个模型会导致内存大小的爆炸。为了解决这些问题，我们引入了一种新颖的任务感知可扩展（TaE）框架，动态分配和更新特定于任务的可训练参数，以从每个增量任务中学习不同的表示，同时通过大多数冻结的模型参数来防止遗忘。为了进一步鼓励特定于类的特征表示，我们开发了质心增强（CEd）方法来指导这些任务感知参数的更新。这种方法旨在自适应地最小化类内特征之间的距离，同时最大化所有可见类的类间特征之间的距离。这种质心增强方法的实用性扩展到所有“从头开始训练”的 CIL 算法。在不同设置下的 CIFAR-100 和 ImageNet100 上进行了大量实验，这表明 TaE 实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.05797v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object**<br />
**Title_cn:** FuncGrasp：从单个带注释的示例对象中学习以对象为中心的神经抓取功能<br />
**Authors:** Hanzhi Chen, Binbin Xu, Stefan Leutenegger<br />
**Abstract:** <details><summary>原文: </summary>We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 FuncGrasp，这是一个框架，可以使用一个带注释的对象和通过分类先验的单视图 RGB-D 观察来推断出看不见的对象的密集而可靠的抓取配置。与之前仅传输一组抓取姿势的作品不同，FuncGrasp 的目标是跨不同实例传输由以对象为中心的连续抓取函数参数化的无限配置。为了简化传输过程，我们提出了神经表面抓取场（NSGF），这是一种在表面上定义的有效神经表示，用于密集编码抓取配置。此外，我们利用球原语利用函数到函数的迁移来建立语义上有意义的分类对应关系，这些对应关系是在没有任何专业知识的情况下以无监督的方式学习的。我们通过在模拟器和现实世界中进行大量实验来展示其有效性。值得注意的是，我们的框架在生成抓取的密度和可靠性方面显着优于几种强大的基线方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05644v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Impact on Public Health Decision Making by Utilizing Big Data Without Domain Knowledge**<br />
**Title_cn:** 在没有领域知识的情况下利用大数据对公共卫生决策的影响<br />
**Authors:** Miao Zhang, Salman Rahman, Vishwali Mhasawade, Rumi Chunara<br />
**Abstract:** <details><summary>原文: </summary>New data sources, and artificial intelligence (AI) methods to extract information from them are becoming plentiful, and relevant to decision making in many societal applications. An important example is street view imagery, available in over 100 countries, and considered for applications such as assessing built environment aspects in relation to community health outcomes. Relevant to such uses, important examples of bias in the use of AI are evident when decision-making based on data fails to account for the robustness of the data, or predictions are based on spurious correlations. To study this risk, we utilize 2.02 million GSV images along with health, demographic, and socioeconomic data from New York City. Initially, we demonstrate that built environment characteristics inferred from GSV labels at the intra-city level may exhibit inadequate alignment with the ground truth. We also find that the average individual-level behavior of physical inactivity significantly mediates the impact of built environment features by census tract, as measured through GSV. Finally, using a causal framework which accounts for these mediators of environmental impacts on health, we find that altering 10% of samples in the two lowest tertiles would result in a 4.17 (95% CI 3.84 to 4.55) or 17.2 (95% CI 14.4 to 21.3) times bigger decrease on the prevalence of obesity or diabetes, than the same proportional intervention on the number of crosswalks by census tract. This work illustrates important issues of robustness and model specification for informing effective allocation of interventions using new data sources.</details>
**Abstract_cn:** <details><summary>译文: </summary>新的数据源以及从中提取信息的人工智能 (AI) 方法变得越来越丰富，并且与许多社会应用中的决策相关。一个重要的例子是街景图像，该图像在 100 多个国家/地区可用，并考虑用于评估与社区健康结果相关的建筑环境方面等应用。与此类用途相关的是，当基于数据的决策未能考虑到数据的稳健性，或者预测基于虚假相关性时，人工智能使用中存在偏见的重要例子就很明显。为了研究这一风险，我们利用了 202 万张 GSV 图像以及来自纽约市的健康、人口和社会经济数据。最初，我们证明从城市内的 GSV 标签推断出的建筑环境特征可能与真实情况不相符。我们还发现，通过 GSV 测量，平均个人层面的身体不活动行为显着介导了人口普查区建成环境特征的影响。最后，使用解释环境对健康影响的这些中介因素的因果框架，我们发现改变两个最低三分位数中 10% 的样本将导致 4.17（95% CI 3.84 至 4.55）或 17.2（95% CI 14.4）与按人口普查区对人行横道数量进行相同比例的干预相比，肥胖或糖尿病患病率的下降幅度要大 21.3 倍。这项工作说明了稳健性和模型规范的重要问题，以便使用新数据源有效分配干预措施。</details>
**PDF:** <http://arxiv.org/pdf/2402.06059v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Contrastive Approach to Prior Free Positive Unlabeled Learning**<br />
**Title_cn:** 先前自由积极无标记学习的对比方法<br />
**Authors:** Anish Acharya, Sujay Sanghavi<br />
**Abstract:** <details><summary>原文: </summary>Positive Unlabeled (PU) learning refers to the task of learning a binary classifier given a few labeled positive samples, and a set of unlabeled samples (which could be positive or negative). In this paper, we propose a novel PU learning framework, that starts by learning a feature space through pretext-invariant representation learning and then applies pseudo-labeling to the unlabeled examples, leveraging the concentration property of the embeddings. Overall, our proposed approach handily outperforms state-of-the-art PU learning methods across several standard PU benchmark datasets, while not requiring a-priori knowledge or estimate of class prior. Remarkably, our method remains effective even when labeled data is scant, where most PU learning algorithms falter. We also provide simple theoretical analysis motivating our proposed algorithms and establish generalization guarantee for our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>正向无标记（PU）学习是指在给定一些标记的正样本和一组未标记样本（可以是正样本或负样本）的情况下学习二元分类器的任务。在本文中，我们提出了一种新颖的 PU 学习框架，首先通过借口不变表示学习来学习特征空间，然后利用嵌入的集中特性将伪标记应用于未标记的示例。总体而言，我们提出的方法在多个标准 PU 基准数据集上轻松优于最先进的 PU 学习方法，同时不需要先验知识或类先验估计。值得注意的是，即使标记数据很少（大多数 PU 学习算法都会出现问题），我们的方法仍然有效。我们还提供简单的理论分析来激发我们提出的算法，并为我们的方法建立泛化保证。</details>
**PDF:** <http://arxiv.org/pdf/2402.06038v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations**<br />
**Title_cn:** 隐藏在众目睽睽之下：对弱势患者群体的不可察觉的对抗性偏见攻击<br />
**Authors:** Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul H. Yi, Vishwa S. Parekh<br />
**Abstract:** <details><summary>原文: </summary>The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能 (AI) 在放射学领域的普及揭示了深度学习 (DL) 模型加剧对弱势患者群体的临床偏见的风险。虽然之前的文献重点关注量化训练有素的深度学习模型所表现出的偏差，但针对深度学习模型的人口统计学目标对抗性偏差攻击及其在临床环境中的影响仍然是医学成像研究领域中尚未充分探索的领域。在这项工作中，我们证明了以人口统计为目标的标签中毒攻击可能会在深度学习模型中引入对抗性诊断不足偏差，并降低代表性不足群体的性能，而不会影响整体模型性能。此外，我们对多个绩效指标和人口群体（如性别、年龄及其交叉子群体）的结果表明，一个群体对不可检测的对抗性偏见攻击的脆弱性与其在模型训练数据中的表示直接相关。</details>
**PDF:** <http://arxiv.org/pdf/2402.05713v1><br />
**Code:** <https://github.com/um2ii/hiddeninplainsight>**<br />
>>**index:** 4<br />
**Title:** **Real-time Holistic Robot Pose Estimation with Unknown States**<br />
**Title_cn:** 未知状态下的实时整体机器人姿态估计<br />
**Authors:** Shikun Ban, Juling Fan, Wentao Zhu, Xiaoxuan Ma, Yu Qiao, Yizhou Wang<br />
**Abstract:** <details><summary>原文: </summary>Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles, which are not always available in real-world scenarios. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work addresses the urgent need for efficient robot pose estimation with unknown states. We propose an end-to-end pipeline for real-time, holistic robot pose estimation from a single RGB image, even in the absence of known robot states. Our method decomposes the problem into estimating camera-to-robot rotation, robot state parameters, keypoint locations, and root depth. We further design a corresponding neural network module for each task. This approach allows for learning multi-facet representations and facilitates sim-to-real transfer through self-supervised learning. Notably, our method achieves inference with a single feedforward, eliminating the need for costly test-time iterative optimization. As a result, it delivers a 12-time speed boost with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time. Code is available at https://oliverbansk.github.io/Holistic-Robot-Pose/.</details>
**Abstract_cn:** <details><summary>译文: </summary>从 RGB 图像估计机器人姿态是计算机视觉和机器人技术中的一个关键问题。虽然以前的方法已经取得了有希望的性能，但大多数方法都假设完全了解机器人的内部状态，例如地面真实的机器人关节角度，在现实场景中并不总是可用。另一方面，现有的在没有关节状态先验的情况下估计机器人姿态的方法面临着沉重的计算负担，因此无法支持实时应用。这项工作解决了对未知状态下高效机器人姿态估计的迫切需求。我们提出了一种端到端的管道，即使在没有已知的机器人状态的情况下，也可以根据单个 RGB 图像进行实时、整体的机器人姿态估计。我们的方法将问题分解为估计相机到机器人的旋转、机器人状态参数、关键点位置和根深度。我们进一步为每个任务设计了相应的神经网络模块。这种方法允许学习多方面表示，并通过自我监督学习促进模拟到真实的迁移。值得注意的是，我们的方法通过单个前馈实现推理，消除了昂贵的测试时迭代优化的需要。因此，它的速度提升了 12 倍，并且具有最先进的精度，首次实现了实时整体机器人姿态估计。代码可在 https://oliverbansk.github.io/Holistic-Robot-Pose/ 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.05655v1><br />
**Code:** <https://github.com/Oliverbansk/Hollistic-Robot-Pose-Estimation>**<br />
>>**index:** 5<br />
**Title:** **Learning pseudo-contractive denoisers for inverse problems**<br />
**Title_cn:** 学习逆问题的伪收缩降噪器<br />
**Authors:** Deliang Wei, Peng Chen, Fang Li<br />
**Abstract:** <details><summary>原文: </summary>Deep denoisers have shown excellent performance in solving inverse problems in signal and image processing. In order to guarantee the convergence, the denoiser needs to satisfy some Lipschitz conditions like non-expansiveness. However, enforcing such constraints inevitably compromises recovery performance. This paper introduces a novel training strategy that enforces a weaker constraint on the deep denoiser called pseudo-contractiveness. By studying the spectrum of the Jacobian matrix, relationships between different denoiser assumptions are revealed. Effective algorithms based on gradient descent and Ishikawa process are derived, and further assumptions of strict pseudo-contractiveness yield efficient algorithms using half-quadratic splitting and forward-backward splitting. The proposed algorithms theoretically converge strongly to a fixed point. A training strategy based on holomorphic transformation and functional calculi is proposed to enforce the pseudo-contractive denoiser assumption. Extensive experiments demonstrate superior performance of the pseudo-contractive denoiser compared to related denoisers. The proposed methods are competitive in terms of visual effects and quantitative values.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度降噪器在解决信号和图像处理中的逆问题方面表现出了出色的性能。为了保证收敛，降噪器需要满足一些 Lipschitz 条件，例如非扩张性。然而，强制实施此类限制不可避免地会损害恢复性能。本文介绍了一种新颖的训练策略，该策略对深度降噪器施加较弱的约束，称为伪收缩性。通过研究雅可比矩阵的谱，揭示了不同降噪器假设之间的关系。推导了基于梯度下降和石川过程的有效算法，并且严格的伪收缩性的进一步假设产生了使用半二次分裂和前向-后向分裂的有效算法。所提出的算法理论上强烈收敛到一个固定点。提出了一种基于全纯变换和函数演算的训练策略来强制执行伪收缩降噪器假设。大量实验证明，与相关降噪器相比，伪收缩降噪器具有优越的性能。所提出的方法在视觉效果和定量值方面具有竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2402.05637v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Extending 6D Object Pose Estimators for Stereo Vision**<br />
**Title_cn:** 扩展立体视觉的 6D 物体姿态估计器<br />
**Authors:** Thomas Pöllabauer, Jan Emrich, Volker Knauthe, Arjan Kuijper<br />
**Abstract:** <details><summary>原文: </summary>Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确、快速、稳健地估计物体的 6D 位姿仍然是一项艰巨的任务。然而，最近使用密集特征从 RGB 图像中直接回归姿势的方法已经取得了最先进的结果。立体视觉提供了观察物体的额外视角，有助于减少姿势模糊和遮挡。此外，立体视觉可以直接推断物体的距离，而单视觉则需要对物体大小的内在了解。为了将最先进的 6D 物体姿态估计扩展到立体，我们创建了 YCB-V 数据集的 BOP 兼容立体版本。我们的方法利用立体视觉，优于最先进的 6D 姿态估计算法，并且可以轻松地用于其他基于密集特征的算法。</details>
**PDF:** <http://arxiv.org/pdf/2402.05610v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only**<br />
**Title_cn:** 仅使用合成数据从历史草图重建灰泥雕像的概念<br />
**Authors:** Thomas Pöllabauer, Julius Kühn<br />
**Abstract:** <details><summary>原文: </summary>In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall. Today, many of these statues are destroyed, but using the original drawings, deriving from the red color also called sinopia, we can reconstruct how the final statue might have looked.We propose a fully-automated approach to reconstruct a point cloud and show preliminary results by generating a color-image, a depth-map, as well as surface normals requiring only a single sketch, and without requiring a collection of other, similar samples. Our proposed solution allows real-time reconstruction on-site, for instance, within an exhibition, or to generate a useful starting point for an expert, trying to manually reconstruct the statue, all while using only synthetic data for training.</details>
**Abstract_cn:** <details><summary>译文: </summary>在中世纪，灰泥工人首先使用一种被称为“sinopia”的红色在墙上画出即将制作的雕像的草图。如今，许多雕像都被毁坏了，但使用原始图纸（源自红色，也称为 sinopia），我们可以重建最终雕像的外观。我们提出了一种全自动方法来重建点云并显示初步的结果通过生成彩色图像、深度图以及表面法线来获得结果，仅需要一个草图，而不需要收集其他类似的样本。我们提出的解决方案允许现场实时重建，例如在展览中，或者为专家生成有用的起点，尝试手动重建雕像，同时仅使用合成数据进行训练。</details>
**PDF:** <http://arxiv.org/pdf/2402.05593v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction**<br />
**Title_cn:** 用于动态运动提取的基于神经图形基元的可变形图像配准<br />
**Authors:** Xia Li, Fabian Zhang, Muheng Li, Damien Weber, Antony Lomax, Joachim Buhmann, Ye Zhang<br />
**Abstract:** <details><summary>原文: </summary>Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR). However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios. This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF). Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network. Uniquely, it enables self-supervised optimization at an ultra-fast speed, negating the need for pre-training on extensive datasets and allowing seamless adaptation to new cases. We validated this approach on the 4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of 1.15\pm1.15 mm within a remarkable time of 1.77 seconds. Notably, our method also addresses the sliding boundary problem, a common challenge in conventional DIR methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>放射治疗中的分次内运动通常使用可变形图像配准 (DIR) 进行建模。然而，现有方法往往难以平衡速度和准确性，限制了它们在临床场景中的适用性。本研究介绍了一种利用神经图形基元 (NGP) 来优化位移矢量场 (DVF) 的新颖方法。我们的方法利用学习的基元，将其处理为 splats，并使用浅层神经网络在空间内进行插值。独特的是，它能够以超快的速度进行自我监督优化，无需对大量数据集进行预训练，并允许无缝适应新案例。我们在 4D-CT 肺部数据集 DIR-lab 上验证了这种方法，在 1.77 秒的时间内实现了 1.15\pm1.15 mm 的目标配准误差 (TRE)。值得注意的是，我们的方法还解决了滑动边界问题，这是传统 DIR 方法中的常见挑战。</details>
**PDF:** <http://arxiv.org/pdf/2402.05568v1><br />
**Code:** null<br />

