## [UPDATED!] **2024-02-09** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Sequential Flow Matching for Generative Modeling**<br />
**Title_cn:** 用于生成建模的顺序流匹配<br />
**Authors:** Jongmin Yoon, Juho Lee<br />
**Abstract:** <details><summary>原文: </summary>Straightening the probability flow of the continuous-time generative models, such as diffusion models or flow-based models, is the key to fast sampling through the numerical solvers, existing methods learn a linear path by directly generating the probability path the joint distribution between the noise and data distribution. One key reason for the slow sampling speed of the ODE-based solvers that simulate these generative models is the global truncation error of the ODE solver, caused by the high curvature of the ODE trajectory, which explodes the truncation error of the numerical solvers in the low-NFE regime. To address this challenge, We propose a novel method called SeqRF, a learning technique that straightens the probability flow to reduce the global truncation error and hence enable acceleration of sampling and improve the synthesis quality. In both theoretical and empirical studies, we first observe the straightening property of our SeqRF. Through empirical evaluations via SeqRF over flow-based generative models, We achieve surpassing results on CIFAR-10, CelebA-$64 \times 64$, and LSUN-Church datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>拉直连续时间生成模型（例如扩散模型或基于流的模型）的概率流是通过数值求解器快速采样的关键，现有方法通过直接生成概率路径之间的联合分布来学习线性路径噪声和数据分布。模拟这些生成模型的基于 ODE 的求解器采样速度慢的一个关键原因是 ODE 求解器的全局截断误差，这是由 ODE 轨迹的高曲率引起的，这会导致数值求解器的截断误差爆炸性地增加。低 NFE 制度。为了应对这一挑战，我们提出了一种称为 SeqRF 的新方法，这是一种学习技术，可以理顺概率流以减少全局截断误差，从而加速采样并提高合成质量。在理论和实证研究中，我们首先观察 SeqRF 的矫直特性。通过 SeqRF 对基于流的生成模型进行实证评估，我们在 CIFAR-10、CelebA-$64 \times 64$ 和 LSUN-Church 数据集上取得了超越的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.06461v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ControlUDA: Controllable Diffusion-assisted Unsupervised Domain Adaptation for Cross-Weather Semantic Segmentation**<br />
**Title_cn:** ControlUDA：用于跨天气语义分割的可控扩散辅助无监督域适应<br />
**Authors:** Fengyi Shen, Li Zhou, Kagan Kucukaytekin, Ziyuan Liu, He Wang, Alois Knoll<br />
**Abstract:** <details><summary>原文: </summary>Data generation is recognized as a potent strategy for unsupervised domain adaptation (UDA) pertaining semantic segmentation in adverse weathers. Nevertheless, these adverse weather scenarios encompass multiple possibilities, and high-fidelity data synthesis with controllable weather is under-researched in previous UDA works. The recent strides in large-scale text-to-image diffusion models (DM) have ushered in a novel avenue for research, enabling the generation of realistic images conditioned on semantic labels. This capability proves instrumental for cross-domain data synthesis from source to target domain owing to their shared label space. Thus, source domain labels can be paired with those generated pseudo target data for training UDA. However, from the UDA perspective, there exists several challenges for DM training: (i) ground-truth labels from target domain are missing; (ii) the prompt generator may produce vague or noisy descriptions of images from adverse weathers; (iii) existing arts often struggle to well handle the complex scene structure and geometry of urban scenes when conditioned only on semantic labels. To tackle the above issues, we propose ControlUDA, a diffusion-assisted framework tailored for UDA segmentation under adverse weather conditions. It first leverages target prior from a pre-trained segmentor for tuning the DM, compensating the missing target domain labels; It also contains UDAControlNet, a condition-fused multi-scale and prompt-enhanced network targeted at high-fidelity data generation in adverse weathers. Training UDA with our generated data brings the model performances to a new milestone (72.0 mIoU) on the popular Cityscapes-to-ACDC benchmark for adverse weathers. Furthermore, ControlUDA helps to achieve good model generalizability on unseen data.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据生成被认为是与恶劣天气下的语义分割相关的无监督域适应（UDA）的有效策略。然而，这些恶劣天气场景包含多种可能性，并且在之前的 UDA 工作中，对可控天气的高保真数据合成的研究还不够。大规模文本到图像扩散模型（DM）的最新进展开辟了一条新的研究途径，能够生成基于语义标签的真实图像。由于共享标签空间，这种功能对于从源域到目标域的跨域数据合成非常有用。因此，源域标签可以与那些生成的伪目标数据配对以训练 UDA。然而，从 UDA 的角度来看，DM 训练存在几个挑战：（i）目标域的真实标签缺失； (ii) 提示生成器可能会对恶劣天气的图像产生模糊或嘈杂的描述； (iii) 当仅以语义标签为条件时，现有艺术常常难以很好地处理城市场景的复杂场景结构和几何形状。为了解决上述问题，我们提出了 ControlUDA，这是一种为恶劣天气条件下的 UDA 分段量身定制的扩散辅助框架。它首先利用来自预训练分割器的目标先验来调整 DM，补偿丢失的目标域标签；它还包含 UDAControlNet，这是一个条件融合的多尺度和即时增强网络，旨在在恶劣天气下生成高保真数据。使用我们生成的数据训练 UDA，使模型性能在流行的 Cityscapes-to-ACDC 恶劣天气基准上达到新的里程碑 (72.0 mIoU)。此外，ControlUDA 有助于在未见过的数据上实现良好的模型泛化性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06446v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Improving 2D-3D Dense Correspondences with Diffusion Models for 6D Object Pose Estimation**<br />
**Title_cn:** 使用扩散模型改进 2D-3D 密集对应以进行 6D 物体姿态估计<br />
**Authors:** Peter Hönig, Stefan Thalhammer, Markus Vincze<br />
**Abstract:** <details><summary>原文: </summary>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>估计 RGB 图像和 3D 空间之间的 2D-3D 对应关系是 6D 物体姿态估计中的一个基本问题。最近的姿势估计器使用密集对应图和点对点算法来估计对象姿势。位姿估计的准确性在很大程度上取决于密集对应图的质量及其承受遮挡、杂乱和具有挑战性的材料特性的能力。目前，密集对应图是使用基于 GAN、自动编码器或直接回归模型的图像到图像转换模型来估计的。然而，图像到图像转换的最新进展使得扩散模型成为在基准数据集上进行评估时的最佳选择。在本研究中，我们比较了基于 GAN 和扩散模型的图像到图像转换网络，用于 6D 物体姿态估计的下游任务。我们的结果表明，基于扩散的图像到图像转换模型优于 GAN，揭示了 6D 物体姿态估计模型进一步改进的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.06436v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting**<br />
**Title_cn:** ImplicitDeepfake：使用 NeRF 和高斯泼溅通过隐式 Deepfake 生成进行合理的换脸<br />
**Authors:** Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek<br />
**Abstract:** <details><summary>原文: </summary>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the recent rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object's shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object's characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Such techniques can have a form of artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or facial expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such relatively simple strategies can produce plausible 3D deepfake-based avatars.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多新兴的深度学习技术对计算机图形学产生了重大影响。最有希望的突破是最近兴起的神经辐射场 (NeRF) 和高斯分布 (GS)。 NeRF 使用少量具有已知相机位置的图像，在神经网络权重中对对象的形状和颜色进行编码，以生成新颖的视图。相比之下，GS 通过将对象的特征编码在高斯分布集合中，提供加速训练和推理，而不会降低渲染质量。这两种技术已在空间计算和其他领域找到了许多用例。另一方面，deepfake方法的出现引发了相当大的争议。此类技术可以采用人工智能生成的视频形式，非常模仿真实的镜头。使用生成模型，他们可以修改面部特征，从而能够创建改变的身份或面部表情，从而展现出与真人极其逼真的外观。尽管存在这些争议，但 Deepfake 可以在质量理想的情况下为头像创建和游戏提供下一代解决方案。为此，我们展示了如何结合所有这些新兴技术以获得更合理的结果。我们的ImplicitDeepfake1使用经典的deepfake算法分别修改所有训练图像，然后在修改后的面部上训练NeRF和GS。这种相对简单的策略可以产生可信的基于深度伪造的 3D 化身。</details>
**PDF:** <http://arxiv.org/pdf/2402.06390v1><br />
**Code:** <https://github.com/quereste/implicit-deepfake>**<br />
>>**index:** 5<br />
**Title:** **Multisource Semisupervised Adversarial Domain Generalization Network for Cross-Scene Sea\textendash Land Clutter Classification**<br />
**Title_cn:** 用于跨场景海\textendash陆地杂波分类的多源半监督对抗域泛化网络<br />
**Authors:** Xiaoxuan Zhang, Quan Pan, Salvador García<br />
**Abstract:** <details><summary>原文: </summary>Deep learning (DL)-based sea\textendash land clutter classification for sky-wave over-the-horizon-radar (OTHR) has become a novel research topic. In engineering applications, real-time predictions of sea\textendash land clutter with existing distribution discrepancies are crucial. To solve this problem, this article proposes a novel Multisource Semisupervised Adversarial Domain Generalization Network (MSADGN) for cross-scene sea\textendash land clutter classification. MSADGN can extract domain-invariant and domain-specific features from one labeled source domain and multiple unlabeled source domains, and then generalize these features to an arbitrary unseen target domain for real-time prediction of sea\textendash land clutter. Specifically, MSADGN consists of three modules: domain-related pseudolabeling module, domain-invariant module, and domain-specific module. The first module introduces an improved pseudolabel method called domain-related pseudolabel, which is designed to generate reliable pseudolabels to fully exploit unlabeled source domains. The second module utilizes a generative adversarial network (GAN) with a multidiscriminator to extract domain-invariant features, to enhance the model's transferability in the target domain. The third module employs a parallel multiclassifier branch to extract domain-specific features, to enhance the model's discriminability in the target domain. The effectiveness of our method is validated in twelve domain generalizations (DG) scenarios. Meanwhile, we selected 10 state-of-the-art DG methods for comparison. The experimental results demonstrate the superiority of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习（DL）的天波超视距雷达（OTHR）海\文本陆地杂波分类已成为一个新颖的研究课题。在工程应用中，实时预测存在分布差异的海/地杂波至关重要。为了解决这个问题，本文提出了一种新颖的多源半监督对抗域泛化网络（MSADGN），用于跨场景海\textendash陆地杂波分类。 MSADGN 可以从一个标记源域和多个未标记源域中提取域不变和域特定特征，然后将这些特征泛化到任意未见过的目标域，以实时预测海\textendash 陆地杂波。具体来说，MSADGN 由三个模块组成：域相关伪标记模块、域不变模块和域特定模块。第一个模块引入了一种改进的伪标签方法，称为域相关伪标签，旨在生成可靠的伪标签以充分利用未标记的源域。第二个模块利用带有多鉴别器的生成对抗网络（GAN）来提取域不变特征，以增强模型在目标域中的可迁移性。第三个模块采用并行多分类器分支来提取特定领域的特征，以增强模型在目标领域的可区分性。我们的方法的有效性在十二个领域泛化（DG）场景中得到了验证。同时，我们选择了10种最先进的DG方法进行比较。实验结果证明了我们方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06315v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain**<br />
**Title_cn:** Masked LoGoNet：医疗领域快速准确的 3D 图像分析<br />
**Authors:** Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath<br />
**Abstract:** <details><summary>原文: </summary>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于数据集构建成本高昂，因此可用的标记训练数据有限，基于标准现代机器学习的成像方法在医学应用中面临挑战。此外，在部署后，这些方法通常用于每天处理大量数据，从而给医疗设施带来高昂的维护成本。在本文中，我们介绍了一种新的神经网络架构，称为 LoGoNet，采用定制的自我监督学习 (SSL) 方法来缓解此类挑战。 LoGoNet 在 U 形架构中集成了一种新颖的特征提取器，利用大内核注意力 (LKA) 和双重编码策略来熟练地捕获长程和短程特征依赖性。这与依赖增加网络容量来增强特征提取的现有方法形成对比。考虑到学习复杂且通常不规则的身体器官形状（例如脾脏）的困难，我们模型中的这种新技术组合在医学图像分割中特别有益。作为补充，我们提出了一种专为 3D 图像量身定制的新型 SSL 方法，以弥补大型标记数据集的缺乏。该方法在多任务学习框架内结合了掩蔽和对比学习技术，并且与 Vision Transformer (ViT) 和基于 CNN 的模型兼容。我们在两个标准数据集（即 BTCV 和 MSD）的众多任务中展示了我们的方法的有效性。与八个最先进模型的基准比较突显了 LoGoNet 在推理时间和准确性方面的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.06190v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **On the Out-Of-Distribution Generalization of Multimodal Large Language Models**<br />
**Title_cn:** 多模态大语言模型的分布外泛化<br />
**Authors:** Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui<br />
**Abstract:** <details><summary>原文: </summary>We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples and test data.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们通过在分布外场景和特定领域任务下的综合评估来研究当前多模态大型语言模型（MLLM）的泛化边界。我们评估了它们在合成图像、现实世界分布变化以及医学和分子图像等专业数据集上的零样本泛化。经验结果表明，MLLM 很难在常见训练领域之外进行泛化，从而限制了它们在不进行调整的情况下的直接应用。为了理解性能不可靠的原因，我们分析了三个假设：语义误解、视觉特征提取不足和映射缺陷。结果表明，绘图缺陷是主要障碍。为了解决这个问题，我们证明上下文学习（ICL）可以显着增强 MLLM 的泛化能力，为克服泛化障碍开辟新途径。我们进一步探讨了 ICL 在分布变化下的鲁棒性，并展示了它对域变化、标签变化以及上下文示例和测试数据之间的虚假相关性变化的脆弱性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06599v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Quantifying and Enhancing Multi-modal Robustness with Modality Preference**<br />
**Title_cn:** 通过模态偏好量化和增强多模态鲁棒性<br />
**Authors:** Zequn Yang, Yake Wei, Ce Liang, Di Hu<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal models have shown a promising capability to effectively integrate information from various sources, yet meanwhile, they are found vulnerable to pervasive perturbations, such as uni-modal attacks and missing conditions. To counter these perturbations, robust multi-modal representations are highly expected, which are positioned well away from the discriminative multi-modal decision boundary. In this paper, different from conventional empirical studies, we focus on a commonly used joint multi-modal framework and theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. This discovery can further explain the limitation of multi-modal robustness and the phenomenon that multi-modal models are often vulnerable to attacks on the specific modality. Moreover, our analysis reveals how the widespread issue, that the model has different preferences for modalities, limits the multi-modal robustness by influencing the essential components and could lead to attacks on the specific modality highly effective. Inspired by our theoretical finding, we introduce a training procedure called Certifiable Robust Multi-modal Training (CRMT), which can alleviate this influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner. Our method demonstrates substantial improvements in performance and robustness compared with existing methods. Furthermore, our training procedure can be easily extended to enhance other robust training strategies, highlighting its credibility and flexibility.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态模型显示出有效整合各种来源信息的良好能力，但同时，它们也容易受到普遍扰动的影响，例如单模态攻击和缺失条件。为了应对这些扰动，人们高度期望鲁棒的多模态表示，其位置远离判别性多模态决策边界。在本文中，与传统的实证研究不同，我们关注常用的联合多模态框架，并从理论上发现，更大的单模态表示裕度和更可靠的模态集成是实现更高鲁棒性的重要组成部分。这一发现可以进一步解释多模态鲁棒性的局限性以及多模态模型往往容易受到特定模态攻击的现象。此外，我们的分析揭示了模型对模态有不同偏好的普遍问题如何通过影响基本组件来限制多模态稳健性，并可能导致对特定模态的高度有效的攻击。受我们的理论发现的启发，我们引入了一种称为可认证鲁棒多模态训练（CRMT）的训练程序，它可以减轻模态偏好的影响，并明确调节基本组成部分，以可认证的方式显着提高鲁棒性。与现有方法相比，我们的方法在性能和鲁棒性方面有了显着的改进。此外，我们的培训程序可以轻松扩展，以增强其他强大的培训策略，突出其可信度和灵活性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06244v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models**<br />
**Title_cn:** 通过潜在部分因果模型揭示多模态对比表示学习<br />
**Authors:** Yuhang Liu, Zhen Zhang, Dong Gong, Biwei Huang, Mingming Gong, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi<br />
**Abstract:** <details><summary>原文: </summary>Multimodal contrastive representation learning methods have proven successful across a range of domains, partly due to their ability to generate meaningful shared representations of complex phenomena. To enhance the depth of analysis and understanding of these acquired representations, we introduce a unified causal model specifically designed for multimodal data. By examining this model, we show that multimodal contrastive representation learning excels at identifying latent coupled variables within the proposed unified model, up to linear or permutation transformations resulting from different assumptions. Our findings illuminate the potential of pre-trained multimodal models, eg, CLIP, in learning disentangled representations through a surprisingly simple yet highly effective tool: linear independent component analysis. Experiments demonstrate the robustness of our findings, even when the assumptions are violated, and validate the effectiveness of the proposed method in learning disentangled representations.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态对比表示学习方法已被证明在一系列领域取得了成功，部分原因在于它们能够生成复杂现象的有意义的共享表示。为了增强对这些获得的表示的分析和理解的深度，我们引入了专门为多模态数据设计的统一因果模型。通过检查该模型，我们表明多模态对比表示学习擅长识别所提出的统一模型中的潜在耦合变量，直至由不同假设产生的线性或排列变换。我们的研究结果阐明了预训练的多模态模型（例如 CLIP）在通过一种令人惊讶的简单但高效的工具：线性独立分量分析来学习解缠结表示方面的潜力。实验证明了我们的研究结果的稳健性，即使假设被违反，并验证了所提出的方法在学习解缠结表示方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06223v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data**<br />
**Title_cn:** GS-CLIP：根据真实世界数据进行对比语言-图像-3D 预训练的高斯泼溅<br />
**Authors:** Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang<br />
**Abstract:** <details><summary>原文: </summary>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object's surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results.</details>
**Abstract_cn:** <details><summary>译文: </summary>以点云表示的 3D Shape 在多模态预训练方面取得了进展，以对齐图像和语言描述，这对于对象识别、分类和检索非常有用。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生了差距。为了解决这个问题，我们提出了 GS-CLIP，首次尝试将 3DGS（3D 高斯分布）引入多模态预训练中以增强 3D 表示。 GS-CLIP 利用预先训练的视觉语言模型，在大量现实世界图像-文本对上学习公共视觉和文本空间，然后学习 3D 编码器，用于对齐每个对象优化的 3DGS。此外，提出了一种新颖的高斯感知融合来提取和融合全局显式特征。作为语言-图像-3D 预训练的通用框架，GS-CLIP 与 3D 主干网络无关。挑战性实验表明，GS-CLIP 显着提高了最先进水平，超越了之前的最佳结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.06198v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting**<br />
**Title_cn:** HeadStudio：使用 3D 高斯泼溅将文本转换为可动画头部头像<br />
**Authors:** Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang<br />
**Abstract:** <details><summary>原文: </summary>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising outcomes obtained through 2D diffusion priors in recent works, current methods face challenges in achieving high-quality and animated avatars effectively. In this paper, we present $\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animated avatars from text prompts. Our method drives 3D Gaussians semantically to create a flexible and achievable appearance through the intermediate FLAME representation. Specifically, we incorporate the FLAME into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2) FLAME-based score distillation sampling, utilizing FLAME-based fine-grained control signal to guide score distillation from the text prompt. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting visually appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. They can be smoothly controlled by real-world speech and video. We hope that HeadStudio can advance digital avatar creation and that the present method can widely be applied across various domains.</details>
**Abstract_cn:** <details><summary>译文: </summary>长期以来，根据文本提示创建数字化身一直是一项令人向往但具有挑战性的任务。尽管在最近的工作中通过 2D 扩散先验获得了有希望的结果，但当前的方法在有效实现高质量和动画化身方面面临着挑战。在本文中，我们提出了 $\textbf{HeadStudio}$，这是一个新颖的框架，它利用 3D 高斯泼溅从文本提示生成逼真的动画头像。我们的方法在语义上驱动 3D 高斯，通过中间 FLAME 表示创建灵活且可实现的外观。具体来说，我们将 FLAME 合并到 3D 表示和分数蒸馏中：1）基于 FLAME 的 3D 高斯泼溅，通过将每个点绑定到 FLAME 网格来驱动 3D 高斯点。 2）基于FLAME的乐谱蒸馏采样，利用基于FLAME的细粒度控制信号从文本提示中指导乐谱蒸馏。大量的实验证明了 HeadStudio 在根据文本提示生成可动画化身、展现视觉上吸引人的外观方面的功效。这些化身能够以 1024 的分辨率渲染高质量实时（$\geq 40$ fps）新颖的视图。它们可以通过现实世界的语音和视频流畅地控制。我们希望 HeadStudio 能够推进数字化身的创作，并且本方法可以广泛应用于各个领域。</details>
**PDF:** <http://arxiv.org/pdf/2402.06149v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Multi-source-free Domain Adaptation via Uncertainty-aware Adaptive Distillation**<br />
**Title_cn:** 通过不确定性感知自适应蒸馏进行多源自由域适应<br />
**Authors:** Yaxuan Song, Jianan Fan, Dongnan Liu, Weidong Cai<br />
**Abstract:** <details><summary>原文: </summary>Source-free domain adaptation (SFDA) alleviates the domain discrepancy among data obtained from domains without accessing the data for the awareness of data privacy. However, existing conventional SFDA methods face inherent limitations in medical contexts, where medical data are typically collected from multiple institutions using various equipment. To address this problem, we propose a simple yet effective method, named Uncertainty-aware Adaptive Distillation (UAD) for the multi-source-free unsupervised domain adaptation (MSFDA) setting. UAD aims to perform well-calibrated knowledge distillation from (i) model level to deliver coordinated and reliable base model initialisation and (ii) instance level via model adaptation guided by high-quality pseudo-labels, thereby obtaining a high-performance target domain model. To verify its general applicability, we evaluate UAD on two image-based diagnosis benchmarks among two multi-centre datasets, where our method shows a significant performance gain compared with existing works. The code will be available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>无源域适应（SFDA）缓解了从域获取的数据之间的域差异，而无需访问数据，以提高数据隐私意识。然而，现有的传统 SFDA 方法在医疗环境中面临着固有的局限性，医疗数据通常是使用各种设备从多个机构收集的。为了解决这个问题，我们提出了一种简单而有效的方法，称为不确定性感知自适应蒸馏（UAD），用于无多源无监督域适应（MSFDA）设置。 UAD 旨在从（i）模型级别执行经过良好校准的知识蒸馏，以提供协调且可靠的基础模型初始化，以及（ii）通过高质量伪标签引导的模型自适应进行实例级别，从而获得高性能的目标域模型。为了验证其普遍适用性，我们在两个多中心数据集的两个基于图像的诊断基准上评估 UAD，与现有的工作相比，我们的方法显示出显着的性能增益。该代码即将推出。</details>
**PDF:** <http://arxiv.org/pdf/2402.06213v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation**<br />
**Title_cn:** 不仅仅是各个部分的总和：集成主干网络以实现少样本分割<br />
**Authors:** Nico Catalano, Alessandro Maranelli, Agnese Chiatti, Matteo Matteucci<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation is a key prerequisite to robust image understanding for applications in \acrlong{ai} and Robotics. \acrlong{fss}, in particular, concerns the extension and optimization of traditional segmentation methods in challenging conditions where limited training examples are available. A predominant approach in \acrlong{fss} is to rely on a single backbone for visual feature extraction. Choosing which backbone to leverage is a deciding factor contributing to the overall performance. In this work, we interrogate on whether fusing features from different backbones can improve the ability of \acrlong{fss} models to capture richer visual features. To tackle this question, we propose and compare two ensembling techniques-Independent Voting and Feature Fusion. Among the available \acrlong{fss} methods, we implement the proposed ensembling techniques on PANet. The module dedicated to predicting segmentation masks from the backbone embeddings in PANet avoids trainable parameters, creating a controlled `in vitro' setting for isolating the impact of different ensembling strategies. Leveraging the complementary strengths of different backbones, our approach outperforms the original single-backbone PANet across standard benchmarks even in challenging one-shot learning scenarios. Specifically, it achieved a performance improvement of +7.37\% on PASCAL-5\textsuperscript{i} and of +10.68\% on COCO-20\textsuperscript{i} in the top-performing scenario where three backbones are combined. These results, together with the qualitative inspection of the predicted subject masks, suggest that relying on multiple backbones in PANet leads to a more comprehensive feature representation, thus expediting the successful application of \acrlong{fss} methods in challenging, data-scarce environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割是 \acrlong{ai} 和机器人应用程序中强大的图像理解的关键先决条件。 \acrlong{fss} 特别涉及传统分割方法在可用训练样本有限的挑战性条件下的扩展和优化。 \acrlong{fss} 中的主要方法是依靠单个主干来提取视觉特征。选择利用哪个骨干网是影响整体性能的决定性因素。在这项工作中，我们询问融合来自不同主干的特征是否可以提高 acrlong{fss} 模型捕获更丰富的视觉特征的能力。为了解决这个问题，我们提出并比较了两种集成技术——独立投票和特征融合。在可用的 \acrlong{fss} 方法中，我们在 PANet 上实现了所提出的集成技术。专用于从 PANet 中的主干嵌入预测分割掩模的模块避免了可训练参数，创建了一个受控的“体外”设置来隔离不同集成策略的影响。利用不同骨干网的互补优势，即使在具有挑战性的一次性学习场景中，我们的方法在标准基准测试中也优于原始的单骨干 PANet。具体来说，在组合三个骨干网的最佳性能场景中，它在 PASCAL-5\textsuperscript{i} 上实现了 +7.37\% 的性能提升，在 COCO-20\textsuperscript{i} 上实现了 +10.68\% 的性能提升。这些结果加上对预测主题掩模的定性检查表明，依赖 PANet 中的多个主干网可以带来更全面的特征表示，从而加快 \acrlong{fss} 方法在具有挑战性的数据稀缺环境中的成功应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.06581v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning**<br />
**Title_cn:** 视频注释器：使用视觉语言模型和主动学习有效构建视频分类器的框架<br />
**Authors:** Amir Ziai, Aneesh Vartakavi<br />
**Abstract:** <details><summary>原文: </summary>High-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Traditional data annotation methods are resource-intensive and inefficient, often leading to a reliance on third-party annotators who are not the domain experts. Hard samples, which are usually the most informative for model training, tend to be difficult to label accurately and consistently without business context. These can arise unpredictably during the annotation process, requiring a variable number of iterations and rounds of feedback, leading to unforeseen expenses and time commitments to guarantee quality.   We posit that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We propose a novel framework we call Video Annotator (VA) for annotating, managing, and iterating on video classification datasets. Our approach offers a new paradigm for an end-user-centered model development process, enhancing the efficiency, usability, and effectiveness of video classifiers. Uniquely, VA allows for a continuous annotation process, seamlessly integrating data collection and model training.   We leverage the zero-shot capabilities of vision-language foundation models combined with active learning techniques, and demonstrate that VA enables the efficient creation of high-quality models. VA achieves a median 6.8 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments at: http://github.com/netflix/videoannotator.</details>
**Abstract_cn:** <details><summary>译文: </summary>高质量且一致的注释是成功开发稳健的机器学习模型的基础。传统的数据标注方法资源密集且效率低下，通常导致对非领域专家的第三方标注者的依赖。硬样本通常对于模型训练来说信息量最大，但在没有业务背景的情况下往往很难准确一致地进行标记。这些在注释过程中可能会出现不可预测的情况，需要不同数量的迭代和反馈轮次，从而导致不可预见的费用和时间投入以保证质量。我们认为，使用人机交互系统让领域专家更直接参与，可以解决许多实际挑战。我们提出了一种称为视频注释器（VA）的新颖框架，用于对视频分类数据集进行注释、管理和迭代。我们的方法为以最终用户为中心的模型开发过程提供了新的范例，提高了视频分类器的效率、可用性和有效性。独特的是，VA 允许连续注释过程，无缝集成数据收集和模型训练。我们利用视觉语言基础模型的零样本功能与主动学习技术相结合，并证明 VA 能够高效创建高质量模型。相对于各种任务中最具竞争力的基线，VA 的平均精度平均提高了 6.8 个百分点。我们发布了一个包含 56 个视频理解任务的 153k 标签的数据集，由三位专业视频编辑器使用 VA 进行注释，同时还发布了复制我们实验的代码：http://github.com/netflix/videoannotator。</details>
**PDF:** <http://arxiv.org/pdf/2402.06560v1><br />
**Code:** <https://github.com/netflix/videoannotator>**<br />
>>**index:** 3<br />
**Title:** **Hybridnet for depth estimation and semantic segmentation**<br />
**Title_cn:** 用于深度估计和语义分割的混合网络<br />
**Authors:** Dalila Sánchez-Escobedo, Xiao Lin, Josep R. Casas, Montse Pardàs<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation and depth estimation are two important tasks in the area of image processing. Traditionally, these two tasks are addressed in an independent manner. However, for those applications where geometric and semantic information is required, such as robotics or autonomous navigation,depth or semantic segmentation alone are not sufficient. In this paper, depth estimation and semantic segmentation are addressed together from a single input image through a hybrid convolutional network. Different from the state of the art methods where features are extracted by a sole feature extraction network for both tasks, the proposed HybridNet improves the features extraction by separating the relevant features for one task from those which are relevant for both. Experimental results demonstrate that HybridNet results are comparable with the state of the art methods, as well as the single task methods that HybridNet is based on.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割和深度估计是图像处理领域的两个重要任务。传统上，这两项任务以独立的方式解决。然而，对于那些需要几何和语义信息的应用，例如机器人或自主导航，仅深度或语义分割是不够的。在本文中，通过混合卷积网络从单个输入图像一起解决深度估计和语义分割。与现有技术中通过单一特征提取网络为两项任务提取特征不同，所提出的 HybridNet 通过将一项任务的相关特征与两项任务相关的特征分开来改进特征提取。实验结果表明，HybridNet 结果与最先进的方法以及 HybridNet 所基于的单任务方法相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.06539v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Feature Density Estimation for Out-of-Distribution Detection via Normalizing Flows**<br />
**Title_cn:** 通过标准化流进行分布外检测的特征密度估计<br />
**Authors:** Evan D. Cook, Marc-Antoine Lavoie, Steven L. Waslander<br />
**Abstract:** <details><summary>原文: </summary>Out-of-distribution (OOD) detection is a critical task for safe deployment of learning systems in the open world setting. In this work, we investigate the use of feature density estimation via normalizing flows for OOD detection and present a fully unsupervised approach which requires no exposure to OOD data, avoiding researcher bias in OOD sample selection. This is a post-hoc method which can be applied to any pretrained model, and involves training a lightweight auxiliary normalizing flow model to perform the out-of-distribution detection via density thresholding. Experiments on OOD detection in image classification show strong results for far-OOD data detection with only a single epoch of flow training, including 98.2% AUROC for ImageNet-1k vs. Textures, which exceeds the state of the art by 7.8%. We additionally explore the connection between the feature space distribution of the pretrained model and the performance of our method. Finally, we provide insights into training pitfalls that have plagued normalizing flows for use in OOD detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>分布外（OOD）检测是在开放世界环境中安全部署学习系统的一项关键任务。在这项工作中，我们研究了通过归一化 OOD 检测流来使用特征密度估计，并提出了一种完全无监督的方法，不需要接触 OOD 数据，避免了研究人员在 OOD 样本选择中的偏差。这是一种事后方法，可应用于任何预训练模型，并涉及训练轻量级辅助归一化流模型以通过密度阈值执行分布外检测。图像分类中的 OOD 检测实验显示，仅使用单个 epoch 的流训练即可实现远 OOD 数据检测，其中 ImageNet-1k 与纹理的 AUROC 为 98.2%，比现有技术高出 7.8%。我们还探讨了预训练模型的特征空间分布与我们方法的性能之间的联系。最后，我们深入探讨了困扰 OOD 检测中使用的标准化流程的训练陷阱。</details>
**PDF:** <http://arxiv.org/pdf/2402.06537v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Transferring facade labels between point clouds with semantic octrees while considering change detection**<br />
**Title_cn:** 在考虑变化检测的同时，使用语义八叉树在点云之间传输立面标签<br />
**Authors:** Sophia Schwarz, Tanja Pilz, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla<br />
**Abstract:** <details><summary>原文: </summary>Point clouds and high-resolution 3D data have become increasingly important in various fields, including surveying, construction, and virtual reality. However, simply having this data is not enough; to extract useful information, semantic labeling is crucial. In this context, we propose a method to transfer annotations from a labeled to an unlabeled point cloud using an octree structure. The structure also analyses changes between the point clouds. Our experiments confirm that our method effectively transfers annotations while addressing changes. The primary contribution of this project is the development of the method for automatic label transfer between two different point clouds that represent the same real-world object. The proposed method can be of great importance for data-driven deep learning algorithms as it can also allow circumventing stochastic transfer learning by deterministic label transfer between datasets depicting the same objects.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云和高分辨率 3D 数据在测量、建筑和虚拟现实等各个领域变得越来越重要。然而，仅仅拥有这些数据还不够。为了提取有用的信息，语义标签至关重要。在这种情况下，我们提出了一种使用八叉树结构将注释从标记点云转移到未标记点云的方法。该结构还分析点云之间的变化。我们的实验证实我们的方法在处理变化的同时有效地传输注释。该项目的主要贡献是开发了代表同一现实世界对象的两个不同点云之间自动标签传输的方法。所提出的方法对于数据驱动的深度学习算法非常重要，因为它还可以通过描述相同对象的数据集之间的确定性标签转移来规避随机转移学习。</details>
**PDF:** <http://arxiv.org/pdf/2402.06531v1><br />
**Code:** <https://github.com/schwarzsophia/transferring_urban_labels_between_pointclouds>**<br />
>>**index:** 6<br />
**Title:** **Classifying point clouds at the facade-level using geometric features and deep learning networks**<br />
**Title_cn:** 使用几何特征和深度学习网络在立面级别对点云进行分类<br />
**Authors:** Yue Tan, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla<br />
**Abstract:** <details><summary>原文: </summary>3D building models with facade details are playing an important role in many applications now. Classifying point clouds at facade-level is key to create such digital replicas of the real world. However, few studies have focused on such detailed classification with deep neural networks. We propose a method fusing geometric features with deep learning networks for point cloud classification at facade-level. Our experiments conclude that such early-fused features improve deep learning methods' performance. This method can be applied for compensating deep learning networks' ability in capturing local geometric information and promoting the advancement of semantic segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>具有立面细节的 3D 建筑模型现在在许多应用中发挥着重要作用。在立面级别对点云进行分类是创建现实世界的数字复制品的关键。然而，很少有研究关注深度神经网络的这种详细分类。我们提出了一种将几何特征与深度学习网络融合的方法，用于立面级别的点云分类。我们的实验得出的结论是，这种早期融合的特征可以提高深度学习方法的性能。该方法可用于补偿深度学习网络捕获局部几何信息的能力，促进语义分割的进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.06506v1><br />
**Code:** <https://github.com/yue-t99/pointnet2-geometricfeatures-facade>**<br />
>>**index:** 7<br />
**Title:** **Iris-SAM: Iris Segmentation Using a Foundational Model**<br />
**Title_cn:** Iris-SAM：使用基础模型进行虹膜分割<br />
**Authors:** Parisa Farmanifard, Arun Ross<br />
**Abstract:** <details><summary>原文: </summary>Iris segmentation is a critical component of an iris biometric system and it involves extracting the annular iris region from an ocular image. In this work, we develop a pixel-level iris segmentation model from a foundational model, viz., Segment Anything Model (SAM), that has been successfully used for segmenting arbitrary objects. The primary contribution of this work lies in the integration of different loss functions during the fine-tuning of SAM on ocular images. In particular, the importance of Focal Loss is borne out in the fine-tuning process since it strategically addresses the class imbalance problem (i.e., iris versus non-iris pixels). Experiments on ND-IRIS-0405, CASIA-Iris-Interval-v3, and IIT-Delhi-Iris datasets convey the efficacy of the trained model for the task of iris segmentation. For instance, on the ND-IRIS-0405 dataset, an average segmentation accuracy of 99.58% was achieved, compared to the best baseline performance of 89.75%.</details>
**Abstract_cn:** <details><summary>译文: </summary>虹膜分割是虹膜生物识别系统的关键组成部分，它涉及从眼部图像中提取环形虹膜区域。在这项工作中，我们从基础模型（即分段任意模型（SAM））开发了像素级虹膜分割模型，该模型已成功用于分割任意对象。这项工作的主要贡献在于在 SAM 对眼部图像进行微调时整合不同的损失函数。特别是，焦点损失的重要性在微调过程中得到了证实，因为它从策略上解决了类别不平衡问题（即虹膜与非虹膜像素）。 ND-IRIS-0405、CASIA-Iris-Interval-v3 和 IIT-Delhi-Iris 数据集上的实验表明了训练模型在虹膜分割任务中的有效性。例如，在 ND-IRIS-0405 数据集上，平均分割精度达到 99.58%，而最佳基线性能为 89.75%。</details>
**PDF:** <http://arxiv.org/pdf/2402.06497v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Deep Learning-Based Auto-Segmentation of Planning Target Volume for Total Marrow and Lymph Node Irradiation**<br />
**Title_cn:** 基于深度学习的全骨髓和淋巴结照射规划目标体积的自动分割<br />
**Authors:** Ricardo Coimbra Brioso, Damiano Dei, Nicola Lambri, Daniele Loiacono, Pietro Mancosu, Marta Scorsetti<br />
**Abstract:** <details><summary>原文: </summary>In order to optimize the radiotherapy delivery for cancer treatment, especially when dealing with complex treatments such as Total Marrow and Lymph Node Irradiation (TMLI), the accurate contouring of the Planning Target Volume (PTV) is crucial. Unfortunately, relying on manual contouring for such treatments is time-consuming and prone to errors. In this paper, we investigate the application of Deep Learning (DL) to automate the segmentation of the PTV in TMLI treatment, building upon previous work that introduced a solution to this problem based on a 2D U-Net model. We extend the previous research (i) by employing the nnU-Net framework to develop both 2D and 3D U-Net models and (ii) by evaluating the trained models on the PTV with the exclusion of bones, which consist mainly of lymp-nodes and represent the most challenging region of the target volume to segment. Our result show that the introduction of nnU-NET framework led to statistically significant improvement in the segmentation performance. In addition, the analysis on the PTV after the exclusion of bones showed that the models are quite robust also on the most challenging areas of the target volume. Overall, our study is a significant step forward in the application of DL in a complex radiotherapy treatment such as TMLI, offering a viable and scalable solution to increase the number of patients who can benefit from this treatment.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了优化癌症治疗的放射治疗，特别是在处理全骨髓和淋巴结照射 (TMLI) 等复杂治疗时，规划目标体积 (PTV) 的准确轮廓至关重要。不幸的是，依靠手动轮廓进行此类治疗非常耗时且容易出错。在本文中，我们研究了深度学习 (DL) 在 TMLI 治疗中自动分割 PTV 的应用，之前的工作基于 2D U-Net 模型介绍了该问题的解决方案。我们扩展了之前的研究（i）通过采用 nnU-Net 框架来开发 2D 和 3D U-Net 模型，以及（ii）通过评估 PTV 上的训练模型，排除骨骼（主要由淋巴结组成）并代表目标体积中最具挑战性的分割区域。我们的结果表明，nnU-NET 框架的引入导致分割性能在统计上显着提高。此外，排除骨骼后对 PTV 的分析表明，该模型在目标体积中最具挑战性的区域上也非常稳健。总体而言，我们的研究是 DL 在复杂放射治疗（例如 TMLI）中的应用向前迈出的重要一步，提供了可行且可扩展的解决方案，以增加可以从这种治疗中受益的患者数量。</details>
**PDF:** <http://arxiv.org/pdf/2402.06494v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Cardiac ultrasound simulation for autonomous ultrasound navigation**<br />
**Title_cn:** 用于自主超声导航的心脏超声模拟<br />
**Authors:** Abdoul Aziz Amadou, Laura Peralta, Paul Dryburgh, Paul Klein, Kaloian Petkov, Richard James Housden, Vivek Singh, Rui Liao, Young-Ho Kim, Florin Christian Ghesu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Ultrasound is well-established as an imaging modality for diagnostic and interventional purposes. However, the image quality varies with operator skills as acquiring and interpreting ultrasound images requires extensive training due to the imaging artefacts, the range of acquisition parameters and the variability of patient anatomies. Automating the image acquisition task could improve acquisition reproducibility and quality but training such an algorithm requires large amounts of navigation data, not saved in routine examinations. Thus, we propose a method to generate large amounts of ultrasound images from other modalities and from arbitrary positions, such that this pipeline can later be used by learning algorithms for navigation. We present a novel simulation pipeline which uses segmentations from other modalities, an optimized volumetric data representation and GPU-accelerated Monte Carlo path tracing to generate view-dependent and patient-specific ultrasound images. We extensively validate the correctness of our pipeline with a phantom experiment, where structures' sizes, contrast and speckle noise properties are assessed. Furthermore, we demonstrate its usability to train neural networks for navigation in an echocardiography view classification experiment by generating synthetic images from more than 1000 patients. Networks pre-trained with our simulations achieve significantly superior performance in settings where large real datasets are not available, especially for under-represented classes. The proposed approach allows for fast and accurate patient-specific ultrasound image generation, and its usability for training networks for navigation-related tasks is demonstrated.</details>
**Abstract_cn:** <details><summary>译文: </summary>超声作为一种用于诊断和介入目的的成像方式已得到广泛认可。然而，图像质量随操作员技能的不同而变化，因为由于成像伪影、采集参数范围和患者解剖结构的变化，采集和解释超声图像需要大量培训。自动化图像采集任务可以提高采集的再现性和质量，但训练这种算法需要大量的导航数据，而不是保存在常规检查中。因此，我们提出了一种从其他模态和任意位置生成大量超声图像的方法，以便稍后可以通过学习算法使用该管道进行导航。我们提出了一种新颖的模拟管道，它使用其他模态的分割、优化的体积数据表示和 GPU 加速的蒙特卡罗路径追踪来生成依赖于视图和患者特定的超声图像。我们通过模型实验广泛验证了管道的正确性，其中评估了结构的尺寸、对比度和散斑噪声特性。此外，我们通过生成 1000 多名患者的合成图像，展示了其在超声心动图视图分类实验中训练导航神经网络的可用性。通过我们的模拟预先训练的网络在无法获得大型真实数据集的情况下实现了显着优越的性能，特别是对于代表性不足的类别。所提出的方法允许快速、准确地生成患者特定的超声图像，并且证明了其对于导航相关任务的训练网络的可用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06463v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal Curve Queries and Attention**<br />
**Title_cn:** CurveFormer++：利用时间曲线查询和注意力的曲线传播进行 3D 车道检测<br />
**Authors:** Yifeng Bai, Zhirong Chen, Pengpeng Liang, Erkang Cheng<br />
**Abstract:** <details><summary>原文: </summary>In autonomous driving, 3D lane detection using monocular cameras is an important task for various downstream planning and control tasks. Recent CNN and Transformer approaches usually apply a two-stage scheme in the model design. The first stage transforms the image feature from a front image into a bird's-eye-view (BEV) representation. Subsequently, a sub-network processes the BEV feature map to generate the 3D detection results. However, these approaches heavily rely on a challenging image feature transformation module from a perspective view to a BEV representation. In our work, we present CurveFormer++, a single-stage Transformer-based method that does not require the image feature view transform module and directly infers 3D lane detection results from the perspective image features. Specifically, our approach models the 3D detection task as a curve propagation problem, where each lane is represented by a curve query with a dynamic and ordered anchor point set. By employing a Transformer decoder, the model can iteratively refine the 3D lane detection results. A curve cross-attention module is introduced in the Transformer decoder to calculate similarities between image features and curve queries of lanes. To handle varying lane lengths, we employ context sampling and anchor point restriction techniques to compute more relevant image features for a curve query. Furthermore, we apply a temporal fusion module that incorporates selected informative sparse curve queries and their corresponding anchor point sets to leverage historical lane information. In the experiments, we evaluate our approach for the 3D lane detection task on two publicly available real-world datasets. The results demonstrate that our method provides outstanding performance compared with both CNN and Transformer based methods. We also conduct ablation studies to analyze the impact of each component in our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶中，使用单目摄像头进行 3D 车道检测是各种下游规划和控制任务的重要任务。最近的 CNN 和 Transformer 方法通常在模型设计中采用两阶段方案。第一阶段将图像特征从正面图像转换为鸟瞰图（BEV）表示。随后，子网络处理 BEV 特征图以生成 3D 检测结果。然而，这些方法严重依赖于从透视图到 BEV 表示的具有挑战性的图像特征转换模块。在我们的工作中，我们提出了 CurveFormer++，一种基于 Transformer 的单阶段方法，不需要图像特征视图变换模块，并直接从透视图像特征推断 3D 车道检测结果。具体来说，我们的方法将 3D 检测任务建模为曲线传播问题，其中每个车道由具有动态且有序锚点集的曲线查询表示。通过使用 Transformer 解码器，该模型可以迭代地细化 3D 车道检测结果。 Transformer 解码器中引入了曲线交叉注意模块来计算图像特征和车道曲线查询之间的相似度。为了处理不同的车道长度，我们采用上下文采样和锚点限制技术来计算曲线查询的更多相关图像特征。此外，我们应用了一个时间融合模块，该模块结合了选定的信息丰富的稀疏曲线查询及其相应的锚点集来利用历史车道信息。在实验中，我们在两个公开可用的真实数据集上评估了 3D 车道检测任务的方法。结果表明，与基于 CNN 和 Transformer 的方法相比，我们的方法提供了出色的性能。我们还进行消融研究，以分析我们方法中每个组成部分的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.06423v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Learning using privileged information for segmenting tumors on digital mammograms**<br />
**Title_cn:** 学习使用特权信息在数字乳房X光照片上分割肿瘤<br />
**Authors:** Ioannis N. Tzortzis, Konstantinos Makantasis, Ioannis Rallis, Nikolaos Bakalos, Anastasios Doulamis, Nikolaos Doulamis<br />
**Abstract:** <details><summary>原文: </summary>Limited amount of data and data sharing restrictions, due to GDPR compliance, constitute two common factors leading to reduced availability and accessibility when referring to medical data. To tackle these issues, we introduce the technique of Learning Using Privileged Information. Aiming to substantiate the idea, we attempt to build a robust model that improves the segmentation quality of tumors on digital mammograms, by gaining privileged information knowledge during the training procedure. Towards this direction, a baseline model, called student, is trained on patches extracted from the original mammograms, while an auxiliary model with the same architecture, called teacher, is trained on the corresponding enhanced patches accessing, in this way, privileged information. We repeat the student training procedure by providing the assistance of the teacher model this time. According to the experimental results, it seems that the proposed methodology performs better in the most of the cases and it can achieve 10% higher F1 score in comparison with the baseline.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于 GDPR 合规性，数据量有限和数据共享限制构成了导致引用医疗数据时可用性和可访问性降低的两个常见因素。为了解决这些问题，我们引入了使用特权信息学习的技术。为了证实这个想法，我们尝试建立一个强大的模型，通过在训练过程中获得特权信息知识来提高数字乳房X光照片上肿瘤的分割质量。朝着这个方向，基线模型（称为学生）在从原始乳房X光照片中提取的补丁上进行训练，而具有相同架构的辅助模型（称为教师）在相应的增强补丁上进行训练，以这种方式访问​​特权信息。这次我们通过提供教师模型的帮助来重复学生训练过程。根据实验结果，似乎所提出的方法在大多数情况下都表现更好，与基线相比，它可以实现高 10% 的 F1 分数。</details>
**PDF:** <http://arxiv.org/pdf/2402.06379v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Taking Class Imbalance Into Account in Open Set Recognition Evaluation**<br />
**Title_cn:** 在开集识别评估中考虑类别不平衡<br />
**Authors:** Joanna Komorniczak, Pawel Ksieniewicz<br />
**Abstract:** <details><summary>原文: </summary>In recent years Deep Neural Network-based systems are not only increasing in popularity but also receive growing user trust. However, due to the closed-world assumption of such systems, they cannot recognize samples from unknown classes and often induce an incorrect label with high confidence. Presented work looks at the evaluation of methods for Open Set Recognition, focusing on the impact of class imbalance, especially in the dichotomy between known and unknown samples. As an outcome of problem analysis, we present a set of guidelines for evaluation of methods in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，基于深度神经网络的系统不仅越来越受欢迎，而且越来越受到用户的信任。然而，由于此类系统的封闭世界假设，它们无法识别来自未知类别的样本，并且经常会产生高置信度的错误标签。所提出的工作着眼于开放集识别方法的评估，重点关注类别不平衡的影响，特别是已知和未知样本之间的二分法。作为问题分析的结果，我们提出了一套评估该领域方法的指南。</details>
**PDF:** <http://arxiv.org/pdf/2402.06331v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **MLS2LoD3: Refining low LoDs building models with MLS point clouds to reconstruct semantic LoD3 building models**<br />
**Title_cn:** MLS2LoD3：使用 MLS 点云细化低 LoD 建筑模型以重建语义 LoD3 建筑模型<br />
**Authors:** Olaf Wysocki, Ludwig Hoegner, Uwe Stilla<br />
**Abstract:** <details><summary>原文: </summary>Although highly-detailed LoD3 building models reveal great potential in various applications, they have yet to be available. The primary challenges in creating such models concern not only automatic detection and reconstruction but also standard-consistent modeling. In this paper, we introduce a novel refinement strategy enabling LoD3 reconstruction by leveraging the ubiquity of lower LoD building models and the accuracy of MLS point clouds. Such a strategy promises at-scale LoD3 reconstruction and unlocks LoD3 applications, which we also describe and illustrate in this paper. Additionally, we present guidelines for reconstructing LoD3 facade elements and their embedding into the CityGML standard model, disseminating gained knowledge to academics and professionals. We believe that our method can foster development of LoD3 reconstruction algorithms and subsequently enable their wider adoption.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管高度详细的 LoD3 建筑模型在各种应用中展现出巨大潜力，但它们尚未可用。创建此类模型的主要挑战不仅涉及自动检测和重建，还涉及标准一致的建模。在本文中，我们介绍了一种新颖的细化策略，通过利用普遍存在的较低 LoD 构建模型和 MLS 点云的准确性来实现 LoD3 重建。这种策略有望实现大规模 LoD3 重建并解锁 LoD3 应用，我们也在本文中对此进行了描述和说明。此外，我们还提出了重建 LoD3 立面元素并将其嵌入 CityGML 标准模型的指南，向学者和专业人士传播所获得的知识。我们相信我们的方法可以促进 LoD3 重建算法的发展，并随后使其得到更广泛的采用。</details>
**PDF:** <http://arxiv.org/pdf/2402.06288v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Insomnia Identification via Electroencephalography**<br />
**Title_cn:** 通过脑电图识别失眠<br />
**Authors:** Olviya Udeshika, Dilshan Lakshitha, Nilantha Premakumara, Surangani Bandara<br />
**Abstract:** <details><summary>原文: </summary>Insomnia is a serious sleep disorder caused by abnormal or excessive neural activity in the brain. An estimated 50 million people worldwide are thought to be affected by this condition, which is the second most severe neurological disease after stroke. In order to ensure a quick recovery, an early and accurate diagnosis of insomnia enables more effective drug and treatment administration. This study proposes a method that uses deep learning to automatically identify patients with insomnia. A set of optimal features are extracted from spectral and temporal domains, including the relative power of {\sigma}, \b{eta} and {\gamma} bands, the total power, the absolute slow wave power, the power ratios of {\theta}, {\alpha}, {\gamma}, \b{eta}, {\theta}/{\alpha}, {\theta}/\b{eta}, {\alpha}/{\gamma} and {\alpha}/\b{eta}, mean, zero crossing rate, mobility, complexity, sleep efficiency and total sleep time, to accurately quantify the differences between insomnia patients and healthy subjects and develops a 1D CNN model for the classification process. With the experiments use Fp2 and C4 EEG channels with 50 insomnia patients and 50 healthy subjects, the proposed model arrives 99.34% accuracy without sleep stage annotation. Using the features only from a single channel, the study proposes a smart solution for insomnia patients which allows machine learning to be to simplify current sleep monitoring hardware and improve in-home ambulatory monitoring.</details>
**Abstract_cn:** <details><summary>译文: </summary>失眠是一种由大脑异常或过度神经活动引起的严重睡眠障碍。据估计，全世界有 5000 万人受到这种疾病的影响，这是继中风之后第二严重的神经系统疾病。为了确保快速康复，对失眠的早期准确诊断可以实现更有效的药物和治疗管理。这项研究提出了一种利用深度学习自动识别失眠患者的方法。从谱域和时域提取一组最优特征，包括{\sigma}、\b{eta}和{\gamma}频段的相对功率、总功率、绝对慢波功率、{ \theta}、{\alpha}、{\gamma}、\b{eta}、{\theta}/{\alpha}、{\theta}/\b{eta}、{\alpha}/{\gamma}和 {\alpha}/\b{eta}、平均值、过零率、移动性、复杂性、睡眠效率和总睡眠时间，以准确量化失眠患者和健康受试者之间的差异，并开发用于分类过程的一维 CNN 模型。通过使用 Fp2 和 C4 脑电通道对 50 名失眠患者和 50 名健康受试者进行实验，所提出的模型在没有睡眠阶段注释的情况下达到了 99.34% 的准确率。该研究仅利用单一通道的功能，为失眠患者提出了一种智能解决方案，使机器学习能够简化当前的睡眠监测硬件并改善家庭动态监测。</details>
**PDF:** <http://arxiv.org/pdf/2402.06251v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Anomaly Unveiled: Securing Image Classification against Adversarial Patch Attacks**<br />
**Title_cn:** 揭秘异常：保护图像分类免受对抗性补丁攻击<br />
**Authors:** Nandish Chattopadhyay, Amira Guesmi, Muhammad Shafique<br />
**Abstract:** <details><summary>原文: </summary>Adversarial patch attacks pose a significant threat to the practical deployment of deep learning systems. However, existing research primarily focuses on image pre-processing defenses, which often result in reduced classification accuracy for clean images and fail to effectively counter physically feasible attacks. In this paper, we investigate the behavior of adversarial patches as anomalies within the distribution of image information and leverage this insight to develop a robust defense strategy. Our proposed defense mechanism utilizes a clustering-based technique called DBSCAN to isolate anomalous image segments, which is carried out by a three-stage pipeline consisting of Segmenting, Isolating, and Blocking phases to identify and mitigate adversarial noise. Upon identifying adversarial components, we neutralize them by replacing them with the mean pixel value, surpassing alternative replacement options. Our model-agnostic defense mechanism is evaluated across multiple models and datasets, demonstrating its effectiveness in countering various adversarial patch attacks in image classification tasks. Our proposed approach significantly improves accuracy, increasing from 38.8\% without the defense to 67.1\% with the defense against LaVAN and GoogleAp attacks, surpassing prominent state-of-the-art methods such as LGS (53.86\%) and Jujutsu (60\%)</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性补丁攻击对深度学习系统的实际部署构成重大威胁。然而，现有的研究主要集中在图像预处理防御上，这往往会导致干净图像的分类精度降低，并且无法有效对抗物理上可行的攻击。在本文中，我们研究了对抗性补丁作为图像信息分布中的异常行为，并利用这种洞察力来开发强大的防御策略。我们提出的防御机制利用一种称为 DBSCAN 的基于聚类的技术来隔离异常图像片段，该技术通过由分段、隔离和阻止阶段组成的三阶段管道来执行，以识别和减轻对抗性噪声。在识别出对抗性成分后，我们通过用平均像素值替换它们来中和它们，超越了其他替换选项。我们的模型无关防御机制在多个模型和数据集上进行了评估，证明了其在对抗图像分类任务中的各种对抗性补丁攻击方面的有效性。我们提出的方法显着提高了准确性，从没有防御的 38.8\% 增加到防御 LaVAN 和 GoogleAp 攻击的 67.1\%，超过了 LGS (53.86\%) 和 Jujutsu (60 \%)</details>
**PDF:** <http://arxiv.org/pdf/2402.06249v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Learning Contrastive Feature Representations for Facial Action Unit Detection**<br />
**Title_cn:** 学习面部动作单元检测的对比特征表示<br />
**Authors:** Ziqiao Shang, Bin Liu, Fei Teng, Tianrui Li<br />
**Abstract:** <details><summary>原文: </summary>The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we employ an importance re-weighting strategy tailored for minority AUs. The resulting loss, denoted as AUNCE, is proposed to encapsulate this strategy. Our experimental assessments, conducted on two widely-utilized benchmark datasets (BP4D and DISFA), underscore the superior performance of our approach compared to state-of-the-art methods in the realm of AU detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部动作单元（AU）检测的主要方法围绕有监督的多标签二元分类问题。现有的方法通常对 AU 的像素级信息进行编码，从而对模型复杂性和表达能力提出了很高的要求。此外，由于存在嘈杂的 AU 标签，这种做法增加了过度拟合的可能性。在本研究中，我们引入了一种由监督和自监督信号增强的对比学习框架。目标是获取判别性特征，这与 AU 检测领域内的传统像素级学习范例不同。为了解决嘈杂的 AU 标签带来的挑战，我们通过引入自监督信号来增强监督信号。这种增强是通过正样本采样实现的，包含三种不同类型的正样本对。此外，为了缓解每种 AU 类型的不平衡分布，我们采用了针对少数 AU 量身定制的重要性重新加权策略。由此产生的损失（表示为 AUNCE）被建议封装该策略。我们在两个广泛使用的基准数据集（BP4D 和 DISFA）上进行的实验评估强调了我们的方法与 AU 检测领域最先进的方法相比具有优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.06165v1><br />
**Code:** <https://github.com/ziqiao-shang/aunce>**<br />
>>**index:** 17<br />
**Title:** **Target Recognition Algorithm for Monitoring Images in Electric Power Construction Process**<br />
**Title_cn:** 电力施工过程监控图像目标识别算法<br />
**Authors:** Hao Song, Wei Lin, Wei Song, Man Wang<br />
**Abstract:** <details><summary>原文: </summary>To enhance precision and comprehensiveness in identifying targets in electric power construction monitoring video, a novel target recognition algorithm utilizing infrared imaging is explored. This algorithm employs a color processing technique based on a local linear mapping method to effectively recolor monitoring images. The process involves three key steps: color space conversion, color transfer, and pseudo-color encoding. It is designed to accentuate targets in the infrared imaging. For the refined identification of these targets, the algorithm leverages a support vector machine approach, utilizing an optimal hyperplane to accurately predict target types. We demonstrate the efficacy of the algorithm, which achieves high target recognition accuracy in both outdoor and indoor electric power construction monitoring scenarios. It maintains a false recognition rate below 3% across various environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了提高电力施工监控视频中目标识别的精度和全面性，探索了一种利用红外成像的目标识别算法。该算法采用基于局部线性映射方法的颜色处理技术来有效地对监控图像进行重新着色。该过程涉及三个关键步骤：色彩空间转换、色彩传输和伪色彩编码。它旨在强调红外成像中的目标。为了精细识别这些目标，该算法利用支持向量机方法，利用最佳超平面来准确预测目标类型。我们展示了该算法的功效，在室外和室内电力施工监控场景中均实现了较高的目标识别精度。在各种环境下，其错误识别率均保持在 3% 以下。</details>
**PDF:** <http://arxiv.org/pdf/2402.06152v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **TETRIS: Towards Exploring the Robustness of Interactive Segmentation**<br />
**Title_cn:** 《俄罗斯方块》：探索交互式分段的稳健性<br />
**Authors:** Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov, Alexander Krapukhin, Denis Shepelev, Konstantin Soshin<br />
**Abstract:** <details><summary>原文: </summary>Interactive segmentation methods rely on user inputs to iteratively update the selection mask. A click specifying the object of interest is arguably the most simple and intuitive interaction type, and thereby the most common choice for interactive segmentation. However, user clicking patterns in the interactive segmentation context remain unexplored. Accordingly, interactive segmentation evaluation strategies rely more on intuition and common sense rather than empirical studies (e.g., assuming that users tend to click in the center of the area with the largest error). In this work, we conduct a real user study to investigate real user clicking patterns. This study reveals that the intuitive assumption made in the common evaluation strategy may not hold. As a result, interactive segmentation models may show high scores in the standard benchmarks, but it does not imply that they would perform well in a real world scenario. To assess the applicability of interactive segmentation methods, we propose a novel evaluation strategy providing a more comprehensive analysis of a model's performance. To this end, we propose a methodology for finding extreme user inputs by a direct optimization in a white-box adversarial attack on the interactive segmentation model. Based on the performance with such adversarial user inputs, we assess the robustness of interactive segmentation models w.r.t click positions. Besides, we introduce a novel benchmark for measuring the robustness of interactive segmentation, and report the results of an extensive evaluation of dozens of models.</details>
**Abstract_cn:** <details><summary>译文: </summary>交互式分割方法依赖于用户输入来迭代更新选择掩模。指定感兴趣对象的点击可以说是最简单和直观的交互类型，因此也是交互式分割的最常见选择。然而，交互式分段上下文中的用户点击模式仍未被探索。因此，交互式分割评估策略更多地依赖于直觉和常识，而不是实证研究（例如，假设用户倾向于点击误差最大的区域的中心）。在这项工作中，我们进行了真实的用户研究来调查真实的用户点击模式。这项研究表明，通用评估策略中做出的直观假设可能并不成立。因此，交互式细分模型可能在标准基准测试中显示出高分，但这并不意味着它们在现实世界场景中表现良好。为了评估交互式分割方法的适用性，我们提出了一种新颖的评估策略，可以对模型的性能进行更全面的分析。为此，我们提出了一种通过对交互式分割模型的白盒对抗攻击进行直接优化来查找极端用户输入的方法。根据此类对抗性用户输入的性能，我们评估了交互式分割模型与点击位置的鲁棒性。此外，我们引入了一种新颖的基准来衡量交互式分割的鲁棒性，并报告了对数十个模型的广泛评估的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.06132v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Multiple Instance Learning for Cheating Detection and Localization in Online Examinations**<br />
**Title_cn:** 用于在线考试作弊检测和定位的多实例学习<br />
**Authors:** Yemeng Liu, Jing Ren, Jianshuo Xu, Xiaomei Bai, Roopdeep Kaur, Feng Xia<br />
**Abstract:** <details><summary>原文: </summary>The spread of the Coronavirus disease-2019 epidemic has caused many courses and exams to be conducted online. The cheating behavior detection model in examination invigilation systems plays a pivotal role in guaranteeing the equality of long-distance examinations. However, cheating behavior is rare, and most researchers do not comprehensively take into account features such as head posture, gaze angle, body posture, and background information in the task of cheating behavior detection. In this paper, we develop and present CHEESE, a CHEating detection framework via multiplE inStancE learning. The framework consists of a label generator that implements weak supervision and a feature encoder to learn discriminative features. In addition, the framework combines body posture and background features extracted by 3D convolution with eye gaze, head posture and facial features captured by OpenFace 2.0. These features are fed into the spatio-temporal graph module by stitching to analyze the spatio-temporal changes in video clips to detect the cheating behaviors. Our experiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam Proctoring (OEP), prove the effectiveness of our method as compared to the state-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on the OEP dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>2019年冠状病毒疫情的蔓延导致许多课程和考试在网上进行。监考系统中的作弊行为检测模型对于保障远程考试的公平性具有举足轻重的作用。然而，作弊行为很少见，大多数研究人员在作弊行为检测任务中没有综合考虑头部姿势、注视角度、身体姿势和背景信息等特征。在本文中，我们开发并提出了 CHEESE，这是一种通过多重 inStance 学习的 CHEating 检测框架。该框架由一个实现弱监督的标签生成器和一个用于学习判别性特征的特征编码器组成。此外，该框架还将3D卷积提取的身体姿势和背景特征与OpenFace 2.0捕获的眼睛注视、头部姿势和面部特征相结合。这些特征通过拼接的方式输入到时空图模块中，分析视频片段中的时空变化，从而检测作弊行为。我们在 UCF-Crime、ShanghaiTech 和 Online Exam Proctoring (OEP) 三个数据集上进行的实验证明了我们的方法与最先进的方法相比的有效性，并获得了 87.58% 的帧级 AUC 分数OEP 数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.06107v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **SIR: Multi-view Inverse Rendering with Decomposable Shadow for Indoor Scenes**<br />
**Title_cn:** SIR：室内场景的可分解阴影多视图逆渲染<br />
**Authors:** Xiaokang Wei, Zhuoman Liu, Yan Luximon<br />
**Abstract:** <details><summary>原文: </summary>We propose SIR, an efficient method to decompose differentiable shadows for inverse rendering on indoor scenes using multi-view data, addressing the challenges in accurately decomposing the materials and lighting conditions. Unlike previous methods that struggle with shadow fidelity in complex lighting environments, our approach explicitly learns shadows for enhanced realism in material estimation under unknown light positions. Utilizing posed HDR images as input, SIR employs an SDF-based neural radiance field for comprehensive scene representation. Then, SIR integrates a shadow term with a three-stage material estimation approach to improve SVBRDF quality. Specifically, SIR is designed to learn a differentiable shadow, complemented by BRDF regularization, to optimize inverse rendering accuracy. Extensive experiments on both synthetic and real-world indoor scenes demonstrate the superior performance of SIR over existing methods in both quantitative metrics and qualitative analysis. The significant decomposing ability of SIR enables sophisticated editing capabilities like free-view relighting, object insertion, and material replacement.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 SIR，这是一种使用多视图数据分解可微阴影以在室内场景上进行逆渲染的有效方法，解决了准确分解材质和照明条件的挑战。与之前在复杂照明环境中努力解决阴影保真度的方法不同，我们的方法明确地学习阴影，以增强未知光位置下材料估计的真实感。 SIR 利用摆姿势的 HDR 图像作为输入，采用基于 SDF 的神经辐射场来进行全面的场景表示。然后，SIR 将影子项与三阶段材料估计方法相结合，以提高 SVBRDF 质量。具体来说，SIR 旨在学习可微分阴影，并辅以 BRDF 正则化，以优化逆渲染精度。对合成和真实室内场景的大量实验证明，SIR 在定量指标和定性分析方面均优于现有方法。 SIR 强大的分解能力可实现复杂的编辑功能，例如自由视图重新照明、对象插入和材质替换。</details>
**PDF:** <http://arxiv.org/pdf/2402.06136v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Maia: A Real-time Non-Verbal Chat for Human-AI Interaction**<br />
**Title_cn:** Maia：用于人机交互的实时非语言聊天<br />
**Authors:** Dragos Costea, Alina Marcu, Cristina Lazar, Marius Leordeanu<br />
**Abstract:** <details><summary>原文: </summary>Face-to-face communication modeling in computer vision is an area of research focusing on developing algorithms that can recognize and analyze non-verbal cues and behaviors during face-to-face interactions. We propose an alternative to text chats for Human-AI interaction, based on non-verbal visual communication only, using facial expressions and head movements that mirror, but also improvise over the human user, to efficiently engage with the users, and capture their attention in a low-cost and real-time fashion. Our goal is to track and analyze facial expressions, and other non-verbal cues in real-time, and use this information to build models that can predict and understand human behavior. We offer three different complementary approaches, based on retrieval, statistical, and deep learning techniques. We provide human as well as automatic evaluations and discuss the advantages and disadvantages of each direction.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉中的面对面交流建模是一个研究领域，重点是开发能够识别和分析面对面互动期间非语言线索和行为的算法。我们提出了一种人机交互文本聊天的替代方案，仅基于非语言视觉通信，使用反映人类用户的面部表情和头部动作，但也即兴发挥，以有效地与用户互动并吸引他们的注意力以低成本和实时的方式。我们的目标是实时跟踪和分析面部表情和其他非语言线索，并使用这些信息构建可以预测和理解人类行为的模型。我们提供三种不同的基于检索、统计和深度学习技术的互补方法。我们提供人工和自动评估，并讨论每个方向的优点和缺点。</details>
**PDF:** <http://arxiv.org/pdf/2402.06385v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A self-supervised framework for learning whole slide representations**<br />
**Title_cn:** 用于学习整个幻灯片表示的自监督框架<br />
**Authors:** Xinhai Hou, Cheng Jiang, Akhil Kondepudi, Yiwei Lyu, Asadur Zaman Chowdury, Honglak Lee, Todd C. Hollon<br />
**Abstract:** <details><summary>原文: </summary>Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy within WSIs to learn high-quality whole-slide representations. We benchmark S3L visual representations on two diagnostic tasks for two biomedical microscopy modalities. S3L significantly outperforms WSI baselines for cancer diagnosis and genetic mutation prediction. Additionally, S3L achieves good performance using both in-domain and out-of-distribution patch encoders, demonstrating good flexibility and generalizability.</details>
**Abstract_cn:** <details><summary>译文: </summary>全玻片成像是生物医学显微镜和计算病理学的基础。然而，全幻灯片图像（WSI）由于其十亿像素大小、不同的组织病理学特征、空间异质性和有限/缺乏数据注释而提出了复杂的计算机视觉挑战。这些挑战突出表明，单独的监督训练可能会导致整个幻灯片表示不理想。自监督表示学习可以为下游诊断任务（例如癌症诊断或分子遗传预测）实现高质量的 WSI 视觉特征学习。在这里，我们提出了一种通用的自监督全幻灯片学习 (S3L) 框架，用于 WSI 的十亿像素级自监督。 S3L 将基于 Transformer 的视觉和语言建模的数据转换策略结合到一个统一的框架中，以生成用于自我监督的配对视图。 S3L 利用 WSI 中固有的区域异质性、组织学特征变异性和信息冗余来学习高质量的全幻灯片表示。我们对两种生物医学显微镜模式的两项诊断任务的 S3L 视觉表示进行基准测试。 S3L 在癌症诊断和基因突变预测方面显着优于 WSI 基线。此外，S3L 使用域内和分布外补丁编码器实现了良好的性能，展示了良好的灵活性和通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06188v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Reconstructing facade details using MLS point clouds and Bag-of-Words approach**<br />
**Title_cn:** 使用 MLS 点云和词袋方法重建立面细节<br />
**Authors:** Thomas Froech, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla<br />
**Abstract:** <details><summary>原文: </summary>In the reconstruction of fa\c{c}ade elements, the identification of specific object types remains challenging and is often circumvented by rectangularity assumptions or the use of bounding boxes. We propose a new approach for the reconstruction of 3D fa\c{c}ade details. We combine MLS point clouds and a pre-defined 3D model library using a BoW concept, which we augment by incorporating semi-global features. We conduct experiments on the models superimposed with random noise and on the TUM-FA\c{C}ADE dataset. Our method demonstrates promising results, improving the conventional BoW approach. It holds the potential to be utilized for more realistic facade reconstruction without rectangularity assumptions, which can be used in applications such as testing automated driving functions or estimating fa\c{c}ade solar potential.</details>
**Abstract_cn:** <details><summary>译文: </summary>在立面元素的重建中，特定对象类型的识别仍然具有挑战性，并且通常通过矩形假设或边界框的使用来规避。我们提出了一种重建 3D 立面细节的新方法。我们使用 BoW 概念将 MLS 点云和预定义的 3D 模型库结合起来，并通过合并半全局特征来增强该模型库。我们在叠加随机噪声的模型和 TUM-FA\c{C}ADE 数据集上进行了实验。我们的方法展示了有希望的结果，改进了传统的 BoW 方法。它具有无需矩形假设即可用于更现实的立面重建的潜力，可用于测试自动驾驶功能或估计立面太阳能潜力等应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.06521v1><br />
**Code:** <https://github.com/thomasfroech/reconstructingfacadedetailsbow>**<br />
>>**index:** 2<br />
**Title:** **A Network for structural dense displacement based on 3D deformable mesh model and optical flow**<br />
**Title_cn:** 基于3D变形网格模型和光流的结构密集位移网络<br />
**Authors:** Peimian Du, Qicheng Guo, Yanru Li<br />
**Abstract:** <details><summary>原文: </summary>This study proposes a Network to recognize displacement of a RC frame structure from a video by a monocular camera. The proposed Network consists of two modules which is FlowNet2 and POFRN-Net. FlowNet2 is used to generate dense optical flow as well as POFRN-Net is to extract pose parameter H. FlowNet2 convert two video frames into dense optical flow. POFRN-Net is inputted dense optical flow from FlowNet2 to output the pose parameter H. The displacement of any points of structure can be calculated from parameter H. The Fast Fourier Transform (FFT) is applied to obtain frequency domain signal from corresponding displacement signal. Furthermore, the comparison of the truth displacement on the First floor of the First video is shown in this study. Finally, the predicted displacements on four floors of RC frame structure of given three videos are exhibited in the last of this study.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究提出了一种网络来识别单目相机视频中 RC 框架结构的位移。所提出的网络由两个模块组成，即 FlowNet2 和 POFRN-Net。 FlowNet2用于生成密集光流，POFRN-Net用于提取姿态参数H。FlowNet2将两个视频帧转换为密集光流。 POFRN-Net从FlowNet2输入密集光流，输出位姿参数H。根据参数H可以计算结构任意点的位移。应用快速傅里叶变换（FFT）从相应的位移信号获得频域信号。此外，本研究还展示了第一个视频一楼的真实位移比较。最后，本研究的最后展示了给定三个视频的 RC 框架结构四层的预测位移。</details>
**PDF:** <http://arxiv.org/pdf/2402.06329v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Halo Reduction in Display Systems through Smoothed Local Histogram Equalization and Human Visual System Modeling**<br />
**Title_cn:** 通过平滑局部直方图均衡和人类视觉系统建模减少显示系统中的光晕<br />
**Authors:** Prasoon Ambalathankandy, Yafei Ou, Masayuki Ikebe<br />
**Abstract:** <details><summary>原文: </summary>Halo artifacts significantly impact display quality. We propose a method to reduce halos in Local Histogram Equalization (LHE) algorithms by separately addressing dark and light variants. This approach results in visually natural images by exploring the relationship between lateral inhibition and halo artifacts in the human visual system.</details>
**Abstract_cn:** <details><summary>译文: </summary>光晕伪像会严重影响显示质量。我们提出了一种通过单独处理暗和亮变量来减少局部直方图均衡（LHE）算法中的光晕的方法。这种方法通过探索人类视觉系统中的侧抑制和光晕伪影之间的关系，产生视觉上自然的图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.06212v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling**<br />
**Title_cn:** ViGoR：通过细粒度奖励模型改善大视觉语言模型的视觉基础<br />
**Authors:** Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran Li<br />
**Abstract:** <details><summary>原文: </summary>By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks. Additionally, we construct a comprehensive and challenging dataset specifically designed to validate the visual grounding capabilities of LVLMs. Finally, we plan to release our human annotation comprising approximately 16,000 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过将自然语言理解以及大型语言模型的生成能力和知识广度与图像感知相结合，最近的大型视觉语言模型（LVLM）在现实世界中表现出了前所未有的推理能力。然而，生成的文本通常会受到视觉输入基础不准确的影响，从而导致错误，例如幻觉不存在的场景元素、丢失场景的重要部分以及推断对象之间不正确的属性和关系。为了解决这些问题，我们引入了一种新颖的框架 ViGoR（通过细粒度奖励模型实现视觉基础），该框架利用细粒度奖励模型来显着增强 LVLM 在预训练基线上的视觉基础。这种改进是通过使用更便宜的人工评估而不是全面监督以及自动化方法来有效实现的。我们通过多个基准的大量指标展示了我们方法的有效性。此外，我们还构建了一个全面且具有挑战性的数据集，专门用于验证 LVLM 的视觉基础能力。最后，我们计划发布由大约 16,000 张图像组成的人工注释，并生成带有细粒度评估的文本对，为社区的相关研究做出贡献。</details>
**PDF:** <http://arxiv.org/pdf/2402.06118v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Image-based Deep Learning for the time-dependent prediction of fresh concrete properties**<br />
**Title_cn:** 基于图像的深度学习，用于新拌混凝土性能随时间的预测<br />
**Authors:** Max Meyer, Amadeus Langer, Max Mehltretter, Dries Beyer, Max Coenen, Tobias Schack, Michael Haist, Christian Heipke<br />
**Abstract:** <details><summary>原文: </summary>Increasing the degree of digitisation and automation in the concrete production process can play a crucial role in reducing the CO$_2$ emissions that are associated with the production of concrete. In this paper, a method is presented that makes it possible to predict the properties of fresh concrete during the mixing process based on stereoscopic image sequences of the concretes flow behaviour. A Convolutional Neural Network (CNN) is used for the prediction, which receives the images supported by information on the mix design as input. In addition, the network receives temporal information in the form of the time difference between the time at which the images are taken and the time at which the reference values of the concretes are carried out. With this temporal information, the network implicitly learns the time-dependent behaviour of the concretes properties. The network predicts the slump flow diameter, the yield stress and the plastic viscosity. The time-dependent prediction potentially opens up the pathway to determine the temporal development of the fresh concrete properties already during mixing. This provides a huge advantage for the concrete industry. As a result, countermeasures can be taken in a timely manner. It is shown that an approach based on depth and optical flow images, supported by information of the mix design, achieves the best results.</details>
**Abstract_cn:** <details><summary>译文: </summary>提高混凝土生产过程的数字化和自动化程度可以在减少与混凝土生产相关的 CO$_2$ 排放方面发挥至关重要的作用。本文提出了一种方法，可以根据混凝土流动行为的立体图像序列来预测搅拌过程中新拌混凝土的特性。卷积神经网络（CNN）用于预测，它接收混合设计信息支持的图像作为输入。此外，网络还接收拍摄图像的时间与执行混凝土参考值的时间之间的时间差形式的时间信息。利用这些时间信息，网络隐式地学习混凝土属性的时间相关行为。该网络预测坍落流直径、屈服应力和塑性粘度。与时间相关的预测可能开辟了确定混合过程中新拌混凝土性能随时间变化的途径。这为混凝土行业提供了巨大的优势。这样，就可以及时采取应对措施。结果表明，基于深度和光流图像的方法在混合设计信息的支持下取得了最佳结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.06611v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in heterogeneous data with cross-domain self-supervised learning**<br />
**Title_cn:** BarlowTwins-CXR：通过跨域自监督学习增强异构数据中的胸部 X 射线异常定位<br />
**Authors:** Haoyue Sheng, Linrui Ma, Jean-Francois Samson, Dianbo Liu<br />
**Abstract:** <details><summary>原文: </summary>Background: Chest X-ray imaging-based abnormality localization, essential in diagnosing various diseases, faces significant clinical challenges due to complex interpretations and the growing workload of radiologists. While recent advances in deep learning offer promising solutions, there is still a critical issue of domain inconsistency in cross-domain transfer learning, which hampers the efficiency and accuracy of diagnostic processes. This study aims to address the domain inconsistency problem and improve autonomic abnormality localization performance of heterogeneous chest X-ray image analysis, by developing a self-supervised learning strategy called "BarlwoTwins-CXR". Methods: We utilized two publicly available datasets: the NIH Chest X-ray Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in a two-stage training process. Initially, self-supervised pre-training was performed using an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50 backbone pre-trained on ImageNet. This was followed by supervised fine-tuning on the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network (FPN). Results: Our experiments showed a significant improvement in model performance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50 accuracy compared to traditional ImageNet pre-trained models. In addition, the Ablation CAM method revealed enhanced precision in localizing chest abnormalities. Conclusion: BarlowTwins-CXR significantly enhances the efficiency and accuracy of chest X-ray image-based abnormality localization, outperforming traditional transfer learning methods and effectively overcoming domain inconsistency in cross-domain scenarios. Our experiment results demonstrate the potential of using self-supervised learning to improve the generalizability of models in medical settings with limited amounts of heterogeneous data.</details>
**Abstract_cn:** <details><summary>译文: </summary>背景：基于胸部 X 射线成像的异常定位对于诊断各种疾病至关重要，但由于复杂的解释和放射科医生不断增加的工作量，面临着重大的临床挑战。尽管深度学习的最新进展提供了有前景的解决方案，但跨域迁移学习仍然存在域不一致的关键问题，这阻碍了诊断过程的效率和准确性。本研究旨在通过开发一种名为“BarlwoTwins-CXR”的自监督学习策略来解决域不一致问题并提高异构胸部 X 射线图像分析的自主异常定位性能。方法：我们利用两个公开可用的数据集：NIH 胸部 X 射线数据集和 VinDr-CXR。 BarlowTwins-CXR 方法分两阶段进行。最初，使用调整后的 Barlow Twins 算法在 NIH 数据集上执行自监督预训练，并在 ImageNet 上预训练 Resnet50 主干网。随后使用带有特征金字塔网络 (FPN) 的 Faster R-CNN 对 VinDr-CXR 数据集进行监督微调。结果：我们的实验表明 BarlowTwins-CXR 的模型性能得到显着改善。与传统 ImageNet 预训练模型相比，该方法的 mAP50 准确率提高了 3%。此外，Ablation CAM 方法显示出定位胸部异常的精确度有所提高。结论：BarlowTwins-CXR显着提高了基于胸部X射线图像的异常定位的效率和准确性，优于传统的迁移学习方法，并有效克服了跨域场景中的域不一致。我们的实验结果证明了使用自我监督学习来提高模型在具有有限异构数据的医疗环境中的通用性的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.06499v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Large Language Models for Captioning and Retrieving Remote Sensing Images**<br />
**Title_cn:** 用于字幕和检索遥感图像的大型语言模型<br />
**Authors:** João Daniel Silva, João Magalhães, Devis Tuia, Bruno Martins<br />
**Abstract:** <details><summary>原文: </summary>Image captioning and cross-modal retrieval are examples of tasks that involve the joint analysis of visual and linguistic information. In connection to remote sensing imagery, these tasks can help non-expert users in extracting relevant Earth observation information for a variety of applications. Still, despite some previous efforts, the development and application of vision and language models to the remote sensing domain have been hindered by the relatively small size of the available datasets and models used in previous studies. In this work, we propose RS-CapRet, a Vision and Language method for remote sensing tasks, in particular image captioning and text-image retrieval. We specifically propose to use a highly capable large decoder language model together with image encoders adapted to remote sensing imagery through contrastive language-image pre-training. To bridge together the image encoder and language decoder, we propose training simple linear layers with examples from combining different remote sensing image captioning datasets, keeping the other parameters frozen. RS-CapRet can then generate descriptions for remote sensing images and retrieve images from textual descriptions, achieving SOTA or competitive performance with existing methods. Qualitative results illustrate that RS-CapRet can effectively leverage the pre-trained large language model to describe remote sensing images, retrieve them based on different types of queries, and also show the ability to process interleaved sequences of images and text in a dialogue manner.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像字幕和跨模式检索是涉及视觉和语言信息联合分析的任务的示例。与遥感图像相关，这些任务可以帮助非专家用户为各种应用提取相关的地球观测信息。尽管如此，尽管之前做出了一些努力，但由于先前研究中使用的可用数据集和模型规模相对较小，视觉和语言模型在遥感领域的开发和应用受到了阻碍。在这项工作中，我们提出了 RS-CapRet，一种用于遥感任务的视觉和语言方法，特别是图像字幕和文本图像检索。我们特别建议使用功能强大的大型解码器语言模型以及通过对比语言图像预训练适应遥感图像的图像编码器。为了将图像编码器和语言解码器连接起来，我们建议通过组合不同遥感图像字幕数据集的示例来训练简单的线性层，并保持其他参数冻结。然后，RS-CapRet 可以生成遥感图像的描述并从文本描述中检索图像，从而实现 SOTA 或与现有方法竞争的性能。定性结果表明，RS-CapRet 可以有效地利用预训练的大语言模型来描述遥感图像，根据不同类型的查询检索遥感图像，并且还显示出以对话方式处理图像和文本交错序列的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.06475v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **FD-Vision Mamba for Endoscopic Exposure Correction**<br />
**Title_cn:** 用于内窥镜曝光校正的 FD-Vision Mamba<br />
**Authors:** Zhuoran Zheng, Jun Zhang<br />
**Abstract:** <details><summary>原文: </summary>In endoscopic imaging, the recorded images are prone to exposure abnormalities, so maintaining high-quality images is important to assist healthcare professionals in performing decision-making. To overcome this issue, We design a frequency-domain based network, called FD-Vision Mamba (FDVM-Net), which achieves high-quality image exposure correction by reconstructing the frequency domain of endoscopic images. Specifically, inspired by the State Space Sequence Models (SSMs), we develop a C-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. A two-path network is built using C-SSM as the basic function cell, and these two paths deal with the phase and amplitude information of the image, respectively. Finally, a degraded endoscopic image is reconstructed by FDVM-Net to obtain a high-quality clear image. Extensive experimental results demonstrate that our method achieves state-of-the-art results in terms of speed and accuracy, and it is noteworthy that our method can enhance endoscopic images of arbitrary resolution. The URL of the code is \url{https://github.com/zzr-idam/FDVM-Net}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在内窥镜成像中，记录的图像很容易出现曝光异常，因此保持高质量的图像对于协助医疗保健专业人员进行决策非常重要。为了克服这个问题，我们设计了一种基于频域的网络，称为FD-Vision Mamba (FDVM-Net)，它通过重建内窥镜图像的频域来实现高质量的图像曝光校正。具体来说，受状态空间序列模型（SSM）的启发，我们开发了一个 C-SSM 模块，它将卷积层的局部特征提取能力与 SSM 捕获远程依赖关系的能力集成在一起。以C-SSM为基本功能单元构建双路径网络，这两条路径分别处理图像的相位和幅度信息。最后，通过FDVM-Net重建退化的内窥镜图像，以获得高质量的清晰图像。大量的实验结果表明，我们的方法在速度和准确性方面取得了最先进的结果，值得注意的是，我们的方法可以增强任意分辨率的内窥镜图像。代码的 URL 为 \url{https://github.com/zzr-idam/FDVM-Net}。</details>
**PDF:** <http://arxiv.org/pdf/2402.06378v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Towards actionability for open medical imaging datasets: lessons from community-contributed platforms for data management and stewardship**<br />
**Title_cn:** 实现开放医学成像数据集的可操作性：社区贡献的数据管理和管理平台的经验教训<br />
**Authors:** Amelia Jiménez-Sánchez, Natalia-Rozalia Avlona, Dovile Juodelyte, Théo Sourget, Caroline Vang-Larsen, Hubert Dariusz Zając, Veronika Cheplygina<br />
**Abstract:** <details><summary>原文: </summary>Medical imaging datasets are fundamental to artificial intelligence (AI) in healthcare. The accuracy, robustness and fairness of diagnostic algorithms depend on the data (and its quality) on which the models are trained and evaluated. Medical imaging datasets have become increasingly available to the public, and are often hosted on Community-Contributed Platforms (CCP), including private companies like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper we investigate medical imaging datasets on CCPs and how they are documented, shared, and maintained. We first highlight some differences between medical imaging and computer vision, particularly in the potentially harmful downstream effects due to poor adoption of recommended dataset management practices. We then analyze 20 (10 medical and 10 computer vision) popular datasets on CCPs and find vague licenses, lack of persistent identifiers and storage, duplicates and missing metadata, with differences between the platforms. We present "actionability" as a conceptual metric to reveal the data quality gap between characteristics of data on CCPs and the desired characteristics of data for AI in healthcare. Finally, we propose a commons-based stewardship model for documenting, sharing and maintaining datasets on CCPs and end with a discussion of limitations and open questions.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学成像数据集是医疗保健领域人工智能 (AI) 的基础。诊断算法的准确性、稳健性和公平性取决于训练和评估模型的数据（及其质量​​）。医学成像数据集越来越多地向公众开放，并且通常托管在社区贡献平台 (CCP) 上，包括 Kaggle 或 HuggingFace 等私营公司。虽然开放数据对于增强数据公共价值的重新分配很重要，但我们发现当前的 CCP 治理模式无法维持共享、记录和评估数据集所需的质量和推荐的做法。在本文中，我们研究了 CCP 的医学成像数据集以及它们的记录、共享和维护方式。我们首先强调医学成像和计算机视觉之间的一些差异，特别是由于未采用推荐的数据集管理实践而导致潜在有害的下游影响。然后，我们分析了 CCP 上的 20 个（10 个医学和 10 个计算机视觉）流行数据集，发现许可证模糊、缺乏持久标识符和存储、重复和缺失元数据，以及平台之间的差异。我们将“可操作性”作为一个概念指标来揭示 CCP 数据特征与医疗保健人工智能数据所需特征之间的数据质量差距。最后，我们提出了一个基于公共资源的管理模型，用于记录、共享和维护 CCP 数据集，并以对局限性和开放性问题的讨论结束。</details>
**PDF:** <http://arxiv.org/pdf/2402.06353v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Towards Chip-in-the-loop Spiking Neural Network Training via Metropolis-Hastings Sampling**<br />
**Title_cn:** 通过 Metropolis-Hastings 采样进行芯片在环尖峰神经网络训练<br />
**Authors:** Ali Safa, Vikrant Jaltare, Samira Sebt, Kameron Gano, Johannes Leugering, Georges Gielen, Gert Cauwenberghs<br />
**Abstract:** <details><summary>原文: </summary>This paper studies the use of Metropolis-Hastings sampling for training Spiking Neural Network (SNN) hardware subject to strong unknown non-idealities, and compares the proposed approach to the common use of the backpropagation of error (backprop) algorithm and surrogate gradients, widely used to train SNNs in literature. Simulations are conducted within a chip-in-the-loop training context, where an SNN subject to unknown distortion must be trained to detect cancer from measurements, within a biomedical application context. Our results show that the proposed approach strongly outperforms the use of backprop by up to $27\%$ higher accuracy when subject to strong hardware non-idealities. Furthermore, our results also show that the proposed approach outperforms backprop in terms of SNN generalization, needing $>10 \times$ less training data for achieving effective accuracy. These findings make the proposed training approach well-suited for SNN implementations in analog subthreshold circuits and other emerging technologies where unknown hardware non-idealities can jeopardize backprop.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了使用 Metropolis-Hastings 采样来训练强未知非理想性的尖峰神经网络 (SNN) 硬件，并将所提出的方法与误差反向传播 (backprop) 算法和代理梯度的常用方法进行了比较，广泛应用于文献中用于训练 SNN。模拟是在芯片在环训练环境中进行的，其中必须训练受到未知失真影响的 SNN，以便在生物医学应用环境中通过测量来检测癌症。我们的结果表明，当受到强烈的硬件非理想性影响时，所提出的方法比使用反向传播的准确性高出 27%$。此外，我们的结果还表明，所提出的方法在 SNN 泛化方面优于反向传播，需要减少 10 倍以上的训练数据才能实现有效的准确性。这些发现使得所提出的训练方法非常适合模拟亚阈值电路和其他新兴技术中的 SNN 实现，在这些技术中，未知的硬件非理想性可能会危及反向传播。</details>
**PDF:** <http://arxiv.org/pdf/2402.06284v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **The Berkeley Single Cell Computational Microscopy (BSCCM) Dataset**<br />
**Title_cn:** 伯克利单细胞计算显微镜 (BSCCM) 数据集<br />
**Authors:** Henry Pinkard, Cherry Liu, Fanice Nyatigo, Daniel A. Fletcher, Laura Waller<br />
**Abstract:** <details><summary>原文: </summary>Computational microscopy, in which hardware and algorithms of an imaging system are jointly designed, shows promise for making imaging systems that cost less, perform more robustly, and collect new types of information. Often, the performance of computational imaging systems, especially those that incorporate machine learning, is sample-dependent. Thus, standardized datasets are an essential tool for comparing the performance of different approaches. Here, we introduce the Berkeley Single Cell Computational Microscopy (BSCCM) dataset, which contains over ~12,000,000 images of 400,000 of individual white blood cells. The dataset contains images captured with multiple illumination patterns on an LED array microscope and fluorescent measurements of the abundance of surface proteins that mark different cell types. We hope this dataset will provide a valuable resource for the development and testing of new algorithms in computational microscopy and computer vision with practical biomedical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算显微镜联合设计了成像系统的硬件和算法，有望使成像系统成本更低、性能更强大并收集新型信息。通常，计算成像系统的性能，尤其是那些包含机器学习的系统，是依赖于样本的。因此，标准化数据集是比较不同方法性能的重要工具。在这里，我们介绍伯克利单细胞计算显微镜 (BSCCM) 数据集，其中包含 400,000 个单个白细胞的超过 12,000,000 张图像。该数据集包含在 LED 阵列显微镜上使用多种照明模式捕获的图像，以及标记不同细胞类型的表面蛋白丰度的荧光测量值。我们希望该数据集将为计算显微镜和计算机视觉以及实际生物医学应用中的新算法的开发和测试提供宝贵的资源。</details>
**PDF:** <http://arxiv.org/pdf/2402.06191v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters**<br />
**Title_cn:** 开发和验证人工智能模型以准确预测脊柱骨盆参数<br />
**Authors:** Edward S. Harake, Joseph R. Linzey, Cheng Jiang, Rushikesh S. Joshi, Mark M. Zaki, Jaes C. Jones, Siri S. Khalsa, John H. Lee, Zachary Wilseck, Jacob R. Joseph, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Objective. Achieving appropriate spinopelvic alignment has been shown to be associated with improved clinical symptoms. However, measurement of spinopelvic radiographic parameters is time-intensive and interobserver reliability is a concern. Automated measurement tools have the promise of rapid and consistent measurements, but existing tools are still limited by some degree of manual user-entry requirements. This study presents a novel artificial intelligence (AI) tool called SpinePose that automatically predicts spinopelvic parameters with high accuracy without the need for manual entry.   Methods. SpinePose was trained and validated on 761 sagittal whole-spine X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle (T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was labeled by 4 reviewers, including fellowship-trained spine surgeons and a fellowship-trained radiologist with neuroradiology subspecialty certification. Median errors relative to the most senior reviewer were calculated to determine model accuracy on test images. Intraclass correlation coefficients (ICC) were used to assess inter-rater reliability.   Results. SpinePose exhibited the following median (interquartile range) parameter errors: SVA: 2.2(2.3)mm, p=0.93; PT: 1.3(1.2){\deg}, p=0.48; SS: 1.7(2.2){\deg}, p=0.64; PI: 2.2(2.1){\deg}, p=0.24; LL: 2.6(4.0){\deg}, p=0.89; T1PA: 1.1(0.9){\deg}, p=0.42; and L1PA: 1.4(1.6){\deg}, p=0.49. Model predictions also exhibited excellent reliability at all parameters (ICC: 0.91-1.0).   Conclusions. SpinePose accurately predicted spinopelvic parameters with excellent reliability comparable to fellowship-trained spine surgeons and neuroradiologists. Utilization of predictive AI tools in spinal imaging can substantially aid in patient selection and surgical planning.</details>
**Abstract_cn:** <details><summary>译文: </summary>客观的。已证明实现适当的脊柱骨盆对齐与改善临床症状有关。然而，脊柱骨盆放射学参数的测量非常耗时，并且观察者间的可靠性是一个问题。自动化测量工具有望实现快速且一致的测量，但现有工具仍然受到一定程度的手动用户输入要求的限制。这项研究提出了一种名为 SpinePose 的新型人工智能 (AI) 工具，可以高精度自动预测脊柱骨盆参数，无需手动输入。方法。 SpinePose 经过 761 幅矢状全脊柱 X 射线的训练和验证，以预测矢状垂直轴 (SVA)、骨盆倾斜 (PT)、骨盆倾角 (PI)、骶骨倾斜 (SS)、腰椎前凸 (LL)、T1 骨盆角 (T1PA) 和 L1 骨盆角 (L1PA)。 4 名评审员对 40 张 X 射线的单独测试集进行了标记，其中包括接受过专科培训的脊柱外科医生和一名经过专科培训且具有神经放射学亚专业认证的放射科医生。计算相对于最资深审阅者的中值误差，以确定测试图像上的模型准确性。组内相关系数（ICC）用于评估评估者间的可靠性。结果。 SpinePose 表现出以下中值（四分位距）参数误差：SVA：2.2(2.3)mm，p=0.93； PT: 1.3(1.2){\deg}, p=0.48; SS: 1.7(2.2){\deg}, p=0.64; PI: 2.2(2.1){\deg}, p=0.24; LL: 2.6(4.0){\deg}, p=0.89; T1PA: 1.1(0.9){\deg}, p=0.42; L1PA：1.4(1.6){\deg}，p=0.49。模型预测在所有参数上也表现出出色的可靠性（ICC：0.91-1.0）。结论。 SpinePose 能够准确预测脊柱骨盆参数，其可靠性可与经过专科培训的脊柱外科医生和神经放射科医生相媲美。在脊柱成像中使用预测性人工智能工具可以极大地帮助患者选择和手术计划。</details>
**PDF:** <http://arxiv.org/pdf/2402.06185v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Domain Generalization with Small Data**<br />
**Title_cn:** 小数据领域泛化<br />
**Authors:** Kecheng Chen, Elena Gal, Hong Yan, Haoliang Li<br />
**Abstract:** <details><summary>原文: </summary>In this work, we propose to tackle the problem of domain generalization in the context of \textit{insufficient samples}. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the \textit{distribution over distributions} (i.e., the global perspective alignment) and the distribution-based contrastive semantic alignment (i.e., the local perspective alignment). Extensive experimental results on three challenging medical datasets show the effectiveness of our proposed method in the context of insufficient data compared with state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们建议在 \textit{样本不足} 的背景下解决域泛化问题。我们建议通过将每个数据点映射到概率嵌入来学习基于概率框架的域不变表示，而不是基于确定性模型提取潜在特征嵌入。具体来说，我们首先将经验最大平均差异（MMD）扩展到一种新的概率MMD，它可以测量由一系列潜在分布而不是潜在点组成的混合分布（即源域）之间的差异。此外，一种新颖的概率 CSA 损失不是基于潜在点对强加对比语义对齐（CSA）损失，而是鼓励正概率嵌入对更接近，同时将其他负概率嵌入对拉开。受益于概率模型捕获的学习表示，我们提出的方法可以将 \textit{分布上的分布} 的测量（即全局视角对齐）和基于分布的对比语义对齐（即局部视角对齐）结合起来。对三个具有挑战性的医学数据集的广泛实验结果表明，与最先进的方法相比，我们提出的方法在数据不足的情况下是有效的。</details>
**PDF:** <http://arxiv.org/pdf/2402.06150v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **ContPhy: Continuum Physical Concept Learning and Reasoning from Videos**<br />
**Title_cn:** ContPhy：从视频中学习和推理连续物理概念<br />
**Authors:** Zhicheng Zheng, Xin Yan, Zhenfang Chen, Jingzhou Wang, Qin Zhi Eddie Lim, Joshua B. Tenenbaum, Chuang Gan<br />
**Abstract:** <details><summary>原文: </summary>We introduce the Continuum Physical Dataset (ContPhy), a novel benchmark for assessing machine physical commonsense. ContPhy complements existing physical reasoning benchmarks by encompassing the inference of diverse physical properties, such as mass and density, across various scenarios and predicting corresponding dynamics. We evaluated a range of AI models and found that they still struggle to achieve satisfactory performance on ContPhy, which shows that the current AI models still lack physical commonsense for the continuum, especially soft-bodies, and illustrates the value of the proposed dataset. We also introduce an oracle model (ContPRO) that marries the particle-based physical dynamic models with the recent large language models, which enjoy the advantages of both models, precise dynamic predictions, and interpretable reasoning. ContPhy aims to spur progress in perception and reasoning within diverse physical settings, narrowing the divide between human and machine intelligence in understanding the physical world. Project page: https://physical-reasoning-project.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了连续体物理数据集（ContPhy），这是一种用于评估机器物理常识的新颖基准。 ContPhy 通过涵盖各种场景中不同物理属性（例如质量和密度）的推理并预测相应的动态，补充了现有的物理推理基准。我们评估了一系列 AI 模型，发现它们仍然难以在 ContPhy 上获得令人满意的性能，这表明当前的 AI 模型仍然缺乏连续体（尤其是软体）的物理常识，并说明了所提出的数据集的价值。我们还引入了一种预言机模型（ContPRO），它将基于粒子的物理动态模型与最近的大型语言模型结合起来，它具有两种模型的优点，精确的动态预测和可解释的推理。 ContPhy 旨在促进不同物理环境中感知和推理的进步，缩小人类和机器智能在理解物理世界方面的鸿沟。项目页面：https://physical-reasoning-project.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2402.06119v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Spatially-Attentive Patch-Hierarchical Network with Adaptive Sampling for Motion Deblurring**<br />
**Title_cn:** 具有自适应采样运动去模糊功能的空间注意力补丁分层网络<br />
**Authors:** Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan<br />
**Abstract:** <details><summary>原文: </summary>This paper tackles the problem of motion deblurring of dynamic scenes. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in non-uniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Most existing approaches achieve a large receptive field by increasing the number of generic convolution layers and kernel size. In this work, we propose a pixel adaptive and feature attentive design for handling large blur variations across different spatial locations and process each test image adaptively. We design a content-aware global-local filtering module that significantly improves performance by considering not only global dependencies but also by dynamically exploiting neighboring pixel information. We further introduce a pixel-adaptive non-uniform sampling strategy that implicitly discovers the difficult-to-restore regions present in the image and, in turn, performs fine-grained refinement in a progressive manner. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate that our approach performs favorably against the state-of-the-art deblurring algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了动态场景的运动去模糊问题。尽管端到端全卷积设计最近在非均匀运动去模糊方面取得了最先进的进展，但它们的性能与复杂性的权衡仍然不是最优的。大多数现有方法通过增加通用卷积层的数量和内核大小来实现大的感受野。在这项工作中，我们提出了一种像素自适应和特征细心的设计，用于处理不同空间位置上的大模糊变化，并自适应地处理每个测试图像。我们设计了一个内容感知的全局局部过滤模块，该模块不仅考虑全局依赖性，还通过动态利用相邻像素信息来显着提高性能。我们进一步引入了一种像素自适应非均匀采样策略，该策略隐式地发现图像中存在的难以恢复的区域，进而以渐进的方式执行细粒度的细化。与现有技术在去模糊基准上的广泛定性和定量比较表明，我们的方法比最先进的去模糊算法表现得更好。</details>
**PDF:** <http://arxiv.org/pdf/2402.06117v1><br />
**Code:** null<br />

