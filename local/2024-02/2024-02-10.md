## [UPDATED!] **2024-02-10** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models**<br />
**Title_cn:** 使用稳定扩散模型合成 B 型主动脉夹层的 CTA 图像数据<br />
**Authors:** Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah<br />
**Abstract:** <details><summary>原文: </summary>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，稳定扩散（SD）在生成人工智能领域获得了广泛关注，从而有助于合成具有独特特征的医学成像数据。其目的是为持续努力做出贡献，重点是克服数据稀缺的限制并提高心血管图像处理的机器学习算法的能力。因此，在本研究中，仅使用有限数量的 CTA 图像作为输入，通过基于用户定义的文本提示微调稳定扩散模型来探索生成合成心脏 CTA 图像的可能性。通过结合定量分析和定性评估对合成数据进行全面评估，临床医生评估生成数据的质量。事实证明，使用文本到图像 (T2I) 稳定扩散模型可以成功生成心脏 CTA 图像。结果表明，经过调整的 T2I CTA 扩散模型能够生成具有急性 B 型主动脉夹层 (TBAD) 医疗状况通常独有的特征的图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.06969v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations**<br />
**Title_cn:** 双子座上医学院：探索多模态大语言模型在医学挑战问题和幻觉方面的能力<br />
**Authors:** Ankit Pal, Malaikannan Sankarasubbu<br />
**Abstract:** <details><summary>原文: </summary>Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs. Python module can be found at https://github.com/promptslab/RosettaEval</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型在医疗保健行业具有潜在的价值，但通过严格的评估来验证其安全性和有效性至关重要。为此，我们全面评估了开源法学硕士和谷歌新的多模式法学硕士（称为 Gemini），涵盖医学推理、幻觉检测和医学视觉问答任务。虽然 Gemini 表现出了能力，但它在诊断准确性方面落后于 MedPaLM 2 和 GPT-4 等最先进的模型。此外，Gemini 在医学 VQA 数据集上的准确率达到 61.45%，明显低于 GPT-4V 88% 的准确率。我们的分析表明，双子座非常容易产生幻觉、过度自信和知识差距，这表明如果不加批判地部署就会存在风险。我们还按医学主题和测试类型进行了详细分析，为开发人员和临床医生提供了可操作的反馈。为了降低风险，我们应用了提高绩效的提示策略。此外，我们还发布了用于医学法学硕士评估的Python模块，并在Hugging Face上为医学领域法学硕士建立了专门的排行榜，以促进未来的研究和开发。 Python 模块可以在 https://github.com/promptslab/RosettaEval 找到</details>
**PDF:** <http://arxiv.org/pdf/2402.07023v1><br />
**Code:** <https://github.com/promptslab/rosettaeval>**<br />
>>**index:** 2<br />
**Title:** **An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation**<br />
**Title_cn:** 脑肿瘤分割处理和迁移学习的优化框架<br />
**Authors:** Tianyi Ren, Ethan Honey, Harshitha Rebala, Abhishek Sharma, Agamdeep Chopra, Mehmet Kurt<br />
**Abstract:** <details><summary>原文: </summary>Tumor segmentation from multi-modal brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology. The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning. However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability. In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation. This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and transfer learning. On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于样本有限、形状差异大以及肿瘤形态分布不均匀，多模态脑 MRI 图像的肿瘤分割是一项具有挑战性的任务。深度学习的最新进展使自动医学图像分割的性能得到了显着提高。然而，模型预测在准确性和普适性方面尚未达到临床应用所需的水平。为了解决 BraTS 2023 挑战 1、2 和 3 中提出的独特问题，我们构建了基于 3D U-Net 模型的脑肿瘤分割优化框架。该框架融合了一系列技术，包括各种预处理和后处理技术以及迁移学习。在验证数据集上，这种多模态脑肿瘤分割框架在挑战 1、2、3 上的平均病变 Dice 得分分别为 0.79、0.72、0.74。</details>
**PDF:** <http://arxiv.org/pdf/2402.07008v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts**<br />
**Title_cn:** 评估分布变化下 3D 图像分割的不确定性估计方法<br />
**Authors:** Masoumeh Javanbakhat, Md Tasnimul Hasan, Cristoph Lippert<br />
**Abstract:** <details><summary>原文: </summary>In recent years, machine learning has witnessed extensive adoption across various sectors, yet its application in medical image-based disease detection and diagnosis remains challenging due to distribution shifts in real-world data. In practical settings, deployed models encounter samples that differ significantly from the training dataset, especially in the health domain, leading to potential performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to capture either unimodal or multimodal aspects in the posterior distribution. Our findings demonstrate that methods capable of addressing multimodal characteristics in the posterior distribution, offer more dependable uncertainty estimates. This research contributes to enhancing the utility of deep learning in healthcare, making diagnostic predictions more robust and trustworthy.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，机器学习在各个领域得到了广泛采用，但由于现实世界数据的分布变化，其在基于医学图像的疾病检测和诊断中的应用仍然具有挑战性。在实际设置中，部署的模型会遇到与训练数据集显着不同的样本，尤其是在健康领域，从而导致潜在的性能问题。这种限制阻碍了深度学习模型在健康应用中的表现力和可靠性。因此，确定能够在卫生部门分布变化的背景下产生可靠的不确定性估计的方法变得至关重要。在本文中，我们探讨了使用尖端贝叶斯和非贝叶斯方法来检测分布偏移样本的可行性，旨在在分割任务中实现可靠且值得信赖的诊断预测。具体来说，我们比较了三种不同的不确定性估计方法，每种方法都旨在捕获后验分布中的单峰或多峰方面。我们的研究结果表明，能够解决后验分布中多模态特征的方法可以提供更可靠的不确定性估计。这项研究有助于增强深度学习在医疗保健领域的实用性，使诊断预测更加稳健和可信。</details>
**PDF:** <http://arxiv.org/pdf/2402.06937v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance**<br />
**Title_cn:** 用于推进农场监控的领域适应性微调蒸馏框架<br />
**Authors:** Raza Imam, Muhammad Huzaifa, Nabil Mansour, Shaher Bano Mirza, Fouad Lamghari<br />
**Abstract:** <details><summary>原文: </summary>In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the Fine-Tune Distillation framework. The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos. Building upon this foundation, the Fine-Tune Distillation framework conducts fine-tuning of student models using the auto-annotated dataset. This process involves transferring knowledge from a large teacher model to a student model, resembling a variant of Knowledge Distillation. The Fine-Tune Distillation framework aims to be adaptable to specific use cases, enabling the transfer of knowledge from the large models to the small models, making it suitable for domain-specific applications. By leveraging our raw dataset collected from Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model, GroundingDINO, the Fine-Tune Distillation framework produces a lightweight deployable model, YOLOv8. This framework demonstrates high performance and computational efficiency, facilitating efficient real-time object detection. Our code is available at \href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们提出了一个用于骆驼农场监控的自动化框架，引入了两个关键贡献：统一自动注释框架和微调蒸馏框架。统一自动注释方法结合了 GroundingDINO (GD) 和分段任意模型 (SAM) 两种模型，可自动注释从监控视频中提取的原始数据集。在此基础上，Fine-Tune Distillation 框架使用自动注释数据集对学生模型进行微调。这个过程涉及将知识从大型教师模型转移到学生模型，类似于知识蒸馏的变体。 Fine-Tune Distillation 框架旨在适应特定用例，实现知识从大型模型到小型模型的迁移，使其适合特定领域的应用程序。通过利用从阿联酋迪拜的 Al-Marmoom 骆驼农场收集的原始数据集以及预先训练的教师模型 GroundingDINO，Fine-Tune Distillation 框架生成了一个轻量级的可部署模型 YOLOv8。该框架表现出高性能和计算效率，有助于高效的实时目标检测。我们的代码位于 \href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}</details>
**PDF:** <http://arxiv.org/pdf/2402.07059v1><br />
**Code:** <https://github.com/razaimam45/fine-tune-distillation>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **A Change Detection Reality Check**<br />
**Title_cn:** 变更检测现实检查<br />
**Authors:** Isaac Corley, Caleb Robinson, Anthony Ortiz<br />
**Abstract:** <details><summary>原文: </summary>In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，遥感文献中提出的变化检测深度学习架构激增。这些方法声称可以在不同的标准基准数据集上提供最先进的性能。然而，该领域真的取得了重大进展吗？在本文中，我们进行的实验得出结论，没有训练技巧或复杂架构变化的简单 U-Net 分割基线仍然是变化检测任务的最佳执行者。</details>
**PDF:** <http://arxiv.org/pdf/2402.06994v1><br />
**Code:** <https://github.com/isaaccorley/a-change-detection-reality-check>**<br />
>>**index:** 2<br />
**Title:** **OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery**<br />
**Title_cn:** OSSAR：迈向机器人辅助手术中的开放式手术活动识别<br />
**Authors:** Long Bai, Guankun Wang, Jie Wang, Xiaoxiao Yang, Huxin Gao, Xin Liang, An Wang, Mobarakol Islam, Hongliang Ren<br />
**Abstract:** <details><summary>原文: </summary>In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at https://github.com/longbai1006/OSSAR.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动化机器人手术和计算机辅助干预领域，了解机器人手术活动至关重要。专用于手术活动识别的现有算法主要迎合预定义的封闭集范式，忽略了现实世界开放集场景的挑战。当存在来自训练阶段未见过的类的测试样本时，此类算法通常会出现问题。为了解决这个问题，我们引入了一种创新的开放集手术活动识别（OSSAR）框架。我们的解决方案利用超球面倒数点策略来增强特征空间中已知类和未知类之间的区别。此外，我们通过改进模型校准来解决对封闭集过度自信的问题，避免将未知类错误分类为已知类。为了支持我们的主张，我们利用公共 JIGSAWS 数据集建立了一个开放式手术活动基准。此外，我们还收集了用于手术活动任务的内窥镜粘膜下剥离的新颖数据集。对这些数据集的广泛比较和消融实验表明，我们的方法明显优于现有的最先进方法。我们提出的解决方案可以有效解决现实手术场景的挑战。我们的代码可在 https://github.com/longbai1006/OSSAR 上公开访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.06985v1><br />
**Code:** <https://github.com/longbai1006/ossar>**<br />
>>**index:** 3<br />
**Title:** **Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI**<br />
**Title_cn:** 通过标记 MRI 的 4D 运动场的跨模式转换进行语音运动异常检测<br />
**Authors:** Xiaofeng Liu, Fangxu Xing, Jiachen Zhuo, Maureen Stone, Jerry L. Prince, Georges El Fakhri, Jonghye Woo<br />
**Abstract:** <details><summary>原文: </summary>Understanding the relationship between tongue motion patterns during speech and their resulting speech acoustic outcomes -- i.e., articulatory-acoustic relation -- is of great importance in assessing speech quality and developing innovative treatment and rehabilitative strategies. This is especially important when evaluating and detecting abnormal articulatory features in patients with speech-related disorders. In this work, we aim to develop a framework for detecting speech motion anomalies in conjunction with their corresponding speech acoustics. This is achieved through the use of a deep cross-modal translator trained on data from healthy individuals only, which bridges the gap between 4D motion fields obtained from tagged MRI and 2D spectrograms derived from speech acoustic data. The trained translator is used as an anomaly detector, by measuring the spectrogram reconstruction quality on healthy individuals or patients. In particular, the cross-modal translator is likely to yield limited generalization capabilities on patient data, which includes unseen out-of-distribution patterns and demonstrates subpar performance, when compared with healthy individuals.~A one-class SVM is then used to distinguish the spectrograms of healthy individuals from those of patients. To validate our framework, we collected a total of 39 paired tagged MRI and speech waveforms, consisting of data from 36 healthy individuals and 3 tongue cancer patients. We used both 3D convolutional and transformer-based deep translation models, training them on the healthy training set and then applying them to both the healthy and patient testing sets. Our framework demonstrates a capability to detect abnormal patient data, thereby illustrating its potential in enhancing the understanding of the articulatory-acoustic relation for both healthy individuals and patients.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解言语过程中舌头运动模式与其产生的言语声学结果之间的关系（即发音-声学关系）对于评估言语质量和制定创新的治疗和康复策略非常重要。这在评估和检测患有言语相关障碍的患者的异常发音特征时尤其重要。在这项工作中，我们的目标是开发一个框架来检测语音运动异常及其相应的语音声学。这是通过使用仅接受健康个体数据训练的深度跨模态转换器来实现的，它弥合了从标记 MRI 获得的 4D 运动场与从语音声学数据导出的 2D 声谱图之间的差距。经过训练的翻译器被用作异常检测器，通过测量健康个体或患者的频谱图重建质量。特别是，与健康个体相比，跨模式翻译器可能对患者数据产生有限的泛化能力，其中包括看不见的分布外模式，并且表现出低于标准的性能。〜然后使用一类 SVM 来区分健康个体的光谱图与患者的光谱图。为了验证我们的框架，我们总共收集了 39 个配对的标记 MRI 和语音波形，其中包括来自 36 名健康个体和 3 名舌癌患者的数据。我们使用 3D 卷积和基于 Transformer 的深度翻译模型，在健康训练集上对其进行训练，然后将其应用于健康和患者测试集。我们的框架展示了检测异常患者数据的能力，从而说明了其在增强健康个体和患者对发音-声学关系的理解方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.06984v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Architectural Neural Backdoors from First Principles**<br />
**Title_cn:** 来自第一原理的建筑神经后门<br />
**Authors:** Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot<br />
**Abstract:** <details><summary>原文: </summary>While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, we conducted a user study, revealing that ML developers can only identify suspicious components in common model definitions as backdoors in 37% of cases, while they surprisingly preferred backdoored models in 33% of cases. To contextualize these results, we find that language models outperform humans at the detection of backdoors. Finally, we discuss defenses against architectural backdoors, emphasizing the need for robust and comprehensive strategies to safeguard the integrity of ML systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然之前的研究通过改变神经网络的参数来设置后门，但最近的工作发现了一个更阴险的威胁：嵌入网络架构定义中的后门。这涉及注入常见的架构组件，例如激活函数和池化层，以巧妙地引入即使在（完全重新）训练后仍然存在的后门行为。然而，建筑后门的全部范围和影响在很大程度上仍未得到探索。鲍伯-伊里扎等人。 [2023]引入了第一个架构后门；他们展示了如何为棋盘模式创建后门，但从未解释如何针对选择的任意触发模式。在这项工作中，我们构建了一个任意触发检测器，可用于在没有人工监督的情况下对架构进行后门处理。这导致我们重新审视架构后门的概念并对它们进行分类，描述 12 种不同的类型。为了衡量检测此类后门的难度，我们进行了一项用户研究，结果显示机器学习开发人员只能在 37% 的情况下将通用模型定义中的可疑组件识别为后门，而令人惊讶的是，在 33% 的情况下他们更喜欢后门模型。为了将这些结果结合起来，我们发现语言模型在后门检测方面优于人类。最后，我们讨论了针对架构后门的防御，强调需要强大而全面的策略来保护机器学习系统的完整性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06957v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Semantic Object-level Modeling for Robust Visual Camera Relocalization**<br />
**Title_cn:** 用于鲁棒视觉相机重定位的语义对象级建模<br />
**Authors:** Yifan Zhu, Lingjuan Miao, Haitao Wu, Zhiqiang Zhou, Weiyi Chen, Longwen Wu<br />
**Abstract:** <details><summary>原文: </summary>Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics. Due to the improvement of CNN-based object detection algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail. However, ellipsoids (quadrics) generated by axis-aligned object detection may limit the accuracy of the object-level representation and degenerate the performance of visual relocalization system. In this paper, we propose a novel method of automatic object-level voxel modeling for accurate ellipsoidal representations of objects. As for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of these modules are entirely intergrated into visual SLAM system. Experimental results show that our semantic object-level mapping and object-based visual relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉重定位对于移动机器人的自主视觉定位和导航至关重要。由于基于CNN的目标检测算法的改进，视觉重定位的鲁棒性大大增强，特别是在经典方法失败的视点上。然而，由轴对齐对象检测生成的椭球体（二次曲面）可能会限制对象级表示的准确性并降低视觉重定位系统的性能。在本文中，我们提出了一种自动对象级体素建模的新方法，用于精确表示对象的椭球体。对于视觉重定位，我们为相机姿态恢复设计了更好的姿态优化策略，以充分利用2D拟合椭圆和3D精确椭球的投影特性。所有这些模块都完全集成到视觉SLAM系统中。实验结果表明，我们的语义对象级映射和基于对象的视觉重定位方法在对新视点的鲁棒性方面显着增强了视觉重定位的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.06951v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Latent Enhancing AutoEncoder for Occluded Image Classification**<br />
**Title_cn:** 用于遮挡图像分类的潜在增强自动编码器<br />
**Authors:** Ketan Kotwal, Tanay Deshmukh, Preeti Gopal<br />
**Abstract:** <details><summary>原文: </summary>Large occlusions result in a significant decline in image classification accuracy. During inference, diverse types of unseen occlusions introduce out-of-distribution data to the classification model, leading to accuracy dropping as low as 50%. As occlusions encompass spatially connected regions, conventional methods involving feature reconstruction are inadequate for enhancing classification performance. We introduce LEARN: Latent Enhancing feAture Reconstruction Network -- An auto-encoder based network that can be incorporated into the classification model before its classifier head without modifying the weights of classification model. In addition to reconstruction and classification losses, training of LEARN effectively combines intra- and inter-class losses calculated over its latent space -- which lead to improvement in recovering latent space of occluded data, while preserving its class-specific discriminative information. On the OccludedPASCAL3D+ dataset, the proposed LEARN outperforms standard classification models (VGG16 and ResNet-50) by a large margin and up to 2% over state-of-the-art methods. In cross-dataset testing, our method improves the average classification accuracy by more than 5% over the state-of-the-art methods. In every experiment, our model consistently maintains excellent accuracy on in-distribution data.</details>
**Abstract_cn:** <details><summary>译文: </summary>大的遮挡会导致图像分类精度显着下降。在推理过程中，各种类型的看不见的遮挡会将分布外的数据引入分类模型，导致准确率下降至 50%。由于遮挡包含空间连接的区域，涉及特征重建的传统方法不足以增强分类性能。我们介绍学习：潜在增强特征重建网络——一种基于自动编码器的网络，可以在分类器头之前合并到分类模型中，而无需修改分类模型的权重。除了重建和分类损失之外，LEARN 的训练还有效地结合了在其潜在空间上计算的类内和类间损失——这导致了在恢复被遮挡数据的潜在空间方面的改进，同时保留了其特定于类的判别信息。在 OcclusionPASCAL3D+ 数据集上，所提出的 LEARN 的性能大幅优于标准分类模型（VGG16 和 ResNet-50），并且比最先进的方法高出 2%。在跨数据集测试中，我们的方法比最先进的方法将平均分类精度提高了 5% 以上。在每次实验中，我们的模型始终保持分布数据的出色准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06936v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Disentangled Latent Energy-Based Style Translation: An Image-Level Structural MRI Harmonization Framework**<br />
**Title_cn:** 解开的基于潜在能量的风格翻译：图像级结构 MRI 协调框架<br />
**Authors:** Mengqi Wu, Lintao Zhang, Pew-Thian Yap, Hongtu Zhu, Mingxia Liu<br />
**Abstract:** <details><summary>原文: </summary>Brain magnetic resonance imaging (MRI) has been extensively employed across clinical and research fields, but often exhibits sensitivity to site effects arising from nonbiological variations such as differences in field strength and scanner vendors. Numerous retrospective MRI harmonization techniques have demonstrated encouraging outcomes in reducing the site effects at image level. However, existing methods generally suffer from high computational requirements and limited generalizability, restricting their applicability to unseen MRIs. In this paper, we design a novel disentangled latent energy-based style translation (DLEST) framework for unpaired image-level MRI harmonization, consisting of (1) site-invariant image generation (SIG), (2) site-specific style translation (SST), and (3) site-specific MRI synthesis (SMS). Specifically, the SIG employs a latent autoencoder to encode MRIs into a low-dimensional latent space and reconstruct MRIs from latent codes. The SST utilizes an energy-based model to comprehend the global latent distribution of a target domain and translate source latent codes toward the target domain, while SMS enables MRI synthesis with a target-specific style. By disentangling image generation and style translation in latent space, the DLEST can achieve efficient style translation. Our model was trained on T1-weighted MRIs from a public dataset (with 3,984 subjects across 58 acquisition sites/settings) and validated on an independent dataset (with 9 traveling subjects scanned in 11 sites/settings) in 4 tasks: (1) histogram and clustering comparison, (2) site classification, (3) brain tissue segmentation, and (4) site-specific MRI synthesis. Qualitative and quantitative results demonstrate the superiority of our method over several state-of-the-arts.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑磁共振成像（MRI）已广泛应用于临床和研究领域，但通常对非生物变化（例如场强和扫描仪供应商的差异）引起的位点效应敏感。许多回顾性 MRI 协调技术在减少图像水平的位点效应方面已显示出令人鼓舞的结果。然而，现有方法通常存在高计算要求和有限的通用性，限制了它们对看不见的 MRI 的适用性。在本文中，我们设计了一种新颖的基于潜在能量的解开风格翻译（DLEST）框架，用于不成对图像级 MRI 协调，包括（1）位点不变图像生成（SIG），（2）位点特定风格翻译（ SST），以及（3）位点特异性 MRI 合成（SMS）。具体来说，SIG 采用潜在自动编码器将 MRI 编码到低维潜在空间中，并从潜在代码重建 MRI。 SST 利用基于能量的模型来理解目标域的全局潜在分布，并将源潜在代码转换为目标域，而 SMS 可以实现具有目标特定风格的 MRI 合成。通过在潜在空间中解开图像生成和风格翻译，DLEST 可以实现高效的风格翻译。我们的模型在公共数据集（58 个采集站点/设置中的 3,984 名受试者）中进行了 T1 加权 MRI 训练，并在独立数据集（在 11 个站点/设置中扫描了 9 名旅行受试者）的 4 项任务中进行了验证：(1) 直方图和聚类比较，（2）位点分类，（3）脑组织分割，（4）位点特异性 MRI 合成。定性和定量结果证明了我们的方法相对于几种最先进的方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06875v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **For Better or For Worse? Learning Minimum Variance Features With Label Augmentation**<br />
**Title_cn:** 不管是好是坏？通过标签增强学习最小方差特征<br />
**Authors:** Muthu Chidambaram, Rong Ge<br />
**Abstract:** <details><summary>原文: </summary>Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年里，数据增强对于成功训练分类任务的深度学习模型至关重要。数据增强技术的一个重要子类（包括标签平滑和混合）不仅涉及修改输入数据，还涉及在模型训练期间修改输入标签。在这项工作中，我们分析了此类方法的标签增强方面所发挥的作用。我们证明，使用标签增强训练的线性可分离数据的线性模型仅学习数据中的最小方差特征，而标准训练（包括权重衰减）可以学习更高的方差特征。我们的结果的一个重要结果是负面的：与标准训练相比，标签平滑和混合对于训练数据的对抗性扰动的稳健性可能较差。我们通过一系列合成数据和图像分类基准的实验来验证我们的理论反映了实践。</details>
**PDF:** <http://arxiv.org/pdf/2402.06855v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Point cloud-based registration and image fusion between cardiac SPECT MPI and CTA**<br />
**Title_cn:** 心脏 SPECT MPI 和 CTA 之间基于点云的配准和图像融合<br />
**Authors:** Shaojie Tang, Penpen Miao, Xingyu Gao, Yu Zhong, Dantong Zhu, Haixing Wen, Zhihui Xu, Qiuyue Wei, Hongping Yao, Xin Huang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>A method was proposed for the point cloud-based registration and image fusion between cardiac single photon emission computed tomography (SPECT) myocardial perfusion images (MPI) and cardiac computed tomography angiograms (CTA). Firstly, the left ventricle (LV) epicardial regions (LVERs) in SPECT and CTA images were segmented by using different U-Net neural networks trained to generate the point clouds of the LV epicardial contours (LVECs). Secondly, according to the characteristics of cardiac anatomy, the special points of anterior and posterior interventricular grooves (APIGs) were manually marked in both SPECT and CTA image volumes. Thirdly, we developed an in-house program for coarsely registering the special points of APIGs to ensure a correct cardiac orientation alignment between SPECT and CTA images. Fourthly, we employed ICP, SICP or CPD algorithm to achieve a fine registration for the point clouds (together with the special points of APIGs) of the LV epicardial surfaces (LVERs) in SPECT and CTA images. Finally, the image fusion between SPECT and CTA was realized after the fine registration. The experimental results showed that the cardiac orientation was aligned well and the mean distance error of the optimal registration method (CPD with affine transform) was consistently less than 3 mm. The proposed method could effectively fuse the structures from cardiac CTA and SPECT functional images, and demonstrated a potential in assisting in accurate diagnosis of cardiac diseases by combining complementary advantages of the two imaging modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>提出了一种基于点云的心脏单光子发射计算机断层扫描（SPECT）心肌灌注图像（MPI）和心脏计算机断层扫描血管造影（CTA）之间的配准和图像融合方法。首先，使用经过训练生成 LV 心外膜轮廓 (LVEC) 点云的不同 U-Net 神经网络对 SPECT 和 CTA 图像中的左心室 (LV) 心外膜区域 (LVER) 进行分割。其次，根据心脏解剖特点，在SPECT和CTA图像体积中手动标记前后室间沟（APIG）的特殊点。第三，我们开发了一个内部程序，用于粗略配准 APIG 的特殊点，以确保 SPECT 和 CTA 图像之间正确的心脏方向对齐。第四，我们采用ICP、SICP或CPD算法来实现SPECT和CTA图像中左室心外膜表面（LVER）的点云（连同APIG的特殊点）的精细配准。最后，经过精细配准后，实现了SPECT与CTA的图像融合。实验结果表明，心脏方向对准良好，最佳配准方法（仿射变换的CPD）的平均距离误差始终小于3 mm。该方法可以有效地融合心脏CTA和SPECT功能图像的结构，并展示了通过结合两种成像方式的优势互补来辅助心脏病的准确诊断的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.06841v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Gyroscope-Assisted Motion Deblurring Network**<br />
**Title_cn:** 陀螺仪辅助运动去模糊网络<br />
**Authors:** Simin Luan, Cong Yang, Zeyd Boukhers, Xue Qin, Dongfeng Cheng, Wei Sui, Zhijun Li<br />
**Abstract:** <details><summary>原文: </summary>Image research has shown substantial attention in deblurring networks in recent years. Yet, their practical usage in real-world deblurring, especially motion blur, remains limited due to the lack of pixel-aligned training triplets (background, blurred image, and blur heat map) and restricted information inherent in blurred images. This paper presents a simple yet efficient framework to synthetic and restore motion blur images using Inertial Measurement Unit (IMU) data. Notably, the framework includes a strategy for training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD) network for blurred image restoration. The rationale is that through harnessing IMU data, we can determine the transformation of the camera pose during the image exposure phase, facilitating the deduction of the motion trajectory (aka. blur trajectory) for each point inside the three-dimensional space. Thus, the synthetic triplets using our strategy are inherently close to natural motion blur, strictly pixel-aligned, and mass-producible. Through comprehensive experiments, we demonstrate the advantages of the proposed framework: only two-pixel errors between our synthetic and real-world blur trajectories, a marked improvement (around 33.17%) of the state-of-the-art deblurring method MIMO on Peak Signal-to-Noise Ratio (PSNR).</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，图像研究在去模糊网络方面引起了广泛关注。然而，由于缺乏像素对齐的训练三元组（背景、模糊图像和模糊热图）以及模糊图像固有的信息有限，它们在现实世界去模糊（尤其是运动模糊）中的实际应用仍然受到限制。本文提出了一个简单而有效的框架，使用惯性测量单元（IMU）数据合成和恢复运动模糊图像。值得注意的是，该框架包括训练三元组生成的策略，以及用于模糊图像恢复的陀螺仪辅助运动去模糊（GAMD）网络。其原理是，通过利用IMU数据，我们可以确定图像曝光阶段相机位姿的变换，从而有利于推导出三维空间内每个点的运动轨迹（又名模糊轨迹）。因此，使用我们的策略的合成三元组本质上接近自然运动模糊、严格像素对齐且可大规模生产。通过全面的实验，我们展示了所提出的框架的优点：我们的合成模糊轨迹与真实世界模糊轨迹之间只有两个像素的误差，比最先进的去模糊方法 MIMO on Peak 有了显着的改进（约 33.17％）信噪比 (PSNR)。</details>
**PDF:** <http://arxiv.org/pdf/2402.06854v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Reciprocal Visibility**<br />
**Title_cn:** 相互可见性<br />
**Authors:** Rakesh John Amala Arokia Nathan, Sigrid Strand, Dmitriy Shutin, Oliver Bimber<br />
**Abstract:** <details><summary>原文: </summary>We propose a guidance strategy to optimize real-time synthetic aperture sampling for occlusion removal with drones by pre-scanned point-cloud data. Depth information can be used to compute visibility of points on the ground for individual drone positions in the air. Inspired by Helmholtz reciprocity, we introduce reciprocal visibility to determine the dual situation - the visibility of potential sampling position in the air from given points of interest on the ground. The resulting visibility map encodes which point on the ground is visible by which magnitude from any position in the air. Based on such a map, we demonstrate a first greedy sampling optimization.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种指导策略，通过预扫描的点云数据来优化实时合成孔径采样，以消除无人机的遮挡。深度信息可用于计算空中各个无人机位置的地面点的可见度。受亥姆霍兹互易性的启发，我们引入了互易可见性来确定双重情况——从地面上给定的兴趣点到空气中潜在采样位置的可见性。生成的可见性图对地面上的哪个点从空中的任何位置可见的程度进行了编码。基于这样的地图，我们演示了第一个贪婪采样优化。</details>
**PDF:** <http://arxiv.org/pdf/2402.06991v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI**<br />
**Title_cn:** 利用多参数术前 MRI 进行治疗性胶质母细胞瘤生存推断<br />
**Authors:** Xiaofeng Liu, Nadya Shusharina, Helen A Shih, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo<br />
**Abstract:** <details><summary>原文: </summary>In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST. To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans. Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments. Furthermore, treatment can be effectively injected into each convolutional layer through the adaptive instance normalization we employ. We evaluate our framework on the BraTS20 ST prediction task. Three treatment options are considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no resection. The evaluation results demonstrate the effectiveness of injecting the treatment for estimating GBM survival.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们的目标是根据术前磁共振（MR）扫描来预测接受不同治疗的胶质母细胞瘤（GBM）患者的生存时间（ST）。通过比较不同治疗方法的ST，可以实现个性化、精准的治疗计划。众所周知，患者的当前状态（如 MR 扫描所示）和治疗选择都是 ST 的原因。虽然之前相关的基于 MR 的胶质母细胞瘤 ST 研究仅关注 MR 扫描与 ST 的直接映射，但并未包括治疗与 ST 之间的潜在因果关系。为了解决这一局限性，我们提出了一种胶质母细胞瘤 ST 的治疗条件回归模型，该模型除了 MR 扫描之外还包含治疗信息。我们的方法使我们能够以统一的方式有效地利用所有治疗的数据，而不必为每种治疗训练单独的模型。此外，通过我们采用的自适应实例归一化，可以有效地将处理注入到每个卷积层中。我们在 BraTS20 ST 预测任务上评估我们的框架。考虑三种治疗方案：大体全切除（GTR）、次全切除（STR）和不切除。评估结果证明了注射治疗对于估计 GBM 存活率的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.06982v1><br />
**Code:** null<br />

