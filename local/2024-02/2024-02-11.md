## [UPDATED!] **2024-02-11** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets**<br />
**Title_cn:** 通过用于新颖性识别和主动学习的语言嵌入实现可解释的安全自动驾驶：使用真实世界数据集的框架和实验分析<br />
**Authors:** Ross Greer, Mohan Trivedi<br />
**Abstract:** <details><summary>原文: </summary>This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results. Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task active learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究探索了自动驾驶数据集中用于主动学习的语言嵌入的集成，重点是新颖性检测。新颖性源于自动驾驶汽车难以导航的意外场景，需要更高水平的推理能力。我们提出的方法采用基于语言的表示来识别新场景，强调安全接管响应和主动学习的双重目的。该研究提出了一种聚类实验，使用对比语言图像预训练 (CLIP) 嵌入来组织数据集并检测新颖性。我们发现，所提出的算法有效地将新场景与源自两个现实世界驾驶数据集（一个车载数据集和一个基础设施安装数据集）的子集集合隔离开来。根据生成的聚类，我们进一步提出了生成元素文本解释的方法，这些元素将分类为新颖的场景与数据池中的其他场景区分开来，并从聚类结果中提供定性示例。我们的结果证明了语言驱动的嵌入在识别新元素和生成数据解释方面的有效性，并且我们进一步讨论了在安全接管、数据管理和多任务主动学习中的潜在应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.07320v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3D Gaussian as a New Vision Era: A Survey**<br />
**Title_cn:** 3D 高斯作为新视觉时代：一项调查<br />
**Authors:** Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯溅射 (3D-GS) 已成为计算机图形学领域的一项重大进步，它提供了明确的场景表示和新颖的视图合成，而无需依赖神经网络，例如神经辐射场 (NeRF)。这项技术在机器人、城市测绘、自主导航和虚拟现实/增强现实等领域有着广泛的应用，仅举几例。鉴于 3D 高斯分布的日益普及和研究的不断扩大，本文对过去一年的相关论文进行了全面的调查。我们根据特征和应用将调查组织为分类法，介绍 3D 高斯分布的理论基础。我们通过这项调查的目标是让新研究人员熟悉 3D 高斯散射，为该领域的开创性工作提供有价值的参考，并启发未来的研究方向，正如我们的结论部分所讨论的。</details>
**PDF:** <http://arxiv.org/pdf/2402.07181v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **An attempt to generate new bridge types from latent space of denoising diffusion Implicit model**<br />
**Title_cn:** 尝试从去噪扩散的潜在空间生成新的桥类型隐式模型<br />
**Authors:** Hongjun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用去噪扩散隐式模型进行桥梁式创新。给图像添加噪声和去噪的过程可以比喻为尸体腐烂的过程，侦探还原受害者被杀的场景，以帮助初学者理解。通过通俗易懂的代数方法，推导出加噪声、去噪的函数公式，让初学者更容易掌握模型的数学原理。利用三跨梁桥、拱桥、斜拉桥、悬索桥的对称结构化图像数据集，基于Python编程语言、TensorFlow和Keras深度学习平台框架，构建并训练去噪扩散隐式模型。从潜在空间采样中，可以生成具有不对称结构的新桥梁类型。去噪扩散隐式模型可以在人类原有桥梁类型的基础上有机地组合不同的结构构件，创造出新的桥梁类型。</details>
**PDF:** <http://arxiv.org/pdf/2402.07129v1><br />
**Code:** <https://github.com/QQ583304953/Bridge-DDIM>**<br />
>>**index:** 4<br />
**Title:** **Self-Correcting Self-Consuming Loops for Generative Model Training**<br />
**Title_cn:** 用于生成模型训练的自校正自消耗循环<br />
**Authors:** Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun<br />
**Abstract:** <details><summary>原文: </summary>As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着合成数据的质量越来越高并且在互联网上激增，机器学习模型越来越多地基于人类和机器生成的数据的混合进行训练。尽管使用合成数据进行表示学习有成功的故事，但使用合成数据进行生成模型训练会产生“自消耗循环”，除非满足某些条件，否则可能会导致训练不稳定甚至崩溃。我们的论文旨在稳定自消耗生成模型训练。我们的理论结果表明，通过引入理想化的校正函数，将数据点映射到更可能处于真实数据分布下的位置，可以使自消耗循环呈指数级地更加稳定。然后，我们提出了自我校正函数，它依赖于专家知识（例如在模拟器中编程的物理定律），旨在自动大规模地逼近理想化的校正器。我们凭经验验证了自校正自消耗循环在具有挑战性的人体运动合成任务上的有效性，并观察到它成功地避免了模型崩溃，即使合成数据与真实数据的比率高达 100%。</details>
**PDF:** <http://arxiv.org/pdf/2402.07087v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy**<br />
**Title_cn:** 通过利用分类数据集及其语义层次结构对视觉语言模型进行开放式 VQA 基准测试<br />
**Authors:** Simon Ging, María A. Bravo, Thomas Brox<br />
**Abstract:** <details><summary>原文: </summary>The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本生成视觉语言模型的评估是一项具有挑战性但又至关重要的工作。通过解决现有视觉问答（VQA）基准的局限性并提出创新的评估方法，我们的研究旨在增进我们对这些模型功能的理解。我们提出了一种基于众所周知的视觉分类数据集的新颖的 VQA 基准，它允许对文本生成视觉语言模型进行精细评估，并将其与判别性视觉语言模型进行比较。为了改进对细粒度分类任务的粗略答案的评估，我们建议使用标签空间的语义层次结构来询问自动生成的有关真实类别的后续问题。最后，我们比较了传统 NLP 和基于 LLM 的指标，以评估给定真实答案的模型预测问题。我们进行了一项人工评估研究，并根据最终指标做出决定。我们将基准测试应用于一套视觉语言模型，并详细比较它们在对象、动作和属性分类方面的能力。我们的贡献旨在为更精确和更有意义的评估奠定基础，促进在令人兴奋的视觉语言建模领域取得有针对性的进展。</details>
**PDF:** <http://arxiv.org/pdf/2402.07270v1><br />
**Code:** <https://github.com/lmb-freiburg/ovqa>**<br />
>>**index:** 2<br />
**Title:** **KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos**<br />
**Title_cn:** KVQ：短视频的万花筒视频质量评估<br />
**Authors:** Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen<br />
**Abstract:** <details><summary>原文: </summary>Short-form UGC video platforms, like Kwai and TikTok, have been an emerging and irreplaceable mainstream media form, thriving on user-friendly engagement, and kaleidoscope creation, etc. However, the advancing content-generation modes, e.g., special effects, and sophisticated processing workflows, e.g., de-artifacts, have introduced significant challenges to recent UGC video quality assessment: (i) the ambiguous contents hinder the identification of quality-determined regions. (ii) the diverse and complicated hybrid distortions are hard to distinguish. To tackle the above challenges and assist in the development of short-form videos, we establish the first large-scale Kaleidoscope short Video database for Quality assessment, termed KVQ, which comprises 600 user-uploaded short videos and 3600 processed videos through the diverse practical processing workflows, including pre-processing, transcoding, and enhancement. Among them, the absolute quality score of each video and partial ranking score among indistinguishable samples are provided by a team of professional researchers specializing in image processing. Based on this database, we propose the first short-form video quality evaluator, i.e., KSVQE, which enables the quality evaluator to identify the quality-determined semantics with the content understanding of large vision language models (i.e., CLIP) and distinguish the distortions with the distortion understanding module. Experimental results have shown the effectiveness of KSVQE on our KVQ database and popular VQA databases.</details>
**Abstract_cn:** <details><summary>译文: </summary>快手、抖音等短视频UGC平台已经成为一种新兴且不可替代的主流媒体形式，凭借用户友好性、万花筒创作等优势而蓬勃发展。复杂的处理工作流程（例如去伪影）给最近的 UGC 视频质量评估带来了重大挑战：（i）模糊的内容阻碍了质量确定区域的识别。 (ii) 混合扭曲多样且复杂，难以区分。为了应对上述挑战并协助短视频的发展，我们建立了第一个用于质量评估的大型万花筒短视频数据库，称为KVQ，该数据库包含600个用户上传的短视频和3600个经过各种实践处理的处理后的视频。处理工作流程，包括预处理、转码和增强。其中，每个视频的绝对质量得分以及不可区分样本中的部分排名得分由专门从事图像处理的专业研究人员团队提供。基于该数据库，我们提出了第一个短格式视频质量评估器，即 KSVQE，它使质量评估器能够通过大型视觉语言模型（即 CLIP）的内容理解来识别质量确定的语义，并区分失真与失真理解模块。实验结果证明了 KSVQE 在我们的 KVQ 数据库和流行的 VQA 数据库上的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.07220v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs**<br />
**Title_cn:** 低水平视觉多模态基础模型的基准：从单图像到成对图像<br />
**Authors:** Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin<br />
**Abstract:** <details><summary>原文: </summary>The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in low-level visual perception and understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception (A1) via visual question answering related to low-level attributes (e.g. clarity, lighting); and the low-level visual description (A2), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at https://github.com/Q-Future/Q-Bench.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型（MLLM）的快速发展引领了计算机视觉的范式转变，朝着多功能基础模型的方向发展。然而，在低级视觉感知和理解方面评估 MLLM 仍然是一个尚未探索的领域。为此，我们设计了基准设置来模拟与低级视觉相关的人类语言反应：通过与低级属性（例如清晰度、照明）相关的视觉问题回答来获得低级视觉感知（A1）；以及低级视觉描述 (A2)，用于评估 MLLM 的低级文本描述。此外，鉴于成对比较可以更好地避免响应的模糊性，并且已被许多人体实验所采用，我们进一步将 MLLM 的低级感知相关问答和描述评估从单个图像扩展到图像对。具体来说，对于感知 (A1)，我们执行 LLVisionQA+ 数据集，其中包含 2,990 个单图像和 1,999 个图像对，每个图像都附有一个关于其低级特征的开放式问题；对于描述 (A2)，我们提出了 LLDescribe+ 数据集，评估 MLLM 对 499 个单图像和 450 对图像的低级描述。此外，我们通过采用基于 softmax 的方法来评估 MLLM 的评估 (A3) 能力，即预测分数，使所有 MLLM 能够生成可量化的质量评级，并在 7 个图像质量评估 (IQA) 数据集中针对人类意见进行测试。通过评估 24 个 MLLM，我们证明了几个 MLLM 在单个图像上具有不错的低级视觉能力，但只有 GPT-4V 在成对比较方面比单个图像评估（如人类）表现出更高的准确性。我们希望我们的基准能够激发进一步的研究，以发现和增强 MLLM 的这些新生功能。数据集将在 https://github.com/Q-Future/Q-Bench 提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.07116v1><br />
**Code:** <https://github.com/Q-Future/Q-Bench>**<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis**<br />
**Title_cn:** BioNeRF：用于视图合成的生物学上合理的神经辐射场<br />
**Authors:** Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa<br />
**Abstract:** <details><summary>原文: </summary>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 BioNeRF，这是一种生物学上合理的架构，可以对 3D 表示的场景进行建模，并通过辐射场合成新的视图。由于 NeRF 依赖网络权重来存储场景的 3 维表示，因此 BioNeRF 实现了一种认知启发机制，将多个来源的输入融合到类似记忆的结构中，提高存储容量并提取更多内在和相关信息。 BioNeRF 还模仿了在锥体细胞中观察到的有关上下文信息的行为，其中内存作为上下文提供，并与两个后续神经模型的输入相结合，一个负责生成体积密度，另一个负责生成用于渲染场景的颜色。实验结果表明，BioNeRF 优于在两个数据集中编码人类感知的质量测量的最新结果：真实世界图像和合成数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.07310v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting**<br />
**Title_cn:** GALA3D：通过布局引导的生成高斯泼溅实现文本到 3D 复杂场景生成<br />
**Authors:** Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang<br />
**Abstract:** <details><summary>原文: </summary>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at https://gala3d.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 GALA3D，即具有 LAyout 引导控制的生成 3D GAussian，用于有效的组合文本到 3D 生成。我们首先利用大型语言模型 (LLM) 生成初始布局，并引入布局引导的 3D 高斯表示，用于具有自适应几何约束的 3D 内容生成。然后，我们提出了一种具有条件扩散的对象场景组合优化机制，以协作生成具有一致的几何形状、纹理、比例和多个对象之间准确交互的真实 3D 场景，同时调整从 LLM 中提取的粗略布局先验，以与生成的场景保持一致。实验表明，GALA3D是一个用户友好的端到端框架，用于最先进的场景级3D内容生成和可控编辑，同时确保场景内对象级实体的高保真度。源代码和模型将在 https://gala3d.github.io/ 提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.07207v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks**<br />
**Title_cn:** 结构重参数化网络低位量化的异常值感知训练<br />
**Authors:** Muqun Niu, Yuan Ren, Boyu Li, Chenchen Ding<br />
**Abstract:** <details><summary>原文: </summary>Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVGG is largely enhanced, particularly when the bitwidth falls below 8.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络 (CNN) 的轻量级设计需要在模型架构和压缩技术方面进行协同设计。作为一种分离训练和推理的新颖设计范式，结构重参数化（SR）网络（例如代表性的 RepVGG）使简单的类 VGG 网络焕发了活力，其高精度可与高级且通常更复杂的网络相媲美。然而，SR网络中的合并过程将异常值引入到权重中，使其分布与传统网络不同，从而增加了量化的难度。为了解决这个问题，我们提出了一种操作员级别的训练改进，称为异常值感知批量归一化（OABN）。此外，为了满足有限位宽的需求，同时保持推理精度，我们开发了一种基于聚类的非均匀量化框架，用于量化感知训练（QAT），名为 ClusterQAT。将 OABN 与 ClusterQAT 集成，RepVGG 的量化性能大大增强，特别是当位宽低于 8 时。</details>
**PDF:** <http://arxiv.org/pdf/2402.07200v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation**<br />
**Title_cn:** 通过观看学习：基于视频的机器人操作学习方法综述<br />
**Authors:** Chrisantus Eze, Christopher Crick<br />
**Abstract:** <details><summary>原文: </summary>Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation. The survey summarizes video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and benchmarks, and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人操作技能的学习因缺乏多样化、公正的数据集而受到阻碍。虽然整理的数据集可以提供帮助，但在普遍性和现实世界的转移方面仍然存在挑战。与此同时，大规模的“野外”视频数据集通过自我监督技术推动了计算机视觉的进步。将其转化为机器人技术，最近的作品探索了通过被动观看大量在线视频来学习操作技能。这种基于视频的学习范式显示出有希望的结果，提供了可扩展的监督，同时减少了数据集偏差。这项调查回顾了视频特征表示学习技术、对象可供性理解、3D 手/身体建模和大规模机器人资源等基础知识，以及从不受控制的视频演示中获取机器人操作技能的新兴技术。我们讨论仅通过观察大规模人类视频进行学习如何增强机器人操作的泛化能力和样本效率。该调查总结了基于视频的学习方法，分析了它们相对于标准数据集、调查指标和基准的优势，并讨论了计算机视觉、自然语言处理和机器人学习交叉领域这一新兴领域的开放挑战和未来方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.07127v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation**<br />
**Title_cn:** 医学图像分割的两阶段多任务自监督学习<br />
**Authors:** Binyan Hu, A. K. Qin<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods. Self-supervised learning offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the knowledge acquired from solving auxiliary tasks to help better solve the target segmentation task. Different auxiliary tasks may have different properties and thus can help the target task to different extents. It is desired to leverage their complementary advantages to enhance the overall assistance to the target task. To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients. However, direct coupling of losses or intermediate gradients risks undesirable interference because the knowledge acquired from solving each auxiliary task at every training step may not always benefit the target task. To address this issue, we propose a two-stage training approach. In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance. In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble knowledge distillation method. Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model. We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate the superiority of our approach over several existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习 (DL) 技术使医学图像分割取得了显着进步，尽管医学应用中固有的数据稀缺对基于 DL 的分割方法提出了巨大的挑战。自监督学习提供了一种解决方案，它通过从可用数据集中创建辅助学习任务，然后利用从解决辅助任务中获得的知识来帮助更好地解决目标分割任务。不同的辅助任务可能具有不同的属性，从而可以在不同程度上帮助目标任务。希望发挥优势互补，增强对目标任务的整体助力。为了实现这一目标，现有方法通常采用联合训练范例，通过整合分割和辅助任务的损失或中间梯度来共同解决它们。然而，损失或中间梯度的直接耦合会带来不必要的干扰，因为在每个训练步骤中解决每个辅助任务所获得的知识可能并不总是有利于目标任务。为了解决这个问题，我们提出了一种两阶段训练方法。在第一阶段，目标分割任务将与每个辅助任务在联合训练和预训练模式下独立共同解决，并通过验证性能选择更好的模型。在第二阶段，使用集成知识蒸馏方法将针对每个辅助任务获得的模型转换为单个模型。我们的方法允许充分利用每个辅助任务来创建多个精英分割模型，然后将它们组合成一个更强大的模型。我们在我们的方法中采用了五个不同性质的辅助任务，并将其应用于 X 射线气胸分割数据集上训练 U-Net 模型。实验结果证明了我们的方法相对于几种现有方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.07119v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Deep Learning for Medical Image Segmentation with Imprecise Annotation**<br />
**Title_cn:** 具有不精确注释的深度学习医学图像分割<br />
**Authors:** Binyan Hu, A. K. Qin<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分割 (MIS) 在医学图像分析中发挥着重要作用，人们在医学图像分析过程的自动化方面投入了大量精力。目前，主流 MIS 方法基于深度神经网络 (DNN)，该网络通常在包含医生生成的注释掩模的数据集上进行训练。然而，在医学领域，不同医生生成的注释掩码本质上是不同的，因为医生可能不必要地生成精确且独特的注释来满足诊断目标。因此，根据某些医生（通常只是单个医生）注释的数据训练的 DNN 模型可能会不合需要地偏向那些注释训练数据的医生，导致将使用训练模型的新医生不满意。为了解决这个问题，这项工作研究了利用多专家注释来增强模型对新医生的适应性，并对 MRI 大脑分割任务进行了初步研究。实验结果表明，在对新医生的少量注释进行轻量级微调后，在具有多专家注释的数据集上训练的模型可以有效地满足新医生的需求。</details>
**PDF:** <http://arxiv.org/pdf/2402.07330v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **The Bias of Harmful Label Associations in Vision-Language Models**<br />
**Title_cn:** 视觉语言模型中有害标签关联的偏差<br />
**Authors:** Caner Hazirbas, Alicia Sun, Yonathan Efroni, Mark Ibrahim<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable performance of foundation vision-language models, the shared representation space for text and vision can also encode harmful label associations detrimental to fairness. While prior work has uncovered bias in vision-language models' (VLMs) classification performance across geography, work has been limited along the important axis of harmful label associations due to a lack of rich, labeled data. In this work, we investigate harmful label associations in the recently released Casual Conversations datasets containing more than 70,000 videos. We study bias in the frequency of harmful label associations across self-provided labels for age, gender, apparent skin tone, and physical adornments across several leading VLMs. We find that VLMs are $4-13$x more likely to harmfully classify individuals with darker skin tones. We also find scaling transformer encoder model size leads to higher confidence in harmful predictions. Finally, we find improvements on standard vision tasks across VLMs does not address disparities in harmful label associations.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管基础视觉语言模型具有出色的性能，但文本和视觉的共享表示空间也可能编码不利于公平的有害标签关联。虽然之前的工作已经发现了跨地域视觉语言模型 (VLM) 分类性能的偏差，但由于缺乏丰富的标记数据，沿着有害标签关联的重要轴的工作受到限制。在这项工作中，我们调查了最近发布的包含 70,000 多个视频的休闲对话数据集中的有害标签关联。我们研究了几个领先的 VLM 中自行提供的年龄、性别、明显肤色和身体装饰标签中有害标签关联频率的偏差。我们发现 VLM 对深色肤色的个体进行有害分类的可能性高出 4-13 美元。我们还发现，缩放 Transformer 编码器模型大小可以提高有害预测的置信度。最后，我们发现跨 VLM 的标准视觉任务的改进并不能解决有害标签关联的差异。</details>
**PDF:** <http://arxiv.org/pdf/2402.07329v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Trade-off Between Spatial and Angular Resolution in Facial Recognition**<br />
**Title_cn:** 面部识别中空间分辨率和角度分辨率之间的权衡<br />
**Authors:** Muhammad Zeshan Alam, Sousso kelowani, Mohamed Elsaeidy<br />
**Abstract:** <details><summary>原文: </summary>Ensuring robustness in face recognition systems across various challenging conditions is crucial for their versatility. State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance. However, light field-based face recognition approaches that leverage angular information face computational limitations. This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved face recognition performance. By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints. Our experimental results demonstrate a notable performance improvement in face recognition systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution.</details>
**Abstract_cn:** <details><summary>译文: </summary>确保人脸识别系统在各种挑战性条件下的鲁棒性对于其多功能性至关重要。最先进的方法通常会结合额外的信息，例如深度、热量或角度数据，以提高性能。然而，利用角度信息的基于光场的人脸识别方法面临计算限制。本文研究了光场表示中的空间角分辨率之间的基本权衡，以实现改进的人脸识别性能。通过利用具有不同角度分辨率的宏像素，同时保持整体图像尺寸，我们的目标是在考虑计算限制的同时，以牺牲空间分辨率为代价来量化角度信息的影响。我们的实验结果表明，通过以空间分辨率为代价将角度分辨率提高到一定程度，人脸识别系统的性能得到显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.07263v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Data Quality Aware Approaches for Addressing Model Drift of Semantic Segmentation Models**<br />
**Title_cn:** 用于解决语义分割模型模型漂移的数据质量感知方法<br />
**Authors:** Samiha Mirza, Vuong D. Nguyen, Pranav Mantini, Shishir K. Shah<br />
**Abstract:** <details><summary>原文: </summary>In the midst of the rapid integration of artificial intelligence (AI) into real world applications, one pressing challenge we confront is the phenomenon of model drift, wherein the performance of AI models gradually degrades over time, compromising their effectiveness in real-world, dynamic environments. Once identified, we need techniques for handling this drift to preserve the model performance and prevent further degradation. This study investigates two prominent quality aware strategies to combat model drift: data quality assessment and data conditioning based on prior model knowledge. The former leverages image quality assessment metrics to meticulously select high-quality training data, improving the model robustness, while the latter makes use of learned feature vectors from existing models to guide the selection of future data, aligning it with the model's prior knowledge. Through comprehensive experimentation, this research aims to shed light on the efficacy of these approaches in enhancing the performance and reliability of semantic segmentation models, thereby contributing to the advancement of computer vision capabilities in real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>在人工智能（AI）快速融入现实世界应用的过程中，我们面临的一个紧迫挑战是模型漂移现象，其中人工智能模型的性能随着时间的推移逐渐下降，损害了其在现实世界、动态环境中的有效性。环境。一旦确定，我们需要处理这种漂移的技术，以保持模型性能并防止进一步退化。本研究研究了两种著名的质量意识策略来对抗模型漂移：数据质量评估和基于先验模型知识的数据调节。前者利用图像质量评估指标精心选择高质量的训练数据，提高模型的鲁棒性，而后者利用从现有模型中学习到的特征向量来指导未来数据的选择，使其与模型的先验知识保持一致。通过全面的实验，本研究旨在揭示这些方法在增强语义分割模型的性能和可靠性方面的功效，从而促进现实场景中计算机视觉能力的进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.07258v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation**<br />
**Title_cn:** Semi-Mamba-UNet：用于半监督医学图像分割的像素级对比交叉监督视觉 Mamba UNet<br />
**Authors:** Ziyang Wang, Chao Ma<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation is essential in diagnostics, treatment planning, and healthcare, with deep learning offering promising advancements. Notably, Convolutional Neural Network (CNN) excel in capturing local image features, whereas Vision Transformer (ViT) adeptly model long-range dependencies through multi-head self-attention mechanisms. Despite their strengths, both CNN and ViT face challenges in efficiently processing long-range dependencies within medical images, often requiring substantial computational resources. This issue, combined with the high cost and limited availability of expert annotations, poses significant obstacles to achieving precise segmentation. To address these challenges, this paper introduces the Semi-Mamba-UNet, which integrates a visual mamba-based UNet architecture with a conventional UNet into a semi-supervised learning (SSL) framework. This innovative SSL approach leverages dual networks to jointly generate pseudo labels and cross supervise each other, drawing inspiration from consistency regularization techniques. Furthermore, we introduce a self-supervised pixel-level contrastive learning strategy, employing a projector pair to further enhance feature learning capabilities. Our comprehensive evaluation on a publicly available MRI cardiac segmentation dataset, comparing against various SSL frameworks with different UNet-based segmentation networks, highlights the superior performance of Semi-Mamba-UNet. The source code has been made publicly accessible.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分割在诊断、治疗计划和医疗保健中至关重要，深度学习提供了有希望的进步。值得注意的是，卷积神经网络（CNN）擅长捕获局部图像特征，而视觉变换器（ViT）则通过多头自注意力机制熟练地对远程依赖关系进行建模。尽管 CNN 和 ViT 各有优势，但它们在有效处理医学图像中的远程依赖性方面都面临着挑战，通常需要大量的计算资源。这个问题，加上专家注释的高成本和有限的可用性，对实现精确分割构成了重大障碍。为了应对这些挑战，本文引入了 Semi-Mamba-UNet，它将基于视觉曼巴的 UNet 架构与传统的 UNet 集成到半监督学习（SSL）框架中。这种创新的 SSL 方法利用双网络共同生成伪标签并相互交叉监督，从一致性正则化技术中汲取灵感。此外，我们引入了自监督像素级对比学习策略，采用投影仪对进一步增强特征学习能力。我们对公开的 MRI 心脏分割数据集进行了全面评估，并与具有不同基于 UNet 的分割网络的各种 SSL 框架进行了比较，突显了 Semi-Mamba-UNet 的卓越性能。源代码已公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.07245v1><br />
**Code:** <https://github.com/ziyangwang007/mamba-unet>**<br />
>>**index:** 6<br />
**Title:** **A novel spatial-frequency domain network for zero-shot incremental learning**<br />
**Title_cn:** 一种用于零样本增量学习的新型空间频域网络<br />
**Authors:** Jie Ren, Yang Zhao, Weichuan Zhang, Changming Sun<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot incremental learning aims to enable the model to generalize to new classes without forgetting previously learned classes. However, the semantic gap between old and new sample classes can lead to catastrophic forgetting. Additionally, existing algorithms lack capturing significant information from each sample image domain, impairing models' classification performance. Therefore, this paper proposes a novel Spatial-Frequency Domain Network (SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module and Attention Feature Alignment (AFA) module to improve the Zero-Shot Translation for Class Incremental algorithm. Firstly, SFFE module is designed which contains a dual attention mechanism for obtaining salient spatial-frequency feature information. Secondly, a novel feature fusion module is conducted for obtaining fused spatial-frequency domain features. Thirdly, the Nearest Class Mean classifier is utilized to select the most suitable category. Finally, iteration between tasks is performed using the Zero-Shot Translation model. The proposed SFDNet has the ability to effectively extract spatial-frequency feature representation from input images, improve the accuracy of image classification, and fundamentally alleviate catastrophic forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets demonstrate that our proposed algorithm outperforms state-of-the-art incremental learning algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>零样本增量学习旨在使模型能够泛化到新的类别，而不会忘记以前学过的类别。然而，新旧样本类之间的语义差距可能会导致灾难性的遗忘。此外，现有算法缺乏从每个样本图像域捕获重要信息，从而损害了模型的分类性能。因此，本文提出了一种新颖的空频域网络（SFDNet），其中包含空频特征提取（SFFE）模块和注意特征对齐（AFA）模块，以改进类增量算法的零样本翻译。首先，设计了SFFE模块，其中包含用于获取显着空间频率特征信息的双重注意机制。其次，采用新颖的特征融合模块来获得融合的空间频域特征。第三，利用最近类均值分类器来选择最合适的类别。最后，使用零样本翻译模型执行任务之间的迭代。所提出的SFDNet能够有效地从输入图像中提取空间频率特征表示，提高图像分类的准确性，并从根本上缓解灾难性遗忘。对 CUB 200-2011 和 CIFAR100 数据集的大量实验表明，我们提出的算法优于最先进的增量学习算法。</details>
**PDF:** <http://arxiv.org/pdf/2402.07216v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations**<br />
**Title_cn:** 用于外科手术期间脑癌检测的高光谱图像的空间光谱分类<br />
**Authors:** H. Fabelo, S. Ortega, D. Ravi, B. R. Kiran, C. Sosa, D. Bulters, G. M. Callico, H. Bulstrode, A. Szolna, J. F. Pineiro, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a noncontact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both supervised and unsupervised machine learning methods. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑癌手术是神经外科的一个主要问题。这些肿瘤对周围正常大脑的弥漫性浸润使得肉眼难以准确识别。由于手术是脑癌的常见治疗方法，因此准确的肿瘤根治性切除可以提高患者的生存率。然而，手术过程中肿瘤边界的识别具有挑战性。高光谱成像是一种非接触、非电离、非侵入性技术，适用于医学诊断。本研究提出了一种考虑高光谱图像的空间和光谱特征的新型分类方法，以帮助神经外科医生在切除过程中准确确定手术时的肿瘤边界，避免过度切除正常组织或无意中留下残留肿瘤。本研究提出的旨在实现有效解决方案的算法由一个混合框架组成，该框架结合了监督和无监督机器学习方法。为了评估所提出的方法，使用了来自五位不同患者的体内受胶质母细胞瘤肿瘤影响的大脑表面的五幅高光谱图像。最终获得的分类图已经过专家的分析和验证。这些初步结果很有希望，能够准确描绘肿瘤区域。</details>
**PDF:** <http://arxiv.org/pdf/2402.07192v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression**<br />
**Title_cn:** PIVOT-Net：基于异构点体素树的点云压缩框架<br />
**Authors:** Jiahao Pang, Kevin Bui, Dong Tian<br />
**Abstract:** <details><summary>原文: </summary>The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations -- point-based, voxel-based, and tree-based representations -- and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel transformer for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云格式的通用性使得许多 3D 应用成为可能，这使得点云的压缩成为实践中的关键阶段。点云以离散 3D 点的形式采样，以有限位深度近似嵌入 3D 中的 2D 表面。然而，实际点云的点分布随着位深度的增加而发生巨大变化，需要不同的方法来进行有效的消耗/分析。对此，提出了异构点云压缩（PCC）框架。我们将典型的点云表示（基于点、基于体素和基于树的表示）及其相关主干统一在基于学习的框架下，以在不同位深度级别压缩输入点云。认识到体素域处理的重要性后，我们通过提出的用于解码的上下文感知上采样和用于特征聚合的增强体素变换器来增强框架。广泛的实验证明了我们的提案在各种点云上的最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.07243v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring**<br />
**Title_cn:** GeoFormer：基于视觉和序列变压器的温室气体监测方法<br />
**Authors:** Madhav Khirwar, Ankur Narang<br />
**Abstract:** <details><summary>原文: </summary>Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision transformer module with a highly efficient time-series transformer module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the efficacy of combining vision and time-series transformer architectures to harness satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.</details>
**Abstract_cn:** <details><summary>译文: </summary>空气污染是全球面临的一项关键环境挑战，通过温室气体排放在气候变化中发挥着重要作用，并对数十亿人的健康产生负面影响。然而，预测污染物的空间和时间模式仍然具有挑战性。地面监测设施的稀缺以及空气污染建模对综合数据集的依赖（许多地区通常无法访问）使这一问题变得更加复杂。在这项工作中，我们介绍了 GeoFormer，这是一个紧凑的模型，它将视觉变换器模块与高效的时间序列变换器模块相结合，用于根据 Sentinel-5P 卫星图像预测地表二氧化氮 (NO2) 浓度。我们使用地面监测站 Sentinel-5P 图像及其相应的 NO2 浓度读数构建的数据集来训练所提出的模型，以预测地表 NO2 测量结果。所提出的模型达到了高精度（MAE 5.65），展示了将视觉和时间序列变压器架构相结合以利用卫星数据来增强温室气体排放洞察的有效性，证明有助于推进全球气候变化监测和排放监管工作。</details>
**PDF:** <http://arxiv.org/pdf/2402.07164v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions**<br />
**Title_cn:** LISR：使用紧支持的径向基函数学习线性 3D 隐式曲面表示<br />
**Authors:** Atharva Pandey, Vishal Yadav, Rajendra Nagar, Santanu Chaudhury<br />
**Abstract:** <details><summary>原文: </summary>Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem. In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces. Radial basis functions provide memory-efficient parameterization of the implicit surface. However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution. In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface. This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature. We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object. We learn linear implicit shapes within a supervised learning framework using ground truth Signed-Distance Field (SDF) data for guidance. The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection. The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the benchmark dataset. We also show the effectiveness of the proposed approach by using it for the 3D shape completion task.</details>
**Abstract_cn:** <details><summary>译文: </summary>从对象的部分且嘈杂的 3D 点云扫描中隐式 3D 表面重建是经典的几何处理和 3D 计算机视觉问题。在文献中，已经开发了各种 3D 形状表示，其存储效率和形状检索有效性各不相同，例如体积、参数和隐式曲面。径向基函数提供隐式曲面的内存高效参数化。然而，我们表明，使用真实隐式曲面和基于线性基的隐式曲面之间的均方误差来训练神经网络不会收敛到全局解。在这项工作中，我们提出了局部支持的紧凑径向基函数，用于隐式表面的线性表示。由于其连续性，这种表示方式使我们能够在任何分辨率下生成具有任意拓扑的 3D 形状。然后，我们提出了一种神经网络架构，用于学习对象 3D 表面的线性隐式形状表示。我们使用地面真实符号距离场（SDF）数据作为指导，在监督学习框架内学习线性隐式形状。由于基础和查询点选择中的数值问题（需要求解大矩阵的逆），经典策略在从给定 3D 点云中查找线性隐式形状时面临困难。与基准数据集上最先进的方法相比，所提出的方法实现了更好的倒角距离和可比较的 F 分数。我们还通过将所提出的方法用于 3D 形状完成任务来展示其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.07301v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **INSITE: labelling medical images using submodular functions and semi-supervised data programming**<br />
**Title_cn:** INSITE：使用子模块函数和半监督数据编程来标记医学图像<br />
**Authors:** Akshat Gautam, Anurag Shandilya, Akshit Srivastava, Venkatapathy Subramanian, Ganesh Ramakrishnan, Kshitij Jadhav<br />
**Abstract:** <details><summary>原文: </summary>The necessity of large amounts of labeled data to train deep models, especially in medical imaging creates an implementation bottleneck in resource-constrained settings. In Insite (labelINg medical imageS usIng submodular funcTions and sEmi-supervised data programming) we apply informed subset selection to identify a small number of most representative or diverse images from a huge pool of unlabelled data subsequently annotated by a domain expert. The newly annotated images are then used as exemplars to develop several data programming-driven labeling functions. These labelling functions output a predicted-label and a similarity score when given an unlabelled image as an input. A consensus is brought amongst the outputs of these labeling functions by using a label aggregator function to assign the final predicted label to each unlabelled data point. We demonstrate that informed subset selection followed by semi-supervised data programming methods using these images as exemplars perform better than other state-of-the-art semi-supervised methods. Further, for the first time we demonstrate that this can be achieved through a small set of images used as exemplars.</details>
**Abstract_cn:** <details><summary>译文: </summary>需要大量标记数据来训练深度模型，尤其是在医学成像领域，这在资源有限的环境中造成了实施瓶颈。在 Insite（使用子模函数和半监督数据编程来标记医学图像）中，我们应用知情子集选择来从大量未标记数据中识别少量最具代表性或多样化的图像，随后由领域专家进行注释。然后将新注释的图像用作示例来开发几种数据编程驱动的标记功能。当给定未标记图像作为输入时，这些标记函数输出预测标签和相似性得分。通过使用标签聚合器函数将最终预测标签分配给每个未标记的数据点，在这些标记函数的输出之间达成共识。我们证明，使用这些图像作为范例的知情子集选择和半监督数据编程方法比其他最先进的半监督方法表现得更好。此外，我们第一次证明这可以通过用作样本的一小组图像来实现。</details>
**PDF:** <http://arxiv.org/pdf/2402.07173v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Supervised Reconstruction for Silhouette Tomography**<br />
**Title_cn:** 轮廓断层扫描的监督重建<br />
**Authors:** Evan Bell, Michael T. McCann, Marc Klasky<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce silhouette tomography, a novel formulation of X-ray computed tomography that relies only on the geometry of the imaging system. We formulate silhouette tomography mathematically and provide a simple method for obtaining a particular solution to the problem, assuming that any solution exists. We then propose a supervised reconstruction approach that uses a deep neural network to solve the silhouette tomography problem. We present experimental results on a synthetic dataset that demonstrate the effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了轮廓断层扫描，这是一种仅依赖于成像系统几何形状的 X 射线计算机断层扫描的新颖形式。我们以数学方式制定轮廓断层扫描，并提供一种简单的方法来获得问题的特定解决方案（假设存在任何解决方案）。然后，我们提出了一种使用深度神经网络的监督重建方法来解决轮廓断层扫描问题。我们在合成数据集上提供了实验结果，证明了所提出方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.07298v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **American Sign Language Video to Text Translation**<br />
**Title_cn:** 美国手语视频到文本翻译<br />
**Authors:** Parsheeta Roy, Ji-Eun Han, Srishti Chouhan, Bhaavanaa Thumu<br />
**Abstract:** <details><summary>原文: </summary>Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties. We replicate and try to improve on a recently published study. We evaluate models using BLEU and rBLEU metrics to ensure translation quality. During our ablation study, we found that the model's performance is significantly influenced by optimizers, activation functions, and label smoothing. Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes. Our source code is available to facilitate replication of our results and encourage future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>手语到文本是一项关键技术，可以打破听力障碍人士的沟通障碍。我们复制并尝试改进最近发表的一项研究。我们使用 BLEU 和 rBLEU 指标评估模型，以确保翻译质量。在我们的消融研究中，我们发现模型的性能受到优化器、激活函数和标签平滑的显着影响。进一步的研究旨在完善视觉特征捕获、提高解码器利用率并集成预先训练的解码器以获得更好的翻译结果。我们的源代码可用于促进我们结果的复制并鼓励未来的研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.07255v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Highlight Removal Method for Capsule Endoscopy Images**<br />
**Title_cn:** 胶囊内窥镜图像的高光去除方法<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Peixuan Liu, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>The images captured by Wireless Capsule Endoscopy (WCE) always exhibit specular reflections, and removing highlights while preserving the color and texture in the region remains a challenge. To address this issue, this paper proposes a highlight removal method for capsule endoscopy images. Firstly, the confidence and feature terms of the highlight region's edges are computed, where confidence is obtained by the ratio of known pixels in the RGB space's R channel to the B channel within a window centered on the highlight region's edge pixel, and feature terms are acquired by multiplying the gradient vector of the highlight region's edge pixel with the iso-intensity line. Subsequently, the confidence and feature terms are assigned different weights and summed to obtain the priority of all highlight region's edge pixels, and the pixel with the highest priority is identified. Then, the variance of the highlight region's edge pixels is used to adjust the size of the sample block window, and the best-matching block is searched in the known region based on the RGB color similarity and distance between the sample block and the window centered on the pixel with the highest priority. Finally, the pixels in the best-matching block are copied to the highest priority highlight removal region to achieve the goal of removing the highlight region. Experimental results demonstrate that the proposed method effectively removes highlights from WCE images, with a lower coefficient of variation in the highlight removal region compared to the Crinimisi algorithm and DeepGin method. Additionally, the color and texture in the highlight removal region are similar to those in the surrounding areas, and the texture is continuous.</details>
**Abstract_cn:** <details><summary>译文: </summary>无线胶囊内窥镜 (WCE) 捕获的图像始终呈现镜面反射，去除高光同时保留该区域的颜色和纹理仍然是一个挑战。针对这一问题，本文提出了一种胶囊内窥镜图像的高光去除方法。首先，计算高亮区域边缘的置信度和特征项，其中置信度是通过以高亮区域边缘像素为中心的窗口内RGB空间的R通道与B通道中已知像素的比率获得的，特征项为通过将高亮区域边缘像素的梯度向量与等强度线相乘获得。随后，对置信度和特征项分配不同的权重并求和以获得所有高亮区域边缘像素的优先级，并识别具有最高优先级的像素。然后，利用高亮区域边缘像素的方差来调整样本块窗口的大小，并根据样本块与窗口中心的RGB颜色相似度和距离在已知区域中搜索最佳匹配块。在具有最高优先级的像素上。最后，将最佳匹配块中的像素复制到最高优先级的高光去除区域，以达到去除高光区域的目的。实验结果表明，该方法有效地去除了WCE图像中的高光，与Crinimisi算法和DeepGin方法相比，高光去除区域的变异系数更低。另外，高光去除区域的颜色和纹理与周围区域相似，并且纹理是连续的。</details>
**PDF:** <http://arxiv.org/pdf/2402.07083v1><br />
**Code:** null<br />

