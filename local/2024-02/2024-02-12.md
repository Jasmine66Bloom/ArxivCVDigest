## [UPDATED!] **2024-02-12** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Trustworthy SR: Resolving Ambiguity in Image Super-resolution via Diffusion Models and Human Feedback**<br />
**Title_cn:** 值得信赖的 SR：通过扩散模型和人类反馈解决图像超分辨率中的模糊性<br />
**Authors:** Cansu Korkmaz, Ege Cirakman, A. Murat Tekalp, Zafer Dogan<br />
**Abstract:** <details><summary>原文: </summary>Super-resolution (SR) is an ill-posed inverse problem with a large set of feasible solutions that are consistent with a given low-resolution image. Various deterministic algorithms aim to find a single solution that balances fidelity and perceptual quality; however, this trade-off often causes visual artifacts that bring ambiguity in information-centric applications. On the other hand, diffusion models (DMs) excel in generating a diverse set of feasible SR images that span the solution space. The challenge is then how to determine the most likely solution among this set in a trustworthy manner. We observe that quantitative measures, such as PSNR, LPIPS, DISTS, are not reliable indicators to resolve ambiguous cases. To this effect, we propose employing human feedback, where we ask human subjects to select a small number of likely samples and we ensemble the averages of selected samples. This strategy leverages the high-quality image generation capabilities of DMs, while recognizing the importance of obtaining a single trustworthy solution, especially in use cases, such as identification of specific digits or letters, where generating multiple feasible solutions may not lead to a reliable outcome. Experimental results demonstrate that our proposed strategy provides more trustworthy solutions when compared to state-of-the art SR methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>超分辨率（SR）是一个不适定的反问题，具有大量与给定的低分辨率图像一致的可行解。各种确定性算法旨在找到平衡保真度和感知质量的单一解决方案；然而，这种权衡通常会导致视觉伪影，从而给以信息为中心的应用程序带来歧义。另一方面，扩散模型 (DM) 擅长生成跨越解决方案空间的各种可行的 SR 图像。接下来的挑战是如何以可靠的方式确定这组解决方案中最有可能的解决方案。我们观察到，PSNR、LPIPS、DISTS 等定量指标并不是解决模糊情况的可靠指标。为此，我们建议采用人类反馈，要求人类受试者选择少量可能的样本，然后对所选样本的平均值进行整合。该策略利用了 DM 的高质量图像生成功能，同时认识到获得单一值得信赖的解决方案的重要性，尤其是在识别特定数字或字母等用例中，生成多个可行的解决方案可能不会产生可靠的结果。实验结果表明，与最先进的 SR 方法相比，我们提出的策略提供了更值得信赖的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.07597v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Re-DiffiNet: Modeling discrepancies in tumor segmentation using diffusion**<br />
**Title_cn:** Re-DiffiNet：使用扩散对肿瘤分割中的差异进行建模<br />
**Authors:** Tianyi Ren, Abhishek Sharma, Juampablo Heras Rivera, Harshitha Rebala, Ethan Honey, Agamdeep Chopra, Mehmet Kurt<br />
**Abstract:** <details><summary>原文: </summary>Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons. Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications. Generative modeling techniques have seen significant improvements in recent times. Specifically, Generative Adversarial Networks (GANs) and Denoising-diffusion-based models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes. In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs. By explicitly modeling the discrepancy, the results show an average improvement of 0.55\% in the Dice score and 16.28\% in HD95 from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model.</details>
**Abstract_cn:** <details><summary>译文: </summary>肿瘤边缘的识别对于胶质母细胞瘤患者的手术决策至关重要，并为神经外科医生提供可靠的帮助。尽管多年来肿瘤分割的深度学习架构取得了进步，但创建适合临床楼层的完全自主系统仍然是一项艰巨的挑战，因为模型预测尚未达到临床应用所需的准确性和普遍性水平。近年来，生成建模技术取得了显着的进步。具体来说，生成对抗网络（GAN）和基于去噪扩散的模型（DDPM）已被用来生成具有更少伪影和更精细属性的更高质量图像。在这项工作中，我们引入了一个名为 Re-Diffinet 的框架，用于使用 DDPM 对 U-Net 等分割模型的输出与地面实况之间的差异进行建模。通过对差异进行显式建模，结果显示，与最先进的 U-Net 分割模型相比，交叉验证的 Dice 分数平均提高了 0.55%，HD95 平均提高了 16.28% 。</details>
**PDF:** <http://arxiv.org/pdf/2402.07354v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO**<br />
**Title_cn:** MODIPHY：使用支持 PHantom 卷积的更快 YOLO 进行物联网多模态模糊检测<br />
**Authors:** Shubhabrata Mukherjee, Cory Beard, Zhu Li<br />
**Abstract:** <details><summary>原文: </summary>Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>弱光条件和遮挡场景会阻碍自动驾驶汽车和安全系统等现实世界物联网 (IoT) 应用中的物体检测。虽然先进的机器学习模型力求准确性，但它们的计算需求与资源受限设备的限制相冲突，从而阻碍了实时性能。在我们当前的研究中，我们通过引入“YOLO Phantom”来应对这一挑战，这是有史以来最小的 YOLO 模型之一。 YOLO Phantom 利用新颖的 Phantom Convolution 模块，实现了与最新 YOLOv8n 模型相当的精度，同时将参数和模型大小减少了 43%，从而使千兆浮点运算 (GFLOP) 显着减少 19%。 YOLO Phantom 利用多模态 RGB 红外数据集上的迁移学习来解决弱光和遮挡问题，使其在不利条件下具有强大的视觉能力。其实际功效在具有先进低光和 RGB 摄像头的物联网平台上得到了证明，无缝连接到基于 AWS 的通知端点，以实现高效的实时对象检测。基准测试显示，与基线 YOLOv8n 模型相比，热检测和 RGB 检测的每秒帧数 (FPS) 分别大幅提升了 17% 和 14%。对于社区贡献，代码和多模式数据集均可在 GitHub 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.07894v1><br />
**Code:** <https://github.com/shubha07m/On-device-computer-vision-experiments-with-IoT>**<br />
>>**index:** 2<br />
**Title:** **Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search**<br />
**Title_cn:** 在混合主动对话式搜索中提出多模式澄清问题<br />
**Authors:** Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam<br />
**Abstract:** <details><summary>原文: </summary>In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>在混合主动对话搜索系统中，澄清问题用于帮助难以在单个查询中表达意图的用户。这些问题旨在揭示用户的信息需求并解决查询歧义。我们假设在多模式信息相关的场景中，可以通过使用非文本信息来改进澄清过程。因此，我们建议添加图像来澄清问题，并制定在开放域、混合主动对话搜索系统中提出多模态澄清问题的新任务。为了促进对此任务的研究，我们收集了一个名为 Melon 的数据集，其中包含超过 4k 个多模态澄清问题，并包含超过 14k 个图像。我们还提出了一种名为 Marto 的多模态查询澄清模型，并采用基于提示的生成式微调策略，以不同的提示进行不同阶段的训练。我们进行了多项分析来了解多模式内容在查询澄清阶段的重要性。实验结果表明，在选择相关图像时，图像的添加可以使检索性能显着提高高达 90%。还进行了广泛的分析，以显示 Marto 与判别性基线相比在有效性和效率方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.07742v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Exploring Perceptual Limitation of Multimodal Large Language Models**<br />
**Title_cn:** 探索多模态大语言模型的感知局限性<br />
**Authors:** Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy. Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs. To facilitate further investigations, we release our code and data.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）最近在回答视觉问题方面表现出了卓越的感知能力，然而，人们对其感知的局限性知之甚少。特别是，虽然先前的工作已经提供了 MLLM 对物体大小敏感的轶事证据，但这种现象及其根本原因尚未得到全面探讨。在这项工作中，我们定量研究了几种最先进的 MLLM 中小视觉物体的感知，并揭示了在回答有关图像中小物体的问题时普遍存在的局限性。接下来，我们确定了可能导致这种限制的四个独立因素——物体质量、大小、干扰因素和位置——并进行受控干预研究，以衡量每个因素对 MLLM 感知的影响。特别是，我们发现较低的物体质量和较小的物体尺寸都会独立地降低 MLLM 回答视觉问题的能力。更令人惊讶的是，我们发现图像中物体的位置和视觉干扰物的存在也会显着降低 MLLM 的问题回答准确性。我们的研究提供了对 MLLM 感知局限性的更好理解，并为分析未来 MLLM 的感知提供了新的评估协议。为了便于进一步调查，我们发布了我们的代码和数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.07384v1><br />
**Code:** <https://github.com/saccharomycetes/mllm-perceptual-limitation>**<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Towards Meta-Pruning via Optimal Transport**<br />
**Title_cn:** 通过最佳传输实现元剪枝<br />
**Authors:** Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh<br />
**Abstract:** <details><summary>原文: </summary>Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches. Our code is available here: https://github.com/alexandertheus/Intra-Fusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经网络的结构修剪通常依赖于识别和丢弃不太重要的神经元，这种做法通常会导致严重的准确性损失，从而需要后续的微调工作。本文介绍了一种名为 Intra-Fusion 的新颖方法，挑战了这种流行的修剪范式。与专注于设计有意义的神经元重要性指标的现有方法不同，Intra-Fusion 重新定义了叠加的修剪过程。通过利用模型融合和最优传输的概念，我们利用不可知论给出的重要性度量来获得更有效的稀疏模型表示。值得注意的是，我们的方法在不需要资源密集型微调的情况下实现了显着的精度恢复，使其成为神经网络压缩的有效且有前途的工具。此外，我们还探讨了如何将融合添加到修剪过程中，以显着减少训练时间，同时保持竞争性能。我们在 CIFAR-10、CIFAR-100 和 ImageNet 等常用数据集上对各种网络的结果进行基准测试。更广泛地说，我们希望所提出的 Intra-Fusion 方法能够激发对主要压缩方法的新替代方案的探索。我们的代码可以在这里找到：https://github.com/alexandertheus/Intra-Fusion。</details>
**PDF:** <http://arxiv.org/pdf/2402.07839v1><br />
**Code:** <https://github.com/alexandertheus/intra-fusion>**<br />
>>**index:** 2<br />
**Title:** **Make it more specific: A novel uncertainty based airway segmentation application on 3D U-Net and its variants**<br />
**Title_cn:** 使其更具体：基于 3D U-Net 的新型不确定性气道分割应用及其变体<br />
**Authors:** Shiyi Wang, Yang Nan, Felder Federico N, Sheng Zhang, Walsh Simon L F, Guang Yang<br />
**Abstract:** <details><summary>原文: </summary>Each medical segmentation task should be considered with a specific AI algorithm based on its scenario so that the most accurate prediction model can be obtained. The most popular algorithms in medical segmentation, 3D U-Net and its variants, can directly implement the task of lung trachea segmentation, but its failure to consider the special tree-like structure of the trachea suggests that there is much room for improvement in its segmentation accuracy. Therefore, a research gap exists because a great amount of state-of-the-art DL algorithms are vanilla 3D U-Net structures, which do not introduce the various performance-enhancing modules that come with special natural image modality in lung airway segmentation. In this paper, we proposed two different network structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet) which are based on U-Net structure and compared the prediction results with the same dataset. Specially, both of the two networks add branch loss and central line loss to learn the feature of fine branch endings of the airways. Uncertainty estimation algorithms are also included to attain confident predictions and thereby, increase the overall trustworthiness of our whole model. In addition, predictions of the lung trachea based on the maximum connectivity rate were calculated and extracted during post-processing for segmentation refinement and pruning.</details>
**Abstract_cn:** <details><summary>译文: </summary>每个医学分割任务都应该根据其场景使用特定的人工智能算法，以获得最准确的预测模型。医学分割中最流行的算法3D U-Net及其变体可以直接实现肺部气管分割的任务，但其未能考虑气管特殊的树状结构，表明其还有很大的改进空间。分割精度。因此，存在研究空白，因为大量最先进的深度学习算法都是普通的 3D U-Net 结构，它们没有引入肺气道分割中特殊自然图像模态的各种性能增强模块。本文基于U-Net结构提出了两种不同的网络结构Branch-Level U-Net（B-UNet）和Branch-Level CE-UNet（B-CE-UNet），并将预测结果与相同的数据集。特别地，两个网络都添加了分支损失和中心线损失来学习气道精细分支末端的特征。还包括不确定性估计算法，以获得可靠的预测，从而提高整个模型的整体可信度。此外，在后处理过程中计算和提取基于最大连接率的肺气管预测，以进行分割细化和修剪。</details>
**PDF:** <http://arxiv.org/pdf/2402.07403v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets**<br />
**Title_cn:** 使用自定义数据集通过机器学习方法检测拉布拉多豆上的蜘蛛螨<br />
**Authors:** Violet Liu, Jason Chen, Ans Qureshi, Mahla Nejati<br />
**Abstract:** <details><summary>原文: </summary>Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着粮食生产需求的不断增长，早期植物病害检测对于保护农作物至关重要；这项研究提出了一种用于植物病害检测的视觉机器学习方法，利用通过 JAI FS-1600D-10GE 相机在现实条件下收集的 RGB 和 NIR 数据来构建 RGBN 数据集。使用 YOLOv8 和顺序 CNN 的两阶段早期植物病害检测模型在具有部分标签的数据集上进行训练，与单阶段端到端分割模型相比，mAP 提高了 3.6%。顺序 CNN 模型利用 RGBN 数据实现了 90.62% 的验证准确率。与使用 ResNet15 和顺序 CNN 模型的 RGB 相比，在分类中使用 RGBN 的验证准确率平均提高了 6.25%。需要进一步的研究和数据集改进来满足粮食生产需求。</details>
**PDF:** <http://arxiv.org/pdf/2402.07895v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Benchmark Grocery Dataset of Realworld Point Clouds From Single View**<br />
**Title_cn:** 来自单一视图的现实世界点云的基准杂货数据集<br />
**Authors:** Shivanand Venkanna Sheshappanavar, Tejas Anvekar, Shivanand Kundargi, Yufan Wang, Chandra Kambhamettu<br />
**Abstract:** <details><summary>原文: </summary>Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired. Existing datasets on groceries are mainly 2D images. Models trained on these datasets are limited to learning features from the regular 2D grids. While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones. Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery. In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples. Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome. Thus, we introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images. We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models. Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks. Project Page: https://bigdatavision.org/3DGrocery100/.</details>
**Abstract_cn:** <details><summary>译文: </summary>细粒度杂货物体识别是一个重要的计算机视觉问题，在自动结账、店内机器人导航和视障人士辅助技术等领域有着广泛的应用。现有的杂货数据集主要是二维图像。在这些数据集上训练的模型仅限于从常规二维网格学习特征。虽然 Kinect 等便携式 3D 传感器通常可用于手机，但 LiDAR 和 TrueDepth 等传感器最近已集成到手机中。尽管有移动 3D 传感器，但目前还没有专门的杂货店真实世界大规模基准 3D 数据集。此外，现有的 3D 数据集缺乏细粒度的杂货类别，且训练样本有限。此外，与传统的照片捕捉相比，通过绕物体移动来收集数据使得数据收集变得很麻烦。因此，我们引入了一个名为 3DGrocery100 的大型杂货数据集。它由 100 个类组成，共有 87,898 个 3D 点云，由 10,755 个 RGB-D 单视图图像创建。我们在六个最新的最先进的 3D 点云分类模型上对我们的数据集进行基准测试。此外，我们还对少量样本和持续学习点云分类任务的数据集进行了基准测试。项目页面：https://bigdatavision.org/3DGrocery100/。</details>
**PDF:** <http://arxiv.org/pdf/2402.07819v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PBADet: A One-Stage Anchor-Free Approach for Part-Body Association**<br />
**Title_cn:** PBADet：一种用于部分身体关联的单阶段无锚方法<br />
**Authors:** Zhongpai Gao, Huayi Zhou, Abhishek Sharma, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu<br />
**Abstract:** <details><summary>原文: </summary>The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.</details>
**Abstract_cn:** <details><summary>译文: </summary>人体部位（例如手、脸）的检测及其与个体的正确关联是一项重要任务，例如对于无处不在的人机界面和动作识别。传统方法通常采用多阶段过程，依赖于繁琐的基于锚的系统，或者不能很好地扩展到更大的零件集。本文提出了 PBADet，这是一种新颖的单阶段、无锚点关联检测方法。基于跨多尺度特征图的无锚对象表示，我们引入了一个单一的零件到主体的中心偏移，它有效地封装了零件与其父主体之间的关系。我们的设计本质上是多功能的，能够管理多个部件与车身的关联，而不会影响检测的准确性或稳健性。对各种数据集的综合实验强调了我们方法的有效性，该方法不仅优于现有的最先进技术，而且还为身体部位关联挑战提供了更简化、更有效的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.07814v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning**<br />
**Title_cn:** 使用深度学习在 CT 和 MRI 上对软组织肿瘤进行最小交互分割<br />
**Authors:** Douwe J. Spaanderman, Martijn P. A. Starmans, Gonnie C. M. van Erp, David F. Hanff, Judith H. Sluijter, Anne-Rose W. Schut, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Grunhagen, Wiro J. Niessen, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers. Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance. We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI. The method requires the user to click six points near the tumor's extreme boundaries. These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network. For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\pm$0.11 (mean $\pm$ standard deviation (SD)) for CT and 0.84$\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists. Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\pm$0.08 for CT, 0.84$\pm$0.09 for T1-weighted MRI, and 0.88\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>分割对于医学成像获得形态学、体积和放射组学生物标志物至关重要。手动分割是准确的，但在放射科医生的临床工作流程中不可行，而自动分割通常会获得低于标准的性能。因此，我们开发了一种基于最小交互深度学习的 CT 和 MRI 软组织肿瘤 (STT) 分割方法。该方法要求用户单击肿瘤极端边界附近的六个点。这六个点被转换为距离图，并与图像一起作为卷积神经网络的输入。为了进行训练和验证，使用了包含 7 个解剖位置的 514 名患者和 9 种 STT 类型的多中心数据集，得出的 Dice 相似系数 (DSC) 为 0.85$\pm$0.11（平均 $\pm$ 标准差 (SD)）与专家放射科医生进行的手动分割相比，CT 为 0.84$\pm$0.12，T1 加权 MRI 为 0.84$\pm$0.12。接下来，该方法在包括四肢五种未见的 STT 表型的数据集上进行了外部验证，CT 达到 0.81$\pm$0.08，T1 加权 MRI 达到 0.84$\pm$0.09，之前未见的 T2 加权达到 0.88\pm0.08脂肪饱和 (FS) MRI。总之，我们的最小交互分割方法有效地分割了 CT 和 MRI 上的不同类型的 STT，并对以前未见过的表型和成像模式具有强大的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.07746v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Signed Distance Field based Segmentation and Statistical Shape Modelling of the Left Atrial Appendage**<br />
**Title_cn:** 基于符号距离场的左心耳分割和统计形状建模<br />
**Authors:** Kristine Aavild Juhl, Jakob Slipsager, Ole de Backer, Klaus Kofoed, Oscar Camara, Rasmus Paulsen<br />
**Abstract:** <details><summary>原文: </summary>Patients with atrial fibrillation have a 5-7 fold increased risk of having an ischemic stroke. In these cases, the most common site of thrombus localization is inside the left atrial appendage (LAA) and studies have shown a correlation between the LAA shape and the risk of ischemic stroke. These studies make use of manual measurement and qualitative assessment of shape and are therefore prone to large inter-observer discrepancies, which may explain the contradictions between the conclusions in different studies. We argue that quantitative shape descriptors are necessary to robustly characterize LAA morphology and relate to other functional parameters and stroke risk.   Deep Learning methods are becoming standardly available for segmenting cardiovascular structures from high resolution images such as computed tomography (CT), but only few have been tested for LAA segmentation. Furthermore, the majority of segmentation algorithms produces non-smooth 3D models that are not ideal for further processing, such as statistical shape analysis or computational fluid modelling. In this paper we present a fully automatic pipeline for image segmentation, mesh model creation and statistical shape modelling of the LAA. The LAA anatomy is implicitly represented as a signed distance field (SDF), which is directly regressed from the CT image using Deep Learning. The SDF is further used for registering the LAA shapes to a common template and build a statistical shape model (SSM). Based on 106 automatically segmented LAAs, the built SSM reveals that the LAA shape can be quantified using approximately 5 PCA modes and allows the identification of two distinct shape clusters corresponding to the so-called chicken-wing and non-chicken-wing morphologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>房颤患者发生缺血性中风的风险增加 5-7 倍。在这些病例中，最常见的血栓定位部位是左心耳 (LAA) 内部，研究表明 LAA 形状与缺血性中风风险之间存在相关性。这些研究采用手动测量和形状定性评估，因此观察者之间容易出现较大差异，这可能解释了不同研究结论之间的矛盾。我们认为，定量形状描述符对于强有力地表征左心耳形态并与其他功能参数和中风风险相关是必要的。深度学习方法正在成为从计算机断层扫描 (CT) 等高分辨率图像中分割心血管结构的标准方法，但只有少数方法经过了左心耳分割测试。此外，大多数分割算法都会生成不平滑的 3D 模型，这些模型不适合进一步处理，例如统计形状分析或计算流体建模。在本文中，我们提出了一种用于左心耳图像分割、网格模型创建和统计形状建模的全自动流程。 LAA 解剖结构隐式表示为带符号距离场 (SDF)，它是使用深度学习从 CT 图像直接回归得到的。 SDF 进一步用于将 LAA 形状注册到通用模板并构建统计形状模型 (SSM)。基于 106 个自动分割的 LAA，构建的 SSM 表明可以使用大约 5 种 PCA 模式来量化 LAA 形状，并允许识别与所谓的鸡翅和非鸡翅形态相对应的两个不同形状簇。</details>
**PDF:** <http://arxiv.org/pdf/2402.07708v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer**<br />
**Title_cn:** AYDIV：通过集成上下文视觉转换器进行适应性强的 3D 物体检测<br />
**Authors:** Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith, Supriyo Chakraborty, Mir Feroskhan<br />
**Abstract:** <details><summary>原文: </summary>Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2</details>
**Abstract_cn:** <details><summary>译文: </summary>结合激光雷达和摄像头数据已显示出增强自动驾驶系统中短距离物体检测的潜力。然而，由于激光雷达的稀疏数据与摄像机的密集分辨率之间的对比，融合在远距离检测方面遇到了困难。此外，两种数据表示的差异进一步使融合方法变得复杂。我们推出 AYDIV，这是一种集成三相对齐过程的新颖框架，专门设计用于增强远距离检测，即使在数据差异的情况下也是如此。 AYDIV由全局上下文融合对齐变换器（GCFAT）组成，它改进了相机特征的提取并提供了对大规模模式的更深入的理解；稀疏融合特征注意力（SFFA），微调激光雷达和相机细节的融合；以及用于全面空间数据融合的体积网格注意力（VGA）。 AYDIV 在 Waymo 开放数据集（WOD）上的性能，mAPH 值（L2 难度）提高了 1.24%，在 Argoverse2 数据集上，AP 值性能提高了 7.40%，证明了其与其他现有基于融合的方法相比的有效性。我们的代码可在 https://github.com/sanjay-810/AYDIV2 上公开获取</details>
**PDF:** <http://arxiv.org/pdf/2402.07680v1><br />
**Code:** <https://github.com/sanjay-810/aydiv2>**<br />
>>**index:** 7<br />
**Title:** **GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance**<br />
**Title_cn:** GBOT：基于图形的 3D 对象跟踪，用于增强现实辅助装配指导<br />
**Authors:** Shiyu Li, Hannah Schieber, Niklas Corell, Bernhard Egger, Julian Kreimeier, Daniel Roth<br />
**Abstract:** <details><summary>原文: </summary>Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.</details>
**Abstract_cn:** <details><summary>译文: </summary>可组装零件的指导是增强现实的一个有前途的领域。增强现实装配引导需要目标物体的实时 6D 物体姿态。特别是在时间紧迫的医疗或工业环境中，对各个部件进行连续且无标记的跟踪对于可视化叠加在目标对象部件上或旁边的指令至关重要。在这方面，用户的手或其他物体的遮挡以及不同组装状态的复杂性使得鲁棒且实时的无标记多物体跟踪变得复杂。为了解决这个问题，我们提出了基于图的对象跟踪（GBOT），这是一种新颖的基于图的单视图 RGB-D 跟踪方法。实时无标记多目标跟踪通过 6D 位姿估计进行初始化，并更新基于图形的装配位姿。通过我们新颖的多状态装配图来实现对各种装配状态的跟踪。我们通过利用各个装配零件的相对姿势来更新多状态装配图。链接该图中的各个对象可以在装配过程中实现更强大的对象跟踪。为了进行评估，我们引入了公开可用且可 3D 打印的装配资产的综合数据集，作为未来工作的基准。合成数据的定量实验和真实测试数据的进一步定性研究表明，GBOT 在实现上下文感知增强现实装配指导方面可以超越现有的工作。数据集和代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.07677v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **A Flow-based Credibility Metric for Safety-critical Pedestrian Detection**<br />
**Title_cn:** 用于安全关键行人检测的基于流的可信度度量<br />
**Authors:** Maria Lyssenko, Christoph Gladisch, Christian Heinzemann, Matthias Woehrle, Rudolph Triebel<br />
**Abstract:** <details><summary>原文: </summary>Safety is of utmost importance for perception in automated driving (AD). However, a prime safety concern in state-of-the art object detection is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance. Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks. To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels. We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset. Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections.</details>
**Abstract_cn:** <details><summary>译文: </summary>安全对于自动驾驶（AD）的感知至关重要。然而，最先进的对象检测中的一个主要安全问题是标准评估方案利用与安全无关的指标来论证足够的检测性能。因此，必须利用补充领域知识来强调评估任务期间对安全至关重要的误检。为了解决规范不足的问题，本文针对行人边界框引入了一种新颖的可信度度量，称为 c-flow。为此，c-flow 依赖于图像序列的互补光流信号，并增强对安全关键误检的分析，而无需额外的标签。我们使用最先进的行人检测器在大型 AD 数据集上实现和评估 c-flow。我们的分析表明，c-flow 允许开发人员识别安全关键的误检测。</details>
**PDF:** <http://arxiv.org/pdf/2402.07642v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles**<br />
**Title_cn:** 联网自动驾驶车辆中具有混合特征融合的协作语义占用预测<br />
**Authors:** Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zimmer, Markus Gross, Andreas Festag, Alois Knoll<br />
**Abstract:** <details><summary>原文: </summary>Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird's eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶汽车中的协作感知利用代理之间的信息交换，旨在提升感知结果。以前基于摄像头的协作 3D 感知方法通常采用 3D 边界框或鸟瞰图作为环境的表示。然而，这些方法不足以提供全面的 3D 环境预测。为了弥补这一差距，我们引入了第一种协作 3D 语义占用预测方法。特别是，它通过 (i) 语义和占用任务特征以及 (ii) 车辆之间共享的压缩正交注意特征的混合融合来改进局部 3D 语义占用预测。此外，由于缺乏为语义占用预测而设计的协作感知数据集，我们增强了当前的协作感知数据集，以包含 3D 协作语义占用标签，以进行更稳健的评估。实验结果强调：(i) 我们的协作语义占用预测比单个车辆的结果高出 30% 以上，(ii) 基于语义占用的模型在后续感知中超过了最先进的协作 3D 检测技术应用程序，展示了道路环境中更高的准确性和丰富的语义意识。</details>
**PDF:** <http://arxiv.org/pdf/2402.07635v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Complete Instances Mining for Weakly Supervised Instance Segmentation**<br />
**Title_cn:** 弱监督实例分割的完整实例挖掘<br />
**Authors:** Zecheng Li, Zening Zeng, Yuqi Liang, Jin-Gang Yu<br />
**Abstract:** <details><summary>原文: </summary>Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.</details>
**Abstract_cn:** <details><summary>译文: </summary>仅使用图像级标签的弱监督实例分割（WSIS）是一项具有挑战性的任务，因为很难将粗略注释与更精细的任务对齐。然而，随着深度神经网络（DNN）的进步，WSIS 引起了极大的关注。遵循基于提案的范例，我们遇到了由于单个实例由多个提案表示而导致的冗余分割问题。例如，我们将一张狗的图片和提案输入网络，并期望只输出一个包含狗的提案，但网络输出多个提案。为了解决这个问题，我们为 WSIS 提出了一种新颖的方法，该方法侧重于通过使用 MaskIoU 头来预测提案的完整性分数，并使用完整实例挖掘（CIM）策略来显式建模冗余分割问题，从而在线细化完整实例并生成精炼的伪标签。我们的方法允许网络了解多个实例和完整实例，并且我们通过结合抗噪声策略进一步提高其鲁棒性。对 PASCAL VOC 2012 和 MS COCO 数据集的实证评估表明，我们的方法以显着的优势实现了最先进的性能。我们的实施将在 https://github.com/Ze ChengLi19/CIM 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.07633v1><br />
**Code:** <https://github.com/ZechengLi19/CIM>**<br />
>>**index:** 11<br />
**Title:** **Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription**<br />
**Title_cn:** Sheet Music Transformer：超越单音转录的端到端光学音乐识别<br />
**Authors:** Antonio Ríos-Vila, Jorge Calvo-Zaragoza, Thierry Paquet<br />
**Abstract:** <details><summary>原文: </summary>State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.</details>
**Abstract_cn:** <details><summary>译文: </summary>迄今为止，最先进的端到端光学音乐识别（OMR）主要是使用单音转录技术来处理复杂的乐谱布局，例如复调音乐，通常采用简化或特定的改编。尽管它们很有效，但这些方法意味着与可扩展性和限制相关的挑战。本文介绍了 Sheet Music Transformer，这是第一个端到端 OMR 模型，旨在转录复杂的乐谱，而无需仅依赖单音策略。我们的模型采用基于 Transformer 的图像到序列框架，可以根据输入图像预测标准数字音乐编码格式的乐谱转录。我们的模型已经在两个复调音乐数据集上进行了测试，并被证明能够有效处理这些复杂的音乐结构。实验结果不仅表明了该模型的能力，还表明它优于最先进的方法，从而有助于端到端 OMR 转录的进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.07596v1><br />
**Code:** <https://github.com/multiscore/smt>**<br />
>>**index:** 12<br />
**Title:** **ClusterTabNet: Supervised clustering method for table detection and table structure recognition**<br />
**Title_cn:** ClusterTabNet：用于表格检测和表格结构识别的监督聚类方法<br />
**Authors:** Marek Polewczyk, Marco Spinaci<br />
**Abstract:** <details><summary>原文: </summary>We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种基于深度学习的新颖方法来对文档中的单词进行聚类，我们将其应用于给定 OCR 输出的表格检测和识别。我们将表结构自下而上解释为单词对（属于同一行、列、标题以及同一表）之间的关系图，并使用 Transformer 编码器模型来预测其邻接矩阵。我们在 PubTables-1M 数据集以及 PubTabNet 和 FinTabNet 数据集上展示了我们的方法的性能。与当前最先进的检测方法（例如 DETR 和 Faster R-CNN）相比，我们的方法实现了相似或更好的精度，同时需要更小的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.07502v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound**<br />
**Title_cn:** TriAug：超声中不平衡乳腺病变稳健分类的分布外检测<br />
**Authors:** Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang<br />
**Abstract:** <details><summary>原文: </summary>Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.</details>
**Abstract_cn:** <details><summary>译文: </summary>不同的疾病，例如乳腺病变的组织学亚型，其发病率差异很大。即使使用大量分布内 (ID) 数据进行训练，模型也经常会遇到属于临床现实中看不见的类别的分布外 (OOD) 样本。为了解决这个问题，我们提出了一种基于乳腺超声图像长尾 OOD 检测任务的新颖框架。它配备了三重状态增强 (TriAug)，可提高 ID 分类准确性，同时保持良好的 OOD 检测性能。同时，我们设计了平衡球损失来处理类别不平衡问题。</details>
**PDF:** <http://arxiv.org/pdf/2402.07452v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **An Empirical Study Into What Matters for Calibrating Vision-Language Models**<br />
**Title_cn:** 关于校准视觉语言模型的重要因素的实证研究<br />
**Authors:** Weijie Tu, Weijian Deng, Dylan Campbell, Stephen Gould, Tom Gedeon<br />
**Abstract:** <details><summary>原文: </summary>Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型（VLM）已成为零样本识别的主要方法，擅长处理不同的场景和重大的分布变化。然而，它们在风险敏感领域的部署需要更深入地了解其不确定性估计能力，这是一个相对未知的领域。在这项研究中，我们探索了不同架构、数据集和训练策略下 VLM 的校准特性。特别是，我们分析了 VLM 在一个域、标签集或层次结构级别进行校准并在不同域、标签集或层次结构级别进行测试时的不确定性估计性能。我们的研究结果表明，虽然 VLM 本身并未针对不确定性进行校准，但温度缩放可以显着且持续地改善校准，即使在分布变化和标签集变化时也是如此。此外，VLM 可以用非常小的一组示例进行校准。通过详细的实验，我们强调了我们的见解的潜在应用和重要性，旨在在关键的现实场景中更可靠、更有效地使用 VLM。</details>
**PDF:** <http://arxiv.org/pdf/2402.07417v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems**<br />
**Title_cn:** 适用于不同异构计算系统的上下文感知多模型对象检测<br />
**Authors:** Justin Davis, Mehmet E. Belviranli<br />
**Abstract:** <details><summary>原文: </summary>In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfying the latency constraints. Our proposed methodology results in improvements of up to 7.5x in energy usage and 2.8x in latency compared to state-of-the-art GPU-based single model OD approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度神经网络（DNN）在连续移动物体检测（OD）任务中得到了广泛采用，特别是在自主系统中。然而，其部署中的一个普遍问题是一刀切的方法，即使用单个 DNN，导致计算资源的利用效率低下。这种低效率在能源受限的系统中尤其有害，因为它降低了整体系统效率。我们发现，可以利用嵌入在输入数据流中的上下文信息（例如运行 OD 模型的摄像机馈送中的帧）来实现更高效的基于多模型的 OD 过程。在本文中，我们提出了 SHIFT，它根据动态变化的上下文信息和计算约束，从各种基于 DNN 的 OD 模型中不断进行选择。在此选择过程中，SHIFT 独特地考虑了多加速器执行，以更好地优化能源效率，同时满足延迟约束。与最先进的基于 GPU 的单模型 OD 方法相比，我们提出的方法使能源使用量提高了 7.5 倍，延迟提高了 2.8 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.07415v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)**<br />
**Title_cn:** 仔细观察对比语言图像预训练 (CLIP) 的鲁棒性<br />
**Authors:** Weijie Tu, Weijian Deng, Tom Gedeon<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 types of out-of-distribution data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比语言图像预训练 (CLIP) 模型在多个具有挑战性的分布变化中表现出了卓越的泛化能力。然而，就其对特定视觉因素变化的鲁棒性而言，仍有很多有待探索。在现实应用中，可靠且安全的系统必须考虑分类准确性之外的其他安全目标，例如预测不确定性。然而，CLIP 模型在此类安全相关功能上的有效性却鲜为人知。在上述推动下，这项工作全面研究了 CLIP 模型的安全目标，特别关注三个关键属性：对视觉因素变化的弹性、校准的不确定性估计以及检测异常输入的能力。为此，我们研究了 83 个 CLIP 模型和 127 个 ImageNet 分类器。它们在架构、（预）训练分布和训练策略方面各不相同。我们考虑 10 种视觉因素（例如形状和图案）、5 种分布外数据以及 8 种具有不同偏移类型（例如纹理、样式和扰动偏移）的自然且具有挑战性的测试条件。我们的研究揭示了一些以前未知的关于 CLIP 模型的见解。例如，它们并不总是比其他 ImageNet 模型更加校准，这与现有的发现相矛盾。此外，我们的分析通过展示训练源设计对三个安全相关属性的深远影响，强调了训练源设计的重要性。我们相信，我们的全面研究可以揭示并帮助指导更强大、更可靠的 CLIP 模型的开发。</details>
**PDF:** <http://arxiv.org/pdf/2402.07410v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Unsupervised Discovery of Object-Centric Neural Fields**<br />
**Title_cn:** 以对象为中心的神经场的无监督发现<br />
**Authors:** Rundong Luo, Hong-Xing Yu, Jiajun Wu<br />
**Abstract:** <details><summary>原文: </summary>We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in unsupervised 3D object discovery from simple synthetic images, they fail to generalize to real-world scenes with visually rich and diverse objects. This limitation stems from their object representations, which entangle objects' intrinsic attributes like shape and appearance with extrinsic, viewer-centric properties such as their 3D location. To address this bottleneck, we propose Unsupervised discovery of Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of objects and models the extrinsics separately. Our approach significantly improves systematic generalization, thus enabling unsupervised learning of high-fidelity object-centric scene representations from sparse real-world images. To evaluate our approach, we collect three new datasets, including two real kitchen environments. Extensive experiments show that uOCF enables unsupervised discovery of visually rich objects from a single real image, allowing applications such as 3D object segmentation and scene manipulation. Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image. Project page: https://red-fairy.github.io/uOCF/</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究从单个图像推断以对象为中心的 3D 场景表示。虽然最近的方法已经显示出从简单的合成图像中进行无监督 3D 对象发现的潜力，但它们无法推广到具有视觉丰富且多样化对象的现实世界场景。这种限制源于它们的对象表示，它将对象的内在属性（如形状和外观）与外在的、以观看者为中心的属性（如其 3D 位置）纠缠在一起。为了解决这个瓶颈，我们提出了以对象为中心的神经场的无监督发现（uOCF）。 uOCF 专注于学习对象的内在因素，并分别对外在因素进行建模。我们的方法显着提高了系统泛化能力，从而能够从稀疏的现实世界图像中无监督地学习以对象为中心的高保真场景表示。为了评估我们的方法，我们收集了三个新数据集，包括两个真实的厨房环境。大量实验表明，uOCF 能够从单个真实图像中无监督地发现视觉丰富的对象，从而允许 3D 对象分割和场景操作等应用。值得注意的是，uOCF 展示了对单个真实图像中看不见的物体的零样本泛化。项目页面：https://red-fairy.github.io/uOCF/</details>
**PDF:** <http://arxiv.org/pdf/2402.07376v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Exploring Saliency Bias in Manipulation Detection**<br />
**Title_cn:** 探索操纵检测中的显着性偏差<br />
**Authors:** Joshua Krinsky, Alan Bettis, Qiuyu Tang, Daniel Moreira, Aparna Bharati<br />
**Abstract:** <details><summary>原文: </summary>The social media-fuelled explosion of fake news and misinformation supported by tampered images has led to growth in the development of models and datasets for image manipulation detection. However, existing detection methods mostly treat media objects in isolation, without considering the impact of specific manipulations on viewer perception. Forensic datasets are usually analyzed based on the manipulation operations and corresponding pixel-based masks, but not on the semantics of the manipulation, i.e., type of scene, objects, and viewers' attention to scene content. The semantics of the manipulation play an important role in spreading misinformation through manipulated images. In an attempt to encourage further development of semantic-aware forensic approaches to understand visual misinformation, we propose a framework to analyze the trends of visual and semantic saliency in popular image manipulation datasets and their impact on detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>社交媒体引发的假新闻和虚假信息的爆炸式增长以及由篡改图像支持的错误信息导致了用于图像操纵检测的模型和数据集的开发的增长。然而，现有的检测方法大多孤立地处理媒体对象，没有考虑特定操作对观看者感知的影响。取证数据集通常基于操作操作和相应的基于像素的掩模进行分析，而不是基于操作的语义，即场景、对象的类型以及观看者对场景内容的关注。操纵的语义在通过操纵图像传播错误信息方面发挥着重要作用。为了鼓励进一步发展语义感知取证方法来理解视觉错误信息，我们提出了一个框架来分析流行图像处理数据集中视觉和语义显着性的趋势及其对检测的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.07338v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Task-conditioned adaptation of visual features in multi-task policy learning**<br />
**Title_cn:** 多任务政策学习中视觉特征的任务条件适应<br />
**Authors:** Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf<br />
**Abstract:** <details><summary>原文: </summary>Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.</details>
**Abstract_cn:** <details><summary>译文: </summary>成功解决各种任务是自主代理的核心能力，这需要灵活地调整底层决策策略，并且正如我们在这项工作中所讨论的那样，还需要调整底层感知模块。一个类比的论点是人类视觉系统，它使用自上而下的信号来集中由当前任务确定的注意力。同样，在这项工作中，我们在多任务策略学习的背景下，根据特定的下游任务调整预先训练的大视觉模型。我们引入了任务条件适配器，不需要微调任何预先训练的权重，结合通过行为克隆训练的单个策略，并且能够解决多个任务。我们根据任务嵌入来调节策略和视觉适配器，如果任务已知，则可以在推理时选择任务嵌入，或者从一组示例演示中推断出任务嵌入。为此，我们提出了一种新的基于优化的估计器。我们在 CortexBench 基准测试的各种任务上评估了该方法，并表明，与现有工作相比，它可以通过单一策略来解决。特别是，我们证明了适应视觉特征是一个关键的设计选择，并且该方法可以推广到给定视觉演示的看不见的任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.07739v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder**<br />
**Title_cn:** SelfSwapper：通过形状不可知的屏蔽自动编码器进行自我监督的面部交换<br />
**Authors:** Jaeseong Lee, Junha Hyung, Sohyun Jeong, Jaegul Choo<br />
**Abstract:** <details><summary>原文: </summary>Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部交换因其各种应用而受到广泛关注。以往的换脸方法大多依赖于跷跷板游戏训练方案，这往往会导致模型训练的不稳定，并由于目标身份泄漏问题而导致出现混合身份的不良样本。本文介绍了形状无关掩模自动编码器（SAMAE）训练方案，这是一种新颖的自监督方法，旨在增强面部交换模型训练。我们的训练方案通过规避传统的跷跷板游戏并通过其自我重建训练制度引入清晰的事实真相，解决了传统训练方法的局限性。它通过屏蔽输入图像的面部区域并利用学习到的分离身份和非身份特征，有效地减少身份泄露。此外，我们使用包括穿孔混淆和随机网格缩放在内的新技术来解决形状错位问题，并建立了一种新的最先进技术，超越了其他基线方法，保留了同一性和非同一性属性，而不牺牲任何一个方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.07370v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs**<br />
**Title_cn:** PIVOT：迭代视觉提示为 VLM 引出可操作的知识<br />
**Authors:** Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型 (VLM) 在从逻辑推理到视觉理解的各种任务中表现出了令人印象深刻的能力。这为与世界进行更丰富的交互打开了大门，例如机器人控制。然而，VLM 仅产生文本输出，而机器人控制和其他空间任务需要输出连续坐标、动作或轨迹。我们如何使 VLM 能够处理此类设置，而不需要对特定于任务的数据进行微调？在本文中，我们提出了一种新颖的 VLM 视觉提示方法，我们称之为迭代视觉优化提示（PIVOT），它将任务转换为迭代视觉问答。在每次迭代中，图像都用 VLM 可以参考的建议的视觉表示进行注释（例如，候选机器人动作、定位或轨迹）。然后，VLM 会选择最适合该任务的选项。这些建议经过迭代完善，使 VLM 最终能够将最佳可用答案归零。我们研究了现实世界机器人导航、现实世界图像操纵、模拟中的指令跟踪以及定位等其他空间推理任务的 PIVOT。我们发现，也许令人惊讶的是，我们的方法可以在没有任何机器人训练数据、在各种环境中导航和其他功能的情况下实现机器人系统的零射击控制。尽管目前的性能远非完美，但我们的工作强调了这种新机制的潜力和局限性，并展示了机器人和空间推理领域中互联网规模 VLM 的一种有前景的方法。网站：pivot-prompt.github.io 和 HuggingFace：https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo。</details>
**PDF:** <http://arxiv.org/pdf/2402.07872v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Real-World Atmospheric Turbulence Correction via Domain Adaptation**<br />
**Title_cn:** 通过域适应进行真实大气湍流校正<br />
**Authors:** Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos<br />
**Abstract:** <details><summary>原文: </summary>Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth's surface. This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects. Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect. However, these synthesized turbulent images can not cover all the range of real-world turbulence effects. Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases. Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training. In this paper, we propose a real-world atmospheric turbulence mitigation model under a domain adaptation framework, which links the supervised simulated atmospheric turbulence correction with the unsupervised real-world atmospheric turbulence correction. We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>大气湍流是日常生活中常见的现象，主要是由地球表面加热不均匀引起的。这种现象会导致采集的图像或视频失真和模糊，并可能严重影响下游视觉任务，特别是那些依赖于从室外环境捕获清晰、稳定的图像或视频的任务，例如准确检测或识别物体。因此，人们提出了模拟大气湍流的方法，并设计了有效的基于深度学习的方法来消除大气湍流效应。然而，这些合成的湍流图像无法涵盖现实世界湍流效应的所有范围。尽管这些模型在合成场景中取得了出色的性能，但在应用于实际情况时总是存在性能下降。此外，减少现实世界的湍流是一项更具挑战性的任务，因为在训练期间没有向模型提供干净的地面实况对应物。在本文中，我们提出了域适应框架下的现实世界大气湍流缓解模型，该模型将有监督的模拟大气湍流校正与无监督的现实世界大气湍流校正联系起来。我们将展示我们提出的方法增强了现实世界大气湍流场景中的性能，提高了图像质量和下游视觉任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.07371v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Wavefront Randomization Improves Deconvolution**<br />
**Title_cn:** 波前随机化改进了反卷积<br />
**Authors:** Amit Kohli, Anastasios N. Angelopoulos, Laura Waller<br />
**Abstract:** <details><summary>原文: </summary>The performance of an imaging system is limited by optical aberrations, which cause blurriness in the resulting image. Digital correction techniques, such as deconvolution, have limited ability to correct the blur, since some spatial frequencies in the scene are not measured adequately due to the aberrations ('zeros' of the system transfer function). We prove that the addition of a random mask to an imaging system removes its dependence on aberrations, reducing the likelihood of zeros in the transfer function and consequently reducing the sensitivity to noise during deconvolution. and consequently result in lower sensitivity to noise during deconvolution. In simulation, we show that this strategy improves image quality over a range of aberration types, aberration strengths, and signal-to-noise ratios.</details>
**Abstract_cn:** <details><summary>译文: </summary>成像系统的性能受到光学像差的限制，光学像差会导致所得图像模糊。数字校正技术（例如反卷积）校正模糊的能力有限，因为由于像差（系统传递函数的“零”）而无法充分测量场景中的某些空间频率。我们证明，向成像系统添加随机掩模可以消除其对像差的依赖，降低传递函数中为零的可能性，从而降低反卷积期间对噪声的敏感性。从而导致反卷积期间对噪声的敏感性较低。在仿真中，我们表明该策略提高了一系列像差类型、像差强度和信噪比的图像质量。</details>
**PDF:** <http://arxiv.org/pdf/2402.07900v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models**<br />
**Title_cn:** Prismatic VLM：研究视觉条件语言模型的设计空间<br />
**Authors:** Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh<br />
**Abstract:** <details><summary>原文: </summary>Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉条件语言模型 (VLM) 在视觉对话、场景理解和机器人任务规划等应用中得到越来越多的采用；的采用催生了 LLaVa、InstructBLIP 和 PaLI-3 等大量新模型。尽管新版本数量众多，但围绕图像预处理、架构和优化的关键设计决策尚未得到充分探索，这使得了解影响模型性能的因素变得充满挑战，而由于缺乏客观、一致的评估，这一挑战变得更加复杂。为了解决这些差距，我们首先编制了一套标准化评估，涵盖视觉问答、语言对象定位以及探测幻觉等特性的目标挑战集；评估可提供对 VLM 功能的校准、细粒度洞察。其次，我们沿着关键设计轴严格研究 VLM，包括预训练的视觉表示和量化使用基础语言模型与指令调整语言模型的权衡等。我们将分析与三个资源贡献相结合：(1) 用于评估 VLM 的统一框架，(2) 用于 VLM 训练的优化、灵活代码，以及 (3) 所有模型的检查点，包括 7-13B 规模的 VLM 系列其性能完全优于开源 VLM 中最先进的 InstructBLIP 和 LLaVa v1.5。</details>
**PDF:** <http://arxiv.org/pdf/2402.07865v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Contrastive Multiple Instance Learning for Weakly Supervised Person ReID**<br />
**Title_cn:** 弱监督行人再识别的对比多实例学习<br />
**Authors:** Jacob Tyo, Zachary C. Lipton<br />
**Abstract:** <details><summary>原文: </summary>The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co. All our code and data are accessible at https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.</details>
**Abstract_cn:** <details><summary>译文: </summary>获取大规模、精确标记的数据集以进行人员重新识别 (ReID) 提出了重大挑战。弱监督的 ReID 已经开始解决这个问题，尽管其性能落后于完全监督的方法。为此，我们引入了对比多实例学习（CMIL），这是一种专为更有效的弱监督 ReID 量身定制的新颖框架。 CMIL 的独特之处在于仅需要单一模型且不需要伪标签，同时利用对比损失——这种技术显着增强了传统 ReID 性能，但在所有先前基于 MIL 的方法中均不存在。通过对三个数据集进行广泛的实验和分析，CMIL 不仅在假设较少的情况下在大规模 SYSU-30k 数据集上达到了最先进的性能，而且始终优于 WL-market1501 和弱标记 MUddy 赛车测试的所有基线。 -iDentification 数据集 (WL-MUDD) 数据集。我们引入并发布了 WL-MUDD 数据集，这是 MUDD 数据集的扩展，具有来自 PerformancePhoto.co 的实际应用程序中自然出现的弱标签。我们所有的代码和数据都可以通过 https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link 访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.07685v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Compressive Recovery of Signals Defined on Perturbed Graphs**<br />
**Title_cn:** 扰动图上定义的信号的压缩恢复<br />
**Authors:** Sabyasachi Ghosh, Ajit Rajwade<br />
**Abstract:** <details><summary>原文: </summary>Recovery of signals with elements defined on the nodes of a graph, from compressive measurements is an important problem, which can arise in various domains such as sensor networks, image reconstruction and group testing. In some scenarios, the graph may not be accurately known, and there may exist a few edge additions or deletions relative to a ground truth graph. Such perturbations, even if small in number, significantly affect the Graph Fourier Transform (GFT). This impedes recovery of signals which may have sparse representations in the GFT bases of the ground truth graph. We present an algorithm which simultaneously recovers the signal from the compressive measurements and also corrects the graph perturbations. We analyze some important theoretical properties of the algorithm. Our approach to correction for graph perturbations is based on model selection techniques such as cross-validation in compressed sensing. We validate our algorithm on signals which have a sparse representation in the GFT bases of many commonly used graphs in the network science literature. An application to compressive image reconstruction is also presented, where graph perturbations are modeled as undesirable graph edges linking pixels with significant intensity difference. In all experiments, our algorithm clearly outperforms baseline techniques which either ignore the perturbations or use first order approximations to the perturbations in the GFT bases.</details>
**Abstract_cn:** <details><summary>译文: </summary>从压缩测量中恢复具有图节点上定义的元素的信号是一个重要问题，它可能出现在传感器网络、图像重建和组测试等各个领域中。在某些场景下，可能无法准确地知道图，并且相对于真实图可能存在一些边缘添加或删除。此类扰动即使数量很少，也会显着影响图傅立叶变换 (GFT)。这阻碍了在地面真值图的 GFT 基中可能具有稀疏表示的信号的恢复。我们提出了一种算法，可以同时从压缩测量中恢复信号并校正图形扰动。我们分析了该算法的一些重要的理论特性。我们校正图形扰动的方法基于模型选择技术，例如压缩感知中的交叉验证。我们在网络科学文献中许多常用图的 GFT 基中具有稀疏表示的信号上验证了我们的算法。还提出了压缩图像重建的应用，其中图扰动被建模为连接具有显着强度差异的像素的不期望的图边缘。在所有实验中，我们的算法明显优于基线技术，后者要么忽略扰动，要么使用 GFT 基中扰动的一阶近似。</details>
**PDF:** <http://arxiv.org/pdf/2402.07637v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Morse sequences**<br />
**Title_cn:** 莫尔斯序列<br />
**Authors:** Gilles Bertrand<br />
**Abstract:** <details><summary>原文: </summary>We introduce the notion of a Morse sequence, which provides a simple and effective approach to discrete Morse theory. A Morse sequence is a sequence composed solely of two elementary operations, that is, expansions (the inverse of a collapse), and fillings (the inverse of a perforation). We show that a Morse sequence may be seen as an alternative way to represent the gradient vector field of an arbitrary discrete Morse function. We also show that it is possible, in a straightforward manner, to make a link between Morse sequences and different kinds of Morse functions. At last, we introduce maximal Morse sequences, which formalize two basic schemes for building a Morse sequence from an arbitrary simplicial complex.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入莫尔斯序列的概念，它为离散莫尔斯理论提供了一种简单有效的方法。莫尔斯序列是仅由两个基本操作组成的序列，即膨胀（塌陷的逆）和填充（穿孔的逆）。我们证明莫尔斯序列可以被视为表示任意离散莫尔斯函数的梯度向量场的替代方法。我们还表明，可以以简单的方式在莫尔斯序列和不同类型的莫尔斯函数之间建立联系。最后，我们引入最大莫尔斯序列，它形式化了从任意单纯复形构建莫尔斯序列的两种基本方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.07526v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Novel definition and quantitative analysis of branch structure with topological data analysis**<br />
**Title_cn:** 利用拓扑数据分析对分支结构进行新颖的定义和定量分析<br />
**Authors:** Haruhisa Oda, Mayuko Kida, Yoichi Nakata, Hiroki Kurihara<br />
**Abstract:** <details><summary>原文: </summary>While branching network structures abound in nature, their objective analysis is more difficult than expected because existing quantitative methods often rely on the subjective judgment of branch structures. This problem is particularly pronounced when dealing with images comprising discrete particles. Here we propose an objective framework for quantitative analysis of branching networks by introducing the mathematical definitions for internal and external structures based on topological data analysis, specifically, persistent homology. We compare persistence diagrams constructed from images with and without plots on the convex hull. The unchanged points in the two diagrams are the internal structures and the difference between the two diagrams is the external structures. We construct a mathematical theory for our method and show that the internal structures have a monotonicity relationship with respect to the plots on the convex hull, while the external structures do not. This is the phenomenon related to the resolution of the image. Our method can be applied to a wide range of branch structures in biology, enabling objective analysis of numbers, spatial distributions, sizes, and more. Additionally, our method has the potential to be combined with other tools in topological data analysis, such as the generalized persistence landscape.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然分支网络结构在自然界中大量存在，但由于现有的定量方法往往依赖于分支结构的主观判断，因此对其进行客观分析比预期更加困难。当处理包含离散粒子的图像时，这个问题尤其明显。在这里，我们通过引入基于拓扑数据分析（特别是持久同源性）的内部和外部结构的数学定义，提出了分支网络定量分析的客观框架。我们比较了由图像构建的持久图，有和没有凸包上的图。两图中不变的点是内部结构，两图的区别是外部结构。我们为我们的方法构建了一个数学理论，并表明内部结构相对于凸包上的图具有单调性关系，而外部结构则没有。这是与图像分辨率有关的现象。我们的方法可以应用于生物学中的各种分支结构，从而能够对数量、空间分布、大小等进行客观分析。此外，我们的方法有可能与拓扑数据分析中的其他工具相结合，例如广义持久性景观。</details>
**PDF:** <http://arxiv.org/pdf/2402.07436v1><br />
**Code:** null<br />

