## [UPDATED!] **2024-02-13** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Learning Continuous 3D Words for Text-to-Image Generation**<br />
**Title_cn:** 学习连续 3D 单词以生成文本到图像<br />
**Authors:** Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, Niki Trigoni<br />
**Abstract:** <details><summary>原文: </summary>Current controls over diffusion models (e.g., through text or ControlNet) for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change. In this paper, we present an approach for allowing users of text-to-image models to have fine-grained control of several attributes in an image. We do this by engineering special sets of input tokens that can be transformed in a continuous manner -- we call them Continuous 3D Words. These attributes can, for example, be represented as sliders and applied jointly with text prompts for fine-grained control over image generation. Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses. Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process. Project Page: https://ttchengab.github.io/continuous_3d_words</details>
**Abstract_cn:** <details><summary>译文: </summary>当前用于图像生成的扩散模型（例如通过文本或 ControlNet）的控制在识别抽象、连续属性（如照明方向或非刚性形状变化）方面存在不足。在本文中，我们提出了一种允许文本到图像模型的用户对图像中的多个属性进行细粒度控制的方法。我们通过设计特殊的输入标记集来实现这一点，这些标记可以以连续的方式进行转换——我们称之为连续 3D 单词。例如，这些属性可以表示为滑块并与文本提示联合应用，以对图像生成进行细粒度控制。仅给定一个网格和一个渲染引擎，我们表明我们的方法可以用于提供对多个 3D 感知属性的连续用户控制，包括时间照明​​、鸟翼方向、移动缩放效果和对象姿势。我们的方法能够同时使用多个连续 3D 单词和文本描述来调节图像创建，同时不会增加生成过程的开销。项目页面：https://tt Chengab.github.io/continuous_3d_words</details>
**PDF:** <http://arxiv.org/pdf/2402.08654v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator**<br />
**Title_cn:** 去噪扩散恢复解决拉普拉斯算子的正向和逆向问题<br />
**Authors:** Amartya Mukherjee, Melissa M. Stadt, Lena Podina, Mohammad Kohandel, Jun Liu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images. More recently, they have been employed to generate solutions to partial differential equations (PDEs). However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise. This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM). DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator. Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator. Our results show that using denoising diffusion restoration significantly improves the estimation of the solution and parameters. Our research, as a result, pioneers the integration of diffusion models with the principles of underlying physics to solve PDEs.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型已成为一类有前途的生成模型，可将噪声输入映射到真实图像。最近，它们已被用来生成偏微分方程（PDE）的解。然而，他们仍然在解决拉普拉斯算子中的逆问题，例如泊松方程，因为幅值较大的特征值会放大测量噪声。本文提出了一种通过使用去噪扩散恢复模型（DDRM）来逆向和正向求解偏微分方程的新方法。 DDRM 用于线性逆问题，通过利用线性算子的奇异值分解 (SVD) 来恢复原始的干净信号。同样，我们提出了一种通过利用拉普拉斯算子的特征值和特征函数来恢复泊松方程中的解和参数的方法。我们的结果表明，使用去噪扩散恢复显着改善了解和参数的估计。因此，我们的研究开创了将扩散模型与基础物理原理相结合来解决偏微分方程的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.08563v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases**<br />
**Title_cn:** 面对扩散模型的奖励过度优化：归纳偏差和首要偏差的视角<br />
**Authors:** Ziyi Zhang, Sen Zhang, Yibing Zhan, Yong Luo, Yonggang Wen, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Reset (TDPO-R), a policy gradient algorithm that exploits the temporal inductive bias of intermediate timesteps, along with a novel reset strategy that targets active neurons to counteract the primacy bias. Empirical results demonstrate the superior efficacy of our algorithms in mitigating reward overoptimization.</details>
**Abstract_cn:** <details><summary>译文: </summary>弥合扩散模型和人类偏好之间的差距对于将其集成到实际的生成工作流程中至关重要。虽然优化下游奖励模型已成为一种有前途的对齐策略，但人们担心学习奖励模型过度优化的风险可能会损害真实性能。在这项工作中，我们通过归纳偏差和首要偏差的视角来面对扩散模型对齐中的奖励过度优化问题。我们首先将当前方法与扩散模型多步去噪过程中固有的时间归纳偏差的分歧确定为过度优化的潜在来源。然后，我们惊讶地发现，我们的批评家模型中的休眠神经元充当了针对过度优化的正则化作用，而活跃的神经元则反映了这种情况下的首要偏见。受这些观察的启发，我们提出了带有批判性活跃神经元重置的时间扩散策略优化（TDPO-R），这是一种利用中间时间步的时间归纳偏差的策略梯度算法，以及一种针对活跃神经元以抵消首要性的新颖重置策略偏见。实证结果证明我们的算法在减轻奖励过度优化方面具有卓越的功效。</details>
**PDF:** <http://arxiv.org/pdf/2402.08552v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence**<br />
**Title_cn:** 认真对待培训：人工智能的人为指导和管理调控<br />
**Authors:** Cary Coglianese, Colton R. Crum<br />
**Abstract:** <details><summary>原文: </summary>Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training. We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training. We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks.</details>
**Abstract_cn:** <details><summary>译文: </summary>人们强烈呼吁对人工智能 (AI) 相关危害进行更强有力的治理，这导致世界各地采用监管学者所谓的基于管理的监管方法。美国和欧洲最近采取的举措，以及国际标准化组织采用的主要自律标准，都有一个共同点：基于核心管理的范式。这些基于管理的举措旨在激励人们加强对人工智能工具培训和开发方式的监督。因此，需要对以人为指导的培训技术进行完善和系统化，以适应这个新兴的基于管理的监管范式时代。如果认真对待，人工指导的训练可以减轻人工智能的一些技术和道德压力，利用人类直觉提高人工智能的性能，并更好地满足公平性和有效可解释性的需求。在本文中，我们讨论了新兴的基于管理的人工智能监管框架与培训期间人工监督的需求之间的联系。我们广泛涵盖了人工指导训练中涉及的一些技术组成部分，然后认为监管机构最关心的人工智能高风险用例应该更多地依赖于人工指导训练，而不是纯数据训练。我们希望促进法律学者和计算机科学家之间的讨论，涉及如何治理应用和风险巨大、异构且动态的技术领域。</details>
**PDF:** <http://arxiv.org/pdf/2402.08466v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model**<br />
**Title_cn:** 使用综合训练的生成模型一对多重建文化文物的 3D 几何形状<br />
**Authors:** Thomas Pöllabauer, Julius Kühn, Jiayi Li, Arjan Kuijper<br />
**Abstract:** <details><summary>原文: </summary>Estimating the 3D shape of an object using a single image is a difficult problem. Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches. Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by multi-modal inputs, such as text prompts. It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples. Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用单个图像估计物体的 3D 形状是一个难题。现代方法对于基于真实照片的一般物体取得了良好的效果，但对于缺乏表现力的表示（例如历史草图）却取得了较差的结果。我们的自动化方法可以从单个草图生成各种详细的 3D 表示，描绘中世纪雕像，并且可以通过多模式输入（例如文本提示）进行引导。它仅依赖合成数据进行训练，因此即使在训练示例数量很少的情况下也可以采用。我们的解决方案允许策展人等领域专家以交互方式重建丢失文物的潜在外观。</details>
**PDF:** <http://arxiv.org/pdf/2402.08310v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Dense Reward View on Aligning Text-to-Image Diffusion with Preference**<br />
**Title_cn:** 关于将文本到图像扩散与偏好对齐的密集奖励视图<br />
**Authors:** Shentao Yang, Tianqi Chen, Mingyuan Zhou<br />
**Abstract:** <details><summary>原文: </summary>Aligning text-to-image diffusion model (T2I) with preference has been gaining increasing research attention. While prior works exist on directly optimizing T2I by preference data, these methods are developed under the bandit assumption of a latent reward on the entire diffusion reverse chain, while ignoring the sequential nature of the generation process. From literature, this may harm the efficacy and efficiency of alignment. In this paper, we take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps of the T2I reverse chain. In particular, we introduce temporal discounting into the DPO-style explicit-reward-free loss, to break the temporal symmetry therein and suit the T2I generation hierarchy. In experiments on single and multiple prompt generation, our method is competitive with strong relevant baselines, both quantitatively and qualitatively. Further studies are conducted to illustrate the insight of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>将文本到图像扩散模型（T2I）与偏好对齐已经受到越来越多的研究关注。虽然先前的工作是通过偏好数据直接优化 T2I，但这些方法是在整个扩散反向链上潜在奖励的强盗假设下开发的，同时忽略了生成过程的顺序性质。从文献来看，这可能会损害对齐的功效和效率。在本文中，我们采用更精细密集的奖励视角，并得出一个易于处理的对齐目标，强调 T2I 反向链的初始步骤。特别是，我们将时间贴现引入到 DPO 风格的显式无奖励损失中，以打破其中的时间对称性并适应 T2I 生成层次结构。在单个和多个提示生成的实验中，我们的方法在定量和定性方面都与强大的相关基线具有竞争力。进行了进一步的研究来说明我们方法的洞察力。</details>
**PDF:** <http://arxiv.org/pdf/2402.08265v1><br />
**Code:** <https://github.com/shentao-yang/dense_reward_t2i>**<br />
>>**index:** 7<br />
**Title:** **Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation**<br />
**Title_cn:** 微调文本到图像的扩散模型以生成按类别的杂散特征<br />
**Authors:** AprilPyone MaungMaung, Huy H. Nguyen, Hitoshi Kiya, Isao Echizen<br />
**Abstract:** <details><summary>原文: </summary>We propose a method for generating spurious features by leveraging large-scale text-to-image diffusion models. Although the previous work detects spurious features in a large-scale dataset like ImageNet and introduces Spurious ImageNet, we found that not all spurious images are spurious across different classifiers. Although spurious images help measure the reliance of a classifier, filtering many images from the Internet to find more spurious features is time-consuming. To this end, we utilize an existing approach of personalizing large-scale text-to-image diffusion models with available discovered spurious images and propose a new spurious feature similarity loss based on neural features of an adversarially robust model. Precisely, we fine-tune Stable Diffusion with several reference images from Spurious ImageNet with a modified objective incorporating the proposed spurious-feature similarity loss. Experiment results show that our method can generate spurious images that are consistently spurious across different classifiers. Moreover, the generated spurious images are visually similar to reference images from Spurious ImageNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种利用大规模文本到图像扩散模型生成虚假特征的方法。尽管之前的工作检测了像 ImageNet 这样的大规模数据集中的虚假特征并引入了 Spurious ImageNet，但我们发现并非所有虚假图像在不同的分类器中都是虚假的。尽管虚假图像有助于衡量分类器的可靠性，但过滤来自互联网的许多图像以查找更多虚假特征非常耗时。为此，我们利用现有的方法，通过可用的发现的虚假图像来个性化大规模文本到图像扩散模型，并提出一种基于对抗鲁棒模型的神经特征的新的虚假特征相似性损失。准确地说，我们使用来自 Spurious ImageNet 的几张参考图像来微调稳定扩散，并修改了目标，并结合了所提出的虚假特征相似性损失。实验结果表明，我们的方法可以生成在不同分类器中始终是虚假的虚假图像。此外，生成的虚假图像在视觉上与来自 Spurious ImageNet 的参考图像相似。</details>
**PDF:** <http://arxiv.org/pdf/2402.08200v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Poisson flow consistency models for low-dose CT image denoising**<br />
**Title_cn:** 低剂量CT图像去噪的泊松流一致性模型<br />
**Authors:** Dennis Hein, Adam Wang, Ge Wang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion and Poisson flow models have demonstrated remarkable success for a wide range of generative tasks. Nevertheless, their iterative nature results in computationally expensive sampling and the number of function evaluations (NFE) required can be orders of magnitude larger than for single-step methods. Consistency models are a recent class of deep generative models which enable single-step sampling of high quality data without the need for adversarial training. In this paper, we introduce a novel image denoising technique which combines the flexibility afforded in Poisson flow generative models (PFGM)++ with the, high quality, single step sampling of consistency models. The proposed method first learns a trajectory between a noise distribution and the posterior distribution of interest by training PFGM++ in a supervised fashion. These pre-trained PFGM++ are subsequently "distilled" into Poisson flow consistency models (PFCM) via an updated version of consistency distillation. We call this approach posterior sampling Poisson flow consistency models (PS-PFCM). Our results indicate that the added flexibility of tuning the hyperparameter D, the dimensionality of the augmentation variables in PFGM++, allows us to outperform consistency models, a current state-of-the-art diffusion-style model with NFE=1 on clinical low-dose CT images. Notably, PFCM is in itself a novel family of deep generative models and we provide initial results on the CIFAR-10 dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散和泊松流模型在各种生成任务中都取得了显着的成功。然而，它们的迭代性质会导致计算成本高昂的采样，并且所需的函数评估 (NFE) 数量可能比单步方法大几个数量级。一致性模型是最新一类深度生成模型，它能够对高质量数据进行单步采样，而无需对抗性训练。在本文中，我们介绍了一种新颖的图像去噪技术，该技术将泊松流生成模型 (PFGM)++ 提供的灵活性与一致性模型的高质量、单步采样相结合。所提出的方法首先通过以监督方式训练 PFGM++ 来学习噪声分布和感兴趣的后验分布之间的轨迹。这些预先训练的 PFGM++ 随后通过一致性蒸馏的更新版本“蒸馏”为泊松流一致性模型 (PFCM)。我们将这种方法称为后验采样泊松流一致性模型（PS-PFCM）。我们的结果表明，调整超参数 D（PFGM++ 中增强变量的维数）的灵活性使我们能够超越一致性模型，这是一种当前最先进的扩散式模型，在临床低水平上 NFE=1。剂量 CT 图像。值得注意的是，PFCM 本身就是一个新颖的深度生成模型家族，我们在 CIFAR-10 数据集上提供了初步结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.08159v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs**<br />
**Title_cn:** PIN：位置插入解锁 VLM 中的对象定位能力<br />
**Authors:** Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.</details>
**Abstract_cn:** <details><summary>译文: </summary>Flamingo 和 GPT-4V 等视觉语言模型 (VLM) 通过将大型语言模型与视觉系统集成而显示出巨大的潜力。然而，这些模型在对象定位的基本计算机视觉任务中面临挑战，因为它们是在多模态数据上进行训练，这些数据主要包含没有明确空间基础的标题。虽然可以使用与 VLM 集成的边界框注释构建自定义的监督训练管道，但这会导致专业化且难以扩展的模型。在本文中，我们的目标是探索基于字幕的 VLM 的局限性，并提出通过 i）保持基于字幕的 VLM 的权重冻结和 ii）不使用任何监督检测数据以更简单的方式应对挑战。为此，我们引入了与输入无关的位置插入（PIN），这是一种可学习的空间提示，包含在冻结的 VLM 内滑动的最小参数集，从而解锁对象定位功能。我们的 PIN 模块通过基于合成数据的简单下一个令牌预测任务进行训练，无需引入新的输出头。我们的实验证明了在各种图像上的强大的零样本定位性能，包括 Pascal VOC、COCO、LVIS 以及绘画或卡通等各种图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.08657v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Test-Time Backdoor Attacks on Multimodal Large Language Models**<br />
**Title_cn:** 对多模态大型语言模型的测试时后门攻击<br />
**Authors:** Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin<br />
**Abstract:** <details><summary>原文: </summary>Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at https://sail-sg.github.io/AnyDoor/.</details>
**Abstract_cn:** <details><summary>译文: </summary>后门攻击通常是通过污染训练数据来执行的，这样触发器就可以在测试阶段激活预定的有害影响。在这项工作中，我们提出了 AnyDoor，这是一种针对多模态大语言模型 (MLLM) 的测试时后门攻击，其中涉及使用对抗性测试图像（共享相同的通用扰动）将后门注入到文本模态中，无需访问或修改的训练数据。 AnyDoor 采用了通用对抗性攻击中使用的类似技术，但其独特之处在于其能够将有害影响的设置和激活时间解耦。在我们的实验中，我们针对 LLaVA-1.5、MiniGPT-4、InstructBLIP 和 BLIP-2 等流行的 MLLM 验证了 AnyDoor 的有效性，并提供了全面的消融研究。值得注意的是，由于后门是通过普遍扰动注入的，AnyDoor可以动态改变其后门触发提示/有害影响，这给防御后门攻击带来了新的挑战。我们的项目页面位于 https://sail-sg.github.io/AnyDoor/。</details>
**PDF:** <http://arxiv.org/pdf/2402.08577v1><br />
**Code:** <https://github.com/sail-sg/anydoor>**<br />
>>**index:** 3<br />
**Title:** **Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast**<br />
**Title_cn:** 史密斯特工：单张图像可以以指数速度越狱一百万多模式 LLM 特工<br />
**Authors:** Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin<br />
**Abstract:** <details><summary>原文: </summary>A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模式大语言模型 (MLLM) 代理可以接收指令、捕获图像、从内存中检索历史记录并决定使用哪些工具。尽管如此，红队工作表明，对抗性图像/提示可能会越狱 MLLM 并导致不一致的行为。在这项工作中，我们报告了多代理环境中更严重的安全问题，称为传染性越狱。它需要对手简单地越狱单个代理，并且在没有对手的任何进一步干预的情况下，（几乎）所有代理都将呈指数级快速感染并表现出有害行为。为了验证传染性越狱的可行性，我们模拟了包含多达一百万个 LLaVA-1.5 代理的多代理环境，并采用随机配对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）对抗性图像输入任何随机选择的代理的内存中足以实现传染性越狱。最后，我们推导出一个简单的原则来确定防御机制是否可以证明抑制传染性越狱的传播，但如何设计满足该原则的实用防御仍然是一个有待研究的悬而未决的问题。我们的项目页面位于 https://sail-sg.github.io/Agent-Smith/。</details>
**PDF:** <http://arxiv.org/pdf/2402.08567v1><br />
**Code:** <https://github.com/sail-sg/agent-smith>**<br />
>>**index:** 4<br />
**Title:** **Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks**<br />
**Title_cn:** 视觉问答教学：将多模态大语言模型解锁到特定领域的视觉多任务<br />
**Authors:** Jusung Lee, Sungguk Cha, Younghyun Lee, Cheoljong Yang<br />
**Abstract:** <details><summary>原文: </summary>Having revolutionized natural language processing (NLP) applications, large language models (LLMs) are expanding into the realm of multimodal inputs. Owing to their ability to interpret images, multimodal LLMs (MLLMs) have been primarily used for vision-language tasks. Currently, MLLMs have not yet been extended for domain-specific visual tasks, which require a more explicit understanding of visual information. We developed a method to transform domain-specific visual and vision-language datasets into a unified question answering format called Visual Question Answering Instruction (VQA-IN), thereby extending MLLM to domain-specific tasks. The VQA-IN was applied to train multiple MLLM architectures using smaller versions of LLMs (sLLMs). The experimental results indicated that the proposed method achieved a high score metric on domainspecific visual tasks while also maintaining its performance on vision-language tasks in a multitask manner.</details>
**Abstract_cn:** <details><summary>译文: </summary>大语言模型 (LLM) 彻底改变了自然语言处理 (NLP) 应用，正在扩展到多模式输入领域。由于其解释图像的能力，多模态法学硕士（MLLM）主要用于视觉语言任务。目前，MLLM 尚未扩展到特定领域的视觉任务，这需要对视觉信息有更明确的理解。我们开发了一种方法，将特定领域的视觉和视觉语言数据集转换为统一的问答格式，称为视觉问答指令（VQA-IN），从而将 MLLM 扩展到特定领域的任务。 VQA-IN 用于使用较小版本的 LLM (sLLM) 来训练多个 MLLM 架构。实验结果表明，所提出的方法在特定领域的视觉任务上取得了高分，同时还以多任务方式保持了其在视觉语言任务上的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08360v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs**<br />
**Title_cn:** NeRF 类比：NeRF 基于示例的视觉属性传输<br />
**Authors:** Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel<br />
**Abstract:** <details><summary>原文: </summary>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场 (NeRF) 对 3D 几何形状和场景外观的特定关系进行编码。我们在这里提出的问题是，我们是否可以以语义上有意义的方式将外观从源 NeRF 转移到目标 3D 几何体上，以便生成的新 NeRF 保留目标几何体，但具有与源 NeRF 类似的外观。为此，我们将经典图像类比从 2D 图像推广到 NeRF。我们利用由大型预训练 2D 图像模型的语义特征驱动的语义亲和力的对应传输来实现多视图一致的外观传输。我们的方法允许探索 3D 几何和外观的混合搭配产品空间。我们表明，我们的方法优于传统的基于风格化的方法，并且大多数用户比几个典型的基线更喜欢我们的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.08622v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields**<br />
**Title_cn:** H2O-SDF：使用物体表面场进行 3D 室内重建的两阶段学习<br />
**Authors:** Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Chul Lee<br />
**Abstract:** <details><summary>原文: </summary>Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments. This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects. A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods. Our proposed approach is validated through several experiments that include ablation studies.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用神经辐射场 (NeRF)、有符号距离场 (SDF) 和占用场的先进技术最近已成为 3D 室内场景重建的解决方案。我们引入了一种新颖的两阶段学习方法 H2O-SDF，它可以区分室内环境中的物体和非物体区域。这种方法实现了微妙的平衡，仔细保留了房间布局的几何完整性，同时还捕捉了特定物体复杂的表面细节。我们的两阶段学习框架的基石是引入物体表面场（OSF），这是一个新颖的概念，旨在减轻持续消失的梯度问题，该问题先前阻碍了其他方法中高频细节的捕获。我们提出的方法通过包括消融研究在内的多项实验得到验证。</details>
**PDF:** <http://arxiv.org/pdf/2402.08138v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation**<br />
**Title_cn:** IM-3D：用于高质量 3D 生成的迭代多视图扩散和重建<br />
**Authors:** Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos<br />
**Abstract:** <details><summary>原文: </summary>Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数文本到 3D 生成器都基于经过数十亿图像训练的现成文本到图像模型。他们使用分数蒸馏采样 (SDS) 的变体，这种方法速度慢，有些不稳定，并且容易出现伪影。缓解措施是将 2D 生成器微调为多视图感知，这可以帮助蒸馏或可以与重建网络结合以直接输出 3D 对象。在本文中，我们进一步探索文本转 3D 模型的设计空间。我们通过考虑视频而不是图像生成器来显着改进多视图生成。结合 3D 重建算法（通过使用高斯分布，可以优化基于图像的鲁棒损失），我们可以直接从生成的视图中生成高质量的 3D 输出。我们的新方法 IM-3D 将 2D 生成器网络的评估数量减少了 10-100 倍，从而实现更高效的管道、更好的质量、更少的几何不一致以及更高的可用 3D 资产产量。</details>
**PDF:** <http://arxiv.org/pdf/2402.08682v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **BdSLW60: A Word-Level Bangla Sign Language Dataset**<br />
**Title_cn:** BdSLW60：单词级孟加拉手语数据集<br />
**Authors:** Husne Ara Rubaiyeat, Hasan Mahmud, Ahsan Habib, Md. Kamrul Hasan<br />
**Abstract:** <details><summary>原文: </summary>Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people. However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets. Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on. In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely. The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional. The dataset was rigorously annotated and cross-checked by 60 annotators. We also introduced a unique approach of a relative quantization-based key frame encoding technique for landmark based sign gesture recognition. We report the benchmarking of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and the code base is accessible from https://github.com/hasanssl/BdSLW60_Code.</details>
**Abstract_cn:** <details><summary>译文: </summary>手语话语是聋哑人日常交流的重要方式。然而，孟加拉手语（BdSL）的研究面临着显着的局限性，这主要是由于缺乏数据集。识别 BdSL (WL-BdSL) 中的字级手势面临着诸多挑战，包括需要注释良好的数据集、从面部或手部标志捕获手势的动态性质、开发合适的机器学习或基于深度学习的模型视频样本等。在本文中，我们通过在无约束和自然的环境中创建一个名为 BdSLW60 的综合 BdSL 字级数据集来解决这些挑战，允许位置和时间变化，并允许手语用户自由改变手的优势。该数据集包含 60 个孟加拉语手语，其中包括 18 名手语者在手语专业人士的监督下提供的 9307 个视频试验。该数据集经过 60 名注释者的严格注释和交叉检查。我们还介绍了一种基于相对量化的关键帧编码技术的独特方法，用于基于地标的手势识别。我们报告了使用支持向量机 (SVM) 测试准确率高达 67.6% 的 BdSLW60 数据集和测试准确率高达 75.1% 的基于注意力的双 LSTM 的 BdSLW60 数据集的基准测试。该数据集可从 https://www.kaggle.com/datasets/hasaniut/bdslw60 获取，代码库可从 https://github.com/hasanssl/BdSLW60_Code 访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.08635v1><br />
**Code:** <https://github.com/hasanssl/bdslw60_code>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Convolutional Neural Networks Towards Facial Skin Lesions Detection**<br />
**Title_cn:** 卷积神经网络用于面部皮肤病变检测<br />
**Authors:** Reza Sarshar, Mohammad Heydari, Elham Akhondzadeh Noughabi<br />
**Abstract:** <details><summary>原文: </summary>Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 2300 patient images were extracted from this raw data. A software tool was developed to crop and label lesions, with input from two treatment experts. In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels. Subsequently, a convolutional neural network model was employed for lesion labeling. The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity. Internal validation involved performance indicators and cross-validation, while external validation compared the model's performance indicators with those of the transfer learning method using the Vgg16 deep network model. Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部分析已成为一个具有多种应用的突出研究领域，包括整容手术项目、美容行业、摄影和娱乐。处理患者图像通常需要专业的图像处理软件。这项研究提供了一个模型，有助于通过卷积神经网络和机器学习方法检测面部图像上的瑕疵和皮肤病变。该方法具有结构简单、速度快、适合图像处理等优点，同时避免了传统方法的复杂性。该模型包括四个主要步骤：区域选择、扫描所选区域、病变诊断和标记识别的病变。这项研究的原始数据是从德黑兰一家专门从事护肤和美容服务的知名诊所收集的。该数据集包括管理信息、临床数据以及面部和侧面图像。从该原始数据中总共提取了 2300 张患者图像。根据两位治疗专家的意见，开发了一种软件工具来裁剪和标记病变。在病灶准备阶段，将所选区域标准化为50 * 50像素。随后，采用卷积神经网络模型进行病变标记。该分类模型表现出很高的准确性，健康皮肤的特异性为 0.98，受损皮肤的特异性为 0.97。内部验证涉及性能指标和交叉验证，外部验证则将模型的性能指标与使用Vgg16深度网络模型的迁移学习方法的性能指标进行比较。与现有研究相比，本研究的结果展示了所提出的模型和方法的有效性和可取性。</details>
**PDF:** <http://arxiv.org/pdf/2402.08592v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis**<br />
**Title_cn:** FESS 损失：用于优化医学图像分析的特征增强空间分割损失<br />
**Authors:** Charulkumar Chodvadiya, Navyansh Mahla, Kinshuk Gaurav Singh, Kshitij Sharad Jadhav<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images. Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分割是医学成像领域的一个关键过程，在诊断、治疗和研究中发挥着举足轻重的作用。它涉及将图像划分为多个区域，代表不同的解剖或病理结构。由于传统方法依赖传统损失函数，因此常常面临平衡空间精度和综合特征表示的挑战。为了克服这个问题，我们提出了特征增强的空间分割损失（FESS Loss），它将对比学习（提取复杂的特征，特别是在医学成像的微妙领域）的优点与 Dice 损失固有的空间精度结合起来。目标是增强医学图像分割中的空间精度和基于特征的表示。 FESS 损失标志着一个显着的进步，提供了更准确和更精细的分割过程，最终有助于提高医学图像分析的精度。此外，FESS 损失在医学领域经常出现的有限注释数据可用性场景中表现出了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08582v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Glass Segmentation with Multi Scales and Primary Prediction Guiding**<br />
**Title_cn:** 多尺度玻璃分割和初步预测引导<br />
**Authors:** Zhiyu Xu, Qingliang Chen<br />
**Abstract:** <details><summary>原文: </summary>Glass-like objects can be seen everywhere in our daily life which are very hard for existing methods to segment them. The properties of transparencies pose great challenges of detecting them from the chaotic background and the vague separation boundaries further impede the acquisition of their exact contours. Moving machines which ignore glasses have great risks of crashing into transparent barriers or difficulties in analysing objects reflected in the mirror, thus it is of substantial significance to accurately locate glass-like objects and completely figure out their contours. In this paper, inspired by the scale integration strategy and the refinement method, we proposed a brand-new network, named as MGNet, which consists of a Fine-Rescaling and Merging module (FRM) to improve the ability to extract spatially relationship and a Primary Prediction Guiding module (PPG) to better mine the leftover semantics from the fused features. Moreover, we supervise the model with a novel loss function with the uncertainty-aware loss to produce high-confidence segmentation maps. Unlike the existing glass segmentation models that must be trained on different settings with respect to varied datasets, our model are trained under consistent settings and has achieved superior performance on three popular public datasets. Code is available at</details>
**Abstract_cn:** <details><summary>译文: </summary>玻璃类物体在我们的日常生活中随处可见，现有的方法很难对其进行分割。透明胶片的特性给从混乱的背景中检测它们带来了巨大的挑战，而模糊的分离边界进一步阻碍了其精确轮廓的获取。忽略玻璃的移动机器存在很大的撞上透明障碍物的风险或难以分析镜子中反射的物体，因此准确定位玻璃类物体并完全弄清楚其轮廓具有重要意义。在本文中，受尺度整合策略和细化方法的启发，我们提出了一种全新的网络，命名为MGNet，它由一个用于提高空间关系提取能力的精细重缩放和合并模块（FRM）和一个主要预测指导模块（PPG）可以更好地从融合特征中挖掘剩余语义。此外，我们使用具有不确定性感知损失的新型损失函数来监督模型，以生成高置信度的分割图。与必须在不同数据集的不同设置下进行训练的现有玻璃分割模型不同，我们的模型在一致的设置下进行训练，并在三个流行的公共数据集上取得了优异的性能。代码可在</details>
**PDF:** <http://arxiv.org/pdf/2402.08571v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Approximately Piecewise E(3) Equivariant Point Networks**<br />
**Title_cn:** 近似分段 E(3) 等变点网络<br />
**Authors:** Matan Atzmon, Jiahui Huang, Francis Williams, Or Litany<br />
**Abstract:** <details><summary>原文: </summary>Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent efforts aim to extend networks that are $E(3)$ equivariant, to accommodate inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In practical settings, however, the partitioning into individually transforming regions is unknown a priori. Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry. Past works have proposed different ways to predict the partition, which may exhibit uncontrolled errors in their ability to maintain equivariance to the actual partition. To this end, we introduce APEN: a general framework for constructing approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is that functions that are equivariant with respect to a finer partition will also maintain equivariance in relation to the true partition. Leveraging this observation, we propose a design where the equivariance approximation error at each layers can be bounded solely in terms of (i) uncertainty quantification of the partition prediction, and (ii) bounds on the probability of failing to suggest a proper subpartition of the ground truth one. We demonstrate the effectiveness of APEN using two data types exemplifying part-based symmetry: (i) real-world scans of room scenes containing multiple furniture-type objects; and, (ii) human motions, characterized by articulated parts exhibiting rigid movement. Our empirical results demonstrate the advantage of integrating piecewise $E(3)$ symmetry into network design, showing a distinct improvement in generalization compared to prior works for both classification and segmentation tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>将对称概念集成到点云神经网络中是提高其泛化能力的有效方法。特别令人感兴趣的是 $E(3)$ 等变点云网络，其中应用于输入的欧几里德变换保留在输出中。最近的努力旨在扩展 $E(3)$ 等变的网络，以适应由多个部分组成的输入，每个部分都表现出局部 $E(3)$ 对称性。然而，在实际设置中，划分为单独的变换区域是先验未知的。分区预测中的错误将不可避免地映射到尊重真实输入对称性的错误。过去的工作提出了不同的方法来预测分区，这些方法可能会在维持与实际分区等方差的能力方面表现出不受控制的错误。为此，我们引入 APEN：构建近似分段 $E(3)$ 等变点网络的通用框架。我们的主要见解是，相对于更精细的划分等变的函数也将保持相对于真实划分的等变。利用这一观察结果，我们提出了一种设计，其中每层的等方差近似误差可以仅根据（i）分区预测的不确定性量化和（ii）未能建议正确的子分区的概率来限制。基本事实一。我们使用两种数据类型来证明基于部分的对称性的 APEN 的有效性：（i）包含多个家具类型对象的房间场景的真实世界扫描； (ii) 人体运动，其特征是铰接部件表现出刚性运动。我们的实证结果证明了将分段 $E(3)$ 对称性集成到网络设计中的优势，与分类和分割任务的先前工作相比，在泛化方面显示出明显的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.08529v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation**<br />
**Title_cn:** P-Mamba：将 Perona Malik Diffusion 与 Mamba 结合起来，实现高效的儿科超声心动图左心室分割<br />
**Authors:** Zi Ye, Tianxiang Chen<br />
**Abstract:** <details><summary>原文: </summary>In pediatric cardiology, the accurate and immediate assessment of cardiac function through echocardiography is important since it can determine whether urgent intervention is required in many emergencies. However, echocardiography is characterized by ambiguity and heavy background noise interference, bringing more difficulty to accurate segmentation. Present methods lack efficiency and are also prone to mistakenly segmenting some background noise areas as the left ventricular area due to noise disturbance. To relieve the two issues, we introduce P-Mamba for efficient pediatric echocardiographic left ventricular segmentation. Specifically, we turn to the recently proposed vision mamba layers in our vision mamba encoder branch to improve the computing and memory efficiency of our model while modeling global dependencies. In the other DWT-based PMD encoder branch, we devise DWT-based Perona-Malik Diffusion (PMD) Blocks that utilize PMD for noise suppression, while simultaneously preserving the local shape cues of the left ventricle. Leveraging the strengths of both the two encoder branches, P-Mamba achieves superior accuracy and efficiency to established models, such as vision transformers with quadratic and linear computational complexity. This innovative approach promises significant advancements in pediatric cardiac imaging and beyond.</details>
**Abstract_cn:** <details><summary>译文: </summary>在儿科心脏病学中，通过超声心动图准确、立即评估心功能非常重要，因为它可以确定在许多紧急情况下是否需要紧急干预。然而，超声心动图具有模糊性和背景噪声干扰重的特点，给精确分割带来了较大难度。目前的方法缺乏效率，并且还容易由于噪声干扰而将一些背景噪声区域错误地分割为左心室区域。为了解决这两个问题，我们引入 P-Mamba 进行有效的儿科超声心动图左心室分割。具体来说，我们转向最近在 Vision Mamba 编码器分支中提出的 Vision Mamba 层，以在建模全局依赖项的同时提高模型的计算和内存效率。在另一个基于 DWT 的 PMD 编码器分支中，我们设计了基于 DWT 的 Perona-Malik 扩散 (PMD) 块，利用 PMD 进行噪声抑制，同时保留左心室的局部形状线索。利用两个编码器分支的优势，P-Mamba 实现了优于已建立模型的精度和效率，例如具有二次和线性计算复杂性的视觉变换器。这种创新方法有望在儿科心脏成像及其他领域取得重大进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.08506v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models**<br />
**Title_cn:** 视觉语言 Transformer 模型的零样本评估和系统评估之间的有趣差异<br />
**Authors:** Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu, Lingjiong Zhu<br />
**Abstract:** <details><summary>原文: </summary>Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs. We also propose a robust way to detect the modified images.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 Transformer 的模型由于其在基准数据集上的卓越（零样本）性能，在过去几年中占据了自然语言处理和其他领域的主导地位。然而，由于这些模型的复杂性和规模，人们对其了解甚少。虽然基于探测的方法被广泛用于理解特定属性，但表示空间的结构并未得到系统的表征；因此，尚不清楚这些模型如何泛化和过度泛化到数据集之外的新输入。在本文中，基于一种新的梯度下降优化方法，我们能够探索常用视觉语言模型的嵌入空间。使用 Imagenette 数据集，我们表明，虽然该模型实现了超过 99% 的零样本分类性能，但它完全未能通过系统评估。使用线性近似，我们提供了一个框架来解释显着的差异。我们还使用不同的模型获得了类似的结果，以支持我们的结果适用于具有连续输入的其他变压器模型。我们还提出了一种稳健的方法来检测修改后的图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.08473v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Latent space configuration for improved generalization in supervised autoencoder neural networks**<br />
**Title_cn:** 用于改进监督自动编码器神经网络泛化的潜在空间配置<br />
**Authors:** Nikita Gabdullin<br />
**Abstract:** <details><summary>原文: </summary>Autoencoders (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS). Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly. In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration. The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration. We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for supervised AE (SAE). Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers. We also show that this leads to more stable and interpretable training. We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to evaluate similarity for unseen classes. We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动编码器 (AE) 是一类简单但功能强大的神经网络，它通过将输入投影到低维潜在空间 (LS) 来压缩数据。虽然LS是根据训练过程中损失函数最小化形成的，但其属性和拓扑不受直接控制。在本文中，我们重点关注 AE LS 属性，并提出了两种获得具有所需拓扑的 LS 的方法，称为 LS 配置。所提出的方法包括使用直接作用于 LS 的几何损失项的损失配置和编码器配置。我们表明，前者可以通过定义监督 AE (SAE) 的 LS 簇的位置和形状来可靠地获得具有所需配置的 LS。了解 LS 配置允许在 LS 中定义相似性度量，以预测标签或估计多个输入的相似性，而无需使用解码器或分类器。我们还表明，这会带来更稳定和可解释的训练。我们表明，使用所提出的方法进行衣服纹理分类训练的 SAE 可以很好地推广到来自 LIP、Market1501 和 WildTrack 数据集的未见数据，而无需进行微调，甚至允许评估未见类别的相似性。我们进一步说明了预配置的 LS 相似性估计与跨数据集搜索和使用不带语言模型的文本查询的基于文本的搜索的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.08441v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Camera Calibration through Geometric Constraints from Rotation and Projection Matrices**<br />
**Title_cn:** 通过旋转和投影矩阵的几何约束进行相机校准<br />
**Authors:** Muhammad Waleed, Abdul Rauf, Murtaza Taj<br />
**Abstract:** <details><summary>原文: </summary>The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality. In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(\theta_p)$) camera parameters. Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix. Thus we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a multitask learning framework. Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix. This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process. Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions. By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) benchmarks. The code and the updated dataset can be found here: https://github.com/CVLABLUMS/CVGL-Camera-Calibration</details>
**Abstract_cn:** <details><summary>译文: </summary>相机标定过程涉及估计内在和外在参数，这对于准确执行 3D 重建、对象跟踪和增强现实等任务至关重要。在这项工作中，我们提出了一种新颖的基于约束的损失来测量内在（焦距：$（f_x，f_y）$和主点：$（p_x，p_y）$）和外在（基线：（$b$），视差：($d$)，平移：$(t_x, t_y, t_z)$，旋转（特别是俯仰）：$(\theta_p)$)相机参数。我们的新颖约束基于相机模型固有的几何属性，包括投影矩阵的解剖结构（消失点、世界原点图像、轴平面）和旋转矩阵的正交性。因此，我们通过多任务学习框架提出了一种新颖的无监督几何约束损失（UGCL）。我们的方法是一种混合方法，利用神经网络的学习能力来估计所需的参数以及相机投影矩阵固有的基础数学特性。这种独特的方法不仅增强了模型的可解释性，而且还促进了更明智的学习过程。此外，我们还引入了新的 CVGL 相机校准数据集，其中包含 900 多种相机参数配置，包含 63,600 个密切反映现实世界条件的图像对。通过对合成数据集和真实数据集进行训练和测试，我们提出的方法与最先进的 (SOTA) 基准相比，展示了所有参数的改进。代码和更新的数据集可以在这里找到：https://github.com/CVLABLUMS/CVGL-Camera-Calibration</details>
**PDF:** <http://arxiv.org/pdf/2402.08437v1><br />
**Code:** <https://github.com/cvlablums/cvgl-camera-calibration>**<br />
>>**index:** 9<br />
**Title:** **Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection**<br />
**Title_cn:** 利用自监督实例对比学习进行雷达目标检测<br />
**Authors:** Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin<br />
**Abstract:** <details><summary>原文: </summary>In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors. Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors. We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning. We aim to pre-train an object detector's backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar mAP@0.5 than a supervised approach using the whole training set.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，在对更安全、更自主的运输系统的需求的推动下，汽车行业已转向集成越来越多的高级驾驶辅助系统 (ADAS)。在用于物体识别任务的传感器阵列中，雷达传感器因其在恶劣天气条件或弱光场景下的能力以及在不同环境下保持一致性能的鲁棒性而成为强大的竞争者。然而，雷达数据集的小规模和这些数据标记的复杂性限制了雷达目标探测器的性能。在计算机视觉中自监督学习的有希望的结果的推动下，本文提出了 RiCL，一种用于预训练雷达目标检测器的实例对比学习框架。我们建议利用雷达的检测和时间信息，使用对比学习以自监督的方式预训练雷达目标检测模型。我们的目标是预训练物体检测器的主干、头部和颈部，以使用更少的数据进行学习。 CARRADA 和 RADDet 数据集上的实验表明了我们的方法在学习距离多普勒图中对象的通用表示方面的有效性。值得注意的是，我们的预训练策略允许我们仅使用 20% 的标记数据来达到与使用整个训练集的监督方法类似的 mAP@0.5。</details>
**PDF:** <http://arxiv.org/pdf/2402.08427v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging**<br />
**Title_cn:** 传输超高场表示以进行低场磁共振成像强度引导脑分割<br />
**Authors:** Kwanseok Oh, Jieun Lee, Da-Woon Heo, Dinggang Shen, Heung-Il Suk<br />
**Abstract:** <details><summary>原文: </summary>Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast. However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI. This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment. Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features. Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on LF features. Beyond such advantages, this strategy can seamlessly be utilized by modulating the contrast of LF features in alignment with UHF guidance, even when employing arbitrary segmentation models. Exhaustive experiments demonstrated that the proposed method significantly outperformed all baseline models on both brain tissue and whole-brain segmentation tasks; further, it exhibited remarkable adaptability and scalability by successfully integrating diverse segmentation models and tasks. These improvements were not only quantifiable but also visible in the superlative visual quality of segmentation masks.</details>
**Abstract_cn:** <details><summary>译文: </summary>超高场 (UHF) 磁共振成像 (MRI)，即 7T MRI，由于其增强的信噪比和磁敏感诱导对比度，可提供内部大脑结构的卓越解剖细节。然而，与低场 (LF) MRI 相比，7T MRI 的广泛使用因其高成本和较低的可及性而受到限制。本研究提出了一种深度学习框架，该框架系统地将输入的 LF 磁共振特征表示与推断的类 7T 特征表示融合起来，用于在 7T 缺失的环境中执行大脑图像分割任务。具体来说，我们的自适应融合模块通过预先训练的网络聚合从 LF 图像导出的类似 7T 的特征，然后将它们细化为可有效吸收 UHF 引导到 LF 图像特征中。使用从这种聚合和同化中获得的强度引导特征，分割模型可以识别仅依赖 LF 特征时通常难以识别的微妙结构表示。除了这些优点之外，即使在采用任意分割模型时，也可以通过根据 UHF 引导调整 LF 特征的对比度来无缝地利用该策略。详尽的实验表明，所提出的方法在脑组织和全脑分割任务上显着优于所有基线模型；此外，它通过成功集成不同的细分模型和任务，表现出卓越的适应性和可扩展性。这些改进不仅是可量化的，而且在分割蒙版的最高视觉质量中也是可见的。</details>
**PDF:** <http://arxiv.org/pdf/2402.08409v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing**<br />
**Title_cn:** 使用随机平滑进行分段的自适应分层认证<br />
**Authors:** Alaa Anani, Tobias Lorenz, Bernt Schiele, Mario Fritz<br />
**Abstract:** <details><summary>原文: </summary>Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing into a coarser hierarchy level, we introduce a novel evaluation paradigm for adaptive hierarchical certification, namely the certified information gain metric, which is proportional to the class granularity level. Our evaluation experiments on real-world challenging datasets such as Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher certified information gain and a lower abstain rate compared to the current state-of-the-art certification method, as well as other non-adaptive versions of it.</details>
**Abstract_cn:** <details><summary>译文: </summary>常见的认证方法在一组扁平的预定义细粒度类上运行。然而，在本文中，我们提出了一种新颖的、更通用的、实用的设置，即图像语义分割的自适应分层认证。在此设置中，认证可以在由细到粗级别组成的多级分层标签空间内。与传统方法中放弃不稳定组件的认证不同，我们的方法自适应地将认证放宽到层次结构中较粗的级别。这种放宽降低了弃权率，同时提供了经过认证的语义上有意义的信息。我们以数学方式表述了问题设置，并首次引入了一种用于图像语义分割的自适应分层认证算法，该算法可以认证层次结构内的图像像素并证明其保证的正确性。由于认证的准确性在遍历到较粗的层次结构级别时没有考虑信息的丢失，因此我们引入了一种用于自适应层次认证的新颖的评估范式，即认证的信息增益度量，它与类粒度级别成正比。我们对现实世界中具有挑战性的数据集（例如 Cityscapes 和 ACDC）进行的评估实验表明，与当前最先进的认证方法以及其他非认证方法相比，我们的自适应算法实现了更高的认证信息增益和更低的弃权率。它的自适应版本。</details>
**PDF:** <http://arxiv.org/pdf/2402.08400v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Visually Dehallucinative Instruction Generation**<br />
**Title_cn:** 视觉幻觉指令生成<br />
**Authors:** Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang<br />
**Abstract:** <details><summary>原文: </summary>In recent years, synthetic visual instructions by generative language model have demonstrated plausible text generation performance on the visual question-answering tasks. However, challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents. This paper presents a novel and scalable method for generating visually dehallucinative instructions, dubbed CAP2QA, that constrains the scope to only image contents. Our key contributions lie in introducing image-aligned instructive QA dataset CAP2QA-COCO and its scalable recipe. In our experiments, we compare synthetic visual instruction datasets that share the same source data by visual instruction tuning and conduct general visual recognition tasks. It shows that our proposed method significantly reduces visual hallucination while consistently improving visual recognition ability and expressiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，生成语言模型的合成视觉指令在视觉问答任务中展示了合理的文本生成性能。然而，生成语言模型的幻觉仍然存在挑战，即生成的图像文本数据包含非预期内容。本文提出了一种新颖且可扩展的方法，用于生成视觉去幻觉指令，称为 CAP2QA，该方法将范围限制为仅图像内容。我们的主要贡献在于引入图像对齐的指导性 QA 数据集 CAP2QA-COCO 及其可扩展的配方。在我们的实验中，我们通过视觉指令调整来比较共享相同源数据的合成视觉指令数据集，并执行一般的视觉识别任务。它表明我们提出的方法显着减少了视幻觉，同时持续提高了视觉识别能力和表现力。</details>
**PDF:** <http://arxiv.org/pdf/2402.08348v1><br />
**Code:** <https://github.com/ncsoft/cap2qa>**<br />
>>**index:** 13<br />
**Title:** **Conditional Information Gain Trellis**<br />
**Title_cn:** 条件信息增益网格<br />
**Authors:** Ufuk Can Bicici, Tuna Han Salih Meral, Lale Akarun<br />
**Abstract:** <details><summary>原文: </summary>Conditional computing processes an input using only part of the neural network's computational units. Learning to execute parts of a deep convolutional network by routing individual samples has several advantages: Reducing the computational burden is an obvious advantage. Furthermore, if similar classes are routed to the same path, that part of the network learns to discriminate between finer differences and better classification accuracies can be attained with fewer parameters. Recently, several papers have exploited this idea to take a particular child of a node in a tree-shaped network or to skip parts of a network. In this work, we follow a Trellis-based approach for generating specific execution paths in a deep convolutional neural network. We have designed routing mechanisms that use differentiable information gain-based cost functions to determine which subset of features in a convolutional layer will be executed. We call our method Conditional Information Gain Trellis (CIGT). We show that our conditional execution mechanism achieves comparable or better model performance compared to unconditional baselines, using only a fraction of the computational resources.</details>
**Abstract_cn:** <details><summary>译文: </summary>条件计算仅使用神经网络计算单元的一部分来处理输入。通过路由单个样本来学习执行深度卷积网络的各个部分有几个优点： 减少计算负担是一个明显的优点。此外，如果相似的类被路由到相同的路径，则网络的该部分学会区分更细微的差异，并且可以使用更少的参数获得更好的分类精度。最近，有几篇论文利用了这个想法来获取树形网络中节点的特定子节点或跳过网络的某些部分。在这项工作中，我们遵循基于网格的方法在深度卷积神经网络中生成特定的执行路径。我们设计了路由机制，使用基于可微信息增益的成本函数来确定将执行卷积层中的哪些特征子集。我们将我们的方法称为条件信息增益网格（CIGT）。我们表明，与无条件基线相比，我们的条件执行机制仅使用一小部分计算资源即可实现可比或更好的模型性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08345v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images**<br />
**Title_cn:** 基于涂鸦的快速弱监督和交互式校正，用于分割整个幻灯片图像<br />
**Authors:** Antoine Habis, Roy Rosman Nathanson, Vannary Meas-Yedid, Elsa D. Angelini, Jean-Christophe Olivo-Marin<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes a dynamic interactive and weakly supervised segmentation method with minimal user interactions to address two major challenges in the segmentation of whole slide histopathology images. First, the lack of hand-annotated datasets to train algorithms. Second, the lack of interactive paradigms to enable a dialogue between the pathologist and the machine, which can be a major obstacle for use in clinical routine.   We therefore propose a fast and user oriented method to bridge this gap by giving the pathologist control over the final result while limiting the number of interactions needed to achieve a good result (over 90\% on all our metrics with only 4 correction scribbles).</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种动态交互式和弱监督的分割方法，以最少的用户交互来解决整个幻灯片组织病理学图像分割的两个主要挑战。首先，缺乏用于训练算法的手工注释数据集。其次，缺乏交互式范例来实现病理学家和机器之间的对话，这可能是在临床常规中使用的主要障碍。因此，我们提出了一种快速且面向用户的方法来弥合这一差距，让病理学家能够控制最终结果，同时限制获得良好结果所需的交互次数（我们所有指标的 90% 以上，仅需要 4 个校正涂鸦）。</details>
**PDF:** <http://arxiv.org/pdf/2402.08333v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **The Paradox of Motion: Evidence for Spurious Correlations in Skeleton-based Gait Recognition Models**<br />
**Title_cn:** 运动悖论：基于骨骼的步态识别模型中虚假相关性的证据<br />
**Authors:** Andy Cătrună, Adrian Cosma, Emilian Rădoi<br />
**Abstract:** <details><summary>原文: </summary>Gait, an unobtrusive biometric, is valued for its capability to identify individuals at a distance, across external outfits and environmental conditions. This study challenges the prevailing assumption that vision-based gait recognition, in particular skeleton-based gait recognition, relies primarily on motion patterns, revealing a significant role of the implicit anthropometric information encoded in the walking sequence. We show through a comparative analysis that removing height information leads to notable performance degradation across three models and two benchmarks (CASIA-B and GREW). Furthermore, we propose a spatial transformer model processing individual poses, disregarding any temporal information, which achieves unreasonably good accuracy, emphasizing the bias towards appearance information and indicating spurious correlations in existing benchmarks. These findings underscore the need for a nuanced understanding of the interplay between motion and appearance in vision-based gait recognition, prompting a reevaluation of the methodological assumptions in this field. Our experiments indicate that "in-the-wild" datasets are less prone to spurious correlations, prompting the need for more diverse and large scale datasets for advancing the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>步态是一种不引人注目的生物识别技术，因其能够在远距离、跨外部服装和环境条件下识别个人的能力而受到重视。这项研究挑战了普遍的假设，即基于视觉的步态识别，特别是基于骨骼的步态识别，主要依赖于运动模式，揭示了步行序列中编码的隐式人体测量信息的重要作用。我们通过比较分析表明，删除高度信息会导致三个模型和两个基准（CASIA-B 和 GREW）的性能显着下降。此外，我们提出了一种处理个体姿势的空间变换器模型，忽略任何时间信息，从而实现了不合理的良好准确性，强调了对外观信息的偏差并表明现有基准中的虚假相关性。这些发现强调需要对基于视觉的步态识别中运动和外观之间的相互作用进行细致入微的理解，从而促使人们重新评估该领域的方法论假设。我们的实验表明，“野外”数据集不太容易出现虚假相关性，这促使需要更多样化和大规模的数据集来推进该领域。</details>
**PDF:** <http://arxiv.org/pdf/2402.08320v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Rethinking U-net Skip Connections for Biomedical Image Segmentation**<br />
**Title_cn:** 重新思考用于生物医学图像分割的 U-net Skip Connections<br />
**Authors:** Frauke Wilm, Jonas Ammeling, Mathias Öttl, Rutger H. J. Fick, Marc Aubreville, Katharina Breininger<br />
**Abstract:** <details><summary>原文: </summary>The U-net architecture has significantly impacted deep learning-based segmentation of medical images. Through the integration of long-range skip connections, it facilitated the preservation of high-resolution features. Out-of-distribution data can, however, substantially impede the performance of neural networks. Previous works showed that the trained network layers differ in their susceptibility to this domain shift, e.g., shallow layers are more affected than deeper layers. In this work, we investigate the implications of this observation of layer sensitivity to domain shifts of U-net-style segmentation networks. By copying features of shallow layers to corresponding decoder blocks, these bear the risk of re-introducing domain-specific information. We used a synthetic dataset to model different levels of data distribution shifts and evaluated the impact on downstream segmentation performance. We quantified the inherent domain susceptibility of each network layer, using the Hellinger distance. These experiments confirmed the higher domain susceptibility of earlier network layers. When gradually removing skip connections, a decrease in domain susceptibility of deeper layers could be observed. For downstream segmentation performance, the original U-net outperformed the variant without any skip connections. The best performance, however, was achieved when removing the uppermost skip connection - not only in the presence of domain shifts but also for in-domain test data. We validated our results on three clinical datasets - two histopathology datasets and one magnetic resonance dataset - with performance increases of up to 10% in-domain and 13% cross-domain when removing the uppermost skip connection.</details>
**Abstract_cn:** <details><summary>译文: </summary>U-net 架构对基于深度学习的医学图像分割产生了重大影响。通过集成远程跳跃连接，它有助于保存高分辨率特征。然而，分布外的数据可能会严重阻碍神经网络的性能。之前的工作表明，经过训练的网络层对这种域转移的敏感性有所不同，例如，浅层比深层受到的影响更大。在这项工作中，我们研究了 U-net 式分割网络的层敏感性对域转移的观察的影响。通过将浅层的特征复制到相应的解码器块，这些特征面临着重新引入特定领域信息的风险。我们使用合成数据集对不同级别的数据分布变化进行建模，并评估对下游细分性能的影响。我们使用 Hellinger 距离量化每个网络层的固有域敏感性。这些实验证实了早期网络层具有更高的域敏感性。当逐渐删除跳跃连接时，可以观察到更深层的域敏感性降低。对于下游分割性能，原始 U 网的性能优于没有任何跳过连接的变体。然而，当删除最上面的跳过连接时，可以获得最佳性能 - 不仅在存在域移位的情况下，而且对于域内测试数据也是如此。我们在三个临床数据集（两个组织病理学数据集和一个磁共振数据集）上验证了我们的结果，当删除最上面的跳跃连接时，域内性能提高了 10%，跨域性能提高了 13%。</details>
**PDF:** <http://arxiv.org/pdf/2402.08276v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss**<br />
**Title_cn:** 通过辅助损失优化编码器来改进机器的图像编码<br />
**Authors:** Kei Iino, Shunsuke Akamatsu, Hiroshi Watanabe, Shohei Enomoto, Akira Sakamoto, Takeharu Eda<br />
**Abstract:** <details><summary>原文: </summary>Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training method.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器图像编码 (ICM) 旨在使用识别模型而不是人类视觉来压缩图像以进行机器分析。因此，在 ICM 中，编码器识别并压缩机器识别任务所需的信息非常重要。学习 ICM 有两种主要方法；基于任务损失和感兴趣区域（ROI）的比特分配优化压缩模型。这些方法为编码器提供了识别能力。然而，当识别模型很深时，任务丢失的优化变得很困难，并且基于 ROI 的方法在评估过程中通常会涉及额外的开销。在这项研究中，我们提出了一种新的学习 ICM 模型训练方法，该方法将辅助损失应用于编码器，以提高其识别能力和率失真性能。与传统训练方法相比，我们的方法在目标检测和语义分割任务中实现了 27.7% 和 20.3% 的 Bjontegaard Delta 率提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.08267v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles**<br />
**Title_cn:** 使用无人机深度学习进行热图像中的物体检测<br />
**Authors:** Minh Dang Tu, Kieu Trang Le, Manh Duong Phung<br />
**Abstract:** <details><summary>原文: </summary>This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that our model achieves a real-time computation speed with a stability rate of over 90%.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出了一种神经网络模型，能够识别无人机收集的热图像中的小型和微小物体。我们的模型由三部分组成：主干、颈部和预测头部。主干是基于YOLOv5的结构开发的，并在末端结合使用了Transformer编码器。颈部包括一个 BI-FPN 块，结合使用滑动窗口和变压器来增加输入预测头的信息。预测头通过使用 Sigmoid 函数评估特征图来进行检测。使用具有注意力和滑动窗口的变压器可以提高识别精度，同时使模型保持合理的参数数量和嵌入式系统的计算要求。在公共数据集 VEDAI 和我们收集的数据集上进行的实验表明，我们的模型比 ResNet、Faster RCNN、ComNet、ViT、YOLOv5、SMPNet 和 DPNetV3 等最先进的方法具有更高的准确性。在嵌入式计算机Jetson AGX上的实验表明，我们的模型实现了实时计算速度，稳定率超过90%。</details>
**PDF:** <http://arxiv.org/pdf/2402.08251v1><br />
**Code:** null<br />

>## **GNN**
>---
>>**index:** 1<br />
**Title:** **Pix2Code: Learning to Compose Neural Visual Concepts as Programs**<br />
**Title_cn:** Pix2Code：学习将神经视觉概念编写为程序<br />
**Authors:** Antonia Wüst, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting<br />
**Abstract:** <details><summary>原文: </summary>The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and concept configurations. Particularly, in stark contrast to neural approaches, we show that Pix2Code's representations remain human interpretable and can be easily revised for improved performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>以无监督的方式从图像中学习抽象概念的挑战在于需要整合视觉感知和可概括的关系推理。此外，这项任务的无监督性质使得人类用户有必要能够理解模型学到的概念并可能纠正错误行为。为了解决视觉概念学习的普遍性和可解释性限制，我们提出了 Pix2Code，这是一个通过利用显式、组合符号和隐式神经表示的能力将程序合成扩展到视觉关系推理的框架。这是通过从图像中检索对象表示并将关系概念合成为 lambda 演算程序来实现的。我们在具有挑战性的推理领域、康定斯基模式和 CURI 上评估 Pix2Code 的不同属性，从而测试其识别可推广到新数据和概念配置的组合视觉概念的能力。特别是，与神经方法形成鲜明对比的是，我们表明 Pix2Code 的表示仍然是人类可解释的，并且可以轻松修改以提高性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08280v1><br />
**Code:** <https://github.com/ml-research/pix2code>**<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance**<br />
**Title_cn:** 通过无分类器指导减轻大视觉语言模型中的物体幻觉<br />
**Authors:** Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu<br />
**Abstract:** <details><summary>原文: </summary>The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.</details>
**Abstract_cn:** <details><summary>译文: </summary>大视觉语言模型（LVLM）的进步日益凸显了它们容易产生图像中不存在物体的幻觉这一关键问题。为了解决这个问题，以前的工作重点是使用专门策划的数据集或强大的 LLM（例如 GPT-3.5）来纠正 LVLM 的输出。然而，这些方法需要昂贵的训练/微调或 API 访问高级 LLM 来纠正模型的生成后输出。在本文中，我们通过引入一个名为 Mitigating幻觉通过无分类引导（MARINE）的框架来应对这一挑战，该框架既无需训练，也无需API，并且可以有效且高效地减少生成过程中的物体幻觉。具体来说，MARINE 通过集成现有的开源视觉模型丰富了 LVLM 的视觉环境，并采用无分类器引导来合并额外的对象接地功能，以提高 LVLM 各代的精度。通过使用多种评估指标对 6 美元流行的 LVLM 进行综合评估，我们证明了 MARINE 的有效性，它甚至优于现有的基于微调的方法。值得注意的是，根据 GPT-4V 的评估，它不仅减少了幻觉，还提高了 LVLM 生成的细节。</details>
**PDF:** <http://arxiv.org/pdf/2402.08680v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Are Semi-Dense Detector-Free Methods Good at Matching Local Features?**<br />
**Title_cn:** 半密集无检测器方法是否擅长匹配局部特征？<br />
**Authors:** Matthieu Vilain, Rémi Giraud, Hugo Germain, Guillaume Bourmaud<br />
**Abstract:** <details><summary>原文: </summary>Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods. Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography. Our code will be made available.</details>
**Abstract_cn:** <details><summary>译文: </summary>半密集无检测器方法（SDF），例如 LoFTR，是目前最流行的图像匹配方法之一。虽然 SDF 方法经过训练来建立两个图像之间的对应关系，但它们的性能几乎完全是使用相对姿态估计指标来评估的。因此，迄今为止，它们建立对应关系的能力与所得到的估计姿势的质量之间的联系很少受到关注。本文是对这一环节研究的首次尝试。我们首先提出一种新颖的基于注意力的结构化图像匹配架构（SAM）。它允许我们在两个数据集（MegaDepth 和 HPatches）上显示反直觉的结果：一方面 SAM 在位姿/单应性估计指标方面优于或与 SDF 方法相当，但另一方面 SDF 方法是匹配精度明显优于 SAM。然后，我们建议将匹配精度的计算限制在纹理区域，并表明在这种情况下 SAM 通常优于 SDF 方法。我们的研究结果强调了在纹理区域建立准确对应关系的能力与所得到的估计姿势/单应性的准确性之间存在很强的相关性。我们的代码将可供使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.08671v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Peeking Behind the Curtains of Residual Learning**<br />
**Title_cn:** 窥视残差学习的幕后<br />
**Authors:** Tunhou Zhang, Feng Yan, Hai Li, Yiran Chen<br />
**Abstract:** <details><summary>原文: </summary>The utilization of residual learning has become widespread in deep and scalable neural nets. However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability. In this paper, we peek behind the curtains of residual learning by uncovering the "dissipating inputs" phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations. We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution. With our theoretical discoveries, we propose "The Plain Neural Net Hypothesis" (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establishes a paradigm to support the training of deep plain neural nets devoid of residual connections. We thoroughly evaluate PNNH-enabled CNN architectures and Transformers on popular vision benchmarks, showing on-par accuracy, up to 0.3% higher training throughput, and 2x better parameter efficiency compared to ResNets and vision Transformers.</details>
**Abstract_cn:** <details><summary>译文: </summary>残差学习的利用在深度且可扩展的神经网络中已经变得广泛。然而，有助于残差学习成功的基本原理仍然难以捉摸，从而阻碍了具有深度可扩展性的普通网络的有效训练。在本文中，我们通过揭示导致普通神经网络收敛失败的“耗散输入”现象来窥视残差学习的幕后：由于非线性，输入通过普通层逐渐受到损害，从而导致学习特征的挑战交涉。我们从理论上证明了普通神经网络如何将输入退化为随机噪声，并强调了残差连接作为解决方案维持更好的存活神经元下限的重要性。凭借我们的理论发现，我们提出了“普通神经网络假说”（PNNH），它将跨非线性层的内部路径确定为残差学习中最关键的部分，并建立了一个范式来支持缺乏深度普通神经网络的训练的剩余连接。我们在流行的视觉基准上全面评估了支持 PNNH 的 CNN 架构和 Transformer，结果显示与 ResNet 和视觉 Transformer 相比，准确度相当，训练吞吐量提高了 0.3%，参数效率提高了 2 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.08645v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild**<br />
**Title_cn:** CrossGaze：野外 3D 视线估计的强大方法<br />
**Authors:** Andy Cătrună, Adrian Cosma, Emilian Rădoi<br />
**Abstract:** <details><summary>原文: </summary>Gaze estimation, the task of predicting where an individual is looking, is a critical task with direct applications in areas such as human-computer interaction and virtual reality. Estimating the direction of looking in unconstrained environments is difficult, due to the many factors that can obscure the face and eye regions. In this work we propose CrossGaze, a strong baseline for gaze estimation, that leverages recent developments in computer vision architectures and attention-based modules. Unlike previous approaches, our method does not require a specialised architecture, utilizing already established models that we integrate in our architecture and adapt for the task of 3D gaze estimation. This approach allows for seamless updates to the architecture as any module can be replaced with more powerful feature extractors. On the Gaze360 benchmark, our model surpasses several state-of-the-art methods, achieving a mean angular error of 9.94 degrees. Our proposed model serves as a strong foundation for future research and development in gaze estimation, paving the way for practical and accurate gaze prediction in real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>凝视估计是预测个人正在看向哪里的任务，是一项关键任务，可直接应用于人机交互和虚拟现实等领域。由于许多因素会遮挡面部和眼睛区域，因此在不受约束的环境中估计注视方向很困难。在这项工作中，我们提出了 CrossGaze，这是一个强大的注视估计基线，它利用了计算机视觉架构和基于注意力的模块的最新发展。与以前的方法不同，我们的方法不需要专门的架构，而是利用我们集成到架构中的已经建立的模型并适应 3D 凝视估计的任务。这种方法允许对架构进行无缝更新，因为任何模块都可以替换为更强大的特征提取器。在 Gaze360 基准测试中，我们的模型超越了几种最先进的方法，实现了 9.94 度的平均角度误差。我们提出的模型为未来注视估计的研究和开发奠定了坚实的基础，为现实场景中实用且准确的注视预测铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.08316v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain**<br />
**Title_cn:** MetaTra：​​用于未知领域广义轨迹预测的元学习<br />
**Authors:** Xiaohe Li, Feilong Huang, Zide Fan, Fangli Mou, Yingyan Hou, Chen Qian, Lijie Wen<br />
**Abstract:** <details><summary>原文: </summary>Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra. This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.</details>
**Abstract_cn:** <details><summary>译文: </summary>轨迹预测在自动驾驶和机器人导航等不同领域引起了广泛关注。然而，由于不同场景下的轨迹模式存在显着差异，在已知环境中训练的模型通常会在未见过的环境中出现问题。为了学习一个可以直接处理看不见的领域而不需要任何模型更新的广义模型，我们提出了一种新颖的基于元学习的轨迹预测方法，称为 MetaTra。该方法结合了双轨迹变压器（Dual-TT），可以彻底探索不同场景中的个体意图和群体运动模式内的相互作用。在此基础上，我们提出了一个元学习框架来模拟源域和目标域之间的泛化过程。此外，为了增强预测结果的稳定性，我们提出了串行和并行训练（SPT）策略以及名为 MetaMix 的特征增强方法。在多个真实世界数据集上的实验结果证实，MetaTra 不仅超越了其他最先进的方法，而且还表现出即插即用的能力，特别是在领域泛化领域。</details>
**PDF:** <http://arxiv.org/pdf/2402.08221v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Translating Images to Road Network:A Non-Autoregressive Sequence-to-Sequence Approach**<br />
**Title_cn:** 将图像转换为道路网络：一种非自回归序列到序列方法<br />
**Authors:** Jiachen Lu, Renyuan Peng, Xinyue Cai, Hang Xu, Hongyang Li, Feng Wen, Wei Zhang, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>The extraction of road network is essential for the generation of high-definition maps since it enables the precise localization of road landmarks and their interconnections. However, generating road network poses a significant challenge due to the conflicting underlying combination of Euclidean (e.g., road landmarks location) and non-Euclidean (e.g., road topological connectivity) structures. Existing methods struggle to merge the two types of data domains effectively, but few of them address it properly. Instead, our work establishes a unified representation of both types of data domain by projecting both Euclidean and non-Euclidean data into an integer series called RoadNet Sequence. Further than modeling an auto-regressive sequence-to-sequence Transformer model to understand RoadNet Sequence, we decouple the dependency of RoadNet Sequence into a mixture of auto-regressive and non-autoregressive dependency. Building on this, our proposed non-autoregressive sequence-to-sequence approach leverages non-autoregressive dependencies while fixing the gap towards auto-regressive dependencies, resulting in success on both efficiency and accuracy. Extensive experiments on nuScenes dataset demonstrate the superiority of RoadNet Sequence representation and the non-autoregressive approach compared to existing state-of-the-art alternatives. The code is open-source on https://github.com/fudan-zvg/RoadNetworkTRansformer.</details>
**Abstract_cn:** <details><summary>译文: </summary>道路网络的提取对于生成高清地图至关重要，因为它可以精确定位道路地标及其互连。然而，由于欧几里德（例如，道路地标位置）和非欧几里德（例如，道路拓扑连通性）结构的潜在组合相互冲突，生成道路网络提出了重大挑战。现有的方法很难有效地合并两种类型的数据域，但很少能正确解决这个问题。相反，我们的工作通过将欧几里德和非欧几里德数据投影到称为 RoadNet 序列的整数序列中，建立了两种类型数据域的统一表示。除了对自回归序列到序列 Transformer 模型进行建模以理解 RoadNet 序列之外，我们还将 RoadNet 序列的依赖性解耦为自回归和非自回归依赖性的混合。在此基础上，我们提出的非自回归序列到序列方法利用非自回归依赖性，同时修复与自回归依赖性的差距，从而在效率和准确性上取得成功。 nuScenes 数据集上的大量实验证明了 RoadNet 序列表示和非自回归方法与现有最先进替代方法相比的优越性。该代码在 https://github.com/fudan-zvg/RoadNetworkTRansformer 上开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.08207v1><br />
**Code:** <https://github.com/fudan-zvg/roadnetworktransformer>**<br />
>>**index:** 6<br />
**Title:** **Optimized Information Flow for Transformer Tracking**<br />
**Title_cn:** 变压器跟踪的优化信息流<br />
**Authors:** Janani Kugarajeevan, Thanikasalam Kokul, Amirthalingam Ramanan, Subha Fernando<br />
**Abstract:** <details><summary>原文: </summary>One-stream Transformer trackers have shown outstanding performance in challenging benchmark datasets over the last three years, as they enable interaction between the target template and search region tokens to extract target-oriented features with mutual guidance. Previous approaches allow free bidirectional information flow between template and search tokens without investigating their influence on the tracker's discriminative capability. In this study, we conducted a detailed study on the information flow of the tokens and based on the findings, we propose a novel Optimized Information Flow Tracking (OIFTrack) framework to enhance the discriminative capability of the tracker. The proposed OIFTrack blocks the interaction from all search tokens to target template tokens in early encoder layers, as the large number of non-target tokens in the search region diminishes the importance of target-specific features. In the deeper encoder layers of the proposed tracker, search tokens are partitioned into target search tokens and non-target search tokens, allowing bidirectional flow from target search tokens to template tokens to capture the appearance changes of the target. In addition, since the proposed tracker incorporates dynamic background cues, distractor objects are successfully avoided by capturing the surrounding information of the target. The OIFTrack demonstrated outstanding performance in challenging benchmarks, particularly excelling in the one-shot tracking benchmark GOT-10k, achieving an average overlap of 74.6\%. The code, models, and results of this work are available at \url{https://github.com/JananiKugaa/OIFTrack}</details>
**Abstract_cn:** <details><summary>译文: </summary>过去三年来，单流 Transformer 跟踪器在具有挑战性的基准数据集中表现出了出色的性能，因为它们支持目标模板和搜索区域标记之间的交互，以在相互指导下提取面向目标的特征。以前的方法允许模板和搜索标记之间的自由双向信息流，而无需研究它们对跟踪器辨别能力的影响。在本研究中，我们对代币的信息流进行了详细的研究，并根据研究结果提出了一种新颖的优化信息流跟踪（OIFTrack）框架来增强跟踪器的判别能力。所提出的 OIFTrack 阻止了早期编码器层中所有搜索标记与目标模板标记的交互，因为搜索区域中的大量非目标标记降低了目标特定特征的重要性。在所提出的跟踪器的较深层编码器层中，搜索令牌被划分为目标搜索令牌和非目标搜索令牌，允许从目标搜索令牌到模板令牌的双向流以捕获目标的外观变化。此外，由于所提出的跟踪器结合了动态背景线索，因此可以通过捕获目标的周围信息成功地避免干扰对象。 OIFTrack 在具有挑战性的基准测试中表现出了出色的性能，特别是在一次性跟踪基准 GOT-10k 中表现出色，实现了 74.6% 的平均重叠。这项工作的代码、模型和结果可在 \url{https://github.com/JananiKugaa/OIFTrack} 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.08195v1><br />
**Code:** <https://github.com/jananikugaa/oiftrack>**<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Learning to Produce Semi-dense Correspondences for Visual Localization**<br />
**Title_cn:** 学习为视觉定位生成半密集对应<br />
**Authors:** Khang Truong Giang, Soohwan Song, Sungho Jo<br />
**Abstract:** <details><summary>原文: </summary>This study addresses the challenge of performing visual localization in demanding conditions such as night-time scenarios, adverse weather, and seasonal changes. While many prior studies have focused on improving image-matching performance to facilitate reliable dense keypoint matching between images, existing methods often heavily rely on predefined feature points on a reconstructed 3D model. Consequently, they tend to overlook unobserved keypoints during the matching process. Therefore, dense keypoint matches are not fully exploited, leading to a notable reduction in accuracy, particularly in noisy scenes. To tackle this issue, we propose a novel localization method that extracts reliable semi-dense 2D-3D matching points based on dense keypoint matches. This approach involves regressing semi-dense 2D keypoints into 3D scene coordinates using a point inference network. The network utilizes both geometric and visual cues to effectively infer 3D coordinates for unobserved keypoints from the observed ones. The abundance of matching information significantly enhances the accuracy of camera pose estimation, even in scenarios involving noisy or sparse 3D models. Comprehensive evaluations demonstrate that the proposed method outperforms other methods in challenging scenes and achieves competitive results in large-scale visual localization benchmarks. The code will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究解决了在夜间场景、恶劣天气和季节变化等苛刻条件下执行视觉定位的挑战。虽然许多先前的研究都集中在提高图像匹配性能以促进图像之间可靠的密集关键点匹配，但现有方法通常严重依赖于重建 3D 模型上的预定义特征点。因此，他们往往会在匹配过程中忽略未观察到的关键点。因此，密集的关键点匹配没有得到充分利用，导致准确性显着降低，特别是在嘈杂的场景中。为了解决这个问题，我们提出了一种新颖的定位方法，该方法基于密集关键点匹配提取可靠的半密集 2D-3D 匹配点。此方法涉及使用点推理网络将半密集 2D 关键点回归到 3D 场景坐标。该网络利用几何和视觉线索，有效地从观察到的关键点推断出未观察到的关键点的 3D 坐标。丰富的匹配信息显着提高了相机姿态估计的准确性，即使在涉及噪声或稀疏 3D 模型的场景中也是如此。综合评估表明，该方法在具有挑战性的场景中优于其他方法，并在大规模视觉定位基准中取得了有竞争力的结果。该代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.08359v1><br />
**Code:** <https://github.com/truongkhang/deviloc>**<br />
>>**index:** 2<br />
**Title:** **Color Image Denoising Using The Green Channel Prior**<br />
**Title_cn:** 使用绿色通道先验进行彩色图像去噪<br />
**Authors:** Zhaoming Kong, Xiaowei Yang<br />
**Abstract:** <details><summary>原文: </summary>Noise removal in the standard RGB (sRGB) space remains a challenging task, in that the noise statistics of real-world images can be different in R, G and B channels. In fact, the green channel usually has twice the sampling rate in raw data and a higher signal-to-noise ratio than red/blue ones. However, the green channel prior (GCP) is often understated or ignored in color image denoising since many existing approaches mainly focus on modeling the relationship among image patches. In this paper, we propose a simple and effective one step GCP-based image denoising (GCP-ID) method, which aims to exploit the GCP for denoising in the sRGB space by integrating it into the classic nonlocal transform domain denoising framework. Briefly, we first take advantage of the green channel to guide the search of similar patches, which improves the patch search quality and encourages sparsity in the transform domain. Then we reformulate RGB patches into RGGB arrays to explicitly characterize the density of green samples. The block circulant representation is utilized to capture the cross-channel correlation and the channel redundancy. Experiments on both synthetic and real-world datasets demonstrate the competitive performance of the proposed GCP-ID method for the color image and video denoising tasks. The code is available at github.com/ZhaomingKong/GCP-ID.</details>
**Abstract_cn:** <details><summary>译文: </summary>标准 RGB (sRGB) 空间中的噪声消除仍然是一项具有挑战性的任务，因为现实世界图像的噪声统计在 R、G 和 B 通道中可能不同。事实上，绿色通道的原始数据采样率通常是红色/蓝色通道的两倍，并且信噪比更高。然而，绿色通道先验（GCP）在彩色图像去噪中经常被低估或忽略，因为许多现有方法主要集中于对图像块之间的关系进行建模。在本文中，我们提出了一种简单有效的基于 GCP 的一步图像去噪（GCP-ID）方法，旨在通过将 GCP 集成到经典的非局部变换域去噪框架中，利用 GCP 在 sRGB 空间中进行去噪。简而言之，我们首先利用绿色通道来引导相似补丁的搜索，这提高了补丁搜索质量并鼓励变换域中的稀疏性。然后，我们将 RGB 色块重新表示为 RGGB 数组，以明确表征绿色样本的密度。块循环表示用于捕获跨通道相关性和通道冗余。对合成数据集和真实数据集的实验证明了所提出的 GCP-ID 方法在彩色图像和视频去噪任务中的竞争性能。代码可在 github.com/ZhaomingKong/GCP-ID 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.08235v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5**<br />
**Title_cn:** 推进数据驱动的天气预报：ERA5 的时间滑动数据增强<br />
**Authors:** Minjong Cheon, Daehyun Kang, Yo-Hwan Choi, Seon-Yu Kang<br />
**Abstract:** <details><summary>原文: </summary>Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis data, have caused a significant revolution within a few years. In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution data, which is often constrained by computational resources, and instead utilizes low-resolution data (2.5 degrees) for global weather prediction and climate data analysis. Our main focus is evaluating data-driven weather prediction (DDWP) frameworks, specifically addressing sample size adequacy, structural improvements to the model, and the ability of climate data to represent current climatic trends. By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approach to data augmentation and processing. Our findings reveal that despite the lower resolution, the proposed approach demonstrates considerable accuracy in predicting atmospheric conditions, effectively rivaling higher-resolution models. Furthermore, the study confirms the model's proficiency in reflecting current climate trends and its potential in predicting future climatic events, underscoring its utility in climate change strategies. This research marks a pivotal step in the realm of meteorological forecasting, showcasing the feasibility of lower-resolution data in producing reliable predictions and opening avenues for more accessible and inclusive climate modeling. The insights gleaned from this study not only contribute to the advancement of climate science but also lay the groundwork for future innovations in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>现代深度学习技术模仿传统的数值天气预报（NWP）模型并源自全球大气再分析数据，在几年内引发了一场重大革命。在这个新范式中，我们的研究引入了一种新颖的策略，偏离了对高分辨率数据的普遍依赖，而高分辨率数据往往受到计算资源的限制，而是利用低分辨率数据（2.5度）进行全球天气预报和气候数据分析。我们的主要重点是评估数据驱动的天气预报 (DDWP) 框架，特别是解决样本量充足性、模型的结构改进以及气候数据代表当前气候趋势的能力。通过使用 FourCastNet 的自适应傅里叶神经算子 (AFNO) 模型和提出的时间滑动方法来膨胀 ECMWF Reanalysis v5 (ERA5) 的数据集，本文通过添加更多变量和一种新的数据增强方法来改进传统方法和加工。我们的研究结果表明，尽管分辨率较低，但所提出的方法在预测大气条件方面表现出相当高的准确性，有效地与更高分辨率的模型相媲美。此外，该研究证实了该模型在反映当前气候趋势方面的能力及其在预测未来气候事件方面的潜力，强调了其在气候变化战略中的实用性。这项研究标志着气象预报领域迈出了关键的一步，展示了低分辨率数据在产生可靠预测方面的可行性，并为更容易获得和更具包容性的气候建模开辟了途径。从这项研究中获得的见解不仅有助于气候科学的进步，而且为该领域未来的创新奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.08185v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Pixel Sentence Representation Learning**<br />
**Title_cn:** 像素句子表示学习<br />
**Authors:** Chenghao Xiao, Zhuoxu Huang, Danlu Chen, G Thomas Hudson, Yizhi Li, Haoran Duan, Chenghua Lin, Jie Fu, Jungong Han, Noura Al Moubayed<br />
**Abstract:** <details><summary>原文: </summary>Pretrained language models are long known to be subpar in capturing sentence and document-level semantics. Though heavily investigated, transferring perturbation-based methods from unsupervised visual representation learning to NLP remains an unsolved problem. This is largely due to the discreteness of subword units brought by tokenization of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs. In this work, we conceptualize the learning of sentence-level textual semantics as a visual representation learning process. Drawing from cognitive and linguistic sciences, we introduce an unsupervised visual sentence representation learning framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous. Our approach is further bolstered by large-scale unsupervised topical alignment training and natural language inference supervision, achieving comparable performance in semantic textual similarity (STS) to existing state-of-the-art NLP methods. Additionally, we unveil our method's inherent zero-shot cross-lingual transferability and a unique leapfrogging pattern across languages during iterative training. To our knowledge, this is the first representation learning method devoid of traditional language models for understanding sentence and document semantics, marking a stride closer to human-like textual comprehension. Our code is available at https://github.com/gowitheflow-1998/Pixel-Linguist</details>
**Abstract_cn:** <details><summary>译文: </summary>众所周知，预训练的语言模型在捕获句子和文档级语义方面表现不佳。尽管经过大量研究，将基于扰动的方法从无监督视觉表示学习转移到 NLP 仍然是一个未解决的问题。这主要是由于语言模型标记化带来的子词单元的离散性，限制了输入的小扰动以形成语义保留的正对。在这项工作中，我们将句子级文本语义的学习概念化为视觉表示学习过程。我们借鉴认知和语言科学的经验，引入了一种无监督的视觉句子表示学习框架，采用基于视觉的文本扰动方法（例如打字错误和词序洗牌），与人类认知模式产生共鸣，并使对文本的扰动能够被感知为连续的。我们的方法得到大规模无监督主题对齐训练和自然语言推理监督的进一步支持，在语义文本相似性（STS）方面实现了与现有最​​先进的 NLP 方法相当的性能。此外，我们还揭示了我们的方法固有的零样本跨语言可迁移性以及迭代训练过程中独特的跨语言跳跃模式。据我们所知，这是第一个无需传统语言模型来理解句子和文档语义的表示学习方法，标志着向类人文本理解迈出了一步。我们的代码位于 https://github.com/gowitheflow-1998/Pixel-Linguist</details>
**PDF:** <http://arxiv.org/pdf/2402.08183v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Learned Image Compression with Text Quality Enhancement**<br />
**Title_cn:** 学习图像压缩和文本质量增强<br />
**Authors:** Chih-Yu Lai, Dung Tran, Kazuhito Koishida<br />
**Abstract:** <details><summary>原文: </summary>Learned image compression has gained widespread popularity for their efficiency in achieving ultra-low bit-rates. Yet, images containing substantial textual content, particularly screen-content images (SCI), often suffers from text distortion at such compressed levels. To address this, we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images, thereby improving the perceptual quality of the reconstructed text. Through rigorous experimentation across diverse datasets and employing state-of-the-art algorithms, our findings reveal significant enhancements in the quality of reconstructed text upon integration of the proposed loss function with appropriate weighting. Notably, we achieve a Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and -28.03% for Word Error Rate (WER) on average by applying the text logit loss for two screenshot datasets. Additionally, we present quantitative metrics tailored for evaluating text quality in image compression tasks. Our findings underscore the efficacy and potential applicability of our proposed text logit loss function across various text-aware image compression contexts.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习图像压缩因其在实现超低比特率方面的效率而受到广泛欢迎。然而，包含大量文本内容的图像，特别是屏幕内容图像 (SCI)，在如此压缩的水平下常常会出现文本失真。为了解决这个问题，我们建议最小化一种新颖的文本逻辑损失，该损失旨在量化原始图像和重建图像之间的文本差异，从而提高重建文本的感知质量。通过对不同数据集进行严格的实验并采用最先进的算法，我们的研究结果表明，将所提出的损失函数与适当的权重相结合后，重建文本的质量显着提高。值得注意的是，通过对两个屏幕截图数据集应用文本 logit 损失，我们的字符错误率 (CER) 的 Bjontegaard delta (BD) 平均为 -32.64%，单词错误率 (WER) 的平均为 -28.03%。此外，我们还提出了为评估图像压缩任务中的文本质量而定制的定量指标。我们的研究结果强调了我们提出的文本 logit 损失函数在各种文本感知图像压缩环境中的有效性和潜在适用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.08643v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing**<br />
**Title_cn:** 具有时间步感知采样的潜在反转，用于免训练非刚性编辑<br />
**Authors:** Yunji Jung, Seokju Lee, Tair Djanibekov, Hyunjung Shim, Jongchul Ye<br />
**Abstract:** <details><summary>原文: </summary>Text-guided non-rigid editing involves complex edits for input images, such as changing motion or compositions within their surroundings. Since it requires manipulating the input structure, existing methods often struggle with preserving object identity and background, particularly when combined with Stable Diffusion. In this work, we propose a training-free approach for non-rigid editing with Stable Diffusion, aimed at improving the identity preservation quality without compromising editability. Our approach comprises three stages: text optimization, latent inversion, and timestep-aware text injection sampling. Inspired by the recent success of Imagic, we employ their text optimization for smooth editing. Then, we introduce latent inversion to preserve the input image's identity without additional model fine-tuning. To fully utilize the input reconstruction ability of latent inversion, we suggest timestep-aware text inject sampling. This effectively retains the structure of the input image by injecting the source text prompt in early sampling steps and then transitioning to the target prompt in subsequent sampling steps. This strategic approach seamlessly harmonizes with text optimization, facilitating complex non-rigid edits to the input without losing the original identity. We demonstrate the effectiveness of our method in terms of identity preservation, editability, and aesthetic quality through extensive experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本引导的非刚性编辑涉及对输入图像的复杂编辑，例如改变周围环境中的运动或构图。由于需要操纵输入结构，现有方法通常难以保留对象身份和背景，特别是与稳定扩散结合使用时。在这项工作中，我们提出了一种使用稳定扩散进行非刚性编辑的免训练方法，旨在在不影响可编辑性的情况下提高身份保存质量。我们的方法包括三个阶段：文本优化、潜在反转和时间步感知文本注入采样。受到 Imagic 最近成功的启发，我们采用其文本优化来实现流畅的编辑。然后，我们引入潜在反转来保留输入图像的身份，而无需额外的模型微调。为了充分利用潜在反转的输入重建能力，我们建议时间步感知文本注入采样。通过在早期采样步骤中注入源文本提示，然后在后续采样步骤中转换到目标提示，可以有效地保留输入图像的结构。这种战略方法与文本优化无缝协调，促进对输入进行复杂的非刚性编辑，而不会丢失原始身份。我们通过大量的实验证明了我们的方法在身份保存、可编辑性和美学质量方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.08601v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **JeFaPaTo -- A joint toolbox for blinking analysis and facial features extraction**<br />
**Title_cn:** JeFaPaTo——眨眼分析和面部特征提取的联合工具箱<br />
**Authors:** Tim Büchner, Oliver Mothes, Orlando Guntinas-Lichius, Joachim Denzler<br />
**Abstract:** <details><summary>原文: </summary>Analyzing facial features and expressions is a complex task in computer vision. The human face is intricate, with significant shape, texture, and appearance variations. In medical contexts, facial structures that differ from the norm, such as those affected by paralysis, are particularly important to study and require precise analysis. One area of interest is the subtle movements involved in blinking, a process that is not yet fully understood and needs high-resolution, time-specific analysis for detailed understanding. However, a significant challenge is that many advanced computer vision techniques demand programming skills, making them less accessible to medical professionals who may not have these skills. The Jena Facial Palsy Toolbox (JeFaPaTo) has been developed to bridge this gap. It utilizes cutting-edge computer vision algorithms and offers a user-friendly interface for those without programming expertise. This toolbox is designed to make advanced facial analysis more accessible to medical experts, simplifying integration into their workflow.   The state of the eye closure is of high interest to medical experts, e.g., in the context of facial palsy or Parkinson's disease. Due to facial nerve damage, the eye-closing process might be impaired and could lead to many undesirable side effects. Hence, more than a simple distinction between open and closed eyes is required for a detailed analysis. Factors such as duration, synchronicity, velocity, complete closure, the time between blinks, and frequency over time are highly relevant. Such detailed analysis could help medical experts better understand the blinking process, its deviations, and possible treatments for better eye care.</details>
**Abstract_cn:** <details><summary>译文: </summary>分析面部特征和表情是计算机视觉中的一项复杂任务。人脸错综复杂，具有显着的形状、纹理和外观变化。在医学背景下，与正常情况不同的面部结构（例如受到瘫痪影响的面部结构）对于研究尤其重要，并且需要精确分析。一个令人感兴趣的领域是眨眼所涉及的微妙运动，这一过程尚未完全理解，需要高分辨率、特定时间的分析才能详细理解。然而，一个重大挑战是许多先进的计算机视觉技术需要编程技能，这使得不具备这些技能的医疗专业人员更难使用它们。耶拿面部麻痹工具箱 (JeFaPaTo) 的开发就是为了弥补这一差距。它利用尖端的计算机视觉算法，并为那些没有编程专业知识的人提供用户友好的界面。该工具箱旨在使医学专家更容易进行高级面部分析，从而简化与其工作流程的集成。医学专家对眼睛闭合的状态非常感兴趣，例如在面瘫或帕金森病的情况下。由于面部神经损伤，闭眼过程可能会受到损害，并可能导致许多不良的副作用。因此，详细分析需要的不仅仅是睁眼和闭眼之间的简单区别。持续时间、同步性、速度、完全闭合、眨眼之间的时间以及随时间变化的频率等因素是高度相关的。这种详细的分析可以帮助医学专家更好地了解眨眼过程、其偏差以及更好的眼部护理的可能治疗方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.08439v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Neural-network Enhanced Video Coding Framework beyond ECM**<br />
**Title_cn:** 超越 ECM 的神经网络增强视频编码框架<br />
**Authors:** Yanchen Zhao, Wenxuan He, Chuanmin Jia, Qizhe Wang, Junru Li, Yue Li, Chaoyi Lin, Kai Zhang, Li Zhang, Siwei Ma<br />
**Abstract:** <details><summary>原文: </summary>In this paper, a hybrid video compression framework is proposed that serves as a demonstrative showcase of deep learning-based approaches extending beyond the confines of traditional coding methodologies. The proposed hybrid framework is founded upon the Enhanced Compression Model (ECM), which is a further enhancement of the Versatile Video Coding (VVC) standard. We have augmented the latest ECM reference software with well-designed coding techniques, including block partitioning, deep learning-based loop filter, and the activation of block importance mapping (BIM) which was integrated but previously inactive within ECM, further enhancing coding performance. Compared with ECM-10.0, our method achieves 6.26, 13.33, and 12.33 BD-rate savings for the Y, U, and V components under random access (RA) configuration, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，提出了一种混合视频压缩框架，该框架作为基于深度学习的方法的示范展示，超越了传统编码方法的限制。所提出的混合框架建立在增强压缩模型（ECM）的基础上，它是通用视频编码（VVC）标准的进一步增强。我们通过精心设计的编码技术增强了最新的 ECM 参考软件，包括块分区、基于深度学习的环路滤波器以及激活块重要性映射 (BIM)（集成但之前在 ECM 中处于非活动状态），进一步增强了编码性能。与 ECM-10.0 相比，我们的方法在随机访问 (RA) 配置下分别实现了 Y、U 和 V 分量的 6.26、13.33 和 12.33 BD 速率节省。</details>
**PDF:** <http://arxiv.org/pdf/2402.08397v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation**<br />
**Title_cn:** 用于审美感知音乐推荐的顺序复杂度审美评估模型<br />
**Authors:** Xin Jin, Wu Zhou, Jingyu Wang, Duo Xu, Yongsen Zheng<br />
**Abstract:** <details><summary>原文: </summary>Computational aesthetic evaluation has made remarkable contribution to visual art works, but its application to music is still rare. Currently, subjective evaluation is still the most effective form of evaluating artistic works. However, subjective evaluation of artistic works will consume a lot of human and material resources. The popular AI generated content (AIGC) tasks nowadays have flooded all industries, and music is no exception. While compared to music produced by humans, AI generated music still sounds mechanical, monotonous, and lacks aesthetic appeal. Due to the lack of music datasets with rating annotations, we have to choose traditional aesthetic equations to objectively measure the beauty of music. In order to improve the quality of AI music generation and further guide computer music production, synthesis, recommendation and other tasks, we use Birkhoff's aesthetic measure to design a aesthetic model, objectively measuring the aesthetic beauty of music, and form a recommendation list according to the aesthetic feeling of music. Experiments show that our objective aesthetic model and recommendation method are effective.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算美学评估对视觉艺术作品做出了显着的贡献，但其在音乐中的应用仍然很少。目前，主观评价仍然是评价艺术作品最有效的形式。然而，对艺术作品的主观评价会消耗大量的人力和物力。时下流行的人工智能生成内容（AIGC）任务已经充斥各个行业，音乐也不例外。但与人类创作的音乐相比，人工智能创作的音乐听起来仍然机械、单调，缺乏审美吸引力。由于缺乏带有评分标注的音乐数据集，我们不得不选择传统的审美方程来客观地衡量音乐的美感。为了提高AI音乐生成的质量，进一步指导计算机音乐制作、合成、推荐等任务，我们利用伯克霍夫的审美测度设计审美模型，客观地衡量音乐的审美美感，并根据音乐的美感。实验表明，我们的客观审美模型和推荐方法是有效的。</details>
**PDF:** <http://arxiv.org/pdf/2402.08300v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Learning semantic image quality for fetal ultrasound from noisy ranking annotation**<br />
**Title_cn:** 从嘈杂的排名注释中学习胎儿超声的语义图像质量<br />
**Authors:** Manxi Lin, Jakob Ambsdorf, Emilie Pi Fogtmann Sejer, Zahra Bashir, Chun Kit Wong, Paraskevas Pegios, Alberto Raheli, Morten Bo Søndergaard Svendsen, Mads Nielsen, Martin Grønnebæk Tolsgaard, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce the notion of semantic image quality for applications where image quality relies on semantic requirements. Working in fetal ultrasound, where ranking is challenging and annotations are noisy, we design a robust coarse-to-fine model that ranks images based on their semantic image quality and endow our predicted rankings with an uncertainty estimate. To annotate rankings on training data, we design an efficient ranking annotation scheme based on the merge sort algorithm. Finally, we compare our ranking algorithm to a number of state-of-the-art ranking algorithms on a challenging fetal ultrasound quality assessment task, showing the superior performance of our method on the majority of rank correlation metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们为图像质量依赖于语义要求的应用引入了语义图像质量的概念。在胎儿超声检查中，排序具有挑战性并且注释充满噪音，我们设计了一个强大的从粗到细的模型，该模型根据图像的语义图像质量对图像进行排序，并为我们的预测排名赋予不确定性估计。为了注释训练数据的排名，我们设计了一种基于合并排序算法的有效排名注释方案。最后，我们将我们的排名算法与许多最先进的排名算法在具有挑战性的胎儿超声质量评估任务上进行比较，显示我们的方法在大多数排名相关指标上的优越性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08294v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization**<br />
**Title_cn:** SepRep-Net：通过模型分离和重新参数化进行多源自由域适应<br />
**Authors:** Ying Jin, Jiaqi Wang, Dahua Lin<br />
**Abstract:** <details><summary>原文: </summary>We consider multi-source free domain adaptation, the problem of adapting multiple existing models to a new domain without accessing the source data. Among existing approaches, methods based on model ensemble are effective in both the source and target domains, but incur significantly increased computational costs. Towards this dilemma, in this work, we propose a novel framework called SepRep-Net, which tackles multi-source free domain adaptation via model Separation and Reparameterization.Concretely, SepRep-Net reassembled multiple existing models to a unified network, while maintaining separate pathways (Separation). During training, separate pathways are optimized in parallel with the information exchange regularly performed via an additional feature merging unit. With our specific design, these pathways can be further reparameterized into a single one to facilitate inference (Reparameterization). SepRep-Net is characterized by 1) effectiveness: competitive performance on the target domain, 2) efficiency: low computational costs, and 3) generalizability: maintaining more source knowledge than existing solutions. As a general approach, SepRep-Net can be seamlessly plugged into various methods. Extensive experiments validate the performance of SepRep-Net on mainstream benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们考虑多源自由域适应，即在不访问源数据的情况下将多个现有模型适应新域的问题。在现有方法中，基于模型集成的方法在源域和目标域中均有效，但会导致计算成本显着增加。针对这一困境，在这项工作中，我们提出了一种名为 SepRep-Net 的新颖框架，该框架通过模型分离和重新参数化来解决多源自由域适应问题。具体来说，SepRep-Net 将多个现有模型重新组装成一个统一的网络，同时保持单独的路径（分离）。在训练期间，单独的路径与通过附加特征合并单元定期执行的信息交换并行优化。通过我们的具体设计，这些路径可以进一步重新参数化为单个路径，以方便推理（重新参数化）。 SepRep-Net 的特点是 1) 有效性：在目标域上的竞争性能，2) 效率：低计算成本，3) 通用性：比现有解决方案维护更多的源知识。作为一种通用方法，SepRep-Net 可以无缝插入各种方法中。大量实验验证了 SepRep-Net 在主流基准上的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08249v1><br />
**Code:** null<br />

