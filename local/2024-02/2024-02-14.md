## [UPDATED!] **2024-02-14** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Magic-Me: Identity-Specific Video Customized Diffusion**<br />
**Title_cn:** Magic-Me：针对特定身份的视频定制扩散<br />
**Authors:** Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng<br />
**Abstract:** <details><summary>原文: </summary>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.</details>
**Abstract_cn:** <details><summary>译文: </summary>为特定身份 (ID) 创建内容已引起人们对生成模型领域的浓厚兴趣。在文本到图像生成（T2I）领域，主题驱动的内容生成取得了长足的进步，并且图像中的ID可控。然而，将其扩展到视频生成尚未得到很好的探索。在这项工作中，我们提出了一种简单而有效的主题身份可控视频生成框架，称为视频自定义扩散（VCD）。 VCD利用由几张图像定义的指定主体ID，加强了身份信息提取，并在初始化阶段注入逐帧相关性，以在很大程度上保留身份的稳定视频输出。为了实现这一目标，我们提出了三个对于高质量 ID 保存至关重要的新颖组件：1）一个 ID 模块，通过提示分割来使用裁剪后的身份进行训练，以解开 ID 信息和背景噪声，从而实现更准确的 ID 令牌学习; 2) 文本转视频 (T2V) VCD 模块，具有 3D 高斯噪声先验，可实现更好的帧间一致性；3) 视频转视频 (V2V) 脸部 VCD 和平铺 VCD 模块，可对脸部进行去模糊处理并升级视频，以实现更好的帧间一致性。更高的分辨率。尽管它很简单，我们还是进行了广泛的实验来验证 VCD 能够生成稳定且高质量的视频，其 ID 优于所选的强基线。此外，由于ID模块的可移植性，VCD还可以与公开的经过微调的文本到图像模型配合良好，进一步提高了其可用性。代码可在 https://github.com/Zhen-Dong/Magic-Me 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.09368v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection**<br />
**Title_cn:** 综合知识增强特征以实现现实世界的零样本食品检测<br />
**Authors:** Pengfei Zhou, Weiqing Min, Jiajun Song, Yang Zhang, Shuqiang Jiang<br />
**Abstract:** <details><summary>原文: </summary>Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.</details>
**Abstract_cn:** <details><summary>译文: </summary>食品计算为计算机视觉带来了多种视角，例如基于视觉的营养和健康食品分析。作为食品计算的一项基本任务，食品检测需要对新颖的看不见的食品对象进行零样本检测（ZSD），以支持现实世界的场景，例如智能厨房和智能餐厅。因此，我们首先通过引入具有丰富属性注释的 FOWA 数据集来对零样本食物检测（ZSFD）任务进行基准测试。与 ZSD 不同，ZSFD 中的细粒度问题（例如类间相似性）使得合成特征不可分割。食物语义属性的复杂性进一步使得当前的ZSD方法区分各种食物类别变得更加困难。为了解决这些问题，我们提出了一种新颖的框架 ZSFDet，通过利用复杂属性之间的交互来解决细粒度问题。具体来说，我们通过多源图对 ZSFDet 中食物类别和属性之间的相关性进行建模，为区分细粒度特征提供先验知识。在 ZSFDet 中，知识增强特征合成器 (KEFS) 通过多源图融合从多个源学习知识表示（例如，知识图的成分相关性）。以语义知识表示的融合为条件，KEFS中的区域特征扩散模型可以生成细粒度的特征来训练有效的零样本检测器。广泛的评估证明了我们的方法 ZSFDet 在 FOWA 和广泛使用的食品数据集 UECFOOD-256 上的卓越性能，与强基线 RRFS 相比，ZSD mAP 显着提高了 1.8% 和 3.7%。在 PASCAL VOC 和 MS COCO 上的进一步实验证明，语义知识的增强也可以提高通用 ZSD 的性能。代码和数据集可在 https://github.com/LanceZPF/KEFS 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.09242v1><br />
**Code:** <https://github.com/lancezpf/kefs>**<br />
>>**index:** 3<br />
**Title:** **Semi-Supervised Diffusion Model for Brain Age Prediction**<br />
**Title_cn:** 用于脑年龄预测的半监督扩散模型<br />
**Authors:** Ayodeji Ijishakin, Sophie Martin, Florence Townend, Federica Agosta, Edoardo Gioele Spinelli, Silvia Basaia, Paride Schito, Yuri Falzone, Massimo Filippi, James Cole, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Brain age prediction models have succeeded in predicting clinical outcomes in neurodegenerative diseases, but can struggle with tasks involving faster progressing diseases and low quality data. To enhance their performance, we employ a semi-supervised diffusion model, obtaining a 0.83(p<0.01) correlation between chronological and predicted age on low quality T1w MR images. This was competitive with state-of-the-art non-generative methods. Furthermore, the predictions produced by our model were significantly associated with survival length (r=0.24, p<0.05) in Amyotrophic Lateral Sclerosis. Thus, our approach demonstrates the value of diffusion-based architectures for the task of brain age prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑年龄预测模型已成功预测神经退行性疾病的临床结果，但在处理涉及进展较快的疾病和低质量数据的任务时可能会遇到困难。为了提高其性能，我们采用半监督扩散模型，在低质量 T1w MR 图像上获得实际年龄和预测年龄之间的 0.83（p<0.01）相关性。这与最先进的非生成方法具有竞争力。此外，我们的模型产生的预测与肌萎缩侧索硬化症的生存长度显着相关（r=0.24，p<0.05）。因此，我们的方法证明了基于扩散的架构对于大脑年龄预测任务的价值。</details>
**PDF:** <http://arxiv.org/pdf/2402.09137v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping**<br />
**Title_cn:** DestripeCycleGAN：用于无监督红外图像去条纹的条纹模拟 CycleGAN<br />
**Authors:** Shiqi Yang, Hanlin Qin, Shuai Yuan, Xiang Yan, Hossein Rahmani<br />
**Abstract:** <details><summary>原文: </summary>CycleGAN has been proven to be an advanced approach for unsupervised image restoration. This framework consists of two generators: a denoising one for inference and an auxiliary one for modeling noise to fulfill cycle-consistency constraints. However, when applied to the infrared destriping task, it becomes challenging for the vanilla auxiliary generator to consistently produce vertical noise under unsupervised constraints. This poses a threat to the effectiveness of the cycle-consistency loss, leading to stripe noise residual in the denoised image. To address the above issue, we present a novel framework for single-frame infrared image destriping, named DestripeCycleGAN. In this model, the conventional auxiliary generator is replaced with a priori stripe generation model (SGM) to introduce vertical stripe noise in the clean data, and the gradient map is employed to re-establish cycle-consistency. Meanwhile, a Haar wavelet background guidance module (HBGM) has been designed to minimize the divergence of background details between the different domains. To preserve vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the denoising generator, which utilizes the Haar wavelet transform as the sampler to decline directional information loss. Moreover, it incorporates the group fusion block (GFB) into skip connections to fuse the multi-scale features and build the context of long-distance dependencies. Extensive experiments on real and synthetic data demonstrate that our DestripeCycleGAN surpasses the state-of-the-art methods in terms of visual quality and quantitative evaluation. Our code will be made public at https://github.com/0wuji/DestripeCycleGAN.</details>
**Abstract_cn:** <details><summary>译文: </summary>CycleGAN 已被证明是一种用于无监督图像恢复的先进方法。该框架由两个生成器组成：一个用于推理的去噪生成器和一个用于对噪声进行建模以满足循环一致性约束的辅助生成器。然而，当应用于红外去条纹任务时，普通辅助发生器在无人监督的约束下持续产生垂直噪声变得具有挑战性。这对循环一致性损失的有效性构成了威胁，导致去噪图像中残留条纹噪声。为了解决上述问题，我们提出了一种新颖的单帧红外图像去条纹框架，名为 DestripeCycleGAN。在该模型中，传统的辅助生成器被替换为先验条纹生成模型（SGM），以在干净的数据中引入垂直条纹噪声，并利用梯度图来重新建立循环一致性。同时，设计了哈尔小波背景引导模块（HBGM），以最大限度地减少不同域之间背景细节的差异。为了保留垂直边缘，提出了多级小波 U-Net（MWUNet）作为去噪生成器，它利用 Haar 小波变换作为采样器来减少方向信息损失。此外，它将组融合块（GFB）合并到跳跃连接中，以融合多尺度特征并构建长距离依赖的上下文。对真实和合成数据的大量实验表明，我们的 DestripeCycleGAN 在视觉质量和定量评估方面超越了最先进的方法。我们的代码将在 https://github.com/0wuji/DestripeCycleGAN 公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.09101v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs**<br />
**Title_cn:** 基于 GAN 的逼真地标引导面部视频修复<br />
**Authors:** Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr<br />
**Abstract:** <details><summary>原文: </summary>Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部视频修复在广泛的应用中发挥着至关重要的作用，包括但不限于视频会议和远程医疗中的障碍物清除、面部表情分析的增强、隐私保护、图形叠加集成和虚拟化妆。由于面部特征的复杂性和人类对面部固有的熟悉性，该领域提出了严峻的挑战，从而提高了对准确和有说服力的完成的需求。在解决与这种情况下的遮挡去除相关的挑战时，我们的重点是从掩模覆盖的面部数据生成完整图像的渐进任务，确保空间和时间的一致性。我们的研究引入了一种专为基于表达的视频修复而设计的网络，采用生成对抗网络（GAN）来处理所有帧中的静态和移动遮挡。通过利用面部标志和无遮挡参考图像，我们的模型可以在帧之间保持用户身份的一致性。我们通过定制的面部表情识别（FER）损失函数进一步增强情感保存，确保详细的修复输出。我们提出的框架展示了以自适应形式熟练消除面部视频遮挡的能力，无论是在帧上显示静态还是动态，同时提供真实且连贯的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.09100v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Extreme Video Compression with Pre-trained Diffusion Models**<br />
**Title_cn:** 使用预先训练的扩散模型进行极限视频压缩<br />
**Authors:** Bohan Li, Yiming Liu, Xueyan Niu, Bo Bai, Lei Deng, Deniz Gündüz<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved remarkable success in generating high quality image and video data. More recently, they have also been used for image compression with high perceptual quality. In this paper, we present a novel approach to extreme video compression leveraging the predictive power of diffusion-based generative models at the decoder. The conditional diffusion model takes several neural compressed frames and generates subsequent frames. When the reconstruction quality drops below the desired level, new frames are encoded to restart prediction. The entire video is sequentially encoded to achieve a visually pleasing reconstruction, considering perceptual quality metrics such as the learned perceptual image patch similarity (LPIPS) and the Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp). Experimental results demonstrate the effectiveness of the proposed scheme compared to standard codecs such as H.264 and H.265 in the low bpp regime. The results showcase the potential of exploiting the temporal relations in video data using generative models. Code is available at: https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在生成高质量图像和视频数据方面取得了显着的成功。最近，它们还被用于具有高感知质量的图像压缩。在本文中，我们提出了一种利用解码器中基于扩散的生成模型的预测能力来进行极端视频压缩的新颖方法。条件扩散模型采用多个神经压缩帧并生成后续帧。当重建质量低于所需水平时，对新帧进行编码以重新开始预测。整个视频按顺序编码，以实现视觉上令人愉悦的重建，考虑感知质量指标，例如学习的感知图像块相似性 (LPIPS) 和 Frechet 视频距离 (FVD)，比特率低至每像素 0.02 位 (bpp) 。实验结果证明了与低 bpp 机制中的 H.264 和 H.265 等标准编解码器相比，所提出的方案的有效性。结果展示了使用生成模型利用视频数据中的时间关系的潜力。代码位于：https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-</details>
**PDF:** <http://arxiv.org/pdf/2402.08934v1><br />
**Code:** <https://github.com/elesionkyrie/extreme-video-compression-with-prediction-using-pre-trainded-diffusion-models->**<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models**<br />
**Title_cn:** MultiMedEval：评估医学视觉语言模型的基准和工具包<br />
**Authors:** Corentin Royer, Bjoern Menze, Anjany Sekuboyina<br />
**Abstract:** <details><summary>原文: </summary>We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models' performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model's overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出了 MultiMedEval，这是一个开源工具包，用于对大型医学视觉语言模型 (VLM) 进行公平且可重复的评估。 MultiMedEval 全面评估了模型在六种多模式任务中的表现，涉及超过 23 个数据集，涵盖超过 11 个医学领域。所选择的任务和性能指标基于其在社区中的广泛采用及其多样性，确保对模型的整体通用性进行彻底评估。我们开源了一个 Python 工具包 (github.com/corentin-ryr/MultiMedEval)，具有简单的界面和设置过程，只需几行代码即可评估任何 VLM。我们的目标是简化 VLM 评估的复杂情况，从而促进未来模型的公平和统一的基准测试。</details>
**PDF:** <http://arxiv.org/pdf/2402.09262v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM**<br />
**Title_cn:** OmniMedVQA：医疗 LVLM 的新型大规模综合评估基准<br />
**Authors:** Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo<br />
**Abstract:** <details><summary>原文: </summary>Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance. Our dataset will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>大视觉语言模型（LVLM）在各种多模态任务中表现出了卓越的能力。然而，它们在医学领域的潜力在很大程度上仍未被开发。跨越各种模式和解剖区域的各种医学图像的稀缺带来了一个重大挑战，这在现实世界的医学应用中至关重要。为了解决这个问题，在本文中，我们介绍了 OmniMedVQA，一种新颖的综合医学视觉问答（VQA）基准。该基准收集自 75 个不同的医学数据集，包括 12 种不同的模式，覆盖 20 多个不同的解剖区域。重要的是，该基准测试中的所有图像均来自真实的医疗场景，确保符合医疗领域的要求以及评估 LVLM 的适用性。通过我们广泛的实验，我们发现现有的 LVLM 很难有效地解决这些医学 VQA 问题。此外，令我们惊讶的是，医学专用的 LVLM 甚至表现出比那些通用领域模型差的性能，这需要在生物医学领域提供更通用、更强大的 LVLM。评估结果不仅揭示了 LVLM 目前在理解真实医学图像方面的局限性，而且凸显了我们数据集的重要性。我们的数据集将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.09181v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Headset: Human emotion awareness under partial occlusions multimodal dataset**<br />
**Title_cn:** 耳机：部分遮挡多模态数据集下的人类情感意识<br />
**Authors:** Fatemeh Ghorbani Lohesara, Davi Rabbouni Freitas, Christine Guillemot, Karen Eguiazarian, Sebastian Knorr<br />
**Abstract:** <details><summary>原文: </summary>The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类交互的体积表示是沉浸式媒体制作和电信应用开发的基本领域之一。特别是在扩展现实 (XR) 应用快速发展的背景下，这种体积数据已被证明是未来 XR 阐述的一项重要技术。在这项工作中，我们提出了一个新的多模式数据库，以帮助推进沉浸式技术的发展。我们提出的数据库提供了符合道德规范和多样化的体积数据，特别是 27 名参与者在说话时展示了面部表情和微妙的身体动作，以及 11 名佩戴头戴式显示器 (HMD) 的参与者。记录系统由体积捕捉 (VoCap) 工作室组成，包括 31 个同步模块、62 个 RGB 摄像头和 31 个深度摄像头。除了纹理网格、点云和多视图 RGB-D 数据外，我们还使用一台 Lytro Illum 相机同时提供光场 (LF) 数据。最后，我们还对面部表情分类、头显移除和点云重建任务的数据集使用进行了评估。该数据集有助于各种 XR 算法的评估和性能测试，包括但不限于面部表情识别和重建、面部重演和体积视频。 HEADSET 及其所有相关原始数据和许可协议将公开用于研究目的。</details>
**PDF:** <http://arxiv.org/pdf/2402.09107v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection**<br />
**Title_cn:** 通过对比预训练进行短视频幽默检测的评论辅助视频语言对齐<br />
**Authors:** Yang Liu, Tongfei Shen, Dong Zhang, Qingying Sun, Shoushan Li, Guodong Zhou<br />
**Abstract:** <details><summary>原文: </summary>The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.</details>
**Abstract_cn:** <details><summary>译文: </summary>情感计算中多模式幽默检测的重要性与日俱增，这与社交媒体平台上短视频共享的影响力不断扩大相关。在本文中，我们提出了一种用于短视频幽默检测（SVHD）的新型两分支分层模型，通过数据增强多模态对比预训练，称为评论辅助视频语言对齐（CVLA）。值得注意的是，我们的 CVLA 不仅对各种模态通道的原始信号进行操作，而且还通过在一致的语义空间内对齐视频和语言组件来产生适当的多模态表示。包括 DY11k 和 UR-FUNNY 在内的两个幽默检测数据集的实验结果表明，CVLA 的性能显着优于最先进的方法和几种有竞争力的基线方法。我们的数据集、代码和模型发布于 https://github.com/yliu-cs/CVLA。</details>
**PDF:** <http://arxiv.org/pdf/2402.09055v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing?**<br />
**Title_cn:** 文本到图像模型能否辅助视觉模态缺失的视觉识别的多模态学习？<br />
**Authors:** Tiantian Feng, Daniel Yang, Digbalay Bose, Shrikanth Narayanan<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of text-to-image models to assist multi-modal learning. Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers. Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模式学习已成为视觉识别领域日益有前景的途径，推动从媒体和教育到医疗保健和交通等不同领域的创新。尽管取得了成功，但视觉识别的多模态学习的稳健性经常受到模态子集（尤其是视觉模态）不可用的挑战。减少多模态学习中缺失模态的传统方法在很大程度上依赖于算法和模态融合方案。相比之下，本文探讨了使用文本到图像模型来辅助多模态学习。具体来说，我们提出了一个简单但有效的多模态学习框架 GTI-MM，通过使用生成变压器来填充缺失的数据，以提高数据效率和针对缺失视觉模态的模型鲁棒性。使用具有视觉识别任务的多个多模态数据集，我们对涉及数据中缺失视觉模态的各种条件（包括模型训练）进行了全面分析。我们的研究结果表明，在训练中缺少视觉数据的情况下，合成图像有利于训练数据的效率，并在涉及训练和测试的视觉数据缺失的情况下提高模型的稳健性。此外，我们证明了 GTI-MM 具有较低的生成数量和简单的提示技术的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09036v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Multi-modality transrectal ultrasound vudei classification for identification of clinically significant prostate cancer**<br />
**Title_cn:** 多模态经直肠超声 vudei 分类用于识别有临床意义的前列腺癌<br />
**Authors:** Hong Wu, Juan Fu, Hongsheng Ye, Yuming Zhong, Xuebin Zhou, Jianhua Zhou, Yi Wang<br />
**Abstract:** <details><summary>原文: </summary>Prostate cancer is the most common noncutaneous cancer in the world. Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become an effective tool for the guidance of prostate biopsies. With the aim of effectively identifying prostate cancer, we propose a framework for the classification of clinically significant prostate cancer (csPCa) from multi-modality TRUS videos. The framework utilizes two 3D ResNet-50 models to extract features from B-mode images and shear wave elastography images, respectively. An adaptive spatial fusion module is introduced to aggregate two modalities' features. An orthogonal regularized loss is further used to mitigate feature redundancy. The proposed framework is evaluated on an in-house dataset containing 512 TRUS videos, and achieves favorable performance in identifying csPCa with an area under curve (AUC) of 0.84. Furthermore, the visualized class activation mapping (CAM) images generated from the proposed framework may provide valuable guidance for the localization of csPCa, thus facilitating the TRUS-guided targeted biopsy. Our code is publicly available at https://github.com/2313595986/ProstateTRUS.</details>
**Abstract_cn:** <details><summary>译文: </summary>前列腺癌是世界上最常见的非皮肤癌。最近，多模态经直肠超声（TRUS）已日益成为指导前列腺活检的有效工具。为了有效识别前列腺癌，我们提出了一个从多模态 TRUS 视频中对具有临床意义的前列腺癌 (csPCa) 进行分类的框架。该框架利用两个 3D ResNet-50 模型分别从 B 模式图像和剪切波弹性成像图像中提取特征。引入自适应空间融合模块来聚合两种模态的特征。正交正则化损失进一步用于减轻特征冗余。所提出的框架在包含 512 个 TRUS 视频的内​​部数据集上进行了评估，并在识别曲线下面积 (AUC) 为 0.84 的 csPCa 方面取得了良好的性能。此外，从所提出的框架生成的可视化类激活映射（CAM）图像可以为 csPCa 的定位提供有价值的指导，从而促进 TRUS 引导的靶向活检。我们的代码可在 https://github.com/2313595986/ProstateTRUS 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.08987v1><br />
**Code:** <https://github.com/2313595986/prostatetrus>**<br />
>>**index:** 7<br />
**Title:** **Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays**<br />
**Title_cn:** 纵向胸部 X 射线差异视觉问答的预训练视觉语言模型<br />
**Authors:** Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin<br />
**Abstract:** <details><summary>原文: </summary>Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist's reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model's performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>差异视觉问答（diff-VQA）是一项具有挑战性的任务，需要根据一对图像之间的差异回答复杂的问题。这项任务在读取胸部 X 射线图像时尤其重要，因为放射科医生经常比较同一患者在不同时间拍摄的多张图像，以在临床实践中跟踪疾病进展及其严重程度的变化。然而，之前的工作重点是为 diff-VQA 任务设计特定的网络架构，错过了使用预训练视觉语言模型 (VLM) 增强模型性能的机会。在这里，我们介绍了一种名为 PLURAL 的新颖 VLM，它针对 diff-VQA 任务对自然和纵向胸部 X 射线数据进行了预训练。该模型采用分步方法开发，首先对自然图像和文本进行预训练，然后使用纵向胸部 X 射线数据进行训练。纵向数据由成对的 X 射线图像、问题解答集和描述肺部异常和疾病随时间变化的放射科医生报告组成。我们的实验结果表明，PLURAL 模型不仅在纵向 X 射线的 diff-VQA 中优于最先进的方法，而且在单个 X 射线图像的传统 VQA 中也优于最先进的方法。通过大量的实验，我们证明了所提出的 VLM 架构和预训练方法在提高模型性能方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.08966v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding**<br />
**Title_cn:** 通过复杂性约束的描述性自动编码来测量概念相似性的可解释性<br />
**Authors:** Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto<br />
**Abstract:** <details><summary>原文: </summary>Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of "conceptual similarity" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate "explanations" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.</details>
**Abstract_cn:** <details><summary>译文: </summary>量化图像之间的相似程度是基于图像的机器学习的关键版权问题。然而，在法律原则中，确定作品之间的相似程度需要主观分析，而事实认定者（法官和陪审团）可以证明这些主观判断存在相当大的可变性。结构相似的图像可以被视为不相似，而场景完全不同的图像可以被视为足够相似以支持复制的主张。我们试图定义和计算图像之间的“概念相似性”概念，即使在不共享重复元素或视觉上相似组件的图像之间也能捕获高级关系。这个想法是使用基本的多模态模型来生成复杂程度不断增加的视觉数据的“解释”（标题）。然后，可以通过区分两个图像所需的标题长度来衡量相似性：可以在描述的早期区分两个高度不同的图像，而概念上不同的图像则需要更多细节来区分。我们对这个定义进行了操作，并表明它与主观（平均人类评估）评估相关，并且在图像到图像和文本到文本相似性基准上都击败了现有基线。除了提供数字之外，我们的方法还通过指出区分源数据的描述的特定粒度级别来提供可解释性。</details>
**PDF:** <http://arxiv.org/pdf/2402.08919v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames in Autonomous Driving Environments**<br />
**Title_cn:** PC-NeRF：在自动驾驶环境中使用稀疏 LiDAR 帧的亲子神经辐射场<br />
**Authors:** Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma<br />
**Abstract:** <details><summary>原文: </summary>Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模 3D 场景重建和新颖的视图合成对于自动驾驶汽车至关重要，尤其是利用时间稀疏的 LiDAR 帧。然而，传统的显式表示仍然是以无限分辨率表示重建和合成场景的重大瓶颈。尽管最近开发的神经辐射场 (NeRF) 在隐式表示方面显示出了引人注目的结果，但使用稀疏 LiDAR 帧进行大规模 3D 场景重建和新颖视图合成的问题仍未得到探索。为了弥补这一差距，我们提出了一种 3D 场景重建和新颖的视图合成框架，称为父子神经辐射场 (PC-NeRF)。该框架基于父NeRF和子NeRF两个模块，实现了层次空间分区和多层次场景表示，包括场景、片段和点级别。多级场景表示增强了稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。经过大量实验，PC-NeRF被证明可以在大规模场景中实现高精度新颖的LiDAR视图合成和3D重建。此外，PC-NeRF 可以有效处理稀疏 LiDAR 帧的情况，并在有限的训练周期内展现出较高的部署效率。我们的方法实现和预训练模型可在 https://github.com/biter0088/pc-nerf 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.09325v1><br />
**Code:** <https://github.com/biter0088/pc-nerf>**<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy**<br />
**Title_cn:** 修剪稀疏张量神经网络支持 3D 超声定位显微镜的深度学习<br />
**Authors:** Brice Rauby, Paul Xing, Jonathan Porée, Maxime Gasse, Jean Provost<br />
**Abstract:** <details><summary>原文: </summary>Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.</details>
**Abstract_cn:** <details><summary>译文: </summary>超声定位显微镜 (ULM) 是一种非侵入性技术，可对体内微血管进行深度成像，分辨率约为 10 微米。 ULM 基于注射到血流中的单个微泡的亚分辨率定位。绘制整个血管结构需要从数千帧中积累微泡轨迹，通常需要几分钟的时间才能获得。通过增加微泡浓度可以减少 ULM 采集时间，但需要更先进的算法来单独检测它们。已经针对此任务提出了几种深度学习方法，但它们仍然仅限于 2D 成像，部分原因是相关的大内存需求。在此，我们建议使用稀疏张量神经网络来减少 2D 中的内存使用量，并提高深度学习架构扩展到 3D 时的内存需求规模。我们研究了几种有效地将超声数据转换为稀疏格式的方法，并研究了相关信息丢失的影响。当应用于 2D 时，与密集网络相比，稀疏公式将内存需求减少了 2 倍，但代价是性能略有下降。在 3D 中，所提出的方法将内存需求降低了两个数量级，同时在高浓度设置中大大优于传统的 ULM。我们表明，3D ULM 中的稀疏张量神经网络具有与 2D ULM 中基于密集深度学习的方法相同的优点，即在计算机中使用更高的浓度并减少采集时间。</details>
**PDF:** <http://arxiv.org/pdf/2402.09359v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge**<br />
**Title_cn:** RibFrac 挑战赛中 CT 的深部肋骨骨折实例分割和分类<br />
**Authors:** Jiancheng Yang, Rui Shi, Liang Jin, Xiaoyang Huang, Kaiming Kuang, Donglai Wei, Shixuan Gu, Jianying Liu, Pengfei Liu, Zhizhong Chai, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts. Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future. As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website. As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques. The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis.</details>
**Abstract_cn:** <details><summary>译文: </summary>肋骨骨折是一种常见且可能严重的损伤，在 CT 扫描中检测起来可能具有挑战性且费力。尽管人们一直在努力解决这一领域，但缺乏大规模注释数据集和评估基准阻碍了深度学习算法的开发和验证。为了解决这个问题，引入了 RibFrac 挑战赛，它提供了来自 660 次 CT 扫描的 5,000 多例肋骨骨折的基准数据集，以及四种临床类别（带状、非移位、移位或节段性）的体素级实例掩模注释和诊断标签。该挑战包括两个轨道：由 FROC 式度量评估的检测（实例分割）轨道和由 F1 式度量评估的分类轨道。 MICCAI 2020挑战赛期间，评估了243个结果，并邀请了7支队伍参加挑战总结。分析显示，几种顶级肋骨骨折检测解决方案的性能与人类专家相当甚至更好。然而，目前的肋骨骨折分类解决方案很难应用于临床，这可能是未来一个有趣的领域。作为活跃的基准和研究资源，RibFrac 挑战赛的数据和在线评估可在挑战赛网站上获取。作为一项独立贡献，我们还通过结合大规模预训练网络和基于点的肋骨分割技术的最新进展，扩展了之前的内部基线。由此产生的FracNet+在肋骨骨折检测方面表现出具有竞争力的性能，这为人工智能辅助肋骨骨折检测和诊断的进一步研究和开发奠定了基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.09372v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection**<br />
**Title_cn:** YOLOv8-AM：带有注意力机制的 YOLOv8，用于儿童手腕骨折检测<br />
**Authors:** Chun-Tse Chien, Rui-Yang Ju, Kuang-Yi Chou, Chien-Sheng Lin, Jen-Shiun Chiang<br />
**Abstract:** <details><summary>原文: </summary>Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist. With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD). In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body. Attention mechanism is one of the hottest methods to improve the model performance. This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture. Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement. Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.</details>
**Abstract_cn:** <details><summary>译文: </summary>日常生活中手腕外伤甚至骨折的情况时有发生，尤其是儿童，骨折病例中所占比例相当大。在进行手术之前，外科医生经常要求患者先进行X射线成像，并根据放射科医生的分析做好准备。随着神经网络的发展，YOLO系列模型已作为计算机辅助诊断（CAD）广泛应用于骨折检测。 2023 年，Ultralytics 推出了最新版本的 YOLO 模型，该模型已用于检测身体各个部位的骨折。注意力机制是提高模型性能最热门的方法之一。这项研究工作提出了YOLOv8-AM，它将注意力机制融入到原始的YOLOv8架构中。具体来说，我们分别采用四个注意力模块：卷积块注意力模块（CBAM）、全局注意力机制（GAM）、高效通道注意力（ECA）和随机注意力（SA）来设计改进模型并在GRAZPEDWRI-DX上进行训练数据集。实验结果表明，基于ResBlock + CBAM（ResCBAM）的YOLOv8-AM模型在IoU 50（mAP 50）下的平均精度从63.6%提高到65.8%，达到了state-of-the-art（SOTA）表现。相反，结合GAM的YOLOv8-AM模型获得了64.2%的mAP 50值，这并不是一个令人满意的增强。因此，我们将ResBlock和GAM结合起来，引入ResGAM设计另一个新的YOLOv8-AM模型，其mAP 50值提高到65.0%。</details>
**PDF:** <http://arxiv.org/pdf/2402.09329v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models**<br />
**Title_cn:** 只有我的模型在我的数据上：一种保护一个模型并欺骗未经授权的黑盒模型的隐私保护方法<br />
**Authors:** Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络广泛应用于现实世界的任务，例如人脸识别和医学图像分类，其中隐私和数据保护至关重要。图像数据如果不受保护，可能会被用来推断个人或上下文信息。现有的隐私保护方法（例如加密）会生成甚至人类都无法识别的扰动图像。对抗性攻击方法甚至禁止授权利益相关者进行自动推理，从而限制了商业和广泛适应的实际激励。这项开创性的研究通过生成人类可感知的图像来解决未经探索的实际隐私保护用例，这些图像保持授权模型的准确推理，同时避开其他具有相似或不同目标的未经授权的黑盒模型，并解决了之前的研究空白。使用的数据集是用于图像分类的 ImageNet、用于身份分类的 Celeba-HQ 数据集和用于情感分类的 AffectNet。我们的结果表明，生成的图像可以成功保持受保护模型的准确性，并将未经授权的黑盒模型的平均准确性在 ImageNet、Celeba-HQ 和 AffectNet 数据集上分别降低至 11.97%、6.63% 和 55.51% 。</details>
**PDF:** <http://arxiv.org/pdf/2402.09316v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Few-Shot Object Detection with Sparse Context Transformers**<br />
**Title_cn:** 使用稀疏上下文转换器进行少样本目标检测<br />
**Authors:** Jie Mei, Mingyuan Jiu, Hichem Sahbi, Xiaoheng Jiang, Mingliang Xu<br />
**Abstract:** <details><summary>原文: </summary>Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data. One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain. However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头检测是模式识别中的一项主要任务，旨在使用用少量标记数据训练的模型来定位对象。主流的小样本方法之一是迁移学习，它包括在目标域中微调检测模型之前在源域中预训练检测模型。然而，微调模型有效识别目标域中的新类具有挑战性，特别是当底层标记训练数据稀缺时。在本文中，我们设计了一种新颖的稀疏上下文变换器（SCT），它有效地利用源域中的对象知识，并从目标域中的少量训练图像中自动学习稀疏上下文。因此，它结合了不同的相关线索，以增强学习检测器的辨别能力并减少类别混乱。我们在两个具有挑战性的少样本目标检测基准上评估了所提出的方法，实证结果表明，与相关的最新技术相比，所提出的方法获得了有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.09315v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?**<br />
**Title_cn:** 在人类中可以立即泛化，但在深度神经网络中泛化滞后$\unicode{x2014}$表征分歧的证据？<br />
**Authors:** Lukas S. Huber, Fred W. Mast, Felix A. Wichmann<br />
**Abstract:** <details><summary>原文: </summary>Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\unicode{x2014}$and sometimes even exceeding that$\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field. However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究发现，在图像分类领域，人类和深度神经网络 (DNN) 之间存在许多行为比较。通常，比较研究通过测量和比较对象类别形成后的表示的相似性来关注学习过程的最终结果。然而，这些表征如何出现的过程$\unicode{x2014}$，即在获取$\unicode{x2014}$过程中观察到的行为变化和中间阶段，很少进行直接和经验性的比较。在这里，我们报告了关于如何在人类观察者和各种经典和最先进的 DNN 中获取可转移表征的详细研究。我们开发了一个受约束的监督学习环境，在其中调整与学习相关的参数，例如起点、输入模式、可用输入数据和提供的反馈。在整个学习过程中，我们评估和比较如何将学习到的表征推广到以前未见过的测试数据。我们的研究结果表明，就绝对分类性能而言，DNN 表现出的数据效率水平与$\unicode{x2014}$相当，有时甚至超过了$\unicode{x2014}$人类学习者，挑战了该领域的一些普遍假设。然而，整个学习过程的比较揭示了显着的表征差异：虽然 DNN 的学习具有明显的泛化滞后的特点，但人类似乎立即获得了可泛化的表征，而无需学习训练集特定信息的初步阶段，这些信息随后才会转移到新的模型中。数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.09303v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **TDViT: Temporal Dilated Video Transformer for Dense Video Tasks**<br />
**Title_cn:** TDViT：用于密集视频任务的时间扩张视频转换器<br />
**Authors:** Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson<br />
**Abstract:** <details><summary>原文: </summary>Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度视频模型（例如 3D CNN 或视频转换器）在稀疏视频任务（即预测每个视频的一个结果）上取得了良好的性能。然而，当将现有的深度视频模型应用于密集视频任务时，即预测每帧一个结果时，就会出现挑战。具体来说，这些模型的部署成本高昂，在处理冗余帧时效率较低，并且难以捕获远程时间相关性。为了克服这些问题，我们提出了一种时间扩张视频变换器（TDViT），它由精心设计的时间扩张变换器块（TDTB）组成。 TDTB可以有效地提取时空表示，并有效减轻时间冗余的负面影响。此外，通过使用分层 TDTB，我们的方法获得了指数扩展的时间感受野，因此可以对远程动态进行建模。在两个不同的密集视频基准上进行了大量的实验，即用于视频对象检测的 ImageNet VID 和用于视频实例分割的 YouTube VIS。出色的实验结果证明了我们方法的卓越效率、有效性和兼容性。代码可在 https://github.com/guanxiongsun/vfe.pytorch 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.09257v1><br />
**Code:** <https://github.com/guanxiongsun/vfe.pytorch>**<br />
>>**index:** 7<br />
**Title:** **Efficient One-stage Video Object Detection by Exploiting Temporal Consistency**<br />
**Title_cn:** 利用时间一致性的高效单阶段视频目标检测<br />
**Authors:** Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson<br />
**Abstract:** <details><summary>原文: </summary>Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，与传统的两级检测器相比，一级检测器在图像数据上取得了有竞争力的精度和更快的速度。然而，在视频对象检测（VOD）领域，大多数现有的VOD方法仍然基于两级检测器。此外，直接将现有的 VOD 方法应用于一级检测器会带来难以承受的计算成本。在本文中，我们首先分析了使用单级检测器进行 VOD 的计算瓶颈。基于分析，我们提出了一个简单而有效的框架来解决计算瓶颈，并通过利用视频帧的时间一致性来实现高效的单阶段 VOD。具体来说，我们的方法由一个用于过滤背景区域的位置先验网络和一个用于跳过特定帧的低级特征图上不必要的计算的尺寸先验网络组成。我们在各种现代一级检测器上测试我们的方法，并在 ImageNet VID 数据集上进行广泛的实验。出色的实验结果证明了我们的方法卓越的有效性、效率和兼容性。代码可在 https://github.com/guanxiongsun/vfe.pytorch 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.09241v1><br />
**Code:** <https://github.com/guanxiongsun/vfe.pytorch>**<br />
>>**index:** 8<br />
**Title:** **Switch EMA: A Free Lunch for Better Flatness and Sharpness**<br />
**Title_cn:** Switch EMA：更好的平坦度和清晰度的免费午餐<br />
**Authors:** Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.</details>
**Abstract_cn:** <details><summary>译文: </summary>指数移动平均 (EMA) 是一种广泛使用的权重平均 (WA) 正则化，用于学习平坦最优值以实现更好的泛化，而无需在深度神经网络 (DNN) 优化中产生额外成本。尽管实现了更好的平坦度，现有的 WA 方法可能会陷入更差的最终性能或需要额外的测试时间计算。这项工作通过单行修改揭示了 EMA 的全部潜力，即在每个 epoch 后将 EMA 参数切换到原始模型，称为 Switch EMA (SEMA)。从理论和实证方面，我们证明 SEMA 可以帮助 DNN 达到泛化最优，从而更好地在平坦度和清晰度之间进行权衡。为了验证SEMA的有效性，我们在视觉和语言数据集上进行了判别、生成和回归任务的比较实验，包括图像分类、自监督学习、对象检测和分割、图像生成、视频预测、属性回归和语言造型。流行优化器和网络的综合结果表明，SEMA 通过提高性能和提高收敛速度，成为 DNN 训练的免费午餐。</details>
**PDF:** <http://arxiv.org/pdf/2402.09240v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Is my Data in your AI Model? Membership Inference Test with Application to Face Images**<br />
**Title_cn:** 我的数据在你们的人工智能模型中吗？应用于人脸图像的隶属推理测试<br />
**Authors:** Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, Javier Ortega-Garcia<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了成员推理测试（MINT），这是一种新颖的方法，旨在凭经验评估在人工智能（AI）模型训练期间是否使用了特定数据。具体来说，我们提出了两种新颖的 MINT 架构，旨在学习当审计模型暴露于训练过程中使用的数据时出现的不同激活模式。第一个架构基于多层感知器 (MLP) 网络，第二个架构基于卷积神经网络 (CNN)。所提出的 MINT 架构在具有挑战性的人脸识别任务上进行了评估，考虑了三种最先进的人脸识别模型。实验使用六个公开数据库进行，总共包含超过 2200 万张人脸图像。此外，根据要测试的人工智能模型的可用上下文，会考虑不同的实验场景。使用我们提出的 MINT 方法实现了高达 90% 的准确率，这表明可以识别 AI 模型是否已经使用特定数据进行了训练。</details>
**PDF:** <http://arxiv.org/pdf/2402.09225v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration**<br />
**Title_cn:** 用于分布外校准的域自适应和子组特定级联温度回归<br />
**Authors:** Jiexin Wang, Jiahao Chen, Bing Su<br />
**Abstract:** <details><summary>原文: </summary>Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy. Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model. However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios. To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration. Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set. We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties. A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets. Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管深度神经网络在给定足够的训练数据的情况下可以产生很高的分类精度，但它们的预测通常过于自信或信心不足，即预测置信度不能真正反映准确性。事后校准通过校准预测置信度来解决这个问题，而无需重新训练分类模型。然而，当前的方法假设测试和验证数据分布之间是一致的，限制了它们对分布外场景的适用性。为此，我们提出了一种新颖的基于元集的级联温度回归方法，用于事后校准。我们的方法通过验证集上的数据增强来模拟各种域转换，从而针对不同的测试集定制细粒度的缩放函数。我们根据预测类别和置信水平将每个元集划分为子组，捕获不同的不确定性。然后训练回归网络来导出特定于类别和特定于置信水平的缩放，从而实现跨元集的校准。 MNIST、CIFAR-10 和 TinyImageNet 上的大量实验结果证明了该方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09204v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Crop and Couple: cardiac image segmentation using interlinked specialist networks**<br />
**Title_cn:** Crop and Couple：使用互连的专家网络进行心脏图像分割<br />
**Authors:** Abbas Khan, Muhammad Asad, Martin Benning, Caroline Roney, Gregory Slabaugh<br />
**Abstract:** <details><summary>原文: </summary>Diagnosis of cardiovascular disease using automated methods often relies on the critical task of cardiac image segmentation. We propose a novel strategy that performs segmentation using specialist networks that focus on a single anatomy (left ventricle, right ventricle, or myocardium). Given an input long-axis cardiac MR image, our method performs a ternary segmentation in the first stage to identify these anatomical regions, followed by cropping the original image to focus subsequent processing on the anatomical regions. The specialist networks are coupled through an attention mechanism that performs cross-attention to interlink features from different anatomies, serving as a soft relative shape prior. Central to our approach is an additive attention block (E-2A block), which is used throughout our architecture thanks to its efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用自动化方法诊断心血管疾病通常依赖于心脏图像分割的关键任务。我们提出了一种新颖的策略，使用专注于单一解剖结构（左心室、右心室或心肌）的专业网络进行分割。给定输入的长轴心脏 MR 图像，我们的方法在第一阶段执行三元分割以识别这些解剖区域，然后裁剪原始图像以将后续处理集中在解剖区域。专家网络通过注意力机制耦合，该机制对来自不同解剖结构的互连特征执行交叉注意力，充当软相对形状先验。我们方法的核心是附加注意力块（E-2A 块），由于其效率，它在我们的整个架构中使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.09156v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Solid Waste Detection in Remote Sensing Images: A Survey**<br />
**Title_cn:** 遥感图像中的固体废物检测：调查<br />
**Authors:** Piero Fraternali, Luca Morandini, Sergio Luis Herrera González<br />
**Abstract:** <details><summary>原文: </summary>The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills. This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data. Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented. Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>非法固体废物处置场的检测和特征分析对于环境保护，特别是减轻污染和健康危害至关重要。管理不当的垃圾填埋场会通过雨水渗透污染土壤和地下水，对动物和人类构成威胁。传统的垃圾填埋场识别方法（例如现场检查）既耗时又昂贵。遥感是识别和监测固体废物处置场的一种经济高效的解决方案，可实现广泛的覆盖范围和随着时间的推移进行重复采集。地球观测（EO）卫星配备了一系列传感器和成像功能，几十年来一直在提供高分辨率数据。研究人员提出了利用遥感图像执行一系列任务的专门技术，例如废物场检测、倾倒场监测以及评估新垃圾填埋场的合适位置。本综述旨在通过描述和比较方法、实施的技术和使用的数据，详细说明固体废物场地检测和监测的最相关建议。此外，由于数据源对于开发有效的固体废物检测模型至关重要，因此对卫星和公开数据集进行了全面概述。最后，本文指出了最新技术中的未解决问题，并讨论了降低成本和提高新型固体废物检测方法有效性的相关研究方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.09066v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision**<br />
**Title_cn:** 具有不配对掩码文本监督的开放词汇分割<br />
**Authors:** Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu<br />
**Abstract:** <details><summary>原文: </summary>Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>当代尖端的开放词汇分割方法通常依赖于图像-掩模-文本三元组，但这种受限注释是劳动密集型的，并且在复杂的现实场景中遇到可扩展性障碍。尽管提出了一些仅通过文本监督来降低标注成本的方法，但监督的不完整性严重限制了通用性和性能。在本文中，我们通过使用独立的图像-掩模和图像-文本对来解放掩模和文本之间的严格对应关系，这些对可以很容易地分别收集。通过这种不成对的掩码文本监督，我们提出了一种新的弱监督开放词汇分割框架（Uni-OVSeg），该框架利用文本描述中的掩码预测和实体的置信对。使用独立的图像-掩码和图像-文本对，我们预测一组二进制掩码，并通过 CLIP 嵌入空间将它们与实体相关联。然而，掩模和实体之间的对应关系中的固有噪声在获得可靠对时提出了重大挑战。有鉴于此，我们主张使用大型视觉语言模型（LVLM）来细化文本描述，并设计多尺度集成来稳定掩模和实体之间的匹配。与纯文本弱监督方法相比，我们的 Uni-OVSeg 在 ADE20K 数据集上实现了 15.5% mIoU 的大幅改进，甚至在具有挑战性的 PASCAL Context-459 数据集上超越了完全监督方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.08960v1><br />
**Code:** <https://github.com/derrickwang005/uni-ovseg.pytorch>**<br />
>>**index:** 14<br />
**Title:** **Learning-based Bone Quality Classification Method for Spinal Metastasis**<br />
**Title_cn:** 基于学习的脊柱转移骨质量分类方法<br />
**Authors:** Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao<br />
**Abstract:** <details><summary>原文: </summary>Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by $+12.54\%$, $+7.23\%$ and $+29.06\%$ for blastic, mixed and lytic lesions, respectively, meanwhile $+12.33\%$, $+23.21\%$ and $+34.25\%$ at vertebrae level.</details>
**Abstract_cn:** <details><summary>译文: </summary>脊柱转移是骨转移中最常见的疾病，可能导致疼痛、不稳定和神经损伤。早期发现脊柱转移瘤对于准确分期和最佳治疗至关重要。通常通过计算机断层扫描 (CT) 扫描来促进诊断，这需要训练有素的放射科医生付出巨大的努力。在本文中，我们探索了一种基于学习的基于CT图像的脊柱转移瘤自动骨质量分类方法。我们同时考虑后外侧脊柱受累分类任务，并采用多任务学习（MTL）技术来提高性能。 MTL 充当归纳偏差的一种形式，通过在相关任务之间共享表示来帮助模型更好地概括每个任务。基于混合类型可以被视为爆炸性和溶解性的先验知识，我们将骨质量分类任务建模为两个二元分类子任务，即是否爆炸性和是否溶解性，并利用多层感知器来组合他们的预测。为了使模型更加鲁棒，泛化能力更好，采用自定进度学习，逐步将样本从简单到复杂纳入训练过程。所提出的基于学习的方法在专有的脊柱转移 CT 数据集上进行了评估。在切片级别，我们的方法对于急变性、混合性和溶解性病变的灵敏度分别显着优于 121 层 DenseNet 分类器 $+12.54\%$、$+7.23\%$ 和 $+29.06\%$，同时 $+ 12.33\%$、$+23.21\%$ 和 $+34.25\%$ 在椎骨水平。</details>
**PDF:** <http://arxiv.org/pdf/2402.08910v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation**<br />
**Title_cn:** 迭代切片传播的椎体弱监督分割<br />
**Authors:** Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao<br />
**Abstract:** <details><summary>原文: </summary>Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>椎体（VB）分割是脊柱疾病医学视觉诊断的重要初步步骤。然而，之前的大多数工作都需要像素/体素方面的强监督，这对于专家来说是昂贵、繁琐且耗时的注释。在本文中，我们提出了一种弱监督迭代脊柱分割（WISS）方法，仅利用单个矢状切片上的四个角标志弱标签来实现 VB CT 图像的自动体积分割。 WISS 首先以迭代自训练方式在带注释的矢状切片上分割 VB。这种自我训练方法在训练和细化训练集中的标签之间交替进行。然后WISS继续使用切片传播方法对整个VB进行切片分割以获得体积分割。我们评估了 WISS 在私人脊柱转移瘤 CT 数据集和公共腰椎 CT 数据集上的性能。在第一个数据集上，WISS 在两个不同的主干网方面取得了明显的改进。对于第二个数据集，WISS 的中矢状切片和 3D CT 体积的骰子系数分别为 $91.7\%$ 和 $83.7\%$，节省了大量标记成本，并且只牺牲了一点分割性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.08892v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation**<br />
**Title_cn:** 用于视频对象分割的深度学习光流的移动对象建议<br />
**Authors:** Ge Shi, Zhili Yang<br />
**Abstract:** <details><summary>原文: </summary>Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.</details>
**Abstract_cn:** <details><summary>译文: </summary>动态场景理解是计算机视觉界最引人注目的领域之一。为了增强动态场景理解，神经网络的像素级分割被广泛接受。像素级分割的最新研究结合了语义和运动信息，并产生了良好的性能。在这项工作中，我们提出了一种最先进的神经网络架构，以准确有效地获得移动对象建议（MOP）。我们首先训练无监督卷积神经网络（UnFlow）来生成光流估计。然后我们将光流网络的输出渲染为全卷积 SegNet 模型。我们工作的主要贡献是（1）在全新的 DAVIS 数据集上微调预训练的光流模型； (2) 利用具有编码器-解码器架构的全卷积神经网络来分割对象。我们使用 TensorFlow 开发代码，并在 AWS EC2 实例上执行训练和评估过程。</details>
**PDF:** <http://arxiv.org/pdf/2402.08882v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition**<br />
**Title_cn:** TikTokActions：用于人类动作识别的 TikTok 衍生视频数据集<br />
**Authors:** Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington<br />
**Abstract:** <details><summary>原文: </summary>The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>TikTok 等平台上标记多媒体内容的种类和数量不断增加，为推进计算机视觉建模提供了机会。我们整理了一个独特的数据集，其中包含 283,582 个独特的视频剪辑，这些视频剪辑被分类为与现代人类行为相关的 386 个主题标签。我们发布此数据集作为宝贵的资源，用于为人体运动建模任务（例如动作识别）构建特定领域的基础模型。为了验证这个数据集（我们将其命名为 TikTokActions），我们进行了两组实验。首先，我们在 TikTokActions 子集上使用基于 ViT 的骨干网对最先进的 VideoMAEv2 进行预训练，然后对 UCF101 和 HMDB51 等流行数据集进行微调和评估。我们发现使用 Tik-Tok 数据集预训练的模型的性能与在更大的动作识别数据集上训练的模型相当（UCF101 上为 95.3%，HMDB51 上为 53.24%）。此外，我们对预训练数据集大小和微调性能之间关系的调查表明，超过某个阈值，较大训练集的增量收益就会减弱。这项工作介绍了一个有用的 TikTok 视频数据集，可供公众使用，并深入了解增加基于视频的基础模型的预训练数据集大小的边际效益。</details>
**PDF:** <http://arxiv.org/pdf/2402.08875v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Depth-aware Volume Attention for Texture-less Stereo Matching**<br />
**Title_cn:** 用于无纹理立体匹配的深度感知体积注意<br />
**Authors:** Tong Zhao, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka, Yintao Wei<br />
**Abstract:** <details><summary>原文: </summary>Stereo matching plays a crucial role in 3D perception and scenario understanding. Despite the proliferation of promising methods, addressing texture-less and texture-repetitive conditions remains challenging due to the insufficient availability of rich geometric and semantic information. In this paper, we propose a lightweight volume refinement scheme to tackle the texture deterioration in practical outdoor scenarios. Specifically, we introduce a depth volume supervised by the ground-truth depth map, capturing the relative hierarchy of image texture. Subsequently, the disparity discrepancy volume undergoes hierarchical filtering through the incorporation of depth-aware hierarchy attention and target-aware disparity attention modules. Local fine structure and context are emphasized to mitigate ambiguity and redundancy during volume aggregation. Furthermore, we propose a more rigorous evaluation metric that considers depth-wise relative error, providing comprehensive evaluations for universal stereo matching and depth estimation models. We extensively validate the superiority of our proposed methods on public datasets. Results demonstrate that our model achieves state-of-the-art performance, particularly excelling in scenarios with texture-less images. The code is available at https://github.com/ztsrxh/DVANet.</details>
**Abstract_cn:** <details><summary>译文: </summary>立体匹配在 3D 感知和场景理解中起着至关重要的作用。尽管有前景的方法不断涌现，但由于丰富的几何和语义信息的可用性不足，解决无纹理和纹理重复的条件仍然具有挑战性。在本文中，我们提出了一种轻量级体积细化方案来解决实际户外场景中的纹理恶化问题。具体来说，我们引入了由地面真实深度图监督的深度体积，捕获图像纹理的相对层次结构。随后，通过结合深度感知层次注意力和目标感知视差注意力模块，对视差差异量进行层次过滤。强调局部精细结构和上下文，以减少卷聚合期间的歧义和冗余。此外，我们提出了一种更严格的评估指标，考虑深度方面的相对误差，为通用立体匹配和深度估计模型提供综合评估。我们在公共数据集上广泛验证了我们提出的方法的优越性。结果表明，我们的模型实现了最先进的性能，尤其是在无纹理图像的场景中表现出色。该代码可从 https://github.com/ztsrxh/DVANet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.08931v1><br />
**Code:** <https://github.com/ztsrxh/DVANet>**<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Less is More: Fewer Interpretable Region via Submodular Subset Selection**<br />
**Title_cn:** 少即是多：通过子模子集选择减少可解释区域<br />
**Authors:** Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像归因算法旨在识别与模型决策高度相关的重要区域。尽管现有的归因解决方案可以有效地分配目标元素的重要性，但仍然面临以下挑战：1）现有的归因方法生成不准确的小区域，从而误导了正确归因的方向；2）模型无法对错误的样本产生良好的归因结果预测。为了解决上述挑战，本文将上述图像归因问题重新建模为子模子集选择问题，旨在使用更少的区域来增强模型的可解释性。为了解决对局部区域缺乏关注的问题，我们构建了一种新颖的子模函数来发现更准确的细粒度解释区域。为了增强所有样本的归因效果，我们还对子区域的选择施加了四种不同的约束，即置信度、有效性、一致性和协作分数，以评估各个子集的重要性。此外，我们的理论分析证实所提出的函数实际上是子模的。大量实验表明，所提出的方法在两个人脸数据集（Celeb-A 和 VGG-Face2）和一个细粒度数据集（CUB-200-2011）上优于 SOTA 方法。对于正确预测的样本，所提出的方法相对于 HSIC-Attribution 提高了删除和插入分数，平均提高了 4.9% 和 2.5%。对于错误预测的样本，与 HSIC-Attribution 算法相比，我们的方法在平均最高置信度和插入分数方面分别实现了 81.0% 和 18.4% 的增益。代码发布于https://github.com/RuoyuChen10/SMDL-Attribution。</details>
**PDF:** <http://arxiv.org/pdf/2402.09164v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Pyramid Attention Network for Medical Image Registration**<br />
**Title_cn:** 用于医学图像配准的金字塔注意力网络<br />
**Authors:** Zhuoyuan Wang, Haiqiao Wang, Yi Wang<br />
**Abstract:** <details><summary>原文: </summary>The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.The pure convolutional neural networks (CNNs) neglect feature enhancement, while current Transformer-based networks are susceptible to information redundancy.To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.Moreover, a multi-head local attention Transformer is introduced as decoder to analyze motion patterns and generate deformation fields.Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several CNN-based and Transformer-based registration networks.Our code is publicly available at https://github.com/JuliusWang-7/PAN.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习的配准网络的出现解决了传统迭代方法中耗时的挑战。然而，当前配准网络全面捕获空间关系的潜力尚未得到充分挖掘，导致在大变形图像中表现不足纯卷积神经网络（CNN）忽略特征增强，而当前基于 Transformer 的网络容易受到信息冗余的影响。为了缓解这些问题，我们提出了一种用于可变形医学图像配准的金字塔注意网络（PAN）。具体来说，提出了PAN 结合了具有通道注意力的双流金字塔编码器来增强特征表示。此外，引入多头局部注意力 Transformer 作为解码器来分析运动模式并生成变形场。在两个公共脑磁共振成像上进行了广泛的实验(MRI) 数据集和一个腹部 MRI 数据集表明，我们的方法实现了良好的配准性能，同时优于几个基于 CNN 和基于 Transformer 的配准网络。我们的代码可在 https://github.com/JuliusWang-7/PAN 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.09016v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding**<br />
**Title_cn:** CLIP-MUSED：CLIP引导的多主题视觉神经信息语义解码<br />
**Authors:** Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He<br />
**Abstract:** <details><summary>原文: </summary>The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于个体差异，解码视觉神经信息的研究面临着将单受试者解码模型推广到多个受试者的挑战。此外，来自单个主题的数据的有限可用性会对模型性能产生限制性影响。尽管现有的多对象解码方法取得了显着进展，但它们仍然存在一些局限性，包括难以提取全局神经反应特征、模型参数随受试者数量线性缩放以及对不同神经反应之间关系的刻画不够充分。会受到各种刺激。为了克服这些限制，我们提出了一种 CLIP 引导的多主体视觉神经信息语义解码（CLIP-MUSED）方法。我们的方法由基于 Transformer 的特征提取器组成，可以有效地对全局神经表示进行建模。它还包含可学习的特定于主题的标记，有助于多主题数据的聚合，而无需线性增加参数。此外，我们采用表征相似性分析（RSA）来指导基于CLIP表征空间中视觉刺激的拓扑关系的标记表征学习，从而能够充分表征不同受试者在不同刺激下的神经反应之间的关系。最后，令牌表示用于多主题语义解码。我们提出的方法优于单受试者解码方法，并在两个 fMRI 数据集上实现了现有多受试者方法中最先进的性能。可视化结果让我们深入了解我们提出的方法的有效性。代码可在 https://github.com/CLIP-MUSED/CLIP-MUSED 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.08994v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Predictive Temporal Attention on Event-based Video Stream for Energy-efficient Situation Awareness**<br />
**Title_cn:** 基于事件的视频流的预测时间注意力以实现节能态势感知<br />
**Authors:** Yiming Bu, Jiayang Liu, Qinru Qiu<br />
**Abstract:** <details><summary>原文: </summary>The Dynamic Vision Sensor (DVS) is an innovative technology that efficiently captures and encodes visual information in an event-driven manner. By combining it with event-driven neuromorphic processing, the sparsity in DVS camera output can result in high energy efficiency. However, similar to many embedded systems, the off-chip communication between the camera and processor presents a bottleneck in terms of power consumption. Inspired by the predictive coding model and expectation suppression phenomenon found in human brain, we propose a temporal attention mechanism to throttle the camera output and pay attention to it only when the visual events cannot be well predicted. The predictive attention not only reduces power consumption in the sensor-processor interface but also effectively decreases the computational workload by filtering out noisy events. We demonstrate that the predictive attention can reduce 46.7% of data communication between the camera and the processor and reduce 43.8% computation activities in the processor.</details>
**Abstract_cn:** <details><summary>译文: </summary>动态视觉传感器 (DVS) 是一项创新技术，能够以事件驱动的方式有效捕获和编码视觉信息。通过将其与事件驱动的神经拟态处理相结合，DVS 摄像机输出的稀疏性可以带来高能效。然而，与许多嵌入式系统类似，相机和处理器之间的片外通信在功耗方面存在瓶颈。受人脑中发现的预测编码模型和期望抑制现象的启发，我们提出了一种时间注意力机制来限制相机输出，并仅在无法很好预测视觉事件时才关注它。预测注意力不仅降低了传感器-处理器接口的功耗，而且还通过滤除噪声事件有效地减少了计算工作量。我们证明，预测注意力可以减少相机和处理器之间 46.7% 的数据通信，并减少处理器中 43.8% 的计算活动。</details>
**PDF:** <http://arxiv.org/pdf/2402.08936v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Registration of Longitudinal Spine CTs for Monitoring Lesion Growth**<br />
**Title_cn:** 用于监测病变生长的纵向脊柱 CT 配准<br />
**Authors:** Malika Sanhinova, Nazim Haouchine, Steve D. Pieper, William M. Wells III, Tracy A. Balboni, Alexander Spektor, Mai Anh Huynh, Jeffrey P. Guenette, Bryan Czajkowski, Sarah Caplan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Accurate and reliable registration of longitudinal spine images is essential for assessment of disease progression and surgical outcome. Implementing a fully automatic and robust registration is crucial for clinical use, however, it is challenging due to substantial change in shape and appearance due to lesions. In this paper we present a novel method to automatically align longitudinal spine CTs and accurately assess lesion progression. Our method follows a two-step pipeline where vertebrae are first automatically localized, labeled and 3D surfaces are generated using a deep learning model, then longitudinally aligned using a Gaussian mixture model surface registration. We tested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3, 6, and 12 months follow-ups leading to 111 registrations. Our experiment showed accurate registration with an average Hausdorff distance of 0.65 mm and average Dice score of 0.92.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确可靠的纵向脊柱图像配准对于评估疾病进展和手术结果至关重要。实现全自动和稳健的配准对于临床使用至关重要，然而，由于病变导致形状和外观发生巨大变化，这具有挑战性。在本文中，我们提出了一种自动对齐纵向脊柱 CT 并准确评估病变进展的新方法。我们的方法遵循两步流程，首先使用深度学习模型自动定位、标记椎骨并生成 3D 表面，然后使用高斯混合模型表面配准进行纵向对齐。我们在 5 名患者的 37 块椎骨上测试了我们的方法，并进行了基线 CT 扫描和 3、6 和 12 个月的随访，最终获得了 111 例注册。我们的实验显示配准准确，平均 Hausdorff 距离为 0.65 毫米，平均 Dice 得分为 0.92。</details>
**PDF:** <http://arxiv.org/pdf/2402.09341v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling**<br />
**Title_cn:** DUDF：具有双曲标度的可微分无符号距离场<br />
**Authors:** Miguel Fainstein, Viviana Siless, Emmanuel Iarussi<br />
**Abstract:** <details><summary>原文: </summary>In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，人们越来越关注训练神经网络来近似无符号距离场 (UDF)，以在 3D 重建的背景下表示开放表面。然而，UDF 在零水平集上是不可微的，这会导致距离和梯度出现显着误差，通常会导致表面碎片化和不连续。在本文中，我们建议学习无符号距离场的双曲缩放，它定义了具有不同边界条件的新 Eikonal 问题。这使得我们的公式能够与最先进的连续可微隐式神经表示网络无缝集成，该网络在文献中主要用于表示带符号的距离场。我们的方法不仅解决了开放表面表示的挑战，而且还展示了重建质量和训练性能的显着改进。此外，未锁定场的可微性允许精确计算基本拓扑属性，例如法线方向和曲率，普遍存在于渲染等下游任务中。通过广泛的实验，我们在各种数据集上并对照竞争基线验证了我们的方法。结果表明，与以前的方法相比，精度提高了，速度提高了一个数量级。</details>
**PDF:** <http://arxiv.org/pdf/2402.08876v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency**<br />
**Title_cn:** 利用生成式人工智能和几何一致性进行本地化防风雨检索<br />
**Authors:** Yannis Kalantidis, Mert Bülent Sarıyıldız, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka<br />
**Abstract:** <details><summary>原文: </summary>State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: https://europe.naverlabs.com/ret4loc</details>
**Abstract_cn:** <details><summary>译文: </summary>最先进的视觉定位方法通常依赖于第一个图像检索步骤，其作用至关重要。然而，在面对不同的条件时，检索常常会遇到困难，例如，天气或一天中的时间，对视觉定位精度产生巨大影响。在本文中，我们改进了这个检索步骤，并将其调整为最终的本地化任务。在我们提倡的几项改变中，我们建议合成从生成文本到图像模型获得的训练集图像的变体，以便自动将训练集扩展到许多特别损害视觉定位的可命名变体。扩展训练集后，我们提出了一种训练方法，该方法利用真实图像和合成图像混合的特殊性和底层几何结构。我们通过实验表明，这些变化可以转化为最具挑战性的视觉定位数据集的巨大改进。项目页面：https://europe.naverlabs.com/ret4loc</details>
**PDF:** <http://arxiv.org/pdf/2402.09237v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process**<br />
**Title_cn:** Traj-LIO：通过稀疏高斯过程的弹性多 LiDAR 多 IMU 状态估计器<br />
**Authors:** Xin Zheng, Jianke Zhu<br />
**Abstract:** <details><summary>原文: </summary>Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure. It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data. To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states. Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures. Moreover, we replace the conventional $\mathrm{SE}(3)$ state representation with the combination of $\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement. Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the community, we will make our source code publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>如今，传感器套件已配备冗余 LiDAR 和 IMU，以减轻与传感器故障相关的风险。对于以前的离散时间和 IMU 驱动的运动系统来说，整合多个异步传感器是一项挑战，这些传感器容易受到异常 IMU 数据的影响。为了解决这些限制，我们引入了一种多 LiDAR 多 IMU 状态估计器，利用高斯过程 (GP) 预测非参数连续时间轨迹，以捕获具有有限控制状态的传感器的时空运动。由于由三种类型的线性时不变随机微分方程驱动的运动学模型独立于外部传感器测量，因此我们提出的方法能够处理不同的传感器配置并对传感器故障具有弹性。此外，我们将传统的$\mathrm{SE}(3)$状态表示替换为$\mathrm{SO}(3)$和向量空间的组合，这使得基于GP的LiDAR惯性系统能够实现实数时间要求。对公共数据集的大量实验证明了我们提出的多 LiDAR 多 IMU 状态估计器的多功能性和弹性。为了为社区做出贡献，我们将公开源代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.09189v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Generalized Portrait Quality Assessment**<br />
**Title_cn:** 广义肖像质量评估<br />
**Authors:** Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce<br />
**Abstract:** <details><summary>原文: </summary>Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动化且强大的肖像质量评估 (PQA) 在智能手机摄影等高影响力应用中至关重要。本文提出了 FHIQA，一种基于学习的 PQA 方法，引入了一种基于图像语义的简单但有效的质量分数重新缩放方法，以提高细粒度图像质量指标的精度，同时确保对训练数据集之外的各种场景设置的鲁棒泛化。所提出的方法通过 PIQ23 基准的大量实验以及与当前技术水平的比较得到验证。 FHIQA 的源代码将在 PIQ23 GitHub 存储库（https://github.com/DXOMARK-Research/PIQ2023）上公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.09178v1><br />
**Code:** <https://github.com/dxomark-research/piq2023>**<br />

