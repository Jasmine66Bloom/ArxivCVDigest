## [UPDATED!] **2024-02-15** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation**<br />
**Title_cn:** 用于文本到图像生成的扩散模型的自玩微调<br />
**Authors:** Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu<br />
**Abstract:** <details><summary>原文: </summary>Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.</details>
**Abstract_cn:** <details><summary>译文: </summary>微调扩散模型仍然是生成人工智能（GenAI）领域尚未开发的前沿领域，特别是与微调大型语言模型（LLM）方面取得的显着进展相比。虽然稳定扩散 (SD) 和 SDXL 等尖端扩散模型依赖于监督微调，但在看到一定量的数据后，它们的性能不可避免地会趋于稳定。最近，强化学习（RL）已被用来利用人类偏好数据来微调扩散模型，但每个文本提示至少需要两张图像（“获胜者”和“失败者”图像）。在本文中，我们介绍了一种称为扩散模型自我调整微调（SPIN-Diffusion）的创新技术，其中扩散模型与其早期版本进行竞争，促进迭代的自我改进过程。我们的方法提供了传统监督微调和强化学习策略的替代方案，显着提高了模型性能和一致性。我们在 Pick-a-Pic 数据集上的实验表明，SPIN-Diffusion 从第一次迭代起就在人类偏好对齐和视觉吸引力方面优于现有的监督微调方法。到第二次迭代时，它在所有指标上都超过了基于 RLHF 的方法的性能，用更少的数据实现了这些结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.10210v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Recovering the Pre-Fine-Tuning Weights of Generative Models**<br />
**Title_cn:** 恢复生成模型的预微调权重<br />
**Authors:** Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen<br />
**Abstract:** <details><summary>原文: </summary>The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成建模的主导范式包括两个步骤：i）在大规模但不安全的数据集上进行预训练，ii）通过微调使预训练模型与人类价值观保持一致。这种做法被认为是安全的，因为当前没有方法可以恢复不安全的预微调模型权重。在本文中，我们证明这种假设通常是错误的。具体来说，我们提出了 Spectral DeTuning，一种可以使用一些低秩（LoRA）微调模型来恢复预微调模型权重的方法。与之前尝试恢复预微调能力的攻击相比，我们的方法旨在恢复精确的预微调权重。我们的方法针对大规模模型（例如个性化稳定扩散和对齐的米斯特拉尔）利用了这一新漏洞。</details>
**PDF:** <http://arxiv.org/pdf/2402.10208v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model**<br />
**Title_cn:** 条件去噪扩散模型的射电天文图像重建<br />
**Authors:** Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer, Slava Voloshynovskiy<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model's performance across different water vapor levels. Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies. While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research.</details>
**Abstract_cn:** <details><summary>译文: </summary>从脏射电图像重建天空模型以实现准确的源定位和通量估计对于研究高红移下的星系演化至关重要，尤其是在使用阿塔卡马大型毫米波阵列（ALMA）等仪器的深场中。随着平方公里阵列 (SKA) 等新项目的开展，对更好的源提取方法的需求日益增长。当前的技术，例如 CLEAN 和 PyBDSF，通常无法检测微弱的光源，这凸显了对更准确方法的需求。这项研究提出使用随机神经网络直接从脏图像重建天空模型。该方法可以精确定位无线电源并测量其具有相关不确定性的通量，这标志着无线电源表征的潜在改进。我们基于 ALMA 的 Cycle 5.3 天线设置，在使用 CASA 工具 simalma 模拟的 10164 个图像上测试了这种方法。我们应用条件去噪扩散概率模型 (DDPM) 进行天空模型重建，然后使用 Photutils 确定源坐标和通量，评估模型在不同水汽水平下的性能。我们的方法在源定位方面表现出色，在信噪比 (SNR) 低至 2 的情况下实现了 90% 以上的完整性。它在通量估计方面也超越了 PyBDSF，准确识别了测试集中 96% 的源的通量，比 CLEAN+ PyBDSF 的 57% 显着提高。条件 DDPM 是图像到图像转换的强大工具，可以准确、稳健地表征无线电源，并且性能优于现有方法。虽然这项研究强调了其在射电天文学中应用的巨大潜力，但我们也承认其使用所伴随的某些局限性，并提出了进一步完善和研究的方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.10204v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network**<br />
**Title_cn:** 通过实例分割神经网络在人类视网膜图像中进行鲁棒的半自动血管追踪<br />
**Authors:** Siyi Chen, Amir H. Kashani, Ji Yi<br />
**Abstract:** <details><summary>原文: </summary>The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map. We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net. We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information. InSegNN paves a way for any subsequent morphological analysis of vascular morphology in relation to retinal diseases.</details>
**Abstract_cn:** <details><summary>译文: </summary>血管系统的形态和层次对于支持新陈代谢的灌注至关重要。人类视网膜是最需要能量的器官之一，视网膜循环通过在视神经乳头 (ONH) 处出现和重新出现的复杂脉管系统来滋养整个视网膜内层。因此，通过血管树追踪从 ONH 开始的血管分支可以说明血管层次结构并允许详细的形态学量化，但仍然是一项具有挑战性的任务。在这里，我们提出了一种通过实例分割神经网络（InSegNN）对人体眼底图像进行鲁棒半自动血管追踪算法的新方法。与语义分割不同，InSegNN 单独分离和标记不同的血管树，因此能够跟踪每棵树的整个分支。我们内置了三种策略来通过时间学习、空间多重采样和动态概率图来提高鲁棒性和准确性。与文献相比，我们在对称最佳骰子 (SBD) 方面实现了 83% 的特异性和 50% 的改进，并且优于基线 U-net。我们已经演示了从眼底图像中追踪单个血管树，并同时保留血管层次结构信息。 InSegNN 为与视网膜疾病相关的血管形态的后续形态学分析铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.10055v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition**<br />
**Title_cn:** 应用于面部表情识别的数据增强和迁移学习方法<br />
**Authors:** Enrico Randellini, Leonardo Rigutini, Claudio Sacca'<br />
**Abstract:** <details><summary>原文: </summary>The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\% for the InceptionResNetV2 model.</details>
**Abstract_cn:** <details><summary>译文: </summary>当我们想要了解一个人的心理状态时，首先要关注的是面部表情。因此，自动识别面部表情的能力是一个非常有趣的研究领域。在本文中，由于可用训练数据集的规模较小，我们提出了一种新颖的数据增强技术，可以提高识别任务的性能。我们应用几何变换并从头开始构建 GAN 模型，能够为每种情感类型生成新的合成图像。因此，在增强数据集上，我们对具有不同架构的预训练卷积神经网络进行微调。为了衡量模型的泛化能力，我们采用了数据库外协议方法，即我们在训练数据集的增强版本上训练模型，并在两个不同的数据库上测试它们。这些技术的结合使得 InceptionResNetV2 模型的平均准确率达到 85% 左右。</details>
**PDF:** <http://arxiv.org/pdf/2402.09982v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation**<br />
**Title_cn:** 文本本地化：分解多概念图像以生成主题驱动的文本到图像<br />
**Authors:** Junjie Shentu, Matthew Watson, Noura Al Moubayed<br />
**Abstract:** <details><summary>原文: </summary>Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images. During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt. Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.</details>
**Abstract_cn:** <details><summary>译文: </summary>主题驱动的文本到图像扩散模型使用户能够使用一些示例图像根据预训练数据集中缺少的新概念定制模型。然而，流行的主题驱动模型主要依赖于单概念输入图像，在处理多概念输入图像时面临着指定目标概念的挑战。为此，我们引入了文本本地化文本到图像模型（Texual Localization）来处理多概念输入图像。在微调过程中，我们的方法采用了一种新颖的交叉注意指导来分解多个概念，在目标概念的视觉表示和文本提示中的标识符标记之间建立明显的联系。实验结果表明，我们的方法在多概念输入图像的图像保真度和图像文本对齐方面优于或与基线模型相当。与自定义扩散相比，我们的硬指导方法在单概念和多概念生成方面的 CLIP-I 分数分别提高了 7.04%、8.13%，CLIP-T 分数分别提高了 2.22%、5.85%。值得注意的是，我们的方法生成与生成图像中的目标概念一致的交叉注意力图，这是现有模型中不具备的功能。</details>
**PDF:** <http://arxiv.org/pdf/2402.09966v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Lester: rotoscope animation through video object segmentation and tracking**<br />
**Title_cn:** Lester：通过视频对象分割和跟踪制作转描动画<br />
**Authors:** Ruben Tous<br />
**Abstract:** <details><summary>原文: </summary>This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 Lester，一种从视频中自动合成复古风格 2D 动画的新颖方法。该方法主要将挑战视为对象分割和跟踪问题。视频帧使用分段任意模型 (SAM) 进行处理，并使用 DeAOT（一种半监督视频对象分割的分层传播方法）通过后续帧跟踪生成的掩模。使用 Douglas-Peucker 算法简化了掩模轮廓的几何形状。最后，可以选择添加面部特征、像素化和基本阴影效果。结果表明，该方法表现出良好的时间一致性，能够正确处理不同姿势和外观、动态镜头、局部镜头和不同背景的视频。与基于扩散模型的视频到视频转换管道相比，所提出的方法提供了更简单和确定性的方法，后者存在时间一致性问题并且不能很好地处理像素化和示意性输出。该方法也比基于 3D 人体姿势估计的技术更实用，后者需要定制的手工 3D 模型，并且它们可以处理的场景类型非常有限。</details>
**PDF:** <http://arxiv.org/pdf/2402.09883v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization**<br />
**Title_cn:** DreamMatcher：外观匹配自我关注，实现语义一致的文本到图像个性化<br />
**Authors:** Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, Seunggyu Chang<br />
**Abstract:** <details><summary>原文: </summary>The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像 (T2I) 个性化的目标是根据用户提供的参考概念定制扩散模型，生成与目标提示一致的概念的不同图像。使用独特的文本嵌入来表示参考概念的传统方法通常无法准确模仿参考的外观。为了解决这个问题，一种解决方案可以明确地将参考图像调节到目标去噪过程中，称为键值替换。然而，之前的工作仅限于本地编辑，因为它们破坏了预训练 T2I 模型的结构路径。为了克服这个问题，我们提出了一种新颖的插件方法，称为 DreamMatcher，它将 T2I 个性化重新表述为语义匹配。具体来说，DreamMatcher 将目标值替换为通过语义匹配对齐的参考值，同时保持结构路径不变，以保留预训练 T2I 模型生成不同结构的多功能能力。我们还引入了语义一致的屏蔽策略，将个性化概念与目标提示引入的不相关区域隔离开来。 DreamMatcher 与现有 T2I 模型兼容，在复杂场景中显示出显着改进。深入的分析证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09812v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model**<br />
**Title_cn:** 检查生成对抗网络鉴别器中的病理偏差：StyleGAN3 模型的案例研究<br />
**Authors:** Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter<br />
**Abstract:** <details><summary>原文: </summary>Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成对抗网络生成逼真的面孔，人类通常无法将其与真实面孔区分开。我们发现，预先训练的 StyleGAN3 模型（一种流行的 GAN 网络）中的判别器会根据图像和面部质量对分数进行系统分层，这对跨性别、种族和其他类别的图像产生不成比例的影响。我们检查了鉴别器在感知种族和性别的轴上对颜色和亮度的偏见；然后我们研究社会心理学刻板印象研究中常见的轴。</details>
**PDF:** <http://arxiv.org/pdf/2402.09786v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement**<br />
**Title_cn:** 具有交叉注意力的扩散模型作为解开的归纳偏差<br />
**Authors:** Tao Yang, Cuiling Lan, Yan Lu, Nanning zheng<br />
**Abstract:** <details><summary>原文: </summary>Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.</details>
**Abstract_cn:** <details><summary>译文: </summary>解缠结表示学习致力于提取观察数据中的内在因素。以无监督的方式分解这些表示是非常具有挑战性的，通常需要定制的损失函数或特定的结构设计。在本文中，我们引入了一个新的视角和框架，证明具有交叉注意力的扩散模型可以作为强大的归纳偏差来促进解缠结表示的学习。我们建议将图像编码为一组概念标记，并将它们视为图像重建的潜在扩散的条件，其中对概念标记的交叉关注用于桥接编码器和扩散之间的交互。在没有任何额外的正则化的情况下，该框架在基准数据集上实现了卓越的解缠结性能，超越了以前所有具有复杂设计的方法。我们进行了全面的消融研究和可视化分析，阐明了该模型的功能。这是第一项揭示具有交叉注意力的扩散模型的强大解缠能力的工作，不需要复杂的设计。我们预计我们的发现将激发更多的研究，探索解纠缠表示学习的扩散，以实现更复杂的数据分析和理解。</details>
**PDF:** <http://arxiv.org/pdf/2402.09712v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Prompt-based Personalized Federated Learning for Medical Visual Question Answering**<br />
**Title_cn:** 基于提示的个性化联合学习医学视觉问答<br />
**Authors:** He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama<br />
**Abstract:** <details><summary>原文: </summary>We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods. Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client. To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters. In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients. Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的基于提示的个性化联合学习（pFL）方法，以解决传统医学视觉问答（VQA）方法中的数据异构性和隐私问题。具体来说，我们将来自不同器官的医学数据集视为客户端，并使用 pFL 为每个客户端训练基于 Transformer 的个性化 VQA 模型。为了解决以前的 pFL 方法中客户端到客户端通信的高计算复杂性，我们通过引入小的可学习参数提示，提出了一种简洁的信息共享系统。此外，所提出的方法引入了可靠性参数，以防止低性能和不相关客户端的负面影响。最后，对各种异构医学数据集的广泛评估证明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09677v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding**<br />
**Title_cn:** MM-Point：多视图信息增强的多模态自监督 3D 点云理解<br />
**Authors:** Hai-Tao Yu, Mofei Song<br />
**Abstract:** <details><summary>原文: </summary>In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在感知中，集成多种感官信息，将视觉信息从 2D 视图映射到 3D 物体，这有利于 3D 环境中的理解。但对于从不同角度渲染的单个2D视图而言，只能提供有限的部分信息。多视图2D信息的丰富性和价值可以为3D对象提供优越的自监督信号。在本文中，我们提出了一种新颖的自监督点云表示学习方法MM-Point，该方法由模态内和模态间相似性目标驱动。 MM-Point的核心在于3D对象与多个2D视图同时进行多模态交互和传输。为了更有效地同时执行基于对比学习的 2D 多视图信息的一致跨模态目标，我们进一步提出了 Multi-MLP 和 Multi-level Augmentation 策略。通过精心设计的变换策略，我们进一步学习2D多视图中的多级不变性。 MM-Point 在各种下游任务中展示了最先进的 (SOTA) 性能。例如，它在合成数据集 ModelNet40 上实现了 92.4% 的峰值准确率，在现实数据集 ScanObjectNN 上实现了 87.8% 的最高准确率，与完全监督的方法相当。此外，我们还展示了其在少镜头分类、3D 零件分割和 3D 语义分割等任务中的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10002v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition**<br />
**Title_cn:** 法学硕士作为桥梁：重新制定扎根多模态命名实体识别<br />
**Authors:** Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan<br />
**Abstract:** <details><summary>原文: </summary>Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>扎根多模态命名实体识别（GMNER）是一项新兴的多模态任务，旨在识别命名实体、实体类型及其相应的视觉区域。 GMNER 任务表现出两个具有挑战性的特性：1）社交媒体中图像文本对之间的弱相关性导致很大一部分命名实体无法接地。 2）类似任务中常用的粗粒度指称表达式（例如，短语本地化、指称表达式理解）和细粒度命名实体之间存在区别。在本文中，我们提出了 RiVEG，这是一个统一框架，通过利用大语言模型（LLM）作为连接桥梁，将 GMNER 重新构建为联合 MNER-VE-VG 任务。这种重新表述带来了两个好处：1）它保持了最佳的 MNER 性能，并且无需使用目标检测方法来预先提取区域特征，从而自然地解决了现有 GMNER 方法的两个主要局限性。 2）引入实体扩展表达和视觉蕴涵（VE）模块，统一了视觉基础（VG）和实体基础（EG）。它使 RiVEG 能够轻松继承任何当前或未来多模态预训练模型的视觉蕴涵和视觉接地功能。大量实验表明，RiVEG 在现有 GMNER 数据集上的表现优于最先进的方法，并在所有三个子任务中实现了 10.65%、6.21% 和 8.83% 的绝对领先。</details>
**PDF:** <http://arxiv.org/pdf/2402.09989v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models**<br />
**Title_cn:** EFUF：有效的细粒度遗忘框架，用于减轻多模态大语言模型中的幻觉<br />
**Authors:** Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai<br />
**Abstract:** <details><summary>原文: </summary>Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在过去几年中引起了越来越多的关注，但它们仍然可能生成包含相应图像中不存在的物体的描述，这种现象称为物体幻觉。为了消除幻觉，现有方法手动注释有幻觉和没有幻觉的配对响应，然后采用各种对齐算法来提高图像和文本之间的对齐能力。然而，它们不仅在微调阶段需要大量的计算资源，而且还需要昂贵的人工注释来构建比对算法所需的配对数据。为了解决这些问题，我们借鉴了遗忘的思想，提出了一种高效的细粒度遗忘框架（EFUF），它可以消除幻觉，而不需要配对数据。大量的实验表明，我们的方法始终如一地减少幻觉，同时以适度的计算开销保持生成质量。我们的代码和数据集将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.09801v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Visually Dehallucinative Instruction Generation: Know What You Don't Know**<br />
**Title_cn:** 视觉去幻觉指令生成：知道你不知道的东西<br />
**Authors:** Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang<br />
**Abstract:** <details><summary>原文: </summary>"When did the emperor Napoleon invented iPhone?" Such hallucination-inducing question is well known challenge in generative language modeling. In this study, we present an innovative concept of visual hallucination, referred to as "I Know (IK)" hallucination, to address scenarios where "I Don't Know" is the desired response. To effectively tackle this issue, we propose the VQAv2-IDK benchmark, the subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators. Stepping further, we present the visually dehallucinative instruction generation method for IK hallucination and introduce the IDK-Instructions visual instruction database. Our experiments show that current methods struggle with IK hallucination. Yet, our approach effectively reduces these hallucinations, proving its versatility across different frameworks and datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>“拿破仑皇帝什么时候发明了iPhone？”这种引起幻觉的问题是生成语言建模中众所周知的挑战。在这项研究中，我们提出了一种创新的视幻觉概念，称为“我知道（IK）”幻觉，以解决“我不知道”是所需反应的场景。为了有效解决这个问题，我们提出了 VQAv2-IDK 基准，VQAv2 的子集包含由人类注释者确定的无法回答的图像-问题对。更进一步，我们提出了 IK 幻觉的视觉去幻觉指令生成方法，并引入了 IDK-Instructions 视觉指令数据库。我们的实验表明，当前的方法很难应对 IK 幻觉。然而，我们的方法有效地减少了这些幻觉，证明了它在不同框架和数据集上的多功能性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09717v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Exploiting Alpha Transparency In Language And Vision-Based AI Systems**<br />
**Title_cn:** 在基于语言和视觉的人工智能系统中利用 Alpha 透明度<br />
**Authors:** David Noever, Forrest McKee<br />
**Abstract:** <details><summary>原文: </summary>This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack's potential breadth. This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or updates. Instead, they require retraining and architectural changes, indicating a persistent hole in multimodal technologies without some future adversarial hardening against such vision-language exploits.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项调查揭示了一种源自 PNG 图像文件格式的新颖漏洞，特别是其 alpha 透明层，及其欺骗多个 AI 视觉系统的潜力。我们的方法使用这个 alpha 层作为人类观察者看不见的秘密通道，但人工智能图像处理器完全可以操作。该漏洞的测试范围涵盖了苹果、微软、谷歌、Salesforce、Nvidia 和 Facebook 等代表性视觉系统，凸显了该攻击的潜在广度。该漏洞对现有和现场视觉系统（从医学成像到自动驾驶技术）的安全协议提出了挑战。我们的实验表明，受影响的系统依赖于卷积神经网络或最新的多模态语言模型，无法通过简单的补丁或更新快速缓解这些漏洞。相反，它们需要重新训练和架构更改，这表明多模式技术中存在持续的漏洞，而未来没有针对此类视觉语言漏洞的对抗性强化。</details>
**PDF:** <http://arxiv.org/pdf/2402.09671v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs**<br />
**Title_cn:** VisIRNet：无人机拍摄的可见光和红外图像对的深度图像对齐<br />
**Authors:** Sedat Ozer, Alain P. Ndigande<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes a deep learning based solution for multi-modal image alignment regarding UAV-taken images. Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment. However, we show that we can achieve state of the art results without using LK-based methods. Our approach carefully utilizes a two-branch based convolutional neural network (CNN) based on feature embedding blocks. We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly. Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment. We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种基于深度学习的解决方案，用于无人机拍摄图像的多模态图像对齐。许多最近提出的最先进的对齐技术依赖于使用基于 Lucas-Kanade (LK) 的解决方案来成功对齐。然而，我们表明，我们可以在不使用基于 LK 的方法的情况下获得最先进的结果。我们的方法谨慎地利用了基于特征嵌入块的两分支卷积神经网络（CNN）。我们提出了我们方法的两种变体，其中在第一个变体（模型A）中，我们直接预测仅要对齐的图像四个角的新坐标；在第二个模型（ModelB）中，我们直接预测单应性矩阵。在图像角上应用对齐会强制算法仅匹配这四个角，而不是计算和匹配许多（关键）点，因为后者可能会导致许多异常值，从而产生不太准确的对齐。与现有的最新基于深度 LK 的架构相比，我们在四个航空数据集上测试了我们提出的方法，并获得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.09635v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering**<br />
**Title_cn:** GES：用于高效辐射场渲染的广义指数泼溅<br />
**Authors:** Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi<br />
**Abstract:** <details><summary>原文: </summary>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯喷射技术的进步显着加速了 3D 重建和生成。然而，它可能需要大量的高斯，这会产生大量的内存占用。本文介绍了 GES（广义指数分布），这是一种采用广义指数函数 (GEF) 来建模 3D 场景的新颖表示方法，需要更少的粒子来表示场景，因此在即插即用的效率上显着优于高斯分布方法基于高斯的公用事业的替代能力。 GES 在原则性 1D 设置和现实 3D 场景中都经过了理论和经验验证。它可以更准确地表示具有锐利边缘的信号，这对于高斯函数来说通常具有挑战性，因为其固有的低通特性。我们的实证分析表明，GEF 在拟合自然发生的信号（例如正方形、三角形和抛物线信号）方面优于高斯，从而减少了对大量分裂操作的需求，这些操作会增加高斯分布的内存占用。借助调频损耗，GES 在新颖视图合成基准测试中实现了具有竞争力的性能，同时所需内存存储量不到 Gaussian Splatting 的一半，并将渲染速度提高了高达 39%。该代码可在项目网站 https://abdullahamdi.com/ges 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.10128v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification**<br />
**Title_cn:** 用于高光谱图像分类的混合 CNN Bi-LSTM 神经网络<br />
**Authors:** Alok Ranjan Sahoo, Pavan Chakraborty<br />
**Abstract:** <details><summary>原文: </summary>Hyper spectral images have drawn the attention of the researchers for its complexity to classify. It has nonlinear relation between the materials and the spectral information provided by the HSI image. Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods. Use of 3-D CNN along with 2-D CNN have shown great success for learning spatial and spectral features. However, it uses comparatively large number of parameters. Moreover, it is not effective to learn inter layer information. Hence, this paper proposes a neural network combining 3-D CNN, 2-D CNN and Bi-LSTM. The performance of this model has been tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data sets. The results are compared with the state of-the-art deep learning-based models. This model performed better in all three datasets. It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱图像因其分类的复杂性而引起了研究人员的关注。材料与 HSI 图像提供的光谱信息之间存在非线性关系。与传统的机器学习方法相比，深度学习方法在学习这种非线性方面表现出了优越性。 3-D CNN 和 2-D CNN 的使用在学习空间和光谱特征方面取得了巨大成功。然而，它使用相对较多的参数。而且，学习层间信息并不有效。因此，本文提出了一种结合 3-D CNN、2-D CNN 和 Bi-LSTM 的神经网络。该模型的性能已经在 Indian Pines(IP)、University of Pavia(PU) 和 Salinas Scene(SA) 数据集上进行了测试。结果与最先进的基于深度学习的模型进行了比较。该模型在所有三个数据集中都表现更好。仅使用 IP、PU 和 SA 数据集中最先进模型的 30% 可训练参数，它就可以分别达到 99.83%、99.98% 和 100% 的准确率。</details>
**PDF:** <http://arxiv.org/pdf/2402.10026v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Is Continual Learning Ready for Real-world Challenges?**<br />
**Title_cn:** 持续学习准备好应对现实世界的挑战了吗？<br />
**Authors:** Theodora Kontogianni, Yuanwen Yue, Siyu Tang, Konrad Schindler<br />
**Abstract:** <details><summary>原文: </summary>Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in realistic settings. Our paper aims to initiate a paradigm shift, advocating for the adoption of continual learning methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管持续学习有着悠久而完善的学术历史，但它在现实世界中的应用仍然相当有限。本文认为，这种差距归因于持续学习的实际挑战与所使用的评估协议之间的不一致，导致所提出的解决方案无法有效解决现实世界设置的复杂性。我们使用新的 3D 语义分割基准 OCL-3DSS 验证了我们的假设并评估了迄今为止的进展。我们通过利用更现实的协议来研究文献中的各种持续学习方案，这些协议需要针对动态、真实场景（例如，在机器人和 3D 视觉应用中）进行在线和持续学习。结果发人深省：所有考虑的方法都表现不佳，明显偏离联合离线训练的上限。这引发了关于现有方法在现实环境中的适用性的问题。我们的论文旨在发起范式转变，倡导通过新的实验协议采用持续学习方法，更好地模拟现实世界的条件，以促进该领域的突破。</details>
**PDF:** <http://arxiv.org/pdf/2402.10130v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations**<br />
**Title_cn:** MIM-Refiner：中间预训练表示的对比学习提升<br />
**Authors:** Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter<br />
**Abstract:** <details><summary>原文: </summary>We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 MIM（掩模图像建模）-Refiner，它是预训练 MIM 模型的对比学习提升工具。 MIM-Refiner 背后的动机植根于这样的见解：MIM 模型中的最佳表示通常位于中间层。因此，MIM-Refiner 利用连接到不同中间层的多个对比头。在每个头中，修改后的最近邻目标有助于构建各自的语义簇。细化过程虽然短暂但有效。在几个时期内，我们将 MIM 模型的功能从低于标准的功能改进为最先进的现成功能。改进在 ImageNet-1K 上使用 data2vec 2.0 进行预训练的 ViT-H，在 ImageNet-1K 上预训练的模型中的线性探测 (84.7%) 和低样本分类方面取得了最先进的结果。在 ImageNet-1K 1-shot 分类中，MIM-Refiner 创下了 64.2% 的新最先进水平，优于使用多达 2000 倍数据进行训练的大型模型，例如 DINOv2-g、OpenCLIP-G 和 MAWS。 6.5B。项目页面：https://ml-jku.github.io/MIM-Refiner</details>
**PDF:** <http://arxiv.org/pdf/2402.10093v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity**<br />
**Title_cn:** 具有统计异质性的视网膜光学相干断层扫描图像分类的联邦学习算法研究<br />
**Authors:** Sanskar Amgain, Prashant Shrestha, Sophia Bano, Ignacio del Valle Torres, Michael Cunniffe, Victor Hernandez, Phil Beales, Binod Bhattarai<br />
**Abstract:** <details><summary>原文: </summary>Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings.   Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：我们应用联邦学习来训练 OCT 图像分类器，模拟具有多个客户端和统计异构数据分布的现实场景，其中客户端中的数据完全缺乏某些类别的样本。方法：我们研究了 FedAvg 和 FedProx 以分散方式训练 OCT 图像分类模型的有效性，解决了与集中数据相关的隐私问题。我们在 IID 和非 IID 设置下将公开可用的 OCT 数据集划分到多个客户端，并对每个客户端的子集进行本地训练。我们针对这些设置评估了两种联邦学习方法 FedAvg 和 FedProx。结果：我们对数据集的实验表明，在 IID 设置下，两种方法的性能与在中央数据池上的训练相当。然而，随着我们增加客户端数据的统计异质性，两种算法的性能都会下降，而在异质性增加的设置中，FedProx 始终比 FedAvg 表现更好。结论：尽管联邦学习在跨多个医疗机构的私有数据利用方面很有效，但大量的客户端和标签的异构分布降低了两种算法的性能。值得注意的是，FedProx 似乎对增加的异质性更加稳健。</details>
**PDF:** <http://arxiv.org/pdf/2402.10035v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **SAWEC: Sensing-Assisted Wireless Edge Computing**<br />
**Title_cn:** SAWEC：传感辅助无线边缘计算<br />
**Authors:** Khandaker Foysal Haque, Francesca Meneghello, Md. Ebtidaul Karim, Francesco Restuccia<br />
**Abstract:** <details><summary>原文: </summary>Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link. In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service. Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided. Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics. Hence, only the part of the frames where any environmental change is detected is transmitted and processed. We evaluated SAWEC by using a 10K 360$^{\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking. We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups. Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches. For reproducibility purposes, we pledge to share our whole dataset and code repository.</details>
**Abstract_cn:** <details><summary>译文: </summary>新兴的移动虚拟现实 (VR) 系统需要通过执行基于深度神经网络 (DNN) 的算法，在超高分辨率视频帧上连续执行复杂的计算机视觉任务。由于最先进的 DNN 需要的计算能力对于移动设备而言过于庞大，因此最近提出了基于无线边缘计算 (WEC) 的技术。然而，现有的WEC方法需要传输和处理大量视频数据，这最终可能使无线链路饱和。在本文中，我们提出了一种新颖的传感辅助无线边缘计算（SAWEC）范例来解决这个问题。 SAWEC 利用有关物理环境的知识，仅将与服务交付相关的数据传输到边缘服务器，从而减少端到端延迟和整体计算负担。我们的直觉是，可以避免传输相对于先前帧没有变化的视频帧部分。具体来说，我们利用无线传感技术来估计环境中物体的位置并获得有关环境动态的见解。因此，仅传输和处理检测到任何环境变化的帧部分。我们使用 10K 360$^{\circ}$ 相机和 Wi-Fi 6 传感系统（以 160 MHz 运行并执行定位和跟踪）来评估 SAWEC。我们在消声室和大厅房间中进行实验，其中有两名受试者在六种不同的设置中。实验结果表明，与最先进的WEC方法相比，SAWEC将信道占用和端到端延迟分别降低了93.81%和96.19%，同时将实例分割性能提高了46.98%。出于可重复性的目的，我们承诺共享我们的整个数据集和代码存储库。</details>
**PDF:** <http://arxiv.org/pdf/2402.10021v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **TIAViz: A Browser-based Visualization Tool for Computational Pathology Models**<br />
**Title_cn:** TIAViz：基于浏览器的计算病理学模型可视化工具<br />
**Authors:** Mark Eastwood, John Pocock, Mostafa Jahanifar, Adam Shephard, Skiros Habib, Ethar Alzaid, Abdullah Alsalemi, Jan Lukas Robertus, Nasir Rajpoot, Shan Raza, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos. This tool is open source and is made available at: https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字病理学在现代医疗保健系统中获得了巨大的关注。从光学显微镜到数字图像的转变带来了改进诊断、提高效率以及将人工智能工具集成到病理学家工作流程中的潜力。其中一个关键方面是可视化。在数字病理学机器学习 (ML) 模型的整个开发过程中，拥有灵活、开放的工具来可视化模型（从输出和预测到用于训练或测试模型的底层注释和图像）至关重要。我们推出了 TIAViz，这是一种内置于 TIAToolbox 中的基于 Python 的可视化工具，它允许将各种信息灵活、交互式、完全可缩放地叠加到整个幻灯片图像上，包括图形、热图、分割、注释和其他 WSI。 UI 基于浏览器，允许在本地、远程计算机或服务器上使用以提供公开可用的演示。该工具是开源的，可从以下位置获取：https://github.com/TissueImageAnalytics/tiatoolbox，并通过 pip 安装 (pip install tiatoolbox) 和 conda 作为 TIAToolbox 的一部分。</details>
**PDF:** <http://arxiv.org/pdf/2402.09990v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Current and future roles of artificial intelligence in retinopathy of prematurity**<br />
**Title_cn:** 人工智能当前和未来在早产儿视网膜病变中的作用<br />
**Authors:** Ali Jafarizadeh, Shadi Farabi Maleki, Parnia Pouya, Navid Sobhi, Mirsaeed Abdollahi, Siamak Pedrammehr, Chee Peng Lim, Houshyar Asadi, Roohallah Alizadehsani, Ru-San Tan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI's potential in ROP detection, classification, diagnosis, and prognosis.</details>
**Abstract_cn:** <details><summary>译文: </summary>早产儿视网膜病变 (ROP) 是一种影响早产儿的严重疾病，导致视网膜血管生长异常、视网膜脱离和潜在失明。虽然过去已使用半自动化系统通过量化视网膜血管特征来诊断 ROP 相关疾病，但传统的机器学习 (ML) 模型面临准确性和过度拟合等挑战。深度学习 (DL) 特别是卷积神经网络 (CNN) 的最新进展显着改进了 ROP 检测和分类。 i-ROP 深度学习 (i-ROP-DL) 系统在检测附加疾病方面也显示出前景，提供可靠的 ROP 诊断潜力。这项研究全面探讨了使用视网膜成像和人工智能 (AI) 检测 ROP 的当代进展和挑战，提供了宝贵的见解，可以指导该领域的进一步研究。基于该领域的 89 项原始研究（综合审查了 1487 项研究），我们得出结论，传统的 ROP 诊断方法存在主观性和人工分析，导致临床决策不一致。人工智能对于改善 ROP 管理有着巨大的前景。本综述探讨了人工智能在 ROP 检测、分类、诊断和预后方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.09975v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **ViGEO: an Assessment of Vision GNNs in Earth Observation**<br />
**Title_cn:** ViGEO：对地球观测中视觉 GNN 的评估<br />
**Authors:** Luca Colomba, Paolo Garza<br />
**Abstract:** <details><summary>原文: </summary>Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs). Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>卫星任务和地球观测（EO）系统是环境监测和及时识别灾难性事件、对自然资源和人造资产（例如植被、水体、森林和建筑物）进行长期监测的基本资产。不同的 EO 任务可以收集多种光谱带宽的信息，例如 MODIS、Sentinel-1 和 Sentinel-2。因此，鉴于机器学习、计算机视觉和标记数据的可用性的最新进展，研究人员通过使用深度神经网络证明了土地利用监测系统和遥感图像分类的可行性和精度。此类系统可以帮助领域专家和政府进行持续的环境监测，从而能够在发生灾难性事件（例如偏远地区的森林野火）时及时进行干预。尽管计算机视觉领域最近取得了进展，但许多工作将其分析限制在卷积神经网络（CNN）以及最近的视觉变换器（ViT）上。鉴于图神经网络 (GNN) 最近在非图数据（例如时间序列和图像）上取得的成功，我们研究了应用于土地覆盖分类任务的最新 Vision GNN 架构 (ViG) 的性能。实验结果表明，ViG 在多类和多标签分类环境中实现了最先进的性能，在大规模基准测试中超越了 ViT 和 ResNet。</details>
**PDF:** <http://arxiv.org/pdf/2402.09962v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community**<br />
**Title_cn:** 社会奖励：通过在线创意社区的数百万用户反馈评估和增强生成式人工智能<br />
**Authors:** Arman Isajanyan, Artur Shatveryan, David Kocharyan, Zhangyang Wang, Humphrey Shi<br />
**Abstract:** <details><summary>原文: </summary>Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art. Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward</details>
**Abstract_cn:** <details><summary>译文: </summary>社会奖励作为社区认可的一种形式，为在线平台的用户参与和贡献内容提供了强大的动力来源。文本条件图像合成的最新进展开创了一个协作时代，人工智能使用户能够制作原创视觉艺术作品，寻求社区验证。然而，在集体社区偏好的背景下评估这些模型会带来明显的挑战。现有的评估方法主要集中在以图像质量和即时对齐为指导的有限规模的用户研究上。这项工作开创了范式转变，揭示了社交奖励——一种创新的奖励建模框架，该框架利用参与生成图像的创意编辑的社交网络用户的隐式反馈。我们借鉴在线视觉创作和编辑平台 Picsart，开始了数据集管理和细化的广泛旅程，产生了第一个百万用户规模的数据集，其中包含用户生成的视觉艺术的隐性人类偏好，名为 Picsart Image-Social。我们的分析揭示了当前指标在对文本到图像模型输出的社区创意偏好进行建模方面的缺点，迫使我们引入一种专门为解决这些限制而定制的新颖的预测模型。严格的定量实验和用户研究表明，我们的社交奖励模型比现有指标更符合社交流行度。此外，我们利用社交奖励来微调文本到图像模型，生成的图像不仅更受社交奖励的青睐，而且也更受其他既定指标的青睐。这些发现强调了社会奖励在评估社区对人工智能生成的艺术作品的欣赏方面的相关性和有效性，与用户的创作目标（创造流行的视觉艺术）建立了更紧密的一致性。代码可以访问 https://github.com/Picsart-AI-Research/Social-Reward</details>
**PDF:** <http://arxiv.org/pdf/2402.09872v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs**<br />
**Title_cn:** 表征嵌入式 HMP 上 EEG 应用的准确性权衡<br />
**Authors:** Zain Taufique, Muhammad Awais Bin Altaf, Antonio Miele, Pasi Liljeberg, Anil Kanduri<br />
**Abstract:** <details><summary>原文: </summary>Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detection on the real-world embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial evaluation of power-performance-accuracy trade-offs of EEG applications at different approximation, power, and performance levels to provide insights into the disciplined tuning of approximation in EEG applications on embedded platforms.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用电池供电的可穿戴设备分析脑电图 (EEG) 记录，以监测大脑活动和神经系统疾病。这些应用需要长期且连续的处理才能产生可​​行的结果。然而，由于可穿戴设备的尺寸较小，无法满足实际用例，因此其能量和计算资源有限。嵌入式异构多核平台（HMP）可以在有限的能量预算内为脑电图应用提供更好的性能。可以进一步利用 EEG 应用程序管道的错误恢复能力，以最大限度地提高 HMP 的性能和能量增益。然而，对嵌入式 HMP 近似值进行严格的调整需要彻底探索精度-性能-功耗的权衡空间。在这项工作中，我们在 Odroid XU3 平台的真实嵌入式 HMP 测试台上描述了三种 EEG 应用程序的错误恢复能力，包括癫痫发作检测、睡眠阶段分类和压力检测。我们对脑电图应用程序在不同近似、功率和性能水平下的功率-性能-精度权衡进行了组合评估，以提供对嵌入式平台上脑电图应用程序近似值的严格调整的见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.09867v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking**<br />
**Title_cn:** 超越卡尔曼滤波器：用于改进对象跟踪的基于深度学习的滤波器<br />
**Authors:** Momir Adžemović, Predrag Tadić, Andrija Petrović, Mladen Nikolić<br />
**Abstract:** <details><summary>原文: </summary>Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的检测跟踪系统通常采用卡尔曼滤波器（KF）进行状态估计。然而，KF 需要特定领域的设计选择，并且不适合处理非线性运动模式。为了解决这些限制，我们提出了两种创新的数据驱动过滤方法。我们的第一种方法采用带有可训练运动模型的贝叶斯滤波器来预测对象的未来位置，并将其预测与从对象检测器获得的观察结果相结合，以提高边界框预测的准确性。此外，它省去了 KF 所特有的大多数特定领域的设计选择。第二种方法是端到端可训练滤波器，它更进一步，通过学习纠正检测器错误，进一步最大限度地减少对领域专业知识的需求。此外，我们还介绍了一系列基于递归神经网络、神经常微分方程和条件神经过程的运动模型架构，并与所提出的滤波方法相结合。我们对多个数据集的广泛评估表明，我们提出的滤波器在对象跟踪方面优于传统的 KF，特别是在非线性运动模式的情况下——我们的滤波器最适合的用例。我们还对滤波器进行噪声鲁棒性分析，并取得了令人信服的积极结果。我们进一步提出了一种新的成本函数，用于将观测值与轨迹相关联。根据运动丰富的 DanceTrack 和 SportsMOT 数据集上的多个指标，我们的跟踪器将这种新的关联成本与我们提出的过滤器相结合，在多对象跟踪中优于传统的 SORT 方法和其他基于运动的跟踪器。</details>
**PDF:** <http://arxiv.org/pdf/2402.09865v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment**<br />
**Title_cn:** 注意模态差距：通过跨模态对齐实现遥感视觉语言模型<br />
**Authors:** Angelos Zavras, Dimitrios Michail, Begüm Demir, Ioannis Papoutsis<br />
**Abstract:** <details><summary>原文: </summary>Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着基础模型的出现，深度学习 (DL) 正在经历范式转变，这些基础模型因其关键但不完整的性质而恰如其分地命名。在这项工作中，我们专注于对比语言图像预训练（CLIP），这是一种开放词汇基础模型，它在许多图像分类任务中实现了高精度，并且通常可以与未经显式训练的完全监督基线竞争。尽管如此，在某些领域，零样本 CLIP 性能仍远未达到最佳，例如遥感 (RS) 和医学图像。与自然图像相比，这些领域不仅表现出根本不同的分布，而且通常依赖于 RGB 之外的互补模式来获得有意义的见解。为此，我们提出了一种方法，旨在将不同的 RS 图像模式与 CLIP 的视觉和文本模式相结合。我们的两阶段过程包括强大的微调 CLIP，以处理分布偏移，并伴随 RS 模态编码器的跨模态对齐，以努力扩展 CLIP 的零样本功能。我们最终在遥感图像分类和跨模态检索任务上展示了我们的方法。我们凭经验证明，在多个 RS 基准数据集上，稳健的微调和跨模式对齐都可以转化为显着的性能提升。值得注意的是，这些增强是在不依赖文本描述的情况下实现的，无需引入任何特定于任务的参数，无需从头开始训练，也不会发生灾难性遗忘。</details>
**PDF:** <http://arxiv.org/pdf/2402.09816v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming**<br />
**Title_cn:** TEXTRON：通过数据编程进行弱监督多语言文本检测<br />
**Authors:** Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan<br />
**Abstract:** <details><summary>原文: </summary>Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的几种基于深度学习（DL）的技术在基于图像的多语言文本检测上表现得相当好。然而，它们的性能在很大程度上依赖于训练数据的可用性和质量。页面级文档图像有多种类型，由多种模式、语言、字体和布局的信息组成。这使得文本检测成为计算机视觉（CV）领域的一个具有挑战性的问题，特别是对于资源匮乏或手写语言。此外，用于文本检测的字级标记数据很缺乏，特别是对于多语言设置和包含印刷和手写文本的印度文字。传统上，印度文字文本检测需要在大量标记数据上训练深度学习模型，但据我们所知，没有相关的数据集可用。此类数据的手动注释需要大量时间、精力和专业知识。为了解决这个问题，我们提出了 TEXTRON，一种基于数据编程的方法，用户可以将各种文本检测方法插入到基于弱监督的学习框架中。人们可以将这种多语言文本检测方法视为不同的基于 CV 的技术和 DL 方法的集合。 TEXTRON 可以利用在大量语言数据上预训练的 DL 模型的预测以及基于 CV 的方法来改进其他语言的文本检测。我们证明，尽管缺乏相应的标记数据，TEXTRON 仍可以提高用印度语言编写的文档的检测性能。此外，通过广泛的实验，我们展示了我们的方法相对于当前最先进的（SOTA）模型所带来的改进，特别是对于手写梵文文本。代码和数据集已在 https://github.com/IITB-LEAP-OCR/TEXTRON 上提供</details>
**PDF:** <http://arxiv.org/pdf/2402.09811v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **A Comprehensive Review on Computer Vision Analysis of Aerial Data**<br />
**Title_cn:** 航空数据计算机视觉分析的综合综述<br />
**Authors:** Vivek Tetarwal, Sandeep Kumar<br />
**Abstract:** <details><summary>原文: </summary>With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着机载平台和成像传感器领域新技术的出现，航空数据分析利用其相对于陆地数据的优势变得非常流行。本文对航空数据分析领域的计算机视觉任务进行了全面回顾。在解决对象检测和跟踪等基本方面时，主要关注的是变化检测、对象分割和场景级分析等关键任务。该论文对不同架构和任务中使用的各种超参数进行了比较。其中很大一部分致力于深入讨论图书馆、它们的分类以及它们与不同领域专业知识的相关性。该论文涵盖了航空数据集、采用的架构细微差别以及与航空数据分析中所有任务相关的评估指标。探讨了计算机视觉任务在不同领域航空数据中的应用，并通过案例研究提供了进一步的见解。本文深入探讨了航空数据分析固有的挑战，并提供了实用的解决方案。此外，还确定了尚未解决的重要问题，为航空数据分析领域的未来研究方向铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.09781v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources**<br />
**Title_cn:** 少即是多：有限资源下的视网膜疾病识别集成学习<br />
**Authors:** Jiahao Wang, Hong Peng, Shengchao Chen, Sufen Ren<br />
**Abstract:** <details><summary>原文: </summary>Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries. This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation). The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images. This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch. Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets. Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in low-resource scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>视网膜光学相干断层扫描 (OCT) 图像为了解眼后段的健康状况提供了重要的见解。因此，自动化图像分析方法的进步势在必行，为临床医生和研究人员提供定量数据，从而促进明智的决策。基于深度学习 (DL) 的方法的应用在执行这些分析任务方面获得了广泛的关注，与劳动密集型的手动分析相比，表现出了卓越的性能。然而，视网膜 OCT 图像的获取通常会带来来自隐私问题和资源密集型标记程序的挑战，这与深度学习模型需要大量数据才能实现卓越性能的普遍观念相矛盾。此外，可用计算资源的限制限制了高性能医疗人工智能的进步，特别是在欠发达地区和国家。本文介绍了一种新颖的集成学习机制，旨在在有限资源（例如数据、计算）下识别视网膜疾病。该机制利用多个预训练模型的见解，促进将其知识转移和适应视网膜 OCT 图像。即使面对有限的标记数据，这种方法也能建立一个强大的模型，从而消除了从头开始学习所需的大量参数的需要。对真实世界数据集的综合实验表明，即使在处理极其受限的标记数据集时，所提出的方法也可以在识别视网膜 OCT 图像方面实现卓越的性能。此外，该方法消除了学习大规模参数的必要性，使其非常适合在资源匮乏的场景中部署。</details>
**PDF:** <http://arxiv.org/pdf/2402.09747v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Region Feature Descriptor Adapted to High Affine Transformations**<br />
**Title_cn:** 适应高仿射变换的区域特征描述符<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Peixuan Liu, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>To address the issue of feature descriptors being ineffective in representing grayscale feature information when images undergo high affine transformations, leading to a rapid decline in feature matching accuracy, this paper proposes a region feature descriptor based on simulating affine transformations using classification. The proposed method initially categorizes images with different affine degrees to simulate affine transformations and generate a new set of images. Subsequently, it calculates neighborhood information for feature points on this new image set. Finally, the descriptor is generated by combining the grayscale histogram of the maximum stable extremal region to which the feature point belongs and the normalized position relative to the grayscale centroid of the feature point's region. Experimental results, comparing feature matching metrics under affine transformation scenarios, demonstrate that the proposed descriptor exhibits higher precision and robustness compared to existing classical descriptors. Additionally, it shows robustness when integrated with other descriptors.</details>
**Abstract_cn:** <details><summary>译文: </summary>针对图像进行高仿射变换时特征描述子无法有效表示灰度特征信息，导致特征匹配精度迅速下降的问题，提出一种基于分类模拟仿射变换的区域特征描述子。该方法首先对具有不同仿射度的图像进行分类，以模拟仿射变换并生成一组新的图像。随后，它计算这个新图像集上特征点的邻域信息。最后，结合特征点所属的最大稳定极值区域的灰度直方图和相对于特征点区域的灰度质心的归一化位置来生成描述符。实验结果比较了仿射变换场景下的特征匹配指标，表明所提出的描述符与现有的经典描述符相比具有更高的精度和鲁棒性。此外，它在与其他描述符集成时表现出鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09724v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Hand Shape and Gesture Recognition using Multiscale Template Matching, Background Subtraction and Binary Image Analysis**<br />
**Title_cn:** 使用多尺度模板匹配、背景扣除和二值图像分析进行手形和手势识别<br />
**Authors:** Ketan Suhaas Saichandran<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a hand shape classification approach employing multiscale template matching. The integration of background subtraction is utilized to derive a binary image of the hand object, enabling the extraction of key features such as centroid and bounding box. The methodology, while simple, demonstrates effectiveness in basic hand shape classification tasks, laying the foundation for potential applications in straightforward human-computer interaction scenarios. Experimental results highlight the system's capability in controlled environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种采用多尺度模板匹配的手形分类方法。利用背景减法的集成来导出手部物体的二值图像，从而能够提取质心和边界框等关键特征。该方法虽然简单，但展示了基本手形分类任务的有效性，为简单的人机交互场景中的潜在应用奠定了基础。实验结果突出了系统在受控环境中的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.09663v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography**<br />
**Title_cn:** 数字减影血管造影中动静脉畸形的时空解缠<br />
**Authors:** Kathleen Baur, Xin Xiong, Erickson Torio, Rose Du, Parikshit Juvekar, Reuben Dorent, Alexandra Golby, Sarah Frisken, Nazim Haouchine<br />
**Abstract:** <details><summary>原文: </summary>Although Digital Subtraction Angiography (DSA) is the most important imaging for visualizing cerebrovascular anatomy, its interpretation by clinicians remains difficult. This is particularly true when treating arteriovenous malformations (AVMs), where entangled vasculature connecting arteries and veins needs to be carefully identified.The presented method aims to enhance DSA image series by highlighting critical information via automatic classification of vessels using a combination of two learning models: An unsupervised machine learning method based on Independent Component Analysis that decomposes the phases of flow and a convolutional neural network that automatically delineates the vessels in image space. The proposed method was tested on clinical DSA images series and demonstrated efficient differentiation between arteries and veins that provides a viable solution to enhance visualizations for clinical use.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管数字减影血管造影 (DSA) 是脑血管解剖可视化最重要的成像技术，但临床医生对其进行解释仍然很困难。在治疗动静脉畸形 (AVM) 时尤其如此，需要仔细识别连接动脉和静脉的缠结脉管系统。所提出的方法旨在通过结合使用两种学习模型对血管进行自动分类来突出关键信息，从而增强 DSA 图像系列：一种基于独立成分分析的无监督机器学习方法，可分解流动的阶段，以及自动描绘图像空间中的血管的卷积神经网络。所提出的方法在临床 DSA 图像系列上进行了测试，并证明了动脉和静脉之间的有效区分，为增强临床使用的可视化提供了可行的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.09636v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **X-maps: Direct Depth Lookup for Event-based Structured Light Systems**<br />
**Title_cn:** X-maps：基于事件的结构光系统的直接深度查找<br />
**Authors:** Wieland Morgenstern, Niklas Gard, Simon Baumann, Anna Hilsmann, Peter Eisert<br />
**Abstract:** <details><summary>原文: </summary>We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种使用事件相机直接估计空间增强现实 (SAR) 应用深度的新方法。这些动态视觉传感器非常适合与激光投影仪配合使用，以在结构光方法中进行深度估计。我们的主要贡献包括将投影仪时间图转换为校正的 X 图、捕获传入事件的 x 轴对应关系以及无需任何额外搜索即可直接进行视差查找。与以前的实现相比，这显着简化了深度估计，使其更加高效，而准确性与基于时间图的过程类似。此外，我们通过简单的时间图校准来补偿廉价激光投影仪的非线性时间行为，从而提高性能并提高深度估计精度。由于深度估计仅通过两次查找来执行，因此它几乎可以立即针对传入事件执行（使用 Python 实现每帧不到 3 毫秒）。这允许实时交互性和响应性，这使得我们的方法特别适合低延迟、高帧速率和直接反馈至关重要的 SAR 体验。我们提出了从转换为 X 地图的数据中获得的宝贵见解，并根据基于时间图的最新结果评估视差估计的深度。我们的项目页面上提供了其他结果和代码：https://fraunhoferhhi.github.io/X-maps/</details>
**PDF:** <http://arxiv.org/pdf/2402.10061v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models**<br />
**Title_cn:** RS-DPO：一种用于大型语言模型对齐的混合拒绝采样和直接偏好优化方法<br />
**Authors:** Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra<br />
**Abstract:** <details><summary>原文: </summary>Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于人类反馈的强化学习 (RLHF) 已被广泛用于使大型语言模型与用户意图保持一致。然而，基于 RLHF 的近端策略优化 (PPO) 偶尔会不稳定，需要大量的超参数微调，并且在对齐过程中最大化估计奖励的计算成本很高。最近，提出了直接偏好优化（DPO）来解决这些挑战。然而，DPO 依赖于人类注释者和替代 LLM 生成的对比响应，而不是政策模型，限制了 RLHF 的有效性。在本文中，我们通过系统地结合拒绝采样 (RS) 和 DPO 来解决这两个挑战。我们提出的方法 RS-DPO 始于监督微调策略模型 (SFT) 的开发。每个提示的一组不同的 k 个响应是直接从 SFT 模型中采样的。 RS-DPO 根据奖励分布识别对比样本对。最后，我们将 DPO 与对比样本一起应用，以使模型符合人类偏好。我们的实验表明，我们提出的方法可以有效地在有限的资源环境下微调 LLM，从而提高与用户意图的一致性。此外，它的性能优于现有方法，包括 RS、PPO 和 DPO。</details>
**PDF:** <http://arxiv.org/pdf/2402.10038v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Any-Shift Prompting for Generalization over Distributions**<br />
**Title_cn:** Any-Shift 提示对分布的泛化<br />
**Authors:** Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek<br />
**Abstract:** <details><summary>原文: </summary>Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.</details>
**Abstract_cn:** <details><summary>译文: </summary>具有即时学习功能的图像语言模型在众多下游视觉任务中显示出显着的进步。然而，传统的即时学习方法过度拟合其训练分布并失去了测试分布的泛化能力。为了提高各种分布变化的泛化能力，我们提出了任意变化提示：一个通用的概率推理框架，考虑提示学习期间训练和测试分布之间的关系。我们通过在分层架构中构建训练和测试提示来明确连接潜在空间中的训练和测试分布。在此框架内，测试提示利用分布关系来指导 CLIP 图像语言模型从训练到任何测试分布的泛化。为了有效地编码分布信息及其关系，我们进一步引入了具有伪移位训练机制的变压器推理网络。该网络在前馈传递中生成包含训练和测试信息的定制测试提示，避免了测试时的额外训练成本。对 23 个数据集的广泛实验证明了任何转变提示对各种分布转变的泛化的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10099v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction**<br />
**Title_cn:** NYCTALE：用于自适应和个性化肺结节侵袭性预测的神经证据变压器<br />
**Authors:** Sadaf Khademi, Anastasia Oikonomou, Konstantinos N. Plataniotis, Arash Mohammadi<br />
**Abstract:** <details><summary>原文: </summary>Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文从灵长类动物大脑有趣的证据积累过程中汲取灵感，并以认知心理学和神经科学模型为指导，介绍了 NYCTALE 框架，这是一种受神经启发、基于证据积累的 Transformer 架构。所提出的受神经启发的 NYCTALE 为肺癌诊断的个性化医疗 (PM) 领域提供了一条新途径。在自然界中，夜猫头鹰是一种以夜间活动而闻名的小型猫头鹰，主要在夜间捕猎。 NYCTALE 以类似的警惕方式运作，即以基于证据的方式处理数据并动态/自适应地做出预测。与传统的基于计算机断层扫描 (CT) 的深度学习 (DL) 模型不同，NYCTALE 仅在积累了足够的证据时才进行预测。换句话说，不是处理所有或预定义的 CT 切片子集，而是为每个人一次提供一个切片。然后，NYCTALE 框架计算与每个新 CT 图像的贡献相关的证据向量。一旦累积的证据总数超过特定阈值，就会做出决定。使用包含 114 名受试者的具有挑战性的内部数据集进行初步实验分析。结果值得注意，表明 NYCTALE 在这个要求较高的小型数据集上的训练数据量减少了大约 60%，但仍优于基准精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.10066v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Feature Accentuation: Revealing 'What' Features Respond to in Natural Images**<br />
**Title_cn:** 特征强调：揭示自然图像中“什么”特征的反应<br />
**Authors:** Chris Hamblin, Thomas Fel, Srijani Saha, Talia Konkle, George Alvarez<br />
**Abstract:** <details><summary>原文: </summary>Efforts to decode neural network vision models necessitate a comprehensive grasp of both the spatial and semantic facets governing feature responses within images. Most research has primarily centered around attribution methods, which provide explanations in the form of heatmaps, showing where the model directs its attention for a given feature. However, grasping 'where' alone falls short, as numerous studies have highlighted the limitations of those methods and the necessity to understand 'what' the model has recognized at the focal point of its attention. In parallel, 'Feature visualization' offers another avenue for interpreting neural network features. This approach synthesizes an optimal image through gradient ascent, providing clearer insights into 'what' features respond to. However, feature visualizations only provide one global explanation per feature; they do not explain why features activate for particular images. In this work, we introduce a new method to the interpretability tool-kit, 'feature accentuation', which is capable of conveying both where and what in arbitrary input images induces a feature's response. At its core, feature accentuation is image-seeded (rather than noise-seeded) feature visualization. We find a particular combination of parameterization, augmentation, and regularization yields naturalistic visualizations that resemble the seed image and target feature simultaneously. Furthermore, we validate these accentuations are processed along a natural circuit by the model. We make our precise implementation of feature accentuation available to the community as the Faccent library, an extension of Lucent.</details>
**Abstract_cn:** <details><summary>译文: </summary>解码神经网络视觉模型的努力需要全面掌握控制图像内特征响应的空间和语义方面。大多数研究主要集中在归因方法上，该方法以热图的形式提供解释，显示模型将注意力集中在给定特征的位置。然而，仅仅掌握“哪里”是不够的，因为大量研究强调了这些方法的局限性，以及了解模型在其关注焦点上识别“什么”的必要性。与此同时，“特征可视化”为解释神经网络特征提供了另一种途径。这种方法通过梯度上升合成最佳图像，从而更清晰地了解特征响应的“内容”。然而，特征可视化仅为每个特征提供一个全局解释；他们没有解释为什么特定图像会激活功能。在这项工作中，我们为可解释性工具包引入了一种新方法“特征强调”，它能够传达任意输入图像中的位置和内容引起特征响应。特征强调的核心是图像种子（而不是噪声种子）特征可视化。我们发现参数化、增强和正则化的特定组合可以产生同时类似于种子图像和目标特征的自然可视化效果。此外，我们验证了模型沿着自然回路处理这些强调。我们将功能强调的精确实现作为 Faccent 库（朗讯科技的扩展）提供给社区。</details>
**PDF:** <http://arxiv.org/pdf/2402.10039v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields**<br />
**Title_cn:** Reg-NF：神经场内隐式表面的有效配准<br />
**Authors:** Stephen Hausler, David Hall, Sutharsan Mahendren, Peyman Moghadam<br />
**Abstract:** <details><summary>原文: </summary>Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经场（基于坐标的神经网络）最近因隐式表示场景而受到欢迎。与基于点云等显式表示的经典方法相比，神经场提供了连续的场景表示，能够以紧凑且适合机器人应用的方式表示 3D 几何和外观。然而，有限的现有方法已经研究了通过直接利用这些连续隐式表示来注册多个神经场。在本文中，我们提出了 Reg-NF，一种基于神经场的配准，它优化两个任意神经场之间的相对 6-DoF 变换，即使这两个场具有不同的比例因子。 Reg-NF 的关键组件包括双向配准损失、多视图表面​​采样以及体积符号距离函数 (SDF) 的利用。我们展示了我们在新的神经场数据集上评估配准问题的方法。我们提供了一套详尽的实验和消融研究来确定我们方法的性能，同时还讨论了局限性，以便为研究界在不受约束的环境中利用神经场的开放挑战提供未来方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.09722v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement**<br />
**Title_cn:** 使用冷冻发生器进行种子优化，实现卓越的零次低光增强<br />
**Authors:** Yuxuan Gu, Yi Jin, Ben Wang, Zhixiang Wei, Xiaoxiao Ma, Pengyang Ling, Haoxuan Wang, Huaian Chen, Enhong Chen<br />
**Abstract:** <details><summary>原文: </summary>In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed. Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset. Extensive experiments on various benchmarks demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们观察到在大量自然图像上进行预训练的生成器本质上具有针对不同场景进行卓越的低光图像增强的有希望的潜力。具体来说，我们将预训练的生成器嵌入到 Retinex 模型中以生成反射图具有增强的细节和生动性，从而恢复因低光条件而退化的特征。更进一步，我们引入了一种新颖的优化策略，它将梯度反向传播到输入种子而不是低光增强模型的参数，从而完整地保留从自然图像中学到的生成知识并实现更快的收敛速度。受益于预先训练的知识和种子优化策略，低光增强模型可以显着规范增强结果的真实性和保真度，从而快速生成高质量图像，而无需在任何低光数据集上进行训练。对各种基准的广泛实验在定性和定量上证明了所提出的方法相对于众多最先进方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.09694v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Enhancing signal detectability in learning-based CT reconstruction with a model observer inspired loss function**<br />
**Title_cn:** 利用模型观察者启发的损失函数增强基于学习的 CT 重建中的信号可检测性<br />
**Authors:** Megan Lantz, Emil Y. Sidky, Ingrid S. Reiser, Xiaochuan Pan, Gregory Ongie<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks used for reconstructing sparse-view CT data are typically trained by minimizing a pixel-wise mean-squared error or similar loss function over a set of training images. However, networks trained with such pixel-wise losses are prone to wipe out small, low-contrast features that are critical for screening and diagnosis. To remedy this issue, we introduce a novel training loss inspired by the model observer framework to enhance the detectability of weak signals in the reconstructions. We evaluate our approach on the reconstruction of synthetic sparse-view breast CT data, and demonstrate an improvement in signal detectability with the proposed loss.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于重建稀疏视图 CT 数据的深度神经网络通常通过最小化一组训练图像上的像素均方误差或类似损失函数来进行训练。然而，经过这种像素级损失训练的网络很容易消除对于筛查和诊断至关重要的小而低对比度的特征。为了解决这个问题，我们引入了一种受模型观察者框架启发的新型训练损失，以增强重建中弱信号的可检测性。我们评估了我们的合成稀疏视图乳腺 CT 数据重建方法，并证明了所提出的损失在信号可检测性方面的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.10010v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge**<br />
**Title_cn:** POBEVM：通过逐步优化目标主体和边缘进行实时视频抠图<br />
**Authors:** Jianming Xian<br />
**Abstract:** <details><summary>原文: </summary>Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge. Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度卷积神经网络 (CNN) 的方法在视频抠图方面取得了出色的性能。其中许多方法可以为目标身体产生准确的 alpha 估计，但通常会产生模糊或不正确的目标边缘。这通常是由以下原因引起的： 1）当前的方法总是不加区别地对待目标体和边缘； 2) 目标体占整个目标的主导地位，仅占目标边缘很小的比例。对于第一个问题，我们提出了一个基于 CNN 的模块，该模块单独优化抠图目标主体和边缘（SOBE）。在此基础上，我们引入了一种实时、无三元图的视频抠图方法，通过逐步优化抠图目标主体和边缘（POBEVM），该方法比以前的方法轻得多，并且在预测目标边缘方面取得了显着改进。对于第二个问题，我们提出了 Edge-L1-Loss (ELL) 函数，该函数在抠图目标边缘上强制执行我们的网络。实验表明，我们的方法在 Distinctions-646 (D646) 和 VideoMatte240K(VM) 数据集上均优于先前的无 trimap 抠图方法，尤其是在边缘优化方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.09731v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm**<br />
**Title_cn:** 实现斑马鱼精密心血管分析：ZACAF 范式<br />
**Authors:** Amir Mohammad Naderi, Jennifer G. Casey, Mao-Hsiang Huang, Rachelle Victorio, David Y. Chiang, Calum MacRae, Hung Cao, Vandana A. Gupta<br />
**Abstract:** <details><summary>原文: </summary>Quantifying cardiovascular parameters like ejection fraction in zebrafish as a host of biological investigations has been extensively studied. Since current manual monitoring techniques are time-consuming and fallible, several image processing frameworks have been proposed to automate the process. Most of these works rely on supervised deep-learning architectures. However, supervised methods tend to be overfitted on their training dataset. This means that applying the same framework to new data with different imaging setups and mutant types can severely decrease performance. We have developed a Zebrafish Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac function in zebrafish. In this work, we further applied data augmentation, Transfer Learning (TL), and Test Time Augmentation (TTA) to ZACAF to improve the performance for the quantification of cardiovascular function quantification in zebrafish. This strategy can be integrated with the available frameworks to aid other researchers. We demonstrate that using TL, even with a constrained dataset, the model can be refined to accommodate a novel microscope setup, encompassing diverse mutant types and accommodating various video recording protocols. Additionally, as users engage in successive rounds of TL, the model is anticipated to undergo substantial enhancements in both generalizability and accuracy. Finally, we applied this approach to assess the cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为一系列生物学研究，量化斑马鱼射血分数等心血管参数已得到广泛研究。由于当前的手动监控技术既耗时又容易出错，因此提出了几种图像处理框架来自动化该过程。这些工作大部分依赖于监督式深度学习架构。然而，监督方法往往在训练数据集上过度拟合。这意味着将相同的框架应用于具有不同成像设置和突变类型的新数据可能会严重降低性能。我们开发了斑马鱼自动心血管评估框架（ZACAF）来量化斑马鱼的心脏功能。在这项工作中，我们进一步将数据增强、迁移学习（TL）和测试时间增强（TTA）应用于ZACAF，以提高斑马鱼心血管功能量化的性能。该策略可以与可用的框架集成以帮助其他研究人员。我们证明，使用 TL，即使数据集有限，也可以改进模型以适应新颖的显微镜设置，涵盖不同的突变体类型并适应各种视频记录协议。此外，随着用户进行连续几轮的 TL，预计该模型的通用性和准确性将得到显着增强。最后，我们应用这种方法来评估 nrap 突变斑马鱼（一种心肌病模型）的心血管功能。</details>
**PDF:** <http://arxiv.org/pdf/2402.09658v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Foul prediction with estimated poses from soccer broadcast video**<br />
**Title_cn:** 根据足球转播视频中的估计姿势进行犯规预测<br />
**Authors:** Jiale Fang, Calvin Yeung, Keisuke Fujii<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction. Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机视觉的最新进展在体育运动员的跟踪和姿势估计方面取得了重大进展。然而，在体育运动中利用姿势估计进行行为预测的研究较少，特别是足球犯规的预测具有挑战性，因为每个球员的图像尺寸较小，并且难以使用例如球和姿势信息。在我们的研究中，我们引入了一种创新的深度学习方法来预测足球犯规。该方法通过整理新颖的足球犯规数据集来集成视频数据、边界框位置、图像细节和姿势信息。我们的模型利用卷积神经网络和循环神经网络（CNN 和 RNN）的组合来有效地合并来自这四种模式的信息。实验结果表明，我们的完整模型优于消融模型，并且所有 RNN 模块、边界框位置和图像以及估计姿态对于犯规预测都是有用的。我们的研究结果对于更深入地理解足球犯规行为具有重要意义，并为该领域未来的研究和实践提供了宝贵的参考。</details>
**PDF:** <http://arxiv.org/pdf/2402.09650v1><br />
**Code:** null<br />

