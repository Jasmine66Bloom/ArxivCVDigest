## [UPDATED!] **2024-02-16** (Publish Time)

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter**<br />
**Title_cn:** PaLM2-VAdapter：逐步对齐的语言模型打造强大的视觉语言适配器<br />
**Authors:** Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang<br />
**Abstract:** <details><summary>原文: </summary>This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance, and stronger scalability. Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文证明了逐步对齐的语言模型可以有效地连接冻结视觉编码器和大型语言模型（LLM）。虽然视觉编码器和法学硕士的基本架构和预训练方法已被广泛研究，但视觉语言适配器的架构和训练策略在最近的工作中存在很大差异。我们的研究对最先进的感知器重采样器架构进行了彻底的探索，并建立了强大的基线。然而，我们观察到，视觉语言与感知器重采样器的对齐表现出缓慢的收敛性和有限的可扩展性，并且缺乏直接监督。为了解决这个问题，我们提出PaLM2-VAdapter，采用渐进对齐的语言模型作为视觉语言适配器。与具有感知器重采样器的强基线相比，我们的方法根据经验显示出更快的收敛速度、更高的性能和更强的可扩展性。对图像和视频的各种视觉问答（VQA）和字幕任务进行的广泛实验表明，我们的模型展示了最先进的视觉理解和多模态推理能力。值得注意的是，我们的方法以比最先进的大型视觉语言模型少 30%~70% 的参数实现了这些进步，标志着效率的显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.10896v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning**<br />
**Title_cn:** 融合弥散加权 MRI 和临床数据，通过深度对比学习预测急性缺血性中风后的功能结果<br />
**Authors:** Chia-Ling Tsai, Hui-Yun Su, Shen-Feng Sung, Wei-Yang Lin, Ying-Ying Su, Tzu-Hsien Yang, Man-Lin Mai<br />
**Abstract:** <details><summary>原文: </summary>Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the prediction of the patient needing long-term care at 3 months after the onset of stroke. Trained and evaluated with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80 and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing models that consolidate both imaging and structured data in the medical domain. If trained with comprehensive clinical variables, including NIHSS and comorbidities, the gain from images on making accurate prediction is not considered substantial, but significant. However, diffusion-weighted MRI can replace NIHSS to achieve comparable level of accuracy combining with other readily available clinical variables for better generalization.</details>
**Abstract_cn:** <details><summary>译文: </summary>中风是一种常见的致残性神经系统疾病，影响约四分之一的 25 岁以上成年人口；超过一半的患者在急性中风发作后仍然预后不良，例如永久性功能依赖甚至死亡。本研究的目的是调查弥散加权 MRI 模式与结构化健康状况相结合在预测功能结果以促进早期干预方面的功效。提出了一种具有两阶段训练的深度融合学习网络：第一阶段侧重于跨模态表示学习，第二阶段侧重于分类。利用监督对比学习来学习区分特征，将两类患者与个体模态嵌入和融合多模态嵌入区分开来。该网络将 DWI 和 ADC 图像以及结构化健康档案数据作为输入。结果是预测患者在中风发作后 3 个月需要长期护理。使用 3297 名患者的数据集进行训练和评估，我们提出的融合模型的 AUC、F1 分数和准确性分别达到 0.87、0.80 和 80.45%，优于在医学领域整合成像和结构化数据的现有模型。如果使用全面的临床变量（包括 NIHSS 和合并症）进行训练，则从图像中进行准确预测的收益并不被认为是实质性的，但却是显着的。然而，弥散加权 MRI 可以取代 NIHSS，与其他现成的临床变量相结合，达到可比的准确度水平，以实现更好的概括。</details>
**PDF:** <http://arxiv.org/pdf/2402.10894v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-modal preference alignment remedies regression of visual instruction tuning on language model**<br />
**Title_cn:** 多模态偏好对齐补救了语言模型上视觉指令调整的回归<br />
**Authors:** Shengzhi Li, Rongyu Lin, Shichao Pei<br />
**Abstract:** <details><summary>原文: </summary>In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>在生产中，多模态大语言模型（MLLM）有望支持互换图像和文本模态的多轮查询。然而，当前使用视觉问答（VQA）数据集训练的 MLLM 可能会出现退化，因为 VQA 数据集缺乏训练底层语言模型的原始文本指令数据集的多样性和复杂性。为了解决这种具有挑战性的退化问题，我们首先收集一个轻量级（6k 条目）VQA 偏好数据集，其中 Gemini 以细粒度方式对 5 个质量指标的答案进行注释，并研究标准监督微调、拒绝采样、直接偏好优化 (DPO)和 SteerLM。我们的研究结果表明，尽管数据规模较小，但借助 DPO，我们能够超越语言模型的指令跟踪能力，在 MT-Bench 上获得 6.73 分，而 Vicuna 的得分为 6.57，LLaVA 的得分为 5.99。文本指令熟练程度的提高与视觉指令性能的提高相关（MM-Vet 上 +4.9\%，LLaVA-Bench 上 +6\%），与之前的 RLHF 方法相比，视觉知识基准的对齐税最小。总之，我们提出了一种基于蒸馏的多模态对齐模型，在小数据集上具有细粒度注释，可以协调 MLLM 的文本和视觉性能，在视觉指令调整后恢复和增强语言能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.10884v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Control Color: Multimodal Diffusion-based Interactive Image Colorization**<br />
**Title_cn:** 控制颜色：基于多模态扩散的交互式图像着色<br />
**Authors:** Zhexin Liang, Zhaochen Li, Shangchen Zhou, Chongyi Li, Chen Change Loy<br />
**Abstract:** <details><summary>原文: </summary>Despite the existence of numerous colorization methods, several limitations still exist, such as lack of user interaction, inflexibility in local colorization, unnatural color rendering, insufficient color variation, and color overflow. To solve these issues, we introduce Control Color (CtrlColor), a multi-modal colorization method that leverages the pre-trained Stable Diffusion (SD) model, offering promising capabilities in highly controllable interactive image colorization. While several diffusion-based methods have been proposed, supporting colorization in multiple modalities remains non-trivial. In this study, we aim to tackle both unconditional and conditional image colorization (text prompts, strokes, exemplars) and address color overflow and incorrect color within a unified framework. Specifically, we present an effective way to encode user strokes to enable precise local color manipulation and employ a practical way to constrain the color distribution similar to exemplars. Apart from accepting text prompts as conditions, these designs add versatility to our approach. We also introduce a novel module based on self-attention and a content-guided deformable autoencoder to address the long-standing issues of color overflow and inaccurate coloring. Extensive comparisons show that our model outperforms state-of-the-art image colorization methods both qualitatively and quantitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管存在多种着色方法，但仍然存在一些局限性，例如缺乏用户交互、局部着色不灵活、显色不自然、颜色变化不足和颜色溢出。为了解决这些问题，我们引入了控制颜色（CtrlColor），这是一种利用预先训练的稳定扩散（SD）模型的多模态着色方法，在高度可控的交互式图像着色方面提供了有前途的功能。虽然已经提出了几种基于扩散的方法，但支持多种模式的着色仍然很重要。在这项研究中，我们的目标是在统一的框架内解决无条件和条件图像着色（文本提示、笔划、示例）并解决颜色溢出和不正确的颜色。具体来说，我们提出了一种有效的方法来编码用户笔画，以实现精确的局部颜色操作，并采用一种实用的方法来约束类似于示例的颜色分布。除了接受文本提示作为条件之外，这些设计还为我们的方法增加了多功能性。我们还引入了一种基于自注意力的新颖模块和内容引导的可变形自动编码器，以解决长期存在的颜色溢出和着色不准确的问题。广泛的比较表明，我们的模型在定性和定量上都优于最先进的图像着色方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.10855v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond**<br />
**Title_cn:** 生成式跨模态检索：在多模态语言模型中记忆图像以供检索及其他使用<br />
**Authors:** Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, Tat-Seng Chua<br />
**Abstract:** <details><summary>原文: </summary>The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成语言模型的最新进展证明了它们记忆文档知识和回忆知识以有效响应用户查询的能力。在此功能的基础上，我们建议使多模态大语言模型（MLLM）能够在其参数内记忆和调用图像。给定用户对视觉内容的查询，MLLM 预计会从其参数中“调用”相关图像作为响应。实现这一目标面临着显着的挑战，包括 MLLM 中内置的视觉记忆和视觉回忆方案。为了解决这些挑战，我们引入了一种生成式跨模式检索框架，该框架分配唯一的标识符字符串来表示图像，并涉及两个训练步骤：学习记忆和学习检索。第一步重点是训练 MLLM 记住图像与其各自标识符之间的关联。后一步教导 MLLM 在给定文本查询输入的情况下生成目标图像的相应标识符。通过在 MLLM 中记忆图像，我们引入了一种新的跨模态检索范例，与之前的判别方法不同。实验表明，即使对于大规模图像候选集，生成范式也能有效且高效地执行。</details>
**PDF:** <http://arxiv.org/pdf/2402.10805v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion**<br />
**Title_cn:** BioFusionNet：通过多特征和多模态数据融合对 ER+ 乳腺癌进行基于深度学习的生存风险分层<br />
**Authors:** Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering<br />
**Abstract:** <details><summary>原文: </summary>Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive multimodal feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field. The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80--4.68, p<0.005). The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data.</details>
**Abstract_cn:** <details><summary>译文: </summary>乳腺癌是影响全球数百万女性的重大健康问题。准确的生存风险分层在指导个性化治疗决策和改善患者预后方面发挥着至关重要的作用。在这里，我们提出了 BioFusionNet，这是一个深度学习框架，它将图像衍生的特征与遗传和临床数据融合起来，以实现全面的患者概况并对 ER+ 乳腺癌患者进行生存风险分层。我们采用多个自监督特征提取器，即 DINO 和 MoCoV3，在组织病理学斑块上进行预训练，以捕获详细的组织病理学图像特征。然后，我们利用变分自动编码器（VAE）来融合这些特征，并利用 VAE 的潜在空间将其输入到自注意力网络中，生成患者级别的特征。接下来，我们开发了一种双交叉注意力机制，将组织病理学特征与遗传数据结合起来，使模型能够捕获它们之间的相互作用。此外，使用前馈网络（FFN）整合临床数据，进一步增强预测性能并实现全面的多模态特征集成。此外，我们引入了加权 Cox 损失函数，专门用于处理不平衡的生存数据，这是该领域的常见挑战。所提出的模型实现了 0.77 的平均一致性指数（C 指数）和 0.84 的时间相关曲线下面积 (AUC)，优于最先进的方法。它在单变量分析中预测对总生存 (OS) 具有预后意义的风险（高与低）（HR=2.99，95% CI：1.88--4.78，p<0.005），并在纳入标准临床病理变量的多变量分析中保持独立显着性（HR=2.91，95% CI：1.80--4.68，p<0.005）。所提出的方法不仅提高了模型性能，而且解决了处理不平衡数据方面的关键差距。</details>
**PDF:** <http://arxiv.org/pdf/2402.10717v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering**<br />
**Title_cn:** 零镜头视频问答的问题指导视觉描述<br />
**Authors:** David Romero, Thamar Solorio<br />
**Abstract:** <details><summary>原文: </summary>We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM). The LLM is our reasoning module, and performs the final step of multiple-choice QA. Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 Q-ViD，一种用于视频问答（视频 QA）的简单方法，与基于复杂架构、计算成本昂贵的管道或使用 GPT 等封闭模型的先前方法不同，Q-ViD 依赖于单个指令感知开放视觉语言模型（InstructBLIP）使用帧描述来处理视频QA。具体来说，我们创建依赖于视频目标问题的字幕说明提示，并利用 InstructBLIP 来获取对当前任务有用的视频帧字幕。随后，我们使用与问题相关的帧标题形成整个视频的描述，并将该信息与问答提示一起提供给大型语言模型 (LLM)。 LLM 是我们的推理模块，执行多项选择 QA 的最后一步。我们简单的 Q-ViD 框架在各种视频 QA 基准（包括 NExT-QA、STAR、How2QA、TVQA 和 IntentQA）上实现了比当前最先进的模型具有竞争力甚至更高的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.10698v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation**<br />
**Title_cn:** 联合语义分割和单目深度估计的高效多任务不确定性<br />
**Authors:** Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich<br />
**Abstract:** <details><summary>原文: </summary>Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular depth estimation as well as efficient multi-task uncertainty quantification. By implicitly leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient.</details>
**Abstract_cn:** <details><summary>译文: </summary>量化预测不确定性成为解决常见挑战的一种可能解决方案，例如过度自信或深度神经网络缺乏可解释性和鲁棒性，尽管计算量通常很大。许多现实世界的应用本质上是多模式的，因此受益于多任务学习。例如，在自动驾驶领域，语义分割和单目深度估计的联合解决方案已被证明是有价值的。在这项工作中，我们首先将不同的不确定性量化方法与联合语义分割和单目深度估计结合起来，并评估它们之间的性能比较。此外，我们还揭示了与单独解决两个任务相比，多任务学习在不确定性质量方面的优势。基于这些见解，我们引入了 EMUFormer，这是一种新颖的师生蒸馏方法，用于联合语义分割和单目深度估计以及高效的多任务不确定性量化。通过隐式地利用教师的预测不确定性，EMUFormer 在 Cityscapes 和 NYUv2 上取得了新的最先进的结果，并且还估计了这两项任务的高质量预测不确定性，这些任务与 Deep Ensemble 相当或优于 Deep Ensemble，尽管它们的顺序是幅度更有效。</details>
**PDF:** <http://arxiv.org/pdf/2402.10580v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Using Left and Right Brains Together: Towards Vision and Language Planning**<br />
**Title_cn:** 共同使用左右脑：迈向视觉和语言规划<br />
**Authors:** Jun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, Jianguo Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型 (LLM) 和大型多模态模型 (LMM) 在各种任务上都表现出了卓越的决策屏蔽能力。但他们本质上是在语言空间内进行规划，缺乏视野和空间想象能力。相比之下，人类在思维过程中利用大脑的左半球和右半球进行语言和视觉规划。因此，我们在这项工作中引入了一种新颖的视觉语言规划框架，以对具有任何形式输入的任务执行并发视觉和语言规划。我们的框架结合了视觉规划来捕捉复杂的环境细节，而语言规划则增强了整个系统的逻辑连贯性。我们评估了我们的框架在视觉语言任务、仅视觉任务和仅语言任务中的有效性。结果证明了我们的方法的卓越性能，表明视觉和语言规划的集成可以产生更好的上下文感知任务执行。</details>
**PDF:** <http://arxiv.org/pdf/2402.10534v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration**<br />
**Title_cn:** 通过多模态数据和辅助任务集成优化皮肤病变分类<br />
**Authors:** Mahapara Khurshid, Mayank Vatsa, Richa Singh<br />
**Abstract:** <details><summary>原文: </summary>The rising global prevalence of skin conditions, some of which can escalate to life-threatening stages if not timely diagnosed and treated, presents a significant healthcare challenge. This issue is particularly acute in remote areas where limited access to healthcare often results in delayed treatment, allowing skin diseases to advance to more critical stages. One of the primary challenges in diagnosing skin diseases is their low inter-class variations, as many exhibit similar visual characteristics, making accurate classification challenging. This research introduces a novel multimodal method for classifying skin lesions, integrating smartphone-captured images with essential clinical and demographic information. This approach mimics the diagnostic process employed by medical professionals. A distinctive aspect of this method is the integration of an auxiliary task focused on super-resolution image prediction. This component plays a crucial role in refining visual details and enhancing feature extraction, leading to improved differentiation between classes and, consequently, elevating the overall effectiveness of the model. The experimental evaluations have been conducted using the PAD-UFES20 dataset, applying various deep-learning architectures. The results of these experiments not only demonstrate the effectiveness of the proposed method but also its potential applicability under-resourced healthcare environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>全球皮肤病患病率不断上升，其中一些如果不及时诊断和治疗可能会升级至危及生命的阶段，这对医疗保健提出了重大挑战。这个问题在偏远地区尤为严重，这些地区获得医疗服务的机会有限，常常导致治疗延迟，从而使皮肤病发展到更严重的阶段。诊断皮肤病的主要挑战之一是其类间差异较小，因为许多皮肤病表现出相似的视觉特征，这使得准确分类具有挑战性。这项研究引入了一种新颖的多模式方法来对皮肤病变进行分类，将智能手机捕获的图像与基本的临床和人口统计信息相结合。这种方法模仿了医疗专业人员所采用的诊断过程。该方法的一个独特之处是集成了专注于超分辨率图像预测的辅助任务。该组件在细化视觉细节和增强特征提取方面发挥着至关重要的作用，从而提高了类之间的区分度，从而提高了模型的整体有效性。实验评估是使用 PAD-UFES20 数据集进行的，应用了各种深度学习架构。这些实验的结果不仅证明了所提出方法的有效性，而且证明了其在资源贫乏的医疗保健环境中的潜在适用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10454v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation**<br />
**Title_cn:** Weak-Mamba-UNet：Visual Mamba 使 CNN 和 ViT 更好地用于基于 Scribble 的医学图像分割<br />
**Authors:** Ziyang Wang, Chao Ma<br />
**Abstract:** <details><summary>原文: </summary>Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像分割越来越依赖深度学习技术，但良好的性能往往伴随着高昂的注释成本。本文介绍了 Weak-Mamba-UNet，这是一种创新的弱监督学习 (WSL) 框架，它利用了卷积神经网络 (CNN)、视觉变换器 (ViT) 和用于医疗领域的尖端 Visual Mamba (VMamba) 架构的功能图像分割，尤其是在处理基于涂鸦的注释时。所提出的 WSL 策略融合了三种不同的架构但相同的对称编码器-解码器网络：用于详细局部特征提取的基于 CNN 的 UNet、用于全面全局上下文理解的基于 Swin Transformer 的 SwinUNet 以及用于高效长时长的基于 VMamba 的 Mamba-UNet。 -范围依赖建模。该框架的关键概念是一种协作和交叉监督机制，它采用伪标签来促进跨网络的迭代学习和细化。 Weak-Mamba-UNet 的有效性在公开的 MRI 心脏分割数据集上进行了验证，该数据集具有经过处理的涂鸦注释，其性能超过了仅使用 UNet 或 SwinUNet 的类似 WSL 框架的性能。这凸显了它在注释稀疏或不精确的场景中的潜力。源代码可供公开访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.10887v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images**<br />
**Title_cn:** HistoSegCap：用于整个幻灯片图像中组织学组织类型的弱监督语义分割的胶囊<br />
**Authors:** Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis<br />
**Abstract:** <details><summary>原文: </summary>Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字病理学涉及将物理组织切片转换为高分辨率的全切片图像 (WSI)，病理学家可以分析受疾病影响的组织。然而，具有大量微观视野的大型组织学载玻片给视觉搜索带来了挑战。为了帮助病理学家，计算机辅助诊断 (CAD) 系统提供视觉辅助，以有效检查 WSI 并识别诊断相关区域。本文提出了一种基于胶囊网络的弱监督语义分割（WSSS）的新型组织病理学图像分析方法，这是此类应用中的第一个。使用数字病理学图集（ADP）数据集对所提出的模型进行评估，并将其性能与其他组织病理学语义分割方法进行比较。这些发现强调了胶囊网络在提高组织病理学图像分析的精度和效率方面的潜力。实验结果表明，所提出的模型在准确性和平均交并（mIoU）指标方面优于传统方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.10851v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning**<br />
**Title_cn:** 增强驱动的鲁棒指纹表示学习预训练<br />
**Authors:** Ekta Gavas, Kaustubh Olpadkar, Anoop Namboodiri<br />
**Abstract:** <details><summary>原文: </summary>Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.</details>
**Abstract_cn:** <details><summary>译文: </summary>指纹识别是生物识别技术的关键组成部分，具有从身份验证到高级搜索工具的多种应用。在本文中，我们提出了一种独特的方法，通过利用基于增强的预训练来导出鲁棒的指纹表示。基于基于 U-Net 的指纹增强的成就，我们的方法采用专门的编码器以自监督的方式从指纹图像中获取表示。我们进一步细化这些表示，旨在增强验证能力。我们的实验结果在公开的指纹数据集上进行了测试，表明与已建立的自我监督训练技术相比，验证性能有了显着提高。我们的研究结果不仅强调了我们方法的有效性，而且为潜在的进步铺平了道路。至关重要的是，我们的研究表明，在不依赖增强样本的情况下从退化图像中提取有意义的指纹表示是可行的。</details>
**PDF:** <http://arxiv.org/pdf/2402.10847v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Training Class-Imbalanced Diffusion Model Via Overlap Optimization**<br />
**Title_cn:** 通过重叠优化训练类不平衡扩散模型<br />
**Authors:** Divin Yan, Lu Qi, Vincent Tao Hu, Ming-Hsuan Yang, Meng Tang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have made significant advances recently in high-quality image synthesis and related tasks. However, diffusion models trained on real-world datasets, which often follow long-tailed distributions, yield inferior fidelity for tail classes. Deep generative models, including diffusion models, are biased towards classes with abundant training images. To address the observed appearance overlap between synthesized images of rare classes and tail classes, we propose a method based on contrastive learning to minimize the overlap between distributions of synthetic images for different classes. We show variants of our probabilistic contrastive learning method can be applied to any class conditional diffusion model. We show significant improvement in image synthesis using our loss for multiple datasets with long-tailed distribution. Extensive experimental results demonstrate that the proposed method can effectively handle imbalanced data for diffusion-based generation and classification models. Our code and datasets will be publicly available at https://github.com/yanliang3612/DiffROP.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型最近在高质量图像合成和相关任务方面取得了重大进展。然而，在现实世界数据集上训练的扩散模型通常遵循长尾分布，尾部类别的保真度较差。深度生成模型，包括扩散模型，偏向于具有丰富训练图像的类。为了解决稀有类别和尾部类别的合成图像之间观察到的外观重叠问题，我们提出了一种基于对比学习的方法，以最小化不同类别的合成图像分布之间的重叠。我们展示了概率对比学习方法的变体可以应用于任何类条件扩散模型。我们使用长尾分布的多个数据集的损失来显示图像合成的显着改进。大量的实验结果表明，所提出的方法可以有效地处理基于扩散的生成和分类模型的不平衡数据。我们的代码和数据集将在 https://github.com/yanliang3612/DiffROP 公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.10821v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **In-Vivo Hyperspectral Human Brain Image Database for Brain Cancer Detection**<br />
**Title_cn:** 用于脑癌检测的体内高光谱人脑图像数据库<br />
**Authors:** H. Fabelo, S. Ortega, A. Szolna, D. Bulters, J. F. Pineiro, S. Kabwama, A. Shanahan, H. Bulstrode, S. Bisshopp, B. R. Kiran, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，高光谱成像在医学应用中的使用变得越来越普遍。研究人员在开发用于医疗应用的高光谱算法时发现的主要障碍之一是缺乏特定的、公开的高光谱医学数据。本文描述的工作是在欧洲项目 HELICoiD（高光谱成像癌症检测）的框架内开发的，该项目的主要目标是应用高光谱成像在神经外科手术期间实时描绘脑肿瘤。在本文中，介绍了生成第一个体内人脑组织高光谱数据库所遵循的方法。使用定制的高光谱采集系统采集数据，该系统能够捕获 400 至 1000 nm 的可见光和近红外 (VNIR) 范围内的信息。对连续捕获同一场景的两个图像的情况进行了可重复性评估。分析表明，该系统在 450 至 900 nm 的光谱范围内工作效率更高。总共获得了 22 名不同患者的 36 张高光谱图像。根据这些数据，使用基于光谱角度映射器算法的半自动方法标记了超过 300 000 个光谱特征。定义了四个不同的类别：正常组织、肿瘤组织、血管和背景元素。所有高光谱数据均已在公共存储库中提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.10776v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **STF: Spatio-Temporal Fusion Module for Improving Video Object Detection**<br />
**Title_cn:** STF：用于改进视频对象检测的时空融合模块<br />
**Authors:** Noreen Anwar, Guillaume-Alexandre Bilodeau, Wassim Bouachir<br />
**Abstract:** <details><summary>原文: </summary>Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task. The objective of our work is to leverage this complementary information to improve detection. Therefore, we propose a spatio-temporal fusion framework (STF). We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust object representations. Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them. Our evaluation is conducted on three different benchmarks including video sequences of moving road users. The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline object detectors. Code is available at https://github.com/noreenanwar/STF-module</details>
**Abstract_cn:** <details><summary>译文: </summary>视频中的连续帧包含冗余，但它们也可能包含检测任务的相关补充信息。我们工作的目标是利用这些补充信息来改进检测。因此，我们提出了时空融合框架（STF）。我们首先引入多帧和单帧注意模块，允许神经网络在附近帧之间共享特征图以获得更鲁棒的对象表示。其次，我们引入了一个双帧融合模块，该模块以可学习的方式合并特征图以改进它们。我们的评估是根据三个不同的基准进行的，包括移动道路使用者的视频序列。所进行的实验表明，与基线目标检测器相比，所提出的时空融合模块可以提高检测性能。代码可在 https://github.com/norenanwar/STF-module 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.10752v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Semi-weakly-supervised neural network training for medical image registration**<br />
**Title_cn:** 用于医学图像配准的半弱监督神经网络训练<br />
**Authors:** Yiwen Li, Yunguan Fu, Iani J. M. B. Gayo, Qianye Yang, Zhe Min, Shaheer U. Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, et.al.<br />
**Abstract:** <details><summary>原文: </summary>For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于训练配准网络，来自分段的相应感兴趣区域（ROI）的弱监督已被证明可以有效地（a）补充无监督方法，以及（b）在无监督损失不可用或无效的配准任务中独立使用。这种通信通知监督需要注释成本，需要大量的专门工作。本文描述了一种半弱监督的配准管道，当只有一个小的相应 ROI 标记数据集可用时，该管道通过利用未标记的图像对来提高模型性能。我们通过网络权重和图像重采样的扰动来检查两种类型的增强方法，以便可以将基于一致性的无监督损失应用于未标记的数据。提出了新颖的 WarpDDF 和 RegCut 方法，以允许图像对和预测的空间变换（即配准网络的相应输入和输出）之间进行交换扰动，这与现有的分类或分割扰动方法不同。使用 589 个男性骨盆 MR 图像（标有八个解剖 ROI）进行的实验显示了配准性能的改进以及各个策略的消融贡献。此外，本研究尝试构建第一个骨盆结构计算图集，通过注册受试者间 MR 来实现，并量化由于所提出的半弱监督而产生的显着差异，并讨论示例图集的潜在临床用途。得出的统计数据。</details>
**PDF:** <http://arxiv.org/pdf/2402.10728v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift**<br />
**Title_cn:** 使用事后置信度估计的语义分割选择性预测及其在分布偏移下的性能<br />
**Authors:** Bruno Laboissiere Camargos Borges, Bruno Machado Pacheco, Danilo Silva<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through experiments on three medical imaging tasks. Our findings show that post-hoc confidence estimators offer a cost-effective approach to reducing the impacts of distribution shift.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割在各种计算机视觉应用中起着至关重要的作用，但其功效往往因缺乏高质量标记数据而受到阻碍。为了应对这一挑战，一个常见的策略是利用根据不同人群的数据（例如公开数据集）训练的模型。然而，这种方法会导致分布转移问题，从而导致感兴趣群体的性能下降。在模型错误可能产生重大后果的情况下，选择性预测方法提供了一种减轻风险并减少对专家监督的依赖的方法。本文研究了低资源环境中语义分割的选择性预测，从而重点关注应用于分布转移下运行的预训练模型的事后置信估计器。我们提出了一种专为语义分割量身定制的新型图像级置信度测量，并通过三个医学成像任务的实验证明了其有效性。我们的研究结果表明，事后置信度估计器提供了一种具有成本效益的方法来减少分布转移的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.10665v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification**<br />
**Title_cn:** 用于全幻灯片图像分类多实例学习的紧凑且去偏的负实例嵌入<br />
**Authors:** Joohyung Lee, Heejeong Nam, Kwanhyung Lee, Sangchul Hahn<br />
**Abstract:** <details><summary>原文: </summary>Whole-slide image (WSI) classification is a challenging task because 1) patches from WSI lack annotation, and 2) WSI possesses unnecessary variability, e.g., stain protocol. Recently, Multiple-Instance Learning (MIL) has made significant progress, allowing for classification based on slide-level, rather than patch-level, annotations. However, existing MIL methods ignore that all patches from normal slides are normal. Using this free annotation, we introduce a semi-supervision signal to de-bias the inter-slide variability and to capture the common factors of variation within normal patches. Because our method is orthogonal to the MIL algorithm, we evaluate our method on top of the recently proposed MIL algorithms and also compare the performance with other semi-supervised approaches. We evaluate our method on two public WSI datasets including Camelyon-16 and TCGA lung cancer and demonstrate that our approach significantly improves the predictive performance of existing MIL algorithms and outperforms other semi-supervised algorithms. We release our code at https://github.com/AITRICS/pathology_mil.</details>
**Abstract_cn:** <details><summary>译文: </summary>全玻片图像 (WSI) 分类是一项具有挑战性的任务，因为 1) WSI 的补丁缺乏注释，2) WSI 具有不必要的可变性，例如染色协议。最近，多实例学习 (MIL) 取得了重大进展，允许基于幻灯片级别而不是补丁级别注释进行分类。然而，现有的 MIL 方法忽略了正常幻灯片的所有补丁都是正常的。使用这个免费注释，我们引入了半监督信号来消除载玻片间变异性的偏差并捕获正常斑块内变异的常见因素。由于我们的方法与 MIL 算法正交，因此我们在最近提出的 MIL 算法的基础上评估我们的方法，并将其性能与其他半监督方法进行比较。我们在包括 Camelyon-16 和 TCGA 肺癌在内的两个公共 WSI 数据集上评估我们的方法，并证明我们的方法显着提高了现有 MIL 算法的预测性能，并且优于其他半监督算法。我们在 https://github.com/AITRICS/pathology_mil 发布了我们的代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.10595v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Real-Time Model-Based Quantitative Ultrasound and Radar**<br />
**Title_cn:** 基于实时模型的定量超声和雷达<br />
**Authors:** Tom Sharon, Yonina C. Eldar<br />
**Abstract:** <details><summary>原文: </summary>Ultrasound and radar signals are highly beneficial for medical imaging as they are non-invasive and non-ionizing. Traditional imaging techniques have limitations in terms of contrast and physical interpretation. Quantitative medical imaging can display various physical properties such as speed of sound, density, conductivity, and relative permittivity. This makes it useful for a wider range of applications, including improving cancer detection, diagnosing fatty liver, and fast stroke imaging. However, current quantitative imaging techniques that estimate physical properties from received signals, such as Full Waveform Inversion, are time-consuming and tend to converge to local minima, making them unsuitable for medical imaging. To address these challenges, we propose a neural network based on the physical model of wave propagation, which defines the relationship between the received signals and physical properties. Our network can reconstruct multiple physical properties in less than one second for complex and realistic scenarios, using data from only eight elements. We demonstrate the effectiveness of our approach for both radar and ultrasound signals.</details>
**Abstract_cn:** <details><summary>译文: </summary>超声波和雷达信号对于医学成像非常有益，因为它们是非侵入性和非电离的。传统成像技术在对比度和物理解释方面存在局限性。定量医学成像可以显示各种物理特性，例如声速、密度、电导率和相对介电常数。这使得它具有更广泛的应用，包括改善癌症检测、诊断脂肪肝和快速中风成像。然而，当前从接收信号估计物理特性的定量成像技术（例如全波形反演）非常耗时，并且往往会收敛到局部最小值，使得它们不适合医学成像。为了解决这些挑战，我们提出了一种基于波传播物理模型的神经网络，它定义了接收信号和物理特性之间的关系。我们的网络可以仅使用八个元素的数据，在不到一秒的时间内针对复杂和现实的场景重建多个物理属性。我们展示了我们的方法对于雷达和超声波信号的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10520v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes**<br />
**Title_cn:** CodaMal：低成本显微镜中疟疾检测的对比域适应<br />
**Authors:** Ishan Rajendrakumar Dave, Tristan de Blegiers, Chen Chen, Mubarak Shah<br />
**Abstract:** <details><summary>原文: </summary>Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adaptive contrastive loss. It reduces the domain shift by promoting similarity between the representations of HCM and its corresponding LCM image, without imposing an additional annotation burden. In addition, the training objective includes object detection objectives with carefully designed augmentations, ensuring the accurate detection of malaria parasites. On the publicly available large-scale M5-dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides 21x speed up during inference, and requires only half learnable parameters than the prior methods. Our code is publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>疟疾是世界范围内的一个主要健康问题，其诊断需要可扩展的解决方案，能够与低成本显微镜 (LCM) 有效配合。基于深度学习的方法在显微图像的计算机辅助诊断方面取得了成功。然而，这些方法需要带注释的图像来显示受疟疾寄生虫影响的细胞及其生命阶段。与注释来自高成本显微镜 (HCM) 的图像相比，注释来自 LCM 的图像显着增加了医学专家的负担。因此，一个实用的解决方案将在 HCM 图像上进行训练，该解决方案在测试过程中应该能够很好地推广到 LCM 图像上。虽然早期的方法采用多阶段学习过程，但它们没有提供端到端的方法。在这项工作中，我们提出了一个端到端学习框架，名为 CodaMal（疟疾对比域适应）。为了弥合 HCM（训练）和 LCM（测试）之间的差距，我们提出了域自适应对比损失。它通过促进 HCM 表示及其相应的 LCM 图像之间的相似性来减少域偏移，而不会造成额外的注释负担。此外，训练目标包括经过精心设计的增强功能的物体检测目标，确保准确检测疟疾寄生虫。在公开的大规模 M5 数据集上，我们提出的方法在平均精度指标 (mAP) 方面比最先进的方法显着提高了 16%，在推理过程中提供了 21 倍的加速，并且只需要之前方法一半的可学习参数。我们的代码是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2402.10478v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Spike-EVPR: Deep Spiking Residual Network with Cross-Representation Aggregation for Event-Based Visual Place Recognition**<br />
**Title_cn:** Spike-EVPR：具有交叉表示聚合的深度尖峰残差网络，用于基于事件的视觉位置识别<br />
**Authors:** Chenming Hu, Zheng Fang, Kuanxu Hou, Delei Kong, Junjie Jiang, Hao Zhuang, Mingyuan Sun, Xinjie Huang<br />
**Abstract:** <details><summary>原文: </summary>Event cameras have been successfully applied to visual place recognition (VPR) tasks by using deep artificial neural networks (ANNs) in recent years. However, previously proposed deep ANN architectures are often unable to harness the abundant temporal information presented in event streams. In contrast, deep spiking networks exhibit more intricate spatiotemporal dynamics and are inherently well-suited to process sparse asynchronous event streams. Unfortunately, directly inputting temporal-dense event volumes into the spiking network introduces excessive time steps, resulting in prohibitively high training costs for large-scale VPR tasks. To address the aforementioned issues, we propose a novel deep spiking network architecture called Spike-EVPR for event-based VPR tasks. First, we introduce two novel event representations tailored for SNN to fully exploit the spatio-temporal information from the event streams, and reduce the video memory occupation during training as much as possible. Then, to exploit the full potential of these two representations, we construct a Bifurcated Spike Residual Encoder (BSR-Encoder) with powerful representational capabilities to better extract the high-level features from the two event representations. Next, we introduce a Shared & Specific Descriptor Extractor (SSD-Extractor). This module is designed to extract features shared between the two representations and features specific to each. Finally, we propose a Cross-Descriptor Aggregation Module (CDA-Module) that fuses the above three features to generate a refined, robust global descriptor of the scene. Our experimental results indicate the superior performance of our Spike-EVPR compared to several existing EVPR pipelines on Brisbane-Event-VPR and DDD20 datasets, with the average Recall@1 increased by 7.61% on Brisbane and 13.20% on DDD20.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，事件摄像机已通过使用深度人工神经网络（ANN）成功应用于视觉地点识别（VPR）任务。然而，先前提出的深度人工神经网络架构通常无法利用事件流中呈现的丰富时间信息。相比之下，深度尖峰网络表现出更复杂的时空动态，并且本质上非常适合处理稀疏异步事件流。不幸的是，直接将时间密集事件量输入尖峰网络会引入过多的时间步长，导致大规模 VPR 任务的训练成本过高。为了解决上述问题，我们提出了一种名为 Spike-EVPR 的新型深度尖峰网络架构，用于基于事件的 VPR 任务。首先，我们引入了两种专为 SNN 定制的新颖事件表示形式，以充分利用事件流中的时空信息，并尽可能减少训练期间的视频内存占用。然后，为了充分利用这两种表示的潜力，我们构建了一个具有强大表示能力的分叉尖峰残差编码器（BSR-Encoder），以更好地从两个事件表示中提取高级特征。接下来，我们介绍共享和特定描述符提取器（SSD-Extractor）。该模块旨在提取两种表示之间共享的特征以及每种表示所特有的特征。最后，我们提出了一个跨描述符聚合模块（CDA-Module），它融合了上述三个特征，生成一个精致的、鲁棒的场景全局描述符。我们的实验结果表明，与 Brisbane-Event-VPR 和 DDD20 数据集上的几个现有 EVPR 管道相比，我们的 Spike-EVPR 具有优越的性能，平均 Recall@1 在 Brisbane 上增加了 7.61%，在 DDD20 上增加了 13.20%。</details>
**PDF:** <http://arxiv.org/pdf/2402.10476v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification**<br />
**Title_cn:** 用于被遮挡人员重新识别的动态补丁感知丰富变压器<br />
**Authors:** Xin Zhang, Keren Fu, Qijun Zhao<br />
**Abstract:** <details><summary>原文: </summary>Person re-identification (re-ID) continues to pose a significant challenge, particularly in scenarios involving occlusions. Prior approaches aimed at tackling occlusions have predominantly focused on aligning physical body features through the utilization of external semantic cues. However, these methods tend to be intricate and susceptible to noise. To address the aforementioned challenges, we present an innovative end-to-end solution known as the Dynamic Patch-aware Enrichment Transformer (DPEFormer). This model effectively distinguishes human body information from occlusions automatically and dynamically, eliminating the need for external detectors or precise image alignment. Specifically, we introduce a dynamic patch token selection module (DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify informative occlusion-free tokens. These tokens are then selected for deriving subsequent local part features. To facilitate the seamless integration of global classification features with the finely detailed local features selected by DPSM, we introduce a novel feature blending module (FBM). FBM enhances feature representation through the complementary nature of information and the exploitation of part diversity. Furthermore, to ensure that DPSM and the entire DPEFormer can effectively learn with only identity labels, we also propose a Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the recent advances in the Segment Anything Model (SAM). As a result, it generates occlusion images that closely resemble real-world occlusions, greatly enhancing the subsequent contrastive learning process. Experiments on occluded and holistic re-ID benchmarks signify a substantial advancement of DPEFormer over existing state-of-the-art approaches. The code will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>行人重新识别 (re-ID) 仍然是一个重大挑战，特别是在涉及遮挡的情况下。先前旨在解决遮挡问题的方法主要集中于通过利用外部语义线索来对齐身体特征。然而，这些方法往往很复杂并且容易受到噪声的影响。为了解决上述挑战，我们提出了一种创新的端到端解决方案，称为动态补丁感知丰富变压器（DPEFormer）。该模型自动动态地有效区分人体信息和遮挡，无需外部探测器或精确的图像对齐。具体来说，我们引入了动态补丁令牌选择模块（DPSM）。 DPSM 利用标签引导的代理令牌作为中介来识别信息丰富的无遮挡令牌。然后选择这些标记来导出后续的局部特征。为了促进全局分类特征与 DPSM 选择的精细局部特征的无缝集成，我们引入了一种新颖的特征混合模块（FBM）。 FBM 通过信息的互补性和零件多样性的利用来增强特征表示。此外，为了确保 DPSM 和整个 DPEFormer 能够仅使用身份标签进行有效学习，我们还提出了现实遮挡增强（ROA）策略。该策略利用了分段任意模型 (SAM) 的最新进展。因此，它生成的遮挡图像与现实世界的遮挡非常相似，极大地增强了后续的对比学习过程。对遮挡和整体 re-ID 基准的实验表明 DPEFormer 相对于现有最先进的方法取得了实质性进步。该代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.10435v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision**<br />
**Title_cn:** DABS-LS：使用区域水平集自我监督的基于深度图集的分割<br />
**Authors:** Hannah G. Mason, Jack H. Noble<br />
**Abstract:** <details><summary>原文: </summary>Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target volumes and that accurately segment the IAC. We hypothesize that DFs that accurately segment the IAC in target images will also facilitate accurate atlas-based localization of the ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately register the entire volume, our novel contribution is an entirely self-supervised training scheme that aims to produce DFs that accurately segment the target structure. This self-supervision is facilitated using a regional level set (LS) inspired loss function. We call our method Deep Atlas Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS outperforms VoxelMorph for IAC segmentation. Tests with publicly available datasets for trachea and kidney segmentation also show significant improvement in segmentation accuracy, demonstrating the generalizability of the method.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工耳蜗 (CI) 是用于治疗重度至极重度听力损失患者的神经修复体。针对特定患者的听觉神经纤维 (ANF) CI 刺激建模可以帮助听力学家改进 CI 编程。这些模型需要 ANF 相对于周围解剖结构和 CI 的定位。定位具有挑战性，因为 ANF 非常小，在临床成像中无法直接看到。在这项工作中，我们假设 ANF 的位置可以从 CT 中具有高对比度的内耳道 (IAC) 位置准确推断，因为 ANF 穿过耳蜗和大脑之间的这条耳道。受 VoxelMorph 的启发，在本文中我们提出了一种基于深度图集的 IAC 分割网络。我们创建了一个单独的图集，其中 IAC 和 ANF 已预先本地化。我们的网络经过训练，可以生成将图集坐标映射到新目标体积的变形场 (DF)，并准确分割 IAC。我们假设，准确分割目标图像中 IAC 的 DF 也将有助于 ANF 的基于图集的准确定位。与 VoxelMorph 不同，VoxelMorph 旨在生成准确记录整个体积的 DF，我们的新颖贡献是一种完全自我监督的训练方案，旨在生成准确分割目标结构的 DF。使用区域水平集（LS）启发的损失函数可以促进这种自我监督。我们将我们的方法称为使用水平集的基于深度图集的分割（DABS-LS）。结果表明，DABS-LS 在 IAC 分割方面优于 VoxelMorph。使用公开的气管和肾脏分割数据集进行的测试也显示出分割准确性的显着提高，证明了该方法的通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10425v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians**<br />
**Title_cn:** GaussianHair：使用光感知高斯模型进行头发建模和渲染<br />
**Authors:** Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu<br />
**Abstract:** <details><summary>原文: </summary>Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the "GaussianHair Scattering Model", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>发型乍一看就反映了文化和种族。在数字时代，各种逼真的人类发型对于高保真数字人类资产的美观性和包容性也至关重要。然而，由于头发数量庞大、几何结构复杂以及与光线的复杂交互，逼真的头发建模和动画实时渲染是一项艰巨的挑战。本文提出了 GaussianHair，一种新颖的显式头发表示。它可以根据图像对头发几何形状和外观进行全面建模，从而促进创新的照明效果和动态动画功能。 GaussianHair 的核心是一个新颖的概念，即将每根发丝表示为一系列相连的圆柱形 3D 高斯基元。这种方法不仅保留了头发的几何结构和外观，而且还允许在 2D 图像平面上进行有效的光栅化，从而促进可微分体积渲染。我们通过“GaussianHair Scattering Model”进一步增强了该模型，擅长重建发丝的细长结构，并在均匀照明下准确捕捉其局部漫反射颜色。通过大量的实验，我们证实 GaussianHair 在几何和外观保真度方面都取得了突破，超越了最先进的头发重建方法所遇到的限制。除了表示之外，GaussianHair 还支持头发的编辑、重新照明和动态渲染，提供与传统 CG 管道工作流程的无缝集成。为了补充这些进步，我们编制了一个广泛的真实人类头发数据集，每个数据集都具有细致的发丝几何形状，以推动该领域的进一步研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.10483v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Explaining generative diffusion models via visual analysis for interpretable decision-making process**<br />
**Title_cn:** 通过可视化分析解释生成扩散模型以实现可解释的决策过程<br />
**Authors:** Ji-Hoon Park, Yeong-Joon Ju, Seong-Whan Lee<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing the diffusion process and answering the aforementioned research questions to render the diffusion process human-understandable. We show how the output is progressively generated in the diffusion process by explaining the level of denoising and highlighting relationships to foundational visual concepts at each time step through the results of experiments with various visual analyses using the tools. Throughout the training of the diffusion model, the model learns diverse visual concepts corresponding to each time-step, enabling the model to predict varying levels of visual concepts at different stages. We substantiate our tools using Area Under Cover (AUC) score, correlation quantification, and cross-attention mapping. Our findings provide insights into the diffusion process and pave the way for further research into explainable diffusion mechanisms.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在生成任务中表现出了卓越的性能。然而，解释扩散过程仍然具有挑战性，因为它是一系列去噪噪声图像，专家很难解释。为了解决这个问题，我们提出了三个研究问题，从模型生成的视觉概念和模型在每个时间步骤中参与的区域的角度来解释扩散过程。我们设计了用于可视化扩散过程并回答上述研究问题的工具，以使扩散过程易于人类理解。我们通过使用工具进行各种视觉分析的实验结果解释去噪水平并强调每个时间步骤与基本视觉概念的关系，从而展示如何在扩散过程中逐步生成输出。在扩散模型的整个训练过程中，模型学习了每个时间步对应的不同视觉概念，使模型能够预测不同阶段不同级别的视觉概念。我们使用覆盖面积 (AUC) 评分、相关性量化和交叉注意力映射来证实我们的工具。我们的研究结果提供了对扩散过程的见解，并为进一步研究可解释的扩散机制铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.10404v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **3D Diffuser Actor: Policy Diffusion with 3D Scene Representations**<br />
**Title_cn:** 3D 扩散器 Actor：具有 3D 场景表示的策略扩散<br />
**Authors:** Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki<br />
**Abstract:** <details><summary>原文: </summary>We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们将扩散策略和 3D 场景表示结合起来进行机器人操作。扩散策略使用条件扩散模型学习以机器人和环境状态为条件的动作分布。最近，它们的表现优于确定性和替代状态条件动作分布学习方法。 3D 机器人策略使用使用感测深度从单个或多个摄像机视图聚合的 3D 场景特征表示。它们在各个摄像机视点上表现出比 2D 同行更好的概括能力。我们统一了这两条工作线，并提出了 3D Diffuser Actor，这是一种神经策略架构，在给定语言指令的情况下，它构建视觉场景及其条件的 3D 表示，以迭代地对机器人末端执行器的 3D 旋转和平移进行去噪。在每次去噪迭代中，我们的模型将末端执行器姿势估计表示为 3D 场景标记，并通过使用对其他 3D 视觉和语言标记的 3D 相对关注来对它们进行特征化，从而预测每个标记的 3D 平移和旋转误差。 3D Diffuser Actor 在 RLBench 上创下了新的最先进水平，在多视图设置上比当前 SOTA 的绝对性能增益为 16.3%，在单视图设置上的绝对性能增益为 13.1%。在 CALVIN 基准测试中，它在零样本未见场景泛化的设置上优于当前的 SOTA，能够成功运行 0.2 个以上的任务，相对提高了 7%。通过一些演示，它也可以在现实世界中发挥作用。我们消除了模型的架构设计选择，例如 3D 场景特征化和 3D 相对关注，并表明它们都有助于泛化。我们的结果表明，3D 场景表示和强大的生成建模是机器人通过演示进行高效学习的关键。</details>
**PDF:** <http://arxiv.org/pdf/2402.10885v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **VATr++: Choose Your Words Wisely for Handwritten Text Generation**<br />
**Title_cn:** VATr++：明智地选择单词以生成手写文本<br />
**Authors:** Bram Vanherle, Vittorio Pippi, Silvia Cascianelli, Nick Michiels, Frank Van Reeth, Rita Cucchiara<br />
**Abstract:** <details><summary>原文: </summary>Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluation protocol for HTG and conduct a comprehensive benchmarking of existing approaches. By doing so, we aim to establish a foundation for fair and meaningful comparisons between HTG strategies, fostering progress in the field.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，由于采用 GAN、Transformers 以及最初的扩散模型的基于学习的解决方案的成功，风格化手写文本生成 (HTG) 受到了极大的关注。尽管人们的兴趣激增，但仍然存在一个关键但尚未得到充分研究的方面——视觉和文本输入对 HTG 模型训练的影响及其对性能的后续影响。这项研究深入研究了尖端的 Styled-HTG 方法，提出了输入准备和训练正则化的策略，使模型能够实现更好的性能并更好地泛化。这些方面通过对几种不同设置和数据集的广泛分析得到验证。此外，在这项工作中，我们超越了性能优化的范围，并解决了 HTG 研究中的一个重大障碍——缺乏标准化的评估协议。特别是，我们提出了 HTG 评估协议的标准化，并对现有方法进行全面的基准测试。通过这样做，我们的目标是为 HTG 策略之间的公平和有意义的比较奠定基础，促进该领域的进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.10798v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PointMamba: A Simple State Space Model for Point Cloud Analysis**<br />
**Title_cn:** PointMamba：用于点云分析的简单状态空间模型<br />
**Authors:** Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, Xiaoqing Ye, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity and is difficult to extend to long sequence modeling due to limited computational resources and so on. Recently, state space models (SSM), a new family of deep sequence models, have presented great potential for sequence modeling in NLP tasks. In this paper, taking inspiration from the success of SSM in NLP, we propose PointMamba, a framework with global modeling and linear complexity. Specifically, by taking embedded point patches as input, we proposed a reordering strategy to enhance SSM's global modeling ability by providing a more logical geometric scanning order. The reordered point tokens are then sent to a series of Mamba blocks to causally capture the point cloud structure. Experimental results show our proposed PointMamba outperforms the transformer-based counterparts on different point cloud analysis datasets, while significantly saving about 44.3% parameters and 25% FLOPs, demonstrating the potential option for constructing foundational 3D vision models. We hope our PointMamba can provide a new perspective for point cloud analysis. The code is available at https://github.com/LMD0311/PointMamba.</details>
**Abstract_cn:** <details><summary>译文: </summary>Transformer 凭借其出色的全局建模能力，已成为点云分析任务的基础架构之一。然而，注意力机制具有二次复杂度，并且由于计算资源有限等原因很难扩展到长序列建模。最近，状态空间模型（SSM）作为一种新的深度序列模型家族，在 NLP 任务中的序列建模方面展现出了巨大的潜力。在本文中，受到 SSM 在 NLP 中成功的启发，我们提出了 PointMamba，一个具有全局建模和线性复杂性的框架。具体来说，通过以嵌入点补丁作为输入，我们提出了一种重新排序策略，通过提供更符合逻辑的几何扫描顺序来增强SSM的全局建模能力。然后，重新排序的点令牌被发送到一系列 Mamba 块，以因果地捕获点云结构。实验结果表明，我们提出的 PointMamba 在不同的点云分析数据集上优于基于 Transformer 的对应方案，同时显着节省了约 44.3% 的参数和 25% 的 FLOP，展示了构建基础 3D 视觉模型的潜在选择。我们希望我们的PointMamba能够为点云分析提供一个新的视角。代码可在 https://github.com/LMD0311/PointMamba 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.10739v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds**<br />
**Title_cn:** 多模型 3D 配准：在杂乱的点云中查找多个移动物体<br />
**Authors:** David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone<br />
**Abstract:** <details><summary>原文: </summary>We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究了 3D 配准问题的一种变体，称为多模型 3D 配准。在多模型配准问题中，我们得到两个点云，描绘一组处于不同姿势的对象（可能包括属于背景的点），我们希望同时重建所有对象在两个点云之间的移动方式。此设置概括了标准 3D 配准，其中人们想要重建单个姿势，例如，描绘静态场景的传感器的运动。此外，它为相关机器人应用提供了基于数学的公式，例如，机器人上的深度传感器感知动态场景，并具有估计其自身运动（从场景的静态部分）的目标，同时恢复机器人的运动。所有动态对象。我们假设基于对应的设置，其中两个点云之间有假定的匹配，并考虑这些对应受到异常值困扰的实际情况。然后，我们提出了一种基于期望最大化（EM）的简单方法，并建立了 EM 方法收敛到基本事实的理论条件。我们在从桌面场景到自动驾驶场景的模拟和真实数据集中评估该方法，并证明其与最先进的场景流方法结合建立密集对应关系时的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10865v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PEGASUS: Personalized Generative 3D Avatars with Composable Attributes**<br />
**Title_cn:** PEGASUS：具有可组合属性的个性化生成 3D 化身<br />
**Authors:** Hyunsoo Cha, Byungjun Kim, Hanbyul Joo<br />
**Abstract:** <details><summary>原文: </summary>We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources. As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity. We present two key approaches to achieve this goal. First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos. Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism. Subsequently, we introduce a zero-shot approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出 PEGASUS，一种从单眼视频源构建个性化生成 3D 面部头像的方法。作为一个组合生成模型，我们的模型可以通过分离控制来选择性地改变目标个体的面部属性（例如头发或鼻子），同时保留身份。我们提出了实现这一目标的两种关键方法。首先，我们提出了一种通过构建具有不同面部属性的目标身份的合成视频集合来构建特定于人的生成 3D 头像的方法，其中视频是通过借用其他单眼视频中不同个体的部分来合成的。通过多次实验，我们通过生成高度真实的看不见的属性来证明我们的方法的卓越性能。随后，我们引入了一种零样本方法，通过利用先前构建的个性化生成模型来更有效地实现相同的生成建模。</details>
**PDF:** <http://arxiv.org/pdf/2402.10636v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model**<br />
**Title_cn:** U$^2$MRPD：通过促进大的潜在扩散模型进行无监督欠采样 MRI 重建<br />
**Authors:** Ziqi Gao, S. Kevin Zhou<br />
**Abstract:** <details><summary>原文: </summary>Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion methods on in-domain datasets while demonstrating the best generalizability on out-of-domain datasets. To the best of our knowledge, U$^2$MRPD is the {\bf first} unsupervised method that demonstrates the universal prowess of a LLDM, %trained on magnitude-only natural images in medical imaging, attaining the best adaptability for both MRI database-free and database-available scenarios and generalizability towards out-of-domain data.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自然图像上预训练的大型潜在扩散模型（LLDM）中的隐式视觉知识非常丰富，并且假设对于自然图像和医学图像来说是通用的。为了检验这一假设，我们引入了一种新的无监督欠采样 MRI 重建框架，通过提示预先训练的大型潜在扩散模型 (U$^2$MRPD)。现有的数据驱动、有监督的欠采样 MRI 重建网络通常对不同数据采集场景的通用性和适应性有限；然而，U$^2$MRPD 通过使用专为复杂值 MRI 图像定制的 MRSampler 提示 LLDM，支持图像特定的 MRI 重建。对于任何单源或多源 MRI 数据集，MRAdapter 进一步提高了 U$^2$MRPD 的性能，同时保持生成图像先验完整。在多个数据集上的实验表明，U$^2$MRPD 在域内数据集上实现了与监督和 MRI 扩散方法相当或更好的性能，同时在域外数据集上展示了最佳的通用性。据我们所知，U$^2$MRPD 是{\bf第一个}无监督方法，展示了 LLDM 的通用能力，%在医学成像中的仅幅度自然图像上进行训练，获得了对 MRI 和 MRI 的最佳适应性无数据库和数据库可用的场景以及对域外数据的通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10609v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Universal Prompt Optimizer for Safe Text-to-Image Generation**<br />
**Title_cn:** 用于安全生成文本到图像的通用提示优化器<br />
**Authors:** Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang<br />
**Abstract:** <details><summary>原文: </summary>Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像（T2I）模型在根据文本提示生成图像方面表现出了出色的性能。然而，这些模型很容易受到不安全输入的影响，从而生成性、骚扰和非法活动图像等不安全内容。现有基于图像检查器、模型微调和嵌入块的研究在实际应用中是不切实际的。因此，\textit{我们提出了第一个用于黑盒场景中安全 T2I 生成的通用提示优化器}。我们首先通过 GPT-3.5 Turbo 构建一个由有毒-清洁提示对组成的数据集。为了引导优化器具有将有毒提示转换为干净提示的能力，同时保留语义信息，我们设计了一种新颖的奖励函数，用于测量生成图像的毒性和文本对齐，并通过近端策略优化来训练优化器。实验表明，我们的方法可以有效降低各种 T2I 模型生成不适当图像的可能性，并且对文本对齐没有显着影响。还可以灵活地与方法结合，以达到更好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.10882v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting**<br />
**Title_cn:** 用于连续一致物理信息的降水临近预报的完全可微拉格朗日卷积神经网络<br />
**Authors:** Peter Pavlík, Martin Výboh, Anna Bou Ezzeddine, Viera Rozinajová<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种用于降水临近预报的卷积神经网络模型，该模型将数据驱动的学习与物理领域知识相结合。我们提出了 LUPIN，一种用于物理信息临近预报的拉格朗日双 U-Net，它借鉴了现有的基于外推法的临近预报方法，并以完全可微分和 GPU 加速的方式实现了数据的拉格朗日坐标系转换，以实现实时预报。端到端的训练和推理。根据我们的评估，LUPINE 匹配并超过了所选基准的性能，为其他拉格朗日机器学习模型打开了大门。</details>
**PDF:** <http://arxiv.org/pdf/2402.10747v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation**<br />
**Title_cn:** 进行廉价的缩放：用于更高分辨率适应的自级联扩散模型<br />
**Authors:** Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have proven to be highly effective in image and video generation; however, they still face composition challenges when generating images of varying sizes due to single-scale training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational and optimization resources, yet achieving a generation capability comparable to low-resolution models remains elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5X training speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with virtually no additional inference time.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型已被证明在图像和视频生成方面非常有效；然而，由于单尺度训练数据，它们在生成不同尺寸的图像时仍然面临构图挑战。使大型预训练扩散模型适应更高分辨率需要大量的计算和优化资源，但实现与低分辨率模型相当的生成能力仍然难以实现。本文提出了一种新颖的自级联扩散模型，该模型利用从训练有素的低分辨率模型中获得的丰富知识来快速适应更高分辨率的图像和视频生成，采用免调整或廉价的上采样器调整范例。自级联扩散模型集成了一系列多尺度上采样器模块，可以有效地适应更高分辨率，同时保留原始组成和生成能力。我们进一步提出了一种枢轴引导的噪声重新调度策略，以加快推理过程并改善局部结构细节。与完全微调相比，我们的方法实现了 5 倍的训练加速，并且只需要额外的 0.002M 调整参数。大量实验表明，我们的方法可以通过仅 10k 步骤的微调来快速适应更高分辨率的图像和视频合成，几乎没有额外的推理时间。</details>
**PDF:** <http://arxiv.org/pdf/2402.10491v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Theoretical Understanding of Learning from Adversarial Perturbations**<br />
**Title_cn:** 从对抗性扰动中学习的理论理解<br />
**Authors:** Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki<br />
**Abstract:** <details><summary>原文: </summary>It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations matches that from standard samples except for specific regions under mild conditions. The code is available at https://github.com/s-kumano/learning-from-adversarial-perturbations.</details>
**Abstract_cn:** <details><summary>译文: </summary>目前尚不完全理解为什么对抗性例子可以欺骗神经网络并在不同网络之间转移。为了阐明这一点，一些研究假设对抗性扰动虽然表现为噪音，但包含类别特征。经验证据表明，在错误标记的对抗样本上训练的网络仍然可以很好地泛化到正确标记的测试样本。然而，对扰动如何包含类特征并有助于泛化的理论理解是有限的。在这项研究中，我们提供了一个理论框架，用于理解使用在相互正交样本上训练的单隐藏层网络从扰动中学习。我们的结果强调，各种对抗性扰动，甚至几个像素的扰动，都包含足够的类特征以进行泛化。此外，我们还发现，除了温和条件下的特定区域外，从扰动中学习时的决策边界与标准样本中的决策边界相匹配。该代码可在 https://github.com/s-kumano/learning-from-adversarial-perturbations 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.10470v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Polyhedral Complex Derivation from Piecewise Trilinear Networks**<br />
**Title_cn:** 分段三线性网络的多面体复数推导<br />
**Authors:** Jin-Hwa Kim<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between the eikonal loss and the planarity of the hypersurfaces.</details>
**Abstract_cn:** <details><summary>译文: </summary>可视化深度神经网络的最新进展提供了对其结构的深入了解以及从连续分段仿射 (CPWA) 函数中的网格提取。与此同时，神经表面表示学习的发展纳入了非线性位置编码，解决了光谱偏差等问题；然而，这对应用基于 CPWA 函数的网格提取技术提出了挑战。我们重点关注作为位置编码的三线性插值方法，提出了理论见解和分析网格提取，展示了在 eikonal 约束下三线性区域内超曲面到平面的变换。此外，我们引入了一种近似三个超曲面之间的交点的方法，有助于更广泛的应用。我们通过倒角距离和效率以及角距离凭经验验证正确性和简约性，同时检查 eikonal 损失和超曲面的平面性之间的相关性。</details>
**PDF:** <http://arxiv.org/pdf/2402.10403v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **ManiFPT: Defining and Analyzing Fingerprints of Generative Models**<br />
**Title_cn:** ManiFPT：定义和分析生成模型的指纹<br />
**Authors:** Hae Jin Song, Mahyar Khayatkhoei, Wael AbdAlmageed<br />
**Abstract:** <details><summary>原文: </summary>Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的工作表明，生成模型在生成的样本上留下了其潜在生成过程的痕迹，广泛地称为生成模型的指纹，并研究了它们在从真实图像中检测合成图像中的效用。然而，这些指纹可以在多大程度上区分各种类型的合成图像并帮助识别潜在的生成过程，但仍有待探索。特别是，据我们所知，指纹的定义仍然不清楚。为此，在这项工作中，我们形式化了生成模型中工件和指纹的定义，提出了一种在实践中计算它们的算法，并最终研究了其在区分大量不同生成模型方面的有效性。我们发现，与现有方法相比，使用我们提出的定义可以显着提高从样本（模型归因）中识别潜在生成过程的任务的性能。此外，我们研究了指纹的结构，并观察到它可以很好地预测不同设计选择对生成过程的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.10401v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)**<br />
**Title_cn:** 用稀疏线性概念嵌入解释 CLIP (SpLiCE)<br />
**Authors:** Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju<br />
**Abstract:** <details><summary>原文: </summary>CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>CLIP 嵌入在广泛的计算机视觉任务中表现出了卓越的性能。然而，这些高维、密集的向量表示不容易解释，限制了它们在需要透明度的下游应用中的有用性。在这项工作中，我们凭经验证明 CLIP 的潜在空间是高度结构化的，因此 CLIP 表示可以分解为其底层语义组件。我们利用这种理解提出了一种新颖的方法，即稀疏线性概念嵌入（SpLiCE），用于将 CLIP 表示转换为人类可解释概念的稀疏线性组合。与以前的工作不同，SpLiCE 不需要概念标签，可以事后应用。通过对多个真实世界数据集的广泛实验，我们验证了 SpLiCE 输出的表示可以解释甚至取代传统的密集 CLIP 表示，保持等效的下游性能，同时显着提高其可解释性。我们还演示了 SpLiCE 表示的几个用例，包括检测虚假相关性、模型编辑和量化数据集中的语义变化。</details>
**PDF:** <http://arxiv.org/pdf/2402.10376v1><br />
**Code:** null<br />

