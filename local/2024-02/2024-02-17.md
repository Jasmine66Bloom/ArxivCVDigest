## [UPDATED!] **2024-02-17** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method**<br />
**Title_cn:** TC-DiffRecon：基于扩散模型和改进的MF-UNet方法的纹理协调MRI重建方法<br />
**Authors:** Chenyan Zhang, Yifei Chen, Zhenxiong Fan, Yiyu Huang, Wenchao Weng, Ruiquan Ge, Dong Zeng, Changmiao Wang<br />
**Abstract:** <details><summary>原文: </summary>Recently, diffusion models have gained significant attention as a novel set of deep learning-based generative methods. These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data. However, as an unconditional generative model, the diffusion model typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap. This often results in image fragmentation and incoherence. Furthermore, the inherent limitations of the diffusion model often lead to excessive smoothing of the generated images. In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors. To address these challenges, we propose a novel diffusion model-based MRI reconstruction method, named TC-DiffRecon, which does not rely on a specific acceleration factor for training. We also suggest the incorporation of the MF-UNet module, designed to enhance the quality of MRI images generated by the model while mitigating the over-smoothing issue to a certain extent. During the image generation sampling process, we employ a novel TCKG module and a Coarse-to-Fine sampling scheme. These additions aim to harmonize image texture, expedite the sampling process, while achieving data consistency. Our source code is available at https://github.com/JustlfC03/TC-DiffRecon.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，扩散模型作为一组新颖的基于深度学习的生成方法受到了极大的关注。这些模型尝试从遵循目标分布的高斯分布中采样数据，并已成功适应 MRI 数据的重建。然而，作为一种无条件生成模型，扩散模型通常会破坏图像协调，因为条件引导引入了数据的一致投影。这通常会导致图像碎片和不连贯。此外，扩散模型的固有局限性常常导致生成的图像过度平滑。同样，一些基于深度学习的模型往往泛化性能较差，这意味着它们的有效性很大程度上受到不同加速因素的影响。为了解决这些挑战，我们提出了一种新颖的基于扩散模型的 MRI 重建方法，名为 TC-DiffRecon，该方法不依赖于特定的加速因子进行训练。我们还建议加入 MF-UNet 模块，旨在提高模型生成的 MRI 图像的质量，同时在一定程度上缓解过度平滑问题。在图像生成采样过程中，我们采用了新颖的 TCKG 模块和从粗到细的采样方案。这些新增内容旨在协调图像纹理、加快采样过程，同时实现数据一致性。我们的源代码位于 https://github.com/JustlfC03/TC-DiffRecon。</details>
**PDF:** <http://arxiv.org/pdf/2402.11274v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model**<br />
**Title_cn:** DiffPoint：使用基于 ViT 的扩散模型进行单视点和多视点云重建<br />
**Authors:** Yu Feng, Xing Shi, Mengli Cheng, Yun Xiong<br />
**Abstract:** <details><summary>原文: </summary>As the task of 2D-to-3D reconstruction has gained significant attention in various real-world scenarios, it becomes crucial to be able to generate high-quality point clouds. Despite the recent success of deep learning models in generating point clouds, there are still challenges in producing high-fidelity results due to the disparities between images and point clouds. While vision transformers (ViT) and diffusion models have shown promise in various vision tasks, their benefits for reconstructing point clouds from images have not been demonstrated yet. In this paper, we first propose a neat and powerful architecture called DiffPoint that combines ViT and diffusion models for the task of point cloud reconstruction. At each diffusion step, we divide the noisy point clouds into irregular patches. Then, using a standard ViT backbone that treats all inputs as tokens (including time information, image embeddings, and noisy patches), we train our model to predict target points based on input images. We evaluate DiffPoint on both single-view and multi-view reconstruction tasks and achieve state-of-the-art results. Additionally, we introduce a unified and flexible feature fusion module for aggregating image features from single or multiple input images. Furthermore, our work demonstrates the feasibility of applying unified architectures across languages and images to improve 3D reconstruction tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于 2D 到 3D 重建任务在各种现实场景中受到了广泛关注，因此能够生成高质量的点云变得至关重要。尽管深度学习模型最近在生成点云方面取得了成功，但由于图像和点云之间的差异，在生成高保真结果方面​​仍然存在挑战。虽然视觉变换器 (ViT) 和扩散模型在各种视觉任务中显示出前景，但它们在从图像重建点云方面的优势尚未得到证明。在本文中，我们首先提出了一种名为 DiffPoint 的简洁而强大的架构，它结合了 ViT 和扩散模型来完成点云重建任务。在每个扩散步骤中，我们将噪声点云划分为不规则的斑块。然后，使用将所有输入视为标记（包括时间信息、图像嵌入和噪声补丁）的标准 ViT 主干，我们训练模型以根据输入图像预测目标点。我们在单视图和多视图重建任务上评估 DiffPoint 并取得了最先进的结果。此外，我们引入了一个统一且灵活的特征融合模块，用于聚合来自单个或多个输入图像的图像特征。此外，我们的工作证明了跨语言和图像应用统一架构来改进 3D 重建任务的可行性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11241v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models**<br />
**Title_cn:** Asclepius：医学多模态大语言模型的频谱评估基准<br />
**Authors:** Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius ensures a comprehensive evaluation by encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks, and exempting from train-validate contamination. We further provide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human specialists, providing insights into their competencies and limitations in various medical contexts. Our work not only advances the understanding of Med-MLLMs' capabilities but also sets a precedent for future evaluations and the safe deployment of these models in clinical environments. We launch and maintain a leaderboard for community assessment of Med-MLLM capabilities (https://asclepius-med.github.io/).</details>
**Abstract_cn:** <details><summary>译文: </summary>医学多模态大语言模型 (Med-MLLM) 的重大突破通过强大的信息综合和医疗决策支持革新了现代医疗保健。然而，由于现实世界诊断框架的复杂性（涵盖不同的医学专业并涉及复杂的临床决策），这些模型通常根据不适合 Med-MLLM 的基准进行评估。此外，这些基准很容易受到数据泄露的影响，因为 Med-MLLM 是在大型公开数据集上进行训练的。因此，对于可靠的 Med-MLLM 评估来说，非常需要一个独立且具有临床代表性的基准。为此，我们引入了 Asclepius，一种新颖的 Med-MLLM 基准，它严格、全面地评估模型在以下方面的能力：不同的医学专业（心血管、胃肠病学等）和不同的诊断能力（感知、疾病分析等）。 Asclepius以提出的3个核心原则为基础，通过涵盖15个医学专业、将临床任务分为3个主要类别和8个子类别，并避免训练验证污染，确保进行全面评估。我们进一步对 6 位 Med-MLLM 进行深入分析，并将他们与 5 位人类专家进行比较，深入了解他们在各种医疗环境中的能力和局限性。我们的工作不仅增进了对 Med-MLLM 功能的理解，还为未来的评估以及这些模型在临床环境中的安全部署奠定了先例。我们启动并维护了 Med-MLLM 能力社区评估排行榜 (https://asclepius-med.github.io/)。</details>
**PDF:** <http://arxiv.org/pdf/2402.11217v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Hand Biometrics in Digital Forensics**<br />
**Title_cn:** 数字取证中的手部生物识别技术<br />
**Authors:** Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri<br />
**Abstract:** <details><summary>原文: </summary>Digital forensic is now an unavoidable part for securing the digital world from identity theft. Higher order of crimes, dealing with a massive database is really very challenging problem for any intelligent system. Biometric is a better solution to win over the problems encountered by digital forensics. Many biometric characteristics are playing their significant roles in forensics over the decades. The potential benefits and scope of hand based modes in forensics have been investigated with an illustration of hand geometry verifi-cation method. It can be applied when effective biometric evidences are properly unavailable; gloves are damaged, and dirt or any kind of liquid can minimize the accessibility and reliability of the fingerprint or palmprint. Due to the crisis of pure uniqueness of hand features for a very large database, it may be relevant for verification only. Some unimodal and multimodal hand based biometrics (e.g. hand geometry, palmprint and hand vein) with several feature extractions, database and verification methods have been discussed with 2D, 3D and infrared images.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字取证现在是保护数字世界免遭身份盗窃的不可避免的一部分。高阶犯罪、处理海量数据库对于任何智能系统来说确实是非常具有挑战性的问题。生物识别是解决数字取证遇到的问题的更好解决方案。几十年来，许多生物识别特征在法医学中发挥着重要作用。通过手几何验证方法的说明，研究了基于手的模式在取证中的潜在好处和范围。当有效的生物识别证据适当不可用时可以应用；手套损坏、污垢或任何类型的液体都会降低指纹或掌纹的可访问性和可靠性。由于非常大的数据库存在手部特征纯粹唯一性的危机，因此它可能仅与验证相关。一些单模态和多模态手部生物识别技术（例如手几何、掌纹和手静脉）以及多种特征提取、数据库和验证方法已经通过 2D、3D 和红外图像进行了讨论。</details>
**PDF:** <http://arxiv.org/pdf/2402.11206v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos**<br />
**Title_cn:** 使用基于多模态机器学习的工具支持专家对对话视频进行人类行为分析<br />
**Authors:** Riku Arakawa, Kiyosu Maeda, Hiromu Yakura<br />
**Abstract:** <details><summary>原文: </summary>Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To solve it, we developed Providence, a visual-programming-based tool based on design considerations derived from a formative study with experts. It enables experts to combine various machine learning algorithms to capture human behavioral cues without writing code. Our study showed its preferable usability and satisfactory output with less cognitive load imposed in accomplishing scene search tasks of conversations, verifying the importance of its customizability and transparency. Furthermore, through the in-the-wild trial, we confirmed the objectivity and reusability of the tool transform experts' workflow, suggesting the advantage of expert-AI teaming in a highly human-contextual domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>对话的多模态场景搜索对于解锁对社会动态的有价值的见解和增强我们的沟通至关重要。虽然会话分析专家拥有自己的知识和技能来查找关键场景，但缺乏全面、用户友好的工具来简化各种多模式查询的处理，从而影响了效率和客观性。为了解决这个问题，我们开发了 Providence，这是一种基于可视化编程的工具，其设计考虑源自与专家的形成性研究。它使专家能够结合各种机器学习算法来捕获人类行为线索，而无需编写代码。我们的研究表明，它具有更好的可用性和令人满意的输出，在完成对话的场景搜索任务时施加的认知负荷更少，验证了其可定制性和透明度的重要性。此外，通过野外试验，我们证实了工具转换专家工作流程的客观性和可重用性，表明专家人工智能团队在高度人性化领域中的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.11145v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review**<br />
**Title_cn:** 用于视觉场景理解的语义感知神经辐射场：综合综述<br />
**Authors:** Thang-Anh-Quan Nguyen, Amine Bourki, Mátyás Macudzinski, Anthony Brunel, Mohammed Bennamoun<br />
**Abstract:** <details><summary>原文: </summary>This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation.</details>
**Abstract_cn:** <details><summary>译文: </summary>这篇综述深入研究了语义感知神经辐射场 (NeRF) 在视觉场景理解中的作用，涵盖了 250 多篇学术论文的分析。它探讨了 NeRF 如何熟练地推断场景中静态和动态对象的 3D 表示。此功能对于生成高质量的新视点、完成缺失的场景细节（修复）、进行全面的场景分割（全景分割）、预测 3D 边界框、编辑 3D 场景以及提取以对象为中心的 3D 模型至关重要。这项研究的一个重要方面是将语义标签作为视点不变函数的应用，它有效地将空间坐标映射到一系列语义标签，从而促进场景内不同对象的识别。总的来说，这项调查强调了语义感知神经辐射场在视觉场景解释中的进展和多样化应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.11141v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation**<br />
**Title_cn:** GraphKD：通过结构化图创建探索知识蒸馏以实现文档对象检测<br />
**Authors:** Ayan Banerjee, Sanket Biswas, Josep Lladós, Umapada Pal<br />
**Abstract:** <details><summary>原文: </summary>Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight distribution and put more weightage on non-text nodes. We encode the complete graph as a knowledge representation and transfer it from the teacher to the student through the proposed distillation loss by effectively capturing both local and global information concurrently. Extensive experimentation on competitive benchmarks demonstrates that the proposed framework outperforms the current state-of-the-art approaches. The code will be available at: https://github.com/ayanban011/GraphKD.</details>
**Abstract_cn:** <details><summary>译文: </summary>文档中的对象检测是通过理解不同元素之间的层次结构和关系来自动化数字或扫描文档中的结构元素识别过程的关键步骤。大型且复杂的模型虽然实现了高精度，但计算成本高昂且内存密集，这使得它们在资源受限的设备上部署不切实际。知识蒸馏使我们能够创建小型且更高效的模型，这些模型保留了大型模型的大部分性能。在这里，我们提出了一个基于图的知识蒸馏框架，以正确识别和定位文档图像中的文档对象。在这里，我们设计了一个结构化图，其中的节点包含提案级特征，边代表不同提案区域之间的关系。此外，为了减少文本偏差，设计了自适应节点采样策略来修剪权重分布并将更多权重放在非文本节点上。我们将完整的图编码为知识表示，并通过有效地同时捕获局部和全局信息，通过所提出的蒸馏损失将其从教师转移到学生。对竞争基准的广泛实验表明，所提出的框架优于当前最先进的方法。该代码位于：https://github.com/ayanban011/GraphKD。</details>
**PDF:** <http://arxiv.org/pdf/2402.11401v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **On Good Practices for Task-Specific Distillation of Large Pretrained Models**<br />
**Title_cn:** 关于大型预训练模型的特定任务蒸馏的良好实践<br />
**Authors:** Juliette Marrie, Michael Arbel, Julien Mairal, Diane Larlus<br />
**Abstract:** <details><summary>原文: </summary>Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of knowledge distillation have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific distillation. To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard data augmentation. This strategy eliminates the need for engineered text prompts and improves distillation of generic models into streamlined specialized networks.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型预训练视觉模型在不同的识别任务中表现出卓越的泛化能力。然而，现实世界的应用程序通常需要针对特定​​问题量身定制的紧凑模型。为此目的，设计了知识蒸馏的变体，使特定于任务的紧凑模型（学生）能够向通用的大型预训练模型（教师）学习。在本文中，我们表明，最近预训练模型的出色鲁棒性和多功能性挑战了文献中建立的常见做法，需要一套新的针对特定任务蒸馏的最佳指南。为了解决下游任务中样本缺乏的问题，我们还展示了基于稳定扩散的 Mixup 变体补充了标准数据增强。该策略消除了对工程文本提示的需要，并改进了将通用模型提炼为简化的专业网络的过程。</details>
**PDF:** <http://arxiv.org/pdf/2402.11305v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Hierarchical Prior-based Super Resolution for Point Cloud Geometry Compression**<br />
**Title_cn:** 用于点云几何压缩的基于分层先验的超分辨率<br />
**Authors:** Dingquan Li, Kede Ma, Jing Wang, Ge Li<br />
**Abstract:** <details><summary>原文: </summary>The Geometry-based Point Cloud Compression (G-PCC) has been developed by the Moving Picture Experts Group to compress point clouds. In its lossy mode, the reconstructed point cloud by G-PCC often suffers from noticeable distortions due to the na\"{i}ve geometry quantization (i.e., grid downsampling). This paper proposes a hierarchical prior-based super resolution method for point cloud geometry compression. The content-dependent hierarchical prior is constructed at the encoder side, which enables coarse-to-fine super resolution of the point cloud geometry at the decoder side. A more accurate prior generally yields improved reconstruction performance, at the cost of increased bits required to encode this side information. With a proper balance between prior accuracy and bit consumption, the proposed method demonstrates substantial Bjontegaard-delta bitrate savings on the MPEG Cat1A dataset, surpassing the octree-based and trisoup-based G-PCC v14. We provide our implementations for reproducible research at https://github.com/lidq92/mpeg-pcc-tmc13.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于几何的点云压缩（G-PCC）是由运动图像专家组开发的，用于压缩点云。在有损模式下，G-PCC 重建的点云经常由于朴素的几何量化（即网格下采样）而遭受明显的失真。本文提出了一种基于分层先验的点云超分辨率方法云几何压缩。在编码器端构建内容相关的分层先验，这使得解码器端能够对点云几何进行从粗到细的超分辨率。更准确的先验通常会提高重建性能，但代价是通过在先前精度和比特消耗之间进行适当平衡，所提出的方法在 MPEG Cat1A 数据集上展示了可观的 Bjontegaard-delta 比特率节省，超越了基于八叉树和基于 trisoup 的 G-PCC v14。我们在 https://github.com/lidq92/mpeg-pcc-tmc13 上提供了可重复研究的实现。</details>
**PDF:** <http://arxiv.org/pdf/2402.11250v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Knowledge Distillation Based on Transformed Teacher Matching**<br />
**Title_cn:** 基于变革型教师匹配的知识蒸馏<br />
**Authors:** Kaixiang Zheng, En-Hui Yang<br />
**Abstract:** <details><summary>原文: </summary>As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel distillation approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at https://github.com/zkxufo/TTM.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为连接逻辑匹配和概率分布匹配的技术，温度缩放在知识蒸馏（KD）中发挥着关键作用。传统上，温度缩放应用于 KD 中教师的 logits 和学生的 logits。受最近一些工作的启发，在本文中，我们放弃了学生端的温度缩放，并系统地研究了 KD 的结果变体，称为变换教师匹配（TTM）。通过将温度缩放重新解释为概率分布的幂变换，我们表明，与原始 KD 相比，TTM 在其目标函数中具有固有的 R\'enyi 熵项，它作为额外的正则化项。大量的实验结果表明，由于这种固有的正则化，TTM 使受过训练的学生比原始 KD 具有更好的泛化能力。为了进一步增强学生匹配教师功率转换概率分布的能力，我们在 TTM 中引入了样本自适应加权系数，从而产生了一种称为加权 TTM (WTTM) 的新颖蒸馏方法。综合实验表明，WTTM虽然简单，但有效，在TTM的基础上进行了改进，达到了state-of-the-art的精度性能。我们的源代码可在 https://github.com/zkxufo/TTM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11148v1><br />
**Code:** <https://github.com/zkxufo/TTM>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Exploiting T-norms for Deep Learning in Autonomous Driving**<br />
**Title_cn:** 利用 T 范数进行自动驾驶深度学习<br />
**Authors:** Mihaela Cătălina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has been at the core of the autonomous driving field development, due to the neural networks' success in finding patterns in raw data and turning them into accurate predictions. Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models' performance. However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving. In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of event detection in autonomous driving. We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-norm-based losses are estimated to require more than 100 GiB, far exceeding the amount of memory normally available, (ii) that t-norm-based losses improve performance, especially when limited labelled data are available, and (iii) that t-norm-based losses can further improve performance when exploited on both labelled and unlabelled data.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于神经网络成功地发现原始数据中的模式并将其转化为准确的预测，深度学习一直是自动驾驶领域发展的核心。此外，最近的神经符号研究表明，通过 t 范数将有关当前问题的可用背景知识纳入损失函数中可以进一步提高深度学习模型的性能。然而，基于 t 范数的损失可能具有非常高的内存要求，因此它们可能无法应用于自动驾驶等复杂的应用领域。在本文中，我们展示了如何定义基于内存效率的 t 范数的损失，从而允许利用 t 范数来执行自动驾驶中的事件检测任务。我们对 ROAD-R 数据集进行了广泛的实验分析，并表明 (i) 我们的建议可以在可用内存少于 25 GiB 的 GPU 上实现和运行，而基于标准 t 范数的损失估计需要超过100 GiB，远远超过通常可用的内存量，(ii) 基于 t-norm 的损失可以提高性能，特别是当可用标记数据有限时，以及 (iii) 基于 t-norm 的损失在被利用时可以进一步提高性能在标记和未标记数据上。</details>
**PDF:** <http://arxiv.org/pdf/2402.11362v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing**<br />
**Title_cn:** ChatEarthNet：全球范围的高质量遥感图像文本数据集<br />
**Authors:** Zhenghang Yuan, Zhitong Xiong, Lichao Mou, Xiao Xiang Zhu<br />
**Abstract:** <details><summary>原文: </summary>An in-depth comprehension of global land cover is essential in Earth observation, forming the foundation for a multitude of applications. Although remote sensing technology has advanced rapidly, leading to a proliferation of satellite imagery, the inherent complexity of these images often makes them difficult for non-expert users to understand. Natural language, as a carrier of human knowledge, can be a bridge between common users and complicated satellite imagery. In this context, we introduce a global-scale, high-quality image-text dataset for remote sensing, providing natural language descriptions for Sentinel-2 data to facilitate the understanding of satellite imagery for common users. Specifically, we utilize Sentinel-2 data for its global coverage as the foundational image source, employing semantic segmentation labels from the European Space Agency's (ESA) WorldCover project to enrich the descriptions of land covers. By conducting in-depth semantic analysis, we formulate detailed prompts to elicit rich descriptions from ChatGPT. To enhance the dataset's quality, we introduce the manual verification process. This step involves manual inspection and correction to refine the dataset, thus significantly improving its accuracy and quality. Finally, we offer the community ChatEarthNet, a large-scale image-text dataset characterized by global coverage, high quality, wide-ranging diversity, and detailed descriptions. ChatEarthNet consists of 163,488 image-text pairs with captions generated by ChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated by ChatGPT-4V(ision). This dataset has significant potential for training vision-language foundation models and evaluating large vision-language models for remote sensing. The dataset will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>深入了解全球土地覆盖对于地球观测至关重要，为多种应用奠定了基础。尽管遥感技术迅速发展，导致卫星图像激增，但这些图像固有的复杂性往往使非专业用户难以理解。自然语言作为人类知识的载体，可以成为普通用户和复杂的卫星图像之间的桥梁。在此背景下，我们推出了全球规模的高质量遥感图文数据集，为Sentinel-2数据提供自然语言描述，以方便普通用户理解卫星图像。具体来说，我们利用 Sentinel-2 的全球覆盖数据作为基础图像源，并采用欧洲航天局 (ESA) WorldCover 项目的语义分割标签来丰富土地覆盖的描述。通过深入的语义分析，我们制定详细的提示，从 ChatGPT 中引出丰富的描述。为了提高数据集的质量，我们引入了手动验证过程。此步骤涉及手动检查和校正以细化数据集，从而显着提高其准确性和质量。最后，我们向社区提供ChatEarthNet，这是一个大规模的图文数据集，具有全球覆盖、高质量、广泛多样性和详细描述的特点。 ChatEarthNet 由 163,488 个带有 ChatGPT-3.5 生成的标题的图像文本对和另外 10,000 个带有 ChatGPT-4V(ision) 生成的标题的图像文本对组成。该数据集在训练视觉语言基础模型和评估大型遥感视觉语言模型方面具有巨大潜力。该数据集将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.11325v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **ICHPro: Intracerebral Hemorrhage Prognosis Classification Via Joint-attention Fusion-based 3d Cross-modal Network**<br />
**Title_cn:** ICHPro：通过基于联合注意力融合的 3d 跨模态网络对脑出血预后进行分类<br />
**Authors:** Xinlei Yu, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Taosheng Xu, Xiang Wan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Intracerebral Hemorrhage (ICH) is the deadliest subtype of stroke, necessitating timely and accurate prognostic evaluation to reduce mortality and disability. However, the multi-factorial nature and complexity of ICH make methods based solely on computed tomography (CT) image features inadequate. Despite the capacity of cross-modal networks to fuse additional information, the effective combination of different modal features remains a significant challenge. In this study, we propose a joint-attention fusion-based 3D cross-modal network termed ICHPro that simulates the ICH prognosis interpretation process utilized by neurosurgeons. ICHPro includes a joint-attention fusion module to fuse features from CT images with demographic and clinical textual data. To enhance the representation of cross-modal features, we introduce a joint loss function. ICHPro facilitates the extraction of richer cross-modal features, thereby improving classification performance. Upon testing our method using a five-fold cross-validation, we achieved an accuracy of 89.11%, an F1 score of 0.8767, and an AUC value of 0.9429. These results outperform those obtained from other advanced methods based on the test dataset, thereby demonstrating the superior efficacy of ICHPro. The code is available at our Github: https://github.com/YU-deep/ICH.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑出血（ICH）是中风中最致命的亚型，需要及时、准确的预后评估，以降低死亡率和残疾率。然而，ICH 的多因素性质和复杂性使得仅基于计算机断层扫描 (CT) 图像特征的方法不够充分。尽管跨模态网络具有融合附加信息的能力，但不同模态特征的有效组合仍然是一个重大挑战。在这项研究中，我们提出了一种基于联合注意力融合的 3D 跨模态网络，称为 ICHPro，它模拟神经外科医生使用的 ICH 预后解释过程。 ICHPro 包括一个联合注意力融合模块，用于将 CT 图像的特征与人口统计和临床文本数据融合。为了增强跨模态特征的表示，我们引入了联合损失函数。 ICHPro有利于提取更丰富的跨模态特征，从而提高分类性能。使用五重交叉验证测试我们的方法后，我们的准确率达到 89.11%，F1 得分为 0.8767，AUC 值为 0.9429。这些结果优于基于测试数据集的其他先进方法获得的结果，从而证明了 ICHPro 的卓越功效。该代码可在我们的 Github 上获取：https://github.com/YU-deep/ICH。</details>
**PDF:** <http://arxiv.org/pdf/2402.11307v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition**<br />
**Title_cn:** ReViT：通过视觉识别的注意力残留连接增强视觉变压器<br />
**Authors:** Anxhelo Diko, Danilo Avola, Marco Cascio, Luigi Cinque<br />
**Abstract:** <details><summary>原文: </summary>Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.</details>
**Abstract_cn:** <details><summary>译文: </summary>Vision Transformer（ViT）自注意力机制的特点是深层特征崩溃，导致低层视觉特征消失。然而，这些特征有助于准确地表示和识别图像中的元素，并提高基于视觉的识别系统的准确性和鲁棒性。遵循这个基本原理，我们提出了一种新颖的剩余注意力学习方法，用于改进基于 ViT 的架构，增加其视觉特征多样性和模型鲁棒性。通过这种方式，所提出的网络可以捕获并保留重要的低级特征，提供有关正在分析的场景中的元素的更多详细信息。该方法的有效性和鲁棒性在 ImageNet1k、CIFAR10、CIFAR100、Oxford Flowers-102 和 Oxford-IIIT Pet 等五个图像分类基准上进行了评估，取得了改进的性能。此外，在 COCO2017 数据集上的实验表明，所设计的方法在实现到空间感知变压器模型中时，可以发现并合并用于对象检测和实例分割的语义和空间关系。</details>
**PDF:** <http://arxiv.org/pdf/2402.11301v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies**<br />
**Title_cn:** 基于交叉伪标记利用强弱数据增强策略的半监督医学图像分割方法<br />
**Authors:** Yifei Chen, Chenyan Zhang, Yifan Ke, Yiyu Huang, Xuezhou Dai, Feiwei Qin, Yongquan Zhang, Xiaodong Zhang, Changmiao Wang<br />
**Abstract:** <details><summary>原文: </summary>Traditional supervised learning methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model's performance and generalizability through data augmentation processing, employing varied strategies for unlabeled data. Concurrently, the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and advanced models using the publicly accessible Kvasir-SEG dataset. Across all four subdivisions containing different proportions of unlabeled data, our model consistently exhibits superior performance. Our source code is available at https://github.com/JustlfC03/DFCPS.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于收集过程具有挑战性、标记成本高、信噪比低以及生物医学图像特征复杂，传统的监督学习方法在医学图像分割方面历来遇到一定的限制。本文提出了一种半监督模型DFCPS，创新性地融合了Fixmatch的概念。通过数据增强处理，对未标记数据采用不同的策略，显着增强了模型的性能和通用性。同时，模型设计适当强调伪标签的生成、过滤和细化过程。引入了交叉伪监督的新概念，将一致性学习与自我训练相结合。这使得模型能够从多个角度充分利用伪标签，从而增强训练多样性。使用可公开访问的 Kvasir-SEG 数据集将 DFCPS 模型与基线模型和高级模型进行比较。在包含不同比例的未标记数据的所有四个细分中，我们的模型始终表现出卓越的性能。我们的源代码可在 https://github.com/JustlfC03/DFCPS 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11273v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Training-free image style alignment for self-adapting domain shift on handheld ultrasound devices**<br />
**Title_cn:** 无需训练的图像风格对齐，可在手持式超声设备上实现自适应域移动<br />
**Authors:** Hongye Zeng, Ke Zou, Zhihao Chen, Yuchong Gao, Hongbo Chen, Haibin Zhang, Kang Zhou, Meng Wang, Rick Siow Mong Goh, Yong Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Handheld ultrasound devices face usage limitations due to user inexperience and cannot benefit from supervised deep learning without extensive expert annotations. Moreover, the models trained on standard ultrasound device data are constrained by training data distribution and perform poorly when directly applied to handheld device data. In this study, we propose the Training-free Image Style Alignment (TISA) framework to align the style of handheld device data to those of standard devices. The proposed TISA can directly infer handheld device images without extra training and is suited for clinical applications. We show that TISA performs better and more stably in medical detection and segmentation tasks for handheld device data. We further validate TISA as the clinical model for automatic measurements of spinal curvature and carotid intima-media thickness. The automatic measurements agree well with manual measurements made by human experts and the measurement errors remain within clinically acceptable ranges. We demonstrate the potential for TISA to facilitate automatic diagnosis on handheld ultrasound devices and expedite their eventual widespread use.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于用户缺乏经验，手持式超声设备面临使用限制，并且在没有大量专家注释的情况下无法从监督深度学习中受益。此外，在标准超声设备数据上训练的模型受到训练数据分布的限制，并且在直接应用于手持设备数据时表现不佳。在本研究中，我们提出了免训练图像风格对齐（TISA）框架，将手持设备数据的风格与标准设备的风格对齐。所提出的 TISA 可以直接推断手持设备图像，无需额外训练，适合临床应用。我们表明，TISA 在手持设备数据的医疗检测和分割任务中表现更好、更稳定。我们进一步验证 TISA 作为自动测量脊柱曲率和颈动脉内膜中层厚度的临床模型。自动测量与人类专家的手动测量非常吻合，测量误差保持在临床可接受的范围内。我们展示了 TISA 促进手持式超声设备自动诊断并加速其最终广泛使用的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.11211v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation**<br />
**Title_cn:** 一种用于轻量级语义分割的多级特征连续聚合的解码方案<br />
**Authors:** Jiwon Yoo, Jangwon Lee, Gyeonghwan Kim<br />
**Abstract:** <details><summary>原文: </summary>Multi-scale architecture, including hierarchical vision transformer, has been commonly applied to high-resolution semantic segmentation to deal with computational complexity with minimum performance loss. In this paper, we propose a novel decoding scheme for semantic segmentation in this regard, which takes multi-level features from the encoder with multi-scale architecture. The decoding scheme based on a multi-level vision transformer aims to achieve not only reduced computational expense but also higher segmentation accuracy, by introducing successive cross-attention in aggregation of the multi-level features. Furthermore, a way to enhance the multi-level features by the aggregated semantics is proposed. The effort is focused on maintaining the contextual consistency from the perspective of attention allocation and brings improved performance with significantly lower computational cost. Set of experiments on popular datasets demonstrates superiority of the proposed scheme to the state-of-the-art semantic segmentation models in terms of computational cost without loss of accuracy, and extensive ablation studies prove the effectiveness of ideas proposed.</details>
**Abstract_cn:** <details><summary>译文: </summary>包括分层视觉变换器在内的多尺度架构已普遍应用于高分辨率语义分割，以最小化性能损失来处理计算复杂性。在本文中，我们在这方面提出了一种新颖的语义分割解码方案，该方案从具有多尺度架构的编码器中获取多级特征。基于多级视觉变换器的解码方案旨在通过在多级特征的聚合中引入连续的交叉注意力，不仅减少计算开销，而且提高分割精度。此外，提出了一种通过聚合语义增强多级特征的方法。这项工作的重点是从注意力分配的角度保持上下文一致性，并以显着降低的计算成本提高性能。对流行数据集的一组实验证明了所提出的方案在计算成本方面优于最先进的语义分割模型，且不损失准确性，并且广泛的消融研究证明了所提出想法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11201v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Dense Matchers for Dense Tracking**<br />
**Title_cn:** 用于密集跟踪的密集匹配器<br />
**Authors:** Tomáš Jelínek, Jonáš Šerých, Jiří Matas<br />
**Abstract:** <details><summary>原文: </summary>Optical flow is a useful input for various applications, including 3D reconstruction, pose estimation, tracking, and structure-from-motion. Despite its utility, the field of dense long-term tracking, especially over wide baselines, has not been extensively explored. This paper extends the concept of combining multiple optical flows over logarithmically spaced intervals as proposed by MFT. We demonstrate the compatibility of MFT with different optical flow networks, yielding results that surpass their individual performance. Moreover, we present a simple yet effective combination of these networks within the MFT framework. This approach proves to be competitive with more sophisticated, non-causal methods in terms of position prediction accuracy, highlighting the potential of MFT in enhancing long-term tracking applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>光流对于各种应用来说都是有用的输入，包括 3D 重建、姿态估计、跟踪和运动结构。尽管它很实用，但密集长期跟踪领域，尤其是在宽基线上，尚未得到广泛探索。本文扩展了 MFT 提出的在对数间隔上组合多个光流的概念。我们展示了 MFT 与不同光流网络的兼容性，产生的结果超越了它们各自的性能。此外，我们在 MFT 框架内提出了这些网络的简单而有效的组合。事实证明，这种方法在位置预测精度方面与更复杂的非因果方法相比具有竞争力，凸显了 MFT 在增强长期跟踪应用方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.11287v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **CoLLaVO: Crayon Large Language and Vision mOdel**<br />
**Title_cn:** CoLLaVO：Crayon 大语言和视觉模型<br />
**Authors:** Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro<br />
**Abstract:** <details><summary>原文: </summary>The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from 'what objects are in the image?' or 'which object corresponds to a specified bounding box?'. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on Vision Language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon Large Language and Vision mOdel (CoLLaVO), which incorporates instruction tuning with crayon prompt as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in zero-shot numerous VL benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型 (LLM) 和指令调优的巨大成功推动了视觉语言模型 (VLM) 向多功能通用模型的发展。然而，目前的 VLM 是否真正具备由“图像中有哪些对象？”确定的高质量对象级图像理解能力仍有待探索。或“哪个对象对应于指定的边界框？”。我们的研究结果表明，当前 VLM 的图像理解能力与其在视觉语言（VL）任务上的零样本性能密切相关。这表明优先考虑基本图像理解对于 VLM 出色完成 VL 任务至关重要。为了增强对象级图像理解，我们提出了蜡笔大语言和视觉模型（CoLLaVO），它将指令调整与蜡笔提示相结合，作为一种基于全景彩色图的新视觉提示调整方案。此外，我们提出了一种 Dual QLoRA 的学习策略，以保留对象级图像理解，而不会在视觉指令调整过程中忘记它，从而在零样本众多 VL 基准测试中实现显着飞跃。</details>
**PDF:** <http://arxiv.org/pdf/2402.11248v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **FViT: A Focal Vision Transformer with Gabor Filter**<br />
**Title_cn:** FViT：带有 Gabor 滤波器的焦点视觉转换器<br />
**Authors:** Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen<br />
**Abstract:** <details><summary>原文: </summary>Vision transformers have achieved encouraging progress in various computer vision tasks. A common belief is that this is attributed to the competence of self-attention in modeling the global dependencies among feature tokens. Unfortunately, self-attention still faces some challenges in dense prediction tasks, such as the high computational complexity and absence of desirable inductive bias. To address these above issues, we revisit the potential benefits of integrating vision transformer with Gabor filter, and propose a Learnable Gabor Filter (LGF) by using convolution. As an alternative to self-attention, we employ LGF to simulate the response of simple cells in the biological visual system to input images, prompting models to focus on discriminative feature representations of targets from various scales and orientations. Additionally, we designed a Bionic Focal Vision (BFV) block based on the LGF. This block draws inspiration from neuroscience and introduces a Multi-Path Feed Forward Network (MPFFN) to emulate the working way of biological visual cortex processing information in parallel. Furthermore, we develop a unified and efficient pyramid backbone network family called Focal Vision Transformers (FViTs) by stacking BFV blocks. Experimental results show that FViTs exhibit highly competitive performance in various vision tasks. Especially in terms of computational efficiency and scalability, FViTs show significantly advantages compared with other counterparts.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉转换器在各种计算机视觉任务中取得了令人鼓舞的进展。人们普遍认为，这归因于自注意力在对特征标记之间的全局依赖关系进行建模时的能力。不幸的是，自注意力在密集预测任务中仍然面临一些挑战，例如计算复杂度高和缺乏理想的归纳偏差。为了解决上述问题，我们重新审视了将视觉变换器与 Gabor 滤波器集成的潜在好处，并提出了一种使用卷积的可学习 Gabor 滤波器（LGF）。作为自注意力的替代方案，我们采用 LGF 来模拟生物视觉系统中简单细胞对输入图像的响应，促使模型专注于不同尺度和方向的目标的区分性特征表示。此外，我们还基于 LGF 设计了仿生焦点视觉 (BFV) 模块。该模块从神经科学中汲取灵感，引入了多路径前馈网络（MPFFN）来模拟生物视觉皮层并行处理信息的工作方式。此外，我们通过堆叠 BFV 块开发了一个统一且高效的金字塔骨干网络系列，称为焦点视觉变换器（FViTs）。实验结果表明，FViT 在各种视觉任务中表现出极具竞争力的性能。特别是在计算效率和可扩展性方面，FViT 与其他同类产品相比显示出显着的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.11303v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search**<br />
**Title_cn:** 基于图的近似最近邻搜索的概率路由<br />
**Authors:** Kejing Lu, Chuan Xiao, Yoshiharu Ishikawa<br />
**Abstract:** <details><summary>原文: </summary>Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate that equipping PEOs can increase throughput on a commonly utilized graph index (HNSW) by a factor of 1.6 to 2.5, and its efficiency consistently outperforms the leading-edge routing technique by 1.1 to 1.4 times.</details>
**Abstract_cn:** <details><summary>译文: </summary>高维空间中的近似最近邻搜索（ANNS）是机器学习领域的关键挑战。近年来，基于图的方法已成为 ANNS 的优越方法，建立了新的技术水平。尽管已经引入了基于图的 ANNS 的各种优化，但它们主要依赖于缺乏正式理论支持的启发式方法。本文旨在通过引入一种在探索图中节点的邻居时提供概率保证的方法来增强基于图的 ANNS 中的路由。我们将问题表述为概率路由，并通过结合局部敏感技术开发两种基线策略。随后，我们引入了 PEO，这是一种新颖的方法，可以有效地识别图中的哪些邻居应考虑进行精确距离计算，从而显着提高实践中的效率。我们的实验表明，配备 PEO 可以将常用图索引 (HNSW) 上的吞吐量提高 1.6 至 2.5 倍，并且其效率始终优于领先的路由技术 1.1 至 1.4 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.11354v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Learning by Reconstruction Produces Uninformative Features For Perception**<br />
**Title_cn:** 通过重构学习会产生无信息的感知特征<br />
**Authors:** Randall Balestriero, Yann LeCun<br />
**Abstract:** <details><summary>原文: </summary>Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.</details>
**Abstract_cn:** <details><summary>译文: </summary>输入空间重建是一种有吸引力的表示学习范式。尽管重建和生成具有可解释性，但我们发现重建学习和感知学习之间存在不一致。我们表明，前者将模型的容量分配给解释观察到的方差的数据子空间——后者的子空间具有无信息特征。例如，监督的 TinyImagenet 任务将图像投影到顶部子空间，解释了 90% 的像素方差，可以用 45% 的测试精度来解决。使用底部子空间代替，仅占像素方差的 20\%，达到 55\% 的测试精度。最后学习的感知特征解释了需要较长的训练时间，例如使用掩码自动编码器。通过去噪学习是缓解这种失调的一种流行策略。我们证明，虽然一些噪声策略（例如掩蔽）确实是有益的，但其他策略（例如加性高斯噪声）则不然。然而，即使在遮罩的情况下，我们发现其好处也会随着遮罩的形状、比率和所考虑的数据集的函数而变化。虽然在不了解感知任务的情况下调整噪声策略似乎具有挑战性，但我们提供了有关如何检测噪声策略是否永远不会有益（无论感知任务如何）的第一个线索。</details>
**PDF:** <http://arxiv.org/pdf/2402.11337v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Enhancing Surgical Performance in Cardiothoracic Surgery with Innovations from Computer Vision and Artificial Intelligence: A Narrative Review**<br />
**Title_cn:** 利用计算机视觉和人工智能的创新提高心胸外科的手术表现：叙述性回顾<br />
**Authors:** Merryn D. Constable, Hubert P. H. Shum, Stephen Clark<br />
**Abstract:** <details><summary>原文: </summary>When technical requirements are high, and patient outcomes are critical, opportunities for monitoring and improving surgical skills via objective motion analysis feedback may be particularly beneficial. This narrative review synthesises work on technical and non-technical surgical skills, collaborative task performance, and pose estimation to illustrate new opportunities to advance cardiothoracic surgical performance with innovations from computer vision and artificial intelligence. These technological innovations are critically evaluated in terms of the benefits they could offer the cardiothoracic surgical community, and any barriers to the uptake of the technology are elaborated upon. Like some other specialities, cardiothoracic surgery has relatively few opportunities to benefit from tools with data capture technology embedded within them (as with robotic-assisted laparoscopic surgery, for example). In such cases, pose estimation techniques that allow for movement tracking across a conventional operating field without using specialist equipment or markers offer considerable potential. With video data from either simulated or real surgical procedures, these tools can (1) provide insight into the development of expertise and surgical performance over a surgeon's career, (2) provide feedback to trainee surgeons regarding areas for improvement, (3) provide the opportunity to investigate what aspects of skill may be linked to patient outcomes which can (4) inform the aspects of surgical skill which should be focused on within training or mentoring programmes. Classifier or assessment algorithms that use artificial intelligence to 'learn' what expertise is from expert surgical evaluators could further assist educators in determining if trainees meet competency thresholds.</details>
**Abstract_cn:** <details><summary>译文: </summary>当技术要求很高且患者结果至关重要时，通过客观运动分析反馈来监控和提高手术技能的机会可能特别有益。这篇叙述性综述综合了技术和非技术手术技能、协作任务表现和姿势估计方面的工作，以说明通过计算机视觉和人工智能的创新来提高心胸外科表现的新机会。这些技术创新可以为心胸外科界带来的好处进行严格评估，并详细阐述了采用该技术的任何障碍。与其他一些专业一样，心胸外科从嵌入数据捕获技术的工具中受益的机会相对较少（例如机器人辅助腹腔镜手术）。在这种情况下，无需使用专业设备或标记即可在传统操作区域进行运动跟踪的姿态估计技术具有相当大的潜力。通过来自模拟或真实手术过程的视频数据，这些工具可以 (1) 深入了解外科医生职业生涯中的专业知识和手术表现的发展，(2) 向见习外科医生提供有关需要改进的领域的反馈，(3) 提供有机会调查技能的哪些方面可能与患者的结果相关，这可以 (4) 告知应在培训或指导计划中重点关注的手术技能方面。使用人工智能来“学习”专家外科评估员的专业知识的分类器或评估算法可以进一步帮助教育工作者确定受训者是否达到能力阈值。</details>
**PDF:** <http://arxiv.org/pdf/2402.11288v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions**<br />
**Title_cn:** 超越文字描述：理解和定位符合人类意图的开放世界物体<br />
**Authors:** Wenxuan Wang, Yisi Zhang, Xingjian He, Yichen Yan, Zijia Zhao, Xinlong Wang, Jing Liu<br />
**Abstract:** <details><summary>原文: </summary>Visual grounding (VG) aims at locating the foreground entities that match the given natural language expression. Previous datasets and methods for classic VG task mainly rely on the prior assumption that the given expression must literally refer to the target object, which greatly impedes the practical deployment of agents in real-world scenarios. Since users usually prefer to provide the intention-based expressions for the desired object instead of covering all the details, it is necessary for the agents to interpret the intention-driven instructions. Thus, in this work, we take a step further to the intention-driven visual-language (V-L) understanding. To promote classic VG towards human intention interpretation, we propose a new intention-driven visual grounding (IVG) task and build a largest-scale IVG dataset named IntentionVG with free-form intention expressions. Considering that practical agents need to move and find specific targets among various scenarios to realize the grounding task, our IVG task and IntentionVG dataset have taken the crucial properties of both multi-scenario perception and egocentric view into consideration. Besides, various types of models are set up as the baselines to realize our IVG task. Extensive experiments on our IntentionVG dataset and baselines demonstrate the necessity and efficacy of our method for the V-L field. To foster future research in this direction, our newly built dataset and baselines will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉基础（VG）旨在定位与给定自然语言表达相匹配的前景实体。以前的经典 VG 任务的数据集和方法主要依赖于给定表达式必须字面上引用目标对象的先验假设，这极大地阻碍了智能体在现实场景中的实际部署。由于用户通常更喜欢提供所需对象的基于意图的表达，而不是涵盖所有细节，因此代理有必要解释意图驱动的指令。因此，在这项工作中，我们在意图驱动的视觉语言（V-L）理解方面更进一步。为了促进经典 VG 向人类意图解释的方向发展，我们提出了一种新的意图驱动视觉基础（IVG）任务，并构建了一个最大规模的 IVG 数据集，名为 IntentionVG，具有自由形式的意图表达。考虑到实际代理需要在各种场景中移动并寻找特定目标来实现接地任务，我们的 IVG 任务和 IntentionVG 数据集考虑了多场景感知和自我中心视图的关键属性。此外，还建立了各种类型的模型作为实现我们的 IVG 任务的基线。对我们的 IntentionVG 数据集和基线进行的大量实验证明了我们的方法在 V-L 领域的必要性和有效性。为了促进这一方向的未来研究，我们新构建的数据集和基线将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.11265v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning**<br />
**Title_cn:** 坚持不懈：寻求统一解决方案以减少深度学习中的捷径<br />
**Authors:** Hadi M. Dolatabadi, Sarah M. Erfani, Christopher Leckie<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bias in decision-making as our test studies. Our analysis of these two failure cases of DNNs reveals that finding a unified solution for shortcut learning in DNNs is not out of reach, and TDA can play a significant role in forming such a framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络 (DNN) 很容易受到捷径学习的影响：它们不是学习预期的任务，而是倾向于在输入和输出之间绘制不确定的关系。捷径学习在神经网络的许多失败案例中普遍存在，这种现象的踪迹可以在它们的普遍性问题、领域转移、对抗性脆弱性，甚至对多数群体的偏见中看到。在本文中，我们认为各种 DNN 问题的原因的共性创造了一个重​​要的机会，应该利用这个机会来寻找快捷学习的统一解决方案。为此，我们概述了拓扑数据分析（TDA），特别是持久同源性（PH）的最新进展，以绘制一个用于检测深度学习捷径的统一路线图。我们使用两个不可学习的例子和决策偏差的案例作为我们的测试研究，通过研究 DNN 中计算图的拓扑特征来证明我们的论点。我们对这两个 DNN 失败案例的分析表明，在 DNN 中找到统一的快捷学习解决方案并非遥不可及，而 TDA 在形成这样一个框架方面可以发挥重要作用。</details>
**PDF:** <http://arxiv.org/pdf/2402.11237v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining**<br />
**Title_cn:** 通过反事实文本引导对比语言-图像预训练了解新闻缩略图的代表性<br />
**Authors:** Yejun Yoon, Seunghyun Yoon, Kunwoo Park<br />
**Abstract:** <details><summary>原文: </summary>This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross-modal matching ability in the target task. Evaluation experiments using NewsTT show that CFT-CLIP outperforms the pretrained models, such as CLIP and BLIP-2. Our code and data will be made accessible to the public after the paper is accepted.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文深入探讨了理解新闻缩略图的代表性的关键挑战，当文章在社交媒体上传播时，新闻缩略图通常是读者的第一个视觉参与。我们关注新闻图像是否代表新闻文本中讨论的主要主题。为了应对挑战，我们引入了 \textsc{NewsTT}，这是一个手动注释的新闻缩略图和文本对的数据集。我们发现预训练的视觉和语言模型（例如 CLIP 和 BLIP-2）难以完成这项任务。由于新闻主题经常涉及命名实体或专有名词，因此预训练模型无法匹配其视觉和文本外观。为了填补这一空白，我们提出了 CFT-CLIP，一种反事实文本引导的对比语言图像预训练框架。我们假设学习将新闻文本与其反事实（其中命名实体被替换）进行对比，可以增强目标任务中的跨模态匹配能力。使用 NewsTT 进行的评估实验表明，CFT-CLIP 优于预训练模型，例如 CLIP 和 BLIP-2。论文被接受后，我们的代码和数据将向公众开放。</details>
**PDF:** <http://arxiv.org/pdf/2402.11159v1><br />
**Code:** null<br />

