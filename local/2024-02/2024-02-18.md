## [UPDATED!] **2024-02-18** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **SDiT: Spiking Diffusion Model with Transformer**<br />
**Title_cn:** SDiT：带变压器的尖峰扩散模型<br />
**Authors:** Shu Yang, Hanzhi Ma, Chengting Yu, Aili Wang, Er-Ping Li<br />
**Abstract:** <details><summary>原文: </summary>Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>尖峰神经网络（SNN）具有低功耗和生物可解释的特性，被认为在节能计算方面具有巨大潜力。然而，SNN 在图像生成任务上的探索仍然非常有限，并且尚未提出基于 SNN 的生成模型的统一有效的结构。在本文中，我们探索了尖峰神经网络中的一种新颖的扩散模型架构。我们利用Transformer来代替主流扩散模型中常用的U-net结构。它可以以相对较低的计算成本和较短的采样时间生成更高质量的图像。它旨在为基于 SNN 的生成模型的研究提供经验基线。在 MNIST、Fashion-MNIST 和 CIFAR-10 数据集上的实验表明，与现有的 SNN 生成模型相比，我们的工作具有很强的竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2402.11588v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GenAD: Generative End-to-End Autonomous Driving**<br />
**Title_cn:** GenAD：生成式端到端自动驾驶<br />
**Authors:** Wenzhao Zheng, Ruiqi Song, Xianda Guo, Long Chen<br />
**Abstract:** <details><summary>原文: </summary>Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>直接从原始传感器生成规划结果是自动驾驶长期以来渴望的解决方案，并且最近引起了越来越多的关注。大多数现有的端到端自动驾驶方法都将这个问题分解为感知、运动预测和规划。然而，我们认为传统的渐进式管道仍然无法全面地模拟整个交通演化过程，例如，自我汽车与其他交通参与者之间的未来交互以及先验的结构轨迹。在本文中，我们探索了一种端到端自动驾驶的新范式，其中关键是根据过去的场景预测自我汽车和周围环境如何演变。我们提出了 GenAD，一个将自动驾驶转化为生成建模问题的生成框架。我们提出了一个以实例为中心的场景标记器，它首先将周围的场景转换为地图感知的实例标记。然后，我们采用变分自动编码器来学习结构潜在空间中的未来轨迹分布，以进行轨迹先验建模。我们进一步采用时间模型来捕获潜在空间中的主体和自我运动，以生成更有效的未来轨迹。 GenAD 最终通过在以实例标记为条件的学习的结构潜在空间中采样分布并使用学习的时间模型生成未来来同时执行运动预测和规划。对广泛使用的 nuScenes 基准进行的大量实验表明，所提出的 GenAD 在以视觉为中心的端到端自动驾驶方面实现了最先进的性能，并且效率很高。</details>
**PDF:** <http://arxiv.org/pdf/2402.11502v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **IRFundusSet: An Integrated Retinal Rundus Dataset with a Harmonized Healthy Label**<br />
**Title_cn:** IRFundusSet：具有统一健康标签的综合视网膜 Rundus 数据集<br />
**Authors:** P. Bilha Githinji, Keming Zhao, Jiantao Wang, Peiwu Qin<br />
**Abstract:** <details><summary>原文: </summary>Ocular conditions are a global concern and computational tools utilizing retinal fundus color photographs can aid in routine screening and management. Obtaining comprehensive and sufficiently sized datasets, however, is non-trivial for the intricate retinal fundus, which exhibits heterogeneities within pathologies, in addition to variations from demographics and acquisition. Moreover, retinal fundus datasets in the public space suffer fragmentation in the organization of data and definition of a healthy observation. We present Integrated Retinal Fundus Set (IRFundusSet), a dataset that consolidates, harmonizes and curates several public datasets, facilitating their consumption as a unified whole and with a consistent is_normal label. IRFundusSet comprises a Python package that automates harmonization and avails a dataset object in line with the PyTorch approach. Moreover, images are physically reviewed and a new is_normal label is annotated for a consistent definition of a healthy observation. Ten public datasets are initially considered with a total of 46064 images, of which 25406 are curated for a new is_normal label and 3515 are deemed healthy across the sources.</details>
**Abstract_cn:** <details><summary>译文: </summary>眼部疾病是一个全球性的问题，利用视网膜眼底彩色照片的计算工具可以帮助日常筛查和管理。然而，对于复杂的视网膜眼底来说，获得全面且足够大小的数据集并非易事，除了人口统计和采集的差异之外，视网膜眼底还表现出病理学的异质性。此外，公共空间中的视网膜眼底数据集在数据组织和健康观察的定义方面存在碎片化问题。我们提出了综合视网膜眼底集（IRFundusSet），这是一个整合、协调和整理多个公共数据集的数据集，促进它们作为一个统一的整体并具有一致的 is_normal 标签来使用。 IRFundusSet 包含一个 Python 包，可自动协调并利用符合 PyTorch 方法的数据集对象。此外，对图像进行物理审查，并注释新的 is_normal 标签，以实现健康观察的一致定义。最初考虑了 10 个公共数据集，总共 46064 张图像，其中 25406 张是为新的 is_normal 标签精心策划的，3515 张被认为是健康的。</details>
**PDF:** <http://arxiv.org/pdf/2402.11488v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Visual Concept-driven Image Generation with Text-to-Image Diffusion Model**<br />
**Title_cn:** 使用文本到图像扩散模型的视觉概念驱动的图像生成<br />
**Authors:** Tanzila Rahman, Shweta Mahajan, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Leonid Sigal<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent Dense CRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a bi-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively (through user studies) with a number of examples and use cases that can combine up to three entangled concepts.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像（TTI）扩散模型在生成复杂且富有想象力的场景的高分辨率图像方面表现出了令人印象深刻的结果。最近的方法通过个性化技术进一步扩展了这些方法，使它们能够使用一些示例图像插图来集成用户图示的概念（例如，用户他/她自己）。然而，生成具有多个交互概念（例如人类主体）以及可能纠缠在一个或多个图像插图中的概念的图像的能力仍然是一种幻想。在这项工作中，我们提出了一个概念驱动的 TTI 个性化框架来解决这些核心挑战。我们以现有作品为基础，学习用户说明的概念的自定义标记，从而允许这些标记与 TTI 模型中的现有文本标记进行交互。然而，重要的是，为了理清并更好地学习相关概念，我们共同学习（潜在）分割掩模，以在用户提供的图像插图中理清这些概念。我们通过引入类似期望最大化（EM）的优化过程来实现这一点，在该过程中，我们交替学习自定义标记和估计包含用户提供的图像中相应概念的掩模。我们根据 U-Net 参数化潜在扩散模型和随后的密集 CRF 优化中的交叉注意力获得这些掩模。我们说明，这种联合交替细化可以导致学习更好的概念标记，以及作为副产品的潜在掩模。我们通过许多示例和用例（通过用户研究）定性和定量地说明了所提出的方法的好处，这些示例和用例可以组合最多三个相互纠缠的概念。</details>
**PDF:** <http://arxiv.org/pdf/2402.11487v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition**<br />
**Title_cn:** 用于广义零样本识别的数据分布蒸馏生成模型<br />
**Authors:** Yijie Wang, Mingjian Hong, Luwen Huangfu, Sheng Huang<br />
**Abstract:** <details><summary>原文: </summary>In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D$^3$GZSL elevates the performance of existing generative GZSL methods, underscoring its potential to refine zero-shot learning practices.The code is available at: https://github.com/PJBQ/D3GZSL.git</details>
**Abstract_cn:** <details><summary>译文: </summary>在零样本学习（ZSL）领域，我们解决了广义零样本学习（GZSL）模型中偏向于可见数据的偏差。为了解决这个问题，我们引入了一个名为 D$^3$GZSL 的端到端生成 GZSL 框架。该框架将可见数据和综合未见数据分别视为分布内和分布外数据，以获得更平衡的模型。 D$^3$GZSL 包含两个核心模块：分布内双空间蒸馏（ID$^2$SD）和分布外间歇蒸馏（O$^2$DBD）。 ID$^2$SD 在嵌入和标签空间中调整师生成果，增强学习连贯性。 O$^2$DBD 为每个批次样本引入了低维分布外表示，捕获可见类别和不可见类别之间的共享结构。我们的方法证明了其在已建立的 GZSL 基准中的有效性，无缝集成到主流生成框架中。大量实验一致表明，D$^3$GZSL 提高了现有生成 GZSL 方法的性能，强调了其改进零样本学习实践的潜力。代码位于：https://github.com/PJBQ/D3GZSL.git</details>
**PDF:** <http://arxiv.org/pdf/2402.11424v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection**<br />
**Title_cn:** MultiCorrupt：用于 3D 物体检测的多模态鲁棒性数据集和 LiDAR-相机融合的基准<br />
**Authors:** Till Beemelmanns, Quan Zhang, Lutz Eckstein<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which multi-modal design choices make such models robust against certain perturbations. The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于自动驾驶的多模态 3D 物体检测模型在 nuScenes 等计算机视觉基准测试中表现出了卓越的性能。然而，它们对密集采样的激光雷达点云和精心校准的传感器阵列的依赖给现实世界的应用带来了挑战。传感器错位、校准错误和采样频率不同等问题会导致激光雷达和摄像头数据的空间和时间错位。此外，激光雷达和摄像头数据的完整性常常会受到恶劣环境条件（例如恶劣天气）的影响，导致遮挡和噪声干扰。为了应对这一挑战，我们引入了 MultiCorrupt，这是一个综合基准测试，旨在评估多模式 3D 对象检测器针对十种不同类型损坏的鲁棒性。我们在 MultiCorrupt 上评估了五种最先进的多模态检测器，并分析了它们的抵抗能力。我们的结果表明，现有方法根据腐败类型及其融合策略表现出不同程度的稳健性。我们提供了关于哪些多模态设计选择使此类模型对某些扰动具有鲁棒性的见解。数据集生成代码和基准测试在 https://github.com/ika-rwth-aachen/MultiCorrupt 上开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.11677v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Efficient Multimodal Learning from Data-centric Perspective**<br />
**Title_cn:** 以数据为中心的高效多模态学习<br />
**Authors:** Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably causes significant performance drop. In this paper, we demonstrate the possibility to beat the scaling law and train a smaller but better MLLM by exploring more informative training data. Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from condensed training data. Remarkably, our Bunny-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-v1.5-13B, on multiple benchmarks. The code, models and data can be found in https://github.com/BAAI-DCAI/Bunny.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在一般视觉理解和推理任务中表现出了显着的能力。然而，它们的部署受到训练和推理中大量计算成本的阻碍，限制了更广泛的研究和用户社区的可访问性。一个简单的解决方案是利用较小的预训练视觉和语言模型，这不可避免地会导致性能显着下降。在本文中，我们证明了通过探索信息更丰富的训练数据来克服缩放定律并训练更小但更好的 MLLM 的可能性。具体来说，我们介绍了 Bunny，这是一个轻量级 MLLM 系列，具有灵活的视觉和语言骨干，可从压缩的训练数据中进行高效的多模态学习。值得注意的是，我们的 Bunny-3B 在多个基准测试中均优于最先进的大型 MLLM，尤其是 LLaVA-v1.5-13B。代码、模型和数据可以在https://github.com/BAAI-DCAI/Bunny找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.11530v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation**<br />
**Title_cn:** MAL：具有时间和蒸馏提示的运动感知损失，用于自监督深度估计<br />
**Authors:** Yup-Jiang Dong, Fang-Lue Zhang, Song-Hai Zhang<br />
**Abstract:** <details><summary>原文: </summary>Depth perception is crucial for a wide range of robotic applications. Multi-frame self-supervised depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the self-supervised methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel distillation scheme between the teacher and student networks in the multi-frame self-supervised depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original distillation scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame self-supervised monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度感知对于广泛的机器人应用至关重要。多帧自监督深度估计方法由于能够利用大规模、未标记的现实世界数据而引起了研究兴趣。然而，自监督方法通常依赖于静态场景的假设，并且它们的性能在动态环境中往往会下降。为了解决这个问题，我们提出了运动感知损失，它利用连续输入帧之间的时间关系以及多帧自监督深度估计方法中教师和学生网络之间的新颖蒸馏方案。具体来说，我们将运动物体的空间位置与输入帧的时间顺序相关联，以消除物体运动引起的误差。同时，我们增强了多框架方法中的原始蒸馏方案，以更好地利用教师网络的知识。 MAL 是一种新颖的即插即用模块，旨在无缝集成到多帧自监督单目深度估计方法中。将 MAL 添加到之前最先进的方法中，在 KITTI 和 CityScapes 基准上，深度估计误差分别减少了 4.2% 和 10.8%。</details>
**PDF:** <http://arxiv.org/pdf/2402.11507v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection**<br />
**Title_cn:** LiRaFusion：用于 3D 物体检测的深度自适应 LiDAR-雷达融合<br />
**Authors:** Jingyu Song, Lingjun Zhao, Katherine A. Skinner<br />
**Abstract:** <details><summary>原文: </summary>We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出 LiRaFusion 来解决用于 3D 物体检测的 LiDAR-雷达融合问题，以填补现有 LiDAR-雷达探测器的性能差距。为了提高这两种模式的特征提取能力，我们设计了一个用于联合体素特征编码的早期融合模块，以及一个通过门控网络自适应融合特征图的中间融合模块。我们对 nuScenes 进行了广泛的评估，以证明 LiRaFusion 有效地利用了 LiDAR 和雷达的互补信息，并比现有方法取得了显着的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.11735v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry**<br />
**Title_cn:** 挑战黑匣子：CNN农林应用归因图综合评价<br />
**Authors:** Lars Nieradzik, Henrike Stephani, Jördis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Janis Keuper<br />
**Abstract:** <details><summary>原文: </summary>In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a better understanding of neural networks in these application areas.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们探讨了神经网络在农业和林业中的可解释性，特别是在肥料处理分类和木材识别方面。这些模型的不透明性质（通常被认为是“黑匣子”）是通过对最先进的归因图（AM）（也称为类激活图（CAM）或显着图）进行广泛评估来解决的。我们对这些 AM 进行全面的定性和定量分析，揭示了关键的实际局限性。调查结果显示，AM 经常无法始终如一地突出关键功能，并且经常与领域专家认为重要的功能不一致。这些差异对 AM 在理解神经网络决策过程中的效用提出了重大问题。我们的研究为农业和林业领域 AM 的可信度和实用性提供了重要的见解，从而有助于更好地理解这些应用领域的神经网络。</details>
**PDF:** <http://arxiv.org/pdf/2402.11670v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models**<br />
**Title_cn:** 逻辑闭环：揭示大型视觉语言模型中的物体幻觉<br />
**Authors:** Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan<br />
**Abstract:** <details><summary>原文: </summary>Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.</details>
**Abstract_cn:** <details><summary>译文: </summary>物体幻觉一直是阻碍大型视觉语言模型（LVLM）更广泛应用的致命弱点。物体幻觉是指 LVLM 声称图像中不存在物体的现象。为了减轻物体幻觉，人们提出了指令调整和基于外部模型的检测方法，这些方法要么需要大量计算资源，要么依赖于外部模型的检测结果。然而，利用 LVLM 本身来缓解物体幻觉仍然是一个尚未开发的领域。在这项工作中，我们采用这样的直觉：LVLM 倾向于对存在的对象做出逻辑一致的响应，但对幻觉的对象做出不一致的响应。因此，我们提出了一种基于逻辑闭环的物体幻觉检测和缓解框架，即LogicCheckGPT。具体来说，我们设计逻辑一致性探测来提出具有逻辑相关性的问题，查询对象的属性，反之亦然。他们的反应能否形成逻辑闭环可以作为对象幻觉的指标。作为一种即插即用的方法，它可以无缝地应用于所有现有的 LVLM。在四个 LVLM 的三个基准上进行的综合实验证明了我们的方法带来的显着改进，表明了其有效性和通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11622v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM**<br />
**Title_cn:** PolypNextLSTM：使用 ConvNext 和 ConvLSTM 的轻量级快速息肉视频分割网络<br />
**Authors:** Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack, Sarah Grube, Alexander Schlaefer<br />
**Abstract:** <details><summary>原文: </summary>Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>单图像 UNet 架构通常用于息肉分割，但缺乏临床医生在诊断息肉时从视频数据中获得的时间洞察力。为了更忠实地反映临床实践，我们提出的解决方案 PolypNextLSTM 利用基于视频的深度学习，利用时间信息以最少的参数开销实现卓越的分割性能，使其适合边缘设备。 PolypNextLSTM 采用类似 UNet 的结构，以 ConvNext-Tiny 作为主干，策略性地省略最后两层以减少参数开销。我们的时间融合模块，即卷积长短期记忆（ConvLSTM），可以有效地利用时间特征。我们的主要新颖之处在于 PolypNextLSTM，它以最精简的参数和最快的模型而脱颖而出，超越了五种最先进的基于图像和视频的深度学习模型的性能。 SUN-SEG 数据集的评估涵盖了易于检测和难以检测的息肉场景，以及包含快速运动和遮挡等具有挑战性的伪像的视频。</details>
**PDF:** <http://arxiv.org/pdf/2402.11585v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media**<br />
**Title_cn:** 用于多尺寸图像分类的新型傅立叶神经算子框架：在 3D 数字多孔介质中的应用<br />
**Authors:** Ali Kashefi, Tapan Mukerji<br />
**Abstract:** <details><summary>原文: </summary>Fourier neural operators (FNOs) are invariant with respect to the size of input images, and thus images with any size can be fed into FNO-based frameworks without any modification of network architectures, in contrast to traditional convolutional neural networks (CNNs). Leveraging the advantage of FNOs, we propose a novel deep-learning framework for classifying images with varying sizes. Particularly, we simultaneously train the proposed network on multi-sized images. As a practical application, we consider the problem of predicting the label (e.g., permeability) of three-dimensional digital porous media. To construct the framework, an intuitive approach is to connect FNO layers to a classifier using adaptive max pooling. First, we show that this approach is only effective for porous media with fixed sizes, whereas it fails for porous media of varying sizes. To overcome this limitation, we introduce our approach: instead of using adaptive max pooling, we use static max pooling with the size of channel width of FNO layers. Since the channel width of the FNO layers is independent of input image size, the introduced framework can handle multi-sized images during training. We show the effectiveness of the introduced framework and compare its performance with the intuitive approach through the example of the classification of three-dimensional digital porous media of varying sizes.</details>
**Abstract_cn:** <details><summary>译文: </summary>傅里叶神经算子 (FNO) 相对于输入图像的大小是不变的，因此与传统的卷积神经网络 (CNN) 相比，任何大小的图像都可以输入基于 FNO 的框架，而无需对网络架构进行任何修改。利用 FNO 的优势，我们提出了一种新颖的深度学习框架，用于对不同大小的图像进行分类。特别是，我们同时在多尺寸图像上训练所提出的网络。作为实际应用，我们考虑预测三维数字多孔介质的标签（例如渗透性）的问题。为了构建该框架，一种直观的方法是使用自适应最大池将 FNO 层连接到分类器。首先，我们证明这种方法仅对固定尺寸的多孔介质有效，而对于不同尺寸的多孔介质则失败。为了克服这个限制，我们引入了我们的方法：我们不使用自适应最大池，而是使用具有 FNO 层通道宽度大小的静态最大池。由于 FNO 层的通道宽度与输入图像大小无关，因此引入的框架可以在训练期间处理多尺寸图像。我们通过不同尺寸的三维数字多孔介质的分类示例，展示了所引入框架的有效性，并将其性能与直观方法进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.11568v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **CPN: Complementary Proposal Network for Unconstrained Text Detection**<br />
**Title_cn:** CPN：用于无约束文本检测的补充提案网络<br />
**Authors:** Longhuang Wu, Shangxuan Tian, Youxin Wang, Pengfei Xiong<br />
**Abstract:** <details><summary>原文: </summary>Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的场景文本检测方法可以分为两种范式：基于分割和基于锚点。虽然基于分段的方法非常适合不规则形状，但它们难以处理紧凑或重叠的布局。相反，基于锚点的方法适用于复杂的布局，但会受到不规则形状的影响。为了加强它们的优点并克服各自的缺点，我们提出了一种互补提案网络（CPN），它无缝并行地集成语义和几何信息以实现卓越的性能。 CPN 包含两个用于提案生成的高效网络：可变形形态语义网络（使用创新的可变形形态算子生成语义提案）和平衡区域提案网络（使用预定义锚点生成几何提案）。为了进一步增强互补性，我们引入了交错特征注意模块，该模块使语义和几何特征能够在提案生成之前进行深入交互。通过利用互补的提案和功能，CPN 的性能优于最先进的方法，在可比较的计算成本下具有显着的优势。具体来说，我们的方法在具有挑战性的基准 ICDAR19-ArT、IC15 和 MSRA-TD500 上分别实现了 3.6%、1.3% 和 1.0% 的改进。我们的方法的代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.11540v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading**<br />
**Title_cn:** 大词汇量阿拉伯语唇读的视觉和几何特征的交叉注意融合<br />
**Authors:** Samar Daou, Ahmed Rekik, Achraf Ben-Hamadou, Abdelaziz Kallel<br />
**Abstract:** <details><summary>原文: </summary>Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area. It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio speech recognition. Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours. However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector. To address this challenge, firstly, we propose a cross-attention fusion-based approach for large lexicon Arabic vocabulary to predict spoken words in videos. Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region. Secondly, we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR) dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers. The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words. Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field. Link to the project page: https://crns-smartvision.github.io/lrwar</details>
**Abstract_cn:** <details><summary>译文: </summary>唇读涉及使用视觉数据通过分析嘴唇和周围区域的运动来识别口语单词。它是一个热门研究课题，具有许多潜在的应用，例如人机交互和增强音频语音识别。最近基于深度学习的工作旨在将从嘴部区域提取的视觉特征与嘴唇轮廓上的标志点相结合。然而，采用简单的组合方法（例如级联）可能不是获得最佳特征向量的最有效方法。为了应对这一挑战，首先，我们针对大型阿拉伯语词汇提出了一种基于交叉注意融合的方法来预测视频中的口语单词。我们的方法利用交叉注意网络的力量来有效地整合在嘴部区域计算的视觉和几何特征。其次，我们介绍了第一个大规模阿拉伯语野外唇读 (LRW-AR) 数据集，其中包含 20,000 个 100 个单词类别的视频，由 36 位说话者说出。在LRW-AR和ArabicVisual数据库上获得的实验结果表明了所提出的方法在识别阿拉伯语单词方面的有效性和鲁棒性。我们的工作提供了将唇读技术应用于阿拉伯语的可行性和有效性的见解，为该领域的进一步研究打开了大门。项目页面链接：https://crns-smartvision.github.io/lrwar</details>
**PDF:** <http://arxiv.org/pdf/2402.11520v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Underestimation of lung regions on chest X-ray segmentation masks assessed by comparison with total lung volume evaluated on computed tomography**<br />
**Title_cn:** 通过与计算机断层扫描评估的总肺体积进行比较来评估胸部 X 射线分割掩模上的肺部区域低估<br />
**Authors:** Przemysław Bombiński, Patryk Szatkowski, Bartłomiej Sobieski, Tymoteusz Kwieciński, Szymon Płotka, Mariusz Adamek, Marcin Banasiuk, Mariusz I. Furmanek, Przemysław Biecek<br />
**Abstract:** <details><summary>原文: </summary>Lung mask creation lacks well-defined criteria and standardized guidelines, leading to a high degree of subjectivity between annotators. In this study, we assess the underestimation of lung regions on chest X-ray segmentation masks created according to the current state-of-the-art method, by comparison with total lung volume evaluated on computed tomography (CT). We show, that lung X-ray masks created by following the contours of the heart, mediastinum, and diaphragm significantly underestimate lung regions and exclude substantial portions of the lungs from further assessment, which may result in numerous clinical errors.</details>
**Abstract_cn:** <details><summary>译文: </summary>肺掩模的创建缺乏明确的标准和标准化指南，导致注释者之间存在高度主观性。在这项研究中，我们通过与计算机断层扫描 (CT) 评估的总肺体积进行比较，评估了根据当前最先进的方法创建的胸部 X 射线分割掩模对肺部区域的低估。我们表明，通过遵循心脏、纵隔和膈肌的轮廓创建的肺部 X 射线掩模显着低估了肺部区域，并将肺部的大部分部分排除在进一步评估之外，这可能会导致许多临床错误。</details>
**PDF:** <http://arxiv.org/pdf/2402.11510v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training**<br />
**Title_cn:** 通过多视角自监督学习和两阶段预训练提高甲状腺超声诊断<br />
**Authors:** Jian Wang, Xin Yang, Xiaohong Jia, Wufeng Xue, Rusi Chen, Yanlin Chen, Xiliang Zhu, Lian Liu, Yan Cao, Jianqiao Zhou, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data. In this study, we proposed a multi-view contrastive self-supervised method to improve thyroid nodule classification and segmentation performance with limited manual labels. Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area. We designed an adaptive loss function that eliminates the limitations of the paired data. Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images. Extensive experiments were conducted on a large-scale dataset collected from multiple centers. The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art self-supervised methods. The two-stage pre-training also significantly exceeded ImageNet pre-training.</details>
**Abstract_cn:** <details><summary>译文: </summary>超声图像中的甲状腺结节分类和分割对于计算机辅助诊断至关重要；然而，由于标记数据不足，它们面临着局限性。在本研究中，我们提出了一种多视图对比自监督方法，以有限的手动标签提高甲状腺结节分类和分割性能。我们的方法对齐同一结节的横向和纵向视图，从而使模型能够更多地关注结节区域。我们设计了一个自适应损失函数，消除了配对数据的限制。此外，我们采用了两阶段预训练来利用 ImageNet 和甲状腺超声图像上的预训练。对从多个中心收集的大规模数据集进行了广泛的实验。结果表明，所提出的方法在有限的手动标签下显着提高了结节分类和分割性能，并且优于最先进的自监督方法。两阶段预训练也显着超过了ImageNet预训练。</details>
**PDF:** <http://arxiv.org/pdf/2402.11497v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule Endoscopy Diagnosis**<br />
**Title_cn:** EndoOOD：胶囊内窥镜诊断中的不确定性分布外检测<br />
**Authors:** Qiaozhi Tan, Long Bai, Guankun Wang, Mobarakol Islam, Hongliang Ren<br />
**Abstract:** <details><summary>原文: </summary>Wireless capsule endoscopy (WCE) is a non-invasive diagnostic procedure that enables visualization of the gastrointestinal (GI) tract. Deep learning-based methods have shown effectiveness in disease screening using WCE data, alleviating the burden on healthcare professionals. However, existing capsule endoscopy classification methods mostly rely on pre-defined categories, making it challenging to identify and classify out-of-distribution (OOD) data, such as undefined categories or anatomical landmarks. To address this issue, we propose the Endoscopy Out-of-Distribution (EndoOOD) framework, which aims to effectively handle the OOD detection challenge in WCE diagnosis. The proposed framework focuses on improving the robustness and reliability of WCE diagnostic capabilities by incorporating uncertainty-aware mixup training and long-tailed in-distribution (ID) data calibration techniques. Additionally, virtual-logit matching is employed to accurately distinguish between OOD and ID data while minimizing information loss. To assess the performance of our proposed solution, we conduct evaluations and comparisons with 12 state-of-the-art (SOTA) methods using two publicly available datasets. The results demonstrate the effectiveness of the proposed framework in enhancing diagnostic accuracy and supporting clinical decision-making.</details>
**Abstract_cn:** <details><summary>译文: </summary>无线胶囊内窥镜 (WCE) 是一种非侵入性诊断程序，可实现胃肠道 (GI) 的可视化。基于深度学习的方法已显示出使用 WCE 数据进行疾病筛查的有效性，减轻了医疗保健专业人员的负担。然而，现有的胶囊内窥镜分类方法大多依赖于预定义的类别，这使得识别和分类分布外（OOD）数据（例如未定义的类别或解剖标志）具有挑战性。为了解决这个问题，我们提出了内窥镜分布外（EndoOOD）框架，旨在有效应对 WCE 诊断中的 OOD 检测挑战。所提出的框架侧重于通过结合不确定性感知混合训练和长尾分布内（ID）数据校准技术来提高 WCE 诊断能力的稳健性和可靠性。此外，还采用虚拟逻辑匹配来准确区分 OOD 和 ID 数据，同时最大限度地减少信息丢失。为了评估我们提出的解决方案的性能，我们使用两个公开可用的数据集与 12 种最先进的 (SOTA) 方法进行评估和比较。结果证明了所提出的框架在提高诊断准确性和支持临床决策方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11476v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection**<br />
**Title_cn:** 有毒的伪造人脸：针对人脸伪造检测的后门攻击<br />
**Authors:** Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39\% BD-AUC) and reduction in visibility (-12.65\% $L_\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \url{https://github.com/JWLiang007/PFF}</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸伪造技术的激增引起了社会的极大关注，从而推动了人脸伪造检测方法的发展。这些方法旨在区分伪造的人脸和真实的人脸，并在实际应用中被证明是有效的。然而，本文介绍了由后门攻击引起的人脸伪造检测场景中一种新颖且以前未被识别的威胁。通过将后门嵌入到模型中并将特定的触发模式合并到输入中，攻击者可以欺骗检测器对伪造的人脸产生错误的预测。为了实现这一目标，本文提出了 \emph{Poisoned Forgery Face} 框架，该框架能够对人脸伪造检测器进行清洁标签后门攻击。我们的方法涉及构建一个可扩展的触发生成器并利用新颖的卷积过程来生成翻译敏感的触发模式。此外，我们采用基于地标区域的相对嵌入方法来增强中毒样本的隐秘性。因此，针对我们的中毒样本进行训练的探测器都嵌入了后门。值得注意的是，我们的方法超越了 SoTA 后门基线，显着提高了攻击成功率 (+16.39\% BD-AUC) 并降低了可见性 (-12.65\% $L_\infty$)。此外，我们的攻击在对抗后门防御方面表现出了良好的性能。我们预计本文将引起人们对人脸伪造检测场景中后门攻击所带来的潜在威胁的更多关注。我们的代码将在 \url{https://github.com/JWLiang007/PFF} 提供</details>
**PDF:** <http://arxiv.org/pdf/2402.11473v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Key Patch Proposer: Key Patches Contain Rich Information**<br />
**Title_cn:** 关键补丁提议者：关键补丁包含丰富信息<br />
**Authors:** Jing Xu, Beiwen Tian, Hao Zhao<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP) designed to select key patches in an image without additional training. Our experiments showcase KPP's robust capacity to capture semantic information by both reconstruction and classification tasks. The efficacy of KPP suggests its potential application in active learning for semantic segmentation. Our source code is publicly available at https://github.com/CA-TT-AC/key-patch-proposer.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了一种名为关键补丁提议器（KPP）的新颖算法，旨在无需额外训练即可选择图像中的关键补丁。我们的实验展示了 KPP 通过重建和分类任务捕获语义信息的强大能力。 KPP 的功效表明其在语义分割主动学习中的潜在应用。我们的源代码可在 https://github.com/CA-TT-AC/key-patch-proposer 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11458v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning**<br />
**Title_cn:** Momentor：利用细粒度时序推理推进视频大语言模型<br />
**Authors:** Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, Siliang Tang<br />
**Abstract:** <details><summary>原文: </summary>Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型 (LLM) 在理解和处理基于文本的任务方面表现出卓越的熟练程度。人们正在做出许多努力将这些属性转移到视频模式，称为视频法学硕士。然而，现有的视频LLM只能捕获粗粒度的语义，无法有效处理与特定视频片段的理解或定位相关的任务。鉴于这些挑战，我们提出了 Momentor，一种能够完成细粒度时间理解任务的视频法学硕士。为了支持Momentor的训练，我们设计了一个自动数据生成引擎来构建Moment-10M，一个具有分段级指令数据的大规模视频指令数据集。我们在 Moment-10M 上训练 Momentor，使其能够执行分段级推理和定位。对多项任务的零样本评估表明，Momentor 在细粒度的时间基础理解和定位方面表现出色。</details>
**PDF:** <http://arxiv.org/pdf/2402.11435v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)**<br />
**Title_cn:** 利用分段任意模型 (SAM) 进行机器驱动图像标记的多光谱自动传输技术 (MATT)<br />
**Authors:** James E. Gallagher, Aryav Gogia, Edward J. Oughton<br />
**Abstract:** <details><summary>原文: </summary>Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, compared to a manually labeled dataset. We consider this an acceptable level of precision loss when considering the time saved during training, especially for rapidly prototyping experimental modeling methods. This research greatly contributes to the study of multispectral object detection by providing a novel and open-source method to rapidly segment, label, and train multispectral object detection models with minimal human interaction. Future research needs to focus on applying these methods to (i) space-based multispectral, and (ii) drone-based hyperspectral imagery.</details>
**Abstract_cn:** <details><summary>译文: </summary>分段任意模型 (SAM) 大大加快了自动分割和标记大型红绿蓝 (RGB) 图像数据集的速度和准确性。然而，SAM 无法分割和标记可见光谱之外的图像，例如多光谱或高光谱图像。因此，本文概述了一种我们称为多光谱自动传输技术（MATT）的方法。通过转置 RGB 图像中的 SAM 分割掩模，我们可以高精度、高效地自动分割和标记多光谱图像。例如，结果表明，利用 MATT 对 2,400 个图像数据集进行分割和标记，开发训练模型的时间减少了 87.8%，将大约 20 小时的手动标记时间减少到仅 2.4 小时。与手动标记的数据集相比，通过 MATT 训练多光谱模型时，这种效率增益仅导致总体平均精度 (mAP) 仅下降 6.7%。考虑到训练期间节省的时间，我们认为这是可接受的精度损失水平，特别是对于快速原型化实验建模方法。这项研究通过提供一种新颖的开源方法来以最少的人类交互快速分割、标记和训练多光谱目标检测模型，为多光谱目标检测的研究做出了巨大贡献。未来的研究需要集中于将这些方法应用于（i）基于空间的多光谱和（ii）基于无人机的高光谱图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.11413v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Aligning Modalities in Vision Large Language Models via Preference Fine-tuning**<br />
**Title_cn:** 通过偏好微调来调整视觉大语言模型中的模态<br />
**Authors:** Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao<br />
**Abstract:** <details><summary>原文: </summary>Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.</details>
**Abstract_cn:** <details><summary>译文: </summary>指令跟随视觉大型语言模型（VLLM）最近在各种任务上取得了重大进展。这些方法融合了强大的预训练视觉模型和大型语言模型（LLM）。由于这些组件是单独训练的，因此学习到的表示需要与其他图像语言对的联合训练保持一致。这个过程并不完美，可能会导致模型产生幻觉——即使核心法学硕士非常真实并且视觉主干具有足够完整的表示，但提供的答案也不能准确反映图像。在这项工作中，我们将幻觉问题视为对齐问题，并通过偏好调整来解决它。具体来说，我们建议 POVID 使用 AI 模型生成反馈数据。我们使用真实指令作为首选响应，并使用两阶段方法来生成非首选数据。首先，我们提示 GPT-4V 将合理的幻觉注入正确答案中。其次，我们扭曲图像以触发 VLLM 固有的幻觉行为。这是一种自动化方法，不依赖于人类数据生成或需要完美的专家，这使得它很容易扩展。最后，这两种生成策略都通过直接偏好优化集成到 RLHF 管道中。在跨广泛基准的实验中，我们表明我们不仅可以减少幻觉，还可以提高跨标准基准的模型性能，优于之前的方法。我们的数据和代码可在 https://github.com/YiyangZhou/POVID 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11411v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods**<br />
**Title_cn:** 使用递归神经网络和图像压缩方法进行 3D 点云压缩<br />
**Authors:** Till Beemelmanns, Yuchen Tao, Bastian Lampe, Lennart Reiher, Raphael van Kempen, Timo Woopen, Lutz Eckstein<br />
**Abstract:** <details><summary>原文: </summary>Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance. Source code and dataset are available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression.</details>
**Abstract_cn:** <details><summary>译文: </summary>存储和传输 LiDAR 点云数据对于许多 AV 应用至关重要，例如训练数据收集、远程控制、云服务或 SLAM。然而，由于数据的稀疏性和无序结构，点云数据很难压缩到小容量。将原始点云数据转换为密集的二维矩阵结构是应用压缩算法的一种有前途的方法。我们提出了一种新的无损且校准的 3D 到 2D 转换，它允许压缩算法有效地利用 2D 表示中的空间相关性。为了压缩结构化表示，我们使用常见的图像压缩方法以及使用循环神经网络的自监督深度压缩方法。我们还将 LiDAR 的强度测量重新排列为密集的 2D 表示，并提出了一种新的指标来评估强度的压缩性能。与基于通用八叉树点云压缩或基于原始点云数据压缩的方法相比，我们的方法实现了最佳的定量和视觉性能。源代码和数据集可在 https://github.com/ika-rwth-aachen/Point-Cloud-Compression 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11680v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Neuromorphic Face Analysis: a Survey**<br />
**Title_cn:** 神经形态面部分析：一项调查<br />
**Authors:** Federico Becattini, Lorenzo Berlincioni, Luca Cultrera, Alberto Del Bimbo<br />
**Abstract:** <details><summary>原文: </summary>Neuromorphic sensors, also known as event cameras, are a class of imaging devices mimicking the function of biological visual systems. Unlike traditional frame-based cameras, which capture fixed images at discrete intervals, neuromorphic sensors continuously generate events that represent changes in light intensity or motion in the visual field with high temporal resolution and low latency. These properties have proven to be interesting in modeling human faces, both from an effectiveness and a privacy-preserving point of view. Neuromorphic face analysis however is still a raw and unstructured field of research, with several attempts at addressing different tasks with no clear standard or benchmark. This survey paper presents a comprehensive overview of capabilities, challenges and emerging applications in the domain of neuromorphic face analysis, to outline promising directions and open issues. After discussing the fundamental working principles of neuromorphic vision and presenting an in-depth overview of the related research, we explore the current state of available data, standard data representations, emerging challenges, and limitations that require further investigation. This paper aims to highlight the recent process in this evolving field to provide to both experienced and newly come researchers an all-encompassing analysis of the state of the art along with its problems and shortcomings.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经形态传感器，也称为事件相机，是一类模仿生物视觉系统功能的成像设备。与以离散间隔捕获固定图像的传统基于帧的相机不同，神经形态传感器以高时间分辨率和低延迟连续生成表示视野中光强度或运动变化的事件。从有效性和隐私保护的角度来看，这些属性已被证明在人脸建模中很有趣。然而，神经形态人脸分析仍然是一个原始且非结构化的研究领域，在没有明确的标准或基准的情况下，人们多次尝试解决不同的任务。这篇调查论文全面概述了神经形态人脸分析领域的功能、挑战和新兴应用，概述了有希望的方向和未解决的问题。在讨论了神经形态视觉的基本工作原理并深入概述了相关研究之后，我们探讨了可用数据的现状、标准数据表示、新出现的挑战以及需要进一步研究的局限性。本文旨在强调这一不断发展的领域的最新进展，为经验丰富的和新来的研究人员提供对现有技术及其问题和缺点的全面分析。</details>
**PDF:** <http://arxiv.org/pdf/2402.11631v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Robust Error-Resistant View Selection Method for 3D Reconstruction**<br />
**Title_cn:** 一种鲁棒、抗错的 3D 重建视图选择方法<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Bin Nan, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method. The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix. The sorting results of each row in the error-resistant matrix determine the candidate view set for each view. By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results. The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决运动结构（SFM）视图选择中由于选择相机基线较小的视图而导致三角测量不确定性增加的问题，本文提出了一种鲁棒的抗错误视图选择方法。该方法利用基于三角测量的计算来获得防错模型，然后使用该模型构建防错矩阵。防错矩阵中每一行的排序结果决定了每个视图的候选视图集合。通过遍历所有视图的候选视图集并基于防错矩阵补全缺失视图，保证了3D重建的完整性。在重建结果中的平均重投影误差和绝对轨迹误差方面对该方法与COLMAP程序中精度最高的穷举法进行了实验比较。该方法在 TUM 数据集和 DTU 数据集上证明重投影误差精度平均降低了 29.40%，绝对轨迹误差平均降低了 5.07%。</details>
**PDF:** <http://arxiv.org/pdf/2402.11431v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training**<br />
**Title_cn:** 通过重新审视数据增强和一致性训练来促进半监督二维人体姿势估计<br />
**Authors:** Huayi Zhou, Mukun Luo, Fei Jiang, Yue Ding, Hongtao Lu<br />
**Abstract:** <details><summary>原文: </summary>The 2D human pose estimation is a basic visual problem. However, supervised learning of a model requires massive labeled images, which is expensive and labor-intensive. In this paper, we aim at boosting the accuracy of a pose estimator by excavating extra unlabeled images in a semi-supervised learning (SSL) way. Most previous consistency-based SSL methods strive to constraint the model to predict consistent results for differently augmented images. Following this consensus, we revisit two core aspects including advanced data augmentation methods and concise consistency training frameworks. Specifically, we heuristically dig various collaborative combinations of existing data augmentations, and discover novel superior data augmentation schemes to more effectively add noise on unlabeled samples. They can compose easy-hard augmentation pairs with larger transformation difficulty gaps, which play a crucial role in consistency-based SSL. Moreover, we propose to strongly augment unlabeled images repeatedly with diverse augmentations, generate multi-path predictions sequentially, and optimize corresponding unsupervised consistency losses using one single network. This simple and compact design is on a par with previous methods consisting of dual or triple networks. Furthermore, it can also be integrated with multiple networks to produce better performance. Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets. Code is released for academic use in \url{https://github.com/hnuzhy/MultiAugs}.</details>
**Abstract_cn:** <details><summary>译文: </summary>二维人体姿态估计是一个基本的视觉问题。然而，模型的监督学习需要大量标记图像，这是昂贵且劳动密集型的。在本文中，我们的目标是通过以半监督学习（SSL）方式挖掘额外的未标记图像来提高姿势估计器的准确性。大多数先前基于一致性的 SSL 方法都努力约束模型以预测不同增强图像的一致结果。遵循这一共识，我们重新审视两个核心方面，包括先进的数据增强方法和简洁的一致性训练框架。具体来说，我们启发式地挖掘现有数据增强的各种协作组合，并发现新颖的高级数据增强方案，以更有效地在未标记样本上添加噪声。它们可以组成具有较大转换难度差距的易难增强对，这在基于一致性的 SSL 中发挥着至关重要的作用。此外，我们建议通过各种增强来重复增强未标记图像，顺序生成多路径预测，并使用一个网络优化相应的无监督一致性损失。这种简单而紧凑的设计与之前由双网络或三网络组成的方法相当。此外，它还可以与多个网络集成以产生更好的性能。与最先进的 SSL 方法相比，我们的方法对公共数据集带来了实质性的改进。代码在 \url{https://github.com/hnuzhy/MultiAugs} 中发布供学术使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.11566v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **The Effectiveness of Random Forgetting for Robust Generalization**<br />
**Title_cn:** 随机遗忘对鲁棒泛化的有效性<br />
**Authors:** Vijaya Raghavan T Ramkumar, Bahram Zonooz, Elahe Arani<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络容易受到对抗性攻击，这可能会损害其性能和准确性。对抗训练（AT）已成为保护神经网络免受此类攻击的流行方法。然而，AT 的一个关键挑战是鲁棒过度拟合，网络在测试数据上的鲁棒性能随着进一步训练而恶化，从而阻碍泛化。受大脑主动遗忘概念的启发，我们引入了一种新颖的学习范式，称为“忘记减轻过度拟合（FOMO）”。 FOMO 在遗忘阶段和重新学习阶段之间交替，前者随机忘记权重的子集，并通过权重重新初始化来调节模型的信息，后者强调学习可概括的特征。我们对基准数据集和对抗性攻击的实验表明，FOMO 通过显着缩小最佳和最后鲁棒测试精度之间的差距，同时提高最先进的鲁棒性，从而减轻鲁棒过度拟合。此外，FOMO 在标准精度和鲁棒精度之间提供了更好的权衡，优于基线对抗方法。最后，我们的框架对自动攻击具有鲁棒性，并提高了许多现实场景中的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.11733v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Learning Conditional Invariances through Non-Commutativity**<br />
**Title_cn:** 通过非交换性学习条件不变性<br />
**Authors:** Abhra Chaudhuri, Serban Georgescu, Anjan Dutta<br />
**Abstract:** <details><summary>原文: </summary>Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\Phi^*_\tau$. We prove that non-commutativity steers the optimization towards $\Phi^*_\tau$ instead of $\varphi^*$, bringing the $\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning $\Phi^*_\tau$, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over $2\%$, approaching the performance of an oracle. Implementation is available at https://github.com/abhrac/nci.</details>
**Abstract_cn:** <details><summary>译文: </summary>不变性学习算法有条件地过滤掉特定领域的随机变量作为干扰项，仅基于数据语义，而不是评估的目标域。我们证明，学习条件不变性的一种可证明最优且样本有效的方法是放宽不变性标准，使其非交换地指向目标域。在域不对称的情况下，即当目标域包含源中不存在的语义相关信息时，跨域平均最优的编码器 $\varphi^*$ 的风险严格受到目标特定风险的下限最优编码器$\Phi^*_\tau$。我们证明非交换性将优化引向 $\Phi^*_\tau$ 而不是 $\varphi^*$，使域之间的 $\mathcal{H}$ 散度降至零，从而产生更严格的界限关于目标风险。我们的理论和实验都表明，非交换不变性（NCI）可以利用源域样本来满足学习 $\Phi^*_\tau$ 的样本复杂性需求，超越域适应的 SOTA 不变性学习算法，有时甚至超过$2\%$，接近预言机的性能。可以在 https://github.com/abhrac/nci 上进行实施。</details>
**PDF:** <http://arxiv.org/pdf/2402.11682v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Interactive Garment Recommendation with User in the Loop**<br />
**Title_cn:** 与用户互动的服装推荐<br />
**Authors:** Federico Becattini, Xiaolin Chen, Andrea Puccia, Haokun Wen, Xuemeng Song, Liqiang Nie, Alberto Del Bimbo<br />
**Abstract:** <details><summary>原文: </summary>Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.</details>
**Abstract_cn:** <details><summary>译文: </summary>推荐时尚单品通常会利用丰富的用户档案，并根据过去的历史和以前的购买行为提出有针对性的建议。在本文中，我们假设没有提供有关用户的先验知识。我们建议通过整合用户反应来动态构建用户档案，因为我们推荐互补的物品来组成一套服装。我们提出了一种强化学习代理，能够建议合适的服装并吸收用户反馈，从而改进其推荐并最大限度地提高用户满意度。为了训练这样的模型，我们采用代理模型来模拟训练循环中的用户反馈。我们在 IQON3000 时尚数据集上进行实验，发现基于强化学习的代理能够通过考虑个人偏好来改进其推荐。此外，事实证明，此类任务对于非强化模型来说是困难的，因为它们无法在训练期间进行探索。</details>
**PDF:** <http://arxiv.org/pdf/2402.11627v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Visual In-Context Learning for Large Vision-Language Models**<br />
**Title_cn:** 大型视觉语言模型的视觉上下文学习<br />
**Authors:** Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen<br />
**Abstract:** <details><summary>原文: </summary>In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use of in-context unlearning further shows promise in resetting specific model knowledge without retraining.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大型视觉语言模型（LVLM）中，上下文学习（ICL）的功效仍然受到跨模式交互和表示差异的挑战的限制。为了克服这些挑战，我们引入了一种新颖的视觉上下文学习（VICL）方法，包括视觉演示检索、面向意图的图像摘要和面向意图的演示合成。我们的方法通过“检索和重新排序”范例检索图像，总结具有任务意图和特定于任务的视觉解析的图像，并组成基于语言的演示，以减少标记数量并缓解跨模式交互问题。对五个视觉推理数据集的实验评估证明了我们方法的有效性。此外，我们广泛的实验利用信息流分析来阐明我们方法的有效性，并研究演示的长度和位置对 LVLM 的影响。使用上下文取消学习进一步显示出在无需重新训练的情况下重置特定模型知识的前景。</details>
**PDF:** <http://arxiv.org/pdf/2402.11574v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Evaluating Adversarial Robustness of Low dose CT Recovery**<br />
**Title_cn:** 评估低剂量 CT 恢复的对抗鲁棒性<br />
**Authors:** Kanchana Vaishnavi Gandikota, Paramanand Chandramouli, Hannah Droege, Michael Moeller<br />
**Abstract:** <details><summary>原文: </summary>Low dose computed tomography (CT) acquisition using reduced radiation or sparse angle measurements is recommended to decrease the harmful effects of X-ray radiation. Recent works successfully apply deep networks to the problem of low dose CT recovery on bench-mark datasets. However, their robustness needs a thorough evaluation before use in clinical settings. In this work, we evaluate the robustness of different deep learning approaches and classical methods for CT recovery. We show that deep networks, including model-based networks encouraging data consistency, are more susceptible to untargeted attacks. Surprisingly, we observe that data consistency is not heavily affected even for these poor quality reconstructions, motivating the need for better regularization for the networks. We demonstrate the feasibility of universal attacks and study attack transferability across different methods. We analyze robustness to attacks causing localized changes in clinically relevant regions. Both classical approaches and deep networks are affected by such attacks leading to changes in the visual appearance of localized lesions, for extremely small perturbations. As the resulting reconstructions have high data consistency with the original measurements, these localized attacks can be used to explore the solution space of the CT recovery problem.</details>
**Abstract_cn:** <details><summary>译文: </summary>建议使用减少辐射或稀疏角度测量进行低剂量计算机断层扫描 (CT) 采集，以减少 X 射线辐射的有害影响。最近的工作成功地将深度网络应用于基准数据集上的低剂量 CT 恢复问题。然而，它们的稳健性在用于临床环境之前需要进行彻底的评估。在这项工作中，我们评估了不同深度学习方法和经典 CT 恢复方法的稳健性。我们表明，深层网络，包括鼓励数据一致性的基于模型的网络，更容易受到无针对性的攻击。令人惊讶的是，我们观察到即使对于这些质量较差的重建，数据一致性也没有受到严重影响，这激发了对网络更好的正则化的需求。我们展示了通用攻击的可行性，并研究了跨不同方法的攻击可转移性。我们分析了对引起临床相关区域局部变化的攻击的鲁棒性。经典方法和深层网络都会受到此类攻击的影响，导致局部病变的视觉外观发生变化，产生极小的扰动。由于重建结果与原始测量数据具有较高的一致性，因此这些局部攻击可用于探索 CT 恢复问题的解空间。</details>
**PDF:** <http://arxiv.org/pdf/2402.11557v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **To use or not to use proprietary street view images in (health and place) research? That is the question**<br />
**Title_cn:** 在（健康和场所）研究中使用或不使用专有街景图像？就是那个问题<br />
**Authors:** Marco Helbich, Matthew Danish, SM Labib, Britta Ricker<br />
**Abstract:** <details><summary>原文: </summary>Computer vision-based analysis of street view imagery has transformative impacts on environmental assessments. Interactive web services, particularly Google Street View, play an ever-important role in making imagery data ubiquitous. Despite the technical ease of harnessing millions of Google Street View images, this article questions the current practices in using this proprietary data source. Our concern lies with Google's terms of service, which prohibit bulk image downloads and the generation of street view image-based indices. To reconcile the challenge of advancing society through groundbreaking research while maintaining data license agreements and legal integrity, it is crucial to adhere to open data principles and utilize open image sources for future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于计算机视觉的街景图像分析对环境评估具有变革性影响。交互式网络服务，特别是谷歌街景，在使图像数据无处不在方面发挥着越来越重要的作用。尽管利用数百万张 Google 街景图像在技术上很容易，但本文对使用此专有数据源的当前做法提出了质疑。我们担心的是 Google 的服务条款，该条款禁止批量图像下载和生成基于街景图像的索引。为了应对通过突破性研究推动社会进步的挑战，同时维护数据许可协议和法律完整性，坚持开放数据原则并利用开放图像源进行未来研究至关重要。</details>
**PDF:** <http://arxiv.org/pdf/2402.11504v1><br />
**Code:** null<br />

