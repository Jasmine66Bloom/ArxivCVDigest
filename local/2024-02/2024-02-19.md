## [UPDATED!] **2024-02-19** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **FiT: Flexible Vision Transformer for Diffusion Model**<br />
**Title_cn:** FiT：用于扩散模型的灵活视觉变压器<br />
**Authors:** Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai<br />
**Abstract:** <details><summary>原文: </summary>Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.</details>
**Abstract_cn:** <details><summary>译文: </summary>自然是无限无分辨率的。在这种现实背景下，现有的扩散模型（例如扩散变压器）在处理训练域之外的图像分辨率时经常面临挑战。为了克服这一限制，我们提出了灵活视觉变压器（FiT），这是一种专门为生成分辨率和纵横比不受限制的图像而设计的变压器架构。与将图像视为静态分辨率网格的传统方法不同，FiT 将图像概念化为动态大小的标记序列。这种视角实现了灵活的训练策略，可以在训练和推理阶段轻松适应不同的纵横比，从而促进分辨率泛化并消除图像裁剪引起的偏差。通过精心调整的网络结构和免训练外推技术的集成，FiT 在分辨率外推生成方面表现出卓越的灵活性。综合实验证明了 FiT 在广泛的分辨率范围内的卓越性能，展示了其在训练分辨率分布内外的有效性。存储库位于 https://github.com/whlzy/FiT。</details>
**PDF:** <http://arxiv.org/pdf/2402.12376v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Mixed Gaussian Flow for Diverse Trajectory Prediction**<br />
**Title_cn:** 用于多种轨迹预测的混合高斯流<br />
**Authors:** Jiahe Chen, Jinkun Cao, Dahua Lin, Kris Kitani, Jiangmiao Pang<br />
**Abstract:** <details><summary>原文: </summary>Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的轨迹预测研究集中利用生成模型。归一化流是一种具有可逆性的优点，可以导出预测轨迹的概率密度。然而，通过基于流的模型从标准高斯映射会损害捕获复杂轨迹模式的能力，忽略训练数据中未被充分代表的运动意图。为了解决这个问题，我们提出了一种基于流的模型，将混合高斯先验转换为未来的轨迹流形。该模型显示出更好的生成不同轨迹模式的能力。此外，通过将每个亚高斯与特定的轨迹子空间相关联，我们可以生成具有可控运动意图的未来轨迹。以这种方式，基于流的模型不再被鼓励简单地寻找预期流形的最大可能性，而是寻找具有明确可解释性的受控流形族。我们提出的方法被证明在对 top-M 生成的候选中的采样对齐轨迹进行定量评估方面显示出最先进的性能。我们还证明它可以生成多样化、可控且不分布的轨迹。代码可在 https://github.com/mulplue/MGF 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12238v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**<br />
**Title_cn:** AnyGPT：具有离散序列建模的统一多模态法学硕士<br />
**Authors:** Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 AnyGPT，这是一种任意对任意的多模态语言模型，它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。 AnyGPT 可以稳定地训练，无需对当前的大语言模型（LLM）架构或训练范式进行任何改变。相反，它完全依赖于数据级预处理，促进新模式无缝集成到法学硕士中，类似于新语言的合并。我们构建了一个以文本为中心的多模态数据集，用于多模态对齐预训练。利用生成模型，我们合成了第一个大规模任意对任意多模式指令数据集。它由 108k 个多轮对话样本组成，这些对话错综复杂地交织着各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT 能够促进任意对任意的多模态对话，同时在所有模态中实现与专用模型相当的性能，证明离散表示可以有效且方便地统一语言模型中的多种模态。演示见 https://junzhan2000.github.io/AnyGPT.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2402.12226v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training**<br />
**Title_cn:** 对抗性特征对齐：通过对抗性训练平衡深度学习的鲁棒性和准确性<br />
**Authors:** Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon<br />
**Abstract:** <details><summary>原文: </summary>Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型的准确性不断提高，但它们仍然容易受到对抗性攻击，这通常会导致对抗性示例的错误分类。对抗性训练用于通过提高针对这些攻击的鲁棒性来缓解这个问题。然而，这种方法通常会降低模型在干净、非对抗性样本上的标准精度。深度学习模型平衡鲁棒性和准确性以保证安全性的必要性是显而易见的，但实现这种平衡仍然具有挑战性，其根本原因尚待阐明。本文提出了一种称为对抗特征对齐（AFA）的新型对抗训练方法来解决这些问题。我们的研究揭示了一个有趣的见解：特征空间内的错位通常会导致错误分类，无论样本是良性的还是对抗性的。 AFA 通过采用基于对比学习的新颖优化算法来减轻潜在的特征错位，从而降低了这种风险。通过我们的评估，我们展示了 AFA 的卓越性能。与交叉熵相比，基线 AFA 提供了比之前的对抗性对比学习方法更高的鲁棒精度，同时将 CIFAR10 和 CIFAR100 上的干净精度下降幅度降至最低，分别为 1.86% 和 8.91%。我们还表明，AFA 和 TRADES 的联合优化，加上使用最新扩散模型的数据增强，实现了最先进的准确性和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12187v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection**<br />
**Title_cn:** 由最大强度投影的 2D 注释监督的 3D 血管分割<br />
**Authors:** Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou<br />
**Abstract:** <details><summary>原文: </summary>Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.</details>
**Abstract_cn:** <details><summary>译文: </summary>血管结构分割在医学分析和临床应用中起着至关重要的作用。 3D 空间中注释血管的复杂性和耗时性阻碍了完全监督分割模型的实际采用。这刺激了对弱监督方法的探索，以减少对昂贵的分割注释的依赖。尽管如此，器官分割中使用的现有弱监督方法（包括点、边界框或涂鸦）在处理稀疏血管结构时表现出次优性能。为了缓解这个问题，我们采用最大强度投影（MIP）将3D体积降维为2D图像以进行有效注释，并利用2D标签为训练3D血管分割模型提供指导和监督。最初，我们使用 2D 投影的注释生成 3D 血管的伪标签。随后，考虑到 2D 标签的获取方法，我们引入了一种弱监督网络，通过 MIP 融合 2D-3D 深度特征，以进一步提高分割性能。此外，我们集成置信度学习和不确定性估计来细化生成的伪标签，然后微调分割网络。我们的方法在五个数据集（包括脑血管、主动脉和冠状动脉）上进行了验证，证明了在分割血管方面具有高度竞争力的性能，并且有可能显着减少血管注释所需的时间和精力。我们的代码位于：https://github.com/gzq17/Weakly-Supervised-by-MIP。</details>
**PDF:** <http://arxiv.org/pdf/2402.12128v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Human Video Translation via Query Warping**<br />
**Title_cn:** 通过查询变形进行人类视频翻译<br />
**Authors:** Haiming Zhu, Yangyang Xu, Shengfeng He<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing diffusion-based video editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the diffusion model, we employ appearance flows to warp the previous frame's query token, aligning it with the current frame's query. This query warping imposes explicit constraints on the outputs of self-attention layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了 QueryWarp，一种用于时间连贯人体运动视频翻译的新颖框架。现有的基于扩散的视频编辑方法仅依赖键和值标记来确保时间一致性，这会破坏局部和结构区域的保存。相反，我们的目标是通过构建来自不同帧的查询标记之间的时间相关性来考虑互补查询先验。最初，我们从源姿势中提取外观流以捕获连续的人体前景运动。随后，在扩散模型的去噪过程中，我们使用外观流来扭曲前一帧的查询标记，将其与当前帧的查询对齐。这种查询扭曲对自注意力层的输出施加了明确的约束，有效地保证了时间连贯的翻译。我们对各种人体运动视频翻译任务进行了实验，结果表明我们的 QueryWarp 框架在质量和数量上都超越了最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.12099v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Direct Consistency Optimization for Compositional Text-to-Image Personalization**<br />
**Title_cn:** 组合文本到图像个性化的直接一致性优化<br />
**Authors:** Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( https://dco-t2i.github.io/).</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像（T2I）扩散模型在对一些个人图像进行微调时，能够生成高度一致性的视觉效果。然而，他们仍然缺乏合成原始预训练模型中可能的不同场景或风格的图像。为了解决这个问题，我们建议通过最大化参考图像的一致性来微调 T2I 模型，同时惩罚与预训练模型的偏差。我们为 T2I 扩散模型设计了一个新颖的训练目标，它对预训练模型进行最小程度的微调以实现一致性。我们的方法被称为 \emph{直接一致性优化}，就像常规扩散损失一样简单，同时显着增强了个性化 T2I 模型的组合性。此外，我们的方法引入了一种新的采样方法，可以控制图像保真度和提示保真度之间的权衡。最后，我们强调对参考图像使用综合标题以进一步增强图像文本对齐的必要性。我们展示了所提出的方法对主题、风格或两者的 T2I 个性化的有效性。特别是，我们的方法产生了优于基线的帕累托前沿。生成的示例和代码位于我们的项目页面（https://dco-t2i.github.io/）中。</details>
**PDF:** <http://arxiv.org/pdf/2402.12004v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models**<br />
**Title_cn:** 潜在扩散模型的隐私保护低阶适应<br />
**Authors:** Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang<br />
**Abstract:** <details><summary>原文: </summary>Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.</details>
**Abstract_cn:** <details><summary>译文: </summary>低秩适应 (LoRA) 是一种有效的策略，用于在训练数据集上调整潜在扩散模型 (LDM)，通过最小化适应损失来生成特定对象。然而，通过 LoRA 适配的 LDM 很容易受到成员推理（MI）攻击，可以判断特定数据点是否属于私人训练数据集，从而面临严重的隐私泄露风险。为了防御 MI 攻击，我们首先提出了一个简单的解决方案：隐私保护 LoRA（PrivateLoRA）。 PrivateLoRA 被表述为最小-最大优化问题，其中通过最大化其 MI 增益来训练代理攻击模型，同时通过最小化适应损失和代理攻击模型的 MI 增益之和来适应 LDM。然而，我们根据经验发现，PrivateLoRA 存在优化不稳定的问题，因为梯度尺度波动较大，阻碍了自适应。为了缓解这个问题，我们提出了 Stable PrivateLoRA，它通过最小化适应损失与 MI 增益的比率来适应 LDM，这隐式地重新调整了梯度，从而稳定了优化。我们全面的实证结果证实，通过 Stable PrivateLoRA 改造的 LDM 可以有效防御 MI 攻击，同时生成高质量图像。我们的代码可在 https://github.com/WilliamLUO0/StablePrivateLoRA 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11989v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation**<br />
**Title_cn:** DiLightNet：用于基于扩散的图像生成的细粒度照明控制<br />
**Authors:** Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种在文本驱动的基于扩散的图像生成过程中实施细粒度照明控制的新方法。虽然现有的扩散模型已经能够在任何照明条件下生成图像，但如果没有额外的指导，这些模型往往会将图像内容和照明相关联。此外，文本提示缺乏必要的表达能力来描述详细的照明设置。为了在图像生成过程中为内容创建者提供对照明的细粒度控制，我们以辐射提示的形式使用详细的照明信息来增强文本提示，即在目标照明下使用均匀规范材质对场景几何进行可视化。然而，产生辐射提示所需的场景几何形状未知。我们的主要观察是，我们只需要引导扩散过程，因此不需要精确的辐射提示；我们只需要将扩散模型指向正确的方向即可。基于这一观察，我们引入了一种在图像生成过程中控制照明的三阶段方法。在第一阶段，我们利用标准的预训练扩散模型在不受控制的照明下生成临时图像。接下来，在第二阶段，我们通过将目标光照传递到名为 DiLightNet 的细化扩散模型，使用根据临时图像推断的前景对象的粗略形状计算出的辐射度提示，重新合成和细化生成图像中的前景对象。为了保留纹理细节，我们将辐射提示与临时合成图像的神经编码相乘，然后将其传递给 DiLightNet。最后，在第三阶段，我们重新合成背景，使其与前景物体上的光照保持一致。我们在各种文本提示和照明条件下演示并验证了我们的照明控制扩散模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.11929v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation**<br />
**Title_cn:** One2Avatar：用于小样本用户适应的生成隐式头部头像<br />
**Authors:** Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang<br />
**Abstract:** <details><summary>原文: </summary>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.</details>
**Abstract_cn:** <details><summary>译文: </summary>从单眼视频构建高质量、个性化头部头像的传统方法需要大量的面部捕捉和训练时间，这对可扩展性提出了重大挑战。本文介绍了一种新颖的方法来创建高质量的头像，每个用户仅使用一张或几张图像。我们从 2407 名受试者的表情多视图数据集中学习了 3D 可动画、照片般真实头部头像的生成模型，并利用它作为从少量图像创建个性化头像的先验。与之前的 3D 感知面部生成模型不同，我们的先验模型是用 3DMM 锚定的神经辐射场主干构建的，我们证明通过基于少量输入的自动解码，它可以更有效地创建头像。我们还通过联合优化 3DMM 拟合和相机校准来处理不稳定的 3DMM 拟合，从而实现更好的少镜头自适应。我们的方法展示了令人信服的结果，并且优于现有的最先进的少镜头化身适应方法，为更高效和个性化的化身创建铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.11909v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization**<br />
**Title_cn:** 注：通过基于直接偏好优化的有效方法生成显着的患者文本摘要<br />
**Authors:** Imjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park<br />
**Abstract:** <details><summary>原文: </summary>The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose "NOTE", which stands for "Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization". NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>出院总结是患者旅程中的重要文件之一，涵盖住院期间经历的所有事件，包括多次就诊、药物、检查、手术/程序以及入院/出院。提供患者进展的总结至关重要，因为它会显着影响未来的护理和计划。因此，临床医生面临着手动收集、组织和组合出院总结所需的所有数据的费力且资源密集型的任务。因此，我们提出“NOTE”，它代表“通过基于直接偏好优化的有效方法显着生成患者文本摘要”。 NOTE 基于重症监护医疗信息集市 III 数据集，总结了患者的单次住院情况。患者事件按顺序组合并用于生成每次住院的出院摘要。在目前情况下，大型语言模型的应用程序编程接口（LLM的API）已广泛使用，但由于医疗机构的隐私保护政策，导入和导出医疗数据提出了重大挑战。此外，为了确保最佳性能，必须为医院内部服务器或程序实施轻量级模型。因此，我们利用DPO和参数高效微调（PEFT）技术来应用保证卓越性能的微调方法。为了演示所开发的NOTE的实际应用，我们提供了一个基于网页的演示软件。未来，我们的目标是部署可供医院临床医生实际使用的软件。 NOT可以用来生成各种摘要，不仅可以生成出院摘要，还可以生成整个患者旅程的摘要，从而减轻临床医生的劳动密集型工作量并提高效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.11882v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image**<br />
**Title_cn:** ComFusion：从单个图像在多个特定场景中生成个性化主题<br />
**Authors:** Yan Hong, Jianfu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in personalizing text-to-image (T2I) diffusion models have shown the capability to generate images based on personalized visual concepts using a limited number of user-provided examples. However, these models often struggle with maintaining high visual fidelity, particularly in manipulating scenes as defined by textual inputs. Addressing this, we introduce ComFusion, a novel approach that leverages pretrained models generating composition of a few user-provided subject images and predefined-text scenes, effectively fusing visual-subject instances with textual-specific scenes, resulting in the generation of high-fidelity instances within diverse scenes. ComFusion integrates a class-scene prior preservation regularization, which leverages composites the subject class and scene-specific knowledge from pretrained models to enhance generation fidelity. Additionally, ComFusion uses coarse generated images, ensuring they align effectively with both the instance image and scene texts. Consequently, ComFusion maintains a delicate balance between capturing the essence of the subject and maintaining scene fidelity.Extensive evaluations of ComFusion against various baselines in T2I personalization have demonstrated its qualitative and quantitative superiority.</details>
**Abstract_cn:** <details><summary>译文: </summary>个性化文本到图像 (T2I) 扩散模型的最新进展已经显示出使用有限数量的用户提供的示例基于个性化视觉概念生成图像的能力。然而，这些模型通常难以保持高视觉保真度，特别是在操纵文本输入定义的场景时。为了解决这个问题，我们引入了 ComFusion，这是一种新颖的方法，它利用预训练模型生成一些用户提供的主题图像和预定义文本场景的组合，有效地将视觉主题实例与文本特定场景融合，从而生成高保真度不同场景中的实例。 ComFusion 集成了类场景先验保留正则化，它利用预训练模型中的主题类和场景特定知识来增强生成保真度。此外，ComFusion 使用粗略生成的图像，确保它们与实例图像和场景文本有效对齐。因此，ComFusion 在捕捉主题本质和保持场景保真度之间保持着微妙的平衡。根据 T2I 个性化中的各种基线对 ComFusion 进行的广泛评估已经证明了其定性和定量的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.11849v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models**<br />
**Title_cn:** UnlearnCanvas：用于对扩散模型的机器遗忘进行基准测试的程式化图像数据集<br />
**Authors:** Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu<br />
**Abstract:** <details><summary>原文: </summary>The rapid advancement of diffusion models (DMs) has not only transformed various real-world industries but has also introduced negative societal concerns, including the generation of harmful content, copyright disputes, and the rise of stereotypes and biases. To mitigate these issues, machine unlearning (MU) has emerged as a potential solution, demonstrating its ability to remove undesired generative capabilities of DMs in various applications. However, by examining existing MU evaluation methods, we uncover several key challenges that can result in incomplete, inaccurate, or biased evaluations for MU in DMs. To address them, we enhance the evaluation metrics for MU, including the introduction of an often-overlooked retainability measurement for DMs post-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates us to evaluate the unlearning of artistic painting styles in conjunction with associated image objects. We show that this dataset plays a pivotal role in establishing a standardized and automated evaluation framework for MU techniques on DMs, featuring 7 quantitative metrics to address various aspects of unlearning effectiveness. Through extensive experiments, we benchmark 5 state-of-the-art MU methods, revealing novel insights into their pros and cons, and the underlying unlearning mechanisms. Furthermore, we demonstrate the potential of UnlearnCanvas to benchmark other generative modeling tasks, such as style transfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce all the results in this work can be found at https://github.com/OPTML-Group/UnlearnCanvas.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型（DM）的快速发展不仅改变了现实世界的各种行业，而且还带来了负面的社会问题，包括有害内容的产生、版权纠纷以及刻板印象和偏见的兴起。为了缓解这些问题，机器学习 (MU) 已成为一种潜在的解决方案，展示了其在各种应用中消除 DM 不需要的生成能力的能力。然而，通过检查现有的 MU 评估方法，我们发现了几个可能导致 DM 中 MU 评估不完整、不准确或有偏差的关键挑战。为了解决这些问题，我们增强了 MU 的评估指标，包括为 DM 遗忘后引入经常被忽视的保留性测量。此外，我们还引入了 UnlearnCanvas，这是一个全面的高分辨率风格化图像数据集，有助于我们结合相关图像对象来评估艺术绘画风格的遗忘。我们表明，该数据集在为 DM 上的 MU 技术建立标准化和自动化评估框架方面发挥着关键作用，具有 7 个定量指标来解决遗忘有效性的各个方面。通过广泛的实验，我们对 5 种最先进的 MU 方法进行了基准测试，揭示了对其优缺点以及潜在的遗忘机制的新颖见解。此外，我们还展示了 UnlearnCanvas 对其他生成建模任务（例如风格迁移）进行基准测试的潜力。 UnlearnCanvas 数据集、基准测试以及重现本工作中所有结果的代码可以在 https://github.com/OPTML-Group/UnlearnCanvas 中找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.11846v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection**<br />
**Title_cn:** WildFake：用于人工智能生成图像检测的大规模挑战性数据集<br />
**Authors:** Yan Hong, Jianfu Zhang<br />
**Abstract:** <details><summary>原文: </summary>The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型的非凡能力使得生成的图像质量如此之高，以至于人类无法区分人工智能（AI）生成的图像和现实生活中的照片。生成技术的发展开辟了新的机遇，但同时也给隐私、真实性和安全性带来了潜在风险。因此，检测人工智能生成的图像对于防止非法活动至关重要。为了评估人工智能生成的图像检测的通用性和鲁棒性，我们提出了一个名为 WildFake 的大规模数据集，其中包括最先进的生成器、不同的对象类别和现实世界的应用程序。 WildFake数据集具有以下优点： 1）Wild集合内容丰富：WildFake从开源社区收集假图像，通过广泛的图像类别和图像风格丰富了其多样性。 2）分层结构：WildFake 包含由不同类型的生成器合成的假图像，从 GAN、扩散模型到其他生成模型。这些关键优势增强了在 WildFake 上训练的检测器的泛化性和鲁棒性，从而证明了 WildFake 对于现实场景中人工智能生成的检测器具有相当大的相关性和有效性。此外，我们广泛的评估实验旨在深入了解不同级别的生成模型的能力，这是 WildFake 独特的层次结构所提供的独特优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.11843v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Statistical Test for Generated Hypotheses by Diffusion Models**<br />
**Title_cn:** 通过扩散模型生成的假设的统计检验<br />
**Authors:** Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi<br />
**Abstract:** <details><summary>原文: </summary>The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. We show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能性能的增强加速了其与科学研究的融合。特别是，使用生成式人工智能来创建科学假设是有前景的，并且越来越多地应用于各个领域。然而，当采用人工智能生成的假设进行关键决策（例如医疗诊断）时，验证其可靠性至关重要。在本研究中，我们考虑使用扩散模型生成的图像进行医学诊断任务，并提出一种统计测试来量化其可靠性。所提出的统计测试背后的基本思想是采用选择性推理框架，其中我们考虑统计测试的条件是生成的图像是由经过训练的扩散模型生成的。使用所提出的方法，可以以 p 值的形式量化医学图像诊断结果的统计可靠性，从而可以在错误率受控的情况下做出决策。我们通过对合成数据集和大脑图像数据集的数值实验展示了所提出的统计测试的理论有效性及其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11789v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models**<br />
**Title_cn:** 鲁棒 CLIP：鲁棒大型视觉语言模型的视觉嵌入的无监督对抗性微调<br />
**Authors:** Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM</details>
**Abstract_cn:** <details><summary>译文: </summary>OpenFlamingo、LLaVA 和 GPT-4 等多模态基础模型越来越多地用于各种实际任务。先前的工作表明，这些模型非常容易受到视觉模式的对抗性攻击。这些攻击可被用来传播虚假信息或欺骗用户，从而带来巨大的风险，这使得大型多模态基础模型的鲁棒性成为一个紧迫的问题。 CLIP 模型或其变体之一在许多视觉语言模型 (VLM) 中用作冻结视觉编码器，例如LLaVA 和 OpenFlamingo。我们提出了一种无监督的对抗性微调方案，以获得鲁棒的 CLIP 视觉编码器，该编码器对依赖于 CLIP 的所有视觉下游任务（VLM、零样本分类）产生鲁棒性。特别是，我们表明，一旦用我们强大的模型替换了原始的 CLIP 模型，恶意第三方提供经过操纵的图像对 VLM 用户进行秘密攻击就不再可能了。无需对 VLM 进行重新训练或微调。代码和稳健模型可在 https://github.com/chs20/RobustVLM 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.12336v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning**<br />
**Title_cn:** ChartX 和 ChartVLM：复杂图表推理的多功能基准和基础模型<br />
**Authors:** Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，许多多功能的多模态大语言模型（MLLM）不断出现。然而，它们查询可视化图表中描述的信息并根据查询内容进行推理的能力仍有待探索。在本文中，为了全面、严格地对图表领域现成的 MLLM 的能力进行基准测试，我们构建了 ChartX，这是一个涵盖 18 个图表类型、7 个图表任务、22 个学科主题和高层次模型的多模态评估集。质量图表数据。此外，我们开发 ChartVLM 为处理强烈依赖于可解释模式的多模态任务提供了新的视角，例如图表或几何图像领域的推理任务。我们在提议的 ChartX 评估集上评估了主流 MLLM 和我们的 ChartVLM 的图表相关能力。大量实验表明，ChartVLM 超越了通用型和图表相关的大型模型，取得了与 GPT-4V 相当的结果。我们相信，我们的研究可以为进一步探索创建更全面的图表评估集和开发更可解释的多模态模型铺平道路。 ChartX 和 ChartVLM 均可在以下位置获取：https://github.com/UniModal4Reasoning/ChartVLM</details>
**PDF:** <http://arxiv.org/pdf/2402.12185v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **LVCHAT: Facilitating Long Video Comprehension**<br />
**Title_cn:** LVCHAT：促进长视频理解<br />
**Authors:** Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He<br />
**Abstract:** <details><summary>原文: </summary>Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat significantly outperforms existing methods by up to 27\% in accuracy on long-video QA datasets and long-video captioning benchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</details>
**Abstract_cn:** <details><summary>译文: </summary>启用大型语言模型 (LLM) 来阅读视频对于多模式 LLM 至关重要。现有的作品在短视频上表现出了希望，而长视频（例如长于约 1 分钟）的理解仍然具有挑战性。主要问题在于视频的过度压缩，即编码的视频表示不足以表示整个视频。为了解决这个问题，我们提出了长视频聊天（LVChat），其中引入了帧可扩展编码（FSE）来根据视频的持续时间动态调整嵌入的数量，以确保长视频不会被过度压缩为几个嵌入。为了处理长度超出训练期间看到的视频的长视频，我们提出了交错帧编码（IFE），重复位置嵌入并交错多组视频以实现长视频输入，避免因视频过长而导致性能下降。实验结果表明，LVChat 在长视频 QA 数据集和长视频字幕基准上的准确率明显优于现有方法，准确率高达 27%。我们的代码发布在https://github.com/wangyu-ustc/LVChat。</details>
**PDF:** <http://arxiv.org/pdf/2402.12079v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models**<br />
**Title_cn:** 脚手架坐标促进大型多模态模型中的视觉语言协调<br />
**Authors:** Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting. Our code is released in https://github.com/leixy20/Scaffold.</details>
**Abstract_cn:** <details><summary>译文: </summary>最先进的大型多模态模型 (LMM) 在视觉语言任务中表现出了卓越的能力。尽管 LMM 具有先进的功能，但在需要使用多级视觉信息进行复杂推理的挑战性场景中，其性能仍然受到限制。现有的 LMM 提示技术侧重于改进文本推理或利用图像预处理工具，缺乏简单通用的视觉提示方案来促进 LMM 中的视觉语言协调。在这项工作中，我们提出支架提示支架协调以促进视觉语言协调。具体来说，Scaffold 在图像内覆盖点矩阵作为视觉信息锚点，并利用多维坐标作为文本位置参考。对各种具有挑战性的视觉语言任务的广泛实验证明了 Scaffold 相对于 GPT-4V 的优越性，并具有文本 CoT 提示。我们的代码发布在 https://github.com/leixy20/Scaffold 中。</details>
**PDF:** <http://arxiv.org/pdf/2402.12058v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric**<br />
**Title_cn:** 使用特定领域的基于余弦的度量对胸部 X 射线报告进行语义文本相似性评估<br />
**Authors:** Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier<br />
**Abstract:** <details><summary>原文: </summary>Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth. Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts. By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学语言处理和深度学习技术已成为改善医疗保健的关键工具，特别是在医学成像和医学文本数据的分析方面。这些多模态数据融合技术有助于改善医学成像的解释，提高诊断准确性、明智的临床决策并改善患者的治疗结果。这些模型的成功依赖于从临床文本中提取和整合语义信息的能力。本文解决了对更强大的方法来评估医疗报告语义内容的需求。传统的自然语言处理方法和指标最初是为了考虑自然语言领域和机器翻译中的语义上下文而设计的，通常无法捕获医学内容固有的复杂语义。在这项研究中，我们引入了一种专门为评估生成的医疗报告与真实情况之间的语义相似性而设计的新颖方法。我们的方法经过验证，证明了其在评估医学上下文中特定领域语义相似性方面的效率。通过将我们的指标应用于最先进的胸部 X 射线报告生成模型，我们获得的结果不仅与传统指标一致，而且还在所考虑的医学领域提供了更具上下文意义的分数。</details>
**PDF:** <http://arxiv.org/pdf/2402.11908v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios**<br />
**Title_cn:** 揭开深度：应对挑战性场景的多模态融合框架<br />
**Authors:** Jialei Xu, Xianming Liu, Junjun Jiang, Kui Jiang, Rui Li, Kai Cheng, Xiangyang Ji<br />
**Abstract:** <details><summary>原文: </summary>Monocular depth estimation from RGB images plays a pivotal role in 3D vision. However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions. While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image. Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources. To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework. Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality. As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas. With the resulting confidence map, we propose a multi-modal fusion network that fuses the final depth in an end-to-end manner. Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>RGB 图像的单目深度估计在 3D 视觉中发挥着关键作用。然而，在夜间或恶劣天气条件等具有挑战性的环境中，其准确性可能会下降。虽然长波红外相机可以在如此具有挑战性的条件下提供稳定的成像，但它们本质上分辨率较低，缺乏 RGB 图像所提供的丰富纹理和语义。由于难以识别和整合来自两个来源的忠实深度线索，当前的方法仅关注单一模式。为了解决这些问题，本文提出了一种新颖的方法，该方法可以识别主要的跨模态深度特征并将其与基于学习的框架相集成。具体来说，我们通过充分利用每种模态的单独深度线索，使用单独的网络独立计算粗略深度图。由于有利的深度分布在两种模式中，我们提出了一种新颖的置信损失，引导置信预测器网络产生指定潜在深度区域的置信图。根据得到的置信图，我们提出了一种多模态融合网络，以端到端的方式融合最终深度。利用所提出的管道，我们的方法展示了在各种困难场景中稳健的深度估计的能力。在具有挑战性的 MS$^2$ 和 ViViD++ 数据集上的实验结果证明了我们方法的有效性和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11826v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion**<br />
**Title_cn:** MM-SurvNet：通过多模态数据融合进行基于深度学习的乳腺癌生存风险分层<br />
**Authors:** Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering<br />
**Abstract:** <details><summary>原文: </summary>Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.</details>
**Abstract_cn:** <details><summary>译文: </summary>生存风险分层是乳腺癌管理临床决策的重要一步。为此，我们通过整合组织病理学成像、遗传和临床数据提出了一种新颖的深度学习方法。它采用视觉转换器（特别是 MaxViT 模型）进行图像特征提取，并使用自我注意力来捕获患者层面的复杂图像关系。双交叉注意力机制将这些特征与遗传数据融合，而临床数据则被纳入最后一层以提高预测准确性。在公共 TCGA-BRCA 数据集上的实验表明，我们的模型使用负对数似然损失函数进行训练，可以实现平均 C 指数为 0.64 的卓越性能，超越现有方法。这一进步有助于制定量身定制的治疗策略，有可能改善患者的治疗结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.11788v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis**<br />
**Title_cn:** 二元不透明度网格：捕获精细的几何细节以进行基于网格的视图合成<br />
**Authors:** Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, Andreas Geiger<br />
**Abstract:** <details><summary>原文: </summary>While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene's geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a "fuzzy" manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然基于表面的视图合成算法由于计算要求低而颇具吸引力，但它们通常难以再现薄结构。相比之下，将场景几何模型建模为体积密度场的更昂贵的方法（例如 NeRF）擅长重建精细的几何细节。然而，密度场通常以“模糊”方式表示几何形状，这阻碍了表面的精确定位。在这项工作中，我们修改密度场以鼓励它们向表面汇聚，而不损害它们重建薄结构的能力。首先，我们采用离散不透明度网格表示而不是连续密度场，这允许不透明度值在表面从零不连续地过渡到一。其次，我们通过为每个像素投射多条光线来消除锯齿，这允许在不使用半透明体素的情况下对遮挡边界和子像素结构进行建模。第三，我们最小化不透明度值的二元熵，这通过鼓励不透明度值在训练结束时二值化来促进表面几何形状的提取。最后，我们开发了一种基于融合的网格划分策略，然后进行网格简化和外观模型拟合。我们的模型生成的紧凑网格可以在移动设备上实时渲染，并且与现有的基于网格的方法相比，可以实现显着更高的视图合成质量。</details>
**PDF:** <http://arxiv.org/pdf/2402.12377v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Colorizing Monochromatic Radiance Fields**<br />
**Title_cn:** 对单色辐射场进行着色<br />
**Authors:** Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi<br />
**Abstract:** <details><summary>原文: </summary>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: https://liquidammonia.github.io/color-nerf.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管神经辐射场 (NeRF) 可以通过使用一组 2D 图像来生成世界的彩色 3D 表示，但当仅提供单色图像时，这种能力就变得不存在。由于颜色对于表示世界是必要的，因此从单色辐射场再现颜色变得至关重要。为了实现这一目标，我们不直接操作单色辐射场，而是将其视为 Lab 颜色空间中的表示预测任务。通过首先使用单色图像构建亮度和密度表示，我们的预测阶段可以基于图像着色模块重新创建颜色表示。然后，我们通过亮度、密度和颜色的表示来重现彩色隐式模型。已经进行了大量的实验来验证我们方法的有效性。我们的项目页面：https://liquidammonia.github.io/color-nerf。</details>
**PDF:** <http://arxiv.org/pdf/2402.12184v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Interpretable Embedding for Ad-hoc Video Search**<br />
**Title_cn:** 用于临时视频搜索的可解释嵌入<br />
**Authors:** Jiaxin Wu, Chong-Wah Ngo<br />
**Abstract:** <details><summary>原文: </summary>Answering query with semantic concepts has long been the mainstream approach for video search. Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos. Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation. This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning. In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content. This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid benchmarked datasets. Concepts are not only effective in pruning false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>用语义概念回答查询长期以来一直是视频搜索的主流方法。直到最近，它的性能还被无概念方法超越，该方法将查询作为视频嵌入到联合空间中。然而，嵌入的功能以及搜索结果是不可解释的，阻碍了视频浏览和查询重新制定的后续步骤。本文将特征嵌入和概念解释集成到神经网络中，以实现统一的双任务学习。通过这种方式，嵌入与语义概念列表相关联，作为视频内容的解释。本文凭经验证明，通过使用嵌入特征或概念，可以在 TRECVid 基准数据集上实现相当大的搜索改进。概念不仅可以有效地修剪误报视频，而且与无概念搜索高度互补，与最先进的方法相比，有很大的改进空间。</details>
**PDF:** <http://arxiv.org/pdf/2402.11812v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment**<br />
**Title_cn:** 用于非 GPS 战场环境中地标识别和移动节点定位的地标立体数据集<br />
**Authors:** Ganesh Sapkota, Sanjay Madria<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1. We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node. In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新策略，使用地标锚节点代替基于无线电的锚节点来获取移动部队或防御部队的虚拟坐标（landmarkID，DISTANCE），这将有助于沿线跟踪和机动部队无法使用 GPS 的战场环境中的安全路径。所提出的策略使用 Yolov5 模型实现地标识别，并使用高效的立体匹配算法实现地标距离估计。我们认为，携带低功耗移动设备的移动节点借助校准的立体视觉相机来捕获战场区域内包含地标的场景的立体图像，这些地标的位置存储在驻留在设备本身内的离线服务器中。我们创建了一个名为 MSTLandmarkv1 的自定义地标图像数据集，其中包含 34 个地标类别，以及这 34 个地标实例的另一个地标立体数据集，称为 MSTLandmarkStereov1。我们使用 MSTLandmarkv1 数据集训练 YOLOv5 模型，并实现了 0.95 mAP @ 0.5 IoU 和 0.767 mAP @ [0.5: 0.95] IoU。我们利用边界框坐标和使用 MSTLandmarkStereov1 的改进 SGM 算法生成的深度图来计算从节点到地标的距离。将检测结果得到的地标ID元组和SGM算法计算出的距离存储为节点的虚拟坐标。在未来的工作中，我们将利用这些虚拟坐标，通过高效的三边测量算法来获取节点的位置，并使用适当的优化方法来优化节点位置。</details>
**PDF:** <http://arxiv.org/pdf/2402.12320v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking**<br />
**Title_cn:** UncertaintyTrack：利用多目标跟踪中的检测和定位不确定性<br />
**Authors:** Chang Won Lee, Steven L. Waslander<br />
**Abstract:** <details><summary>原文: </summary>Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack</details>
**Abstract_cn:** <details><summary>译文: </summary>由于研究界的强烈兴趣和目标检测方法的稳步改进，多目标跟踪（MOT）方法最近性能显着提升。大多数跟踪方法遵循检测跟踪（TBD）范例，盲目地信任传入的检测，而不了解其相关的定位不确定性。缺乏不确定性意识会给自动驾驶等安全关键任务带来问题，乘客可能会因为错误检测传播到下游任务（包括 MOT）而面临风险。虽然概率对象检测方面的现有工作可以预测盒子周围的定位不确定性，但自动驾驶的 2D MOT 方面还没有研究这些估计是否足够有意义，可以在对象跟踪中有效利用。我们引入了 UncertaintyTrack，这是一个扩展集合，可应用于多个 TBD 跟踪器，以解释概率对象检测器的定位不确定性估计。 Berkeley Deep Drive MOT 数据集上的实验表明，我们的方法和信息不确定性估计的结合将 ID 切换的数量减少了约 19%，并将 mMOTA 提高了 2-3%。源代码位于 https://github.com/TRAILab/UncertaintyTrack</details>
**PDF:** <http://arxiv.org/pdf/2402.12303v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Zero shot VLMs for hate meme detection: Are we there yet?**<br />
**Title_cn:** 用于仇恨模因检测的零样本 VLM：我们到了吗？<br />
**Authors:** Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee<br />
**Abstract:** <details><summary>原文: </summary>Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>社交媒体上的多媒体内容正在迅速发展，模因作为一种独特的形式日益受到重视。不幸的是，一些恶意用户利用模因来针对个人或弱势社区，因此必须识别和解决此类仇恨模因的实例。为了通过开发仇恨模因检测模型来解决这个问题，人们进行了广泛的研究。然而，传统机器/深度学习模型的一个显着限制是需要标记数据集才能进行准确分类。最近，研究界见证了几种视觉语言模型的出现，这些模型在各种任务中表现出了出色的性能。在这项研究中，我们的目标是研究这些视觉语言模型在处理复杂任务（例如仇恨模因检测）方面的功效。我们使用各种提示设置来专注于仇恨/有害模因的零样本分类。通过我们的分析，我们观察到大型 VLM 仍然容易受到零样本仇恨模因检测的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.12198v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers**<br />
**Title_cn:** 使用双向交叉注意力变压器感知更长的序列<br />
**Authors:** Markus Hiller, Krista A. Ehinger, Tom Drummond<br />
**Abstract:** <details><summary>原文: </summary>We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (`what') and location (`where') to develop alongside each other over multiple layers -- allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的双向 Transformer 架构（BiXT），它在计算成本和内存消耗方面随输入大小线性扩展，但不会遭受其他高效的基于 Transformer 的方法所见的性能下降或仅限于一种输入模式的影响。 BiXT 受到感知器架构的启发，但用高效的双向交叉注意力模块取代了迭代注意力，其中输入标记和潜在变量同时相互关注，利用两者之间自然出现的注意力对称性。这种方法解锁了类感知器架构所经历的关键瓶颈，并使语义（“什么”）和位置（“哪里”）的处理和解释能够在多个层上并行开发——允许其直接应用于密集和基于实例的任务类似。通过将效率与完整 Transformer 架构的通用性和性能相结合，BiXT 可以处理更长的序列，如点云或更高特征分辨率的图像，并在点云部分分割、语义图像分割和图像分类等一系列任务中实现具有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.12138v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization**<br />
**Title_cn:** 通过基于梯度的目标定位实现可解释的激光雷达点云语义分割<br />
**Authors:** Abhishek Kuriyal, Vaibhav Kumar<br />
**Abstract:** <details><summary>原文: </summary>Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at https://github.com/geoai4cities/pGS-CAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>LiDAR 点云的语义分割 (SS) 对于城市规划和自动驾驶等许多应用至关重要。虽然在解释图像的 SS 预测方面已经取得了很大进展，但解释点云 SS 预测仍然是一个挑战。本文介绍了 pGS-CAM，这是一种基于梯度的新型方法，用于在神经网络激活层中生成显着图。受 Grad-CAM（使用梯度来突出局部重要性）的启发，pGS-CAM 在各种数据集（SemanticKITTI、Paris-Lille3D、DALES）和 3D 深度学习架构（KPConv、RandLANet）上稳健且有效。我们的实验表明，pGS-CAM 通过突出每个点的贡献，有效地强调了 SS 架构中间激活中的特征学习。这使我们能够更好地了解 SS 模型如何进行预测并确定潜在的改进领域。相关代码可在https://github.com/geoai4cities/pGS-CAM获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12098v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **ISCUTE: Instance Segmentation of Cables Using Text Embedding**<br />
**Title_cn:** ISCUTE：使用文本嵌入对电缆进行实例分割<br />
**Authors:** Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro<br />
**Abstract:** <details><summary>原文: </summary>In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在机器人和自动化领域，传统的对象识别和实例分割方法在感知电线、电缆和软管等可变形线性对象（DLO）时面临着巨大的挑战。这一挑战主要源于缺乏形状、颜色和纹理等独特属性，这就需要量身定制的解决方案来实现精确识别。在这项工作中，我们提出了一种基于基础模型的 DLO 实例分割技术，该技术是文本提示且用户友好的。具体来说，我们的方法结合了 CLIPSeg 模型的文本条件语义分割功能和分段任意模型 (SAM) 的零样本泛化功能。我们证明我们的方法在 DLO 实例分割上超过了 SOTA 性能，达到了 $91.21\%$ 的 mIoU。我们还引入了丰富多样的 DLO 特定数据集用于实例分割。</details>
**PDF:** <http://arxiv.org/pdf/2402.11996v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling**<br />
**Title_cn:** 具有可微分 ROI 建议网络和软 ROI 池化的胸部 X 光弱监督对象检测<br />
**Authors:** Philip Müller, Felix Meissen, Georgios Kaissis, Daniel Rueckert<br />
**Abstract:** <details><summary>原文: </summary>Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn</details>
**Abstract_cn:** <details><summary>译文: </summary>弱监督对象检测 (WSup-OD) 提高了图像分类算法的实用性和可解释性，而无需额外的监督。然而，由于自然图像的对象特征（即病理）非常不同，多实例学习在自然图像任务中的成功并不能很好地转化为医学图像。在这项工作中，我们提出了弱监督 ROI 建议网络（WSRPN），这是一种使用专门的兴趣区域（ROI-注意）模块动态生成边界框建议的新方法。 WSRPN 与经典的骨干头部分类算法很好地集成，并且仅通过图像标签监督即可进行端到端训练。我们通过实验证明，我们的新方法在胸部 X 射线图像中疾病定位这一具有挑战性的任务中优于现有方法。代码：https://github.com/philip-mueller/wsrpn</details>
**PDF:** <http://arxiv.org/pdf/2402.11985v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Event-Based Motion Magnification**<br />
**Title_cn:** 基于事件的运动放大<br />
**Authors:** Yutian Chen, Shi Guo, Fangzheng Yu, Feng Zhang, Jinwei Gu, Tianfan Xue<br />
**Abstract:** <details><summary>原文: </summary>Detecting and magnifying imperceptible high-frequency motions in real-world scenarios has substantial implications for industrial and medical applications. These motions are characterized by small amplitudes and high frequencies. Traditional motion magnification methods rely on costly high-speed cameras or active light sources, which limit the scope of their applications. In this work, we propose a dual-camera system consisting of an event camera and a conventional RGB camera for video motion magnification, containing temporally-dense information from the event stream and spatially-dense data from the RGB images. This innovative combination enables a broad and cost-effective amplification of high-frequency motions. By revisiting the physical camera model, we observe that estimating motion direction and magnitude necessitates the integration of event streams with additional image features. On this basis, we propose a novel deep network for event-based video motion magnification that addresses two primary challenges: firstly, the high frequency of motion induces a large number of interpolated frames (up to 80), which our network mitigates with a Second-order Recurrent Propagation module for better handling of long-term frame interpolations; and secondly, magnifying subtle motions is sensitive to noise, which we address by utilizing a temporal filter to amplify motion at specific frequencies and reduce noise impact. We demonstrate the effectiveness and accuracy of our dual-camera system and network through extensive experiments in magnifying small-amplitude, high-frequency motions, offering a cost-effective and flexible solution for motion detection and magnification.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测和放大现实场景中难以察觉的高频运动对工业和医疗应用具有重大影响。这些运动的特点是振幅小、频率高。传统的运动放大方法依赖于昂贵的高速相机或主动光源，这限制了其应用范围。在这项工作中，我们提出了一种双摄像头系统，由事件摄像头和传统 RGB 摄像头组成，用于视频运动放大，包含来自事件流的时间密集信息和来自 RGB 图像的空间密集数据。这种创新组合能够广泛且经济高效地放大高频运动。通过重新审视物理相机模型，我们发现估计运动方向和幅度需要将事件流与附加图像特征集成。在此基础上，我们提出了一种用于基于事件的视频运动放大的新型深度网络，该网络解决了两个主要挑战：首先，高频率的运动会产生大量的插值帧（最多 80 个），我们的网络通过第二个方法来缓解这种情况-顺序循环传播模块，以便更好地处理长期帧插值；其次，放大细微的运动对噪声很敏感，我们通过利用时间滤波器来放大特定频率的运动并减少噪声影响来解决这个问题。我们通过放大小幅度、高频运动的大量实验证明了双摄像头系统和网络的有效性和准确性，为运动检测和放大提供了经济高效且灵活的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.11957v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Separating common from salient patterns with Contrastive Representation Learning**<br />
**Title_cn:** 通过对比表征学习区分常见模式和显着模式<br />
**Authors:** Robin Louiset, Edouard Duchesnay, Antoine Grigis, Pietro Gori<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations. On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.). In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis. We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning. Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis. Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比分析是表征学习的一个子领域，旨在将两个数据集（背景（即健康受试者）和目标（即患病受试者））之间的共同变异因素与仅存在于中的显着变异因素分开。目标数据集。尽管它们具有相关性，但当前基于变分自动编码器的模型在学习语义表达表示方面表现不佳。另一方面，对比表示学习在各种应用（分类、聚类等）中表现出了巨大的性能飞跃。在这项工作中，我们建议利用对比学习的能力来学习非常适合对比分析的语义表达表示。我们根据 InfoMax 原则重新表述它，并确定两个互信息项最大化和一个最小化。我们将前两项分解为对齐项和均匀性项，就像对比学习中通常所做的那样。然后，我们提出了一种新颖的互信息最小化策略，以防止常见分布和显着分布之间的信息泄漏。我们在三个视觉数据集和三个医学数据集上验证了我们的方法 SepCLR，这些数据集专门用于评估对比分析中的模式分离能力。代码可在 https://github.com/neurospin-projects/2024_rlouiset_sep_clr 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11928v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Modularized Networks for Few-shot Hateful Meme Detection**<br />
**Title_cn:** 用于少量仇恨模因检测的模块化网络<br />
**Authors:** Rui Cao, Roy Ka-Wei Lee, Jing Jiang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们解决了在资源匮乏的环境中检测仇恨模因的挑战，其中只有少数标记的示例可用。我们的方法利用了低秩自适应（LoRA）的组合性，这是一种广泛使用的参数高效调整技术。我们首先使用 LoRA 对与仇恨模因检测相关的选定任务进行微调，从而生成一套 LoRA 模块。这些模块具有用于检测仇恨模因的基本推理技能。然后，我们使用少数可用的带注释样本来训练模块编辑器，该模块根据 LoRA 模块的相关性为其分配权重。该模型的可学习参数与 LoRA 模块的数量成正比。这种模块化网络以法学硕士为基础，并通过 LoRA 模块进行增强，在仇恨模因检测的背景下表现出增强的泛化能力。我们的评估涵盖了三个数据集，这些数据集专为在几次学习环境中检测仇恨模因而设计。所提出的方法表现出比传统上下文学习更优越的性能，而传统的上下文学习在推理过程中的计算量也更大。然后，我们使用少数可用的带注释样本来训练模块编辑器，该模块根据 LoRA 模块的相关性为它们分配权重。该模型的可学习参数与 LoRA 模块的数量成正比。这种模块化网络以法学硕士为基础，并通过 LoRA 模块进行增强，在仇恨模因检测的背景下表现出增强的泛化能力。我们的评估涵盖了三个数据集，这些数据集专为在几次学习环境中检测仇恨模因而设计。所提出的方法表现出优于传统上下文学习的性能，而传统的上下文学习在推理过程中的计算量也更大。</details>
**PDF:** <http://arxiv.org/pdf/2402.11845v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Rock Classification Based on Residual Networks**<br />
**Title_cn:** 基于残差网络的岩石分类<br />
**Authors:** Sining Zhoubian, Yuyang Wang, Zhihuan Jiang<br />
**Abstract:** <details><summary>原文: </summary>Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using convolutional neural networks is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt data augmentation methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck transformer blocks may influence model performance. We discover that models with more than one bottleneck transformer block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>岩石分类是一个重要的地质问题，因为它提供了重要的地层信息。然而，使用卷积神经网络对这个问题的探索还不够。为了解决这个问题，我们提出了两种使用残差神经网络的方法。我们首先采用数据增强方法来扩大我们的数据集。通过在 ResNet34 的基础上修改核大小、归一化方法和组合，我们在测试数据集上实现了 70.1% 的准确率，与常规 Resnet34 相比提高了 3.5%。此外，使用像 BotNet 这样的类似主干网，它包含多头自注意力，我们还在模型中使用内部残差连接。这提高了模型的性能，在测试数据集上实现了 73.7% 的准确率。我们还探讨了瓶颈变压器块的数量如何影响模型性能。我们发现具有多个瓶颈变压器块的模型可能无法进一步提高性能。最后，我们相信我们的方法可以激发与该问题相关的未来工作，并且我们的模型设计可以促进新残差模型架构的开发。</details>
**PDF:** <http://arxiv.org/pdf/2402.11831v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **SDGE: Stereo Guided Depth Estimation for 360° Camera Sets**<br />
**Title_cn:** SDGE：360° 相机组的立体引导深度估计<br />
**Authors:** Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji<br />
**Abstract:** <details><summary>原文: </summary>Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\deg} perception. These 360{\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\deg} cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度估计是自动驾驶中的一项关键技术，通常使用多摄像头系统来实现 360{\deg} 感知。这些 360{\deg} 相机组通常具有有限或低质量的重叠区域，使得多视图立体方法不适用于整个图像。或者，单目方法可能无法产生一致的跨视图预测。为了解决这些问题，我们提出了立体引导深度估计（SGDE）方法，该方法通过明确利用重叠上的多视图立体结果来增强整个图像的深度估计。我们建议构建虚拟针孔相机来解决鱼眼相机的畸变问题，并统一两种类型的360{\deg}相机的处理。为了处理不稳定运动引起的相机位姿变化噪声，该方法采用自校准方法来获得具有较小重叠的相邻相机的高精度相对位姿。这些使得能够使用鲁棒的立体方法来在重叠区域中获得高质量的先验深度。该先验不仅用作附加输入，还用作伪标签，增强深度估计方法的准确性并提高跨视图预测的一致性。 SGDE 的有效性在一个鱼眼相机数据集 Synthetic Urban 和两个针孔相机数据集 DDAD 和 nuScenes 上进行评估。我们的实验表明，SGDE 对于监督和自监督深度估计都是有效的，并凸显了我们的方法在推进下游自动驾驶技术（例如 3D 对象检测和占用预测）方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.11791v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model**<br />
**Title_cn:** FOD-Swin-Net：使用基于变压器的深度模型的纤维取向分布的角度超分辨率<br />
**Authors:** Mateus Oliveira da Silva, Caio Pinheiro Santana, Diedre Santos do Carmo, Letícia Rittner<br />
**Abstract:** <details><summary>原文: </summary>Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required directions on initial acquisition. Evaluations of the reconstructed FOD with Angular Correlation Coefficient and qualitative visualizations reveal superior performance than the state-of-the-art in HCP testing data. Open source code for reproducibility is available at https://github.com/MICLab-Unicamp/FOD-Swin-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>识别和表征脑纤维束可以帮助了解许多疾病和病症。此过程中的一个重要步骤是使用扩散加权磁共振成像 (DW-MRI) 估计纤维方向。然而，获得可靠的方向估计需要高分辨率数据，导致采集时间较长，而临床上并不总是可用。在这项工作中，我们探索使用来自更快采集的自动角度超分辨率来克服这一挑战。使用公开的人类连接组计划 (HCP) DW-MRI 数据，我们训练了基于变压器的深度学习架构，以实现纤维取向分布 (FOD) 的角度超分辨率。我们基于补丁的方法 FOD-Swin-Net 能够使从 32 个方向驱动的单壳重建与多壳 288 方向 FOD 重建相当，从而大大减少初始采集时所需方向的数量。使用角度相关系数和定性可视化对重建的 FOD 进行评估，显示出比最先进的 HCP 测试数据更优越的性能。可重复性的开源代码可在 https://github.com/MICLab-Unicamp/FOD-Swin-Net 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11775v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation**<br />
**Title_cn:** 强化学习作为预测级联的简约替代方案：图像分割的案例研究<br />
**Authors:** Bharat Srikishan, Anika Tabassum, Srikanth Allu, Ramakrishnan Kannan, Nikhil Muralidhar<br />
**Abstract:** <details><summary>原文: </summary>Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser .</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习架构在目标检测和图像分割等计算机视觉任务上实现了最先进的 (SOTA) 性能。这可能归因于使用了在大型数据集上执行的过度参数化的整体深度学习架构。尽管这种架构可以提高准确性，但这通常伴随着推理过程中计算和内存需求的大幅增加。虽然这在传统机器学习管道中不是问题，但最近机器学习和物联网等领域的融合使得如此大型的架构无法在资源匮乏的环境中执行。在这种情况下，之前的工作提出了决策级联，其中输入通过复杂性不断增加的模型传递，直到实现所需的性能。然而，我们认为级联预测由于浪费中间计算而导致计算成本增加。为了解决这个问题，我们提出 PaSeR（强化学习简约分割），一种非级联、成本感知的学习管道，作为级联架构的替代方案。通过对现实世界和标准数据集的实验评估，我们证明 PaSeR 实现了更好的准确性，同时相对于级联模型最小化了计算成本。此外，我们引入了一个新的指标 IoU/GigaFlop 来评估成本和性能之间的平衡。在电池材料相分割的实际任务中，PaSeR 在 IoU/GigaFlop 指标上相对于基线的性能至少提高了 174%。我们还展示了 PaSeR 对在噪声 MNIST 数据集上训练的互补模型的适应性，与 SOTA 模型相比，它在 IoU/GigaFlop 上实现了 13.4% 的最低性能改进。代码和数据可在 https://github.com/scailab/paser 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11760v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Evaluating Image Review Ability of Vision Language Models**<br />
**Title_cn:** 评估视觉语言模型的图像审查能力<br />
**Authors:** Shigeki Saito, Kazuki Hayashi, Yusuke Ide, Yusuke Sakai, Kazuma Onishi, Toma Suzuki, Seiji Gobara, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe<br />
**Abstract:** <details><summary>原文: </summary>Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模视觉语言模型（LVLM）是能够通过单个模型处理图像和文本输入的语言模型。本文探讨了使用 LVLM 生成图像评论文本。 LVLM 审查图像的能力尚未完全了解，这凸显了对其审查能力进行系统评估的必要性。与图像说明不同，评论文本可以从图像构图和曝光等多种角度进行撰写。评论视角的多样性使得很难唯一地确定对图像的单一正确评论。为了应对这一挑战，我们引入了一种基于排名相关分析的评估方法，其中评论文本由人类和 LVLM 进行排名，然后测量这些排名之间的相关性。我们通过创建一个旨在评估最近 LVLM 的图像审查能力的基准数据集来进一步验证这种方法。我们对数据集的实验表明，LVLM，特别是那些在其他评估环境中已证明具有优越性的 LVLM，擅长区分高质量和不合格的图像评论。</details>
**PDF:** <http://arxiv.org/pdf/2402.12121v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models**<br />
**Title_cn:** 内窥镜凿子：术中成像雕刻 3D 解剖模型<br />
**Authors:** Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath<br />
**Abstract:** <details><summary>原文: </summary>Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTs offer patient-specific insights of complex anatomy, enabling real-time intraoperative navigation to complement endoscopy imaging. However, surgery elicits anatomical changes not represented in the preoperative model, generating an inaccurate basis for navigation during surgery progression.   Methods: We propose a first vision-based approach to update the preoperative 3D anatomical model leveraging intraoperative endoscopic video for navigated sinus surgery where relative camera poses are known. We rely on comparisons of intraoperative monocular depth estimates and preoperative depth renders to identify modified regions. The new depths are integrated in these regions through volumetric fusion in a truncated signed distance function representation to generate an intraoperative 3D model that reflects tissue manipulation.   Results: We quantitatively evaluate our approach by sequentially updating models for a five-step surgical progression in an ex vivo specimen. We compute the error between correspondences from the updated model and ground-truth intraoperative CT in the region of anatomical modification. The resulting models show a decrease in error during surgical progression as opposed to increasing when no update is employed.   Conclusion: Our findings suggest that preoperative 3D anatomical models can be updated using intraoperative endoscopy video in navigated sinus surgery. Future work will investigate improvements to monocular depth estimation as well as removing the need for external navigation systems. The resulting ability to continuously update the patient model may provide surgeons with a more precise understanding of the current anatomical state and paves the way toward a digital twin paradigm for sinus surgery.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：术前成像在鼻窦手术中发挥着关键作用，CT 可以为患者提供复杂解剖结构的具体见解，从而实现实时术中导航以补充内窥镜成像。然而，手术会引起术前模型中未体现的解剖学变化，从而在手术进展过程中产生不准确的导航基础。方法：我们提出了第一种基于视觉的方法来更新术前 3D 解剖模型，利用术中内窥镜视频进行导航鼻窦手术，其中相对相机姿势已知。我们依靠术中单眼深度估计和术前深度渲染的比较来识别修改区域。新的深度通过截断符号距离函数表示中的体积融合集成在这些区域中，以生成反映组织操作的术中 3D 模型。结果：我们通过连续更新离体标本中五步手术进展的模型来定量评估我们的方法。我们计算更新模型与解剖修改区域的真实术中 CT 之间的对应误差。由此产生的模型显示，在手术进展过程中误差有所减少，而不是在不进行更新时误差增加。结论：我们的研究结果表明，在导航鼻窦手术中可以使用术中内窥镜视频更新术前 3D 解剖模型。未来的工作将研究单目深度估计的改进以及消除对外部导航系统的需求。由此产生的持续更新患者模型的能力可以使外科医生更准确地了解当前的解剖状态，并为鼻窦手术的数字孪生范式铺平道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.11840v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships**<br />
**Title_cn:** Open3DSG：来自点云的开放词汇 3D 场景图，具有可查询对象和开放集关系<br />
**Authors:** Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, Timo Ropinski<br />
**Abstract:** <details><summary>原文: </summary>Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的 3D 场景图预测方法依赖于标记数据集来训练一组固定的已知对象类和关系类别的模型。我们提出了 Open3DSG，这是一种在开放世界中学习 3D 场景图预测的替代方法，无需标记场景图数据。我们将 3D 场景图预测主干的特征与强大的开放世界 2D 视觉语言基础模型的特征空间共同嵌入。这使我们能够以零样本的方式从 3D 点云预测 3D 场景图，方法是从开放词汇表中查询对象类，并从具有场景图特征和查询对象类作为上下文的扎根 LLM 中预测对象间关系。 Open3DSG 是第一个 3D 点云方法，不仅可以预测显式的开放词汇对象类，还可以预测不限于预定义标签集的开放集关系，从而可以表达罕见以及特定的对象和关系。预测的 3D 场景图。我们的实验表明，Open3DSG 可以有效地预测任意对象类别及其描述空间、支持、语义和比较关系的复杂对象间关系。</details>
**PDF:** <http://arxiv.org/pdf/2402.12259v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **A Lightweight Parallel Framework for Blind Image Quality Assessment**<br />
**Title_cn:** 一种轻量级并行盲图像质量评估框架<br />
**Authors:** Qunyue Huang, Bin Fang<br />
**Abstract:** <details><summary>原文: </summary>Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer. In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的盲图像质量评估（BIQA）方法侧重于设计基于卷积神经网络（CNN）或变压器的复杂网络。此外，一些BIQA方法以两阶段训练的方式增强模型的性能。尽管取得了显着的进步，但这些方法显着增加了模型的参数数量，因此需要更多的训练时间和计算资源。为了解决上述问题，我们提出了一种用于 BIQA 的轻量级并行框架（LPF）。首先，我们使用预先训练的特征提取网络提取视觉特征。此外，我们构建了一个简单而有效的特征嵌入网络（FEN）来转换视觉特征，旨在生成包含显着失真信息的潜在表示。为了提高潜在表示的鲁棒性，我们提出了两个新颖的自监督子任务，包括样本级类别预测任务和批次级质量比较任务。提出样本级类别预测任务来帮助模型进行粗粒度的失真感知。制定批次级质量比较任务是为了增强训练数据，从而提高潜在表示的鲁棒性。最后，潜在表示被输入失真感知质量回归网络（DaQRN），该网络模拟人类视觉系统（HVS），从而生成准确的质量分数。多个基准数据集的实验结果表明，所提出的方法比最先进的方法具有更优越的性能。此外，大量分析证明该方法具有较低的计算复杂度和更快的收敛速度。</details>
**PDF:** <http://arxiv.org/pdf/2402.12043v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenge**<br />
**Title_cn:** 计算机视觉和仿真中的环视鱼眼光学器件：调查和挑战<br />
**Authors:** Daniel Jakab, Brian Michael Deegan, Sushil Sharma, Eoin Martino Grua, Jonathan Horgan, Enda Ward, Pepijn Van De Ven, Anthony Scanlan, Ciaran Eising<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS. The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality. When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle's surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing. However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature. Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation. The industry has turned to simulation as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery. We examine different simulation methods (such as model-driven and data-driven simulations) and discuss the simulators' ability (or lack thereof) to model real-world optical performance. Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们对汽车环视鱼眼光学进行了调查，重点关注光学伪影对自动驾驶和 ADAS 中计算机视觉任务的影响。汽车行业在应用最先进的计算机视觉来增强道路安全并提供自动驾驶功能方面取得了进步。在车辆上使用摄像头系统时，特别需要宽视野来捕捉整个车辆的周围环境，例如低速操纵、自动停车和茧感测等领域。然而，环视相机面临的一个关键挑战是鱼眼相机的强烈光学像差，这一领域在文献中很少受到关注。此外，还需要一个全面的数据集来测试车辆自动化中的安全关键场景。该行业已将模拟作为一种经济有效的策略，用于创建具有环视摄像机图像的合成数据集。我们研究不同的模拟方法（例如模型驱动和数据驱动模拟），并讨论模拟器模拟真实世界光学性能的能力（或缺乏能力）。总的来说，本文强调了汽车鱼眼数据集中的光学像差，以及模拟鱼眼数据集中光学现实的局限性，重点关注环视光学系统中的计算机视觉。</details>
**PDF:** <http://arxiv.org/pdf/2402.12041v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization**<br />
**Title_cn:** AICAtack：基于注意力优化的对抗性图像字幕攻击<br />
**Authors:** Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习研究的最新进展在计算机视觉（CV）和自然语言处理（NLP）的许多任务中取得了显着的成就。 CV 和 NLP 的交叉点是图像描述问题，其中相关模型针对对抗性攻击的鲁棒性尚未得到很好的研究。在本文中，我们提出了一种新颖的对抗性攻击策略，称为 AICAtack（基于注意力的图像字幕攻击），旨在通过对图像的微妙扰动来攻击图像字幕模型。在黑盒攻击场景中运行，我们的算法不需要访问目标模型的架构、参数或梯度信息。我们引入了一种基于注意力的候选选择机制，该机制可识别要攻击的最佳像素，然后采用差分进化（DE）来扰动像素的 RGB 值。我们通过对具有多个受害者模型的基准数据集进行广泛的实验来证明 AICAtack 的有效性。实验结果表明，我们的方法通过有效地分布输出中单词的对齐和语义，超越了当前的前沿技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.11940v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training**<br />
**Title_cn:** PhySU-Net：具有自监督预训练的 rPPG 长时态上下文转换器<br />
**Authors:** Marko Savic, Guoying Zhao<br />
**Abstract:** <details><summary>原文: </summary>Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context. Supervised rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training. Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training. Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data.</details>
**Abstract_cn:** <details><summary>译文: </summary>远程光电体积描记法 (rPPG) 是一项很有前途的技术，它通过面部视频非接触式测量心脏活动。最近的方法利用具有有限时间建模能力的卷积网络或忽略长时间上下文。有监督的 rPPG 方法也受到数据稀缺的严重限制。在这项工作中，我们提出了 PhySU-Net，这是第一个长时空图 rPPG 变换网络和一种自监督预训练策略，利用未标记的数据来改进我们的模型。我们的策略利用传统方法和图像掩蔽为自监督预训练提供伪标签。我们的模型在两个公共数据集（OBF 和 VIPL-HR）上进行了测试，并在监督训练中表现出了卓越的性能。此外，我们证明了我们的自我监督预训练策略通过利用从未标记数据中学习到的表示进一步提高了我们模型的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.11913v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Language-guided Image Reflection Separation**<br />
**Title_cn:** 语言引导的图像反射分离<br />
**Authors:** Haofeng Zhong, Yuchen Hong, Shuchen Weng, Jinxiu Liang, Boxin Shi<br />
**Abstract:** <details><summary>原文: </summary>This paper studies the problem of language-guided reflection separation, which aims at addressing the ill-posed reflection separation problem by introducing language descriptions to provide layer content. We propose a unified framework to solve this problem, which leverages the cross-attention mechanism with contrastive learning strategies to construct the correspondence between language descriptions and image layers. A gated network design and a randomized training strategy are employed to tackle the recognizable layer ambiguity. The effectiveness of the proposed method is validated by the significant performance advantage over existing reflection separation methods on both quantitative and qualitative comparisons.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文研究了语言引导的反射分离问题，旨在通过引入语言描述来提供分层内容来解决不适定的反射分离问题。我们提出了一个统一的框架来解决这个问题，它利用交叉注意力机制和对比学习策略来构建语言描述和图像层之间的对应关系。采用门控网络设计和随机训练策略来解决可识别层的模糊性。该方法的有效性通过在定量和定性比较方面优于现有反射分离方法的显着性能优势得到验证。</details>
**PDF:** <http://arxiv.org/pdf/2402.11874v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability**<br />
**Title_cn:** 以容量和可扩展性推动 3D 形状生成的自回归模型<br />
**Authors:** Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations. However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework Argus3D in terms of capacity. Concretely, our approach introduces discrete representation learning based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>自回归模型通过对网格空间中的联合分布进行建模，在二维图像生成方面取得了令人印象深刻的结果。在本文中，我们将自回归模型扩展到3D领域，并通过同时改进自回归模型的容量和可扩展性来寻求更强的3D形状生成能力。首先，我们利用公开的 3D 数据集来促进大型模型的训练。它由大约 900,000 个对象的全面集合组成，具有网格、点、体素、渲染图像和文本标题等多种属性。这个多样化的标记数据集被称为 Objaverse-Mix，使我们的模型能够从各种对象变化中学习。然而，直接应用 3D 自回归遇到了严峻的挑战，即对体积网格的高计算要求以及沿网格维度的自回归顺序不明确，导致 3D 形状质量较差。为此，我们在容量方面提出了一个新颖的框架Argus3D。具体来说，我们的方法引入了基于潜在向量而不是体积网格的离散表示学习，这不仅降低了计算成本，而且还通过以更容易处理的顺序学习联合分布来保留基本的几何细节。因此，可以通过简单地将各种条件输入（例如点云、类别、图像和文本）连接到潜在向量来实现条件生成的能力。此外，由于我们模型架构的简单性，我们自然地将我们的方法扩展到具有令人印象深刻的 36 亿个参数的更大模型，进一步提高了多功能 3D 生成的质量。对四代任务的大量实验表明，Argus3D 可以跨多个类别合成多样化且忠实的形状，取得了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.12225v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Pan-Mamba: Effective pan-sharpening with State Space Model**<br />
**Title_cn:** Pan-Mamba：使用状态空间模型进行有效的全色锐化<br />
**Authors:** Xuanhua He, Ke Cao, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, Man Zhou<br />
**Abstract:** <details><summary>原文: </summary>Pan-sharpening involves integrating information from lowresolution multi-spectral and high-resolution panchromatic images to generate high-resolution multi-spectral counterparts. While recent advancements in the state space model, particularly the efficient long-range dependency modeling achieved by Mamba, have revolutionized computer vision community, its untapped potential in pan-sharpening motivates our exploration. Our contribution, Pan-Mamba, represents a novel pansharpening network that leverages the efficiency of the Mamba model in global information modeling. In Pan-Mamba, we customize two core components: channel swapping Mamba and cross-modal Mamba, strategically designed for efficient cross-modal information exchange and fusion. The former initiates a lightweight cross-modal interaction through the exchange of partial panchromatic and multispectral channels, while the latter facilities the information representation capability by exploiting inherent cross-modal relationships. Through extensive experiments across diverse datasets, our proposed approach surpasses state-of-theart methods, showcasing superior fusion results in pan-sharpening. To the best of our knowledge, this work is the first attempt in exploring the potential of the Mamba model and establishes a new frontier in the pan-sharpening techniques. The source code is available at https://github.com/alexhe101/Pan-Mamba .</details>
**Abstract_cn:** <details><summary>译文: </summary>全色锐化涉及整合来自低分辨率多光谱和高分辨率全色图像的信息以生成高分辨率多光谱对应物。虽然状态空间模型的最新进展，特别是 Mamba 实现的高效远程依赖建模，已经彻底改变了计算机视觉社区，但其在全色锐化方面尚未开发的潜力激发了我们的探索。我们的贡献 Pan-Mamba 代表了一种新颖的全色锐化网络，它利用了 Mamba 模型在全局信息建模中的效率。在 Pan-Mamba 中，我们定制了两个核心组件：渠道交换 Mamba 和跨模态 Mamba，从战略上旨在实现高效的跨模态信息交换和融合。前者通过部分全色和多光谱通道的交换启动轻量级的跨模态交互，而后者则通过利用固有的跨模态关系来促进信息表示能力。通过对不同数据集的广泛实验，我们提出的方法超越了最先进的方法，在全色锐化中展示了卓越的融合结果。据我们所知，这项工作是探索 Mamba 模型潜力的首次尝试，并建立了全色锐化技术的新领域。源代码可在 https://github.com/alexhe101/Pan-Mamba 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12192v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Spatiotemporal Illumination Model for 3D Image Fusion in Optical Coherence Tomography**<br />
**Title_cn:** 光学相干断层扫描中 3D 图像融合的时空照明模型<br />
**Authors:** Stefan Ploner, Jungeun Won, Julia Schottenhamml, Jessica Girgis, Kenneth Lam, Nadia Waheed, James Fujimoto, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>Optical coherence tomography (OCT) is a non-invasive, micrometer-scale imaging modality that has become a clinical standard in ophthalmology. By raster-scanning the retina, sequential cross-sectional image slices are acquired to generate volumetric data. In-vivo imaging suffers from discontinuities between slices that show up as motion and illumination artifacts. We present a new illumination model that exploits continuity in orthogonally raster-scanned volume data. Our novel spatiotemporal parametrization adheres to illumination continuity both temporally, along the imaged slices, as well as spatially, in the transverse directions. Yet, our formulation does not make inter-slice assumptions, which could have discontinuities. This is the first optimization of a 3D inverse model in an image reconstruction context in OCT. Evaluation in 68 volumes from eyes with pathology showed reduction of illumination artifacts in 88\% of the data, and only 6\% showed moderate residual illumination artifacts. The method enables the use of forward-warped motion corrected data, which is more accurate, and enables supersampling and advanced 3D image reconstruction in OCT.</details>
**Abstract_cn:** <details><summary>译文: </summary>光学相干断层扫描 (OCT) 是一种非侵入性微米级成像方式，已成为眼科临床标准。通过光栅扫描视网膜，获取连续的横截面图像切片以生成体积数据。体内成像存在切片之间的不连续性，表现为运动和照明伪影。我们提出了一种新的照明模型，该模型利用正交光栅扫描体数据的连续性。我们新颖的时空参数化在时间上（沿着成像切片）以及空间上（在横向）上都遵循照明连续性。然而，我们的公式没有做出切片间假设，这可能会存在不连续性。这是 OCT 图像重建背景下 3D 逆模型的首次优化。对来自病理学眼睛的 68 个体积的评估显示，88% 的数据中照明伪影减少，只有 6% 的数据显示中度残余照明伪影。该方法能够使用更准确的前向扭曲运动校正数据，并在 OCT 中实现超采样和高级 3D 图像重建。</details>
**PDF:** <http://arxiv.org/pdf/2402.12114v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic**<br />
**Title_cn:** 两种基于层次分析法和模糊逻辑的在线地图匹配算法<br />
**Authors:** Jeremy J. Lin, Tomoro Mochida, Riley C. W. O'Neill, Atsuro Yoshida, Masashi Yamazaki, Akinobu Sasada<br />
**Abstract:** <details><summary>原文: </summary>Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们本文的目的是开发新的地图匹配算法并改进以前的工作。我们提出了两种关键方法：层次分析法（AHP）图匹配和模糊逻辑图匹配。层次分析法是一种将数学分析与人类判断相结合的决策方法，而模糊逻辑是一种基于真实程度的计算方法，旨在对从0到1的不精确推理模式进行建模，而不是通常的布尔逻辑。在这些算法中，我们将层次分析法应用于地图匹配的方法是本文新开发的，同时，我们将模糊逻辑应用于地图匹配的方法除了一些小的变化外与现有的研究基本相同。由于这两种方法的共同特点是处理不精确的信息且实现简单，因此我们决定使用这些方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.11866v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications**<br />
**Title_cn:** DIO：用于机器人和计算机视觉应用的室内物体 3D 网格模型数据集<br />
**Authors:** Nillan Nimal, Wenbin Li, Ronald Clark, Sajad Saeedi<br />
**Abstract:** <details><summary>原文: </summary>The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects. These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene. The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model. Based on the obtained models, this process was effective in modelling the geometry and texture of real-world objects with high fidelity. An active 3D scanner was also utilized to accelerate the process for large objects. All generated models and captured images are made available on the website of the project.</details>
**Abstract_cn:** <details><summary>译文: </summary>创建真实世界物体的准确虚拟模型对于计算机视觉、人工智能和机器学习等机器人模拟和应用至关重要。本文记录了用于生成现实世界对象网格模型数据库的不同方法。这些方法解决了使用 CAD 软件手动生成模型的繁琐且耗时的过程。本质上，数码单反相机/手机摄像头用于获取目标物体的图像。这些图像使用名为 Meshroom 的摄影测量软件进行处理，以生成场景的密集表面重建。 Meshroom 生成的结果使用网格编辑软件 MeshLab 进行编辑和简化，以生成最终模型。基于所获得的模型，该过程可以有效地对现实世界对象的几何和纹理进行高保真度建模。还利用主动 3D 扫描仪来加速大型物体的处理过程。所有生成的模型和捕获的图像都可以在该项目的网站上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.11836v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before**<br />
**Title_cn:** 避免对比学习中的特征抑制：学习以前没有学过的东西<br />
**Authors:** Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, Bryan Hooi<br />
**Abstract:** <details><summary>原文: </summary>Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures.</details>
**Abstract_cn:** <details><summary>译文: </summary>自监督对比学习已成为从未标记数据中获取高质量表示的强大方法。然而，最近在标准对比学习（例如，SimCLR、CLIP）中发现了特征抑制：在单个端到端训练阶段，对比模型仅捕获对比视图中的部分共享信息，而忽略了其他可能有用的信息。通过特征抑制，对比模型通常无法学习能够完成各种下游任务的足够表示。为了缓解特征抑制问题并确保对比模型能够学习全面的表示，我们开发了一种新颖的多阶段对比学习（MCL）框架。与通常导致特征抑制的标准对比学习不同，MCL 逐步学习前一阶段尚未探索的新特征，同时保留已学过的特征。在各种公开可用的基准上进行的广泛实验验证了我们提出的框架的有效性。此外，我们还证明了所提出的 MCL 可以适应各种流行的对比学习主干，并通过学习标准对比学习程序无法获得的特征来提高其性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.11816v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling**<br />
**Title_cn:** 通过去噪进行正则化：贝叶斯模型和 Langevin-within-split Gibbs 采样<br />
**Authors:** Elhadji C. Faye, Mame Diarra Fall, Nicolas Dobigeon<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文通过推导去噪正则化 (RED) 范式的概率对应项，介绍了用于图像反演的贝叶斯框架。它还实现了专门针对基于渐近精确数据增强 (AXDA) 从所得后验分布进行采样而定制的蒙特卡罗算法。所提出的算法是分割吉布斯采样 (SGS) 的近似实例，其中嵌入了一个 Langevin Monte Carlo 步骤。该方法适用于去模糊、修复和超分辨率等常见成像任务，并通过大量数值实验证明了其有效性。这些贡献通过在概率框架内利用数据驱动的正则化策略推进了成像中的贝叶斯推理。</details>
**PDF:** <http://arxiv.org/pdf/2402.12292v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models**<br />
**Title_cn:** DriveVLM：自动驾驶和大型视觉语言模型的融合<br />
**Authors:** Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao<br />
**Abstract:** <details><summary>原文: </summary>A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust spatial understanding and real-time inference speed. Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>城市环境中自动驾驶的主要障碍是理解复杂的长尾场景，例如具有挑战性的道路条件和微妙的人类行为。我们推出 DriveVLM，这是一种利用视觉语言模型 (VLM) 来增强场景理解和规划能力的自动驾驶系统。 DriveVLM 集成了用于场景描述、场景分析和分层规划的思想链 (CoT) 模块的独特组合。此外，认识到 VLM 在空间推理和繁重计算要求方面的局限性，我们提出了 DriveVLM-Dual，这是一种混合系统，可以将 DriveVLM 与传统自动驾驶管道的优势相结合。 DriveVLM-Dual 实现了强大的空间理解和实时推理速度。对 nuScenes 数据集和 SUP-AD 数据集进行的大量实验证明了 DriveVLM 的有效性以及 DriveVLM-Dual 的增强性能，在复杂且不可预测的驾驶条件下超越了现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.12289v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Revisiting Data Augmentation in Deep Reinforcement Learning**<br />
**Title_cn:** 重新审视深度强化学习中的数据增强<br />
**Authors:** Jianshu Hu, Yunpeng Jiang, Paul Weng<br />
**Abstract:** <details><summary>原文: </summary>Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近在基于图像的深度强化学习（DRL）中提出了各种数据增强技术。尽管他们凭经验证明了数据增强对于提高样本效率或泛化能力的有效性，但应该首选哪种技术并不总是明确的。为了解决这个问题，我们分析了现有的方法，以更好地理解它们并揭示它们之间的联系。值得注意的是，通过表达这些方法的 Q 目标的方差和经验参与者/批评者损失的方差，我们可以分析它们不同组成部分的效果并进行比较。我们进一步解释了在计算目标 Q 值时选择不同的数据增强变换如何影响这些方法。该分析就如何以更原则性的方式利用数据增强提出了建议。此外，我们还引入了一个名为 tangent prop 的正则化术语，该术语先前在计算机视觉中提出，但据我们所知，它对 DRL 的适应是新颖的。我们评估我们的主张并在多个领域验证我们的分析。与不同的相关基线相比，我们证明它在大多数环境中实现了最先进的性能，并在一些复杂环境中表现出更高的样本效率和更好的泛化能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.12181v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations**<br />
**Title_cn:** 考试监控系统：检测在线考试中的异常行为<br />
**Authors:** Dinh An Ngo, Thanh Dat Nguyen, Thi Le Chi Dang, Huy Hoan Le, Ton Bao Ho, Vo Thanh Khang Nguyen, Truong Thanh Hung Nguyen<br />
**Abstract:** <details><summary>原文: </summary>Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our "Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.</details>
**Abstract_cn:** <details><summary>译文: </summary>过去十年中，在线考试作弊已成为一个普遍问题，尤其是在新冠肺炎 (COVID-19) 大流行期间。为了解决学术不诚实问题，我们的“考试监控系统：检测在线考试中的异常行为”旨在帮助监考人员识别学生的异常行为。我们的系统在实时场景中检测作弊、提供有价值的信息并帮助监考人员决策方面表现出很高的准确性和速度。本文概述了我们的方法和系统在缓解在线考试中普遍存在的作弊问题方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12179v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Major TOM: Expandable Datasets for Earth Observation**<br />
**Title_cn:** 主要 TOM：可扩展的地球观测数据集<br />
**Authors:** Alistair Francis, Mikolaj Czerkawski<br />
**Abstract:** <details><summary>原文: </summary>Deep learning models are increasingly data-hungry, requiring significant resources to collect and compile the datasets needed to train them, with Earth Observation (EO) models being no exception. However, the landscape of datasets in EO is relatively atomised, with interoperability made difficult by diverse formats and data structures. If ever larger datasets are to be built, and duplication of effort minimised, then a shared framework that allows users to combine and access multiple datasets is needed. Here, Major TOM (Terrestrial Observation Metaset) is proposed as this extensible framework. Primarily, it consists of a geographical indexing system based on a set of grid points and a metadata structure that allows multiple datasets with different sources to be merged. Besides the specification of Major TOM as a framework, this work also presents a large, open-access dataset, MajorTOM-Core, which covers the vast majority of the Earth's land surface. This dataset provides the community with both an immediately useful resource, as well as acting as a template for future additions to the Major TOM ecosystem. Access: https://huggingface.co/Major-TOM</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型对数据的需求越来越大，需要大量资源来收集和编译训练它们所需的数据集，地球观测 (EO) 模型也不例外。然而，EO 中的数据集格局相对分散，不同的格式和数据结构使得互操作性变得困难。如果要构建更大的数据集，并最大限度地减少重复工作，则需要一个允许用户组合和访问多个数据集的共享框架。这里，Major TOM（陆地观测元集）被提议作为这种可扩展框架。它主要由基于一组网格点的地理索引系统和允许合并具有不同来源的多个数据集的元数据结构组成。除了将 Major TOM 规范作为框架之外，这项工作还提出了一个大型的开放访问数据集 MajorTOM-Core，它覆盖了地球陆地表面的绝大多数。该数据集为社区提供了立即可用的资源，并充当主要 TOM 生态系统未来添加内容的模板。访问：https://huggingface.co/Major-TOM</details>
**PDF:** <http://arxiv.org/pdf/2402.12095v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview**<br />
**Title_cn:** 反问题变分和机器学习方法的鲁棒性和探索：概述<br />
**Authors:** Alexander Auras, Kanchana Vaishnavi Gandikota, Hannah Droege, Michael Moeller<br />
**Abstract:** <details><summary>原文: </summary>This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文试图概述使用变分方法和机器学习解决成像逆问题的当前方法。特别关注点估计器及其对抗对抗性扰动的鲁棒性。在这种情况下，提供了一维玩具问题的数值实验结果，显示了不同方法的稳健性并通过经验验证了理论保证。本次审查的另一个重点是通过明确的指导来探索数据一致性解决方案的子空间，以满足特定的语义或纹理属性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12072v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **InMD-X: Large Language Models for Internal Medicine Doctors**<br />
**Title_cn:** InMD-X：内科医生的大型语言模型<br />
**Authors:** Hansle Gwon, Imjin Ahn, Hyoje Jung, Byeolhee Kim, Young-Hak Kim, Tae Joon Jun<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了 InMD-X，这是专门为满足内科医生 (IMD) 的独特特征和需求而设计的多个大型语言模型的集合。 InMD-X 代表了自然语言处理领域的突破性发展，提供了一套针对内科领域各个方面进行微调的语言模型。这些模型涵盖广泛的医学子专业，使 IMD 能够进行更高效、更准确的研究、诊断和记录。 InMD-X 的多功能性和适应性使其成为改善医疗保健行业、加强医疗保健专业人员之间的沟通和推进医学研究的宝贵工具。 InMD-X 中的每个模型都经过精心定制，以解决 IMD 面临的特定挑战，确保临床文本分析和决策支持的最高精确度和全面性。本文概述了 InMD-X 的设计、开发和评估，展示了其彻底改变内科医生与医疗数据和信息交互方式的潜力。我们展示了广泛测试的结果，证明了 InMD-X 在现实医疗场景中的有效性和实用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.11883v1><br />
**Code:** null<br />

