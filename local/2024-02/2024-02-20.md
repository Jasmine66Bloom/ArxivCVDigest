## [UPDATED!] **2024-02-20** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies**<br />
**Title_cn:** AnnoTheia：用于视听语音技术的半自动注释工具包<br />
**Authors:** José-M. Acosta-Triana, David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos<br />
**Abstract:** <details><summary>原文: </summary>More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>世界各地使用超过 7,000 种已知语言。然而，由于缺乏标注资源，目前语音技术只覆盖了其中的一小部分。尽管自我监督的语音表示、最近的大量语音语料库收集以及挑战的组织已经缓解了这种不平等，但大多数研究主要以英语为基准。当涉及听觉和视觉语音模式的任务得到解决时，这种情况会更加严重。为了促进视听语音技术的低资源语言的研究，我们推出了 AnnoTheia，一个半自动注释工具包，可以检测一个人何时在场景中说话以及相应的转录。此外，为了展示为感兴趣的语言准备 AnnoTheia 的完整过程，我们还描述了将用于主动说话者检测的预训练模型调整为西班牙语，使用的数据库最初不是为此类任务设想的。 AnnoTheia 工具包、教程和预训练模型可在 GitHub 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.13152v1><br />
**Code:** <https://github.com/joactr/annotheia>**<br />
>>**index:** 2<br />
**Title:** **Neural Network Diffusion**<br />
**Title_cn:** 神经网络扩散<br />
**Authors:** Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在图像和视频生成方面取得了显着的成功。在这项工作中，我们证明扩散模型还可以 \textit{生成高性能的神经网络参数}。我们的方法很简单，利用自动编码器和标准潜在扩散模型。自动编码器提取经过训练的网络参数子集的潜在表示。然后训练扩散模型以从随机噪声中合成这些潜在参数表示。然后，它生成新的表示，并通过自动编码器的解码器传递，其输出可用作网络参数的新子集。在各种架构和数据集中，我们的扩散过程始终以最小的额外成本生成与经过训练的网络相比具有可比较或改进性能的模型。值得注意的是，我们凭经验发现生成的模型与经过训练的网络的表现不同。我们的结果鼓励对扩散模型的多功能使用进行更多探索。</details>
**PDF:** <http://arxiv.org/pdf/2402.13144v1><br />
**Code:** <https://github.com/nus-hpc-ai-lab/neural-network-diffusion>**<br />
>>**index:** 3<br />
**Title:** **VGMShield: Mitigating Misuse of Video Generative Models**<br />
**Title_cn:** VGMShield：减少视频生成模型的滥用<br />
**Authors:** Yan Pang, Yang Zhang, Tianhao Wang<br />
**Abstract:** <details><summary>原文: </summary>With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   Furthermore, anticipating future generative model improvements, we propose a {\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着视频生成技术的快速发展，人们可以方便地利用视频生成模型来创建适合自己特定需求的视频。尽管如此，人们也越来越担心它们可能被滥用来制造和传播虚假信息。在这项工作中，我们介绍了 VGMShield：一组三个简单但开创性的缓解措施，贯穿虚假视频生成的生命周期。我们从\textit{假视频检测}开始尝试了解生成的视频是否存在唯一性以及我们是否可以将它们与真实视频区分开来；然后，我们研究 \textit{tracing} 问题，该问题将假视频映射回生成它的模型。为此，我们建议利用专注于{\it时空动态}的预训练模型作为骨干来识别视频中的不一致之处。通过对七个最先进的开源模型的实验，我们证明当前模型仍然无法完美处理时空关系，因此我们可以以近乎完美的精度完成检测和追踪。此外，预测未来生成模型的改进，我们提出了一种{\it Prevention}方法，该方法向图像添加不可见的扰动，使生成的视频看起来不真实。与虚假视频检测和跟踪相结合，我们的多方面解决方案可以有效减少视频生成模型的滥用。</details>
**PDF:** <http://arxiv.org/pdf/2402.13126v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Visual Style Prompting with Swapping Self-Attention**<br />
**Title_cn:** 通过交换自我注意力来提示视觉风格<br />
**Authors:** Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh<br />
**Abstract:** <details><summary>原文: </summary>In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available \href{https://curryjung.github.io/VisualStylePrompt/}{here}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在不断发展的文本到图像生成领域，扩散模型已成为内容创建的强大工具。尽管其能力非凡，现有模型在实现风格一致的受控生成方面仍然面临挑战，需要昂贵的微调，或者由于内容泄漏而常常无法充分传输视觉元素。为了应对这些挑战，我们提出了一种新颖的方法，即我们的方法，可以在保持特定风格元素和细微差别的同时生成各种图像。在去噪过程中，我们保留原始特征的查询，同时将键和值与后期自注意力层中的参考特征交换。这种方法允许视觉风格提示，无需任何微调，确保生成的图像保持忠实的风格。通过对各种样式和文本提示的广泛评估，我们的方法展示了优于现有方法的优越性，最好地反映了参考文献的风格，并确保生成的图像最准确地匹配文本提示。我们的项目页面位于 \href{https://curryjung.github.io/VisualStylePrompt/}{here}。</details>
**PDF:** <http://arxiv.org/pdf/2402.12974v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence**<br />
**Title_cn:** 模式分析与机器智能文献综述<br />
**Authors:** Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li<br />
**Abstract:** <details><summary>原文: </summary>By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过巩固分散的知识，文献综述提供了对所研究主题的全面理解。然而，过多的评论，尤其是在蓬勃发展的模式分析和机器智能（PAMI）领域，引起了研究人员和评论者的担忧。针对这些担忧，本分析旨在从不同角度对 PAMI 领域的评论进行全面回顾。首先，提出了基于大语言模型的文献计量指标来自动评估文献评论。为了实现这一点，构建了一个名为 RiPAMI 的元数据数据库和一个主题数据集，用于获取 PAMI 评论的统计特征。与传统的文献计量测量不同，所提出的文章级指标提供了实时和现场归一化的评论量化评估，而不依赖于用户定义的关键词。其次，基于这些指标，研究对不同评论进行比较分析，揭示不同领域、不同时期、不同期刊的出版物特征。新出现的人工智能生成的文献评论也受到了评估，观察到的差异表明，大多数人工智能生成的评论在几个方面仍然落后于人类撰写的评论。第三，我们简要提供了对代表性 PAMI 评论的主观评价，并介绍了基于论文结构的文献评论类型学。这种类型可以提高学者阅读和撰写评论的清晰度和有效性，同时也可以作为人工智能系统生成组织良好的评论的指南。最后，本分析深入探讨了文献综述当前面临的挑战，并展望了其未来的发展方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.12928v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection**<br />
**Title_cn:** 消除欺骗：采用视觉语言模型进行通用 Deepfake 检测<br />
**Authors:** Sohail Ahmed Khan, Duc-Tien Dang-Nguyen<br />
**Abstract:** <details><summary>原文: </summary>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成对抗网络 (GAN) 的最新进展和扩散模型的出现极大地简化了高度真实且可广泛访问的合成内容的制作。因此，迫切需要有效的通用检测机制来减轻深度造假带来的潜在风险。在本文中，我们探讨了预训练视觉语言模型（VLM）与最新的通用深度伪造检测自适应方法相结合的有效性。继该领域之前的研究之后，我们仅使用单个数据集 (ProGAN) 来调整 CLIP 以进行深度伪造检测。然而，与之前的研究仅依赖 CLIP 的视觉部分而忽略其文本部分相比，我们的分析表明保留文本部分至关重要。因此，我们采用的简单且轻量级的基于 Prompt Tuning 的自适应策略比之前的 SOTA 方法的性能提高了 5.01% mAP 和 6.61% 准确度，同时利用了不到三分之一的训练数据（200k 图像与 720k 图像相比）。为了评估我们提出的模型的实际适用性，我们对各种场景进行了全面评估。这涉及对来自 21 个不同数据集的图像进行严格测试，包括由基于 GAN、基于扩散和商业工具生成的图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.12927v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models**<br />
**Title_cn:** RealCompo：现实主义和组合性之间的动态平衡改进了文本到图像的扩散模型<br />
**Authors:** Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在文本到图像的生成方面取得了显着的进步。然而，现有模型在面对多对象组合生成时仍然存在许多困难。在本文中，我们提出了一种新的免训练且迁移友好的文本到图像生成框架，即 RealCompo，旨在利用文本到图像和布局到图像模型的优势来增强真实感和生成图像的组合性。提出了一种直观且新颖的平衡器来动态平衡两个模型在去噪过程中的优势，允许即插即用使用任何模型而无需额外的训练。大量实验表明，我们的 RealCompo 在多对象合成生成中始终优于最先进的文本到图像模型和布局到图像模型，同时保持生成图像的令人满意的真实性和合成性。代码可在 https://github.com/YangLing0818/RealCompo 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.12908v1><br />
**Code:** <https://github.com/yangling0818/realcompo>**<br />
>>**index:** 8<br />
**Title:** **A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal Representation**<br />
**Title_cn:** 一种从骨骼表示重建管状形状的几何算法<br />
**Authors:** Guoqing Zhang, Songzi Cat, Juzi Cat<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel approach for the reconstruction of tubular shapes from skeletal representations. Our method processes all skeletal points as a whole, eliminating the need for splitting input structure into multiple segments. We represent the tubular shape as a truncated signed distance function (TSDF) in a voxel hashing manner, in which the signed distance between a voxel center and the object is computed through a simple geometric algorithm. Our method does not involve any surface sampling scheme or solving large matrix equations, and therefore is a faster and more elegant solution for tubular shape reconstruction compared to other approaches. Experiments demonstrate the efficiency and effectiveness of the proposed method. Code is avaliable at https://github.com/wlsdzyzl/Dragon.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种从骨骼表示重建管状形状的新方法。我们的方法将所有骨架点作为一个整体进行处理，无需将输入结构拆分为多个片段。我们以体素散列方式将管状形状表示为截断符号距离函数（TSDF），其中体素中心与物体之间的符号距离是通过简单的几何算法计算的。我们的方法不涉及任何表面采样方案或求解大型矩阵方程，因此与其他方法相比，这是一种更快、更优雅的管状形状重建解决方案。实验证明了该方法的效率和有效性。代码可在 https://github.com/wlsdzyzl/Dragon 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12797v1><br />
**Code:** <https://github.com/wlsdzyzl/dragon>**<br />
>>**index:** 9<br />
**Title:** **Two-stage Rainfall-Forecasting Diffusion Model**<br />
**Title_cn:** 两阶段降雨量预报扩散模型<br />
**Authors:** XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在降雨预测方面取得了巨大的成就。然而，目前的预测方法存在一定的局限性，例如生成的图像模糊和空间位置不正确。为了克服这些挑战，我们提出了两阶段降雨预报扩散模型（TRDM），旨在提高长期降雨预报的准确性并解决时空建模之间的性能不平衡问题。 TRDM 是一种用于降雨预测任务的两阶段方法。第一阶段的任务是捕获鲁棒的时间信息，同时在低分辨率条件下保留空间信息。第二阶段的任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。我们在 MRMS 和瑞典雷达数据集上展示了最先进的结果。我们的项目是开源的，可在 GitHub 上获取：\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}。</details>
**PDF:** <http://arxiv.org/pdf/2402.12779v1><br />
**Code:** <https://github.com/clearlyzerolxd/trdm>**<br />
>>**index:** 10<br />
**Title:** **MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion**<br />
**Title_cn:** MuLan：用于渐进式多对象扩散的多模态 LLM 代理<br />
**Authors:** Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou<br />
**Abstract:** <details><summary>原文: </summary>Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https://github.com/measure-infinity/mulan-code.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的文本到图像模型仍然难以生成多个对象的图像，特别是在处理它们的空间位置、相对大小、重叠和属性绑定方面。在本文中，我们开发了一种免训练的 Multimodal-LLM 代理（MuLan），通过具有规划和反馈控制的渐进式多对象生成来解决这些挑战，就像人类画家一样。 MuLan 利用大型语言模型 (LLM) 将提示分解为一系列子任务，每个子任务仅生成一个对象，该对象通过稳定扩散以先前生成的对象为条件。与现有的基于 LLM 的方法不同，MuLan 仅在开始时生成一个高级计划，而每个对象的确切大小和位置由 LLM 和每个子任务的注意力指导确定。此外，木兰采用视觉语言模型（VLM）为每个子任务中生成的图像提供反馈，并在违反原始提示时控制扩散模型重新生成图像。因此，木兰每一步中的每个模型只需要解决它专门处理的一个简单的子任务。我们从不同的基准中收集了 200 个包含具有空间关系和属性绑定的多对象的提示来评估 MuLan。结果证明了 MuLan 在生成多个对象方面优于基线。该代码可在 https://github.com/measure-infinity/mulan-code 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12741v1><br />
**Code:** <https://github.com/measure-infinity/mulan-code>**<br />
>>**index:** 11<br />
**Title:** **MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction**<br />
**Title_cn:** MVDiffusion++：用于单视图或稀疏视图 3D 对象重建的密集高分辨率多视图扩散模型<br />
**Authors:** Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种用于 3D 对象重建的神经架构 MVDiffusion++，在给定一张或几张没有相机姿势的图像的情况下，可以合成对象的密集且高分辨率的视图。 MVDiffusion++ 通过两个令人惊讶的简单想法实现了卓越的灵活性和可扩展性：1）“无姿势架构”，其中 2D 潜在特征之间的标准自注意力学习跨任意数量的条件和生成视图的 3D 一致性，而无需显式使用相机姿势信息; 2）“视图丢弃策略”，在训练期间丢弃大量输出视图，从而减少训练时的内存占用，并在测试时实现密集且高分辨率的视图合成。我们使用 Objaverse 进行训练，使用 Google Scanned Objects 进行评估，并使用标准的新颖视图合成和 3D 重建指标，其中 MVDiffusion++ 显着优于当前的技术水平。我们还通过将 MVDiffusion++ 与文本到图像生成模型相结合来演示文本到 3D 应用示例。</details>
**PDF:** <http://arxiv.org/pdf/2402.12712v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation**<br />
**Title_cn:** DiffusionNOCS：管理 Sim2Real 多模态类别级姿势估计中的对称性和不确定性<br />
**Authors:** Takuya Ikeda, Sergey Zakharov, Tianyi Ko, Muhammad Zubair Irshad, Robert Lee, Katherine Liu, Rares Ambrus, Koichi Nishiwaki<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了类别级姿态估计的挑战性问题。当前用于此任务的最先进方法在处理对称对象以及尝试仅通过合成数据训练推广到新环境时面临挑战。在这项工作中，我们通过提出一种概率模型来解决这些挑战，该模型依赖于扩散来估计对于恢复部分对象形状以及建立姿态估计所必需的对应关系至关重要的密集规范图。此外，我们引入了关键组件，通过利用具有多模态输入表示的扩散模型的强度来提高性能。我们通过在一系列真实数据集上进行测试来证明我们的方法的有效性。尽管仅根据我们生成的合成数据进行训练，但我们的方法实现了最先进的性能和前所未有的泛化质量，超越了基线，甚至超越了那些在目标领域专门训练过的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.12647v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples**<br />
**Title_cn:** CounterCurate：通过反事实示例增强物理和语义视觉语言组合推理<br />
**Authors:** Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee<br />
**Abstract:** <details><summary>原文: </summary>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 CounterCurate，一个全面提高对比和生成多模态模型的视觉语言组合推理能力的框架。特别是，我们发现了两个尚未充分探索的关键问题：忽视物理推理（计数和位置理解）以及使用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一种解决这些差距的方法。我们首先关注 CLIP 和 LLaVA 等多模态模型在基于物理的组合推理中的近机性能。然后，我们使用基础图像生成模型 GLIGEN 应用简单的数据增强来生成微调数据，从而显着提高性能：在我们新策划的 Flickr30k-Positions 基准测试中，CLIP 和 LLaVA 分别 +33% 和 +37%。此外，我们利用高性能文本生成和图像生成模型（特别是 GPT-4V 和 DALLE-3）的功能来策划具有挑战性的语义反事实，从而进一步增强在 SugarCrepe 等基准上的组合推理能力，其中 CounterCurate 优于 GPT-4V 。</details>
**PDF:** <http://arxiv.org/pdf/2402.13254v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Touch, Vision, and Language Dataset for Multimodal Alignment**<br />
**Title_cn:** 用于多模式对齐的触摸、视觉和语言数据集<br />
**Authors:** Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg<br />
**Abstract:** <details><summary>原文: </summary>Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>触摸是人类重要的感知方式，但尚未被纳入多模态生成语言模型中。这部分是由于获得触觉数据的自然语言标签的困难以及将触觉读数与视觉观察和语言描述对齐的复杂性。作为弥补这一差距的一步，这项工作引入了一个包含 44K 个野外视觉-触摸对的新数据集，其中包含由人类注释的英语标签 (10%) 和来自 GPT-4V 的文本伪标签 (90%) 。我们使用该数据集来训练视觉语言对齐的触觉编码器以进行开放词汇分类，并训练触摸视觉语言（TVL）模型以使用经过训练的编码器生成文本。结果表明，通过结合触摸，TVL 模型比在任何一对模态上训练的现有模型提高了（+29% 分类准确率）触摸-视觉-语言对齐。尽管数据集只有一小部分是人工标记的，但 TVL 模型在新的触摸视觉上表现出比 GPT-4V (+12%) 和开源视觉语言模型 (+32%) 更好的视觉触觉理解了解基准。代码和数据：https://tactile-vlm.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2402.13232v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts**<br />
**Title_cn:** 欺骗你的多式联运法学硕士有多容易？欺骗性提示的实证分析<br />
**Authors:** Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan<br />
**Abstract:** <details><summary>原文: </summary>The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）的显着进步并没有使它们免受挑战，特别是在处理提示中的欺骗性信息的情况下，从而在这种情况下产生幻觉反应。为了定量评估此漏洞，我们推出了 MAD-Bench，这是一个精心策划的基准测试，包含 850 个测试样本，分为 6 个类别，例如不存在的对象、对象计数、空间关系和视觉混乱。我们提供对流行 MLLM 的全面分析，范围从 GPT-4V、Gemini-Pro 到开源模型，例如 LLaVA-1.5 和 CogVLM。根据经验，我们观察到 GPT-4V 与其他模型之间存在显着的性能差距；之前强大的指令调整模型，例如 LRV-Instruction 和 LLaVA-RLHF，在这个新基准上无效。虽然 GPT-4V 在 MAD-Bench 上达到了 75.02% 的准确率，但我们实验中任何其他模型的准确率在 5% 到 35% 之间。我们进一步提出了一种补救措施，在欺骗性提示中添加一个额外的段落，以鼓励模型在回答问题之前三思而后行。令人惊讶的是，这种简单的方法甚至可以将准确率提高一倍；然而，绝对数字仍然太低，无法令人满意。我们希望 MAD-Bench 能够作为一个有价值的基准来刺激进一步的研究，以增强模型抵御欺骗性提示的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.13220v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog**<br />
**Title_cn:** OLViT：通过基于注意力的嵌入进行视频对话的多模态状态跟踪<br />
**Authors:** Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling<br />
**Abstract:** <details><summary>原文: </summary>We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了对象语言视频转换器（OLViT）——一种在基于多模式注意的对话状态跟踪器上运行的视频对话的新颖模型。现有的视频对话模型难以解决需要视频内的空间和时间定位、长期时间推理以及跨多个对话回合的准确对象跟踪的问题。 OLViT 通过根据对象状态跟踪器 (OST) 和语言状态跟踪器 (LST) 的输出维护全局对话状态来解决这些挑战：OST 关注视频中最重要的对象，而 LST 则跟踪视频中最重要的对象。对先前对话轮次最重要的语言共同引用。与以前的工作形成鲜明对比的是，我们的方法本质上是通用的，因此能够学习最相关的对象和回合的连续多模式对话状态表示。因此，它们可以无缝集成到大型语言模型（LLM）中，并在处理不同数据集和任务时提供高度灵活性。对具有挑战性的 DVD（响应分类）和 SIMMC 2.1（响应生成）数据集的评估表明，OLViT 在这两个数据集上实现了新的最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13146v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **ConVQG: Contrastive Visual Question Generation with Multimodal Guidance**<br />
**Title_cn:** ConVQG：利用多模态指导生成对比视觉问题<br />
**Authors:** Li Mi, Syrielle Montariol, Javiera Castillo-Navarro, Xianjie Dai, Antoine Bosselut, Devis Tuia<br />
**Abstract:** <details><summary>原文: </summary>Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>提出有关视觉环境的问题是智能代理理解丰富的多方面场景的重要方式，这提高了视觉问题生成（VQG）系统的重要性。除了基于图像之外，现有的 VQG 系统还可以使用文本约束（例如预期答案或知识三元组）来生成有针对性的问题。这些约束允许 VQG 系统指定问题内容或利用无法仅从图像内容获得的外部常识知识。然而，使用文本约束生成重点问题，同时强制与图像内容保持高度相关性仍然是一个挑战，因为 VQG 系统经常忽略一种或两种形式的基础。在这项工作中，我们提出了对比视觉问题生成（ConVQG），这是一种使用双重对比目标来区分使用两种模式生成的问题和基于单一模式生成的问题的方法。对知识感知和标准 VQG 基准的实验表明，ConVQG 优于最先进的方法，并生成基于图像、文本引导和知识丰富的问题​​。与非对比基线相比，我们的人类评估结果还显示出对 ConVQG 问题的偏好。</details>
**PDF:** <http://arxiv.org/pdf/2402.12846v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Model Composition for Multimodal Large Language Models**<br />
**Title_cn:** 多模态大语言模型的模型组合<br />
**Authors:** Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型 (MLLM) 的最新发展取得了快速进展，正在朝着创建能够理解各种模态输入的多功能 MLLM 的目标迈进。然而，现有方法通常依赖于配对多模态指令数据的联合训练，这是资源密集型的并且难以扩展到新模态。在本文中，我们提出了一种新的范式，通过现有 MLLM 的模型组合来创建一个保留每个原始模型的模态理解能力的新模型。我们的基本实现 NaiveMC 通过重用模态编码器和合并 LLM 参数展示了该范例的有效性。此外，我们引入DAMC来解决合并过程中的参数干扰和不匹配问题，从而提高模型性能。为了促进这一领域的研究，我们提出了 MCUB，这是评估 MLLM 理解不同模式输入的能力的基准。该基准测试和其他四个多模态理解任务的实验显示出相对于基线的显着改进，证明模型组合可以创建能够处理来自多种模态的输入的通用模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.12750v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering**<br />
**Title_cn:** 模态感知与大型语言模型的集成，用于基于知识的视觉问答<br />
**Authors:** Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang<br />
**Abstract:** <details><summary>原文: </summary>Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于知识的视觉问答（KVQA）已被广泛研究，以利用外部知识（例如知识图（KG））回答视觉问题。尽管已经提出了一些利用大型语言模型（LLM）作为隐式知识源的尝试，但由于 LLM 可能会产生幻觉，这仍然具有挑战性。此外，图像、KG 和 LLM 等多种知识源无法轻松针对复杂场景进行调整。为了解决这些问题，我们提出了一种与 KVQA (MAIL) 法学硕士的新颖的模态感知集成。它仔细利用多模态知识进行图像理解和知识推理。具体来说，（i）我们提出了一种利用 LLM 的两阶段提示策略，将图像密集地体现为具有详细视觉特征的场景图； （ii）我们通过将提到的实体与外部事实联系起来构建耦合概念图。 (iii) 为充分的多模态融合而设计定制的伪暹罗图介质融合。我们利用两个图中共享的实体作为媒介来桥接紧密的模态间交换，同时通过限制媒介内的融合来最大限度地保留有洞察力的模态内学习。对两个基准数据集的大量实验显示了 MAIL 在资源减少 24 倍的情况下的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12728v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey**<br />
**Title_cn:** NeRF 和 3D 高斯分布如何重塑 SLAM：一项调查<br />
**Authors:** Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi<br />
**Abstract:** <details><summary>原文: </summary>Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的二十年中，同步定位与建图 (SLAM) 领域的研究经历了重大发展，凸显了其在实现未知环境自主探索方面的关键作用。这种演变的范围从手工方法到深度学习时代，再到最近以神经辐射场 (NeRF) 和 3D 高斯分布 (3DGS) 表示为重点的发展。认识到越来越多的研究以及缺乏对该主题的全面调查，本文旨在通过辐射领域的最新进展，首次全面概述 SLAM 的进展。它揭示了背景、演变路径、固有优势和局限性，并为突出动态进展和具体挑战提供了基本参考。</details>
**PDF:** <http://arxiv.org/pdf/2402.13255v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields**<br />
**Title_cn:** 提高相机位姿和分解低阶张量辐射场联合优化的鲁棒性<br />
**Authors:** Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种算法，允许仅使用 2D 图像作为监督，联合细化由分解的低秩张量表示的相机姿态和场景几何形状。首先，我们基于 1D 信号进行了一项试点研究，并将我们的发现与 3D 场景相关联，其中基于体素的 NeRF 的朴素联合姿势优化很容易导致次优解决方案。此外，基于频谱分析，我们建议在 2D 和 3D 辐射场上应用卷积高斯滤波器，以实现从粗到细的训练计划，从而实现联合相机姿态优化。利用分解的低秩张量的分解特性，我们的方法实现了与强力 3D 卷积等效的效果，并且只产生很少的计算开销。为了进一步提高联合优化的鲁棒性和稳定性，我们还提出了平滑二维监督、随机缩放内核参数和边缘引导损失掩模的技术。广泛的定量和定性评估表明，我们提出的框架在新颖的视图合成以及快速收敛优化方面实现了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13252v1><br />
**Code:** <https://github.com/nemo1999/joint-tensorf>**<br />
>>**index:** 3<br />
**Title:** **OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow**<br />
**Title_cn:** OccFlowNet：通过可微渲染和占用流实现自监督占用估计<br />
**Authors:** Simon Boeder, Fabian Gigengack, Benjamin Risse<br />
**Abstract:** <details><summary>原文: </summary>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义占用作为一种突出的 3D 场景表示最近获得了巨大的关注。然而，大多数现有方法依赖于具有细粒度 3D 体素标签的大型且昂贵的数据集进行训练，这限制了它们的实用性和可扩展性，增加了该领域自我监控学习的需求。在这项工作中，我们提出了一种受神经辐射场 (NeRF) 启发的新颖的占用估计方法，仅使用 2D 标签，这种方法更容易获取。特别是，我们采用可微分体积渲染来预测深度​​和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，我们引入了相邻时间步长的时间渲染。此外，我们引入占用流作为处理场景中动态对象并确保其时间一致性的机制。通过广泛的实验，我们证明，与使用 3D 标签的方法相比，仅 2D 监督就足以实现最先进的性能，同时优于并发 2D 方法。当将 2D 监督与 3D 标签、时间渲染和占用流相结合时，我们的性能显着优于之前所有的占用估计模型。我们的结论是，所提出的渲染监督和占用流程促进了占用估计，并进一步缩小了该领域自我监督学习的差距。</details>
**PDF:** <http://arxiv.org/pdf/2402.12792v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **FlashTex: Fast Relightable Mesh Texturing with LightControlNet**<br />
**Title_cn:** FlashTex：使用 LightControlNet 进行快速可重新照明网格纹理<br />
**Authors:** Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala<br />
**Abstract:** <details><summary>原文: </summary>Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.</details>
**Abstract_cn:** <details><summary>译文: </summary>即使对于专业的视觉内容创建者来说，手动创建 3D 网格纹理也非常耗时。我们提出了一种基于用户提供的文本提示自动对输入 3D 网格进行纹理化的快速方法。重要的是，我们的方法将照明与最终纹理中的表面材质/反射率分开，以便网格可以在任何照明环境中正确地重新照亮和渲染。我们引入了 LightControlNet，这是一种基于 ControlNet 架构的新文本到图像模型，它允许将所需的照明指定为模型的调节图像。然后，我们的文本到纹理管道分两个阶段构建纹理。第一阶段使用 LightControlNet 生成一组稀疏的视觉一致的网格参考视图。第二阶段应用基于分数蒸馏采样 (SDS) 的纹理优化，与 LightControlNet 配合使用以提高纹理质量，同时将表面材质与照明分离。我们的管道比以前的文本到纹理方法要快得多，同时生成高质量且可重新点亮的纹理。</details>
**PDF:** <http://arxiv.org/pdf/2402.13251v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **VideoPrism: A Foundational Visual Encoder for Video Understanding**<br />
**Title_cn:** VideoPrism：用于视频理解的基础视觉编码器<br />
**Authors:** Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了 VideoPrism，这是一种通用视频编码器，可以使用单个冻结模型来处理各种视频理解任务。我们在包含 36M 高质量视频字幕对和 582M 带有噪声并行文本（例如 ASR 转录本）的视频剪辑的异构语料库上预训练 VideoPrism。预训练方法通过语义视频嵌入的全局局部蒸馏和令牌洗牌方案改进了屏蔽自动编码，使 VideoPrism 能够主要关注视频模态，同时利用与视频相关的宝贵文本。我们在四组视频理解任务上广泛测试 VideoPrism，从网络视频问答到科学简历，在 33 个视频理解基准测试中的 30 个上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13217v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model**<br />
**Title_cn:** 使用 CoRTe 进行跨域迁移学习：从黑盒到轻量级分割模型的一致且可靠的迁移<br />
**Authors:** Claudia Cuttano, Antonio Tavera, Fabio Cermelli, Giuseppe Averta, Barbara Caputo<br />
**Abstract:** <details><summary>原文: </summary>Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多实际应用需要在未标记的数据集上训练语义分割模型并在低资源硬件上执行。从经过训练的源模型中提取可能代表第一个的解决方案，但没有考虑训练数据的不同分布。无监督域适应（UDA）技术声称可以解决域转移问题，但在大多数情况下假设源数据或可访问的白盒源模型可用，而在实际应用中，由于商业和/或安全原因，这些模型通常不可用。在本文中，我们研究了一种更具挑战性的设置，其中假设我们只能访问黑盒源模型预测，则必须在目标未标记数据集上训练轻量级模型以进行语义分割。我们的方法名为 CoRTe，由（i）一个伪标签函数组成，该函数使用其相对置信度从黑盒源模型中提取可靠的知识，（ii）一个伪标签细化方法，用于保留和增强从黑盒源模型中学到的新信息。目标数据上的学生模型，以及（iii）使用提取的伪标签对模型进行一致的训练。我们在两种合成到真实的设置上对 CoRTe 进行基准测试，在使用黑盒模型传输目标数据分布的轻量级模型知识时展示了显着的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.13122v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Improve Cross-Architecture Generalization on Dataset Distillation**<br />
**Title_cn:** 改进数据集蒸馏的跨架构泛化<br />
**Authors:** Binglin Zhou, Linhao Zhong, Wentao Chen<br />
**Abstract:** <details><summary>原文: </summary>Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed "model pool". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据集蒸馏是机器学习中的一种实用方法，旨在从较大的现有数据集创建较小的合成数据集。然而，现有的蒸馏方法主要采用基于模型的范例，其中合成数据集继承了特定于模型的偏差，限制了其对替代模型的泛化性。为了应对这一限制，我们提出了一种称为“模型池”的新颖方法。这种方法涉及在数据蒸馏过程中根据特定的概率分布从不同的模型池中选择模型。此外，我们将模型池与已建立的知识蒸馏方法相结合，并将知识蒸馏应用于蒸馏数据集的测试过程。我们的实验结果在测试时验证了模型池方法在一系列现有模型中的有效性，证明了与现有方法相比的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13007v1><br />
**Code:** <https://github.com/distill-generalization-group/distill-generalization>**<br />
>>**index:** 5<br />
**Title:** **Efficient Parameter Mining and Freezing for Continual Object Detection**<br />
**Title_cn:** 用于持续目标检测的高效参数挖掘和冻结<br />
**Authors:** Angelo G. Menezes, Augusto J. Peterlevitz, Mateus A. Chinelatto, André C. P. L. F. de Carvalho<br />
**Abstract:** <details><summary>原文: </summary>Continual Object Detection is essential for enabling intelligent agents to interact proactively with humans in real-world settings. While parameter-isolation strategies have been extensively explored in the context of continual learning for classification, they have yet to be fully harnessed for incremental object detection scenarios. Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural pruning, we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates. The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within object detection models, offering promising avenues for future research and application in real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>持续的对象检测对于使智能代理能够在现实世界中主动与人类交互至关重要。虽然参数隔离策略在持续学习分类的背景下得到了广泛的探索，但它们尚未完全用于增量对象检测场景。从先前专注于挖掘单个神经元响应的研究中汲取灵感，并整合神经剪枝最新发展的见解，我们提出了有效的方法来确定哪些层对于网络在连续更新中维持检测器的性能最重要。所提出的研究结果强调了层级参数隔离在促进对象检测模型中的增量学习方面的巨大优势，为未来在现实场景中的研究和应用提供了有希望的途径。</details>
**PDF:** <http://arxiv.org/pdf/2402.12624v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Video ReCap: Recursive Captioning of Hour-Long Videos**<br />
**Title_cn:** 视频回顾：长达一小时的视频的递归字幕<br />
**Authors:** Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius<br />
**Abstract:** <details><summary>原文: </summary>Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数视频字幕模型旨在处理几秒钟的短视频剪辑并输出描述低级视觉概念（例如对象、场景、原子动作）的文本。然而，大多数现实世界的视频会持续几分钟或几小时，并且具有跨越不同时间粒度的复杂层次结构。我们提出了 Video ReCap，这是一种递归视频字幕模型，可以处理长度截然不同（从 1 秒到 2 小时）的视频输入，并在多个层次结构级别输出视频字幕。递归视频语言架构利用了不同视频层次结构之间的协同作用，可以有效地处理长达一小时的视频。我们利用课程学习训练方案来学习视频的层次结构，从描述原子动作的剪辑级字幕开始，然后关注片段级描述，最后为长达一小时的视频生成摘要。此外，我们通过使用 8,267 个手动收集的远程视频摘要增强 Ego4D 来引入 Ego4D-HCap 数据集。我们的递归模型可以灵活地生成不同层次的字幕，同时也可用于其他复杂的视频理解任务，例如 EgoSchema 上的 VideoQA。数据、代码和模型可在以下网址获取：https://sites.google.com/view/vidrecap</details>
**PDF:** <http://arxiv.org/pdf/2402.13250v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data**<br />
**Title_cn:** 使用生物力学模型和综合训练数据从视频进行 3D 运动学估计<br />
**Authors:** Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk, Ajay Seth, Xucong Zhang<br />
**Abstract:** <details><summary>原文: </summary>Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的人体 3D 运动学估计对于人类健康和移动性的各种应用（例如康复、伤害预防和诊断）至关重要，因为它有助于了解运动过程中所经历的生物力学负荷。传统的基于标记的动作捕捉在财务投资、时间和所需专业知识方面都很昂贵。此外，由于缺乏准确注释的数据集，现有的无标记运动捕捉方法面临着二维关键点检测不可靠、解剖精度有限和泛化能力低等挑战。在这项工作中，我们提出了一种新颖的生物力学感知网络，该网络可以直接从两个输入视图输出 3D 运动学，同时考虑生物力学先验信息和时空信息。为了训练模型，我们创建了合成数据集 ODAH，其中包含通过对齐 SMPL-X 模型的身体网格和全身 OpenSim 骨骼模型生成的精确运动学注释。我们广泛的实验表明，所提出的方法仅在合成数据上进行训练，在跨多个数据集进行评估时优于以前最先进的方法，揭示了增强基于视频的人体动作捕捉的有希望的方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.13172v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space**<br />
**Title_cn:** 通过 Logits 空间上的最大均值差异正则化实现公平<br />
**Authors:** Hao-Wei Chung, Ching-Hao Chiu, Yu-Jen Chen, Yiyu Shi, Tsung-Yi Ho<br />
**Abstract:** <details><summary>原文: </summary>Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>公平性在医疗保健和面部识别中的机器学习等高风险应用的机器学习中变得越来越重要。然而，我们看到了先前的 Logits 空间约束方法的不足。因此，我们提出了一种新颖的框架 Logits-MMD，它通过对具有最大平均差异的输出 logits 施加约束来实现公平条件。此外，定量分析和实验结果表明，我们的框架具有更好的性能，优于以前的方法，并在两个面部识别数据集和一个动物数据集上达到了最先进的水平。最后，我们展示了实验结果并证明我们的去偏方法有效地实现了公平条件。</details>
**PDF:** <http://arxiv.org/pdf/2402.13061v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition**<br />
**Title_cn:** 用于连续视觉语音识别的传统混合解码器和 CTC/Attention 解码器的比较<br />
**Authors:** David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos<br />
**Abstract:** <details><summary>原文: </summary>Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于深度学习的兴起和大规模视听数据库的可用性，视觉语音识别（VSR）领域取得了最新进展。与其他语音处理任务类似，这些端到端 VSR 系统通常基于编码器-解码器架构。虽然编码器有些通用，但已经探索了多种解码方法，例如基于深度神经网络与隐马尔可夫模型相结合的传统混合模型（DNN-HMM）或连接主义时间分类（CTC）范例。然而，有些语言和任务的数据是稀缺的，在这种情况下，不同类型的解码器之间没有明确的比较。因此，我们的研究重点是传统的 DNN-HMM 解码器及其最先进的 CTC/Attention 解码器如何根据用于估计的数据量而表现。我们还分析了我们的视觉语音特征能够在多大程度上适应未经过明确训练的场景，无论是考虑类似的数据集还是为不同语言收集的另一个数据集。结果表明，传统范式达到了提高数据稀缺场景中的 CTC/Attention 模型的识别率，同时减少了训练时间和更少的参数。</details>
**PDF:** <http://arxiv.org/pdf/2402.13004v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **MapTrack: Tracking in the Map**<br />
**Title_cn:** MapTrack：在地图中追踪<br />
**Authors:** Fei Wang, Ruohui Zhang, Chenglin Chen, Min Yang, Yun Bai<br />
**Abstract:** <details><summary>原文: </summary>Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.</details>
**Abstract_cn:** <details><summary>译文: </summary>多目标跟踪（MOT）旨在为每个目标保持稳定且不间断的轨迹。大多数最先进的方法首先检测每个帧中的对象，然后使用运动模型和外观相似性在新检测和现有轨迹之间实现数据关联。尽管取得了令人满意的结果，但遮挡和人群很容易导致检测缺失和扭曲，进而导致关联缺失和错误。在本文中，我们首先重新审视经典的跟踪器 DeepSORT，通过在拥挤和遮挡场景中检测不可用或检测质量低时更加信任预测，显着增强其对人群和遮挡的鲁棒性。具体来说，我们提出了一个由三种轻量级即插即用算法组成的新框架：概率图、预测图和协方差自适应卡尔曼滤波器。概率图识别未检测到的对象是否确实从视野中消失（例如，从图像中消失或进入建筑物），或者只是由于遮挡或其他原因而暂时未被检测到。仍在概率图中的未检测到的目标的轨迹通过状态估计直接扩展。预测图确定一个对象是否在人群中，当观察发生严重变形时，我们优先考虑状态估计而不是观察，这是通过协方差自适应卡尔曼滤波器完成的。所提出的方法名为 MapTrack，在流行的多目标跟踪基准（例如 MOT17 和 MOT20）上取得了最先进的结果。尽管性能优越，我们的方法仍然简单、在线和实时。后续代码将会开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.12968v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Cell Graph Transformer for Nuclei Classification**<br />
**Title_cn:** 用于细胞核分类的细胞图转换器<br />
**Authors:** Wei Lou, Guanbin Li, Xiang Wan, Haofeng Li<br />
**Abstract:** <details><summary>原文: </summary>Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT</details>
**Abstract_cn:** <details><summary>译文: </summary>细胞核分类是组织病理学图像计算机辅助诊断的关键步骤。过去，各种方法都采用图神经网络（GNN）来分析细胞图，通过将细胞核视为顶点来建模细胞间关系。然而，它们受到 GNN 机制的限制，仅通过固定边在本地节点之间传递消息。为了解决这个问题，我们开发了一个单元图转换器（CGT），它将节点和边视为输入标记，以实现所有节点之间的可学习邻接和信息交换。然而，使用单元图训练变压器提出了另一个挑战。初始化不当的特征可能会导致自注意力分数嘈杂和收敛性较差，特别是在处理具有大量连接的单元图时。因此，我们进一步提出了一种新颖的拓扑感知预训练方法，利用图卷积网络（GCN）来学习特征提取器。预训练的特征可以抑制不合理的相关性，从而简化 CGT 的微调。实验结果表明，所提出的具有拓扑感知预训练的单元图转换器显着改善了核分类结果，并实现了最先进的性能。代码和模型可在 https://github.com/lhaof/CGT 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.12946v1><br />
**Code:** <https://github.com/lhaof/cgt>**<br />
>>**index:** 7<br />
**Title:** **UniCell: Universal Cell Nucleus Classification via Prompt Learning**<br />
**Title_cn:** UniCell：通过快速学习进行通用细胞核分类<br />
**Authors:** Junjia Huang, Haofeng Li, Xiang Wan, Guanbin Li<br />
**Abstract:** <details><summary>原文: </summary>The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell</details>
**Abstract_cn:** <details><summary>译文: </summary>多类细胞核的识别可以显着促进组织病理学诊断的过程。目前有许多病理数据集可用，但它们的注释不一致。大多数现有方法需要对每个数据集进行单独训练以推断相关标签，并且缺乏跨数据集通用知识的使用，从而限制了识别质量。在本文中，我们提出了一种通用细胞核分类框架（UniCell），它采用一种新颖的提示学习机制来统一预测来自不同数据集域的病理图像的相应类别。特别是，我们的框架采用端到端架构进行核检测和分类，并利用灵活的预测头来适应各种数据集。此外，我们开发了一个动态提示模块（DPM），它利用多个数据集的属性来增强功能。 DPM 首先集成数据集和语义类别的嵌入，然后利用集成的提示来细化图像表示，有效地收获相关细胞类型和数据源之间的共享知识。实验结果表明，所提出的方法在四个核检测和分类基准上有效地实现了最先进的结果。代码和模型可在 https://github.com/lhaof/UniCell 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.12938v1><br />
**Code:** <https://github.com/lhaof/unicell>**<br />
>>**index:** 8<br />
**Title:** **Advancements in Point Cloud-Based 3D Defect Detection and Classification for Industrial Systems: A Comprehensive Survey**<br />
**Title_cn:** 工业系统基于点云的 3D 缺陷检测和分类的进展：全面调查<br />
**Authors:** Anju Rani, Daniel Ortiz-Arroyo, Petar Durdevic<br />
**Abstract:** <details><summary>原文: </summary>In recent years, 3D point clouds (PCs) have gained significant attention due to their diverse applications across various fields such as computer vision (CV), condition monitoring, virtual reality, robotics, autonomous driving etc. Deep learning (DL) has proven effective in leveraging 3D PCs to address various challenges previously encountered in 2D vision. However, the application of deep neural networks (DNN) to process 3D PCs presents its own set of challenges. To address these challenges, numerous methods have been proposed. This paper provides an in-depth review of recent advancements in DL-based condition monitoring (CM) using 3D PCs, with a specific focus on defect shape classification and segmentation within industrial applications for operational and maintenance purposes. Recognizing the crucial role of these aspects in industrial maintenance, the paper provides insightful observations that offer perspectives on the strengths and limitations of the reviewed DL-based PC processing methods. This synthesis of knowledge aims to contribute to the understanding and enhancement of CM processes, particularly within the framework of remaining useful life (RUL), in industrial systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，3D点云（PC）因其在计算机视觉（CV）、状态监测、虚拟现实、机器人、自动驾驶等各个领域的广泛应用而受到广泛关注。深度学习（DL）已被证明是有效的利用 3D PC 解决以前在 2D 视觉中遇到的各种挑战。然而，应用深度神经网络 (DNN) 处理 3D PC 也面临着一系列挑战。为了应对这些挑战，人们提出了多种方法。本文深入回顾了使用 3D PC 基于深度学习的状态监测 (CM) 的最新进展，特别关注工业应用中用于操作和维护目的的缺陷形状分类和分割。认识到这些方面在工业维护中的关键作用，本文提供了富有洞察力的观察结果，为所审查的基于深度学习的 PC 处理方法的优点和局限性提供了观点。这种知识综合旨在促进对 CM 过程的理解和增强，特别是在工业系统的剩余使用寿命 (RUL) 框架内。</details>
**PDF:** <http://arxiv.org/pdf/2402.12923v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets**<br />
**Title_cn:** SolarPanel 分割：不完美数据集的自我监督学习<br />
**Authors:** Sankarshanaa Sagaram, Aditya Kasliwal, Krish Didwania, Laven Srivastava, Pallavi Kailas, Ujjwal Verma<br />
**Abstract:** <details><summary>原文: </summary>The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>太阳能的日益普及需要先进的监控和维护方法，以确保太阳能电池板安装的最佳性能。在这种情况下，一个关键组成部分是从航空或卫星图像中准确分割太阳能电池板，这对于识别操作问题和评估效率至关重要。本文解决了面板分割中的重大挑战，特别是注释数据的稀缺性以及监督学习的手动注释的劳动密集型性质。我们探索并应用自我监督学习（SSL）来解决这些挑战。我们证明 SSL 显着增强了各种条件下的模型泛化能力，并减少了对手动注释数据的依赖，为稳健且适应性强的太阳能电池板分割解决方案铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.12843v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Radar-Based Recognition of Static Hand Gestures in American Sign Language**<br />
**Title_cn:** 基于雷达的美国手语静态手势识别<br />
**Authors:** Christian Schuessler, Wenxuan Zhang, Johanna Bräunig, Marcel Hoffmann, Michael Stelzig, Martin Vossiek<br />
**Abstract:** <details><summary>原文: </summary>In the fast-paced field of human-computer interaction (HCI) and virtual reality (VR), automatic gesture recognition has become increasingly essential. This is particularly true for the recognition of hand signs, providing an intuitive way to effortlessly navigate and control VR and HCI applications. Considering increased privacy requirements, radar sensors emerge as a compelling alternative to cameras. They operate effectively in low-light conditions without capturing identifiable human details, thanks to their lower resolution and distinct wavelength compared to visible light.   While previous works predominantly deploy radar sensors for dynamic hand gesture recognition based on Doppler information, our approach prioritizes classification using an imaging radar that operates on spatial information, e.g. image-like data. However, generating large training datasets required for neural networks (NN) is a time-consuming and challenging process, often falling short of covering all potential scenarios. Acknowledging these challenges, this study explores the efficacy of synthetic data generated by an advanced radar ray-tracing simulator. This simulator employs an intuitive material model that can be adjusted to introduce data diversity.   Despite exclusively training the NN on synthetic data, it demonstrates promising performance when put to the test with real measurement data. This emphasizes the practicality of our methodology in overcoming data scarcity challenges and advancing the field of automatic gesture recognition in VR and HCI applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>在快节奏的人机交互（HCI）和虚拟现实（VR）领域，自动手势识别变得越来越重要。对于手势识别来说尤其如此，它提供了一种直观的方式来轻松导航和控制 VR 和 HCI 应用程序。考虑到日益增长的隐私要求，雷达传感器成为摄像头的一个引人注目的替代品。由于与可见光相比，它们的分辨率较低且波长不同，因此它们可以在弱光条件下有效运行，而不会捕获可识别的人体细节。虽然之前的工作主要部署雷达传感器来基于多普勒信息进行动态手势识别，但我们的方法优先使用基于空间信息（例如，空间信息）的成像雷达进行分类。类似图像的数据。然而，生成神经网络 (NN) 所需的大型训练数据集是一个耗时且具有挑战性的过程，通常无法覆盖所有潜在场景。认识到这些挑战，本研究探讨了先进雷达射线追踪模拟器生成的合成数据的功效。该模拟器采用直观的材料模型，可以调整该模型以引入数据多样性。尽管专门使用合成数据训练神经网络，但在使用真实测量数据进行测试时，它表现出了良好的性能。这强调了我们的方法在克服数据稀缺挑战和推进 VR 和 HCI 应用中自动手势识别领域的实用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12800v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **GOOD: Towards Domain Generalized Orientated Object Detection**<br />
**Title_cn:** 好：迈向领域广义目标检测<br />
**Authors:** Qi Bi, Beichen Zhou, Jingjun Yi, Wei Ji, Haolan Zhan, Gui-Song Xia<br />
**Abstract:** <details><summary>原文: </summary>Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>面向目标检测在过去几年中得到了迅速发展，但这些方法大多数都假设训练和测试图像处于相同的统计分布下，这与现实相去甚远。在本文中，我们提出了域广义定向对象检测任务，旨在探索定向对象检测器在任意不可见目标域上的泛化。学习领域广义面向对象检测器尤其具有挑战性，因为跨域风格变化不仅对内容表示产生负面影响，而且还会导致不可靠的方向预测。为了解决这些挑战，我们提出了一种广义的面向对象检测器（GOOD）。在新兴的对比语言图像预训练（CLIP）的风格幻觉之后，它由两个关键组成部分组成，即旋转感知的内容一致性学习（RAC）和风格一致性学习（SEC）。所提出的 RAC 允许定向对象检测器从风格多样化的样本中学习稳定的方向表示。所提出的 SEC 进一步稳定了不同图像风格内容表示的泛化能力。对多个跨域设置的大量实验显示了 GOOD 的最先进性能。源代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.12765v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization**<br />
**Title_cn:** BronchoTrack：用于分支级支气管镜定位的气道管腔跟踪<br />
**Authors:** Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu<br />
**Abstract:** <details><summary>原文: </summary>Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>实时定位支气管镜对于确保干预质量至关重要。然而，大多数现有方法都难以在速度和泛化之间取得平衡。为了应对这些挑战，我们推出了 BronchoTrack，这是一种创新的实时框架，用于精确的分支级定位，包括管腔检测、跟踪和气道关联。为了实现实时性能，我们采用基准轻量级检测器来进行高效的管腔检测。我们是第一个将多目标跟踪引入支气管镜定位的公司，减轻因支气管镜快速移动和复杂气道结构引起的管腔识别中的时间混乱。为了确保跨患者病例的泛化，我们提出了一种基于语义气道图的免训练检测气道关联方法，该方法对支气管树结构的层次结构进行编码。对九个患者数据集的实验表明，BronchoTrack 的定位精度为 85.64%，同时访问最多此外，我们使用猪模型在体内动物研究中测试了 BronchoTrack，成功地将支气管镜定位到第 8 代气道中。实验评估强调了 BronchoTrack 在令人满意的准确性和泛化性方面的实时性能，展示了其临床应用潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.12763v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Fingerprint Presentation Attack Detector Using Global-Local Model**<br />
**Title_cn:** 使用全局局部模型的指纹呈现攻击检测器<br />
**Authors:** Haozhe Liu, Wentian Zhang, Feng Liu, Haoqian Wu, Linlin Shen<br />
**Abstract:** <details><summary>原文: </summary>The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This paper thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by $\sim$10% in terms of TDR (91.19% versus 80.74%).</details>
**Abstract_cn:** <details><summary>译文: </summary>自动指纹识别系统（AFRS）易受演示攻击（PA）的影响促进了PA检测（PAD）技术的蓬勃发展。然而，PAD方法受到信息丢失和泛化能力差的限制，从而催生了新的PA材料和指纹传感器。因此，本文提出了一种基于全局-局部模型的PAD（RTK-PAD）方法来在一定程度上克服这些局限性。所提出的方法由三个模块组成，称为：1）全局模块； 2）本地模块； 3）反思模块。通过采用基于剪切的全局模块，可以实现从整个指纹图像的非局部特征预测的全局欺骗分数。而通过使用基于纹理修复的本地模块，获得了从指纹补丁预测的本地欺骗分数。这两个模块不是独立的，而是通过我们提出的重新思考模块连接起来，根据全局欺骗分数为本地模块定位两个判别性补丁。最后，通过平均全局和局部欺骗分数得到的融合欺骗分数用于 PAD。我们在 LivDet 2017 上评估的实验结果表明，当错误检测率（FDR）等于 1.0% 时，所提出的 RTK-PAD 可以实现 2.28% 的平均分类误差（ACE）和 91.19% 的真实检测率（TDR），这就 TDR 而言，显着优于最先进的方法 $\sim$10%（91.19% 与 80.74%）。</details>
**PDF:** <http://arxiv.org/pdf/2402.12754v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning**<br />
**Title_cn:** CST：参数和内存高效迁移学习的校准侧调<br />
**Authors:** Feng Chen<br />
**Abstract:** <details><summary>原文: </summary>Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks. Besides, this paper carried out extensive experiments using five benchmark datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved.</details>
**Abstract_cn:** <details><summary>译文: </summary>在物体检测中实现普遍的高精度是相当具有挑战性的，目前业界的主流焦点在于检测特定类别的物体。然而，部署一个或多个目标检测网络需要一定量的 GPU 内存用于训练和存储容量用于推理。这对如何在资源有限的条件下有效协调多个目标检测任务提出了挑战。本文介绍了一种称为“校准侧调整”的轻量级微调策略，该策略集成了适配器调整和侧调整的各个方面，以适应 Transformer 中采用的成功技术，以便与 ResNet 一起使用。校准侧调整架构包含最大转换校准，利用少量附加参数来增强网络性能，同时保持平稳的训练过程。此外，本文对多种微调策略进行了分析，并在ResNet中实现了它们的应用，从而拓展了目标检测网络微调策略的研究。此外，本文使用五个基准数据集进行了广泛的实验。实验结果表明，该方法优于其他最先进的技术，并且在微调方案的复杂性和性能之间实现了更好的平衡。</details>
**PDF:** <http://arxiv.org/pdf/2402.12736v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images**<br />
**Title_cn:** PAC-FNO：用于识别低质量图像的并行结构全分量傅立叶神经算子<br />
**Authors:** Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee, Kookjin Lee, Noseong Park<br />
**Abstract:** <details><summary>原文: </summary>A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发图像识别模型的标准做法是在特定图像分辨率上训练模型，然后部署它。然而，在现实世界的推理中，模型经常遇到分辨率与训练集不同的图像和/或受到自然变化（例如天气变化、噪声类型和压缩伪影）的影响。虽然传统的解决方案涉及针对不同分辨率或输入变化训练多个模型，但这些方法的计算成本很高，因此在实践中无法扩展。为此，我们提出了一种新颖的神经网络模型，即并行结构和全组件傅立叶神经算子（PAC-FNO）来解决该问题。与传统的前馈神经网络不同，PAC-FNO 在频域中运行，使其能够在单个模型中处理不同分辨率的图像。我们还提出了一种用于训练 PAC-FNO 的两阶段算法，对原始下游模型进行了最小的修改。此外，所提出的 PAC-FNO 已准备好与现有的图像识别模型一起使用。通过七个图像识别基准对方法进行了广泛的评估，我们发现所提出的 PAC-FNO 将现有基线模型在各种分辨率的图像上以及推理时图像中各种类型的自然变化的性能提高了高达 77.1%。</details>
**PDF:** <http://arxiv.org/pdf/2402.12721v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition**<br />
**Title_cn:** 学习用于少样本动作识别的域不变时间动力学<br />
**Authors:** Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei<br />
**Abstract:** <details><summary>原文: </summary>Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the self-supervised signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头动作识别旨在仅使用有限数量的样本快速使预训练模型适应具有分布变化的新数据。主要挑战包括如何识别和利用预训练模型学到的可转移知识。我们的中心假设是，动态系统中潜在变量之间的时间不变性有助于可转移性（域不变性）。因此，我们提出了 DITeD，即域不变时间动力学来进行知识转移。为了检测时间不变性部分，我们提出了一个在预训练期间采用两阶段训练策略的生成框架。具体来说，我们明确地模拟不变的动态，包括时间动态生成和转换，以及变体视觉和域编码器。然后我们用自监督信号预训练模型以学习表示。之后，我们修复整个表示模型并调整分类器。在适应过程中，我们修复了可传输的时间动态并更新图像编码器。 DITeD 在标准少样本动作识别数据集中优于领先替代方案的准确性揭示了我们方法的有效性。此外，我们验证了学习到的时间动态转换和时间动态生成模块具有可转移的品质。</details>
**PDF:** <http://arxiv.org/pdf/2402.12706v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T**<br />
**Title_cn:** wmh_seg：基于 Transformer 的 U-Net，用于 1.5T、3T 和 7T 的鲁棒和自动白质高信号分割<br />
**Authors:** Jinghang Li, Tales Santini, Yuanzhe Huang, Joseph M. Mettenburg, Tamer S. Ibrahima, Howard J. Aizensteina, Minjie Wu<br />
**Abstract:** <details><summary>原文: </summary>White matter hyperintensity (WMH) remains the top imaging biomarker for neurodegenerative diseases. Robust and accurate segmentation of WMH holds paramount significance for neuroimaging studies. The growing shift from 3T to 7T MRI necessitates robust tools for harmonized segmentation across field strengths and artifacts. Recent deep learning models exhibit promise in WMH segmentation but still face challenges, including diverse training data representation and limited analysis of MRI artifacts' impact. To address these, we introduce wmh_seg, a novel deep learning model leveraging a transformer-based encoder from SegFormer. wmh_seg is trained on an unmatched dataset, including 1.5T, 3T, and 7T FLAIR images from various sources, alongside with artificially added MR artifacts. Our approach bridges gaps in training diversity and artifact analysis. Our model demonstrated stable performance across magnetic field strengths, scanner manufacturers, and common MR imaging artifacts. Despite the unique inhomogeneity artifacts on ultra-high field MR images, our model still offers robust and stable segmentation on 7T FLAIR images. Our model, to date, is the first that offers quality white matter lesion segmentation on 7T FLAIR images.</details>
**Abstract_cn:** <details><summary>译文: </summary>白质高信号（WMH）仍然是神经退行性疾病的首要影像生物标志物。稳健而准确的 WMH 分割对于神经影像学研究具有至关重要的意义。从 3T MRI 到 7T MRI 的不断转变需要强大的工具来协调场强和伪影的分割。最近的深度学习模型在 WMH 分割方面展现了前景，但仍然面临挑战，包括多样化的训练数据表示和对 MRI 伪影影响的有限分析。为了解决这些问题，我们引入了 wmh_seg，这是一种利用 SegFormer 基于转换器的编码器的新型深度学习模型。 wmh_seg 在无与伦比的数据集上进行训练，包括来自各种来源的 1.5T、3T 和 7T FLAIR 图像，以及人工添加的 MR 伪像。我们的方法弥补了训练多样性和工件分析方面的差距。我们的模型在磁场强度、扫描仪制造商和常见 MR 成像伪影方面表现出了稳定的性能。尽管超高场 MR 图像上存在独特的不均匀伪影，我们的模型仍然可以在 7T FLAIR 图像上提供稳健且稳定的分割。迄今为止，我们的模型是第一个在 7T FLAIR 图像上提供高质量白质病变分割的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.12701v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **TorchCP: A Library for Conformal Prediction based on PyTorch**<br />
**Title_cn:** TorchCP：基于 PyTorch 的共形预测库<br />
**Authors:** Hongxin Wei, Jianguo Huang<br />
**Abstract:** <details><summary>原文: </summary>TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.</details>
**Abstract_cn:** <details><summary>译文: </summary>TorchCP 是一个用于深度学习模型保形预测研究的 Python 工具箱。它包含用于分类和回归任务（包括多维输出）的事后和训练方法的各种实现。 TorchCP 基于 PyTorch (Paszke et al., 2019) 构建，利用矩阵计算的优势提供简洁高效的推理实现。该代码根据 LGPL 许可证获得许可，并在 $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$ 上开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.12683v1><br />
**Code:** <https://github.com/ml-stat-sustech/torchcp>**<br />
>>**index:** 19<br />
**Title:** **Object-level Geometric Structure Preserving for Natural Image Stitching**<br />
**Title_cn:** 自然图像拼接的对象级几何结构保留<br />
**Authors:** Wenxiao Cai, Wankou Yang<br />
**Abstract:** <details><summary>原文: </summary>The topic of stitching images with globally natural structures holds paramount significance. Current methodologies exhibit the ability to preserve local geometric structures, yet fall short in maintaining relationships between these geometric structures. In this paper, we endeavor to safeguard the overall, OBJect-level structures within images based on Global Similarity Prior, while concurrently mitigating distortion and ghosting artifacts with OBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric structures with semantic information, enhancing the algorithm's ability to preserve objects in a manner that aligns more intuitively with human perception. We seek to identify spatial constraints that govern the relationships between various geometric boundaries. Recognizing that multiple geometric boundaries collectively define complete objects, we employ triangular meshes to safeguard not only individual geometric structures but also the overall shapes of objects within the images. Empirical evaluations across multiple image stitching datasets demonstrate that our method establishes a new state-of-the-art benchmark in image stitching. Our implementation and dataset is publicly available at https://github.com/RussRobin/OBJ-GSP .</details>
**Abstract_cn:** <details><summary>译文: </summary>将图像与全局自然结构拼接的主题具有至关重要的意义。当前的方法表现出保留局部几何结构的能力，但在维持这些几何结构之间的关系方面存在不足。在本文中，我们致力于基于全局相似先验保护图像内的整体对象级结构，同时使用 OBJ-GSP 减轻失真和重影伪影。我们的方法利用分段任意模型来提取具有语义信息的几何结构，从而增强算法以更直观地符合人类感知的方式保留对象的能力。我们寻求识别控制各种几何边界之间关系的空间约束。认识到多个几何边界共同定义了完整的对象，我们采用三角形网格不仅可以保护单个几何结构，还可以保护图像中对象的整体形状。对多个图像拼接数据集的实证评估表明，我们的方法在图像拼接方面建立了新的最先进基准。我们的实现和数据集可在 https://github.com/RussRobin/OBJ-GSP 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12677v1><br />
**Code:** <https://github.com/russrobin/obj-gsp>**<br />
>>**index:** 20<br />
**Title:** **Neuromorphic Synergy for Video Binarization**<br />
**Title_cn:** 视频二值化的神经形态协同<br />
**Authors:** Shijie Lin, Xiang Zhang, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan<br />
**Abstract:** <details><summary>原文: </summary>Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.</details>
**Abstract_cn:** <details><summary>译文: </summary>双峰对象，例如用于相机校准的棋盘图案、用于对象跟踪的标记以及路标上的文本等，在我们的日常生活中很普遍，并且作为一种视觉形式来嵌入可以轻松识别的信息。视觉系统。虽然强度图像的二值化对于提取双峰对象中的嵌入信息至关重要，但之前很少有工作考虑由于视觉传感器和环境之间的相对运动而导致的模糊图像的二值化任务。模糊的图像可能会导致二值化质量下降，从而降低视觉系统运行的下游应用的性能。最近，神经形态相机提供了减轻运动模糊的新功能，但首先对图像进行去模糊然后以实时方式二值化并非易事。在这项工作中，我们提出了一种基于事件的二值重建方法，该方法利用双峰目标属性的先验知识在事件空间和图像空间中独立地执行推理，并将两个域的结果合并以生成清晰的二值图像。我们还开发了一种有效的集成方法来将该二进制图像传播到高帧率二进制视频。最后，我们开发了一种新方法来自然地融合事件和图像以进行无监督阈值识别。所提出的方法在公开可用的和我们收集的数据序列中进行了评估，并表明所提出的方法可以优于 SOTA 方法，可以在纯 CPU 设备上实时生成高帧率二进制视频。</details>
**PDF:** <http://arxiv.org/pdf/2402.12644v1><br />
**Code:** <https://github.com/eleboss/ebr>**<br />
>>**index:** 21<br />
**Title:** **YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection**<br />
**Title_cn:** YOLO-Ant：采用深度可分离卷积和大内核设计的轻量级检测器，用于天线干扰源检测<br />
**Authors:** Xiaoyu Tang, Xingming Chen, Jintao Cheng, Jin Wu, Rui Fan, Chengxi Zhang, Zebo Zhou<br />
**Abstract:** <details><summary>原文: </summary>In the era of 5G communication, removing interference sources that affect communication is a resource-intensive task. The rapid development of computer vision has enabled unmanned aerial vehicles to perform various high-altitude detection tasks. Because the field of object detection for antenna interference sources has not been fully explored, this industry lacks dedicated learning samples and detection models for this specific task. In this article, an antenna dataset is created to address important antenna interference source detection issues and serves as the basis for subsequent research. We introduce YOLO-Ant, a lightweight CNN and transformer hybrid detector specifically designed for antenna interference source detection. Specifically, we initially formulated a lightweight design for the network depth and width, ensuring that subsequent investigations were conducted within a lightweight framework. Then, we propose a DSLK-Block module based on depthwise separable convolution and large convolution kernels to enhance the network's feature extraction ability, effectively improving small object detection. To address challenges such as complex backgrounds and large interclass differences in antenna detection, we construct DSLKVit-Block, a powerful feature extraction module that combines DSLK-Block and transformer structures. Considering both its lightweight design and accuracy, our method not only achieves optimal performance on the antenna dataset but also yields competitive results on public datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>在5G通信时代，去除影响通信的干扰源是一项资源密集型任务。计算机视觉的快速发展使得无人机能够执行各种高空探测任务。由于天线干扰源的物体检测领域尚未得到充分探索，该行业缺乏专门针对该特定任务的学习样本和检测模型。本文创建了一个天线数据集来解决重要的天线干扰源检测问题，并作为后续研究的基础。我们推出 YOLO-Ant，这是一种轻量级 CNN 和变压器混合检测器，专为天线干扰源检测而设计。具体来说，我们最初对网络深度和宽度进行了轻量级设计，确保后续研究在轻量级框架内进行。然后，我们提出了基于深度可分离卷积和大卷积核的DSLK-Block模块来增强网络的特征提取能力，有效提高小目标检测。为了解决天线检测中复杂背景和较大类间差异等挑战，我们构建了 DSLKVit-Block，这是一个结合了 DSLK-Block 和 Transformer 结构的强大特征提取模块。考虑到其轻量级设计和准确性，我们的方法不仅在天线数据集上实现了最佳性能，而且还在公共数据集上产生了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.12641v1><br />
**Code:** <https://github.com/scnu-rislab/yolo-ant>**<br />

>## **GNN**
>---
>>**index:** 1<br />
**Title:** **Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach**<br />
**Title_cn:** 以对象为中心的深度神经网络中的视觉推理：一种比较认知方法<br />
**Authors:** Guillermo Puebla, Jeffrey S. Bowers<br />
**Abstract:** <details><summary>原文: </summary>Achieving visual reasoning is a long-term goal of artificial intelligence. In the last decade, several studies have applied deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of generalization of the relations learned. However, in recent years, object-centric representation learning has been put forward as a way to achieve visual reasoning within the deep learning framework. Object-centric models attempt to model input scenes as compositions of objects and relations between them. To this end, these models use several kinds of attention mechanisms to segregate the individual objects in a scene from the background and from other objects. In this work we tested relation learning and generalization in several object-centric models, as well as a ResNet-50 baseline. In contrast to previous research, which has focused heavily in the same-different task in order to asses relational reasoning in DNNs, we use a set of tasks -- with varying degrees of difficulty -- derived from the comparative cognition literature. Our results show that object-centric models are able to segregate the different objects in a scene, even in many out-of-distribution cases. In our simpler tasks, this improves their capacity to learn and generalize visual relations in comparison to the ResNet-50 baseline. However, object-centric models still struggle in our more difficult tasks and conditions. We conclude that abstract visual reasoning remains an open challenge for DNNs, including object-centric models.</details>
**Abstract_cn:** <details><summary>译文: </summary>实现视觉推理是人工智能的长期目标。在过去的十年中，一些研究将深度神经网络（DNN）应用于从图像中学习视觉关系的任务，在所学关系的泛化方面取得了一定的成果。然而，近年来，以对象为中心的表示学习被提出作为在深度学习框架内实现视觉推理的一种方法。以对象为中心的模型尝试将输入场景建模为对象的组合以及它们之间的关系。为此，这些模型使用多种注意机制将场景中的各个对象与背景和其他对象分开。在这项工作中，我们在几个以对象为中心的模型以及 ResNet-50 基线中测试了关系学习和泛化。之前的研究主要关注同异任务以评估 DNN 中的关系推理，与此相反，我们使用了一组来自比较认知文献的具有不同难度的任务。我们的结果表明，即使在许多不分布的情况下，以对象为中心的模型也能够分离场景中的不同对象。在我们更简单的任务中，与 ResNet-50 基线相比，这提高了他们学习和概括视觉关系的能力。然而，以对象为中心的模型在我们更困难的任务和条件下仍然举步维艰。我们的结论是，抽象视觉推理仍然是 DNN 面临的一个开放挑战，包括以对象为中心的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.12675v1><br />
**Code:** <https://github.com/GuillermoPuebla/object-centric-reasoning>**<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Slot-VLM: SlowFast Slots for Video-Language Modeling**<br />
**Title_cn:** Slot-VLM：用于视频语言建模的 SlowFast 插槽<br />
**Authors:** Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu<br />
**Abstract:** <details><summary>原文: </summary>Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大型语言模型 (LLM) 进步的推动下，视频语言模型 (VLM) 正在开辟视频理解的新领域。一个关键的挑战是开发一种有效的方法，将视频内容封装到一组代表性令牌中，以与法学硕士保持一致。在这项工作中，我们引入了 Slot-VLM，这是一种新颖的框架，旨在根据对象和事件的视觉表示生成语义分解的视频标记，以促进 LLM 推理。特别是，我们设计了一个 SlowFast Slots 模块，即 SF-Slots，它自适应地将来自 CLIP 视觉编码器的密集视频标记聚合到一组代表性插槽中。为了同时考虑空间对象细节和变化的时间动态，SF-Slots 采用双分支结构构建。 Slow-Slots 分支专注于从高空间分辨率但低（慢）帧采样率的特征中提取以对象为中心的槽，强调详细的对象信息。相反，Fast-Slots 分支被设计为从高时间采样率但低空间分辨率特征中学习以事件为中心的时隙。这些互补的槽位组合在一起形成视觉上下文，作为法学硕士的输入以实现高效的问题回答。我们的实验结果证明了 Slot-VLM 的有效性，它在视频问答方面实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13088v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing**<br />
**Title_cn:** UniEdit：用于视频运动和外观编辑的统一免调优框架<br />
**Authors:** Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本引导视频编辑的最新进展在外观编辑（例如风格化）方面展示了有希望的结果。然而，将视频编辑与图像编辑区分开来的时间维度上的视频动作编辑（例如，从吃东西到挥手）尚未得到充分探索。在这项工作中，我们提出了 UniEdit，这是一个免调整框架，通过利用反转生成框架中预先训练的文本到视频生成器的功能，支持视频运动和外观编辑。为了在保留源视频内容的同时实现运动编辑，基于时间和空间自注意力层分别编码帧间和帧内依赖性的见解，我们引入辅助运动参考和重建分支来产生文本引导的运动和源分别具有特点。然后将获得的特征通过时间和空间自注意力层注入到主编辑路径中。大量实验表明，UniEdit 涵盖了视频动作编辑和各种外观编辑场景，并超越了最先进的方法。我们的代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.13185v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PIP-Net: Pedestrian Intention Prediction in the Wild**<br />
**Title_cn:** PIP-Net：野外行人意图预测<br />
**Authors:** Mohsen Azarmi, Mahdi Rezaei, He Wang, Sebastien Glaser<br />
**Abstract:** <details><summary>原文: </summary>Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶汽车（AV）准确的行人意图预测（PIP）是该领域当前的研究挑战之一。在本文中，我们介绍了 PIP-Net，这是一种新颖的框架，旨在预测现实城市场景中自动驾驶汽车的行人过路意图。我们提供两种 PIP-Net 变体，专为不同的摄像机安装和设置而设计。利用驾驶场景的运动学数据和空间特征，所提出的模型采用了基于循环和时间注意力的解决方案，超越了最先进的性能。为了增强道路使用者的视觉表示及​​其与自我车辆的接近度，我们引入了分类深度特征图，结合局部运动流特征，提供了对场景动态的丰富洞察。此外，我们还探讨了扩大摄像头视野的影响，从围绕自我车辆的一个摄像头增加到三个摄像头，从而增强模型的情境感知。该模型根据交通场景和道路环境，能够提前4秒预测行人过街意图，这是当前行人意图预测研究的突破。最后，我们首次提出了 Urban-PIP 数据集，这是一个定制的行人意图预测数据集，在现实世界的自动驾驶场景中具有多摄像头注释。</details>
**PDF:** <http://arxiv.org/pdf/2402.12810v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer**<br />
**Title_cn:** RhythmFormer：基于分层时间周期变换器提取 rPPG 信号<br />
**Authors:** Bochao Zou, Zizheng Guo, Jiansheng Chen, Huimin Ma<br />
**Abstract:** <details><summary>原文: </summary>Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc. Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the Transformer was assumed to be advantageous for such signals. However, existing approaches have not conclusively demonstrated the superior performance of Transformer over traditional convolutional neural network methods, this gap may stem from a lack of thorough exploration of rPPG periodicity. In this paper, we propose RhythmFormer, a fully end-to-end transformer-based method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG. The core module, Hierarchical Temporal Periodic Transformer, hierarchically extracts periodic features from multiple temporal scales. It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem is proposed to guide self-attention to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly. RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches. The codes are available at https://github.com/zizheng-guo/RhythmFormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>远程光电体积描记法（rPPG）是一种基于面部视频检测生理信号的非接触式方法，在医疗保健、情感计算、反欺骗等各种应用中具有很高的潜力。由于rPPG的周期性，长期假设 Transformer 的范围依赖性捕获能力对于此类信号是有利的。然而，现有方法尚未最终证明 Transformer 相对于传统卷积神经网络方法的优越性能，这种差距可能源于缺乏对 rPPG 周期性的彻底探索。在本文中，我们提出了 RhythmFormer，一种完全基于端到端变压器的方法，用于通过明确利用 rPPG 的准周期性质来提取 rPPG 信号。核心模块Hierarchical Temporal periodic Transformer从多个时间尺度分层提取周期性特征。它利用基于时域周期性的动态稀疏注意力，允许对 rPPG 特征进行细粒度建模。此外，提出了一种融合干来有效地引导自注意力到rPPG特征，并且可以很容易地转移到现有方法中以显着提高其性能。与以前的方法相比，RhythmFormer 在综合实验中以更少的参数实现了最先进的性能，并降低了计算复杂性。代码可在 https://github.com/zizheng-guo/RhythmFormer 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.12788v1><br />
**Code:** <https://github.com/zizheng-guo/rhythmformer>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning**<br />
**Title_cn:** VADv2：通过概率规划实现端到端矢量化自动驾驶<br />
**Authors:** Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang<br />
**Abstract:** <details><summary>原文: </summary>Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.</details>
**Abstract_cn:** <details><summary>译文: </summary>从大规模驾驶演示中学习类人驾驶策略是有前途的，但规划的不确定性和非确定性使其具有挑战性。在这项工作中，为了应对不确定性问题，我们提出了VADv2，一种基于概率规划的端到端驾驶模型。 VADv2以流式方式将多视图图像序列作为输入，将传感器数据转换为环境令牌嵌入，输出动作的概率分布，并对一个动作进行采样来控制车辆。仅使用摄像头传感器，VADv2 就能在 CARLA Town05 基准上实现最先进的闭环性能，显着优于所有现有方法。即使没有基于规则的包装器，它也能以完全端到端的方式稳定运行。闭环演示位于 https://hgao-cv.github.io/VADv2。</details>
**PDF:** <http://arxiv.org/pdf/2402.13243v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research**<br />
**Title_cn:** 用于城市测绘和目标跟踪研究的四旋翼飞行器的设计和飞行演示<br />
**Authors:** Collin Hague, Nick Kakavitsas, Jincheng Zhang, Chris Beam, Andrew Willis, Artur Wolek<br />
**Abstract:** <details><summary>原文: </summary>This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research. The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection. An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate. The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots. A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm. The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了用于城市测绘、危险规避和目标跟踪研究的带有成像传感器的小型四旋翼飞行器的硬件设计和飞行演示。该车配备了五个摄像头，包括两对可实现近乎全向视角的鱼眼立体摄像头和一个两轴万向摄像头。运行机器人操作系统软件的板载 NVIDIA Jetson Orin Nano 计算机用于数据收集。实现了自主跟踪行为来协调四旋翼飞行器和万向摄像机的运动，以跟踪移动的 GPS 坐标。该数据收集系统通过飞行测试进行了演示，该测试跟踪一辆移动的带有 GPS 标签的车辆穿过一系列道路和停车场。使用直接稀疏里程计 (DSO) 算法根据收集的图像重建环境地图。四旋翼飞行器的性能还通过噪声、通信范围、悬停时的电池电压和最大速度测试来表征。</details>
**PDF:** <http://arxiv.org/pdf/2402.13195v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **exploreCOSMOS: Interactive Exploration of Conditional Statistical Shape Models in the Web-Browser**<br />
**Title_cn:** exploreCOSMOS：网络浏览器中条件统计形状模型的交互式探索<br />
**Authors:** Maximilian Hahn, Bernhard Egger<br />
**Abstract:** <details><summary>原文: </summary>Statistical Shape Models of faces and various body parts are heavily used in medical image analysis, computer vision and visualization. Whilst the field is well explored with many existing tools, all of them aim at experts, which limits their applicability. We demonstrate the first tool that enables the convenient exploration of statistical shape models in the browser, with the capability to manipulate the faces in a targeted manner. This manipulation is performed via a posterior model given partial observations. We release our code and application on GitHub https://github.com/maximilian-hahn/exploreCOSMOS</details>
**Abstract_cn:** <details><summary>译文: </summary>面部和身体各个部位的统计形状模型大量用于医学图像分析、计算机视觉和可视化。虽然许多现有工具对该领域进行了很好的探索，但所有这些工具都针对专家，这限制了它们的适用性。我们演示了第一个工具，可以在浏览器中方便地探索统计形状模型，并且能够有针对性地操纵面部。这种操作是通过给定部分观察结果的后验模型来执行的。我们在 GitHub 上发布了代码和应用程​​序 https://github.com/maximilian-hahn/exploreCOSMOS</details>
**PDF:** <http://arxiv.org/pdf/2402.13131v1><br />
**Code:** <https://github.com/maximilian-hahn/explorecosmos>**<br />
>>**index:** 4<br />
**Title:** **Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera**<br />
**Title_cn:** 注意出瞳间隙：重新审视标准全光相机的本质<br />
**Authors:** Tim Michels, Daniel Mäckelmann, Reinhard Koch<br />
**Abstract:** <details><summary>原文: </summary>Among the common applications of plenoptic cameras are depth reconstruction and post-shot refocusing. These require a calibration relating the camera-side light field to that of the scene. Numerous methods with this goal have been developed based on thin lens models for the plenoptic camera's main lens and microlenses. Our work addresses the often-overlooked role of the main lens exit pupil in these models and specifically in the decoding process of standard plenoptic camera (SPC) images. We formally deduce the connection between the refocusing distance and the resampling parameter for the decoded light field and provide an analysis of the errors that arise when the exit pupil is not considered. In addition, previous work is revisited with respect to the exit pupil's role and all theoretical results are validated through a ray-tracing-based simulation. With the public release of the evaluated SPC designs alongside our simulation and experimental data we aim to contribute to a more accurate and nuanced understanding of plenoptic camera optics.</details>
**Abstract_cn:** <details><summary>译文: </summary>全光相机的常见应用包括深度重建和拍摄后重新聚焦。这些需要将相机侧光场与场景光场相关联的校准。基于全光相机主透镜和微透镜的薄透镜模型，已经开发了许多实现此目标的方法。我们的工作解决了主镜头出射光瞳在这些模型中经常被忽视的作用，特别是在标准全光相机 (SPC) 图像的解码过程中。我们正式推导了解码光场的重聚焦距离和重采样参数之间的联系，并对不考虑出瞳时出现的误差进行了分析。此外，我们重新审视了之前关于出瞳作用的工作，并且所有理论结果都通过基于光线追踪的模拟进行了验证。通过公开发布经过评估的 SPC 设计以及我们的模拟和实验数据，我们的目标是帮助人们更准确、更细致地了解全光相机光学器件。</details>
**PDF:** <http://arxiv.org/pdf/2402.12891v1><br />
**Code:** <https://gitlab.com/ungetym/blender-camera-generator>**<br />
>>**index:** 5<br />
**Title:** **ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation**<br />
**Title_cn:** ICON：通过病变感知混合增强提高放射学报告生成的报告间一致性<br />
**Authors:** Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu<br />
**Abstract:** <details><summary>原文: </summary>Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.</details>
**Abstract_cn:** <details><summary>译文: </summary>先前关于放射学报告生成的研究在提高生成报告的临床准确性方面取得了重大进展。在本文中，我们强调它应具备的另一个关键品质，即报告间一致性，指的是为语义等效的射线照片生成一致报告的能力。对于确保系统的可信度而言，这种质量甚至比整体报告的准确性更重要，因为容易提供相互矛盾的结果的系统会严重削弱用户的信任。遗憾的是，现有方法很难保持报告间的一致性，表现出对常见模式的偏见和对病变变异的敏感性。为了解决这个问题，我们提出了 ICON，它提高了放射学报告生成的报告间一致性。为了增强系统捕获语义等效病变相似性的能力，我们的方法首先从输入图像中提取病变并检查其特征。然后，我们引入了一种病变感知混合增强技术，通过在训练阶段对它们进行线性插值，确保语义等效病变的表示与相同的属性对齐。对三个公开可用的胸部 X 射线数据集进行的广泛实验验证了我们方法的有效性，无论是在提高生成报告的一致性和准确性方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.12844v1><br />
**Code:** <https://github.com/wjhou/icon>**<br />
>>**index:** 6<br />
**Title:** **A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis**<br />
**Title_cn:** 用于在文本到图像合成中生成模型首选提示的用户友好框架<br />
**Authors:** Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>精心设计的提示已经证明了指导文本到图像模型生成令人惊叹的图像的潜力。尽管现有的提示工程方法可以提供高级指导，但由于新手用户输入的提示与模型偏好的提示之间存在差异，新手用户通过手动输入提示来达到预期结果具有挑战性。为了弥合用户输入行为和模型训练数据集之间的分布差距，我们首先构建了一个新颖的粗细粒度提示数据集（CFP），并提出了一种新颖的用户友好的细粒度文本生成框架（UF-FGTG）用于自动提示优化。对于 CFP，我们为文本到图像任务构建了一个新颖的数据集，该数据集结合了粗粒度和细粒度的提示，以促进自动提示生成方法的开发。对于 UF-FGTG，我们提出了一种新颖的框架，可以自动将用户输入提示转换为模型首选提示。具体来说，我们提出了一个提示细化器，它不断重写提示，使用户能够选择符合其独特需求的结果。同时，我们将文本到图像模型中与图像相关的损失函数集成到文本生成的训练过程中，以生成模型首选的提示。此外，我们提出了一个自适应特征提取模块，以确保生成结果的多样性。实验表明，与之前最先进的方法相比，我们的方法能够生成更具视觉吸引力和多样化的图像，在六个质量和美学指标上平均提高了 5%。</details>
**PDF:** <http://arxiv.org/pdf/2402.12760v1><br />
**Code:** <https://github.com/naylenv/uf-fgtg>**<br />
>>**index:** 7<br />
**Title:** **Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference**<br />
**Title_cn:** 使用多模型推理专家的引导组合对 OCT 图像进行去噪<br />
**Authors:** Aytaç Özkan, Elena Stoykova, Thomas Sikora, Violeta Madjarova<br />
**Abstract:** <details><summary>原文: </summary>In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy. Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation. Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE). This method combines block-matched implementation of the SMoE algorithm with an enhanced autoencoder architecture, offering efficient speckle noise reduction while retaining critical image details. Our method stands out by providing improved edge definition and reduced processing time. Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics.</details>
**Abstract_cn:** <details><summary>译文: </summary>在光学相干断层扫描 (OCT) 中，散斑噪声严重影响图像质量，影响诊断准确性。当前的方法，包括传统的滤波和深度学习技术，在降噪和细节保留方面存在局限性。为了解决这些挑战，本研究引入了一种新颖的去噪算法，即具有多模型推理和自动编码器的专家块匹配引导混合算法 (BM-SMoE-AE)。该方法将 SMoE 算法的块匹配实现与增强的自动编码器架构相结合，在保留关键图像细节的同时提供高效的散斑噪声抑制。我们的方法因提供改进的边缘清晰度和减少处理时间而脱颖而出。与现有去噪技术的比较分析表明，BM-SMoE-AE 在保持图像完整性和增强 OCT 图像在医学诊断中的可用性方面具有卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.12735v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation**<br />
**Title_cn:** 使用运动模仿和基于物理的模拟推进基于单目视频的步态分析<br />
**Authors:** Nikolaos Smyrnakis, Tasos Karakostas, R. James Cotton<br />
**Abstract:** <details><summary>原文: </summary>Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.</details>
**Abstract_cn:** <details><summary>译文: </summary>对从智能手机获得的视频进行步态分析将为检测和量化步态障碍提供许多临床机会。然而，现有的从视频中估计步态参数的方法可能会产生物理上令人难以置信的结果。为了克服这个问题，我们使用强化学习来训练策略来控制人体运动的物理模拟，以复制视频中看到的运动。这迫使推断的运动在物理上合理，同时提高推断的步长和行走速度的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.12676v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective**<br />
**Title_cn:** 机器学习在数据变化方面的进展的全面回顾：跨领域视角<br />
**Authors:** Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen<br />
**Abstract:** <details><summary>原文: </summary>Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus provide a novel scope for researchers to explore contemporary technical strategies, learn industrial applications, and identify future directions for addressing data change challenges.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的人工智能（AI）技术在各个学术领域和行业中展现出显着的发展。然而，在现实世界中，动态数据给部署人工智能模型带来了主要挑战。意外的数据变化会导致人工智能模型的性能严重下降。根据数据变化的背景，我们确定了两个主要的相关研究领域：领域转移和概念漂移。尽管这两个流行的研究领域旨在解决分布变化和非平稳数据流问题，但其基本属性仍然相似，这也鼓励了类似的技术方法。在这篇综述中，我们将领域转移和概念漂移重新组合为一个研究问题，即数据变化问题，并系统地概述了这两个研究领域的最先进方法。我们提出了一个三阶段问题分类方案来链接两个技术领域的关键思想。因此，我们为研究人员探索当代技术策略、学习工业应用并确定应对数据变化挑战的未来方向提供了一个新颖的范围。</details>
**PDF:** <http://arxiv.org/pdf/2402.12627v1><br />
**Code:** null<br />

