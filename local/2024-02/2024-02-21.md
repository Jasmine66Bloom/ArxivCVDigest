## [UPDATED!] **2024-02-21** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Geometry-Informed Neural Networks**<br />
**Title_cn:** 几何信息神经网络<br />
**Authors:** Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter<br />
**Abstract:** <details><summary>原文: </summary>We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了几何信息神经网络（GINN）的概念，其中包括（i）在几何约束下学习，（ii）神经场作为合适的表示，以及（iii）为几何中经常遇到的欠定系统生成不同的解决方案任务。值得注意的是，GINN 公式不需要训练数据，因此可以被视为纯粹由约束驱动的生成模型。我们添加了显式的多样性损失来减轻模式崩溃。我们考虑了几个约束，特别是组件的连通性，我们通过莫尔斯理论将其转换为可微损失。通过实验，我们证明了 GINN 学习范式在复杂程度不断增加的一系列二维和三维场景中的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14009v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning**<br />
**Title_cn:** 独特的图像字幕：在 CLIP 引导强化学习中利用真实字幕<br />
**Authors:** Antoine Chaffin, Ewa Kijak, Vincent Claveau<br />
**Abstract:** <details><summary>原文: </summary>Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用教师强制训练图像字幕模型会产生非常通用的样本，而更独特的字幕在检索应用程序中非常有用，或者生成描述图像的替代文本以供可访问性。强化学习（RL）允许使用生成的标题和输入图像之间的跨模式检索相似度分数作为指导训练的奖励，从而产生更独特的标题。最近的研究表明，预先训练的跨模式检索模型可用于提供这种奖励，完全消除对参考标题的需要。然而，我们在本文中认为，Ground Truth (GT) 字幕在这个 RL 框架中仍然有用。我们提出了一种新的图像字幕模型训练策略，以不同的方式利用 GT 字幕。首先，它们可用于训练一个简单的 MLP 判别器，该判别器可作为正则化以防止奖励黑客攻击并确保生成的字幕的流畅性，从而产生扩展用于多模态输入的文本 GAN 设置。其次，它们可以作为 RL 策略中的附加轨迹，从而产生由 GT 与图像的相似性加权的教师强制损失。该目标充当基于 GT 字幕分发的附加学习信号。第三，当添加到用于计算建议的对比奖励以减少梯度估计的方差的字幕池中时，它们可以作为强大的基线。 MS-COCO 上的实验证明了所提出的训练策略在保持高写作质量的同时生成高度独特的字幕的兴趣。</details>
**PDF:** <http://arxiv.org/pdf/2402.13936v1><br />
**Code:** <https://github.com/nohtow/wtf-rl>**<br />
>>**index:** 3<br />
**Title:** **Tumor segmentation on whole slide images: training or prompting?**<br />
**Title_cn:** 整个幻灯片图像上的肿瘤分割：训练还是提示？<br />
**Authors:** Huaqian Wu, Clara Brémond-Martin, Kévin Bouaou, Cédric Clouchoux<br />
**Abstract:** <details><summary>原文: </summary>Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>肿瘤分割是癌症诊断的关键任务。鉴于组织学中整个幻灯片图像 (WSI) 的巨大尺寸，用于 WSI 分类的深度学习方法主要在补丁级别或超像素级别上运行。然而，这些解决方案通常难以捕获全局 WSI 信息，并且无法直接生成二进制掩码。对 WSI 进行下采样并执行语义分割是另一种可能的方法。虽然这种方法提供了计算效率，但它需要大量的注释数据，因为分辨率降低可能会导致信息丢失。视觉提示是一种新颖的范例，它允许模型通过对输入空间进行细微修改来执行新任务，而不是调整模型本身。这种方法在许多计算机视觉任务中展现出了有希望的结果。在本文中，我们展示了视觉提示在三个不同器官肿瘤分割背景下的功效。与针对此特定任务训练的经典方法相比，我们的研究结果表明，通过适当的提示示例，视觉提示可以实现可比或更好的性能，而无需进行大量微调。</details>
**PDF:** <http://arxiv.org/pdf/2402.13932v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion**<br />
**Title_cn:** NeuralDiffuser：具有主要视觉特征引导扩散的可控 fMRI 重建<br />
**Authors:** Haoyu Li, Hao Wu, Badong Chen<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于潜在扩散模型 (LDM) 从功能性磁共振成像 (fMRI) 中重建视觉刺激可以对大脑进行细粒度检索。重建细节（例如结构、背景、纹理、颜色等）的凝聚力对齐仍然是一个挑战。此外，即使在相同条件下，LDM 也会产生不同的图像结果。为此，我们首先揭示了基于 LDM 的方法的神经科学视角，该方法是基于来自大量图像的预训练知识的自上而下的创建，但缺乏细节驱动的自下而上的感知，导致细节不忠实。我们提出了 NeuralDiffuser，它引入了主要视觉特征指导，以梯度的形式提供细节线索，扩展了基于 LDM 的方法的自下而上的过程，以实现忠实的语义和细节。我们还开发了一种新颖的指导策略，以确保重复重建的一致性，而不是各种结果的一致性。我们获得了 NeuralDiffuser 在自然感官数据集 (NSD) 上最先进的性能，它提供了更忠实的细节和一致的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.13809v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Scalable Methods for Brick Kiln Detection and Compliance Monitoring from Satellite Imagery: A Deployment Case Study in India**<br />
**Title_cn:** 利用卫星图像进行砖窑检测和合规性监测的可扩展方法：印度的部署案例研究<br />
**Authors:** Rishabh Mondal, Zeel B Patel, Vannsh Jani, Nipun Batra<br />
**Abstract:** <details><summary>原文: </summary>Air pollution kills 7 million people annually. Brick manufacturing industry is the second largest consumer of coal contributing to 8%-14% of air pollution in Indo-Gangetic plain (highly populated tract of land in the Indian subcontinent). As brick kilns are an unorganized sector and present in large numbers, detecting policy violations such as distance from habitat is non-trivial. Air quality and other domain experts rely on manual human annotation to maintain brick kiln inventory. Previous work used computer vision based machine learning methods to detect brick kilns from satellite imagery but they are limited to certain geographies and labeling the data is laborious. In this paper, we propose a framework to deploy a scalable brick kiln detection system for large countries such as India and identify 7477 new brick kilns from 28 districts in 5 states in the Indo-Gangetic plain. We then showcase efficient ways to check policy violations such as high spatial density of kilns and abnormal increase over time in a region. We show that 90% of brick kilns in Delhi-NCR violate a density-based policy. Our framework can be directly adopted by the governments across the world to automate the policy regulations around brick kilns.</details>
**Abstract_cn:** <details><summary>译文: </summary>空气污染每年导致 700 万人死亡。砖块制造业是第二大煤炭消费行业，造成印度恒河平原（印度次大陆人口稠密的地区）8%-14%的空气污染。由于砖窑是一个无组织的部门并且数量庞大，因此检测距栖息地距离等政策违规行为并非易事。空气质量和其他领域专家依靠人工注释来维护砖窑库存。以前的工作使用基于计算机视觉的机器学习方法从卫星图像中检测砖窑，但它们仅限于某些地理位置，并且标记数据很费力。在本文中，我们提出了一个框架，为印度等大国部署可扩展的砖窑检测系统，并识别来自印度恒河平原 5 个邦的 28 个地区的 7477 个新砖窑。然后，我们展示了检查政策违规行为的有效方法，例如窑炉的高空间密度以及一个地区随时间的异常增长。我们发现德里国家首都区 90% 的砖窑违反了基于密度的政策。我们的框架可以被世界各国政府直接采用，以实现砖窑相关政策法规的自动化。</details>
**PDF:** <http://arxiv.org/pdf/2402.13796v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion**<br />
**Title_cn:** Cas-DiffCom：用于婴儿纵向超分辨率 3D 医学图像补全的级联扩散模型<br />
**Authors:** Lianghu Guo, Tianli Tao, Xinyi Cai, Zihao Zhu, Jiawei Huang, Lixuan Zhu, Zhuoyang Gu, Haifeng Tang, Rui Zhou, Siyan Han, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution. We applied our proposed method to the Baby Connectome Project (BCP) dataset. The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion. We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field.</details>
**Abstract_cn:** <details><summary>译文: </summary>婴儿早期是行为和神经认知快速且动态的神经发育时期。纵向磁共振成像（MRI）是通过捕获大脑结构的发育轨迹来研究这一关键阶段的有效工具。然而，由于参与者退出和扫描失败，纵向MRI采集总是遇到严重的数据缺失问题，这使得纵向婴儿脑图谱构建和发育轨迹描绘相当具有挑战性。由于基于人工智能的生成模型的发展，神经图像补全已成为保留尽可能多的可用数据的强大技术。然而，当前的图像补全方法通常会遇到每个个体在时间维度上不一致的问题，从而影响整体质量。为了解决这个问题，我们的论文提出了一种两级级联扩散模型 Cas-DiffCom，用于密集和纵向 3D 婴儿脑 MRI 完成和超分辨率。我们将我们提出的方法应用于婴儿连接组项目（BCP）数据集。实验结果验证了Cas-DiffCom在纵向婴儿脑图像补全方面实现了个体一致性和高保真度。我们进一步将生成的婴儿大脑图像应用于两个下游任务，即脑组织分割和发育轨迹描绘，以宣告其在神经科学领域以任务为导向的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.13776v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model**<br />
**Title_cn:** SRNDiff：使用条件扩散模型进行短期降雨临近预报<br />
**Authors:** Xudong Ling, Chaorong Li, Fengqing Qin, Peng Yang, Yuanyuan Huang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型广泛应用于图像生成中，因为它们可以生成高质量且真实的样本。这与生成对抗网络（GAN）和变分自动编码器（VAE）形成鲜明对比，它们在图像质量方面有一些局限性。我们将扩散模型引入降水预报任务，并提出了一种使用条件扩散模型的短期降水临近预报基于历史观测数据，称为 SRNDiff。通过在去噪过程中加入额外的条件解码器模块，SRNDiff 实现了端到端的条件降雨预测。 SRNDiff 由两个网络组成：去噪网络和条件编码器网络。条件网络由多个独立的UNet网络组成。这些网络提取不同分辨率的条件特征图，提供准确的条件信息，指导扩散模型进行条件生成。SRNDiff 在预测精度方面超越了 GAN，尽管它需要更多的计算资源。SRNDiff 模型在训练过程中表现出更高的稳定性和效率。基于GAN的方法，生成高质量的降水分布样本，更好地反映未来实际降水情况。这充分验证了扩散模型在降水预报中的优势和潜力，为加强降水预报提供了新的见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.13737v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation**<br />
**Title_cn:** 具有 2D 三平面和 3D 小波表示的混合视频扩散模型<br />
**Authors:** Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo<br />
**Abstract:** <details><summary>原文: </summary>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).</details>
**Abstract_cn:** <details><summary>译文: </summary>由于视频错综复杂的高维性和复杂性，生成合成所需真实内容的高质量视频是一项具有挑战性的任务。最近的几种基于扩散的方法通过使用传统的视频自动编码器架构将视频压缩到较低维的潜在空间，表现出了相当的性能。然而，这种采用标准逐帧 2D 和 3D 卷积的方法无法充分利用视频的时空性质。为了解决这个问题，我们提出了一种新颖的混合视频扩散模型，称为 HVDM，它可以更有效地捕获时空依赖性。 HVDM 通过混合视频自动编码器进行训练，该编码器提取视频的解缠结表示，包括：(i) 由 2D 投影潜伏捕获的全局上下文信息 (ii) 通过小波分解的 3D 卷积捕获的局部体积信息 (iii)用于改进视频重建的频率信息。基于这种解开的表示，我们的混合自动编码器提供了更全面的视频潜在能力，丰富了生成的具有精细结构和细节的视频。视频生成基准（UCF101、SkyTimelapse 和 TaiChi）的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频、和视频动态控制）。</details>
**PDF:** <http://arxiv.org/pdf/2402.13729v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Flexible Physical Camouflage Generation Based on a Differential Approach**<br />
**Title_cn:** 基于差分方法的灵活物理伪装生成<br />
**Authors:** Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, Quan Pan<br />
**Abstract:** <details><summary>原文: </summary>This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究引入了一种新颖的神经渲染方法，在广泛的 3D 渲染框架内专门为对抗性伪装量身定制。我们的方法名为 FPA，它超越了传统技术，忠实地模拟光照条件和材质变化，确保在 3D 目标上呈现细致入微且真实的纹理。为了实现这一目标，我们采用一种生成方法，从扩散模型中学习对抗模式。这涉及到结合专门设计的对抗性损失和隐蔽约束损失，以保证物理世界中伪装的对抗性和隐蔽性。此外，我们展示了所提出的贴纸模式伪装的有效性，展示了其在不损害对抗信息的情况下覆盖目标的能力。通过实证和物理实验，FPA在攻击成功率和可转移性方面表现出了很强的性能。此外，设计的贴纸模式迷彩，加上隐蔽性约束，可以适应环境，产生不同风格的纹理。我们的研究结果强调了 FPA 方法在对抗性伪装应用中的多功能性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.13575v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **ToDo: Token Downsampling for Efficient Generation of High-Resolution Images**<br />
**Title_cn:** ToDo：令牌下采样以高效生成高分辨率图像<br />
**Authors:** Ethan Smith, Nayan Saxena, Aninda Saha<br />
**Abstract:** <details><summary>原文: </summary>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.</details>
**Abstract_cn:** <details><summary>译文: </summary>注意力机制对于图像扩散模型至关重要，然而，它们的二次计算复杂性限制了我们在合理的时间和内存限制内可以处理的图像大小。本文研究了生成图像模型中密集注意力的重要性，生成图像模型通常包含冗余特征，使其适合稀疏注意力机制。我们提出了一种新颖的免训练方法 ToDo，它依赖于键和值标记的标记下采样，以将稳定扩散推理加速到常见尺寸的 2 倍，以及高分辨率（如 2048x2048）的 4.5 倍或更多。我们证明，我们的方法在平衡有效吞吐量和保真度方面优于以前的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.13573v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models**<br />
**Title_cn:** 对比提示可改善文本到图像扩散模型中的解缠结<br />
**Authors:** Chen Wu, Fernando De la Torre<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a "baseline" that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像的扩散模型在图像合成方面取得了显着的性能，而文本界面并不总是提供对某些图像因素的细粒度控制。例如，更改文本中的单个标记可能会对图像产生意想不到的影响。本文展示了对无分类器指导的简单修改可以帮助理清文本到图像模型中的图像因素。我们的方法“对比指导”的关键思想是用两个最小标记不同的提示来表征预期因素：正提示描述要合成的图像，基线提示充当解开其他因素的“基线”。对比指导是一种通用方法，我们在三种情况下说明其优点：(1) 指导在对象类上训练的特定领域扩散模型，(2) 获得用于文本到图像生成的连续、类似装备的控制，以及(3) 提高零样本图像编辑器的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13490v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Scene Prior Filtering for Depth Map Super-Resolution**<br />
**Title_cn:** 用于深度图超分辨率的场景先验过滤<br />
**Authors:** Zhengxue Wang, Zhiqiang Yan, Ming-Hsuan Yang, Jinshan Pan, Jian Yang, Ying Tai, Guangwei Gao<br />
**Abstract:** <details><summary>原文: </summary>Multi-modal fusion is vital to the success of super-resolution of depth images. However, commonly used fusion strategies, such as addition and concatenation, fall short of effectively bridging the modal gap. As a result, guided image filtering methods have been introduced to mitigate this issue. Nevertheless, it is observed that their filter kernels usually encounter significant texture interference and edge inaccuracy. To tackle these two challenges, we introduce a Scene Prior Filtering network, SPFNet, which utilizes the priors surface normal and semantic map from large-scale models. Specifically, we design an All-in-one Prior Propagation that computes the similarity between multi-modal scene priors, \textit{i.e.}, RGB, normal, semantic, and depth, to reduce the texture interference. In addition, we present a One-to-one Prior Embedding that continuously embeds each single-modal prior into depth using Mutual Guided Filtering, further alleviating the texture interference while enhancing edges. Our SPFNet has been extensively evaluated on both real and synthetic datasets, achieving state-of-the-art performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态融合对于深度图像超分辨率的成功至关重要。然而，常用的融合策略，例如加法和串联，无法有效地弥合模态间隙。因此，引入了引导图像过滤方法来缓解这个问题。然而，据观察，它们的滤波器内核通常会遇到明显的纹理干扰和边缘不准确。为了解决这两个挑战，我们引入了场景先验过滤网络 SPFNet，它利用大规模模型的先验表面法线和语义图。具体来说，我们设计了一种一体式先验传播，它计算多模态场景先验、\textit{i.e.}、RGB、法线、语义和深度之间的相似性，以减少纹理干扰。此外，我们提出了一种一对一的先验嵌入，它使用相互引导过滤将每个单模态先验连续嵌入到深度中，进一步减轻纹理干扰，同时增强边缘。我们的 SPFNet 已在真实数据集和合成数据集上进行了广泛评估，实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13876v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models**<br />
**Title_cn:** VL-Trojan：针对自回归视觉语言模型的多模式指令后门攻击<br />
**Authors:** Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>自回归视觉语言模型 (VLM) 在多模态环境中展示了令人印象深刻的几次学习能力。最近，提出了多模式指令调整来进一步增强指令跟踪能力。然而，我们发现了指令调整期间自回归 VLM 的后门攻击所带来的潜在威胁。攻击者可以通过注入带有嵌入在指令或图像中的触发器的中毒样本来植入后门，从而能够利用预定义的触发器恶意操纵受害者模型的预测。然而，自回归 VLM 中的冻结视觉编码器对传统图像触发器的学习施加了限制。此外，攻击者在访问受害者模型的参数和架构时可能会遇到限制。为了应对这些挑战，我们提出了一种多模式指令后门攻击，即VL-Trojan。我们的方法通过隔离和聚类策略促进图像触发学习，并通过迭代字符级文本触发生成方法增强黑盒攻击效率。我们的攻击在推理过程中成功诱导了目标输出，显着超过了 ASR 的基线 (+62.52\%)。此外，它还展示了跨各种模型规模和少量上下文推理场景的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.13851v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models**<br />
**Title_cn:** CODIS：多模态大语言模型的上下文相关视觉理解基准测试<br />
**Authors:** Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner. View our project website at https://thunlp-mt.github.io/CODIS.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型（MLLM）在结合视觉和语言的各种任务中表现出了有希望的结果。随着这些模型越来越融入研究和应用，对其能力进行全面评估变得越来越重要。然而，大多数现有的基准测试都没有考虑到，在某些情况下，图像需要在更广泛的背景下进行解释。在这项工作中，我们引入了一个名为 CODIS 的新基准，旨在评估模型使用自由格式文本中提供的上下文来增强视觉理解的能力。我们的研究结果表明，MLLM 在此基准上的表现始终低于人类。进一步的分析证实，这些模型很难有效地提取和利用上下文信息来提高对图像的理解。这强调了增强 MLLM 以上下文相关方式理解视觉效果的能力的迫切需要。查看我们的项目网站：https://thunlp-mt.github.io/CODIS。</details>
**PDF:** <http://arxiv.org/pdf/2402.13607v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation**<br />
**Title_cn:** 用于电子商务产品描述生成的多模式上下文调整方法<br />
**Authors:** Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种从图像生成产品描述的新设置，并通过营销关键词进行增强。它利用视觉和文本信息的综合力量来创建更适合产品独特功能的描述。对于这种设置，以前的方法利用视觉和文本编码器对图像和关键字进行编码，并采用基于语言模型的解码器来生成产品描述。然而，由于同品类产品具有相似的文案，生成的描述往往不准确且笼统，并且在大规模样本上优化整体框架使得模型专注于常用词而忽略了产品特征。为了缓解这个问题，我们提出了一种简单有效的多模态上下文调优方法，名为 ModICT，该方法引入类似的产品样本作为参考，并利用语言模型的上下文学习能力来生成描述。在训练过程中，我们保持视觉编码器和语言模型冻结，专注于优化负责创建多模式上下文引用和动态提示的模块。这种方法保留了大型语言模型（LLM）的语言生成能力，促进了描述多样性的大幅增加。为了评估 ModICT 在各种语言模型规模和类型中的有效性，我们从电子商务领域内的三个不同产品类别收集数据。大量实验表明，与传统方法相比，ModICT 显着提高了生成结果的准确性（在 Rouge-L 上提高了 3.3%）和多样性（在 D-5 上提高了 9.4%）。我们的研究结果强调了 ModICT 作为一种有价值的工具的潜力，可以在广泛的应用中增强产品描述的自动生成。</details>
**PDF:** <http://arxiv.org/pdf/2402.13587v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement**<br />
**Title_cn:** 通过部分相关性增强改进视频语料库时刻检索<br />
**Authors:** Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng<br />
**Abstract:** <details><summary>原文: </summary>Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query. The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains information-rich frames, and not all are relevant to the query. Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content. (2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information. Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR. However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval. We argue that effectively capturing the partial relevance between the query and video is essential for the VCMR task. To this end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video retrieval and moment localization. To align with their distinct objectives, we implement specialized partial relevance enhancement strategies. For video retrieval, we introduce a multi-modal collaborative video retriever, generating distinct query representations tailored for different modalities by modality-specific pooling, ensuring a more effective match. For moment localization, we propose the focus-then-fuse moment localizer, utilizing modality-specific gates to capture essential content, followed by fusing multi-modal information for moment localization. Experimental results on TVR and DiDeMo datasets show that the proposed model outperforms the baselines, achieving a new state-of-the-art of VCMR.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频语料库时刻检索（VCMR）是一种新的视频检索任务，旨在使用自然语言文本作为查询从大量未经修剪的视频语料库中检索相关时刻。视频与查询之间的相关性是部分的，主要表现在两个方面：（1）范围：未经修剪的视频包含信息丰富的帧，并且并非所有帧都与查询相关。通常仅在相关时刻观察到强相关性，这强调了捕获关键内容的重要性。 (2) 模态：查询与不同模态的相关性不同；动作描述与视觉元素更加一致，而角色对话与文本信息更加相关。识别并解决这些特定于模态的细微差别对于 VCMR 中的有效检索至关重要。然而，现有的方法通常平等地对待所有视频内容，导致次优的时刻检索。我们认为，有效捕获查询和视频之间的部分相关性对于 VCMR 任务至关重要。为此，我们提出了部分相关性增强模型（PREM）来提高 VCMR。 VCMR 涉及两个子任务：视频检索和时刻定位。为了与他们的独特目标保持一致，我们实施专门的部分相关性增强策略。对于视频检索，我们引入了多模态协作视频检索器，通过特定于模态的池生成针对不同模态定制的不同查询表示，确保更有效的匹配。对于矩定位，我们提出了先聚焦然后融合矩定位器，利用特定于模态的门来捕获基本内容，然后融合多模态信息以进行矩定位。 TVR 和 DiDeMo 数据集上的实验结果表明，所提出的模型优于基线，实现了 VCMR 的新的最先进水平。</details>
**PDF:** <http://arxiv.org/pdf/2402.13576v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment**<br />
**Title_cn:** 认知视觉语言映射器：通过增强的视觉知识对齐促进多模式理解<br />
**Authors:** Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang<br />
**Abstract:** <details><summary>原文: </summary>Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>评估和重新思考大型多模态模型 (LMM) 的现状，我们观察到广泛使用的视觉语言投影方法（例如 Q-former 或 MLP）侧重于图像文本描述的对齐，但忽略了视觉知识维度对齐，即将视觉效果与其相关知识联系起来。视觉知识在分析、推断和解释视觉信息方面发挥着重要作用，有助于提高基于知识的视觉问题答案的准确性。在本文中，我们主要探索通过视觉语言知识对齐来改进 LMM，特别是针对挑战基于知识的视觉问答（VQA）。为此，我们提出了一个认知视觉语言映射器（CVLM），其中包含一个预训练的视觉知识对齐器（VKA）和一个用于多模式指令调整阶段的细粒度知识适配器（FKA）。具体来说，我们基于小语言模型和视觉编码器之间的交互设计了VKA，在收集的图像知识对上对其进行训练，以实现视觉知识获取和投影。 FKA 用于提取图像的细粒度视觉知识并将其注入大型语言模型 (LLM)。我们对基于知识的 VQA 基准进行了广泛的实验，实验结果表明 CVLM 显着提高了 LMM 在基于知识的 VQA 上的性能（平均增益 5.0%）。消融研究也分别验证了 VKA 和 FKA 的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.13561v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting**<br />
**Title_cn:** 使用聚类识别不必要的 3D 高斯分布以快速渲染 3D 高斯分布<br />
**Authors:** Joongho Jo, Hyeongwon Kim, Jongsun Park<br />
**Abstract:** <details><summary>原文: </summary>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯泼溅 (3D-GS) 是一种新的渲染方法，在速度和图像质量方面均优于神经辐射场 (NeRF)。 3D-GS利用数百万个3D高斯来表示3D场景，并将这些高斯投影到2D图像平面上进行渲染。然而，在渲染过程中，当前视图方向存在大量不必要的 3D 高斯，导致与其识别相关的大量计算成本。在本文中，我们提出了一种计算缩减技术，可以实时快速识别不必要的 3D 高斯，以在不影响图像质量的情况下渲染当前视图。这是通过对距离较近的 3D 高斯进行离线聚类，然后在运行时将这些聚类投影到 2D 图像平面上来实现的。此外，我们分析了在 GPU 上执行时与所提出的技术相关的瓶颈，并提出了一种无缝支持所提出的方案的高效硬件架构。对于 Mip-NeRF360 数据集，所提出的技术在 2D 图像投影之前平均排除 63% 的 3D 高斯，这在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算量减少了近 38.3%。与 GPU 相比，所提出的加速器还实现了 10.7 倍的加速。</details>
**PDF:** <http://arxiv.org/pdf/2402.13827v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields**<br />
**Title_cn:** SealD-NeRF：通过神经辐射场对动态场景进行交互式像素级编辑<br />
**Authors:** Zhentao Huang, Yukun Shi, Neil Bruce, Minglun Gong<br />
**Abstract:** <details><summary>原文: </summary>The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation. Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality. The key issue is developing a neural representation that supports local edits for real-time updates. Current NeRF editing methods, offering pixel-level adjustments or detailed geometry and color modifications, are mostly limited to static scenes. This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network. It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐式神经表示（尤其是神经辐射场 (NeRF)）的广泛采用凸显了对隐式 3D 模型编辑功能日益增长的需求，这对于场景后处理和 3D 内容创建等任务至关重要。尽管之前在 NeRF 编辑方面做出了努力，但由于编辑灵活性和质量的限制，挑战仍然存在。关键问题是开发一种支持本地编辑以进行实时更新的神经表示。目前的 NeRF 编辑方法提供像素级调整或详细的几何和颜色修改，但大多仅限于静态场景。本文介绍了 SealD-NeRF，它是 Seal-3D 的扩展，用于动态设置中的像素级编辑，专门针对 D-NeRF 网络。它通过将编辑操作映射到特定时间范围、冻结负责动态场景表示的变形网络以及使用师生方法来集成更改，允许跨序列进行一致的编辑。</details>
**PDF:** <http://arxiv.org/pdf/2402.13510v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **SDXL-Lightning: Progressive Adversarial Diffusion Distillation**<br />
**Title_cn:** SDXL-Lightning：渐进式对抗扩散蒸馏<br />
**Authors:** Shanchuan Lin, Anran Wang, Xiao Yang<br />
**Abstract:** <details><summary>原文: </summary>We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种扩散蒸馏方法，该方法在基于 SDXL 的一步/少步 1024px 文本到图像生成中实现了新的最先进技术。我们的方法结合了渐进式和对抗式蒸馏，以实现质量和模式覆盖范围之间的平衡。在本文中，我们讨论了理论分析、鉴别器设计、模型制定和训练技术。我们将经过精炼的 SDXL-Lightning 模型作为 LoRA 和完整的 UNet 权重进行开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.13929v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification**<br />
**Title_cn:** MSTAR：用于时间序列分类的多尺度骨干架构搜索<br />
**Authors:** Tue M. Cao, Nhat H. Tran, Hieu H. Pham, Hung T. Nguyen, Le P. Nguyen<br />
**Abstract:** <details><summary>原文: </summary>Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.</details>
**Abstract_cn:** <details><summary>译文: </summary>之前的大多数时间序列分类（TSC）方法都强调感受野和频率的重要性，而忽略了时间分辨率。因此，当他们将广泛的感受野集成到分类模型中时，不可避免地会遇到可扩展性问题。其他方法虽然对大型数据集具有更好的适应能力，但需要手动设计，但由于每个数据集的独特性而无法达到最佳架构。我们通过提出一种新颖的多尺度搜索空间和神经架构搜索（NAS）框架来克服这些挑战，它解决了频率和时间分辨率的问题，发现了特定数据集的合适尺度。我们进一步表明，我们的模型可以作为使用强大的 Transformer 模块的骨干，该模块具有未经训练和预训练的权重。我们的搜索空间在四个不同领域的四个数据集上达到了最先进的性能，同时为每个数据引入了十多个高度微调的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.13822v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization**<br />
**Title_cn:** 通过一致性正则化将量化感知训练推向全精度性能<br />
**Authors:** Junbiao Pang, Tianyang Cai, Baochang Zhang, Jiaqi Wu, Ye Tao<br />
**Abstract:** <details><summary>原文: </summary>Existing Quantization-Aware Training (QAT) methods intensively depend on the complete labeled dataset or knowledge distillation to guarantee the performances toward Full Precision (FP) accuracies. However, empirical results show that QAT still has inferior results compared to its FP counterpart. One question is how to push QAT toward or even surpass FP performances. In this paper, we address this issue from a new perspective by injecting the vicinal data distribution information to improve the generalization performances of QAT effectively. We present a simple, novel, yet powerful method introducing an Consistency Regularization (CR) for QAT. Concretely, CR assumes that augmented samples should be consistent in the latent feature space. Our method generalizes well to different network architectures and various QAT methods. Extensive experiments demonstrate that our approach significantly outperforms the current state-of-the-art QAT methods and even FP counterparts.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的量化感知训练（QAT）方法强烈依赖于完整的标记数据集或知识蒸馏来保证全精度（FP）精度的性能。然而，实证结果表明，与 FP 相比，QAT 的结果仍然较差。一个问题是如何推动 QAT 接近甚至超越 FP 性能。在本文中，我们从一个新的角度解决了这个问题，通过注入邻近数据分布信息来有效提高QAT的泛化性能。我们提出了一种简单、新颖但功能强大的方法，为 QAT 引入一致性正则化 (CR)。具体来说，CR 假设增强样本在潜在特征空间中应该是一致的。我们的方法可以很好地推广到不同的网络架构和各种 QAT 方法。大量的实验表明，我们的方法明显优于当前最先进的 QAT 方法，甚至 FP 方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.13497v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions**<br />
**Title_cn:** BEE-NET：一种深度神经网络，用于识别野外身体情绪表达<br />
**Authors:** Mohammad Mahdi Dehshibi, David Masip<br />
**Abstract:** <details><summary>原文: </summary>In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们研究了环境因素，特别是所涉及的场景和物体，如何通过肢体语言影响情绪的表达。为此，我们引入了一种新颖的多流深度卷积神经网络，名为 BEE-NET。我们还提出了一种新的后期融合策略，将地点和物体的元信息合并为学习过程中的先验知识。我们提出的概率池模型利用这些信息来生成潜在空间中可用和预期不可用上下文信息的联合概率分布。重要的是，我们的融合策略是可微分的，允许端到端训练和捕获数据点之间隐藏的关联，而不需要进一步的后处理或正则化。为了评估我们的深度模型，我们使用了身体语言数据库（BoLD），它是目前用于自动识别野外身体情绪表达（AIBEE）的最大可用数据库。我们的实验结果表明，我们提出的方法比 AIBEE 目前最先进的方法领先 2.07%，情感识别得分达到 66.33%。</details>
**PDF:** <http://arxiv.org/pdf/2402.13955v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery**<br />
**Title_cn:** BenchCloudVision：遥感图像云检测和分割深度学习方法的基准分析<br />
**Authors:** Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane<br />
**Abstract:** <details><summary>原文: </summary>Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \url{https://github.com/toelt-llc/cloud\_segmentation\_comparative}.</details>
**Abstract_cn:** <details><summary>译文: </summary>配备光学传感器的卫星可捕获高分辨率图像，为各种环境现象提供有价值的见解。近年来，针对遥感领域的一些挑战的研究激增，从不同景观中的水检测到山区和地形的分割。正在进行的研究目标是提高卫星图像分析的精度和效率。特别是，人们越来越重视开发准确的水体检测、雪和云的方法，这对于环境监测、资源管理和灾害响应非常重要。在此背景下，本文重点研究遥感图像的云分割。由于基于光学传感器的应用中存在云，准确的遥感数据分析可能具有挑战性。应用和研究等最终产品的质量直接受到云检测的影响，云检测在遥感数据处理流程中发挥着关键作用。本文研究了应用于云识别的七种尖端语义分割和检测算法，进行基准分析以评估其架构方法并确定性能最佳的方法。为了提高模型的适应性，对训练期间使用的图像类型和光谱带数量等关键要素进行了分析。此外，这项研究还尝试开发机器学习算法，仅使用少数光谱带​​（包括 RGB 和 RGBN-IR 组合）即可执行云分割。该模型针对各种应用和用户场景的灵活性是通过使用 Sentinel-2 和 Landsat-8 的图像作为数据集来评估的。可以使用此 github 链接中的材料复制此基准：\url{https://github.com/toelt-llc/cloud\_segmentation\_comparative}。</details>
**PDF:** <http://arxiv.org/pdf/2402.13918v1><br />
**Code:** <https://github.com/toelt-llc/cloud_segmentation_comparative>**<br />
>>**index:** 3<br />
**Title:** **Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps**<br />
**Title_cn:** 零 BEV：任何第一人称模式到 BEV 地图的零镜头投影<br />
**Authors:** Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf<br />
**Abstract:** <details><summary>原文: </summary>Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>鸟瞰图 (BEV) 地图是一种重要的几何结构表示，广泛应用于机器人技术，特别是自动驾驶车辆和地面机器人。现有算法要么需要几何投影的深度信息（该信息并不总是可靠可用），要么以完全监督的方式进行端到端训练，以将视觉第一人称观察映射到 BEV 表示，因此仅限于输出模态他们已经接受过培训。相比之下，我们提出了一种新模型，能够将第一人称视角中可用的任何模态执行零样本投影到相应的 BEV 地图。这是通过将几何逆透视投影与模态变换分开来实现的，例如。 RGB 占用。该方法是通用的，我们展示了投射到 BEV 三种不同模式的实验：语义分割、运动向量和第一人称检测到的对象边界框。我们通过实验表明，该模型优于竞争方法，特别是广泛使用的采用单目深度估计的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.13848v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images**<br />
**Title_cn:** 使用双参数 MR 图像的强化学习对前列腺癌进行弱监督定位<br />
**Authors:** Martynas Pocius, Wen Yan, Dean C. Barratt, Mark Emberton, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed<br />
**Abstract:** <details><summary>原文: </summary>In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种基于强化学习的弱监督定位系统。我们通过引入一种新颖的奖励定义来训练控制器函数来定位图像中的感兴趣区域，该奖励定义利用非二值化分类概率，该概率由预先训练的二元分类器生成，该分类器对图像或图像作物中的对象存在进行分类。然后，对象存在分类器可以通过量化图像包含对象的可能性来通知控制器其定位质量。这种方法使我们能够最大限度地减少任何潜在的标签或通过人类标签传播的人类偏见，以实现完全监督的本地化。我们评估了我们提出的在前列腺真实临床双参数 MR 图像的大型数据集上进行癌性病变定位任务的方法。与常用的多实例学习弱监督定位和完全监督基线的比较表明，我们提出的方法优于多实例学习，并且仅使用图像级分类标签进行训练，其性能与完全监督学习相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.13778v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mask-up: Investigating Biases in Face Re-identification for Masked Faces**<br />
**Title_cn:** 蒙面：调查蒙面人脸重新识别中的偏差<br />
**Authors:** Siddharth D Jaiswal, Ankit Kr. Verma, Animesh Mukherjee<br />
**Abstract:** <details><summary>原文: </summary>AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 AI 的人脸识别系统 (FRS) 现已作为 MLaaS 解决方案在世界各地广泛分布和部署，自 COVID-19 大流行以来，其任务范围包括从购买 SIM 卡时验证个人面部到监视公民等任务。据报道，这些系统中存在针对边缘化群体的广泛偏见，并导致了高度歧视性的结果。大流行后的世界已使戴口罩成为常态，但 FRS 却未能跟上时代的变化。因此，这些系统容易受到基于面罩的面部遮挡的影响。在这项研究中，我们审核了 4 个商业 FRS 和 9 个开源 FRS，以完成五个基准数据集（总共 14,722 张图像）中不同种类的蒙版和未蒙版图像之间的人脸重新识别任务。这些模拟了在世界所有主要国家部署的现实验证/监视任务。其中 3 个商业 FRS 和 5 个开源 FRS 非常不准确；他们进一步延续了对非白人的偏见，最低准确率为 0%。对 85 名人类参与者进行的同一任务的调查也得出 40% 的低准确度。因此，正如文献中经常假设的那样，管道中的人机交互调节并不能减轻人们的担忧。我们的大规模研究表明，此类服务的开发者、立法者和用户需要重新思考 FRS 背后的设计原则，特别是对于面部重新识别任务，并认识到观察到的偏差。</details>
**PDF:** <http://arxiv.org/pdf/2402.13771v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Explainable Classification Techniques for Quantum Dot Device Measurements**<br />
**Title_cn:** 量子点器件测量的可解释分类技术<br />
**Authors:** Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak<br />
**Abstract:** <details><summary>原文: </summary>In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.</details>
**Abstract_cn:** <details><summary>译文: </summary>在物理科学中，对图像数据的鲁棒特征表示的需求不断增加：广义上的二维数据的图像采集现在广泛应用于许多领域，包括我们在这里考虑的量子信息科学。虽然传统的图像特征在这种情况下被广泛使用，但它们的使用正在迅速被基于神经网络的技术所取代，这些技术通常会牺牲可解释性来换取高精度。为了改善这种权衡，我们提出了一种基于数据的合成技术，可以产生可解释的特征。我们使用可解释的提升机（EBM）证明，这种方法在不牺牲准确性的情况下提供了卓越的可解释性。具体来说，我们表明该技术在量子点调谐的背景下具有有意义的好处，在当前的发展阶段需要人工干预。</details>
**PDF:** <http://arxiv.org/pdf/2402.13699v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation**<br />
**Title_cn:** 用于零样本全景和语义分割的可泛化语义视觉查询生成<br />
**Authors:** Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Hiroshi Murase<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training. Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging. To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries. First, a feature extractor is trained by CON to link the vision and semantics for providing target queries. Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images. To address the lack of unseen categories, a generator is required. However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings. Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners. In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away. To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and supervised by real semantic embeddings. Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing.</details>
**Abstract_cn:** <details><summary>译文: </summary>零镜头全景分割（ZPS）旨在识别前景实例和背景内容，而无需在训练中包含包含未见过类别的图像。由于视觉数据的稀疏性以及从可见类别推广到不可见类别的困难，这项任务仍然具有挑战性。为了更好地泛化到未见过的类，我们提出了条件标记对齐和循环转换（CONCAT），以产生可泛化的语义视觉查询。首先，特征提取器经过 CON 训练，将视觉和语义联系起来，以提供目标查询。正式地，CON 被提议将语义查询与从完整图像和掩模图像中提取的 CLIP 视觉 CLS 标记对齐。为了解决看不见的类别的缺乏，需要一个生成器。然而，合成伪视觉查询（即针对未见过的类别的视觉查询）的差距之一是通过语义嵌入描述细粒度的视觉细节。因此，我们采用 CAT 以语义-视觉和视觉-语义的方式训练生成器。在语义视觉中，提出了视觉查询对比，通过拉动包含片段的相应目标的伪视觉查询，同时将不带片段的目标推开，来对视觉的高粒度进行建模。为了确保生成的查询保留语义信息，在视觉语义中，伪视觉查询被映射回语义并由真实语义嵌入进行监督。 ZPS 的实验实现了超过 SOTA 的 5.2% hPQ 提升。我们还检查了归纳 ZPS 和开放词汇语义分割，并获得了比较结果，同时测试速度提高了 2 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.13697v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Robustness of Deep Neural Networks for Micro-Doppler Radar Classification**<br />
**Title_cn:** 微多普勒雷达分类深度神经网络的鲁棒性<br />
**Authors:** Mikolaj Czerkawski, Carmine Clemente, Craig MichieCraig Michie, Christos Tachtatzis<br />
**Abstract:** <details><summary>原文: </summary>With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples.</details>
**Abstract_cn:** <details><summary>译文: </summary>雷达数据处理深度分类器的强大功能带来了学习数据集特定特征的风险，这些特征不能很好地泛化。在这项工作中，评估了在相同数据上训练和测试的两种深度卷积架构的鲁棒性。当遵循标准训练实践时，两个分类器都对输入表示的微妙时间变化表现出敏感性，这是一种携带最少语义内容的增强。此外，这些模型非常容易受到对抗性例子的影响。小的时间变化和对抗性例子都是模型对不能很好概括的特征过度拟合的结果。作为一种补救措施，研究表明，对对抗性示例和时间增强样本进行训练可以减少这种影响，并导致模型具有更好的泛化能力。最后，事实证明，基于节奏速度图表示而不是多普勒时间运行的模型自然更容易受到对抗性示例的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.13651v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition**<br />
**Title_cn:** 用于场景文本识别的类感知掩模引导特征细化<br />
**Authors:** Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景文本识别是一个快速发展的领域，由于场景文本的复杂性和多样性，包括复杂的背景、多样的字体、灵活的排列和意外遮挡等，面临着众多挑战。在本文中，我们提出了一种称为类感知掩模引导特征细化（CAM）的新颖方法来应对这些挑战。我们的方法引入了从标准字体生成的规范类感知字形掩码，以有效抑制背景和文本样式噪声，从而增强特征辨别力。此外，我们设计了一个特征对齐和融合模块，以合并规范掩模指导，以进一步细化文本识别的特征。通过增强规范掩模特征和文本特征之间的对齐，该模块确保更有效的融合，最终提高识别性能。我们首先在六个标准文本识别基准上评估 CAM，以证明其有效性。此外，尽管模型尺寸较小，但在六个更具挑战性的数据集上，CAM 的平均性能提升了 4.1%，优于最先进的方法。我们的研究强调了结合规范掩模指导和对齐特征细化技术对于稳健的场景文本识别的重要性。该代码可在 https://github.com/MelosY/CAM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.13643v1><br />
**Code:** <https://github.com/melosy/cam>**<br />
>>**index:** 10<br />
**Title:** **Delving into Dark Regions for Robust Shadow Detection**<br />
**Title_cn:** 深入研究黑暗区域以实现稳健的阴影检测<br />
**Authors:** Huankang Guan, Ke Xu, Rynson W. H. Lau<br />
**Abstract:** <details><summary>原文: </summary>Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions. We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values). Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions. Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection. Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations. To this end, we formulate an effective dark-region recommendation (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets. Code is available at https://github.com/guanhuankang/ShadowDetection2021.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>阴影检测是一项具有挑战性的任务，因为它需要全面了解阴影特征和全局/局部照明条件。我们从实验中观察到，最先进的深度方法在区分黑暗区域（即具有低强度值的区域）中的阴影像素与非阴影像素时往往具有更高的错误率。我们对这个问题的主要见解是，现有的方法通常从整个图像全局学习有区别的阴影特征，覆盖整个强度值范围，并且可能无法学习黑暗区域中阴影和非阴影像素之间的细微差别。因此，如果我们可以设计一个模型来关注更窄范围的低强度区域，它可能能够学习更好的阴影检测判别特征。受这一见解的启发，我们提出了一种新颖的阴影检测方法，该方法首先学习整个图像的全局上下文线索，然后放大黑暗区域以学习局部阴影表示。为此，我们制定了一种有效的暗区域推荐（DRR）模块来推荐低强度值的区域，以及一种新颖的暗感知阴影分析（DASA）模块来从推荐的暗区域中学习暗感知阴影特征。大量实验表明，所提出的方法在三个流行的阴影检测数据集上优于最先进的方法。代码可在 https://github.com/guanhuankang/ShadowDetection2021.git 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.13631v1><br />
**Code:** <https://github.com/guanhuankang/shadowdetection2021>**<br />
>>**index:** 11<br />
**Title:** **YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information**<br />
**Title_cn:** YOLOv9：使用可编程梯度信息学习您想学习的内容<br />
**Authors:** Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao<br />
**Abstract:** <details><summary>原文: </summary>Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.</details>
**Abstract_cn:** <details><summary>译文: </summary>如今的深度学习方法重点关注如何设计最合适的目标函数，使得模型的预测结果能够最接近真实情况。同时，必须设计一个适当的架构，可以帮助获取足够的信息进行预测。现有方法忽略了一个事实，即当输入数据经过逐层特征提取和空间变换时，大量信息将会丢失。本文将深入研究数据通过深度网络传输时数据丢失的重要问题，即信息瓶颈和可逆函数。我们提出了可编程梯度信息（PGI）的概念来应对深度网络实现多个目标所需的各种变化。 PGI可以为目标任务计算目标函数提供完整的输入信息，从而获得可靠的梯度信息来更新网络权值。此外，还设计了一种新的轻量级网络架构——基于梯度路径规划的通用高效层聚合网络（GELAN）。 GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用传统的卷积算子即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得从头开始训练的模型能够比使用大数据集预训练的state-of-the-art模型获得更好的结果，比较结果如图1所示。源代码是网址：https://github.com/WongKinYiu/yolov9。</details>
**PDF:** <http://arxiv.org/pdf/2402.13616v1><br />
**Code:** <https://github.com/wongkinyiu/yolov9>**<br />
>>**index:** 12<br />
**Title:** **Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion**<br />
**Title_cn:** 通过深度补全的聚类学习逐像素连续深度表示<br />
**Authors:** Chen Shenglun, Zhang Hong, Ma XinZhu, Wang Zhihui, Li Haojie<br />
**Abstract:** <details><summary>原文: </summary>Depth completion is a long-standing challenge in computer vision, where classification-based methods have made tremendous progress in recent years. However, most existing classification-based methods rely on pre-defined pixel-shared and discrete depth values as depth categories. This representation fails to capture the continuous depth values that conform to the real depth distribution, leading to depth smearing in boundary regions. To address this issue, we revisit depth completion from the clustering perspective and propose a novel clustering-based framework called CluDe which focuses on learning the pixel-wise and continuous depth representation. The key idea of CluDe is to iteratively update the pixel-shared and discrete depth representation to its corresponding pixel-wise and continuous counterpart, driven by the real depth distribution. Specifically, CluDe first utilizes depth value clustering to learn a set of depth centers as the depth representation. While these depth centers are pixel-shared and discrete, they are more in line with the real depth distribution compared to pre-defined depth categories. Then, CluDe estimates offsets for these depth centers, enabling their dynamic adjustment along the depth axis of the depth distribution to generate the pixel-wise and continuous depth representation. Extensive experiments demonstrate that CluDe successfully reduces depth smearing around object boundaries by utilizing pixel-wise and continuous depth representation. Furthermore, CluDe achieves state-of-the-art performance on the VOID datasets and outperforms classification-based methods on the KITTI dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度补全是计算机视觉领域长期存在的挑战，近年来基于分类的方法取得了巨大进展。然而，大多数现有的基于分类的方法依赖于预定义的像素共享和离散深度值作为深度类别。这种表示无法捕获符合真实深度分布的连续深度值，导致边界区域的深度拖尾。为了解决这个问题，我们从聚类的角度重新审视深度补全，并提出了一种名为 CluDe 的新型基于聚类的框架，该框架专注于学习像素级和连续的深度表示。 CluDe 的关键思想是在真实深度分布的驱动下，迭代地将像素共享和离散深度表示更新为其相应的像素级和连续对应物。具体来说，CluDe首先利用深度值聚类来学习一组深度中心作为深度表示。虽然这些深度中心是像素共享且离散的，但与预定义的深度类别相比，它们更符合真实的深度分布。然后，CluDe 估计这些深度中心的偏移量，使其能够沿着深度分布的深度轴进行动态调整，以生成逐像素且连续的深度表示。大量实验表明，CluDe 通过利用像素级和连续深度表示，成功减少了对象边界周围的深度模糊。此外，CluDe 在 VOID 数据集上实现了最先进的性能，并在 KITTI 数据集上优于基于分类的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.13579v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **TransGOP: Transformer-Based Gaze Object Prediction**<br />
**Title_cn:** TransGOP：基于 Transformer 的注视对象预测<br />
**Authors:** Binglu Wang, Chenxi Guo, Yang Jin, Haisheng Xia, Nian Liu<br />
**Abstract:** <details><summary>原文: </summary>Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>注视目标预测旨在预测人类观看的物体的位置和类别。之前的凝视目标预测工作使用基于 CNN 的目标检测器来预测目标的位置。然而，我们发现基于 Transformer 的物体检测器可以预测零售场景中密集物体的更准确的物体位置。此外，Transformer的远距离建模能力可以帮助建立人头和注视对象之间的关系，这对于GOP任务很重要。为此，本文将Transformer引入注视目标预测领域，并提出了一种基于Transformer的端到端注视目标预测方法TransGOP。具体来说，TransGOP 使用现成的基于 Transformer 的对象检测器来检测对象的位置，并在注视回归器中设计基于 Transformer 的注视自动编码器来建立远距离注视关系。此外，为了改进注视热图回归，我们提出了一种对象到注视交叉注意机制，让注视自动编码器的查询从对象检测器学习全局内存位置知识。最后，为了使整个框架进行端到端训练，我们提出了一种 Gaze Box loss，通过增强注视对象框中的注视热图能量来联合优化对象检测器和注视回归器。对 GOO-Synth 和 GOO-Real 数据集的大量实验表明，我们的 TransGOP 在所有轨道（即对象检测、注视估计和注视对象预测）上都实现了最先进的性能。我们的代码将在 https://github.com/chenxi-Guo/TransGOP.git 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.13578v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A Two-Stage Dual-Path Framework for Text Tampering Detection and Recognition**<br />
**Title_cn:** 用于文本篡改检测和识别的两阶段双路径框架<br />
**Authors:** Guandong Li, Xian Yang, Wenpin Ma<br />
**Abstract:** <details><summary>原文: </summary>Document tamper detection has always been an important aspect of tamper detection. Before the advent of deep learning, document tamper detection was difficult. We have made some explorations in the field of text tamper detection based on deep learning. Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition. It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered). By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.). The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results. Audit point positioning uses detection frameworks and controls thresholds for high and low density detection. Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction. After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913.</details>
**Abstract_cn:** <details><summary>译文: </summary>文档篡改检测一直是篡改检测的一个重要方面。在深度学习出现之前，文档篡改检测很困难。我们在基于深度学习的文本篡改检测领域做了一些探索。我们的Ps篡改检测方法包括三个步骤：特征辅助、审核点定位和篡改识别。它涉及分级过滤和分级输出（被篡改/疑似篡改/未篡改）。通过结合人工篡改数据特征，我们模拟和增强了各种场景下的数据样本（添加噪声/替换的裁剪、单个字符/空格替换、涂抹/拼接、亮度/对比度调整等）。辅助功能包括exif/二进制流关键字检索/噪声，用于根据结果进行分支检测。审核点定位使用检测框架并控制高密度和低密度检测的阈值。篡改识别采用双路双流识别网络，具有RGB和ELA流特征提取。通过自相关百分位数池化降维后，融合输出经过 vlad 处理，准确率达到 0.804，召回率达到 0.659，精度达到 0.913。</details>
**PDF:** <http://arxiv.org/pdf/2402.13545v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Unsupervised learning based object detection using Contrastive Learning**<br />
**Title_cn:** 使用对比学习的基于无监督学习的对象检测<br />
**Authors:** Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali Jannesari<br />
**Abstract:** <details><summary>原文: </summary>Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning.   Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels.   In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection. We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection. The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \textbf{89.2\%}, marking a significant breakthrough of approximately \textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision.</details>
**Abstract_cn:** <details><summary>译文: </summary>训练基于图像的物体检测器提出了巨大的挑战，因为它不仅需要物体检测的复杂性，而且还需要在潜在多样化和嘈杂的环境中精确定位物体的复杂性。然而，图像的收集本身通常很简单；例如，安装在车辆上的摄像头可以轻松捕获各种现实场景中的大量数据。鉴于此，我们引入了一种通过无监督/自监督学习来训练单级目标检测器的突破性方法。我们最先进的方法有可能彻底改变标签流程，大大减少与手动注释相关的时间和成本。此外，它为以前无法​​实现的研究机会铺平了道路，特别是对于缺乏广泛标签的大型、多样化且具有挑战性的数据集。与主要针对分类任务的流行无监督学习方法相比，我们的方法面临着对象检测的独特挑战。我们率先提出了图像内对比学习以及图像间对比学习的概念，从而能够获取对象检测所必需的关键位置信息。该方法熟练地学习并表示该位置信息，从而生成信息丰富的热图。我们的结果展示了 \textbf{89.2\%} 的出色准确度，标志着计算机视觉领域无监督对象检测领域相对于随机初始化的约 \textbf{15x} 的重大突破。</details>
**PDF:** <http://arxiv.org/pdf/2402.13465v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving**<br />
**Title_cn:** 基于大语言模型的自动驾驶混合推理<br />
**Authors:** Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg<br />
**Abstract:** <details><summary>原文: </summary>Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型 (LLM) 因其理解文本和图像、生成类人文本以及执行复杂推理任务的能力而受到广泛关注。然而，他们将这种高级推理与自然语言文本相结合以在动态情况下进行决策的能力还需要进一步探索。在这项研究中，我们研究了法学硕士如何适应和应用算术和常识推理的组合，特别是在自动驾驶场景中。我们假设法学硕士的混合推理能力可以通过分析检测到的物体和传感器数据、了解驾驶法规和物理定律并提供额外的背景来改善自动驾驶。这解决了复杂的场景，例如在能见度较低（由于天气条件）的情况下做出的决策，而传统方法可能无法满足这些场景。我们通过将大型语言模型 (LLM) 的答案与 CARLA 内人类生成的基本事实进行比较，根据准确性评估了大型语言模型 (LLM)。结果表明，当图像（检测到的物体）和传感器数据组合输入 LLM 时，它可以为自动驾驶汽车在各种天气条件下的制动和油门控制提供精确的信息。该公式和答案可以帮助自动驾驶系统的决策。</details>
**PDF:** <http://arxiv.org/pdf/2402.13602v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs**<br />
**Title_cn:** 法学硕士遇见长视频：利用法学硕士中的交互式视觉适配器促进长视频理解<br />
**Authors:** Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang<br />
**Abstract:** <details><summary>原文: </summary>Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.</details>
**Abstract_cn:** <details><summary>译文: </summary>长视频理解是多媒体和人工智能交叉领域的一个重大且持续的挑战。采用大型语言模型（LLM）来理解视频成为一种新兴且有前途的方法。然而，由于大量的视频标记，这种方法会产生很高的计算成本，由于标记聚合而导致视觉清晰度降低，并且在回答视频相关问题时面临不相关的视觉标记带来的挑战。为了缓解这些问题，我们在法学硕士中提出了交互式视觉适配器（IVA），旨在增强与细粒度视觉元素的交互。具体来说，我们首先利用视觉编码器和预训练的因果变换器将长视频转换为时间视频标记，然后将它们与视频指令一起输入到法学硕士中。随后，我们将 IVA（包含轻量级时间帧选择器和空间特征交互器）集成到 LLM 的内部模块中，以捕获指令感知和细粒度的视觉信号。因此，所提出的视频法学硕士通过适当的长视频建模和精确的视觉交互促进了对长视频内容的全面理解。我们对九个视频理解基准进行了广泛的实验，实验结果表明，我们的交互式视觉适配器显着提高了视频法学硕士在长视频 QA 任务上的性能。消融研究进一步验证了 IVA 在长短视频理解中的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.13546v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks**<br />
**Title_cn:** VOOM：使用分层地标的鲁棒视觉对象里程计和绘图<br />
**Authors:** Yutong Wang, Chaoyang Jiang, Xieyuanli Chen<br />
**Abstract:** <details><summary>原文: </summary>In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，面向对象的同步定位与建图（SLAM）因其能够在保持计算效率的同时提供高级语义信息而受到越来越多的关注。一些研究人员尝试通过将建模对象残差集成到束调整中来提高定位精度。然而，很少有系统能够表现出比基于特征的视觉 SLAM 系统更好的结果，因为一般的粗略对象模型（例如长方体或椭球体）不如特征点准确。在本文中，我们提出了一种视觉对象里程计和建图框架 VOOM，以从粗到细的方式使用高级对象和低级点作为分层地标，而不是在捆绑调整中直接使用对象残差。首先，我们介绍了一种改进的观测模型和一种新颖的对偶二次曲面数据关联方法，用于表示物理对象。它有助于创建密切反映现实的 3D 地图。接下来，我们使用对象信息来增强特征点的数据关联，从而更新地图。在视觉对象测距后端中，更新的地图用于进一步优化相机姿势和对象。同时，在我们的视觉对象映射过程中，利用对象和基于点的共视图来执行局部束调整。实验表明，VOOM 在定位方面优于面向对象的 SLAM 和特征点 SLAM 系统（例如 ORB-SLAM2）。我们方法的实现可以在 https://github.com/yutongwangBIT/VOOM.git 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.13609v1><br />
**Code:** <https://github.com/yutongwangbit/voom>**<br />
>>**index:** 2<br />
**Title:** **Event-aware Video Corpus Moment Retrieval**<br />
**Title_cn:** 事件感知视频语料库时刻检索<br />
**Authors:** Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng<br />
**Abstract:** <details><summary>原文: </summary>Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query. Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos. Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval. The model extracts event representations through event reasoning and hierarchical event encoding. The event reasoning module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels. We also introduce anchor multi-head self-attenion to encourage Transformer to capture the relevance of adjacent content in the video. The training of EventFormer is conducted by two-branch contrastive learning and dual optimization for two sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo benchmarks show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results. Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频语料库时刻检索 (VCMR) 是一项实用的视频检索任务，专注于使用自然语言查询在大量未修剪视频的语料库中识别特定时刻。现有的 VCMR 方法通常依赖于帧感知视频检索，计算查询帧和视频帧之间的相似性，以根据最大帧相似性对视频进行排名。然而，这种方法忽略了嵌入在帧之间信息中的语义结构，即事件、人类理解视频的关键要素。受此启发，我们提出了 EventFormer，这是一种明确利用视频中的事件作为视频检索的基本单元的模型。该模型通过事件推理和分层事件编码来提取事件表示。事件推理模块将连续且视觉上相似的帧表示分组为事件，而分层事件编码在帧和事件级别对信息进行编码。我们还引入了锚点多头自注意力，以鼓励 Transformer 捕获视频中相邻内容的相关性。 EventFormer的训练是通过对VCMR的两个子任务进行双分支对比学习和对偶优化来进行的。 TVR、ANetCaps 和 DiDeMo 基准测试的大量实验显示了 EventFormer 在 VCMR 中的有效性和效率，取得了新的最先进的结果。此外，EventFormer 的有效性也在部分相关视频检索任务上得到了验证。</details>
**PDF:** <http://arxiv.org/pdf/2402.13566v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization**<br />
**Title_cn:** EffLoc：用于高效 6 自由度相机重定位的轻量级视觉转换器<br />
**Authors:** Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei<br />
**Abstract:** <details><summary>原文: </summary>Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.</details>
**Abstract_cn:** <details><summary>译文: </summary>相机重新定位在计算机视觉中至关重要，在 AR、无人机、机器人和自动驾驶中都有应用。它根据图像估计 3D 相机位置和方向 (6-DoF)。与 SLAM 等传统方法不同，最近的进展是使用深度学习进行直接端到端姿态估计。我们提出了 EffLoc，一种用于单图像相机重定位的新型高效视觉转换器。 EffLoc 的分层布局、内存限制的自注意力和前馈层提高了内存效率和通道间通信。我们引入的顺序群体注意力（SGA）模块通过多样化输入特征、减少冗余和扩展模型容量来提高计算效率。 EffLoc 在效率和准确性方面表现出色，优于 AtLoc 和 MapNet 等现有方法。它在大规模户外汽车驾驶场景中蓬勃发展，确保简单性、端到端可训练性，并消除手工制作的损失函数。</details>
**PDF:** <http://arxiv.org/pdf/2402.13537v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images**<br />
**Title_cn:** 基于多尺度时空变换器的不平衡纵向学习用于不规则时间序列图像的青光眼预测<br />
**Authors:** Xikai Yang, Jian Wu, Xi Wang, Yuchen Yuan, Ning Li Wang, Pheng-Ann Heng<br />
**Abstract:** <details><summary>原文: </summary>Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>青光眼是导致进行性视神经纤维损伤和不可逆失明的主要眼病之一，困扰着数百万人。青光眼预测很好地解决了潜在患者的早期筛查和干预，有助于防止病情进一步恶化。它利用一系列历史眼底图像来预测未来发生青光眼的可能性。然而，不规则的抽样性质和不平衡的类别分布是疾病预测方法发展的两个挑战。为此，我们引入了基于为序列图像输入量身定制的变压器架构的多尺度时空变压器网络（MST-former），它可以有效地从时间和空间维度上的序列图像中学习代表性语义信息。具体来说，我们采用多尺度结构来提取各种分辨率的特征，这可以很大程度上利用每个图像中编码的丰富空间信息。此外，我们设计了一个时间距离矩阵以非线性方式缩放时间注意力，可以有效处理不规则采样的数据。此外，我们引入了温度控制的平衡 Softmax 交叉熵损失来解决类别不平衡问题。对青光眼预测序列眼底图像 (SIGF) 数据集的大量实验证明了所提出的 MST 前方法的优越性，青光眼预测的 AUC 达到 98.6%。此外，我们的方法在阿尔茨海默病神经影像倡议（ADNI）MRI数据集上表现出出色的泛化能力，对轻度认知障碍和阿尔茨海默病预测的准确率达到90.3%，大幅优于对比方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.13475v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Real-time 3D-aware Portrait Editing from a Single Image**<br />
**Title_cn:** 从单个图像进行实时 3D 感知肖像编辑<br />
**Authors:** Qingyan Bai, Yinghao Xu, Zifan Shi, Hao Ouyang, Qiuyu Wang, Ceyuan Yang, Xuan Wang, Gordon Wetzstein, Yujun Shen, Qifeng Chen<br />
**Abstract:** <details><summary>原文: </summary>This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出了 3DPE，这是一种实用工具，可以按照给定的提示（如参考图像或文本描述）以 3D 感知方式有效地编辑面部图像。为此，从 3D 肖像生成器和文本到图像模型中提炼出一个轻量级模块，它们分别提供面部几何形状的先验知识和开放词汇编辑功能。与现有方法相比，这种设计带来了两个引人注目的优势。首先，我们的系统通过前馈网络实现实时编辑（即每个图像约 0.04 秒），比第二个竞争对手快 100 倍以上。其次，由于强大的先验，我们的模块可以专注于编辑相关变体的学习，从而能够在训练阶段同时处理各种类型的编辑，并进一步支持在训练阶段快速适应用户指定的新颖类型的编辑。推理（例如，每个案例约 5 分钟的微调）。代码、模型和界面将公开，以方便未来的研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.14000v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A unified framework of non-local parametric methods for image denoising**<br />
**Title_cn:** 图像去噪非局部参数方法的统一框架<br />
**Authors:** Sébastien Herbreteau, Charles Kervrann<br />
**Abstract:** <details><summary>原文: </summary>We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively. Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises. Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods. Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches. While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种用于单图像去噪的非局部方法的统一视图，其中 BM3D 是最流行的代表，该方法通过根据噪声块的相似性将噪声块收集在一起以便协作处理它们来进行操作。我们的一般估计框架基于二次风险的最小化，分两步近似，并适应光子和电子噪声。第一步依靠无偏风险估计（URE），第二步依靠“内部适应”（一个借用自深度学习理论的概念），我们表明我们的方法能够重新解释和协调以前的状态艺术非本地方法。在此框架内，我们提出了一种称为 NL-Ridge 的新型降噪器，它利用补丁的线性组合。虽然概念上更简单，但我们证明 NL-Ridge 的性能优于成熟的最先进的单图像降噪器。</details>
**PDF:** <http://arxiv.org/pdf/2402.13816v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters**<br />
**Title_cn:** 自带角色：自动生成自定义角色面部动画的整体解决方案<br />
**Authors:** Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian<br />
**Abstract:** <details><summary>原文: </summary>Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>虚拟角色动画一直是虚拟现实（VR）领域的一个基础研究问题。面部动画在有效传达虚拟人的情感和态度方面发挥着至关重要的作用。然而，创建此类面部动画可能具有挑战性，因为当前的方法通常涉及使用昂贵的运动捕捉设备或人类动画师在调整动画参数方面投入大量时间和精力。在本文中，我们提出了一种自动制作虚拟人脸动画的整体解决方案。在我们的解决方案中，首先训练深度学习模型，通过估计混合形状系数将面部表情从输入面部图像重新定位到虚拟人脸。该方法提供了生成具有不同外观和混合形状拓扑的角色的动画的灵活性。其次，使用Unity 3D开发了实用的工具包，使其兼容最流行的VR应用程序。该工具包接受图像和视频作为输入来为目标虚拟人脸制作动画，并使用户能够操纵动画结果。此外，在人机循环（HITL）精神的启发下，我们利用用户反馈进一步提高模型和工具包的性能，从而增加定制属性以满足用户偏好。我们将公开整个解决方案的代码，它有可能加速 VR 应用程序中使用的面部动画的生成。</details>
**PDF:** <http://arxiv.org/pdf/2402.13724v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning**<br />
**Title_cn:** SimPro：实现现实长尾半监督学习的简单概率框架<br />
**Authors:** Chaoqun Du, Yizeng Han, Gao Huang<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督学习的最新进展集中在一个更现实但更具挑战性的任务上：解决标记数据中的不平衡问题，而未标记数据的类别分布仍然未知且可能不匹配。该领域当前的方法通常预先假设有关未标记数据的类别分布的严格假设，从而将模型的适应性限制在某些分布范围内。在这项研究中，我们提出了一种新颖的方法，引入了一个高度适应性的框架，称为 SimPro，它不依赖于任何关于未标记数据分布的预定义假设。我们的框架以概率模型为基础，通过显式解耦条件类分布和边缘类分布的建模，创新地改进了期望最大化（EM）算法。这种分离有助于在最大化阶段提供类分布估计的封闭式解决方案，从而形成贝叶斯分类器。贝叶斯分类器反过来又提高了期望阶段伪标签的质量。值得注意的是，SimPro 框架不仅具有理论保证，而且易于实施。此外，我们引入了两种新颖的类别分布，扩大了评估范围。我们的方法在不同的基准和数据分布场景中展示了一致的最先进的性能。我们的代码可在 https://github.com/LeapLabTHU/SimPro 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.13505v1><br />
**Code:** <https://github.com/leaplabthu/simpro>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Corrective Machine Unlearning**<br />
**Title_cn:** 纠正机器遗忘<br />
**Authors:** Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal<br />
**Abstract:** <details><summary>原文: </summary>Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于使用从互联网获取的大规模训练数据集，机器学习模型越来越面临数据完整性的挑战。我们研究模型开发人员在检测到某些数据被操纵或不正确时可以采取哪些措施。这种被操纵的数据可能会造成不利影响，例如容易受到后门样本的影响、系统偏差，以及通常会降低某些输入域的准确性。通常，所有被操纵的训练样本都是未知的，并且只有受影响数据的一小部分代表性子集被标记。我们将“纠正性机器遗忘”形式化为减轻受未知操作影响的数据对训练模型的影响的问题，可能只知道受影响样本的子集。我们证明，纠正性遗忘问题与传统的面向隐私的遗忘问题有着显着不同的要求。我们发现大多数现有的忘却方法，包括从头开始的黄金标准再训练，都需要识别大部分被操纵的数据，以进行有效的纠正性忘却。然而，一种方法 SSD 在仅用一小部分被操纵的样本来消除不利影响方面取得了有限的成功，这表明了这种设置的易处理性。我们希望我们的工作能够促进研究开发更好的纠正性遗忘方法，并为从业者提供新的策略来应对网络规模培训带来的数据完整性挑战。</details>
**PDF:** <http://arxiv.org/pdf/2402.14015v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks**<br />
**Title_cn:** 使用机载全卷积网络进行高通量视觉纳米无人机到纳米无人机的相对定位<br />
**Authors:** Luca Crupi, Alessandro Giusti, Daniele Palossi<br />
**Abstract:** <details><summary>原文: </summary>Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes.</details>
**Abstract_cn:** <details><summary>译文: </summary>无人机对无人机的相对定位是任何群体行动的基本组成部分。我们在微型纳米无人机（即直径 10 厘米）的背景下解决了这项任务，由于其尺寸减小而带来的新用例，人们对这种无人机的兴趣日益增长。其多功能性的代价是有限的板载资源，即传感器、处理单元和内存，这限制了板载算法的复杂性。克服这些限制的传统解决方案是直接部署在纳米无人机上的轻量级深度学习模型。这项工作仅使用灰度低分辨率相机和机载超低功耗片上系统（SoC）来解决纳米无人机之间具有挑战性的相对姿态估计。我们提出了一种基于新颖的基于视觉的全卷积神经网络 (FCNN) 的垂直集成系统，该系统在使用 GWT GAP8 SoC 扩展的 Crazyflie 纳米无人机上以 39Hz 运行，功耗为 101mW。我们将 FCNN 与三个最先进的 (SoA) 系统进行比较。考虑到性能最佳的 SoA 方法，我们的模型在 30k 图像的真实数据集上，水平图像坐标上的 R 平方从 32% 提高到 47%，垂直图像坐标上从 18% 提高到 55%。最后，我们的现场测试表明，与之前的 SoA 工作相比，平均跟踪误差降低了 37%，并且整个电池寿命长达 4 分钟的耐久性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.13756v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models**<br />
**Title_cn:** 用于评估视觉语言模型中性别偏见的统一框架和数据集<br />
**Authors:** Ashutosh Sathe, Prachi Jain, Sunayana Sitaram<br />
**Abstract:** <details><summary>原文: </summary>Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型视觉语言模型（VLM）在工业界和学术界得到广泛采用。在这项工作中，我们建立了一个统一的框架来系统地评估 VLM 中的性别职业偏见。我们的评估涵盖了最新 VLM 支持的所有推理模式，包括图像到文本、文本到文本、文本到图像和图像到图像。我们构建了一个合成的高质量文本和图像数据集，模糊了专业行为中的性别差异，以衡量性别偏见。在我们对最新视觉语言模型（VLM）的基准测试中，我们观察到不同的输入输出模式会导致不同的偏差幅度和方向。我们希望我们的工作能够帮助指导未来改进 VLM 的进展，以学习社会公正的表征。我们将发布我们的数据和代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.13636v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Adversarial Purification and Fine-tuning for Robust UDC Image Restoration**<br />
**Title_cn:** 用于鲁棒 UDC 图像恢复的对抗性净化和微调<br />
**Authors:** Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu<br />
**Abstract:** <details><summary>原文: </summary>This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against adversarial attacks. Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to adversarial perturbations. Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and black-box attacking methods. This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques. Following the assessment, we introduce a defense framework integrating adversarial purification with subsequent fine-tuning processes. First, our approach employs diffusion-based adversarial purification, effectively neutralizing adversarial perturbations. Then, we apply the fine-tuning methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained. The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical adversarial attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究深入研究了屏下摄像头 (UDC) 图像恢复模型的增强，重点关注其针对对抗性攻击的鲁棒性。尽管 UDC 技术采用创新的无缝显示集成方法，但仍面临着独特的图像质量下降挑战，而对对抗性扰动的敏感性加剧了这一挑战。我们的研究首先采用多种白盒和黑盒攻击方法对基于深度学习的 UDC 图像恢复模型进行了深入的鲁棒性评估。该评估对于理解当前 UDC 图像恢复技术的漏洞至关重要。经过评估，我们引入了一个将对抗性净化与后续微调过程相结合的防御框架。首先，我们的方法采用基于扩散的对抗性净化，有效地中和对抗性扰动。然后，我们应用微调方法进一步细化图像恢复模型，确保保持恢复图像的质量和保真度。我们提出的方法的有效性通过广泛的实验得到了验证，显示出针对典型对抗性攻击的弹性显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.13629v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel**<br />
**Title_cn:** 探索每像素微比特语义图像压缩的极限<br />
**Authors:** Jordan Dotzel, Bahaa Kotb, James Dotzel, Mohamed Abdelfattah, Zhiru Zhang<br />
**Abstract:** <details><summary>原文: </summary>Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology. We push semantic compression as low as 100 $\mu$bpp (up to $10,000\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 $\mu$bpp level represents a soft limit on semantic compression at standard image resolutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统方法（例如 JPEG）通过对结构信息（例如像素值或频率内容）进行操作来执行图像压缩。这些方法对于标准图像尺寸下大约每像素一位 (bpp) 和更高的比特率有效。相比之下，基于文本的语义压缩使用自然语言直接存储概念及其关系，自然语言随着人类的发展而有效地表示这些显着概念。这些方法可以在极低的比特率下运行，忽略位置、大小和方向等结构信息。在这项工作中，我们使用 OpenAI 的 GPT-4V 和 DALL-E3 来探索图像压缩的质量压缩前沿并确定当前技术的局限性。通过引入迭代反射过程来改进解码图像，我们将语义压缩推至低至 100 $\mu$bpp（比 JPEG 小高达 $10,000\times$）。我们进一步假设这个 100 $\mu$bpp 水平代表了标准图像分辨率下语义压缩的软限制。</details>
**PDF:** <http://arxiv.org/pdf/2402.13536v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Feature Matching Method Based on Multi-Level Refinement Strategy**<br />
**Title_cn:** 一种基于多级细化策略的特征匹配方法<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Jiaxing Ma, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Feature matching is a fundamental and crucial process in visual SLAM, and precision has always been a challenging issue in feature matching. In this paper, based on a multi-level fine matching strategy, we propose a new feature matching method called KTGP-ORB. This method utilizes the similarity of local appearance in the Hamming space generated by feature descriptors to establish initial correspondences. It combines the constraint of local image motion smoothness, uses the GMS algorithm to enhance the accuracy of initial matches, and finally employs the PROSAC algorithm to optimize matches, achieving precise matching based on global grayscale information in Euclidean space. Experimental results demonstrate that the KTGP-ORB method reduces the error by an average of 29.92% compared to the ORB algorithm in complex scenes with illumination variations and blur.</details>
**Abstract_cn:** <details><summary>译文: </summary>特征匹配是视觉SLAM中一个基础且关键的过程，而精度一直是特征匹配中的一个具有挑战性的问题。在本文中，基于多级精细匹配策略，我们提出了一种新的特征匹配方法，称为KTGP-ORB。该方法利用特征描述符生成的汉明空间中局部外观的相似性来建立初始对应关系。结合局部图像运动平滑度的约束，利用GMS算法增强初始匹配的精度，最后利用PROSAC算法进行优化匹配，实现基于欧几里得空间中全局灰度信息的精确匹配。实验结果表明，在光照变化和模糊的复杂场景中，KTGP-ORB方法比ORB算法平均降低了29.92%的误差。</details>
**PDF:** <http://arxiv.org/pdf/2402.13488v1><br />
**Code:** null<br />

