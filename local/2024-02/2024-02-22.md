## [UPDATED!] **2024-02-22** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Cameras as Rays: Pose Estimation via Ray Diffusion**<br />
**Title_cn:** 相机作为光线：通过光线扩散进行姿势估计<br />
**Authors:** Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani<br />
**Abstract:** <details><summary>原文: </summary>Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.</details>
**Abstract_cn:** <details><summary>译文: </summary>估计相机姿态是 3D 重建的一项基本任务，并且在稀疏视图 (<10) 的情况下仍然具有挑战性。与追求相机外参全局参数化的自上而下预测的现有方法相比，我们提出了相机姿态的分布式表示，将相机视为一束光线。这种表示允许与空间图像特征紧密耦合，从而提高姿态精度。我们观察到这种表示自然适合设置级别的转换器，并开发了一种基于回归的方法，将图像块映射到相应的光线。为了捕获稀疏视图姿态推断中固有的不确定性，我们采用这种方法来学习去噪扩散模型，该模型允许我们在提高性能的同时采样合理的模式。我们提出的方法（基于回归和扩散）展示了 CO3D 上相机姿态估计的最先进性能，同时推广到未见过的对象类别和野外捕获。</details>
**PDF:** <http://arxiv.org/pdf/2402.14817v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models**<br />
**Title_cn:** 定制视频：文本到视频扩散模型的一次性运动定制<br />
**Authors:** Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava<br />
**Abstract:** <details><summary>原文: </summary>Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion. Our project page can be found at https://anonymous-314.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像定制在文本到图像（T2I）扩散模型中得到了广泛的研究，产生了令人印象深刻的结果和应用。随着文本到视频（T2V）扩散模型的出现，其时间对应模型——运动定制——尚未得到很好的研究。为了解决一次性运动定制的挑战，我们提出了定制视频，它可以根据单个参考视频对运动进行建模，并使其适应具有空间和时间变化的新主题和场景。它利用时间注意力层上的低秩适应 (LoRA)，为参考视频中的特定运动建模定制预训练的 T2V 扩散模型。为了在训练过程中理清空间和时间信息，我们引入了外观吸收器的新概念，它在运动学习之前将原始外观与单个参考视频分离。我们提出的方法可以以即插即用的方式轻松扩展到各种下游任务，包括自定义视频生成和编辑、视频外观定制和多运动组合。我们的项目页面可以在 https://anonymous-314.github.io 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.14780v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning**<br />
**Title_cn:** 使用自我监督学习在胸部 X 射线中进行零次小儿结核病检测<br />
**Authors:** Daniel Capellán-Martín, Abhijeet Parida, Juan J. Gómez-Valverde, Ramon Sanchez-Jacob, Pooneh Roshanitabrizi, Marius G. Linguraru, María J. Ledesma-Carbayo, Syed M. Anwar<br />
**Abstract:** <details><summary>原文: </summary>Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB. Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection. However, challenges include data scarcity and lack of generalizability. In this context, we propose a novel self-supervised paradigm leveraging Vision Transformers (ViT) for improved TB detection in CXR, enabling zero-shot pediatric TB detection. We demonstrate improvements in TB detection performance ($\sim$12.7% and $\sim$13.4% top AUC/AUPR gains in adults and children, respectively) when conducting self-supervised pre-training when compared to fully-supervised (i.e., non pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR in adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB detection. As a result, this work demonstrates that self-supervised learning on adult CXRs effectively extends to challenging downstream tasks such as pediatric TB detection, where data are scarce.</details>
**Abstract_cn:** <details><summary>译文: </summary>结核病（TB）仍然是一个重大的全球健康挑战，其中儿科病例是一个主要问题。世界卫生组织 (WHO) 提倡使用胸部 X 光检查 (CXR) 进行结核病筛查。然而，放射科医生的目视解释可能是主观的、耗时的并且容易出错，特别是在儿童结核病中。人工智能 (AI) 驱动的计算机辅助检测 (CAD) 工具，特别是那些利用深度学习的工具，在增强肺部疾病检测方面显示出希望。然而，挑战包括数据稀缺和缺乏普遍性。在这种背景下，我们提出了一种新颖的自我监督范例，利用 Vision Transformers (ViT) 来改进 CXR 中的结核病检测，从而实现零样本儿童结核病检测。我们证明，与完全监督（即非预训练）相比，进行自我监督预训练时，结核病检测性能有所提高（成人和儿童的最高 AUC/AUPR 分别提高了 $\sim$12.7% 和 $\sim$13.4%）。 -训练的）ViT 模型，在成人结核病检测中实现了 0.959 AUC 和 0.962 AUPR 的最佳性能，在零次儿童结核病检测中实现了 0.697 AUC 和 0.607 AUPR 的最佳性能。因此，这项工作表明，成人 CXR 的自我监督学习有效地扩展到具有挑战性的下游任务，例如数据稀缺的儿科结核病检测。</details>
**PDF:** <http://arxiv.org/pdf/2402.14741v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Visual Hallucinations of Multi-modal Large Language Models**<br />
**Title_cn:** 多模态大语言模型的视觉幻觉<br />
**Authors:** Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong<br />
**Abstract:** <details><summary>原文: </summary>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</details>
**Abstract_cn:** <details><summary>译文: </summary>幻视 (VH) 是指多模态法学硕士 (MLLM) 在视觉问答中想象出有关图像的错误细节。现有研究仅在现有图像数据集中发现 VH 实例，由于此类 VH 实例的多样性有限，导致对 VH 下 MLLM 性能的理解存在偏差。在这项工作中，我们提出了一个名为 VHTest 的工具来生成一组不同的 VH 实例。具体来说，VHTest在现有图像数据集中找到一些初始VH实例（例如COCO），为每个VH模式生成文本描述，并使用文本到图像生成模型（例如DALL-E-3）生成VH图像根据文字描述。我们使用 VHTest 收集了一个基准数据集，其中包含 8 种 VH 模式下的 1,200 个 VH 实例。我们发现现有的 MLLM（例如 GPT-4V、LLaVA-1.5 和 MiniGPT-v2）对我们基准测试中的大部分实例产生幻觉。此外，我们发现使用我们的基准数据集微调 MLLM 可以降低其产生幻觉的可能性，而不会牺牲其在其他基准上的性能。我们的基准测试是公开的：https://github.com/wenhuang2000/VHTest。</details>
**PDF:** <http://arxiv.org/pdf/2402.14683v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Debiasing Text-to-Image Diffusion Models**<br />
**Title_cn:** 消除文本到图像扩散模型的偏差<br />
**Authors:** Ruifei He, Chuhui Xue, Haoru Tan, Wenqing Zhang, Yingchen Yu, Song Bai, Xiaojuan Qi<br />
**Abstract:** <details><summary>原文: </summary>Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI diffusion models. We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process. Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models. Our code will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于学习的文本到图像 (TTI) 模型（例如稳定扩散）彻底改变了各个领域中视觉内容的生成方式。然而，最近的研究表明，当前最先进的 TTI 系统中存在不可忽视的社会偏见，这引起了人们的严重担忧。在这项工作中，我们的目标是解决 TTI 扩散模型中的社会偏见。我们首先将问题设置形式化，并使用偏见群体的文本描述来建立一个不安全的方向来指导扩散过程。接下来，我们将问题简化为权重优化问题，并尝试使用强化求解器“策略梯度”，该求解器表现出收敛速度较慢的次优性能。此外，为了克服限制，我们提出了迭代分布对齐（IDA）方法。尽管它很简单，但我们表明 IDA 在解决 TTI 扩散模型中的社会偏见方面表现出效率和快速收敛。我们的代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.14577v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning**<br />
**Title_cn:** 通过离散扩散进行大规模无动作视频预训练，以实现高效的策略学习<br />
**Authors:** Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li<br />
**Abstract:** <details><summary>原文: </summary>Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习能够完成多项任务的通才实体代理提出了挑战，这主要源于动作标记机器人数据集的稀缺。相比之下，存在大量的人类视频，捕捉复杂的任务以及与物理世界的交互。利用无动作的人类视频进行预训练和转移知识以通过有限的机器人演示促进机器人策略学习的前景广阔。在本文中，我们介绍了一种新颖的框架，该框架利用统一的离散扩散将人类视频的生成预训练和少量带有动作标记的机器人视频的策略微调结合起来。我们首先将人类和机器人视频压缩成统一的视频令牌。在预训练阶段，我们采用具有掩模和替换扩散策略的离散扩散模型来预测潜在空间中的未来视频标记。在微调阶段，我们利用想象中的未来视频来指导在有限的机器人数据集上训练的低级动作学习。实验表明，与以前最先进的方法相比，我们的方法可以生成用于规划的高保真未来视频，并增强了微调策略，具有卓越的泛化能力。我们的项目网站位于 https://video-diff.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.14407v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment**<br />
**Title_cn:** 基于扩散模型的视觉补偿引导和视觉差异分析用于无参考图像质量评估<br />
**Authors:** Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao<br />
**Abstract:** <details><summary>原文: </summary>Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的自由能引导无参考图像质量评估（NR-IQA）方法仍然面临着在学习图像像素级特征信息和捕获高级特征信息以及有效利用所获得的高级特征信息之间寻找平衡的问题。级别特征信息仍然是一个挑战。作为一类新颖的最先进（SOTA）生成模型，扩散模型展现了对复杂关系进行建模的能力，能够全面理解图像并更好地学习高级和低级视觉特征。鉴于这些，我们率先将扩散模型探索到 NR-IQA 领域。首先，我们设计了一种新的扩散恢复网络，利用生成的增强图像和含噪声图像，结合扩散模型去噪过程中获得的非线性特征作为高级视觉信息。其次，设计了两个视觉评估分支来综合分析获得的高级特征信息。其中包括基于 Transformer 架构和噪声嵌入策略的视觉补偿引导分支，以及基于 ResNet 架构和残差转置注意块构建的视觉差异分析分支。在七个公共 NR-IQA 数据集上进行了大量实验，结果表明所提出的模型优于 NR-IQA 的 SOTA 方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.14401v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing**<br />
**Title_cn:** 渐进残差对齐：GAN 反演和图像属性编辑的双流框架<br />
**Authors:** Hao Li, Mengqi Huang, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao<br />
**Abstract:** <details><summary>原文: </summary>GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage. The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the GAN generator. In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner. Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于GAN的图像属性编辑首先利用GAN反转将真实图像投影到GAN的潜在空间中，然后操纵相应的潜在代码。最近的反演方法主要利用额外的高位特征来改善图像细节保留，因为低位代码无法忠实地重建源图像，导致细节丢失。然而，现有作品在编辑过程中无法准确补充丢失的细节，可编辑性较差。主要原因是它们一次不加区别地注入所有丢失的细节，这本质上会导致细节的位置和数量与源图像过度拟合，从而导致编辑后的图像内容不一致和伪影。这项工作认为，细节应该以多阶段由粗到细的方式逐渐注入重建和编辑过程中，以获得更好的细节保留和高可编辑性。因此，提出了一种新颖的双流框架来准确补充每个阶段的细节。重建流用于将粗到细的丢失细节嵌入到残差特征中，然后自适应地将它们添加到 GAN 生成器中。在编辑流中，残余特征通过我们的选择性注意机制精确对齐，然后以多阶段的方式注入到编辑过程中。大量的实验表明，与现有方法相比，我们的框架在重建精度和编辑质量方面均具有优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14398v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation**<br />
**Title_cn:** 心外膜脂肪组织分割的不确定性驱动和对抗性校准学习<br />
**Authors:** Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li<br />
**Abstract:** <details><summary>原文: </summary>Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR. Second, an adversarial training strategy is introduced to calibrate the segmentation feature map and consider the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale adversarial loss directly improves the ability to discriminate the similarity between organizations. Experiments on both the cardiac public MRI dataset (ACDC) and the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and adversarial calibration learning can be used to provide additional information for modeling multi-scale ambiguities.</details>
**Abstract_cn:** <details><summary>译文: </summary>心外膜脂肪组织（EAT）是一种内脏脂肪，可以分泌大量脂肪因子来影响心肌和冠状动脉。 EAT体积和密度可作为独立的风险标志物，通过无创磁共振图像测量体积是评估EAT的最佳方法。然而，由于 EAT 和心包积液之间的对比度较低以及运动伪影的存在，分割 EAT 具有挑战性。我们提出了一种新颖的特征潜在空间多级监督网络（SPDNet），具有不确定性驱动和对抗性校准学习，以增强分割以实现更准确的 EAT 体积估计。该网络首先通过将不确定性建模为特征潜在空间中的高斯分布，并使用贝叶斯估计作为正则化，解决开放医疗环境中低质量或分布不均的医学图像导致的 EAT 边缘模糊问题优化 SwinUNETR 的约束。其次，引入对抗性训练策略来校准分割特征图，并考虑不确定性引导的预测分割和地面真值分割之间的多尺度特征差异，综合多尺度对抗性损失直接提高判别相似性的能力组织之间。在心脏公共 MRI 数据集（ACDC）和真实世界临床队列 EAT 数据集上的实验表明，所提出的网络优于主流模型，验证了不确定性驱动和对抗性校准学习可用于为多尺度建模提供附加信息含糊之处。</details>
**PDF:** <http://arxiv.org/pdf/2402.14349v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Typographic Text Generation with Off-the-Shelf Diffusion Model**<br />
**Title_cn:** 使用现成的扩散模型生成印刷文本<br />
**Authors:** KhayTze Peong, Seiichi Uchida, Daichi Haraguchi<br />
**Abstract:** <details><summary>原文: </summary>Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion. The former functions to generate text images under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background. We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts. We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''. Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的基于扩散的生成模型在生成文本图像的能力方面表现出了希望，但是指定生成文本的样式的限制使得它们在版式设计领域中表现不足。本文提出了一种印刷文本生成系统，用于在印刷设计上添加和修改文本，同时指定字体样式、颜色和文本效果。所提出的系统是两种现成的扩散模型方法（ControlNet 和混合潜在扩散）的新颖组合。前者的功能是在指定笔划轮廓的边缘条件的指导下生成文本图像。后者混合潜在扩散模型 (LDM) 中的潜在噪声，将印刷文本自然地添加到现有背景上。我们首先证明，给定适当的文本边缘，ControlNet 可以生成指定字体的文本，同时结合提示描述的效果。我们进一步引入文本边缘操作作为一种直观且可定制的方式来生成具有复杂效果（例如“阴影”和“反射”）的文本。最后，通过所提出的系统，我们成功地在预定义背景上添加和修改文本，同时保持其整体连贯性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14314v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Font Style Interpolation with Diffusion Models**<br />
**Title_cn:** 使用扩散模型进行字体样式插值<br />
**Authors:** Tetta Kondo, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>Fonts have huge variations in their styles and give readers different impressions. Therefore, generating new fonts is worthy of giving new impressions to readers. In this paper, we employ diffusion models to generate new font styles by interpolating a pair of reference fonts with different styles. More specifically, we propose three different interpolation approaches, image-blending, condition-blending, and noise-blending, with the diffusion models. We perform qualitative and quantitative experimental analyses to understand the style generation ability of the three approaches. According to experimental results, three proposed approaches can generate not only expected font styles but also somewhat serendipitous font styles. We also compare the approaches with a state-of-the-art style-conditional Latin-font generative network model to confirm the validity of using the diffusion models for the style interpolation task.</details>
**Abstract_cn:** <details><summary>译文: </summary>字体的风格差异很大，给读者带来不同的印象。因此，生成新的字体值得给读者带来新的印象。在本文中，我们采用扩散模型通过插入一对具有不同样式的参考字体来生成新的字体样式。更具体地说，我们提出了三种不同的插值方法，即图像混合、条件混合和噪声混合以及扩散模型。我们进行定性和定量实验分析，以了解三种方法的风格生成能力。根据实验结果，提出的三种方法不仅可以生成预期的字体样式，还可以生成一些偶然的字体样式。我们还将这些方法与最先进的风格条件拉丁字体生成网络模型进行比较，以确认使用扩散模型进行风格插值任务的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14311v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation**<br />
**Title_cn:** 将视觉上下文学习与掩模图像建模相结合以改进超声分割的简单框架<br />
**Authors:** Yuyue Zhou, Banafshe Felfeliyan, Shrimanti Ghosh, Jessica Knight, Fatima Alves-Pereira, Christopher Keen, Jessica Küpper, Abhilash Rakkunedeth Hareendranathan, Jacob L. Jaremko<br />
**Abstract:** <details><summary>原文: </summary>Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability. Visual in-context learning (ICL) is a new and exciting area of research in computer vision. Unlike conventional deep learning, ICL emphasizes the model's ability to adapt to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we proposed a new simple visual ICL method called SimICL, combining visual ICL pairing images with masked image modeling (MIM) designed for self-supervised learning. We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection. We used a test set containing 3822 images from 18 patients for bony region segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16. This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets. This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的深度学习模型对图像进行一张一张的处理，在医学成像领域需要昂贵且耗时的专家标记，并且特定领域的限制限制了模型的通用性。视觉情境学习（ICL）是计算机视觉领域一个令人兴奋的新研究领域。与传统的深度学习不同，ICL 强调模型根据给定示例快速适应新任务的能力。受 MAE-VQGAN 的启发，我们提出了一种名为 SimICL 的新的简单视觉 ICL 方法，将视觉 ICL 配对图像与专为自监督学习而设计的掩模图像建模 (MIM) 相结合。我们在带有有限注释的腕部超声（美国）数据集中验证了我们的骨结构分割方法，其中临床目标是分割骨结构以帮助进一步的骨折检测。我们使用包含来自 18 名患者的 3822 个图像的测试集进行骨区域分割。 SimICL 实现了非常高的 Dice 系数 (DC) 0.96 和 Jaccard 指数 (IoU) 0.92，超越了最先进的分割和视觉 ICL 模型（最大 DC 0.86 和 IoU 0.76），并且 SimICL DC 和 IoU 不断增加高达 0.10 和 0.16。这种与有限的手动注释的高度一致性表明 SimICL 可以用于训练 AI 模型，甚至可以在美国的小型数据集上进行训练。与传统方法相比，这可以大大减少人类专家进行图像标记所需的时间，并增强人工智能辅助在美国图像分析中的实际使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.14300v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion**<br />
**Title_cn:** MVD$^2$：用于多视图扩散的高效多视图 3D 重建<br />
**Authors:** Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD$^2$, an efficient 3D reconstruction method for multiview diffusion (MVD) images. MVD$^2$ aggregates image features into a 3D feature volume by projection and convolution and then decodes volumetric features into a 3D mesh. We train MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme. MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为一种有前途的 3D 生成技术，多视图扩散（MVD）由于其在通用性、质量和效率方面的优势而受到了广泛的关注。通过使用 3D 数据微调预训练的大型图像扩散模型，MVD 方法首先根据图像或文本提示生成 3D 对象的多个视图，然后通过多视图 3D 重建来重建 3D 形状。然而，生成图像中的稀疏视图和不一致的细节使得 3D 重建具有挑战性。我们提出了 MVD$^2$，一种用于多视图扩散 (MVD) 图像的高效 3D 重建方法。 MVD$^2$通过投影和卷积将图像特征聚合成3D特征体积，然后将体积特征解码成3D网格。我们使用 3D 形状集合和 3D 形状渲染视图提示的 MVD 图像来训练 MVD$^2$。为了解决生成的多视图图像与 3D 形状的真实视图之间的差异，我们设计了一种简单而有效的视图相关训练方案。 MVD$^2$ 提高了 MVD 的 3D 生成质量，并且对各种 MVD 方法快速且鲁棒。训练后，它可以在一秒内有效地从多视图图像中解码 3D 网格。我们使用 Zero-123++ 和 ObjectVerse-LVIS 3D 数据集训练 MVD$^2$，并使用合成图像和真实图像作为提示，展示了其从不同 MVD 方法生成的多视图图像生成 3D 模型的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14253v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **PALO: A Polyglot Large Multimodal Model for 5B People**<br />
**Title_cn:** PALO：面向 5B 人群的多语言大型多模式模型<br />
**Authors:** Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan<br />
**Abstract:** <details><summary>原文: </summary>In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了追求更具包容性的视觉语言模型（VLM），本研究引入了一种名为 \textsc{Palo} 的大型多语言多模态模型。 \textsc{Palo} 提供 10 种主​​要语言的视觉推理功能，包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语，覆盖总共 $\sim$5B 人（65\占世界人口的百分比）。我们的方法涉及一种半自动翻译方法，使用微调的大语言模型将多模式指令数据集从英语调整为目标语言，从而确保高语言保真度，同时由于最少的手动工作而允许可扩展性。不同指令集的结合有助于我们提高多种语言的整体性能，特别是那些代表性不足的语言，如印地语、阿拉伯语、孟加拉语和乌尔都语。生成的模型在三个尺度（1.7B、7B 和 13B 参数）上进行训练，以显示泛化性和可扩展性，与强大的基线相比，我们观察到了显着的改进。我们还为即将推出的方法提出了第一个多语言多模式基准，以评估其跨语言的视觉语言推理能力。代码：https://github.com/mbzuai-oryx/PALO。</details>
**PDF:** <http://arxiv.org/pdf/2402.14818v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset**<br />
**Title_cn:** 使用 MATH-Vision 数据集测量多模态数学推理<br />
**Authors:** Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The project is available at https://mathvision-cuhk.github.io</details>
**Abstract_cn:** <details><summary>译文: </summary>大型多模态模型 (LMM) 的最新进展在视觉上下文中的数学推理方面显示出了有希望的结果，模型在 MathVista 等现有基准上接近人类水平的表现。然而，我们观察到这些基准所涵盖的问题多样性和主题广度存在显着局限性。为了解决这个问题，我们推出了 MATH-Vision (MATH-V) 数据集，这是一个精心策划的集合，包含 3,040 个高质量数学问题，其视觉背景来自真实的数学竞赛。我们的数据集跨越 16 个不同的数学学科，分为 5 个难度级别，为评估 LMM 的数学推理能力提供了一套全面且多样化的挑战。通过广泛的实验，我们揭示了当前 LMM 与人类在 MATH-V 上的表现之间存在显着的性能差距，强调了 LMM 进一步进步的必要性。此外，我们详细的分类可以对 LMM 进行彻底的错误分析，为指导未来的研究和开发提供有价值的见解。该项目位于 https://mathvision-cuhk.github.io</details>
**PDF:** <http://arxiv.org/pdf/2402.14804v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models**<br />
**Title_cn:** DualFocus：在多模态大语言模型中整合宏观和微观视角<br />
**Authors:** Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang<br />
**Abstract:** <details><summary>原文: </summary>We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance. Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions. We introduced a DualFocus mechanism where the model concentrates on the image from a macro perspective, responses to the question, and identifies suitable sub-regions to zoom in for subsequent micro perspective analysis. Via the integration of answers from both macro and micro perspectives, the model is adept at addressing tasks that encompass global, detailed, and combined considerations. To endows the DualFocus mechanism in MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and adapted it to align with the training regimen of DualFocus. Through comparative studies across different model sizes and benchmarks, we demonstrate DualFocus's superiority in balancing detailed examination with holistic insight, significantly reducing hallucination instances in MLLMs and improving their performance in various vision-language tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 DualFocus，这是一种新颖的框架，用于将宏观和微观视角集成到多模态大语言模型 (MLLM) 中，以增强视觉语言任务性能。当前的 MLLM 通常只关注预定义分辨率的输入，导致涉及当地区域的详细问题存在缺陷。我们引入了 DualFocus 机制，模型从宏观角度聚焦图像，回答问题，并识别合适的子区域进行放大以进行后续的微观透视分析。通过整合宏观和微观角度的答案，该模型擅长解决包含全局、详细和综合考虑的任务。为了在 MLLM 中赋予 DualFocus 机制，我们策划了一个源自视觉基因组 (VG) 的定制数据集，并对其进行了调整以与 DualFocus 的训练方案保持一致。通过不同模型大小和基准的比较研究，我们证明了 DualFocus 在平衡详细检查与整体洞察力方面的优越性，显着减少 MLLM 中的幻觉实例并提高其在各种视觉语言任务中的表现。</details>
**PDF:** <http://arxiv.org/pdf/2402.14767v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation**<br />
**Title_cn:** RoboScript：跨真实和模拟的自由形式操作任务的代码生成<br />
**Authors:** Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo. We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.</details>
**Abstract_cn:** <details><summary>译文: </summary>Embodied AI 见证了开放世界机器人操作的高级任务规划和代码生成方面的快速进展。然而，以前的研究将大量精力放在大规模语言或多模态模型的一般常识推理和任务规划能力上，而在确保生成的代码在真实机器人上的可部署性以及自主机器人系统的其他基本组件上投入的精力相对较少，包括机器人感知、运动规划和控制。为了弥合这种“理想与现实”的差距，本文提出了 \textbf{RobotScript}，这是一个平台，用于 1) 由代码生成驱动的可部署机器人操作管道； 2）自由形式自然语言的机器人操作任务的代码生成基准。 RobotScript 平台基于机器人操作系统 (ROS) 的抽象，强调与模拟和真实机器人的统一接口，从而解决了这一差距，确保语法合规性和 Gazebo 的模拟验证。我们展示了代码生成框架在多个机器人实施例中的适应性，包括 Franka 和 UR5 机器人手臂以及多个夹具。此外，我们的基准评估了物理空间和约束的推理能力，突出了 GPT-3.5、GPT-4 和 Gemini 在处理复杂物理交互方面的差异。最后，我们对整个系统进行了全面的评估，探索管道中的每个模块：代码生成、感知、运动规划，甚至对象几何属性，如何影响系统的整体性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14623v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective**<br />
**Title_cn:** 少即是多：从 EOS 决策角度减轻多模态幻觉<br />
**Authors:** Zihao Yue, Liang Zhang, Qin Jin<br />
**Abstract:** <details><summary>原文: </summary>Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型多模态模型 (LMM) 经常遭受多模态幻觉的困扰，其中它们可能会创建视觉输入中不存在的内容。在本文中，我们探讨了这个问题的新角度：过于详细的训练数据阻碍了模型及时终止生成的能力，导致持续输出超出视觉感知限制。通过研究模型如何决定用 EOS（特殊的句尾标记）终止生成，我们发现模型通过将生成的文本与图像进行比较来评估整个序列的完整性。这一观察结果表明，该模型具有根据其视觉感知做出正确 EOS 决策的内在潜力，以避免输出过长。为了利用这种潜力，我们探索了两种减轻多模态幻觉的方法：训练目标使模型能够通过从常规指令数据中学习来减少幻觉，以及数据过滤策略以防止有害的训练数据加剧模型幻觉。这两种方法都显着提高了 LMM 的幻觉性能，而不需要任何额外的数据或知识。</details>
**PDF:** <http://arxiv.org/pdf/2402.14545v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Uncertainty-Aware Evaluation for Vision-Language Models**<br />
**Title_cn:** 视觉语言模型的不确定性评估<br />
**Authors:** Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities.   Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.</details>
**Abstract_cn:** <details><summary>译文: </summary>GPT-4、LLaVA 和 CogVLM 等视觉语言模型最近因其在多种视觉语言任务中的出色表现而广受欢迎。然而，当前的评估方法忽略了一个重要组成部分：不确定性，这对于 VLM 的综合评估至关重要。为了解决这一疏忽，我们提出了一个将不确定性量化纳入评估 VLM 的基准。我们的分析涵盖 20 多个 VLM，重点关注多项选择视觉问答 (VQA) 任务。我们检查了 5 个数据集上的模型，以评估各种视觉语言功能。使用共形预测作为不确定性估计方法，我们证明模型的不确定性与其准确性不一致。具体来说，我们表明具有最高准确度的模型也可能具有最高的不确定性，这证实了测量 VLM 的不确定性的重要性。我们的实证研究结果还揭示了模型不确定性与其语言模型部分之间的相关性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14418v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **Consolidating Attention Features for Multi-view Image Editing**<br />
**Title_cn:** 整合多视图图像编辑的注意力特征<br />
**Authors:** Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre<br />
**Abstract:** <details><summary>原文: </summary>Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型文本到图像模型支持多种图像编辑技术，使用文本提示甚至空间控制。然而，将这些编辑方法应用于描绘单个场景的多视图图像会导致 3D 不一致的结果。在这项工作中，我们专注于基于空间控制的几何操作，并介绍一种跨各种视图整合编辑过程的方法。我们基于两个见解：（1）在整个生成过程中保持一致的特征有助于实现多视图编辑的一致性，（2）自注意力层中的查询显着影响图像结构。因此，我们建议通过强制查询的一致性来提高编辑图像的几何一致性。为此，我们引入了 QNeRF，这是一种根据编辑图像的内部查询特征进行训练的神经辐射场。经过训练后，QNeRF 可以渲染 3D 一致的查询，然后在生成过程中将其软注入回自注意力层中，从而极大地提高多视图一致性。我们通过渐进式迭代方法改进流程，更好地整合扩散时间步长中的查询。我们将我们的方法与一系列现有技术进行比较，并证明它可以实现更好的多视图一致性和对输入场景的更高保真度。这些优点使我们能够以更少的视觉伪影来训练 NeRF，并且与目标几何形状更好地对齐。</details>
**PDF:** <http://arxiv.org/pdf/2402.14792v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis**<br />
**Title_cn:** FrameNeRF：一种简单高效的小样本新颖视图合成框架<br />
**Authors:** Yan Xing, Pan Wang, Ligang Liu, Daolun Li, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>We present a novel framework, called FrameNeRF, designed to apply off-the-shelf fast high-fidelity NeRF models with fast training speed and high rendering quality for few-shot novel view synthesis tasks. The training stability of fast high-fidelity models is typically constrained to dense views, making them unsuitable for few-shot novel view synthesis tasks. To address this limitation, we utilize a regularization model as a data generator to produce dense views from sparse inputs, facilitating subsequent training of fast high-fidelity models. Since these dense views are pseudo ground truth generated by the regularization model, original sparse images are then used to fine-tune the fast high-fidelity model. This process helps the model learn realistic details and correct artifacts introduced in earlier stages. By leveraging an off-the-shelf regularization model and a fast high-fidelity model, our approach achieves state-of-the-art performance across various benchmark datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个名为 FrameNeRF 的新颖框架，旨在应用现成的快速高保真 NeRF 模型，该模型具有快速训练速度和高渲染质量，适用于少样本新颖的视图合成任务。快速高保真模型的训练稳定性通常受限于密集视图，这使得它们不适合少量的新颖视图合成任务。为了解决这个限制，我们利用正则化模型作为数据生成器，从稀疏输入生成密集视图，从而促进快速高保真模型的后续训练。由于这些密集视图是由正则化模型生成的伪地面实况，因此原始稀疏图像随后用于微调快速高保真模型。此过程有助于模型学习真实的细节并纠正早期阶段引入的工件。通过利用现成的正则化模型和快速高保真模型，我们的方法在各种基准数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14586v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection**<br />
**Title_cn:** NeRF-Det++：将语义提示和透视感知深度监督结合起来进行室内多视图 3D 检测<br />
**Authors:** Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang<br />
**Abstract:** <details><summary>原文: </summary>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at https://github.com/mrsempress/NeRF-Detplusplus.</details>
**Abstract_cn:** <details><summary>译文: </summary>NeRF-Det 通过创新地利用 NeRF 来增强表示学习，在室内多视图 3D 检测中取得了令人印象深刻的性能。尽管其性能显着，但我们发现其当前设计中的三个决定性缺陷，包括语义模糊、采样不当和深度监督利用不足。为了解决上述问题，我们提出了三种相应的解决方案：1）语义增强。我们将免费提供的 3D 分割注释投影到 2D 平面上，并利用相应的 2D 语义图作为监督信号，显着增强了多视图检测器的语义感知。 2) 视角感知采样。我们没有采用统一采样策略，而是提出了透视感知采样策略，在相机附近密集采样，在远处稀疏采样，更有效地收集有价值的几何线索。 3）序数剩余深度监督。与直接回归难以优化的深度值相反，我们将每个场景的深度范围划分为固定数量的序数箱，并将深度预测重新表述为深度箱的分类以及深度箱的回归的组合。剩余深度值，从而有利于深度学习过程。由此产生的算法 NeRF-Det++ 在 ScanNetV2 和 ARKITScenes 数据集中表现出了吸引人的性能。值得注意的是，在 ScanNetV2 中，NeRF-Det++ 在 mAP@0.25 中比竞争对手 NeRF-Det 的性能高出 +1.9%，在 mAP@0.50$ 中比竞争对手 NeRF-Det 高出 +3.5%。该代码将在 https://github.com/mrsempress/NeRF-Detplusplus 上公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.14464v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization**<br />
**Title_cn:** TaylorGrid：通过直接基于泰勒的网格优化实现快速、高质量的隐式场学习<br />
**Authors:** Renyi Mao, Qingshan Xu, Peng Zheng, Ye Wang, Tieru Wu, Rui Ma<br />
**Abstract:** <details><summary>原文: </summary>Coordinate-based neural implicit representation or implicit fields have been widely studied for 3D geometry representation or novel view synthesis. Recently, a series of efforts have been devoted to accelerating the speed and improving the quality of the coordinate-based implicit field learning. Instead of learning heavy MLPs to predict the neural implicit values for the query coordinates, neural voxels or grids combined with shallow MLPs have been proposed to achieve high-quality implicit field learning with reduced optimization time. On the other hand, lightweight field representations such as linear grid have been proposed to further improve the learning speed. In this paper, we aim for both fast and high-quality implicit field learning, and propose TaylorGrid, a novel implicit field representation which can be efficiently computed via direct Taylor expansion optimization on 2D or 3D grids. As a general representation, TaylorGrid can be adapted to different implicit fields learning tasks such as SDF learning or NeRF. From extensive quantitative and qualitative comparisons, TaylorGrid achieves a balance between the linear grid and neural voxels, showing its superiority in fast and high-quality implicit field learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于坐标的神经隐式表示或隐式场已被广泛研究用于 3D 几何表示或新颖的视图合成。最近，一系列的努力致力于加快基于坐标的隐式场学习的速度和提高质量。神经体素或网格与浅 MLP 相结合，而不是学习重 MLP 来预测查询坐标的神经隐式值，以实现高质量的隐式场学习，并减少优化时间。另一方面，线性网格等轻量级场表示被提出来进一步提高学习速度。在本文中，我们的目标是快速和高质量的隐式场学习，并提出了 TaylorGrid，一种新颖的隐式场表示，可以通过 2D 或 3D 网格上的直接泰勒展开优化来有效计算。作为通用表示，TaylorGrid 可以适应不同的隐式领域学习任务，例如 SDF 学习或 NeRF。通过广泛的定量和定性比较，TaylorGrid实现了线性网格和神经体素之间的平衡，显示了其在快速、高质量隐式场学习方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14415v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields**<br />
**Title_cn:** Mip-Grid：神经辐射场的抗锯齿网格表示<br />
**Authors:** Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering "jaggies" or "blurry" images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see https://stnamjef.github.io/mipgrid.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管神经辐射场 (NeRF) 在表示 3D 场景和生成新颖的视图图像方面取得了显着的成就，但在大多数现有方法中，在不同相机距离渲染“锯齿”或“模糊”图像的混叠问题仍然没有得到解决。最近提出的 mip-NeRF 通过渲染截头圆锥体而不是射线解决了这一挑战。然而，它依赖 MLP 架构来表示辐射场，错过了最新的基于网格的方法所提供的快速训练速度。在这项工作中，我们提出了 mip-Grid，这是一种新颖的方法，它将抗锯齿技术集成到基于网格的辐射场表示中，在享受快速训练时间的同时减轻锯齿伪影。所提出的方法通过在共享网格表示上应用简单的卷积运算来生成多尺度网格，并使用尺度感知坐标从生成的多尺度网格中检索不同尺度的特征。为了测试有效性，我们将所提出的方法集成到最近两种代表性的基于网格的方法 TensoRF 和 K-Planes 中。实验结果表明，mip-Grid 极大地提高了两种方法的渲染性能，甚至在多尺度数据集上优于 mip-NeRF，同时显着缩短了训练时间。代码和演示视频请参见https://stnamjef.github.io/mipgrid.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.14196v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **GaussianPro: 3D Gaussian Splatting with Progressive Propagation**<br />
**Title_cn:** GaussianPro：具有渐进传播的 3D 高斯泼溅<br />
**Authors:** Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen<br />
**Abstract:** <details><summary>原文: </summary>The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D高斯泼溅（3DGS）的出现最近给神经渲染领域带来了一场革命，促进了实时速度的高质量渲染。然而，3DGS 在很大程度上依赖于运动结构 (SfM) 技术生成的初始化点云。当处理不可避免地包含无纹理表面的大型场景时，SfM 技术总是无法在这些表面中产生足够的点，并且无法为 3DGS 提供良好的初始化。因此，3DGS 面临优化困难和渲染质量低的问题。在本文中，受经典多视图立体 (MVS) 技术的启发，我们提出了 GaussianPro，这是一种应用渐进传播策略来指导 3D 高斯的致密化的新颖方法。与 3DGS 中使用的简单分割和克隆策略相比，我们的方法利用场景的现有重建几何形状的先验和补丁匹配技术来生成具有准确位置和方向的新高斯。大规模和小规模场景的实验验证了我们方法的有效性，我们的方法在Waymo数据集上显着超越了3DGS，在PSNR方面表现出1.15dB的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.14650v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation**<br />
**Title_cn:** TIE-KD：用于单目深度估计的独立于教师且可解释的知识蒸馏<br />
**Authors:** Sangwon Choi, Daejune Choi, Duksu Kim<br />
**Abstract:** <details><summary>原文: </summary>Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models. To mitigate this, we introduce a novel Teacher-Independent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher's output, enabling feature-based knowledge distillation solely from the teacher's response. This approach allows for efficient student learning, leveraging the strengths of feature-based distillation. Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目深度估计 (MDE) 对于许多应用至关重要，但受到精确深度学习模型的大量计算需求的阻碍。为了缓解这个问题，我们引入了一种新颖的独立于教师的可解释知识蒸馏（TIE-KD）框架，该框架简化了从复杂的教师模型到紧凑的学生网络的知识转移，消除了对架构相似性的需要。 TIE-KD 的基石是深度概率图 (DPM)，这是一种可解释的特征图，可以解释教师的输出，从而能够仅从教师的响应中提取基于特征的知识。这种方法可以利用基于特征的蒸馏的优势，实现学生的高效学习。对 KITTI 数据集的广泛评估表明，TIE-KD 不仅优于传统的基于响应的 KD 方法，而且在不同的教师和学生架构中表现出一致的功效。 TIE-KD 的稳健性和适应性凸显了其在需要高效和可解释模型的应用中的潜力，证实了其在现实世界部署的实用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14340v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition**<br />
**Title_cn:** WeakSAM：分割任何东西满足弱监督实例级识别<br />
**Authors:** Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang<br />
**Abstract:** <details><summary>原文: </summary>Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用不精确监督的弱监督视觉识别是一个关键但具有挑战性的学习问题。它显着降低了人工标记成本，并且传统上依赖于多实例学习和伪标记。本文介绍了 WeakSAM，并利用视觉基础模型（即分段任意模型（SAM））中包含的预先学习的世界知识来解决弱监督目标检测（WSOD）和分割问题。 WeakSAM 通过自适应 PGT 生成和感兴趣区域 (RoI) 丢弃正则化，解决了传统 WSOD 再训练中的两个关键限制，即伪地面实况 (PGT) 不完整性和嘈杂的 PGT 实例。它还解决了 SAM 在自动对象检测和分割时需要提示和类别无意识的问题。我们的结果表明，WeakSAM 在 WSOD 和 WSIS 基准测试中显着超越了之前最先进的方法，并且大幅提升，即平均分别提高了 7.4% 和 8.5%。代码可在 \url{https://github.com/hustvl/WeakSAM} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.14812v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Transformer Model for Boundary Detection in Continuous Sign Language**<br />
**Title_cn:** 连续手语边界检测的 Transformer 模型<br />
**Authors:** Razieh Rastgoo, Kourosh Kiani, Sergio Escalera<br />
**Abstract:** <details><summary>原文: </summary>Sign Language Recognition (SLR) has garnered significant attention from researchers in recent years, particularly the intricate domain of Continuous Sign Language Recognition (CSLR), which presents heightened complexity compared to Isolated Sign Language Recognition (ISLR). One of the prominent challenges in CSLR pertains to accurately detecting the boundaries of isolated signs within a continuous video stream. Additionally, the reliance on handcrafted features in existing models poses a challenge to achieving optimal accuracy. To surmount these challenges, we propose a novel approach utilizing a Transformer-based model. Unlike traditional models, our approach focuses on enhancing accuracy while eliminating the need for handcrafted features. The Transformer model is employed for both ISLR and CSLR. The training process involves using isolated sign videos, where hand keypoint features extracted from the input video are enriched using the Transformer model. Subsequently, these enriched features are forwarded to the final classification layer. The trained model, coupled with a post-processing method, is then applied to detect isolated sign boundaries within continuous sign videos. The evaluation of our model is conducted on two distinct datasets, including both continuous signs and their corresponding isolated signs, demonstrates promising results.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，手语识别 (SLR) 引起了研究人员的极大关注，特别是连续手语识别 (CSLR) 的复杂领域，与孤立手语识别 (ISLR) 相比，其复杂性更高。 CSLR 的突出挑战之一涉及准确检测连续视频流中孤立标志的边界。此外，现有模型对手工特征的依赖对实现最佳精度提出了挑战。为了克服这些挑战，我们提出了一种利用基于 Transformer 的模型的新方法。与传统模型不同，我们的方法侧重于提高准确性，同时消除对手工制作特征的需求。 ISLR 和 CSLR 均采用 Transformer 模型。训练过程涉及使用孤立的手势视频，其中使用 Transformer 模型丰富了从输入视频中提取的手部关键点特征。随后，这些丰富的特征被转发到最终的分类层。然后，将经过训练的模型与后处理方法相结合，用于检测连续标志视频中的孤立标志边界。我们的模型的评估是在两个不同的数据集上进行的，包括连续符号和相应的孤立符号，显示出有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.14720v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening**<br />
**Title_cn:** 用于增强宫颈异常筛查的两阶段细胞病理学图像合成<br />
**Authors:** Zhenrong Shen, Manman Fei, Xin Wang, Jiangdong Cai, Sheng Wang, Lichi Zhang, Qian Wang<br />
**Abstract:** <details><summary>原文: </summary>Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis. Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training data with high-quality annotations to achieve promising performance. Pathological image synthesis is naturally raised to minimize the efforts in data collection and annotation. However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells. In this paper, we propose a two-stage image synthesis framework to create synthetic data for augmenting cervical abnormality screening. In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells. In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell Synthesizer are built upon the pre-trained Stable Diffusion via parameter-efficient fine-tuning methods for customizing cytopathological image contents and extending spatial layout controllability, respectively. Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its data augmentation effectiveness in enhancing the performance of abnormal cervical cell detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动薄层细胞学检测 (TCT) 筛查可以帮助病理学家发现宫颈异常，从而实现准确、高效的宫颈癌诊断。目前的自动TCT筛查系统大多涉及异常宫颈细胞检测，这通常需要大规模、多样化的训练数据和高质量的注释才能获得有希望的性能。病理图像合成自然而然地被提出，以最大限度地减少数据收集和注释的工作量。然而，生成逼真的大尺寸细胞病理学图像，同时合成小尺寸异常宫颈细胞的视觉上合理的外观是具有挑战性的。在本文中，我们提出了一个两阶段图像合成框架来创建合成数据以增强宫颈异常筛查。在第一个全局图像生成阶段，正常图像生成器旨在生成充满正常宫颈细胞的细胞病理学图像。在第二个局部细胞编辑阶段，从生成的图像中随机选择正常细胞，然后使用所提出的异常细胞合成器将其转换为不同类型的异常细胞。正常图像生成器和异常细胞合成器均基于预先训练的稳定扩散，通过参数有效的微调方法分别定制细胞病理学图像内容和扩展空间布局可控性。我们的实验证明了所提出的合成框架的合成图像质量、多样性和可控性，并验证了其数据增强在增强异常宫颈细胞检测性能方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14707v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **QIS : Interactive Segmentation via Quasi-Conformal Mappings**<br />
**Title_cn:** QIS：通过准共形映射进行交互式分割<br />
**Authors:** Han Zhang, Daoping Zhang, Lok Ming Lui<br />
**Abstract:** <details><summary>原文: </summary>Image segmentation plays a crucial role in extracting important objects of interest from images, enabling various applications. While existing methods have shown success in segmenting clean images, they often struggle to produce accurate segmentation results when dealing with degraded images, such as those containing noise or occlusions. To address this challenge, interactive segmentation has emerged as a promising approach, allowing users to provide meaningful input to guide the segmentation process. However, an important problem in interactive segmentation lies in determining how to incorporate minimal yet meaningful user guidance into the segmentation model. In this paper, we propose the quasi-conformal interactive segmentation (QIS) model, which incorporates user input in the form of positive and negative clicks. Users mark a few pixels belonging to the object region as positive clicks, indicating that the segmentation model should include a region around these clicks. Conversely, negative clicks are provided on pixels belonging to the background, instructing the model to exclude the region near these clicks from the segmentation mask. Additionally, the segmentation mask is obtained by deforming a template mask with the same topology as the object of interest using an orientation-preserving quasiconformal mapping. This approach helps to avoid topological errors in the segmentation results. We provide a thorough analysis of the proposed model, including theoretical support for the ability of QIS to include or exclude regions of interest or disinterest based on the user's indication. To evaluate the performance of QIS, we conduct experiments on synthesized images, medical images, natural images and noisy natural images. The results demonstrate the efficacy of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像分割在从图像中提取重要的感兴趣对象方面发挥着至关重要的作用，从而实现各种应用。虽然现有方法在分割干净图像方面取得了成功，但在处理退化图像（例如包含噪声或遮挡的图像）时，它们通常难以产生准确的分割结果。为了应对这一挑战，交互式分割已成为一种有前景的方法，它允许用户提供有意义的输入来指导分割过程。然而，交互式分割中的一个重要问题在于确定如何将最少但有意义的用户指导合并到分割模型中。在本文中，我们提出了准共形交互式分割（QIS）模型，该模型以正面和负面点击的形式纳入用户输入。用户将属于对象区域的一些像素标记为正点击，表明分割模型应包括这些点击周围的区域。相反，对属于背景的像素提供负点击，指示模型从分割掩模中排除这些点击附近的区域。此外，分割掩模是通过使用保持方向的准共形映射对具有与感兴趣对象相同的拓扑的模板掩模进行变形来获得的。这种方法有助于避免分割结果中的拓扑错误。我们对所提出的模型进行了全面分析，包括 QIS 根据用户指示包含或排除感兴趣或不感兴趣区域的能力的理论支持。为了评估 QIS 的性能，我们对合成图像、医学图像、自然图像和噪声自然图像进行了实验。结果证明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14695v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Quadruplet Loss For Improving the Robustness to Face Morphing Attacks**<br />
**Title_cn:** 四联体损失提高面对变形攻击的鲁棒性<br />
**Authors:** Iurii Medvedev, Nuno Gonçalves<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in deep learning have revolutionized technology and security measures, necessitating robust identification methods. Biometric approaches, leveraging personalized characteristics, offer a promising solution. However, Face Recognition Systems are vulnerable to sophisticated attacks, notably face morphing techniques, enabling the creation of fraudulent documents. In this study, we introduce a novel quadruplet loss function for increasing the robustness of face recognition systems against morphing attacks. Our approach involves specific sampling of face image quadruplets, combined with face morphs, for network training. Experimental results demonstrate the efficiency of our strategy in improving the robustness of face recognition networks against morphing attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习的最新进展彻底改变了技术和安全措施，需要强大的识别方法。利用个性化特征的生物识别方法提供了一个有前途的解决方案。然而，人脸识别系统很容易受到复杂的攻击，特别是人脸变形技术，从而可以创建欺诈性文档。在本研究中，我们引入了一种新颖的四元组损失函数，用于提高人脸识别系统针对变形攻击的鲁棒性。我们的方法涉及对面部图像四联体进行特定采样，并结合面部变形来进行网络训练。实验结果证明了我们的策略在提高人脸识别网络抵御变形攻击的鲁棒性方面的效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.14665v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation**<br />
**Title_cn:** 克服医学图像分割自监督对比学习中的维度崩溃<br />
**Authors:** Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised learning (SSL) approaches have achieved great success when the amount of labeled data is limited. Within SSL, models learn robust feature representations by solving pretext tasks. One such pretext task is contrastive learning, which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them. In this work, we investigate the application of contrastive learning to the domain of medical image analysis. Our findings reveal that MoCo v2, a state-of-the-art contrastive learning method, encounters dimensional collapse when applied to medical images. This is attributed to the high degree of inter-image similarity shared between the medical images. To address this, we propose two key contributions: local feature learning and feature decorrelation. Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features. Our experimental findings demonstrate that our contributions significantly enhance the model's performance in the downstream task of medical segmentation, both in the linear evaluation and full fine-tuning settings. This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>当标记数据量有限时，自监督学习（SSL）方法取得了巨大成功。在 SSL 中，模型通过解决借口任务来学习强大的特征表示。其中一个借口任务是对比学习，它涉及形成相似和不相似的输入样本对，指导模型区分它们。在这项工作中，我们研究了对比学习在医学图像分析领域的应用。我们的研究结果表明，MoCo v2（一种最先进的对比学习方法）在应用于医学图像时会遇到维度崩溃。这归因于医学图像之间共享的高度图像间相似性。为了解决这个问题，我们提出了两个关键贡献：局部特征学习和特征去相关。局部特征学习提高了模型关注图像局部区域的能力，而特征去相关则消除了特征之间的线性相关性。我们的实验结果表明，我们的贡献显着提高了模型在医学分割下游任务中的性能，无论是在线性评估还是完全微调设置中。这项工作说明了有效地使 SSL 技术适应医学成像任务特征的重要性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14611v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **High-Speed Detector For Low-Powered Devices In Aerial Grasping**<br />
**Title_cn:** 适用于空中抓取低功耗设备的高速探测器<br />
**Authors:** Ashish Kumar, Laxmidhar Behera<br />
**Abstract:** <details><summary>原文: </summary>Autonomous aerial harvesting is a highly complex problem because it requires numerous interdisciplinary algorithms to be executed on mini low-powered computing devices. Object detection is one such algorithm that is compute-hungry. In this context, we make the following contributions: (i) Fast Fruit Detector (FFD), a resource-efficient, single-stage, and postprocessing-free object detector based on our novel latent object representation (LOR) module, query assignment, and prediction strategy. FFD achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded device while co-existing with other time-critical sub-systems such as control, grasping, SLAM, a major achievement of this work. (ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time. (iii) an open-source fruit detection dataset having plenty of very small-sized instances that are difficult to detect. Our exhaustive evaluations on our and MinneApple dataset show that FFD, being only a single-scale detector, is more accurate than many representative detectors, e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale Faster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and multi-scale YOLO-v8 by 0.3 while being considerably faster.</details>
**Abstract_cn:** <details><summary>译文: </summary>自主空中收割是一个非常复杂的问题，因为它需要在微型低功率计算设备上执行大量跨学科算法。对象检测就是这样一种需要大量计算的算法。在这方面，我们做出了以下贡献：（i）快速水果检测器（FFD），一种资源高效、单阶段且无需后处理的对象检测器，基于我们新颖的潜在对象表示（LOR）模块、查询分配、和预测策略。 FFD在最新的10W NVIDIA Jetson-NX嵌入式设备上实现了100FPS@FP32精度，同时与控制、抓取、SLAM等其他时间关键的子系统共存，是这项工作的重大成就。 （ii）一种生成大量训练数据的方法，无需对水果图像进行详尽的手动标记，因为它们由大量实例组成，这增加了标记成本和时间。 (iii) 一个开源水果检测数据集，其中包含大量难以检测的非常小的实例。我们对我们和 MinneApple 数据集的详尽评估表明，FFD 只是一个单尺度检测器，比许多代表性检测器（例如FFD 比单尺度 Faster-RCNN 好 10.7AP，比多尺度 Faster-RCNN 好 2.3AP，比最新的单尺度 YOLO-v8 好 8AP，比多尺度 YOLO-v8 好 0.3，同时速度更快。</details>
**PDF:** <http://arxiv.org/pdf/2402.14591v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Deep vessel segmentation based on a new combination of vesselness filters**<br />
**Title_cn:** 基于血管过滤器新组合的深层血管分割<br />
**Authors:** Guillaume Garret, Antoine Vacavant, Carole Frindel<br />
**Abstract:** <details><summary>原文: </summary>Vascular segmentation represents a crucial clinical task, yet its automation remains challenging. Because of the recent strides in deep learning, vesselness filters, which can significantly aid the learning process, have been overlooked. This study introduces an innovative filter fusion method crafted to amplify the effectiveness of vessel segmentation models. Our investigation seeks to establish the merits of a filter-based learning approach through a comparative analysis. Specifically, we contrast the performance of a U-Net model trained on CT images with an identical U-Net configuration trained on vesselness hyper-volumes using matching parameters. Our findings, based on two vascular datasets, highlight improved segmentations, especially for small vessels, when the model's learning is exposed to vessel-enhanced inputs.</details>
**Abstract_cn:** <details><summary>译文: </summary>血管分割是一项至关重要的临床任务，但其自动化仍然具有挑战性。由于最近深度学习的进步，可以显着帮助学习过程的血管过滤器被忽视了。这项研究引入了一种创新的滤波器融合方法，旨在增强血管分割模型的有效性。我们的调查旨在通过比较分析来确定基于过滤器的学习方法的优点。具体来说，我们将在 CT 图像上训练的 U-Net 模型的性能与使用匹配参数在血管超体积上训练的相同 U-Net 配置进行比较。我们的研究结果基于两个血管数据集，突出显示了当模型的学习暴露于血管增强输入时，分割得到了改进，特别是对于小血管。</details>
**PDF:** <http://arxiv.org/pdf/2402.14509v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition**<br />
**Title_cn:** 实现视觉地点识别预训练模型的无缝适应<br />
**Authors:** Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan<br />
**Abstract:** <details><summary>原文: </summary>Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at https://github.com/Lu-Feng/SelaVPR.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究表明，在通用视觉学习任务中使用大规模数据预训练的视觉模型可以为各种视觉感知问题提供有用的特征表示。然而，很少有人尝试在视觉位置识别（VPR）中利用预先训练的基础模型。由于模型预训练和VPR任务在训练目标和数据上存在固有的差异，如何弥合差距并充分释放VPR预训练模型的能力仍然是一个需要解决的关键问题。为此，我们提出了一种新方法来实现 VPR 预训练模型的无缝适应。具体来说，为了获得专注于区分地点的显着地标的全局和局部特征，我们设计了一种混合适应方法来有效地实现全局和局部适应，其中仅调整轻量级适配器而不调整预训练模型。此外，为了指导有效的适应，我们提出了相互最近邻局部特征损失，这确保为局部匹配产生适当的密集局部特征，并避免重新排序时耗时的空间验证。实验结果表明，我们的方法在训练数据和训练时间更少的情况下优于最先进的方法，并且仅使用基于 RANSAC 空间验证的两阶段 VPR 方法的 3% 检索运行时间。它在 MSLS 挑战排行榜上排名第一（提交时）。代码发布于https://github.com/Lu-Feng/SelaVPR。</details>
**PDF:** <http://arxiv.org/pdf/2402.14505v1><br />
**Code:** <https://github.com/Lu-Feng/SelaVPR>**<br />
>>**index:** 10<br />
**Title:** **Reimagining Anomalies: What If Anomalies Were Normal?**<br />
**Title_cn:** 重新想象异常：如果异常是正常的怎么办？<br />
**Authors:** Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, Marius Kloft<br />
**Abstract:** <details><summary>原文: </summary>Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore "what-if scenarios." Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习的方法在图像异常检测方面取得了突破，但其复杂性给理解为什么实例被预测为异常带来了相当大的挑战。我们引入了一种新颖的解释方法，该方法可以为每个异常生成多个反事实示例，捕获不同的异常概念。反事实示例是对被异常检测器视为正常的异常进行的修改。该方法提供了触发异常检测器的机制的高级语义解释，允许用户探索“假设场景”。对各种图像数据集的定性和定量分析表明，应用于最先进的异常检测器的方法可以实现检测器的高质量语义解释。</details>
**PDF:** <http://arxiv.org/pdf/2402.14469v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR**<br />
**Title_cn:** S^2Former-OR：用于 OR 中场景图生成的单级双峰变压器<br />
**Authors:** Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng<br />
**Abstract:** <details><summary>原文: </summary>Scene graph generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR). However, previous works have primarily relied on the multi-stage learning that generates semantic scene graphs dependent on intermediate processes with pose estimation and object detection, which may compromise model efficiency and efficacy, also impose extra annotation burden. In this study, we introduce a novel single-stage bimodal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner. Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction. Concurrently, a Geometry-Visual Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point cloud features. Moreover, based on the augmented feature, we propose a novel relation-sensitive transformer decoder that embeds dynamic entity-pair queries and relational trait priors, which enables the direct prediction of entity-pair relations for graph generation without intermediate steps. Extensive experiments have validated the superior SGG performance and lower computational cost of S^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3% Precision increase and 24.2M reduction in model parameters. We further compared our method with generic single-stage SGG methods with broader metrics for a comprehensive evaluation, with consistently better performance achieved. The code will be made available.</details>
**Abstract_cn:** <details><summary>译文: </summary>外科手术的场景图生成 (SGG) 对于增强手术室 (OR) 的整体认知智能至关重要。然而，以前的工作主要依赖于多阶段学习，生成依赖于姿态估计和对象检测的中间过程的语义场景图，这可能会损害模型的效率和功效，还会带来额外的注释负担。在这项研究中，我们为 OR 中的 SGG 引入了一种新颖的单级双模态 Transformer 框架，称为 S^2Former-OR，旨在以端到端的方式互补地利用 SGG 的多视图 2D 场景和 3D 点云。具体来说，我们的模型采用视图同步传输方案来鼓励多视图视觉信息交互。同时，几何-视觉凝聚操作旨在将协同 2D 语义特征集成到 3D 点云特征中。此外，基于增强的特征，我们提出了一种新颖的关系敏感转换器解码器，它嵌入了动态实体对查询和关系特征先验，这使得能够直接预测实体对关系以进行图生成，而无需中间步骤。大量实验验证了 S^2Former-OR 在 4D-OR 基准上与当前 OR-SGG 方法相比具有优越的 SGG 性能和更低的计算成本，例如，精度提高了 3%，模型参数减少了 24.2M。我们进一步将我们的方法与具有更广泛指标的通用单阶段 SGG 方法进行比较，以进行综合评估，并始终取得更好的性能。该代码将可供使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.14461v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks**<br />
**Title_cn:** 使用自适应图卷积网络建模 3D 婴儿动力学<br />
**Authors:** Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos<br />
**Abstract:** <details><summary>原文: </summary>Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.</details>
**Abstract_cn:** <details><summary>译文: </summary>婴儿神经发育评估的可靠方法对于及早发现可能需要及时干预的医疗问题至关重要。自发运动活动或“动力学”被证明可以为即将到来的神经发育提供强有力的替代测量。然而，其评估总体上是定性的和主观的，侧重于视觉识别的、特定年龄的手势。在这里，我们采用另一种方法，根据对个体运动模式的数据驱动评估来预测婴儿的神经发育成熟度。我们利用经过姿势估计处理的婴儿 3D 视频记录来提取解剖标志的时空序列，并应用自适应图卷积网络来预测实际年龄。我们表明，我们的数据驱动方法比基于手动设计特征的传统机器学习基线取得了改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.14400v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Semantic Image Synthesis with Unconditional Generator**<br />
**Title_cn:** 使用无条件生成器进行语义图像合成<br />
**Authors:** Jungwoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, Youngjung Uh<br />
**Abstract:** <details><summary>原文: </summary>Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks. Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models. Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks. The proxy masks are prepared from the feature maps of random samples in the generator by simple clustering. The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples. Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks. Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo. Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义图像合成（SIS）旨在生成与给定语义掩模匹配的逼真图像。尽管最近的进展允许高质量的结果和精确的空间控制，但它们需要大量的语义分割数据集来训练模型。相反，我们建议采用预先训练的无条件生成器，并根据代理掩码重新排列其特征图。代理掩码是通过简单的聚类从生成器中随机样本的特征图准备的。特征重新排列器学习重新排列原始特征图以匹配来自原始样本本身或来自随机样本的代理掩码的形状。然后我们引入一个语义映射器，它根据各种输入条件（包括语义掩码）生成代理掩码。我们的方法适用于各种应用，例如真实图像的自由形式空间编辑、草图到照片，甚至涂鸦到照片。实验验证了我们的方法在一系列数据集上的优势：人脸、动物面孔和建筑物。</details>
**PDF:** <http://arxiv.org/pdf/2402.14395v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Reading Relevant Feature from Global Representation Memory for Visual Object Tracking**<br />
**Title_cn:** 从全局表示存储器中读取相关特征以进行视觉对象跟踪<br />
**Authors:** Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, Wenqiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.</details>
**Abstract_cn:** <details><summary>译文: </summary>模板或历史帧的参考特征对于视觉对象跟踪至关重要。先前的作品利用固定模板或内存中的所有功能来进行视觉对象跟踪。然而，由于视频的动态特性，不同搜索区域在不同时间步所需的参考历史信息也不一致。因此，使用模板和内存中的所有特征可能会导致冗余并损害跟踪性能。为了缓解这个问题，我们提出了一种新颖的跟踪范式，由相关性注意机制和全局表示记忆组成，它可以自适应地帮助搜索区域从参考特征中选择最相关的历史信息。具体来说，这项工作中提出的相关性注意机制与以前的方法不同，它可以通过全局访问跨帧信息来动态选择和构建当前帧的最佳全局表示记忆。而且，它可以灵活地从构建的记忆中读取相关历史信息，减少冗余，抵消有害信息的负面影响。大量实验验证了所提出方法的有效性，在五个具有挑战性的数据集上以 71 FPS 实现了具有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14392v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints**<br />
**Title_cn:** GAM-Depth：利用梯度感知掩模和语义约束的自监督室内深度估计<br />
**Authors:** Anqi Cheng, Zhiyuan Yang, Haiyue Zhu, Kezhi Mao<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.</details>
**Abstract_cn:** <details><summary>译文: </summary>自监督深度估计已经发展成为一种图像重建任务，可以最大限度地减少光度损失。虽然最近的方法在室内深度估计方面取得了长足的进步，但它们经常在无纹理区域产生不一致的深度估计，并且在对象边界处产生令人不满意的深度差异。为了解决这些问题，在这项工作中，我们提出了 GAM-Depth，它是基于两个新颖的组件开发的：梯度感知掩模和语义约束。梯度感知掩模通过根据梯度大小分配权重，能够对关键区域和无纹理区域进行自适应和鲁棒的监督。室内自监督深度估计的语义约束的结合改善了对象边界处的深度差异，利用协同优化网络以及从预训练分割模型派生的代理语义标签。对 NYUv2、ScanNet 和 InteriorNet 等三个室内数据集的实验研究表明，GAM-Depth 优于现有方法并实现了最先进的性能，这标志着室内深度估计向前迈出了有意义的一步。我们的代码将在 https://github.com/AnqiCheng1234/GAM-Depth 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.14354v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Subobject-level Image Tokenization**<br />
**Title_cn:** 子对象级图像标记化<br />
**Authors:** Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale Fung<br />
**Abstract:** <details><summary>原文: </summary>Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models will be open-sourced at https://github.com/ChenDelong1999/subobjects.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 Transformer 的视觉模型通常将图像标记为固定大小的方形块作为输入单元，缺乏对图像内容的适应性，并且忽略了固有的像素分组结构。受语言模型中广泛采用的子词标记化的启发，我们提出了子对象级别的图像标记器，其中子对象由分割模型（例如，分割任何模型）获得的语义上有意义的图像片段表示。为了实现基于子对象标记化的学习系统，我们首先引入了序列到序列自动编码器（SeqAE），将不同大小和形状的子对象片段压缩为紧凑的嵌入向量，然后将子对象嵌入输入到视觉语言的大型语言模型中学习。实证结果表明，与传统的补丁级标记化相比，我们的子对象级标记化显着促进了将图像转换为对象和属性描述的有效学习。代码和模型将在 https://github.com/ChenDelong1999/subobjects 开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.14327v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5**<br />
**Title_cn:** YOLO-TLA：基于YOLOv5的高效轻量小物体检测模型<br />
**Authors:** Peng Gao, Chun-Lin Ji, Tao Yu, Ru-Yue Yuan<br />
**Abstract:** <details><summary>原文: </summary>Object detection, a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small objects. In this paper, we propose YOLO-TLA, an advanced object detection model building on YOLOv5. We first introduce an additional detection layer for small objects in the neck network pyramid architecture, thereby producing a feature map of a larger scale to discern finer features of small objects. Further, we integrate the C3CrossCovn module into the backbone network. This module uses sliding window feature extraction, which effectively minimizes both computational demand and the number of parameters, rendering the model more compact. Additionally, we have incorporated a global attention mechanism into the backbone network. This mechanism combines the channel information with global information to create a weighted feature map. This feature map is tailored to highlight the attributes of the object of interest, while effectively ignoring irrelevant details. In comparison to the baseline YOLOv5s model, our newly developed YOLO-TLA model has shown considerable improvements on the MS COCO validation dataset, with increases of 4.6% in mAP@0.5 and 4% in mAP@0.5:0.95, all while keeping the model size compact at 9.49M parameters. Further extending these improvements to the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in mAP@0.5 and mAP@0.5:0.95, respectively, with a total of 27.53M parameters. These results validate the YOLO-TLA model's efficient and effective performance in small object detection, achieving high accuracy with fewer parameters and computational demands.</details>
**Abstract_cn:** <details><summary>译文: </summary>目标检测是计算机视觉的一个重要方面，在准确性和鲁棒性方面取得了显着进步。尽管取得了这些进步，实际应用仍然面临着显着的挑战，主要是小物体检测不准确或漏检。在本文中，我们提出了 YOLO-TLA，这是一种基于 YOLOv5 的高级目标检测模型。我们首先在颈部网络金字塔架构中引入一个针对小物体的附加检测层，从而产生更大尺度的特征图来辨别小物体的更精细特征。此外，我们将 C3CrossCovn 模块集成到主干网络中。该模块采用滑动窗口特征提取，有效减少了计算需求和参数数量，使模型更加紧凑。此外，我们还在主干网络中纳入了全局注意力机制。该机制将通道信息与全局信息相结合以创建加权特征图。该特征图经过定制，可以突出显示感兴趣对象的属性，同时有效地忽略不相关的细节。与基线 YOLOv5s 模型相比，我们新开发的 YOLO-TLA 模型在 MS COCO 验证数据集上显示出相当大的改进，mAP@0.5 增加了 4.6%，mAP@0.5:0.95 增加了 4%，同时保持模型不变尺寸紧凑，参数为 9.49M。将这些改进进一步扩展到YOLOv5m模型，增强版本的mAP@0.5和mAP@0.5:0.95分别增加了1.7%和1.9%，总共有27.53M参数。这些结果验证了YOLO-TLA模型在小物体检测方面高效且有效的性能，以更少的参数和计算需求实现了高精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.14309v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Secure Navigation using Landmark-based Localization in a GPS-denied Environment**<br />
**Title_cn:** 在 GPS 拒绝的环境中使用基于地标的定位进行安全导航<br />
**Authors:** Ganesh Sapkota, Sanjay Madria<br />
**Abstract:** <details><summary>原文: </summary>In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability. Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops. Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology. Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive. This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield. Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmarks and pre-defined hazard maps. It performs point inclusion tests on the convex hull of the trajectory segments to ensure the safety and survivability of a moving entity and determines the next point forward decisions. We present a simulated battlefield scenario for two different approaches (with EKF and without EKF) that guide a moving entity through an obstacle and hazard-free path. Using the proposed method, we observed a percent error of 6.51% lengthwise in safe trajectory estimation with an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error (FDE) of 3.27m. The results demonstrate that our approach not only ensures the safety of the mobile units by keeping them within the secure trajectory but also enhances operational effectiveness by adapting to the evolving threat landscape.</details>
**Abstract_cn:** <details><summary>译文: </summary>在现代战场场景中，依赖 GPS 进行导航可能是一个严重的漏洞。对手经常采用策略来拒绝或欺骗 GPS 信号，因此需要采用替代方法来对机动部队进行定位和导航。 DV-HOP 等无范围定位方法依赖于基于无线电的锚点及其平均跳跃距离，这在动态和稀疏网络拓扑中会受到准确性和稳定性的影响。 SLAM 和视觉里程计等基于视觉的方法使用传感器融合技术来生成地图和姿态估计，这些技术更加复杂且计算成本较高。本文提出了一种新颖的框架，它将基于地标的定位（LanBLoc）与扩展卡尔曼滤波器（EKF）相结合，以预测战场上移动实体的未来状态。我们的框架通过考虑可识别的地标和预定义的危险地图，利用部队控制中心生成的安全轨迹信息。它对轨迹段的凸包进行点包含测试，以确保移动实体的安全性和生存性，并确定下一个点的前进决策。我们提出了两种不同方法（使用 EKF 和不使用 EKF）的模拟战场场景，引导移动实体穿过障碍物和无危险的路径。使用所提出的方法，我们观察到安全轨迹估计的纵向百分比误差为 6.51%，平均位移误差 (ADE) 为 2.97m，最终位移误差 (FDE) 为 3.27m。结果表明，我们的方法不仅通过将移动单元保持在安全轨迹内来确保移动单元的安全，而且还通过适应不断变化的威胁形势来提高操作效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.14280v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets**<br />
**Title_cn:** 自监督压力图人体关键点检测方法：优化跨数据集的泛化和计算效率<br />
**Authors:** Chengzhang Yu, Xianjun Yang, Wenxia Bao, Shaonan Wang, Zhiming Yao<br />
**Abstract:** <details><summary>原文: </summary>In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96\%$ in FLOPs and $1.11\%$ in parameter count compared to the baseline methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>在 RGB 图像不足的环境中，压力图是一种可行的替代方案，引起了学术界的关注。本研究引入了一种新颖的自监督压力图关键点检测（SPMKD）方法，解决了当前从压力图中提取人体关键点的专门设计中的差距。我们贡献的核心是编码器-融合器-解码器（EFD）模型，它是一个强大的框架，集成了用于精确人体关键点检测的轻量级编码器、用于高效梯度传播的融合器以及将人体关键点转换为重建压力图的解码器。这种结构通过分类到回归权重转移（CRWT）方法进一步增强，该方法通过初始分类任务训练来微调准确性。这项创新不仅在无需手动注释的情况下增强了人类关键点泛化能力，而且还展示了卓越的效率和泛化能力，与基线方法相比，FLOP 次数减少至仅 $5.96\%$，参数数量减少至 $1.11\%$。</details>
**PDF:** <http://arxiv.org/pdf/2402.14241v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer**<br />
**Title_cn:** 使用修补频谱图变压器的压缩鲁棒合成语音检测<br />
**Authors:** Amit Kumar Singh Yadav, Ziyue Xiang, Kratika Bhagtani, Paolo Bestagini, Stefano Tubaro, Edward J. Delp<br />
**Abstract:** <details><summary>原文: </summary>Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT generalizes well than several existing methods on detecting synthetic speech from an out-of-distribution dataset. We also evaluate robustness of PS3DT to detect telephone quality synthetic speech and synthetic speech shared on social platforms (compressed speech). PS3DT is robust to compression and can detect telephone quality synthetic speech better than several existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多深度学习合成语音生成工具都很容易获得。合成语音的使用导致了金融欺诈、冒充他人和错误信息的传播。因此，人们提出了可以检测合成语音的取证方法。现有方法通常在一个数据集上过度拟合，并且在实际场景中（例如检测社交平台上共享的合成语音），其性能大幅下降。在本文中，我们提出了补丁频谱图合成语音检测变压器（PS3DT），这是一种合成语音检测器，可将时域语音信号转换为梅尔频谱图，并使用变压器神经网络对其进行补丁处理。我们评估了 PS3DT 在 ASVspoof2019 数据集上的检测性能。我们的实验表明，与使用频谱图进行合成语音检测的其他方法相比，PS3DT 在 ASVspoof2019 数据集上表现良好。我们还研究了 PS3DT 在 In-the-Wild 数据集上的泛化性能。 PS3DT 比几种现有的从分布外数据集中检测合成语音的方法具有更好的泛化能力。我们还评估了 PS3DT 的鲁棒性，以检测电话质量合成语音和社交平台上共享的合成语音（压缩语音）。 PS3DT 对压缩具有鲁棒性，并且可以比几种现有方法更好地检测电话质量合成语音。</details>
**PDF:** <http://arxiv.org/pdf/2402.14205v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention**<br />
**Title_cn:** 提示：具有掩模感知编码和增强注意力的高质量 INPainting Transformer<br />
**Authors:** Shuang Chen, Amir Atapour-Abarghouei, Hubert P. H. Shum<br />
**Abstract:** <details><summary>原文: </summary>Existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in speech recognition, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的图像修复方法利用基于卷积的下采样方法来减少空间维度。这可能会导致损坏图像中的信息丢失，其中可用信息本质上是稀疏的，特别是对于大面积缺失区域的情况。 Transformer 内自注意力机制的最新进展导致了包括修复在内的许多计算机视觉任务的显着改进。然而，受计算成本的限制，现有方法无法充分利用此类模型的远程建模能力的功效。在本文中，我们提出了一种端到端的高质量 INpainting Transformer，缩写为 HINT，它由一种新颖的掩模感知像素洗牌下采样模块（MPD）组成，用于保留从损坏图像中提取的可见信息，同时保持可用于模型内进行高级推论的信息的完整性。此外，我们提出了空间激活通道注意层（SCAL），这是一种有效的自注意机制，解释空间意识以在多个尺度上对损坏的图像进行建模。为了进一步提高 SCAL 的有效性，受语音识别领域最新进展的推动，我们引入了一种三明治结构，将前馈网络放置在 SCAL 模块之前和之后。我们在 CelebA、CelebA-HQ、Places2 和 Dunhuang 四个数据集上展示了 HINT 与当代最先进模型相比的卓越性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14185v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis**<br />
**Title_cn:** Snap Video：用于文本到视频合成的缩放时空转换器<br />
**Authors:** Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods. See our website at https://snap-research.github.io/snapvideo/.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于生成图像的当代模型显示出卓越的质量和多功能性。受这些优势的影响，研究界将它们重新用于生成视频。由于视频内容高度冗余，我们认为天真地将图像模型的进步引入视频生成领域会降低运动保真度、视觉质量并损害可扩展性。在这项工作中，我们构建了 Snap Video，这是一种视频优先的模型，可以系统地解决这些挑战。为此，我们首先扩展 EDM 框架以考虑空间和时间冗余像素并自然支持视频生成。其次，我们展示了 U-Net（图像生成背后的主力）在生成视频时扩展性很差，需要大量的计算开销。因此，我们提出了一种新的基于 Transformer 的架构，其训练速度比 U-Net 快 3.31 倍（推理速度快约 4.5 倍）。这使我们能够首次有效地训练具有数十亿个参数的文本到视频模型，在许多基准上达到最先进的结果，并生成具有更高质量、时间一致性和运动的视频复杂。用户研究表明，我们的模型比最新的方法更受青睐。请参阅我们的网站 https://snap-research.github.io/snapvideo/。</details>
**PDF:** <http://arxiv.org/pdf/2402.14797v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot**<br />
**Title_cn:** Multi-HMR：单次多人全身人体网格恢复<br />
**Authors:** Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez, Thomas Lucas<br />
**Abstract:** <details><summary>原文: </summary>We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image. Predictions encompass the whole body, i.e, including hands and facial expressions, using the SMPL-X parametric model and spatial location in the camera coordinate system. Our model detects people by predicting coarse 2D heatmaps of person centers, using features produced by a standard Vision Transformer (ViT) backbone. It then predicts their whole-body pose, shape and spatial location using a new cross-attention module called the Human Prediction Head (HPH), with one query per detected center token, attending to the entire set of features. As direct prediction of SMPL-X parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames of Full-Body Subjects dataset, containing humans close to the camera with diverse hand poses. We show that incorporating this dataset into training further enhances predictions, particularly for hands, enabling us to achieve state-of-the-art performance. Multi-HMR also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token. This simple design achieves strong performance on whole-body and body-only benchmarks simultaneously. We train models with various backbone sizes and input resolutions. In particular, using a ViT-S backbone and $448\times448$ input images already yields a fast and competitive model with respect to state-of-the-art methods, while considering larger models and higher resolutions further improve performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出了 Multi-HMR，这是一种强大的单次模型，用于从单个 RGB 图像恢复多人 3D 人体网格。使用 SMPL-X 参数模型和相机坐标系中的空间位置，预测涵盖整个身体，即包括手和面部表情。我们的模型通过使用标准 Vision Transformer (ViT) 主干生成的特征来预测人员中心的粗略 2D 热图来检测人员。然后，它使用称为人类预测头 (HPH) 的新交叉注意力模块来预测他们的全身姿势、形状和空间位置，每个检测到的中心标记有一个查询，关注整组特征。由于直接预测 SMPL-X 参数会产生次优结果，因此我们引入了 CUFFS；全身主体特写镜头数据集，包含靠近相机且具有不同手势的人类。我们表明，将该数据集纳入训练中可以进一步增强预测，特别是对于手部的预测，使我们能够实现最先进的性能。如果可用，Multi-HMR 还可以通过对每个图像标记的相机光线方向进行编码来考虑相机内在特性。这种简单的设计同时在全身和仅身体基准测试中实现了强大的性能。我们训练具有各种主干尺寸和输入分辨率的模型。特别是，使用 ViT-S 主干和 448\times448$ 输入图像已经产生了相对于最先进的方法而言快速且有竞争力的模型，同时考虑更大的模型和更高分辨率进一步提高性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.14654v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation**<br />
**Title_cn:** CCPA：通过对比服装和姿势增强进行长期人员重新识别<br />
**Authors:** Vuong D. Nguyen, Shishir K. Shah<br />
**Abstract:** <details><summary>原文: </summary>Long-term Person Re-Identification (LRe-ID) aims at matching an individual across cameras after a long period of time, presenting variations in clothing, pose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and Pose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body shape information which is cloth-invariant using a Relation Graph Attention Network. Training a robust LRe-ID model requires a wide range of clothing variations and expensive cloth labeling, which is lacked in current LRe-ID datasets. To address this, we perform clothing and pose transfer across identities to generate images of more clothing variations and of different persons wearing similar clothing. The augmented batch of images serve as inputs to our proposed Fine-grained Contrastive Losses, which not only supervise the Re-ID model to learn discriminative person embeddings under long-term scenarios but also ensure in-distribution data generation. Results on LRe-ID datasets demonstrate the effectiveness of our CCPA framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>长期人员重新识别（LRe-ID）旨在在很长一段时间后通过摄像机匹配个人，呈现服装、姿势和观点的变化。在这项工作中，我们提出了 CCPA：LRe-ID 的对比服装和姿势增强框架。除了外观之外，CCPA 使用关系图注意网络捕获衣服不变的身体形状信息。训练强大的 LRe-ID 模型需要广泛的服装变化和昂贵的布料标签，而当前的 LRe-ID 数据集中缺乏这些。为了解决这个问题，我们跨身份进行服装和姿势转移，以生成更多服装变化以及穿着相似服装的不同人的图像。增强后的图像批次作为我们提出的细粒度对比损失的输入，它不仅监督 Re-ID 模型以学习长期场景下的有区别的人物嵌入，而且还确保分布数据生成。 LRe-ID 数据集的结果证明了我们的 CCPA 框架的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.14454v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Learning to Kern -- Set-wise Estimation of Optimal Letter Space**<br />
**Title_cn:** 学习紧缩——最佳字母空间的集合估计<br />
**Authors:** Kei Nakatsuru, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>Kerning is the task of setting appropriate horizontal spaces for all possible letter pairs of a certain font. One of the difficulties of kerning is that the appropriate space differs for each letter pair. Therefore, for a total of 52 capital and small letters, we need to adjust $52 \times 52 = 2704$ different spaces. Another difficulty is that there is neither a general procedure nor criterion for automatic kerning; therefore, kerning is still done manually or with heuristics. In this paper, we tackle kerning by proposing two machine-learning models, called pairwise and set-wise models. The former is a simple deep neural network that estimates the letter space for two given letter images. In contrast, the latter is a Transformer-based model and estimates the letter spaces for three or more given letter images. For example, the set-wise model simultaneously estimates 2704 spaces for 52 letter images for a certain font. Among the two models, the set-wise model is not only more efficient but also more accurate because its internal self-attention mechanism allows for more consistent kerning for all letters. Experimental results on about 2500 Google fonts and their quantitative and qualitative analyses show that the set-wise model has an average estimation error of only about 5.3 pixels when the average letter space of all fonts and letter pairs is about 115 pixels.</details>
**Abstract_cn:** <details><summary>译文: </summary>字距调整是为某种字体的所有可能的字母对设置适当的水平间距的任务。字距调整的困难之一是每个字母对的适当间距不同。因此，总共52个大小写字母，我们需要调整$52 \times 52 = 2704$不同的空格。另一个困难是自动字距调整既没有通用的程序也没有标准。因此，字距调整仍然是手动或启发式完成的。在本文中，我们通过提出两种机器学习模型来解决字距调整问题，称为成对模型和集合模型。前者是一个简单的深度神经网络，用于估计两个给定字母图像的字母空间。相比之下，后者是基于 Transformer 的模型，可估计三个或更多给定字母图像的字母空间。例如，set-wise 模型同时估计某种字体的 52 个字母图像的 2704 个空格。在这两个模型中，set-wise 模型不仅更高效，而且更准确，因为其内部的自注意力机制允许所有字母的字距调整更加一致。对约2500种谷歌字体的实验结果及其定量和定性分析表明，当所有字体和字母对的平均字母间距约为115像素时，set-wise模型的平均估计误差仅为约5.3像素。</details>
**PDF:** <http://arxiv.org/pdf/2402.14313v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving**<br />
**Title_cn:** 用于自动驾驶中边缘视频压缩和元宇宙集成的分布式辐射场<br />
**Authors:** Eugen Šlapak, Matúš Dopiriak, Mohammad Abdullah Al Faruque, Juraj Gazda, Marco Levorato<br />
**Abstract:** <details><summary>原文: </summary>The metaverse is a virtual space that combines physical and digital elements, creating immersive and connected digital worlds. For autonomous mobility, it enables new possibilities with edge computing and digital twins (DTs) that offer virtual prototyping, prediction, and more. DTs can be created with 3D scene reconstruction methods that capture the real world's geometry, appearance, and dynamics. However, sending data for real-time DT updates in the metaverse, such as camera images and videos from connected autonomous vehicles (CAVs) to edge servers, can increase network congestion, costs, and latency, affecting metaverse services. Herein, a new method is proposed based on distributed radiance fields (RFs), multi-access edge computing (MEC) network for video compression and metaverse DT updates. RF-based encoder and decoder are used to create and restore representations of camera images. The method is evaluated on a dataset of camera images from the CARLA simulator. Data savings of up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFs instead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) qualitative metrics for the reconstructed images. Possible uses and challenges for the metaverse and autonomous mobility are also discussed.</details>
**Abstract_cn:** <details><summary>译文: </summary>虚拟宇宙是一个结合了物理和数字元素的虚拟空间，创造了沉浸式、互联的数字世界。对于自主移动性，它通过边缘计算和提供虚拟原型、预测等功能的数字孪生 (DT) 实现了新的可能性。 DT 可以使用 3D 场景重建方法来创建，以捕获现实世界的几何形状、外观和动态。然而，在元宇宙中发送实时 DT 更新数据（例如从联网自动驾驶车辆 (CAV) 到边缘服务器的摄像头图像和视频）可能会增加网络拥塞、成本和延迟，从而影响元宇宙服务。在此，提出了一种基于分布式辐射场（RF）、多接入边缘计算（MEC）网络的视频压缩和元节DT更新的新方法。基于 RF 的编码器和解码器用于创建和恢复相机图像的表示。该方法在来自 CARLA 模拟器的相机图像数据集上进行评估。通过使用 RF 代替 I 帧，H.264 I 帧 - P 帧对可节省高达 80% 的数据，同时保持高峰值信噪比 (PSNR) 和结构相似性指数测量 (SSIM) ）重建图像的定性指标。还讨论了虚拟宇宙和自主移动性的可能用途和挑战。</details>
**PDF:** <http://arxiv.org/pdf/2402.14642v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Place Anything into Any Video**<br />
**Title_cn:** 将任何内容放入任何视频中<br />
**Authors:** Ziling Liu, Jinyu Yang, Mingqi Gao, Feng Zheng<br />
**Abstract:** <details><summary>原文: </summary>Controllable video editing has demonstrated remarkable potential across diverse applications, particularly in scenarios where capturing or re-capturing real-world videos is either impractical or costly. This paper introduces a novel and efficient system named Place-Anything, which facilitates the insertion of any object into any video solely based on a picture or text description of the target object or element. The system comprises three modules: 3D generation, video reconstruction, and 3D target insertion. This integrated approach offers an efficient and effective solution for producing and editing high-quality videos by seamlessly inserting realistic objects. Through a user study, we demonstrate that our system can effortlessly place any object into any video using just a photograph of the object. Our demo video can be found at https://youtu.be/afXqgLLRnTE. Please also visit our project page https://place-anything.github.io to get access.</details>
**Abstract_cn:** <details><summary>译文: </summary>可控视频编辑在各种应用中展示了巨大的潜力，特别是在捕获或重新捕获现实世界视频不切实际或成本高昂的情况下。本文介绍了一种名为 Place-Anything 的新颖而高效的系统，该系统可以仅根据目标对象或元素的图片或文本描述将任何对象插入到任何视频中。该系统包括三个模块：3D生成、视频重建和3D目标插入。这种集成方法通过无缝插入逼真的对象，为制作和编辑高质量视频提供了高效且有效的解决方案。通过用户研究，我们证明我们的系统只需使用对象的照片即可轻松地将任何对象放入任何视频中。我们的演示视频可以在 https://youtu.be/afXqgLLRnTE 找到。另请访问我们的项目页面 https://place-anything.github.io 进行访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.14316v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding**<br />
**Title_cn:** Swin3D++：用于 3D 室内场景理解的有效多源预训练<br />
**Authors:** Yu-Qi Yang, Yu-Xiao Guo, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision. However, 3D vision domain suffers from the lack of 3D data, and simply combining multiple 3D datasets for pretraining a 3D backbone does not yield significant improvement, due to the domain discrepancies among different 3D datasets that impede effective feature learning. In this work, we identify the main sources of the domain discrepancies between 3D indoor scene datasets, and propose Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on multi-source 3D point clouds. Swin3D++ introduces domain-specific mechanisms to Swin3D's modules to address domain discrepancies and enhance the network capability on multi-source pretraining. Moreover, we devise a simple source-augmentation strategy to increase the pretraining data scale and facilitate supervised pretraining. We validate the effectiveness of our design, and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining methods on typical indoor scene understanding tasks. Our code and models will be released at https://github.com/microsoft/Swin3D</details>
**Abstract_cn:** <details><summary>译文: </summary>数据多样性和丰富性对于提高自然语言处理和 2D 视觉模型的性能和泛化至关重要。然而，3D 视觉领域缺乏 3D 数据，并且简单地组合多个 3D 数据集来预训练 3D 主干网并不能产生显着的改进，因为不同 3D 数据集之间的领域差异阻碍了有效的特征学习。在这项工作中，我们确定了 3D 室内场景数据集之间域差异的主要来源，并提出了 Swin3D++，这是一种基于 Swin3D 的增强架构，用于对多源 3D 点云进行有效的预训练。 Swin3D++ 在 Swin3D 的模块中引入了特定领域的机制，以解决领域差异并增强网络在多源预训练上的能力。此外，我们设计了一种简单的源增强策略来增加预训练数据规模并促进监督预训练。我们验证了设计的有效性，并证明 Swin3D++ 在典型的室内场景理解任务上超越了最先进的 3D 预训练方法。我们的代码和模型将在 https://github.com/microsoft/Swin3D 发布</details>
**PDF:** <http://arxiv.org/pdf/2402.14215v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Self-supervised Visualisation of Medical Image Datasets**<br />
**Title_cn:** 医学图像数据集的自监督可视化<br />
**Authors:** Ifeoma Veronica Nwabufo, Jan Niklas Böhm, Philipp Berens, Dmitry Kobak<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning. A recent self-supervised learning method, $t$-SimCNE, uses contrastive learning to directly train a 2D representation suitable for visualisation. When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters. In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy. We found that increasing the set of data augmentations to include arbitrary rotations improved the results in terms of class separability, compared to data augmentations used for natural images. Our 2D representations show medically relevant structures and can be used to aid data exploration and annotation, improving on common approaches for data visualisation.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于数据增强的自监督学习方法（例如 SimCLR、BYOL 或 DINO）允许获得图像数据集的语义有意义的表示，并且在监督微调之前得到广泛使用。最近的一种自监督学习方法 $t$-SimCNE 使用对比学习来直接训练适合可视化的 2D 表示。当应用于自然图像数据集时，$t$-SimCNE 产生具有语义有意义的簇的 2D 可视化。在这项工作中，我们使用 $t$-SimCNE 来可视化医学图像数据集，包括来自皮肤病学、组织学和血液显微镜的示例。我们发现，与用于自然图像的数据增强相比，增加数据增强集以包括任意旋转可以改善类可分离性方面的结果。我们的 2D 表示显示医学相关结构，可用于帮助数据探索和注释，改进数据可视化的常见方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.14566v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion**<br />
**Title_cn:** CLCE：一种改进交叉熵和对比学习以实现优化学习融合的方法<br />
**Authors:** Zijun Long, George Killick, Lipeng Zhuang, Gerardo Aragon-Camarasa, Zaiqiao Meng, Richard Mccreadie<br />
**Abstract:** <details><summary>原文: </summary>State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks, achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in transfer learning settings with the BEiT-3 model. Importantly, our proposed CLCE approach effectively mitigates the dependency of contrastive learning on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of contrastive learning in budget-limited hardware environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>最先进的预训练图像模型主要采用两阶段方法：在大规模数据集上进行初始无监督预训练，然后使用交叉熵损失（CE）进行特定于任务的微调。然而，事实证明，CE 会损害模型的泛化性和稳定性。虽然最近采用对比学习的工作通过提高嵌入的质量和产生更好的决策边界来解决其中一些限制，但它们经常忽视硬负挖掘的重要性，并依赖于使用大样本批次的资源密集型和缓慢的训练。为了解决这些问题，我们引入了一种名为 CLCE 的新颖方法，它将标签感知对比学习与 CE 集成在一起。我们的方法不仅保持了两种损失函数的优势，而且还以协同的方式利用硬负挖掘来提高性能。实验结果表明，CLCE 在 12 个基准测试中的 Top-1 准确率方面显着优于 CE，在使用 BEiT-3 模型的少样本学习场景中实现了高达 3.52% 的增益，在迁移学习设置中实现了 3.41% 的增益。重要的是，我们提出的 CLCE 方法有效地减轻了对比学习对大批量大小（例如每批 4096 个样本）的依赖，这一限制此前限制了对比学习在预算有限的硬件环境中的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.14551v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging**<br />
**Title_cn:** 医学影像中专家级视觉语言基础模型的人口统计学偏差<br />
**Authors:** Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel<br />
**Abstract:** <details><summary>原文: </summary>Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients. Such demographic biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information. Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能 (AI) 的进步已在医学成像应用中实现了专家级的性能。值得注意的是，自我监督的视觉语言基础模型可以检测广泛的病理，而无需依赖显式的训练注释。然而，至关重要的是要确保这些人工智能模型不会反映或放大人类偏见，从而使女性或黑人患者等历史上被边缘化的群体处于不利地位。这种偏见的表现可能会系统性地延迟某些患者亚群的基本医疗护理。在这项研究中，我们跨五个全球来源的数据集调查了胸部 X 射线诊断中最先进的视觉语言基础模型的算法公平性。我们的研究结果表明，与经过委员会认证的放射科医生相比，这些基础模型始终对边缘群体诊断不足，在交叉亚组（例如黑人女性患者）中的诊断率甚至更高。这种人口统计偏差存在于广泛的病理学和人口统计属性中。对模型嵌入的进一步分析揭示了其对人口统计信息的重要编码。在医学成像中部署具有这些偏见的人工智能系统可能会加剧现有的护理差异，对公平的医疗服务构成潜在挑战，并引发有关其临床应用的道德问题。</details>
**PDF:** <http://arxiv.org/pdf/2402.14815v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion**<br />
**Title_cn:** GeneOH Diffusion：通过去噪扩散实现可推广的手-物体交互去噪<br />
**Authors:** Xueyi Liu, Li Yi<br />
**Abstract:** <details><summary>原文: </summary>In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a "denoising via diffusion" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们解决了手与物体交互（HOI）去噪这一具有挑战性的问题。给定错误的交互序列，目标是细化不正确的手部轨迹，以消除交互伪影，以获得感知上真实的序列。这一挑战涉及复杂的交互噪声，包括不自然的手部姿势和不正确的手部-物体关系，以及对新交互和不同噪声模式进行稳健泛化的必要性。我们通过一种新颖的方法 GeneOH Diffusion 来应对这些挑战，该方法结合了两个关键设计：名为 GeneOH 的创新的以接触为中心的 HOI 表示和新的域通用去噪方案。以联系人为中心的表示 GeneOH 以信息方式参数化 HOI 过程，促进跨各种 HOI 场景的增强泛化。新的去噪方案包括一个规范的去噪模型，该模型经过训练，可以将噪声数据样本从白化噪声空间投影到干净的数据流形，以及“通过扩散去噪”策略，该策略可以通过首先将它们扩散到与通过规范降噪器进行白化噪声空间和​​清洁。对四个具有显着域变化的基准进行的广泛实验证明了我们方法的卓越有效性。 GeneOH Diffusion 还显示出各种下游应用的前景。项目网站：https://meowuu7.github.io/GeneOH-Diffusion/。</details>
**PDF:** <http://arxiv.org/pdf/2402.14810v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation**<br />
**Title_cn:** Cyber​​Demo：增强模拟人体演示以实现现实世界的灵巧操作<br />
**Authors:** Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang<br />
**Abstract:** <details><summary>原文: </summary>We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出 Cyber​​Demo，这是一种机器人模仿学习的新颖方法，它利用模拟人类演示来完成现实世界的任务。通过在模拟环境中结合广泛的数据增强，Cyber​​Demo 在转移到现实世界时优于传统的域内现实世界演示，可以处理不同的物理和视觉条件。无论数据收集的经济性和便利性如何，Cyber​​Demo 在各种任务的成功率方面都优于基准方法，并且对以前未见过的对象表现出普遍性。例如，它可以旋转新颖的四阀和五阀，尽管人类演示只涉及三阀。我们的研究证明了模拟人体演示对于现实世界灵巧操作任务的巨大潜力。更多详细信息可以在 https://cyber-demo.github.io 找到</details>
**PDF:** <http://arxiv.org/pdf/2402.14795v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams**<br />
**Title_cn:** 一类用于快速比较持久图的拓扑伪距离<br />
**Authors:** Rolando Kindelan Nuñez, Mircea Petrache, Mauricio Cerda, Nancy Hitschfeld<br />
**Abstract:** <details><summary>原文: </summary>Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal). In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced and classical Wasserstein distances at the high-complexity extreme, while being computationally lighter and close to Persistence Statistics at the lower complexity extreme, and thus allow users to interpolate between the two metrics. We build theoretical comparisons to show how to fit our new distances at an intermediate level between persistence vectorizations and Wasserstein distances. We also experimentally verify that ETDs outperform PSs in terms of accuracy and outperform Wasserstein and Sliced Wasserstein distances in terms of computational complexity.</details>
**Abstract_cn:** <details><summary>译文: </summary>持久性图 (PD) 在拓扑数据分析中发挥着核心作用，并且用于越来越多的应用程序中。 PD 数据的比较需要计算大量 PD 之间的比较指标，这些指标准确、理论上合理且计算速度快。特别是对于更密集的多维PD，缺乏这样的比较指标。一方面，Wasserstein 型距离具有较高的精度和理论保证，但其计算成本较高。另一方面，诸如持久性统计（PS）之类的向量化之间的距离具有较低的计算成本，但缺乏准确性保证，并且通常它们不能保证区分PD（即不同PD的两个PS向量可能相等）。在这项工作中，我们引入了一类称为扩展拓扑伪距离（ETD）的伪距离，它具有可调的复杂度，并且可以在高复杂度极限下近似切片和经典 Wasserstein 距离，同时在较低复杂度下计算量更轻且接近持久性统计复杂性极端，因此允许用户在两​​个指标之间进行插值。我们建立了理论比较，以展示如何在持久性矢量化和 Wasserstein 距离之间的中间水平上拟合我们的新距离。我们还通过实验验证 ETD 在准确性方面优于 PS，在计算复杂性方面优于 Wasserstein 和切片 Wasserstein 距离。</details>
**PDF:** <http://arxiv.org/pdf/2402.14489v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision Tuning**<br />
**Title_cn:** VLPose：通过语言视觉调整弥合姿势估计中的领域差距<br />
**Authors:** Jingyao Li, Pengguang Chen, Xuan Ju, Hong Xu, Jiaya Jia<br />
**Abstract:** <details><summary>原文: </summary>Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios. However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality. With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient. Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies. Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose. VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains. Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies.</details>
**Abstract_cn:** <details><summary>译文: </summary>得益于深度学习技术的进步，人体姿势估计（HPE）在自然场景中取得了重大进展。然而，由于领域差距，这些模型在绘画、雕塑等人工场景中表现不佳，制约了虚拟现实和增强现实的发展。随着模型规模的增长，在自然数据和人工数据上重新训练整个模型的计算成本昂贵且效率低下。我们的研究旨在通过有效的调整策略来弥合自然场景和人工场景之间的领域差距。利用语言模型的潜力，我们通过称为 VLPose 的新颖框架增强了传统姿势估计模型在不同场景中的适应性。 VLPose 利用语言和视觉之间的协同作用，将姿态估计模型的泛化性和鲁棒性扩展到传统领域之外。与最先进的调整策略相比，我们的方法在 HumanArt 和 MSCOCO 上分别表现出 2.26% 和 3.74% 的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.14456v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation**<br />
**Title_cn:** HR-APR：具有不确定性估计和相机重定位分层细化的 APR 不可知框架<br />
**Authors:** Changkun Liu, Shuai Chen, Yukun Zhao, Huajian Huang, Victor Prisacariu, Tristan Braud<br />
**Abstract:** <details><summary>原文: </summary>Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4\% and 15.2\% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.</details>
**Abstract_cn:** <details><summary>译文: </summary>绝对姿势回归器（APR）直接从单目图像估计相机姿势，但对于不同的查询，其准确性不稳定。不确定性感知 APR 提供了估计姿态的不确定性信息，减轻了这些不可靠预测的影响。然而，现有的不确定性建模技术通常与特定的 APR 架构相结合，导致与最先进的 (SOTA) APR 方法相比性能欠佳。这项工作引入了一种新颖的 APR 不可知框架 HR-APR，它将不确定性估计公式化为查询和数据库特征之间的余弦相似度估计。它不依赖也不影响APR网络架构，灵活且计算高效。此外，我们利用姿态细化的不确定性来增强 APR 的性能。大量实验证明了我们框架的有效性，在 7Scenes 和 Cambridge Landmarks 数据集上减少了 27.4% 和 15.2% 的计算开销，同时保持单图像 APR 的 SOTA 精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.14371v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **An Error-Matching Exclusion Method for Accelerating Visual SLAM**<br />
**Title_cn:** 一种加速视觉SLAM的错误匹配排除方法<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Jiaxing Ma, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang<br />
**Abstract:** <details><summary>原文: </summary>In Visual SLAM, achieving accurate feature matching consumes a significant amount of time, severely impacting the real-time performance of the system. This paper proposes an accelerated method for Visual SLAM by integrating GMS (Grid-based Motion Statistics) with RANSAC (Random Sample Consensus) for the removal of mismatched features. The approach first utilizes the GMS algorithm to estimate the quantity of matched pairs within the neighborhood and ranks the matches based on their confidence. Subsequently, the Random Sample Consensus (RANSAC) algorithm is employed to further eliminate mismatched features. To address the time-consuming issue of randomly selecting all matched pairs, this method transforms it into the problem of prioritizing sample selection from high-confidence matches. This enables the iterative solution of the optimal model. Experimental results demonstrate that the proposed method achieves a comparable accuracy to the original GMS-RANSAC while reducing the average runtime by 24.13% on the KITTI, TUM desk, and TUM doll datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>在视觉SLAM中，实现精确的特征匹配需要消耗大量时间，严重影响系统的实时性能。本文提出了一种视觉 SLAM 的加速方法，通过将 GMS（基于网格的运动统计）与 RANSAC（随机样本共识）相结合来去除不匹配的特征。该方法首先利用 GMS 算法来估计邻域内匹配对的数量，并根据其置信度对匹配进行排名。随后，采用随机样本一致性（RANSAC）算法进一步消除不匹配的特征。为了解决随机选择所有匹配对的耗时问题，该方法将其转化为从高置信度匹配中优先选择样本的问题。这使得能够迭代求解最优模型。实验结果表明，该方法在 KITTI、TUM desk 和 TUM doll 数据集上实现了与原始 GMS-RANSAC 相当的精度，同时将平均运行时间降低了 24.13%。</details>
**PDF:** <http://arxiv.org/pdf/2402.14345v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Vision-Language Navigation with Embodied Intelligence: A Survey**<br />
**Title_cn:** 具身智能的视觉语言导航：一项调查<br />
**Authors:** Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan<br />
**Abstract:** <details><summary>原文: </summary>As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.</details>
**Abstract_cn:** <details><summary>译文: </summary>作为人工智能领域的长期愿景，体现智能的核心目标是提高智能体与环境的感知、理解和交互能力。视觉语言导航（VLN）作为实现具身智能的关键研究路径，重点探索智能体如何利用自然语言与人类有效沟通，接收和理解指令，并最终依靠视觉信息实现精确导航。 VLN 集成了人工智能、自然语言处理、计算机视觉和机器人技术。该领域面临技术挑战，但显示出人机交互等应用潜力。然而，由于从语言理解到动作执行涉及复杂的过程，VLN面临着视觉信息和语言指令的对齐、提高泛化能力等诸多挑战。本综述系统回顾了VLN的研究进展，并详细阐述了具身智能的VLN的研究方向。我们详细总结了其系统架构以及基于方法和常用基准数据集的研究，全面分析了当前研究面临的问题和挑战，并探讨了该领域未来的发展方向，旨在为研究人员提供实际参考。</details>
**PDF:** <http://arxiv.org/pdf/2402.14304v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **A Landmark-Aware Visual Navigation Dataset**<br />
**Title_cn:** 地标感知视觉导航数据集<br />
**Authors:** Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok<br />
**Abstract:** <details><summary>原文: </summary>Map representation learned by expert demonstrations has shown promising research value. However, recent advancements in the visual navigation field face challenges due to the lack of human datasets in the real world for efficient supervised representation learning of the environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building. We collect RGB observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过专家演示学习的地图表示显示出有希望的研究价值。然而，由于现实世界中缺乏有效的环境监督表示学习的人类数据集，视觉导航领域的最新进展面临着挑战。我们提出了一个地标感知视觉导航（LAVN）数据集，以允许对以人为中心的探索策略和地图构建进行监督学习。当人类注释者探索虚拟和现实世界环境时，我们收集 RGB 观察和人类点击对，以实现对空间的全覆盖探索。人类注释者还沿着每个轨迹提供不同的地标示例，我们直觉地认为这将简化地图或图形构建和定位的任务。当学习在环境中探索时，这些人类点击可以作为路径点预测的直接监督。我们的数据集涵盖了广泛的场景，包括室内环境中的房间以及室外的走道。数据集可从 DOI 获取：10.5281/zenodo.10608067。</details>
**PDF:** <http://arxiv.org/pdf/2402.14281v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training**<br />
**Title_cn:** 通过基于知识的自我训练进行基于重建的异常定位<br />
**Authors:** Cheng Qian, Xiaoxian Lao, Chunguang Li<br />
**Abstract:** <details><summary>原文: </summary>Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training. Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples. Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used. We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>异常定位涉及定位图像内的异常区域，是一项重要的工业任务。基于重构的方法因其低复杂性和高可解释性而被广泛用于异常定位。大多数现有的基于重建的方法仅使用正常样本来构建模型。如果在异常定位过程中适当利用异常样本，可以提高定位性能。然而，通常只有弱标记的异常样本可用，这限制了改进。很多时候，我们可以获得领域专家总结的一些异常知识。利用这些知识可以帮助我们更好地利用异常样本，从而进一步提高定位性能。在本文中，我们提出了一种新颖的基于重建的方法，称为知识知情自训练（KIST），该方法通过自训练将知识集成到重建模型中。具体来说，KIST 除了正常样本之外还利用弱标记的异常样本，并利用知识来生成异常样本的像素级伪标签。基于伪标签，使用了一种新颖的损失，它促进正常像素的重建，同时抑制异常像素的重建。我们在不同的数据集上进行了实验，并证明了 KIST 相对于现有基于重建的方法的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.14246v1><br />
**Code:** null<br />

