## [UPDATED!] **2024-02-23** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **A Study of Shape Modeling Against Noise**<br />
**Title_cn:** 抗噪声形状建模研究<br />
**Authors:** Cheng Long, Adrian Barbu<br />
**Abstract:** <details><summary>原文: </summary>Shape modeling is a challenging task with many potential applications in computer vision and medical imaging. There are many shape modeling methods in the literature, each with its advantages and applications. However, many shape modeling methods have difficulties handling shapes that have missing pieces or outliers. In this regard, this paper introduces shape denoising, a fundamental problem in shape modeling that lies at the core of many computer vision and medical imaging applications and has not received enough attention in the literature. The paper introduces six types of noise that can be used to perturb shapes as well as an objective measure for the noise level and for comparing methods on their shape denoising capabilities. Finally, the paper evaluates seven methods capable of accomplishing this task, of which six are based on deep learning, including some generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>形状建模是一项具有挑战性的任务，在计算机视觉和医学成像中有许多潜在的应用。文献中有许多形状建模方法，每种方法都有其优点和应用。然而，许多形状建模方法难以处理缺少部分或异常值的形状。在这方面，本文介绍了形状去噪，这是形状建模中的一个基本问题，是许多计算机视觉和医学成像应用的核心，但在文献中尚未得到足够的重视。本文介绍了可用于扰乱形状的六种类型的噪声，以及噪声水平的客观测量方法以及比较其形状去噪能力的方法。最后，论文评估了七种能够完成这项任务的方法，其中六种是基于深度学习的，包括一些生成模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.15587v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition**<br />
**Title_cn:** Gen4Gen：用于生成多概念组合的生成数据管道<br />
**Authors:** Chun-Hsiao Yeh, Ta-Ying Cheng, He-Yen Hsieh, Chuan-En Lin, Yi Ma, Andrew Markham, Niki Trigoni, H. T. Kung, Yubei Chen<br />
**Abstract:** <details><summary>原文: </summary>Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions. Using this, we create a dataset called MyCanvas, that can be used to benchmark the task of multi-concept personalization. In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized text-to-image diffusion methods. We provide a simple baseline built on top of Custom Diffusion with empirical prompting strategies for future researchers to evaluate on MyCanvas. We show that by improving data quality and prompting strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的文本到图像扩散模型能够学习和合成包含新颖的个性化概念（例如，他们自己的宠物或特定物品）的图像，只需几个训练示例。本文解决了个性化文本到图像扩散模型领域中的两个相互关联的问题。首先，当前的个性化技术无法可靠地扩展到多个概念——我们假设这是由于预训练数据集中的复杂场景和简单文本描述之间的不匹配（例如，LAION）。其次，对于包含多个个性化概念的图像，缺乏一个整体指标来评估性能，不仅评估个性化概念的相似程度，还评估图像中是否存在所有概念以及图像是否准确反映整体文本描述。为了解决这些问题，我们引入了 Gen4Gen，这是一种半自动数据集创建管道，利用生成模型将个性化概念与文本描述一起组合成复杂的组合。使用它，我们创建了一个名为 MyCanvas 的数据集，可用于对多概念个性化任务进行基准测试。此外，我们设计了一个包含两个分数（CP-CLIP 和 TI-CLIP）的综合指标，以更好地量化多概念、个性化文本到图像扩散方法的性能。我们提供了一个建立在自定义扩散之上的简单基线，并提供了经验提示策略，供未来研究人员在 MyCanvas 上进行评估。我们表明，通过提高数据质量和提示策略，我们可以显着提高多概念个性化图像生成质量，而不需要对模型架构或训练算法进行任何修改。</details>
**PDF:** <http://arxiv.org/pdf/2402.15504v1><br />
**Code:** <https://github.com/louisYen/Gen4Gen>**<br />
>>**index:** 3<br />
**Title:** **ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation**<br />
**Title_cn:** ProTIP：文本到图像扩散模型对抗随机扰动的概率鲁棒性验证<br />
**Authors:** Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao<br />
**Abstract:** <details><summary>原文: </summary>Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the "just-right" number of stochastic perturbations whenever the verification target is met. Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we demonstrate an application of ProTIP to rank commonly used defence methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像 (T2I) 扩散模型 (DM) 在基于简单文本描述生成高质量图像方面表现出了令人印象深刻的能力。然而，正如许多深度学习 (DL) 模型所常见的那样，DM 缺乏鲁棒性。虽然有人尝试将 T2I DM 的稳健性作为二元问题或最坏情况问题进行评估，但他们无法回答只要找到对抗性示例 (AE)，模型的总体稳健性如何。在本研究中，我们首先引入 T2I DM 鲁棒性的概率概念；然后建立一个有效的框架ProTIP，用统计保证对其进行评估。主要挑战来自：i）生成过程的高计算成本； ii) 确定受扰动的输入是否是 AE 涉及比较两个输出分布，这从根本上来说比其他深度学习任务更难，例如分类，其中通过错误预测标签来识别 AE。为了应对这些挑战，我们在统计测试中采用有效且无用的早期停止规则的序贯分析来识别 AE，并在满足验证目标时采用自适应浓度不等式来动态确定“恰到好处”的随机扰动数量。实证实验验证了 ProTIP 相对于常见 T2I DM 的有效性和效率。最后，我们演示了 ProTIP 对常用防御方法进行排名的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.15429v1><br />
**Code:** <https://github.com/wellzline/protip>**<br />
>>**index:** 4<br />
**Title:** **On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey**<br />
**Title_cn:** 关于监督和无监督去噪方法的归一化等方差性质：一项调查<br />
**Authors:** Sébastien Herbreteau, Charles Kervrann<br />
**Abstract:** <details><summary>原文: </summary>Image denoising is probably the oldest and still one of the most active research topic in image processing. Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of convolutional neural networks and supervised deep learning. In this paper, we propose a survey of guided tour of supervised and unsupervised learning methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in supervised learning. It is conceived as a tutorial organizing in a comprehensive framework current approaches. We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them. Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed with most of supervised methods. It is of paramount importance that intensity shifting or scaling applied to the input image results in a corresponding change in the denoiser output.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像去噪可能是图像处理中最古老且仍然是最活跃的研究课题之一。许多方法论概念在过去几十年中被引入，并且近年来显着提高了性能，特别是随着卷积神经网络和监督深度学习的出现。在本文中，我们提出了对图像去噪的监督和无监督学习方法的引导之旅的调查，对这一演变过程中阐述的主要原理进行了分类，并特别关注监督学习的最新发展。它被认为是在综合框架中组织当前方法的教程。我们深入探讨了文献中最高效方法的基本原理和局限性，并强调了其中许多方法之间的共同特征。最后，我们关注归一化等方差属性，令人惊讶的是，大多数监督方法都无法保证这一属性。至关重要的是，应用于输入图像的强度移动或缩放会导致降噪器输出发生相应的变化。</details>
**PDF:** <http://arxiv.org/pdf/2402.15352v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Label-efficient Multi-organ Segmentation Method with Diffusion Model**<br />
**Title_cn:** 具有扩散模型的标签高效多器官分割方法<br />
**Authors:** Yongzhi Huang, Jinxin Zhu, Haseeb Hassan, Liyilei Su, Jingyu Li, Binding Huang<br />
**Abstract:** <details><summary>原文: </summary>Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. Various supervised-learning approaches have been proposed recently. However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice. In this study, we present a label-efficient learning approach using a pre-trained diffusion model for multi-organ segmentation tasks in CT images. First, a denoising diffusion model was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images. Then the pre-trained denoising diffusion network was transferred to the downstream multi-organ segmentation task, effectively creating a semi-supervised learning model that requires only a small amount of labeled data. Furthermore, linear classification and fine-tuning decoder strategies were employed to enhance the network's segmentation performance. Our generative model at 256x256 resolution achieves impressive performance in terms of Fr\'echet inception distance, spatial Fr\'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1\%, respectively. These results affirm the diffusion model's ability to generate diverse and realistic 2D CT images. Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios. Remarkably, even with only 1\% and 10\% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56\% and 78.51\% after fine-tuning, respectively. The method achieves a DSC score of 51.81\% using just four labeled CT scans. These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning heavily reliant on large-scale labeled data.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机断层扫描（CT）图像中多个器官的准确分割在计算机辅助诊断系统中起着至关重要的作用。最近提出了各种监督学习方法。然而，这些方法严重依赖于大量高质量的标记数据，而在实践中获得这些数据的成本很高。在这项研究中，我们提出了一种标签高效的学习方法，使用预先训练的扩散模型来执行 CT 图像中的多器官分割任务。首先，使用未标记的 CT 数据训练去噪扩散模型，生成额外的二维 (2D) CT 图像。然后将预训练的去噪扩散网络转移到下游的多器官分割任务中，有效地创建了仅需要少量标记数据的半监督学习模型。此外，采用线性分类和微调解码器策略来增强网络的分割性能。我们的生成模型在 256x256 分辨率下在 Fr\'echet 起始距离、空间 Fr\'echet 起始距离和 F1 分数方面取得了令人印象深刻的性能，值分别为 11.32、46.93 和 73.1%。这些结果证实了扩散模型生成多样化且真实的 2D CT 图像的能力。此外，与 FLARE 2022 数据集上最先进的方法相比，我们的方法实现了有竞争力的多器官分割性能，特别是在有限的标记数据场景中。值得注意的是，即使只有 1\% 和 10\% 的标记数据，我们的方法在微调后也分别实现了 71.56\% 和 78.51\% 的 Dice 相似系数 (DSC)。该方法仅使用四次标记的 CT 扫描即可获得 51.81% 的 DSC 分数。这些结果证明了我们的方法在克服严重依赖大规模标记数据的监督学习的局限性方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15216v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Modified CycleGAN for the synthesization of samples for wheat head segmentation**<br />
**Title_cn:** 修改后的 CycleGAN 用于合成小麦头部分割的样本<br />
**Authors:** Jaden Myers, Keyhan Najafian, Farhad Maleki, Katie Ovens<br />
**Abstract:** <details><summary>原文: </summary>Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through supervised learning approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a generative adversarial network (GAN) to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated synthetic dataset for wheat head segmentation. This dataset was then used to develop a deep-learning model for semantic segmentation. The resulting model achieved a Dice score of 83.4\% on an internal dataset and Dice scores of 79.6% and 83.6% on two external Global Wheat Head Detection datasets. While we proposed this approach in the context of wheat head segmentation, it can be generalized to other crop types or, more broadly, to images with dense, repeated patterns such as those found in cellular imagery.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习模型已用于各种图像处理任务。然而，这些模型大多数是通过监督学习方法开发的，这在很大程度上依赖于大规模注释数据集的可用性。开发此类数据集既乏味又昂贵。在没有带注释的数据集的情况下，可以使用合成数据进行模型开发；然而，由于模拟数据和真实数据之间存在巨大差异（一种称为域差距的现象），所得模型在应用于真实数据时通常表现不佳。在这项研究中，我们的目标是通过首先计算模拟大规模带注释的数据集，然后使用生成对抗网络（GAN）来填补模拟图像和真实图像之间的差距来应对这一挑战。这种方法产生了一个合成数据集，可以有效地用于训练深度学习模型。使用这种方法，我们开发了一个用于小麦头部分割的真实注释合成数据集。然后使用该数据集开发用于语义分割的深度学习模型。生成的模型在内部数据集上获得了 83.4% 的 Dice 分数，在两个外部全球小麦头检测数据集上获得了 79.6% 和 83.6% 的 Dice 分数。虽然我们在小麦头部分割的背景下提出了这种方法，但它可以推广到其他作物类型，或更广泛地推广到具有密集、重复模式的图像，例如细胞图像中发现的图像。</details>
**PDF:** <http://arxiv.org/pdf/2402.15135v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation**<br />
**Title_cn:** RoboEXP：通过机器人操作的交互式探索的动作条件场景图<br />
**Authors:** Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li<br />
**Abstract:** <details><summary>原文: </summary>Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities. The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG. We apply our system across various real-world settings in a zero-shot manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before. Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人需要探索周围环境以适应和处理未知环境中的任务。先前的工作提出了构建环境的场景图，但通常假设环境是静态的，忽略需要主动交互的区域。这严重限制了它们在家庭和办公室环境中处理更复杂任务的能力：在摆好桌子之前，机器人必须探索抽屉和橱柜以找到所有器具和调味品。在这项工作中，我们介绍了交互式场景探索的新颖任务，其中机器人自主探索环境并生成捕获底层环境结构的动作条件场景图（ACSG）。 ACSG 既考虑了低级信息（例如几何和语义），也考虑了高级信息（例如场景中不同实体之间的动作条件关系）。为此，我们提出了机器人探索（RoboEXP）系统，该系统结合了大型多模态模型（LMM）和显式内存设计来增强我们系统的功能。机器人推理探索什么以及如何探索物体，通过交互过程积累新信息并逐步构建 ACSG。我们以零样本的方式将我们的系统应用于各种现实世界的设置，展示了其在探索和建模以前从未见过的环境方面的有效性。利用构建的 ACSG，我们展示了 RoboEXP 系统在促进各种现实世界操纵任务方面的有效性和效率，这些任务涉及刚性的铰接物体、嵌套物体（如俄罗斯套娃）和可变形物体（如布料）。</details>
**PDF:** <http://arxiv.org/pdf/2402.15487v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries**<br />
**Title_cn:** Text2Pic Swift：增强大型图书馆的长文本到图像检索<br />
**Authors:** Zijun Long, Xuri Ge, Richard Mccreadie, Joemon Jose<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following this, the Summary-based Re-ranking (SR) stage further refines these selections based on concise query summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder, specifically designed to tackle the challenges of ambiguous queries and to facilitate both stages of the retrieval process, thereby significantly improving computational efficiency via vector-based similarity assessments. Our evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000, alongside reductions in training and retrieval durations by 68.75% and 99.79%, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像检索通过使用文本查询搜索图像，在数字图书馆、电子商务平台和多媒体数据库等各种应用中发挥着至关重要的作用。尽管多模态大型语言模型（MLLM）取得了进步，提供了领先的性能，但它们在大规模、多样化和模糊检索场景中的适用性受到大量计算需求和单射嵌入生成的限制。本文介绍了 Text2Pic Swift 框架，该框架专为高效、稳健地检索与大型数据集中的广泛文本描述相对应的图像而定制。该框架采用两层方法：最初的基于实体的排名 (ER) 阶段通过多查询到多目标策略解决冗长文本查询中固有的歧义，有效缩小后续分析的潜在候选范围。接下来，基于摘要的重新排名 (SR) 阶段根据简洁的查询摘要进一步细化这些选择。此外，我们提出了一种新颖的 Decoupling-BEiT-3 编码器，专门设计用于解决模糊查询的挑战并促进检索过程的两个阶段，从而通过基于向量的相似性评估显着提高计算效率。我们在 AToMiC 数据集上进行的评估表明，Text2Pic Swift 的 Recall@1000 提高了 11.06%，同时训练和检索时间分别缩短了 68.75% 和 99.79%，从而优于当前的 MLLM。</details>
**PDF:** <http://arxiv.org/pdf/2402.15276v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Large Multimodal Agents: A Survey**<br />
**Title_cn:** 大型多式联运代理：调查<br />
**Authors:** Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li<br />
**Abstract:** <details><summary>原文: </summary>Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型（LLM）在支持基于文本的人工智能代理方面取得了卓越的性能，赋予它们类似于人类的决策和推理能力。与此同时，出现了一种新兴的研究趋势，重点是将这些由法学硕士支持的人工智能代理扩展到多模式领域。此扩展使人工智能代理能够解释和响应不同的多模式用户查询，从而处理更复杂和细致的任务。在本文中，我们对 LLM 驱动的多模式代理进行了系统回顾，我们将其称为大型多模式代理（简称 LMA）。首先，我们介绍了开发 LMA 所涉及的基本组成部分，并将当前的研究主体分为四种不同的类型。随后，我们回顾了整合多个 LMA 的协作框架，以提高集体效率。该领域的关键挑战之一是现有研究中使用的评估方法多种多样，阻碍了不同 LMA 之间的有效比较。因此，我们编制了这些评估方法并建立了一个全面的框架来弥补差距。该框架旨在标准化评估，促进更有意义的比较。在总结我们的审查时，我们强调了 LMA 的广泛应用，并提出了未来可能的研究方向。我们的讨论旨在为这个快速发展的领域的未来研究提供有价值的见解和指南。最新资源列表位于 https://github.com/jun0wanan/awesome-large-multimodal-agents。</details>
**PDF:** <http://arxiv.org/pdf/2402.15116v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multimodal Transformer With a Low-Computational-Cost Guarantee**<br />
**Title_cn:** 具有低计算成本保证的多模态变压器<br />
**Authors:** Sungjin Park, Edward Choi<br />
**Abstract:** <details><summary>原文: </summary>Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs but also matches or even outperforms established models.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 Transformer 的模型显着提高了一系列多模态理解任务的性能，例如视觉问答和动作识别。然而，多模态 Transformer 明显受到多头注意力与输入序列长度的二次复杂度的影响，特别是当模态数量增加时。为了解决这个问题，我们引入了低成本多模态变压器（LoCoMT），这是一种新颖的多模态注意力机制，旨在以最小的性能损失降低训练和推理过程中的计算成本。具体来说，通过为每个注意力头分配不同的多模态注意力模式，LoCoMT 可以灵活地控制多模态信号，并在理论上确保与现有多模态 Transformer 变体相比降低计算成本。在两个多模态数据集（即 Audioset 和 MedVidCL）上的实验结果表明，LoCoMT 不仅减少了 GFLOP，而且匹配甚至优于已建立的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.15096v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Distilling Adversarial Robustness Using Heterogeneous Teachers**<br />
**Title_cn:** 使用异质教师提炼对抗鲁棒性<br />
**Authors:** Jieren Deng, Aaron Palmer, Rigel Mahmood, Ethan Rathbun, Jinbo Bi, Kaleel Mahmood, Derek Aguiar<br />
**Abstract:** <details><summary>原文: </summary>Achieving resiliency against adversarial attacks is necessary prior to deploying neural network classifiers in domains where misclassification incurs substantial costs, e.g., self-driving cars or medical imaging. Recent work has demonstrated that robustness can be transferred from an adversarially trained teacher to a student model using knowledge distillation. However, current methods perform distillation using a single adversarial and vanilla teacher and consider homogeneous architectures (i.e., residual networks) that are susceptible to misclassify examples from similar adversarial subspaces. In this work, we develop a defense framework against adversarial attacks by distilling adversarial robustness using heterogeneous teachers (DARHT). In DARHT, the student model explicitly represents teacher logits in a student-teacher feature map and leverages multiple teachers that exhibit low adversarial example transferability (i.e., exhibit high performance on dissimilar adversarial examples). Experiments on classification tasks in both white-box and black-box scenarios demonstrate that DARHT achieves state-of-the-art clean and robust accuracies when compared to competing adversarial training and distillation methods in the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Comparisons with homogeneous and heterogeneous teacher sets suggest that leveraging teachers with low adversarial example transferability increases student model robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>在错误分类会产生巨大成本的领域（例如自动驾驶汽车或医学成像）部署神经网络分类器之前，必须实现对抗性攻击的弹性。最近的工作表明，可以使用知识蒸馏将鲁棒性从经过对抗性训练的教师转移到学生模型。然而，当前的方法使用单个对抗性和普通教师进行蒸馏，并考虑容易对来自类似对抗性子空间的示例进行错误分类的同质架构（即残差网络）。在这项工作中，我们通过使用异构教师（DARHT）提炼对抗性鲁棒性，开发了一个针对对抗性攻击的防御框架。在 DARHT 中，学生模型在学生-教师特征图中明确表示教师逻辑，并利用表现出低对抗性示例可转移性的多个教师（即，在不同的对抗性示例上表现出高性能）。白盒和黑盒场景中的分类任务实验表明，与 CIFAR-10、CIFAR-100 和 Tiny 中的竞争性对抗训练和蒸馏方法相比，DARHT 实现了最先进的干净且稳健的准确度ImageNet 数据集。与同质和异质教师集的比较表明，利用具有低对抗性示例可转移性的教师可以提高学生模型的稳健性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15586v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales**<br />
**Title_cn:** 更大规模的鲁棒且可解释的视觉任务的层次不变性<br />
**Authors:** Shuren Qi, Yushu Zhang, Chao Wang, Zhihua Xia, Jian Weng, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize this theoretical framework into a given task. With the over-completeness, discriminative features w.r.t. the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We demonstrate the above arguments with accuracy, invariance, and efficiency results on texture, digit, and parasite classification experiments. Furthermore, at the application level, our representations are explored in real-world forensics tasks on adversarial perturbations and Artificial Intelligence Generated Content (AIGC). Such applications reveal that the proposed strategy not only realizes the theoretically promised invariance, but also exhibits competitive discriminability even in the era of deep learning. For robust and interpretable vision tasks at larger scales, hierarchical invariant representation can be considered as an effective alternative to traditional CNN and invariants.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发强大且可解释的视觉系统是迈向值得信赖的人工智能的关键一步。在这方面，一个有前途的范例考虑在基本图像表示中嵌入任务所需的不变结构，例如几何不变性。然而，这种不变的表示通常表现出有限的可辨别性，限制了它们在大规模可信视觉任务中的应用。对于这个开放问题，我们对层次不变性进行了系统的研究，从理论、实践和应用的角度探讨了这个话题。在理论层面上，我们展示了如何使用类似卷积神经网络（CNN）的分层架构但以完全可解释的方式构建超完备不变量。提供了一般蓝图、具体定义、不变属性和数值实现。在实践层面，我们讨论如何将这个理论框架定制到给定的任务中。由于过度完整性，歧视性特征 w.r.t.该任务可以以类似神经架构搜索（NAS）的方式自适应地形成。我们通过纹理、数字和寄生虫分类实验的准确性、不变性和效率结果证明了上述论点。此外，在应用程序层面，我们的表示在关于对抗性扰动和人工智能生成内容（AIGC）的现实世界取证任务中进行了探索。这些应用表明，所提出的策略不仅实现了理论上承诺的不变性，而且即使在深度学习时代也表现出竞争性的可区分性。对于更大规模的稳健且可解释的视觉任务，分层不变表示可以被视为传统 CNN 和不变量的有效替代方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.15430v1><br />
**Code:** <https://github.com/shurenqi/hir>**<br />
>>**index:** 3<br />
**Title:** **Optimized Deployment of Deep Neural Networks for Visual Pose Estimation on Nano-drones**<br />
**Title_cn:** 用于纳米无人机视觉姿态估计的深度神经网络的优化部署<br />
**Authors:** Matteo Risso, Francesco Daghero, Beatrice Alessandra Motetti, Daniele Jahier Pagliari, Enrico Macii, Massimo Poncino, Alessio Burrello<br />
**Abstract:** <details><summary>原文: </summary>Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining popularity due to their small size, enabling new tasks such as indoor navigation or people monitoring. Nonetheless, their size and simple electronics pose severe challenges in implementing advanced onboard intelligence. This work proposes a new automatic optimization pipeline for visual pose estimation tasks using Deep Neural Networks (DNNs). The pipeline leverages two different Neural Architecture Search (NAS) algorithms to pursue a vast complexity-driven exploration in the DNNs' architectural space. The obtained networks are then deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low power System-on-Chip leveraging a set of novel software kernels for the efficient fused execution of critical DNN layer sequences. Our results improve the state-of-the-art reducing inference latency by up to 3.22x at iso-error.</details>
**Abstract_cn:** <details><summary>译文: </summary>小型自主无人机 (UAV) 由于体积小而越来越受欢迎，可以实现室内导航或人员监控等新任务。尽管如此，它们的尺寸和简单的电子设备对实现先进的机载智能提出了严峻的挑战。这项工作提出了一种使用深度神经网络（DNN）的视觉姿态估计任务的新自动优化流程。该管道利用两种不同的神经架构搜索 (NAS) 算法在 DNN 架构空间中进行大量复杂性驱动的探索。然后将获得的网络部署在配备并行超低功耗片上系统的现成纳米无人机上，利用一组新颖的软件内核来高效融合执行关键的 DNN 层序列。我们的结果改进了最先进的技术，在 iso-error 下将推理延迟减少了高达 3.22 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.15273v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Self-Supervised Pre-Training for Table Structure Recognition Transformer**<br />
**Title_cn:** 表结构识别变压器的自监督预训练<br />
**Authors:** ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau<br />
**Abstract:** <details><summary>原文: </summary>Table structure recognition (TSR) aims to convert tabular images into a machine-readable format. Although hybrid convolutional neural network (CNN)-transformer architecture is widely used in existing approaches, linear projection transformer has outperformed the hybrid architecture in numerous vision tasks due to its simplicity and efficiency. However, existing research has demonstrated that a direct replacement of CNN backbone with linear projection leads to a marked performance drop. In this work, we resolve the issue by proposing a self-supervised pre-training (SSP) method for TSR transformers. We discover that the performance gap between the linear projection transformer and the hybrid CNN-transformer can be mitigated by SSP of the visual encoder in the TSR model. We conducted reproducible ablation studies and open-sourced our code at https://github.com/poloclub/unitable to enhance transparency, inspire innovations, and facilitate fair comparisons in our domain as tables are a promising modality for representation learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>表格结构识别（TSR）旨在将表格图像转换为机器可读的格式。尽管混合卷积神经网络（CNN）-变压器架构在现有方法中广泛使用，但线性投影变压器由于其简单性和效率而在许多视觉任务中优于混合架构。然而，现有研究表明，用线性投影直接替换 CNN 主干会导致性能明显下降。在这项工作中，我们通过提出一种 TSR 变压器的自监督预训练（SSP）方法来解决这个问题。我们发现，线性投影变换器和混合 CNN 变换器之间的性能差距可以通过 TSR 模型中视觉编码器的 SSP 来缩小。我们进行了可重复的消融研究，并在 https://github.com/poloclub/unitable 开源了我们的代码，以提高透明度、激发创新并促进我们领域内的公平比较，因为表格是表示学习的一种有前景的方式。</details>
**PDF:** <http://arxiv.org/pdf/2402.15578v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Closing the AI generalization gap by adjusting for dermatology condition distribution differences across clinical settings**<br />
**Title_cn:** 通过调整临床环境中皮肤病状况分布差异来缩小人工智能泛化差距<br />
**Authors:** Rajeev V. Rikhye, Aaron Loh, Grace Eunhae Hong, Preeti Singh, Margaret Ann Smith, Vijaytha Muralidharan, Doris Wong, Rory Sayres, Michelle Phung, Nicolas Betancourt, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Recently, there has been great progress in the ability of artificial intelligence (AI) algorithms to classify dermatological conditions from clinical photographs. However, little is known about the robustness of these algorithms in real-world settings where several factors can lead to a loss of generalizability. Understanding and overcoming these limitations will permit the development of generalizable AI that can aid in the diagnosis of skin conditions across a variety of clinical settings. In this retrospective study, we demonstrate that differences in skin condition distribution, rather than in demographics or image capture mode are the main source of errors when an AI algorithm is evaluated on data from a previously unseen source. We demonstrate a series of steps to close this generalization gap, requiring progressively more information about the new source, ranging from the condition distribution to training data enriched for data less frequently seen during training. Our results also suggest comparable performance from end-to-end fine tuning versus fine tuning solely the classification layer on top of a frozen embedding model. Our approach can inform the adaptation of AI algorithms to new settings, based on the information and resources available.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，人工智能（AI）算法根据临床照片对皮肤病进行分类的能力取得了巨大进步。然而，人们对这些算法在现实环境中的鲁棒性知之甚少，在现实环境中，有几个因素可能导致普遍性的丧失。了解并克服这些限制将有助于开发通用人工智能，帮助诊断各种临床环境中的皮肤状况。在这项回顾性研究中，我们证明，当根据以前未见过的来源的数据评估人工智能算法时，错误的主要来源是皮肤状况分布的差异，而不是人口统计或图像捕获模式的差异。我们演示了一系列步骤来缩小这种泛化差距，逐渐需要更多有关新源的信息，从条件分布到针对训练期间不常见的数据进行丰富的训练数据。我们的结果还表明，端到端微调与仅微调冻结嵌入模型之上的分类层的性能相当。我们的方法可以根据可用的信息和资源，通知人工智能算法适应新的设置。</details>
**PDF:** <http://arxiv.org/pdf/2402.15566v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep Networks Always Grok and Here is Why**<br />
**Title_cn:** 深度网络总是让人摸不着头脑，原因如下<br />
**Authors:** Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk<br />
**Abstract:** <details><summary>原文: </summary>Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the density of the so-called 'linear regions' (aka, spline partition regions) that tile the DNN input space, and serves as a utile progress measure for training. We provide the first evidence that for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the DNN mapping smoother there) and towards the decision boundary (making the DNN mapping less smooth there). Grokking occurs post phase transition as a robust partition of the input space emerges thanks to the linearization of the DNN mapping around the training points. Website: https://bit.ly/grok-adversarial</details>
**Abstract_cn:** <details><summary>译文: </summary>Grokking（或延迟泛化）是深度神经网络 (DNN) 中的泛化在实现接近零训练误差很久之后才发生的现象。先前的研究报告了在特定受控设置中发生的 grokking，例如使用大范数参数初始化的 DNN 或在算法数据集上训练的变压器。我们证明，grokking 实际上更为广泛，并且在各种实际环境中得以实现，例如在 CIFAR10 上训练卷积神经网络 (CNN) 或在 Imagenette 上训练 Resnet。我们引入了延迟鲁棒性的新概念，即 DNN 深入研究对抗性示例并在插值和/或泛化后很长时间内变得鲁棒。我们基于 DNN 输入输出映射局部复杂性的新度量，对延迟泛化和延迟鲁棒性的出现进行了分析解释。我们的局部复杂性测量所谓的“线性区域”（又名样条分区区域）的密度，这些区域平铺 DNN 输入空间，并作为训练的实用进度度量。我们提供了第一个证据，证明对于分类问题，线性区域在训练期间经历相变，然后它们从训练样本迁移（使那里的 DNN 映射更平滑）并移向决策边界（使那里的 DNN 映射不太平滑）。 Grokking 发生在相变后，由于训练点周围的 DNN 映射的线性化，输入空间的稳健划分出现了。网站：https://bit.ly/grok-adversarial</details>
**PDF:** <http://arxiv.org/pdf/2402.15555v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts**<br />
**Title_cn:** 共同监督学习：通过专家的分层混合提高弱到强的泛化能力<br />
**Authors:** Yuejiang Liu, Alexandre Alahi<br />
**Abstract:** <details><summary>原文: </summary>Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consistency, leveraging their dependencies to reject potential annotation noises. We validate the proposed method through visual recognition tasks on the OpenAI weak-to-strong benchmark and additional multi-domain datasets. Our code is available at \url{https://github.com/yuejiangliu/csl}.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于缺乏称职的监管者，指导在互联网规模数据上预训练的强大模型的行为可能很困难。最近的研究表明，尽管存在监督噪音，但在针对特定目标进行微调时，强大的学生模型可能会超越其较弱的老师。然而，这种从弱到强的概括的有效性仍然有限，特别是在存在巨大能力差距的情况下。在本文中，我们建议通过利用一组多元化的专业教师而不是单一的通才教师来共同监督优秀的学生来应对这一挑战。我们的方法类似于经典的专家分层混合，有两个为共同监督量身定制的组成部分：（i）我们逐步交替学生培训和教师分配，利用优秀学生的成长来确定合理的监督； （ii）我们保守地强制师生和局部-全局一致性，利用它们的依赖性来拒绝潜在的注释噪音。我们通过 OpenAI 弱到强基准和其他多域数据集上的视觉识别任务来验证所提出的方法。我们的代码可以在 \url{https://github.com/yuejianliu/csl} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15505v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks**<br />
**Title_cn:** 视网膜专题图增强卷积神经网络的鲁棒性<br />
**Authors:** Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet<br />
**Abstract:** <details><summary>原文: </summary>Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function. This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep convolutional neural networks (CNNs). Retinotopic mapping was integrated into the inputs of standard off-the-shelf convolutional neural networks (CNNs), which were then retrained on the ImageNet task. As expected, the logarithmic-polar mapping improved the network's ability to handle arbitrary image zooms and rotations, particularly for isolated objects. Surprisingly, the retinotopically mapped network achieved comparable performance in classification. Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted. This replicates a crucial ability of the human visual system that is absent in typical convolutional neural networks (CNNs). These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes.</details>
**Abstract_cn:** <details><summary>译文: </summary>中央凹视觉是包括人类在内的许多动物共有的一个特征，尽管它对生物视觉功能做出了重大贡献，但尚未在机器学习应用中得到充分利用。这项研究调查了视网膜专题图（中央凹视觉的关键组成部分）在集成到深度卷积神经网络（CNN）中时是否可以增强图像分类和定位性能。视网膜主题映射被集成到标准现成卷积神经网络 (CNN) 的输入中，然后在 ImageNet 任务上进行重新训练。正如预期的那样，对数极坐标映射提高了网络处理任意图像缩放和旋转的能力，特别是对于孤立的对象。令人惊讶的是，视网膜局部映射网络在分类方面取得了相当的性能。此外，当变换的中心点移动时，网络表现出改进的分类定位。这复制了人类视觉系统的一项关键能力，而这在典型的卷积神经网络 (CNN) 中是不存在的。这些发现表明，视网膜专题图可能是重要的前注意视觉过程的基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.15480v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Benchmarking the Robustness of Panoptic Segmentation for Automated Driving**<br />
**Title_cn:** 自动驾驶全景分割鲁棒性基准测试<br />
**Authors:** Yiting Wang, Haonan Zhao, Daniel Gummadi, Mehrdad Dianati, Kurt Debattista, Valentina Donzella<br />
**Abstract:** <details><summary>原文: </summary>Precise situational awareness is required for the safe decision-making of assisted and automated driving (AAD) functions. Panoptic segmentation is a promising perception technique to identify and categorise objects, impending hazards, and driveable space at a pixel level. While segmentation quality is generally associated with the quality of the camera data, a comprehensive understanding and modelling of this relationship are paramount for AAD system designers. Motivated by such a need, this work proposes a unifying pipeline to assess the robustness of panoptic segmentation models for AAD, correlating it with traditional image quality. The first step of the proposed pipeline involves generating degraded camera data that reflects real-world noise factors. To this end, 19 noise factors have been identified and implemented with 3 severity levels. Of these factors, this work proposes novel models for unfavourable light and snow. After applying the degradation models, three state-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation networks are used to analyse their robustness. The variations of the segmentation performance are then correlated to 8 selected image quality metrics. This research reveals that: 1) certain specific noise factors produce the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian noise; 2) the ViT-based panoptic segmentation backbones show better robustness to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and CW-SSIM) correlate strongly with panoptic segmentation performance and therefore they can be used as predictive metrics for network performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>辅助和自动驾驶 (AAD) 功能的安全决策需要精确的态势感知。全景分割是一种很有前途的感知技术，可以在像素级别对物体、迫在眉睫的危险和可驾驶空间进行识别和分类。虽然分割质量通常与相机数据的质量相关，但对这种关系的全面理解和建模对于 AAD 系统设计人员来说至关重要。出于这种需求，这项工作提出了一个统一的流程来评估 AAD 全景分割模型的鲁棒性，并将其与传统图像质量相关联。所提出的流程的第一步涉及生成反映现实世界噪声因素的降级相机数据。为此，已确定并实施了 19 个噪声因素，分为 3 个严重级别。在这些因素中，这项工作提出了不利光和雪的新模型。应用退化模型后，使用三个最先进的基于 CNN 和视觉变换器 (ViT) 的全景分割网络来分析其鲁棒性。然后将分割性能的变化与 8 个选定的图像质量指标相关联。本研究表明：1）某些特定噪声因素对全景分割影响最大，即镜头上的水滴和高斯噪声； 2）基于ViT的全景分割主干对所考虑的噪声因素表现出更好的鲁棒性； 3）一些图像质量指标（即LPIPS和CW-SSIM）与全景分割性能密切相关，因此它们可以用作网络性能的预测指标。</details>
**PDF:** <http://arxiv.org/pdf/2402.15469v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Outlier detection by ensembling uncertainty with negative objectness**<br />
**Title_cn:** 通过将不确定性与负客观性结合起来进行异常值检测<br />
**Authors:** Anja Delić, Matej Grcić, Siniša Šegvić<br />
**Abstract:** <details><summary>原文: </summary>Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.</details>
**Abstract_cn:** <details><summary>译文: </summary>异常值检测是监督视觉识别的安全关键应用中的一项重要功能。大多数现有方法通过鼓励标准封闭集模型在负训练数据中产生低置信度预测来提供最佳结果。然而，这种方法将预测不确定性与负类别的识别混为一谈。因此，我们重新考虑对对应于 K 个真实类别和一个异常值类别的 K+1 个逻辑的直接预测。这种设置使我们能够制定一个新颖的异常分数，作为分布内不确定性和异常值类后验（我们称之为负客观性）的集合。现在，由于 i) 高预测不确定性或 ii) 与负数据的相似性，可以独立检测异常值。我们将我们的方法嵌入到一个密集的预测架构中，并在 K+2 类上进行掩模级识别。训练过程鼓励新的 K+2 类在粘贴的负面实例中学习负面对象性。无论是否对真实负数据进行训练，我们的模型在图像范围和像素级异常值检测的标准基准上都优于当前最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.15374v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks**<br />
**Title_cn:** AutoMMLab：根据计算机视觉任务的语言指令自动生成可部署模型<br />
**Authors:** Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu<br />
**Abstract:** <details><summary>原文: </summary>Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters. Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation. We further develop a new benchmark, called LAMP, for studying key components in the end-to-end prompt-based model training pipeline. Code, model, and data will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动化机器学习 (AutoML) 是旨在自动化机器学习开发过程的技术集合。虽然传统的 AutoML 方法已成功应用于模型开发的几个关键步骤（例如超参数优化），但缺乏能够自动化整个端到端模型生产工作流程的 AutoML 系统。为了填补这一空白，我们推出了 AutoMMLab，这是一个由 LLM 授权的通用 AutoML 系统，它遵循用户的语言指令来自动化计算机视觉任务的整个模型生产工作流程。所提出的 AutoMMLab 系统有效地利用法学硕士作为连接 AutoML 和 OpenMMLab 社区的桥梁，使非专业人士能够通过用户友好的语言界面轻松构建特定于任务的模型。具体来说，我们提出 RU-LLaMA 来了解用户的请求并调度整个管道，并提出一种基于 LLM 的新型超参数优化器 HPO-LLaMA 来有效搜索最佳超参数。实验表明，我们的AutoMMLab系统具有通用性，涵盖了广泛的主流任务，包括分类、检测、分割和关键点估计。我们进一步开发了一个名为 LAMP 的新基准，用于研究端到端基于提示的模型训练管道中的关键组件。代码、模型和数据将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.15351v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection**<br />
**Title_cn:** 低秩表示满足深度展开：用于高光谱异常检测的通用且可解释的网络<br />
**Authors:** Chenyu Li, Bing Zhang, Danfeng Hong, Jing Yao, Jocelyn Chanussot<br />
**Abstract:** <details><summary>原文: </summary>Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from low resolution, simple background, and small size of the detection data. These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection. To this end, we build a new set of HAD benchmark datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently. In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Method of Multipliers (ADMM) optimizer with the deep network, guiding its search process and imparting a level of interpretability to parameter optimization. Additionally, the integration of physical models with DL techniques eliminates the need for manual parameter tuning. The manually tuned parameters are seamlessly transformed into trainable parameters for deep neural networks, facilitating a more efficient and automated optimization process. Extensive experiments conducted on the AIR-HAD dataset show the superiority of our LRR-Net$^+$ in terms of detection performance and generalization ability, compared to top-performing rivals. Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this paper will be made available freely and openly at \url{https://sites.google.com/view/danfeng-hong}.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前的高光谱异常检测（HAD）基准数据集存在分辨率低、背景简单、检测数据量小等问题。这些因素还限制了众所周知的低秩表示（LRR）模型在背景和目标特征分离的鲁棒性以及对手动参数选择的依赖方面的性能。为此，我们构建了一套新的HAD基准数据集，用于提高HAD算法在复杂场景下的鲁棒性，简称AIR-HAD。因此，我们通过深入展开字典可学习的 LLR 模型，提出了一种通用且可解释的 HAD 网络，名为 LRR-Net$^+$，它能够以更通用的方式在光谱上解耦背景结构和对象属性并消除偏差由重要干扰目标同时引入。此外，LRR-Net$^+$将交替方向乘子法（ADMM）优化器的求解过程与深层网络相结合，指导其搜索过程并赋予参数优化一定程度的可解释性。此外，物理模型与深度学习技术的集成消除了手动参数调整的需要。手动调整的参数无缝转换为深度神经网络的可训练参数，促进更高效和自动化的优化过程。在 AIR-HAD 数据集上进行的大量实验表明，与表现最好的竞争对手相比，我们的 LRR-Net$^+$ 在检测性能和泛化能力方面具有优越性。此外，本文中的可编译代码和我们的 AIR-HAD 基准数据集将在 \url{https://sites.google.com/view/danfeng-hong} 上免费公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.15335v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding**<br />
**Title_cn:** OpenSUN3D：第一届开放词汇 3D 场景理解研讨会挑战<br />
**Authors:** Francis Engelmann, Ayca Takmaz, Jonas Schult, Elisabetta Fedele, Johanna Wald, Songyou Peng, Xi Wang, Or Litany, Siyu Tang, Federico Tombari, et.al.<br />
**Abstract:** <details><summary>原文: </summary>This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.</details>
**Abstract_cn:** <details><summary>译文: </summary>本报告概述了与 ICCV 2023 联合举办的 OpenSUN3D 开放词汇 3D 场景理解研讨会所举办的挑战赛。该研讨会系列的目标是提供一个探索和讨论开放词汇 3D 场景理解任务的平台，包括但不限于分割、检测和映射。我们概述了研讨会上举办的挑战，介绍了挑战数据集、评估方法以及获胜方法的简要描述。有关更多详细信息，请参阅 https://opensun3d.github.io/index_iccv23.html。</details>
**PDF:** <http://arxiv.org/pdf/2402.15321v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Representing Online Handwriting for Recognition in Large Vision-Language Models**<br />
**Title_cn:** 在大型视觉语言模型中表示在线手写识别<br />
**Authors:** Anastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, Claudiu Musat<br />
**Abstract:** <details><summary>原文: </summary>The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to or better than state-of-the-art online handwriting recognizers. Wide applicability is shown through results with two different VLM families, on multiple public datasets. Our approach can be applied to off-the-shelf VLMs, does not require any changes in their architecture, and can be used in both fine-tuning and parameter-efficient tuning. We perform a detailed ablation study to identify the key elements of the proposed representation.</details>
**Abstract_cn:** <details><summary>译文: </summary>带有触摸屏和手写笔的平板电脑的采用正在增加，其一个关键功能是将手写内容转换为文本，从而实现搜索、索引和人工智能辅助。与此同时，视觉语言模型（VLM）现在成为图像理解的首选解决方案，这要归功于它们在各种任务中的最先进的性能以及统一的训练、微调方法的简单性，并进行推理。虽然 VLM 在基于图像的任务上获得了高性能，但在简单应用时，即通过将手写内容渲染为图像并执行光学字符识别 (OCR) 时，它们在手写识别方面表现不佳。在本文中，我们研究了 VLM 的在线手写识别，超越了简单的 OCR。我们提出了一种新颖的数字墨水（在线手写）标记化表示，其中包括按时间顺序排列的笔画序列作为文本和图像。我们表明，这种表示产生的结果与最先进的在线手写识别器相当或更好。两个不同的 VLM 系列在多个公共数据集上的结果显示了广泛的适用性。我们的方法可以应用于现成的 VLM，不需要对其架构进行任何更改，并且可以用于微调和参数高效调整。我们进行了详细的消融研究，以确定所提出的表示的关键要素。</details>
**PDF:** <http://arxiv.org/pdf/2402.15307v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection**<br />
**Title_cn:** EMIFF：用于车辆-基础设施协作 3D 物体检测的增强型多尺度图像特征融合<br />
**Authors:** Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang<br />
**Abstract:** <details><summary>原文: </summary>In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶中，协作感知利用来自车辆和基础设施的多视角摄像头，提供超越单一车辆视角的具有丰富路况语义背景的全局有利位置。目前，车辆基础设施协作 3D (VIC3D) 目标检测仍然存在两大挑战： 融合多视图图像时由于摄像机之间的时间异步导致的固有姿态误差； $2)$ 由于通信带宽有限而导致传输过程中的信息丢失。为了解决这些问题，我们提出了一种用于 VIC3D 任务的新型基于相机的 3D 检测框架，即增强型多尺度图像特征融合（EMIFF）。为了充分利用车辆和基础设施的整体视角，我们提出了多尺度交叉注意（MCA）和相机感知通道遮蔽（CCM）模块，以在尺度、空间和通道级别增强基础设施和车辆特征，以纠正姿态误差由相机异步引入。我们还引入了具有通道和空间压缩块的特征压缩（FC）模块，以提高传输效率。实验表明，EMIFF 在 DAIR-V2X-C 数据集上实现了 SOTA，在传输成本相当的情况下，显着优于之前的早期融合和后期融合方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.15272v1><br />
**Code:** <https://github.com/bosszhe/emiff>**<br />
>>**index:** 13<br />
**Title:** **GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation**<br />
**Title_cn:** GS-EMA：将梯度手术指数移动平均与边界感知对比学习相结合，以增强动脉瘤分割中的域泛化<br />
**Authors:** Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi<br />
**Abstract:** <details><summary>原文: </summary>The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑动脉瘤的自动分割对于准确诊断和治疗计划至关重要。面对来自不同医疗机构的 3D 旋转血管造影 (3DRA) 数据的显着领域转移和类别不平衡，这项任务变得具有挑战性。这些变化包括图像外观、强度分布、分辨率和动脉瘤大小的差异，所有这些都使分割过程变得复杂。为了解决这些问题，我们提出了一种新颖的领域泛化策略，该策略采用梯度手术指数移动平均（GS-EMA）优化技术与边界感知对比学习（BACL）相结合。我们的方法的独特之处在于，它能够通过学习领域不变的特征来适应新的、看不见的领域，从而提高跨不同临床数据集的动脉瘤分割的鲁棒性和准确性。结果表明，我们提出的方法可以提取更多的域不变特征，最大限度地减少过度分割并捕获更完整的动脉瘤结构。</details>
**PDF:** <http://arxiv.org/pdf/2402.15239v1><br />
**Code:** <https://github.com/fmlinks/domain>**<br />
>>**index:** 14<br />
**Title:** **Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning**<br />
**Title_cn:** 通过 Transwarp 对比学习进行脑血管分割的无监督域适应<br />
**Authors:** Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Kun Wu, Nishant Ravikumar, Alejandro F. Frangi<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督域适应（UDA）旨在将标记的源分布与未标记的目标分布对齐，以获得域不变的预测模型。由于跨模态医学数据表现出显着的域内和域间变化，并且大多数数据未标记，因此 UDA 更加重要，同时在医学图像分析中具有挑战性。本文提出了一种简单但有效的 UDA 对比学习框架，以缩小标记源和未标记目标分布之间的域间差距。我们的方法在脑血管数据集上得到了验证。实验结果表明，我们的方法可以从标记的 3DRA 模态数据中学习潜在特征，并提高未标记的 MRA 模态数据中的血管分割性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.15237v1><br />
**Code:** <https://github.com/fmlinks/domain>**<br />
>>**index:** 15<br />
**Title:** **Attention-Guided Masked Autoencoders For Learning Image Representations**<br />
**Title_cn:** 用于学习图像表示的注意力引导掩模自动编码器<br />
**Authors:** Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski<br />
**Abstract:** <details><summary>原文: </summary>Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.</details>
**Abstract_cn:** <details><summary>译文: </summary>掩码自动编码器 (MAE) 已成为计算机视觉任务无监督预训练的强大方法。虽然普通 MAE 同样重视重建图像的各个部分，但我们建议通过注意力引导损失函数来通知重建过程。通过利用无监督对象发现方面的进步，我们获得了场景的注意力图，我们在损失函数中使用该注意力图来更加重视重建相关对象，从而有效地激励模型学习更多以对象为中心的表示，而不会影响已建立的掩蔽策略。我们的评估表明，我们的预训练模型比普通 MAE 能够学习更好的潜在表示，这通过多个基准上改进的线性探测和 k-NN 分类结果得到证明，同时使 ViT 在不同背景下更加稳健。</details>
**PDF:** <http://arxiv.org/pdf/2402.15172v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing**<br />
**Title_cn:** 视觉语音与语言的结合：用于高效且上下文感知的视觉语音处理的 VSP-LLM 框架<br />
**Authors:** Jeong Hun Yeo, Seunghee Han, Minsu Kim, Yong Man Ro<br />
**Abstract:** <details><summary>原文: </summary>In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM can more effectively recognize and translate lip movements with just 15 hours of labeled data, compared to the recent translation model trained with 433 hours of labeld data.</details>
**Abstract_cn:** <details><summary>译文: </summary>在视觉语音处理中，由于嘴唇运动的模糊性，上下文建模能力是最重要的要求之一。例如，同音词，即具有相同嘴唇动作但发出不同声音的单词，可以通过考虑上下文来区分。在本文中，我们提出了一种新颖的框架，即与法学硕士相结合的视觉语音处理（VSP-LLM），通过发挥法学硕士的压倒性力量来最大化上下文建模能力。具体来说，VSP-LLM 旨在执行视觉语音识别和翻译的多任务，其中给定的指令控制任务的类型。通过采用自监督视觉语音模型将输入视频映射到 LLM 的输入潜在空间。针对输入帧中存在冗余信息的事实，我们提出了一种新颖的重复数据删除方法，通过使用视觉语音单元来减少嵌入的视觉特征。通过所提出的重复数据删除和低秩适配器（LoRA），可以以计算高效的方式训练 VSP-LLM。在翻译数据集（MuAViC 基准）中，我们证明，与最近使用 433 小时的标记数据训练的翻译模型相比，VSP-LLM 只需 15 小时的标记数据即可更有效地识别和翻译嘴唇运动。</details>
**PDF:** <http://arxiv.org/pdf/2402.15151v1><br />
**Code:** <https://github.com/sally-sh/vsp-llm>**<br />
>>**index:** 17<br />
**Title:** **PUAD: Frustratingly Simple Method for Robust Anomaly Detection**<br />
**Title_cn:** PUAD：用于稳健异常检测的极其简单的方法<br />
**Authors:** Shota Sugawara, Ryuji Imamura<br />
**Abstract:** <details><summary>原文: </summary>Developing an accurate and fast anomaly detection model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the anomaly can be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an out-of-distribution detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple out-of-distribution detection method on the feature space against state-of-the-art reconstruction-based approaches. Despite the simplicity of our proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection) achieves state-of-the-art performance on the MVTec LOCO AD dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发准确、快速的异常检测模型是实时计算机视觉应用中的一项重要任务。人们已经进行了大量的研究来开发一个单一的模型来检测结构或逻辑异常，这些异常本质上是不同的。大多数现有方法隐含地假设可以通过识别异常位置来表示异常。然而，我们认为逻辑异常（例如对象数量错误）无法通过空间特征图很好地表示，需要采用替代方法。此外，我们还重点研究了通过在特征空间上使用分布外检测方法来检测逻辑异常的可能性，该方法聚合了特征图的空间信息。作为演示，我们提出了一种方法，该方法将特征空间上的简单分布外检测方法与最先进的基于重建的方法相结合。尽管我们的提案很简单，但我们的方法 PUAD（可描绘和不可描绘异常检测）在 MVTec LOCO AD 数据集上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.15143v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Fiducial Focus Augmentation for Facial Landmark Detection**<br />
**Title_cn:** 用于面部标志检测的基准焦点增强<br />
**Authors:** Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth Balasubramanian<br />
**Abstract:** <details><summary>原文: </summary>Deep learning methods have led to significant improvements in the performance on the facial landmark detection (FLD) task. However, detecting landmarks in challenging settings, such as head pose changes, exaggerated expressions, or uneven illumination, continue to remain a challenge due to high variability and insufficient samples. This inadequacy can be attributed to the model's inability to effectively acquire appropriate facial structure information from the input images. To address this, we propose a novel image augmentation technique specifically designed for the FLD task to enhance the model's understanding of facial structures. To effectively utilize the newly proposed augmentation technique, we employ a Siamese architecture-based training mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to achieve collective learning of high-level feature representations from two different views of the input images. Furthermore, we employ a Transformer + CNN-based network with a custom hourglass module as the robust backbone for the Siamese framework. Extensive experiments show that our approach outperforms multiple state-of-the-art approaches across various benchmark datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习方法显着提高了面部标志检测（FLD）任务的性能。然而，由于高变异性和样本不足，在具有挑战性的环境中检测地标仍然是一个挑战，例如头部姿势变化、夸张的表情或不均匀的照明。这种不足可归因于模型无法有效地从输入图像中获取适当的面部结构信息。为了解决这个问题，我们提出了一种专门为 FLD 任务设计的新型图像增强技术，以增强模型对面部结构的理解。为了有效利用新提出的增强技术，我们采用基于暹罗架构的训练机制和基于深度典型相关分析（DCCA）的损失，以实现从输入图像的两个不同视图进行高级特征表示的集体学习。此外，我们采用基于 Transformer + CNN 的网络和自定义沙漏模块作为 Siamese 框架的强大骨干。大量的实验表明，我们的方法在各种基准数据集上优于多种最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.15044v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **DeepSet SimCLR: Self-supervised deep sets for improved pathology representation learning**<br />
**Title_cn:** DeepSet SimCLR：用于改进病理表示学习的自监督深度集<br />
**Authors:** David Torpey, Richard Klein<br />
**Abstract:** <details><summary>原文: </summary>Often, applications of self-supervised learning to 3D medical data opt to use 3D variants of successful 2D network architectures. Although promising approaches, they are significantly more computationally demanding to train, and thus reduce the widespread applicability of these methods away from those with modest computational resources. Thus, in this paper, we aim to improve standard 2D SSL algorithms by modelling the inherent 3D nature of these datasets implicitly. We propose two variants that build upon a strong baseline model and show that both of these variants often outperform the baseline in a variety of downstream tasks. Importantly, in contrast to previous works in both 2D and 3D approaches for 3D medical data, both of our proposals introduce negligible additional overhead over the baseline, improving the democratisation of these approaches for medical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>通常，自监督学习在 3D 医疗数据中的应用选择使用成功 2D 网络架构的 3D 变体。尽管这些方法很有前途，但它们对训练的计算要求明显更高，因此降低了这些方法的广泛适用性，远离那些具有适度计算资源的方法。因此，在本文中，我们的目标是通过隐式建模这些数据集固有的 3D 性质来改进标准 2D SSL 算法。我们提出了两种基于强大基线模型的变体，并表明这两种变体在各种下游任务中通常都优于基线。重要的是，与之前针对 3D 医疗数据的 2D 和 3D 方法的工作相比，我们的两项提案在基线上引入的额外开销可以忽略不计，从而提高了这些医疗应用方法的民主化程度。</details>
**PDF:** <http://arxiv.org/pdf/2402.15598v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **State Space Models for Event Cameras**<br />
**Title_cn:** 事件相机的状态空间模型<br />
**Authors:** Nikola Zubić, Mathias Gehrig, Davide Scaramuzza<br />
**Abstract:** <details><summary>原文: </summary>Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minimal performance degradation when tested at higher frequencies than the training input. Traditional RNN and Transformer models exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31 mAP, highlighting the effectiveness of SSMs in event-based vision tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>如今，处理事件摄像机数据的最先进的深度神经网络首先将事件的时间窗口转换为密集的网格状输入表示。因此，当部署在比训练时更高的推理频率（即更小的时间窗口）时，它们表现出较差的通用性。我们通过将具有可学习时间尺度参数的状态空间模型（SSM）引入基于事件的视觉来应对这一挑战。这种设计适应不同的频率，而不需要在不同的频率下重新训练网络。此外，我们研究了两种在较高频率部署模型时抵消混叠效应的策略。我们根据基于 RNN 和 Transformer 架构的现有方法，跨各种基准（包括 Gen1 和 1 Mpx 事件相机数据集）全面评估我们的方法。我们的结果表明，基于 SSM 的模型训练速度提高了 33%，并且在比训练输入更高的频率进行测试时，性能下降也最小。传统 RNN 和 Transformer 模型表现出超过 20 mAP 的性能下降，其中 SSM 下降了 3.31 mAP，凸显了 SSM 在基于事件的视觉任务中的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15584v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **MambaIR: A Simple Baseline for Image Restoration with State-Space Model**<br />
**Title_cn:** MambaIR：状态空间模型图像恢复的简单基线<br />
**Authors:** Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, Shu-Tao Xia<br />
**Abstract:** <details><summary>原文: </summary>Recent years have witnessed great progress in image restoration thanks to the advancements in modern deep neural networks e.g. Convolutional Neural Network and Transformer. However, existing restoration backbones are usually limited due to the inherent local reductive bias or quadratic computational complexity. Recently, Selective Structured State Space Model e.g., Mamba, has shown great potential for long-range dependencies modeling with linear complexity, but it is still under-explored in low-level computer vision. In this work, we introduce a simple but strong benchmark model, named MambaIR, for image restoration. In detail, we propose the Residual State Space Block as the core component, which employs convolution and channel attention to enhance the capabilities of the vanilla Mamba. In this way, our MambaIR takes advantage of local patch recurrence prior as well as channel interaction to produce restoration-specific feature representation. Extensive experiments demonstrate the superiority of our method, for example, MambaIR outperforms Transformer-based baseline SwinIR by up to 0.36dB, using similar computational cost but with a global receptive field. Code is available at \url{https://github.com/csguoh/MambaIR}.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，由于现代深度神经网络的进步，图像恢复领域取得了巨大进步。卷积神经网络和变压器。然而，由于固有的局部还原偏差或二次计算复杂性，现有的恢复主干通常受到限制。最近，选择性结构化状态空间模型（例如 Mamba）在具有线性复杂性的远程依赖关系建模方面表现出了巨大的潜力，但它在低级计算机视觉中仍处于探索之中。在这项工作中，我们引入了一个简单但强大的基准模型，名为 MambaIR，用于图像恢复。具体来说，我们提出残差状态空间块作为核心组件，它采用卷积和通道注意力来增强普通曼巴的能力。通过这种方式，我们的 MambaIR 利用局部补丁重现先验以及通道交互来生成特定于恢复的特征表示。大量实验证明了我们方法的优越性，例如，使用类似的计算成本但具有全局感受野，MambaIR 的性能比基于 Transformer 的基线 SwinIR 提高了 0.36dB。代码可在 \url{https://github.com/csguoh/MambaIR} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15648v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Seamless Human Motion Composition with Blended Positional Encodings**<br />
**Title_cn:** 具有混合位置编码的无缝人体运动合成<br />
**Authors:** German Barquero, Sergio Escalera, Cristina Palmero<br />
**Abstract:** <details><summary>原文: </summary>Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.</details>
**Abstract_cn:** <details><summary>译文: </summary>条件人体运动生成是一个重要主题，在虚拟现实、游戏和机器人领域有许多应用。虽然之前的工作重点是生成由文本、音乐或场景引导的运动，但这些通常会导致仅限于较短持续时间的孤立运动。相反，我们解决了由一系列不同的文本描述引导的长的、连续的序列的生成。在这种情况下，我们引入了 FlowMDM，这是第一个基于扩散的模型，可以生成无缝的人体运动合成 (HMC)，无需任何后处理或冗余的去噪步骤。为此，我们引入了混合位置编码，这是一种在去噪链中利用绝对和相对位置编码的技术。更具体地说，全局运动相干性在绝对阶段恢复，而平滑且真实的过渡则在相对阶段建立。因此，我们在 Babel 和 HumanML3D 数据集上的准确性、真实性和平滑度方面取得了最先进的结果。由于其以姿势为中心的交叉注意力，FlowMDM 在每个运动序列仅使用单个描述进行训练时表现出色，这使得它能够在推理时对不同的文本描述具有鲁棒性。最后，为了解决现有 HMC 指标的局限性，我们提出了两个新指标：峰值加加速度和加加速度下面积，以检测突变。</details>
**PDF:** <http://arxiv.org/pdf/2402.15509v1><br />
**Code:** <https://github.com/BarqueroGerman/FlowMDM>**<br />
>>**index:** 3<br />
**Title:** **Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling**<br />
**Title_cn:** 通过逐像素密度分布建模进行半监督计数<br />
**Authors:** Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Zhou Su, Xiaopeng Hong, Deyu Meng<br />
**Abstract:** <details><summary>原文: </summary>This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文重点关注半监督人群计数，其中仅标记了一小部分训练数据。我们将像素级密度值制定为概率分布，而不是单个确定性值。在此基础上，我们提出了半监督人群计数模型。首先，我们设计了一个像素级分布匹配损失来衡量预测和地面实况之间像素级密度分布的差异；其次，我们通过使用密度标记来增强 Transformer 解码器，以专门化解码器的前向。不同的密度区间；第三，我们设计了交错一致性自监督学习机制，以有效地从未标记数据中学习。对四个数据集进行的广泛实验表明，我们的方法在各种标记比率设置下明显优于竞争对手。代码将在 https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.15297v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Descripción automática de secciones delgadas de rocas: una aplicación Web**<br />
**Title_cn:** 道路自动部分说明：网页应用程序<br />
**Authors:** Stalyn Paucar, Christian Mejía-Escobar y Víctor Collaguazo<br />
**Abstract:** <details><summary>原文: </summary>The identification and characterization of various rock types is one of the fundamental activities for geology and related areas such as mining, petroleum, environment, industry and construction. Traditionally, a human specialist is responsible for analyzing and explaining details about the type, composition, texture, shape and other properties using rock samples collected in-situ or prepared in a laboratory. The results become subjective based on experience, in addition to consuming a large investment of time and effort. The present proposal uses artificial intelligence techniques combining computer vision and natural language processing to generate a textual and verbal description from a thin section image of rock. We build a dataset of images and their respective textual descriptions for the training of a model that associates the relevant features of the image extracted by EfficientNetB7 with the textual description generated by a Transformer network, reaching an accuracy value of 0.892 and a BLEU value of 0.71. This model can be a useful resource for research, professional and academic work, so it has been deployed through a Web application for public use.</details>
**Abstract_cn:** <details><summary>译文: </summary>各种岩石类型的识别和表征是地质学以及采矿、石油、环境、工业和建筑等相关领域的基本活动之一。传统上，人类专家负责使用现场收集或实验室制备的岩石样本来分析和解释有关类型、成分、纹理、形状和其他属性的详细信息。除了耗费大量时间和精力之外，结果还变得基于经验而变得主观。本提案使用结合计算机视觉和自然语言处理的人工智能技术，从岩石的薄片图像生成文本和口头描述。我们构建了图像及其各自文本描述的数据集，用于训练模型，将 EfficientNetB7 提取的图像的相关特征与 Transformer 网络生成的文本描述相关联，达到 0.892 的准确度值和 0.71 的 BLEU 值。该模型可以成为研究、专业和学术工作的有用资源，因此已通过 Web 应用程序进行部署以供公众使用。</details>
**PDF:** <http://arxiv.org/pdf/2402.15039v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation Learning of Vision-based Autonomous Driving**<br />
**Title_cn:** Cohere3D：利用时间一致性进行基于视觉的自动驾驶的无监督表示学习<br />
**Authors:** Yichen Xie, Hongge Chen, Gregory P. Meyer, Yong Jae Lee, Eric M. Wolff, Masayoshi Tomizuka, Wei Zhan, Yuning Chai, Xin Huang<br />
**Abstract:** <details><summary>原文: </summary>Due to the lack of depth cues in images, multi-frame inputs are important for the success of vision-based perception, prediction, and planning in autonomous driving. Observations from different angles enable the recovery of 3D object states from 2D image inputs if we can identify the same instance in different input frames. However, the dynamic nature of autonomous driving scenes leads to significant changes in the appearance and shape of each instance captured by the camera at different time steps. To this end, we propose a novel contrastive learning algorithm, Cohere3D, to learn coherent instance representations in a long-term input sequence robust to the change in distance and perspective. The learned representation aids in instance-level correspondence across multiple input frames in downstream tasks. In the pretraining stage, the raw point clouds from LiDAR sensors are utilized to construct the long-term temporal correspondence for each instance, which serves as guidance for the extraction of instance-level representation from the vision-based bird's eye-view (BEV) feature map. Cohere3D encourages a consistent representation for the same instance at different frames but distinguishes between representations of different instances. We evaluate our algorithm by finetuning the pretrained model on various downstream perception, prediction, and planning tasks. Results show a notable improvement in both data efficiency and task performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于图像中缺乏深度线索，多帧输入对于自动驾驶中基于视觉的感知、预测和规划的成功非常重要。如果我们能够识别不同输入帧中的相同实例，那么从不同角度进行观察就可以从 2D 图像输入中恢复 3D 对象状态。然而，自动驾驶场景的动态特性导致相机在不同时间步捕获的每个实例的外观和形状发生显着变化。为此，我们提出了一种新颖的对比学习算法 Cohere3D，用于学习长期输入序列中对距离和视角变化具有鲁棒性的连贯实例表示。学习到的表示有助于下游任务中跨多个输入帧的实例级对应。在预训练阶段，利用激光雷达传感器的原始点云来构建每个实例的长期时间对应关系，这为从基于视觉的鸟瞰图（BEV）中提取实例级表示提供指导特征图。 Cohere3D 鼓励同一实例在不同帧上保持一致的表示，但区分不同实例的表示。我们通过对各种下游感知、预测和规划任务的预训练模型进行微调来评估我们的算法。结果显示数据效率和任务性能都有显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.15583v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **CI w/o TN: Context Injection without Task Name for Procedure Planning**<br />
**Title_cn:** CI w/o TN：没有任务名称的上下文注入用于程序规划<br />
**Authors:** Xinjie Li<br />
**Abstract:** <details><summary>原文: </summary>This paper explores the challenge of procedure planning in instructional videos, which involves creating goal-directed plans based on visual start and goal observations from videos. Previous research has tackled this problem with gradually weaker training supervision, from heavy intermediate visual observations or language instructions to task class supervision. However, with the advent of large language models, even given only the task name, these models can produce a detailed plan. In this study, we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information. Specifically, we hypothesize that previous intermediate supervisions can serve as context information, and we use captions of visual start and goal observations as a much cheaper form of supervision. This approach greatly reduces the labeling cost since the captions can be easily obtained by large pre-trained vision-language foundation models. Technically, we apply BLIP to generate captions as supervision to train the context feature with contrastive learning loss. Afterward, the context feature is fed into the generator to aid in plan generation. Our experiments on two datasets with varying scales demonstrate that our model can achieve comparable performance on multiple metrics, which validates our hypothesis.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文探讨了教学视频中程序规划的挑战，其中涉及根据视频中的视觉起点和目标观察创建目标导向的计划。之前的研究通过逐渐弱化的训练监督来解决这个问题，从大量的中间视觉观察或语言指令到任务类监督。然而，随着大型语言模型的出现，即使只给出任务名称，这些模型也可以产生详细的计划。在这项研究中，我们提出了一个更弱的设置，没有任务名称作为监督，目前现有的大型语言模型无法解决这个问题，因为它们需要良好的提示和足够的信息。具体来说，我们假设之前的中间监督可以作为上下文信息，并且我们使用视觉开始和目标观察的标题作为一种更便宜的监督形式。这种方法大大降低了标记成本，因为可以通过大型预训练视觉语言基础模型轻松获得字幕。从技术上讲，我们应用 BLIP 生成字幕作为监督，通过对比学习损失来训练上下文特征。然后，上下文特征被输入生成器以帮助生成计划。我们对两个不同规模的数据集的实验表明，我们的模型可以在多个指标上实现可比的性能，这验证了我们的假设。</details>
**PDF:** <http://arxiv.org/pdf/2402.15579v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?**<br />
**Title_cn:** 组合参数高效模块是否可以提高少样本传输精度？<br />
**Authors:** Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, Xi Chen<br />
**Abstract:** <details><summary>原文: </summary>Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full fine-tuning and training a LoRA from scratch. Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters. Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>参数高效的微调是在下游任务上有效微调大型语言和视觉模型的标准。具体来说，低秩适应的效率促进了数百个自定义 LoRA 模块的创建和共享，每个模块都根据来自各种下游任务的不同数据进行训练。在本文中，我们探讨了 LoRA 模块的可组合性，检查组合这些预训练模块是否可以增强对未见过的下游任务的泛化。我们的研究涉及评估两种方法：(a) 统一组合，涉及对具有相等权重的上游 LoRA 模块进行平均；(b) 学习组合，我们学习每个上游模块的权重并执行加权平均。我们在视觉和语言模型上的实验结果表明，在少数镜头设置中，只有有限数量的样本可用于下游任务，统一和学习的合成方法都会带来更好的传输准确性；优于从头开始的全面微调和训练 LoRA。此外，在全镜头设置中，学习合成的性能与常规 LoRA 训练相当，但可训练参数的数量明显较少。我们的研究揭示了均匀构图在增强低镜头设置中的可转移性方面的潜力，而无需引入额外的可学习参数。</details>
**PDF:** <http://arxiv.org/pdf/2402.15414v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Genie: Generative Interactive Environments**<br />
**Title_cn:** Genie：生成交互环境<br />
**Authors:** Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们介绍 Genie，这是第一个通过无标签的互联网视频以无监督方式训练的生成交互环境。该模型可以被提示生成无数种通过文本、合成图像、照片甚至草图描述的动作可控的虚拟世界。在 11B 参数下，Genie 可以被视为基础世界模型。它由时空视频分词器、自回归动力学模型和简单且可扩展的潜在动作模型组成。 Genie 使用户能够在生成的环境中逐帧进行操作，尽管训练时没有任何真实动作标签或世界模型文献中常见的其他特定领域要求。此外，由此产生的学习潜在动作空间有助于训练智能体模仿未见过的视频中的行为，为未来训练多面手智能体开辟道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.15391v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Source-Guided Similarity Preservation for Online Person Re-Identification**<br />
**Title_cn:** 用于在线人员重新识别的源引导相似性保留<br />
**Authors:** Hamza Rami, Jhony H. Giraldo, Nicolas Winckler, Stéphane Lathuilière<br />
**Abstract:** <details><summary>原文: </summary>Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于人员重新识别 (Re-ID) 的在线无监督域适应 (OUDA) 是不断将在注释良好的源域数据集上训练的模型适应作为数据流观察的目标域的任务。在 OUDA 中，行人重新识别模型面临两个主要挑战：灾难性遗忘和领域转移。在这项工作中，我们提出了一种新的源引导相似性保留（S2P）框架来缓解这两个问题。我们的框架基于提取由源图像组成的支持集，最大限度地提高与目标数据的相似度。该支持集用于识别在学习过程中必须保留的特征相似性。 S2P 可以整合多种现有的 UDA 方法来减轻灾难性遗忘。我们的实验表明，S2P 在多个真实到真实和合成到真实的挑战性 OUDA 基准测试中优于以前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.15206v1><br />
**Code:** <https://github.com/ramimmhamza/s2p>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm**<br />
**Title_cn:** 通过进化算法进行低频黑盒后门攻击<br />
**Authors:** Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang<br />
**Abstract:** <details><summary>原文: </summary>While convolutional neural networks (CNNs) have achieved success in computer vision tasks, it is vulnerable to backdoor attacks. Such attacks could mislead the victim model to make attacker-chosen prediction with a specific trigger pattern. Until now, the trigger injection of existing attacks is mainly limited to spatial domain. Recent works take advantage of perceptual properties of planting specific patterns in the frequency domain, which only reflect indistinguishable pixel-wise perturbations in pixel domain. However, in the black-box setup, the inaccessibility of training process often renders more complex trigger designs. Existing frequency attacks simply handcraft the magnitude of spectrum, introducing anomaly frequency disparities between clean and poisoned data and taking risks of being removed by image processing operations (such as lossy compression and filtering). In this paper, we propose a robust low-frequency black-box backdoor attack (LFBA), which minimally perturbs low-frequency components of frequency spectrum and maintains the perceptual similarity in spatial space simultaneously. The key insight of our attack restrict the search for the optimal trigger to low-frequency region that can achieve high attack effectiveness, robustness against image transformation defenses and stealthiness in dual space. We utilize simulated annealing (SA), a form of evolutionary algorithm, to optimize the properties of frequency trigger including the number of manipulated frequency bands and the perturbation of each frequency component, without relying on the knowledge from the victim classifier. Extensive experiments on real-world datasets verify the effectiveness and robustness of LFBA against image processing operations and the state-of-the-art backdoor defenses, as well as its inherent stealthiness in both spatial and frequency space, making it resilient against frequency inspection.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然卷积神经网络（CNN）在计算机视觉任务中取得了成功，但它很容易受到后门攻击。此类攻击可能会误导受害者模型，以特定的触发模式做出攻击者选择的预测。到目前为止，现有攻击的触发注入主要局限于空间域。最近的工作利用了在频域中植入特定模式的感知特性，这仅反映了像素域中不可区分的像素级扰动。然而，在黑盒设置中，训练过程的不可访问性往往导致触发器设计更加复杂。现有的频率攻击只是简单地手工调整频谱的大小，在干净数据和中毒数据之间引入异常频率差异，并冒着被图像处理操作（例如有损压缩和过滤）删除的风险。在本文中，我们提出了一种鲁棒的低频黑盒后门攻击（LFBA），它最大限度地干扰频谱的低频分量，同时保持空间空间的感知相似性。我们攻击的关键见解将最佳触发的搜索限制在低频区域，从而实现高攻击效率、针对图像变换防御的鲁棒性以及对偶空间中的隐身性。我们利用模拟退火（SA）（一种进化算法）来优化频率触发的属性，包括操纵频带的数量和每个频率分量的扰动，而不依赖于受害者分类器的知识。对真实世界数据集的大量实验验证了 LFBA 针对图像处理操作和最先进的后门防御的有效性和鲁棒性，以及其在空间和频率空间中固有的隐秘性，使其能够抵抗频率检查。</details>
**PDF:** <http://arxiv.org/pdf/2402.15653v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise**<br />
**Title_cn:** 用于在存在散斑噪声的情况下恢复图像的袋装深度图像先验<br />
**Authors:** Xi Chen, Zhewen Hou, Christopher A. Metzler, Arian Maleki, Shirin Jalali<br />
**Abstract:** <details><summary>原文: </summary>We investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. Our theoretical contributions include establishing the first existing theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our theoretical results capture the dependence of MSE upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we introduce the concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with projected gradient descent. Furthermore, we show how employing Newton-Schulz algorithm for calculating matrix inverses within the iterations of PGD reduces the computational complexity of the algorithm. We will show that this method achieves the state-of-the-art performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究了基于似然的方法的理论和算法方面，用于从受散斑（乘性）噪声影响的多组测量（称为外观）中恢复复值信号。我们的理论贡献包括在深度图像先验假设下建立最大似然估计的均方误差（MSE）的第一个现有理论上限。我们的理论结果捕捉到了 MSE 对深度图像先验参数数量、外观数量、信号维数以及每次外观测量次数的依赖性。在算法方面，我们引入了袋装深度图像先验（Bagged-DIP）的概念，并将其与投影梯度下降相结合。此外，我们还展示了如何使用 Newton-Schulz 算法在 PGD 迭代中计算矩阵逆来降低算法的计算复杂度。我们将证明这种方法实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.15635v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles**<br />
**Title_cn:** 通过自动驾驶车辆的不确定性改进可解释的对象诱发模型<br />
**Authors:** Shihong Ling, Yue Wan, Xiaowei Jia, Na Du<br />
**Abstract:** <details><summary>原文: </summary>The rapid evolution of automated vehicles (AVs) has the potential to provide safer, more efficient, and comfortable travel options. However, these systems face challenges regarding reliability in complex driving scenarios. Recent explainable AV architectures neglect crucial information related to inherent uncertainties while providing explanations for actions. To overcome such challenges, our study builds upon the "object-induced" model approach that prioritizes the role of objects in scenes for decision-making and integrates uncertainty assessment into the decision-making process using an evidential deep learning paradigm with a Beta prior. Additionally, we explore several advanced training strategies guided by uncertainty, including uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA dataset, our findings underscore that the model, through these enhancements, not only offers a clearer comprehension of AV decisions and their underlying reasoning but also surpasses existing baselines across a broad range of scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶汽车 (AV) 的快速发展有可能提供更安全、更高效、更舒适的出行选择。然而，这些系统在复杂驾驶场景下的可靠性方面面临挑战。最近的可解释自动驾驶架构在提供行动解释时忽略了与固有不确定性相关的关键信息。为了克服这些挑战，我们的研究建立在“对象诱导”模型方法的基础上，该方法优先考虑场景中对象的角色以进行决策，并使用具有 Beta 先验的证据深度学习范式将不确定性评估集成到决策过程中。此外，我们探索了几种由不确定性引导的高级训练策略，包括不确定性引导的数据重新加权和增强。利用 BDD-OIA 数据集，我们的研究结果强调，通过这些增强功能，该模型不仅可以更清晰地理解 AV 决策及其基本推理，而且还超越了广泛场景中的现有基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.15572v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration**<br />
**Title_cn:** CLIPPER+：用于鲁棒全局注册的快速最大派系算法<br />
**Authors:** Kaveh Fathian, Tyler Summers<br />
**Abstract:** <details><summary>原文: </summary>We present CLIPPER+, an algorithm for finding maximal cliques in unweighted graphs for outlier-robust global registration. The registration problem can be formulated as a graph and solved by finding its maximum clique. This formulation leads to extreme robustness to outliers; however, finding the maximum clique is an NP-hard problem, and therefore approximation is required in practice for large-size problems. The performance of an approximation algorithm is evaluated by its computational complexity (the lower the runtime, the better) and solution accuracy (how close the solution is to the maximum clique). Accordingly, the main contribution of CLIPPER+ is outperforming the state-of-the-art in accuracy while maintaining a relatively low runtime. CLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the graph by removing vertices that have a small core number and cannot be a part of the maximum clique. This will result in a smaller graph, on which the maximum clique can be estimated considerably faster. We evaluate the performance of CLIPPER+ on standard graph benchmarks, as well as synthetic and real-world point cloud registration problems. These evaluations demonstrate that CLIPPER+ has the highest accuracy and can register point clouds in scenarios where over $99\%$ of associations are outliers. Our code and evaluation benchmarks are released at https://github.com/ariarobotics/clipperp.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 CLIPPER+，一种在未加权图中查找最大派系以实现异常值鲁棒全局配准的算法。配准问题可以用图表示，并通过找到其最大派系来解决。该公式对异常值具有极高的鲁棒性；然而，找到最大团是一个NP困难问题，因此在实践中对于大尺寸问题需要近似。近似算法的性能通过其计算复杂度（运行时间越低越好）和解精度（解与最大团的接近程度）来评估。因此，CLIPPER+ 的主要贡献是在准确性方面超越最先进的技术，同时保持相对较低的运行时间。 CLIPPER+ 建立在先前的工作（CLIPPER [1] 和 PMC [2]）的基础上，并通过删除核心数较小且不能成为最大团的一部分的顶点来修剪图。这将产生一个较小的图，在该图上可以更快地估计最大团。我们评估 CLIPPER+ 在标准图形基准以及合成和现实点云配准问题上的性能。这些评估表明 CLIPPER+ 具有最高的准确性，并且可以在超过 $99%$ 的关联为异常值的情况下注册点云。我们的代码和评估基准发布于 https://github.com/ariarobotics/clipperp。</details>
**PDF:** <http://arxiv.org/pdf/2402.15464v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review**<br />
**Title_cn:** 人口贩运调查中多媒体地理定位的计算机视觉：系统文献综述<br />
**Authors:** Opeyemi Bamigbade, John Sheppard, Mark Scanlon<br />
**Abstract:** <details><summary>原文: </summary>The task of multimedia geolocation is becoming an increasingly essential component of the digital forensics toolkit to effectively combat human trafficking, child sexual exploitation, and other illegal acts. Typically, metadata-based geolocation information is stripped when multimedia content is shared via instant messaging and social media. The intricacy of geolocating, geotagging, or finding geographical clues in this content is often overly burdensome for investigators. Recent research has shown that contemporary advancements in artificial intelligence, specifically computer vision and deep learning, show significant promise towards expediting the multimedia geolocation task. This systematic literature review thoroughly examines the state-of-the-art leveraging computer vision techniques for multimedia geolocation and assesses their potential to expedite human trafficking investigation. This includes a comprehensive overview of the application of computer vision-based approaches to multimedia geolocation, identifies their applicability in combating human trafficking, and highlights the potential implications of enhanced multimedia geolocation for prosecuting human trafficking. 123 articles inform this systematic literature review. The findings suggest numerous potential paths for future impactful research on the subject.</details>
**Abstract_cn:** <details><summary>译文: </summary>多媒体地理定位的任务正在成为数字取证工具包中越来越重要的组成部分，以有效打击人口贩运、儿童性剥削和其他非法行为。通常，当通过即时消息和社交媒体共享多媒体内容时，基于​​元数据的地理位置信息会被删除。地理定位、地理标记或在这些内容中寻找地理线索的复杂性往往对调查人员来说过于繁重。最近的研究表明，当代人工智能的进步，特别是计算机视觉和深度学习，为加快多媒体地理定位任务带来了巨大的希望。这篇系统的文献综述彻底研究了利用计算机视觉技术进行多媒体地理定位的最先进技术，并评估了它们加快人口贩运调查的潜力。其中包括对基于计算机视觉的方法在多媒体地理定位中的应用的全面概述，确定其在打击人口贩运方面的适用性，并强调增强多媒体地理定位对起诉人口贩运的潜在影响。本系统文献综述包含 123 篇文章。研究结果为未来对该主题进行有影响力的研究提出了许多潜在的途径。</details>
**PDF:** <http://arxiv.org/pdf/2402.15448v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Optimal Transport on the Lie Group of Roto-translations**<br />
**Title_cn:** 旋转平移李群上的最优传输<br />
**Authors:** Daan Bon, Gautam Pai, Gijs Bellaard, Olga Mula, Remco Duits<br />
**Abstract:** <details><summary>原文: </summary>The roto-translation group SE2 has been of active interest in image analysis due to methods that lift the image data to multi-orientation representations defined on this Lie group. This has led to impactful applications of crossing-preserving flows for image de-noising, geodesic tracking, and roto-translation equivariant deep learning. In this paper, we develop a computational framework for optimal transportation over Lie groups, with a special focus on SE2. We make several theoretical contributions (generalizable to matrix Lie groups) such as the non-optimality of group actions as transport maps, invariance and equivariance of optimal transport, and the quality of the entropic-regularized optimal transport plan using geodesic distance approximations. We develop a Sinkhorn like algorithm that can be efficiently implemented using fast and accurate distance approximations of the Lie group and GPU-friendly group convolutions. We report valuable advancements in the experiments on 1) image barycenters, 2) interpolation of planar orientation fields, and 3) Wasserstein gradient flows on SE2. We observe that our framework of lifting images to SE2 and optimal transport with left-invariant anisotropic metrics leads to equivariant transport along dominant contours and salient line structures in the image. This yields sharper and more meaningful interpolations compared to their counterparts on $\mathbb{R}^2$</details>
**Abstract_cn:** <details><summary>译文: </summary>旋转平移组 SE2 因其将图像数据提升为在该李组上定义的多方向表示的方法而在图像分析中引起了人们的积极兴趣。这导致了交叉保留流在图像去噪、测地线跟踪和旋转平移等变深度学习方面的有效应用。在本文中，我们开发了一个用于李群上最优传输的计算框架，特别关注 SE2。我们做出了一些理论贡献（可推广到矩阵李群），例如作为传输映射的群行为的非最优性、最优传输的不变性和等变性，以及使用测地距离近似的熵正则化最优传输计划的质量。我们开发了一种类似 Sinkhorn 的算法，可以使用快速准确的李群距离近似和 GPU 友好的组卷积来有效实现。我们报告了 1) 图像重心、2) 平面方向场插值和 3) SE2 上的 Wasserstein 梯度流实验中的宝贵进展。我们观察到，我们将图像提升到 SE2 的框架以及具有左不变各向异性度量的最佳传输导致沿着图像中的主要轮廓和显着线结构的等变传输。与 $\mathbb{R}^2$ 上的对应插值相比，这会产生更清晰、更有意义的插值</details>
**PDF:** <http://arxiv.org/pdf/2402.15322v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding**<br />
**Title_cn:** 眼见为实：通过 CLIP 引导解码减轻大视觉语言模型中的幻觉<br />
**Authors:** Ailin Deng, Zhirui Chen, Bryan Hooi<br />
**Abstract:** <details><summary>原文: </summary>Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallucination across multiple LVLM families while preserving the utility of text generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>大视觉语言模型（LVLM）很容易出现物体幻觉，即它们生成的文本包含不存在的物体，这极大地限制了它们的可靠性和实用性。当前的方法通常依赖于模型的标记可能性或其他内部信息、对其他数据集的指令调整或合并复杂的外部工具。我们首先对句子级 LVLM 幻觉进行实证分析，发现与标记似然相比，CLIP 与图像的相似性可以作为更强、更稳健的幻觉指标。受此启发，我们引入了 CLIP 引导解码（CGD）方法，这是一种简单但有效的免训练方法，可减少解码时的物体幻觉。 CGD 使用 CLIP 通过增强生成的文本与图像的视觉基础来指导模型的解码过程。实验表明，CGD 可以有效减轻多个 LVLM 系列中的物体幻觉，同时保留文本生成的实用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15300v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Font Impression Estimation in the Wild**<br />
**Title_cn:** 野外字体印象估计<br />
**Authors:** Kazuki Kitajima, Daichi Haraguchi, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses the challenging task of estimating font impressions from real font images. We use a font dataset with annotation about font impressions and a convolutional neural network (CNN) framework for this task. However, impressions attached to individual fonts are often missing and noisy because of the subjective characteristic of font impression annotation. To realize stable impression estimation even with such a dataset, we propose an exemplar-based impression estimation approach, which relies on a strategy of ensembling impressions of exemplar fonts that are similar to the input image. In addition, we train CNN with synthetic font images that mimic scanned word images so that CNN estimates impressions of font images in the wild. We evaluate the basic performance of the proposed estimation method quantitatively and qualitatively. Then, we conduct a correlation analysis between book genres and font impressions on real book cover images; it is important to note that this analysis is only possible with our impression estimation method. The analysis reveals various trends in the correlation between them - this fact supports a hypothesis that book cover designers carefully choose a font for a book cover considering the impression given by the font.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了从真实字体图像估计字体印象的挑战性任务。我们使用带有字体印象注释的字体数据集和卷积神经网络（CNN）框架来完成此任务。然而，由于字体印象注释的主观特征，附加到单个字体的印象经常丢失和嘈杂。为了即使使用这样的数据集也能实现稳定的印象估计，我们提出了一种基于示例的印象估计方法，该方法依赖于与输入图像相似的示例字体的印象集成策略。此外，我们使用模仿扫描文字图像的合成字体图像来训练 CNN，以便 CNN 估计字体图像的真实印象。我们定量和定性地评估了所提出的估计方法的基本性能。然后，我们对真实书籍封面图像上的书籍类型和字体印象进行相关性分析；值得注意的是，这种分析只能通过我们的印象估计方法来实现。分析揭示了它们之间相关性的各种趋势 - 这一事实支持了这样一个假设：书籍封面设计者考虑字体给人的印象，仔细选择书籍封面的字体。</details>
**PDF:** <http://arxiv.org/pdf/2402.15236v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Which Model to Transfer? A Survey on Transferability Estimation**<br />
**Title_cn:** 要转移哪个模型？可迁移性估计调查<br />
**Authors:** Yuhe Ding, Bo Jiang, Aijing Yu, Aihua Zheng, Jian Liang<br />
**Abstract:** <details><summary>原文: </summary>Transfer learning methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks. With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task. Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive. Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings. In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation. Each category is systematically defined, accompanied by a comprehensive taxonomy. Besides, we address challenges and outline future research directions, intending to provide a comprehensive guide to aid researchers and practitioners.</details>
**Abstract_cn:** <details><summary>译文: </summary>迁移学习方法致力于利用现有源预训练模型或数据集中的相关知识来解决下游目标任务。随着当今可用预训练模型的规模和数量的增加，提前评估它们是否适合特定的目标任务变得至关重要。模型可转移性估计是一个新兴且不断增长的兴趣领域，旨在提出一种度量来量化这种适用性，而无需单独训练它们，这在计算上是令人望而却步的。尽管最近在该领域取得了广泛的进展，但它们具有自定义的术语定义和实验设置。在本次调查中，我们首次回顾了该领域的现有进展，并将它们分为两个不同的领域：无源模型可转移性估计和源相关模型可转移性估计。每个类别都经过系统定义，并附有全面的分类法。此外，我们还应对挑战并概述未来的研究方向，旨在为研究人员和从业人员提供全面的指导。</details>
**PDF:** <http://arxiv.org/pdf/2402.15231v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators**<br />
**Title_cn:** BSPA：探索针对图像生成器的黑盒隐形即时攻击<br />
**Authors:** Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, Jun Zhu<br />
**Abstract:** <details><summary>原文: </summary>Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future.</details>
**Abstract_cn:** <details><summary>译文: </summary>超大图像生成器为不同领域提供了巨大的变革潜力。它允许用户设计特定的提示，通过一些黑盒 API 生成逼真的图像。然而，一些研究表明，图像生成器特别容易受到攻击，并通过手动设计的有毒文本生成不适合工作（NSFW）内容，尤其是人类观察者难以察觉的内容。我们迫切需要大量通用且可转移的提示来提高图像生成器的安全性，尤其是黑盒发布的API。然而，它们受到劳动密集型设计流程的限制，并且严重依赖给定指令的质量。为了实现这一目标，我们引入了一种黑盒隐形提示攻击（BSPA），它采用检索器来模拟来自 API 用户的攻击。它可以有效地利用过滤器分数来调整敏感词的检索空间以匹配输入提示，从而为图像生成器量身定制隐秘提示。值得注意的是，这种方法与模型无关，不需要内部访问模型的特征，确保其适用于各种图像生成器。在BSPA的基础上，我们构建了一个自动化提示工具和一个全面的提示攻击数据集（NSFWeval）。大量实验表明，BSPA 有效地探索了各种最先进的可用黑盒模型中的安全漏洞，包括 Stable Diffusion XL、Midjourney 和 DALL-E 2/3。此外，我们开发了一个弹性文本过滤器并提供有针对性的建议，以确保图像生成器的安全性，免受未来的即时攻击。</details>
**PDF:** <http://arxiv.org/pdf/2402.15218v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Convergence Analysis of Blurring Mean Shift**<br />
**Title_cn:** 模糊均值漂移的收敛性分析<br />
**Authors:** Ryoya Yamasaki, Toshiyuki Tanaka<br />
**Abstract:** <details><summary>原文: </summary>Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm, is a kernel-based iterative method for data clustering, where data points are clustered according to their convergent points via iterative blurring. In this paper, we analyze convergence properties of the BMS algorithm by leveraging its interpretation as an optimization procedure, which is known but has been underutilized in existing convergence studies. Whereas existing results on convergence properties applicable to multi-dimensional data only cover the case where all the blurred data point sequences converge to a single point, this study provides a convergence guarantee even when those sequences can converge to multiple points, yielding multiple clusters. This study also shows that the convergence of the BMS algorithm is fast by further leveraging geometrical characterization of the convergent points.</details>
**Abstract_cn:** <details><summary>译文: </summary>模糊均值平移（BMS）算法是均值平移算法的一种变体，是一种基于核的迭代数据聚类方法，其中数据点通过迭代模糊根据其收敛点进行聚类。在本文中，我们利用 BMS 算法作为优化过程的解释来分析 BMS 算法的收敛特性，这种优化过程是已知的，但在现有的收敛研究中尚未得到充分利用。虽然适用于多维数据的收敛特性的现有结果仅涵盖所有模糊数据点序列收敛到单个点的情况，但即使这些序列可以收敛到多个点，产生多个簇，本研究也提供了收敛保证。这项研究还表明，通过进一步利用收敛点的几何特征，BMS 算法的收敛速度很快。</details>
**PDF:** <http://arxiv.org/pdf/2402.15146v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Fine-tuning CLIP Text Encoders with Two-step Paraphrasing**<br />
**Title_cn:** 通过两步释义微调 CLIP 文本编码器<br />
**Authors:** Hyunjae Kim, Seunghyun Yoon, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang<br />
**Abstract:** <details><summary>原文: </summary>Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比语言图像预训练（CLIP）模型在各种视觉语言任务中取得了相当大的成功，例如文本到图像检索，其中模型需要有效处理自然语言输入以产生准确的视觉输出。然而，当前模型在处理输入查询中的语言变化（例如释义）方面仍然面临限制，这使得在现实应用程序中处理广泛的用户查询具有挑战性。在本研究中，我们引入了一种简单的微调方法来增强 CLIP 模型的释义表示。我们的方法涉及两步释义生成过程，其中我们利用大型语言模型从网络规模的图像标题自动创建两类释义。随后，我们使用这些生成的释义微调 CLIP 文本编码器，同时冻结图像编码器。我们生成的模型（我们称之为 ParaCLIP）在各种任务上都比基线 CLIP 模型有了显着改进，包括释义检索（排名相似度得分提高了 2.0% 和 5.6%）、视觉基因组关系和归因，以及七种语义文本相似度任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.15120v1><br />
**Code:** null<br />

