## [UPDATED!] **2024-02-24** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging**<br />
**Title_cn:** Sandwich GAN：基于相位掩模的防眩光图像重建<br />
**Authors:** Xiaopeng Peng, Erin F. Fleet, Abbie T. Watnik, Grover A. Swartzlander<br />
**Abstract:** <details><summary>原文: </summary>Conventional camera systems are susceptible to the adverse effects of laser dazzle, which may over-saturate an image or cause permanent damage to pixels. To address this problem, we developed an approach combining point spread function engineering whereby a wavefront-coded mask in the pupil plane blurs both the laser and scene, together with a deep neural sandwich network. In addition to protecting the sensor, our approach jointly removes the laser from the scene and reconstructs a satisfactory deblurred image. Image recovery is achieved by wrapping two generative adversarial networks (GANs) around a learnable non-blind image deconvolution module. We trained the Sandwich GAN (SGAN) to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold - the point at which the bare system without the phase mask may exhibit damage. The end-to-end training includes physics-based modeling of the imaging system whereby a laser having an arbitrary angle of incidence is superimposed on images from a large publicly available library. The trained system was validated in the laboratory for laser strengths up to $10^4$ times the saturation value. The proposed image restoration model quantitatively and qualitatively outperforms other methods for a wide range of scene contents, illumination conditions, laser strengths, and noise characteristics.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的相机系统容易受到激光眩光的不利影响，这可能会使图像过度饱和或对像素造成永久性损坏。为了解决这个问题，我们开发了一种结合点扩散函数工程的方法，其中光瞳平面中的波前编码掩模与深度神经三明治网络一起模糊激光和场景。除了保护传感器之外，我们的方法还共同从场景中移除激光并重建令人满意的去模糊图像。图像恢复是通过在可学习的非盲图像反卷积模块周围包裹两个生成对抗网络（GAN）来实现的。我们训练了 Sandwich GAN (SGAN)，以将峰值激光辐照度抑制高达传感器饱和阈值的 10^6 倍，即没有相位掩模的裸系统可能会出现损坏的点。端到端训练包括基于物理的成像系统建模，其中具有任意入射角的激光叠加在来自大型公共可用库的图像上。经过训练的系统在实验室中经过验证，激光强度高达饱和值的 10^4$ 倍。对于广泛的场景内容、照明条件、激光强度和噪声特性，所提出的图像恢复模型在数量和质量上都优于其他方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.15919v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Enhanced Droplet Analysis Using Generative Adversarial Networks**<br />
**Title_cn:** 使用生成对抗网络增强液滴分析<br />
**Authors:** Tan-Hanh Pham, Kim-Doang Nguyen<br />
**Abstract:** <details><summary>原文: </summary>Precision devices play an important role in enhancing production quality and productivity in agricultural systems. Therefore, the optimization of these devices is essential in precision agriculture. Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance. However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect. To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using generative adversarial networks (GAN). The GAN model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution. The results demonstrate that the model can generate high-quality images with the size of $1024\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset. As a result, the detection model achieves a 16.06\% increase in mean average precision (mAP) when utilizing the synthetic dataset. To the best of our knowledge, this work stands as the first to employ a generative model for augmenting droplet detection. Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks. This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices.</details>
**Abstract_cn:** <details><summary>译文: </summary>精密设备在提高农业系统的生产质量和生产力方面发挥着重要作用。因此，这些设备的优化在精准农业中至关重要。最近，随着深度学习的进步，已经有几项研究旨在利用其能力来提高喷雾系统的性能。然而，这些方法的有效性在很大程度上取决于训练数据集的大小，而训练数据集的收集成本昂贵且耗时。为了解决训练样本不足的挑战，本文提出了一种替代解决方案，即使用生成对抗网络（GAN）生成液滴的人工图像。 GAN 模型使用高速摄像机捕获的小型数据集进行训练，能够生成分辨率逐渐增加的图像。结果表明，该模型可以生成大小为$1024\times1024$的高质量图像。此外，这项研究利用计算机视觉和深度学习的最新进展，使用合成数据集开发光滴探测器。因此，在利用合成数据集时，检测模型的平均精度 (mAP) 提高了 16.06%。据我们所知，这项工作是第一个采用生成模型来增强液滴检测的工作。其意义不仅在于优化喷嘴设计以构建高效喷雾系统，还在于解决各种精准农业任务中数据不足的共同挑战。这项工作为保护资源、同时努力实现最佳和可持续的农业实践做出了重要贡献。</details>
**PDF:** <http://arxiv.org/pdf/2402.15909v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models**<br />
**Title_cn:** HIR-Diff：通过改进的扩散模型进行无监督高光谱图像恢复<br />
**Authors:** Li Pang, Xiangyu Rui, Long Cui, Hongzhong Wang, Deyu Meng, Xiangyong Cao<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks. Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability. To alleviate these issues, this paper proposes an unsupervised HSI restoration framework with pre-trained diffusion model (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix. Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved diffusion model where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled. The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\times$ acceleration for denoising) with little performance decrease. Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting. The code is available at https://github.com/LiPang/HIRDiff.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱图像（HSI）恢复旨在从退化的观测中恢复干净的图像，并在下游任务中发挥着至关重要的作用。现有的基于模型的方法在利用手工先验对复杂图像特征进行精确建模方面存在局限性，而基于深度学习的方法泛化能力较差。为了缓解这些问题，本文提出了一种带有预训练扩散模型（HIR-Diff）的无监督 HSI 恢复框架，该框架从两个低秩分量（即缩小图像和系数矩阵）的乘积恢复干净的 HSI。具体来说，缩小后的图像具有低光谱维度，位于像场中，可以从我们改进的扩散模型中推断出，其中设计了具有总变分（TV）先验的新引导函数，以确保缩小后的图像能够很好地采样。基于奇异值分解（SVD）和秩揭示 QR（RRQR）分解，可以有效地预先估计系数矩阵。此外，还提出了一种新颖的指数噪声调度来加速恢复过程（去噪加速大约 5 倍），而性能几乎没有下降。大量的实验结果验证了我们的方法在各种 HSI 恢复任务上的性能和速度方面的优越性，包括 HSI 去噪、噪声 HSI 超分辨率和噪声 HSI 修复。代码可在 https://github.com/LiPang/HIRDiff 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15865v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation**<br />
**Title_cn:** 用于材料微观结构 3D 重建和性能评估的生成机器学习模型<br />
**Authors:** Yilin Zheng, Zhigong Song<br />
**Abstract:** <details><summary>原文: </summary>The reconstruction of 3D microstructures from 2D slices is considered to hold significant value in predicting the spatial structure and physical properties of materials.The dimensional extension from 2D to 3D is viewed as a highly challenging inverse problem from the current technological perspective.Recently,methods based on generative adversarial networks have garnered widespread attention.However,they are still hampered by numerous limitations,including oversimplified models,a requirement for a substantial number of training samples,and difficulties in achieving model convergence during training.In light of this,a novel generative model that integrates the multiscale properties of U-net with and the generative capabilities of GAN has been proposed.Based on this,the innovative construction of a multi-scale channel aggregation module,a multi-scale hierarchical feature aggregation module and a convolutional block attention mechanism can better capture the properties of the material microstructure and extract the image information.The model's accuracy is further improved by combining the image regularization loss with the Wasserstein distance loss.In addition,this study utilizes the anisotropy index to accurately distinguish the nature of the image,which can clearly determine the isotropy and anisotropy of the image.It is also the first time that the generation quality of material samples from different domains is evaluated and the performance of the model itself is compared.The experimental results demonstrate that the present model not only shows a very high similarity between the generated 3D structures and real samples but is also highly consistent with real data in terms of statistical data analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>从2D切片重建3D微观结构被认为在预测材料的空间结构和物理性质方面具有重要价值。从2D到3D的维度扩展从当前技术角度来看被视为一个极具挑战性的反问题。最近，基于方法生成对抗网络的研究受到了广泛的关注。然而，它们仍然受到许多限制的阻碍，包括模型过于简化、需要大量训练样本、训练过程中模型难以收敛等。有鉴于此，一种新颖的生成对抗网络提出了融合U-net的多尺度特性和GAN的生成能力的模型，并在此基础上创新性地构建了多尺度通道聚合模块、多尺度层次特征聚合模块和卷积块注意力模块该机制可以更好地捕捉材料微观结构的特性并提取图像信息。通过将图像正则化损失与Wasserstein距离损失相结合，进一步提高了模型的精度。此外，本研究利用各向异性指数来准确区分材料的性质。图像，可以清晰地判断图像的各向同性和各向异性。这也是首次对不同领域的材料样本的生成质量进行评估并比较模型本身的性能。实验结果表明，本模型不仅显示出生成的3D结构与真实样本之间非常高的相似性，而且在统计数据分析方面也与真实数据高度一致。</details>
**PDF:** <http://arxiv.org/pdf/2402.15815v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT**<br />
**Title_cn:** 智能导演：使用 ChatGPT 的动态视觉合成自动框架<br />
**Authors:** Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu<br />
**Abstract:** <details><summary>原文: </summary>With the rise of short video platforms represented by TikTok, the trend of users expressing their creativity through photos and videos has increased dramatically. However, ordinary users lack the professional skills to produce high-quality videos using professional creation software. To meet the demand for intelligent and user-friendly video creation tools, we propose the Dynamic Visual Composition (DVC) task, an interesting and challenging task that aims to automatically integrate various media elements based on user requirements and create storytelling videos. We propose an Intelligent Director framework, utilizing LENS to generate descriptions for images and video frames and combining ChatGPT to generate coherent captions while recommending appropriate music names. Then, the best-matched music is obtained through music retrieval. Then, materials such as captions, images, videos, and music are integrated to seamlessly synthesize the video. Finally, we apply AnimeGANv2 for style transfer. We construct UCF101-DVC and Personal Album datasets and verified the effectiveness of our framework in solving DVC through qualitative and quantitative comparisons, along with user studies, demonstrating its substantial potential.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着以TikTok为代表的短视频平台的崛起，用户通过照片和视频表达创意的趋势急剧增加。然而，普通用户缺乏使用专业创作软件制作高质量视频的专业技能。为了满足智能和用户友好的视频创作工具的需求，我们提出了动态视觉合成（DVC）任务，这是一项有趣且具有挑战性的任务，旨在根据用户需求自动集成各种媒体元素并创建讲故事的视频。我们提出了一个智能导演框架，利用 LENS 生成图像和视频帧的描述，并结合 ChatGPT 生成连贯的字幕，同时推荐适当的音乐名称。然后，通过音乐检索得到最匹配的音乐。然后，将字幕、图像、视频和音乐等素材整合起来，无缝合成视频。最后，我们应用 AnimeGANv2 进行风格迁移。我们构建了 UCF101-DVC 和个人相册数据集，并通过定性和定量比较以及用户研究验证了我们的框架在解决 DVC 方面的有效性，展示了其巨大的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.15746v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA**<br />
**Title_cn:** 弥合 2D 和 3D 视觉问答之间的差距：3D VQA 的融合方法<br />
**Authors:** Wentao Mo, Yang Liu<br />
**Abstract:** <details><summary>原文: </summary>In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at $\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.</details>
**Abstract_cn:** <details><summary>译文: </summary>在 3D 视觉问答 (3D VQA) 中，完整注释数据的稀缺和有限的视觉内容多样性阻碍了对新场景和 3D 概念的泛化（例如，ScanQA 和 SQA 数据集中仅使用了大约 800 个场景）。当前的方法度假村用 2D 信息补充 3D 推理。然而，这些方法面临着挑战：要么使用自上而下的 2D 视图，引入过于复杂且有时与问题无关的视觉线索，要么依赖 2D VLM 的全局聚合场景/图像级表示，从而失去了细粒度的视觉 -语言相关性。为了克服这些限制，我们的方法利用问题条件 2D 视图选择过程，精确定位语义相关的 2D 输入以获得关键的视觉线索。然后，我们通过两分支 Transformer 结构将此 2D 知识集成到 3D-VQA 系统中。该结构采用双变压器设计，紧凑地结合了 2D 和 3D 模态，并捕获模态之间的细粒度相关性，使它们能够相互增强。整合上述提出的机制，我们提出了 BridgeQA，它为 3D-VQA 的基于多模态 Transformer 的架构提供了全新的视角。实验验证了 BridgeQA 在 3D-VQA 数据集上达到了最先进的水平，并且显着优于现有解决方案。代码可在 $\href{https://github.com/matthewdm0816/BridgeQA}{\text{此 URL}}$ 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15933v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Multimodal Instruction Tuning with Conditional Mixture of LoRA**<br />
**Title_cn:** 使用 LoRA 的条件混合进行多模式指令调整<br />
**Authors:** Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wenpeng Yin, Lifu Huang<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks. Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions. As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential. However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks. To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型（MLLM）在不同领域的各种任务中表现出了卓越的熟练程度，并且越来越注重提高其对未见过的多模态任务的零样本泛化能力。多模态指令调优已成为通过指令对不同多模态任务上的预训练模型进行微调来实现零样本泛化的成功策略。随着 MLLM 的复杂性和规模不断增长，对参数高效的微调方法的需求变得至关重要，例如低秩适应 (LoRA)，它可以使用最少的参数集进行微调。然而，在多模态指令调优中应用 LoRA 会带来任务干扰的挑战，这会导致性能下降，特别是在处理广泛的多模态任务时。为了解决这个问题，本文介绍了一种将多模式指令调整与条件混合 LoRA (MixLoRA) 相结合的新颖方法。它在 LoRA 的基础上进行了创新，动态构建适合每个输入实例的独特需求的低秩适应矩阵，旨在减轻任务干扰。在各种多模态评估数据集上的实验结果表明，MixLoRA不仅优于同等甚至更高等级的传统LoRA，证明了其在多种多模态任务中的有效性和适应性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15896v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology**<br />
**Title_cn:** FedMM：计算病理学中具有模态异质性的联合多模态学习<br />
**Authors:** Yuanzhe Peng, Jieming Bian, Jie Xu<br />
**Abstract:** <details><summary>原文: </summary>The fusion of complementary multimodal information is crucial in computational pathology for accurate diagnostics. However, existing multimodal learning approaches necessitate access to users' raw data, posing substantial privacy risks. While Federated Learning (FL) serves as a privacy-preserving alternative, it falls short in addressing the challenges posed by heterogeneous (yet possibly overlapped) modalities data across various hospitals. To bridge this gap, we propose a Federated Multi-Modal (FedMM) learning framework that federatedly trains multiple single-modal feature extractors to enhance subsequent classification performance instead of existing FL that aims to train a unified multimodal fusion model. Any participating hospital, even with small-scale datasets or limited devices, can leverage these federated trained extractors to perform local downstream tasks (e.g., classification) while ensuring data privacy. Through comprehensive evaluations of two publicly available datasets, we demonstrate that FedMM notably outperforms two baselines in accuracy and AUC metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>互补的多模态信息的融合对于计算病理学的准确诊断至关重要。然而，现有的多模式学习方法需要访问用户的原始数据，从而带来巨大的隐私风险。虽然联邦学习 (FL) 作为一种隐私保护替代方案，但它无法解决各医院异构（但可能重叠）模式数据带来的挑战。为了弥补这一差距，我们提出了一种联合多模态（FedMM）学习框架，该框架联合训练多个单模态特征提取器以增强后续分类性能，而不是旨在训练统一多模态融合模型的现有 FL。任何参与的医院，即使拥有小规模数据集或有限的设备，也可以利用这些联合训练的提取器来执行本地下游任务（例如分类），同时确保数据隐私。通过对两个公开数据集的综合评估，我们证明 FedMM 在准确性和 AUC 指标方面明显优于两个基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.15858v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Parameter-efficient Prompt Learning for 3D Point Cloud Understanding**<br />
**Title_cn:** 用于 3D 点云理解的参数高效快速学习<br />
**Authors:** Hongyu Sun, Yongcai Wang, Wang Chen, Haoran Deng, Deying Li<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a parameter-efficient prompt tuning method, named PPT, to adapt a large multi-modal model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on time-consuming prompt engineering. We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted prompts with learnable contexts to automate the prompt tuning process. Then, we lock the pre-trained backbone instead of adopting the full fine-tuning paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance prompt tuning for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method.Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, few-shot learning, and part segmentation. The implementation is available at https://github.com/auniquesun/PPT.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种名为 PPT 的参数高效提示调整方法，以适应大型多模态模型以进行 3D 点云理解。现有策略的计算和存储成本相当昂贵，并且依赖于耗时的即时工程。我们从三个方面解决问题。首先，设计了 PromptLearner 模块，用可学习的上下文代替手工制作的提示，以自动化提示调整过程。然后，我们锁定预训练的主干，而不是采用完整的微调范式，以大幅提高参数效率。最后，在目标任务附近布置了一个轻量级的 PointAdapter 模块，以增强对 3D 点云理解的快速调整。综合实验证明了该方法优越的参数和数据效率。同时，我们在4个公共数据集和多个3D任务（即点云识别、少样本学习和零件分割）上获得了新记录。该实现可在 https://github.com/auniquesun/PPT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15823v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation**<br />
**Title_cn:** 使用 GPT-4 生成的描述性提示（无需人工注释）提高多模态医学图像的 SAM 零样本性能<br />
**Authors:** Zekun Jiang, Dongjie Cheng, Ziyuan Qin, Jun Gao, Qicheng Lao, Kang Li, Le Zhang<br />
**Abstract:** <details><summary>原文: </summary>This study develops and evaluates a novel multimodal medical image zero-shot segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images, thereby enhancing SAM for zero-shot segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box prompts, and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective multimodal medical image zero-shot segmentation algorithm, highlighting the significant contribution of GPT-4 to zero-shot segmentation. By integrating foundational models such as GPT-4, GLIP, and SAM, it could enhance the capability to address complex problems in specialized domains. The code is available at: https://github.com/JZK00/TV-SAM.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究开发并评估了一种新型多模态医学图像零样本分割算法，名为文本-视觉-提示 SAM (TV-SAM)，无需任何手动注释。 TV-SAM结合并集成了大语言模型GPT-4、视觉语言模型GLIP和Segment Anything Model (SAM)，从医学图像中自主生成描述性文本提示和视觉边界框提示，从而增强SAM的零样本分割。对涵盖八种成像模式的七个公共数据集进行了综合评估，以证明 TV-SAM 无需额外训练即可有效分割各种模式中看不见的目标，显着优于 SAM AUTO 和 GSAM，与 SAM BBOX 的性能与黄金标准边界框提示紧密匹配，并超越了 ISIC 和 WBC 等特定数据集的最新水平。研究表明TV-SAM是一种有效的多模态医学图像零样本分割算法，凸显了GPT-4对零样本分割的显着贡献。通过集成GPT-4、GLIP和SAM等基础模型，可以增强解决专业领域复杂问题的能力。该代码位于：https://github.com/JZK00/TV-SAM。</details>
**PDF:** <http://arxiv.org/pdf/2402.15759v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation**<br />
**Title_cn:** GAOKAO-MM：中国人类水平的多模态模型评估基准<br />
**Authors:** Yi Zong, Xipeng Qiu<br />
**Abstract:** <details><summary>原文: </summary>The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moderate distance towards Artificial General Intelligence (AGI) and provide insights facilitating the development of multilingual LVLMs.</details>
**Abstract_cn:** <details><summary>译文: </summary>大视觉语言模型（LVLM）在图像感知和语言理解方面表现出了强大的能力。然而，现有的多模态基准侧重于初级感知能力和常识知识，不足以反映 LVLM 的综合能力。我们提出了 GAOKAO-MM，这是一个基于中国高考（GAOKAO）的多模态基准，由 8 个科目和 12 种图像组成，例如图表、函数图、地图和照片。 GAOKAO-MM源自中国本土背景，对模型的感知、理解、知识和推理等能力设定了人类水平的要求。我们评估了10个LVLM，发现它们的准确率都低于50%，其中GPT-4-Vison（48.1%）、Qwen-VL-Plus（41.2%）和Gemini-Pro-Vision（35.1%）排名处于前三名的位置。我们的多维度分析结果表明，LVLM 与通用人工智能 (AGI) 的距离适中，并为促进多语言 LVLM 的发展提供了见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.15745v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge**<br />
**Title_cn:** CLIPose：利用预先训练的视觉语言知识进行类别级物体姿态估计<br />
**Authors:** Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen<br />
**Abstract:** <details><summary>原文: </summary>Most of existing category-level object pose estimation methods devote to learning the object category information from point cloud modality. However, the scale of 3D datasets is limited due to the high cost of 3D data collection and annotation. Consequently, the category features extracted from these limited point cloud samples may not be comprehensive. This motivates us to investigate whether we can draw on knowledge of other modalities to obtain category information. Inspired by this motivation, we propose CLIPose, a novel 6D pose framework that employs the pre-trained vision-language model to develop better learning of object category information, which can fully leverage abundant semantic knowledge in image and text modalities. To make the 3D encoder learn category-specific features more efficiently, we align representations of three modalities in feature space via multi-modal contrastive learning. In addition to exploiting the pre-trained knowledge of the CLIP's model, we also expect it to be more sensitive with pose parameters. Therefore, we introduce a prompt tuning approach to fine-tune image encoder while we incorporate rotations and translations information in the text descriptions. CLIPose achieves state-of-the-art performance on two mainstream benchmark datasets, REAL275 and CAMERA25, and runs in real-time during inference (40FPS).</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数现有的类别级物体姿态估计方法致力于从点云模态学习物体类别信息。然而，由于 3D 数据收集和注释的成本高昂，3D 数据集的规模受到限制。因此，从这些有限的点云样本中提取的类别特征可能并不全面。这促使我们研究是否可以利用其他方式的知识来获取类别信息。受这一动机的启发，我们提出了 CLIPose，一种新颖的 6D 姿势框架，它采用预先训练的视觉语言模型来更好地学习对象类别信息，可以充分利用图像和文本模态中丰富的语义知识。为了使 3D 编码器更有效地学习特定类别的特征，我们通过多模态对比学习在特征空间中对齐三种模态的表示。除了利用 CLIP 模型的预训练知识之外，我们还期望它对姿态参数更加敏感。因此，我们引入了一种即时调整方法来微调图像编码器，同时将旋转和平移信息合并到文本描述中。 CLIPose 在两个主流基准数据集 REAL275 和 CAMERA25 上实现了最先进的性能，并在推理过程中实时运行 (40FPS)。</details>
**PDF:** <http://arxiv.org/pdf/2402.15726v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DeepLight: Reconstructing High-Resolution Observations of Nighttime Light With Multi-Modal Remote Sensing Data**<br />
**Title_cn:** DeepLight：利用多模态遥感数据重建夜间光的高分辨率观测<br />
**Authors:** Lixian Zhang, Runmin Dong, Shuai Yuan, Jinxiao Zhang, Mengxuan Chen, Juepeng Zheng, Haohuan Fu<br />
**Abstract:** <details><summary>原文: </summary>Nighttime light (NTL) remote sensing observation serves as a unique proxy for quantitatively assessing progress toward meeting a series of Sustainable Development Goals (SDGs), such as poverty estimation, urban sustainable development, and carbon emission. However, existing NTL observations often suffer from pervasive degradation and inconsistency, limiting their utility for computing the indicators defined by the SDGs. In this study, we propose a novel approach to reconstruct high-resolution NTL images using multi-modal remote sensing data. To support this research endeavor, we introduce DeepLightMD, a comprehensive dataset comprising data from five heterogeneous sensors, offering fine spatial resolution and rich spectral information at a national scale. Additionally, we present DeepLightSR, a calibration-aware method for building bridges between spatially heterogeneous modality data in the multi-modality super-resolution. DeepLightSR integrates calibration-aware alignment, an auxiliary-to-main multi-modality fusion, and an auxiliary-embedded refinement to effectively address spatial heterogeneity, fuse diversely representative features, and enhance performance in $8\times$ super-resolution (SR) tasks. Extensive experiments demonstrate the superiority of DeepLightSR over 8 competing methods, as evidenced by improvements in PSNR (2.01 dB $ \sim $ 13.25 dB) and PIQE (0.49 $ \sim $ 9.32). Our findings underscore the practical significance of our proposed dataset and model in reconstructing high-resolution NTL data, supporting efficiently and quantitatively assessing the SDG progress.</details>
**Abstract_cn:** <details><summary>译文: </summary>夜间灯光（NTL）遥感观测是定量评估实现贫困估计、城市可持续发展和碳排放等一系列可持续发展目标（SDG）进展情况的独特指标。然而，现有的 NTL 观测结果往往普遍退化和不一致，限制了它们计算可持续发展目标定义的指标的效用。在这项研究中，我们提出了一种使用多模态遥感数据重建高分辨率 NTL 图像的新方法。为了支持这项研究工作，我们引入了 DeepLightMD，这是一个综合数据集，包含来自五个异构传感器的数据，在全国范围内提供精细的空间分辨率和丰富的光谱信息。此外，我们还提出了 DeepLightSR，一种校准感知方法，用于在多模态超分辨率中的空间异构模态数据之间建立桥梁。 DeepLightSR 集成了校准感知对齐、辅助到主多模态融合和辅助嵌入细化，可有效解决空间异质性、融合不同的代表性特征，并增强 8 倍超分辨率 (SR) 任务的性能。大量实验证明了 DeepLightSR 相对于 8 种竞争方法的优越性，PSNR (2.01 dB $ \sim $ 13.25 dB) 和 PIQE (0.49 $ \sim $ 9.32) 的改进证明了这一点。我们的研究结果强调了我们提出的数据集和模型在重建高分辨率 NTL 数据方面的实际意义，支持有效、定量地评估可持续发展目标的进展。</details>
**PDF:** <http://arxiv.org/pdf/2402.15659v1><br />
**Code:** null<br />

>## **3DGS**
>---
>>**index:** 1<br />
**Title:** **Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting**<br />
**Title_cn:** Spec-Gaussian：3D 高斯泼溅的各向异性视图相关外观<br />
**Authors:** Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin<br />
**Abstract:** <details><summary>原文: </summary>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 高斯喷射 (3D-GS) 的最新进展不仅促进了通过现代 GPU 光栅化管道的实时渲染，而且还获得了最先进的渲染质量。然而，尽管 3D-GS 在标准数据集上具有出色的渲染质量和性能，但在精确建模镜面反射和各向异性组件时经常遇到困难。这个问题源于球谐函数（SH）表示高频信息的能力有限。为了克服这一挑战，我们引入了 Spec-Gaussian，这是一种利用各向异性球面高斯 (ASG) 外观场而不是 SH 来对每个 3D 高斯的视图相关外观进行建模的方法。此外，我们还开发了从粗到精的训练策略，以提高学习效率并消除现实场景中因过度拟合而导致的漂浮物。我们的实验结果表明，我们的方法在渲染质量方面超越了现有方法。借助 ASG，我们显着提高了 3D-GS 对具有镜面反射和各向异性分量的场景进行建模的能力，而无需增加 3D 高斯的数量。这一改进扩展了 3D GS 的适用性，可以处理具有镜面和各向异性表面的复杂场景。</details>
**PDF:** <http://arxiv.org/pdf/2402.15870v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **DART: Depth-Enhanced Accurate and Real-Time Background Matting**<br />
**Title_cn:** DART：深度增强的准确实时背景抠图<br />
**Authors:** Hanxi Li, Guofeng Li, Bo Li, Lin Wu, Yan Cheng<br />
**Abstract:** <details><summary>原文: </summary>Matting with a static background, often referred to as ``Background Matting" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows.   In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model's output undergoes refinement through Bayesian inference, incorporating a background depth prior. The posterior prediction is then translated into a "trimap," which is subsequently fed into a state-of-the-art matting algorithm to generate more precise alpha mattes. To ensure real-time matting capabilities, a critical requirement for many real-world applications, we distill the backbone of our model from a larger and more versatile BGM network. Our experiments demonstrate the superior performance of the proposed method. Moreover, thanks to the distillation operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device. This high efficiency underscores DART's immense potential for deployment in mobile applications}</details>
**Abstract_cn:** <details><summary>译文: </summary>静态背景抠图，通常被称为“背景抠图”（BGM），由于其在网络广播和照片编辑等各种实际应用中的关键作用，在计算机视觉界引起了极大的关注。然而，实现高精度的背景抠图仍然是一个艰巨的挑战，主要是由于传统 RGB 图像固有的局限性。这些局限性表现为对不同的照明条件和不可预见的阴影的敏感性。在本文中，我们利用 RGB-Depth (RGB -D) 摄像头实时增强背景抠图性能，称为 DART。首先，我们采用原始的基于 RGB 的 BGM 算法来合并深度信息。最终模型的输出通过贝叶斯推理进行细化，并合并背景深度先验。然后后验预测被转换为“三元图”，随后将其输入最先进的抠图算法以生成更精确的 alpha 遮罩。为了确保实时抠图功能（这是许多实际应用程序的关键要求），我们从更大、更通用的 BGM 网络中提取了模型的骨干。我们的实验证明了所提出方法的优越性能。此外，由于蒸馏操作，我们的方法在中端边缘计算设备上实现了每秒 33 帧 (fps) 的卓越处理速度。这种高效率凸显了 DART 在移动应用程序中部署的巨大潜力}</details>
**PDF:** <http://arxiv.org/pdf/2402.15820v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification**<br />
**Title_cn:** 宫颈癌分类的可解释对比和成本敏感学习<br />
**Authors:** Ashfiqun Mustari, Rushmia Ahmed, Afsara Tasnim, Jakia Sultana Juthi, G M Shahariar<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种使用预先训练的卷积神经网络（CNN）对宫颈癌细胞进行分类的有效系统。我们首先对五个预训练的 CNN 进行微调，并通过优先考虑具有较高相关成本或重要性的某些类别的准确性，最大限度地降低错误分类的总体成本。为了进一步提高模型的性能，引入了监督对比学习，使模型更善于捕获重要的特征和模式。为了在 SIPaKMeD 数据集上评估所提出的系统，进行了广泛的实验。实验结果证明了所开发系统的有效性，准确率达到97.29%。为了使我们的系统更值得信赖，我们采用了几种可解释的人工智能技术来解释模型如何做出特定决策。该系统的实现可以在 https://github.com/isha-67/CervicalCancerStudy 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.15905v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Multi-Object Tracking by Hierarchical Visual Representations**<br />
**Title_cn:** 通过分层视觉表示进行多目标跟踪<br />
**Authors:** Jinkun Cao, Jiangmiao Pang, Kris Kitani<br />
**Abstract:** <details><summary>原文: </summary>We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects' compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种用于多目标跟踪的新的视觉分层表示范例。通过关注对象的组成视觉区域并与背景上下文信息进行对比，而不是仅坚持诸如边界框之类的语义视觉线索，可以更有效地区分对象。这种组合-语义-上下文层次结构可以灵活地集成到不同的基于外观的多对象跟踪方法中。我们还提出了一种基于注意力的视觉特征模块来融合分层视觉表示。该方法在多个多目标跟踪基准上实现了基于查询的方法中最先进的准确性和时间效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.15895v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-graph Graph Matching for Coronary Artery Semantic Labeling**<br />
**Title_cn:** 冠状动脉语义标记的多图图形匹配<br />
**Authors:** Chen Zhao, Zhihui Xu, Pukar Baral, Michel Esposito, Weihua Zhou<br />
**Abstract:** <details><summary>原文: </summary>Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information. However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches. To address this challenge, we model the vascular tree as a graph and propose a multi-graph graph matching (MGM) algorithm for coronary artery semantic labeling. The MGM algorithm assesses the similarity between arterials in multiple vascular tree graphs, taking into account the cycle consistency between each pair of graphs. This ensures that unannotated arterial segments are appropriately labeled by matching them with annotated segments. Through the incorporation of anatomical graph structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling. This approach presents a novel tool for coronary artery analysis using ICA videos, offering valuable insights into vascular health and pathology.</details>
**Abstract_cn:** <details><summary>译文: </summary>冠状动脉疾病 (CAD) 是全球死亡的主要原因，而侵入性冠状动脉造影 (ICA) 仍然是评估血管解剖信息的金标准。然而，基于深度学习的方法在为动脉段生成语义标签时遇到了挑战，这主要是由于动脉分支之间的形态相似性。为了应对这一挑战，我们将血管树建模为图，并提出了一种用于冠状动脉语义标记的多图图匹配（MGM）算法。 MGM 算法评估多个血管树图中动脉之间的相似性，同时考虑每对图之间的循环一致性。这确保通过将未注释的动脉段与注释的段匹配来适当地标记它们。通过结合解剖图结构、放射组学特征和语义映射，所提出的 MGM 模型在冠状动脉语义标记方面实现了令人印象深刻的 0.9471 准确度。这种方法提供了一种使用 ICA 视频进行冠状动脉分析的新工具，为血管健康和病理学提供了宝贵的见解。</details>
**PDF:** <http://arxiv.org/pdf/2402.15894v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study**<br />
**Title_cn:** 使用苏木精和曙红全幻灯片图像进行神经胶质瘤诊断的多实例学习：一项印度队列研究<br />
**Authors:** Ekansh Chauhan, Amit Sharma, Megha S Uppin, C. V. Jawahar, Vinod P. K<br />
**Abstract:** <details><summary>原文: </summary>Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies. This study advances patient care with findings from rigorous multiple-instance-learning experimentations across various feature extractors and aggregators in brain tumor histopathology. It establishes new performance benchmarks in glioma subtype classification across multiple datasets, including a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research. Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification. Moreover, it establishes new benchmarks in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&E stained whole slide images for the IPD-Brain dataset. The work also highlights a significant correlation between the model decision-making processes and the diagnostic reasoning of pathologists, underscoring its capability to mimic professional diagnostic procedures.</details>
**Abstract_cn:** <details><summary>译文: </summary>脑肿瘤是一种严重且危及生命的疾病，需要精确的诊断和量身定制的治疗策略。这项研究利用脑肿瘤组织病理学中各种特征提取器和聚合器的严格多实例学习实验的结果来推进患者护理。它为多个数据集的神经胶质瘤亚型分类建立了新的性能基准，包括一个专注于印度人口统计的新数据集 (IPD-Brain)，为现有研究提供了宝贵的资源。使用在组织病理学数据集上预训练的 ResNet-50 进行特征提取，结合 DTFD 特征聚合器，我们的方法在 IPD-Brain 上实现了最先进的 AUC 88.08，在 TCGA-Brain 数据集上实现了三向 AUC 95.81神经胶质瘤亚型分类。此外，它通过 IPD-Brain 数据集的 H&E 染色整个幻灯片图像，在分级和检测 IHC 分子生物标志物（IDH1（突变体 R132H）、TP53、ATRX、Ki-67）方面建立了新的基准。这项工作还强调了模型决策过程与病理学家的诊断推理之间的显着相关性，强调了其模仿专业诊断程序的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.15832v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition**<br />
**Title_cn:** 半监督文本识别的顺序视觉和语义一致性<br />
**Authors:** Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景文本识别（STR）是一项具有挑战性的任务，需要大规模的注释数据进行训练。然而，收集和标记真实文本图像既昂贵又耗时，这限制了真实数据的可用性。因此，大多数现有的 STR 方法都采用合成数据，这可能会引入域差异并降低 STR 模型的性能。为了缓解这个问题，最近的半监督 STR 方法通过在同一图像的弱增强视图和强增强视图之间强制执行字符级一致性正则化来利用未标记的真实数据。然而，这些方法忽略了词级一致性，这对于序列识别任务至关重要。本文提出了一种新颖的 STR 半监督学习方法，该方法从视觉和语义方面结合了词级一致性正则化。具体来说，我们设计了一个最短路径对齐模块来对齐不同视图的顺序视觉特征并最小化它们的距离。此外，我们采用强化学习框架来优化嵌入空间中预测字符串的语义相似度。我们对几个标准和具有挑战性的 STR 基准进行了广泛的实验，并证明了我们提出的方法相对于现有半监督 STR 方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15806v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer**<br />
**Title_cn:** IRConStyle：使用对比学习和风格迁移的图像恢复框架<br />
**Authors:** Dongqi Fan, Xin Zhao, Liang Chang<br />
**Abstract:** <details><summary>原文: </summary>Recently, the contrastive learning paradigm has achieved remarkable success in high-level tasks such as classification, detection, and segmentation. However, contrastive learning applied in low-level tasks, like image restoration, is limited, and its effectiveness is uncertain. This raises a question: Why does the contrastive learning paradigm not yield satisfactory results in image restoration? In this paper, we conduct in-depth analyses and propose three guidelines to address the above question. In addition, inspired by style transfer and based on contrastive learning, we propose a novel module for image restoration called \textbf{ConStyle}, which can be efficiently integrated into any U-Net structure network. By leveraging the flexibility of ConStyle, we develop a \textbf{general restoration network} for image restoration. ConStyle and the general restoration network together form an image restoration framework, namely \textbf{IRConStyle}. To demonstrate the capability and compatibility of ConStyle, we replace the general restoration network with transformer-based, CNN-based, and MLP-based networks, respectively. We perform extensive experiments on various image restoration tasks, including denoising, deblurring, deraining, and dehazing. The results on 19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based network and significantly enhance performance. For instance, ConStyle NAFNet significantly outperforms the original NAFNet on SOTS outdoor (dehazing) and Rain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB with 85% fewer parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，对比学习范式在分类、检测和分割等高级任务中取得了显着的成功。然而，应用于图像恢复等低级任务的对比学习是有限的，并且其有效性不确定。这就提出了一个问题：为什么对比学习范式在图像恢复方面没有产生令人满意的结果？在本文中，我们进行了深入分析并提出了三个指导方针来解决上述问题。此外，受风格迁移的启发并基于对比学习，我们提出了一种新颖的图像恢复模块，称为 \textbf{ConStyle}，它可以有效地集成到任何 U-Net 结构网络中。通过利用 ConStyle 的灵活性，我们开发了一个用于图像恢复的 \textbf{通用恢复网络}。 ConStyle和通用恢复网络一起构成了图像恢复框架，即\textbf{IRConStyle}。为了证明 ConStyle 的功能和兼容性，我们分别用基于 Transformer 的网络、基于 CNN 的网络和基于 MLP 的网络替换了一般的恢复网络。我们对各种图像恢复任务进行了广泛的实验，包括去噪、去模糊、去雨和去雾。 19 项基准测试的结果表明，ConStyle 可以与任何基于 U-Net 的网络集成，并显着提高性能。例如，ConStyle NAFNet 在 SOTS 户外（除雾）和 Rain100H（除雨）数​​据集上的性能显着优于原始 NAFNet，PSNR 提高了 4.16 dB 和 3.58 dB，参数减少了 85%。</details>
**PDF:** <http://arxiv.org/pdf/2402.15784v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning**<br />
**Title_cn:** Res-VMamba：使用选择性状态空间模型和深度残差学习进行细粒度食品类别视觉分类<br />
**Authors:** Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, Di Jiang, Dai-Shi Chen<br />
**Abstract:** <details><summary>原文: </summary>Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design. The research results show that VMamba surpasses current SOTA models in fine-grained and food classification. The proposed Res-VMamba further improves the classification accuracy to 79.54\% without pretrained weight. Our findings elucidate that our proposed methodology establishes a new benchmark for SOTA performance in food recognition on the CNFOOD-241 dataset. The code can be obtained on GitHub: https://github.com/ChiShengChen/ResVMamba.</details>
**Abstract_cn:** <details><summary>译文: </summary>食物分类是开发食物视觉任务的基础，在新兴的计算营养领域发挥着关键作用。由于食品的复杂性需要细粒度的分类，最近的学术研究主要修改卷积神经网络（CNN）和/或视觉变换器（ViT）来进行食品类别分类。然而，为了学习细粒度的特征，CNN主干需要额外的结构设计，而包含自注意力模块的ViT增加了计算复杂度。近几个月来，一种新的序列状态空间 (S4) 模型，通过选择机制和扫描 (S6) 计算（俗称 Mamba），与 Transformer 架构相比，展现了卓越的性能和计算效率。 VMamba模型将Mamba机制融入到图像任务（例如分类）中，目前在ImageNet数据集上建立了state-of-the-art（SOTA）。在这项研究中，我们引入了学术上被低估的食物数据集 CNFOOD-241，并开创性地将残差学习框架集成到 VMamba 模型中，以同时利用原始 VMamba 架构设计中固有的全局和局部状态特征。研究结果表明，VMamba 在细粒度和食品分类方面超越了当前的 SOTA 模型。所提出的 Res-VMamba 在没有预训练权重的情况下进一步将分类精度提高到 79.54%。我们的研究结果表明，我们提出的方法为 CNFOOD-241 数据集上食品识别的 SOTA 性能建立了新的基准。代码可以在GitHub上获取：https://github.com/ChiShengChen/ResVMamba。</details>
**PDF:** <http://arxiv.org/pdf/2402.15761v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models Revisited**<br />
**Title_cn:** 检测即跟踪：重新审视点云多重扫描深度学习模型<br />
**Authors:** Lingji Chen<br />
**Abstract:** <details><summary>原文: </summary>Conventional tracking paradigm takes in instantaneous measurements such as range and bearing, and produces object tracks across time. In applications such as autonomous driving, lidar measurements in the form of point clouds are usually passed through a "virtual sensor" realized by a deep learning model, to produce "measurements" such as bounding boxes, which are in turn ingested by a tracking module to produce object tracks. Very often multiple lidar sweeps are accumulated in a buffer to merge and become the input to the virtual sensor. We argue in this paper that such an input already contains temporal information, and therefore the virtual sensor output should also contain temporal information, not just instantaneous values for the time corresponding to the end of the buffer. In particular, we present the deep learning model called MULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object, a pair of bounding boxes at both the end time and the beginning time of the input buffer. This is achieved with fairly straightforward changes in commonly used lidar detection models, and with only marginal extra processing, but the resulting symmetry is satisfying. Such paired detections make it possible not only to construct rudimentary trackers fairly easily, but also to construct more sophisticated trackers that can exploit the extra information conveyed by the pair and be robust to choices of motion models and object birth/death models. We have conducted preliminary training and experimentation using Waymo Open Dataset, which shows the efficacy of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的跟踪范例采用瞬时测量（例如距离和方位），并生成随时间变化的对象轨迹。在自动驾驶等应用中，点云形式的激光雷达测量通常通过深度学习模型实现的“虚拟传感器”，以产生诸如边界框之类的“测量结果”，然后由跟踪模块摄取产生对象轨迹。通常，多个激光雷达扫描会累积在缓冲区中以合并并成为虚拟传感器的输入。我们在本文中认为，这样的输入已经包含时间信息，因此虚拟传感器输出也应该包含时间信息，而不仅仅是与缓冲区末尾相对应的时间的瞬时值。特别是，我们提出了称为多扫描配对检测器（MULSPAD）的深度学习模型，它为每个检测到的对象在输入缓冲区的结束时间和开始时间生成一对边界框。这是通过对常用激光雷达检测模型进行相当简单的更改来实现的，并且仅进行边际额外处理，但所得的对称性令人满意。这种配对检测不仅可以相当容易地构建基本的跟踪器，而且还可以构建更复杂的跟踪器，这些跟踪器可以利用配对传递的额外信息，并对运动模型和对象出生/死亡模型的选择具有鲁棒性。我们使用 Waymo 开放数据集进行了初步训练和实验，这表明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.15756v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **GiMeFive: Towards Interpretable Facial Emotion Classification**<br />
**Title_cn:** GiMeFive：迈向可解释的面部情绪分类<br />
**Authors:** Jiawen Wang, Leah Kawka<br />
**Abstract:** <details><summary>原文: </summary>Deep convolutional neural networks have been shown to successfully recognize facial emotions for the past years in the realm of computer vision. However, the existing detection approaches are not always reliable or explainable, we here propose our model GiMeFive with interpretations, i.e., via layer activations and gradient-weighted class activation mapping. We compare against the state-of-the-art methods to classify the six facial emotions. Empirical results show that our model outperforms the previous methods in terms of accuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated FER GiMeFive. Furthermore, we explain our work in real-world image and video examples, as well as real-time live camera streams. Our code and supplementary material are available at https: //github.com/werywjw/SEP-CVDL.</details>
**Abstract_cn:** <details><summary>译文: </summary>过去几年，深度卷积神经网络在计算机视觉领域已被证明可以成功识别面部情绪。然而，现有的检测方法并不总是可靠或可解释的，我们在这里提出了带有解释的模型 GiMeFive，即通过层激活和梯度加权类激活映射。我们与最先进的方法进行比较，对六种面部情绪进行分类。实证结果表明，我们的模型在两个面部情绪识别 (FER) 基准和我们聚合的 FER GiMeFive 的准确性方面优于以前的方法。此外，我们还通过现实世界的图像和视频示例以及实时摄像头流来解释我们的工作。我们的代码和补充材料可在 https://github.com/werywjw/SEP-CVDL 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15662v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation**<br />
**Title_cn:** RAUCA：通过强大而准确的伪装生成对车辆探测器进行新型物理对抗攻击<br />
**Authors:** Jiawei Zhou, Linye Lyu, Daojing He, Yu Li<br />
**Abstract:** <details><summary>原文: </summary>Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle, resulting in suboptimal attack performance. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both simulation and real-world settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>对抗性伪装因其在多视图攻击性能方面的优越性而被广泛用于针对车辆探测器的物理攻击。一种有前途的方法涉及使用可微神经渲染器通过梯度反向传播促进对抗性伪装优化。然而，现有的方法通常难以在渲染过程中捕获环境特征或产生可以精确映射到目标车辆的对抗性纹理，从而导致攻击性能不佳。此外，这些方法忽略了不同的天气条件，降低了在不同天气场景下生成的伪装的效果。为了应对这些挑战，我们提出了一种稳健且准确的迷彩生成方法，即 RAUCA。 RAUCA的核心是一种新颖的神经渲染组件Neural Renderer Plus (NRP)，它可以精确地投影车辆纹理并渲染具有光照、天气等环境特征的图像。此外，我们集成了用于伪装生成的多天气数据集，利用 NRP 来增强攻击鲁棒性。六种流行的物体检测器的实验结果表明，RAUCA 在模拟和现实环境中始终优于现有方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.15853v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation**<br />
**Title_cn:** NaVid：基于视频的 VLM 计划视觉和语言导航的下一步<br />
**Authors:** Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He<br />
**Abstract:** <details><summary>原文: </summary>Vision-and-Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision-making and instruction following. We train NaVid with 550k navigation samples collected from VLN-CE trajectories, including action-planning and instruction-reasoning samples, along with 665k large-scale web data. Extensive experiments show that NaVid achieves SOTA performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉和语言导航（VLN）是嵌入式人工智能的一个关键研究问题，旨在使智能体能够按照语言指令在看不见的环境中进行导航。在这个领域，无论是对于分布外的场景还是从模拟到真实的泛化都是一个长期存在的挑战。在本文中，我们提出了 NaVid，一种基于视频的大视觉语言模型（VLM），以缩小这种泛化差距。 NaVid 首次努力展示 VLM 无需任何地图、里程表和深度输入即可实现最先进水平导航性能的能力。按照人类指令，NaVid 只需要来自机器人配备的单目 RGB 摄像头的动态视频流即可输出下一步动作。我们的公式模仿了人类的导航方式，自然地消除了里程表噪音以及地图或深度输入的 Sim2Real 间隙带来的问题。此外，我们基于视频的方法可以有效地将机器人的历史观察结果编码为决策和遵循指令的时空上下文。我们使用从 VLN-CE 轨迹收集的 55 万个导航样本（包括行动规划和指令推理样本）以及 66.5 万个大规模网络数据来训练 NaVid。大量实验表明，NaVid 在仿真环境和现实世界中实现了 SOTA 性能，展示了卓越的跨数据集和 Sim2Real 传输能力。因此，我们相信我们提出的 VLM 方法不仅为导航代理而且为该研究领域规划了下一步。</details>
**PDF:** <http://arxiv.org/pdf/2402.15852v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Design, Implementation and Analysis of a Compressed Sensing Photoacoustic Projection Imaging System**<br />
**Title_cn:** 压缩感知光声投影成像系统的设计、实现与分析<br />
**Authors:** Markus Haltmeier, Matthias Ye, Karoline Felbermayer, Florian Hinterleitner, Peter Burgholzer<br />
**Abstract:** <details><summary>原文: </summary>Significance: Compressed sensing (CS) uses special measurement designs combined with powerful mathematical algorithms to reduce the amount of data to be collected while maintaining image quality. This is relevant to almost any imaging modality, and in this paper we focus on CS in photoacoustic projection imaging (PAPI) with integrating line detectors (ILDs).   Aim: Our previous research involved rather general CS measurements, where each ILD can contribute to any measurement. In the real world, however, the design of CS measurements is subject to practical constraints. In this research, we aim at a CS-PAPI system where each measurement involves only a subset of ILDs, and which can be implemented in a cost-effective manner.   Approach: We extend the existing PAPI with a self-developed CS unit. The system provides structured CS matrices for which the existing recovery theory cannot be applied directly. A random search strategy is applied to select the CS measurement matrix within this class for which we obtain exact sparse recovery.   Results: We implement a CS PAPI system for a compression factor of $4:3$, where specific measurements are made on separate groups of 16 ILDs. We algorithmically design optimal CS measurements that have proven sparse CS capabilities. Numerical experiments are used to support our results.   Conclusions: CS with proven sparse recovery capabilities can be integrated into PAPI, and numerical results support this setup. Future work will focus on applying it to experimental data and utilizing data-driven approaches to enhance the compression factor and generalize the signal class.</details>
**Abstract_cn:** <details><summary>译文: </summary>意义：压缩感知（CS）使用特殊的测量设计与强大的数学算法相结合，在保持图像质量的同时减少要收集的数据量。这几乎与所有成像方式相关，在本文中，我们重点关注具有集成线探测器 (ILD) 的光声投影成像 (PAPI) 中的 CS。目标：我们之前的研究涉及相当一般的 CS 测量，其中每个 ILD 都可以对任何测量做出贡献。然而，在现实世界中，CS 测量的设计受到实际限制。在这项研究中，我们的目标是建立一个 CS-PAPI 系统，其中每次测量仅涉及 ILD 的一个子集，并且可以以经济有效的方式实施。方法：我们用自主开发的 CS 单元扩展现有的 PAPI。该系统提供了现有恢复理论无法直接应用的结构化CS矩阵。应用随机搜索策略来选择此类中的 CS 测量矩阵，为此我们获得了精确的稀疏恢复。结果：我们实施了压缩系数为 4:3 的 CS PAPI 系统，其中对 16 个 ILD 的不同组进行了特定测量。我们通过算法设计最佳 CS 测量，并已证明稀疏 CS 功能。数值实验用于支持我们的结果。结论：具有经过验证的稀疏恢复能力的 CS 可以集成到 PAPI 中，并且数值结果支持这种设置。未来的工作将集中于将其应用于实验数据，并利用数据驱动的方法来增强压缩因子并概括信号类别。</details>
**PDF:** <http://arxiv.org/pdf/2402.15750v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Traditional Transformation Theory Guided Model for Learned Image Compression**<br />
**Title_cn:** 传统变换理论指导的学习图像压缩模型<br />
**Authors:** Zhiyuan Li, Chenyang Ge, Shun Li<br />
**Abstract:** <details><summary>原文: </summary>Recently, many deep image compression methods have been proposed and achieved remarkable performance. However, these methods are dedicated to optimizing the compression performance and speed at medium and high bitrates, while research on ultra low bitrates is limited. In this work, we propose a ultra low bitrates enhanced invertible encoding network guided by traditional transformation theory, experiments show that our codec outperforms existing methods in both compression and reconstruction performance. Specifically, we introduce the Block Discrete Cosine Transformation to model the sparsity of features and employ traditional Haar transformation to improve the reconstruction performance of the model without increasing the bitstream cost.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，许多深度图像压缩方法被提出并取得了显着的性能。然而，这些方法致力于优化中高码率下的压缩性能和速度，而对超低码率的研究有限。在这项工作中，我们提出了一种以传统变换理论为指导的超低比特率增强可逆编码网络，实验表明我们的编解码器在压缩和重建性能方面都优于现有方法。具体来说，我们引入块离散余弦变换来对特征的稀疏性进行建模，并采用传统的哈尔变换来提高模型的重建性能，而不增加比特流成本。</details>
**PDF:** <http://arxiv.org/pdf/2402.15744v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution**<br />
**Title_cn:** 一种用于图像超分辨率的异构动态卷积神经网络<br />
**Authors:** Chunwei Tian, Xuanyu Zhang, Jia Ren, Wangmeng Zuo, Yanning Zhang, Chia-Wen Lin<br />
**Abstract:** <details><summary>原文: </summary>Convolutional neural networks can automatically learn features via deep network architectures and given input samples. However, robustness of obtained models may have challenges in varying scenes. Bigger differences of a network architecture are beneficial to extract more complementary structural information to enhance robustness of an obtained super-resolution model. In this paper, we present a heterogeneous dynamic convolutional network in image super-resolution (HDSRNet). To capture more information, HDSRNet is implemented by a heterogeneous parallel network. The upper network can facilitate more contexture information via stacked heterogeneous blocks to improve effects of image super-resolution. Each heterogeneous block is composed of a combination of a dilated, dynamic, common convolutional layers, ReLU and residual learning operation. It can not only adaptively adjust parameters, according to different inputs, but also prevent long-term dependency problem. The lower network utilizes a symmetric architecture to enhance relations of different layers to mine more structural information, which is complementary with a upper network for image super-resolution. The relevant experimental results show that the proposed HDSRNet is effective to deal with image resolving. The code of HDSRNet can be obtained at https://github.com/hellloxiaotian/HDSRNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络可以通过深度网络架构和给定的输入样本自动学习特征。然而，所获得的模型的鲁棒性在不同场景中可能面临挑战。网络架构的较大差异有利于提取更多互补的结构信息，以增强所获得的超分辨率模型的鲁棒性。在本文中，我们提出了一种图像超分辨率的异构动态卷积网络（HDSRNet）。为了捕获更多信息，HDSRNet 通过异构并行网络实现。上层网络可以通过堆叠异构块来提供更多上下文信息，以提高图像超分辨率的效果。每个异构块由扩张的动态公共卷积层、ReLU 和残差学习操作的组合组成。它不仅可以根据不同的输入自适应地调整参数，还可以防止长期依赖问题。下层网络利用对称架构来增强不同层之间的关系，以挖掘更多的结构信息，这与上层网络的图像超分辨率互补。相关实验结果表明，所提出的HDSRNet能够有效地处理图像解析。 HDSRNet的代码可以在https://github.com/helloxiaotian/HDSRNet获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15704v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **General Purpose Image Encoder DINOv2 for Medical Image Registration**<br />
**Title_cn:** 用于医学图像配准的通用图像编码器 DINOv2<br />
**Authors:** Xinrui Song, Xuanang Xu, Pingkun Yan<br />
**Abstract:** <details><summary>原文: </summary>Existing medical image registration algorithms rely on either dataset specific training or local texture-based features to align images. The former cannot be reliably implemented without large modality-specific training datasets, while the latter lacks global semantics thus could be easily trapped at local minima. In this paper, we present a training-free deformable image registration method, DINO-Reg, leveraging a general purpose image encoder DINOv2 for image feature extraction. The DINOv2 encoder was trained using the ImageNet data containing natural images. We used the pretrained DINOv2 without any finetuning. Our method feeds the DINOv2 encoded features into a discrete optimizer to find the optimal deformable registration field. We conducted a series of experiments to understand the behavior and role of such a general purpose image encoder in the application of image registration. Combined with handcrafted features, our method won the first place in the recent OncoReg Challenge. To our knowledge, this is the first application of general vision foundation models in medical image registration.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的医学图像配准算法依赖于数据集特定的训练或基于局部纹理的特征来对齐图像。如果没有大型特定于模态的训练数据集，前者就无法可靠地实现，而后者缺乏全局语义，因此很容易陷入局部最小值。在本文中，我们提出了一种免训练的可变形图像配准方法 DINO-Reg，利用通用图像编码器 DINOv2 进行图像特征提取。 DINOv2 编码器使用包含自然图像的 ImageNet 数据进行训练。我们使用预训练的 DINOv2，无需任何微调。我们的方法将 DINOv2 编码特征输入离散优化器中，以找到最佳的可变形配准字段。我们进行了一系列实验来了解这种通用图像编码器在图像配准应用中的行为和作用。结合手工制作的特征，我们的方法在最近的 OncoReg 挑战赛中获得了第一名。据我们所知，这是通用视觉基础模型在医学图像配准中的首次应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.15687v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Scalable Density-based Clustering with Random Projections**<br />
**Title_cn:** 具有随机投影的可扩展的基于密度的聚类<br />
**Authors:** Haochuan Xu, Ninh Pham<br />
**Abstract:** <details><summary>原文: </summary>We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic clustering structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other clustering algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand several hours or cannot run due to memory constraints.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 sDBSCAN，一种具有余弦距离的高维可扩展的基于密度的聚类算法。利用随机投影的邻域保留特性，sDBSCAN 可以快速识别核心点及其邻域，这是基于密度的聚类的主要障碍。理论上，sDBSCAN 在温和条件下以高概率输出类似于 DBSCAN 的聚类结构。为了进一步促进 sDBSCAN，我们提出了 sOPTICS，这是一种可扩展的 OPTICS，用于交互式探索内在聚类结构。我们还通过随机核特征将 sDBSCAN 和 sOPTICS 扩展到 L2、L1、$\chi^2$ 和 Jensen-Shannon 距离。根据经验，在现实世界的百万点数据集上，sDBSCAN 比许多其他聚类算法要快得多，并且提供更高的准确性。在这些数据集上，sDBSCAN 和 sOPTICS 在几分钟内运行，而 scikit-learn 的对应项需要几个小时或由于内存限制而无法运行。</details>
**PDF:** <http://arxiv.org/pdf/2402.15679v1><br />
**Code:** null<br />

