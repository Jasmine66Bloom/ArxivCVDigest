## [UPDATED!] **2024-02-25** (Publish Time)

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis**<br />
**Title_cn:** RoboCodeX：用于机器人行为综合的多模式代码生成<br />
**Authors:** Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人行为综合，即理解多模态输入并为机器人生成精确的物理控制的问题，是体现人工智能的重要组成部分。尽管在应用多模态大语言模型进行高级理解方面取得了成功，但将这些概念性理解转化为详细的机器人动作，同时实现跨各种场景的泛化仍然具有挑战性。在本文中，我们提出了一种用于广义机器人行为综合的树结构多模式代码生成框架，称为 RoboCodeX。 RoboCodeX 将高级人类指令分解为多个以对象为中心的操作单元，其中包括可供性和安全约束等物理偏好，并应用代码生成来引入跨各种机器人平台的泛化能力。为了进一步增强将概念和感知理解映射到控制命令的能力，收集了专门的多模态推理数据集用于预训练，并引入了迭代自更新方法用于监督微调。大量实验表明，RoboCodeX 在模拟器和真实机器人中的四种不同类型的操作任务和一种导航任务中均实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.16117v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages**<br />
**Title_cn:** TMT：语音、图像和文本之间的三模态翻译，将不同模态处理为不同语言<br />
**Authors:** Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi Maiti, Siddhant Arora, Xuankai Chang, Shinji Watanabe, Yong Man Ro<br />
**Abstract:** <details><summary>原文: </summary>The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>联合处理多模态信息的能力正在成为一项重要任务。然而，配对多模态数据的数量有限以及多模态学习中大量的计算需求阻碍了发展。我们提出了一种新颖的三模态翻译（TMT）模型，可以在跨越语音、图像和文本的任意模态之间进行翻译。我们引入了一种新颖的观点，将不同的模态解释为不同的语言，并将多模态翻译视为一个成熟的机器翻译问题。为此，我们将语音和图像数据标记为离散标记，这提供了跨模态的统一接口，并显着降低了计算成本。在所提出的TMT中，多模态编码器-解码器进行核心翻译，而特定于模态的处理仅在标记化和去标记化阶段进行。我们在所有六种模态翻译任务上评估了所提出的 TMT。 TMT 的表现始终优于单一模型，这表明统一任务不仅有利于实用性，而且有利于性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.16021v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision**<br />
**Title_cn:** 通过掩蔽输入梯度揭示痴呆症检测：模型可解释性和精度的 JSM 方法<br />
**Authors:** Yasmine Mustafa, Tie Luo<br />
**Abstract:** <details><summary>原文: </summary>The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression. We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities. Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习和人工智能的发展极大地重塑了技术格局。然而，它们在医学等关键领域的有效应用不仅需要卓越的性能，还需要值得信赖。虽然可解释性起着关键作用，但现有的可解释人工智能（XAI）方法通常无法揭示“聪明的汉斯”行为，即模型利用数据中的虚假相关性或偏差做出（不可概括的）正确预测。同样，当前的事后 XAI 方法很容易生成不合理的反事实示例。在本文中，我们使用通过雅可比显着图（JSM）实现的创新{\em模型调试}方法来处理 XAI。为了将问题具体化，我们采用阿尔茨海默氏病 (AD) 诊断作为用例，其动机是其对人类生活的重大影响以及其早期检测的巨大挑战（源于其发展的复杂性）。我们引入了一种可解释的多模态模型，用于 AD 多阶段进展的分类，将 JSM 纳入模态不可知的工具，提供对指示大脑异常的体积变化的见解。我们的广泛评估（包括消融研究）证明了使用 JSM 进行模型调试和解释的有效性，同时也显着提高了模型的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.16008v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Towards Accurate Post-training Quantization for Reparameterized Models**<br />
**Title_cn:** 实现重新参数化模型的准确训练后量化<br />
**Authors:** Luoming Zhang, Yefei He, Wen Fei, Zhenyu Lou, Weijia Wu, YangWei Ying, Hong Zhou<br />
**Abstract:** <details><summary>原文: </summary>Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization combines multiple branches into a single convolution with an affine layer. During training, the affine layer accelerates convergence and amplifies the output of the convolution to better accommodate samples with outliers. Additionally, Across-block Calibration leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with quantization parameters. Comprehensive experiments demonstrate the effectiveness of RepAPQ across various models and tasks. Our framework outperforms previous methods by approximately 1\% for 8-bit PTQ and 2\% for 6-bit PTQ, showcasing its superior performance. The code is available at \url{https://github.com/ilur98/DLMC-QUANT}.</details>
**Abstract_cn:** <details><summary>译文: </summary>模型重新参数化是一种广泛接受的技术，可在不影响性能的情况下提高推理速度。然而，当前的训练后量化（PTQ）方法在应用于重新参数化模型时通常会导致精度显着下降。这主要是由特定于通道和特定于样本的异常值引起的，这些异常值仅出现在特定样本和通道上并影响量化参数的选择。为了解决这个问题，我们提出了 RepAPQ，这是一种新颖的框架，可以保留量化重参数化模型的准确性。与以前使用均方误差（MSE）作为测量值的框架不同，我们利用平均绝对误差（MAE）来减轻异常值对量化参数的影响。我们的框架包括两个主要组件：量化保护重新参数化和跨块校准。为了有效校准，量化保护重新参数化将多个分支组合成具有仿射层的单个卷积。在训练过程中，仿射层加速收敛并放大卷积的输出，以更好地适应具有异常值的样本。此外，跨块校准利用阶段输出的测量作为监督来解决 MAE 引入的梯度问题，并增强与量化参数的层间相关性。综合实验证明了 RepAPQ 在各种模型和任务中的有效性。我们的框架在 8 位 PTQ 上比以前的方法提高了大约 1%，在 6 位 PTQ 上比以前的方法提高了 2%，展示了其卓越的性能。该代码可在 \url{https://github.com/ilur98/DLMC-QUANT} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.16121v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging**<br />
**Title_cn:** 整合预处理方法和卷积神经网络以实现医学成像中的有效肿瘤检测<br />
**Authors:** Ha Anh Vu<br />
**Abstract:** <details><summary>原文: </summary>This research presents a machine-learning approach for tumor detection in medical images using convolutional neural networks (CNNs). The study focuses on preprocessing techniques to enhance image features relevant to tumor detection, followed by developing and training a CNN model for accurate classification. Various image processing techniques, including Gaussian smoothing, bilateral filtering, and K-means clustering, are employed to preprocess the input images and highlight tumor regions. The CNN model is trained and evaluated on a dataset of medical images, with augmentation and data generators utilized to enhance model generalization. Experimental results demonstrate the effectiveness of the proposed approach in accurately detecting tumors in medical images, paving the way for improved diagnostic tools in healthcare.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究提出了一种使用卷积神经网络（CNN）在医学图像中检测肿瘤的机器学习方法。该研究的重点是预处理技术，以增强与肿瘤检测​​相关的图像特征，然后开发和训练 CNN 模型以进行准确分类。采用各种图像处理技术，包括高斯平滑、双边滤波和 K 均值聚类，对输入图像进行预处理并突出显示肿瘤区域。 CNN 模型在医学图像数据集上进行训练和评估，并使用增强和数据生成器来增强模型泛化。实验结果证明了所提出的方法在准确检测医学图像中的肿瘤方面的有效性，为改进医疗保健诊断工具铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.16221v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **ARIN: Adaptive Resampling and Instance Normalization for Robust Blind Inpainting of Dunhuang Cave Paintings**<br />
**Title_cn:** ARIN：敦煌石窟壁画鲁棒盲修复的自适应重采样和实例归一化<br />
**Authors:** Alexander Schmidt, Prathmesh Madhu, Andreas Maier, Vincent Christlein, Ronak Kosti<br />
**Abstract:** <details><summary>原文: </summary>Image enhancement algorithms are very useful for real world computer vision tasks where image resolution is often physically limited by the sensor size. While state-of-the-art deep neural networks show impressive results for image enhancement, they often struggle to enhance real-world images. In this work, we tackle a real-world setting: inpainting of images from Dunhuang caves. The Dunhuang dataset consists of murals, half of which suffer from corrosion and aging. These murals feature a range of rich content, such as Buddha statues, bodhisattvas, sponsors, architecture, dance, music, and decorative patterns designed by different artists spanning ten centuries, which makes manual restoration challenging. We modify two different existing methods (CAR, HINet) that are based upon state-of-the-art (SOTA) super resolution and deblurring networks. We show that those can successfully inpaint and enhance these deteriorated cave paintings. We further show that a novel combination of CAR and HINet, resulting in our proposed inpainting network (ARIN), is very robust to external noise, especially Gaussian noise. To this end, we present a quantitative and qualitative comparison of our proposed approach with existing SOTA networks and winners of the Dunhuang challenge. One of the proposed methods HINet) represents the new state of the art and outperforms the 1st place of the Dunhuang Challenge, while our combination ARIN, which is robust to noise, is comparable to the 1st place. We also present and discuss qualitative results showing the impact of our method for inpainting on Dunhuang cave images.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像增强算法对于现实世界的计算机视觉任务非常有用，其中图像分辨率通常受到传感器尺寸的物理限制。虽然最先进的深度神经网络在图像增强方面显示出令人印象深刻的结果，但它们通常难以增强现实世界的图像。在这项工作中，我们解决了一个现实世界的场景：敦煌石窟图像的修复。敦煌数据集由壁画组成，其中一半遭受腐蚀和老化。这些壁画内容丰富，如佛像、菩萨、赞助人、建筑、舞蹈、音乐、装饰图案等，由跨越十个世纪的不同艺术家设计，这给手工修复带来了挑战。我们修改了两种不同的现有方法（CAR、HINet），它们基于最先进的（SOTA）超分辨率和去模糊网络。我们证明这些可以成功修复和增强这些恶化的洞穴壁画。我们进一步表明，CAR 和 HINet 的新颖组合，产生了我们提出的修复网络（ARIN），对外部噪声（尤其是高斯噪声）非常鲁棒。为此，我们对我们提出的方法与现有 SOTA 网络和敦煌挑战赛的获胜者进行了定量和定性比较。所提出的方法之一（HINet）代表了最新的技术水平，超过了敦煌挑战赛的第一名，而我们的组合 ARIN 对噪声具有鲁棒性，可与第一名相媲美。我们还展示并讨论了定性结果，显示了我们的修复方法对敦煌洞穴图像的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.16188v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MoodCapture: Depression Detection Using In-the-Wild Smartphone Images**<br />
**Title_cn:** MoodCapture：使用野外智能手机图像检测抑郁症<br />
**Authors:** Subigya Nepal, Arvind Pillai, Weichen Wang, Tess Griffin, Amanda C. Collins, Michael Heinz, Damien Lekkas, Shayan Mirjafari, Matthew Nemesure, George Price, et.al.<br />
**Abstract:** <details><summary>原文: </summary>MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives. We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days. Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: \textit{``I have felt down, depressed, or hopeless''}. Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting. We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment. Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools.</details>
**Abstract_cn:** <details><summary>译文: </summary>MoodCapture 提出了一种新颖的方法，可以根据人们日常生活中智能手机前置摄像头自动捕捉的图像来评估抑郁症。我们收集了 90 天内 N=177 名被诊断患有重度抑郁症的参与者的 125,000 多张野外照片。当参与者回答 PHQ-8 抑郁症调查问题时，图像被自然捕捉：\textit{``我感到沮丧、沮丧或绝望''}。我们的分析探讨了重要的图像属性，例如角度、主色、位置、对象和照明。我们证明，使用面部标志训练的随机森林可以将样本分类为抑郁或非抑郁，并有效预测原始 PHQ-8 分数。我们的事后分析通过消融研究、特征重要性分析和偏差评估提供了一些见解。重要的是，我们评估了用户对使用 MoodCapture 基于共享照片检测抑郁症的担忧，提供了对隐私问题的重要见解，为未来基于野外图像的心理健康评估工具的设计提供了信息。</details>
**PDF:** <http://arxiv.org/pdf/2402.16182v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **XAI-based gait analysis of patients walking with Knee-Ankle-Foot orthosis using video cameras**<br />
**Title_cn:** 使用摄像机对使用膝踝足矫形器行走的患者进行基于 XAI 的步态分析<br />
**Authors:** Arnav Mishra, Aditi Shetkar, Ganesh M. Bapat, Rajdeep Ojha, Tanmay Tulsidas Verlekar<br />
**Abstract:** <details><summary>原文: </summary>Recent technological advancements in artificial intelligence and computer vision have enabled gait analysis on portable devices such as cell phones. However, most state-of-the-art vision-based systems still impose numerous constraints for capturing a patient's video, such as using a static camera and maintaining a specific distance from it. While these constraints are manageable under professional observation, they pose challenges in home settings. Another issue with most vision-based systems is their output, typically a classification label and confidence value, whose reliability is often questioned by medical professionals. This paper addresses these challenges by presenting a novel system for gait analysis robust to camera movements and providing explanations for its output. The study utilizes a dataset comprising videos of subjects wearing two types of Knee Ankle Foot Orthosis (KAFO), namely "Locked Knee" and "Semi-flexion," for mobility, along with metadata and ground truth for explanations. The ground truth highlights the statistical significance of seven features captured using motion capture systems to differentiate between the two gaits. To address camera movement challenges, the proposed system employs super-resolution and pose estimation during pre-processing. It then identifies the seven features - Stride Length, Step Length and Duration of single support of orthotic and non-orthotic leg, Cadence, and Speed - using the skeletal output of pose estimation. These features train a multi-layer perceptron, with its output explained by highlighting the features' contribution to classification. While most state-of-the-art systems struggle with processing the video or training on the proposed dataset, our system achieves an average accuracy of 94%. The model's explainability is validated using ground truth and can be considered reliable.</details>
**Abstract_cn:** <details><summary>译文: </summary>人工智能和计算机视觉领域的最新技术进步使得对手机等便携式设备进行步态分析成为可能。然而，大多数最先进的基于视觉的系统仍然对捕获患者视频施加许多限制，例如使用静态摄像机并与其保持特定距离。虽然这些限制在专业观察下是可以控制的，但它们在家庭环境中提出了挑战。大多数基于视觉的系统的另一个问题是它们的输出，通常是分类标签和置信值，其可靠性经常受到医疗专业人员的质疑。本文通过提出一种新颖的步态分析系统来解决这些挑战，该系统对相机运动具有鲁棒性，并为其输出提供解释。该研究利用一个数据集，其中包括佩戴两种类型膝踝足矫形器（KAFO）的受试者的视频，即“锁定膝盖”和“半屈曲”，以实现移动性，以及用于解释的元数据和基本事实。地面事实强调了使用运动捕捉系统捕获的七个特征的统计显着性，以区分两种步态。为了解决相机运动挑战，所提出的系统在预处理过程中采用超分辨率和姿态估计。然后，它使用姿势估计的骨骼输出来识别七个特征 - 步长、步长和矫形和非矫形腿的单支撑持续时间、节奏和速度。这些特征训练多层感知器，其输出通过突出特征对分类的贡献来解释。虽然大多数最先进的系统都难以处理视频或对建议的数据集进行训练，但我们的系统实现了 94% 的平均准确率。该模型的可解释性使用地面实况进行了验证，可以认为是可靠的。</details>
**PDF:** <http://arxiv.org/pdf/2402.16175v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation**<br />
**Title_cn:** 用于遥感图像分割的带有噪声标签的任务特定预训练<br />
**Authors:** Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu<br />
**Abstract:** <details><summary>原文: </summary>In recent years, self-supervision has drawn a lot of attention in remote sensing society due to its ability to reduce the demand of exact labels in supervised deep learning model training. Self-supervision methods generally utilize image-level information to pretrain models in an unsupervised fashion. Though these pretrained encoders show effectiveness in many downstream tasks, their performance on segmentation tasks is often not as good as that on classification tasks. On the other hand, many easily available label sources (e.g., automatic labeling tools and land cover land use products) exist, which can provide a large amount of noisy labels for segmentation model training. In this work, we propose to explore the under-exploited potential of noisy labels for segmentation task specific pretraining, and exam its robustness when confronted with mismatched categories and different decoders during fine-tuning. Specifically, we inspect the impacts of noisy labels on different layers in supervised model training to serve as the basis of our work. Experiments on two datasets indicate the effectiveness of task specific supervised pretraining with noisy labels. The findings are expected to shed light on new avenues for improving the accuracy and versatility of pretraining strategies for remote sensing image segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，自监督因其能够减少监督深度学习模型训练中精确标签的需求而在遥感界引起了广泛关注。自监督方法通常利用图像级信息以无监督的方式预训练模型。尽管这些预训练编码器在许多下游任务中显示出有效性，但它们在分割任务上的性能通常不如在分类任务上的性能。另一方面，存在许多容易获得的标签源（例如自动标签工具和土地覆盖土地利用产品），可以为分割模型训练提供大量噪声标签。在这项工作中，我们建议探索噪声标签在分割任务特定预训练中未充分利用的潜力，并在微调过程中面对不匹配的类别和不同的解码器时检查其鲁棒性。具体来说，我们在监督模型训练中检查噪声标签对不同层的影响，作为我们工作的基础。对两个数据集的实验表明了使用噪声标签进行特定任务的监督预训练的有效性。这些发现有望为提高遥感图像分割预训练策略的准确性和多功能性提供新途径。</details>
**PDF:** <http://arxiv.org/pdf/2402.16164v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A statistical method for crack detection in 3D concrete images**<br />
**Title_cn:** 3D混凝土图像裂缝检测的统计方法<br />
**Authors:** Vitalii Makogin, Duc Nguyen, Evgeny Spodarev<br />
**Abstract:** <details><summary>原文: </summary>In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. However, classical methods and Machine Learning algorithms often incur high computational costs when dealing with the substantial size of input images. Hence, a robust algorithm is needed to pre-detect crack regions, enabling focused analysis and reducing computational overhead. The proposed approach addresses this challenge by offering a streamlined method for identifying crack regions in CT images with high probability. By efficiently identifying areas of interest, our algorithm allows for a more focused examination of potential anomalies within the material structure. Through comprehensive testing on both semi-synthetic and real 3D CT images, we validate the efficiency of our approach in enhancing crack segmentation while reducing computational resource requirements.</details>
**Abstract_cn:** <details><summary>译文: </summary>在实际应用中，有效分割大规模计算机断层扫描（CT）图像中的裂纹对于理解材料的结构完整性具有重要意义。然而，经典方法和机器学习算法在处理大量输入图像时通常会产生很高的计算成本。因此，需要一种强大的算法来预先检测裂纹区域，从而实现集中分析并减少计算开销。所提出的方法通过提供一种简化的方法来以高概率识别 CT 图像中的裂纹区域，从而解决了这一挑战。通过有效识别感兴趣的区域，我们的算法可以更集中地检查材料结构内的潜在异常情况。通过对半合成和真实 3D CT 图像进行全面测试，我们验证了我们的方法在增强裂纹分割同时减少计算资源需求方面的效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.16126v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis**<br />
**Title_cn:** 无源无监督域适应的关键设计选择：深入的实证分析<br />
**Authors:** Andrea Maracani, Raffaello Camoriano, Elisa Maiettini, Davide Talon, Lorenzo Rosasco, Lorenzo Natale<br />
**Abstract:** <details><summary>原文: </summary>This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain. Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance, serving as an essential reference and providing key insights. Lastly, we release the source code of our experimental framework. This facilitates the construction, training, and testing of SF-UDA methods, enabling systematic large-scale experimental analysis and supporting further research efforts in this field.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究为图像分类中的无源无监督域适应（SF-UDA）提供了一个全面的基准框架，旨在对 SF-UDA 方法中多个关键设计因素之间的复杂关系进行严格的实证理解。该研究实证检验了多种 SF-UDA 技术，评估其在数据集之间的一致性、对特定超参数的敏感性以及不同骨干架构系列的适用性。此外，它详尽地评估了预训练数据集和策略，特别关注监督和自监督方法，以及微调对源域的影响。我们的分析还强调了现有基准实践中的差距，指导 SF-UDA 研究采用更有效和通用的方法。它强调了主干架构和预训练数据集选择对 SF-UDA 性能的重要性，作为重要参考并提供关键见解。最后，我们发布了实验框架的源代码。这有助于 SF-UDA 方法的构建、培训和测试，实现系统的大规模实验分析并支持该领域的进一步研究工作。</details>
**PDF:** <http://arxiv.org/pdf/2402.16090v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Deep Homography Estimation for Visual Place Recognition**<br />
**Title_cn:** 用于视觉位置识别的深度单应性估计<br />
**Authors:** Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei Jiang, Chun Yuan<br />
**Abstract:** <details><summary>原文: </summary>Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉位置识别 (VPR) 是机器人定位和增强现实等许多应用的一项基本任务。最近，由于精度和效率之间的权衡，分层 VPR 方法受到了相当多的关注。他们通常首先使用全局特征来检索候选图像，然后验证匹配的局部特征的空间一致性以进行重新排序。然而，后者通常依赖于 RANSAC 算法来拟合单应性，该算法耗时且不可微。这使得现有方法只能在全局特征提取中训练网络。在这里，我们提出了一种基于变压器的深度单应性估计（DHE）网络，它将主干网络提取的密集特征图作为输入，并拟合单应性以进行快速且可学习的几何验证。此外，我们设计了inliers损失的重投影误差来训练DHE网络，而无需额外的单应性标签，它也可以与骨干网络联合训练，以帮助其提取更适合局部匹配的特征。对基准数据集的大量实验表明，我们的方法可以优于几种最先进的方法。并且比使用RANSAC的主流分层VPR方法快一个数量级以上。代码发布于https://github.com/Lu-Feng/DHE-VPR。</details>
**PDF:** <http://arxiv.org/pdf/2402.16086v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving**<br />
**Title_cn:** 基于机器学习的自动驾驶车辆意图轨迹识别与预测<br />
**Authors:** Hanyi Yu, Shuning Huo, Mengran Zhu, Yulu Gong, Yafei Xiang<br />
**Abstract:** <details><summary>原文: </summary>In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology. Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles. However, this period has also witnessed several traffic safety incidents involving self-driving vehicles. For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus. At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change. It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus. This incident highlights the current technological shortcomings and safety concerns associated with autonomous lane-changing behavior, despite the rapid advancements in autonomous driving technology. Lane-changing is among the most common and hazardous behaviors in highway driving, significantly impacting traffic safety and flow. Therefore, lane-changing is crucial for traffic safety, and accurately predicting drivers' lane change intentions can markedly enhance driving safety. This paper introduces a deep learning-based prediction method for autonomous driving lane change behavior, aiming to facilitate safe lane changes and thereby improve road safety.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，互联网技术的扩展和自动化的进步引起了自动驾驶技术的高度关注。沃尔沃、奔驰、特斯拉等主要汽车制造商已逐步推出从辅助驾驶汽车到半自动驾驶汽车的产品。不过，这段时间也发生了多起涉及自动驾驶车辆的交通安全事件。例如，2016 年 3 月，一辆谷歌自动驾驶汽车与一辆公共汽车发生了轻微碰撞。事故发生时，自动驾驶车辆试图并入右侧车道，但在变道过程中未能动态响应实时环境信息。它错误地认为接近的公交车会减速以避免它，导致与公交车发生低速碰撞。尽管自动驾驶技术快速进步，但这一事件凸显了当前与自动变道行为相关的技术缺陷和安全问题。变道是高速公路驾驶中最常见、最危险的行为之一，严重影响交通安全和流量。因此，变道对于交通安全至关重要，准确预测驾驶员的变道意图可以显着提高驾驶安全性。本文介绍了一种基于深度学习的自动驾驶变道行为预测方法，旨在促进安全变道，从而提高道路安全。</details>
**PDF:** <http://arxiv.org/pdf/2402.16036v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Semi-supervised Open-World Object Detection**<br />
**Title_cn:** 半监督开放世界物体检测<br />
**Authors:** Sahal Shaji Mullappilly, Abhishek Singh Gehlot, Rao Muhammad Anwer, Fahad Shahbaz Khan, Hisham Cholakkal<br />
**Abstract:** <details><summary>原文: </summary>Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information. We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的开放世界对象检测（OWOD）问题设置首先区分已知和未知类，然后在后续任务中引入标签时逐步学习未知对象。然而，当前的 OWOD 公式在增量学习阶段严重依赖外部人类预言机进行知识输入。这种对运行时的依赖使得这种表述在现实世界的部署中不太现实。为了解决这个问题，我们引入了一种更现实的公式，称为半监督开放世界检测（SS-OWOD），它通过以半监督方式投射 OWOD 的增量学习阶段来降低注释成本。我们证明了最先进的 OWOD 探测器的性能在所提出的 SS-OWOD 设置中急剧恶化。因此，我们引入了一种新颖的 SS-OWOD 检测器，名为 SS-OWFormer，它利用特征对齐方案来更好地对齐原始图像和增强图像之间的对象查询表示，以利用大量未标记数据和少量标记数据。我们进一步引入了一种用于未知检测的伪标记方案，该方案利用解码器对象查询的固有能力来捕获特定于对象的信息。我们展示了 SS-OWOD 问题设置和遥感物体检测方法的有效性，提出了精心策划的分割和基线性能评估。我们在 MS COCO、PASCAL、Objects365 和 DOTA 等 4 个数据集上的实验证明了我们方法的有效性。我们的源代码、模型和拆分可在此处获取 - https://github.com/sahalshajim/SS-OWFormer</details>
**PDF:** <http://arxiv.org/pdf/2402.16013v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Cross-Resolution Land Cover Classification Using Outdated Products and Transformers**<br />
**Title_cn:** 使用过时产品和变压器的跨分辨率土地覆盖分类<br />
**Authors:** Huan Ni, Yubin Zhao, Haiyan Guan, Cheng Jiang, Yongshi Jie, Xing Wang, Yiyang Shen<br />
**Abstract:** <details><summary>原文: </summary>Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模高分辨率土地覆盖分类是构建地球系统模型、解决生态和资源问题的先决条件。卫星传感器技术的进步提高了空间分辨率并扩大了覆盖范围。尽管如此，缺乏高分辨率标记数据仍然是一个挑战，阻碍了土地覆盖分类方法的大规模应用。在本文中，我们提出了一种基于 Transformer 的弱监督方法，使用过时的数据进行跨分辨率土地覆盖分类。首先，为了捕获远程依赖关系而不丢失对象的细粒度细节，我们提出了一种基于使用动态稀疏注意力的反向差分机制（RDM）的类似 U-Net 的 Transformer。其次，我们提出了一种基于最优传输（OT）的抗噪声损失计算（ANLC）模块。抗噪声损失计算根据OT矩阵识别可信区域（CA）和模糊区域（VA），缓解过时土地覆盖产品中噪声的影响。通过引入带有权重的弱监督损失并采用无监督损失，训练了基于 RDM 的类 U-Net Transformer。采用1 m分辨率的遥感图像和美国六个州的相应地面实况来验证该方法的性能。实验使用2013年以来过时的30 m分辨率的土地覆盖产品作为训练标签，并生成2017年以来1 m分辨率的土地覆盖图。结果表明，与最先进的方法相比，该方法具有优越性。代码可在 https://github.com/yu-ni1989/ANLC-Former 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.16001v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **VOLoc: Visual Place Recognition by Querying Compressed Lidar Map**<br />
**Title_cn:** VOLoc：通过查询压缩激光雷达地图进行视觉地点识别<br />
**Authors:** Xudong Cai, Yongcai Wang, Zhe Huang, Yu Shao, Deying Li<br />
**Abstract:** <details><summary>原文: </summary>The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a \emph{Geometry-Preserving Compressor} (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the \emph{Querying Point Cloud} (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attention-based aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-to-Lidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at https://github.com/Master-cai/VOLoc.</details>
**Abstract_cn:** <details><summary>译文: </summary>城市规模激光雷达地图的出现使得使用移动摄像头进行城市规模地点识别的潜力成为可能。然而，为了存储效率，城市尺度的激光雷达地图通常需要进行压缩，这增加了压缩激光雷达地图中直接视觉地点识别的难度。本文提出了 VOLoc，一种准确高效的视觉地点识别方法，该方法利用几何相似性通过实时捕获的图像序列直接查询压缩的激光雷达地图。在离线阶段，VOLoc 使用 emph{几何保留压缩器} (GPC) 压缩激光雷达图，其中压缩是可逆的，这是下游 6DoF 位姿估计的关键要求。在在线阶段，VOLoc提出了在线几何恢复模块（GRM），该模块由在线视觉里程计（VO）和点云优化模块组成，从而在线恢复相机周围的局部场景结构以构建\emph {查询点云}（QPC）。然后QPC由相同的GPC压缩，并由基于注意力的聚合模块聚合为全局描述符，以在向量空间中查询压缩的激光雷达图。还提出了迁移学习机制来提高聚合网络的准确性和通用性。广泛的评估表明，VOLoc 的定位精度甚至优于 Lidar-to-Lidar 位置识别，创下了低端移动相机使用压缩 Lidar 地图的新记录。该代码可在 https://github.com/Master-cai/VOLOc 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.15961v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis**<br />
**Title_cn:** ViSTec：用于运动技术识别和战术分析的视频建模<br />
**Authors:** Yuchen He, Zeqing Yuan, Yihong Wu, Liqi Cheng, Dazhen Deng, Yingcai Wu<br />
**Abstract:** <details><summary>原文: </summary>The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph. Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies. More details are available at: https://ViSTec2024.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>球拍运动的广泛普及推动了对广播视频战术分析的巨大需求。然而，现有的手动方法需要费力的注释，而最近利用视频感知模型的尝试仅限于球轨迹等低级注释，忽略了需要了解击球技术的战术。由于球拍运动视频中频繁的遮挡和运动引起的模糊，最先进的动作分割模型也难以实现技术识别。为了应对这些挑战，我们提出了 ViSTec，这是一种基于视频的运动技术识别模型，其灵感来自人类认知，可将稀疏的视觉数据与丰富的上下文洞察相结合。我们的方法集成了一个图表，以明确地模拟笔划序列中的策略知识，并通过上下文归纳偏差增强技术识别。联合训练两阶段动作感知模型，以与图中的上下文知识保持一致。实验表明，我们的方法明显优于现有模型。与中国国家乒乓球队专家的案例研究验证了我们的模型自动分析技术动作和战术策略的能力。更多详细信息请访问：https://ViSTec2024.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.15952v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation**<br />
**Title_cn:** AVI-Talking：学习音频-视频指令以生成富有表现力的 3D 说话人脸<br />
**Authors:** Yasheng Sun, Wenqing Chu, Hang Zhou, Kaisiyuan Wang, Hideki Koike<br />
**Abstract:** <details><summary>原文: </summary>While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker's speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human speech for generating an expressive talking face that aligns with the speaking status. In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation. This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces. Instead of directly learning facial movements from human speech, our two-stage strategy involves the LLMs first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the speech. Subsequently, a diffusion-based generative network executes these instructions. This two-stage process, coupled with the incorporation of LLMs, enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications. Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然在实现 3D 语音驱动的说话人脸生成的精确唇形同步方面已经取得了相当大的进展，但将富有表现力的面部细节合成与说话者的说话状态相结合的任务仍然具有挑战性。我们的目标是直接利用人类语音传达的固有风格信息来生成与说话状态一致的富有表现力的说话面孔。在本文中，我们提出了 AVI-Talking，一种用于生成富有表现力的说话面孔的视听教学系统。该系统利用大型语言模型 (LLM) 提供的强大上下文推理和幻觉功能来指导 3D 说话面孔的真实合成。我们的两阶段策略不是直接从人类语音中学习面部动作，而是让法学硕士首先理解音频信息并生成暗示与语音无缝对应的表情面部细节的指令。随后，基于扩散的生成网络执行这些指令。这个两阶段过程与法学硕士的结合增强了模型的可解释性，并为用户提供了理解指令和指定所需操作或修改的灵活性。大量的实验展示了我们的方法在制作生动的说话面孔、富有表现力的面部动作和一致的情绪状态方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.16124v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **InstructEdit: Instruction-based Knowledge Editing for Large Language Models**<br />
**Title_cn:** InstructEdit：大型语言模型的基于指令的知识编辑<br />
**Authors:** Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen<br />
**Abstract:** <details><summary>原文: </summary>Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines. To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization. Code and datasets will be available in https://github.com/zjunlp/EasyEdit.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型的知识编辑可以提供有效的解决方案来改变模型的行为，而不会对整体性能产生负面影响。然而，当前的方法遇到了跨任务通用性有限的问题，每个任务都需要一个不同的编辑器，这极大地阻碍了更广泛的应用。为了解决这个问题，我们首先分析知识编辑中的多任务泛化问题。具体来说，我们开发了一种基于指令的编辑技术，称为 InstructEdit，它有助于编辑器使用简单的指令同时适应各种任务性能。每个LLM只有一个统一的编辑器，我们凭经验证明InstructEdit可以提高编辑器的控制力，导致多任务编辑设置的可靠性平均提高14.86%。此外，涉及坚持未见任务的实验表明，InstructEdit 始终超越以前的强大基线。为了进一步研究基于指令的知识编辑的潜在机制，我们分析了编辑梯度方向的主成分，揭示了指令可以通过更强的 OOD 泛化来帮助控制优化方向。代码和数据集将在 https://github.com/zjunlp/EasyEdit 中提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.16123v1><br />
**Code:** <https://github.com/zjunlp/easyedit>**<br />
>>**index:** 3<br />
**Title:** **LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding**<br />
**Title_cn:** LSTP：语言引导的时空即时学习，用于长格式视频文本理解<br />
**Authors:** Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng<br />
**Abstract:** <details><summary>原文: </summary>Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管视频语言建模取得了进展，但解释长格式视频以响应特定任务的语言查询的计算挑战仍然存在，这很大程度上是由于高维视频数据的复杂性以及语言和视觉线索在空间和时间上的错位。为了解决这个问题，我们引入了一种称为语言引导时空提示学习（LSTP）的新方法。该方法具有两个关键组件：具有光流先验的时间提示采样器（TPS），可利用时间信息有效地提取相关视频内容；以及空间提示求解器（SPS），可熟练捕获视觉和文本元素之间复杂的空间关系。通过将 TPS 和 SPS 与内聚训练策略相协调，我们的框架显着提高了计算效率、时间理解和时空对齐。使用各种视频语言预训练 (VLP) 和大型语言模型 (LLM) 对两个具有挑战性的任务（视频问答和基于视频的时间问题）进行实证评估，证明了我们提出的 LSTP 的卓越性能、速度和多功能性范例。</details>
**PDF:** <http://arxiv.org/pdf/2402.16050v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **One-stage Prompt-based Continual Learning**<br />
**Title_cn:** 一阶段基于提示的持续学习<br />
**Authors:** Youngeun Kim, Yuhang Li, Priyadarshini Panda<br />
**Abstract:** <details><summary>原文: </summary>Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%. We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains ~ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ~1.4% on public class-incremental continual learning benchmarks including CIFAR-100, ImageNet-R, and DomainNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于提示的持续学习（PCL）作为一种有前途的持续学习解决方案而受到了广泛关注，因为它实现了最先进的性能，同时防止隐私侵犯和内存开销问题。尽管如此，由于存在两个 Vision Transformer (ViT) 前馈阶段，现有的 PCL 方法面临着巨大的计算负担。一个是查询 ViT，生成提示查询以选择提示池内的提示；另一个是主干 ViT，它将选定的提示和图像标记之间的信息混合在一起。为了解决这个问题，我们引入了一种单阶段 PCL 框架，直接使用中间层的令牌嵌入作为提示查询。此设计消除了对查询 ViT 的额外前馈阶段的需求，从而使训练和推理的计算成本降低了约 50%，边际精度下降 < 1%。我们进一步引入了查询池正则化（QR）损失，它调节提示查询和提示池之间的关系，以提高表示能力。 QR 损失仅在训练期间应用，因此从 QR 损失推断时不存在计算开销。通过 QR 损失，我们的方法在推理过程中保持了约 50% 的计算成本降低，并且在公共类增量持续学习基准（包括 CIFAR-100、ImageNet-R 和 DomainNet）上比之前的两阶段 PCL 方法高出约 1.4% 。</details>
**PDF:** <http://arxiv.org/pdf/2402.16189v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention**<br />
**Title_cn:** StochCA：一种利用交叉注意力预训练模型的新方法<br />
**Authors:** Seungwon Seo, Suho Lee, Sangheum Hwang<br />
**Abstract:** <details><summary>原文: </summary>Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through fine-tuning pretrained models on target tasks. However, na\"{\i}ve fine-tuning may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures. This method modifies the Transformer's self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning. Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. Our code is available at https://github.com/daintlab/stochastic_cross_attention</details>
**Abstract_cn:** <details><summary>译文: </summary>利用大规模预训练模型是提高各种目标任务性能的众所周知的策略。它通常是通过针对目标任务微调预训练模型来实现的。然而，简单的微调可能无法充分利用预训练模型中嵌入的知识。在这项研究中，我们引入了一种新颖的微调方法，称为随机交叉注意（StochCA），专门针对 Transformer 架构。该方法修改了 Transformer 的自注意力机制，在微调时选择性地利用预训练模型中的知识。具体来说，在每个块中，不是自注意力，而是根据预定义的概率随机执行交叉注意力，其中键和值是从预训练模型的相应块中提取。通过这样做，目标模型的查询和通道混合多层感知器层针对目标任务进行微调，以学习如何有效地利用预训练模型的丰富表示。为了提高 StochCA 的有效性，我们在迁移学习和领域泛化领域的基准上进行了广泛的实验，其中预训练模型的利用至关重要。我们的实验结果表明，StochCA 在这两个领域都优于最先进的方法。此外，我们证明 StochCA 是对现有方法的补充，即它可以与现有方法相结合以进一步提高性能。我们的代码位于 https://github.com/daintlab/stochastic_cross_attention</details>
**PDF:** <http://arxiv.org/pdf/2402.16092v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining**<br />
**Title_cn:** 深入研究区域：利用区域信息转换器进行单图像去雨<br />
**Authors:** Baiang Li, Zhao Zhang, Huan Zheng, Xiaogang Xu, Yanyan Wei, Jingyi Zhang, Jicong Fan, Meng Wang<br />
**Abstract:** <details><summary>原文: </summary>Transformer-based Single Image Deraining (SID) methods have achieved remarkable success, primarily attributed to their robust capability in capturing long-range interactions. However, we've noticed that current methods handle rain-affected and unaffected regions concurrently, overlooking the disparities between these areas, resulting in confusion between rain streaks and background parts, and inabilities to obtain effective interactions, ultimately resulting in suboptimal deraining outcomes. To address the above issue, we introduce the Region Transformer (Regformer), a novel SID method that underlines the importance of independently processing rain-affected and unaffected regions while considering their combined impact for high-quality image reconstruction. The crux of our method is the innovative Region Transformer Block (RTB), which integrates a Region Masked Attention (RMA) mechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention selection of rain-affected and unaffected regions and local modeling of mixed scales. The RMA generates attention maps tailored to these two regions and their interactions, enabling our model to capture comprehensive features essential for rain removal. To better recover high-frequency textures and capture more local details, we develop the MGFB as a compensation module to complete local mixed scale modeling. Extensive experiments demonstrate that our model reaches state-of-the-art performance, significantly improving the image deraining quality. Our code and trained models are publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于 Transformer 的单图像去雨 (SID) 方法取得了显着的成功，这主要归功于其捕获远程交互的强大能力。然而，我们注意到，目前的方法同时处理受降雨影响和未受影响的区域，忽略了这些区域之间的差异，导致雨条纹和背景部分之间的混淆，并且无法获得有效的相互作用，最终导致除雨效果不佳。为了解决上述问题，我们引入了区域变换器（Regformer），这是一种新颖的 SID 方法，强调独立处理受降雨影响和未受影响区域的重要性，同时考虑它们对高质量图像重建的综合影响。我们方法的关键是创新的区域变换器块（RTB），它集成了区域屏蔽注意力（RMA）机制和混合门前向块（MGFB）。我们的 RTB 用于受降雨影响和未受影响区域的注意力选择以及混合尺度的局部建模。 RMA 生成针对这两个区域及其相互作用的注意力图，使我们的模型能够捕获除雨所必需的综合特征。为了更好地恢复高频纹理并捕获更多局部细节，我们开发了MGFB作为补偿模块来完成局部混合尺度建模。大量的实验表明，我们的模型达到了最先进的性能，显着提高了图像去雨质量。我们的代码和经过训练的模型是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2402.16033v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction**<br />
**Title_cn:** GenNBV：主动 3D 重建的通用次最佳视图策略<br />
**Authors:** Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang<br />
**Abstract:** <details><summary>原文: </summary>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然神经辐射领域的最新进展使得大规模场景能够实现真实的数字化，但图像捕获过程仍然耗时且费力。之前的工作尝试使用 Next-Best-View (NBV) 策略来自动执行此过程以进行主动 3D 重建。然而，现有的 NBV 策略严重依赖于手工制定的标准、有限的操作空间或每个场景的优化表示。这些约束限制了它们的跨数据集通用性。为了克服这些问题，我们提出了 GenNBV，一种端到端的通用 NBV 政策。我们的策略采用基于强化学习（RL）的框架，并将典型的有限动作空间扩展到 5D 自由空间。它使我们的代理无人机能够从任何角度进行扫描，甚至在训练期间与看不见的几何图形进行交互。为了提高跨数据集的通用性，我们还提出了一种新颖的多源状态嵌入，包括几何、语义和动作表示。我们使用 Isaac Gym 模拟器以及 Houses3K 和 OmniObject3D 数据集建立了一个基准来评估此 NBV 政策。实验表明，我们的策略对这些数据集中不可见的建筑规模对象的覆盖率分别达到了 98.26% 和 97.12%，优于之前的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.16174v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Spectrum Extraction and Clipping for Implicitly Linear Layers**<br />
**Title_cn:** 隐式线性层的频谱提取和裁剪<br />
**Authors:** Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram<br />
**Abstract:** <details><summary>原文: </summary>We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们展示了自动微分在高效、正确地计算和控制隐式线性算子的频谱方面的有效性，隐式线性算子是丰富的层类型系列，包括所有标准卷积层和密集层。我们提供了第一个适用于一般卷积层的裁剪方法，并阐明了先前工作中导致正确性问题的表示限制。我们研究了批量归一化层与卷积层连接时的效果，并展示了如何将我们的裁剪方法应用于它们的组合。通过使用各种实验将我们的算法与最先进的方法的准确性和性能进行比较，我们表明它们更加精确和高效，并具有更好的泛化性和对抗鲁棒性。我们在 https://github.com/Ali-E/FastClip 提供了使用我们的方法的代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.16017v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation**<br />
**Title_cn:** 通过领域同化进行医学成像的对抗性鲁棒迁移学习<br />
**Authors:** Xiaohui Chen, Tie Luo<br />
**Abstract:** <details><summary>原文: </summary>In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\em vulnerability} to adversarial attacks. This paper proposes a {\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion. We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks. The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>在医学成像领域，广泛的研究致力于利用其揭示患者关键诊断特征的潜力。人工智能 (AI) 驱动的医疗诊断依靠复杂的机器学习和深度学习模型来分析、检测和识别医学图像中的疾病。尽管这些模型具有卓越的性能，且具有高精度，但它们仍面临着可信度问题。对原始图像引入微妙的扰动使对手能够操纵预测输出，将其重定向到其他目标或非目标类别。此外，公开可用的医学图像的稀缺构成了可靠训练的瓶颈，导致当代算法依赖于基于大量自然图像的预训练模型，这种做法称为迁移学习。然而，自然图像和医学图像之间存在显着的领域差异，这导致迁移学习产生的人工智能模型对对抗性攻击表现出更高的脆弱性。本文提出了一种{\em域同化}方法，将纹理和颜色适应引入到迁移学习中，然后使用纹理保留组件来抑制不需要的失真。我们系统地分析了迁移学习在不同数据模式下面对各种对抗性攻击时的性能，总体目标是增强模型在医学成像任务中的稳健性和安全性。结果表明，它在降低攻击效率方面非常有效，有助于在生物医学应用中实现更值得信赖的迁移学习。</details>
**PDF:** <http://arxiv.org/pdf/2402.16005v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **An Image Enhancement Method for Improving Small Intestinal Villi Clarity**<br />
**Title_cn:** 一种提高小肠绒毛清晰度的图像增强方法<br />
**Authors:** Shaojie Zhang, Yinghui Wang, Peixuan Liu, Wei Li, Jinlong Yang, Tao Yan, Yukai Wang, Liangyi Huang, Mingfeng Wang, Ibragim R. Atadjanov<br />
**Abstract:** <details><summary>原文: </summary>This paper presents, for the first time, an image enhancement methodology designed to enhance the clarity of small intestinal villi in Wireless Capsule Endoscopy (WCE) images. This method first separates the low-frequency and high-frequency components of small intestinal villi images using guided filtering. Subsequently, an adaptive light gain factor is generated based on the low-frequency component, and an adaptive gradient gain factor is derived from the convolution results of the Laplacian operator in different regions of small intestinal villi images. The obtained light gain factor and gradient gain factor are then combined to enhance the high-frequency components. Finally, the enhanced high-frequency component is fused with the original image to achieve adaptive sharpening of the edges of WCE small intestinal villi images. The experiments affirm that, compared to established WCE image enhancement methods, our approach not only accentuates the edge details of WCE small intestine villi images but also skillfully suppresses noise amplification, thereby preventing the occurrence of edge overshooting.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文首次提出了一种图像增强方法，旨在增强无线胶囊内窥镜（WCE）图像中小肠绒毛的清晰度。该方法首先使用引导滤波分离小肠绒毛图像的低频和高频分量。随后，基于低频分量生成自适应光增益因子，并根据拉普拉斯算子在小肠绒毛图像的不同区域的卷积结果导出自适应梯度增益因子。然后将获得的光增益因子和梯度增益因子组合起来以增强高频分量。最后将增强后的高频分量与原始图像融合，实现WCE小肠绒毛图像边缘的自适应锐化。实验证实，与现有的WCE图像增强方法相比，我们的方法不仅突出了WCE小肠绒毛图像的边缘细节，而且巧妙地抑制了噪声放大，从而防止了边缘过冲的发生。</details>
**PDF:** <http://arxiv.org/pdf/2402.15977v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks**<br />
**Title_cn:** 迈向鲁棒图像拼接：针对兼容攻击的自适应抵抗学习<br />
**Authors:** Zhiying Jiang, Xingyuan Li, Jinyuan Liu, Xin Fan, Risheng Liu<br />
**Abstract:** <details><summary>原文: </summary>Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image. Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications. Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms. In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against adversarial attacks. Specifically, we introduce a stitching-oriented attack~(SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure. To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive adversarial training~(AAT) to balance attack resistance with stitching precision. In this way, we relieve the gap between the routine adversarial training and benign models, ensuring resilience without quality compromise. Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance. Furthermore, AAT emerges as a more robust solution against adversarial perturbations, delivering superior stitching results. Code is available at:https://github.com/Jzy2017/TRIS.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像拼接将从不同角度捕获的图像无缝集成到单个宽视场图像中。这种集成不仅扩大了捕获的场景，还增强了计算机视觉应用中的整体感知。给定一对捕获的图像，人类视觉系统未注意到的细微扰动和扭曲往往会攻击对应匹配，从而损害图像拼接算法的性能。鉴于这一挑战，本文首次尝试提高图像拼接对抗对抗攻击的鲁棒性。具体来说，我们引入了一种面向拼接的攻击（SoA），专门用于放大重叠区域内的对齐损失，从而针对特征匹配过程。为了建立抗攻击模型，我们深入研究了拼接架构的鲁棒性，并开发了自适应对抗训练（AAT）来平衡抗攻击性和拼接精度。通过这种方式，我们缩小了常规对抗性训练和良性模型之间的差距，确保了弹性而不影响质量。对现实世界和合成数据集的综合评估验证了 SoA 在拼接性能方面的恶化。此外，AAT 成为一种针对对抗性扰动的更强大的解决方案，可提供卓越的拼接结果。代码位于：https://github.com/Jzy2017/TRIS。</details>
**PDF:** <http://arxiv.org/pdf/2402.15959v1><br />
**Code:** null<br />

