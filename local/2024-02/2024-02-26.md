## [UPDATED!] **2024-02-26** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Stochastic Conditional Diffusion Models for Semantic Image Synthesis**<br />
**Title_cn:** 用于语义图像合成的随机条件扩散模型<br />
**Authors:** Juyeon Ko, Inho Kong, Hyunwoo J. Kim<br />
**Abstract:** <details><summary>原文: </summary>Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). It can be applied to diverse real-world practices such as photo editing or content creation. However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion. Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义图像合成（SIS）是生成与语义图（标签）相对应的真实图像的任务。它可以应用于各种现实世界的实践，例如照片编辑或内容创建。然而，在实际应用中，SIS 经常遇到嘈杂的用户输入。为了解决这个问题，我们提出了随机条件扩散模型（SCDM），它是一种稳健的条件扩散模型，具有为带有噪声标签的 SIS 量身定制的新颖的前向和生成过程。它通过标签扩散随机扰动语义标签图来增强鲁棒性，标签扩散通过离散扩散来扩散标签。通过标签的扩散，随着时间步长的增加，噪声和干净的语义图变得相似，最终在 $t=T$ 时变得相同。这有利于生成接近干净图像的图像，从而实现稳健的生成。此外，我们提出了一种按类别的噪声计划，以根据类别差异地扩散标签。我们证明，所提出的方法通过对基准数据集进行广泛的实验和分析来生成高质量的样本，包括模拟现实应用中人为错误的新颖的实验设置。</details>
**PDF:** <http://arxiv.org/pdf/2402.16506v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Outline-Guided Object Inpainting with Diffusion Models**<br />
**Title_cn:** 使用扩散模型进行轮廓引导的对象修复<br />
**Authors:** Markus Pobitzer, Filip Janicki, Mattia Rigotti, Cristiano Malossi<br />
**Abstract:** <details><summary>原文: </summary>Instance segmentation datasets play a crucial role in training accurate and robust computer vision models. However, obtaining accurate mask annotations to produce high-quality segmentation datasets is a costly and labor-intensive process. In this work, we show how this issue can be mitigated by starting with small annotated instance segmentation datasets and augmenting them to effectively obtain a sizeable annotated dataset. We achieve that by creating variations of the available annotated object instances in a way that preserves the provided mask annotations, thereby resulting in new image-mask pairs to be added to the set of annotated images. Specifically, we generate new images using a diffusion-based inpainting model to fill out the masked area with a desired object class by guiding the diffusion through the object outline. We show that the object outline provides a simple, but also reliable and convenient training-free guidance signal for the underlying inpainting model that is often sufficient to fill out the mask with an object of the correct class without further text guidance and preserve the correspondence between generated images and the mask annotations with high precision. Our experimental results reveal that our method successfully generates realistic variations of object instances, preserving their shape characteristics while introducing diversity within the augmented area. We also show that the proposed method can naturally be combined with text guidance and other image augmentation techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>实例分割数据集在训练准确且稳健的计算机视觉模型中发挥着至关重要的作用。然而，获得准确的掩模注释以生成高质量的分割数据集是一个成本高昂且劳动密集型的过程。在这项工作中，我们展示了如何通过从小型带注释的实例分割数据集开始并对其进行扩充以有效地获得相当大的带注释数据集来缓解此问题。我们通过以保留提供的掩模注释的方式创建可用带注释的对象实例的变体来实现这一点，从而导致新的图像掩模对添加到带注释的图像集中。具体来说，我们使用基于扩散的修复模型生成新图像，通过引导扩散穿过对象轮廓，用所需的对象类填充遮罩区域。我们表明，对象轮廓为底层修复模型提供了一个简单、可靠且方便的免训练指导信号，该信号通常足以用正确类别的对象填充掩模，而无需进一步的文本指导，并保留之间的对应关系生成的图像和掩模注释具有高精度。我们的实验结果表明，我们的方法成功生成了对象实例的真实变化，保留了它们的形状特征，同时在增强区域内引入了多样性。我们还表明，所提出的方法可以自然地与文本引导和其他图像增强技术相结合。</details>
**PDF:** <http://arxiv.org/pdf/2402.16421v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Placing Objects in Context via Inpainting for Out-of-distribution Segmentation**<br />
**Title_cn:** 通过修复将对象放置在上下文中以进行分布外分割<br />
**Authors:** Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez<br />
**Abstract:** <details><summary>原文: </summary>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images.</details>
**Abstract_cn:** <details><summary>译文: </summary>当将语义分割模型部署到现实世界时，它不可避免地会遇到训练过程中看不见的语义类。因此，为了安全部署此类系统，准确评估和提高其异常分割能力至关重要。然而，获取和标记语义分割数据的成本很高，而且意外情况是长尾的且具有潜在危险。事实上，现有的异常分割数据集捕获的异常数量有限，缺乏真实性或具有很强的领域变化。在本文中，我们提出了将对象放入上下文（POC）管道，以通过扩散模型将任何对象实际添加到任何图像中。 POC 可用于轻松扩展具有任意数量对象的任何数据集。在我们的实验中，我们基于 POC 生成的数据提出了不同的异常分割数据集，并表明 POC 可以提高最近最先进的异常微调方法在几个标准化基准中的性能。 POC对于学习新课程也很有效。例如，我们使用它通过添加 Pascal 类的子集来编辑 Cityscapes 样本，并表明在此类数据上训练的模型实现了与 Pascal 训练的基线相当的性能。这证实了在 POC 生成的图像上训练的模型的模拟与真实差距较小。</details>
**PDF:** <http://arxiv.org/pdf/2402.16392v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Generative AI in Vision: A Survey on Models, Metrics and Applications**<br />
**Title_cn:** 视觉中的生成人工智能：模型、指标和应用的调查<br />
**Authors:** Gaurav Raut, Apoorv Singh<br />
**Abstract:** <details><summary>原文: </summary>Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成式人工智能模型通过创建真实且多样化的数据样本，给各个领域带来了革命性的变化。在这些模型中，扩散模型已成为生成高质量图像、文本和音频的强大方法。本调查论文全面概述了生成式人工智能传播和遗留模型，重点关注其底层技术、跨不同领域的应用及其挑战。我们深入研究扩散模型的理论基础，包括去噪扩散概率模型（DDPM）和基于分数的生成模型等概念。此外，我们还探索了这些模型在文本到图像、图像修复和图像超分辨率等方面的多种应用，展示了它们在创造性任务和数据增强方面的潜力。通过综合现有研究并强调该领域的关键进展，本次调查旨在为研究人员和从业者提供对生成式人工智能传播和遗留模型的全面了解，并激发这一令人兴奋的人工智能领域的未来创新。</details>
**PDF:** <http://arxiv.org/pdf/2402.16369v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Gradient-Guided Modality Decoupling for Missing-Modality Robustness**<br />
**Title_cn:** 用于缺失模态鲁棒性的梯度引导模态解耦<br />
**Authors:** Hao Wang, Shengda Luo, Guosheng Hu, Jianguo Zhang<br />
**Abstract:** <details><summary>原文: </summary>Multimodal learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.</details>
**Abstract_cn:** <details><summary>译文: </summary>输入数据不完整（模态缺失）的多模态学习既实用又具有挑战性。在这项工作中，我们对这一挑战进行了深入分析，发现模态优势对模型训练有显着的负面影响，极大地降低了缺失的模态性能。在 Grad-CAM 的推动下，我们引入了一种新的指标——梯度，来监控和减少广泛存在于缺失模态场景中的模态主导。为了帮助这个指标，我们提出了一种新颖的梯度引导模态解耦（GMD）方法来解耦对主导模态的依赖。具体来说，GMD 消除了不同模态中冲突的梯度分量以实现这种解耦，从而显着提高了性能。此外，为了灵活处理模态不完整数据，我们设计了一种参数高效的动态共享（DS）框架，该框架可以根据一种模态是否可用自适应地打开/关闭网络参数。我们对三个流行的多模态基准进行了广泛的实验，包括用于医学分割的 BraTS 2018、用于情感分析的 CMU-MOSI 和 CMU-MOSEI。结果表明，我们的方法可以显着优于竞争对手，显示了所提出的解决方案的有效性。我们的代码发布在这里：https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling。</details>
**PDF:** <http://arxiv.org/pdf/2402.16318v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space**<br />
**Title_cn:** CLIP 嵌入空间中具有语言驱动损失的红外和可见图像融合<br />
**Authors:** Yuhao Wang, Lingjuan Miao, Zhiqiang Zhou, Lei Zhang, Yajun Qiao<br />
**Abstract:** <details><summary>原文: </summary>Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外-可见光图像融合（IVIF）由于两种图像模式的高度互补特性而引起了广泛关注。由于缺乏真实的融合图像，当前基于深度学习的方法的融合输出在很大程度上取决于数学定义的损失函数。由于在没有地面实况的情况下很难在数学上很好地定义融合图像，因此现有融合方法的性能受到限制。在本文中，我们首先提出使用自然语言来表达IVIF的目标，这样可以避免当前损失中融合输出的显式数学建模，并充分利用语言表达的优势来提高融合性能。为此，我们提出了一个全面的语言表达融合目标，并使用 CLIP 将相关文本编码到多模态嵌入空间中。然后，通过建立嵌入向量之间的关系来表示融合目标和输入图像模态，在嵌入空间中构建语言驱动的融合模型。最后，导出语言驱动的损失，通过监督训练使实际的 IVIF 与嵌入式语言驱动的融合模型保持一致。实验表明，我们的方法可以获得比现有技术更好的融合结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.16267v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency**<br />
**Title_cn:** CMC：通过跨视图多平面一致性进行少样本新颖视图合成<br />
**Authors:** Hanxin Zhu, Tianyu He, Zhibo Chen<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场（NeRF）由于其连续表示场景的能力，在新颖的视图合成中表现出了令人印象深刻的结果，特别是在虚拟现实（VR）和增强现实（AR）中。然而，当只有几个输入视图图像可用时，NeRF 往往会过度拟合给定的视图，从而使像素的估计深度共享几乎相同的值。与之前通过引入复杂先验或额外监督来进行正则化的方法不同，我们提出了一种简单而有效的方法，可以明确地在输入视图之间构建深度感知的一致性来应对这一挑战。我们的主要见解是，通过强制在不同的输入视图中重复采样相同的空间点，我们能够加强视图之间的交互，从而缓解过度拟合问题。为了实现这一目标，我们在分层表示（\textit{即多平面图像）上构建神经网络，因此可以在多个离散平面上对采样点进行重新采样。此外，为了规范看不见的目标视图，我们将不同输入视图的渲染颜色和深度限制为相同。虽然简单，但大量的实验表明，我们提出的方法可以比最先进的方法实现更好的合成质量。</details>
**PDF:** <http://arxiv.org/pdf/2402.16407v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field**<br />
**Title_cn:** SPC-NeRF：基于体素的辐射场的空间预测压缩<br />
**Authors:** Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao<br />
**Abstract:** <details><summary>原文: </summary>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time.</details>
**Abstract_cn:** <details><summary>译文: </summary>用显式体素网格（EVG）表示神经辐射场（NeRF）是改进 NeRF 的一个有前途的方向。然而，由于存储成本过高，EVG 表示的存储和传输效率不高。目前的EVG压缩方法主要继承了神经网络压缩的剪枝、量化等方法，没有充分利用体素的空间相关性。受繁荣的数字图像压缩技术的启发，本文提出了 SPC-NeRF，一种在 EVG 压缩中应用空间预测编码的新颖框架。所提出的框架可以有效地消除空间冗余，以获得更好的压缩性能。此外，我们对比特率进行建模并设计了一种新颖形式的损失函数，我们可以联合优化压缩比和失真以实现更高的编码效率。大量实验表明，与最先进的方法 VQRF 相比，我们的方法在多个代表性测试数据集上可以节省 32% 的比特，并且训练时间相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.16366v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection**<br />
**Title_cn:** mAPm：多尺度注意力金字塔模块，用于增强 RLD 检测中的尺度变化<br />
**Authors:** Yunusa Haruna, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Isah Bello, Adamu Lawan<br />
**Abstract:** <details><summary>原文: </summary>Detecting objects across various scales remains a significant challenge in computer vision, particularly in tasks such as Rice Leaf Disease (RLD) detection, where objects exhibit considerable scale variations. Traditional object detection methods often struggle to address these variations, resulting in missed detections or reduced accuracy. In this study, we propose the multi-scale Attention Pyramid module (mAPm), a novel approach that integrates dilated convolutions into the Feature Pyramid Network (FPN) to enhance multi-scale information ex-traction. Additionally, we incorporate a global Multi-Head Self-Attention (MHSA) mechanism and a deconvolutional layer to refine the up-sampling process. We evaluate mAPm on YOLOv7 using the MRLD and COCO datasets. Compared to vanilla FPN, BiFPN, NAS-FPN, PANET, and ACFPN, mAPm achieved a significant improvement in Average Precision (AP), with a +2.61% increase on the MRLD dataset compared to the baseline FPN method in YOLOv7. This demonstrates its effectiveness in handling scale variations. Furthermore, the versatility of mAPm allows its integration into various FPN-based object detection models, showcasing its potential to advance object detection techniques.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测不同尺度的物体仍然是计算机视觉领域的一个重大挑战，特别是在稻叶病 (RLD) 检测等任务中，其中物体表现出相当大的尺度变化。传统的物体检测方法通常难以解决这些变化，从而导致检测失败或准确性降低。在这项研究中，我们提出了多尺度注意力金字塔模块（mAPm），这是一种将扩张卷积集成到特征金字塔网络（FPN）中以增强多尺度信息提取的新颖方法。此外，我们还采用了全局多头自注意力（MHSA）机制和反卷积层来改进上采样过程。我们使用 MRLD 和 COCO 数据集在 YOLOv7 上评估 mAPm。与普通 FPN、BiFPN、NAS-FPN、PANET 和 ACFPN 相比，mAPm 在平均精度 (AP) 方面取得了显着提高，与 YOLOv7 中的基线 FPN 方法相比，在 MRLD 数据集上提高了 +2.61%。这证明了它在处理规模变化方面的有效性。此外，mAPm 的多功能性允许其集成到各种基于 FPN 的目标检测模型中，展示了其推进目标检测技术的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.16291v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification**<br />
**Title_cn:** 智能已知和新型飞机识别——战斗识别从分类到相似学习的转变<br />
**Authors:** Ahmad Saeed, Haasha Bin Atif, Usman Habib, Mohsin Bilal<br />
**Abstract:** <details><summary>原文: </summary>Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>低分辨率遥感图像中的精确飞机识别是航空领域一项具有挑战性但又至关重要的任务，尤其是战斗识别。这项研究通过一种新颖的、可扩展的、人工智能驱动的解决方案解决了这个问题。遥感图像中战斗识别的主要障碍是除了已知类型之外还准确识别新型/未知类型的飞机。传统方法、人类专家驱动的战斗识别和图像分类在识别新类别方面存在不足。我们的方法采用相似性学习来辨别各种军用和民用飞机的特征。它可以识别已知和新颖的飞机类型，利用度量学习进行识别，并利用监督式小样本学习进行飞机类型分类。为了应对有限的低分辨率遥感数据的挑战，我们提出了一种端到端框架，通过以完全监督的方式训练通用嵌入器来适应军用飞机识别的多样化和多功能过程。与早期飞机图像分类方法的比较分析表明，我们的方法对于飞机图像分类（F1 分数飞机类型为 0.861）是有效的，并且在量化新颖类型的识别方面处于领先地位（F1 分数二分区为 0.936）。所提出的方法有效地解决了遥感数据的固有挑战，从而为数据集质量设定了新标准。该研究为领域专家开辟了新的途径，并展示了区分各种飞机类型的独特能力，有助于为实时飞机识别提供更强大、更适应领域的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.16486v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Edge Detectors Can Make Deep Convolutional Neural Networks More Robust**<br />
**Title_cn:** 边缘检测器可以使深度卷积神经网络更加鲁棒<br />
**Authors:** Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Jia-Wei Wang, Ji-En Ma, You-Tong Fang<br />
**Abstract:** <details><summary>原文: </summary>Deep convolutional neural networks (DCNN for short) are vulnerable to examples with small perturbations. Improving DCNN's robustness is of great significance to the safety-critical applications, such as autonomous driving and industry automation. Inspired by the principal way that human eyes recognize objects, i.e., largely relying on the shape features, this paper first employs the edge detectors as layer kernels and designs a binary edge feature branch (BEFB for short) to learn the binary edge features, which can be easily integrated into any popular backbone. The four edge detectors can learn the horizontal, vertical, positive diagonal, and negative diagonal edge features, respectively, and the branch is stacked by multiple Sobel layers (using edge detectors as kernels) and one threshold layer. The binary edge features learned by the branch, concatenated with the texture features learned by the backbone, are fed into the fully connected layers for classification. We integrate the proposed branch into VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets. Experimental results demonstrate the BEFB is lightweight and has no side effects on training. And the accuracy of the BEFB integrated models is better than the original ones on all datasets when facing FGSM, PGD, and C\&W attacks. Besides, BEFB integrated models equipped with the robustness enhancing techniques can achieve better classification accuracy compared to the original models. The work in this paper for the first time shows it is feasible to enhance the robustness of DCNNs through combining both shape-like features and texture features.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度卷积神经网络（简称 DCNN）容易受到小扰动样本的影响。提高DCNN的鲁棒性对于自动驾驶、工业自动化等安全关键应用具有重要意义。受人眼识别物体主要依靠形状特征的方式的启发，本文首先采用边缘检测器作为层核，设计一个二值边缘特征分支（简称BEFB）来学习二值边缘特征，可以轻松集成到任何流行的骨干网中。四个边缘检测器可以分别学习水平、垂直、正对角线和负对角线边缘特征，并且分支由多个Sobel层（使用边缘检测器作为内核）和一个阈值层堆叠。分支学习到的二进制边缘特征与主干学习到的纹理特征相连接，被输入到全连接层进行分类。我们将所提出的分支分别集成到 VGG16 和 ResNet34 中，并在多个数据集上进行实验。实验结果表明BEFB是轻量级的并且对训练没有副作用。并且在面对 FGSM、PGD 和 C\&W 攻击时，BEFB 集成模型在所有数据集上的准确性都优于原始模型。此外，与原始模型相比，配备鲁棒性增强技术的BEFB集成模型可以实现更好的分类精度。本文的工作首次表明通过结合形状特征和纹理特征来增强 DCNN 的鲁棒性是可行的。</details>
**PDF:** <http://arxiv.org/pdf/2402.16479v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **DEYO: DETR with YOLO for End-to-End Object Detection**<br />
**Title_cn:** DEYO：DETR 与 YOLO 一起用于端到端目标检测<br />
**Authors:** Haodong Ouyang<br />
**Abstract:** <details><summary>原文: </summary>The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset. However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs. Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs. To address these issues, we have devised an innovative training methodology termed step-by-step training. Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector. In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch. Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO). Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy. Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure. Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO.</details>
**Abstract_cn:** <details><summary>译文: </summary>DETR 的训练范式在很大程度上取决于在 ImageNet 数据集上预训练其骨干网。然而，图像分类任务和一对一匹配策略提供的监督信号有限，导致 DETR 的颈部预训练不充分。此外，训练初期匹配的不稳定性导致DETR的优化目标不一致。为了解决这些问题，我们设计了一种创新的培训方法，称为逐步培训。具体来说，在训练的第一阶段，我们采用经典检测器，通过一对多匹配策略进行预训练，来初始化端到端检测器的主干和颈部。在训练的第二阶段，我们冻结了端到端检测器的主干和颈部，需要从头开始训练解码器。通过逐步训练的应用，我们推出了第一个利用纯卷积结构编码器的实时端到端目标检测模型，DETR with YOLO (DEYO)。在不依赖任何补充训练数据的情况下，DEYO 在速度和准确性上都超越了所有现有的实时物体检测器。此外，综合DEYO系列可以使用单个8GB RTX 4060 GPU完成在COCO数据集上的第二阶段训练，大大减少了训练支出。源代码和预训练模型可在 https://github.com/ouyanghaodong/DEYO 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.16370v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation**<br />
**Title_cn:** SPINEPS——使用多类语义和实例分割的两阶段方法对 T2 加权 MR 图像进行自动全脊柱分割<br />
**Authors:** Hendrik Möller, Robert Graf, Joachim Schmitt, Benjamin Keinert, Matan Atad, Anjany Sekuboyina, Felix Streckenbach, Hanna Schön, Florian Kofler, Thomas Kroencke, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Purpose. To present SPINEPS, an open-source deep learning approach for semantic and instance segmentation of 14 spinal structures (ten vertebra substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in whole body T2w MRI.   Methods. During this HIPPA-compliant, retrospective study, we utilized the public SPIDER dataset (218 subjects, 63% female) and a subset of the German National Cohort (1423 subjects, mean age 53, 49% female) for training and evaluation. We combined CT and T2w segmentations to train models that segment 14 spinal structures in T2w sagittal scans both semantically and instance-wise. Performance evaluation metrics included Dice similarity coefficient, average symmetrical surface distance, panoptic quality, segmentation quality, and recognition quality. Statistical significance was assessed using the Wilcoxon signed-rank test. An in-house dataset was used to qualitatively evaluate out-of-distribution samples.   Results. On the public dataset, our approach outperformed the baseline (instance-wise vertebra dice score 0.929 vs. 0.907, p-value<0.001). Training on auto-generated annotations and evaluating on manually corrected test data from the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for intervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER dataset during training increased these scores to 0.920, 0.967, 0.958, respectively.   Conclusions. The proposed segmentation approach offers robust segmentation of 14 spinal structures in T2w sagittal images, including the spinal cord, spinal canal, intervertebral discs, endplate, sacrum, and vertebrae. The approach yields both a semantic and instance mask as output, thus being easy to utilize. This marks the first publicly available algorithm for whole spine segmentation in sagittal T2w MR imaging.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的。介绍 SPINEPS，一种开源深度学习方法，用于全身 T2w MRI 中 14 个脊柱结构（十个椎骨子结构、椎间盘、脊髓、椎管和骶骨）的语义和实例分割。方法。在这项符合 HIPPA 标准的回顾性研究中，我们利用公共 SPIDER 数据集（218 名受试者，63% 女性）和德国国家队列的子集（1423 名受试者，平均年龄 53 岁，49% 女性）进行培训和评估。我们结合 CT 和 T2w 分割来训练模型，在 T2w 矢状扫描中从语义和实例角度分割 14 个脊柱结构。性能评估指标包括Dice相似系数、平均对称表面距离、全景质量、分割质量和识别质量。使用 Wilcoxon 符号秩检验评估统计显着性。使用内部数据集来定性评估分布外样本。结果。在公共数据集上，我们的方法优于基线（实例方面的椎骨骰子得分 0.929 与 0.907，p 值<0.001）。对自动生成的注释进行训练并对来自 GNC 的手动校正测试数据进行评估，得出椎骨的全局 dice 分数为 0.900，椎间盘的全局骰子分数为 0.960，椎管的全局骰子分数为 0.947。在训练期间合并 SPIDER 数据集将这些分数分别提高到 0.920、0.967、0.958。结论。所提出的分割方法可以对 T2w 矢状图像中的 14 个脊柱结构进行稳健分割，包括脊髓、椎管、椎间盘、终板、骶骨和椎骨。该方法产生语义和实例掩码作为输出，因此易于使用。这标志着矢状 T2w MR 成像中第一个公开可用的整个脊柱分割算法。</details>
**PDF:** <http://arxiv.org/pdf/2402.16368v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **What Text Design Characterizes Book Genres?**<br />
**Title_cn:** 书籍类型的文本设计有何特点？<br />
**Authors:** Daichi Haraguchi, Brian Kenji Iwana, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>This study analyzes the relationship between non-verbal information (e.g., genres) and text design (e.g., font style, character color, etc.) through the classification of book genres using text design on book covers. Text images have both semantic information about the word itself and other information (non-semantic information or visual design), such as font style, character color, etc. When we read a word printed on some materials, we receive impressions or other information from both the word itself and the visual design. Basically, we can understand verbal information only from semantic information, i.e., the words themselves; however, we can consider that text design is helpful for understanding other additional information (i.e., non-verbal information), such as impressions, genre, etc. To investigate the effect of text design, we analyze text design using words printed on book covers and their genres in two scenarios. First, we attempted to understand the importance of visual design for determining the genre (i.e., non-verbal information) of books by analyzing the differences in the relationship between semantic information/visual design and genres. In the experiment, we found that semantic information is sufficient to determine the genre; however, text design is helpful in adding more discriminative features for book genres. Second, we investigated the effect of each text design on book genres. As a result, we found that each text design characterizes some book genres. For example, font style is useful to add more discriminative features for genres of ``Mystery, Thriller \& Suspense'' and ``Christian books \& Bibles.''</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究通过书籍封面上的文本设计对书籍类型进行分类，分析非语言信息（例如类型）与文本设计（例如字体样式、字符颜色等）之间的关系。文本图像既具有单词本身的语义信息，也具有其他信息（非语义信息或视觉设计），例如字体样式、字符颜色等。当我们阅读某些材料上打印的单词时，我们会收到印象或其他信息无论是单词本身还是视觉设计。基本上，我们只能从语义信息（即单词本身）来理解言语信息；然而，我们可以认为文本设计有助于理解其他附加信息（即非语言信息），例如印象、类型等。为了研究文本设计的效果，我们使用书籍封面上印刷的文字来分析文本设计以及他们在两种情况下的流派。首先，我们试图通过分析语义信息/视觉设计与体裁之间关系的差异来理解视觉设计对于确定书籍体裁（即非语言信息）的重要性。在实验中，我们发现语义信息足以判断流派；然而，文本设计有助于为书籍类型添加更多区分特征。其次，我们研究了每种文本设计对书籍类型的影响。结果，我们发现每种文本设计都体现了某些书籍类型的特征。例如，字体样式有助于为“神秘、惊悚\&悬疑”和“基督教书籍\&圣经”类型添加更多区分功能。</details>
**PDF:** <http://arxiv.org/pdf/2402.16356v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM**<br />
**Title_cn:** BLO-SAM：基于双层优化的 SAM 防过拟合微调<br />
**Authors:** Li Zhang, Youwei Liang, Pengtao Xie<br />
**Abstract:** <details><summary>原文: </summary>The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding. Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>Segment Anything Model (SAM) 是一种在数百万张图像和分割掩模上进行预训练的基础模型，具有显着先进的语义分割（计算机视觉中的一项基本任务）。尽管有其优势，SAM 仍面临两大挑战。首先，它很难自主分割特定对象，因为它依赖用户手动输入点或边界框等提示来识别目标对象。其次，由于预训练数据（主要由通用领域图像组成）与下游任务中使用的数据之间的分布差异，SAM 在擅长特定下游任务（例如医学成像）方面面临挑战。目前这些问题的解决方案涉及对 SAM 进行微调，通常会导致过度拟合，这在数据非常有限的场景（例如医学成像）中是一个值得注意的问题。为了克服这些限制，我们引入了 BLO-SAM，它基于双层优化 (BLO) 对 SAM 进行微调。我们的方法通过优化可学习的提示嵌入，允许自动图像分割，无需手动提示。此外，它通过训练模型的权重参数和提示嵌入训练数据集的两个独立子集（每个子集处于不同的优化级别）来显着降低过度拟合的风险。我们将 BLO-SAM 应用于一般和医学领域的各种语义分割任务。结果表明，BLO-SAM 比各种最先进的图像语义分割方法具有优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.16338v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models**<br />
**Title_cn:** 更精细：研究和增强大视觉语言模型中的细粒度视觉概念识别<br />
**Authors:** Jeonghwan Kim, Heng Ji<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs. In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability.</details>
**Abstract_cn:** <details><summary>译文: </summary>指令调整大型视觉语言模型 (LVLM) 的最新进展使模型能够轻松生成基于图像的高级解释。虽然这种能力很大程度上归功于大型语言模型（LLM）中包含的丰富的世界知识，但我们的工作揭示了它们在六种不同基准设置的细粒度视觉分类（FGVC）方面的缺点。最近最先进的 LVLM，如 LLaVa-1.5、InstructBLIP 和 GPT-4V，不仅在分类性能方面严重恶化，例如 LLaVA-1.5 的斯坦福狗的 EM 平均下降 65.58，而且还难以尽管它们能够生成整体图像级描述，但仍可以根据输入图像中出现的概念生成具有详细属性的准确解释。深入分析表明，指令调整的 LVLM 表现出模态差距，当给定对应于同一概念的文本和视觉输入时，会显示出差异，从而阻止图像模态利用 LLM 内丰富的参数知识。为了进一步推动社区朝这个方向努力，我们提出了一个以多粒度属性为中心的评估基准 Finer，旨在为评估 LVLM 的细粒度视觉理解能力奠定基础，并显着提高可解释性。</details>
**PDF:** <http://arxiv.org/pdf/2402.16315v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer**<br />
**Title_cn:** MV-Swin-T：使用多视图 Swin Transformer 进行乳房 X 光检查分类<br />
**Authors:** Sushmita Sarker, Prithul Sarker, George Bebis, Alireza Tavakkoli<br />
**Abstract:** <details><summary>原文: </summary>Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at https://github.com/prithuls/MV-Swin-T</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的乳腺癌分类深度学习方法主要集中于单视图分析。然而，在临床实践中，放射科医生同时检查乳房 X 光检查中的所有视图，利用这些视图中的固有相关性来有效检测肿瘤。认识到多视图分析的重要性，一些研究引入了独立处理乳房X光照片视图的方法，无论是通过不同的卷积分支还是简单的融合策略，无意中导致关键的视图间相关性的丢失。在本文中，我们提出了一种完全基于变压器的创新多视图网络，以解决乳腺X线摄影图像分类的挑战。我们的方法引入了一种新颖的基于移位窗口的动态注意块，促进多视图信息的有效集成，并促进该信息在空间特征图级别的视图之间的连贯传输。此外，我们使用 CBIS-DDSM 和 Vin-Dr Mammo 数据集，对不同设置下基于 Transformer 的模型的性能和有效性进行了全面的比较分析。我们的代码可在 https://github.com/prithuls/MV-Swin-T 上公开获取</details>
**PDF:** <http://arxiv.org/pdf/2402.16298v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation**<br />
**Title_cn:** 用于高效注释的核实例分割的少样本学习<br />
**Authors:** Yu Ming, Zihao Wu, Jie Yang, Danyi Li, Yuan Gao, Changxin Gao, Gui-Song Xia, Yuanqing Li, Li Liang, Jin-Gang Yu<br />
**Abstract:** <details><summary>原文: </summary>Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations.</details>
**Abstract_cn:** <details><summary>译文: </summary>从组织病理学图像中分割细胞核实例需要对细胞核实例进行极其费力且依赖于专家的注释。作为该任务的一个有前景的解决方案，注释高效的深度学习范式最近引起了很多研究兴趣，例如弱/半监督学习、生成对抗学习等。在本文中，我们建议制定注释高效的核心实例从少样本学习（FSL）的角度进行分割。我们工作的动机是，随着计算病理学的繁荣，越来越多的完全注释的数据集可以公开访问，我们希望利用这些外部数据集来协助对只有非常有限的注释的目标数据集进行核实例分割。为了实现这一目标，我们采用基于元学习的 FSL 范式，但是在适应我们的任务之前，必须在两个实质性方面进行定制。首先，由于新的类可能与外部数据集的类不一致，因此我们将少样本实例分割（FSIS）的基本定义扩展到广义少样本实例分割（GFSIS）。其次，为了应对细胞核分割的内在挑战，包括相邻细胞之间的接触、细胞异质性等，我们进一步在 GFSIS 网络中引入结构指导机制，最终形成统一的结构引导广义少样本实例分割（SGFSIS）框架。对几个可公开访问的数据集进行的大量实验表明，SGFSIS 可以超越其他注释高效的学习基线，包括半监督学习、简单迁移学习等，其性能与注释率低于 5% 的完全监督学习相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.16280v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking**<br />
**Title_cn:** SeqTrack3D：探索稳健 3D 点云跟踪的序列信息<br />
**Authors:** Yu Lin, Zhiheng Li, Yubo Cui, Zheng Fang<br />
**Abstract:** <details><summary>原文: </summary>3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 单目标跟踪 (SOT) 对于自动驾驶和移动机器人来说是一项重要且具有挑战性的任务。大多数现有方法在两个连续帧之间执行跟踪，而忽略目标在一系列帧上的运动模式，这会导致稀疏点场景中的性能下降。为了突破这一限制，我们引入了序列到序列跟踪范例和名为 SeqTrack3D 的跟踪器来捕获连续帧中的目标运动。与之前主要采用三种策略的方法不同：匹配两个连续点云、预测相对运动或利用顺序点云来解决特征退化问题，我们的 SeqTrack3D 结合了历史点云和边界框序列。这种新颖的方法通过利用历史框中的位置先验来确保稳健的跟踪，即使在具有稀疏点的场景中也是如此。在大规模数据集上进行的大量实验表明，SeqTrack3D 实现了新的最先进的性能，在 NuScenes 上提高了 6.00%，在 Waymo 数据集上提高了 14.13%。该代码将在 https://github.com/aron-lin/seqtrack3d 上公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.16249v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices**<br />
**Title_cn:** 基于移动设备上无人机交通视频的实时车辆检测和城市交通行为分析<br />
**Authors:** Yuan Zhu, Yanqiang Wang, Yadong An, Hong Yang, Yiming Pan<br />
**Abstract:** <details><summary>原文: </summary>This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文重点研究基于无人机交通视频的实时车辆检测和城市交通行为分析系统。利用无人机采集交通数据，结合YOLOv8模型和SORT跟踪算法，在iOS移动平台上实现物体检测和跟踪功能。针对交通数据采集和分析问题，采用动态计算方法对性能进行实时处理，计算出车辆的微观和宏观交通参数，进行实时交通行为分析并可视化。实验结果表明，车辆目标检测准确率达到98.27%，召回率达到87.93%，实时处理能力稳定在30帧/秒。这项工作集成了无人机技术、iOS开发和深度学习技术，在移动设备上集成交通视频采集、物体检测、物体跟踪和交通行为分析功能。它为轻量级交通信息采集和数据分析提供了新的可能性，为交通主管部门提高分析道路交通状况和解决交通问题的效率提供了创新的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.16246v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection**<br />
**Title_cn:** HSONet：用于高分辨率遥感图像变化检测的连体前景关联驱动的硬例样本优化网络<br />
**Authors:** Chao Tao, Dongsheng Kuang, Zhenyang Huang, Chengli Peng, Haifeng Li<br />
**Abstract:** <details><summary>原文: </summary>In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network (HSONet). To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses. To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases. Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples.</details>
**Abstract_cn:** <details><summary>译文: </summary>在后期的训练阶段，模型判断变化能力的进一步提高取决于变化检测（CD）模型对困难案例的学习程度；然而，学习困难案例样本还存在两个额外的挑战：（1）变化标签是有限的，并且往往仅指向前景目标，而困难案例样本在背景中普遍存在，这导致优化专注于前景的损失函数目标而忽视背景疑难案例，我们称之为不平衡。 (2) 光影、目标遮挡、季节变化等复杂情况会诱发困难样本，在缺乏监督信息和场景信息的情况下，模型很难直接学习困难样本来准确获得变化信息的特征表示，我们称之为缺失。我们提出了一种暹罗前景关联驱动的硬案例样本优化网络（HSONet）。为了解决这种不平衡问题，我们提出了平衡优化损失函数来调节前景和背景的优化焦点，通过损失值的分布来确定困难情况样本，并在损失项中引入动态权重来逐步转移优化随着训练的进行，损失的焦点从前台到后台困难情况。为了解决这种缺失，我们借助场景上下文来理解困难样本，提出场景-前景关联模块，利用潜在的遥感空间场景信息对前景中感兴趣的目标与相关上下文之间的关联进行建模，以获得场景嵌入，并将该信息应用于硬案例的特征强化。对四个公共数据集的实验表明，HSONet 优于当前最先进的 CD 方法，特别是在检测困难样本方面。</details>
**PDF:** <http://arxiv.org/pdf/2402.16242v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing**<br />
**Title_cn:** COMAE：零样本哈希的综合属性探索<br />
**Authors:** Yihang Zhou, Qingqing Long, Yuchen Yan, Xiao Luo, Zeyu Dong, Xuezhi Wang, Zhen Meng, Pengfei Wang, Yuanchun Zhou<br />
**Abstract:** <details><summary>原文: </summary>Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios. While considerable success has been achieved, there still exist urgent limitations. Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes. Also, the continuous-value attributes are not fully harnessed. In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints. By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes. Then COMAE utilizes contrastive learning to comprehensively depict the context of attributes, rather than instance-independent optimization. Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively. Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes.</details>
**Abstract_cn:** <details><summary>译文: </summary>零样本哈希（ZSH）由于其在大规模检索场景中的效率和泛化性而取得了巨大的成功。尽管取得了相当大的成功，但仍然存在紧迫的局限性。现有的工作忽略了表示和属性的局部性关系，这些局部性关系在可见类和不可见类之间具有有效的可转移性。此外，连续值属性没有得到充分利用。为此，我们对 ZSH 进行了综合属性探索，名为 COMAE，它通过三个精心设计的探索（即逐点一致性约束、逐对一致性约束和类一致性约束）来描述从可见类到未见类的关系。通过从所提出的属性原型网络回归属性，COMAE 学习与视觉属性相关的局部特征。然后 COMAE 利用对比学习来全面描述属性的上下文，而不是与实例无关的优化。最后，类约束被设计为更有效地结合学习哈希码、图像表示和视觉属性。流行的 ZSH 数据集上的实验结果表明，COMAE 的性能优于最先进的哈希技术，特别是在具有大量未见标签类的场景中。</details>
**PDF:** <http://arxiv.org/pdf/2402.16424v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **DCVSMNet: Double Cost Volume Stereo Matching Network**<br />
**Title_cn:** DCVSMNet：双成本体积立体匹配网络<br />
**Authors:** Mahmoud Tahmasebi, Saif Huq, Kevin Meehan, Marion McAfee<br />
**Abstract:** <details><summary>原文: </summary>We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes. Each cost volume is processed separately, and a coupling module is proposed to fuse the geometry information extracted from the upper and lower cost volumes. DCVSMNet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods. The results on several bench mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo and BGNet at the cost of greater inference time.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了双成本体积立体匹配网络（DCVSMNet），这是一种新颖的架构，其特征是两个小的上部（分组）和下部（规范相关）成本体积。每个成本卷被单独处理，并且提出了一个耦合模块来融合从上部和下部成本卷中提取的几何信息。 DCVSMNet 是一种快速立体匹配网络，具有 67 ms 的推理时间和强大的泛化能力，与最先进的方法相比，可以产生有竞争力的结果。多个基准数据集的结果表明，DCVSMNet 比 CGI-Stereo 和 BGNet 等方法获得了更好的精度，但代价是更长的推理时间。</details>
**PDF:** <http://arxiv.org/pdf/2402.16473v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions**<br />
**Title_cn:** 基于成对子模函数的分布式大于内存子集选择<br />
**Authors:** Maximilian Böther, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic, Srikumar Ramalingam<br />
**Abstract:** <details><summary>原文: </summary>Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset. We show that these algorithms find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多学习问题都取决于子集选择的基本问题，即识别重要且有代表性的点的子集。例如，在机器学习训练中选择最重要的样本不仅可以降低训练成本，还可以提高模型质量。子模性是凸性的离散模拟，通常用于解决子集选择问题。然而，现有的优化子模函数的算法是顺序的，并且现有的分布式方法需要至少一台中央机来拟合目标子集。在本文中，我们通过提出一种具有可证明的近似保证的新型分布式边界算法，放宽了对目标子集拥有中央机器的要求。该算法迭代地限制最小和最大效用值，以选择高质量点并丢弃不重要的点。当边界没有找到完整的子集时，我们使用多轮、基于分区的分布式贪心算法来识别剩余的子集。我们表明，这些算法能够在 CIFAR-100 和 ImageNet 上找到高质量的子集，与集中式方法相比，质量几乎没有损失或没有损失，并且可以扩展到包含 130 亿个点的数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.16442v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Analysis of Embeddings Learned by End-to-End Machine Learning Eye Movement-driven Biometrics Pipeline**<br />
**Title_cn:** 通过端到端机器学习眼动驱动的生物识别管道学习的嵌入分析<br />
**Authors:** Mehedi Hasan Raju, Lee Friedman, Dillon J Lohr, Oleg V Komogortsev<br />
**Abstract:** <details><summary>原文: </summary>This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning. Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data. Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis. We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics. Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings. We also explored the reliability and consistency of the embeddings under varying data conditions. Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings. The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文扩展了生物识别系统中时间持久性的基本概念，特别关注机器学习促进的眼动生物识别领域。与之前主要专注于开发生物识别认证系统的研究不同，我们的研究深入研究了这些系统学习到的嵌入，特别是检查它们响应不同输入数据的时间持久性、可靠性和生物识别功效。利用两个公开可用的眼动数据集，我们采用最先进的 Eye Know You Too 机器学习管道进行分析。我们的目标是验证眼动生物识别中机器学习衍生的嵌入是否反映了传统生物识别中观察到的时间持久性。我们的方法涉及进行广泛的实验，以评估输入数据的不同长度和质量如何影响眼动生物识别技术的性能，更具体地说是如何影响学习的嵌入。我们还探讨了不同数据条件下嵌入的可靠性和一致性。采用三个关键指标（肯德尔一致性系数、互相关性和等错误率）来定量评估我们的发现。结果表明，虽然数据长度显着影响学习嵌入的稳定性，但是嵌入之间的相互相关性影响很小。</details>
**PDF:** <http://arxiv.org/pdf/2402.16399v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts**<br />
**Title_cn:** Impression-CLIP：字体的对比形状印象嵌入<br />
**Authors:** Yugo Kubota, Daichi Haraguchi, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>Fonts convey different impressions to readers. These impressions often come from the font shapes. However, the correlation between fonts and their impression is weak and unstable because impressions are subjective. To capture such weak and unstable cross-modal correlation between font shapes and their impressions, we propose Impression-CLIP, which is a novel machine-learning model based on CLIP (Contrastive Language-Image Pre-training). By using the CLIP-based model, font image features and their impression features are pulled closer, and font image features and unrelated impression features are pushed apart. This procedure realizes co-embedding between font image and their impressions. In our experiment, we perform cross-modal retrieval between fonts and impressions through co-embedding. The results indicate that Impression-CLIP achieves better retrieval accuracy than the state-of-the-art method. Additionally, our model shows the robustness to noise and missing tags.</details>
**Abstract_cn:** <details><summary>译文: </summary>字体向读者传达不同的印象。这些印象通常来自字体形状。然而，字体与其印象之间的相关性很弱且不稳定，因为印象是主观的。为了捕捉字体形状与其印象之间这种弱且不稳定的跨模态相关性，我们提出了 Impression-CLIP，这是一种基于 CLIP（对比语言-图像预训练）的新型机器学习模型。通过使用基于CLIP的模型，字体图像特征和它们的印象特征被拉近，并且字体图像特征和不相关的印象特征被推开。该过程实现了字体图像与其印象之间的共同嵌入。在我们的实验中，我们通过共同嵌入在字体和印象之间执行跨模式检索。结果表明，Impression-CLIP 比最先进的方法具有更好的检索精度。此外，我们的模型显示了对噪声和缺失标签的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.16350v1><br />
**Code:** null<br />

