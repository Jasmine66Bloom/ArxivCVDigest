## [UPDATED!] **2024-02-27** (Publish Time)

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **ShapeLLM: Universal 3D Object Understanding for Embodied Interaction**<br />
**Title_cn:** ShapeLLM：用于实体交互的通用 3D 对象理解<br />
**Authors:** Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma<br />
**Abstract:** <details><summary>原文: </summary>This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated evaluation benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 ShapeLLM，这是第一个专为体现交互而设计的 3D 多模态大语言模型 (LLM)，探索使用 3D 点云和语言的通用 3D 对象理解。 ShapeLLM 基于改进的 3D 编码器，通过将 ReCon 扩展到 ReCon++，受益于多视图图像蒸馏以增强几何理解。通过利用 ReCon++ 作为 LLM 的 3D 点云输入编码器，ShapeLLM 在构建的指令跟踪数据上进行训练，并在我们新的人工策划的评估基准 3D MM-Vet 上进行测试。 ReCon++ 和 ShapeLLM 在 3D 几何理解和语言统一的 3D 交互任务（例如具体视觉基础）中实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.17766v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Adaptive quantization with mixed-precision based on low-cost proxy**<br />
**Title_cn:** 基于低成本代理的混合精度自适应量化<br />
**Authors:** Junzhe Chen, Qiao Yang, Senmao Tian, Shunli Zhang<br />
**Abstract:** <details><summary>原文: </summary>It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.</details>
**Abstract_cn:** <details><summary>译文: </summary>在资源有限的硬件上部署复杂的神经网络模型至关重要。本文提出了一种新颖的模型量化方法，称为基于低成本代理的自适应混合精度模型量化（LCPAQ），该方法包含三个关键模块。考虑到硬件限制，设计了硬件感知模块，同时开发了自适应混合精度量化模块，利用 Hessian 矩阵和 Pareto 前沿技术来评估量化灵敏度。整数线性规划用于微调不同层的量化。然后，低成本代理神经架构搜索模块有效地探索理想的量化超参数。 ImageNet 上的实验表明，所提出的 LCPAQ 实现了与现有混合精度模型相当或更高的量化精度。值得注意的是，与现有方法相比，LCPAQ 的搜索时间缩短了 1/200，这为资源有限的设备的实际量化使用提供了捷径。</details>
**PDF:** <http://arxiv.org/pdf/2402.17706v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning**<br />
**Title_cn:** MCF-VC：减轻多模态视频字幕的类增量学习中的灾难性遗忘<br />
**Authors:** Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li<br />
**Abstract:** <details><summary>原文: </summary>To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress. In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning. After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC). As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks. Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task. Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class. In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate. Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决由于顺序输入中旧类别不可见而导致的灾难性遗忘问题，基于相对简单的分类任务的现有工作已经取得了一些进展。相比之下，视频字幕在多模态场景中是一个更复杂的任务，在增量学习领域尚未得到探索。在分析具有顺序输入的视频时识别出这种稳定性可塑性问题后，我们最初提出了一种在多模态视频字幕（MCF-VC）的类增量学习中减轻灾难性遗忘的方法。为了在宏观层面有效保持旧任务的良好性能，我们设计了基于线性参数掩模和费舍尔灵敏度的细粒度灵敏度选择（FgSS），以从旧任务中挑选有用的知识。此外，为了更好地在特定特征级别上约束新旧任务的知识特征，我们创建了两阶段知识蒸馏（TsKD），它能够在权衡旧任务的同时很好地学习新任务。具体来说，我们设计了两个蒸馏损失，分别约束语义注意特征图的跨模态语义信息和最终输出的文本信息，从而在学习时保留旧类的模型间和模型内的风格化知识新班级。为了说明我们的模型抵抗遗忘的能力，我们设计了一个度量CIDER_t来检测阶段遗忘率。我们在公共数据集MSR-VTT上的实验表明，所提出的方法在不重放旧样本的情况下显着地抵抗了先前任务的遗忘，并且在新任务上表现良好。</details>
**PDF:** <http://arxiv.org/pdf/2402.17680v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Neural Video Compression with Feature Modulation**<br />
**Title_cn:** 具有特征调制的神经视频压缩<br />
**Authors:** Jiahao Li, Bin Li, Yan Lu<br />
**Abstract:** <details><summary>原文: </summary>The emerging conditional coding-based neural video codec (NVC) shows superiority over commonly-used residual coding-based codec and the latest NVC already claims to outperform the best traditional codec. However, there still exist critical problems blocking the practicality of NVC. In this paper, we propose a powerful conditional coding-based NVC that solves two critical problems via feature modulation. The first is how to support a wide quality range in a single model. Previous NVC with this capability only supports about 3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent feature of the current frame via the learnable quantization scaler. During the training, we specially design the uniform quantization parameter sampling mechanism to improve the harmonization of encoding and quantization. This results in a better learning of the quantization scaler and helps our NVC support about 11.4 dB PSNR range. The second is how to make NVC still work under a long prediction chain. We expose that the previous SOTA NVC has an obvious quality degradation problem when using a large intra-period setting. To this end, we propose modulating the temporal feature with a periodically refreshing mechanism to boost the quality. %Besides solving the above two problems, we also design a single model that can support both RGB and YUV colorspaces. Notably, under single intra-frame setting, our codec can achieve 29.7\% bitrate saving over previous SOTA NVC with 16\% MACs reduction. Our codec serves as a notable landmark in the journey of NVC evolution. The codes are at https://github.com/microsoft/DCVC.</details>
**Abstract_cn:** <details><summary>译文: </summary>新兴的基于条件编码的神经视频编解码器 (NVC) 显示出优于常用的基于残差编码的编解码器的优越性，并且最新的 NVC 已经声称优于最好的传统编解码器。然而，仍然存在阻碍 NVC 实用化的关键问题。在本文中，我们提出了一种强大的基于条件编码的 NVC，它通过特征调制解决了两个关键问题。首先是如何在单一模型中支持广泛的质量范围。以前具有此功能的 NVC 平均仅支持约 3.8 dB PSNR 范围。为了解决这个限制，我们通过可学习的量化缩放器来调制当前帧的潜在特征。在训练过程中，我们专门设计了统一量化参数采样机制，以提高编码和量化的协调性。这可以更好地学习量化缩放器，并帮助我们的 NVC 支持约 11.4 dB PSNR 范围。第二个是如何让NVC在长预测链下仍然工作。我们发现，之前的 SOTA NVC 在使用较大的周期内设置时存在明显的质量下降问题。为此，我们建议通过定期刷新机制来调制时间特征以提高质量。 %除了解决上述两个问题之外，我们还设计了一个可以同时支持RGB和YUV色彩空间的单一模型。值得注意的是，在单帧内设置下，我们的编解码器比之前的 SOTA NVC 可以节省 29.7% 的比特率，并减少 16% 的 MAC。我们的编解码器是 NVC 发展历程中的一个显着里程碑。代码位于 https://github.com/microsoft/DCVC。</details>
**PDF:** <http://arxiv.org/pdf/2402.17414v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation**<br />
**Title_cn:** 通过选择性熵蒸馏实现稳健高效的云边缘弹性模型适应<br />
**Authors:** Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan<br />
**Abstract:** <details><summary>原文: </summary>The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的深度学习范式通常涉及在服务器上训练深度模型，然后将该模型或其精炼模型部署到资源有限的边缘设备。通常，由于服务器端和边缘端的模型适配成本可能很高，模型一旦部署（至少在一段时间内）就应保持固定。然而，在许多现实场景中，测试环境可能会动态变化（称为分布变化），这通常会导致性能下降。因此，人们必须及时调整边缘模型以获得有希望的性能。此外，随着在边缘收集的数据不断增加，这种范式也无法进一步适应云模型以获得更好的性能。为了解决这些问题，我们遇到两个主要挑战：1）边缘模型的计算能力有限，可能只支持前向传播； 2）在延迟敏感的场景中，云和边缘设备之间的数据传输预算是有限的。在本文中，我们建立了一种云边缘弹性模型自适应（CEMA）范式，其中边缘模型只需要执行前向传播，并且边缘模型可以在线自适应。在我们的CEMA中，为了减轻通信负担，我们设计了两个标准来排除不必要的样本上传到云端，即动态不可靠和低信息样本排除。基于上传的样本，我们通过样本重播策略从更强的基础模型提取到边缘模型来更新和分配归一化层的仿射参数。 ImageNet-C 和 ImageNet-R 上的大量实验结果验证了我们 CEMA 的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17316v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment**<br />
**Title_cn:** LiveHPS：自由环境下基于激光雷达的场景级人体姿态和形状估计<br />
**Authors:** Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma<br />
**Abstract:** <details><summary>原文: </summary>For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于以人为中心的大规模场景，3D 人体全局姿势和形状的细粒度建模对于场景理解具有重要意义，并且可以使许多实际应用受益。在本文中，我们提出了 LiveHPS，这是一种基于单激光雷达的新颖方法，用于场景级人体姿势和形状估计，不受任何光照条件和可穿戴设备的限制。特别是，我们设计了一种蒸馏机制来减轻激光雷达点云的分布变化效应，并利用连续帧中存在的时空几何和动态信息来解决遮挡和噪声干扰。 LiveHPS 以其高效的配置和高质量的输出，非常适合实际应用。此外，我们提出了一个巨大的人体运动数据集，名为 FreeMotion，它是在各种场景中收集的不同人体姿势、形状和平移的数据集。它由来自校准和同步 LiDAR、摄像机和 IMU 的多模式和多视图采集数据组成。对我们的新数据集和其他公共数据集进行的广泛实验证明了我们方法的 SOTA 性能和稳健性。我们将很快发布我们的代码和数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.17171v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization**<br />
**Title_cn:** 用于多类异常检测和定位的结构性师生正态学习<br />
**Authors:** Hanqiu Deng, Xingyu Li<br />
**Abstract:** <details><summary>原文: </summary>Visual anomaly detection is a challenging open-set task aimed at identifying unknown anomalous patterns while modeling normal data. The knowledge distillation paradigm has shown remarkable performance in one-class anomaly detection by leveraging teacher-student network feature comparisons. However, extending this paradigm to multi-class anomaly detection introduces novel scalability challenges. In this study, we address the significant performance degradation observed in previous teacher-student models when applied to multi-class anomaly detection, which we identify as resulting from cross-class interference. To tackle this issue, we introduce a novel approach known as Structural Teacher-Student Normality Learning (SNL): (1) We propose spatial-channel distillation and intra-&inter-affinity distillation techniques to measure structural distance between the teacher and student networks. (2) We introduce a central residual aggregation module (CRAM) to encapsulate the normal representation space of the student network. We evaluate our proposed approach on two anomaly detection datasets, MVTecAD and VisA. Our method surpasses the state-of-the-art distillation-based algorithms by a significant margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class anomaly detection and localization tasks, respectively. Furthermore, our algorithm outperforms the current state-of-the-art unified models on both MVTecAD and VisA.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉异常检测是一项具有挑战性的开放集任务，旨在在对正常数据进行建模的同时识别未知的异常模式。通过利用师生网络特征比较，知识蒸馏范式在一类异常检测中表现出了卓越的性能。然而，将此范例扩展到多类异常检测会带来新的可扩展性挑战。在这项研究中，我们解决了以前的师生模型在应用于多类异常检测时观察到的显着性能下降问题，我们将其确定为跨类干扰造成的。为了解决这个问题，我们引入了一种称为结构性师生常态学习（SNL）的新方法：（1）我们提出了空间通道蒸馏和内部和内部亲和力蒸馏技术来测量教师和学生网络之间的结构距离。 （2）我们引入了一个中央残差聚合模块（CRAM）来封装学生网络的正常表示空间。我们在两个异常检测数据集 MVTecAD 和 VisA 上评估我们提出的方法。在多类异常检测和定位任务中，我们的方法在 MVTecAD 上超过了最先进的基于蒸馏的算法，分别超过了 3.9% 和 1.5%，在 VisA 上超过了 1.2% 和 2.5%。此外，我们的算法在 MVTecAD 和 VisA 上均优于当前最先进的统一模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.17091v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living**<br />
**Title_cn:** ADL4D：为日常生活的 4D 活动建立上下文丰富的数据集<br />
**Authors:** Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach<br />
**Abstract:** <details><summary>原文: </summary>Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time. We inte- grate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).</details>
**Abstract_cn:** <details><summary>译文: </summary>手-物体交互（HOI）以空间和时间上下文为条件，例如周围的物体、先前的动作和未来的意图（例如，抓取和切换动作根据物体的接近度和轨迹障碍而有很大差异）。然而，现有的 4D HOI（随时间变化的 3D HOI）数据集仅限于一个主体与一个对象交互。这限制了在这些数据集上训练的基于学习的 HOI 方法的泛化。我们引入了 ADL4D，这是一个最多包含两个受试者的数据集，这些受试者与执行日常生活活动（ADL）（例如早餐或午餐准备活动）的不同对象组进行交互。随着时间的推移，多个对象之间完成特定任务的转换引入了现有数据集中缺乏的独特上下文。我们的数据集由 75 个序列组成，总共 110 万个 RGB-D 帧、手部和物体姿势以及每只手的细粒度动作注释。我们开发了一种用于多视图多手 3D 姿势注释的自动系统，能够随时间跟踪手部姿势。我们针对公开可用的数据集对其进行集成和测试。最后，我们在手部网格恢复（HMR）和手部动作分割（HAS）任务上评估我们的数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.17758v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **VRP-SAM: SAM with Visual Reference Prompt**<br />
**Title_cn:** VRP-SAM：带有视觉参考提示的 SAM<br />
**Authors:** Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新颖的视觉参考提示（VRP）编码器，使分段任何模型（SAM）能够利用带注释的参考图像作为分割提示，从而创建 VRP-SAM 模型。本质上，VRP-SAM可以利用带注释的参考图像来理解特定对象并对目标图像中的特定对象进行分割。值得注意的是，VRP编码器可以支持参考图像的多种注释格式，包括\textbf{point}、\textbf{box}、\textbf{scribble}和\textbf{mask}。 VRP-SAM在SAM框架内实现了突破，扩展了其多功能性和适用性，同时保留了SAM的固有优势，从而增强了用户友好性。为了增强VRP-SAM的泛化能力，VRP编码器采用元学习策略。为了验证 VRP-SAM 的有效性，我们对 Pascal 和 COCO 数据集进行了广泛的实证研究。值得注意的是，VRP-SAM 在视觉参考分割方面以最少的可学习参数实现了最先进的性能。此外，VRP-SAM 表现出强大的泛化能力，使其能够对不可见的对象进行分割并实现跨域分割。</details>
**PDF:** <http://arxiv.org/pdf/2402.17726v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation**<br />
**Title_cn:** MedContext：学习上下文线索以实现高效的体积医学分割<br />
**Authors:** Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan<br />
**Abstract:** <details><summary>原文: </summary>Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain. To address this limitation, existing works typically perform transfer learning or design dedicated pretraining-finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation. Our approach effectively learns self supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios. Our code and pretrained models are available at https://github.com/hananshafi/MedContext</details>
**Abstract_cn:** <details><summary>译文: </summary>体积医学分割是 3D 医学图像分析的关键组成部分，可描绘不同的语义区域。深度神经网络显着改进了体积医学分割，但它们通常需要大规模注释数据才能实现更好的性能，而获得这些数据可能成本高昂且令人望而却步。为了解决这个限制，现有的工作通常执行迁移学习或设计专用的预训练微调阶段来学习代表性特征。然而，源域和目标域之间的不匹配使得学习体积数据的最佳表示变得具有挑战性，而多阶段训练需要更高的计算能力以及仔细选择特定于阶段的设计选择。相比之下，我们提出了一个名为 MedContext 的通用训练框架，它与架构无关，可以合并到任何现有的 3D 医学分割训练框架中。我们的方法有效地学习自我监督的上下文线索与监督体素分割任务，而不需要大规模注释体积医学数据或专用的预训练微调阶段。所提出的方法通过学习在输出分割空间中重建缺失的器官或器官的一部分来引入网络中的上下文知识。 MedContext 的有效性在多个 3D 医学数据集和四种最先进的模型架构中得到了验证。我们的方法展示了跨数据集和不同架构的分割性能的一致增益，即使在少量数据场景中也是如此。我们的代码和预训练模型可在 https://github.com/hananshafi/MedContext 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.17725v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification**<br />
**Title_cn:** SDF2Net：用于 PolSAR 图像分类的浅层到深层特征融合网络<br />
**Authors:** Mohammed Q. Alkhatib, M. Sami Zitouni, Mina Al-Saad, Nour Aburaed, Hussain Al-Ahmad<br />
**Abstract:** <details><summary>原文: </summary>Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.</details>
**Abstract_cn:** <details><summary>译文: </summary>偏振合成孔径雷达 (PolSAR) 图像包含有价值的信息，可以促进广泛的土地覆盖解释并生成不同的输出产品。从 PolSAR 数据中提取有意义的特征所面临的挑战与光学图像中遇到的挑战不同。深度学习 (DL) 方法为克服 PolSAR 特征提取中的这些挑战提供了有效的解决方案。卷积神经网络 (CNN) 利用内核功能考虑局部信息和 PolSAR 数据的复值性质，在捕获 PolSAR 图像特征方面发挥着至关重要的作用。在本研究中，提出了一种新颖的三分支复值 CNN 融合，称为浅到深特征融合网络（SDF2Net），用于 PolSAR 图像分类。为了验证所提出方法的性能，使用弗莱福兰和旧金山的机载合成孔径雷达（AIRSAR）数据集以及 ESAR Oberpfaffenhofen 数据集将分类结果与多种最先进的方法进行了比较。结果表明，所提出的方法显示了整体精度的提高，AIRSAR 数据集提高了 1.3% 和 0.8%，ESAR 数据集提高了 0.5%。对 Flevoland 数据进行的分析强调了 SDF2Net 模型的有效性，即使在只有 1% 的采样率的情况下，整体准确率也高达 96.01%。</details>
**PDF:** <http://arxiv.org/pdf/2402.17672v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data**<br />
**Title_cn:** 通过未标记数据的不确定性估计来减轻语义分割中的分布变化<br />
**Authors:** David S. W. Williams, Daniele De Martini, Matthew Gadd, Paul Newman<br />
**Abstract:** <details><summary>原文: </summary>Knowing when a trained segmentation model is encountering data that is different to its training data is important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs). This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation. To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解经过训练的分割模型何时遇到与其训练数据不同的数据非常重要。从性能和保证的角度来看，理解和减轻其影响在其应用中发挥着重要作用——这是自动驾驶汽车 (AV) 等应用中的一个安全问题。这项工作提出了一种分割网络，可以检测由具有挑战性的测试域引起的错误，而无需在单次前向传递中进行任何附加注释。由于注释成本限制了标记数据集的多样性，因此我们使用易于获取、未经整理和未标记的数据来学习通过有选择地强制数据增强的一致性来执行不确定性估计。为此，使用了基于 SAX 数据集的新颖分割基准，其中包括跨越三个自动驾驶领域的标记测试数据，范围从密集的城市到越野。所提出的方法名为 Gamma-SSL，在这一困难的基准上始终优于不确定性估计和分布外 (OoD) 技术 - 接收者操作特性 (ROC) 曲线下面积高达 10.7%，而下面积高达 19.2%。三种场景中最具挑战性的精确召回（PR）曲线。</details>
**PDF:** <http://arxiv.org/pdf/2402.17653v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling**<br />
**Title_cn:** Masked Gamma-SSL：通过掩模图像建模学习不确定性估计<br />
**Authors:** David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini<br />
**Abstract:** <details><summary>原文: </summary>This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出了一种语义分割网络，可以在单次前向传递中产生高质量的不确定性估计。我们通过掩模图像建模（MIM）方法利用基础模型和未标记数据集的一般表示，该方法对增强超参数具有鲁棒性，并且比以前的技术更简单。对于安全关键应用中使用的神经网络，训练数据中的偏差可能会导致错误；因此，了解网络在运行时的限制并采取相应的行动至关重要。为此，我们在多个测试领域测试了我们提出的方法，包括 SAX 分段基准，其中包括来自密集城市、农村和越野驾驶领域的标记测试数据。在这个困难的基准测试中，所提出的方法始终优于不确定性估计和分布外（OoD）技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.17622v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation**<br />
**Title_cn:** 先适应后比较：跨域少样本分割的新视角<br />
**Authors:** Jonas Herzog<br />
**Abstract:** <details><summary>原文: </summary>Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.</details>
**Abstract_cn:** <details><summary>译文: </summary>当面对来自与训练域不同的域的图像时，少镜头分割性能会大幅下降，从而有效地限制了现实世界的用例。为了缓解这个问题，最近出现了跨域少样本分割（CD-FSS）。解决此任务的工作主要尝试以跨域泛化的方式学习源域上的分段。令人惊讶的是，我们可以超越这些方法，同时消除训练阶段并删除它们的主要分割网络。我们证明测试时任务适应是 CD-FSS 成功的关键。任务适应是通过将小型网络附加到传统分类预训练主干的特征金字塔来实现的。为了避免在监督微调中过度拟合少数标记样本，输入图像的增强视图之间的一致性可以在学习附加层的参数时起到指导作用。尽管我们自我限制在测试时不使用除少数标记样本之外的任何图像，但我们在 CD-FSS 中实现了新的最先进的性能，这证明需要重新考虑该任务的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17614v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images**<br />
**Title_cn:** 用于检测电致发光太阳能电池图像缺陷的预训练范式的大规模评估<br />
**Authors:** David Torpey, Lawrence Pratt, Richard Klein<br />
**Abstract:** <details><summary>原文: </summary>Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques. We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance. The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>预训练已被证明可以提高许多领域的性能，包括语义分割，特别是在标记数据有限的领域。在这项工作中，我们对电致发光图像中太阳能电池缺陷检测（SCDD）的各种预训练方法进行了大规模评估和基准测试，这是一个标记数据集有限的领域。我们涵盖了语义分割的监督训练、半监督学习和两种自监督技术。我们还尝试了分布内和分布外 (OOD) 预训练，并观察这如何影响下游性能。结果表明，大型 OOD 数据集 (COCO) 上的监督训练、大型 OOD 数据集 (ImageNet) 上的自监督预训练和半监督预训练 (CCT) 都在平均交并集 (mIoU) 方面产生了统计上相当的性能。我们实现了 SCDD 的最新技术，并证明某些预训练方案可以在代表性不足的类别上带来优异的性能。此外，我们还提供了一个包含 22000 美元图像的大规模未标记 EL 图像数据集，以及一个包含 642 美元图像的标记语义分割 EL 数据集，用于进一步研究开发该领域的自监督和半监督训练技术。</details>
**PDF:** <http://arxiv.org/pdf/2402.17611v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label**<br />
**Title_cn:** Scribble 隐藏类：利用其类标签促进基于 Scribble 的弱监督语义分割<br />
**Authors:** Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu<br />
**Abstract:** <details><summary>原文: </summary>Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用稀疏涂鸦监督的基于涂鸦的弱监督语义分割正在获得关注，因为与完全注释的替代方案相比，它降低了注释成本。现有方法主要通过将标记像素扩散到具有局部监督线索的未标记像素来生成伪标签。然而，这种扩散过程未能利用全局语义和特定于类的线索，而这对于语义分割很重要。在本研究中，我们提出了一种类驱动的涂鸦推广网络，它利用图像级类和全局语义提供的涂鸦注释和伪标签进行监督。直接采用伪标签可能会误导分割模型，因此我们设计了一个定位校正模块来纠正特征空间中的前景表示。为了进一步结合两种监督的优点，我们还引入了距离熵损失来减少不确定性，它根据涂鸦和伪标签边界确定的可靠区域来调整每像素的置信权重。在具有不同质量的涂鸦注释的 ScribbleSup 数据集上进行的实验优于之前的所有方法，证明了我们方法的优越性和鲁棒性。代码位于 https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network 。</details>
**PDF:** <http://arxiv.org/pdf/2402.17555v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM**<br />
**Title_cn:** 具有自适应分辨率 SAM 的稳健无监督人群计数和定位<br />
**Authors:** Jia Wan, Qiangqiang Wu, Wei Lin, Antoni B. Chan<br />
**Abstract:** <details><summary>原文: </summary>The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的人群计数模型需要大量的训练数据，注释起来非常耗时。为了解决这个问题，我们提出了一种简单而有效的人群计数方法，利用分段一切无处不在模型（SEEM）（分段任何模型（SAM）的改编版）来生成用于训练人群计数模型的伪标签。然而，我们的初步调查显示，SEEM 在密集人群场景中的表现有限，主要是由于在高密度区域遗漏了许多人。为了克服这一限制，我们提出了一种自适应分辨率 SEEM 来处理人群场景中人员的尺度变化、遮挡和重叠。除此之外，我们引入了一种基于高斯混合模型的鲁棒定位方法，用于预测预测的人物面具中的头部位置。给定掩模和点伪标签，我们提出了一个鲁棒的损失函数，该函数旨在根据 SEEM 的预测排除不确定区域，从而增强计数网络的训练过程。最后，我们提出了一种生成伪标签的迭代方法。该方法旨在通过识别高密度区域中更多的微小人物来提高分割掩模的质量，而这些人物在第一个伪标记阶段经常被遗漏。总体而言，我们提出的方法在人群计数中实现了最佳的无监督性能，同时也与一些监督方法的结果相当。这使其成为一种高效且多功能的人群计数工具，特别是在无法获得标记数据的情况下。</details>
**PDF:** <http://arxiv.org/pdf/2402.17514v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation**<br />
**Title_cn:** FedLPPA：学习联合弱监督医学图像分割的个性化提示和聚合<br />
**Authors:** Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang<br />
**Abstract:** <details><summary>原文: </summary>Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>联邦学习（FL）有效缓解了政策和隐私问题带来的数据孤岛挑战，隐式地利用更多数据进行深度模型训练。然而，传统的集中式 FL 模型需要应对不同的多中心数据，尤其是在面对显着的数据异质性时，尤其是在医疗环境中。在医学图像分割领域，削减注释成本的迫切需要放大了利用点、涂鸦等稀疏注释的弱监督技术的重要性。实用的 FL 范式应适应不同站点的不同注释格式，这研究课题仍有待研究。在这种背景下，我们提出了一种具有可学习提示和聚合功能的新型个性化 FL 框架（FedLPPA），以统一利用异构弱监督进行医学图像分割。在FedLPPA中，维护了一个可学习的通用知识提示，并辅以多个可学习的个性化数据分布提示和代表监督稀疏性的提示。通过双重注意机制与样本特征集成，这些提示使每个本地任务解码器能够熟练地适应本地分布和监督形式。同时，引入了一种基于即时相似性的双解码器策略，以增强弱监督学习中伪标签的生成，减轻局部数据固有的过度拟合和噪声积累，同时采用自适应聚合方法来定制任务基于参数的解码器。对涉及不同模式的三种不同医学图像分割任务的广泛实验强调了 FedLPPA 的优越性，其功效与完全监督的集中训练的功效非常相似。我们的代码和数据将可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.17502v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Automated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging**<br />
**Title_cn:** 使用原始超声成像对儿童语音中的语音片段进行自动分类<br />
**Authors:** Saja Al Ani, Joanne Cleland, Ahmed Zoha<br />
**Abstract:** <details><summary>原文: </summary>Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication. Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial. Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in speech therapy clinics.</details>
**Abstract_cn:** <details><summary>译文: </summary>言语障碍（SSD）被定义为言语声音产生的持续障碍，导致言语清晰度下降和言语交流障碍。早期识别和干预 SSD 儿童并及时转诊至言语和语言治疗师 (SLT) 进行治疗至关重要。言语障碍的自动检测被认为是检查和筛查大量人群的有效方法。本研究的重点是通过提出一种将超声舌成像（UTI）与深度学习模型相结合的技术解决方案来推进幼儿期SSD的自动诊断。引入的 FusionNet 模型将 UTI 数据与提取的纹理特征相结合，对 UTI 进行分类。总体目标是提高 UTI 分析的准确性和效率，特别是对与 SSD 相关的语音进行分类。本研究将 FusionNet 方法与标准深度学习方法进行了比较，强调了 FusionNet 模型在 UTI 分类方面的出色改进结果以及多元学习在改善言语治疗诊所中 UTI 分类方面的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.17482v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Model X-ray:Detect Backdoored Models via Decision Boundary**<br />
**Title_cn:** 模型 X 射线：通过决策边界检测后门模型<br />
**Authors:** Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks (DNNs) have revolutionized various industries, leading to the rise of Machine Learning as a Service (MLaaS). In this paradigm, well-trained models are typically deployed through APIs. However, DNNs are susceptible to backdoor attacks, which pose significant risks to their applications. This vulnerability necessitates a method for users to ascertain whether an API is compromised before usage. Although many backdoor detection methods have been developed, they often operate under the assumption that the defender has access to specific information such as details of the attack, soft predictions from the model API, and even the knowledge of the model parameters, limiting their practicality in MLaaS scenarios. To address it, in this paper, we begin by presenting an intriguing observation: the decision boundary of the backdoored model exhibits a greater degree of closeness than that of the clean model. Simultaneously, if only one single label is infected, a larger portion of the regions will be dominated by the attacked label. Building upon this observation, we propose Model X-ray, a novel backdoor detection approach for MLaaS through the analysis of decision boundaries. Model X-ray can not only identify whether the target API is infected by backdoor attacks but also determine the target attacked label under the all-to-one attack strategy. Importantly, it accomplishes this solely by the hard prediction of clean inputs, regardless of any assumptions about attacks and prior knowledge of the training details of the model. Extensive experiments demonstrated that Model X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络 (DNN) 彻底改变了各个行业，导致机器学习即服务 (MLaaS) 的兴起。在这种范例中，训练有素的模型通常通过 API 进行部署。然而，DNN 很容易受到后门攻击，这对其应用程序构成重大风险。此漏洞需要一种方法让用户在使用之前确定 API 是否受到损害。尽管已经开发了许多后门检测方法，但它们通常假设防御者可以访问特定信息，例如攻击细节、模型 API 的软预测，甚至模型参数的知识，从而限制了它们在以下领域的实用性： MLaaS 场景。为了解决这个问题，在本文中，我们首先提出一个有趣的观察：后门模型的决策边界比干净模型的决策边界表现出更大程度的接近度。同时，如果只有一个标签被感染，则更大一部分区域将被受攻击的标签所控制。基于这一观察，我们提出了 Model X-ray，这是一种通过分析决策边界进行 MLaaS 的新型后门检测方法。 Model X-ray不仅可以识别目标API是否被后门攻击感染，还可以在一对一攻击策略下确定目标被攻击标签。重要的是，它仅通过对干净输入的硬预测来实现这一点，而不管有关攻击的任何假设和模型训练细节的先验知识。大量实验表明，Model X-ray 可以有效地实现跨不同后门攻击、数据集和架构的 MLaaS。</details>
**PDF:** <http://arxiv.org/pdf/2402.17465v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images**<br />
**Title_cn:** 使用 CT、PET 和 MRI 多模态图像分割任何头颈部肿瘤模型<br />
**Authors:** Jintao Ren, Mathis Rasmussen, Jasper Nijkamp, Jesper Grau Eriksen, Stine Korreman<br />
**Abstract:** <details><summary>原文: </summary>Deep learning presents novel opportunities for the auto-segmentation of gross tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods usually necessitate significant manual refinement. This study investigates the Segment Anything Model (SAM), recognized for requiring minimal human prompting and its zero-shot generalization ability across natural images. We specifically examine MedSAM, a version of SAM fine-tuned with large-scale public medical images. Despite its progress, the integration of multi-modality images (CT, PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's application in HNC GTV segmentation, we assess its performance in both zero-shot and fine-tuned scenarios using single (CT-only) and fused multi-modality images. Our study demonstrates that fine-tuning SAM significantly enhances its segmentation accuracy, building upon the already effective zero-shot results achieved with bounding box prompts. These findings open a promising avenue for semi-automatic HNC GTV segmentation.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习为头颈癌 (HNC) 中大体肿瘤体积 (GTV) 的自动分割提供了新的机会，但全自动方法通常需要大量的手动细化。本研究调查了分段任意模型 (SAM)，该模型因需要最少的人类提示及其在自然图像中的零样本泛化能力而受到认可。我们专门研究了 MedSAM，这是根据大规模公共医学图像进行微调的 SAM 版本。尽管取得了进展，但整合多模态图像（CT、PET、MRI）以进行有效的 GTV 描绘仍然是一个挑战。重点关注 SAM 在 HNC GTV 分割中的应用，我们使用单个（仅 CT）和融合的多模态图像评估其在零样本和微调场景中的性能。我们的研究表明，在通过边界框提示实现的已经有效的零样本结果的基础上，微调 SAM 显着提高了其分割精度。这些发现为半自动 HNC GTV 分割开辟了一条充满希望的途径。</details>
**PDF:** <http://arxiv.org/pdf/2402.17454v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction**<br />
**Title_cn:** ViTaL：使用视觉变换器和线性投影进行特征缩减的叶子图像中自动植物病害识别的高级框架<br />
**Authors:** Abhishek Sebastian, Annis Fathima A, Pragna R, Madhan Kumar S, Yaswanth Kannan G, Vinay Murali<br />
**Abstract:** <details><summary>原文: </summary>Our paper introduces a robust framework for the automated identification of diseases in plant leaf images. The framework incorporates several key stages to enhance disease recognition accuracy. In the pre-processing phase, a thumbnail resizing technique is employed to resize images, minimizing the loss of critical image details while ensuring computational efficiency. Normalization procedures are applied to standardize image data before feature extraction. Feature extraction is facilitated through a novel framework built upon Vision Transformers, a state-of-the-art approach in image analysis. Additionally, alternative versions of the framework with an added layer of linear projection and blockwise linear projections are explored. This comparative analysis allows for the evaluation of the impact of linear projection on feature extraction and overall model performance. To assess the effectiveness of the proposed framework, various Convolutional Neural Network (CNN) architectures are utilized, enabling a com- prehensive evaluation of linear projection's influence on key evaluation metrics. The findings demonstrate the efficacy of the proposed framework, with the top- performing model achieving a Hamming loss of 0.054. Furthermore, we propose a novel hardware design specifically tailored for scanning diseased leaves in an omnidirectional fashion. The hardware implementation utilizes a Raspberry Pi Compute Module to address low-memory configurations, ensuring practicality and affordability. This innovative hardware solution enhances the overall feasibility and accessibility of the proposed automated disease identification system. This research contributes to the field of agriculture by offering valuable insights and tools for the early detection and management of plant diseases, potentially leading to improved crop yields and enhanced food security.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们的论文介绍了一个强大的框架，用于自动识别植物叶子图像中的疾病。该框架包含几个关键阶段，以提高疾病识别的准确性。在预处理阶段，采用缩略图调整技术来调整图像大小，在保证计算效率的同时最大限度地减少关键图像细节的损失。标准化程序用于在特征提取之前标准化图像数据。通过基于 Vision Transformers（一种最先进的图像分析方法）构建的新颖框架来促进特征提取。此外，还探索了添加了线性投影和块线性投影层的框架的替代版本。这种比较分析可以评估线性投影对特征提取和整体模型性能的影响。为了评估所提出框架的有效性，利用了各种卷积神经网络（CNN）架构，从而能够全面评估线性投影对关键评估指标的影响。研究结果证明了所提出框架的有效性，表现最好的模型实现了 0.054 的汉明损失。此外，我们提出了一种新颖的硬件设计，专门用于以全方位方式扫描患病叶子。硬件实现利用 Raspberry Pi 计算模块来解决低内存配置问题，确保实用性和经济性。这种创新的硬件解决方案增强了所提出的自动疾病识别系统的整体可行性和可访问性。这项研究为植物病害的早期发现和管理提供了宝贵的见解和工具，为农业领域做出了贡献，有可能提高作物产量并增强粮食安全。</details>
**PDF:** <http://arxiv.org/pdf/2402.17424v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **PANDAS: Prototype-based Novel Class Discovery and Detection**<br />
**Title_cn:** PANDAS：基于原型的新类发现和检测<br />
**Authors:** Tyler L. Hayes, César R. de Souza, Namil Kim, Jiwon Kim, Riccardo Volpi, Diane Larlus<br />
**Abstract:** <details><summary>原文: </summary>Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state of the art for this task while being computationally more affordable.</details>
**Abstract_cn:** <details><summary>译文: </summary>目标检测器通常在一组固定的类别上进行一次性训练。然而，这种封闭世界的假设在实践中是不现实的，因为探测器在野外部署后将不可避免地出现新的类别。在这项工作中，我们研究了扩展针对一组基类训练的检测器的方法，以便它能够 i）发现新类的存在，以及 ii）自动丰富其功能，以便能够检测这些新发现的类以及基础的。我们提出了 PANDAS，一种新类别发现和检测的方法。它从未标记的数据中发现代表新类的簇，并用原型代表新旧类。在推理过程中，基于距离的分类器使用这些原型为每个检测到的对象实例分配标签。我们的方法的简单性使其具有广泛的适用性。我们通过实验证明了 PANDAS 在 VOC 2012 和 COCO-to-LVIS 基准上的有效性。它的性能优于这项任务的现有技术，同时计算成本也更便宜。</details>
**PDF:** <http://arxiv.org/pdf/2402.17420v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification**<br />
**Title_cn:** CARZero：放射学零样本分类的交叉注意力对齐<br />
**Authors:** Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S. Kevin Zhou<br />
**Abstract:** <details><summary>原文: </summary>The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs, focusing on image-text alignment. However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports. To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology Zero-Shot Classification (CARZero). Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics. This representation is then linearly projected to form an image-text similarity matrix for cross-modality alignment. Additionally, recognizing the pivotal role of prompt selection in zero-shot learning, CARZero incorporates a Large Language Model-based prompt alignment strategy. This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual prompt design. Our approach is simple yet effective, demonstrating state-of-the-art performance in zero-shot classification on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases. This achievement is attributed to our new image-text alignment strategy, which effectively addresses the complex relationship between medical images and reports.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过在大规模图像文本对上使用预训练模型，重点关注图像文本对齐，推动了零样本学习在医学领域的进步。然而，现有的方法主要依靠余弦相似度进行对齐，这可能无法完全捕捉医学图像和报告之间的复杂关系。为了解决这一差距，我们引入了一种称为放射学零样本分类的交叉注意力对齐（CARZero）的新颖方法。我们的方法创新地利用交叉注意机制来处理图像和报告特征，创建相似性表示，更准确地反映医学语义中复杂的关系。然后线性投影该表示以形成用于跨模态对齐的图像文本相似度矩阵。此外，认识到提示选择在零样本学习中的关键作用，CARZero 结合了基于大语言模型的提示对齐策略。该策略将不同的诊断表达式标准化为训练和推理阶段的统一格式，克服了手动提示设计的挑战。我们的方法简单而有效，在五个官方胸片诊断测试集上展示了零样本分类的最先进性能，包括在罕见疾病长尾分布的数据集上取得了显着的结果。这一成就归功于我们新的图文对齐策略，该策略有效地解决了医学图像和报告之间的复杂关系。</details>
**PDF:** <http://arxiv.org/pdf/2402.17417v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **An Efficient MLP-based Point-guided Segmentation Network for Ore Images with Ambiguous Boundary**<br />
**Title_cn:** 一种基于MLP的高效的边界模糊矿石图像点引导分割网络<br />
**Authors:** Guodong Sun, Yuting Peng, Le Cheng, Mengya Xu, An Wang, Bo Wu, Hongliang Ren, Yang Zhang<br />
**Abstract:** <details><summary>原文: </summary>The precise segmentation of ore images is critical to the successful execution of the beneficiation process. Due to the homogeneous appearance of the ores, which leads to low contrast and unclear boundaries, accurate segmentation becomes challenging, and recognition becomes problematic. This paper proposes a lightweight framework based on Multi-Layer Perceptron (MLP), which focuses on solving the problem of edge burring. Specifically, we introduce a lightweight backbone better suited for efficiently extracting low-level features. Besides, we design a feature pyramid network consisting of two MLP structures that balance local and global information thus enhancing detection accuracy. Furthermore, we propose a novel loss function that guides the prediction points to match the instance edge points to achieve clear object boundaries. We have conducted extensive experiments to validate the efficacy of our proposed method. Our approach achieves a remarkable processing speed of over 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our method delivers a consistently high level of accuracy, with impressive performance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$ respectively, as compared to the currently available state-of-the-art techniques, when tested on the ore image dataset. The source code will be released at \url{https://github.com/MVME-HBUT/ORENEXT}.</details>
**Abstract_cn:** <details><summary>译文: </summary>矿石图像的精确分割对于选矿过程的成功执行至关重要。由于矿石外观均质，导致对比度低、边界不清晰，准确分割变得具有挑战性，识别也成为问题。本文提出了一种基于多层感知器（MLP）的轻量级框架，重点解决边缘毛刺问题。具体来说，我们引入了一个更适合有效提取低级特征的轻量级主干。此外，我们设计了一个由两个 MLP 结构组成的特征金字塔网络，可以平衡局部和全局信息，从而提高检测精度。此外，我们提出了一种新颖的损失函数，引导预测点匹配实例边缘点，以实现清晰的对象边界。我们进行了广泛的实验来验证我们提出的方法的有效性。我们的方法实现了每秒超过 27 帧 (FPS) 的卓越处理速度，模型大小仅为 73 MB。此外，与在矿石图像数据集上进行测试时，采用了当前可用的最先进技术。源代码将在 \url{https://github.com/MVME-HBUT/ORENEXT} 发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.17370v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection**<br />
**Title_cn:** SDDGR：用于类增量对象检测的稳定的基于扩散的深度生成重放<br />
**Authors:** Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek<br />
**Abstract:** <details><summary>原文: </summary>In the field of class incremental learning (CIL), genera- tive replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the con- tinuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the com- plexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text- to-diffusion networks to generate realistic and diverse syn- thetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distilla- tion technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, pre- venting misclassification as background elements. Exten- sive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.</details>
**Abstract_cn:** <details><summary>译文: </summary>在课堂增量学习（CIL）领域，随着生成模型的不断改进，生成重播作为一种减轻灾难性遗忘的方法变得越来越重要。然而，它在类增量对象检测（CIOD）中的应用受到了极大的限制，这主要是由于涉及多个标签的场景的复杂性。在本文中，我们为 CIOD 提出了一种称为稳定扩散深度生成重放（SDDGR）的新方法。我们的方法利用基于扩散的生成模型和预先训练的文本扩散网络来生成真实且多样化的合成图像。 SDDGR 采用迭代细化策略来生成包含旧类别的高质量图像。此外，我们采用 L2 知识蒸馏技术来提高合成图像中先验知识的保留。此外，我们的方法包括对新任务图像中的旧对象进行伪标记，防止错误分类为背景元素。对 COCO 2017 数据集的大量实验表明，SDDGR 显着优于现有算法，在各种 CIOD 场景中实现了新的最先进算法。源代码将向公众开放。</details>
**PDF:** <http://arxiv.org/pdf/2402.17323v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge -- Multi-Task Robustness Track**<br />
**Title_cn:** 用于第一届 VCL 挑战赛密集视觉预测解决方案的普通多任务框架——多任务鲁棒性赛道<br />
**Authors:** Zehui Chen, Qiuchen Wang, Zhenyu Li, Jiaming Liu, Shanghang Zhang, Feng Zhao<br />
**Abstract:** <details><summary>原文: </summary>In this report, we present our solution to the multi-task robustness track of the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We propose a vanilla framework named UniNet that seamlessly combines various visual perception algorithms into a multi-task model. Specifically, we choose DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance segmentation, and depth estimation tasks, respectively. The final submission is a single model with InternImage-L backbone, and achieves a 49.6 overall score (29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation set. Besides, we provide some interesting observations in our experiments which may facilitate the development of multi-task learning in dense visual prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本报告中，我们介绍了 ICCV 2023 研讨会上第一届视觉持续学习 (VCL) 挑战赛的多任务稳健性赛道的解决方案。我们提出了一个名为 UniNet 的普通框架，它将各种视觉感知算法无缝地组合成一个多任务模型。具体来说，我们选择 DETR3D、Mask2Former 和 BinsFormer 分别用于 3D 对象检测、实例分割和深度估计任务。最终提交的是具有 InternImage-L 主干的单一模型，在 SHIFT 验证集上获得了 49.6 的总分（29.5 Det mAP、80.3 mTPS、46.4 Seg mAP 和 7.93 silog）。此外，我们在实验中提供了一些有趣的观察结果，这可能有助于密集视觉预测中多任务学习的发展。</details>
**PDF:** <http://arxiv.org/pdf/2402.17319v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Scaling Supervised Local Learning with Augmented Auxiliary Networks**<br />
**Title_cn:** 使用增强辅助网络扩展监督本地学习<br />
**Authors:** Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We also propose to linearly reduce the depth of auxiliary networks as the hidden layer goes deeper, ensuring sufficient network capacity while reducing the computational cost of auxiliary networks. Our extensive experiments on four image classification datasets (i.e., CIFAR-10, SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up to tens of local layers with a comparable accuracy to BP-trained networks while reducing GPU memory usage by around 40%. The proposed AugLocal method, therefore, opens up a myriad of opportunities for training high-performance deep neural networks on resource-constrained platforms.Code is available at https://github.com/ChenxiangMA/AugLocal.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络通常使用端到端反向传播（BP）的全局误差信号进行训练，这不仅在生物学上不可信，而且还存在更新锁定问题，并且需要巨大的内存消耗。局部学习通过梯度隔离辅助网络独立更新每一层，为解决上述问题提供了一种有前途的替代方案。然而，现有的局部学习方法与 BP 方法相比，面临着巨大的精度差距，特别是对于大规模网络。这是由于本地层与其后续网络层之间的耦合较弱，因为跨层没有梯度通信。为了解决这个问题，我们提出了一种增强的本地学习方法，称为 AugLocal。 AugLocal通过从其后续网络层中统一选择一小部分层子集来构建每个隐藏层的辅助网络，以增强它们的协同作用。我们还建议随着隐藏层的加深而线性减少辅助网络的深度，保证足够的网络容量，同时降低辅助网络的计算成本。我们对四个图像分类数据集（即 CIFAR-10、SVHN、STL-10 和 ImageNet）进行的广泛实验表明，AugLocal 可以有效地扩展到数十个局部层，其精度与 BP 训练的网络相当，同时减少 GPU 内存使用约 40%。因此，所提出的 AugLocal 方法为在资源受限的平台上训练高性能深度神经网络提供了无数的机会。代码可在 https://github.com/ChenyangMA/AugLocal 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17318v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation**<br />
**Title_cn:** 我们如何赢得 BraTS 2023 成人胶质瘤挑战赛？只是假装而已！用于脑肿瘤分割的增强型合成数据增强和模型集成<br />
**Authors:** André Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger<br />
**Abstract:** <details><summary>原文: </summary>Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95 14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the validation set.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习是分割脑肿瘤的最先进技术。然而，这需要大量高质量的数据，而这些数据很难获得，尤其是在医学领域。因此，我们的解决方案通过使用非常规的数据增强机制来解决这个问题。生成对抗网络和配准用于大量增加可用样本的数量，用于训练三种不同的深度学习模型以进行脑肿瘤分割，这是 BraTS2023 挑战赛的第一个任务。第一个模型是标准 nnU-Net，第二个是 Swin UNETR，第三个是 BraTS 2021 挑战赛的获胜解决方案。除了合成数据的生成之外，整个流程都建立在 nnU-Net 实现上。卷积算法和转换器的使用能够填补彼此的知识空白。使用新的指标，我们的最佳解决方案在验证集中实现了骰子结果 0.9005、0.8673、0.8509 和 HD95 14.940、14.467、17.699（整个肿瘤、肿瘤核心和增强肿瘤）。</details>
**PDF:** <http://arxiv.org/pdf/2402.17317v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Explicit Interaction for Fusion-Based Place Recognition**<br />
**Title_cn:** 基于融合的地点识别的显式交互<br />
**Authors:** Jingyi Xu, Junyi Ma, Qi Wu, Zijie Zhou, Yue Wang, Xieyuanli Chen, Ling Pei<br />
**Abstract:** <details><summary>原文: </summary>Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于融合的地点识别是一种新兴技术，联合利用多模态感知数据，在 GPS 无法识别的情况下为机器人和自动驾驶车辆识别先前访问过的地点。最近基于融合的地点识别方法以隐式方式结合了多模态特征。虽然取得了显着的成果，但他们没有明确考虑个体模态在融合系统中提供的功能。因此，多模态特征融合的好处可能没有得到充分发挥。在本文中，我们提出了一种新颖的基于融合的网络，称为 EINet，以实现两种模式的显式交互。 EINet 使用 LiDAR 范围来长时间监控更强大的视觉特征，同时使用相机 RGB 数据来提高 LiDAR 点云的辨别力。此外，我们基于 nuScenes 数据集为地点识别任务开发了一个新的基准。为了通过全面比较为未来的研究建立这一基准，我们引入了监督和自我监督的培训方案以及评估协议。我们对所提出的基准进行了广泛的实验，实验结果表明，与最先进的基于融合的地点识别方法相比，我们的 EINet 表现出更好的识别性能以及扎实的泛化能力。我们的开源代码和基准测试发布于：https://github.com/BIT-XJY/EINet。</details>
**PDF:** <http://arxiv.org/pdf/2402.17264v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework**<br />
**Title_cn:** 基于深度学习的语音和视觉合成通过多层自适应框架改进网络钓鱼攻击检测<br />
**Authors:** Tosin Ige, Christopher Kiekintveld, Aritran Piplai<br />
**Abstract:** <details><summary>原文: </summary>The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>攻击者不断改进其网络钓鱼技术以绕过现有最先进的网络钓鱼检测方法，这给工业界和学术界研究人员带来了巨大的挑战，因为当前的方法无法检测复杂的网络钓鱼攻击。因此，当前的反网络钓鱼方法仍然容易受到复杂的网络钓鱼的影响，因为攻击者采用的策略越来越复杂，而且开发新策略以逃避检测的速度也越来越快。在这项研究中，我们提出了一种适应性框架，它将深度学习和 Randon Forest 相结合来读取图像、从深度伪造视频中合成语音以及对各种分层预测进行自然语言处理，以显着提高用于网络钓鱼攻击检测的机器学习模型的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.17249v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging**<br />
**Title_cn:** SDR-Former：使用 3D 多相成像进行肝脏病变分类的连体双分辨率变压器<br />
**Authors:** Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu<br />
**Abstract:** <details><summary>原文: </summary>Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction capabilities. Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase's influence on the diagnostic outcome. The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset. The experimental results affirm the efficacy of the proposed framework. To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public. This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is accessible at:https://bit.ly/3IyYlgN.</details>
**Abstract_cn:** <details><summary>译文: </summary>多期 CT 和 MR 扫描中肝脏病变的自动分类具有临床意义，但具有挑战性。本研究提出了一种新颖的连体双分辨率变压器 (SDR-Former) 框架，专为具有不同相位计数的 3D 多相 CT 和 MR 成像中的肝脏病变分类而设计。所提出的 SDR-Former 利用简化的连体神经网络（SNN）来处理多相成像输入，在保持计算效率的同时拥有强大的特征表示。混合双分辨率变压器 (DR-Former) 进一步丰富了 SNN 的权重共享功能，其中包括 3D 卷积神经网络 (CNN) 和分别用于处理高分辨率和低分辨率图像的定制 3D Transformer。这种混合子架构擅长捕获详细的局部特征并理解全局上下文信息，从而提高 SNN 的特征提取能力。此外，还引入了一种新颖的自适应相位选择模块（APSM），促进特定相位的相互通信并动态调整每个相位对诊断结果的影响。所提出的 SDR-Former 框架已通过对两个临床数据集（三相 CT 数据集和八相 MR 数据集）的综合实验进行了验证。实验结果证实了所提出框架的有效性。为了支持科学界，我们向公众发布用于肝脏病变分析的广泛多相 MR 数据集。这一开创性的数据集是该领域第一个公开可用的多相 MR 数据集，也是 MICCAI LLD-MMRI 挑战赛的基础。该数据集可通过以下网址访问：https://bit.ly/3IyYlgN。</details>
**PDF:** <http://arxiv.org/pdf/2402.17246v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Preserving Fairness Generalization in Deepfake Detection**<br />
**Title_cn:** 在 Deepfake 检测中保持公平泛化<br />
**Authors:** Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu<br />
**Abstract:** <details><summary>原文: </summary>Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管近年来已经开发出有效的深度伪造检测模型，但最近的研究表明，这些模型可能会导致种族和性别等人口群体之间的不公平表现差异。这可能会导致特定群体面临不公平的目标或被排除在检测之外，从而可能导致错误分类的深度伪造品操纵公众舆论并破坏对模型的信任。解决这个问题的现有方法是提供公平损失函数。它在域内评估方面表现出良好的公平性，但在跨域测试方面却无法保持公平性。这凸显了公平泛化在打击深度造假中的重要性。在这项工作中，我们提出了第一种通过同时考虑特征、损失和优化方面来解决 Deepfake 检测中的公平性泛化问题的方法。我们的方法采用解缠学习来提取人口统计和与领域无关的伪造特征，将它们融合起来以鼓励在平坦的损失环境中进行公平学习。对著名的 Deepfake 数据集进行的大量实验证明了我们方法的有效性，在跨域 Deepfake 检测过程中保持公平性方面超越了最先进的方法。代码可在 https://github.com/Purdue-M2/Fairness-Generalization 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.17229v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Deployment Prior Injection for Run-time Calibratable Object Detection**<br />
**Title_cn:** 用于运行时可校准对象检测的部署预注入<br />
**Authors:** Mo Zhou, Yiding Yang, Haoxiang Li, Vishal M. Patel, Gang Hua<br />
**Abstract:** <details><summary>原文: </summary>With a strong alignment between the training and test distributions, object relation as a context prior facilitates object detection. Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time. Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update. Such kind of capability requires the model to explicitly learn disentangled representations with respect to context prior. To achieve this, we introduce an additional graph input to the detector, where the graph represents the deployment context prior, and its edge values represent object relations. Then, the detector behavior is trained to bound to the graph with a modified training objective. As a result, during the test phase, any suitable deployment context prior can be injected into the detector via graph edits, hence calibrating, or "re-biasing" the detector towards the given prior at run-time without parameter update. Even if the deployment prior is unknown, the detector can self-calibrate using deployment prior approximated using its own predictions. Comprehensive experimental results on the COCO dataset, as well as cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness of the run-time calibratable detector.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于训练和测试分布之间具有很强的一致性，对象关系作为上下文先验有利于对象检测。然而，它会变成一种有害但不可避免的训练集偏差，因为测试分布在空间和时间上的变化不同。然而，现有的检测器无法在测试阶段之前在没有参数更新的情况下纳入部署上下文。这种能力要求模型显式地学习相对于上下文先验的解开表示。为了实现这一点，我们向检测器引入了一个额外的图输入，其中该图表示先验的部署上下文，其边缘值表示对象关系。然后，使用修改后的训练目标训练检测器行为以绑定到图。因此，在测试阶段，任何合适的部署上下文先验都可以通过图形编辑注入到检测器中，从而在运行时将检测器校准或“重新偏置”到给定的先验，而无需更新参数。即使部署先验未知，探测器也可以使用自己的预测近似的部署先验进行自校准。 COCO 数据集上的综合实验结果以及 Objects365 数据集上的跨数据集测试证明了运行时可校准检测器的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17207v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction**<br />
**Title_cn:** PE-MVCNet：用于肺栓塞预测的多视图和跨模态融合网络<br />
**Authors:** Zhaoxin Guo, Zhipeng Wang, Ruiquan Ge, Jianxun Yu, Feiwei Qin, Yuan Tian, Yuqing Peng, Yonghong Li, Changmiao Wang<br />
**Abstract:** <details><summary>原文: </summary>The early detection of a pulmonary embolism (PE) is critical for enhancing patient survival rates. Both image-based and non-image-based features are of utmost importance in medical classification tasks. In a clinical setting, physicians tend to rely on the contextual information provided by Electronic Medical Records (EMR) to interpret medical imaging. However, very few models effectively integrate clinical information with imaging data. To address this shortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet, which capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR data. This method comprises the Image-only module with an integrated multi-view block, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module. These modules cooperate to extract comprehensive features that subsequently generate predictions for PE. We conducted experiments using the publicly accessible Stanford University Medical Center dataset, achieving an AUROC of 94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model outperforms existing methodologies, corroborating that our multimodal fusion model excels compared to models that use a single data modality.</details>
**Abstract_cn:** <details><summary>译文: </summary>肺栓塞（PE）的早期发现对于提高患者生存率至关重要。基于图像和非图像的特征在医学分类任务中都至关重要。在临床环境中，医生倾向于依赖电子病历 (EMR) 提供的上下文信息来解释医学成像。然而，很少有模型能够有效地将临床信息与影像数据整合起来。为了解决这个缺点，我们建议采用一种多模态融合方法，称为 PE-MVCNet，它利用计算机断层扫描肺血管造影成像和 EMR 数据。该方法包括具有集成多视图块的仅图像模块、仅EMR模块和跨模态注意融合（CMAF）模块。这些模块协作提取综合特征，随后生成 PE 预测。我们使用可公开访问的斯坦福大学医学中心数据集进行了实验，获得了 94.1% 的 AUROC、90.2% 的准确率和 90.6% 的 F1 分数。我们提出的模型优于现有方法，证实了我们的多模态融合模型比使用单一数据模态的模型更优秀。</details>
**PDF:** <http://arxiv.org/pdf/2402.17187v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **Lane2Seq: Towards Unified Lane Detection via Sequence Generation**<br />
**Title_cn:** Lane2Seq：通过序列生成实现统一车道检测<br />
**Authors:** Kunyang Zhou<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks. For example, Lane2Seq gets 97.95\% and 97.42\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新颖的基于序列生成的车道检测框架，称为 Lane2Seq。它通过将车道检测作为序列生成任务来统一各种车道检测格式。这与以前的车道检测方法不同，以前的车道检测方法依赖于精心设计的特定于任务的头部网络和相应的损失函数。 Lane2Seq仅采用简单的基于变压器的编码器-解码器架构，具有简单的交叉熵损失。此外，我们提出了一种基于强化学习的新的多格式模型调整，将特定于任务的知识纳入 Lane2Seq 中。实验结果表明，这种简单的序列生成范例不仅统一了车道检测，而且在基准测试中实现了具有竞争力的性能。例如，Lane2Seq 在 Tusimple 和 LLAMAS 数据集上获得 97.95\% 和 97.42\% F1 分数，为两个基准建立了新的最先进结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.17172v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Few-shot adaptation for morphology-independent cell instance segmentation**<br />
**Title_cn:** 与形态无关的细胞实例分割的少样本适应<br />
**Authors:** Ram J. Zaveri, Voke Brume, Gianfranco Doretto<br />
**Abstract:** <details><summary>原文: </summary>Microscopy data collections are becoming larger and more frequent. Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them. This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections. This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria. We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a few-shot domain adaptation approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy. Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>显微镜数据收集变得越来越大、越来越频繁。要从中受益，需要准确、精确的定量分析工具（例如细胞实例分割）。由于数据的可变性，这具有挑战性，需要重新训练分割模型以保持新集合的高精度。这对于分割具有细长和非凸形态的细胞（如细菌）尤其需要。我们建议通过引入少量域适应方法来减少重新训练模型所需的注释量和计算能力，该方法只需要注释要处理的新数据的一到五个单元，并快速适应模型以保持高精度。我们的结果表明，在适应极具挑战性的细菌数据集后，准确性显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.17165v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer**<br />
**Title_cn:** NocPlace：使用生成和遗传知识转移进行夜间视觉地点识别<br />
**Authors:** Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang<br />
**Abstract:** <details><summary>原文: </summary>Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images. Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. Following this, an unpaired image-to-image translation network is trained on this dataset. Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the Daytime VPR model. Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉位置识别 (VPR) 在计算机视觉中至关重要，旨在从大量已知图像中检索与查询图像相似的数据库图像。然而，与许多与视觉相关的任务一样，由于夜间图像的稀缺，基于学习的 VPR 在夜间的性能通常会下降。具体来说，VPR需要解决夜间到白天的跨域问题，而不仅仅是单一夜间域的问题。针对这些问题，我们提出了 NocPlace，它利用生成的大规模、多视图、夜间 VPR 数据集，在学习的全局描述符中嵌入抵御耀眼灯光和极端黑暗的能力。首先，我们建立了一个名为 NightCities 的昼夜城市场景数据集，捕捉全球 60 个城市的不同夜间场景和照明变化。接下来，在此数据集上训练不配对的图像到图像翻译网络。使用这个经过训练的翻译网络，我们处理现有的 VPR 数据集，从而获得其夜间版本。然后使用夜间风格图像、原始标签和从白天 VPR 模型继承的描述符对 NocPlace 进行微调。对各种夜间 VPR 测试集的综合实验表明，NocPlace 大大超越了以前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17159v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **Efficiently Leveraging Linguistic Priors for Scene Text Spotting**<br />
**Title_cn:** 有效利用语言先验进行场景文本识别<br />
**Authors:** Nguyen Nguyen, Yapeng Tian, Chenliang Xu<br />
**Abstract:** <details><summary>原文: </summary>Incorporating linguistic knowledge can improve scene text recognition, but it is questionable whether the same holds for scene text spotting, which typically involves text detection and recognition. This paper proposes a method that leverages linguistic knowledge from a large text corpus to replace the traditional one-hot encoding used in auto-regressive scene text spotting and recognition models. This allows the model to capture the relationship between characters in the same word. Additionally, we introduce a technique to generate text distributions that align well with scene text datasets, removing the need for in-domain fine-tuning. As a result, the newly created text distributions are more informative than pure one-hot encoding, leading to improved spotting and recognition performance. Our method is simple and efficient, and it can easily be integrated into existing auto-regressive-based approaches. Experimental results show that our method not only improves recognition accuracy but also enables more accurate localization of words. It significantly improves both state-of-the-art scene text spotting and recognition pipelines, achieving state-of-the-art results on several benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>结合语言知识可以改善场景文本识别，但对于通常涉及文本检测和识别的场景文本识别是否也同样如此值得怀疑。本文提出了一种利用大型文本语料库中的语言知识来取代自回归场景文本识别和识别模型中使用的传统单热编码的方法。这使得模型能够捕获同一单词中字符之间的关系。此外，我们引入了一种生成与场景文本数据集良好匹配的文本分布的技术，从而消除了域内微调的需要。因此，新创建的文本分布比纯粹的单热编码提供更多信息，从而提高发现和识别性能。我们的方法简单高效，并且可以轻松集成到现有的基于自回归的方法中。实验结果表明，我们的方法不仅提高了识别精度，而且能够更准确地定位单词。它显着改进了最先进的场景文本识别和识别管道，在多个基准测试中取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.17134v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution**<br />
**Title_cn:** SAM-DiffSR：用于图像超分辨率的结构调制扩散模型<br />
**Authors:** Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang<br />
**Abstract:** <details><summary>原文: </summary>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at https://github.com/lose4578/SAM-DiffSR.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散的超分辨率（SR）模型最近因其强大的恢复能力而引起了广泛关注。但传统的扩散模型从单一分布中执行噪声采样，限制了它们处理跨语义区域的现实世界场景和复杂纹理的能力。随着分段任何模型（SAM）的成功，生成足够细粒度的区域掩模可以增强基于扩散的SR模型的细节恢复。然而，直接将 SAM 集成到 SR 模型中会导致计算成本更高。在本文中，我们提出了 SAM-DiffSR 模型，该模型可以利用采样噪声过程中 SAM 的细粒度结构信息来提高图像质量，而无需在推理过程中增加计算成本。在训练过程中，我们将结构位置信息编码到 SAM 的分割掩模中。然后，通过将编码掩模调制为采样噪声，将其集成到前向扩散过程中。这种调整使我们能够独立调整每个相应分割区域内的噪声平均值。训练扩散模型来估计这种调制噪声。至关重要的是，我们提出的框架不会改变反向扩散过程，并且在推理时不需要 SAM。实验结果证明了我们提出的方法的有效性，展示了在抑制伪影方面的优越性能，并且在 DIV2K 数据集上的 PSNR 方面最大超过现有的基于扩散的方法 0.74 dB。代码和数据集可在 https://github.com/lose4578/SAM-DiffSR 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17133v1><br />
**Code:** <https://github.com/lose4578/SAM-DiffSR>**<br />
>>**index:** 34<br />
**Title:** **OSCaR: Object State Captioning and State Change Representation**<br />
**Title_cn:** OSCaR：对象状态描述和状态变化表示<br />
**Authors:** Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu<br />
**Abstract:** <details><summary>原文: </summary>The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large language models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.</details>
**Abstract_cn:** <details><summary>译文: </summary>智能模型推断和理解物体状态变化的能力是人工智能研究的一个至关重要但要求很高的方面，特别是通过现实世界中人类交互的视角。这项任务涉及描述复杂的视觉环境，识别活动对象，并解释通过语言传达的它们的变化。传统方法将对象描述和状态变化检测隔离开来，只能提供有限的动态环境视图。而且，依靠一小组符号词来表示变化，限制了语言的表达能力。为了应对这些挑战，在本文中，我们介绍了对象状态描述和状态变化表示（OSCaR）数据集和基准。 OSCaR 由 14,084 个带注释的视频片段组成，其中包含来自各种以自我为中心的视频集合中的近 1,000 个独特对象。它为评估多模式大语言模型（MLLM）设置了一个新的测试平台。我们的实验表明，虽然 MLLM 表现出一定的技能，但它们缺乏对对象状态变化的充分理解。该基准包括一个微调模型，尽管具有初始功能，但仍需要显着提高准确性和泛化能力，以便有效理解这些变化。我们的代码和数据集可在 https://github.com/nguyennm1024/OSCaR 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17128v1><br />
**Code:** null<br />
>>**index:** 35<br />
**Title:** **In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking**<br />
**Title_cn:** 热红外物体跟踪贝叶斯过滤的防御和复兴<br />
**Authors:** Peng Gao, Shi-Min Li, Feng Gao, Fei Wang, Ru-Yue Yuan, Hamido Fujita<br />
**Abstract:** <details><summary>原文: </summary>Deep learning-based methods monopolize the latest research in the field of thermal infrared (TIR) object tracking. However, relying solely on deep learning models to obtain better tracking results requires carefully selecting feature information that is beneficial to representing the target object and designing a reasonable template update strategy, which undoubtedly increases the difficulty of model design. Thus, recent TIR tracking methods face many challenges in complex scenarios. This paper introduces a novel Deep Bayesian Filtering (DBF) method to enhance TIR tracking in these challenging situations. DBF is distinctive in its dual-model structure: the system and observation models. The system model leverages motion data to estimate the potential positions of the target object based on two-dimensional Brownian motion, thus generating a prior probability. Following this, the observation model comes into play upon capturing the TIR image. It serves as a classifier and employs infrared information to ascertain the likelihood of these estimated positions, creating a likelihood probability. According to the guidance of the two models, the position of the target object can be determined, and the template can be dynamically updated. Experimental analysis across several benchmark datasets reveals that DBF achieves competitive performance, surpassing most existing TIR tracking methods in complex scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习的方法垄断了热红外（TIR）目标跟踪领域的最新研究。然而，单纯依靠深度学习模型来获得更好的跟踪结果需要仔细选择有利于表示目标物体的特征信息并设计合理的模板更新策略，这无疑增加了模型设计的难度。因此，最近的 TIR 跟踪方法在复杂场景中面临着许多挑战。本文介绍了一种新颖的深度贝叶斯过滤 (DBF) 方法，可在这些具有挑战性的情况下增强 TIR 跟踪。 DBF 的独特之处在于其双模型结构：系统模型和观测模型。系统模型利用运动数据基于二维布朗运动来估计目标物体的潜在位置，从而生成先验概率。此后，观察模型在捕获 TIR 图像时发挥作用。它充当分类器并利用红外信息来确定这些估计位置的可能性，从而创建似然概率。根据两个模型的引导，可以确定目标物体的位置，并且可以动态更新模板。对多个基准数据集的实验分析表明，DBF 实现了具有竞争力的性能，在复杂场景中超越了大多数现有的 TIR 跟踪方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17098v1><br />
**Code:** null<br />

>## **OCR**
>---
>>**index:** 1<br />
**Title:** **Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System**<br />
**Title_cn:** 推进生成模型评估：一种用于 OCR 系统中真实图像合成和比较的新算法<br />
**Authors:** Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri Golilarz<br />
**Abstract:** <details><summary>原文: </summary>This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images. Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images. This approach significantly enhances the evaluation methodology by refining the Fr\'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality. Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation. By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output. This breakthrough in evaluation and comparison is crucial for advancing the field of OCR, especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究解决了生成模型领域的一个关键挑战，特别是在合成图像的生成和评估方面。考虑到生成模型固有的复杂性以及缺乏标准化的比较程序，我们的研究引入了一种开创性的算法来客观地评估合成图像的真实性。该方法通过细化 Fr'echet 起始距离 (FID) 分数显着增强了评估方法，从而可以对图像质量进行更精确和主观的评估。我们的算法专门用于解决生成和评估阿拉伯手写数字的真实图像的挑战，由于图像生成中真实性的主观本质，这项任务在传统上几乎是不可能完成的。通过提供系统和客观的框架，我们的方法不仅可以比较不同的生成模型，而且还为改进其设计和输出铺平了道路。这一评估和比较方面的突破对于推进 OCR 领域至关重要，特别是对于呈现独特复杂性的脚本，并为高质量合成图像的生成和评估树立了新标准。</details>
**PDF:** <http://arxiv.org/pdf/2402.17204v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention**<br />
**Title_cn:** CAD-SIGNet：使用分层草图实例引导注意力从点云进行 CAD 语言推理<br />
**Authors:** Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, Djamila Aouada<br />
**Abstract:** <details><summary>原文: </summary>Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算机辅助设计 (CAD) 领域的逆向工程一直是人们长期以来的愿望，但尚未完全实现。其主要目标是通过 3D 扫描揭示物理对象背后的 CAD 流程。我们提出了 CAD-SIGNet，这是一种端到端的可训练和自回归架构，用于恢复 CAD 模型的设计历史，该模型表示为来自输入点云的一系列草图和挤压。我们的模型通过点云和 CAD 语言嵌入之间的分层交叉注意力来学习视觉语言表示。特别是，提出了一种新的 Sketch 实例引导注意力（SGA）模块，以重建草图的细粒度细节。由于其自回归特性，CAD-SIGNet 不仅可以在给定输入点云的情况下重建相应 CAD 模型的独特完整设计历史，而且还提供多种合理的设计选择。这通过为设计人员提供多个后续选择以及设计过程来实现交互式逆向工程场景。对公开可用的 CAD 数据集进行的广泛实验展示了我们的方法在两种设置中针对现有基线模型的有效性，即完整的设计历史恢复和点云的条件自动完成。</details>
**PDF:** <http://arxiv.org/pdf/2402.17678v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Structure-Guided Adversarial Training of Diffusion Models**<br />
**Title_cn:** 结构引导的扩散模型对抗训练<br />
**Authors:** Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在各种生成应用中表现出了卓越的功效。虽然现有模型侧重于最小化数据分布建模的去噪分数匹配损失的加权和，但它们的训练主要强调实例级优化，忽略了每个小批量内有价值的结构信息，表明样本之间的成对关系。为了解决这个限制，我们引入了结构引导的扩散模型对抗训练（SADM）。在这种开创性的方法中，我们迫使模型学习每个训练批次中样本之间的流形结构。为了确保模型捕获数据分布中真实的流形结构，我们提倡在极小极大游戏中针对新颖的结构鉴别器对扩散生成器进行对抗性训练，将真实的流形结构与生成的流形结构区分开来。 SADM 极大地改进了现有的扩散变换器 (DiT)，并在 12 个数据集的图像生成和跨域微调任务中优于现有方法，在 ImageNet 上为类条件图像建立了新的最先进的 FID 1.58 和 2.11分别以 256x256 和 512x512 的分辨率生成。</details>
**PDF:** <http://arxiv.org/pdf/2402.17563v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains**<br />
**Title_cn:** 激光雷达 3D 物体探测器对不可见领域的泛化能力的实证研究<br />
**Authors:** George Eskandar, Chongzhe Zhang, Abhishek Kaushik, Karim Guirguis, Mohamed Sayed, Bin Yang<br />
**Abstract:** <details><summary>原文: </summary>3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 物体检测器 (3D-OD) 对于了解许多机器人任务（尤其是自动驾驶）中的环境至关重要。通过激光雷达传感器包含 3D 信息可大大提高准确性。然而，此类探测器在未经训练的领域（即不同位置、传感器、天气等）表现不佳，限制了它们在安全关键型应用中的可靠性。存在使 3D-OD 适应这些领域的方法；然而，这些方法将 3D-OD 视为黑匣子，忽略了底层架构决策和源域训练策略。相反，我们深入研究 3D-OD 的细节，将我们的精力集中在影响域适应之前的鲁棒性的基本因素上。我们系统地研究了 3D-OD 鲁棒性和领域适应中经常被忽视的四种设计选择（以及它们之间的相互作用）：架构、体素编码、数据增强和锚定策略。我们通过六个基准评估了它们对九种最先进的 3D-OD 鲁棒性的影响，这些基准涵盖三种类型的领域差距 - 传感器类型、天气和位置。我们的主要发现是：(1) 具有局部点特征的 Transformer 主干比 3D CNN 更稳健，(2) 测试时锚点大小调整对于跨地理位置的适应至关重要，无需重新训练即可显着提高分数，(3) 源域增强功能使模型能够推广到低分辨率传感器，并且（4）令人惊讶的是，与使用恶劣天气数据进行训练相比，直接使用更干净的天气数据进行训练时，对恶劣天气的鲁棒性得到了提高。我们概述了我们的主要结论和发现，为开发更强大的 3D-OD 提供实用指导。</details>
**PDF:** <http://arxiv.org/pdf/2402.17562v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Diffusion Model-Based Image Editing: A Survey**<br />
**Title_cn:** 基于扩散模型的图像编辑：调查<br />
**Authors:** Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao<br />
**Abstract:** <details><summary>原文: </summary>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>去噪扩散模型已成为各种图像生成和编辑任务的强大工具，促进以无条件或输入条件方式合成视觉内容。它们背后的核心思想是学习逆转逐渐向图像添加噪声的过程，从而使它们能够从复杂的分布中生成高质量的样本。在本次调查中，我们对使用扩散模型进行图像编辑的现有方法进行了详尽的概述，涵盖了该领域的理论和实践方面。我们从多个角度对这些作品进行了深入的分析和分类，包括学习策略、用户输入条件以及可以完成的一系列特定编辑任务。此外，我们特别关注图像修复和修复，探索早期的传统上下文驱动和当前的多模态条件方法，对其方法进行全面分析。为了进一步评估文本引导图像编辑算法的性能，我们提出了一个系统基准 EditEval，其具有创新指标 LMM Score。最后，我们解决了当前的局限性并展望了未来研究的一些潜在方向。随附的存储库发布于 https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods。</details>
**PDF:** <http://arxiv.org/pdf/2402.17525v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Interactive Multi-Head Self-Attention with Linear Complexity**<br />
**Title_cn:** 具有线性复杂度的交互式多头自注意力<br />
**Authors:** Hankyul Kang, Ming-Hsuan Yang, Jongbin Ryu<br />
**Abstract:** <details><summary>原文: </summary>We propose an efficient interactive method for multi-head self-attention via decomposition. For existing methods using multi-head self-attention, the attention operation of each head is computed independently. However, we show that the interactions between cross-heads of the attention matrix enhance the information flow of the attention operation. Considering that the attention matrix of each head can be seen as a feature of networks, it is beneficial to establish connectivity between them to capture interactions better. However, a straightforward approach to capture the interactions between the cross-heads is computationally prohibitive as the complexity grows substantially with the high dimension of an attention matrix. In this work, we propose an effective method to decompose the attention operation into query- and key-less components. This will result in a more manageable size for the attention matrix, specifically for the cross-head interactions. Expensive experimental results show that the proposed cross-head interaction approach performs favorably against existing efficient attention methods and state-of-the-art backbone models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种通过分解进行多头自注意力的有效交互方法。对于使用多头自注意力的现有方法，每个头的注意力操作是独立计算的。然而，我们表明注意力矩阵十字头之间的相互作用增强了注意力操作的信息流。考虑到每个头的注意力矩阵可以被视为网络的一个特征，因此有利于在它们之间建立连接以更好地捕获交互。然而，捕获十字头之间相互作用的直接方法在计算上是令人望而却步的，因为复杂性随着注意力矩阵的高维度而大幅增长。在这项工作中，我们提出了一种有效的方法将注意力操作分解为无查询和无键组件。这将使注意力矩阵的大小更易于管理，特别是对于十字头交互。昂贵的实验结果表明，所提出的跨头交互方法相对于现有的高效注意力方法和最先进的骨干模型表现良好。</details>
**PDF:** <http://arxiv.org/pdf/2402.17507v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model**<br />
**Title_cn:** JPEG-AI验证模型中的比特率匹配算法优化<br />
**Authors:** Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit rate matching, resulting in suboptimal performance due to an unsuitable model. The proposed methodology offers a gradual algorithmic optimization for matching bit rates, resulting in a fourfold acceleration and over 1% improvement in BD-rate at the base operation point. At the high operation point, the acceleration increases up to sixfold.</details>
**Abstract_cn:** <details><summary>译文: </summary>与经典压缩框架相比，基于神经网络（NN）的图像压缩研究显示出优越的性能。与经典框架中的手工设计变换不同，基于神经网络的模型学习非线性变换，提供更紧凑的位表示，并在并行设备上实现比经典模型更快的编码速度。这些特性引起了科学界和工业界的关注，从而催生了 JPEG-AI 标准化活动。 JPEG-AI标准化过程的验证模型已经在开发中，并且已经超越了先进的VVC帧内编解码器。为了生成具有所需每像素位数的重建图像并评估 JPEG-AI 验证模型和 VVC 帧内的 BD 速率性能，采用了比特率匹配。然而，JPEG-AI 验证模型的当前状态在比特率匹配期间经历了显着的减速，导致由于模型不合适而导致性能不理想。所提出的方法提供了匹配比特率的渐进算法优化，从而在基本操作点实现四倍加速和 BD 速率提高超过 1%。在高操作点，加速度增加至六倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.17487v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction**<br />
**Title_cn:** 利用增强的点集查询进行矢量化地图构建<br />
**Authors:** Zihao Liu, Xiaoyu Zhang, Guangwei Liu, Ji Zhao, Ningyi Xu<br />
**Abstract:** <details><summary>原文: </summary>In autonomous driving, the high-definition (HD) map plays a crucial role in localization and planning. Recently, several methods have facilitated end-to-end online map construction in DETR-like frameworks. However, little attention has been paid to the potential capabilities of exploring the query mechanism. This paper introduces MapQR, an end-to-end method with an emphasis on enhancing query capabilities for constructing online vectorized maps. Although the map construction is essentially a point set prediction task, MapQR utilizes instance queries rather than point queries. These instance queries are scattered for the prediction of point sets and subsequently gathered for the final matching. This query design, called the scatter-and-gather query, shares content information in the same map element and avoids possible inconsistency of content information in point queries. We further exploit prior information to enhance an instance query by adding positional information embedded from their reference points. Together with a simple and effective improvement of a BEV encoder, the proposed MapQR achieves the best mean average precision (mAP) and maintains good efficiency on both nuScenes and Argoverse 2. In addition, integrating our query design into other models can boost their performance significantly. The code will be available at https://github.com/HXMap/MapQR.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶中，高清地图在定位和规划中发挥着至关重要的作用。最近，有几种方法促进了类似 DETR 框架中的端到端在线地图构建。然而，很少有人关注探索查询机制的潜在能力。本文介绍了 MapQR，这是一种端到端方法，重点是增强构建在线矢量化地图的查询能力。虽然地图构建本质上是点集预测任务，但 MapQR 使用实例查询而不是点查询。这些实例查询被分散以用于点集的预测，并随后被聚集以用于最终匹配。这种查询设计称为分散和聚集查询，在同一地图元素中共享内容信息，并避免点查询中可能出现的内容信息不一致的情况。我们进一步利用先验信息通过添加从参考点嵌入的位置信息来增强实例查询。加上对 BEV 编码器的简单有效的改进，所提出的 MapQR 实现了最佳平均精度（mAP），并在 nuScenes 和 Argoverse 2 上保持了良好的效率。此外，将我们的查询设计集成到其他模型中可以显着提高其性能。该代码可在 https://github.com/HXMap/MapQR 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17430v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning**<br />
**Title_cn:** LSPT：视觉表示学习的长期空间提示调整<br />
**Authors:** Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li<br />
**Abstract:** <details><summary>原文: </summary>Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉提示调整（VPT）技术因其使用称为提示的专门可学习标记使预先训练的视觉变换器（ViT）适应下游视觉任务的能力而受到关注。当代 VPT 方法，尤其是与自监督视觉转换器一起使用时，通常默认引入主要来自模型先前块的新的可学习提示或门控提示标记。这种方法的一个关键监督是它们未能利用远程先前区块的潜力作为每个自我监督的 ViT 中的提示来源。为了弥补这一关键差距，我们引入了长期空间提示调整（LSPT）——一种视觉表征学习的革命性方法。 LSPT 从人类大脑的复杂性中汲取灵感，巧妙地融入了长期门控提示。此功能用作时间编码，抑制忘记从早期块获取的参数的风险。 LSPT 进一步增强了其能力，引入了补丁标记，充当空间编码。这是战略性设计的，旨在永久积累阶级意识特征，从而增强模型区分和识别视觉类别的能力。为了验证我们提出的方法的有效性，我们在 5 个 FGVC 和 19 个 VTAB-1K 基准测试中进行了严格的实验。我们的实证研究结果强调了 LSPT 的优越性，展示了其在视觉提示调整性能方面设定新基准的能力。</details>
**PDF:** <http://arxiv.org/pdf/2402.17406v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer**<br />
**Title_cn:** CAPT：使用 Transformer 从单点云进行类别级清晰度估计<br />
**Authors:** Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, Takeshi Oishi<br />
**Abstract:** <details><summary>原文: </summary>The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>估计关节参数的能力对于机器人和计算机视觉的各种应用至关重要。在本文中，我们提出 CAPT：使用 Transformer 从点云进行类别级清晰度估计。 CAPT 使用基于端到端变压器的架构，从单点云对关节对象进行关节参数和状态估计。所提出的 CAPT 方法能够准确估计各种关节对象的关节参数和状态，具有高精度和鲁棒性。该论文还介绍了一种运动损失方法，该方法通过强调关节对象的动态特征来提高关节估计性能。此外，本文提出了双重投票策略，为框架提供从粗到细的参数估计。几个类别数据集的实验结果表明，我们的方法优于现有的清晰度估计替代方法。我们的研究为在铰接式对象分析中应用基于 Transformer 的架构提供了一个有前景的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.17360v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Image-Text Matching with Multi-View Attention**<br />
**Title_cn:** 具有多视图注意力的图像文本匹配<br />
**Authors:** Rui Cheng, Wanqing Cui<br />
**Abstract:** <details><summary>原文: </summary>Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by diverse attention heads with different view codes. And then concatenate these representations into one for matching. A diversity objective is also used to promote diversity between attention heads. With this method, models are able to encode images and text from different views and attend to more key points. So we can get representations that contain more information. When doing retrieval tasks, the matching scores between images and texts can be calculated from different aspects, leading to better matching performance. Experiment results on MSCOCO and Flickr30K show that our proposed model brings improvements over existing models. Further case studies show that different attention heads can focus on different contents and finally obtain a more comprehensive representation.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的图文匹配双流模型在保证检索速度的同时表现出了良好的性能，受到了工业界和学术界的广泛关注。这些方法使用单一表示形式分别对图像和文本进行编码，并通过余弦相似度或向量的内积获得匹配分数。然而，双流模型的性能通常不是最优的。一方面，单一的表示很难全面覆盖复杂的内容。另一方面，在这种缺乏交互的框架中，匹配多个含义具有挑战性，从而导致信息被忽略。为了解决上述问题并提高双流模型的性能，我们提出了一种用于双流图像文本匹配 MVAM 的多视图注意方法（\textbf{M}ulti-\textbf{V}iew \textbf {A}注意\textbf{M}模型）。它首先通过具有不同视图代码的不同注意力头学习多个图像和文本表示。然后将这些表示连接成一个以进行匹配。多样性目标也用于促进注意力头之间的多样性。通过这种方法，模型能够从不同的角度对图像和文本进行编码，并关注更多的关键点。因此我们可以获得包含更多信息的表示。在进行检索任务时，可以从不同方面计算图像和文本之间的匹配分数，从而获得更好的匹配性能。 MSCOCO 和 Flickr30K 上的实验结果表明，我们提出的模型比现有模型带来了改进。进一步的案例研究表明，不同的注意力头可以关注不同的内容，最终获得更全面的表征。</details>
**PDF:** <http://arxiv.org/pdf/2402.17237v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology**<br />
**Title_cn:** 特征重新嵌入：迈向计算病理学的基础模型级性能<br />
**Authors:** Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu<br />
**Abstract:** <details><summary>原文: </summary>Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.</details>
**Abstract_cn:** <details><summary>译文: </summary>多实例学习 (MIL) 是计算病理学中使用最广泛的框架，涵盖分型、诊断、预后等。然而，现有的 MIL 范例通常需要离线实例特征提取器，例如预训练的 ResNet 或基础模型。这种方法缺乏在特定下游任务中进行特征微调的能力，限制了其适应性和性能。为了解决这个问题，我们提出了一种重新嵌入的区域变压器（R$^2$T），用于在线重新嵌入实例特征，它捕获细粒度的局部特征并建立跨不同区域的连接。与现有的专注于预训练强大的特征提取器或设计复杂的实例聚合器的作品不同，R$^2$T 是为在线重新嵌入实例特征而定制的。它作为一个便携式模块，可以无缝集成到主流 MIL 模型中。在常见计算病理学任务上的大量实验结果验证了：1）特征重新嵌入将基于ResNet-50特征的MIL模型的性能提高到基础模型特征的水平，并进一步增强了基础模型特征的性能； 2）R$^2$T可以为各种MIL模型带来更显着的性能改进； 3) R$^2$T-MIL 作为 R$^2$T 增强型 AB-MIL，大大优于其他最新方法。代码位于：~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}。</details>
**PDF:** <http://arxiv.org/pdf/2402.17228v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization**<br />
**Title_cn:** CharacterGen：通过多视图姿势规范化从单张图像高效生成 3D 角色<br />
**Authors:** Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu<br />
**Abstract:** <details><summary>原文: </summary>In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在数字内容创作领域，从单个图像生成高质量的 3D 角色具有挑战性，特别是考虑到各种身体姿势的复杂性以及自遮挡和姿势模糊的问题。在本文中，我们介绍了CharacterGen，这是一个为高效生成3D 角色而开发的框架。 CharacterGen 引入了简化的生成流程以及图像调节的多视图扩散模型。该模型有效地将输入姿势校准为规范形式，同时保留输入图像的关键属性，从而解决不同姿势带来的挑战。基于变压器的、可推广的稀疏视图重建模型是我们方法的另一个核心组成部分，有助于从多视图图像创建详细的 3D 模型。我们还采用纹理反投影策略来生成高质量的纹理图。此外，我们还整理了一个动漫角色数据集，以多种姿势和视图呈现，以训练和评估我们的模型。我们的方法已经通过定量和定性实验进行了彻底评估，显示出其在生成具有高质量形状和纹理的 3D 角色方面的熟练程度，为下游应用（例如绑定和动画）做好了准备。</details>
**PDF:** <http://arxiv.org/pdf/2402.17214v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **CharNeRF: 3D Character Generation from Concept Art**<br />
**Title_cn:** CharNeRF：从概念艺术生成 3D 角色<br />
**Authors:** Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan<br />
**Abstract:** <details><summary>原文: </summary>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model's inferencing capabilities are influenced by the training data's characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 建模在 AR/VR 和游戏领域具有重要意义，可以实现艺术创造力和实际应用。然而，这个过程通常非常耗时并且需要高水平的技能。在本文中，我们提出了一种从一致的周转概念艺术创建 3D 角色体积表示的新颖方法，该概念艺术作为 3D 建模行业的标准输入。虽然神经辐射场 (NeRF) 已经成为基于图像的 3D 重建领域的游戏规则改变者，但据我们所知，尚无已知的研究可以优化概念艺术的流程。为了利用概念艺术的潜力，通过其定义的身体姿势和特定的视角，我们建议将其编码为我们模型的先验。我们训练网络通过可学习的视图方向参与的多头自注意力层将这些先验用于各种 3D 点。此外，我们还证明了射线采样和表面采样的结合可以增强我们网络的推理能力。我们的模型能够生成高质量的 360 度角色视图。随后，我们提供了一个简单的指南，以更好地利用我们的模型来提取 3D 网格。值得注意的是，我们模型的推理能力受到训练数据特征的影响，主要关注单头、两条手臂和两条腿的角色。尽管如此，我们的方法仍然是通用的，并且适用于不同主题的概念艺术，而不对数据强加任何特定的假设。</details>
**PDF:** <http://arxiv.org/pdf/2402.17115v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **LoDIP: Low light phase retrieval with deep image prior**<br />
**Title_cn:** LoDIP：具有深度图像先验的低光相位检索<br />
**Authors:** Raunak Manekar, Elisa Negrini, Minh Pham, Daniel Jacobs, Jaideep Srivastava<br />
**Abstract:** <details><summary>原文: </summary>Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage. However, most PR methods struggle in low dose scenario due to the presence of very high shot noise. Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging. But these depend on a time series of measurements, rendering them unsuitable for single-image applications. Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context. Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR. In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval. Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>相位检索 (PR) 是科学成像中的一项基本挑战，它使相干衍射成像 (CDI) 等纳米级技术成为可能。在样品易受辐射损伤的应用中，低辐射剂量成像变得非常重要。然而，由于存在非常高的散粒噪声，大多数 PR 方法在低剂量情况下都很困难。以原位 CDI 为代表的光学数据采集装置的进步已显示出低剂量成像的潜力。但这些依赖于测量的时间序列，使得它们不适合单图像应用。类似地，在计算方面，数据驱动的相位检索技术不容易适应单图像环境。基于深度学习的单图像方法（例如深度图像先验）对于各种成像任务都有效，但在应用于 PR 时却表现有限。在这项工作中，我们提出了 LoDIP，它将原位 CDI 设置与隐式神经先验的力量相结合，以解决单图像低剂量相位检索的问题。定量评估证明了 LoDIP 在此任务上的优越性能以及对真实实验场景的适用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17745v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding**<br />
**Title_cn:** 使用对比学习和几何展开在 3D-PLI 中分析人类海马的区域组织<br />
**Authors:** Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid<br />
**Abstract:** <details><summary>原文: </summary>Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解人脑的皮质组织需要针对不同结构和功能成像数据的可解释描述符。 3D 偏振光成像 (3D-PLI) 是一种以高分辨率可视化死后大脑中纤维结构的成像方式，还可以捕获细胞体的存在，例如识别海马亚区。然而，3D-PLI 图像中丰富的纹理使得这种模式特别难以分析，并且仍然需要建立表征建筑图案的最佳实践。在这项工作中，我们展示了一种新方法，通过将展开方法的最新进展与使用自监督对比学习方法获得的深层纹理特征相结合，来分析 3D-PLI 中人类海马体的区域组织。我们在表征中识别出与海马子区域的经典描述非常一致的聚类，从而为所开发的方法提供了有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17744v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PHNet: Patch-based Normalization for Portrait Harmonization**<br />
**Title_cn:** PHNet：基于补丁的标准化肖像协调<br />
**Authors:** Karen Efremyan, Elizaveta Petrova, Evgeny Kaskov, Alexander Kapitanov<br />
**Abstract:** <details><summary>原文: </summary>A common problem for composite images is the incompatibility of their foreground and background components. Image harmonization aims to solve this problem, making the whole image look more authentic and coherent. Most existing solutions predict lookup tables (LUTs) or reconstruct images, utilizing various attributes of composite images. Recent approaches have primarily focused on employing global transformations like normalization and color curve rendering to achieve visual consistency, and they often overlook the importance of local visual coherence. We present a patch-based harmonization network consisting of novel Patch-based normalization (PN) blocks and a feature extractor based on statistical color transfer. Extensive experiments demonstrate the network's high generalization capability for different domains. Our network achieves state-of-the-art results on the iHarmony4 dataset. Also, we created a new human portrait harmonization dataset based on FFHQ and checked the proposed method to show the generalization ability by achieving the best metrics on it. The benchmark experiments confirm that the suggested patch-based normalization block and feature extractor effectively improve the network's capability to harmonize portraits. Our code and model baselines are publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>合成图像的一个常见问题是其前景和背景组件的不兼容。图像协调就是为了解决这个问题，让整个图像看起来更加真实、连贯。大多数现有解决方案利用合成图像的各种属性来预测查找表 (LUT) 或重建图像。最近的方法主要集中于采用归一化和颜色曲线渲染等全局变换来实现视觉一致性，并且它们经常忽视局部视觉一致性的重要性。我们提出了一种基于补丁的协调网络，由新颖的基于补丁的归一化（PN）块和基于统计颜色传输的特征提取器组成。大量的实验证明了该网络对于不同领域的高泛化能力。我们的网络在 iHarmony4 数据集上取得了最先进的结果。此外，我们基于 FFHQ 创建了一个新的人体肖像协调数据集，并检查了所提出的方法，以通过在其上实现最佳指标来显示泛化能力。基准实验证实，所建议的基于补丁的归一化块和特征提取器有效地提高了网络协调肖像的能力。我们的代码和模型基线是公开的。</details>
**PDF:** <http://arxiv.org/pdf/2402.17561v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis**<br />
**Title_cn:** AVS-Net：采用自适应体素大小的点采样进行 3D 点云分析<br />
**Authors:** Hongcheng Yang, Dingkang Liang, Dingyuan Zhang, Xingyu Jiang, Zhe Liu, Zhikang Zou, Yingying Zhu<br />
**Abstract:** <details><summary>原文: </summary>Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency. Code will be available at https://github.com/yhc2021/AVS-Net.</details>
**Abstract_cn:** <details><summary>译文: </summary>高效的下采样在点云学习中起着至关重要的作用，特别是对于大规模 3D 场景。现有的下采样方法要么需要巨大的计算负担，要么牺牲细粒度的几何信息。本文提出了一种先进的采样器，可实现高精度和高效率。所提出的方法利用基于体素的采样作为基础，但有效地解决了体素大小确定和关键几何线索保存方面的挑战。具体来说，我们提出了一个体素适应模块，它参考基于点的下采样率自适应地调整体素大小。这确保采样结果呈现出有利于理解各种 3D 对象或场景的分布。此外，我们引入了一个与任意体素大小兼容的网络，用于采样和特征提取，同时保持高效率。我们的方法在 ShapeNetPart 和 ScanNet 基准测试中实现了最先进的精度，并且效率很高。代码将在 https://github.com/yhc2021/AVS-Net 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.17521v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**<br />
**Title_cn:** EMO：Emote Portrait Alive - 在弱条件下使用音视频扩散模型生成富有表现力的肖像视频<br />
**Authors:** Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo<br />
**Abstract:** <details><summary>原文: </summary>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们通过关注音频提示和面部动作之间的动态和微妙关系来解决增强头部说话视频生成的真实性和表现力的挑战。我们发现传统技术的局限性，这些技术往往无法捕捉人类表情的全部范围和个人面部风格的独特性。为了解决这些问题，我们提出了 EMO，这是一种利用直接音频到视频合成方法的新颖框架，绕过了对中间 3D 模型或面部标志的需求。我们的方法可确保整个视频中的无缝帧过渡和一致的身份保留，从而产生高度表现力和逼真的动画。实验结果表明，EMO不仅能够制作令人信服的口语视频，还能够制作各种风格的歌唱视频，在表现力和真实感方面明显优于现有的最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17485v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing**<br />
**Title_cn:** 通过部分-整体-层次结构消息传递生成 3D 零件组装<br />
**Authors:** Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao<br />
**Abstract:** <details><summary>原文: </summary>Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成式 3D 零件组装涉及了解零件关系并预测其 6-DoF 姿势，以组装真实的 3D 形状。先前的工作通常关注各个部分的几何形状，而忽略了对象的部分整体层次结构。利用两个关键观察结果：1）超级零件姿势提供了有关零件姿势的强烈提示，2）由于超级零件较少，预测超级零件姿势更容易，我们提出了一种用于高效 3D 零件组装的零件-整体-层次结构消息传递网络。我们首先通过对没有任何语义标签的几何相似部件进行分组来引入超级部件。然后我们采用部分-整体分层编码器，其中超级部分编码器根据输入部分预测潜在的超级部分姿势。随后，我们使用潜在姿势变换点云，将其馈送到零件编码器以聚合超级零件信息并推理零件关系以预测所有零件姿势。在训练中，只需要真实的部分姿势。在推理过程中，超级部件的预测潜在姿势增强了可解释性。 PartNet 数据集上的实验结果表明，我们的方法在零件和连接精度方面实现了最先进的性能，并实现了可解释的分层零件装配。</details>
**PDF:** <http://arxiv.org/pdf/2402.17464v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction**<br />
**Title_cn:** VastGaussian：用于大型场景重建的 Vast 3D 高斯<br />
**Authors:** Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的基于 NeRF 的大场景重建方法通常在视觉质量和渲染速度方面存在局限性。虽然最近的 3D 高斯溅射在小规模和以对象为中心的场景中效果很好，但由于视频内存有限、优化时间长和明显的外观变化，将其扩展到大型场景会带来挑战。为了应对这些挑战，我们提出了 VastGaussian，这是第一个基于 3D 高斯分布在大型场景上进行高质量重建和实时渲染的方法。我们提出了一种渐进式分区策略，将大场景划分为多个单元，其中训练摄像机和点云按照空域感知可见性标准正确分布。这些单元经过并行优化后合并成一个完整的场景。我们还将解耦外观建模引入优化过程中，以减少渲染图像中的外观变化。我们的方法优于现有的基于 NeRF 的方法，并在多个大型场景数据集上实现了最先进的结果，从而实现了快速优化和高保真实时渲染。</details>
**PDF:** <http://arxiv.org/pdf/2402.17427v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model**<br />
**Title_cn:** DiffuseKronA：个性化扩散模型的参数高效微调方法<br />
**Authors:** Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen<br />
**Abstract:** <details><summary>原文: </summary>In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \textbf{\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\% and 99.947\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes \textit{DiffuseKronA} more interpretable and even can achieve up to a 50\% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, \textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at \href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在主题驱动的文本到图像（T2I）生成模型领域，DreamBooth 和 BLIP-Diffusion 等最新发展已经取得了令人印象深刻的结果，但由于其密集的微调需求和大量的参数要求而遇到了限制。虽然 DreamBooth 中的低秩适应 (LoRA) 模块减少了可训练参数，但它引入了对超参数的明显敏感性，导致参数效率和 T2I 个性化图像合成质量之间的折衷。为了解决这些限制，我们引入了 \textbf{\textit{DiffuseKronA}}，一种新颖的基于 Kronecker 产品的自适应模块，与 LoRA-DreamBooth 和原始 DreamBooth 相比，它不仅显着减少了 35\% 和 99.947\% 的参数数量，分别，而且还提高了图像合成的质量。至关重要的是， \textit{DiffuseKronA} 缓解了超参数敏感性问题，在各种超参数上提供一致的高质量生成，从而减少了广泛微调的必要性。此外，更可控的分解使得 \textit{DiffuseKronA} 更具可解释性，甚至可以实现高达 50% 的减少，结果与 LoRA-Dreambooth 相当。根据多样化和复杂的输入图像和文本提示进行评估，\textit{DiffuseKronA} 始终优于现有模型，生成更高质量的多样化图像，具有更高的保真度和更准确的对象颜色分布，同时保持卓越的参数效率，从而呈现出T2I 生成建模领域取得了实质性进展。我们的项目页面包含代码链接和预先训练的检查点，可在 \href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17412v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Sora Generates Videos with Stunning Geometrical Consistency**<br />
**Title_cn:** Sora 生成具有令人惊叹的几何一致性的视频<br />
**Authors:** Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, Ming-Ming Cheng<br />
**Abstract:** <details><summary>原文: </summary>The recently developed Sora model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena. Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively. In this paper, we introduce a new benchmark that assesses the quality of the generated videos based on their adherence to real-world physics principles. We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality. From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules. Project page: https://sora-geometrical-consistency.github.io/</details>
**Abstract_cn:** <details><summary>译文: </summary>最近开发的 Sora 模型 [1] 在视频生成方面表现出了卓越的能力，引发了关于其模拟现实世界现象的能力的激烈讨论。尽管它越来越受欢迎，但缺乏既定的指标来定量评估其对现实世界物理的保真度。在本文中，我们引入了一个新的基准，该基准根据生成的视频是否符合现实世界的物理原理来评估其质量。我们采用一种将生成的视频转换为 3D 模型的方法，利用 3D 重建的准确性在很大程度上取决于视频质量的前提。从 3D 重建的角度来看，我们使用构建的 3D 模型满足的几何约束的保真度作为代理来衡量生成的视频符合现实世界物理规则的程度。项目页面：https://sora-geometrical-consistency.github.io/</details>
**PDF:** <http://arxiv.org/pdf/2402.17403v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching**<br />
**Title_cn:** 用于局部感知 3D 刚性点云匹配的耦合拉普拉斯特征图<br />
**Authors:** Matteo Bastico, Etienne Decencière, Laurent Corté, Yannick Tillier, David Ryckelynck<br />
**Abstract:** <details><summary>原文: </summary>Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云匹配是计算机视觉、医学和机器人领域的一项关键技术，主要涉及寻找点云对或体素之间的对应关系。在一些实际场景中，强调局部差异对于准确识别正确匹配至关重要，从而增强匹配过程的整体鲁棒性和可靠性。常用的形状描述符有一些局限性，并且通常无法提供有关配对几何形状的有意义的局部见解。在这项工作中，我们提出了一种基于图拉普拉斯特征图的新技术，通过考虑精细的局部结构来匹配点云。为了处理拉普拉斯特征图的顺序和符号模糊性，我们引入了一种新的算子，称为耦合拉普拉斯算子，它可以轻松地为多个严格配准的几何图形生成对齐的特征空间。我们表明，这些对齐的高维空间之间的相似性为匹配形状提供了局部有意义的分数。我们最初以逐点方式评估所提出技术的性能，特别关注使用 MVTec 3D-AD 数据集进行对象异常定位的任务。此外，我们定义了一项新的医疗任务，称为自动骨侧估计（BSE），我们通过从耦合特征空间导出的全局相似性得分来解决该任务。为了测试它，我们提出了一个从各种公共数据集中收集骨表面结构的基准。我们的匹配技术基于耦合拉普拉斯算子，在这两项任务上都达到了令人印象深刻的准确性，从而优于其他方法。重现我们实验的代码可在 https://github.com/matteo-bastico/CoupledLaplacian 和补充代码中公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17372v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**<br />
**Title_cn:** 学习动态四面体以实现高质量的头部说话合成<br />
**Authors:** Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang<br />
**Abstract:** <details><summary>原文: </summary>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近在隐式表示方面的工作，例如神经辐射场（NeRF），已经促进了从视频序列中生成逼真且可动画化的头部化身。这些隐式方法仍然面临视觉伪影和抖动的问题，因为缺乏显式几何约束对精确建模复杂的面部变形提出了根本性挑战。在本文中，我们介绍了动态四面体（DynTet），这是一种新颖的混合表示，它通过神经网络对显式动态网格进行编码，以确保各种运动和视点之间的几何一致性。 DynTet 通过基于坐标的网络进行参数化，该网络学习符号距离、变形和材料纹理，将训练数据锚定到预定义的四面体网格中。利用 Marching Tetrahedra，DynTet 可以通过一致的拓扑有效解码纹理网格，从而通过可微分光栅器实现快速渲染，并通过像素损失进行监督。为了提高训练效率，我们结合了经典的 3D Morphable 模型来促进几何学习，并定义一个规范空间来简化纹理学习。由于 DynTet 中采用了有效的几何表示，这些优点很容易实现。与之前的作品相比，根据各种指标，DynTet 在保真度、唇形同步和实时性能方面都有显着改进。除了生成稳定且具有视觉吸引力的合成视频之外，我们的方法还输出动态网格，这有望实现许多新兴应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.17364v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **ICP-Flow: LiDAR Scene Flow Estimation with ICP**<br />
**Title_cn:** ICP-Flow：利用 ICP 进行 LiDAR 场景流量估计<br />
**Authors:** Yancong Lin, Holger Caesar<br />
**Abstract:** <details><summary>原文: </summary>Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景流表征了自动驾驶车辆在附近时间步捕获的两次 LiDAR 扫描之间的 3D 运动。流行的方法将场景流视为逐点无约束流向量，可以通过预先大规模训练或推理时耗时的优化来学习。然而，这些方法没有考虑到自动驾驶中的物体通常是刚性运动的。我们将这种刚性运动假设纳入我们的设计中，其目标是通过扫描关联对象，然后估计局部刚性变换。我们提出了 ICP-Flow，一种免学习的流量估计器。我们设计的核心是传统的迭代最近点（ICP）算法，它随着时间的推移对齐对象并输出相应的刚性变换。至关重要的是，为了帮助 ICP，我们提出了一种基于直方图的初始化，可以发现最可能的翻译，从而为 ICP 提供一个良好的起点。然后从严格的变换中恢复完整的场景流。我们在 Waymo 数据集上优于最先进的基线（包括监督模型），并在 Argoverse-v2 和 nuScenes 上具有竞争力。此外，我们训练了一个前馈神经网络，由模型中的伪标签进行监督，并在所有能够实时推理的模型中实现了最佳性能。我们验证了我们的模型在具有较长时间间隙（长达 0.5 秒）的场景流估计方面的优势，而其他模型无法提供有意义的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.17351v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Denoising Diffusion Models for Inpainting of Healthy Brain Tissue**<br />
**Title_cn:** 用于修复健康脑组织的去噪扩散模型<br />
**Authors:** Alicia Durrer, Philippe C. Cattin, Julia Wolleb<br />
**Abstract:** <details><summary>原文: </summary>This paper is a contribution to the "BraTS 2023 Local Synthesis of Healthy Brain Tissue via Inpainting Challenge". The task of this challenge is to transform tumor tissue into healthy tissue in brain magnetic resonance (MR) images. This idea originates from the problem that MR images can be evaluated using automatic processing tools, however, many of these tools are optimized for the analysis of healthy tissue. By solving the given inpainting task, we enable the automatic analysis of images featuring lesions, and further downstream tasks. Our approach builds on denoising diffusion probabilistic models. We use a 2D model that is trained using slices in which healthy tissue was cropped out and is learned to be inpainted again. This allows us to use the ground truth healthy tissue during training. In the sampling stage, we replace the slices containing diseased tissue in the original 3D volume with the slices containing the healthy tissue inpainting. With our approach, we achieve comparable results to the competing methods. On the validation set our model achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In future we plan to extend our 2D model to a 3D model, allowing to inpaint the region of interest as a whole without losing context information of neighboring slices.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文是对“BraTS 2023 通过修复挑战局部合成健康脑组织”的贡献。本次挑战的任务是在脑磁共振（MR）图像中将肿瘤组织转化为健康组织。这个想法源于这样一个问题：MR 图像可以使用自动处理工具进行评估，然而，这些工具中的许多工具都针对健康组织的分析进行了优化。通过解决给定的修复任务，我们可以自动分析具有病变特征的图像以及进一步的下游任务。我们的方法建立在去噪扩散概率模型的基础上。我们使用一个 2D 模型，该模型使用切片进行训练，其中健康组织被剪掉并学会再次修复。这使我们能够在训练期间使用真实的健康组织。在采样阶段，我们用包含健康组织修复的切片替换原始3D体积中包含患病组织的切片。通过我们的方法，我们取得了与竞争方法相当的结果。在验证集上，我们的模型实现了 0.7804 的平均 SSIM、20.3525 的 PSNR 和 0.0113 的 MSE。将来，我们计划将 2D 模型扩展到 3D 模型，从而可以对整个感兴趣区域进行修复，而不会丢失相邻切片的上下文信息。</details>
**PDF:** <http://arxiv.org/pdf/2402.17307v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **DivAvatar: Diverse 3D Avatar Generation with a Single Prompt**<br />
**Title_cn:** DivAvatar：通过单一提示生成多样化的 3D 头像<br />
**Authors:** Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao<br />
**Abstract:** <details><summary>原文: </summary>Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于扩散模型的进步，文本到头像的生成最近取得了重大进展。然而，大多数现有的工作仍然受到有限的多样性的限制，对于给定的文本提示生成的头像在外观上存在细微的差异。我们设计了 DivAvatar，这是一个可以生成多样化头像的新颖框架，通过单个文本提示为 3D 创意人员提供大量独特且丰富多样的 3D 头像。与大多数利用特定于场景的 3D 表示（例如 NeRF）的现有工作不同，DivAvatar 对 3D 生成模型（即 EVA3D）进行了微调，允许在推理时间内通过简单的噪声采样生成不同的化身。 DivAvatar 有两个关键设计有助于实现世代多样性和视觉质量。第一个是训练阶段的噪声采样技术，这对于生成不同的外观至关重要。第二个是语义感知缩放机制和新颖的深度损失，前者通过对特定身体部位进行单独微调来产生高文本保真度的外观，后者通过平滑特征空间中生成的网格来极大地提高几何质量。大量实验表明，DivAvatar 在生成不同外观的头像方面具有很强的通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17292v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network**<br />
**Title_cn:** 通过扩散模型和组自动编码器超分辨率网络增强高光谱图像<br />
**Authors:** Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong<br />
**Abstract:** <details><summary>原文: </summary>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的高光谱图像（HSI）超分辨率（SR）方法很难有效地捕获复杂的光谱空间关系和低级细节，而扩散模型代表了一种有前途的生成模型，以其在复杂关系建模和学习高级和低级细节方面的卓越性能而闻名。低级视觉特征。扩散模型在 HSI SR 中的直接应用受到模型收敛困难和推理时间延长等挑战的阻碍。在这项工作中，我们引入了一种新颖的组自动编码器（GAE）框架，该框架与扩散模型协同结合，构建了高效的 HSI SR 模型（DMGASR）。我们提出的 GAE 框架将高维 HSI 数据编码到扩散模型工作的低维潜在空间中，从而减轻了训练扩散模型的难度，同时保持带相关性并大大减少了推理时间。自然和遥感高光谱数据集的实验结果表明，所提出的方法在视觉和测量上都优于其他最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17285v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Differentiable Biomechanics Unlocks Opportunities for Markerless Motion Capture**<br />
**Title_cn:** 可微分生物力学为无标记运动捕捉带来机遇<br />
**Authors:** R. James Cotton<br />
**Abstract:** <details><summary>原文: </summary>Recent developments have created differentiable physics simulators designed for machine learning pipelines that can be accelerated on a GPU. While these can simulate biomechanical models, these opportunities have not been exploited for biomechanics research or markerless motion capture. We show that these simulators can be used to fit inverse kinematics to markerless motion capture data, including scaling the model to fit the anthropomorphic measurements of an individual. This is performed end-to-end with an implicit representation of the movement trajectory, which is propagated through the forward kinematic model to minimize the error from the 3D markers reprojected into the images. The differential optimizer yields other opportunities, such as adding bundle adjustment during trajectory optimization to refine the extrinsic camera parameters or meta-optimization to improve the base model jointly over trajectories from multiple participants. This approach improves the reprojection error from markerless motion capture over prior methods and produces accurate spatial step parameters compared to an instrumented walkway for control and clinical populations.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的发展创建了专为机器学习管道设计的可微分物理模拟器，可以在 GPU 上加速。虽然这些可以模拟生物力学模型，但这些机会尚未被用于生物力学研究或无标记运动捕捉。我们表明，这些模拟器可用于将逆运动学拟合到无标记运动捕捉数据，包括缩放模型以适应个体的拟人化测量。这是通过运动轨迹的隐式表示进行端到端的，该表示通过正向运动学模型传播，以最大限度地减少重新投影到图像中的 3D 标记的误差。差分优化器产生了其他机会，例如在轨迹优化期间添加捆绑调整以细化外部相机参数或元优化以共同改进来自多个参与者的轨迹的基本模型。与之前的方法相比，这种方法改善了无标记运动捕捉的重投影误差，并且与用于控制和临床人群的仪表走道相比，产生了准确的空间步长参数。</details>
**PDF:** <http://arxiv.org/pdf/2402.17192v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator**<br />
**Title_cn:** 使用商品移动机械手打开现实世界中的橱柜和抽屉<br />
**Authors:** Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta<br />
**Abstract:** <details><summary>原文: </summary>Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.</details>
**Abstract_cn:** <details><summary>译文: </summary>拉开柜子和抽屉在感知（从机载传感器推断物体的关节参数）、规划（生成符合严格任务限制的运动计划）和控制（在对环境施加力的同时建立和保持接触）方面提出了许多困难的技术挑战。在这项工作中，我们构建了一个端到端系统，使商品移动机械手（Stretch RE2）能够在各种以前未见过的现实世界环境中拉开橱柜和抽屉。我们对该系统进行了为期 4 天的现实世界测试，涵盖 13 个不同现实世界环境中的 31 个不同对象。我们的系统在未见过的环境中零次打开新颖的橱柜和抽屉的成功率达到 61%。对故障模式的分析表明，感知错误是我们系统面临的最重大挑战。我们将开源代码和模型，供其他人复制和构建我们的系统。</details>
**PDF:** <http://arxiv.org/pdf/2402.17767v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning**<br />
**Title_cn:** PLReMix：用伪标签松弛对比表示学习对抗噪声标签<br />
**Authors:** Xiaoyu Liu, Beitong Zhou, Cheng Cheng<br />
**Abstract:** <details><summary>原文: </summary>Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，对比表示学习（CRL）在噪声标签学习（LNL）中的应用显示出了有希望的进步，因为它具有学习均匀分布表示以更好地区分噪声标签的卓越能力。然而，CRL 主要用作预训练技术，导致复杂的多阶段训练流程。我们还观察到，将 CRL 与监督 LNL 方法结合起来会降低性能。在 CRL 中使用同一类的不同图像作为负对会造成 CRL 和监督损失之间的优化冲突。为了解决这两个问题，我们提出了一种端到端的 PLReMix 框架，通过引入伪标签松弛（PLR）对比损失来缓解损失之间的冲突，从而避免复杂的流程。这种 PLR 损失通过过滤掉在预测概率的前 k 个索引处重叠的不适当的负对，构建每个样本的可靠负集，从而产生比普通 CRL 更紧凑的语义簇。此外，采用二维高斯混合模型（GMM），通过同时利用语义信息和模型输出来区分干净样本和噪声样本，该模型是对先前广泛使用的一维形式的扩展。 PLR 损失和半监督损失同时应用于 GMM 划分的干净样本和噪声样本上的训练。在多个基准数据集上的实验证明了所提出方法的有效性。我们提出的 PLR 损失是可扩展的，可以轻松集成到其他 LNL 方法中并提高其性能。代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.17589v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning**<br />
**Title_cn:** 展示和减少视觉语言表征学习的捷径<br />
**Authors:** Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke<br />
**Abstract:** <details><summary>原文: </summary>Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言模型（VLM）主要依靠对比训练来学习图像和标题的通用表示。我们关注的是当一张图像与多个标题相关联时的情况，每个标题包含所有标题之间共享的信息以及每个标题关于图像中描绘的场景的唯一信息。在这种情况下，尚不清楚对比损失是否足以学习包含字幕提供的所有信息的任务最佳表示，或者对比学习设置是否鼓励学习最小化对比损失的简单捷径。我们引入了视觉语言的合成快捷方式：一个训练和评估框架，我们将合成快捷方式注入到图像文本数据中。我们表明，从头开始训练或使用包含这些合成快捷方式的数据进行微调的对比 VLM 主要学习代表快捷方式的特征。因此，对比损失不足以学习任务最佳表示，即包含图像和相关标题之间共享的所有任务相关信息的表示。我们研究了两种在训练和评估框架中减少快捷学习的方法：（i）潜在目标解码和（ii）隐式特征修改。我们的经验表明，这两种方法都提高了评估任务的性能，但在使用我们的快捷学习框架进行训练和评估时仅部分减少了快捷学习。因此，我们展示了对比视觉语言表示学习的快捷学习框架的困难和挑战。</details>
**PDF:** <http://arxiv.org/pdf/2402.17510v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme**<br />
**Title_cn:** MGE：免训练的高效模型生成和增强方案<br />
**Authors:** Xuan Wang, Zeshan Pang, Yuliang Lu, Xuehu Yan<br />
**Abstract:** <details><summary>原文: </summary>To provide a foundation for the research of deep learning models, the construction of model pool is an essential step. This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance. Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases. Moreover, the time consumed in generating models accounts for only 1\% of the time required for normal model training. More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in few-shot tasks. And the behavioral dissimilarity of generated models has the potential of adversarial defense.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了给深度学习模型的研究提供基础，模型池的构建是必不可少的一步。本文提出了一种免训练且高效的模型生成和增强方案（MGE）。该方案在模型生成过程中主要考虑两个方面：模型参数的分布和模型性能。实验结果表明，生成的模型与正常训练获得的模型相当，在某些情况下甚至更优越。而且，生成模型所消耗的时间仅占正常模型训练所需时间的1\%。更重要的是，随着 Evolution-MGE 的增强，生成的模型在少样本任务中表现出有竞争力的泛化能力。生成模型的行为差异具有对抗性防御的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.17486v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks**<br />
**Title_cn:** ArcSin：用于语言驱动视觉任务的自适应范围余弦相似度注入噪声<br />
**Authors:** Yang Liu, Xiaomin Yu, Gongyu Zhang, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin<br />
**Abstract:** <details><summary>原文: </summary>In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks. The code will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项研究中，我们解决了弥合语言学习和视觉任务推理之间模态差距的挑战性任务，包括视觉问答（VQA）、图像字幕（IC）和视觉蕴涵（VE）。我们在零样本跨模态传输设置中训练这些任务的模型，在该领域中，以前最先进的方法依赖于固定尺度的噪声注入，通常会损害原始模态嵌入的语义内容。为了解决这个问题，我们提出了一种称为自适应范围余弦相似度注入噪声（ArcSin）的新方法。首先，我们引入了一种创新的自适应噪声尺度，可以有效地生成具有更多可变性的文本元素，同时保留原始文本特征的完整性。其次，采用相似池策略，通过扩大整体噪声规模来扩大领域泛化潜力。这种双重策略有效地扩大了原始域的范围，同时维护了内容的完整性。我们的实证结果表明，这些模型在性能方面与图像训练的模型非常接近。具体来说，我们的方法比之前最先进的方法有了显着的改进，在 S-Cap 和 M-Cap 中分别实现了 1.9 和 1.1 CIDEr 点的增益。此外，我们观察到 VQA、VQA-E 和 VE 的准确率分别提高了 1.5 个百分点 (pp)、1.4 个百分点和 1.4 个百分点，突破了图像训练模型基准的限制内可实现的界限。代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.17298v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning**<br />
**Title_cn:** 组合零样本学习中基于上下文和多样性驱动的特异性<br />
**Authors:** Yun Li, Zhe Liu, Hang Chen, Lina Yao<br />
**Abstract:** <details><summary>原文: </summary>Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object pairs based on a limited set of observed examples. Current CZSL methodologies, despite their advancements, tend to neglect the distinct specificity levels present in attributes. For instance, given images of sliced strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic `Red-Strawberry', despite the former being more informative. They also suffer from ballooning search space when shifting from Close-World (CW) to Open-World (OW) CZSL. To address the issues, we introduce the Context-based and Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our framework evaluates the specificity of attributes by considering the diversity of objects they apply to and their related context. This novel approach allows for more accurate predictions by emphasizing specific attribute-object pairs and improves composition filtering in OW-CZSL. We conduct experiments in both CW and OW scenarios, and our model achieves state-of-the-art results across three datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>组合零样本学习（CZSL）旨在基于一组有限的观察到的示例来识别看不见的属性-对象对。当前的 CZSL 方法尽管取得了进步，但往往忽略了属性中存在的不同特异性水平。例如，给定切片草莓的图像，他们可能无法将“切片草莓”优先于通用的“红草莓”，尽管前者信息更丰富。当从封闭世界 (CW) 转移到开放世界 (OW) CZSL 时，它们还会遭受搜索空间膨胀的困扰。为了解决这些问题，我们引入了基于情境和多样性驱动的 CZSL 特异性学习框架（CDS-CZSL）。我们的框架通过考虑属性所应用的对象的多样性及其相关上下文来评估属性的特殊性。这种新颖的方法通过强调特定的属性-对象对来实现更准确的预测，并改进 OW-CZSL 中的组合过滤。我们在 CW 和 OW 场景中进行了实验，我们的模型在三个数据集上取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.17251v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning**<br />
**Title_cn:** Diffusion 遇上 DAgger：增强手眼模仿学习<br />
**Authors:** Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta<br />
**Abstract:** <details><summary>原文: </summary>A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过模仿训练的策略的常见失败模式是在测试时复合执行错误。当学习到的策略遇到专家演示中未出现的状态时，该策略就会失败，从而导致退化行为。解决此问题的数据集聚合或 DAgger 方法只是收集更多数据来覆盖这些故障状态。然而，在实践中，这通常非常昂贵。在这项工作中，我们提出了 Diffusion Meets DAgger (DMD)，这是一种获得 DAgger 优势的方法，而无需付出手眼模仿学习问题的代价。 DMD 不是收集新样本来覆盖分布外状态，而是利用扩散模型的最新进展来通过扩散模型创建这些样本。这使得通过很少的演示就能获得强大的性能。在 Franka Research 3 上进行的非抓握式推动实验中，我们表明，DMD 只需 8 名专家演示即可实现 80% 的成功率，而幼稚行为克隆仅达到 20%。 DMD 的性能也比基于 NeRF 的竞争增强方案高出 50%。</details>
**PDF:** <http://arxiv.org/pdf/2402.17768v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Towards Fairness-Aware Adversarial Learning**<br />
**Title_cn:** 迈向公平意识的对抗性学习<br />
**Authors:** Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan<br />
**Abstract:** <details><summary>原文: </summary>Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管对抗性训练（AT）已被证明可以有效增强模型的鲁棒性，但最近暴露的鲁棒性公平性问题尚未得到很好的解决，即不同类别之间的鲁棒准确性存在显着差异。在本文中，我们没有统一评估模型的平均类别表现，而是通过考虑各个类别的最坏情况分布来深入研究稳健公平性问题。我们提出了一种新颖的学习范式，称为公平感知对抗学习（FAAL）。作为传统 AT 的推广，我们将对抗训练问题重新定义为最小-最大-最大框架，以确保训练模型的鲁棒性和公平性。具体来说，通过利用分布鲁棒优化，我们的方法旨在找到不同类别之间的最差分布，并且保证解决方案以高概率获得上限性能。特别是，FAAL 可以在仅两个 epoch 内将不公平的鲁棒模型微调为公平，而不会影响整体的干净和鲁棒精度。对各种图像数据集的广泛实验验证了所提出的 FAAL 与其他最先进的方法相比的优越性能和效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.17729v1><br />
**Code:** <https://github.com/TrustAI/FAAL>**<br />
>>**index:** 3<br />
**Title:** **Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners**<br />
**Title_cn:** 视觉和听觉：具有扩散潜在对准器的开放域视音频生成<br />
**Authors:** Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen<br />
**Abstract:** <details><summary>原文: </summary>Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/</details>
**Abstract_cn:** <details><summary>译文: </summary>视音频内容创作是电影行业和专业用户的核心技术。最近，现有的基于扩散的方法分别处理视频和音频的生成，这阻碍了从学术界到工业界的技术转移。在这项工作中，我们的目标是通过精心设计的基于优化的跨视音频和联合视音频生成框架来填补空白。我们观察到现成的视频或音频生成模型的强大生成能力。因此，我们建议将现有的强模型与共享的潜在表示空间连接起来，而不是从头开始训练巨型模型。具体来说，我们提出了一种带有预训练 ImageBind 模型的多模态潜在对准器。我们的潜在对齐器与分类器指导共享相似的核心，分类器指导在推理时间内指导扩散去噪过程。通过精心设计的优化策略和损失函数，我们展示了我们的方法在联合视频音频生成、视觉引导音频生成和音频引导视觉生成任务上的卓越性能。项目网站可以在https://yzxing87.github.io/Seeing-and-Hearing/找到</details>
**PDF:** <http://arxiv.org/pdf/2402.17723v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Bayesian Differentiable Physics for Cloth Digitalization**<br />
**Title_cn:** 用于布料数字化的贝叶斯微分物理<br />
**Authors:** Deshan Gong, Ningtao Mao, He Wang<br />
**Abstract:** <details><summary>原文: </summary>We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种布料数字化的新方法。与从相对随意的设置下捕获的数据中学习的现有方法不同，我们建议从严格测试的测量协议中捕获的数据中学习，并找到衣服的合理物理参数。然而，目前缺乏此类数据，因此我们首先提出一个具有精确布料测量的新数据集。此外，由于数据捕获过程的性质，数据大小比当前深度学习中的数据小得多。为了从小数据中学习，我们提出了一种新的贝叶斯可微布料模型来估计真实布料的复杂材料异质性。它可以从非常有限的数据样本中提供高度准确的数字化。通过详尽的评估和比较，我们表明我们的方法在布料数字化方面是准确的，从有限的数据样本中学习是有效的，并且在捕捉材料变化方面具有普遍性。代码和数据可用 https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization</details>
**PDF:** <http://arxiv.org/pdf/2402.17664v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing**<br />
**Title_cn:** CustomSketching：基于草图的图像合成和编辑的草图概念提取<br />
**Authors:** Chufeng Xiao, Hongbo Fu<br />
**Abstract:** <details><summary>原文: </summary>Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型文本到图像 (T2I) 模型的个性化技术允许用户整合参考图像中的新概念。然而，现有方法主要依赖于文本描述，导致对定制图像的控制有限，并且无法支持细粒度和本地编辑（例如形状、姿势和细节）。在本文中，我们将草图视为一种直观且通用的表示形式，可以促进这种控制，例如捕获形状信息的轮廓线和表示纹理的流线。这促使我们探索草图概念提取的一项新任务：给定一个或多个草图图像对，我们的目标是提取一个特殊的草图概念，该概念连接图像和草图之间的对应关系，从而实现基于草图的图像合成和编辑细粒度的级别。为了实现这一目标，我们引入了 CustomSketching，这是一个用于提取新颖草图概念的两阶段框架。考虑到一个对象通常可以通过一般形状的轮廓和内部细节的附加笔画来描绘，我们引入了双草图表示来减少草图描绘中固有的模糊性。我们采用形状损失和正则化损失来平衡优化过程中的保真度和可编辑性。通过广泛的实验、用户研究和多次应用，我们证明我们的方法是有效的并且优于适应的基线。</details>
**PDF:** <http://arxiv.org/pdf/2402.17624v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web**<br />
**Title_cn:** OmniACT：为桌面和 Web 启用多模式通才自治代理的数据集和基准<br />
**Authors:** Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov<br />
**Abstract:** <details><summary>原文: </summary>For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such as "Send an email to John Doe mentioning the time and place to meet". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.</details>
**Abstract_cn:** <details><summary>译文: </summary>几十年来，人机交互基本上都是手动的。即使在今天，几乎所有在计算机上完成的生产性工作都需要人工在每一步进行输入。自主虚拟代理代表了许多这些琐碎任务自动化的令人兴奋的一步。虚拟代理将使技术能力有限的用户能够充分利用计算机系统的可能性。它们还可以有效地简化大量计算机任务，从日历管理到复杂的旅行预订，而无需人工干预。在本文中，我们介绍了 OmniACT，这是第一个用于评估代理生成可执行程序以完成计算机任务的能力的数据集和基准。我们的范围超出了传统的网络自动化，涵盖了各种桌面应用程序。该数据集包含“播放下一首歌曲”等基本任务，以及“向 John Doe 发送电子邮件，提及见面的时间和地点”等长期任务。具体来说，给定一对屏幕图像和一个基于视觉的自然语言任务，目标是生成能够完全执行该任务的脚本。我们在基准测试中运行了几个强大的基线语言模型代理。最强的基线 GPT-4 在我们的基准测试中表现最好，但是，在生成能够完成任务的可执行脚本方面，其性能水平仍然仅达到人类熟练程度的 15%，这表明我们的任务对传统 Web 代理的挑战。我们的基准测试提供了一个平台来衡量和评估语言模型代理在自动化计算机任务方面的进展，并激励未来的工作构建连接大型语言模型和计算机屏幕视觉基础的多模态模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.17553v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Adapting Learned Image Codecs to Screen Content via Adjustable Transformations**<br />
**Title_cn:** 通过可调整的转换使学习的图像编解码器适应屏幕内容<br />
**Authors:** H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach<br />
**Abstract:** <details><summary>原文: </summary>As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着学习图像编解码器（LIC）变得越来越普遍，它们对分布外数据的低编码效率成为某些应用程序的瓶颈。为了在不破坏向后兼容性的情况下提高屏幕内容（SC）图像的LIC性能，我们建议在编码管道中引入参数化和可逆线性变换，而不改变底层基线编解码器的操作流程。我们设计了两个神经网络在我们的设置中充当预过滤器和后过滤器，以提高编码效率并帮助从编码伪影中恢复。与基线 LIC 相比，我们的端到端训练解决方案可在 SC 压缩上节省高达 10% 的比特率，同时仅引入 1% 的额外参数。</details>
**PDF:** <http://arxiv.org/pdf/2402.17544v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control**<br />
**Title_cn:** 具有概率扩展控制的多模态学习稀疏检索<br />
**Authors:** Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke<br />
**Abstract:** <details><summary>原文: </summary>Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in multimodal settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal</details>
**Abstract_cn:** <details><summary>译文: </summary>学习稀疏检索 (LSR) 是一系列神经方法，它将查询和文档编码为稀疏词汇向量，可以通过倒排索引高效地进行索引和检索。我们探索 LSR 在多模态领域的应用，重点是文本图像检索。虽然 LSR 在文本检索方面取得了成功，但其在多模态检索中的应用仍未得到充分探索。目前的方法（例如 LexLIP 和 STAIR）需要对大量数据集进行复杂的多步骤训练。我们提出的方法有效地将密集向量从冻结密集模型转换为稀疏词汇向量。我们通过新的训练算法解决高维协同激活和语义偏差的问题，使用伯努利随机变量来控制查询扩展。两个密集模型（BLIP，ALBEF）和两个数据集（MSCOCO，Flickr30k）的实验表明，我们提出的算法有效地减少了共同激活和语义偏差。我们性能最佳的稀疏模型优于最先进的文本图像 LSR 模型，训练时间更短，GPU 内存要求更低。我们的方法为在多模态环境中训练 LSR 检索模型提供了有效的解决方案。我们的代码和模型检查点位于 github.com/thongnt99/lsr-multimodal</details>
**PDF:** <http://arxiv.org/pdf/2402.17535v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Black-box Adversarial Attacks Against Image Quality Assessment Models**<br />
**Title_cn:** 针对图像质量评估模型的黑盒对抗攻击<br />
**Authors:** Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang<br />
**Abstract:** <details><summary>原文: </summary>The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation. To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement. This paper makes the first attempt to explore the black-box adversarial attacks on NR-IQA models. Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation. Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of adversarial examples towards an opposite direction with maximum deviation. On this basis, we finally develop an efficient and effective black-box attack method against NR-IQA models. Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method. And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models.</details>
**Abstract_cn:** <details><summary>译文: </summary>无参考图像质量评估 (NR-IQA) 的目标是根据其主观评价来预测图像的感知质量。为了将 NR-IQA 模型付诸实践，有必要研究其模型细化的潜在漏洞。本文首次尝试探索 NR-IQA 模型的黑盒对抗攻击。具体来说，我们首先将攻击问题表述为最大化原始图像和扰动图像的估计质量得分之间的偏差，同时限制扰动图像失真以保持视觉质量。在这样的公式下，我们然后设计了一个双向损失函数，以将对抗性示例的估计质量分数误导到具有最大偏差的相反方向。在此基础上，我们最终开发出一种高效且有效的针对NR-IQA模型的黑盒攻击方法。大量实验表明，所有评估的 NR-IQA 模型都容易受到所提出的攻击方法的影响。而且生成的扰动是不可转移的，使它们能够服务于不同 IQA 模型的专业研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.17533v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis**<br />
**Title_cn:** AlignMiF：用于 LiDAR-相机联合合成的几何对齐多模态隐式场<br />
**Authors:** Tao Tang, Guangrun Wang, Yixing Lao, Peng Chen, Jie Liu, Liang Lin, Kaicheng Yu, Xiaodan Liang<br />
**Abstract:** <details><summary>原文: </summary>Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI). These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).</details>
**Abstract_cn:** <details><summary>译文: </summary>神经隐式场已经成为新颖视图合成的事实​​上的标准。最近，存在一些探索在单个字段中融合多种模态的方法，旨在共享来自不同模态的隐式特征以提高重建性能。然而，这些模式经常表现出不一致的行为：优化一种模式（例如激光雷达）可能会对另一种模式（例如相机性能）产生不利影响，反之亦然。在这项工作中，我们对激光雷达-相机联合合成的多模态隐场进行了全面分析，揭示了根本问题在于不同传感器的未对准。此外，我们引入了 AlignMiF，一种几何对齐的多模态隐式场，具有两个建议的模块：几何感知对齐（GAA）和共享几何初始化（SGI）。这些模块有效地对齐不同模态的粗略几何形状，显着增强激光雷达和相机数据之间的融合过程。通过对各种数据集和场景的广泛实验，我们证明了我们的方法在促进统一神经场内激光雷达和相机模式之间更好交互的有效性。具体来说，我们提出的 AlignMiF 比最近的隐式融合方法取得了显着的改进（KITTI-360 和 Waymo 数据集上的图像 PSNR +2.01 和 +3.11），并且始终超越单一模态性能（LiDAR 倒角距离在各自的数据集）。</details>
**PDF:** <http://arxiv.org/pdf/2402.17483v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization**<br />
**Title_cn:** JPEG-AI标准化中空间质量图的比特分布研究与实现<br />
**Authors:** Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup<br />
**Abstract:** <details><summary>原文: </summary>Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes. As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality. Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y.</details>
**Abstract_cn:** <details><summary>译文: </summary>目前，对基于神经网络的图像压缩编解码器的需求很高。与经典框架中使用的手工变换相比，这些编解码器采用非线性变换来创建紧凑的位表示，并促进设备上更快的编码速度。科学界和工业界对这些特性非常感兴趣，从而促成了 JPEG-AI 的标准化工作。 JPEG-AI验证模型已发布，目前正在标准化开发中。利用神经网络，它在基本操作点上的 BD 速率比经典编解码器 VVC intra 的性能高出 10% 以上。研究人员将这一成功归因于空间域中灵活的比特分布，这与使用恒定质量点生成的 VVC 内部锚点形成鲜明对比。然而，我们的研究表明，VVC 内部通过实现各种块大小显示出更具适应性的位分布结构。根据我们的观察，我们提出了一种空间比特分配方法来优化 JPEG-AI 验证模型的比特分布并提高视觉质量。此外，通过应用VVC比特分配策略，可以进一步提高JPEG-AI验证模式的客观性能，使得PSNR-Y最大增益达到0.45 dB。</details>
**PDF:** <http://arxiv.org/pdf/2402.17470v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal Correspondence**<br />
**Title_cn:** V2C-Long：具有时空对应性的纵向皮层重建<br />
**Authors:** Fabian Bongratz, Jan Fecht, Anne-Marie Rickmann, Christian Wachinger<br />
**Abstract:** <details><summary>原文: </summary>Reconstructing the cortex from longitudinal MRI is indispensable for analyzing morphological changes in the human brain. Despite the recent disruption of cortical surface reconstruction with deep learning, challenges arising from longitudinal data are still persistent. Especially the lack of strong spatiotemporal point correspondence hinders downstream analyses due to the introduced noise. To address this issue, we present V2C-Long, the first dedicated deep learning-based cortex reconstruction method for longitudinal MRI. In contrast to existing methods, V2C-Long surfaces are directly comparable in a cross-sectional and longitudinal manner. We establish strong inherent spatiotemporal correspondences via a novel composition of two deep mesh deformation networks and fast aggregation of feature-enhanced within-subject templates. The results on internal and external test data demonstrate that V2C-Long yields cortical surfaces with improved accuracy and consistency compared to previous methods. Finally, this improvement manifests in higher sensitivity to regional cortical atrophy in Alzheimer's disease.</details>
**Abstract_cn:** <details><summary>译文: </summary>从纵向 MRI 重建皮质对于分析人脑的形态变化是必不可少的。尽管深度学习最近破坏了皮质表面重建，但纵向数据带来的挑战仍然持续存在。特别是缺乏强时空点对应性，由于引入的噪声而阻碍了下游分析。为了解决这个问题，我们提出了 V2C-Long，这是第一个专门用于纵向 MRI 的基于深度学习的皮层重建方法。与现有方法相比，V2C-Long 表面可以在横截面和纵向上直接进行比较。我们通过两个深度网格变形网络的新颖组合和特征增强的主题内模板的快速聚合来建立强大的固有时空对应关系。内部和外部测试数据的结果表明，与以前的方法相比，V2C-Long 产生的皮质表面具有更高的准确性和一致性。最后，这种改善体现在对阿尔茨海默氏病局部皮质萎缩的敏感性更高。</details>
**PDF:** <http://arxiv.org/pdf/2402.17438v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis**<br />
**Title_cn:** 用于噪声传播分析的傅里叶域插值神经网络的新颖图像空间形式<br />
**Authors:** Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer<br />
**Abstract:** <details><summary>原文: </summary>Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for validation. The framework was tested on retrospectively undersampled invivo brain images. Results: Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics. Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo simulations, as well as via an auto-differentiation approach. The noise resilience is well characterized, as in the case of classical Parallel Imaging. Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo simulations. Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during CNN inference, analogous to geometry-factor maps in traditional parallel imaging methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：开发多层卷积神经网络 (CNN) 的图像空间形式，用于 MRI 重建中的傅立叶域插值，并分析估计 CNN 推理过程中的噪声传播。理论和方法：使用复值整流器线性单元的傅立叶域（也称为 k 空间）中的非线性激活表示为与激活掩码的元素相乘。该操作被转化为图像空间中的卷积。在 k 空间中进行网络训练后，该方法为重建图像相对于混叠线圈图像的导数提供了代数表达式，混叠线圈图像充当图像空间中网络的输入张量。这允许通过分析方式估计网络推理中的方差并用于描述噪声特征。使用蒙特卡罗模拟和基于自微分的数值方法进行验证。该框架在回顾性欠采样体内大脑图像上进行了测试。结果：在图像域中进行的推理与 k 空间中的推理几乎相同，并通过相应的定量指标进行了强调。从解析表达式获得的噪声方差图与通过蒙特卡罗模拟以及通过自动微分方法获得的噪声方差图相对应。与经典并行成像的情况一样，噪声恢复能力得到了很好的表征。 Komolgorov-Smirnov 检验证明了通过蒙特卡罗模拟获得的方差图中体素大小的高斯分布。结论：用于 k 空间插值的神经网络的准等效图像空间形式能够在 CNN 推理过程中快速准确地描述噪声特征，类似于传统并行成像方法中的几何因子图。</details>
**PDF:** <http://arxiv.org/pdf/2402.17410v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Accelerating Diffusion Sampling with Optimized Time Steps**<br />
**Title_cn:** 通过优化时间步长加速扩散采样<br />
**Authors:** Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li<br />
**Abstract:** <details><summary>原文: </summary>Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散概率模型（DPM）在高分辨率图像合成中表现出了卓越的性能，但由于通常需要大量采样步骤，因此其采样效率仍然不够理想。 DPM 高阶数值 ODE 求解器的最新进展使得能够以更少的采样步骤生成高质量图像。虽然这是一个重大的进步，但大多数采样方法仍然采用统一的时间步长，这在使用少量步长时并不是最佳的。为了解决这个问题，我们提出了一个设计优化问题的通用框架，为 DPM 的特定数值 ODE 求解器寻求更合适的时间步长。该优化问题旨在最小化 ODE 的真实解与数值求解器对应的近似解之间的距离。可以使用约束信任域方法有效地解决该问题，耗时不到 15 秒。我们使用像素和潜在空间 DPM 对无条件和条件采样进行的广泛实验表明，当与最先进的采样方法 UniPC 相结合时，我们优化的时间步显着提高了 FID 分数方面的图像生成性能。与使用统一时间步长的数据集（例如 CIFAR-10 和 ImageNet）进行比较。</details>
**PDF:** <http://arxiv.org/pdf/2402.17376v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents**<br />
**Title_cn:** SocialCVAE：通过交互条件潜伏预测行人轨迹<br />
**Authors:** Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin<br />
**Abstract:** <details><summary>原文: </summary>Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).</details>
**Abstract_cn:** <details><summary>译文: </summary>行人轨迹预测是许多应用中的关键技术，可提供对人类行为的洞察并预测人类未来的运动。大多数现有的经验模型都是通过使用具有确定性性质的可解释的数学术语通过观察到的人类行为明确制定的，而最近的工作重点是开发与基于学习的技术相结合的混合模型，以实现强大的表达能力，同时保持可解释性。然而，从经验模型中学习到的转向行为的确定性限制了模型的实际性能。为了解决这个问题，这项工作提出了用于预测行人轨迹的社会条件变分自动编码器（SocialCVAE），它采用 CVAE 来探索人类运动决策中的行为不确定性。 SocialCVAE 通过利用社会可解释的交互能量图作为 CVAE 的条件来学习社会合理的运动随机性，该条件说明了每个行人当地邻里区域的未来占用情况。能量图是使用基于能量的交互模型生成的，该模型预测行人与邻居交互的能量成本（即排斥强度）。包括 25 个场景的两个公共基准测试的实验结果表明，与最先进的方法相比，SocialCVAE 显着提高了预测精度，平均位移误差 (ADE) 提高了 16.85%，最终位移误差提高了 69.18%（ FDE）。</details>
**PDF:** <http://arxiv.org/pdf/2402.17339v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Method of Tracking and Analysis of Fluorescent-Labeled Cells Using Automatic Thresholding and Labeling**<br />
**Title_cn:** 使用自动阈值和标记跟踪和分析荧光标记细胞的方法<br />
**Authors:** Mizuki Fukasawa, Tomokazu Fukuda, Takuya Akashi<br />
**Abstract:** <details><summary>原文: </summary>High-throughput screening using cell images is an efficient method for screening new candidates for pharmaceutical drugs. To complete the screening process, it is essential to have an efficient process for analyzing cell images. This paper presents a new method for efficiently tracking cells and quantitatively detecting the signal ratio between cytoplasm and nuclei. Existing methods include those that use image processing techniques and those that utilize artificial intelligence (AI). However, these methods do not consider the correspondence of cells between images, or require a significant amount of new learning data to train AI. Therefore, our method uses automatic thresholding and labeling algorithms to compare the position of each cell between images, and continuously measure and analyze the signal ratio of cells. This paper describes the algorithm of our method. Using the method, we experimented to investigate the effect of the number of opening and closing operations during the binarization process on the tracking of the cells. Through the experiment, we determined the appropriate number of opening and closing processes.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用细胞图像进行高通量筛选是筛选新候选药物的有效方法。为了完成筛选过程，必须有一个有效的细胞图像分析流程。本文提出了一种有效跟踪细胞并定量检测细胞质与细胞核之间信号比的新方法。现有的方法包括使用图像处理技术的方法和利用人工智能（AI）的方法。然而，这些方法没有考虑图像之间细胞的对应关系，或者需要大量新的学习数据来训练人工智能。因此，我们的方法使用自动阈值和标记算法来比较图像之间每个细胞的位置，并连续测量和分析细胞的信号比。本文描述了我们方法的算法。使用该方法，我们实验研究了二值化过程中打开和关闭操作的次数对细胞跟踪的影响。通过实验，我们确定了合适的打开和关闭进程的数量。</details>
**PDF:** <http://arxiv.org/pdf/2402.17310v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Learning Exposure Correction in Dynamic Scenes**<br />
**Title_cn:** 学习动态场景中的曝光校正<br />
**Authors:** Jin Liu, Bo Wang, Chuanming Wang, Huiyuan Fu, Huadong Ma<br />
**Abstract:** <details><summary>原文: </summary>Capturing videos with wrong exposure usually produces unsatisfactory visual effects. While image exposure correction is a popular topic, the video counterpart is less explored in the literature. Directly applying prior image-based methods to input videos often results in temporal incoherence with low visual quality. Existing research in this area is also limited by the lack of high-quality benchmark datasets. To address these issues, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. In addition, we propose a Video Exposure Correction Network (VECNet) based on Retinex theory, which incorporates a two-stream illumination learning mechanism to enhance the overexposure and underexposure factors, respectively. The estimated multi-frame reflectance and dual-path illumination components are fused at both feature and image levels, leading to visually appealing results. Experimental results demonstrate that the proposed method outperforms existing image exposure correction and underexposed video enhancement methods. The code and dataset will be available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>以错误的曝光拍摄视频通常会产生令人不满意的视觉效果。虽然图像曝光校正是一个热门话题，但文献中对视频曝光校正的探讨较少。直接将现有的基于图像的方法应用于输入视频通常会导致时间不连贯且视觉质量低下。该领域的现有研究也因缺乏高质量的基准数据集而受到限制。为了解决这些问题，我们构建了第一个真实世界的配对视频数据集，包括曝光不足和曝光过度的动态场景。为了实现空间对齐，我们利用两个数码单反相机和一个分束器同时捕获不正确和正常曝光的视频。此外，我们提出了一种基于Retinex理论的视频曝光校正网络（VECNet），它结合了双流照明学习机制来分别增强过度曝光和曝光不足的因素。估计的多帧反射率和双路照明分量在特征和图像级别上融合，从而产生视觉上吸引人的结果。实验结果表明，该方法优于现有的图像曝光校正和曝光不足的视频增强方法。代码和数据集即将推出。</details>
**PDF:** <http://arxiv.org/pdf/2402.17296v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **An Interpretable Evaluation of Entropy-based Novelty of Generative Models**<br />
**Title_cn:** 基于熵的生成模型新颖性的可解释评估<br />
**Authors:** Jingwei Zhang, Cheuk Ting Li, Farzan Farnia<br />
**Abstract:** <details><summary>原文: </summary>The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$ with respect to distribution $P_\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components. Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples. We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型框架和架构的大规模发展需要有原则的方法来评估模型与参考数据集或基线生成模型相比的新颖性。虽然最近的文献广泛研究了生成模型的质量、多样性和泛化性的评估，但机器学习社区尚未充分研究模型相对于基线模型的新颖性评估。在这项工作中，我们关注多模态生成模型下的新颖性评估，并尝试回答以下问题：给定生成模型 $\mathcal{G}$ 的样本和参考数据集 $\mathcal{S}$，我们如何才能比 $\mathcal{S}$ 更频繁地发现和计算 $\mathcal{G}$ 表达的模式。我们为所描述的任务引入了谱方法，并提出了基于内核的熵新颖性（KEN）分数来量化分布 $P_\mathcal{G}$ 相对于分布 $P_\mathcal{S}$ 的基于模式的新颖性。我们分析解释了具有亚高斯分量的混合分布下 KEN 得分的行为。接下来，我们开发了一种基于 Cholesky 分解的方法来计算观察样本的 KEN 分数。我们通过展示合成和真实图像分布的几个数值结果来支持基于 KEN 的新颖性量化。我们的数值结果表明所提出的方法在检测新颖模式和比较最先进的生成模型方面取得了成功。</details>
**PDF:** <http://arxiv.org/pdf/2402.17287v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **One-Shot Structure-Aware Stylized Image Synthesis**<br />
**Title_cn:** 一次性结构感知风格化图像合成<br />
**Authors:** Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong<br />
**Abstract:** <details><summary>原文: </summary>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然基于 GAN 的模型在图像风格化任务中取得了成功，但它们在对各种输入图像进行风格化时常常难以保持结构。最近，扩散模型已被用于图像风格化，但仍然缺乏保持输入图像的原始质量的能力。在此基础上，我们提出了 OSASIS：一种新颖的一次性风格化方法，在结构保存方面具有鲁棒性。我们证明 OSASIS 能够有效地将语义与图像结构分开，从而使其能够控制给定输入所实现的内容和风格的级别。我们将 OSASIS 应用于各种实验设置，包括域外参考图像的风格化和文本驱动操作的风格化。结果表明，OSASIS 优于其他风格化方法，尤其是对于训练期间很少遇到的输入图像，为通过扩散模型进行风格化提供了一种有前景的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.17275v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation**<br />
**Title_cn:** Playground v2.5：增强文本到图像生成美学质量的三个见解<br />
**Authors:** Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi<br />
**Abstract:** <details><summary>原文: </summary>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们分享了在文本到图像生成模型中实现最先进的美学质量的三个见解。我们专注于模型改进的三个关键方面：增强颜色和对比度、改进多个纵横比的生成以及改进以人为中心的精细细节。首先，我们深入研究噪声表在训练扩散模型中的重要性，展示其对真实性和视觉保真度的深远影响。其次，我们解决了在图像生成中适应各种纵横比的挑战，强调准备平衡的分桶数据集的重要性。最后，我们研究了将模型输出与人类偏好保持一致的关键作用，确保生成的图像与人类的感知期望产生共鸣。通过广泛的分析和实验，Playground v2.5 在各种条件和宽高比下展示了最先进的美学质量性能，优于 SDXL 和 Playground v2 等广泛使用的开源模型以及闭源模型商业系统，例如 DALLE 3 和 Midjourney v5.2。我们的模型是开源的，我们希望 Playground v2.5 的开发为旨在提高基于扩散的图像生成模型的美学质量的研究人员提供有价值的指导。</details>
**PDF:** <http://arxiv.org/pdf/2402.17245v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **VCD: Knowledge Base Guided Visual Commonsense Discovery in Images**<br />
**Title_cn:** VCD：知识库引导图像中的视觉常识发现<br />
**Authors:** Xiangqing Shen, Yurun Song, Siwei Wu, Rui Xia<br />
**Abstract:** <details><summary>原文: </summary>Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore propose a generative model (VCDM) that integrates a vision-language model with instruction tuning to tackle VCD. Automatic and human evaluations demonstrate VCDM's proficiency in VCD, particularly outperforming GPT-4V in implicit commonsense discovery. The value of VCD is further demonstrated by its application to two downstream tasks, including visual commonsense evaluation and visual question answering. The data and code will be made available on GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉常识包含有关视觉数据中的对象属性、关系和行为的知识。发现视觉常识可以提供对图像更全面、更丰富的理解，增强计算机视觉系统的推理和决策能力。然而，现有视觉常识发现研究中定义的视觉常识是粗粒度且不完整的。在这项工作中，我们从自然语言处理中的常识知识库ConceptNet中汲取灵感，系统地定义了视觉常识的类型。基于此，我们引入了一个新任务，视觉常识发现（VCD），旨在提取图像中不同对象中包含的不同类型的细粒度常识。因此，我们从 Visual Genome 和 ConceptNet for VCD 构建了一个数据集 (VCDD)，其中包含超过 100,000 张图像和 1400 万个对象常识对。我们还提出了一种生成模型（VCDM），它将视觉语言模型与指令调整相结合来处理 VCD。自动和人工评估证明了 VCDM 在 VCD 方面的熟练程度，特别是在隐式常识发现方面优于 GPT-4V。 VCD的价值通过其在视觉常识评估和视觉问答这两个下游任务中的应用得到了进一步证明。数据和代码将在 GitHub 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.17213v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Purified and Unified Steganographic Network**<br />
**Title_cn:** 纯净统一的隐写网络<br />
**Authors:** Guobiao Li, Sheng Li, Zicong Luo, Zhenxing Qian, Xinpeng Zhang<br />
**Abstract:** <details><summary>原文: </summary>Steganography is the art of hiding secret data into the cover media for covert communication. In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising. Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size. It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication. To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet). It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys. We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks. We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery. Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture. It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network. Code is available at \url{https://github.com/albblgb/PUSNet}</details>
**Abstract_cn:** <details><summary>译文: </summary>隐写术是将秘密数据隐藏到封面媒体中以进行秘密通信的艺术。近年来，越来越多的基于深度神经网络（DNN）的隐写方案被提出来训练隐写网络进行秘密嵌入和恢复，这被证明是有前途的。与手工制作的隐写工具相比，隐写网络的规模往往很大。这就引发了人们对如何在不知不觉中有效地将这些网络传输给发送者和接收者以促进秘密通信的担忧。为了解决这个问题，我们在本文中提出了一种纯化统一的隐写网络（PUSNet）。它在纯化的网络中执行普通的机器学习任务，该任务可以被触发到隐写网络中，以使用不同的密钥进行秘密嵌入或恢复。我们将 PUSNet 的构建公式化为稀疏权重填充问题，以便在纯化网络和隐写网络之间灵活切换。我们进一步将 PUSNet 实例化为图像去噪网络，其中隐藏了两个隐写网络，用于秘密图像嵌入和恢复。综合实验表明，我们的 PUSNet 在单一架构中的秘密图像嵌入、秘密图像恢复和图像去噪方面取得了良好的性能。它还被证明能够在纯化网络中不知不觉地携带隐写网络。代码可在 \url{https://github.com/albblgb/PUSNet} 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.17210v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain**<br />
**Title_cn:** 通过减轻压缩域的增强偏差来增强压缩图像的质量<br />
**Authors:** Qunliang Xing, Mai Xu, Shengxi Li, Xin Deng, Meisong Zheng, Huaida Liu, Ying Chen<br />
**Abstract:** <details><summary>原文: </summary>Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images. However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain. This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality. In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images. Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain. Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain. Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的压缩图像质量增强方法侧重于将增强域与原始域对齐以产生逼真的图像。然而，这些方法表现出对压缩域的普遍增强偏见，无意中认为它比原始域更现实。这种偏差使得增强图像与压缩图像非常相似，从而降低了它们的感知质量。在本文中，我们提出了一种简单而有效的方法来减轻这种偏差并提高压缩图像的质量。我们的方法采用以压缩图像为关键条件的条件鉴别器，然后结合域散度正则化来主动将增强域与压缩域分开。通过这种双重策略，我们的方法能够区分压缩域，并使增强域更接近原始域。全面的质量评估证实了我们的方法相对于其他最先进的方法的优越性，并且不会产生推理开销。</details>
**PDF:** <http://arxiv.org/pdf/2402.17200v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models**<br />
**Title_cn:** Sora：大视觉模型的背景、技术、局限性和机遇回顾<br />
**Authors:** Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>Sora 是一种文本到视频生成 AI 模型，由 OpenAI 于 2024 年 2 月发布。该模型经过训练，可以根据文本指令生成现实或想象场景的视频，并显示出模拟物理世界的潜力。本文基于公开技术报告和逆向工程，对文本转视频人工智能模型的模型背景、相关技术、应用、剩余挑战和未来方向进行了全面回顾。我们首先追踪 Sora 的开发并研究用于构建这个“世界模拟器”的底层技术。然后，我们详细描述了Sora在电影制作、教育、营销等多个行业中的应用和潜在影响。我们讨论了广泛部署 Sora 需要解决的主要挑战和限制，例如确保安全和公正的视频生成。最后，我们讨论了 Sora 和视频生成模型的未来发展，以及该领域的进步如何实现人机交互的新方式，从而提高视频生成的生产力和创造力。</details>
**PDF:** <http://arxiv.org/pdf/2402.17177v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces**<br />
**Title_cn:** Deep Umbra：城市空间中阳光照射计算的生成方法<br />
**Authors:** Kazi Shahrukh Omar, Gustavo Moreira, Daniel Hodczak, Maryam Hosseini, Nicola Colaninno, Marcos Lage, Fabio Miranda<br />
**Abstract:** <details><summary>原文: </summary>Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional generative adversarial network that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world. Deep Umbra is available at https://urbantk.org/shadows.</details>
**Abstract_cn:** <details><summary>译文: </summary>阳光和阴影在城市空间的利用、繁荣和发展中发挥着至关重要的作用。虽然获得阳光对于城市环境的成功至关重要，但阴影可以在炎热季节提供遮荫的地方，减轻热岛效应并提高行人舒适度。正确量化大型城市环境中的阳光照射和阴影是解决当今城市面临的一些重要挑战的关键。在本文中，我们提出了 Deep Umbra，这是一种新颖的计算框架，可以在全球范围内量化阳光的进入和阴影。我们的框架基于条件生成对抗网络，该网络考虑城市的物理形态来计算一年中不同季节累积阳光照射的高分辨率空间信息。我们使用来自七个不同城市的数据来训练我们的模型，并通过大量实验展示其较低的总体 RMSE（低于 0.1）以及其对不属于训练集的城市的可扩展性。此外，我们还提供了一组案例研究和一个综合数据集，其中包含世界六大洲 100 多个城市的阳光获取信息。 Deep Umbra 可以在 https://urbantk.org/shadows 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.17169v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Video as the New Language for Real-World Decision Making**<br />
**Title_cn:** 视频作为现实世界决策的新语言<br />
**Authors:** Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans<br />
**Abstract:** <details><summary>原文: </summary>Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>互联网上的文本和视频数据都很丰富，并且通过下一个标记或帧预测支持大规模自监督学习。然而，它们并没有得到同等的利用：语言模型对现实世界产生了重大影响，而视频生成仍然主要局限于媒体娱乐。然而，视频数据捕获了难以用语言表达的有关物理世界的重要信息。为了解决这一差距，我们讨论了一个未被充分重视的机会，即扩展视频生成以解决现实世界中的任务。我们观察视频如何像语言一样作为一个统一的界面来吸收互联网知识并代表不同的任务。此外，我们还演示了如何像语言模型一样，通过上下文学习、规划和强化学习等技术，视频生成可以充当规划器、代理、计算引擎和环境模拟器。我们确定了机器人、自动驾驶和科学等领域的重大影响机会，最近的工作证明了视频生成中的这种先进功能是如何触手可及的。最后，我们确定了视频生成中阻碍进展的关键挑战。解决这些挑战将使视频生成模型能够与语言模型一起在更广泛的人工智能应用中展示独特的价值。</details>
**PDF:** <http://arxiv.org/pdf/2402.17139v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Transparent Image Layer Diffusion using Latent Transparency**<br />
**Title_cn:** 使用潜在透明度的透明图像层扩散<br />
**Authors:** Lvmin Zhang, Maneesh Agrawala<br />
**Abstract:** <details><summary>原文: </summary>We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a "latent transparency" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 LayerDiffusion，一种使大规模预训练潜在扩散模型能够生成透明图像的方法。该方法允许生成单个透明图像或多个透明层。该方法学习“潜在透明度”，将 alpha 通道透明度编码到预训练潜在扩散模型的潜在流形中。它通过将增加的透明度调节为潜在偏移，并对预训练模型的原始潜在分布进行最小程度的更改，从而保留了大型扩散模型的生产就绪质量。这样，任何潜在扩散模型都可以通过使用调整后的潜在空间进行微调来转换为透明图像生成器。我们使用人机循环收集方案收集的 1M 透明图像层对来训练模型。我们表明，潜在透明度可以应用于不同的开源图像生成器，或者适应各种条件控制系统，以实现前景/背景条件层生成、联合层生成、层内容的结构控制等应用。 用户研究研究发现，在大多数情况下 (97%)，用户更喜欢我们原生生成的透明内容，而不是以前的临时解决方案（例如生成然后抠图）。用户还报告说，我们生成的透明图像的质量可与 Adob​​e Stock 等真正的商业透明资源相媲美。</details>
**PDF:** <http://arxiv.org/pdf/2402.17113v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality**<br />
**Title_cn:** T-HITL 有效解决图像生成中的问题关联并保持整体视觉质量<br />
**Authors:** Susan Epstein, Li Chen, Alessandro Vecchiato, Ankit Jain<br />
**Abstract:** <details><summary>原文: </summary>Generative AI image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning. We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality. We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level. Our contributions to scholarship are two-fold. By defining problematic associations in the context of machine learning models and generative AI, we introduce a conceptual and technical taxonomy for addressing some of these associations. Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations. This mitigation need not be a tradeoff, but rather an enhancement.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成式人工智能图像模型可能会无意中生成有问题的人物表示。过去的研究指出，世界各地有数以百万计的用户每天都在使用这些模型，而这些模型，包括通过对人进行有问题的表述，有可能加剧和加速现实世界的歧视和其他危害（Bianchi 等人，2023）。在本文中，我们重点解决人口群体和语义概念之间产生的有问题的关联，这些关联可能反映和强化社会数据中嵌入的负面叙述。基于社会学文献（Blumer，1958）并将表征映射到模型行为，我们开发了一种分类法来研究图像生成模型中存在问题的关联。我们探索了模型级别微调作为解决这些关联的方法的有效性，并将视觉质量的潜在降低确定为传统微调的限制。我们还提出了一种采用两次人机交互 (T-HITL) 的新方法，该方法有望在减少有问题的关联和保持视觉质量方面有所改进。我们通过提供 T-HITL 在模型级别解决的三个有问题的关联的证据来证明 T-HITL 的有效性。我们对学术的贡献有两个方面。通过在机器学习模型和生成人工智能的背景下定义有问题的关联，我们引入了概念和技术分类法来解决其中一些关联。最后，我们提供了一种方法 T-HITL，它可以解决这些关联并同时保持图像模型生成的视觉质量。这种缓解不一定是一种权衡，而是一种增强。</details>
**PDF:** <http://arxiv.org/pdf/2402.17101v1><br />
**Code:** null<br />

