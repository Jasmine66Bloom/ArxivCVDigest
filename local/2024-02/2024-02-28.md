## [UPDATED!] **2024-02-28** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation**<br />
**Title_cn:** MambaMIR：用于联合医学图像重建和不确定性估计的任意屏蔽曼巴<br />
**Authors:** Jiahao Huang, Liutao Yang, Fanwen Wang, Yinzhe Wu, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang<br />
**Abstract:** <details><summary>原文: </summary>The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at https://github.com/ayanglab/MambaMIR.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的 Mamba 模型在视觉表征学习方面表现出了卓越的适应性，包括在医学成像任务中。本研究介绍了 MambaMIR（一种基于 Mamba 的医学图像重建模型）及其基于生成对抗网络的变体 MambaMIR-GAN。我们提出的 MambaMIR 继承了原始 Mamba 模型的几个优点，例如线性复杂性、全局感受野和动态权重。创新的任意掩模机制有效地使 Mamba 适应我们的图像重建任务，为后续基于蒙特卡罗的不确定性估计提供随机性。对各种医学图像重建任务（包括覆盖膝盖、胸部和腹部等解剖区域的快速 MRI 和 SVCT）进行的实验表明，MambaMIR 和 MambaMIR-GAN 实现了与现有技术相当或更好的重建结果。 -艺术方法。此外，估计的不确定性图可以进一步了解重建质量的可靠性。该代码可在 https://github.com/ayanglab/MambaMIR 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18451v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model**<br />
**Title_cn:** 使用注意力引导的去噪扩散异常检测模型进行客观且可解释的乳房美容评估<br />
**Authors:** Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun<br />
**Abstract:** <details><summary>原文: </summary>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着乳腺癌治疗领域的不断进步，术后美容效果的评估由于其对患者生活质量的重大影响而变得越来越重要。然而，由于专家标签固有的主观性，评估乳房美容面临着挑战。在这项研究中，我们提出了一种新颖的自动化方法，即注意力引导去噪扩散异常检测（AG-DDAD），旨在评估手术后的乳房美容情况，解决传统监督学习和现有异常检测模型的局限性。我们的方法利用无标签蒸馏（DINO）自监督视觉变换器（ViT）的注意力机制与扩散模型相结合，实现高质量图像重建和判别区域的精确变换。通过在主要包含正常美容的未标记数据上训练扩散模型，我们采用无监督的异常检测视角来自动对美容进行评分。真实世界的数据实验证明了我们方法的有效性，为美容评估提供了视觉上吸引人的表示和可量化的分数。与常用的基于规则的程序相比，我们的全自动方法消除了手动注释的需要并提供客观的评估。此外，我们的异常检测模型展现了最先进的性能，在准确性上超越了现有模型。我们的研究超越了乳房美容的范围，代表了医学领域无监督异常检测的重大进步，从而为未来的研究铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.18362v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping**<br />
**Title_cn:** LatentSwap：一种高效的人脸交换潜在代码映射框架<br />
**Authors:** Changho Choi, Minho Kim, Junhyeok Lee, Hyoung-Kyu Song, Younggeun Kim, Seungryong Kim<br />
**Abstract:** <details><summary>原文: </summary>We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator. Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward. The loss objective consists of only three terms, and can effectively control the face swap results between source and target images. By attaching a pre-trained GAN inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models. We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks. The source code and models can be found at \url{https://github.com/usingcolor/LatentSwap}.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出 LatentSwap，一个简单的面部交换框架，生成给定生成器的面部交换潜在代码。利用随机采样的潜在代码，我们的框架很轻，除了使用预先训练的模型之外不需要数据集，训练过程也快速而简单。损失目标仅由三项组成，可以有效控制源图像和目标图像之间的人脸交换结果。通过附加独立于模型的预训练 GAN 反转模型并使用 StyleGAN2 生成器，我们的模型可生成与其他竞争性人脸交换模型相当的逼真且高分辨率的图像。我们表明，我们的框架适用于其他生成器，例如 StyleNeRF，为 3D 感知面部交换铺平了道路，并且还与其他下游 StyleGAN2 生成器任务兼容。源代码和模型可以在 \url{https://github.com/usingcolor/LatentSwap} 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.18351v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes**<br />
**Title_cn:** FineDiffusion：扩展扩散模型以生成具有 10,000 个类别的细粒度图像<br />
**Authors:** Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai<br />
**Abstract:** <details><summary>原文: </summary>The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散模型的类条件图像生成以生成高质量和多样化的图像而闻名。然而，大多数先前的工作都集中在生成一般类别的图像，例如 ImageNet-1k 中的 1000 个类别。更具挑战性的任务，大规模细粒度图像生成，仍然是有待探索的边界。在这项工作中，我们提出了一种称为 FineDiffusion 的参数高效策略，用于微调大型预训练扩散模型，以生成具有 10,000 个类别的大规模细粒度图像。 FineDiffusion 仅通过微调分层类嵌入器、偏差项和归一化层参数即可显着加速训练并减少存储开销。为了进一步提高细粒度类别的图像生成质量，我们提出了一种新的细粒度图像生成采样方法，该方法利用专为细粒度类别定制的超类条件指导来取代传统的无分类器指导采样。与完全微调相比，FineDiffusion 实现了 1.56 倍的训练加速，并且仅需要存储总模型参数的 1.77%，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进的 FID。广泛的定性和定量实验证明了我们的方法与其他参数有效的微调方法相比的优越性。代码和更多生成的结果可在我们的项目网站上找到：https://finediffusion.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2402.18331v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Balancing Act: Distribution-Guided Debiasing in Diffusion Models**<br />
**Title_cn:** 平衡法：扩散模型中分布引导的去偏<br />
**Authors:** Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型 (DM) 已成为强大的生成模型，具有前所未有的图像生成能力。这些模型广泛用于数据增强和创意应用。然而，DM 反映了训练数据集中存在的偏差。这在面孔的背景下尤其令人担忧，其中 DM 更喜欢一个人口统计亚组而不是其他亚组（例如女性与男性）。在这项工作中，我们提出了一种在不依赖额外数据或模型重新训练的情况下消除 DM 偏差的方法。具体来说，我们提出了分布指导，它强制生成的图像遵循规定的属性分布。为了实现这一点，我们基于以下关键见解：去噪 UNet 的潜在特征拥有丰富的人口统计语义，并且可以利用相同的特征来指导去偏生成。我们训练属性分布预测器 (ADP) - 一个小型 mlp，将潜在特征映射到属性分布。 ADP 使用现有属性分类器生成的伪标签进行训练。拟议的 ADP 分配指南使我们能够公平生成。我们的方法减少了单个/多个属性的偏差，并且对于无条件和文本条件扩散模型来说，其性能显着优于基线。此外，我们提出了通过使用生成的数据重新平衡训练集来训练公平属性分类器的下游任务。</details>
**PDF:** <http://arxiv.org/pdf/2402.18206v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning**<br />
**Title_cn:** DecisionNCE：通过隐式偏好学习体现多模态表示<br />
**Authors:** Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning. Project Page: https://2toinf.github.io/DecisionNCE/</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态预训练已成为实现自主机器人表征学习三大目标的有效策略：1）提取局部和全局任务进展信息； 2）加强视觉表现的时间一致性； 3）捕获轨迹级语言基础。大多数现有方法通过单独的目标来解决这些问题，但通常会达到次优的解决方案。在本文中，我们提出了一个通用的统一目标，它可以同时从图像序列中提取有意义的任务进展信息，并将其与语言指令无缝对齐。我们发现，通过隐式偏好，视觉轨迹本质上与其相应的语言指令比不匹配的配对更好地对齐，流行的 Bradley-Terry 模型可以通过适当的奖励重新参数化转化为表示学习。由此产生的框架 DecisionNCE 反映了 InfoNCE 风格的目标，但专门针对决策任务量身定制，提供了一个具体的表示学习框架，可以优雅地提取本地和全局任务进展特征，并通过隐式时间对比学习强制执行时间一致性，同时通过多模态联合编码确保轨迹级指令落地。对模拟机器人和真实机器人的评估表明，DecisionNCE 有效地促进了多样化的下游策略学习任务，为统一表示和奖励学习提供了通用的解决方案。项目页面：https://2toinf.github.io/DecisionNCE/</details>
**PDF:** <http://arxiv.org/pdf/2402.18137v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Context-aware Talking Face Video Generation**<br />
**Title_cn:** 上下文感知说话人脸视频生成<br />
**Authors:** Meidai Xuanyuan, Yuwang Wang, Honglei Guo, Qionghai Dai<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we consider a novel and practical case for talking face video generation. Specifically, we focus on the scenarios involving multi-people interactions, where the talking context, such as audience or surroundings, is present. In these situations, the video generation should take the context into consideration in order to generate video content naturally aligned with driving audios and spatially coherent to the context. To achieve this, we provide a two-stage and cross-modal controllable video generation pipeline, taking facial landmarks as an explicit and compact control signal to bridge the driving audio, talking context and generated videos. Inside this pipeline, we devise a 3D video diffusion model, allowing for efficient contort of both spatial conditions (landmarks and context video), as well as audio condition for temporally coherent generation. The experimental results verify the advantage of the proposed method over other baselines in terms of audio-video synchronization, video fidelity and frame consistency.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们考虑了一个新颖且实用的人脸视频生成案例。具体来说，我们关注涉及多人交互的场景，其中存在谈话上下文，例如观众或周围环境。在这些情况下，视频生成应考虑上下文，以便生成与驾驶音频自然对齐并在空间上与上下文一致的视频内容。为了实现这一目标，我们提供了一个两阶段、跨模式的可控视频生成管道，将面部标志作为明确而紧凑的控制信号，以桥接驾驶音频、谈话上下文和生成的视频。在此管道内，我们设计了一个 3D 视频扩散模型，允许有效扭曲空间条件（地标和上下文视频）以及音频条件以实现时间相干的生成。实验结果验证了该方法在音视频同步、视频保真度和帧一致性方面相对于其他基线的优势。</details>
**PDF:** <http://arxiv.org/pdf/2402.18092v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis**<br />
**Title_cn:** 用于姿势引导人体图像合成的从粗到细的潜在扩散<br />
**Authors:** Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai<br />
**Abstract:** <details><summary>原文: </summary>Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at https://github.com/YanzuoLu/CFLD.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型是一种很有前景的图像生成方法，已被用于姿势引导人体图像合成（PGPIS），具有具有竞争力的性能。虽然现有方法只是将人的外观与目标姿势对齐，但由于缺乏对源人图像的高级语义理解，它们很容易过度拟合。在本文中，我们提出了一种新颖的 PGPIS 粗到细潜在扩散（CFLD）方法。在缺乏图像标题对和文本提示的情况下，我们开发了一种纯粹基于图像的新颖的训练范式来控制预训练的文本到图像扩散模型的生成过程。感知细化解码器旨在逐步细化一组可学习的查询，并提取人物图像的语义理解作为粗粒度的提示。这允许在不同阶段解耦细粒度的外观和姿势信息控制，从而避免潜在的过度拟合问题。为了生成更真实的纹理细节，提出了一种混合粒度注意模块，将多尺度细粒度外观特征编码为偏差项，以增强粗粒度提示。 DeepFashion 基准的定量和定性实验结果都证明了我们的方法相对于 PGPIS 的最新技术的优越性。代码可在 https://github.com/YanzuoLu/CFLD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18078v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model**<br />
**Title_cn:** SynArtifact：通过视觉语言模型对合成图像中的伪影进行分类和消除<br />
**Authors:** Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao<br />
**Abstract:** <details><summary>原文: </summary>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.</details>
**Abstract_cn:** <details><summary>译文: </summary>在快速发展的图像合成领域，一个严峻的挑战是存在复杂的伪影，这些伪影会损害合成图像的感知真实感。为了减少伪影并提高合成图像的质量，我们微调视觉语言模型（VLM）作为伪影分类器，以自动识别和分类各种伪影，并为进一步优化生成模型提供监督。具体来说，我们开发了一个全面的伪影分类法，并构建了一个带有伪影注释的合成图像数据集，用于微调 VLM，名为 SynArtifact-1K。经过微调的 VLM 表现出卓越的伪影识别能力，比基线高出 25.66%。据我们所知，这是第一次提出这种端到端的工件分类任务和解决方案。最后，我们利用 VLM 的输出作为反馈来完善生成模型以减轻伪影。可视化结果和用户研究表明，细化扩散模型合成的图像质量得到了明显提高。</details>
**PDF:** <http://arxiv.org/pdf/2402.18068v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine**<br />
**Title_cn:** OpenMEDLab：医学多模态基础模型的开源平台<br />
**Authors:** Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.</details>
**Abstract_cn:** <details><summary>译文: </summary>推进通用人工智能的新兴趋势，例如 GPTv4 和 Gemini，重塑了机器学习和许多其他研究领域的研究（学术界和工业界）格局。然而，此类基础模型的特定领域应用（例如在医学中）仍然没有受到影响或通常处于非常早期的阶段。它将需要一套单独的迁移学习和模型适应技术，通过进一步扩展和注入领域知识和数据这些模型。如果将数据、算法和预先训练的基础模型集中在一起并以有组织的方式开源，则可以大大加速此类技术的发展。在这项工作中，我们提出了 OpenMEDLab，一个用于多模态基础模型的开源平台。它不仅囊括了针对一线临床和生物信息学应用提示和微调大型语言和视觉模型的开创性尝试的解决方案，而且还利用大规模多模态医疗数据构建特定领域的基础模型。重要的是，它为各种医学图像模式、临床文本、蛋白质工程等提供了一组预先训练的基础模型。在下游任务的各种基准中，还为每个收集的方法和模型展示了鼓舞人心且有竞争力的结果。我们欢迎医疗人工智能领域的研究人员不断为OpenMEDLab贡献前沿的方法和模型，可以通过https://github.com/openmedlab访问。</details>
**PDF:** <http://arxiv.org/pdf/2402.18028v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift**<br />
**Title_cn:** 打破黑匣子：针对分布偏移的置信引导模型反转攻击<br />
**Authors:** Xinhao Liu, Yingzhao Jiang, Zetao Lin<br />
**Abstract:** <details><summary>原文: </summary>Model inversion attacks (MIAs) seek to infer the private training data of a target classifier by generating synthetic images that reflect the characteristics of the target class through querying the model. However, prior studies have relied on full access to the target model, which is not practical in real-world scenarios. Additionally, existing black-box MIAs assume that the image prior and target model follow the same distribution. However, when confronted with diverse data distribution settings, these methods may result in suboptimal performance in conducting attacks. To address these limitations, this paper proposes a \textbf{C}onfidence-\textbf{G}uided \textbf{M}odel \textbf{I}nversion attack method called CG-MI, which utilizes the latent space of a pre-trained publicly available generative adversarial network (GAN) as prior information and gradient-free optimizer, enabling high-resolution MIAs across different data distributions in a black-box setting. Our experiments demonstrate that our method significantly \textbf{outperforms the SOTA black-box MIA by more than 49\% for Celeba and 58\% for Facescrub in different distribution settings}. Furthermore, our method exhibits the ability to generate high-quality images \textbf{comparable to those produced by white-box attacks}. Our method provides a practical and effective solution for black-box model inversion attacks.</details>
**Abstract_cn:** <details><summary>译文: </summary>模型反转攻击（MIA）试图通过查询模型生成反映目标类特征的合成图像来推断目标分类器的私有训练数据。然而，先前的研究依赖于对目标模型的完全访问，这在现实场景中并不实用。此外，现有的黑盒 MIA 假设图像先验和目标模型遵循相同的分布。然而，当面对不同的数据分布设置时，这些方法可能会导致攻击性能不佳。为了解决这些限制，本文提出了一种名为 CG-MI 的 \textbf{C}onfidence-\textbf{G}uided \textbf{M}odel \textbf{I}nversion 攻击方法，该方法利用预编码的潜在空间经过训练的公开可用的生成对抗网络（GAN）作为先验信息和无梯度优化器，在黑盒设置中实现跨不同数据分布的高分辨率 MIA。我们的实验表明，我们的方法显着\textbf{在不同的分布设置中，Celeba 的性能优于 SOTA 黑盒 MIA 超过 49\%，Facescrub 的性能优于 58\%}。此外，我们的方法展示了生成高质量图像\textbf{可与白盒攻击生成的图像相媲美}的能力。我们的方法为黑盒模型反转攻击提供了实用且有效的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.18027v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis**<br />
**Title_cn:** PolyOculus：基于图像的同时多视图新颖视图合成<br />
**Authors:** Jason J. Yu, Tristan Aumentado-Armstrong, Fereshteh Forghani, Konstantinos G. Derpanis, Marcus A. Brubaker<br />
**Abstract:** <details><summary>原文: </summary>This paper considers the problem of generative novel view synthesis (GNVS), generating novel, plausible views of a scene given a limited number of known views. Here, we propose a set-based generative model that can simultaneously generate multiple, self-consistent new views, conditioned on any number of known views. Our approach is not limited to generating a single image at a time and can condition on zero, one, or more views. As a result, when generating a large number of views, our method is not restricted to a low-order autoregressive generation approach and is better able to maintain generated image quality over large sets of images. We evaluate the proposed model on standard NVS datasets and show that it outperforms the state-of-the-art image-based GNVS baselines. Further, we show that the model is capable of generating sets of camera views that have no natural sequential ordering, like loops and binocular trajectories, and significantly outperforms other methods on such tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文考虑了生成新颖视图合成（GNVS）的问题，即在给定有限数量的已知视图的情况下生成新颖的、合理的场景视图。在这里，我们提出了一种基于集合的生成模型，它可以同时生成多个、自洽的新视图，以任意数量的已知视图为条件。我们的方法不限于一次生成单个图像，并且可以以零个、一个或多个视图为条件。因此，当生成大量视图时，我们的方法不限于低阶自回归生成方法，并且能够更好地在大量图像上保持生成的图像质量。我们在标准 NVS 数据集上评估了所提出的模型，并表明它优于最先进的基于图像的 GNVS 基线。此外，我们表明该模型能够生成没有自然顺序排序的相机视图集，例如循环和双目轨迹，并且在此类任务上显着优于其他方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.17986v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction**<br />
**Title_cn:** 基于视觉语言模型的利用视觉上下文提取的字幕评估方法<br />
**Authors:** Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki<br />
**Abstract:** <details><summary>原文: </summary>Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.</details>
**Abstract_cn:** <details><summary>译文: </summary>鉴于视觉和语言建模的不断进步，对机器生成的图像描述的准确评估仍然至关重要。为了更接近人类偏好地评估字幕，指标需要区分不同质量和内容的字幕。然而，传统的指标无法超越单词的表面匹配或嵌入的相似性进行比较。因此，他们仍然需要改进。本文提出了VisCE$^2$，一种基于视觉语言模型的字幕评估方法。我们的方法侧重于视觉上下文，它指的是图像的详细内容，包括对象、属性和关系。通过提取它们并将其组织成结构化格式，我们用视觉上下文取代了人工编写的参考文献，并帮助 VLM 更好地理解图像，从而提高评估性能。通过对多个数据集的元评估，我们验证了 VisCE$^2$ 在捕获字幕质量方面优于传统的预训练指标，并表现出与人类判断的卓越一致性。</details>
**PDF:** <http://arxiv.org/pdf/2402.17969v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images**<br />
**Title_cn:** 多模态学习改善电影 MR 图像中的心脏晚期机械激活检测<br />
**Authors:** Jiarui Xing, Nian Wu, Kenneth Bilchick, Frederick Epstein, Miaomiao Zhang<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a multimodal deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images. More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection. An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs. Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection. Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种多模态深度学习框架，该框架利用先进的图像技术来提高严重依赖于常规采集的标准图像的临床分析的性能。更具体地说，我们开发了一个联合学习网络，首次利用受激回波位移编码（DENSE）获得的心肌应变的准确性和可重复性来指导后期机械激活中的电影心脏磁共振（CMR）成像分析。 LMA）检测。利用图像配准网络从标准电影 CMR 中获取心脏运动的知识，这是应变值的重要特征估计器。我们的框架由两个主要部分组成：（i）一个 DENSE 监督应变网络，利用从配准网络学习到的潜在运动特征来预测心肌应变； (ii) LMA 网络利用预测应变进行有效的 LMA 检测。实验结果表明，我们提出的工作极大地提高了电影 CMR 图像的应变分析和 LMA 检测的性能，与 DENSE 的成就更加一致。</details>
**PDF:** <http://arxiv.org/pdf/2402.18507v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding**<br />
**Title_cn:** TAMM：用于 3D 形状理解的 TriAdapter 多模态学习<br />
**Authors:** Zhihao Zhang, Shengcao Cao, Yu-Xiong Wang<br />
**Abstract:** <details><summary>原文: </summary>The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \url{https://alanzhangcs.github.io/tamm-page}.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前 3D 形状数据集规模有限，阻碍了 3D 形状理解的进步，并激发了多模态学习方法，将学习的知识从数据丰富的 2D 图像和语言模态转移到 3D 形状。然而，尽管图像和语言表示已经通过 CLIP 等跨模态模型进行了对齐，但我们发现在现有的多模态 3D 表示学习方法中，图像模态的贡献不如语言。这是由于 2D 图像中的域偏移和每种模态的不同焦点。为了在预训练中更有效地利用这两种模式，我们引入了 TriAdapter 多模式学习（TAMM）——一种基于三个协同适配器的新型两阶段学习方法。首先，我们的 CLIP 图像适配器通过调整 CLIP 的视觉表示来合成图像-文本对，从而缩小 3D 渲染图像和自然图像之间的域差距。随后，我们的双适配器将 3D 形状表示空间解耦为两个互补的子空间：一个专注于视觉属性，另一个专注于语义理解，这确保了更全面、更有效的多模态预训练。大量实验表明，TAMM 能够持续增强各种 3D 编码器架构、预训练数据集和下游任务的 3D 表示。值得注意的是，我们将 Objaverse-LVIS 上的零样本分类精度从 46.8 提高到 50.7，并将 ModelNet40 上的 5 路 10 样本线性探测分类精度从 96.1 提高到 99.0。项目页面：\url{https://alanzhangcs.github.io/tamm-page}。</details>
**PDF:** <http://arxiv.org/pdf/2402.18490v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information**<br />
**Title_cn:** 使用 PET/CT 放射组学和临床信息预测头颈癌的无复发生存期<br />
**Authors:** Mona Furukawa, Daniel R. McGowan, Bartłomiej W. Papież<br />
**Abstract:** <details><summary>原文: </summary>The 5-year survival rate of Head and Neck Cancer (HNC) has not improved over the past decade and one common cause of treatment failure is recurrence. In this paper, we built Cox proportional hazard (CoxPH) models that predict the recurrence free survival (RFS) of oropharyngeal HNC patients. Our models utilise both clinical information and multimodal radiomics features extracted from tumour regions in Computed Tomography (CT) and Positron Emission Tomography (PET). Furthermore, we were one of the first studies to explore the impact of segmentation accuracy on the predictive power of the extracted radiomics features, through under- and over-segmentation study. Our models were trained using the HEad and neCK TumOR (HECKTOR) challenge data, and the best performing model achieved a concordance index (C-index) of 0.74 for the model utilising clinical information and multimodal CT and PET radiomics features, which compares favourably with the model that only used clinical information (C-index of 0.67). Our under- and over-segmentation study confirms that segmentation accuracy affects radiomics extraction, however, it affects PET and CT differently.</details>
**Abstract_cn:** <details><summary>译文: </summary>头颈癌 (HNC) 的 5 年生存率在过去十年中并未提高，治疗失败的常见原因之一是复发。在本文中，我们建立了 Cox 比例风险 (CoxPH) 模型来预测口咽部 HNC 患者的无复发生存期 (RFS)。我们的模型利用从计算机断层扫描 (CT) 和正电子发射断层扫描 (PET) 中的肿瘤区域提取的临床信息和多模态放射组学特征。此外，我们是通过欠分割和过度分割研究探索分割准确性对提取的放射组学特征的预测能力的影响的首批研究之一。我们的模型使用 HEad and neCK TumOR (HECKTOR) 挑战数据进行训练，利用临床信息和多模态 CT 和 PET 放射组学特征的模型，表现最佳的模型达到了 0.74 的一致性指数 (C 指数)，这与仅使用临床信息的模型（C 指数为 0.67）。我们的欠分割和过度分割研究证实，分割精度会影响放射组学提取，但是，它对 PET 和 CT 的影响不同。</details>
**PDF:** <http://arxiv.org/pdf/2402.18417v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **A Multimodal Handover Failure Detection Dataset and Baselines**<br />
**Title_cn:** 多模式切换失败检测数据集和基线<br />
**Authors:** Santosh Thoduka, Nico Hochgeschwender, Juergen Gall, Paul G. Plöger<br />
**Abstract:** <details><summary>原文: </summary>An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器人和人类之间的物体切换是一种协调动作，很容易因沟通不畅、动作不正确和意外的物体属性等原因而失败。现有的切换故障检测和预防工作重点是防止由于物体滑动或外部干扰而导致的故障。然而，缺乏考虑人类参与者造成的不可预防的故障的数据集和评估方法。为了解决这一缺陷，我们提出了多模式切换故障检测数据集，其中包含人类参与者引起的故障，例如忽略机器人或不释放物体。我们还提出了两种用于切换失败检测的基线方法：(i) 使用 3D CNN 的视频分类方法和 (ii) 时间动作分割方法，该方法对人类动作、机器人动作和动作的整体结果进行联合分类。结果表明，视频是一种重要的方式，但使用力-扭矩数据和夹具位置有助于提高故障检测和动作分割的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18319v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding**<br />
**Title_cn:** 用于视觉丰富网页理解的分层多模态预训练<br />
**Authors:** Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu, Kai Yu<br />
**Abstract:** <details><summary>原文: </summary>The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.</details>
**Abstract_cn:** <details><summary>译文: </summary>网页和扫描/数字生成的文档（图像、PDF 等）等视觉效果丰富的文档日益流行，导致学术界和工业界对自动文档理解和信息提取的兴趣日益浓厚。尽管各种文档模式（包括图像、文本、布局和结构）有助于人类信息检索，但这些模式的互连性质给神经网络带来了挑战。在本文中，我们介绍了 WebLM，这是一种多模式预训练网络，旨在解决仅对网页中的 HTML 文本和结构模式进行建模的局限性。 WebLM 不是将文档图像处理为统一的自然图像，而是集成了文档图像的层次结构，以增强对基于标记语言的文档的理解。此外，我们提出了几个预训练任务来有效地模拟文本、结构和图像模态之间的交互。实证结果表明，预训练的 WebLM 在多个网页理解任务中显着超越了之前最先进的预训练模型。预训练的模型和代码可在 https://github.com/X-LANCE/weblm 上获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18262v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Polos: Multimodal Metric Learning from Human Feedback for Image Captioning**<br />
**Title_cn:** Polos：根据图像字幕的人类反馈进行多模态度量学习<br />
**Authors:** Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura<br />
**Abstract:** <details><summary>原文: </summary>Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.</details>
**Abstract_cn:** <details><summary>译文: </summary>建立与人类判断紧密结合的自动评估指标对于有效开发图像字幕模型至关重要。最近的数据驱动指标表明，与 CIDEr 等经典指标相比，与人类判断的相关性更强；然而，它们缺乏足够的能力来处理幻觉和泛化不同的图像和文本，部分原因是它们仅使用从与图像字幕评估无关的任务中学习的嵌入来计算标量相似性。在这项研究中，我们提出了 Polos，一种用于图像字幕模型的监督自动评估指标。 Polos 使用并行特征提取机制计算多模态输入的分数，该机制利用通过大规模对比学习训练的嵌入。为了训练 Polos，我们引入了来自人类反馈的多模态度量学习 (M$^2$LHF)，这是一个基于人类反馈开发度量的框架。我们构建了 Polaris 数据集，其中包含来自 550 名评估者的 131K 条人类判断，大约比标准数据集大十倍。我们的方法在 Composite、Flickr8K-Expert、Flickr8K-CF、PASCAL-50S、FOIL 和 Polaris 数据集上实现了最先进的性能，从而证明了其有效性和鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18091v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding**<br />
**Title_cn:** M3-VRD：多模态多任务多教师视觉丰富形式文档理解<br />
**Authors:** Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo Garza, Josiah Poon, Luca Cagliero<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种突破性的多模式、多任务、多教师联合粒度知识蒸馏模型，用于视觉丰富的表单文档理解。该模型旨在通过促进令牌和实体表示之间的细微关联来利用细粒度和粗粒度级别的见解，从而解决表单文档中固有的复杂性。此外，我们引入了新的粒间和跨粒损失函数，以进一步细化多样化的多教师知识蒸馏转移过程，呈现分布差距和对表单文档的统一理解。通过对公开的表单文档理解数据集进行全面评估，我们提出的模型始终优于现有基线，展示了其在处理视觉上复杂的表单文档的复杂结构和内容方面的功效。</details>
**PDF:** <http://arxiv.org/pdf/2402.17983v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **All in a Single Image: Large Multimodal Models are In-Image Learners**<br />
**Title_cn:** 一切都在一个图像中：大型多模态模型是图像内学习器<br />
**Authors:** Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy Ka-Wei Lee, Ee-Peng Lim<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVista and Hallusionbench to test the effectiveness of I$^2$L in complex multimodal reasoning tasks and mitigating language hallucination and visual illusion. Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of I$^2$L. Our code is publicly available at https://github.com/AGI-Edgerunners/IIL.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种新的上下文学习 (ICL) 机制，称为图像内学习 (I$^2$L)，它将演示示例、视觉提示和指令组合到单个图像中，以增强 GPT-4V 的功能。与之前依赖将图像转换为文本或将视觉输入合并到语言模型中的方法不同，I$^2$L 将所有信息整合到一张图像中，并主要利用图像处理、理解和推理能力。这有几个优点：它避免了对复杂图像的不准确的文本描述，提供了定位演示示例的灵活性，减少了输入负担，并通过消除对多个图像和冗长文本的需要来避免超出输入限制。为了进一步结合不同 ICL 方法的优势，我们引入了一种自动策略，为给定任务中的数据示例选择适当的 ICL 方法。我们在 MathVista 和 Hallusionbench 上进行了实验，测试 I$^2$L 在复杂的多模态推理任务以及减轻语言幻觉和视错觉方面的有效性。此外，我们还探讨了图像分辨率、演示示例数量及其位置对 I$^2$L 有效性的影响。我们的代码可在 https://github.com/AGI-Edgerunners/IIL 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.17971v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images**<br />
**Title_cn:** NToP：NeRF 支持的大规模数据集生成，用于顶视图鱼眼图像中的 2D 和 3D 人体姿势估计<br />
**Authors:** Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz<br />
**Abstract:** <details><summary>原文: </summary>Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain. However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective. Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after finetuning on our training set. A similarly finetuned HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用鱼眼相机进行顶视图人体姿态估计（HPE）呈现出一个有前途且创新的应用领域。然而，捕捉这一观点的数据集的可用性极其有限，尤其是那些具有高质量 2D 和 3D 关键点注释的数据集。为了解决这一差距，我们利用神经辐射场 (NeRF) 技术的功能建立了一个综合管道，用于从现有 2D 和 3D 数据集生成人体姿势数据集，专门针对顶视图鱼眼视角定制。通过这个流程，我们创建了一个新颖的数据集 NToP570K（由 NeRF 驱动的鱼眼相机俯视人体姿势数据集，包含超过 57 万张图像），并对其在增强 2D 和 3D 俯视人体神经网络方面的功效进行了广泛的评估姿态估计。在对训练集进行微调后，预训练的 ViTPose-B 模型在 2D HPE 验证集上的 AP 提高了 33.3%。经过类似微调的 HybrIK-Transformer 模型在验证集上的 3D HPE 的 PA-MPJPE 减少了 53.7 毫米。</details>
**PDF:** <http://arxiv.org/pdf/2402.18196v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Gradient Reweighting: Towards Imbalanced Class-Incremental Learning**<br />
**Title_cn:** 梯度重新加权：走向不平衡的班级增量学习<br />
**Authors:** Jiangpeng He, Fengqing Zhu<br />
**Abstract:** <details><summary>原文: </summary>Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>类别增量学习 (CIL) 训练模型不断从非平稳数据中识别新类别，同时保留学到的知识。当应用于以非均匀分布为特征的现实世界数据时，CIL 出现了一个主要挑战，它引入了双重不平衡问题，涉及（i）旧任务的存储样本和新类数据之间的差异（相间不平衡），以及（ ii）每个单独任务中严重的类别不平衡（阶段内不平衡）。我们表明，这种双重不平衡问题会导致 FC 层中的权重出现偏差，导致梯度更新倾斜，从而导致 CI​​L 中的过度/欠拟合和灾难性遗忘。我们的方法通过重新加权平衡优化和无偏分类器学习的梯度来解决这个问题。此外，我们观察到不平衡的遗忘，矛盾的是，由于大量的训练数据在后续学习阶段变得不可用，实例丰富的类在 CIL 期间遭受更高的性能下降。为了解决这个问题，我们进一步引入了一种分布感知的知识蒸馏损失，通过将输出逻辑与丢失的训练数据的分布按比例对齐来减少遗忘。我们在 CIFAR-100、ImageNetSubset 和 Food101 上跨各种评估协议验证了我们的方法，并展示了与现有工作相比的一致改进，显示出在现实场景中应用 CIL 的巨大潜力，并增强了鲁棒性和有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18528v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection**<br />
**Title_cn:** 阳光明媚到暴雨：跨天气知识蒸馏，实现稳健的 3D 物体检测<br />
**Authors:** Xun Huang, Hai Wu, Xin Li, Xiaoliang Fan, Chenglu Wen, Cheng Wang<br />
**Abstract:** <details><summary>原文: </summary>LiDAR-based 3D object detection models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny</details>
**Abstract_cn:** <details><summary>译文: </summary>由于扫描信号质量下降且噪声较大，基于 LiDAR 的 3D 物体检测模型传统上在雨天条件下表现不佳。先前的研究试图通过模拟雨噪声来解决这个问题，以提高检测模型的鲁棒性。然而，模拟和实际受降雨影响的数据点之间存在显着差异。在这项工作中，我们提出了一种新颖的降雨模拟方法，称为 DRET，它结合了动力学和多雨环境理论，为扩展 3D 检测训练的可用真实降雨数据提供了一种经济有效的方法。此外，我们提出了一种晴天到雨天知识蒸馏（SRKD）方法来增强雨天条件下的 3D 检测。在 WaymoOpenDataset 大规模数据集上进行的大量实验表明，当与最先进的 DSVT 模型和其他经典 3D 检测器相结合时，我们提出的框架在不损失效率的情况下展示了检测精度的显着提高。值得注意的是，我们的框架还提高了阳光条件下的检测能力，因此无论天气是雨天还是晴天，都可以为 3D 检测提供强大的解决方案</details>
**PDF:** <http://arxiv.org/pdf/2402.18493v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Multi-objective Differentiable Neural Architecture Search**<br />
**Title_cn:** 多目标可微神经架构搜索<br />
**Authors:** Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter<br />
**Abstract:** <details><summary>原文: </summary>Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.</details>
**Abstract_cn:** <details><summary>译文: </summary>多目标优化 (MOO) 中的帕累托前沿分析（即找到一组不同的帕累托最优解决方案）具有挑战性，尤其是对于神经网络训练等昂贵的目标。通常，在 MOO 神经架构搜索 (NAS) 中，我们的目标是跨设备平衡性能和硬件指标。先前的 NAS 方法通过将硬件约束合并到目标函数中来简化此任务，但分析 Pareto 前沿需要搜索每个约束。在这项工作中，我们提出了一种新颖的 NAS 算法，该算法对用户偏好进行编码，以实现性能和硬件指标之间的权衡，并在一次搜索运行中产生跨多个设备的代表性且多样化的架构。为此，我们通过超网络参数化跨设备和多个目标的联合架构分布，该超网络可以根据硬件功能和偏好向量进行调节，从而实现向新设备的零样本迁移。使用多达 19 个硬件设备和 3 个物镜进行的广泛实验展示了我们方法的有效性和可扩展性。最后，我们表明，在没有额外成本的情况下，我们的方法在不同质量的搜索空间和数据集上优于现有的 MOO NAS 方法，包括 ImageNet-1k 上的 MobileNetV3 和机器翻译上的 Transformer 空间。</details>
**PDF:** <http://arxiv.org/pdf/2402.18213v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation**<br />
**Title_cn:** CFDNet：具有对比特征蒸馏的可推广雾立体匹配网络<br />
**Authors:** Zihua Liu, Yizhou Li, Masatoshi Okutomi<br />
**Abstract:** <details><summary>原文: </summary>Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>雾天场景下的立体匹配仍然是一项具有挑战性的任务，因为散射效应会降低可见度并导致密集对应匹配的特征不太明显。虽然以前的一些基于学习的方法集成了物理散射函数以同时进行立体匹配和去雾，但简单地去除雾可能无助于深度估计，因为雾本身可以提供关键的深度线索。在这项工作中，我们介绍了一个基于对比特征蒸馏（CFD）的框架。该策略将合并的干净雾特征的特征蒸馏与对比学习相结合，确保对雾深度提示和干净匹配特征的平衡依赖。该框架有助于增强模型在干净和有雾环境中的泛化能力。对合成数据集和真实数据集的综合实验证实了我们方法的卓越强度和适应性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18181v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision**<br />
**Title_cn:** Ef-QuantFace：具有小数据和低位精度的简化人脸识别<br />
**Authors:** William Gazali, Jocelyn Michelle Kho, Joshua Santoso, Williem<br />
**Abstract:** <details><summary>原文: </summary>In recent years, model quantization for face recognition has gained prominence. Traditionally, compressing models involved vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential. This paper addresses this by introducing an efficiency-driven approach, fine-tuning the model with just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective quantization is achievable with a smaller dataset, presenting a new paradigm. Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed model training for face recognition. The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach. This paper advances model quantization by highlighting the efficiency and optimal results with small data and training time.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，人脸识别的模型量化得到了重视。传统上，压缩模型涉及庞大的数据集（例如 580 万张图像的 MS1M 数据集）以及大量的训练时间，这就提出了这样的数据量是否必要的问题。本文通过引入一种效率驱动的方法来解决这个问题，仅用最多 14,000 张图像（比 MS1M 小 440 倍）对模型进行微调。我们证明可以使用较小的数据集实现有效的量化，从而提出了一种新的范例。此外，我们结合了基于评估的度量损失，并在 IJB-C 数据集上实现了出色的 96.15% 准确率，建立了一种新的最先进的人脸识别压缩模型训练。随后的分析深入研究了潜在的应用，强调了这种方法的变革力量。本文通过强调小数据和训练时间的效率和最佳结果来推进模型量化。</details>
**PDF:** <http://arxiv.org/pdf/2402.18163v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction**<br />
**Title_cn:** 通过通道先验和伽玛校正的轻量级低光图像增强网络<br />
**Authors:** Shyang-En Weng, Shaou-Gang Miaou, Ricky Christanto<br />
**Abstract:** <details><summary>原文: </summary>Human vision relies heavily on available ambient light to perceive objects. Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts. Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario. We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory. This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction. The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria. Furthermore, we utilized knowledge distillation with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time. The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类视觉很大程度上依赖于可用的环境光来感知物体。低光场景带来两个不同的挑战：由于照明不足而导致的信息丢失和不良的亮度变化。低光图像增强（LLIE）是指为处理这种情况而定制的图像增强技术。我们介绍了 CPGA-Net，这是一种创新的 LLIE 网络，它通过深度学习将暗/亮通道先验和伽玛校正结合起来，并集成了受大气散射模型和 Retinex 理论启发的功能。这种方法结合了传统和深度学习方法的使用，在一个简单而高效的架构框架内设计，专注于基本特征提取。由此产生的 CPGA-Net 是一个轻量级网络，只有 0.25 万个参数和 0.030 秒的推理时间，但它在客观和主观评估标准上都比现有的 LLIE 方法具有优越的性能。此外，我们利用具有可解释因素的知识蒸馏，提出了一个高效版本，可实现 0.18 万个参数和 0.006 秒的推理时间。所提出的方法为 LLIE 注入了新的解决方案理念，在具有挑战性的低光场景中提供了实际应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.18147v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **UniMODE: Unified Monocular 3D Object Detection**<br />
**Title_cn:** UniMODE：统一单目 3D 物体检测<br />
**Authors:** Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao<br />
**Abstract:** <details><summary>原文: </summary>Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection.</details>
**Abstract_cn:** <details><summary>译文: </summary>实现统一的单目 3D 物体检测（包括室内和室外场景）在机器人导航等应用中非常重要。然而，涉及各种数据场景来训练模型会带来挑战，因为它们的特征显着不同，例如不同的几何属性和异构域分布。为了应对这些挑战，我们构建了一个基于鸟瞰图（BEV）检测范例的检测器，其中显式特征投影有利于解决使用多种数据场景来训练检测器时的几何学习模糊性。然后，我们将经典的 BEV 检测架构分为两个阶段，并提出一种不均匀的 BEV 网格设计来处理上述挑战引起的收敛不稳定。此外，我们开发了一种稀疏 BEV 特征投影策略来降低计算成本，并开发了一种统一的域对齐方法来处理异构域。结合这些技术，得出了统一的检测器 UniMODE，它在具有挑战性的 Omni3D 数据集（包括室内和室外场景的大规模数据集）上超越了之前最先进的技术 4.9% AP_3D，揭示了首次成功的泛化BEV 探测器统一 3D 物体检测。</details>
**PDF:** <http://arxiv.org/pdf/2402.18573v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures**<br />
**Title_cn:** 轮胎 X 射线图像中的缺陷检测：传统方法与深层结构的结合<br />
**Authors:** Andrei Cozma, Landon Harris, Hairong Qi, Ping Ji, Wenpeng Guo, Song Yuan<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了一种通过利用传统特征提取方法（例如局部二值模式（LBP）和灰度共生矩阵（GLCM）特征以及基于傅立叶和小波的特征提取方法）在轮胎 X 射线图像中自动检测缺陷的鲁棒方法。功能，并辅以先进的机器学习技术。认识到轮胎 X 射线图像的复杂图案和纹理所固有的挑战，该研究强调了特征工程对于增强缺陷检测系统性能的重要性。通过将这些特征的组合与随机森林 (RF) 分类器精心集成，并将它们与 YOLOv8 等先进模型进行比较，该研究不仅对传统特征在缺陷检测中的性能进行了基准测试，而且还探索了经典方法和现代方法之间的协同作用。实验结果表明，这些传统特征经过微调并与机器学习模型相结合，可以显着提高轮胎缺陷检测的准确性和可靠性，旨在为轮胎制造的自动化质量保证树立新标准。</details>
**PDF:** <http://arxiv.org/pdf/2402.18527v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Detection of Micromobility Vehicles in Urban Traffic Videos**<br />
**Title_cn:** 城市交通视频中微型车辆的检测<br />
**Authors:** Khalil Sabri, Célia Djilali, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Wassim Bouachir<br />
**Abstract:** <details><summary>原文: </summary>Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects. Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.</details>
**Abstract_cn:** <details><summary>译文: </summary>城市交通环境给物体检测带来了独特的挑战，特别是随着电动滑板车和自行车等微型移动车辆的不断增加。为了解决这个对象检测问题，这项工作引入了一种自适应检测模型，它将单帧对象检测的准确性和速度与视频对象检测框架提供的更丰富的功能结合起来。这是通过将通过运动流处理的连续帧的聚合特征图应用到 YOLOX 架构来完成的。这种融合为 YOLOX 检测能力带来了时间视角，可以更好地理解城市交通模式并大幅提高检测可靠性。我们的模型在针对城市微交通场景的定制数据集上进行了测试，展示了对现有最先进方法的实质性改进，表明需要考虑时空信息来检测如此小而薄的物体。我们的方法增强了在具有挑战性的条件下的检测，包括遮挡、确保时间一致性并有效减轻运动模糊。</details>
**PDF:** <http://arxiv.org/pdf/2402.18503v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** 分离与征服：通过弱监督语义分割的分解和表示来解耦共现<br />
**Authors:** Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song<br />
**Abstract:** <details><summary>原文: </summary>Attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in weakly supervised semantic segmentation (WSSS). In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted to guarantee the correctness of knowledge and further facilitate the discrepancy among co-occurring objects. We streamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence without external supervision. Extensive experiments are conducted, validating the efficiency of our method tackling co-occurrence and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code will be available.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于共现对象的频繁耦合和图像级标签的有限监督，具有挑战性的共现问题广泛存在，并导致弱监督语义分割（WSSS）中对象的错误激活。在这项工作中，我们设计了一种“分离并征服”方案 SeCo，从图像空间和特征空间的维度来解决这个问题。在图像空间中，我们建议通过将图像细分为块来通过图像分解来“分离”同时出现的对象。重要的是，我们为每个补丁分配一个来自类激活图（CAM）的类别标签，这在空间上有助于消除共上下文偏差并指导后续表示。在特征空间中，我们建议通过利用多粒度知识对比增强语义表示来“克服”错误激活。为此，设计了双师单学生架构，并进行标签引导对比，以保证知识的正确性，并进一步促进共现对象之间的差异。我们端到端地简化了多阶段 WSSS 管道，并在没有外部监督的情况下解决了共现问题。进行了大量的实验，验证了我们的方法处理共现的效率以及在 PASCAL VOC 和 MS COCO 上相对于之前的单阶段甚至多阶段竞争对手的优越性。代码将可用。</details>
**PDF:** <http://arxiv.org/pdf/2402.18467v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization**<br />
**Title_cn:** 用于单域泛化的快速驱动的动态以对象为中心的学习<br />
**Authors:** Deng Li, Aming Wu, Yaowei Wang, Yahong Han<br />
**Abstract:** <details><summary>原文: </summary>Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>单域泛化旨在从单个源域数据中学习模型，以在其他未见过的目标域上实现泛化性能。现有的工作主要集中在提高静态网络的泛化能力。然而，静态网络无法动态适应不同图像场景的多样化变化，导致泛化能力有限。不同的场景表现出不同程度的复杂度，并且在跨域场景下图像的复杂度也存在显着差异。在本文中，我们提出了一种基于即时学习的动态以对象为中心的感知网络，旨在适应图像复杂性的变化。具体来说，我们提出了一种基于提示学习的以对象为中心的门控模块，以将注意力集中在各种场景提示引导的以对象为中心的特征上。然后，利用以对象为中心的门控掩模，动态选择模块在空间和通道维度上动态选择高度相关的特征区域，使模型能够自适应地感知以对象为中心的相关特征，从而增强泛化能力。对图像分类和目标检测中的单域泛化任务进行了广泛的实验。实验结果表明，我们的方法优于最先进的方法，这验证了我们提出的方法的有效性和总体性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18447v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation**<br />
**Title_cn:** 通过深度参数估计增强多媒体理解网络鲁棒性的模块化系统<br />
**Authors:** Francesco Barbato, Umberto Michieli, Mehmet Karim Yucel, Pietro Zanuttigh, Mete Ozay<br />
**Abstract:** <details><summary>原文: </summary>In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.</details>
**Abstract_cn:** <details><summary>译文: </summary>在多媒体理解任务中，损坏的样本构成了严峻的挑战，因为当输入到机器学习模型时，它们会导致性能下降。过去，已经提出了三组方法来处理噪声数据：i）增强器和降噪器模块以提高噪声数据的质量，ii）数据增强方法，以及 iii）域适应策略。所有上述方法都存在限制其适用性的缺点；第一个具有较高的计算成本，并且需要成对的干净损坏的数据进行训练，而其他的仅允许部署它们所训练的相同任务/网络（即，当上游和下游任务/网络相同时）。在本文中，我们提出 SyMPIE 来解决这些缺点。为此，我们设计了一个小型、模块化和高效（仅 2GFLOPs 即可处理全高清图像）的系统，以增强输入数据，以最小的计算成本实现强大的下游多媒体理解。我们的 SyMPIE 在上游任务/网络上进行了预训练，该任务/网络不应与下游任务/网络匹配，并且不需要配对的干净损坏样本。我们的主要见解是，现实世界任务中发现的大多数输入损坏可以通过对图像颜色通道或具有小内核的空间滤波器的全局操作进行建模。我们在多个数据集和任务上验证了我们的方法，例如图像分类（在 ImageNetC、ImageNetC-Bar、VizWiz 和新提出的名为 ImageNetC-mixed 的混合损坏基准）和语义分割（在 Cityscapes、ACDC 和 DarkZurich）全面提高约 5% 的相对准确度增益。我们的方法的代码和新的 ImageNetC 混合基准将在发布后提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.18402v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Robust Quantification of Percent Emphysema on CT via Domain Attention: the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study**<br />
**Title_cn:** 通过领域注意力对 CT 上的肺气肿百分比进行稳健量化：动脉粥样硬化 (MESA) 肺研究的多种族研究<br />
**Authors:** Xuzhe Zhang, Elsa D. Angelini, Eric A. Hoffman, Karol E. Watson, Benjamin M. Smith, R. Graham Barr, Andrew F. Laine<br />
**Abstract:** <details><summary>原文: </summary>Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans. Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density. Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study. To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework. We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors. We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于涉及不同扫描仪类型扫描以及转化为临床扫描的大规模研究来说，计算机断层扫描（CT）对肺气肿的稳健量化仍然具有挑战性。现有研究探索了应对这一挑战的几个方向，包括密度校正、噪声过滤、回归、基于隐马尔可夫测量场（HMMF）模型的分割和体积调整的肺密度。尽管取得了一些有希望的结果，但之前的研究要么需要繁琐的工作流程，要么对下游肺气肿亚型进行有限的机会，限制了大规模研究的有效适应。为了缓解这一困境，我们基于现有的 HMMF 分割框架开发了一个端到端的深度学习框架。我们首先证明，由于缺乏扫描仪先验，常规 UNet 无法复制现有的 HMMF 结果。然后，我们设计了一种新颖的领域注意力模块，将图像特征与定量扫描仪先验融合，从而显着改善结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.18383v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis**<br />
**Title_cn:** 增强道路安全：基于激光雷达的树木间隙分析<br />
**Authors:** Miriam Louise Carnot, Eric Peukert, Bogdan Franczyk<br />
**Abstract:** <details><summary>原文: </summary>In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road. Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape. The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences. By automating this process, municipalities can address potential road space constraints, enhancing safety for all. They may also save valuable time by carrying out the inspections more systematically. Our open-source code gives communities inspiration on how to automate the process themselves.</details>
**Abstract_cn:** <details><summary>译文: </summary>在确保道路安全的努力中，确保道路上方有足够的垂直间隙非常重要。树木或其他植被经常生长在道路上方，遮挡交通标志和灯光，并对交通参与者构成危险。由于缺乏深度信息，从简单图像准确估计这个空间具有挑战性。这就是激光雷达技术发挥作用的地方，它是一种显示三维视角的激光扫描传感器。目前，街道级激光雷达点云主要应用于自动驾驶领域。然而，这些扫描也为城市管理开辟了可能性。在本文中，我们提出了一种新的点云算法，可以自动检测生长在街道上且需要修剪的树木部分。我们的系统使用语义分割来过滤相关点和下游处理步骤，以创建所需的体积以保持道路上方畅通。挑战包括模糊的道路、激光雷达点云的嘈杂的非结构化性质以及道路形状的评估。所识别的不合规树木点可以从点云投影到图像上，为市政当局提供处理此类事件的视觉辅助。通过自动化这一过程，市政当局可以解决潜在的道路空间限制，提高所有人的安全。他们还可以通过更系统地进行检查来节省宝贵的时间。我们的开源代码为社区提供了如何自动化流程的灵感。</details>
**PDF:** <http://arxiv.org/pdf/2402.18309v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Feature Denoising For Low-Light Instance Segmentation Using Weighted Non-Local Blocks**<br />
**Title_cn:** 使用加权非局部块进行低光实例分割的特征去噪<br />
**Authors:** Joanne Lin, Nantheera Anantrasirichai, David Bull<br />
**Abstract:** <details><summary>原文: </summary>Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于低光图像所带来的挑战，例如由于低光子数、颜色失真和对比度降低导致的散粒噪声，低光图像的实例分割在很大程度上仍未得到探索。在本文中，我们提出了一种端到端解决方案来解决这一具有挑战性的任务。基于Mask R-CNN，我们提出的方法在特征提取器中实现了加权非局部（NL）块。这种集成可以在特征级别实现固有的去噪过程。因此，我们的方法消除了训练期间对齐地面实况图像的需要，从而支持对现实世界低光数据集的训练。我们在每一层引入额外的可学习权重，以增强网络对现实世界噪声特征的适应性，这些特征以不同的方式影响不同的特征尺度。实验结果表明，所提出的方法优于预训练的 Mask R-CNN，平均精度 (AP) 提高了 +10.0，并且引入加权 NL 块进一步将 AP 提高了 +1.0。</details>
**PDF:** <http://arxiv.org/pdf/2402.18307v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving**<br />
**Title_cn:** EchoTrack：用于自动驾驶的听觉参考多目标跟踪<br />
**Authors:** Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack model and its components. The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了听觉参考多对象跟踪（AR-MOT）的任务，该任务基于音频表达动态跟踪视频序列中的特定对象，这在自动驾驶中是一个具有挑战性的问题。由于缺乏音视频语义建模能力，现有工作主要集中在基于文本的多目标跟踪，这往往以跟踪质量、交互效率甚至辅助系统的安全性为代价，限制了多目标跟踪的应用。此类方法在自动驾驶中的应用。在本文中，我们从音视频融合和音视频跟踪的角度深入研究AR-MOT问题。我们提出了 EchoTrack，一个带有双流视觉转换器的端到端 AR-MOT 框架。双流与我们的双向频域交叉注意融合模块（Bi-FCFM）交织在一起，该模块双向融合来自频域和时空域的音频和视频特征。此外，我们提出了视听对比跟踪学习（ACTL）机制，通过有效学习不同音频和视频对象之间的同质特征来提取表达和视觉对象之间的同质语义特征。除了架构设计之外，我们还建立了第一套大规模 AR-MOT 基准测试，包括 Echo-KITTI、Echo-KITTI+ 和 Echo-BDD。对既定基准的大量实验证明了所提出的 EchoTrack 模型及其组件的有效性。源代码和数据集将在 https://github.com/lab206/EchoTrack 上公开提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.18302v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Grid-Based Continuous Normal Representation for Anomaly Detection**<br />
**Title_cn:** 用于异常检测的基于网格的连续正态表示<br />
**Authors:** Joo Chan Lee, Taejune Kim, Eunbyung Park, Simon S. Woo, Jong Hwan Ko<br />
**Abstract:** <details><summary>原文: </summary>There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing the input and the directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose GRAD, a novel anomaly detection method for representing normal features within a "continuous" feature space, enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that GRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, GRAD effectively handles diverse classes in a single model thanks to the high-granularity global representation. In an evaluation using the MVTec AD dataset, GRAD significantly outperforms the previous state-of-the-art method by reducing 65.0\% of the error for multi-class unified anomaly detection. The project page is available at https://tae-mo.github.io/grad/.</details>
**Abstract_cn:** <details><summary>译文: </summary>以无监督方式进行异常检测已经取得了重大进展，其中只有正常图像可用于训练。最近的几种方法旨在基于内存检测异常，将输入与直接存储的正常特征（或训练后的特征与正常图像）进行比较。然而，这种基于记忆的方法在由最近邻居或注意机制实现的离散特征空间上运行，分别遭受较差的泛化或输出与输入相同的身份快捷方式问题。此外，大多数现有方法旨在检测单类异常，导致在呈现多类对象时性能不令人满意。为了解决上述所有挑战，我们提出了 GRAD，一种新颖的异常检测方法，用于表示“连续”特征空间内的正常特征，通过将空间特征转换为坐标并将其映射到连续网格来实现。此外，我们精心设计了为异常检测量身定制的网格，代表局部和全局正常特征并有效地融合它们。我们广泛的实验表明，GRAD 成功地概括了正常特征并减轻了身份捷径，此外，由于高粒度的全局表示，GRAD 可以有效地在单个模型中处理不同的类别。在使用 MVTec AD 数据集的评估中，GRAD 显着优于之前最先进的方法，将多类统一异常检测的误差降低了 65.0%。项目页面位于 https://tae-mo.github.io/grad/。</details>
**PDF:** <http://arxiv.org/pdf/2402.18293v1><br />
**Code:** <https://github.com/tae-mo/GRAD>**<br />
>>**index:** 12<br />
**Title:** **FSL Model can Score Higher as It Is**<br />
**Title_cn:** FSL 模型可以得分更高<br />
**Authors:** Yunwei Bai, Ying Kiat Tan, Tsuhan Chen<br />
**Abstract:** <details><summary>原文: </summary>In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>在日常生活中，我们往往会直视人脸识别机器，而不是侧向脸部，以呈现我们的正面，以增加被正确识别的机会。少样本学习 (FSL) 分类本身就具有挑战性，因为模型必须识别属于训练期间以前未见过的类别的图像。因此，测试期间扭曲且非典型的查询或支持图像可能会使模型的正确预测变得更具挑战性。在我们的工作中，为了增加测试期间正确预测的机会，我们的目标是通过图像到图像的转换生成测试类的新样本，从而纠正经过训练的 FSL 模型的测试输入。 FSL 模型通常在具有足够样本的类上进行训练，然后在具有少量样本的类上进行测试。我们提出的方法首先捕获测试图像的风格或形状，然后识别合适的训练类样本。然后，它将测试图像的样式或形状传输到训练类图像，以生成更多测试类样本，然后基于一组生成的样本（而不是仅一个样本）执行分类。我们的方法有潜力使经过训练的 FSL 模型在测试阶段获得更高的分数，而无需任何额外的训练或数据集。根据我们的实验，通过仅使用 1 个额外生成的样本来增强支持集，我们就可以在由动物面孔或交通标志组成的数据集上训练 FSL 模型实现约 2% 的改进。通过增加支持集和查询，我们可以实现更多的性能改进。我们的 Github 存储库是公开可用的。</details>
**PDF:** <http://arxiv.org/pdf/2402.18292v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis**<br />
**Title_cn:** 电子显微镜中的自我监督学习：建立高级图像分析的基础模型<br />
**Authors:** Bashir Kazimi, Karina Ruzaeva, Stefan Sandfeld<br />
**Abstract:** <details><summary>原文: </summary>In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们探索了从未标记的电子显微镜数据集进行自我监督学习的潜力，朝着建立该领域的基础模型迈出了一步。我们展示了自监督预训练如何促进一系列下游任务的有效微调，包括语义分割、去噪、噪声和背景去除以及超分辨率。对不同模型复杂性和感受野大小的实验揭示了一个显着的现象，即较低复杂性的微调模型始终优于具有随机权重初始化的更复杂模型。我们展示了在电子显微镜背景下跨各种下游任务的自监督预训练的多功能性，从而实现更快的收敛和更好的性能。我们得出的结论是，自监督预训练是一种强大的催化剂，当可用的注释数据有限且计算成本的有效扩展很重要时，它尤其有利。</details>
**PDF:** <http://arxiv.org/pdf/2402.18286v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods**<br />
**Title_cn:** EAN-MapNet：利用锚点邻域构建高效的矢量化高清地图<br />
**Authors:** Huiyuan Xiong, Jun Shen, Taohong Zhu, Yuelong Pan<br />
**Abstract:** <details><summary>原文: </summary>High-definition (HD) map is crucial for autonomous driving systems. Most existing works design map elements detection heads based on the DETR decoder. However, the initial queries lack integration with the physical location feature of map elements, and vanilla self-attention entails high computational complexity. Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods. Firstly, we design query units based on the physical location feature of anchor neighborhoods. Non-neighborhood central anchors effectively assist the neighborhood central anchors in fitting to the target points, significantly improving the prediction accuracy. Then, we introduce grouped local self-attention (GL-SA), which innovatively utilizes local queries as the medium for feature interaction, thereby substantially reducing the computational complexity of self-attention while facilitating ample feature interaction among queries. On nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs. Furthermore, it considerably reduces memory consumption by 8198M compared to the baseline.</details>
**Abstract_cn:** <details><summary>译文: </summary>高清（HD）地图对于自动驾驶系统至关重要。大多数现有作品基于DETR解码器设计地图元素检测头。然而，初始查询缺乏与地图元素的物理位置特征的集成，并且普通的自注意力需要很高的计算复杂度。因此，我们提出了 EAN-MapNet，用于使用锚点邻域高效构建高清地图。首先，我们根据锚邻域的物理位置特征设计查询单元。非邻域中心锚有效辅助邻域中心锚拟合目标点，显着提高预测精度。然后，我们引入了分组局部自注意力（GL-SA），它创新性地利用局部查询作为特征交互的媒介，从而大大降低了自注意力的计算复杂度，同时促进查询之间充分的特征交互。在 nuScenes 数据集上，EAN-MapNet 在训练 24 个 epoch 后实现了 63.0 mAP 的最先进性能。此外，与基线相比，它显着减少了 8198M 的内存消耗。</details>
**PDF:** <http://arxiv.org/pdf/2402.18278v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **On the Accuracy of Edge Detectors in Number Plate Extraction**<br />
**Title_cn:** 边缘检测器在车牌提取中的准确性研究<br />
**Authors:** Bashir Olaniyi Sadiq<br />
**Abstract:** <details><summary>原文: </summary>Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system. This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle. As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection. This paper presents a method of number plate extraction using edge detection technique. Edges in number plates are identified with changes in the intensity of pixel values. Therefore, these edges are identified using a single based pixel or collection of pixel-based approach. The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented. Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric</details>
**Abstract_cn:** <details><summary>译文: </summary>边缘检测作为预处理阶段是车牌提取系统的基本且重要的方面。这是因为可以使用车牌来识别特定车辆，因为每个车牌对于车辆来说都是唯一的。这样，可以利用边缘检测原理提取车牌系统中线条和形状不同的字符。本文提出了一种利用边缘检测技术提取车牌的方法。车牌中的边缘通过像素值强度的变化来识别。因此，这些边缘是使用基于单个像素或基于像素的集合的方法来识别的。实验了这些边缘检测算法在噪声和清洁环境中提取车牌的效率。使用 Pratt 品质因数 (PFOM) 作为性能指标在 MATLAB 2017b 中获得实验结果</details>
**PDF:** <http://arxiv.org/pdf/2402.18251v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data**<br />
**Title_cn:** Image2Flow：混合图像和图形卷积神经网络，用于根据 3D 心脏 MRI 数据快速进行患者特定肺动脉分割和 CFD 流场计算<br />
**Authors:** Tina Yao, Endrit Pajaziti, Michael Quail, Silvia Schievano, Jennifer A Steeden, Vivek Muthurangu<br />
**Abstract:** <details><summary>原文: </summary>Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics. However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming simulation. This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields.   This study used 135 3D cardiac MRIs from both a public and private dataset. The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes. CFD simulations were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset. The dataset was split 85/10/15 for training, validation and testing. Image2Flow, a hybrid image and graph convolutional neural network, was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values. Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons. Centerline comparisons of Image2Flow and CFD simulations performed using machine learning segmentation were also performed.   Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92). The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively. Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes.   This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation. Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment.</details>
**Abstract_cn:** <details><summary>译文: </summary>计算流体动力学（CFD）可用于评估血流动力学。然而，其日常使用受到劳动密集型手动分割、CFD 网格创建和耗时模拟的限制。本研究旨在训练深度学习模型，以根据 3D 心脏 MRI 数据生成患者特定的肺动脉体积网格，并直接估计 CFD 流场。本研究使用了来自公共和私人数据集的 135 个 3D 心脏 MRI。 MRI 中的肺动脉被手动分割并转换为体积网格。 CFD 模拟在地面实况网格上进行，并插值到点对点对应的网格上以创建地面实况数据集。数据集分为 85/10/15 部分用于训练、验证和测试。 Image2Flow 是一种混合图像和图形卷积神经网络，经过训练可将肺动脉模板转换为患者特定的解剖结构和 CFD 值。 Image2Flow 根据分段进行评估，并使用节点比较来评估 CFD 预测的准确性。还对使用机器学习分割进行的 Image2Flow 和 CFD 模拟进行了中心线比较。 Image2Flow 实现了出色的分割精度，中值 Dice 得分为 0.9（IQR：0.86-0.92）。压力和速度大小的中位节点归一化绝对误差分别为 11.98% (IQR: 9.44-17.90%) 和 8.06% (IQR: 7.54-10.41)。中心线分析显示，在机器学习生成的体积网格上模拟的 Image2Flow 和传统 CFD 之间没有显着差异。这项概念验证研究表明，可以同时执行基于患者特定体积网格的分割以及压力和流场估计。 Image2Flow 在约 205 毫秒内完成分割和 CFD，比手动方法快约 7000 倍，使其在临床环境中更加可行。</details>
**PDF:** <http://arxiv.org/pdf/2402.18236v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Zero-Shot Aerial Object Detection with Visual Description Regularization**<br />
**Title_cn:** 具有视觉描述正则化的零样本空中物体检测<br />
**Authors:** Zhengqing Zang, Chenyu Lin, Chenwei Tang, Tao Wang, Jiancheng Lv<br />
**Abstract:** <details><summary>原文: </summary>Existing object detection models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient object detection methods on aerial images. In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的目标检测模型主要在大规模标记数据集上进行训练。然而，为新颖的空中物体类别注释数据非常昂贵，因为它非常耗时并且可能需要专业知识。因此，需要研究航空图像上的标签有效的目标检测方法。在这项工作中，我们提出了一种用于空中物体检测的零样本方法，称为视觉描述正则化或 DescReg。具体来说，我们识别了空中物体的弱语义视觉相关性，并旨在通过预先描述其视觉外观来解决这一挑战。我们建议将描述中传达的先前类间视觉相似性注入到嵌入学习中，而不是直接将描述编码到遭受表示差距问题的类嵌入空间中。注入过程是通过新设计的相似性感知三元组损失来完成的，该损失结合了表示空间的结构化正则化。我们对三个具有挑战性的空中物体检测数据集（包括 DIOR、xView 和 DOTA）进行了广泛的实验。结果表明，通过复杂的投影设计和生成框架，DescReg 显着优于最先进的 ZSD 方法，例如，在 DIOR 上，DESReg 在未见类别上的 mAP 优于最佳报告的 ZSD 方法 4.5 mAP，在 HM 上优于 8.1 mAP。我们通过将 DescReg 集成到生成 ZSD 方法以及改变检测架构来进一步展示 DescReg 的通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18233v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments**<br />
**Title_cn:** 溢油无人机：无人机捕获的分段 RGB 图像数据集，用于港口环境中的溢油检测<br />
**Authors:** T. De Kerf, S. Sels, S. Samsonova, S. Vanlanduit<br />
**Abstract:** <details><summary>原文: </summary>The high incidence of oil spills in port areas poses a serious threat to the environment, prompting the need for efficient detection mechanisms. Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection. Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents. Currently, there's a scarcity of datasets employing RGB images for oil spill detection in maritime settings. This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms. The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a convolutional neural network trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection. This underscores the dataset's practicality for real-world applications, offering crucial resources for environmental conservation in port environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>港口地区漏油事件的高发对环境构成了严重威胁，迫切需要有效的检测机制。为此使用自动化无人机可以显着提高溢油检测的速度和准确性。这些进步不仅加快了清理工作，减少了环境危害，而且还加强了污染者的责任，有可能阻止未来发生类似的事件。目前，使用 RGB 图像进行海上溢油检测的数据集很稀缺。本文提出了一个独特的带注释的数据集，旨在解决这一差距，利用神经网络在桌面和边缘计算平台上进行分析。该数据集通过无人机捕获，包含 1268 张图像，分为石油、水和其他类别，使用 Unet 模型架构训练的卷积神经网络在石油检测方面实现了 0.71 的 F1 分数。这强调了该数据集在现实世界应用中的实用性，为港口环境的环境保护提供了重要资源。</details>
**PDF:** <http://arxiv.org/pdf/2402.18202v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Out-of-Distribution Detection using Neural Activation Prior**<br />
**Title_cn:** 使用神经激活先验进行分布外检测<br />
**Authors:** Weilin Wan, Weizhong Zhang, Cheng Jin<br />
**Abstract:** <details><summary>原文: </summary>Out-of-distribution detection is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios.In this paper, we propose a simple but effective Neural Activation Prior (NAP) for out-of-distribution detection (OOD). Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few of its neurons being activated with a larger response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is each channel in a model fully trained on ID dataset would play a role in detecting a certain pattern in the samples within the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Thus, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. This approach is plug-and-play and does not lead to any performance degradation on in-distribution data classification and requires no extra training or statistics from training or external datasets. Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator. Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications. Experimental results show that our method achieves the state-of-the-art performance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates the power of the proposed prior.</details>
**Abstract_cn:** <details><summary>译文: </summary>分布外检测是在现实世界中部署机器学习模型来处理未见过的场景的关键技术。在本文中，我们提出了一种简单但有效的神经激活先验（NAP），用于分布外检测（OOD） ）。我们的神经激活先验基于一个关键观察，即对于经过充分训练的神经网络的全局池化层之前的通道，其一些神经元被分布内 (ID) 样本激活为具有更大响应的概率明显高于 OOD 样本。直观的解释是，在 ID 数据集上充分训练的模型中的每个通道都将在检测 ID 数据集中样本中的某种模式中发挥作用，并且当在输入中检测到该模式时，可以激活一些神经元并产生较大的响应样本。因此，提出了一种基于此先验的新评分函数，以强调这些强烈激活的神经元在 OOD 检测中的作用。这种方法是即插即用的，不会导致分布内数据分类的任何性能下降，并且不需要来自训练或外部数据集的额外训练或统计。请注意，以前的方法主要依赖于神经网络的后全局池化特征，而我们利用的通道内分布信息将被全局池化算子丢弃。因此，我们的方法与现有方法正交，并且可以在各种应用中与它们有效地结合。实验结果表明，我们的方法在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上实现了最先进的性能，这证明了所提出的先验的强大功能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18162v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction**<br />
**Title_cn:** OccTransformer：改进 BEVFormer，以实现仅 3D 相机的占用预测<br />
**Authors:** Jian Liu, Sipeng Zhang, Chuixin Kong, Wenyuan Zhang, Yuhang Wu, Yikang Ding, Borun Xu, Ruibo Ming, Donglai Wei, Xianming Liu<br />
**Abstract:** <details><summary>原文: </summary>This technical report presents our solution, "occTransformer" for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023. Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques. Firstly, we employed data augmentation to increase the diversity of the training data and improve the model's generalization ability. Secondly, we used a strong image backbone to extract more informative features from the input data. Thirdly, we incorporated a 3D unet head to better capture the spatial information of the scene. Fourthly, we added more loss functions to better optimize the model. Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance. Most importantly, we integrated 3D detection model StreamPETR to enhance the model's ability to detect objects in the scene. Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge.</details>
**Abstract_cn:** <details><summary>译文: </summary>本技术报告介绍了我们在 CVPR 2023 的自动驾驶挑战赛中用于 3D 占用预测赛道的解决方案“occTransformer”。我们的方法建立在强大的基线 BEVFormer 的基础上，并通过几种简单而有效的技术提高了其性能。首先，我们采用数据增强来增加训练数据的多样性并提高模型的泛化能力。其次，我们使用强大的图像主干从输入数据中提取更多信息特征。第三，我们加入了 3Dunet 头，以更好地捕捉场景的空间信息。第四，我们添加了更多损失函数以更好地优化模型。此外，我们使用了 Occ 模型 BevDet 和 SurroundOcc 的集成方法来进一步提高性能。最重要的是，我们集成了 3D 检测模型 StreamPETR，以增强模型检测场景中物体的能力。使用这些方法，我们的解决方案在自动驾驶挑战赛的 3D 占用预测赛道上取得了 49.23 miou 的成绩。</details>
**PDF:** <http://arxiv.org/pdf/2402.18140v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Classes Are Not Equal: An Empirical Study on Image Recognition Fairness**<br />
**Title_cn:** 类不平等：图像识别公平性的实证研究<br />
**Authors:** Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了关于图像识别公平性的实证研究，即 ImageNet 等平衡数据上的极端类别准确度差异。我们通过实验证明，类别不相等，并且公平性问题在各种数据集、网络架构和模型容量的图像分类模型中普遍存在。此外，还发现了公平性的几个有趣的特性。首先，不公平在于有问题的表示而不是分类器偏差。其次，通过提出的模型预测偏差概念，我们研究了优化期间有问题的表示的根源。我们的研究结果表明，模型往往对更难以识别的类别表现出更大的预测偏差。这意味着更多其他课程将与更难的课程混淆。那么误报（FP）将主导优化中的学习，从而导致其准确性较差。此外，我们得出的结论是，数据增强和表示学习算法通过在一定程度上促进图像分类的公平性来提高整体性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18133v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Understanding the Role of Pathways in a Deep Neural Network**<br />
**Title_cn:** 了解深度神经网络中路径的作用<br />
**Authors:** Lei Lyu, Chen Pang, Jihua Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding adversarial attacks, object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在人工智能应用中表现出了优越的性能，但其内部工作机制的不透明是其应用中的一大缺陷。流行的基于单元的解释是对刺激-反应数据的统计观察，它无法显示神经网络固有机制的详细内部过程。在这项工作中，我们分析了在分类任务中训练的卷积神经网络（CNN），并提出了一种算法来提取单个像素的扩散路径，以识别与对象类相关的输入图像中的像素位置。这些路径使我们能够测试对分类很重要的因果成分，并且基于路径的表示在类别之间可以清楚地区分。我们发现图像中单个像素的几个最大路径往往会穿过对分类很重要的每一层中的特征图。并且同一类别图像的大路径比不同类别图像的大路径趋势更加一致。我们还应用这些途径来理解对抗性攻击、对象完成和运动感知。此外，所有层中特征图上的路径总数可以清楚地区分原始样本、变形样本和目标样本。</details>
**PDF:** <http://arxiv.org/pdf/2402.18132v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation**<br />
**Title_cn:** PRCL：半监督语义分割的概率表示对比学习<br />
**Authors:** Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu, Baigui Sun<br />
**Abstract:** <details><summary>原文: </summary>Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过对比学习，半监督语义分割（S4）取得了巨大的突破。然而，由于注释有限，对未标记图像的指导是由模型本身生成的，这不可避免地存在噪声并干扰无监督训练过程。为了解决这个问题，我们提出了一个鲁棒的基于对比的 S4 框架，称为概率表示对比学习（PRCL）框架，以增强无监督训练过程的鲁棒性。我们通过多元高斯分布将像素级表示建模为概率表示（PR），并调整模糊表示的贡献以容忍对比学习中指导不准确的风险。此外，我们通过收集整个训练过程中的所有 PR 来引入全球分布原型 (GDP)。由于GDP包含同一类的所有表示的信息，因此它对表示中的瞬时噪声具有鲁棒性，并且承受表示的类内方差。此外，我们还根据 GDP 生成虚拟负数（VN）以参与对比学习过程。对两个公共基准的广泛实验证明了我们的 PRCL 框架的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18117v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **UniVS: Unified and Universal Video Segmentation with Prompts as Queries**<br />
**Title_cn:** UniVS：以提示作为查询的统一通用视频分割<br />
**Authors:** Minghan Li, Shuai Li, Xindong Zhang, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \url{https://github.com/MinghanLi/UniVS}.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管统一图像分割（IS）最近取得了进展，但开发统一视频分割（VS）模型仍然是一个挑战。这主要是因为通用类别指定的 VS 任务需要检测所有对象并在连续帧中跟踪它们，而提示引导的 VS 任务需要在整个视频中通过视觉/文本提示重新识别目标，这使得处理具有相同架构的不同任务。我们尝试解决这些问题，并通过使用提示作为查询，提出了一种新颖的统一 VS 架构，即 UniVS。 UniVS 将前一帧中目标的提示特征平均作为其初始查询，以显式解码掩码，并在掩码解码器中引入目标明智的提示交叉注意层，以将提示特征集成到内存池中。通过将前一帧中实体的预测掩模作为视觉提示，UniVS 将不同的 VS 任务转换为提示引导的目标分割，消除了启发式帧间匹配过程。我们的框架不仅统一了不同的VS任务，而且自然地实现了通用的训练和测试，确保跨不同场景的稳健性能。 UniVS 在 10 个具有挑战性的 VS 基准测试中显示了性能和通用性之间值得称赞的平衡，涵盖视频实例、语义、全景、对象和引用分割任务。代码可以在 \url{https://github.com/MinghanLi/UniVS} 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.18115v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Dual-Context Aggregation for Universal Image Matting**<br />
**Title_cn:** 用于通用图像抠图的双上下文聚合<br />
**Authors:** Qinglin Liu, Xiaoqian Lv, Wei Yu, Changyong Guo, Shengping Zhang<br />
**Abstract:** <details><summary>原文: </summary>Natural image matting aims to estimate the alpha matte of the foreground from a given image. Various approaches have been explored to address this problem, such as interactive matting methods that use guidance such as click or trimap, and automatic matting methods tailored to specific objects. However, existing matting methods are designed for specific objects or guidance, neglecting the common requirement of aggregating global and local contexts in image matting. As a result, these methods often encounter challenges in accurately identifying the foreground and generating precise boundaries, which limits their effectiveness in unforeseen scenarios. In this paper, we propose a simple and universal matting framework, named Dual-Context Aggregation Matting (DCAM), which enables robust image matting with arbitrary guidance or without guidance. Specifically, DCAM first adopts a semantic backbone network to extract low-level features and context features from the input image and guidance. Then, we introduce a dual-context aggregation network that incorporates global object aggregators and local appearance aggregators to iteratively refine the extracted context features. By performing both global contour segmentation and local boundary refinement, DCAM exhibits robustness to diverse types of guidance and objects. Finally, we adopt a matting decoder network to fuse the low-level features and the refined context features for alpha matte estimation. Experimental results on five matting datasets demonstrate that the proposed DCAM outperforms state-of-the-art matting methods in both automatic matting and interactive matting tasks, which highlights the strong universality and high performance of DCAM. The source code is available at \url{https://github.com/Windaway/DCAM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>自然图像抠图旨在估计给定图像中前景的 alpha 遮罩。人们已经探索了各种方法来解决这个问题，例如使用点击或三分图等指导的交互式抠图方法，以及针对特定对象定制的自动抠图方法。然而，现有的抠图方法是针对特定对象或指导而设计的，忽略了图像抠图中聚合全局和局部上下文的常见要求。因此，这些方法在准确识别前景和生成精确边界方面经常遇到挑战，这限制了它们在不可预见的场景中的有效性。在本文中，我们提出了一种简单且通用的抠图框架，称为双上下文聚合抠图（DCAM），它可以在任意指导或无指导的情况下实现鲁棒的图像抠图。具体来说，DCAM首先采用语义骨干网络从输入图像和指导中提取低级特征和上下文特征。然后，我们引入了一种双上下文聚合网络，该网络结合了全局对象聚合器和局部外观聚合器来迭代地细化提取的上下文特征。通过执行全局轮廓分割和局部边界细化，DCAM 对不同类型的引导和对象表现出鲁棒性。最后，我们采用抠图解码器网络来融合低级特征和细化的上下文特征以进行 alpha 抠图估计。五个抠图数据集的实验结果表明，所提出的 DCAM 在自动抠图和交互式抠图任务中均优于最先进的抠图方法，这凸显了 DCAM 的强大通用性和高性能。源代码可在 \url{https://github.com/Windaway/DCAM} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18109v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Spannotation: Enhancing Semantic Segmentation for Autonomous Navigation with Efficient Image Annotation**<br />
**Title_cn:** Spanotation：通过高效的图像注释增强自主导航的语义分割<br />
**Authors:** Samuel O. Folorunsho, William R. Norris<br />
**Abstract:** <details><summary>原文: </summary>Spannotation is an open source user-friendly tool developed for image annotation for semantic segmentation specifically in autonomous navigation tasks. This study provides an evaluation of Spannotation, demonstrating its effectiveness in generating accurate segmentation masks for various environments like agricultural crop rows, off-road terrains and urban roads. Unlike other popular annotation tools that requires about 40 seconds to annotate an image for semantic segmentation in a typical navigation task, Spannotation achieves similar result in about 6.03 seconds. The tools utility was validated through the utilization of its generated masks to train a U-Net model which achieved a validation accuracy of 98.27% and mean Intersection Over Union (mIOU) of 96.66%. The accessibility, simple annotation process and no-cost features have all contributed to the adoption of Spannotation evident from its download count of 2098 (as of February 25, 2024) since its launch. Future enhancements of Spannotation aim to broaden its application to complex navigation scenarios and incorporate additional automation functionalities. Given its increasing popularity and promising potential, Spannotation stands as a valuable resource in autonomous navigation and semantic segmentation. For detailed information and access to Spannotation, readers are encouraged to visit the project's GitHub repository at https://github.com/sof-danny/spannotation</details>
**Abstract_cn:** <details><summary>译文: </summary>Spannotation 是一款开源用户友好工具，专为语义分割的图像注释而开发，特别是在自主导航任务中。本研究对 Spannotation 进行了评估，证明了其在为农作物行、越野地形和城市道路等各种环境生成准确分割掩模方面的有效性。与其他流行的注释工具在典型导航任务中需要大约 40 秒来注释图像以进行语义分割不同，Spannotation 在大约 6.03 秒内实现了类似的结果。该工具实用程序通过利用其生成的掩码来训练 U-Net 模型进行了验证，该模型的验证准确度为 98.27%，平均交集比 (mIOU) 为 96.66%。可访问性、简单的注释过程和免费功能都有助于 Spannotation 的采用，从其推出以来的下载量为 2098 次（截至 2024 年 2 月 25 日）可见一斑。 Spannotation 的未来增强旨在将其应用范围扩大到复杂的导航场景，并纳入额外的自动化功能。鉴于其日益普及和广阔的潜力，Spannotation 成为自主导航和语义分割领域的宝贵资源。有关 Spannotation 的详细信息和访问权限，鼓励读者访问该项目的 GitHub 存储库：https://github.com/sof-danny/spannotation</details>
**PDF:** <http://arxiv.org/pdf/2402.18084v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Human Shape and Clothing Estimation**<br />
**Title_cn:** 人体形状和服装估计<br />
**Authors:** Aayush Gupta, Aditya Gulati, Himanshu, Lakshya LNU<br />
**Abstract:** <details><summary>原文: </summary>Human shape and clothing estimation has gained significant prominence in various domains, including online shopping, fashion retail, augmented reality (AR), virtual reality (VR), and gaming. The visual representation of human shape and clothing has become a focal point for computer vision researchers in recent years. This paper presents a comprehensive survey of the major works in the field, focusing on four key aspects: human shape estimation, fashion generation, landmark detection, and attribute recognition. For each of these tasks, the survey paper examines recent advancements, discusses their strengths and limitations, and qualitative differences in approaches and outcomes. By exploring the latest developments in human shape and clothing estimation, this survey aims to provide a comprehensive understanding of the field and inspire future research in this rapidly evolving domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>人体形状和服装估计在在线购物、时尚零售、增强现实 (AR)、虚拟现实 (VR) 和游戏等各个领域都得到了显着的重视。近年来，人体形状和服装的视觉表示已成为计算机视觉研究人员关注的焦点。本文对该领域的主要工作进行了全面的综述，重点关注四个关键方面：人体形状估计、时尚生成、地标检测和属性识别。对于每一项任务，调查论文都检查了最近的进展，讨论了它们的优点和局限性，以及方法和结果的质量差异。通过探索人体形状和服装估计的最新发展，本次调查旨在提供对该领域的全面了解，并启发该快速发展领域的未来研究。</details>
**PDF:** <http://arxiv.org/pdf/2402.18032v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach**<br />
**Title_cn:** 飞行器多基地雷达 RCS 签名识别：贝叶斯融合方法<br />
**Authors:** Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz Erdogmus, Tales Imbiriba<br />
**Abstract:** <details><summary>原文: </summary>Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven drones, correlating target aspect angles to Radar Cross Section (RCS) measurements in an anechoic chamber. Comparing against single radar Automated Target Recognition (ATR) systems and suboptimal fusion methods, our empirical results demonstrate that the OBF method integrated with RBC significantly enhances classification accuracy compared to other fusion methods and single radar configurations.</details>
**Abstract_cn:** <details><summary>译文: </summary>无人机 (UAV) 的雷达自动目标识别 (RATR) 涉及发射电磁波 (EMW) 并根据接收到的雷达回波执行目标类型识别，这对于国防和航空航天应用至关重要。先前的研究强调了 RATR 中多基地雷达配置相对于单基地雷达配置的优势。然而，多基地雷达配置中的融合方法通常在概率上不理想地组合来自各个雷达的分类向量。为了解决这个问题，我们提出了一个完全贝叶斯 RATR 框架，采用最佳贝叶斯融合 (OBF) 来聚合来自多个雷达的分类概率向量。 OBF 基于预期的 0-1 损失，更新目标无人机类型的递归贝叶斯分类 (RBC) 后验分布，以多个时间步长的历史观察结果为条件。我们使用七架无人机的模拟随机游走轨迹来评估该方法，将目标方位角与消声室中的雷达截面（RCS）测量相关联。与单雷达自动目标识别（ATR）系统和次优融合方法相比，我们的实证结果表明，与其他融合方法和单雷达配置相比，与 RBC 集成的 OBF 方法显着提高了分类精度。</details>
**PDF:** <http://arxiv.org/pdf/2402.17987v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks**<br />
**Title_cn:** 通过辅助对抗性防御网络增强跟踪鲁棒性<br />
**Authors:** Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou<br />
**Abstract:** <details><summary>原文: </summary>Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. These attack methods have garnered considerable attention from researchers in recent years. However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking. To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process. DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments. We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability. Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead. We will make our code publicly available soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉对象跟踪中的对抗性攻击通过在图像中引入难以察觉的扰动，显着降低了高级跟踪器的性能。近年来，这些攻击方法引起了研究人员的广泛关注。然而，目前仍然缺乏专门针对视觉目标跟踪设计对抗性防御方法的研究。为了解决这些问题，我们提出了一种名为 DuaLossDef 的有效附加预处理网络，它可以消除跟踪过程中的对抗性扰动。 DuaLossDef 部署在跟踪器的搜索分支或模板分支之前，以将防御变换应用于输入图像。此外，它可以作为即插即用模块与其他视觉跟踪器无缝集成，无需任何参数调整。我们使用对抗性训练来训练 DuaLossDef，特别是使用 Dua-Loss 来生成对抗性样本，同时攻击跟踪器的分类和回归分支。在 OTB100、LaSOT 和 VOT2018 基准上进行的大量实验表明，DuaLossDef 在自适应和非自适应攻击场景中都针对对抗性攻击方法保持了出色的防御鲁棒性。而且，当将防御网络转移到其他跟踪器时，它表现出可靠的可转移性。最后，DuaLossDef 的处理时间高达 5ms/帧，允许与现有高速跟踪器无缝集成，而不会引入大量计算开销。我们将很快公开我们的代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.17976v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments**<br />
**Title_cn:** 从泛化到精确：探索 SAM 在手术环境中的工具分割<br />
**Authors:** Kanyifeechukwu J. Oguine, Roger D. Soberanis-Mukul, Nathan Drenkow, Mathias Unberath<br />
**Abstract:** <details><summary>原文: </summary>Purpose: Accurate tool segmentation is essential in computer-aided procedures. However, this task conveys challenges due to artifacts' presence and the limited training data in medical scenarios. Methods that generalize to unseen data represent an interesting venue, where zero-shot segmentation presents an option to account for data limitation. Initial exploratory works with the Segment Anything Model (SAM) show that bounding-box-based prompting presents notable zero-short generalization. However, point-based prompting leads to a degraded performance that further deteriorates under image corruption. We argue that SAM drastically over-segment images with high corruption levels, resulting in degraded performance when only a single segmentation mask is considered, while the combination of the masks overlapping the object of interest generates an accurate prediction. Method: We use SAM to generate the over-segmented prediction of endoscopic frames. Then, we employ the ground-truth tool mask to analyze the results of SAM when the best single mask is selected as prediction and when all the individual masks overlapping the object of interest are combined to obtain the final predicted mask. We analyze the Endovis18 and Endovis17 instrument segmentation datasets using synthetic corruptions of various strengths and an In-House dataset featuring counterfactually created real-world corruptions. Results: Combining the over-segmented masks contributes to improvements in the IoU. Furthermore, selecting the best single segmentation presents a competitive IoU score for clean images. Conclusions: Combined SAM predictions present improved results and robustness up to a certain corruption level. However, appropriate prompting strategies are fundamental for implementing these models in the medical domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>目的：准确的工具分割对于计算机辅助程序至关重要。然而，由于医疗场景中伪影的存在和有限的训练数据，这项任务带来了挑战。推广到看不见的数据的方法代表了一个有趣的场所，其中零样本分割提供了解决数据限制的选项。分段任意模型 (SAM) 的初步探索性工作表明，基于边界框的提示呈现出显着的零短泛化能力。然而，基于点的提示会导致性能下降，在图像损坏的情况下进一步恶化。我们认为，SAM 严重过度分割具有高损坏级别的图像，导致仅考虑单个分割掩模时性能下降，而与感兴趣对象重叠的掩模组合会生成准确的预测。方法：我们使用 SAM 生成内窥镜帧的过分割预测。然后，我们使用 ground-truth 工具 mask 来分析当选择最佳单个 mask 作为预测时以及当与感兴趣对象重叠的所有单独 mask 组合以获得最终预测 mask 时 SAM 的结果。我们使用各种强度的合成损坏和具有反事实创建的真实世界损坏的内部数据集来分析 Endovis18 和 Endovis17 仪器分割数据集。结果：结合过度分割的掩模有助于改善 IoU。此外，选择最佳的单一分割可以为干净图像提供有竞争力的 IoU 分数。结论：组合 SAM 预测在一定的损坏水平下呈现出改进的结果和鲁棒性。然而，适当的提示策略是在医学领域实施这些模型的基础。</details>
**PDF:** <http://arxiv.org/pdf/2402.17972v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping**<br />
**Title_cn:** 利用稀疏数据进行快速高光谱光热中红外光谱成像，用于妇科癌症组织亚型分析<br />
**Authors:** Reza Reihanisaransari, Chalapathi Charan Gajjela, Xinyu Wu, Ragib Ishrak, Sara Corvigno, Yanping Zhong, Jinsong Liu, Anil K. Sood, David Mayerich, Sebastian Berisha, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Ovarian cancer detection has traditionally relied on a multi-step process that includes biopsy, tissue staining, and morphological analysis by experienced pathologists. While widely practiced, this conventional approach suffers from several drawbacks: it is qualitative, time-intensive, and heavily dependent on the quality of staining. Mid-infrared (MIR) hyperspectral photothermal imaging is a label-free, biochemically quantitative technology that, when combined with machine learning algorithms, can eliminate the need for staining and provide quantitative results comparable to traditional histology. However, this technology is slow. This work presents a novel approach to MIR photothermal imaging that enhances its speed by an order of magnitude. Our method significantly accelerates data collection by capturing a combination of high-resolution and interleaved, lower-resolution infrared band images and applying computational techniques for data interpolation. We effectively minimize data collection requirements by leveraging sparse data acquisition and employing curvelet-based reconstruction algorithms. This method enables the reconstruction of high-quality, high-resolution images from undersampled datasets and achieving a 10X improvement in data acquisition time. We assessed the performance of our sparse imaging methodology using a variety of quantitative metrics, including mean squared error (MSE), structural similarity index (SSIM), and tissue subtype classification accuracies, employing both random forest and convolutional neural network (CNN) models, accompanied by ROC curves. Our statistically robust analysis, based on data from 100 ovarian cancer patient samples and over 65 million data points, demonstrates the method's capability to produce superior image quality and accurately distinguish between different gynecological tissue types with segmentation accuracy exceeding 95%.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统上，卵巢癌检测依赖于多步骤过程，包括由经验丰富的病理学家进行的活检、组织染色和形态学分析。虽然这种传统方法被广泛应用，但存在几个缺点：它是定性的、耗时的，并且严重依赖于染色的质量。中红外（MIR）高光谱光热成像是一种无标记的生化定量技术，与机器学习算法相结合，可以消除染色的需要，并提供与传统组织学相当的定量结果。然而，这项技术进展缓慢。这项工作提出了一种新的中红外光热成像方法，将其速度提高了一个数量级。我们的方法通过捕获高分辨率和交错的低分辨率红外波段图像的组合并应用数据插值计算技术，显着加速了数据收集。我们通过利用稀疏数据采集和采用基于曲线的重建算法，有效地最小化数据收集要求。该方法能够从欠采样数据集重建高质量、高分辨率图像，并将数据采集时间缩短 10 倍。我们使用各种定量指标评估稀疏成像方法的性能，包括均方误差 (MSE)、结构相似性指数 (SSIM) 和组织亚型分类精度，同时采用随机森林和卷积神经网络 (CNN) 模型，并附有ROC曲线。我们基于 100 个卵巢癌患者样本和超过 6500 万个数据点的数据进行统计稳健分析，证明该方法能够产生卓越的图像质量并准确区分不同的妇科组织类型，分割精度超过 95%。</details>
**PDF:** <http://arxiv.org/pdf/2402.17960v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing**<br />
**Title_cn:** 用于多光源白平衡的关注照明分解模型<br />
**Authors:** Dongyoung Kim, Jinwoo Kim, Junsang Yu, Seon Joo Kim<br />
**Abstract:** <details><summary>原文: </summary>White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>许多商用相机中的白平衡 (WB) 算法假设单一且均匀的照明，当场景中存在具有不同色度的多个光源时，会导致不良结果。先前对多光源白平衡的研究通常是在像素级别预测照明，而没有完全掌握场景的实际照明条件，包括光源的数量和颜色。这通常会导致不自然的结果，缺乏整体一致性。为了解决这个问题，我们提出了一种利用槽位注意力的深度白平衡模型，其中每个槽位负责代表各个光源。这种设计使模型能够生成各个光源的色度和权重图，然后将其融合以组成最终的照明图。此外，我们提出了质心匹配损失，它根据颜色范围调节每个槽的激活，从而增强模型以更有效地分离照明。我们的方法在单光源和多光源 WB 基准上实现了最先进的性能，并且还提供了附加信息，例如场景中的光源数量及其色度。此功能允许进行照明编辑，这是现有方法无法实现的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.18277v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus**<br />
**Title_cn:** 用于像差感知散焦深度的自监督空间变异 PSF 估计<br />
**Authors:** Zhuofeng Wu, Yusuke Monno, Masatoshi Okutomi<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们解决了像差感知离焦深度（DfD）的任务，它考虑了真实相机的空间变化点扩散函数（PSF）。为了有效地获得真实相机的空间变化 PSF，而不需要任何地面真实 PSF，我们提出了一种新颖的自监督学习方法，该方法利用一对真实的清晰和模糊图像，可以通过改变相机的光圈设置来轻松捕获这些图像。相机。在我们的 PSF 估计中，我们假设旋转对称 PSF 并引入极坐标系以更准确地学习 PSF 估计网络。我们还处理真实 DfD 情况下发生的焦点呼吸现象。合成数据和真实数据的实验结果证明了我们的方法在 PSF 估计和深度估计方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18175v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **NiteDR: Nighttime Image De-Raining with Cross-View Sensor Cooperative Learning for Dynamic Driving Scenes**<br />
**Title_cn:** NiteDR：夜间图像除雨与交叉视角传感器协作学习动态驾驶场景<br />
**Authors:** Cidan Shi, Lihuang Fang, Han Wu, Xiaoyu Xian, Yukai Shi, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation. Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility. Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios. Additionally, driving assistance systems suffer from reduced capabilities in capturing and discerning the surrounding environment, posing a threat to driving safety. Single-view information captured by single-modal sensors cannot comprehensively depict the entire scene. To address these challenges, we developed an image de-raining framework tailored for rainy nighttime driving scenes. It aims to remove rain artifacts, enrich scene representation, and restore useful information. Specifically, we introduce cooperative learning between visible and infrared images captured by different sensors. By cross-view fusion of these multi-source data, the scene within the images gains richer texture details and enhanced contrast. We constructed an information cleaning module called CleanNet as the first stage of our framework. Moreover, we designed an information fusion module called FusionNet as the second stage to fuse the clean visible images with infrared images. Using this stage-by-stage learning strategy, we obtain de-rained fusion images with higher quality and better visual perception. Extensive experiments demonstrate the effectiveness of our proposed Cross-View Cooperative Learning (CVCL) in adverse driving scenarios in low-light rainy environments. The proposed approach addresses the gap in the utilization of existing rain removal algorithms in specific low-light conditions.</details>
**Abstract_cn:** <details><summary>译文: </summary>在现实环境中，室外成像系统经常受到雨水退化等干扰的影响。尤其是在夜间驾驶场景中，照明不足且不均匀，使场景笼罩在黑暗中，导致图像质量和可视性下降。尤其是在自动驾驶领域，RGB传感器的视觉感知能力在这种恶劣的场景下会出现急剧下降。此外，驾驶辅助系统捕捉和识别周围环境的能力下降，对驾驶安全构成威胁。单模态传感器捕获的单视图信息无法全面描述整个场景。为了应对这些挑战，我们开发了针对夜间雨天驾驶场景的图像去雨框架。它的目的是消除雨水伪影，丰富场景表示，并恢复有用的信息。具体来说，我们引入了不同传感器捕获的可见光和红外图像之间的协作学习。通过这些多源数据的跨视图融合，图像内的场景获得更丰富的纹理细节和增强的对比度。我们构建了一个名为 CleanNet 的信息清理模块作为框架的第一阶段。此外，我们设计了一个名为 FusionNet 的信息融合模块作为第二阶段，将干净的可见光图像与红外图像融合。使用这种逐步学习策略，我们获得了具有更高质量和更好视觉感知的去雨融合图像。大量实验证明了我们提出的跨视图合作学习（CVCL）在弱光雨天环境中不利驾驶场景中的有效性。所提出的方法解决了在特定低光条件下利用现有除雨算法的差距。</details>
**PDF:** <http://arxiv.org/pdf/2402.18172v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging**<br />
**Title_cn:** 被动快照编码孔径双像素 RGB-D 成像<br />
**Authors:** Bhargav Ghanekar, Salman Siddique Khan, Vivek Boominathan, Pranav Sharma, Shreyas Singh, Kaushik Mitra, Ashok Veeraraghavan<br />
**Abstract:** <details><summary>原文: </summary>Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.</details>
**Abstract_cn:** <details><summary>译文: </summary>无源、紧凑、单次 3D 传感在许多应用领域非常有用，例如显微镜、医学​​成像、手术导航和自动驾驶等可能存在外形尺寸、时间和功率限制的应用领域。在短成像距离内、以超紧凑的外形尺寸以及被动快照方式获取 RGB-D 场景信息具有挑战性。双像素 (DP) 传感器是实现这一目标的潜在解决方案。 DP 传感器以两个交错的像素阵列收集来自镜头两个不同半部的光线，从而捕获场景的两个略有不同的视图，就像立体相机系统一样。然而，使用 DP 传感器成像意味着散焦模糊大小与视图之间的视差成正比。这在视差估计与去模糊精度之间产生了权衡。为了改善这种权衡效果，我们提出了CADS（编码孔径双像素传感），其中我们在成像镜头中使用编码孔径以及DP传感器。在我们的方法中，我们在端到端优化设置中共同学习最佳编码模式和重建算法。我们的 CADS 成像系统在各种光圈设置下，与简单的 DP 传感相比，全焦点 (AIF) 估计的 PSNR 提高了 1.5dB 以上，深度估计质量提高了 5-6%。此外，我们还为 DSLR 摄影设置以及内窥镜和皮肤镜外形构建了拟议的 CADS 原型。我们新颖的编码双像素传感方法以被动、快照和紧凑的方式在模拟和现实实验中展示了准确的 RGB-D 重建结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.18102v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs**<br />
**Title_cn:** 从总结到行动：使用开放世界 API 增强复杂任务的大型语言模型<br />
**Authors:** Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, Hang Xu<br />
**Abstract:** <details><summary>原文: </summary>The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类与动物的区别在于人类使用和创造工具的独特能力。工具使人类能够克服生理限制，促进创造伟大的文明。同样，启用具有学习外部工具使用能力的大型语言模型（LLM）等基础模型可能是实现通用人工智能的关键一步。该领域之前的研究主要采用两种不同的方法来增强法学硕士的工具调用能力。第一种方法强调构建用于模型微调的相关数据集。相比之下，第二种方法旨在通过情境学习策略充分利用法学硕士固有的推理能力。在这项工作中，我们引入了一种新颖的工具调用管道，旨在控制大量现实世界的 API。该管道反映了人工任务解决过程，解决了现实生活中复杂的用户查询。在每一步中，我们都会指导法学硕士总结所取得的成果并确定下一步的行动方针。我们将这个管道称为“从摘要到行动”，简称 Sum2Act。我们在 ToolBench 基准上对 Sum2Act 管道进行的实证评估显示出显着的性能改进，优于 ReAct 和 DFSDT 等既定方法。这凸显了 Sum2Act 在增强法学硕士应对复杂现实任务方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18157v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting**<br />
**Title_cn:** 用于以自我为中心的热图到 3D 姿势提升的注意力传播网络<br />
**Authors:** Taeho Kang, Youngki Lee<br />
**Abstract:** <details><summary>原文: </summary>We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\% reduction of error in an MPJPE metric. Our source code is available in GitHub.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 EgoTAP，一种热图到 3D 姿态提升方法，用于高精度立体自我中心 3D 姿态估计。以自我为中心的相机视图中严重的自遮挡和视线外的肢体使得准确的姿势估计成为一个具有挑战性的问题。为了应对这一挑战，先前的方法采用联合热图-身体姿势的概率 2D 表示，但热图到 3D 姿势的转换仍然是一个不准确的过程。我们提出了一种由网格 ViT 编码器和传播网络组成的新颖的热图到 3D 提升方法。 Grid ViT 编码器使用自注意力将联合热图总结为有效的特征嵌入。然后，传播网络利用骨骼信息来估计 3D 姿态，以更好地估计模糊关节的位置。我们的方法在定性和定量上显着优于之前最先进的方法，MPJPE 指标的误差减少了 23.9%。我们的源代码可以在 GitHub 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.18330v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation**<br />
**Title_cn:** SFTformer：用于雷达回波外推的时空相关解耦变压器<br />
**Authors:** Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Fanglong Yao, Xian Sun, Kun Fu<br />
**Abstract:** <details><summary>原文: </summary>Extrapolating future weather radar echoes from past observations is a complex task vital for precipitation nowcasting. The spatial morphology and temporal evolution of radar echoes exhibit a certain degree of correlation, yet they also possess independent characteristics. {Existing methods learn unified spatial and temporal representations in a highly coupled feature space, emphasizing the correlation between spatial and temporal features but neglecting the explicit modeling of their independent characteristics, which may result in mutual interference between them.} To effectively model the spatiotemporal dynamics of radar echoes, we propose a Spatial-Frequency-Temporal correlation-decoupling Transformer (SFTformer). The model leverages stacked multiple SFT-Blocks to not only mine the correlation of the spatiotemporal dynamics of echo cells but also avoid the mutual interference between the temporal modeling and the spatial morphology refinement by decoupling them. Furthermore, inspired by the practice that weather forecast experts effectively review historical echo evolution to make accurate predictions, SFTfomer incorporates a joint training paradigm for historical echo sequence reconstruction and future echo sequence prediction. Experimental results on the HKO-7 dataset and ChinaNorth-2021 dataset demonstrate the superior performance of SFTfomer in short(1h), mid(2h), and long-term(3h) precipitation nowcasting.</details>
**Abstract_cn:** <details><summary>译文: </summary>根据过去的观测推断未来天气雷达回波是一项对于临近降水预报至关重要的复杂任务。雷达回波的空间形态和时间演化表现出一定程度的相关性，但也具有独立的特征。 {现有方法在高度耦合的特征空间中学习统一的时空表示，强调时空特征之间的相关性，但忽略了对其独立特征的显式建模，这可能导致它们之间的相互干扰。}针对雷达回波，我们提出了一种空间-频率-时间相关解耦变换器（SFTformer）。该模型利用堆叠的多个SFT-Blocks不仅挖掘回声单元时空动态的相关性，而且通过解耦避免时间建模和空间形态细化之间的相互干扰。此外，受天气预报专家有效回顾历史回波演化以做出准确预测的实践的启发，SFTfomer 结合了历史回波序列重建和未来回波序列预测的联合训练范例。在HKO-7数据集和ChinaNorth-2021数据集上的实验结果表明，SFTfomer在短（1h）、中（2h）和长期（3h）降水临近预报方面具有优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18044v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Representing 3D sparse map points and lines for camera relocalization**<br />
**Title_cn:** 表示 3D 稀疏地图点和线以进行相机重新定位<br />
**Authors:** Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉定位和地图绘制的最新进展在集成点和线特征方面取得了相当大的成功。然而，扩展本地化框架以包含额外的映射组件经常会导致对专用于匹配任务的内存和计算资源的需求增加。在这项研究中，我们展示了轻量级神经网络如何学习表示 3D 点和线特征，并通过利用多个学习映射的力量来展示领先的姿势准确性。具体来说，我们利用单个转换器块来编码线特征，有效地将它们转换为独特的点状描述符。随后，我们将这些点和线描述符集视为不同但相互关联的特征集。通过在多个图层中集成自注意力和交叉注意力，我们的方法在使用两个简单的 MLP 回归 3D 地图之前有效地细化每个特征。在综合实验中，我们的室内定位结果在基于点和线辅助配置方面均超过了 Hloc 和 Limap。此外，在户外场景中，我们的方法取得了显着的领先优势，标志着与最先进的基于学习的方法相比的最显着的增强。这项工作的源代码和演示视频已公开：https://thpjp.github.io/pl2map/</details>
**PDF:** <http://arxiv.org/pdf/2402.18011v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture**<br />
**Title_cn:** 选择适当的多光谱相机曝光设置和辐射校准方法，用于表型分析和精准农业<br />
**Authors:** Vaishali Swaminathan, J. Alex Thomasson, Robert G. Hardin, Nithya Rajan<br />
**Abstract:** <details><summary>原文: </summary>Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making. The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough. The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings. This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%). The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure. A simulation of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).</details>
**Abstract_cn:** <details><summary>译文: </summary>数据的辐射准确性对于定量精准农业至关重要，可以为建模和决策提供可靠且可重复的数据。曝光时间和增益设置对多光谱图像辐射精度的影响尚未得到充分探索。本研究的目的是确定与默认的自动曝光 (AE) 设置相比，在图像采集过程中采用固定曝光 (FE) 时间是否可以提高图像的辐射精度。这涉及量化自动曝光的误差并确定理想的曝光值，在该值内辐射平均绝对百分比误差 (MAPE) 最小 (< 5%)。结果表明，FE 正射马赛克比 AE 正射马赛克更接近地面实况（较高的 R2 和较低的 MAPE）。确定了捕捉树冠和土壤物体的理想曝光范围，不会因曝光不足而丢失信息或因曝光过度而导致饱和。 AE 误差模拟表明，对于确定的理想范围内的曝光设置，蓝色、绿色、红色和近红外波段的 MAPE < 5%，红边波段的 MAPE < 7%，并且在超出理想曝光上限时呈指数增长。此外，使用两个不同生长季节的植被指数 (VI) 预测植物总氮吸收量 (g/plant) 更接近真实情况（大多数情况下，R2 > 0.40，MAPE = 12 至 14%，p < 0.05）：与 AE 图像的预测相比，使用 FE（大多数情况下，R2 < 0.13，MAPE = 15 至 18%，p >= 0.05）。</details>
**PDF:** <http://arxiv.org/pdf/2402.18553v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Windowed-FourierMixer: Enhancing Clutter-Free Room Modeling with Fourier Transform**<br />
**Title_cn:** Windowed-FourierMixer：通过傅里叶变换增强整洁的房间建模<br />
**Authors:** Bruno Henriques, Benjamin Allaert, Jean-Philippe Vandeborre<br />
**Abstract:** <details><summary>原文: </summary>With the growing demand for immersive digital applications, the need to understand and reconstruct 3D scenes has significantly increased. In this context, inpainting indoor environments from a single image plays a crucial role in modeling the internal structure of interior spaces as it enables the creation of textured and clutter-free reconstructions. While recent methods have shown significant progress in room modeling, they rely on constraining layout estimators to guide the reconstruction process. These methods are highly dependent on the performance of the structure estimator and its generative ability in heavily occluded environments. In response to these issues, we propose an innovative approach based on a U-Former architecture and a new Windowed-FourierMixer block, resulting in a unified, single-phase network capable of effectively handle human-made periodic structures such as indoor spaces. This new architecture proves advantageous for tasks involving indoor scenes where symmetry is prevalent, allowing the model to effectively capture features such as horizon/ceiling height lines and cuboid-shaped rooms. Experiments show the proposed approach outperforms current state-of-the-art methods on the Structured3D dataset demonstrating superior performance in both quantitative metrics and qualitative results. Code and models will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着沉浸式数字应用需求的不断增长，理解和重建 3D 场景的需求显着增加。在这种情况下，从单个图像修复室内环境在建模室内空间的内部结构方面发挥着至关重要的作用，因为它可以创建有纹理且整洁的重建。虽然最近的方法在房间建模方面取得了重大进展，但它们依赖于约束布局估计器来指导重建过程。这些方法高度依赖于结构估计器的性能及其在严重遮挡环境中的生成能力。针对这些问题，我们提出了一种基于 U-Former 架构和新的 Windowed-FourierMixer 模块的创新方法，从而形成一个统一的单相网络，能够有效处理室内空间等人造周期性结构。事实证明，这种新架构对于涉及对称性普遍存在的室内场景的任务是有利的，使模型能够有效地捕捉地平线/天花板高度线和长方体形状的房间等特征。实验表明，所提出的方法在 Structured3D 数据集上优于当前最先进的方法，在定量指标和定性结果方面都表现出卓越的性能。代码和模型将公开。</details>
**PDF:** <http://arxiv.org/pdf/2402.18287v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling**<br />
**Title_cn:** 3DSFLabelling：通过伪自动标记增强 3D 场景流估计<br />
**Authors:** Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, Zhujin Liang, Yi Shan, Dalong Du<br />
**Abstract:** <details><summary>原文: </summary>Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.</details>
**Abstract_cn:** <details><summary>译文: </summary>从 LiDAR 点云学习 3D 场景流存在很大的困难，包括从合成数据集到真实场景的泛化能力差、真实世界 3D 标签的稀缺以及真实稀疏 LiDAR 点云的性能不佳。我们从自动标记的角度提出了一种新颖的方法，旨在为现实世界的 LiDAR 点云生成大量 3D 场景流伪标签。具体来说，我们采用刚体运动的假设来模拟自动驾驶场景中潜在的物体级刚性运动。通过更新多个锚框的不同运动属性，获得整个场景的刚性运动分解。此外，我们还开发了一种新颖的用于全局和局部运动的 3D 场景流数据增强方法。通过基于增强运动参数完美合成目标点云，我们可以轻松获得点云中大量与真实场景高度一致的3D场景流标签。在包括 LiDAR KITTI、nuScenes 和 Argoverse 在内的多个现实数据集上，我们的方法优于之前所有有监督和无监督的方法，无需手动标记。令人印象深刻的是，我们的方法在 LiDAR KITTI 数据集上将 EPE3D 指标降低了十倍，将误差从 $0.190m$ 减少到仅仅 $0.008m$ 。</details>
**PDF:** <http://arxiv.org/pdf/2402.18146v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport**<br />
**Title_cn:** 通过原型最优传输进行无监督跨域图像检索<br />
**Authors:** Bin Li, Ye Shi, Qian Yu, Jingya Wang<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督跨域图像检索（UCIR）旨在检索跨不同域共享同一类别的图像，而不依赖于标记数据。先前的方法通常将 UCIR 问题分解为两个不同的任务：域内表示学习和跨域特征对齐。然而，这些分离的策略忽视了这些任务之间的潜在协同作用。本文介绍了 ProtoOT，这是一种专门为 UCIR 量身定制的新型最优传输公式，它将域内特征表示学习和跨域对齐集成到一个统一的框架中。 ProtoOT 利用 K 均值聚类方法的优势来有效管理 UCIR 固有的分布不平衡。通过利用 K 均值生成初始原型并近似类边缘分布，我们相应地修改了最优传输中的约束，显着提高了其在 UCIR 场景中的性能。此外，我们将对比学习纳入 ProtoOT 框架中，以进一步改进表示学习。这鼓励具有相似语义的特征之间的局部语义一致性，同时还明确地强制特征和不匹配原型之间的分离，从而增强全局辨别力。 ProtoOT 在基准数据集上明显超越了现有的最先进方法。值得注意的是，在 DomainNet 上，ProtoOT 的平均 P@200 提升了 24.44%，在 Office-Home 上，它的 P@15 提升了 12.12%。代码可在 https://github.com/HCVLAB/ProtoOT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18411v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Generalizable Two-Branch Framework for Image Class-Incremental Learning**<br />
**Title_cn:** 图像类增量学习的可推广二分支框架<br />
**Authors:** Chao Wu, Xiaobin Chang, Ruixuan Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep neural networks often severely forget previously learned knowledge when learning new knowledge. Various continual learning (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements.In this paper, a novel two-branch continual learning framework is proposed to further enhance most existing CL methods. Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight convolutional network. The output of each main branch block is modulated by the output of the corresponding side branch block. Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells.Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度神经网络在学习新知识时常常会严重忘记以前学过的知识。人们提出了各种持续学习（CL）方法来从不同角度处理这种灾难性遗忘问题，并取得了实质性的改进。本文提出了一种新颖的两分支持续学习框架，以进一步增强大多数现有的 CL 方法。具体来说，主分支可以是任何现有的 CL 模型，新引入的侧分支是轻量级卷积网络。每个主分支块的输出由相应侧分支块的输出调制。这样一个简单的两分支模型就可以通过普通的优化设置轻松实现和学习，无需任何提示。在多个图像数据集上进行的各种设置的广泛实验表明，所提出的框架比最先进的方法产生了一致的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.18086v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding**<br />
**Title_cn:** IBD：通过图像偏向解码减轻大型视觉语言模型中的幻觉<br />
**Authors:** Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, Jun Liu<br />
**Abstract:** <details><summary>原文: </summary>Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管取得了快速的发展和广泛的应用，大视觉语言模型（LVLM）面临着容易产生幻觉的严峻挑战。过度依赖语言先验已被认为是导致这些幻觉的关键因素。在本文中，我们建议通过引入一种新颖的图像偏向解码（IBD）技术来缓解这个问题。我们的方法通过将传统 LVLM 的预测与基于图像的 LVLM 的预测进行对比，得出下一个标记的概率分布，从而放大与图像内容高度相关的正确信息，同时减轻由于过度依赖文本而导致的幻觉错误。我们进一步进行全面的统计分析以验证我们方法的可靠性，并设计自适应调整策略以在不同条件下实现稳健和灵活的处理。多个评估指标的实验结果验证了我们的方法，尽管不需要额外的训练数据并且只需要最小限度地增加模型参数，就可以显着减少 LVLM 中的幻觉并增强生成响应的真实性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18476v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models**<br />
**Title_cn:** 大视觉语言模型图像推理与描述的认知评估基准<br />
**Authors:** Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen<br />
**Abstract:** <details><summary>原文: </summary>Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.</details>
**Abstract_cn:** <details><summary>译文: </summary>大视觉语言模型（LVLM）尽管最近取得了成功，但其认知能力却很难得到全面测试。受人类认知测试中普遍使用的“Cookie Theft”任务的启发，我们提出了一种新颖的评估基准，使用具有丰富语义的图像来评估 LVLM 的高级认知能力。它定义了八种推理能力，由图像描述任务和视觉问答任务组成。我们对著名的 LVLM 的评估表明，LVLM 与人类的认知能力仍然存在很大差距。</details>
**PDF:** <http://arxiv.org/pdf/2402.18409v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Probabilistic Bayesian optimal experimental design using conditional normalizing flows**<br />
**Title_cn:** 使用条件归一化流的概率贝叶斯最优实验设计<br />
**Authors:** Rafael Orozco, Felix J. Herrmann, Peng Chen<br />
**Abstract:** <details><summary>原文: </summary>Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\times$ 320) parameters at high image resolution, high-dimensional (640 $\times$ 386) observations, and binary mask designs to select the most informative observations.</details>
**Abstract_cn:** <details><summary>译文: </summary>贝叶斯最优实验设计（OED）寻求在预算限制下进行信息最丰富的实验，以将系统的先验知识从贝叶斯框架中的实验数据更新为后验知识。此类问题在计算上具有挑战性，因为（1）对某些最优性标准进行昂贵且重复的评估，通常涉及系统参数和实验数据的双重积分，（2）当系统参数和设计变量是高维的，(3) 如果设计变量是二元的，则优化是组合的并且高度非凸的，通常会导致非鲁棒设计。为了使贝叶斯 OED 问题的解决方案在实际应用中高效、可扩展且稳健，我们提出了一种新颖的联合优化方法。该方法同时执行 (1) 可扩展条件归一化流 (CNF) 的训练，以有效地最大化联合学习实验设计的预期信息增益 (EIG) (2) 使用伯努利分布优化二元实验设计的概率公式。我们展示了我们提出的方法在实际 MRI 数据采集问题中的性能，这是最具挑战性的贝叶斯 OED 问题之一，在高图像分辨率下具有高维（320 $\times$ 320）参数，高维（640 $\ times$ 386）观测值，并采用二元掩模设计来选择信息最丰富的观测值。</details>
**PDF:** <http://arxiv.org/pdf/2402.18337v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Location-guided Head Pose Estimation for Fisheye Image**<br />
**Title_cn:** 鱼眼图像的位置引导头部姿势估计<br />
**Authors:** Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, Dah-Jye Lee<br />
**Abstract:** <details><summary>原文: </summary>Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \textcolor{blue}{a} fisheye-\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>带有鱼眼镜头或超广角镜头的相机可覆盖透视投影无法建模的广阔视野。图像外围区域严重的鱼眼\textcolor{blue}{lens}失真导致在未失真图像上训练的\textcolor{blue}{现有}头部姿势估计模型的性能下降。本文提出了一种头部姿势估计的新方法，该方法利用图像中头部位置的知识来减少鱼眼失真的负面影响。我们开发了一个端到端的卷积神经网络来通过头部姿势和头部位置的多任务学习来估计头部姿势。我们提出的网络直接从鱼眼图像估计头部姿势，无需进行校正或校准操作。我们还为我们的实验创建了三个流行的头部姿势估计数据集 BIWI、300W-LP 和 AFLW2000 的 \textcolor{blue}{a} Fisheye-\textcolor{blue}{ Distorted} 版本。实验结果表明，与其他最先进的一阶段和两阶段方法相比，我们的网络显着提高了头部姿势估计的准确性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18320v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **NERV++: An Enhanced Implicit Neural Video Representation**<br />
**Title_cn:** NERV++：增强的隐式神经视频表示<br />
**Authors:** Ahmed Ghorbel, Wassim Hamidouche, Luce Morin<br />
**Abstract:** <details><summary>原文: </summary>Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint. Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability. Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks. We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation. NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs. This achievement narrows the gap to autoencoder-based video coding, marking a significant stride in INR-based video compression research.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经场，也称为隐式神经表示 (INR)，已显示出表示、生成和操作各种数据类型的卓越能力，允许以低内存占用进行连续数据重建。尽管前景广阔，但应用于视频压缩的 INR 仍需要大幅提高其率失真性能，并且需要大量参数和长时间训练迭代来捕获高频细节，限制了其更广泛的适用性。解决这个问题仍然是一项相当具有挑战性的任务，这将使 INR 在压缩任务中更容易获得。我们通过引入视频神经表示 NeRV++（一种增强的隐式神经视频表示）来解决这些缺点，它比原始 NeRV 解码器架构更直接而有效的增强，具有夹着上采样块的可分离的 conv2d 残差块（SCRB）（ UB），以及用于改进特征表示的双线性插值跳跃层。 NeRV++允许视频直接表示为神经网络近似的函数，并且显着增强了表示能力，超越了当前基于INR的视频编解码器。我们在 UVG、MCL JVC 和 Bunny 数据集上评估我们的方法，在使用 INR 的视频压缩方面取得了有竞争力的结果。这一成果缩小了与基于自动编码器的视频编码的差距，标志着基于 INR 的视频压缩研究取得了重大进步。</details>
**PDF:** <http://arxiv.org/pdf/2402.18305v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Development of Context-Sensitive Formulas to Obtain Constant Luminance Perception for a Foreground Object in Front of Backgrounds of Varying Luminance**<br />
**Title_cn:** 开发上下文相关公式以获得变化亮度背景下前景物体的恒定亮度感知<br />
**Authors:** Ergun Akleman, Bekir Tevfik Akgun, Adil Alpkocak<br />
**Abstract:** <details><summary>原文: </summary>In this article, we present a framework for developing context-sensitive luminance correction formulas that can produce constant luminance perception for foreground objects. Our formulas make the foreground object slightly translucent to mix with the blurred version of the background. This mix can quickly produce any desired illusion of luminance in foreground objects based on the luminance of the background. The translucency formula has only one parameter; the relative size of the foreground object, which is a number between zero and one. We have identified the general structure of the translucency formulas as a power function of the relative size of the foreground object. We have implemented a web-based interactive program in Shadertoy. Using this program, we determined the coefficients of the polynomial exponents of the power function. To intuitively control the coefficients of the polynomial functions, we have used a B\'{e}zier form. Our final translucency formula uses a quadratic polynomial and requires only three coefficients. We also identified a simpler affine formula, which requires only two coefficients. We made our program publicly available in Shadertoy so that anyone can access and improve it. In this article, we also explain how to intuitively change the polynomial part of the formula. Using our explanation, users change the polynomial part of the formula to obtain their own perceptively constant luminance. This can be used as a crowd-sourcing experiment for further improvement of the formula.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一个用于开发上下文相关亮度校正公式的框架，该公式可以为前景对象产生恒定的亮度感知。我们的公式使前景对象稍微半透明，以与背景的模糊版本混合。这种混合可以根据背景的亮度快速在前景对象中产生任何所需的亮度幻觉。半透明度公式只有一个参数；前景对象的相对大小，是 0 到 1 之间的数字。我们已经将半透明度公式的一般结构确定为前景对象相对大小的幂函数。我们在 Shadertoy 中实现了一个基于网络的交互程序。使用该程序，我们确定了幂函数多项式指数的系数。为了直观地控制多项式函数的系数，我们使用了 B\'{e}zier 形式。我们最终的半透明度公式使用二次多项式，仅需要三个系数。我们还确定了一个更简单的仿射公式，它只需要两个系数。我们在 Shadertoy 中公开提供我们的程序，以便任何人都可以访问和改进它。在这篇文章中，我们还解释了如何直观地改变公式的多项式部分。使用我们的解释，用户更改公式的多项式部分以获得他们自己感知的恒定亮度。这可以作为进一步改进公式的众包实验。</details>
**PDF:** <http://arxiv.org/pdf/2402.18288v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Region-Aware Exposure Consistency Network for Mixed Exposure Correction**<br />
**Title_cn:** 用于混合曝光校正的区域感知曝光一致性网络<br />
**Authors:** Jin Liu, Huiyuan Fu, Chuanming Wang, Huadong Ma<br />
**Abstract:** <details><summary>原文: </summary>Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>曝光校正的目的是增强因曝光不当而受到影响的图像，以达到满意的视觉效果。尽管最近取得了进展，但现有方法通常可以减轻输入图像中的过度曝光或曝光不足的问题，并且它们仍然难以处理混合曝光的图像，即一张图像同时包含曝光过度和曝光不足的区域。混合曝光分布不均匀并导致不同的表示，这使得在统一过程中解决具有挑战性。在本文中，我们介绍了一种有效的区域感知曝光校正网络（RECNet），它可以通过自适应学习和桥接不同区域曝光表示来处理混合曝光。具体来说，为了解决混合曝光差异带来的挑战，我们开发了一个区域感知去曝光模块，该模块可以有效地将混合曝光场景的区域特征转换为曝光不变的特征空间。同时，由于去曝光操作不可避免地会减少判别信息，因此我们引入了一种混合尺度恢复单元，该单元集成了曝光不变特征和未处理特征来恢复局部信息。为了进一步实现全局图像中均匀的曝光分布，我们在区域内曝光一致性和区域间曝光连续性的约束下提出了曝光对比正则化策略。在各种数据集上进行了大量的实验，实验结果证明了我们提出的方法的优越性和泛化性。代码发布于：https://github.com/kravrolens/RECNet。</details>
**PDF:** <http://arxiv.org/pdf/2402.18217v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Learning Invariant Inter-pixel Correlations for Superpixel Generation**<br />
**Title_cn:** 学习超像素生成的不变像素间相关性<br />
**Authors:** Sen Xu, Shikui Wei, Tao Ruan, Lixin Liao<br />
**Abstract:** <details><summary>原文: </summary>Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过用可学习的特征替代手工制作的特征，深度超像素算法取得了显着的进步。尽管如此，我们观察到，作为中层表示操作的现有深度超像素方法仍然对训练数据集中嵌入的统计属性（例如颜色分布、高级语义）敏感。因此，可学习特征的判别能力受到限制，导致像素分组性能不理想，特别是在无法训练的应用场景中。为了解决这个问题，我们提出了内容解缠超像素（CDS）算法来选择性地分离不变的像素间相关性和统计属性，即风格噪声。具体来说，我们首先构建与原始 RGB 图像同源但具有显着风格变化的辅助模态。然后，在互信息的驱动下，我们提出跨模态的局部网格相关性对齐，以减少自适应选择的特征的分布差异并学习不变的像素间相关性。然后，我们执行全局式互信息最小化，以强制不变内容和训练数据样式的分离。四个基准数据集的实验结果证明了我们的方法在边界遵守、泛化和效率方面相对于现有最先进方法的优越性。代码和预训练模型可在 https://github.com/rookiie/CDSpixel 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18201v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Misalignment-Robust Frequency Distribution Loss for Image Transformation**<br />
**Title_cn:** 图像变换的失准鲁棒频率分布损失<br />
**Authors:** Zhangkai Ni, Juncheng Wu, Zian Wang, Wenhan Yang, Hanli Wang, Lin Ma<br />
**Abstract:** <details><summary>原文: </summary>This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL</details>
**Abstract_cn:** <details><summary>译文: </summary>本文旨在解决基于深度学习的图像转换方法（例如图像增强和超分辨率）中的常见挑战，这些方法严重依赖于具有像素级对齐的精确对齐的配对数据集。然而，创建精确对齐的配对图像提出了重大挑战，并阻碍了基于此类数据训练的方法的进步。为了克服这一挑战，本文引入了一种新颖且简单的频率分布损失（FDL），用于计算频域内的分布距离。具体来说，我们使用离散傅立叶变换（DFT）将图像特征变换到频域。随后，分别处理频率分量（幅度和相位）以形成 FDL 损失函数。由于频域中全局信息的周到利用，我们的方法被经验证明作为训练约束是有效的。广泛的实验评估（重点关注图像增强和超分辨率任务）表明 FDL 优于现有的失准鲁棒损失函数。此外，我们还探索了 FDL 在仅依赖于完全错位数据的图像风格迁移方面的潜力。我们的代码位于：https://github.com/eezkni/FDL</details>
**PDF:** <http://arxiv.org/pdf/2402.18192v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Reflection Removal Using Recurrent Polarization-to-Polarization Network**<br />
**Title_cn:** 使用循环偏振到偏振网络去除反射<br />
**Authors:** Wenjiao Bian, Yusuke Monno, Masatoshi Okutomi<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses reflection removal, which is the task of separating reflection components from a captured image and deriving the image with only transmission components. Considering that the existence of the reflection changes the polarization state of a scene, some existing methods have exploited polarized images for reflection removal. While these methods apply polarized images as the inputs, they predict the reflection and the transmission directly as non-polarized intensity images. In contrast, we propose a polarization-to-polarization approach that applies polarized images as the inputs and predicts "polarized" reflection and transmission images using two sequential networks to facilitate the separation task by utilizing the interrelated polarization information between the reflection and the transmission. We further adopt a recurrent framework, where the predicted reflection and transmission images are used to iteratively refine each other. Experimental results on a public dataset demonstrate that our method outperforms other state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文讨论了反射去除，即从捕获的图像中分离反射分量并导出仅包含透射分量的图像的任务。考虑到反射的存在会改变场景的偏振状态，现有的一些方法已经利用偏振图像来去除反射。虽然这些方法应用偏振图像作为输入，但它们直接将反射和透射预测为非偏振强度图像。相比之下，我们提出了一种偏振到偏振的方法，该方法应用偏振图像作为输入，并使用两个顺序网络预测“偏振”反射和透射图像，以利用反射和透射之间相互关联的偏振信息来促进分离任务。我们进一步采用循环框架，其中预测的反射和透射图像用于迭代地相互细化。公共数据集上的实验结果表明，我们的方法优于其他最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.18178v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Digging Into Normal Incorporated Stereo Matching**<br />
**Title_cn:** 深入研究正常合并的立体匹配<br />
**Authors:** Zihua Liu, Songyan Zhang, Zhicheng Wang, Masatoshi Okutomi<br />
**Abstract:** <details><summary>原文: </summary>Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, disparity estimation in low-texture, occluded, and bordered regions still remains a bottleneck that limits the performance. To tackle these challenges, geometric guidance like plane information is necessary as it provides intuitive guidance about disparity consistency and affinity similarity. In this paper, we propose a normal incorporated joint learning framework consisting of two specific modules named non-local disparity propagation(NDP) and affinity-aware residual learning(ARL). The estimated normal map is first utilized for calculating a non-local affinity matrix and a non-local offset to perform spatial propagation at the disparity level. To enhance geometric consistency, especially in low-texture regions, the estimated normal map is then leveraged to calculate a local affinity matrix, providing the residual learning with information about where the correction should refer and thus improving the residual learning efficiency. Extensive experiments on several public datasets including Scene Flow, KITTI 2015, and Middlebury 2014 validate the effectiveness of our proposed method. By the time we finished this work, our approach ranked 1st for stereo matching across foreground pixels on the KITTI 2015 dataset and 3rd on the Scene Flow dataset among all the published works.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管基于学习的立体匹配算法取得了显着的进步，但低纹理、遮挡和边框区域的视差估计仍然是限制性能的瓶颈。为了应对这些挑战，像平面信息这样的几何指导是必要的，因为它提供了有关视差一致性和亲和力相似性的直观指导。在本文中，我们提出了一个正常的联合学习框架，由两个特定的模块组成，即非局部视差传播（NDP）和亲和力感知残差学习（ARL）。估计的法线图首先用于计算非局部亲和力矩阵和非局部偏移以在视差级别执行空间传播。为了增强几何一致性，特别是在低纹理区域，然后利用估计的法线图来计算局部亲和力矩阵，为残差学习提供有关校正应该参考的位置的信息，从而提高残差学习效率。对多个公共数据集（包括 Scene Flow、KITTI 2015 和 Middlebury 2014）的广泛实验验证了我们提出的方法的有效性。当我们完成这项工作时，在所有已发表的作品中，我们的方法在 KITTI 2015 数据集上的前景像素立体匹配方面排名第一，在场景流数据集上排名第三。</details>
**PDF:** <http://arxiv.org/pdf/2402.18171v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Boosting Neural Representations for Videos with a Conditional Decoder**<br />
**Title_cn:** 使用条件解码器增强视频的神经表示<br />
**Authors:** Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang, Hongwei Qin, Jun Zhang<br />
**Abstract:** <details><summary>原文: </summary>Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs.</details>
**Abstract_cn:** <details><summary>译文: </summary>隐式神经表示（INR）已成为一种有前途的视频存储和处理方法，在各种视频任务中显示出卓越的多功能性。然而，现有方法通常无法充分利用其表示能力，这主要是由于目标帧解码过程中中间特征对齐不充分。本文介绍了当前隐式视频表示方法的通用增强框架。具体来说，我们利用带有时间感知仿射变换模块的条件解码器，该模块使用帧索引作为先验条件来有效地将中间特征与目标帧对齐。此外，我们引入了一个类似 NeRV 的正弦块来生成不同的中间特征并实现更平衡的参数分布，从而增强模型的容量。通过高频信息保留重建损失，我们的方法成功地提高了视频回归的重建质量和收敛速度的多个基线INR，并表现出优异的修复和插值结果。此外，我们集成了一致的熵最小化技术，并基于这些增强的 INR 开发视频编解码器。 UVG 数据集上的实验证实，我们的增强型编解码器显着优于基线 INR，并且与传统和基于学习的编解码器相比，提供有竞争力的速率失真性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18152v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Learning to Deblur Polarized Images**<br />
**Title_cn:** 学习去模糊偏振图像<br />
**Authors:** Chu Zhou, Minggui Teng, Xinyu Zhou, Chao Xu, Boxin Sh<br />
**Abstract:** <details><summary>原文: </summary>A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.</details>
**Abstract_cn:** <details><summary>译文: </summary>偏振相机可以在一次拍摄中捕获具有不同偏振器角度的四个偏振图像，这在基于偏振的视觉应用中非常有用，因为可以根据捕获的偏振直接计算偏振度 (DoP) 和偏振角 (AoP)图片。然而，由于片上微偏光片阻挡了部分光线，导致传感器通常需要较长的曝光时间，因此捕获的偏光图像容易出现相机抖动引起的运动模糊，导致计算的 DoP 和 AoP 明显下降。传统图像的去模糊方法在处理偏振图像时通常表现出退化的性能，因为它们只关注去模糊而不考虑偏振约束。在本文中，我们提出了一种偏振图像去模糊管道，通过采用分而治之的策略以偏振感知的方式解决该问题，将问题明确地分解为两个不适定的子问题，并设计一个二元问题。阶段神经网络分别处理两个子问题。实验结果表明，我们的方法在合成图像和真实图像上均实现了最先进的性能，并且可以提高基于偏振的视觉应用（例如图像去雾和反射去除）的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18134v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization**<br />
**Title_cn:** 使用多级优化的掩蔽自动编码器中的下游任务引导掩蔽学习<br />
**Authors:** Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie<br />
**Abstract:** <details><summary>原文: </summary>Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: https://github.com/Alexiland/MLOMAE</details>
**Abstract_cn:** <details><summary>译文: </summary>掩码自动编码器（MAE）是视觉表示学习中自监督预训练的一种著名方法。它通过随机屏蔽图像补丁并使用未屏蔽的图像重建这些屏蔽补丁来进行操作。 MAE 的一个关键限制在于它忽略了不同补丁的不同信息量，因为它统一选择要屏蔽的补丁。为了克服这个问题，一些方法提出了基于补丁信息的屏蔽。然而，这些方法通常不考虑下游任务的具体要求，可能导致这些任务的表示不理想。为此，我们引入了多级优化掩码自动编码器（MLO-MAE），这是一种新颖的框架，它利用下游任务的端到端反馈来在预训练期间学习最佳掩码策略。我们的实验结果凸显了 MLO-MAE 在视觉表征学习方面的显着进步。与现有方法相比，它在不同的数据集和任务上表现出了显着的改进，展示了其适应性和效率。我们的代码位于：https://github.com/Alexiland/MLOMAE</details>
**PDF:** <http://arxiv.org/pdf/2402.18128v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment**<br />
**Title_cn:** G4G：具有细粒度模内对齐的高保真说话人脸生成的通用框架<br />
**Authors:** Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu<br />
**Abstract:** <details><summary>原文: </summary>Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G's success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管有大量已完成的研究，但通过与任意音频相对应的高度同步的嘴唇运动来实现高保真说话面部生成仍然是该领域的重大挑战。已发表研究的缺点继续让许多研究人员感到困惑。本文介绍了 G4G，这是一种具有细粒度模内对齐的高保真说话面部生成的通用框架。无论给定的音调或音量如何，G4G 都可以重现原始视频的高保真度，同时产生高度同步的嘴唇运动。 G4G成功的关键在于使用对角矩阵来增强音频图像模内特征的普通对齐，这显着增加了正负样本之间的比较学习。此外，还引入了多尺度监督模块，以全面重现原始视频在面部区域的感知保真度，同时强调嘴唇运动和输入音频的同步。然后使用融合网络进一步融合面部区域和其余区域。我们的实验结果表明，在重现原始视频质量以及高度同步的说话嘴唇方面取得了重大成就。 G4G 是一个性能卓越的通用框架，它可以生成比当前最先进的方法更接近真实水平的谈话视频。</details>
**PDF:** <http://arxiv.org/pdf/2402.18122v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Block and Detail: Scaffolding Sketch-to-Image Generation**<br />
**Title_cn:** 块和细节：脚手架草图到图像的生成<br />
**Authors:** Vishnu Sarukkai, Lu Yuan, Mia Tang, Maneesh Agrawala, Kayvon Fatahalian<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel sketch-to-image tool that aligns with the iterative refinement process of artists. Our tool lets users sketch blocking strokes to coarsely represent the placement and form of objects and detail strokes to refine their shape and silhouettes. We develop a two-pass algorithm for generating high-fidelity images from such sketches at any point in the iterative process. In the first pass we use a ControlNet to generate an image that strictly follows all the strokes (blocking and detail) and in the second pass we add variation by renoising regions surrounding blocking strokes. We also present a dataset generation scheme that, when used to train a ControlNet architecture, allows regions that do not contain strokes to be interpreted as not-yet-specified regions rather than empty space. We show that this partial-sketch-aware ControlNet can generate coherent elements from partial sketches that only contain a small number of strokes. The high-fidelity images produced by our approach serve as scaffolds that can help the user adjust the shape and proportions of objects or add additional elements to the composition. We demonstrate the effectiveness of our approach with a variety of examples and evaluative comparisons.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种新颖的草图到图像工具，该工具与艺术家的迭代细化过程相一致。我们的工具允许用户绘制块笔划以粗略地表示对象的位置和形式，并允许用户绘制细节笔划以细化其形状和轮廓。我们开发了一种两遍算法，用于在迭代过程中的任何点从此类草图生成高保真图像。在第一遍中，我们使用 ControlNet 生成严格遵循所有笔划（块和细节）的图像，在第二遍中，我们通过对块笔划周围的区域进行再噪处理来添加变化。我们还提出了一种数据集生成方案，当用于训练 ControlNet 架构时，允许将不包含笔画的区域解释为尚未指定的区域而不是空白区域。我们证明了这种部分草图感知的 ControlNet 可以从仅包含少量笔画的部分草图生成连贯的元素。我们的方法生成的高保真图像充当支架，可以帮助用户调整对象的形状和比例或向构图添加其他元素。我们通过各种示例和评估比较来证明我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18116v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Six-Point Method for Multi-Camera Systems with Reduced Solution Space**<br />
**Title_cn:** 具有减少解空间的多摄像机系统的六点法<br />
**Authors:** Banglei Guan, Ji Zhao, Laurent Kneip<br />
**Abstract:** <details><summary>原文: </summary>Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of a multi-camera system, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated. Extensive experiments demonstrate that our solvers are more accurate than the state-of-the-art six-point methods, while achieving better performance in efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用点对应（PC）的相对姿态估计是一种广泛使用的技术。通用相机最少需要六台 PC 的配置。在本文中，我们提出了几种使用六台 PC 来计算多相机系统的 6DOF 相对位姿的最小解算器，包括用于广义相机的最小解算器和用于双相机装备实际配置的两个最小解算器。方程构造基于旋转和平移的解耦。旋转由凯莱或四元数参数化表示，平移可以通过使用隐变量技术来消除。当 PC 的子集在两个视图中关联相同的相机时，就会发现并证明光线束约束。这是减少解数和生成数值稳定求解器的关键。此外，还枚举了多摄像机系统六点问题的所有配置。大量的实验表明，我们的求解器比最先进的六点方法更准确，同时在效率方面取得了更好的表现。</details>
**PDF:** <http://arxiv.org/pdf/2402.18066v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Fast and Interpretable 2D Homography Decomposition: Similarity-Kernel-Similarity and Affine-Core-Affine Transformations**<br />
**Title_cn:** 快速且可解释的 2D 单应性分解：相似性-核-相似性和仿射-核心-仿射变换<br />
**Authors:** Shen Cai, Zhanhao Wu, Lingxi Guo, Jiachun Wang, Siyu Zhang, Junchi Yan, Shuhan Shen<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present two fast and interpretable decomposition methods for 2D homography, which are named Similarity-Kernel-Similarity (SKS) and Affine-Core-Affine (ACA) transformations respectively. Under the minimal $4$-point configuration, the first and the last similarity transformations in SKS are computed by two anchor points on target and source planes, respectively. Then, the other two point correspondences can be exploited to compute the middle kernel transformation with only four parameters. Furthermore, ACA uses three anchor points to compute the first and the last affine transformations, followed by computation of the middle core transformation utilizing the other one point correspondence. ACA can compute a homography up to a scale with only $85$ floating-point operations (FLOPs), without even any division operations. Therefore, as a plug-in module, ACA facilitates the traditional feature-based Random Sample Consensus (RANSAC) pipeline, as well as deep homography pipelines estimating $4$-point offsets. In addition to the advantages of geometric parameterization and computational efficiency, SKS and ACA can express each element of homography by a polynomial of input coordinates ($7$th degree to $9$th degree), extend the existing essential Similarity-Affine-Projective (SAP) decomposition and calculate 2D affine transformations in a unified way. Source codes are released in https://github.com/cscvlab/SKS-Homography.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了两种快速且可解释的二维单应性分解方法，分别称为相似性-核相似性（SKS）和仿射-核心-仿射（ACA）变换。在最小 4$ 点配置下，SKS 中的第一个和最后一个相似变换分别由目标平面和源平面上的两个锚点计算。然后，可以利用其他两点对应关系来计算仅具有四个参数的中间核变换。此外，ACA 使用三个锚点来计算第一个和最后一个仿射变换，然后利用另一个点对应关系计算中间核心变换。 ACA 只需 85 美元的浮点运算 (FLOP) 即可计算出最大规模的单应性，甚至不需要任何除法运算。因此，作为一个插件模块，ACA 促进了传统的基于特征的随机样本共识 (RANSAC) 管道，以及估计 4 美元点偏移的深度单应性管道。除了几何参数化和计算效率的优点外，SKS和ACA还可以通过输入坐标的多项式（$7$次到$9$次）来表达单应性的每个元素，扩展了现有本质的相似性仿射射影（SAP ）统一分解并计算二维仿射变换。源代码发布在 https://github.com/cscvlab/SKS-Homography。</details>
**PDF:** <http://arxiv.org/pdf/2402.18008v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction**<br />
**Title_cn:** QN-Mixer：用于稀疏视图 CT 重建的拟牛顿 MLP 混合器模型<br />
**Authors:** Ishak Ayad, Nicolas Larue, Maï K. Nguyen<br />
**Abstract:** <details><summary>原文: </summary>Inverse problems span across diverse fields. In medical contexts, computed tomography (CT) plays a crucial role in reconstructing a patient's internal structure, presenting challenges due to artifacts caused by inherently ill-posed inverse problems. Previous research advanced image quality via post-processing and deep unrolling algorithms but faces challenges, such as extended convergence times with ultra-sparse data. Despite enhancements, resulting images often show significant artifacts, limiting their effectiveness for real-world diagnostic applications. We aim to explore deep second-order unrolling algorithms for solving imaging inverse problems, emphasizing their faster convergence and lower time complexity compared to common first-order methods like gradient descent. In this paper, we introduce QN-Mixer, an algorithm based on the quasi-Newton approach. We use learned parameters through the BFGS algorithm and introduce Incept-Mixer, an efficient neural architecture that serves as a non-local regularization term, capturing long-range dependencies within images. To address the computational demands typically associated with quasi-Newton algorithms that require full Hessian matrix computations, we present a memory-efficient alternative. Our approach intelligently downsamples gradient information, significantly reducing computational requirements while maintaining performance. The approach is validated through experiments on the sparse-view CT problem, involving various datasets and scanning protocols, and is compared with post-processing and deep unrolling state-of-the-art approaches. Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, all while reducing the number of unrolling iterations required.</details>
**Abstract_cn:** <details><summary>译文: </summary>逆问题跨越不同的领域。在医学领域，计算机断层扫描 (CT) 在重建患者内部结构方面发挥着至关重要的作用，但由于固有的不适定逆问题造成的伪影而带来了挑战。先前的研究通过后处理和深度展开算法提高了图像质量，但面临挑战，例如超稀疏数据的收敛时间延长。尽管进行了增强，但生成的图像通常会显示出明显的伪影，从而限制了其在现实世界诊断应用中的有效性。我们的目标是探索用于解决成像逆问题的深度二阶展开算法，强调与梯度下降等常见的一阶方法相比，它们具有更快的收敛速度和更低的时间复杂度。在本文中，我们介绍了 QN-Mixer，一种基于拟牛顿方法的算法。我们通过 BFGS 算法使用学习到的参数，并引入 Incept-Mixer，这是一种高效的神经架构，可用作非局部正则化项，捕获图像内的远程依赖性。为了解决通常与需要完整 Hessian 矩阵计算的拟牛顿算法相关的计算需求，我们提出了一种内存高效的替代方案。我们的方法智能地对梯度信息进行下采样，在保持性能的同时显着降低计算要求。该方法通过稀疏视图 CT 问题的实验进行了验证，涉及各种数据集和扫描协议，并与后处理和深度展开最先进的方法进行了比较。我们的方法优于现有方法，并在 SSIM 和 PSNR 方面实现了最先进的性能，同时减少了所需的展开迭代次数。</details>
**PDF:** <http://arxiv.org/pdf/2402.17951v1><br />
**Code:** null<br />

