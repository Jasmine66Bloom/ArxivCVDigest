## [UPDATED!] **2024-02-29** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models**<br />
**Title_cn:** DistriFusion：高分辨率扩散模型的分布式并行推理<br />
**Authors:** Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na\"{\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at https://github.com/mit-han-lab/distrifuser.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型在合成高质量图像方面取得了巨大成功。然而，由于巨大的计算成本，利用扩散模型生成高分辨率图像仍然具有挑战性，导致交互式应用程序的延迟过高。在本文中，我们建议 DistriFusion 通过利用多个 GPU 的并行性来解决这个问题。我们的方法将模型输入拆分为多个补丁，并将每个补丁分配给 GPU。然而，简单地实现这样的算法会破坏补丁之间的交互并失去保真度，而合并这样的交互将产生巨大的通信开销。为了克服这一困境，我们观察到相邻扩散的输入之间的高度相似性步骤并提出置换补丁并行性，它通过重用前一个时间步骤中预先计算的特征图来利用扩散过程的顺序性质，为当前步骤提供上下文。因此，我们的方法支持异步通信，可以流水线化大量实验表明，我们的方法可以应用于最近的 Stable Diffusion XL，而不会降低质量，并且在 8 台 NVIDIA A100 上比一台 NVIDIA A100 实现高达 6.1$\times$ 的加速。我们的代码可在 https:// 公开获取github.com/mit-han-lab/distrifuser。</details>
**PDF:** <http://arxiv.org/pdf/2402.19481v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Towards Generalizable Tumor Synthesis**<br />
**Title_cn:** 迈向可推广的肿瘤合成<br />
**Authors:** Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou<br />
**Abstract:** <details><summary>原文: </summary>Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.</details>
**Abstract_cn:** <details><summary>译文: </summary>肿瘤合成可以在医学图像中创建人造肿瘤，从而促进用于肿瘤检测和分割的人工智能模型的训练。然而，肿瘤合成的成功取决于创建视觉上真实的肿瘤，这些肿瘤可以推广到多个器官，此外，由此产生的人工智能模型能够检测来自不同领域（例如医院）的图像中的真实肿瘤。本文利用一项重要的观察结果，在广义肿瘤合成方面迈出了一步：早期肿瘤（< 2cm）在计算机断层扫描（CT）中往往具有相似的成像特征，无论它们起源于肝脏、胰腺还是肾脏。我们已经确定，生成式 AI 模型（例如扩散模型）可以创建泛化到一系列器官的真实肿瘤，即使仅对来自一个器官的有限数量的肿瘤示例进行训练。此外，我们还表明，在这些合成肿瘤上训练的人工智能模型可以推广到从 CT 体积中检测和分割真实肿瘤，涵盖广泛的患者人口统计数据、成像方案和医疗设施。</details>
**PDF:** <http://arxiv.org/pdf/2402.19470v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Humanoid Locomotion as Next Token Prediction**<br />
**Title_cn:** 人形运动作为下一个令牌预测<br />
**Authors:** Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik<br />
**Abstract:** <details><summary>原文: </summary>We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们将现实世界的人形控制视为下一个标记预测问题，类似于预测语言中的下一个单词。我们的模型是通过感觉运动轨迹的自回归预测训练的因果变换器。为了考虑数据的多模态性质，我们以模态对齐的方式执行预测，并且对于每个输入标记从相同模态预测下一个标记。这种通用的公式使我们能够利用缺少模式的数据，例如没有动作的视频轨迹。我们根据来自先前神经网络策略、基于模型的控制器、动作捕捉数据和人类 YouTube 视频的一组模拟轨迹来训练我们的模型。我们展示了我们的模型能够让全尺寸的人形机器人零射击地在旧金山行走。即使仅使用 27 小时的步行数据进行训练，我们的模型也可以转移到现实世界，并且可以泛化到训练期间未见过的命令，例如倒退行走。这些发现表明，通过感觉运动轨迹的生成模型来学习具有挑战性的现实世界控制任务是一条有希望的道路。</details>
**PDF:** <http://arxiv.org/pdf/2402.19469v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Listening to the Noise: Blind Denoising with Gibbs Diffusion**<br />
**Title_cn:** 聆听噪音：使用吉布斯扩散进行盲降噪<br />
**Authors:** David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno Régaldo-Saint Blancard<br />
**Abstract:** <details><summary>原文: </summary>In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the diffusion model. We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of "noise" parameters means constraining models of the evolution of the Universe.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，去噪问题与深度生成模型的发展交织在一起。特别是，扩散模型像降噪器一样进行训练，并且它们建模的分布与贝叶斯图像中的降噪先验一致。然而，通过基于扩散的后验采样去噪需要已知噪声水平和协方差，从而防止盲目去噪。我们通过引入吉布斯扩散（GDiff）克服了这一限制，吉布斯扩散是一种解决信号和噪声参数后采样问题的通用方法。假设存在任意参数高斯噪声，我们开发了一种吉布斯算法，该算法从经过训练的条件扩散模型中交替采样步骤，以将信号映射到噪声分布族之前，并使用蒙特卡罗采样器来推断噪声参数。我们的理论分析强调了潜在的陷阱，指导诊断使用，并量化由扩散模型引起的吉布斯平稳分布中的误差。我们展示了我们的方法：1）涉及具有未知幅度和光谱指数的有色噪声的自然图像的盲去噪，以及2）宇宙学问题，即宇宙微波背景数据的分析，其中“噪声”参数的贝叶斯推断意味着约束模型宇宙的演化。</details>
**PDF:** <http://arxiv.org/pdf/2402.19455v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **SeD: Semantic-Aware Discriminator for Image Super-Resolution**<br />
**Title_cn:** SeD：用于图像超分辨率的语义感知鉴别器<br />
**Authors:** Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen<br />
**Abstract:** <details><summary>原文: </summary>Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成对抗网络（GAN）已被广泛用于恢复图像超分辨率（SR）任务中的生动纹理。特别是，利用一个判别器使 SR 网络能够以对抗性训练的方式学习现实世界高质量图像的分布。然而，分布学习过于粗粒度，容易受到虚拟纹理的影响，导致生成结果违反直觉。为了缓解这个问题，我们提出了简单有效的语义感知判别器（表示为 SeD），它鼓励 SR 网络通过引入图像语义作为条件来学习细粒度分布。具体来说，我们的目标是从训练有素的语义提取器中挖掘图像的语义。在不同的语义下，鉴别器能够自适应地单独区分真假图像，从而引导 SR 网络学习更细粒度的语义感知纹理。为了获得准确和丰富的语义，我们充分利用最近流行的具有广泛数据集的预训练视觉模型（PVM），然后通过精心设计的空间交叉注意模块将其语义特征合并到鉴别器中。通过这种方式，我们提出的语义感知鉴别器使 SR 网络能够生成更加逼真和令人愉悦的图像。对两个典型任务（即 SR 和 Real SR）的大量实验证明了我们提出的方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.19387v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Structure Preserving Diffusion Models**<br />
**Title_cn:** 结构保持扩散模型<br />
**Authors:** Haoye Lu, Spencer Szabados, Yaoliang Yu<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，扩散模型已成为领先的分布学习方法。在这里，我们引入了结构保持扩散过程，这是一系列用于学习具有附加结构（例如群对称性）的分布的扩散过程，通过开发扩散过渡步骤保持所述对称性的理论条件。在实现等变数据采样轨迹的同时，我们通过开发一系列能够学习本质上对称的分布的不同对称等变扩散模型来举例说明这些结果。对合成数据集和真实数据集的实证研究用于验证开发的模型是否符合所提出的理论，并且能够在样本平等方面比现有方法实现更高的性能。我们还展示了如何使用所提出的模型来实现理论上保证的等变图像降噪，而无需事先了解图像方向。</details>
**PDF:** <http://arxiv.org/pdf/2402.19369v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation**<br />
**Title_cn:** 通过在线适应的混合潜在扩散模型生成工业缺陷的新方法<br />
**Authors:** Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang<br />
**Abstract:** <details><summary>原文: </summary>Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a "trimap" mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository https://github.com/GrandpaXun242/AdaBLDM.git</details>
**Abstract_cn:** <details><summary>译文: </summary>有效应对工业异常检测 (AD) 的挑战需要提供充足的缺陷样本，而在工业环境中，缺陷样本的稀缺往往会阻碍这一限制。本文介绍了一种新颖的算法，旨在增加缺陷样本，从而提高 AD 性能。该方法针对缺陷样本生成定制了混合潜在扩散模型，采用扩散模型在潜在空间中生成缺陷样本。由“trimap”蒙版和文本提示控制的特征编辑过程可细化生成的样本。图像生成推理过程分为三个阶段：自由扩散阶段、编辑扩散阶段和在线解码器适应阶段。这种复杂的推理策略可产生具有多种模式变化的高质量合成缺陷样本，从而基于增强的训练集显着提高 AD 准确性。具体来说，在广泛认可的 MVTec AD 数据集上，所提出的方法将 AD 的最新 (SOTA) 性能提升了 1.5%、1.9% 和 3.1%，其中 AD 指标 AP、IAP 和 IAP90 ， 分别。这项工作的实现代码可以在GitHub存储库中找到https://github.com/GrandpaXun242/AdaBLDM.git</details>
**PDF:** <http://arxiv.org/pdf/2402.19330v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly**<br />
**Title_cn:** DiffAssemble：用于 2D 和 3D 重组的统一图扩散模型<br />
**Authors:** Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue<br />
**Abstract:** <details><summary>原文: </summary>Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble</details>
**Abstract_cn:** <details><summary>译文: </summary>重组任务在许多领域发挥着基础作用，并且存在多种方法来解决特定的重组问题。在这种情况下，我们假设通用的统一模型可以有效地解决所有问题，而不管输入数据类型（图像、3D 等）如何。我们引入了 DiffAssemble，这是一种基于图神经网络 (GNN) 的架构，它学习使用扩散模型公式来解决重组任务。我们的方法将集合的元素（无论是 2D 块还是 3D 对象片段）视为空间图的节点。通过将噪声引入元素的位置和旋转并迭代地对其进行去噪以重建相干的初始姿态来执行训练。 DiffAssemble 在大多数 2D 和 3D 重组任务中实现了最先进 (SOTA) 的结果，并且是第一个基于学习的方法，可以解决 2D 旋转和平移难题。此外，我们还强调它显着减少了运行时间，其执行速度比最快的基于优化的解谜方法快 11 倍。代码可在 https://github.com/IIT-PAVIS/DiffAssemble 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.19302v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts**<br />
**Title_cn:** 通过小波域损失训练生成图像超分辨率模型可以更好地控制伪影<br />
**Authors:** Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan<br />
**Abstract:** <details><summary>原文: </summary>Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a "good" solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.</details>
**Abstract_cn:** <details><summary>译文: </summary>超分辨率（SR）是一个不适定的反问题，其中与给定的低分辨率图像一致的可行解集的大小非常大。人们已经提出了许多算法来在可行的解决方案中找到“好的”解决方案，从而在保真度和感知质量之间取得平衡。不幸的是，所有已知的方法在尝试重建高频（HF）图像细节时都会产生伪影和幻觉。一个基本问题是：模型能否学会区分真实图像细节和伪影？尽管最近的一些工作侧重于细节和伪影的区分，但这是一个非常具有挑战性的问题，尚未找到令人满意的解决方案。本文表明，与 RGB 域或傅立叶空间损失相比，通过使用小波域损失函数训练基于 GAN 的 SR 模型，可以更好地学习真实 HF 细节与伪影的表征。尽管小波域损失之前已在文献中使用过，但尚未在 SR 任务中使用过。更具体地说，我们仅在 HF 小波子带上而不是在 RGB 图像上训练鉴别器，并且通过小波子带上的保真度损失来训练生成器，以使其对结构的尺度和方向敏感。大量的实验结果表明，我们的模型根据多种客观测量和视觉评估实现了更好的感知失真权衡。</details>
**PDF:** <http://arxiv.org/pdf/2402.19215v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Disentangling representations of retinal images with generative models**<br />
**Title_cn:** 用生成模型解开视网膜图像的表示<br />
**Authors:** Sarah Müller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens<br />
**Abstract:** <details><summary>原文: </summary>Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demonstrate the effectiveness of this novel loss function in disentangling the learned subspaces. Our results show that our model provides a new perspective on the complex relationship between patient attributes and technical confounders in retinal fundus image generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>视网膜眼底图像在眼部疾病的早期检测中发挥着至关重要的作用，最近的研究甚至证明了它们在检测心血管危险因素和神经系统疾病方面的潜力。然而，技术因素对这些图像的影响可能会给眼科中可靠的人工智能应用带来挑战。例如，大型眼底群体常常受到相机类型、图像质量或照明水平等因素的困扰，承担着学习捷径的风险，而不是图像生成过程背后的因果关系。在这里，我们引入了一种新颖的视网膜眼底图像群体模型，该模型有效地将患者属性与相机效果分开，从而实现可控且高度逼真的图像生成。为了实现这一目标，我们提出了一种基于距离相关性的新型解缠结损失。通过定性和定量分析，我们证明了这种新颖的损失函数在解开学习子空间方面的有效性。我们的结果表明，我们的模型为视网膜眼底图像生成中患者属性和技术混杂因素之间的复杂关系提供了新的视角。</details>
**PDF:** <http://arxiv.org/pdf/2402.19186v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach**<br />
**Title_cn:** 用于自动超声心动图视图识别的图卷积神经网络：整体方法<br />
**Authors:** Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein-Arne Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef<br />
**Abstract:** <details><summary>原文: </summary>To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the availability of fully annotated 3D images is limited, we generate synthetic US images from 3D meshes by training an adversarial denoising diffusion model. Experiments were conducted on synthetic and clinical cases for view recognition and structure detection. The approach yielded good performance on synthetic images and, despite being exclusively trained on synthetic data, it already showed potential when applied to clinical images. With this proof-of-concept, we aim to demonstrate the benefits of graphs to improve cardiac view recognition that can ultimately lead to better efficiency in cardiac diagnosis.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了促进心脏超声 (US) 诊断，临床实践建立了多个心脏标准视图，这些视图可作为诊断测量的参考点并定义从中获取图像的视口。自动视图识别涉及将这些图像分组为标准视图类别。尽管深度学习技术已经成功实现了这一目标，但由于心脏结构的正确位置、姿势和潜在闭塞等因素，它们仍然难以充分验证图像对于特定测量的适用性。我们的方法超越了视图分类，并结合了心脏的 3D 网格重建，可以实现更多下游任务，例如分割和姿势估计。在这项工作中，我们探索通过图卷积学习 3D 心脏网格，使用类似的技术来学习自然图像中的 3D 网格，例如人体姿势估计。由于完全注释的 3D 图像的可用性有限，我们通过训练对抗性去噪扩散模型从 ​​3D 网格生成合成 US 图像。在合成和临床案例上进行了视图识别和结构检测的实验。该方法在合成图像上产生了良好的性能，尽管专门针对合成数据进行了训练，但它在应用于临床图像时已经显示出潜力。通过这一概念验证，我们的目标是展示图表在改善心脏视图识别方面的优势，最终提高心脏诊断的效率。</details>
**PDF:** <http://arxiv.org/pdf/2402.19062v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis**<br />
**Title_cn:** WDM：用于高分辨率医学图像合成的 3D 小波扩散模型<br />
**Authors:** Paul Friedrich, Julia Wolleb, Florentin Bieder, Alicia Durrer, Philippe C. Cattin<br />
**Abstract:** <details><summary>原文: </summary>Due to the three-dimensional nature of CT- or MR-scans, generative modeling of medical images is a particularly challenging task. Existing approaches mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit the high-dimensional data into the limited GPU memory. However, these approaches may introduce artifacts and potentially restrict the model's applicability for certain downstream tasks. This work presents WDM, a wavelet-based medical image synthesis framework that applies a diffusion model on wavelet decomposed images. The presented approach is a simple yet effective way of scaling diffusion models to high resolutions and can be trained on a single 40 GB GPU. Experimental results on BraTS and LIDC-IDRI unconditional image generation at a resolution of $128 \times 128 \times 128$ show state-of-the-art image fidelity (FID) and sample diversity (MS-SSIM) scores compared to GANs, Diffusion Models, and Latent Diffusion Models. Our proposed method is the only one capable of generating high-quality images at a resolution of $256 \times 256 \times 256$.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于 CT 或 MR 扫描的三维性质，医学图像的生成建模是一项特别具有挑战性的任务。现有方法大多采用 patch-wise、slice-wise 或级联生成技术来将高维数据放入有限的 GPU 内存中。然而，这些方法可能会引入工件，并可能限制模型对某些下游任务的适用性。这项工作提出了 WDM，一种基于小波的医学图像合成框架，它将扩散模型应用于小波分解图像。所提出的方法是将扩散模型扩展到高分辨率的简单而有效的方法，并且可以在单个 40 GB GPU 上进行训练。分辨率为 $128 × 128 × 128$ 的 BraTS 和 LIDC-IDRI 无条件图像生成的实验结果显示，与 GAN、扩散相比，具有最先进的图像保真度 (FID) 和样本多样性 (MS-SSIM) 分数模型和潜在扩散模型。我们提出的方法是唯一能够生成分辨率为 256 × 256 × 256$ 的高质量图像的方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.19043v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling**<br />
**Title_cn:** 通过内关系和相互关系建模进行剂量预测驱动的放射治疗参数回归<br />
**Authors:** Jiaqi Cui, Yuanyuan Xu, Jianghong Xiao, Yuchen Fei, Jiliu Zhou, Xingcheng Peng, Yan Wang<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps. However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy. To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage. In stage one, we combine transformer and convolutional neural network (CNN) to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression. In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression. Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习通过预测准确的剂量分布图促进了放射治疗的自动化。然而，现有方法无法得出可直接输入治疗计划系统（TPS）的理想放疗参数，阻碍了放疗的完全自动化。为了实现更彻底的自动放疗，在本文中，我们提出了一种新颖的两阶段框架来直接回归放疗参数，包括剂量图预测阶段和放疗参数回归阶段。在第一阶段，我们结合变压器和卷积神经网络（CNN）来预测具有丰富的全局和局部信息的真实剂量图，为后续参数回归提供准确的剂量学知识。在第二阶段，设计了两个复杂的模块，即内部关系建模（Intra-RM）模块和相互关系建模（Inter-RM）模块，以利用器官特定和器官共享的特征来获取精确的参数回归。直肠癌数据集的实验结果证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18879v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence**<br />
**Title_cn:** 增强隐写文本提取：评估 NLP 模型对准确性和语义连贯性的影响<br />
**Authors:** Mingyang Li, Maoqin Yuan, Luyao Li, Han Pengsihua<br />
**Abstract:** <details><summary>原文: </summary>This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.</details>
**Abstract_cn:** <details><summary>译文: </summary>本研究讨论了一种将图像隐写技术与自然语言处理（NLP）大型模型相结合的新方法，旨在提高提取隐写文本的准确性和鲁棒性。传统的最低有效位（LSB）隐写技术在处理复杂的字符编码（例如汉字）时面临信息提取准确性和鲁棒性的挑战。为了解决这个问题，本研究提出了一种创新的 LSB-NLP 混合框架。该框架集成了NLP大型模型的先进能力，如错误检测、纠正和语义一致性分析以及信息重构技术，从而显着增强了隐写文本提取的鲁棒性。实验结果表明，LSB-NLP混合框架在提高隐写文本的提取精度方面表现出色，尤其是在处理汉字方面。该研究结果不仅证实了图像隐写技术与NLP大模型相结合的有效性，而且为信息隐藏领域的研究和应用提出了新的思路。这种跨学科方法的成功实施展示了图像隐写技术与自然语言处理技术相结合在解决复杂信息处理问题方面的巨大潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.18849v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **ViewFusion: Towards Multi-View Consistency via Interpolated Denoising**<br />
**Title_cn:** ViewFusion：通过插值去噪实现多视图一致性<br />
**Authors:** Xianghui Yang, Yan Zuo, Sameera Ramasinghe, Loris Bazzani, Gil Avraham, Anton van den Hengel<br />
**Abstract:** <details><summary>原文: </summary>Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过扩散模型的新颖视图合成已显示出生成多样化和高质量图像的巨大潜力。然而，这些流行方法中图像生成的独立过程给维持多视图一致性带来了挑战。为了解决这个问题，我们引入了 ViewFusion，这是一种新颖的免训练算法，可以无缝集成到现有的预训练扩散模型中。我们的方法采用自回归方法，隐式地利用先前生成的视图作为下一个视图生成的上下文，确保新颖视图生成过程中稳健的多视图一致性。通过通过插值去噪融合已知视图信息的扩散过程，我们的框架成功地将单视图条件模型扩展为在多视图条件设置中工作，而无需任何额外的微调。大量的实验结果证明了 ViewFusion 在生成一致且详细的新颖视图方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18842v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D**<br />
**Title_cn:** 基于文本转3D的分数蒸馏采样的定量评估<br />
**Authors:** Xiaohan Fei, Chethan Parameshwara, Jiawei Mo, Xiaolong Li, Ashwin Swaminathan, CJ Taylor, Paolo Favaro, Stefano Soatto<br />
**Abstract:** <details><summary>原文: </summary>The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于在用于图像生成的预训练扩散模型上使用了分数蒸馏采样 (SDS) 方法，从文本提示创建 3D 内容的生成模型的发展取得了长足的进步。然而，SDS 方法也是一些伪影的根源，例如 Janus 问题、文本提示与生成的 3D 模型之间的错位以及 3D 模型不准确。虽然现有方法严重依赖于通过对有限样本集进行目视检查来对这些工件进行定性评估，但在这项工作中，我们提出了更客观的定量评估指标，我们通过人工评级进行交叉验证，并显示对失败案例的分析SDS 技术。我们通过设计一种新颖的计算高效的基线模型来证明这种分析的有效性，该模型在解决所有上述工件的同时，在所提出的指标上实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18780v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers**<br />
**Title_cn:** Panda-70M：与多个跨模态教师一起为 70M 视频添加字幕<br />
**Authors:** Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>数据和注释的质量限制了下游模型的质量。虽然存在大型文本语料库和图像文本对，但收集高质量的视频文本数据要困难得多。首先，手动标注比较耗时，因为它需要注释者观看整个视频。其次，视频具有时间维度，由堆叠在一起的多个场景组成，并显示多个动作。因此，为了建立具有高质量字幕的视频数据集，我们提出了一种利用多模式输入的自动方法，例如文本视频描述、字幕和单个视频帧。具体来说，我们从公开的 HD-VILA-100M 数据集中精选了 380 万个高分辨率视频。然后，我们将它们分割成语义一致的视频剪辑，并应用多个跨模态教师模型来获取每个视频的字幕。接下来，我们在一个小子集上微调检索模型，其中手动选择每个视频的最佳标题，然后在整个数据集中使用该模型来选择最佳标题作为注释。通过这种方式，我们获得了 7000 万个视频以及高质量的文本字幕。我们将该数据集命名为 Panda-70M。我们展示了所提出的数据集在三个下游任务上的价值：视频字幕、视频和文本检索以及文本驱动的视频生成。根据所提议的数据训练的模型在所有任务的大多数指标上得分明显更高。</details>
**PDF:** <http://arxiv.org/pdf/2402.19479v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **The All-Seeing Project V2: Towards General Relation Comprehension of the Open World**<br />
**Title_cn:** 全视计划V2：迈向开放世界的一般关系理解<br />
**Authors:** Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et.al.<br />
**Abstract:** <details><summary>原文: </summary>We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data. In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at https://github.com/OpenGVLab/all-seeing.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 All-Seeing Project V2：一个新的模型和数据集，旨在理解图像中的对象关系。具体来说，我们提出了 All-Seeing Model V2 (ASMv2)，它将文本生成、对象定位和关系理解的公式集成到关系对话 (ReC) 任务中。利用这个统一的任务，我们的模型不仅在感知和识别图像中的所有对象方面表现出色，而且在掌握它们之间复杂的关系图方面也表现出色，从而减少了多模态大型语言模型（MLLM）经常遇到的关系幻觉。为了促进关系理解方面的 MLLM 的训练和评估，我们创建了第一个高质量的 ReC 数据集（{AS-V2），该数据集与标准指令调优数据的格式保持一致。此外，我们设计了一个新的基准，称为基于循环的关系探测评估（CRPE），用于全面评估 MLLM 的关系理解能力。值得注意的是，我们的 ASMv2 在这个关系感知基准上的整体准确率达到了 52.04，大大超过了 LLaVA-1.5 的 43.14。我们希望我们的工作能够激发更多未来的研究，并为通用人工智能的发展做出贡献。我们的项目发布于 https://github.com/OpenGVLab/all-seeing。</details>
**PDF:** <http://arxiv.org/pdf/2402.19474v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning**<br />
**Title_cn:** TV-TREES：用于神经符号视频推理的多模态蕴涵树<br />
**Authors:** Kate Sanders, Nathaniel Weir, Benjamin Van Durme<br />
**Abstract:** <details><summary>原文: </summary>It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>对电视剪辑等复杂的多模式内容进行问答具有挑战性。部分原因是当前的视频语言模型依赖于单一模态推理，在长输入上的性能较低，并且缺乏互用性。我们提出了 TV-TREES，第一个多模态蕴涵树生成器。 TV-TREES 是一种视频理解方法，通过在视频直接蕴涵的简单前提和更高层次的结论之间生成蕴涵关系树，促进可解释的联合模态推理。然后，我们引入多模态蕴涵树生成的任务来评估此类方法的推理质量。我们的方法在具有挑战性的 TVQA 数据集上的实验结果证明了在完整视频剪辑上的不可解释的、最先进的零样本性能，说明了与黑盒方法的两全其美的对比。</details>
**PDF:** <http://arxiv.org/pdf/2402.19467v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Navigating Hallucinations for Reasoning of Unintentional Activities**<br />
**Title_cn:** 导航幻觉以推理无意识的活动<br />
**Authors:** Shresth Grover, Vibhav Vineet, Yogesh S Rawat<br />
**Abstract:** <details><summary>原文: </summary>In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了一项理解视频中无意识的人类活动的新任务。我们将这个问题形式化为零样本场景下的推理任务，给定一个无意识活动的视频，我们想知道为什么它从有意转变为无意。我们首先评估当前最先进的大型多模态模型在这一推理任务上的有效性，并观察到它们患有幻觉。我们进一步提出了一种新颖的提示技术，称为“思想之梦”（DoT），它允许模型导航幻觉思想以实现更好的推理。为了评估此任务的性能，我们还引入了三种不同的专门指标，旨在量化模型的推理能力。我们在两个不同的数据集（OOP 和 UCF-Crimes）上进行实验，我们的研究结果表明，DOT 提示技术能够优于标准提示，同时最大限度地减少幻觉。</details>
**PDF:** <http://arxiv.org/pdf/2402.19405v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Entity-Aware Multimodal Alignment Framework for News Image Captioning**<br />
**Title_cn:** 用于新闻图像字幕的实体感知多模态对齐框架<br />
**Authors:** Junzhe Zhang, Huixuan Zhang, Xiaojun Wan<br />
**Abstract:** <details><summary>原文: </summary>News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>新闻图像字幕任务是图像字幕任务的一种变体，它需要模型用新闻图像和相关新闻文章生成信息更丰富的字幕。多模态大语言模型近年来发展迅速，在新闻图像字幕任务中具有广阔的前景。然而，根据我们的实验，常见的 MLLM 不擅长在零样本设置下生成实体。在新闻图像字幕数据集上进行简单微调后，它们处理实体信息的能力仍然有限。为了获得更强大的模型来处理多模态实体信息，我们设计了两个多模态实体感知对齐任务和一个对齐框架来对齐模型并生成新闻图像标题。我们的方法在 GoodNews 数据集上的 CIDEr 分数（72.33 -> 86.29）和 NYTimes800k 数据集上的 CIDEr 分数（70.83 -> 85.61）取得了比以前最先进的模型更好的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.19404v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing**<br />
**Title_cn:** 抑制和重新平衡：迈向广义多模态人脸反欺骗<br />
**Authors:** Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong Tang, Alex Kot<br />
**Abstract:** <details><summary>原文: </summary>Face Anti-Spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. With advancements in sensor manufacture and multi-modal learning techniques, many multi-modal FAS approaches have emerged. However, they face challenges in generalizing to unseen attacks and deployment conditions. These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality. To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities. For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients. Besides, we provide the first large-scale benchmark for evaluating multi-modal FAS performance under domain generalization scenarios. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. Source code and protocols will be released on https://github.com/OMGGGGG/mmdg.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸反欺骗 (FAS) 对于保护人脸识别系统免受演示攻击至关重要。随着传感器制造和多模态学习技术的进步，许多多模态 FAS 方法已经出现。然而，他们在泛化到未见过的攻击和部署条件方面面临挑战。这些挑战源于（1）模态不可靠性，深度和红外等一些模态传感器在不同的环境中经历显着的域变化，导致跨模态特征融合期间不可靠信息的传播；（2）模态不平衡，其中训练过度依赖主导模式会阻碍其他模式的融合，从而降低针对仅使用主导模式无法区分的攻击类型的有效性。为了解决模态不可靠性问题，我们提出了不确定性引导交叉适配器（U-Adapter）来识别每种模态中不可靠的检测区域，并抑制不可靠区域对其他模态的影响。对于模态不平衡，我们提出了一种重新平衡模态梯度调制（ReGrad）策略，通过自适应调整梯度来重新平衡所有模态的收敛速度。此外，我们还提供了第一个用于评估领域泛化场景下多模式 FAS 性能的大规模基准。大量的实验表明我们的方法优于最先进的方法。源代码和协议将在 https://github.com/OMGGGGGG/mmdg 上​​发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.19298v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Modular Blind Video Quality Assessment**<br />
**Title_cn:** 模块化盲视频质量评估<br />
**Authors:** Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, Kede Ma<br />
**Abstract:** <details><summary>原文: </summary>Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model, and a method of training it to improve its modularity. Specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods. Furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities. Last, our BVQA model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers.</details>
**Abstract_cn:** <details><summary>译文: </summary>盲视频质量评估 (BVQA) 在评估和改善各种基于视频的平台和服务的最终用户的观看体验方面发挥着关键作用。当代基于深度学习的模型主要以积极下采样的格式分析视频内容，而忽视了实际空间分辨率和帧速率对视频质量的影响。在本文中，我们提出了一种模块化 BVQA 模型，以及一种训练它以提高其模块化性的方法。具体来说，我们的模型包括基本质量预测器、空间整流器和时间整流器，分别响应视频质量的视觉内容和失真、空间分辨率和帧速率变化。在训练过程中，空间和时间整流器会以一定的概率被丢弃，以使基本质量预测器成为独立的 BVQA 模型，该模型应该与整流器一起更好地工作。对专业生成的内容和用户生成的内容视频数据库的大量实验表明，我们的质量模型实现了优于当前方法或可比的性能。此外，我们模型的模块化提供了一个很好的机会来分析现有视频质量数据库的空间和时间复杂性。最后，我们的 BVQA 模型可以经济高效地添加其他与质量相关的视频属性，例如动态范围和色域作为额外的整流器。</details>
**PDF:** <http://arxiv.org/pdf/2402.19276v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition**<br />
**Title_cn:** MaskFi：用于多模态人类活动识别的 WiFi 和视觉表示的无监督学习<br />
**Authors:** Jianfei Yang, Shijie Tang, Yuecong Xu, Yunjiao Zhou, Lihua Xie<br />
**Abstract:** <details><summary>原文: </summary>Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming. Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality. Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect. In this paper, we propose a novel unsupervised multimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training. We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in representation learning. Benefiting from our unsupervised learning procedure, the network requires only a small amount of annotated data for finetuning and can adapt to the new environment with better performance. We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类活动识别（HAR）在医疗保健、安全监控和元宇宙游戏等各个领域发挥着越来越重要的作用。尽管已经开发出许多基于计算机视觉的 HAR 方法并显示出突出的性能，但它们在不利的视觉条件（特别是低照度）下仍然存在鲁棒性差的问题，这促使基于 WiFi 的 HAR 成为一种良好的补充模式。使用 WiFi 和视觉模式的现有解决方案依赖于收集起来非常麻烦的大量标记数据。在本文中，我们提出了一种新颖的无监督多模式 HAR 解决方案 MaskFi，它仅利用未标记的视频和 WiFi 活动数据进行模型训练。我们提出了一种新算法，即屏蔽 WiFi 视觉建模 (MI2M)，该算法使模型能够通过预测表示学习中的屏蔽部分来学习跨模态和单模态特征。受益于我们的无监督学习过程，网络只需要少量的注释数据进行微调，并且可以以更好的性能适应新环境。我们对内部收集的两个 WiFi 视觉数据集进行了广泛的实验，我们的方法在鲁棒性和准确性方面实现了人类活动识别和人类身份识别。</details>
**PDF:** <http://arxiv.org/pdf/2402.19258v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts**<br />
**Title_cn:** 大型多模式模型中的印刷攻击可以通过提供更多信息的提示来缓解<br />
**Authors:** Hao Cheng, Erjia Xiao, Renjing Xu<br />
**Abstract:** <details><summary>原文: </summary>Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform amazing emergent abilities on various multimodal tasks in the joint space of vision and language. However, the Typographic Attack, which shows disruption to VLMs, has also been certified as a security vulnerability to LMMs. In this work, we first comprehensively investigate the distractibility of LMMs by typography. In particular, we introduce the Typographic Dataset designed to evaluate distractibility across various multi-modal subtasks, such as object recognition, visual attributes detection, enumeration, arithmetic computation, and commonsense reasoning. To further study the effect of typographic patterns on performance, we also scrutinize the effect of tuning various typographic factors, encompassing font size, color, opacity, and spatial positioning of typos. We discover that LMMs can partially distinguish visual contents and typos when confronting typographic attacks, which suggests that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images. Inspired by such phenomena, we demonstrate that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images. Furthermore, we also prove that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos. Finally, we propose a prompt information enhancement method that can effectively mitigate the effects of typography.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型多模态模型（LMM）依靠预先训练的视觉语言模型（VLM）和大型语言模型（LLM）在视觉和语言联合空间中的各种多模态任务上表现出惊人的涌现能力。然而，显示对 VLM 造成破坏的印刷攻击也已被认证为 LMM 的安全漏洞。在这项工作中，我们首先通过排版全面研究 LMM 的分散性。特别是，我们引入了印刷数据集，旨在评估各种多模式子任务的分散性，例如对象识别、视觉属性检测、枚举、算术计算和常识推理。为了进一步研究印刷图案对性能的影响，我们还仔细研究了调整各种印刷因素的影响，包括字体大小、颜色、不透明度和印刷错误的空间定位。我们发现 LMM 在面对排版攻击时可以部分地区分视觉内容和拼写错误，这表明视觉编码器的嵌入包含足够的信息来区分图像中的视觉内容和拼写错误。受此类现象的启发，我们证明，通过提供更多信息性文本来匹配图像，可以显着提高 CLIP 对充满拼写错误的图像进行零样本分类的性能。此外，我们还证明 LMM 可以利用信息更丰富的提示来利用嵌入中的信息来区分视觉内容和拼写错误。最后，我们提出了一种提示信息增强方法，可以有效减轻排版的影响。</details>
**PDF:** <http://arxiv.org/pdf/2402.19150v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models**<br />
**Title_cn:** 通过大型视觉语言模型中的对比学习增强视觉文档理解<br />
**Authors:** Xin Li, Yunfei Wu, Xinghua Jiang, Zhihao Guo, Mingming Gong, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun<br />
**Abstract:** <details><summary>原文: </summary>Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，大型视觉语言模型（LVLM）的出现受到了各个领域越来越多的关注，特别是在视觉文档理解（VDU）领域。与传统的视觉语言任务不同，VDU 特别关注包含丰富文档元素的文本丰富场景。然而，细粒度特征的重要性在 LVLM 社区中很大程度上仍未得到探索，导致在文本丰富的场景中性能不佳。在本文中，我们将其缩写为细粒度特征崩溃问题。为了填补这一空白，我们提出了一种对比学习框架，称为文档对象对比学习（DoCo），专门为 VDU 的下游任务量身定制。 DoCo利用辅助多模态编码器来获取文档对象的特征，并将其与LVLM视觉编码器生成的视觉特征对齐，从而增强了文本丰富场景中的视觉表示。它可以表示视觉整体表示与文档对象的多模态细粒度特征之间的对比学习可以帮助视觉编码器获取更有效的视觉线索，从而增强 LVLM 中文本丰富文档的理解。我们还证明了所提出的 DoCo 作为一种即插即用的预训练方法，可用于各种 LVLM 的预训练，而不会在推理过程中增加计算复杂性。 VDU 多个基准的大量实验结果表明，配备我们提出的 DoCo 的 LVLM 可以实现卓越的性能，并缩小 VDU 和通用视觉语言任务之间的差距。</details>
**PDF:** <http://arxiv.org/pdf/2402.19014v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction**<br />
**Title_cn:** GoalNet：面向目标区域的行人轨迹预测<br />
**Authors:** Ching-Lin Lee, Zhi-Xuan Wang, Kuan-Ting Lai, Amar Fadillah<br />
**Abstract:** <details><summary>原文: </summary>Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the "goals" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian's trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>预测道路上行人的未来轨迹是自动驾驶的一项重要任务。行人轨迹预测受到场景路径、行人意图和决策的影响，是一个多模态问题。最近的大多数研究使用过去的轨迹来预测各种潜在的未来轨迹分布，这没有考虑场景背景和行人目标。我们建议首先使用场景上下文和观察到的轨迹来预测目标点，然后重用目标点来预测未来轨迹，而不是直接预测未来轨迹。通过利用场景上下文和观察到的轨迹的信息，可以将不确定性限制在几个目标区域，这些目标区域代表行人的“目标”。在本文中，我们提出了 GoalNet，一种基于行人目标区域的新轨迹预测神经网络。我们的网络可以预测行人的轨迹和边界框。整体模型高效、模块化，其输出可以根据使用场景而改变。实验结果表明，GoalNet 在 JAAD 上将之前的最先进性能显着提高了 48.7%，在 PIE 数据集上显着提高了 40.8%。</details>
**PDF:** <http://arxiv.org/pdf/2402.19002v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition**<br />
**Title_cn:** 感知、聊天，然后适应：开放世界视频识别基础模型的多模态知识转移<br />
**Authors:** Boyu Chen, Siran Chen, Kunchang Li, Qinglin Xu, Yu Qiao, Yali Wang<br />
**Abstract:** <details><summary>原文: </summary>Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>开放世界的视频识别具有挑战性，因为传统网络不能很好地适应复杂的环境变化。或者，具有丰富知识的基础模型最近显示了它们的泛化能力。然而，如何应用这些知识进行开放世界视频识别尚未得到充分探索。为此，我们提出了一个通用的知识转移管道，它逐步利用和集成基础模型中的外部多模态知识，以促进开放世界的视频识别。我们将其命名为 PCA，基于 Percept、Chat 和 Adapt 三个阶段。首先，我们执行感知过程以减少视频域间隙并获取外部视觉知识。其次，我们在聊天阶段生成丰富的语言语义作为外部文本知识。最后，我们在适应阶段混合外部多模态知识，通过将多模态知识适应模块插入网络中。我们对三个具有挑战性的开放世界视频基准测试（TinyVIRAT、ARID 和 QV-Pipe）进行了广泛的实验。我们的方法在所有三个数据集上都实现了最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18951v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration**<br />
**Title_cn:** 用于可变形多模态医学图像配准的模态不可知结构图像表示学习<br />
**Authors:** Tony C. W. Mok, Zi Li, Yunhao Bai, Jianpeng Zhang, Wei Liu, Yan-Jie Zhou, Ke Yan, Dakai Jin, Yu Shi, Xiaoli Yin, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations. However, the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in multimodal scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities. In this paper, we propose a modality-agnostic structural representation learning method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware contrastive learning to learn discriminative and contrast-invariance deep structural image representations (DSIR) without the need for anatomical delineations or pre-aligned training images. We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration. Comprehensive results demonstrate that our method is superior to the conventional local structural representation and statistical-based similarity measures in terms of discriminability and accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于众多医学图像分析研究和图像引导放射治疗来说，在不同的成像模式之间建立密集的解剖对应关系是一个基础但具有挑战性的过程。现有的多模态图像配准算法依赖于基于统计的相似性度量或局部结构图像表示。然而，前者对局部变化的噪声敏感，而后者的辨别力不足以应对多模态扫描中的复杂解剖结构，导致在确定不同模态扫描之间的解剖对应关系时产生模糊性。在本文中，我们提出了一种与模态无关的结构表示学习方法，该方法利用深度邻域自相似性（DNS）和解剖感知对比学习来学习判别性和对比度不变的深层结构图像表示（DSIR），而无需解剖学轮廓或预先对齐的训练图像。我们在多相 CT、腹部 MR-CT 和脑部 MR T1w-T2w 配准上评估我们的方法。综合结果表明，我们的方法在可辨别性和准确性方面优于传统的局部结构表示和基于统计的相似性度量。</details>
**PDF:** <http://arxiv.org/pdf/2402.18933v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Aligning Knowledge Graph with Visual Perception for Object-goal Navigation**<br />
**Title_cn:** 将知识图与视觉感知相结合以实现对象目标导航<br />
**Authors:** Nuo Xu, Wen Wang, Rong Yang, Mengjie Qin, Zheyuan Lin, Wei Song, Chunlong Zhang, Jason Gu, Chao Li<br />
**Abstract:** <details><summary>原文: </summary>Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. Code available: https://github.com/nuoxu/AKGVP.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象目标导航是一项具有挑战性的任务，需要根据第一人称视觉观察引导智能体到达特定对象。智能体理解周围环境的能力对于成功找到目标起着至关重要的作用。然而，现有的基于知识图的导航器通常依赖于离散分类单热向量和计票策略来构建场景的图形表示，这会导致与视觉图像的不一致。为了提供更准确和连贯的场景描述并解决这种错位问题，我们提出了用于对象目标导航的知识图与视觉感知对齐（AKGVP）方法。从技术上讲，我们的方法引入了分层场景架构的连续建模，并利用视觉语言预训练来使自然语言描述与视觉感知保持一致。连续知识图架构和多模态特征对齐的集成使导航器具有卓越的零样本导航能力。我们使用 AI2-THOR 模拟器广泛评估我们的方法，并进行一系列实验来证明我们的导航器的有效性和效率。可用代码：https://github.com/nuoxu/AKGVP。</details>
**PDF:** <http://arxiv.org/pdf/2402.18892v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition**<br />
**Title_cn:** T3DNet：压缩点云模型以实现轻量级 3D 识别<br />
**Authors:** Zhiyuan Yang, Yunjiao Zhou, Lihua Xie, Jianfei Yang<br />
**Abstract:** <details><summary>原文: </summary>3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices. However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency. There has been a lack of research on how to compress 3D point cloud models into lightweight models. In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and disTillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques such as pruning or quantization, we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D点云已广泛应用于许多移动应用场景，包括自动驾驶和移动设备上的3D传感。然而，现有的 3D 点云模型往往庞大且笨重，由于内存要求高且非实时延迟，使其难以部署在边缘设备上。如何将3D点云模型压缩为轻量级模型一直缺乏研究。在本文中，我们提出了一种称为 T3DNet（具有增强和蒸馏功能的微型 3D 网络）的方法来解决这个问题。我们发现网络增强后的微小模型对于教师来说更容易提炼。我们没有通过剪枝或量化等技术逐渐减少参数，而是预先定义一个微小模型，并通过增强网络和原始模型的辅助监督来提高其性能。我们在几个公共数据集上评估我们的方法，包括 ModelNet40、ShapeNet 和 ScanObjectNN。我们的方法可以在不显着牺牲精度的情况下实现高压缩率，相对于现有方法在三个数据集上实现最先进的性能。令人惊讶的是，我们的 T3DNet 比原始模型小 58 倍，速度快 54 倍，但在 ModelNet40 数据集上的准确率仅下降 1.4%。</details>
**PDF:** <http://arxiv.org/pdf/2402.19264v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Trajectory Consistency Distillation**<br />
**Title_cn:** 轨迹一致性蒸馏<br />
**Authors:** Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, Tat-Jen Cham<br />
**Abstract:** <details><summary>原文: </summary>Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency distillation technique to achieve impressive performance in accelerating text-to-image synthesis. However, we observed that LCM struggles to generate images with both clarity and detailed intricacy. To address this limitation, we initially delve into and elucidate the underlying causes. Our investigation identifies that the primary issue stems from errors in three distinct areas. Consequently, we introduce Trajectory Consistency Distillation (TCD), which encompasses trajectory consistency function and strategic stochastic sampling. The trajectory consistency function diminishes the distillation errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE. Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model. Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs.</details>
**Abstract_cn:** <details><summary>译文: </summary>潜在一致性模型（LCM）将一致性模型扩展到潜在空间，并利用引导一致性蒸馏技术在加速文本到图像合成方面取得令人印象深刻的性能。然而，我们观察到 LCM 很难生成既清晰又详细的图像。为了解决这一限制，我们首先深入研究并阐明根本原因。我们的调查发现，主要问题源于三个不同领域的错误。因此，我们引入了轨迹一致性蒸馏（TCD），它包含轨迹一致性函数和策略随机采样。轨迹一致性函数通过扩大自洽边界条件的范围并赋予 TCD 精确追踪概率流 ODE 整个轨迹的能力来减少蒸馏误差。此外，战略随机抽样专门设计用于规避多步一致性抽样中固有的累积误差，该抽样经过精心定制以补充 TCD 模型。实验表明，TCD 不仅在低 NFE 下显着提高了图像质量，而且在高 NFE 下与教师模型相比还产生了更详细的结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.19159v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Weakly Supervised Monocular 3D Detection with a Single-View Image**<br />
**Title_cn:** 使用单视图图像的弱监督单目 3D 检测<br />
**Authors:** Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu<br />
**Abstract:** <details><summary>原文: </summary>Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>单目 3D 检测 (M3D) 旨在从单视图图像中精确定位 3D 对象，这通常涉及 3D 检测框的劳动密集型注释。最近研究了弱监督 M3D，通过利用许多现有的 2D 注释来消除 3D 注释过程，但它通常需要额外的训练数据，例如 LiDAR 点云或多视图图像，这大大降低了其在各种应用中的适用性和可用性。我们提出了 SKD-WM3D，这是一种弱监督的单目 3D 检测框架，它利用深度信息仅通过单视图图像实现 M3D，无需任何 3D 注释或其他训练数据。 SKD-WM3D 的一个关键设计是自知识蒸馏框架，它通过融合深度信息将图像特征转换为类似 3D 的表示，并有效减轻单目场景中固有的深度模糊性，而推理中的计算开销很小。此外，我们设计了一种不确定性感知蒸馏损失和一种梯度目标转移调制策略，分别促进知识获取和知识转移。大量实验表明 SKD-WM3D 明显超越了最先进的方法，甚至与许多完全监督的方法相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.19144v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation**<br />
**Title_cn:** 基于运动注意机制和帧级自蒸馏的连续手语识别<br />
**Authors:** Qidan Zhu, Jing Li, Fei Yuan, Quan Gan<br />
**Abstract:** <details><summary>原文: </summary>Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression, and obtain a dynamic representation of image changes. And for the first time, we apply the self-distillation method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level Self-Distillation (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.</details>
**Abstract_cn:** <details><summary>译文: </summary>面部表情、头部运动、身体运动和手势运动的变化是手语识别中的显着线索，目前大多数连续手语识别（CSLR）研究方法主要集中在视频序列中的静态图像的帧级特征提取上阶段，而忽略了图像的动态变化。在本文中，我们提出了一种新颖的运动注意机制来捕获手语表达过程中局部运动区域的扭曲变化，并获得图像变化的动态表示。并且首次将自蒸馏方法应用于连续手语的帧级特征提取，通过自蒸馏相邻阶段的特征并使用高阶特征，在不增加计算资源的情况下改进了特征表达作为教师引导低阶功能。两者的结合构成了我们提出的基于运动注意机制和帧级自蒸馏的CSLR整体模型（MAM-FSD），提高了模型的推理能力和鲁棒性。我们在三个公开的数据集上进行了实验，实验结果表明我们提出的方法可以有效地提取视频中的手语运动信息，提高CSLR的准确性并达到state-of-the-art水平。</details>
**PDF:** <http://arxiv.org/pdf/2402.19118v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness**<br />
**Title_cn:** FlatNAS：优化神经架构搜索中的平坦度以实现分布外的鲁棒性<br />
**Authors:** Matteo Gambella, Fabrizio Pittorino, Manuel Roveri<br />
**Abstract:** <details><summary>原文: </summary>Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning. FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration. The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular benchmark datasets in the literature.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经架构搜索（NAS）为神经网络（NN）架构的自动定义铺平了道路，吸引了越来越多的研究关注并提供了各种场景的解决方案。本研究介绍了一种新颖的 NAS 解决方案，称为平面神经架构搜索 (FlatNAS)，该解决方案探索了基于权重扰动鲁棒性的新颖品质因数与采用锐度感知最小化 (SAM) 的单神经网络优化之间的相互作用。 FlatNAS 是文献中第一个在 NAS 过程中系统地探索 NN 损失景观中平坦区域的工作，同时联合优化它们在分布内数据上的性能、分布外 (OOD) 鲁棒性以及约束数量其架构中的参数。与当前主要关注 OOD 算法的研究不同，FlatNAS 成功评估了 NN 架构对 OOD 鲁棒性的影响，这是机器和深度学习实际应用中的一个关键方面。 FlatNAS 通过在 NAS 探索中仅使用分布数据，在性能、OOD 泛化和参数数量之间实现了良好的权衡。 NAS 设计的模型的 OOD 鲁棒性通过关注输入数据损坏的鲁棒性进行评估，并使用文献中流行的基准数据集。</details>
**PDF:** <http://arxiv.org/pdf/2402.19102v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets**<br />
**Title_cn:** 具有多目标优化和量化重建偏移的可变速率学习图像压缩<br />
**Authors:** Fatih Kamisli, Fabien Racape, Hyomin Choi<br />
**Abstract:** <details><summary>原文: </summary>Achieving successful variable bitrate compression with computationally simple algorithms from a single end-to-end learned image or video compression model remains a challenge. Many approaches have been proposed, including conditional auto-encoders, channel-adaptive gains for the latent tensor or uniformly quantizing all elements of the latent tensor. This paper follows the traditional approach to vary a single quantization step size to perform uniform quantization of all latent tensor elements. However, three modifications are proposed to improve the variable rate compression performance. First, multi objective optimization is used for (post) training. Second, a quantization-reconstruction offset is introduced into the quantization operation. Third, variable rate quantization is used also for the hyper latent. All these modifications can be made on a pre-trained single-rate compression model by performing post training. The algorithms are implemented into three well-known image compression models and the achieved variable rate compression results indicate negligible or minimal compression performance loss compared to training multiple models. (Codes will be shared at https://github.com/InterDigitalInc/CompressAI)</details>
**Abstract_cn:** <details><summary>译文: </summary>使用计算简单的算法从单个端到端学习的图像或视频压缩模型成功实现可变比特率压缩仍然是一个挑战。已经提出了许多方法，包括条件自动编码器、潜在张量的通道自适应增益或均匀量化潜在张量的所有元素。本文遵循传统方法来改变单个量化步长，以对所有潜在张量元素进行均匀量化。然而，提出了三种修改来提高可变率压缩性能。首先，多目标优化用于（后）训练。其次，将量化重建偏移引入到量化操作中。第三，可变速率量化也用于超潜伏。所有这些修改都可以通过执行后训练在预训练的单速率压缩模型上进行。这些算法被实施到三个众所周知的图像压缩模型中，所获得的可变速率压缩结果表明与训练多个模型相比，压缩性能损失可以忽略不计或最小。 （代码将在 https://github.com/InterDigitalInc/CompressAI 共享）</details>
**PDF:** <http://arxiv.org/pdf/2402.18930v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **SeMoLi: What Moves Together Belongs Together**<br />
**Title_cn:** SeMoLi：一起移动的就属于一起<br />
**Authors:** Jenny Seidenschwarz, Aljoša Ošep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixé<br />
**Abstract:** <details><summary>原文: </summary>We tackle semi-supervised object detection based on motion cues. Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision. We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns. Revisiting correlation clustering in the context of message passing networks, we learn to group those motion patterns to cluster points to object instances. By estimating the full extent of the objects, we obtain per-scan 3D bounding boxes that we use to supervise a Lidar object detection network. Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train object detectors across datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们解决基于运动线索的半监督对象检测。最近的结果表明，基于启发式的聚类方法与对象跟踪器相结合，可用于伪标记移动对象的实例，并将其用作监督信号，在激光雷达数据中训练 3D 对象检测器，而无需人工监督。我们重新思考这种方法，并建议对象检测以及运动启发的伪标记都可以通过数据驱动的方式来解决。我们利用场景流估计的最新进展来获取点轨迹，从中提取长期的、与类别无关的运动模式。在消息传递网络的背景下重新审视相关聚类，我们学习将这些运动模式分组以将点聚类到对象实例。通过估计对象的完整范围，我们获得每次扫描的 3D 边界框，用于监督激光雷达对象检测网络。我们的方法不仅优于之前基于启发式的方法（57.5 AP，比之前的工作提高了 14 倍），更重要的是，我们表明我们可以跨数据集伪标记和训练对象检测器。</details>
**PDF:** <http://arxiv.org/pdf/2402.19463v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?**<br />
**Title_cn:** 在交互式分割中利用人工智能预测和专家修订注释：持续调整还是全面培训？<br />
**Authors:** Tiezheng Zhang, Xiaoxi Chen, Chongyu Qu, Alan Yuille, Zongwei Zhou<br />
**Abstract:** <details><summary>原文: </summary>Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes. (2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly revised annotations--often account for a very small fraction--to the AI training remains marginal. This paper proposes Continual Tuning to address the problems from two perspectives: network design and data reuse. Firstly, we design a shared network for all classes followed by class-specific networks dedicated to individual classes. To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes. Secondly, we reuse a small fraction of data with previous annotations to avoid over-computing. The selection of such data relies on the importance estimate of each data. The importance score is computed by combining the uncertainty and consistency of AI predictions. Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>交互式分割是人工智能算法和人类专业知识的集成，可以提高医疗保健中大规模、详细注释数据集的准确性和效率。人类专家修改人工智能预测的注释，反过来，人工智能通过学习这些修改后的注释来改进其预测。这个交互过程不断提高注释的质量，直到不需要专家进行重大修改。关键的挑战是如何利用人工智能预测和专家修订的注释来迭代改进人工智能。出现两个问题：（1）灾难性遗忘的风险——如果仅使用专家修订的课程进行重新训练，人工智能往往会忘记之前学习的课程。 (2) 使用人工智能预测和专家修订注释重新训练人工智能时计算效率低下；此外，考虑到数据集中主要的人工智能预测注释，新修订的注释（通常只占很小的一部分）对人工智能训练的贡献仍然很小。本文提出Continual Tuning，从网络设计和数据复用两个角度解决该问题。首先，我们为所有类别设计一个共享网络，然后是专用于各个类别的特定类别网络。为了减少遗忘，我们冻结以前学习过的类的共享网络，只更新修订后的类的特定于类的网络。其次，我们重用一小部分带有先前注释的数据，以避免过度计算。此类数据的选择依赖于每个数据的重要性估计。重要性得分是通过结合人工智能预测的不确定性和一致性来计算的。我们的实验表明，持续调优的速度比从头开始重复训练 AI 快 16 倍，而且不会影响性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.19423v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **PEM: Prototype-based Efficient MaskFormer for Image Segmentation**<br />
**Title_cn:** PEM：用于图像分割的基于原型的 Efficient MaskFormer<br />
**Authors:** Niccolò Cavagnero, Gabriele Rosi, Claudia Ruttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli<br />
**Abstract:** <details><summary>原文: </summary>Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近基于变压器的架构在图像分割领域显示出了令人印象深刻的结果。由于其灵活性，它们在单一统一框架下的多个分割任务（例如语义和全景）中获得了出色的性能。为了实现如此令人印象深刻的性能，这些架构采用密集型操作并需要大量计算资源，而这些资源通常不可用，尤其是在边缘设备上。为了填补这一空白，我们提出了基于原型的 Efficient MaskFormer (PEM)，这是一种基于变压器的高效架构，可以在多个分割任务中运行。 PEM提出了一种新颖的基于原型的交叉注意力，它利用视觉特征的冗余来限制计算并在不损害性能的情况下提高效率。此外，PEM 引入了高效的多尺度特征金字塔网络，由于可变形卷积和基于上下文的自调制的结合，能够有效地提取具有高语义内容的特征。我们在语义和全景分割这两个任务上对所提出的 PEM 架构进行了基准测试，并在两个不同的数据集 Cityscapes 和 ADE20K 上进行了评估。 PEM 在每个任务和数据集上都表现出出色的性能，优于特定于任务的架构，同时与计算成本高昂的基线相当甚至更好。</details>
**PDF:** <http://arxiv.org/pdf/2402.19422v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance**<br />
**Title_cn:** 评估神经网络相对于人类表现的视觉连续腐败鲁棒性<br />
**Authors:** Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik<br />
**Abstract:** <details><summary>原文: </summary>While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness. Yet such robustness is seemingly effortless for human perception. In this paper, we propose visually-continuous corruption robustness (VCR) -- an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments. Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然神经网络 (NN) 在 ImageNet 上的图像分类方面已经超越了人类的准确性，但它们通常缺乏针对图像损坏的鲁棒性，即损坏鲁棒性。然而，对于人类的感知来说，这种鲁棒性似乎毫不费力。在本文中，我们提出了视觉连续腐败鲁棒性（VCR）——腐败鲁棒性的延伸，允许在与人类感知质量相对应的广泛且连续的变化范围内评估腐败鲁棒性（即从原始图像到完整图像）所有感知视觉信息的失真），以及用于神经网络评估的两个新颖的人类感知指标。为了将神经网络的 VCR 与人类感知进行比较，我们对 7,718 名人类参与者的 14 种常用图像损坏和具有不同训练目标（例如，标准、对抗性、损坏鲁棒性）、不同训练目标的最先进的鲁棒神经网络模型进行了广泛的实验。架构（例如，卷积神经网络、视觉变换器）和不同数量的训练数据增强。我们的研究表明：1）评估针对持续腐败的稳健性可以揭示现有基准未检测到的稳健性不足；因此，2）神经网络与人类鲁棒性之间的差距比之前已知的要大；最后，3）一些图像损坏对人类感知有类似的影响，为更具成本效益的鲁棒性评估提供了机会。我们的验证集包含 14 个图像损坏、人类鲁棒性数据和评估代码，作为工具箱和基准提供。</details>
**PDF:** <http://arxiv.org/pdf/2402.19401v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition**<br />
**Title_cn:** 第六届野外情感行为分析（ABAW）大赛<br />
**Authors:** Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu<br />
**Abstract:** <details><summary>原文: </summary>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition, which is part of the respective Workshop held in conjunction with IEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in understanding human emotions and behaviors, crucial for the development of human-centered technologies. In more detail, the Competition focuses on affect related benchmarking tasks and comprises of five sub-challenges: i) Valence-Arousal Estimation (the target is to estimate two continuous affect dimensions, valence and arousal), ii) Expression Recognition (the target is to recognise between the mutually exclusive classes of the 7 basic expressions and 'other'), iii) Action Unit Detection (the target is to detect 12 action units), iv) Compound Expression Recognition (the target is to recognise between the 7 mutually exclusive compound expression classes), and v) Emotional Mimicry Intensity Estimation (the target is to estimate six continuous emotion dimensions). In the paper, we present these Challenges, describe their respective datasets and challenge protocols (we outline the evaluation metrics) and present the baseline systems as well as their obtained performance. More information for the Competition can be found in: \url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了第六届野外情感行为分析 (ABAW) 竞赛，该竞赛是与 IEEE CVPR 2024 联合举办的相应研讨会的一部分。第六届 ABAW 竞赛解决了理解人类情感和行为方面的当代挑战，这对于理解人类情感和行为至关重要。发展以人为本的技术。更详细地说，比赛侧重于情感相关的基准测试任务，包括五个子挑战：i）效价-唤醒估计（目标是估计两个连续的情感维度，效价和唤醒），ii）表情识别（目标是识别 7 个基本表达式和“其他”的互斥类别），iii）动作单元检测（目标是检测 12 个动作单元），iv）复合表达式识别（目标是识别 7 个互斥的类别）复合表达类别），以及 v）情绪拟态强度估计（目标是估计六个连续的情绪维度）。在本文中，我们提出了这些挑战，描述了它们各自的数据集和挑战协议（我们概述了评估指标），并介绍了基线系统及其获得的性能。有关比赛的更多信息，请访问：\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}。</details>
**PDF:** <http://arxiv.org/pdf/2402.19344v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **One model to use them all: Training a segmentation model with complementary datasets**<br />
**Title_cn:** 一种模型可以使用所有这些：使用互补数据集训练分割模型<br />
**Authors:** Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Marius Distler, Jürgen Weitz, Stefanie Speidel<br />
**Abstract:** <details><summary>原文: </summary>Understanding a surgical scene is crucial for computer-assisted surgery systems to provide any intelligent assistance functionality. One way of achieving this scene understanding is via scene segmentation, where every pixel of a frame is classified and therefore identifies the visible structures and tissues. Progress on fully segmenting surgical scenes has been made using machine learning. However, such models require large amounts of annotated training data, containing examples of all relevant object classes. Such fully annotated datasets are hard to create, as every pixel in a frame needs to be annotated by medical experts and, therefore, are rarely available. In this work, we propose a method to combine multiple partially annotated datasets, which provide complementary annotations, into one model, enabling better scene segmentation and the use of multiple readily available datasets. Our method aims to combine available data with complementary labels by leveraging mutual exclusive properties to maximize information. Specifically, we propose to use positive annotations of other classes as negative samples and to exclude background pixels of binary annotations, as we cannot tell if they contain a class not annotated but predicted by the model. We evaluate our method by training a DeepLabV3 on the publicly available Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary segmented anatomical structures. Our approach successfully combines 6 classes into one model, increasing the overall Dice Score by 4.4% compared to an ensemble of models trained on the classes individually. By including information on multiple classes, we were able to reduce confusion between stomach and colon by 24%. Our results demonstrate the feasibility of training a model on multiple datasets. This paves the way for future work further alleviating the need for one large, fully segmented datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解手术场景对于计算机辅助手术系统提供智能辅助功能至关重要。实现这种场景理解的一种方法是通过场景分割，其中对帧的每个像素进行分类，从而识别可见的结构和组织。使用机器学习在完全分割手术场景方面取得了进展。然而，此类模型需要大量带注释的训练数据，其中包含所有相关对象类的示例。这种完全注释的数据集很难创建，因为帧中的每个像素都需要由医学专家注释，因此很少可用。在这项工作中，我们提出了一种方法，将多个部分注释的数据集（提供补充注释）组合到一个模型中，从而实现更好的场景分割和使用多个现成的数据集。我们的方法旨在通过利用互斥属性将可用数据与互补标签相结合以最大化信息。具体来说，我们建议使用其他类的正注释作为负样本，并排除二进制注释的背景像素，因为我们无法判断它们是否包含未注释但由模型预测的类。我们通过在公开的德累斯顿外科解剖数据集上训练 DeepLabV3 来评估我们的方法，该数据集提供了二进制分段解剖结构的多个子集。我们的方法成功地将 6 个类别合并到一个模型中，与单独训练各个类别的模型集合相比，总体 Dice 分数提高了 4.4%。通过纳入多个类别的信息，我们能够将胃和结肠之间的混淆减少 24%。我们的结果证明了在多个数据集上训练模型的可行性。这为未来的工作铺平了道路，进一步减轻了对大型、完全分段数据集的需求。</details>
**PDF:** <http://arxiv.org/pdf/2402.19340v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification**<br />
**Title_cn:** 缝合间隙：将情境感知知识与视觉变换器融合以实现高级图像分类<br />
**Authors:** Delfina Sol Martinez Pandiani, Nicolas Lazzari, Valentina Presutti<br />
**Abstract:** <details><summary>原文: </summary>The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute KG embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual transformer embeddings. Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances. Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification. The posthoc interpretability analyses reveal the visual transformer's proficiency in capturing pixel-level visual attributes, contrasting with our method's efficacy in representing more abstract and semantic scene elements. We demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding for AC image classification. This work suggests a strong potential of neuro-symbolic methods for knowledge integration and robust image representation for use in downstream intricate visual comprehension tasks. All the materials and code are available online.</details>
**Abstract_cn:** <details><summary>译文: </summary>对自动高级图像理解的需求不断增长，特别是在检测图像中的抽象概念（AC）方面，强调了创新和更可解释的方法的必要性。这些方法需要将传统的深度视觉方法与人类用来在复杂的语义层面解释图像的细致入微的、依赖于上下文的知识相协调。在这项工作中，我们利用文化图像的情境感知知识来增强 AC 图像分类的性能和可解释性。我们自动从图像中提取感知语义单元，然后对其进行建模并集成到 ARTstract 知识图 (AKG) 中。该资源捕获了从 14,000 多个带有 AC 标签的文化图像中收集的情境感知语义。此外，我们还通过高级语言框架增强了 AKG。我们计算 KG 嵌入，并使用相对表示和混合方法进行实验，将这些嵌入与视觉变压器嵌入融合。最后，为了可解释性，我们通过检查模型与训练实例的相似性来进行事后定性分析。我们的结果表明，我们的混合 KGE-ViT 方法优于 AC 图像分类中的现有技术。事后可解释性分析揭示了视觉转换器在捕获像素级视觉属性方面的熟练程度，与我们的方法在表示更抽象和语义场景元素方面的功效形成鲜明对比。我们证明了 KGE 嵌入的情境感知知识和深度视觉模型的 AC 图像分类的感官知觉理解之间的协同作用和互补性。这项工作表明神经符号方法在知识整合和鲁棒图像表示方面具有强大的潜力，可用于下游复杂的视觉理解任务。所有材料和代码都可以在线获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.19339v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction**<br />
**Title_cn:** 具有细粒度视觉语义交互的可概括的整个幻灯片图像分类<br />
**Authors:** Hao Li, Ying Chen, Yifei Chen, Wenxian Yang, Bowen Ding, Yuchen Han, Liansheng Wang, Rongshan Yu<br />
**Abstract:** <details><summary>原文: </summary>Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel "Fine-grained Visual-Semantic Interaction" (FiVE) framework for WSI classification. It is designed to enhance the model's generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments.</details>
**Abstract_cn:** <details><summary>译文: </summary>整个幻灯片图像 (WSI) 分类通常被表述为多实例学习 (MIL) 问题。最近，视觉语言模型（VLM）在 WSI 分类中表现出了卓越的性能。然而，现有的方法利用粗粒度的致病描述进行视觉表示监督，不足以捕获致病图像的复杂视觉外观，阻碍了模型在不同下游任务上的通用性。此外，处理高分辨率 WSI 的计算成本可能很高。在本文中，我们提出了一种用于 WSI 分类的新颖的“细粒度视觉语义交互”（FiVE）框架。它旨在通过利用局部视觉模式和细粒度病理语义之间的相互作用来增强模型的通用性。具体来说，通过精心设计的查询，我们首先利用大型语言模型从各种非标准化原始报告中提取细粒度的病理描述。然后，输出描述被重建为用于训练的细粒度标签。通过引入特定于任务的细粒度语义 (TFS) 模块，我们能够提示捕获 WSI 中的关键视觉信息，从而增强表示学习并显着增强泛化能力。此外，考虑到病理视觉模式冗余地分布在组织切片上，我们在训练期间对视觉实例的子集进行采样。我们的方法表现出强大的泛化性和可移植性，在 TCGA 肺癌数据集上明显优于同行，在几次实验中准确率至少高出 9.19%。</details>
**PDF:** <http://arxiv.org/pdf/2402.19326v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **CAMixerSR: Only Details Need More "Attention"**<br />
**Title_cn:** CAMixerSR：只有细节需要更多“关注”<br />
**Authors:** Yan Wang, Shijie Zhao, Yi Liu, Junlin Li, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution. We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了满足对大图像（2K-8K）超分辨率（SR）快速增长的需求，流行的方法遵循两个独立的轨道：1）通过内容感知路由加速现有网络，2）通过令牌混合器精炼。尽管直接，但它们遇到了不可避免的缺陷（例如，不灵活的路线或非歧视性处理），限制了质量复杂性权衡的进一步改进。为了消除这些缺点，我们通过提出一个内容感知混合器（CAMixer）来集成这些方案，它为简单的上下文分配卷积，并为稀疏纹理分配额外的可变形窗口注意。具体来说，CAMixer 使用可学习的预测器来生成多个引导程序，包括窗口扭曲的偏移量、用于分类窗口的掩码以及赋予卷积动态属性的卷积注意力，从而自适应地调节注意力以包含更多有用的纹理并提高卷积的表示能力。我们进一步引入全局分类损失来提高预测器的准确性。通过简单地堆叠CAMixer，我们获得了CAMixerSR，它在大图像SR、轻量级SR和全向图像SR上实现了优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.19289v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation**<br />
**Title_cn:** PrPSeg：全景肾脏病理分割的通用命题学习<br />
**Authors:** Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jialin Yue, Juming Xiong, Lining Yu, Yifei Wu, Mengmeng Yin, Yu Wang, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research. The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus). Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge. In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy.   In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解肾脏病理学的解剖结构对于推进疾病诊断、治疗评估和临床研究至关重要。复杂的肾脏系统由多个层面的各种组成部分组成，包括区域（皮质、髓质）、功能单位（肾小球、肾小管）和细胞（足细胞、肾小球中的系膜细胞）。先前的研究主要忽视了临床知识中对象之间复杂的空间相互关系。在这项研究中，我们引入了一种新颖的通用命题学习方法，称为全景肾脏病理分割（PrPSeg），旨在通过整合广泛的肾脏解剖知识来分割肾脏内的全面全景结构。在本文中，我们建议（1）设计一个全面的肾脏病理通用命题矩阵，便于将分类和空间关系纳入分割过程； （2）基于token的动态头单网络架构，改进了部分标签图像分割和未来数据放大的能力； (3) 解剖学损失函数，量化肾脏的对象间关系。</details>
**PDF:** <http://arxiv.org/pdf/2402.19286v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays**<br />
**Title_cn:** 通过在最少注释的 X 射线上进行稳健的斑块提取来检测脊柱骨赘<br />
**Authors:** Soumya Snigdha Kundu, Yuanhan Mo, Nicharee Srikijkasemwat, Bartłomiej W. Papiez<br />
**Abstract:** <details><summary>原文: </summary>The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths. This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays. A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours. A final patch classification accuracy of 84.5\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%. This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes. The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray.</details>
**Abstract_cn:** <details><summary>译文: </summary>关节炎的发生和进展与骨赘密切相关，骨赘是一种微小且难以捉摸的骨生长物。本文介绍了脊柱 X 射线中脊柱骨赘自动检测的首批成果之一。基于深度学习驱动的椎骨分割和掩模轮廓的放大，提出了一种称为 SegPatch 的新型自动补丁提取过程。最终补丁分类准确度达到 84.5%，比基于基线平铺的补丁生成技术高出 9.5%。这表明，即使注释有限，SegPatch 也可以为检测骨赘等微小结构提供卓越的性能。所提出的方法有可能帮助临床医生加快在脊柱 X 射线中手动识别骨赘的过程。</details>
**PDF:** <http://arxiv.org/pdf/2402.19263v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition**<br />
**Title_cn:** CricaVPR：用于视觉位置识别的跨图像相关感知表示学习<br />
**Authors:** Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, Chun Yuan<br />
**Abstract:** <details><summary>原文: </summary>Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the self-attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features. The code is released at https://github.com/Lu-Feng/CricaVPR.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年中，视觉位置识别（VPR）中的大多数方法都使用神经网络来产生特征表示。这些网络通常仅使用该图像本身来生成位置图像的全局表示，而忽略跨图像变化（例如视点和照明），这限制了它们在具有挑战性的场景中的鲁棒性。在本文中，我们提出了一种具有跨图像相关意识的鲁棒全局表示方法，名为 CricaVPR。我们的方法使用自注意力机制来关联批次中的多个图像。这些图像可以在同一地点以不同的条件或视角拍摄，甚至可以从不同的地点拍摄。因此，我们的方法可以利用跨图像变化作为指导表示学习的线索，从而确保产生更稳健的特征。为了进一步提高鲁棒性，我们提出了一种多尺度卷积增强自适应方法，使预训练的视觉基础模型适应VPR任务，该方法引入多尺度局部信息以进一步增强跨图像相关感知表示。实验结果表明，我们的方法在训练时间显着减少的情况下大幅优于最先进的方法。我们的方法使用 512 维全局特征在 Pitts30k 上实现了 94.5% R@1。代码发布于https://github.com/Lu-Feng/CricaVPR。</details>
**PDF:** <http://arxiv.org/pdf/2402.19231v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Effective Message Hiding with Order-Preserving Mechanisms**<br />
**Title_cn:** 通过保序机制有效隐藏消息<br />
**Authors:** Gao Yu, Qiu Xuchong, Ye Zihan<br />
**Abstract:** <details><summary>原文: </summary>Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility. While convolutional neural networks have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging. This challenge arises because convolutional operations struggle to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities. To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities. Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities. Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility. We will make our code publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>消息隐藏是一种在封面图像中隐藏秘密消息位的技术，旨在实现消息容量、恢复准确性和不可察觉性之间的最佳平衡。虽然卷积神经网络显着提高了消息容量和不可感知性，但实现高恢复精度仍然具有挑战性。出现这一挑战是因为卷积运算难以保持消息位的顺序并有效解决这两种模式之间的差异。为了解决这个问题，我们提出了 StegaFormer，这是一种基于 MLP 的创新框架，旨在保留位顺序并实现模态之间的全局融合。具体来说，StegaFormer 包含三个关键组件：保序消息编码器 (OPME)、解码器 (OPMD) 和全局消息图像融合 (GMIF)。 OPME 和 OPMD 旨在通过将整个序列分割成等长的片段并在编码和解码期间合并顺序信息来保留消息位的顺序。同时，GMIF采用跨模态融合机制来有效融合两种不相关模态的特征。 COCO 和 DIV2K 数据集上的实验结果表明，StegaFormer 在恢复精度、消息容量和不可感知性方面超越了现有的最先进方法。我们将公开我们的代码。</details>
**PDF:** <http://arxiv.org/pdf/2402.19160v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **A SAM-guided Two-stream Lightweight Model for Anomaly Detection**<br />
**Title_cn:** SAM引导的两流轻量级异常检测模型<br />
**Authors:** Chenghao Li, Lei Qi, Xin Geng<br />
**Abstract:** <details><summary>原文: </summary>In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.</details>
**Abstract_cn:** <details><summary>译文: </summary>在工业异常检测中，模型效率和移动友好性成为实际应用中的主要关注点。同时，Segment Anything (SAM) 令人印象深刻的泛化能力引起了学术界的广泛关注，使其成为定位未见异常和多样化现实世界模式的理想选择。在本文中，考虑到这两个关键因素，我们提出了一种用于无监督异常检测（STLM）的 SAM 引导的双流轻量级模型，该模型不仅符合两个实际应用需求，而且还利用了 SAM 强大的泛化能力。我们采用两个轻量级图像编码器，即我们的双流轻量级模块，以 SAM 知识为指导。具体来说，一个流被训练为在正常区域和异常区域中生成有区别的和一般的特征表示，而另一个流则重建没有异常的相同图像，这有效地增强了两流表示在面对异常区域时的区分度。此外，我们采用共享掩模解码器和特征聚合模块来生成异常图。我们在 MVTec AD 基准上进行的实验表明，STLM 具有约 16M 参数并实现了 20ms 的推理时间，在性能方面可与最先进的方法有效竞争，像素级 AUC 为 98.26%，像素级 AUC 为 94.92%。专业。我们进一步在更困难的数据集（例如 VisA 和 DAGM）上进行实验，以证明 STLM 的有效性和通用性。</details>
**PDF:** <http://arxiv.org/pdf/2402.19145v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **ProtoP-OD: Explainable Object Detection with Prototypical Parts**<br />
**Title_cn:** ProtoP-OD：使用原型部件进行可解释的对象检测<br />
**Authors:** Pavlos Rath-Manakidis, Frederik Strothmann, Tobias Glasmachers, Laurenz Wiskott<br />
**Abstract:** <details><summary>原文: </summary>Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \emph{semantics} that the model is focusing on. This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection. These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes. This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability. We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测变压器行为的解释和可视化往往会突出显示模型关注的图像中的位置，但它对模型关注的\emph{语义}提供的了解有限。本文介绍了检测变压器的扩展，它构造典型的局部特征并将其用于对象检测。这些自定义特征（我们称之为原型零件）被设计为相互排斥并与模型的分类保持一致。所提出的扩展由瓶颈模块（原型颈部）组成，它计算原型激活的离散表示以及将原型与对象类相匹配的新损失项。这种设置可以在原型颈部中产生可解释的表示，从而可以目视检查模型感知的图像内容，并更好地理解模型的可靠性。我们通过实验表明，我们的方法仅会产生有限的性能损失，并且我们提供了示例来证明我们的方法提供的解释的质量，我们认为这超过了性能损失。</details>
**PDF:** <http://arxiv.org/pdf/2402.19142v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **BigGait: Learning Gait Representation You Want by Large Vision Models**<br />
**Title_cn:** BigGait：通过大视觉模型学习您想要的步态表示<br />
**Authors:** Dingqiang Ye, Chao Fan, Jingzhe Ma, Xiaoming Liu, Shiqi Yu<br />
**Abstract:** <details><summary>原文: </summary>Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an unsupervised manner, drawing from design principles of established gait representation construction approaches. Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code will be available at https://github.com/ShiqiYu/OpenGait.</details>
**Abstract_cn:** <details><summary>译文: </summary>步态识别是最关键的远程识别技术之一，并逐渐扩展到研究和工业界。然而，现有的步态识别方法严重依赖于监督学习驱动的特定任务上游来提供明确的步态表示，这不可避免地引入昂贵的注释成本并可能导致累积错误。为了摆脱这一趋势，这项工作基于与任务无关的大视觉模型（LVM）产生的通用知识，探索了有效的步态表示，并提出了一个简单而高效的步态框架，称为 BigGait。具体来说，BigGait 中的步态表示提取器 (GRE) 借鉴已建立的步态表示构建方法的设计原则，以无监督的方式有效地将通用知识转化为隐式步态特征。 CCPG、CAISA-B*和SUSTech1K上的实验结果表明，BigGait在大多数情况下在自域和跨域任务中都显着优于先前的方法，并为学习下一代步态表示提供了更实用的范式。最终，我们深入研究了基于 LVM 的步态识别的潜在挑战和有希望的方向，旨在激发这一新兴主题的未来工作。源代码可在 https://github.com/ShiqiYu/OpenGait 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.19122v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection**<br />
**Title_cn:** 利用中间编码器块的表示进行合成图像检测<br />
**Authors:** Christos Koutlis, Symeon Papadopoulos<br />
**Abstract:** <details><summary>原文: </summary>The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at https://github.com/mever-team/rine.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近开发并公开的合成图像生成方法和服务使得按需创建极其逼真的图像成为可能，这给在线信息的完整性和安全性带来了巨大的风险。最先进的合成图像检测（SID）研究为从基础模型中提取特征的优势提供了强有力的证据。然而，此类提取的特征大多封装了高级视觉语义，而不是细粒度的细节，这对于 SID 任务更为重要。相反，浅层编码低级视觉信息。在这项工作中，我们利用 CLIP 图像编码器的中间 Transformer 块通过轻量级网络提取的图像表示，将它们映射到能够很好地泛化的可学习的伪造感知向量空间。我们还采用可训练模块将每个 Transformer 块的重要性纳入最终预测。通过在 20 个测试数据集上进行评估，我们的方法与最先进的方法进行了比较，结果显示平均绝对性能提高了 +10.6%。值得注意的是，性能最佳的模型只需要一个时期的训练（约 8 分钟）。代码可在 https://github.com/mever-team/rine 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.19091v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **VideoMAC: Video Masked Autoencoders Meet ConvNets**<br />
**Title_cn:** VideoMAC：视频屏蔽自动编码器遇见卷积网络<br />
**Authors:** Gensheng Pei, Tao Chen, Xiruo Jiang, Huafeng Liu, Zeren Sun, Yazhou Yao<br />
**Abstract:** <details><summary>原文: </summary>Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\textbf{5.2\%} / \textbf{6.4\%} $\mathcal{J}\&\mathcal{F}$), body part propagation (+\textbf{6.3\%} / \textbf{3.1\%} mIoU), and human pose tracking (+\textbf{10.2\%} / \textbf{11.1\%} PCK@0.1).</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，自监督学习技术的进步，如掩模自动编码器（MAE），极大地影响了图像和视频的视觉表示学习。然而，值得注意的是，现有蒙版图像/视频建模中的主要方法过度依赖资源密集型视觉变换器（ViT）作为特征编码器。在本文中，我们提出了一种称为 \textbf{VideoMAC} 的新方法，它将视频屏蔽自动编码器与资源友好的卷积网络相结合。具体来说，VideoMAC 对随机采样的视频帧对采用对称掩码。为了防止掩模图案耗散的问题，我们利用用稀疏卷积算子实现的ConvNet作为编码器。同时，我们提出了一种简单而有效的掩模视频建模（MVM）方法，一种由在线编码器和指数移动平均目标编码器组成的双编码器架构，旨在促进视频中帧间重建的一致性。此外，我们还证明 VideoMAC 使经典 (ResNet) / 现代 (ConvNeXt) 卷积编码器能够利用 MVM 的优势，在下游任务上优于基于 ViT 的方法，包括视频对象分割 (+\textbf{5.2\%} / \ textbf{6.4\%} $\mathcal{J}\&\mathcal{F}$)、身体部位传播 (+\textbf{6.3\%} / \textbf{3.1\%} mIoU) 和人体姿势跟踪 ( +\textbf{10.2\%} / \textbf{11.1\%} PCK@0.1)。</details>
**PDF:** <http://arxiv.org/pdf/2402.19082v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research**<br />
**Title_cn:** VEnvision3D：用于 3D 多任务模型研究的综合感知数据集<br />
**Authors:** Jiahao Zhou, Chen Long, Yue Xie, Jialiang Wang, Boheng Li, Haiping Wang, Zhe Chen, Zhen Dong<br />
**Abstract:** <details><summary>原文: </summary>Developing a unified multi-task foundation model has become a critical challenge in computer vision research. In the current field of 3D computer vision, most datasets solely focus on a relatively limited set of tasks, which complicates the concurrent training requirements of various downstream tasks. This makes the training of multi-objective networks difficult to proceed with, which further hinders the development of foundation models in the 3D vision field. In this paper, we introduce VEnvision3D, a large 3D synthetic perception dataset for multi-task learning, including depth completion, segmentation, upsampling, place recognition, and 3D reconstruction. Since the data for each task was collected in the same scenarios, tasks are inherently aligned in terms of the utilized data. Therefore, such a unique attribute can assist in exploring the potential for the multi-task model and even the foundation model without separate training methods. Several new benchmarks based on the characteristics of the proposed dataset were presented. Extensive studies were performed on end-to-end models, revealing new observations, challenges, and opportunities for future research. In addition, we designed a straightfoward multi-task network to uncover the ability that VEnvision3D can offer for the foundation model. Our dataset and code will be open-sourced upon acceptance.</details>
**Abstract_cn:** <details><summary>译文: </summary>开发统一的多任务基础模型已成为计算机视觉研究中的关键挑战。在当前的 3D 计算机视觉领域，大多数数据集仅关注相对有限的一组任务，这使得各种下游任务的并发训练要求变得复杂。这使得多目标网络的训练难以进行，进一步阻碍了3D视觉领域基础模型的发展。在本文中，我们介绍了 VEnvision3D，这是一个用于多任务学习的大型 3D 合成感知数据集，包括深度补全、分割、上采样、位置识别和 3D 重建。由于每个任务的数据都是在相同的场景中收集的，因此任务在所使用的数据方面本质上是一致的。因此，这种独特的属性可以帮助探索多任务模型甚至基础模型的潜力，而无需单独的训练方法。提出了一些基于所提出的数据集特征的新基准。对端到端模型进行了广泛的研究，揭示了新的观察结果、挑战和未来研究的机遇。此外，我们设计了一个简单的多任务网络来揭示 VEnvision3D 可以为基础模型提供的功能。我们的数据集和代码将在接受后开源。</details>
**PDF:** <http://arxiv.org/pdf/2402.19059v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments**<br />
**Title_cn:** DOZE：动态环境中开放词汇零样本对象导航的数据集<br />
**Authors:** Ji Ma, Hongming Dai, Yao Mu, Pengying Wu, Hao Wang, Xiaowei Chi, Yang Fei, Shanghang Zhang, Chang Liu<br />
**Abstract:** <details><summary>原文: </summary>Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset could be found at https://DOZE-Dataset.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>零射击对象导航（ZSON）要求代理在陌生的环境中自主定位和接近看不见的对象，并且已成为嵌入式人工智能领域中一项特别具有挑战性的任务。现有用于开发 ZSON 算法的数据集缺乏对动态障碍物、对象属性多样性和场景文本的考虑，因此与现实世界的情况存在明显差异。为了解决这些问题，我们提出了动态环境中开放词汇零射击对象导航（DOZE）的数据集，其中包含十个高保真 3D 场景和超过 18k 的任务，旨在模拟复杂、动态的现实世界场景。具体来说，DOZE 场景具有多个移动的人形障碍、大量开放词汇对象、各种不同属性的对象以及有价值的文本提示。此外，与仅提供代理和静态障碍物之间的碰撞检查的现有数据集不同，我们通过集成检测代理和移动障碍物之间的碰撞的功能来增强 DOZE。这种新颖的功能可以评估代理在动态环境中的防撞能力。我们在 DOZE 上测试了四种代表性的 ZSON 方法，揭示了现有方法在导航效率、安全性和物体识别准确性方面还有很大的改进空间。我们的数据集可以在 https://DOZE-Dataset.github.io/ 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.19007v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation**<br />
**Title_cn:** RSAM-Seg：基于 SAM 的遥感图像语义分割先验知识集成方法<br />
**Authors:** Jie Zhang, Xubing Yang, Rui Jiang, Wei Shao, Li Zhang<br />
**Abstract:** <details><summary>原文: </summary>The development of high-resolution remote sensing satellites has provided great convenience for research work related to remote sensing. Segmentation and extraction of specific targets are essential tasks when facing the vast and complex remote sensing images. Recently, the introduction of Segment Anything Model (SAM) provides a universal pre-training model for image segmentation tasks. While the direct application of SAM to remote sensing image segmentation tasks does not yield satisfactory results, we propose RSAM-Seg, which stands for Remote Sensing SAM with Semantic Segmentation, as a tailored modification of SAM for the remote sensing field and eliminates the need for manual intervention to provide prompts. Adapter-Scale, a set of supplementary scaling modules, are proposed in the multi-head attention blocks of the encoder part of SAM. Furthermore, Adapter-Feature are inserted between the Vision Transformer (ViT) blocks. These modules aim to incorporate high-frequency image information and image embedding features to generate image-informed prompts. Experiments are conducted on four distinct remote sensing scenarios, encompassing cloud detection, field monitoring, building detection and road mapping tasks . The experimental results not only showcase the improvement over the original SAM and U-Net across cloud, buildings, fields and roads scenarios, but also highlight the capacity of RSAM-Seg to discern absent areas within the ground truth of certain datasets, affirming its potential as an auxiliary annotation method. In addition, the performance in few-shot scenarios is commendable, underscores its potential in dealing with limited datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>高分辨率遥感卫星的发展为遥感相关研究工作提供了极大便利。面对海量、复杂的遥感图像，特定目标的分割和提取是必不可少的任务。最近，Segment Anything Model（SAM）的推出为图像分割任务提供了通用的预训练模型。虽然直接将 SAM 应用于遥感图像分割任务并不能产生令人满意的结果，但我们提出了 RSAM-Seg（即带语义分割的遥感 SAM），作为针对遥感领域的 SAM 的定制修改，并且消除了对 SAM 的需求。手动干预以提供提示。 Adapter-Scale是一组补充缩放模块，是在SAM编码器部分的多头注意块中提出的。此外，适配器功能被插入视觉转换器（ViT）块之间。这些模块旨在结合高频图像信息和图像嵌入功能来生成图像提示。实验在四种不同的遥感场景中进行，包括云检测、现场监测、建筑物检测和道路测绘任务。实验结果不仅展示了原始 SAM 和 U-Net 在云、建筑物、田野和道路场景中的改进，而且还强调了 RSAM-Seg 识别某些数据集的真实情况中缺失区域的能力，肯定了其潜力作为辅助标注方法。此外，在少样本场景中的性能值得称赞，强调了其处理有限数据集的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2402.19004v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement**<br />
**Title_cn:** 喉血管分类两步异构迁移学习分析：问题与改进<br />
**Authors:** Xinyi Fang, Chak Fong Chong, Kei Long Wong, Yapeng Wang, Tiankui Zhang, Sio-Kei Im<br />
**Abstract:** <details><summary>原文: </summary>Transferring features learned from natural to medical images for classification is common. However, challenges arise due to the scarcity of certain medical image types and the feature disparities between natural and medical images. Two-step transfer learning has been recognized as a promising solution for this issue. However, choosing an appropriate intermediate domain would be critical in further improving the classification performance. In this work, we explore the effectiveness of using color fundus photographs of the diabetic retina dataset as an intermediate domain for two-step heterogeneous learning (THTL) to classify laryngeal vascular images with nine deep-learning models. Experiment results confirm that although the images in both the intermediate and target domains share vascularized characteristics, the accuracy is drastically reduced compared to one-step transfer learning, where only the last layer is fine-tuned (e.g., ResNet18 drops 14.7%, ResNet50 drops 14.8%). By analyzing the Layer Class Activation Maps (LayerCAM), we uncover a novel finding that the prevalent radial vascular pattern in the intermediate domain prevents learning the features of twisted and tangled vessels that distinguish the malignant class in the target domain. To address the performance drop, we propose the Step-Wise Fine-Tuning (SWFT) method on ResNet in the second step of THTL, resulting in substantial accuracy improvements. Compared to THTL's second step, where only the last layer is fine-tuned, accuracy increases by 26.1% for ResNet18 and 20.4% for ResNet50. Additionally, compared to training from scratch, using ImageNet as the source domain could slightly improve classification performance for laryngeal vascular, but the differences are insignificant.</details>
**Abstract_cn:** <details><summary>译文: </summary>将从自然图像学到的特征转移到医学图像进行分类是很常见的。然而，由于某些医学图像类型的稀缺以及自然图像和医学图像之间的特征差异，出现了挑战。两步迁移学习已被认为是解决此问题的有前途的解决方案。然而，选择合适的中间域对于进一步提高分类性能至关重要。在这项工作中，我们探索了使用糖尿病视网膜数据集的彩色眼底照片作为两步异构学习（THTL）的中间域来通过九个深度学习模型对喉部血管图像进行分类的有效性。实验结果证实，虽然中间域和目标域中的图像共享血管化特征，但与仅对最后一层进行微调的一步迁移学习相比，准确率大大降低（例如，ResNet18 下降了 14.7%，ResNet50 下降了14.8%）。通过分析层类激活图（LayerCAM），我们发现了一个新发现，即中间域中普遍存在的放射状血管模式阻止了学习区分目标域中恶性类别的扭曲和缠结血管的特征。为了解决性能下降的问题，我们在 THTL 的第二步中提出了 ResNet 上的逐步微调（SWFT）方法，从而显着提高了精度。与 THTL 的第二步（仅对最后一层进行微调）相比，ResNet18 的准确率提高了 26.1%，ResNet50 的准确率提高了 20.4%。此外，与从头开始训练相比，使用ImageNet作为源域可以稍微提高喉部血管的分类性能，但差异并不显着。</details>
**PDF:** <http://arxiv.org/pdf/2402.19001v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection**<br />
**Title_cn:** COFT-AD：用于少样本异常检测的对比微调<br />
**Authors:** Jingyi Liao, Xun Xu, Manh Cuong Nguyen, Adam Goodge, Chuan Sheng Foo<br />
**Abstract:** <details><summary>原文: </summary>Existing approaches towards anomaly detection~(AD) often rely on a substantial amount of anomaly-free data to train representation and density models. However, large anomaly-free datasets may not always be available before the inference stage; in which case an anomaly detection model must be trained with only a handful of normal samples, a.k.a. few-shot anomaly detection (FSAD). In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques. Firstly, we employ a model pre-trained on a large source dataset to initialize model weights. Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to fine-tune on the few-shot target domain data. To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples. We evaluate few-shot anomaly detection on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的异常检测（AD）方法通常依赖于大量无异常数据来训练表示和密度模型。然而，在推理阶段之前，大型无异常数据集可能并不总是可用；在这种情况下，异常检测模型必须仅使用少量正常样本进行训练，即少样本异常检测（FSAD）。在本文中，我们提出了一种新颖的方法来应对 FSAD 的挑战，该方法结合了两种重要的技术。首先，我们采用在大型源数据集上预训练的模型来初始化模型权重。其次，为了改善源域和目标域之间的协变量偏移，我们采用对比训练来对少样本目标域数据进行微调。为了学习下游 AD 任务的合适表示，我们还结合了跨实例正对以鼓励正常样本紧密聚集，并结合负对以更好地分离正常样本和合成负样本。我们在 3 个受控 AD 任务和 4 个现实世界 AD 任务上评估了少样本异常检测，以证明所提出方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18998v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **Theoretically Achieving Continuous Representation of Oriented Bounding Boxes**<br />
**Title_cn:** 理论上实现定向边界框的连续表示<br />
**Authors:** Zikai Xiao, Guo-Ye Yang, Xue Yang, Tai-Jiang Mu, Junchi Yan, Shi-min Hu<br />
**Abstract:** <details><summary>原文: </summary>Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based object representation. For fairness and transparency of experiments, we have developed a modularized benchmark based on the open-source deep learning framework Jittor's detection toolbox JDet for OOD evaluation. On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.</details>
**Abstract_cn:** <details><summary>译文: </summary>人们在定向对象检测（OOD）方面投入了大量的精力。然而，关于定向边界框 (OBB) 表示的不连续性的一个持久问题仍未解决，这是现有 OOD 方法的固有瓶颈。本文力图从理论上保证彻底解决这一问题，结束这方面的临时努力。先前的研究通常只能解决两种不连续情况中的一种：旋转和纵横比，并且经常无意中引入解码不连续性，例如文献中讨论了解码不完整性 (DI) 和解码歧义 (DA)。具体来说，我们提出了一种称为连续 OBB (COBB) 的新颖表示方法，它可以轻松集成到现有检测器中，例如Faster-RCNN 作为插件。从理论上讲，它可以确保边界框回归的连续性，据我们所知，基于矩形的对象表示的文献尚未实现这一点。为了实验的公平和透明，我们基于开源深度学习框架Jittor的检测工具箱JDet开发了一个模块化基准用于OOD评估。在流行的 DOTA 数据集上，通过将 Faster-RCNN 集成为相同的基线模型，我们的新方法比同行方法 Gliding Vertex 优于同行方法 Gliding Vertex 1.13% mAP50（相对改进 1.54%）和 2.46% mAP75（相对改进 5.91%），没有任何影响技巧。</details>
**PDF:** <http://arxiv.org/pdf/2402.18975v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging**<br />
**Title_cn:** 致力于床旁超声成像中乳腺癌分类的分布外检测<br />
**Authors:** Jennie Karlsson, Marisa Wodrich, Niels Christian Overgaard, Freja Sahlin, Kristina Lång, Anders Heyden, Ida Arvidsson<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习已被证明在医学应用中具有巨大潜力。在这样的关键领域，拥有值得信赖的算法非常令人感兴趣，这些算法能够判断何时无法保证可靠的评估。检测分布外 (OOD) 样本是构建安全分类器的关键一步。先前的研究表明可以在护理点超声图像中对乳腺癌进行分类，本研究使用三种不同的方法研究 OOD 检测：softmax、能量评分和深度集成。所有方法都在三个不同的 OOD 数据集上进行测试。结果表明，能量评分方法优于 softmax 方法，在其中两个数据集上表现良好。集成方法是最稳健的，在检测所有三个 OOD 数据集的 OOD 样本方面表现最好。</details>
**PDF:** <http://arxiv.org/pdf/2402.18960v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching**<br />
**Title_cn:** 通过主动教学促进遥感图像中的半监督目标检测<br />
**Authors:** Boxuan Zhang, Zengmao Wang, Bo Du<br />
**Abstract:** <details><summary>原文: </summary>The lack of object-level annotations poses a significant challenge for object detection in remote sensing images (RSIs). To address this issue, active learning (AL) and semi-supervised learning (SSL) techniques have been proposed to enhance the quality and quantity of annotations. AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples. In this letter, we propose a novel AL method to boost semi-supervised object detection (SSOD) for remote sensing images with a teacher student network, called SSOD-AT. The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs). Meanwhile, the RoICM is utilized to identify the top-K uncertain images. To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on object-level prototypes of different categories using both labeled and pseudo-labeled images. Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for object detection in RSIs. Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL.</details>
**Abstract_cn:** <details><summary>译文: </summary>缺乏对象级注释给遥感图像（RSI）中的对象检测带来了重大挑战。为了解决这个问题，主动学习（AL）和半监督学习（SSL）技术被提出来提高注释的质量和数量。 AL 侧重于选择信息最丰富的样本进行注释，而 SSL 则利用未标记样本中的知识。在这封信中，我们提出了一种新颖的 AL 方法，通过师生网络增强遥感图像的半监督目标检测 (SSOD)，称为 SSOD-AT。所提出的方法结合了 RoI 比较模块 (RoICM) 来生成感兴趣区域 (RoI) 的高置信度伪标签。同时，利用RoICM来识别top-K不确定图像。为了减少用于人类标记的前 K 个不确定图像中的冗余，使用标记图像和伪标记图像基于不同类别的对象级原型引入了多样性标准。对 DOTA 和 DIOR 这两个流行数据集的大量实验表明，我们提出的方法优于 RSI 中对象检测的最先进方法。与 SOTA 方法的最佳性能相比，所提出的方法在整个 AL 的大多数情况下实现了 1% 的改进。</details>
**PDF:** <http://arxiv.org/pdf/2402.18958v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution**<br />
**Title_cn:** 超越 Dropout：实现通用图像超分辨率的有趣解决方案<br />
**Authors:** Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng<br />
**Abstract:** <details><summary>原文: </summary>Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics. Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven benchmark datasets including both synthetic and real-world scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度学习带来了单图像超分辨率（SISR）性能的巨大飞跃。虽然大多数现有工作都假设一个简单且固定的退化模型（例如双三次下采样），但 Blind SR 的研究旨在提高模型在未知退化情况下的泛化能力。最近，Kong 等人率先研究了一种更适合使用 Dropout 的 Blind SR 训练策略。尽管这种方法确实通过减轻过度拟合带来了实质性的泛化改进，但我们认为 Dropout 同时引入了不良的副作用，损害了模型忠实重建精细细节的能力。我们在论文中展示了理论和实验分析，此外，我们提出了另一种简单而有效的训练策略，通过简单地调整模型的一阶和二阶特征统计量来增强模型的泛化能力。实验结果表明，我们的方法可以作为与模型无关的正则化，并且在七个基准数据集（包括合成场景和真实场景）上优于 Dropout。</details>
**PDF:** <http://arxiv.org/pdf/2402.18929v1><br />
**Code:** null<br />
>>**index:** 28<br />
**Title:** **Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering**<br />
**Title_cn:** 边缘计算通过自适应时空语义过滤实现实时视频分析<br />
**Authors:** Xiang Chen, Wenjie Zhu, Jiayuan Chen, Tong Zhang, Changyan Yi, Jun Cai<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes a novel edge computing enabled real-time video analysis system for intelligent visual devices. The proposed system consists of a tracking-assisted object detection module (TAODM) and a region of interesting module (ROIM). TAODM adaptively determines the offloading decision to process each video frame locally with a tracking algorithm or to offload it to the edge server inferred by an object detection model. ROIM determines each offloading frame's resolution and detection model configuration to ensure that the analysis results can return in time. TAODM and ROIM interact jointly to filter the repetitive spatial-temporal semantic information to maximize the processing rate while ensuring high video analysis accuracy. Unlike most existing works, this paper investigates the real-time video analysis systems where the intelligent visual device connects to the edge server through a wireless network with fluctuating network conditions. We decompose the real-time video analysis problem into the offloading decision and configurations selection sub-problems. To solve these two sub-problems, we introduce a double deep Q network (DDQN) based offloading approach and a contextual multi-armed bandit (CMAB) based adaptive configurations selection approach, respectively. A DDQN-CMAB reinforcement learning (DCRL) training framework is further developed to integrate these two approaches to improve the overall video analyzing performance. Extensive simulations are conducted to evaluate the performance of the proposed solution, and demonstrate its superiority over counterparts.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种用于智能视觉设备的新型边缘计算实时视频分析系统。所提出的系统由跟踪辅助目标检测模块（TAODM）和感兴趣区域模块（ROIM）组成。 TAODM 自适应地确定卸载决策，使用跟踪算法在本地处理每个视频帧，或将其卸载到由对象检测模型推断的边缘服务器。 ROIM确定每个卸载帧的分辨率和检测模型配置，以确保分析结果能够及时返回。 TAODM和ROIM联合交互，过滤重复的时空语义信息，在保证较高的视频分析精度的同时最大化处理速率。与大多数现有工作不同，本文研究了实时视频分析系统，其中智能视觉设备通过具有波动网络条件的无线网络连接到边缘服务器。我们将实时视频分析问题分解为卸载决策和配置选择子问题。为了解决这两个子问题，我们分别引入了基于双深 Q 网络（DDQN）的卸载方法和基于上下文多臂老虎机（CMAB）的自适应配置选择方法。进一步开发了 DDQN-CMAB 强化学习（DCRL）训练框架来集成这两种方法，以提高整体视频分析性能。进行了广泛的模拟来评估所提出的解决方案的性能，并证明其相对于同类解决方案的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18927v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection**<br />
**Title_cn:** 基于视觉变压器的简单而有效的网络，用于伪装物体和显着物体检测<br />
**Authors:** Chao Hao, Zitong Yu, Xin Liu, Jun Xu, Huanjing Yue, Jingyu Yang<br />
**Abstract:** <details><summary>原文: </summary>Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.</details>
**Abstract_cn:** <details><summary>译文: </summary>伪装目标检测（COD）和显着目标检测（SOD）是过去几十年来广泛研究的两个不同但密切相关的计算机视觉任务。虽然它们的目的相同，都是将图像分割为二值前景和背景区域，但它们的区别在于 COD 专注于图像中隐藏的隐藏对象，而 SOD 专注于图像中最突出的对象。之前的作品通过堆叠各种手工设计的模块和多尺度特征取得了良好的性能。然而，这些精心设计的复杂网络通常在一项任务上表现良好，但在另一项任务上却表现不佳。在这项工作中，我们提出了一种基于视觉 Transformer (ViT) 的简单而有效的网络 (SENet)，通过采用基于非对称 ViT 的编码器-解码器结构的简单设计，我们在这两项任务上都产生了有竞争力的结果，表现出比精心制作的。此外，为了增强 Transformer 建模局部信息的能力（这对于像素级二进制分割任务很重要），我们提出了局部信息捕获模块（LICM）。我们还提出了一种基于二元交叉熵（BCE）和交并集（IoU）损失的动态加权损失（DW损失），它引导网络根据情况更多地关注那些较小且更难找到的目标对象到他们的大小。此外，我们还探讨了SOD和COD的联合训练问题，并提出了联合训练冲突的初步解决方案，进一步提高了SOD的性能。对多个基准数据集的广泛实验证明了我们方法的有效性。代码可在 https://github.com/linuxsino/SENet 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18922v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation**<br />
**Title_cn:** 分解和组合：减轻杂散相关性的组合方法<br />
**Authors:** Fahimeh Hosseini Noohdani, Parsa Hosseini, Arian Yazdan Parast, Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah<br />
**Abstract:** <details><summary>原文: </summary>While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence). In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence). Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM. Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the counterfactual ones. Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training. The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然标准经验风险最小化 (ERM) 训练被证明对于分布内数据的图像分类是有效的，但它在分布外样本上表现不佳。图像分类分布偏移的主要来源之一是图像的组成性质。具体来说，除了确定标签的主要对象或组件之外，通常还存在一些其他图像组件，这可能导致训练环境和测试环境之间的输入分布发生变化。更重要的是，这些成分可能与标签存在虚假相关性。为了解决这个问题，我们提出了分解和组合（DaC），它通过基于组合图像元素的组合方法来提高对相关性偏移的鲁棒性。根据我们的观察，使用 ERM 训练的模型通常高度关注因果成分或与标签具有高度虚假相关性的成分（特别是在模型具有高置信度的数据点中）。事实上，根据虚假相关性的数量以及基于因果或非因果成分进行分类的难易程度，模型通常会关注其中之一（在高置信度的样本上）。接下来，我们首先尝试使用 ERM 训练的模型的类激活图来识别图像的因果成分。之后，我们通过组合图像并根据增强数据（包括反事实数据）重新训练模型来对图像进行干预。除了其高可解释性之外，这项工作还提出了一种通过干预图像来进行组平衡的方法，而无需组标签或有关训练期间虚假特征的信息。与之前的方法相比，该方法在相关偏移中对组标签进行相同量的监督时，总体上具有更好的最差组准确度。</details>
**PDF:** <http://arxiv.org/pdf/2402.18919v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection**<br />
**Title_cn:** SNE-RoadSegV2：推进自由空间检测的异构特征融合和易错意识<br />
**Authors:** Yi Feng, Yu Ma, Qijun Chen, Ioannis Pitas, Rui Fan<br />
**Abstract:** <details><summary>原文: </summary>Feature-fusion networks with duplex encoders have proven to be an effective technique to solve the freespace detection problem. However, despite the compelling results achieved by previous research efforts, the exploration of adequate and discriminative heterogeneous feature fusion, as well as the development of fallibility-aware loss functions remains relatively scarce. This paper makes several significant contributions to address these limitations: (1) It presents a novel heterogeneous feature fusion block, comprising a holistic attention module, a heterogeneous feature contrast descriptor, and an affinity-weighted feature recalibrator, enabling a more in-depth exploitation of the inherent characteristics of the extracted features, (2) it incorporates both inter-scale and intra-scale skip connections into the decoder architecture while eliminating redundant ones, leading to both improved accuracy and computational efficiency, and (3) it introduces two fallibility-aware loss functions that separately focus on semantic-transition and depth-inconsistent regions, collectively contributing to greater supervision during model training. Our proposed heterogeneous feature fusion network (SNE-RoadSegV2), which incorporates all these innovative components, demonstrates superior performance in comparison to all other freespace detection algorithms across multiple public datasets. Notably, it ranks the 1st on the official KITTI Road benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>具有双工编码器的特征融合网络已被证明是解决自由空间检测问题的有效技术。然而，尽管先前的研究工作取得了令人信服的成果，但对充分且有区别的异构特征融合的探索以及易错意识损失函数的开发仍然相对较少。本文为解决这些限制做出了几个重大贡献：（1）它提出了一种新颖的异构特征融合块，包括整体注意模块、异构特征对比度描述符和亲和力加权特征重新校准器，从而实现更深入的利用提取特征的固有特征，（2）它将尺度间和尺度内跳跃连接合并到解码器架构中，同时消除冗余，从而提高准确性和计算效率，（3）它引入了两种错误- 感知损失函数，分别关注语义转换和深度不一致区域，共同促进模型训练期间更好的监督。我们提出的异构特征融合网络（SNE-RoadSegV2）融合了所有这些创新组件，与跨多个公共数据集的所有其他自由空间检测算法相比，表现出了卓越的性能。值得注意的是，它在官方 KITTI Road 基准测试中排名第一。</details>
**PDF:** <http://arxiv.org/pdf/2402.18918v1><br />
**Code:** null<br />
>>**index:** 32<br />
**Title:** **Rethinking Multi-domain Generalization with A General Learning Objective**<br />
**Title_cn:** 以通用学习目标重新思考多领域泛化<br />
**Authors:** Zhaorui Tan, Xi Yang, Kaizhu Huang<br />
**Abstract:** <details><summary>原文: </summary>Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.</details>
**Abstract_cn:** <details><summary>译文: </summary>多域泛化（mDG）的普遍目标是最小化训练和测试分布之间的差异，以增强边缘到标签的分布映射。然而，现有的 mDG 文献缺乏通用的学习目标范式，并且经常对静态目标边缘分布施加约束。在本文中，我们建议利用 $Y$ 映射来放松约束。我们重新思考 mDG 的学习目标，并设计一个新的 \textbf{一般学习目标} 来解释和分析大多数现有的 mDG 智慧。这个总体目标分为两个协同目的：学习与领域无关的条件特征和最大化后验。探索还扩展到两个有效的正则化项，它们包含先验信息并抑制无效因果关系，从而缓解宽松约束带来的问题。理论上，我们为与域无关的条件特征的域对齐提供了一个上限，揭示了许多以前的 mDG 努力实际上\textbf{部分优化目标}，从而导致性能有限。因此，我们的研究将一般学习目标提炼为四个实用组成部分，提供通用、稳健且灵活的机制来处理复杂的领域转换。大量的实证结果表明，所提出的带有 $Y$ 映射的目标可以在各种下游任务（包括回归、分割和分类）中带来更好的 mDG 性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.18853v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Debiased Novel Category Discovering and Localization**<br />
**Title_cn:** 去偏见的小说类别发现和本地化<br />
**Authors:** Juexiao Feng, Yuhong Yang, Yanchun Xie, Yaqian Li, Yandong Guo, Yuchen Guo, Yuwei He, Liuyu Xiang, Guiguang Ding<br />
**Abstract:** <details><summary>原文: </summary>In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度学习中的目标检测得到了快速发展。然而，大多数现有的目标检测模型仅在封闭集数据集上表现良好，忽略了训练集中未定义类别的大量潜在目标。这些物体通常被检测器识别为背景或错误地分类为预定义类别。在本文中，我们关注新类别发现和定位（NCDL）这一具有挑战性的问题，旨在训练能够检测训练数据中存在的类别的检测器，同时还主动发现、定位和聚类新类别。我们分析了现有的 NCDL 方法并确定了核心问题：目标检测器往往偏向于可见的目标，这导致忽略不可见的目标。为了解决这个问题，我们首先提出了一种去偏区域挖掘（DRM）方法，该方法以互补的方式结合了类无关区域提议网络（RPN）和类感知 RPN。此外，我们建议通过利用未标记数据的半监督对比学习来改进表示网络。最后，我们采用简单高效的小批量 K 均值聚类方法进行新类发现。我们对 NCDL 基准进行了广泛的实验，结果表明所提出的 DRM 方法显着优于以前的方法，建立了新的最先进方法。</details>
**PDF:** <http://arxiv.org/pdf/2402.18821v1><br />
**Code:** null<br />
>>**index:** 34<br />
**Title:** **OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition**<br />
**Title_cn:** OpticalDR：用于隐私保护抑郁症识别的深度光学成像模型<br />
**Authors:** Yuchen Pan, Junjun Jiang, Kui Jiang, Zhihao Wu, Keyuan Yu, Xianming Liu<br />
**Abstract:** <details><summary>原文: </summary>Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>抑郁症识别（DR）提出了相当大的挑战，特别是在人们对隐私日益关注的背景下。传统的DR技术自动诊断需要使用人脸图像，无疑暴露了患者的身份特征并带来隐私风险。为了减轻与不当披露患者面部图像相关的潜在风险，我们设计了一种新的成像系统，可以擦除捕获的面部图像的身份信息，同时保留与疾病相关的特征。身份信息恢复是不可逆的，同时保留准确 DR 所需的基本疾病相关特征。更具体地说，我们尝试通过可学习的镜头记录去识别的面部图像（尽可能擦除可识别的特征），该镜头结合以下DR任务以及一系列人脸分析相关的辅助任务进行了优化端到端的方式。这些上述策略构成了我们最终的光学深度抑郁识别网络（OpticalDR）。在 CelebA、AVEC 2013 和 AVEC 2014 数据集上的实验表明，我们的 OpticalDR 已经实现了最先进的隐私保护性能，在流行的面部识别模型上平均 AUC 为 0.51，DR 的 MAE/RMSE 为 7.53 的竞争结果AVEC 2013 上的 /8.48 和 AVEC 2014 上的 7.89/8.82 分别。</details>
**PDF:** <http://arxiv.org/pdf/2402.18786v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **RoadRunner -- Learning Traversability Estimation for Autonomous Off-road Driving**<br />
**Title_cn:** RoadRunner——学习自主越野驾驶的可通行性估计<br />
**Authors:** Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Atha, Julian Nubert, Curtis Padgett, Marco Hutter, Patrick Spieler<br />
**Abstract:** <details><summary>原文: </summary>Autonomous navigation at high speeds in off-road environments necessitates robots to comprehensively understand their surroundings using onboard sensing only. The extreme conditions posed by the off-road setting can cause degraded camera image quality due to poor lighting and motion blur, as well as limited sparse geometric information available from LiDAR sensing when driving at high speeds. In this work, we present RoadRunner, a novel framework capable of predicting terrain traversability and an elevation map directly from camera and LiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, by fusing sensory information, handling of uncertainty, and generation of contextually informed predictions about the geometry and traversability of the terrain while operating at low latency. In contrast to existing methods relying on classifying handcrafted semantic classes and using heuristics to predict traversability costs, our method is trained end-to-end in a self-supervised fashion. The RoadRunner network architecture builds upon popular sensor fusion network architectures from the autonomous driving domain, which embed LiDAR and camera information into a common Bird's Eye View perspective. Training is enabled by utilizing an existing traversability estimation stack to generate training data in hindsight in a scalable manner from real-world off-road driving datasets. Furthermore, RoadRunner improves the system latency by a factor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for traversability costs and elevation map predictions. We demonstrate the effectiveness of RoadRunner in enabling safe and reliable off-road navigation at high speeds in multiple real-world driving scenarios through unstructured desert environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>在越野环境中高速自主导航需要机器人仅使用机载传感来全面了解周围环境。越野环境造成的极端条件可能会导致相机图像质量下降，原因是照明不佳和运动模糊，以及高速行驶时激光雷达传感提供的稀疏几何信息有限。在这项工作中，我们提出了 RoadRunner，这是一种能够直接根据相机和 LiDAR 传感器输入预测地形可穿越性和高程图的新颖框架。 RoadRunner 通过融合感知信息、处理不确定性以及生成有关地形几何形状和可通行性的上下文预测，同时以低延迟运行，实现可靠的自主导航。与依赖于对手工语义类进行分类并使用启发式方法来预测可遍历性成本的现有方法相比，我们的方法以自我监督的方式进行端到端训练。 RoadRunner 网络架构建立在自动驾驶领域流行的传感器融合网络架构之上，该架构将 LiDAR 和摄像头信息嵌入到通用鸟瞰视角中。训练是通过利用现有的可通行性估计堆栈从现实世界的越野驾驶数据集以可扩展的方式生成事后训练数据来实现的。此外，RoadRunner 将系统延迟提高了大约 4 倍，从 500 毫秒缩短到 140 毫秒，同时提高了可通行成本和高程图预测的准确性。我们展示了 RoadRunner 在穿越非结构化沙漠环境的多个真实驾驶场景中实现安全可靠的高速越野导航的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.19341v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds**<br />
**Title_cn:** PCDepth：基于模式的互补学习，用于单目深度估计，两全其美<br />
**Authors:** Haotian Liu, Sanqing Qu, Fan Lu, Zongtao Bu, Florian Roehrbein, Alois Knoll, Guang Chen<br />
**Abstract:** <details><summary>原文: </summary>Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>事件摄像机可以以高时间分辨率记录场景动态，即使在低光照条件下也可以为单目深度估计 (MDE) 提供丰富的场景细节。因此，现有的 MDE 补充学习方法融合了图像中的强度信息和事件数据中的场景细节，以更好地理解场景。然而，大多数方法直接在像素级融合两种模态，忽略了有吸引力的互补性主要影响仅占据几个像素的高级模式。例如，事件数据可能会补充场景对象的轮廓。在本文中，我们将场景离散化为一组高级模式来探索互补性，并提出了一种用于单目深度估计（PCDepth）的基于模式的互补学习架构。具体来说，PCDepth 包括两个主要组件：一个互补的视觉表示学习模块，用于将场景离散为高级模式并集成跨模态的互补模式；以及一个精炼的深度估计器，旨在场景重建和深度预测，同时保持效率与准确性的平衡。通过基于模式的互补学习，PCDepth 充分利用了两种模式，并实现了比现有方法更准确的预测，特别是在具有挑战性的夜间场景中。在 MVSEC 和 DSEC 数据集上的大量实验验证了我们的 PCDepth 的有效性和优越性。值得注意的是，与最先进的技术相比，PCDepth 在 MVSEC 夜间场景中的准确性提高了 37.9%。</details>
**PDF:** <http://arxiv.org/pdf/2402.18925v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting**<br />
**Title_cn:** SwitchLight：物理驱动架构和人像补光预训练框架的协同设计<br />
**Authors:** Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, Sanghyun Woo<br />
**Abstract:** <details><summary>原文: </summary>We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种联合设计的人体肖像重新照明方法，该方法将物理引导架构与预训练框架相结合。借鉴库克-托伦斯反射率模型，我们精心配置了架构设计，以精确模拟光与表面的相互作用。此外，为了克服高质量光舞台数据稀缺的限制，我们开发了一种自我监督的预训练策略。这种准确的物理建模和扩展的训练数据集的新颖组合为重新照亮现实主义树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2402.18848v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Loss-Free Machine Unlearning**<br />
**Title_cn:** 无损失机器忘却<br />
**Authors:** Jack Foster, Stefan Schoepf, Alexandra Brintrup<br />
**Abstract:** <details><summary>原文: </summary>We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种无需再训练和无标签的机器取消学习方法。大多数现有的机器去学习方法都需要对模型进行微调，以在保持性能的同时删除信息。这在计算上是昂贵的，并且需要在模型的生命周期内存储整个数据集。免再训练方法通常利用费舍尔信息，该信息来自损失，并且需要可能无法获得的标记数据。因此，我们提出了选择性突触抑制算法的扩展，用 Fisher 信息矩阵的对角线替换模型输出的 l2 范数的梯度以近似灵敏度。我们使用 ResNet18 和 Vision Transformer 在一系列实验中评估我们的方法。结果表明，我们的无标记方法与现有的最先进方法相比具有竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2402.19308v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **HyenaPixel: Global Image Context with Convolutions**<br />
**Title_cn:** HyenaPixel：带有卷积的全局图像上下文<br />
**Authors:** Julian Spravil, Sebastian Houben, Sven Behnke<br />
**Abstract:** <details><summary>原文: </summary>In vision tasks, a larger effective receptive field (ERF) is associated with better performance. While attention natively supports global context, convolution requires multiple stacked layers and a hierarchical structure for large context. In this work, we extend Hyena, a convolution-based attention replacement, from causal sequences to the non-causal two-dimensional image space. We scale the Hyena convolution kernels beyond the feature map size up to 191$\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework. For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks. Combining HyenaPixel with attention further increases accuracy to 83.6%. We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena.</details>
**Abstract_cn:** <details><summary>译文: </summary>在视觉任务中，更大的有效感受野（ERF）与更好的性能相关。虽然注意力本身支持全局上下文，但卷积需要多个堆叠层和用于大上下文的分层结构。在这项工作中，我们将基于卷积的注意力替换 Hyena 从因果序列扩展到非因果二维图像空间。我们将 Hyena 卷积核扩展到特征图大小之外，高达 191$\times$191，以最大化 ERF，同时保持像素数量的次二次复杂度。我们将二维 Hyena、HyenaPixel 和双向 Hyena 集成到 MetaFormer 框架中。对于图像分类，HyenaPixel 和双向 Hyena 分别实现了 83.0% 和 83.5% 的具有竞争力的 ImageNet-1k top-1 准确率，同时优于其他大内核网络。将 HyenaPixel 与注意力相结合，进一步将准确率提高到 83.6%。我们将注意力的成功归因于后期阶段缺乏空间偏差，并通过双向鬣狗支持这一发现。</details>
**PDF:** <http://arxiv.org/pdf/2402.19305v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Feature boosting with efficient attention for scene parsing**<br />
**Title_cn:** 通过有效关注场景解析来增强特征<br />
**Authors:** Vivek Singh, Shailza Sharma, Fabio Cuzzolin<br />
**Abstract:** <details><summary>原文: </summary>The complexity of scene parsing grows with the number of object and scene classes, which is higher in unrestricted open scenes. The biggest challenge is to model the spatial relation between scene elements while succeeding in identifying objects at smaller scales. This paper presents a novel feature-boosting network that gathers spatial context from multiple levels of feature extraction and computes the attention weights for each level of representation to generate the final class labels. A novel `channel attention module' is designed to compute the attention weights, ensuring that features from the relevant extraction stages are boosted while the others are attenuated. The model also learns spatial context information at low resolution to preserve the abstract spatial relationships among scene elements and reduce computation cost. Spatial attention is subsequently concatenated into a final feature set before applying feature boosting. Low-resolution spatial attention features are trained using an auxiliary task that helps learning a coarse global scene structure. The proposed model outperforms all state-of-the-art models on both the ADE20K and the Cityscapes datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>场景解析的复杂性随着对象和场景类的数量而增加，在不受限制的开放场景中更高。最大的挑战是对场景元素之间的空间关系进行建模，同时成功识别较小尺度的对象。本文提出了一种新颖的特征增强网络，该网络从多个级别的特征提取中收集空间上下文，并计算每个表示级别的注意力权重以生成最终的类标签。设计了一种新颖的“通道注意力模块”来计算注意力权重，确保相关提取阶段的特征得到增强，而其他阶段的特征得到增强。该模型还以低分辨率学习空间上下文信息，以保留场景元素之间的抽象空间关系并降低计算成本。在应用特征增强之前，空间注意力随后被连接到最终的特征集中。使用有助于学习粗略全局场景结构的辅助任务来训练低分辨率空间注意特征。所提出的模型在 ADE20K 和 Cityscapes 数据集上均优于所有最先进的模型。</details>
**PDF:** <http://arxiv.org/pdf/2402.19250v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MemoNav: Working Memory Model for Visual Navigation**<br />
**Title_cn:** MemoNav：视觉导航的工作记忆模型<br />
**Authors:** Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang<br />
**Abstract:** <details><summary>原文: </summary>Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像目标导航是一项具有挑战性的任务，需要代理在不熟悉的环境中导航到图像指示的目标。利用不同场景记忆的现有方法存在探索效率低下的问题，因为它们使用所有历史观察结果来进行决策，而不考虑与目标相关的部分。为了解决这一限制，我们提出了 MemoNav，这是一种用于图像目标导航的新型内存模型，它利用受工作内存启发的管道来提高导航性能。具体来说，我们采用三种类型的导航存储器。地图上的节点特征存储在短期记忆（STM）中，因为这些特征是动态更新的。然后，遗忘模块会保留信息丰富的 STM 部分以提高效率。我们还引入长期记忆（LTM），通过逐步聚合 STM 特征来学习全局场景表示。随后，图注意模块对保留的 STM 和 LTM 进行编码，以生成工作记忆 (WM)，其中包含高效导航所必需的场景特征。这三种内存类型之间的协同作用使代理能够学习和利用拓扑图中与目标相关的场景特征，从而提高了导航性能。我们对多目标任务的评估表明，在 Gibson 和 Matterport3D 场景中，MemoNav 在所有难度级别上都显着优于以前的方法。定性结果进一步说明 MemoNav 规划了更高效的路线。</details>
**PDF:** <http://arxiv.org/pdf/2402.19161v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification**<br />
**Title_cn:** 用于无监督可见光-红外人员重新识别的多原型渐进对比学习<br />
**Authors:** Jiangming Shi, Xiangbo Yin, Yaoxing Wang, Xiaofeng Liu, Yuan Xie, Yanyun Qu<br />
**Abstract:** <details><summary>原文: </summary>Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID problem using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on shared information, overlooking disparity. To address the problem, we propose a Progressive Contrastive Learning with Multi-Prototype (PCLMP) method for USVI-ReID. In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center. This hard prototype is used in the contrastive loss to emphasize disparity. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards hard samples, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%. The source codes will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>无监督可见光-红外人员重识别（USVI-ReID）旨在将红外图像中的指定人员与不带注释的可见光图像进行匹配，反之亦然。 USVI-ReID 是一项具有挑战性但尚未充分探索的任务。大多数现有方法使用基于聚类的对比学习来解决 USVI-ReID 问题，该学习简单地采用聚类中心作为人的表示。然而，集群中心主要关注共享信息，忽视差异性。为了解决这个问题，我们提出了一种用于 USVI-ReID 的渐进式对比学习多原型（PCLMP）方法。简而言之，我们首先通过选择距聚类中心距离最大的样本来生成硬原型。这个硬原型用于对比损失以强调差异。此外，我们不是严格地将查询图像与特定原型对齐，而是通过随机挑选集群内的样本来生成动态原型。这种动态原型用于保留特征的自然多样性，同时减少同时学习共同信息和不同信息时的不稳定性。最后，我们引入了渐进式学习策略，逐渐将模型的注意力转移到硬样本上，避免集群恶化。在公开可用的 SYSU-MM01 和 RegDB 数据集上进行的大量实验验证了所提出方法的有效性。 PCLMP 的性能优于现有最先进的方法，平均 mAP 提高了 3.9%。源代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2402.19026v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow**<br />
**Title_cn:** LoLiSRFlow：通过基于跨尺度变压器的条件流联合单图像低光增强和超分辨率<br />
**Authors:** Ziyu Yue, Jiaxin Gao, Sihan Xie, Yang Liu, Zhixun Su<br />
**Abstract:** <details><summary>原文: </summary>The visibility of real-world images is often limited by both low-light and low-resolution, however, these issues are only addressed in the literature through Low-Light Enhancement (LLE) and Super- Resolution (SR) methods. Admittedly, a simple cascade of these approaches cannot work harmoniously to cope well with the highly ill-posed problem for simultaneously enhancing visibility and resolution. In this paper, we propose a normalizing flow network, dubbed LoLiSRFLow, specifically designed to consider the degradation mechanism inherent in joint LLE and SR. To break the bonds of the one-to-many mapping for low-light low-resolution images to normal-light high-resolution images, LoLiSRFLow directly learns the conditional probability distribution over a variety of feasible solutions for high-resolution well-exposed images. Specifically, a multi-resolution parallel transformer acts as a conditional encoder that extracts the Retinex-induced resolution-and-illumination invariant map as the previous one. And the invertible network maps the distribution of usually exposed high-resolution images to a latent distribution. The backward inference is equivalent to introducing an additional constrained loss for the normal training route, thus enabling the manifold of the natural exposure of the high-resolution image to be immaculately depicted. We also propose a synthetic dataset modeling the realistic low-light low-resolution degradation, named DFSR-LLE, containing 7100 low-resolution dark-light/high-resolution normal sharp pairs. Quantitative and qualitative experimental results demonstrate the effectiveness of our method on both the proposed synthetic and real datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>现实世界图像的可见度通常受到低光和低分辨率的限制，然而，这些问题仅在文献中通过低光增强（LLE）和超分辨率（SR）方法来解决。诚然，这些方法的简单级联不能和谐地工作，以很好地处理高度不适定的问题，同时提高可见性和分辨率。在本文中，我们提出了一种标准化流网络，称为 LoLiSRFLow，专门设计用于考虑联合 LLE 和 SR 固有的退化机制。为了打破低光低分辨率图像到正常光高分辨率图像的一对多映射的束缚，LoLiSRFLow 直接学习高分辨率曝光良好图像的各种可行解决方案的条件概率分布。具体来说，多分辨率并行变换器充当条件编码器，提取 Retinex 引起的分辨率和照明不变图作为前一个图。可逆网络将通常暴露的高分辨率图像的分布映射到潜在分布。后向推理相当于为正常训练路线引入额外的约束损失，从而能够完美地描绘高分辨率图像的自然曝光的流形。我们还提出了一个模拟真实低光低分辨率退化的合成数据集，名为 DFSR-LLE，包含 7100 个低分辨率暗光/高分辨率正常锐利对。定量和定性实验结果证明了我们的方法在所提出的合成数据集和真实数据集上的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2402.18871v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Gradient Alignment for Cross-Domain Face Anti-Spoofing**<br />
**Title_cn:** 跨域人脸反欺骗的梯度对齐<br />
**Authors:** Binh M. Le, Simon S. Woo<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features. Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance. The code is available at https://github.com/leminhbinh0209/CVPR24-FAS.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸反欺骗 (FAS) 领域泛化 (DG) 的最新进展引起了广泛关注。传统方法侧重于设计学习目标和附加模块来隔离特定于领域的特征，同时在其表示中保留领域不变的特征。然而，此类方法通常缺乏对域不变特征的一致维护或域特定特征的完全删除的保证。此外，FAS 的 DG 的大多数先前工作并不能确保收敛到局部平坦最小值，这已被证明对 DG 是有利的。在本文中，我们介绍了 GAC-FAS，这是一种新颖的学习目标，它鼓励模型收敛到最佳平坦最小值，而无需额外的学习模块。与传统的锐度感知最小化器不同，GAC-FAS 识别每个域的上升点，并调节这些点处的泛化梯度更新，以与经验风险最小化 (ERM) 梯度更新保持一致。这种独特的方法专门指导模型对域转移具有鲁棒性。我们通过对具有挑战性的跨域 FAS 数据集进行严格测试来证明 GAC-FAS 的功效，并在其中建立了最先进的性能。该代码可从 https://github.com/leminhbinh0209/CVPR24-FAS 获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.18817v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **BFRFormer: Transformer-based generator for Real-World Blind Face Restoration**<br />
**Title_cn:** BFRFormer：基于 Transformer 的现实世界盲脸恢复生成器<br />
**Authors:** Guojing Ge, Qi Song, Guibo Zhu, Yuting Zhang, Jinglu Chen, Miao Xin, Ming Tang, Jinqiao Wang<br />
**Abstract:** <details><summary>原文: </summary>Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks. To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at https://github.com/s8Znk/BFRFormer.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于未知且复杂的退化，盲人脸恢复是一项具有挑战性的任务。尽管基于人脸先验的方法和基于参考的方法最近已经证明了高质量的结果，但恢复的图像往往包含过度平滑的结果，并且当退化严重时会丢失身份保留的细节。据观察，这归因于短程依赖性，即卷积神经网络的内在限制。为了对远程依赖性进行建模，我们提出了一种基于 Transformer 的盲脸恢复方法，名为 BFRFormer，以端到端的方式重建具有更多身份保留细节的图像。在 BFRFormer 中，为了消除块效应，开发了小波鉴别器和聚合注意力模块，并自适应地应用谱归一化和平衡一致性调节来分别解决训练不稳定和过拟合问题。大量的实验表明，我们的方法在合成数据集和四个真实数据集上优于最先进的方法。源代码、Casia-Test 数据集和预训练模型发布于 https://github.com/s8Znk/BFRFormer。</details>
**PDF:** <http://arxiv.org/pdf/2402.18811v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Learning a Generalized Physical Face Model From Data**<br />
**Title_cn:** 从数据中学习广义的物理人脸模型<br />
**Authors:** Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley<br />
**Abstract:** <details><summary>原文: </summary>Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a simulation-free manner. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于物理的模拟是 3D 面部动画的一种强大方法，因为产生的变形受物理约束控制，可以轻松解决自碰撞、响应外力并执行逼真的解剖编辑。如今的方法是数据驱动的，其中有限元的驱动是从捕获的皮肤几何形状推断出来的。不幸的是，由于初始化材质空间和单独学习每个角色的变形模型的复杂性，这些方法尚未被广泛采用，这通常需要熟练的艺术家进行长时间的网络训练。在这项工作中，我们的目标是通过提出一种通用的物理面部模型，使基于物理的面部动画更容易实现，该模型是我们以无模拟的方式从大型 3D 面部数据集中学习的。经过训练后，我们的模型可以快速适应任何看不见的身份，并自动生成准备制作动画的物理面部模型。验配就像提供单个 3D 面部扫描甚至单个面部图像一样简单。适配后，我们提供直观的动画控制，以及跨角色重新定位动画的能力。一直以来，生成的动画都允许物理效果，例如避免碰撞、重力、麻痹、骨骼重塑等。</details>
**PDF:** <http://arxiv.org/pdf/2402.19477v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation**<br />
**Title_cn:** 光谱与空间的结合：协调 3D 形状匹配和插值<br />
**Authors:** Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard<br />
**Abstract:** <details><summary>原文: </summary>Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管 3D 形状匹配和插值高度相关，但它们经常被单独研究并按顺序应用以关联不同的 3D 形状，从而导致性能次优。在这项工作中，我们提出了一个统一的框架来预测 3D 形状之间的逐点对应和形状插值。为此，我们将深层功能图框架与经典表面变形模型相结合，以映射光谱域和空间域中的形状。一方面，通过合并空间图，与之前的形状匹配功能图方法相比，我们的方法获得了更准确、更平滑的逐点对应关系。另一方面，通过引入光谱图，我们的方法摆脱了常用但计算成本昂贵的测地距离约束，这些约束仅对近等距形状变形有效。此外，我们提出了一种新颖的测试时间适应方案来捕获姿势主导和形状主导的变形。使用不同的具有挑战性的数据集，我们证明了我们的方法在形状匹配和插值方面都优于以前最先进的方法，甚至与监督方法相比也是如此。</details>
**PDF:** <http://arxiv.org/pdf/2402.18920v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey**<br />
**Title_cn:** 用于 3D 人体姿势估计和网格恢复的深度学习：一项调查<br />
**Authors:** Yang Liu, Changzhen Qiu, Zhiyong Zhang<br />
**Abstract:** <details><summary>原文: </summary>3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics. Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area. In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references. To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations. We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions. A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 人体姿态估计和网格恢复吸引了计算机视觉、自动驾驶和机器人等许多领域的广泛研究兴趣。 3D 人体姿势估计和网格恢复的深度学习最近蓬勃发展，提出了许多方法来解决该领域的不同问题。在本文中，为了激发未来的研究，我们通过深入研究 200 多篇参考文献，全面回顾了过去五年该领域深度学习方法的最新进展。据我们所知，这项调查可以说是第一个全面涵盖 3D 人体姿态估计深度学习方法的调查，包括单人和多人方法，以及人体网格恢复，包括基于显式模型和隐式表示。我们还提供了几个公开数据集的比较结果，以及富有洞察力的观察和启发未来的研究方向。定期更新的项目页面可以在 https://github.com/liuyangme/SOTA-3DHPE-HMR 找到。</details>
**PDF:** <http://arxiv.org/pdf/2402.18844v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **NARUTO: Neural Active Reconstruction from Uncertain Target Observations**<br />
**Title_cn:** NARUTO：从不确定目标观察中进行神经主动重建<br />
**Authors:** Ziyue Feng, Huangying Zhan, Zheng Chen, Qingan Yan, Xiangyu Xu, Changjiang Cai, Bing Li, Qilun Zhu, Yi Xu<br />
**Abstract:** <details><summary>原文: </summary>We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们推出了 NARUTO，一种神经主动重建系统，它将混合神经表示与不确定性学习相结合，从而实现高保真表面重建。我们的方法利用多分辨率哈希网格作为映射主干，选择它是因为其卓越的收敛速度和捕获高频局部特征的能力。我们工作的核心是结合不确定性学习模块，该模块动态量化重建不确定性，同时积极开展环境改造。通过利用学习到的不确定性，我们提出了一种新颖的不确定性聚合策略，用于目标搜索和有效的路径规划。我们的系统通过针对不确定的观测进行自主探索，并以卓越的完整性和保真度重建环境。我们还通过主动射线采样策略增强 SOTA 神经 SLAM 系统，展示了这种不确定性感知方法的实用性。使用室内场景模拟器对 NARUTO 在各种环境中进行的广泛评估证实了其在主动重建方面的卓越性能和最先进的状态，其在 Replica 和 MP3D 等基准数据集上令人印象深刻的结果就证明了这一点。</details>
**PDF:** <http://arxiv.org/pdf/2402.18771v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses**<br />
**Title_cn:** 通过基于分束器的混合镜头进行高分辨率光场成像的无监督学习<br />
**Authors:** Jianxin Lei, Chengcai Xu, Langqing Shi, Junhui Hou, Ping Zhou<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset. The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image. Subsequently, we propose an unsupervised learning-based super-resolution framework with the hybrid light field dataset, which adaptively settles the light field spatial super-resolution problem with a complex degradation model. Specifically, we design two loss functions based on pre-trained models that enable the super-resolution network to learn the detailed features and light field parallax structure with only one ground truth. Extensive experiments demonstrate the same superiority of our approach with supervised learning-based state-of-the-art ones. To our knowledge, it is the first end-to-end unsupervised learning-based spatial super-resolution approach in light field imaging research, whose input is available from our beam splitter-based hybrid light field system. The hardware and software together may help promote the application of light field super-resolution to a great extent.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们设计了一种基于分束器的混合光场成像原型，以同时记录4D光场图像和高分辨率2D图像，并制作混合光场数据集。 2D图像可以被认为是4D光场图像的低分辨率中心子孔径图像对应的高分辨率地面实况。随后，我们提出了一种基于混合光场数据集的无监督学习超分辨率框架，它通过复杂的退化模型自适应地解决了光场空间超分辨率问题。具体来说，我们基于预训练模型设计了两种损失函数，使超分辨率网络能够仅用一个基本事实来学习详细特征和光场视差结构。大量的实验证明了我们的方法与基于监督学习的最先进方法具有相同的优越性。据我们所知，这是光场成像研究中第一个基于端到端无监督学习的空间超分辨率方法，其输入可从我们基于分束器的混合光场系统获得。硬件和软件的结合将在很大程度上有助于推动光场超分辨率的应用。</details>
**PDF:** <http://arxiv.org/pdf/2402.19020v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Retrieval-Augmented Generation for AI-Generated Content: A Survey**<br />
**Title_cn:** 人工智能生成内容的检索增强生成：一项调查<br />
**Authors:** Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui<br />
**Abstract:** <details><summary>原文: </summary>The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable foundation model architectures, and the availability of ample high-quality datasets. While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator. We distill the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Project: https://github.com/hymie122/RAG-Survey</details>
**Abstract_cn:** <details><summary>译文: </summary>模型算法的进步、可扩展的基础模型架构以及充足的高质量数据集的可用性促进了人工智能生成内容（AIGC）的发展。尽管 AIGC 取得了令人瞩目的表现，但它仍然面临着挑战，例如难以维护最新的长尾知识、数据泄露的风险以及与训练和推理相关的高成本。检索增强生成（RAG）最近成为解决此类挑战的范例。特别是，RAG 引入了信息检索过程，该过程通过从可用数据存储中检索相关对象来增强 AIGC 结果，从而提高准确性和鲁棒性。在本文中，我们全面回顾了将 RAG 技术集成到 AIGC 场景中的现有努力。我们首先根据检索器如何增强生成器对 RAG 基础进行分类。我们提取了各种检索器和生成器的增强方法的基本抽象。这种统一的视角涵盖了所有 RAG 场景，阐明了有助于未来潜在进步的进步和关键技术。我们还总结了 RAG 的其他增强方法，促进 RAG 系统的有效工程和实施。然后从另一个角度，我们调查了 RAG 在不同模式和任务中的实际应用，为研究人员和从业者提供有价值的参考。此外，我们介绍了 RAG 的基准，讨论了当前 RAG 系统的局限性，并提出了未来研究的潜在方向。项目：https://github.com/hymie122/RAG-Survey</details>
**PDF:** <http://arxiv.org/pdf/2402.19473v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress**<br />
**Title_cn:** 终身基准：快速进步时代的高效模型评估<br />
**Authors:** Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, Samuel Albanie<br />
**Abstract:** <details><summary>原文: </summary>Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the "benchmark exhaustion" problem.</details>
**Abstract_cn:** <details><summary>译文: </summary>标准化基准推动机器学习的进步。然而，随着重复测试，随着算法过度利用基准特性，过度拟合的风险也会增加。在我们的工作中，我们寻求通过编制不断扩大的大规模基准（称为“终身基准”）来缓解这一挑战。作为我们方法的示例，我们创建了 Lifelong-CIFAR10 和 Lifelong-ImageNet，分别包含（目前）1.69M 和 1.98M 测试样本。在减少过度拟合的同时，终身基准带来了一个关键挑战：在不断扩大的样本集中评估越来越多的模型的成本很高。为了应对这一挑战，我们还引入了一个高效的评估框架：排序\&搜索（S&S），它通过利用动态编程算法有选择地对测试样本进行排名和子选择来重用先前评估的模型，从而实现具有成本效益的终身基准测试。对 31,000 个模型的广泛实证评估表明，S&S 实现了高效的近似精度测量，将单个 A100 GPU 上的计算成本从 180 个 GPU 天减少到 5 个 GPU 小时（减少了 1000 倍），并且近似误差较低。因此，终身基准为“基准耗尽”问题提供了稳健、实用的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.19472v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Towards Safe and Reliable Autonomous Driving: Dynamic Occupancy Set Prediction**<br />
**Title_cn:** 迈向安全可靠的自动驾驶：动态占用集预测<br />
**Authors:** Wenbo Shao, Jiahui Xu, Wenhao Yu, Jun Li, Hong Wang<br />
**Abstract:** <details><summary>原文: </summary>In the rapidly evolving field of autonomous driving, accurate trajectory prediction is pivotal for vehicular safety. However, trajectory predictions often deviate from actual paths, particularly in complex and challenging environments, leading to significant errors. To address this issue, our study introduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancing trajectory prediction capabilities. This method effectively combines advanced trajectory prediction networks with a DOS prediction module, overcoming the shortcomings of existing models. It provides a comprehensive and adaptable framework for predicting the potential occupancy sets of traffic participants. The main contributions of this research include: 1) A novel DOS prediction model tailored for complex scenarios, augmenting traditional trajectory prediction; 2) The development of unique DOS representations and evaluation metrics; 3) Extensive validation through experiments, demonstrating enhanced performance and adaptability. This research contributes to the advancement of safer and more efficient intelligent vehicle and transportation systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>在快速发展的自动驾驶领域，准确的轨迹预测对于车辆安全至关重要。然而，轨迹预测通常会偏离实际路径，特别是在复杂且具有挑战性的环境中，从而导致重大错误。为了解决这个问题，我们的研究引入了一种动态占用集（DOS）预测的新方法，增强了轨迹预测能力。该方法有效地将先进的轨迹预测网络与DOS预测模块结合起来，克服了现有模型的缺点。它提供了一个全面且适应性强的框架，用于预测交通参与者的潜在占用集。这项研究的主要贡献包括：1）针对复杂场景量身定制的新型 DOS 预测模型，增强了传统的轨迹预测； 2）开发独特的DOS表示和评估指标； 3）通过实验进行广泛验证，展示出增强的性能和适应性。这项研究有助于推进更安全、更高效的智能车辆和交通系统。</details>
**PDF:** <http://arxiv.org/pdf/2402.19385v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma**<br />
**Title_cn:** 基于人工智能的肿瘤免疫微环境数字评分可预测晚期食管胃腺癌维持免疫治疗的益处<br />
**Authors:** Quoc Dang Vu, Caroline Fong, Anderley Gordon, Tom Lund, Tatiany L Silveira, Daniel Rodrigues, Katharina von Loga, Shan E Ahmed Raza, David Cunningham, Nasir Rajpoot<br />
**Abstract:** <details><summary>原文: </summary>Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival. However, our understanding of the tumour immune microenvironment in OG cancers remains limited. In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p < 0.05) as well as those who could potentially benefit from ICI with statistical significance (p < 0.05) for both progression free and overall survival. Our findings suggest that T cells that express FOXP3 seem to heavily influence the patient treatment response and survival outcome. We also observed that higher levels of CD8+PD1+ cells are consistently linked to poor prognosis for both OS and PFS, regardless of ICI.</details>
**Abstract_cn:** <details><summary>译文: </summary>胃癌和食管癌 (OG) 是全球癌症死亡的主要原因。在 OG 癌症中，最近的研究表明 PDL1 免疫检查点抑制剂 (ICI) 与化疗相结合可提高患者的生存率。然而，我们对 OG 癌症中肿瘤免疫微环境的了解仍然有限。在这项研究中，我们研究了在 PLATFORM 试验 (NCT02678182) 中接受一线氟嘧啶和铂类化疗的晚期食管胃腺癌 (OGA) 患者的多重免疫荧光 (mIF) 图像，以预测治疗效果并探索患者对维持 durvalumab（PDL1 抑制剂）有反应的生物学基础。我们提出的基于人工智能 (AI) 的标记成功地从无反应者中识别出反应者 (p < 0.05) 以及那些可能从 ICI 中受益的人，对于无进展生存期和总生存期具有统计显着性 (p < 0.05)。我们的研究结果表明，表达 FOXP3 的 T 细胞似乎严重影响患者的治疗反应和生存结果。我们还观察到，无论 ICI 如何，较高水平的 CD8+PD1+ 细胞始终与 OS 和 PFS 的不良预后相关。</details>
**PDF:** <http://arxiv.org/pdf/2402.19296v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **SIFT-Aided Rectified 2D-DIC for Displacement and Strain Measurements in Asphalt Concrete Testing**<br />
**Title_cn:** SIFT 辅助修正 2D-DIC 用于沥青混凝土测试中的位移和应变测量<br />
**Authors:** Zehui Zhu, Imad L. Al-Qadi<br />
**Abstract:** <details><summary>原文: </summary>Two-dimensional digital image correlation (2D-DIC) is a widely used optical technique to measure displacement and strain during asphalt concrete (AC) testing. An accurate 2-D DIC measurement can only be achieved when the camera's principal axis is perpendicular to the planar specimen surface. However, this requirement may not be met during testing due to device constraints. This paper proposes a simple and reliable method to correct errors induced by non-perpendicularity. The method is based on image feature matching and rectification. No additional equipment is needed. A theoretical error analysis was conducted to quantify the effect of a non-perpendicular camera alignment on measurement accuracy. The proposed method was validated numerically using synthetic images and experimentally in an AC fracture test. It achieved relatively high accuracy, even under considerable camera rotation angle and large deformation. As a pre-processing technique, the proposed method showed promising performance in assisting the recently developed CrackPropNet for automated crack propagation measurement under a non-perpendicular camera alignment.</details>
**Abstract_cn:** <details><summary>译文: </summary>二维数字图像相关 (2D-DIC) 是一种广泛使用的光学技术，用于测量沥青混凝土 (AC) 测试过程中的位移和应变。只有当相机的主轴垂直于平面样品表面时，才能实现精确的 2-D DIC 测量。然而，由于设备限制，在测试过程中可能无法满足这一要求。本文提出了一种简单可靠的方法来纠正非垂直引起的误差。该方法基于图像特征匹配和校正。不需要额外的设备。进行了理论误差分析，以量化非垂直相机对准对测量精度的影响。使用合成图像对所提出的方法进行了数值验证，并在交流断裂测试中进行了实验。即使在相当大的相机旋转角度和大变形的情况下，它也能实现相对较高的精度。作为一种预处理技术，所提出的方法在协助最近开发的 CrackPropNet 在非垂直相机对准下进行自动裂纹扩展测量方面表现出了良好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2402.19279v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching**<br />
**Title_cn:** 学习立体匹配的视图内和跨视图几何知识<br />
**Authors:** Rui Gong, Weide Liu, Zaiwang Gu, Xulei Yang, Jun Cheng<br />
**Abstract:** <details><summary>原文: </summary>Geometric knowledge has been shown to be beneficial for the stereo matching task. However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked. To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge. ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding. Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships. This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities. Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models.</details>
**Abstract_cn:** <details><summary>译文: </summary>几何知识已被证明对于立体匹配任务是有益的。然而，先前将几何见解集成到立体匹配算法中的尝试主要集中于单个图像的几何知识，而忽略了关键的交叉视图因素，例如遮挡和匹配唯一性。为了解决这一差距，我们提出了一种新颖的视图内和跨视图几何知识学习网络（ICGNet），专门用于吸收视图内和跨视图几何知识。 ICGNet 利用兴趣点的力量作为视图内几何理解的渠道。同时，它利用这些点之间的对应关系来捕获跨视图几何关系。这种双重结合使所提出的 ICGNet 能够在其学习过程中利用视图内和跨视图几何知识，从而大大提高其估计差异的能力。我们广泛的实验证明了 ICGNet 相对于当代领先模型的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2402.19270v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction**<br />
**Title_cn:** 精细结构感知采样：单视图人体重建中像素对齐隐式模型的新采样训练方案<br />
**Authors:** Kennard Yanting Chan, Fayao Liu, Guosheng Lin, Chuan Sheng Foo, Weisi Lin<br />
**Abstract:** <details><summary>原文: </summary>Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out. Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively. Our code is publicly available at https://github.com/kcyt/FSS.</details>
**Abstract_cn:** <details><summary>译文: </summary>像素对齐隐式模型，例如 PIFu、PIFuHD 和 ICON，用于单视图穿着人体重建。这些模型需要使用抽样训练方案进行训练。现有的采样训练方案要么无法捕获薄表面（例如耳朵、手指），要么在重建的网格中导致噪声伪影。为了解决这些问题，我们引入了精细结构感知采样（FSS），这是一种新的采样训练方案，用于训练用于单视图人体重建的像素对齐隐式模型。 FSS 通过主动适应表面的厚度和复杂性来解决上述问题。此外，与现有的采样训练方案不同，FSS 展示了如何在训练过程中利用样本点的法线来改善结果。最后，为了进一步改进训练过程，FSS 提出了用于像素对齐隐式模型的网格厚度损失信号。一旦对像素对齐隐式函数框架进行了轻微的修改，引入这种损失在计算上就变得可行。我们的结果表明，我们的方法在定性和定量上都显着优于 SOTA 方法。我们的代码可在 https://github.com/kcyt/FSS 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2402.19197v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **VIXEN: Visual Text Comparison Network for Image Difference Captioning**<br />
**Title_cn:** VIXEN：用于图像差异字幕的视觉文本比较网络<br />
**Authors:** Alexander Black, Jing Shi, Yifei Fai, Tu Bui, John Collomosse<br />
**Abstract:** <details><summary>原文: </summary>We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 VIXEN - 一种以文本形式简洁总结一对图像之间的视觉差异的技术，以突出显示任何内容操纵。我们提出的网络以成对的方式线性映射图像特征，为预训练的大型语言模型构建软提示。我们通过对最近的 InstructPix2Pix 数据集中通过提示到提示编辑框架生成的综合操作图像进行训练，解决了现有图像差异字幕 (IDC) 数据集中训练数据量低和缺乏操作多样性的挑战。我们使用 GPT-3 生成的变更摘要来扩充该数据集。我们表明，VIXEN 可以为不同的图像内容和编辑类型生成最先进、易于理解的差异字幕，从而可以潜在地缓解通过操纵图像内容传播的错误信息。代码和数据可在 http://github.com/alexblck/vixen 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.19119v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling**<br />
**Title_cn:** 使用局部结构采样进行图像压缩感知编码的深度网络<br />
**Authors:** Wenxue Cui, Xingtao Wang, Xiaopeng Fan, Shaohui Liu, Xinwei Gao, Debin Zhao<br />
**Abstract:** <details><summary>原文: </summary>Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: 1) The widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency. 2) The optimization-based reconstruction methods generally maintain a much higher computational complexity. In this paper, we propose a new CNN based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. At last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods, while maintaining fast computational speed.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的图像压缩感知（CS）编码框架通常解决基于测量编码和基于优化的图像重建的逆问题，但仍然存在以下两个挑战：1）广泛使用的随机采样矩阵，例如高斯随机矩阵（GRM） ），通常会导致测量编码效率低下。 2）基于优化的重建方法通常保持较高的计算复杂度。在本文中，我们提出了一种新的基于 CNN 的使用局部结构采样的图像 CS 编码框架（称为 CSCNet），该框架包括三个功能模块：局部结构采样、测量编码和拉普拉斯金字塔重建。在所提出的框架中，首先开发了一种新的局部结构采样矩阵，而不是GRM，它能够通过局部感知采样策略增强测量之间的相关性。此外，设计的局部结构采样矩阵可以在训练过程中与其他功能模块联合优化。采样后，产生具有高相关性的测量结果，然后由第三方图像编解码器将其编码为最终比特流。最后，提出了拉普拉斯金字塔重建网络，以有效地将目标图像从测量域恢复到图像域。大量的实验结果表明，所提出的方案优于现有最先进的 CS 编码方法，同时保持快速的计算速度。</details>
**PDF:** <http://arxiv.org/pdf/2402.19111v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **DeepEraser: Deep Iterative Context Mining for Generic Text Eraser**<br />
**Title_cn:** DeepEraser：通用文本橡皮擦的深度迭代上下文挖掘<br />
**Authors:** Hao Feng, Wendi Wang, Shaokai Liu, Jiajun Deng, Wengang Zhou, Houqiang Li<br />
**Abstract:** <details><summary>原文: </summary>In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们提出了 DeepEraser，一种用于通用文本删除的有效深度网络。 DeepEraser 利用循环架构，通过迭代操作擦除图像中的文本。我们的想法来自于擦除铅笔稿的过程，其中指定要删除的文本区域受到持续监控，文本逐渐减弱，确保彻底、干净的擦除。从技术上讲，在每次迭代时，都会部署一个创新的擦除模块，该模块不仅显式聚合之前的擦除进度，而且还挖掘额外的语义上下文来擦除目标文本。通过迭代细化，文本区域逐渐被更合适的内容替换，最终收敛到相对准确的状态。此外，引入了自定义掩模生成策略，以提高 DeepEraser 自适应文本删除的能力，而不是不加区别地删除图像中的所有文本。我们的 DeepEraser 非常紧凑，只有 140 万个参数，并以端到端的方式进行训练。为了验证其有效性，在几个流行的基准上进行了广泛的实验，包括 SCUT-Syn、SCUT-EnsText 和 Oxford Synthetic 文本数据集。定量和定性结果证明了我们的 DeepEraser 相对于最先进方法的有效性，以及其在自定义蒙版文本删除方面强大的泛化能力。代码和预训练模型可在 https://github.com/fh2019ustc/DeepEraser 获取</details>
**PDF:** <http://arxiv.org/pdf/2402.19108v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors**<br />
**Title_cn:** 使用视频序列深度视觉先验去除大气湍流<br />
**Authors:** P. Hill, N. Anantrasirichai, A. Achim, D. R. Bull<br />
**Abstract:** <details><summary>原文: </summary>Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects. Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content. Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content. In this paper, we address these problems with a self-supervised learning method that does not require ground truth. The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences. Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window. This efficiently learns spatio-temporal priors leading to a system that effectively mitigates atmospheric turbulence distortions. The experiments show that our method improves visual quality results qualitatively and quantitatively.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于其扭曲效应，大气湍流对视觉图像的解释和视觉感知提出了挑战。基于模型的方法已被用来解决这个问题，但此类方法经常受到与移动内容相关的伪影的影响。相反，基于深度学习的方法依赖于大型且多样化的数据集，这些数据集可能无法有效地表示任何特定内容。在本文中，我们用不需要基本事实的自我监督学习方法来解决这些问题。所提出的方法不依赖于正在处理的单个数据序列之外的任何数据集，而且还能够提高任何输入原始序列或预处理序列的质量。具体来说，我们的方法基于加速的深度图像先验（DIP），但使用像素改组和时间滑动窗口集成时间信息。这可以有效地学习时空先​​验，从而形成一个可以有效减轻大气湍流扭曲的系统。实验表明，我们的方法定性和定量地提高了视觉质量结果。</details>
**PDF:** <http://arxiv.org/pdf/2402.19041v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation**<br />
**Title_cn:** PrivatEyes：使用联合安全多方计算进行基于外观的注视估计<br />
**Authors:** Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf Küsters, Andreas Bulling<br />
**Abstract:** <details><summary>原文: </summary>Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.</details>
**Abstract_cn:** <details><summary>译文: </summary>最新的注视估计方法需要大规模的训练数据，但它们的收集和交换会带来重大的隐私风险。我们提出了 PrivatEyes——第一个基于联合学习（FL）和安全多方计算（MPC）的基于外观的注视估计的隐私增强训练方法。 PrivatEyes 能够在不同用户的多个本地数据集上训练注视估计器，并基于服务器对各个估计器的更新进行安全聚合。即使大多数聚合服务器是恶意的，PrivatEyes 也能保证个人注视数据保持私密性。我们还引入了一种新的数据泄漏攻击 DualView，该攻击表明 PrivatEyes 比以前的方法更有效地限制私人训练数据的泄漏。对 MPIIGaze、MPIIFaceGaze、GazeCapture 和 NVGaze 数据集的评估进一步表明，改进的隐私不会导致注视估计精度降低或计算成本大幅提高 - 两者与非安全数据集相当。</details>
**PDF:** <http://arxiv.org/pdf/2402.18970v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **OHTA: One-shot Hand Avatar via Data-driven Implicit Priors**<br />
**Title_cn:** OHTA：通过数据驱动的隐式先验实现一次性手部头像<br />
**Authors:** Xiaozheng Zheng, Chao Wen, Zhuo Su, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们深入研究了一次性手部头像的创建，从单个图像中快速获得高保真且可驾驶的手部表示。随着数字人类领域的蓬勃发展，对快速、个性化的手部头像创建的需求变得越来越重要。现有技术通常需要大量输入数据，并且在某些情况下可能很麻烦甚至不切实际。为了增强可访问性，我们提出了一种新颖的方法 OHTA（一次性手部头像），可以仅从一张图像创建详细的手部头像。 OHTA 通过学习和利用数据驱动的手先验来解决这个数据有限问题的固有困难。具体来说，我们设计了一个手部先验模型，最初用于 1）利用可用数据学习各种手部先验，随后用于 2）利用先验知识反转和拟合目标身份。 OHTA 展示了仅依靠单个图像即可创建具有一致动画质量的高保真手部头像的能力。此外，我们通过各种应用说明了 OHTA 的多功能性，包括文本到头像转换、手动编辑和身份潜在空间操作。</details>
**PDF:** <http://arxiv.org/pdf/2402.18969v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts**<br />
**Title_cn:** WWW：通过解释神经元概念来解释神经网络的内容、位置和原因的统一框架<br />
**Authors:** Yong Hyun Ahn, Hyeon Bae Kim, Seong Tae Kim<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in neural networks have showcased their remarkable capabilities across various domains. Despite these successes, the "black box" problem still remains. Addressing this, we propose a novel framework, WWW, that offers the 'what', 'where', and 'why' of the neural network decisions in human-understandable terms. Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain 'what'. To address the 'where' and 'why', we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs. Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate 'how' reliable the prediction is. Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability. WWW provides a unified solution for explaining 'what', 'where', and 'why', introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经网络的最新进展展示了它们在各个领域的卓越能力。尽管取得了这些成功，“黑匣子”问题仍然存在。为了解决这个问题，我们提出了一个新颖的框架 WWW，它以人类可理解的术语提供神经网络决策的“内容”、“地点”和“原因”。具体来说，WWW 利用自适应选择进行概念发现，采用自适应余弦相似性和阈值技术来有效地解释“什么”。为了解决“哪里”和“为什么”，我们提出了一种神经元激活图（NAM）与沙普利值的新颖组合，为各个输入生成局部概念图和热图。此外，WWW 引入了一种预测不确定性的方法，利用热图相似性来估计预测的“可靠性”。 WWW 的实验评估表明在定量和定性指标方面均具有卓越的性能，在可解释性方面优于现有方法。 WWW提供了解释“什么”、“在哪里”和“为什么”的统一解决方案，引入了从全局解释中进行本地化解释的方法，并提供了适应各种体系结构的即插即用解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2402.18956v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography**<br />
**Title_cn:** 脑神经束成像的解剖引导纤维轨迹分布估计<br />
**Authors:** Lei Xie, Qingrun Zeng, Huajun Zhou, Guoqiang Xie, Mingchu Li, Jiahao Huang, Jianan Cui, Hao Chen, Yuanjing Feng<br />
**Abstract:** <details><summary>原文: </summary>Diffusion MRI tractography is an important tool for identifying and analyzing the intracranial course of cranial nerves (CNs). However, the complex environment of the skull base leads to ambiguous spatial correspondence between diffusion directions and fiber geometry, and existing diffusion tractography methods of CNs identification are prone to producing erroneous trajectories and missing true positive connections. To overcome the above challenge, we propose a novel CNs identification framework with anatomy-guided fiber trajectory distribution, which incorporates anatomical shape prior knowledge during the process of CNs tracing to build diffusion tensor vector fields. We introduce higher-order streamline differential equations for continuous flow field representations to directly characterize the fiber trajectory distribution of CNs from the tract-based level. The experimental results on the vivo HCP dataset and the clinical MDM dataset demonstrate that the proposed method reduces false-positive fiber production compared to competing methods and produces reconstructed CNs (i.e. CN II, CN III, CN V, and CN VII/VIII) that are judged to better correspond to the known anatomy.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散 MRI 纤维束成像是识别和分析颅神经 (CN) 颅内走行的重要工具。然而，复杂的颅底环境导致扩散方向和纤维几何形状之间的空间对应关系不明确，现有的CNs识别扩散纤维束成像方法容易产生错误的轨迹并丢失真正的正连接。为了克服上述挑战，我们提出了一种具有解剖引导纤维轨迹分布的新型 CNs 识别框架，该框架在 CNs 追踪过程中结合了解剖形状先验知识来构建扩散张量矢量场。我们引入了连续流场表示的高阶流线微分方程，以直接从基于区域的水平表征 CN 的纤维轨迹分布。在 vivo HCP 数据集和临床 MDM 数据集上的实验结果表明，与竞争方法相比，所提出的方法减少了假阳性纤维的产生，并产生了重建的 CN（即 CN II、CN III、CN V 和 CN VII/VIII），被认为更好地对应于已知的解剖结构。</details>
**PDF:** <http://arxiv.org/pdf/2402.18856v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning**<br />
**Title_cn:** GDCNet：使用深度学习对回波平面成像数据进行无校准几何失真校正<br />
**Authors:** Marina Manso Jimeno, Keren Bachi, George Gardner, Yasmin L. Hurd, John Thomas Vaughan Jr., Sairam Geethanath<br />
**Abstract:** <details><summary>原文: </summary>Functional magnetic resonance imaging techniques benefit from echo-planar imaging's fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images. Traditional methods leverage a field map or voxel displacement map for distortion correction. However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance. This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction. GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets. Among the compared models, the 2D self-supervised configuration resulted in a statistically significant improvement to normalized mutual information between distortion-corrected functional and T1-weighted images compared to the benchmark methods FUGUE and TOPUP. Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>功能性磁共振成像技术受益于平面回波成像的快速图像采集，但容易受到主磁场不均匀性的影响，导致图像中出现几何失真和信号丢失伪影。传统方法利用场图或体素位移图进行畸变校正。然而，体素位移图估计需要额外的序列采集，并且估计的准确性影响校正性能。这项工作实现了一种称为 GDCNet 的新颖方法，该方法通过非线性配准到 T1 加权解剖图像来估计几何畸变图，并将其应用于畸变校正。 GDCNet 在回顾性和前瞻性获取的数据集中展示了功能图像的快速畸变校正。在比较模型中，与基准方法 FUGUE 和 TOPUP 相比，2D 自监督配置导致失真校正功能图像和 T1 加权图像之间的归一化互信息在统计上显着改善。此外，GDCNet 模型在预期数据集中的处理速度比 TOPUP 快 14 倍。</details>
**PDF:** <http://arxiv.org/pdf/2402.18777v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Exploration of Learned Lifting-Based Transform Structures for Fully Scalable and Accessible Wavelet-Like Image Compression**<br />
**Title_cn:** 探索基于学习提升的变换结构以实现完全可扩展且可访问的类小波图像压缩<br />
**Authors:** Xinyue Li, Aous Naman, David Taubman<br />
**Abstract:** <details><summary>原文: </summary>This paper provides a comprehensive study on features and performance of different ways to incorporate neural networks into lifting-based wavelet-like transforms, within the context of fully scalable and accessible image compression. Specifically, we explore different arrangements of lifting steps, as well as various network architectures for learned lifting operators. Moreover, we examine the impact of the number of learned lifting steps, the number of channels, the number of layers and the support of kernels in each learned lifting operator. To facilitate the study, we investigate two generic training methodologies that are simultaneously appropriate to a wide variety of lifting structures considered. Experimental results ultimately suggest that retaining fixed lifting steps from the base wavelet transform is highly beneficial. Moreover, we demonstrate that employing more learned lifting steps and more layers in each learned lifting operator do not contribute strongly to the compression performance. However, benefits can be obtained by utilizing more channels in each learned lifting operator. Ultimately, the learned wavelet-like transform proposed in this paper achieves over 25% bit-rate savings compared to JPEG 2000 with compact spatial support.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文在完全可扩展和可访问的图像压缩的背景下，对将神经网络纳入基于提升的类小波变换的不同方法的特征和性能进行了全面的研究。具体来说，我们探索了提升步骤的不同安排，以及学习提升操作员的各种网络架构。此外，我们还检查了学习提升步骤数量、通道数量、层数以及每个学习提升算子中内核支持的影响。为了促进研究，我们研究了两种通用的训练方法，它们同时适用于所考虑的各种举升结构。实验结果最终表明，保留基础小波变换的固定提升步骤是非常有益的。此外，我们证明，在每个学习的提升算子中采用更多的学习提升步骤和更多的层对压缩性能没有太大贡献。然而，通过利用每个有学识的起重操作员的更多渠道可以获得好处。最终，与具有紧凑空间支持的 JPEG 2000 相比，本文提出的学习型小波变换实现了超过 25% 的比特率节省。</details>
**PDF:** <http://arxiv.org/pdf/2402.18761v1><br />
**Code:** null<br />

