## [UPDATED!] **2024-03-01** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks**<br />
**Title_cn:** Diff-Plugin：重振基于扩散的低级任务的细节<br />
**Authors:** Yuhao Liu, Fang Liu, Zhanghan Ke, Nanxuan Zhao, Rynson W. H. Lau<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. However, due to the randomness in the diffusion process, they often struggle with handling diverse low-level tasks that require details preservation. To overcome this limitation, we present a new Diff-Plugin framework to enable a single pre-trained diffusion model to generate high-fidelity results across a variety of low-level tasks. Specifically, we first propose a lightweight Task-Plugin module with a dual branch design to provide task-specific priors, guiding the diffusion process in preserving image content. We then propose a Plugin-Selector that can automatically select different Task-Plugins based on the text instruction, allowing users to edit images by indicating multiple low-level tasks with natural language. We conduct extensive experiments on 8 low-level vision tasks. The results demonstrate the superiority of Diff-Plugin over existing methods, particularly in real-world scenarios. Our ablations further validate that Diff-Plugin is stable, schedulable, and supports robust training across different dataset sizes.</details>
**Abstract_cn:** <details><summary>译文: </summary>在大规模数据集上训练的扩散模型在图像合成方面取得了显着的进展。然而，由于扩散过程的随机性，他们经常难以处理需要保留细节的各种低级任务。为了克服这一限制，我们提出了一个新的 Diff-Plugin 框架，使单个预训练的扩散模型能够在各种低级任务中生成高保真结果。具体来说，我们首先提出了一个具有双分支设计的轻量级任务插件模块，以提供特定于任务的先验，指导保留图像内容的扩散过程。然后，我们提出了一个插件选择器，它可以根据文本指令自动选择不同的任务插件，允许用户通过用自然语言指示多个低级任务来编辑图像。我们对 8 项低级视觉任务进行了广泛的实验。结果证明了 Diff-Plugin 相对于现有方法的优越性，特别是在现实场景中。我们的消融进一步验证了 Diff-Plugin 的稳定性、可调度性，并且支持跨不同数据集大小的稳健训练。</details>
**PDF:** <http://arxiv.org/pdf/2403.00644v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Graph Theory and GNNs to Unravel the Topographical Organization of Brain Lesions in Variants of Alzheimer's Disease Progression**<br />
**Title_cn:** 图论和 GNN 揭示阿尔茨海默病进展变体中脑损伤的拓扑结构<br />
**Authors:** Leopold Hebert-Stevens, Gabriel Jimenez, Benoit Delatour, Lev Stimmer, Daniel Racoceanu<br />
**Abstract:** <details><summary>原文: </summary>This study utilizes graph theory and deep learning to assess variations in Alzheimer's disease (AD) neuropathologies, focusing on classic (cAD) and rapid (rpAD) progression forms. It analyses the distribution of amyloid plaques and tau tangles in postmortem brain tissues. Histopathological images are converted into tau-pathology-based graphs, and derived metrics are used for statistical analysis and in machine learning classifiers. These classifiers incorporate SHAP value explainability to differentiate between cAD and rpAD. Graph neural networks (GNNs) demonstrate greater efficiency than traditional CNN methods in analyzing this data, preserving spatial pathology context. Additionally, GNNs provide significant insights through explainable AI techniques. The analysis shows denser networks in rpAD and a distinctive impact on brain cortical layers: rpAD predominantly affects middle layers, whereas cAD influences both superficial and deep layers of the same cortical regions. These results suggest a unique neuropathological network organization for each AD variant.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究利用图论和深度学习来评估阿尔茨海默病 (AD) 神经病理学的变化，重点关注经典 (cAD) 和快速 (rpAD) 进展形式。它分析了死后脑组织中淀粉样斑块和 tau 蛋白缠结的分布。组织病理学图像被转换为​​基于 tau 病理学的图表，导出的指标用于统计分析和机器学习分类器。这些分类器结合了 SHAP 值的可解释性来区分 cad 和 rpAD。图神经网络 (GNN) 在分析这些数据、保留空间病理学背景方面表现出比传统 CNN 方法更高的效率。此外，GNN 通过可解释的人工智能技术提供了重要的见解。分析显示 rpAD 中的网络更加密集，并对大脑皮质层产生独特的影响：rpAD 主要影响中层，而 cad 影响同一皮质区域的浅层和深层。这些结果表明每种 AD 变体都有独特的神经病理学网络组织。</details>
**PDF:** <http://arxiv.org/pdf/2403.00636v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Rethinking Few-shot 3D Point Cloud Semantic Segmentation**<br />
**Title_cn:** 重新思考少样本 3D 点云语义分割<br />
**Authors:** Zhaochong An, Guolei Sun, Yun Liu, Fayao Liu, Zongwei Wu, Dan Wang, Luc Van Gool, Serge Belongie<br />
**Abstract:** <details><summary>原文: </summary>This paper revisits few-shot 3D point cloud semantic segmentation (FS-PCS), with a focus on two significant issues in the state-of-the-art: foreground leakage and sparse point distribution. The former arises from non-uniform point sampling, allowing models to distinguish the density disparities between foreground and background for easier segmentation. The latter results from sampling only 2,048 points, limiting semantic information and deviating from the real-world practice. To address these issues, we introduce a standardized FS-PCS setting, upon which a new benchmark is built. Moreover, we propose a novel FS-PCS model. While previous methods are based on feature optimization by mainly refining support features to enhance prototypes, our method is based on correlation optimization, referred to as Correlation Optimization Segmentation (COSeg). Specifically, we compute Class-specific Multi-prototypical Correlation (CMC) for each query point, representing its correlations to category prototypes. Then, we propose the Hyper Correlation Augmentation (HCA) module to enhance CMC. Furthermore, tackling the inherent property of few-shot training to incur base susceptibility for models, we propose to learn non-parametric prototypes for the base classes during training. The learned base prototypes are used to calibrate correlations for the background class through a Base Prototypes Calibration (BPC) module. Experiments on popular datasets demonstrate the superiority of COSeg over existing methods. The code is available at: https://github.com/ZhaochongAn/COSeg</details>
**Abstract_cn:** <details><summary>译文: </summary>本文重新审视了少样本 3D 点云语义分割 (FS-PCS)，重点关注当前最先进的两个重要问题：前景泄漏和稀疏点分布。前者源于非均匀点采样，允许模型区分前景和背景之间的密度差异，以便更容易分割。后者仅采样 2,048 个点，限制了语义信息并偏离了现实世界的实践。为了解决这些问题，我们引入了标准化的 FS-PCS 设置，并在此基础上构建了新的基准。此外，我们提出了一种新颖的 FS-PCS 模型。以前的方法基于特征优化，主要通过细化支持特征来增强原型，而我们的方法基于相关性优化，称为相关性优化分割（COSeg）。具体来说，我们为每个查询点计算特定于类的多原型相关性（CMC），表示其与类别原型的相关性。然后，我们提出超相关增强（HCA）模块来增强 CMC。此外，为了解决小样本训练的固有特性会导致模型的基础敏感性，我们建议在训练期间学习基础类的非参数原型。学习的基础原型用于通过基础原型校准（BPC）模块来校准背景类的相关性。对流行数据集的实验证明了 COSeg 相对于现有方法的优越性。代码位于：https://github.com/ZhaochongAn/COSeg</details>
**PDF:** <http://arxiv.org/pdf/2403.00592v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset**<br />
**Title_cn:** 通过自动导出的数据集改善文本到图像生成中的显式空间关系<br />
**Authors:** Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, Eneko Agirre, Frank Keller<br />
**Abstract:** <details><summary>原文: </summary>Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the 'unseen' split, showing that SD$_{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的工作已经观察到，当前的文本到图像系统不能准确反映对象之间的明确空间关系，例如“左侧”或“下方”。我们假设这是因为用于训练这些模型的图像标题中很少出现明确的空间关系。我们提出了一种自动方法，根据现有图像，生成包含 14 个显式空间关系的合成标题。我们引入了 Spatial Relation for Generation (SR4G) 数据集，其中包含 990 万个用于训练的图像标题对，以及超过 6 万个用于评估的标题。为了测试泛化，我们还提供了一个“看不见的”分割，其中训练和测试标题中的对象集是不相交的。 SR4G 是第一个可用于空间微调文本到图像系统的数据集。我们表明，微调两个不同的稳定扩散模型（表示为 SD$_{SR4G}$）可使 VISOR 指标提高多达 9 个点。 “看不见的”分割中存在改进，表明 SD$_{SR4G}$ 能够泛化到看不见的物体。 SD$_{SR4G}$ 用更少的参数改进了最先进的技术，并避免了复杂的架构。我们的分析表明，所有关系的改善都是一致的。数据集和代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2403.00587v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Rethinking cluster-conditioned diffusion models**<br />
**Title_cn:** 重新思考集群条件扩散模型<br />
**Authors:** Nikolas Adaloglou, Tim Kaiser, Felix Michels, Markus Kollmann<br />
**Abstract:** <details><summary>原文: </summary>We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们对使用聚类分配的扩散模型的图像级调节进行了全面的实验研究。我们阐明了有关图像聚类的各个组件如何影响三个数据集的图像合成。通过结合图像聚类和扩散模型的最新进展，我们表明，给定图像合成（视觉组）方面的最佳聚类粒度，聚类调节可以实现最先进的 FID（即 CIFAR10 上的 1.67、2.17）和 CIFAR100），同时获得强大的训练样本效率。最后，我们提出了一种新颖的方法来导出聚类上界，该方法仅使用基于特征的聚类来减少视觉组的搜索空间。与现有方法不同，我们发现聚类和聚类条件图像生成之间没有显着联系。代码和集群分配将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.00570v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Deformable One-shot Face Stylization via DINO Semantic Guidance**<br />
**Title_cn:** 通过 DINO 语义指导进行可变形的一次性面部风格化<br />
**Authors:** Yang Zhou, Zichong Chen, Hui Huang<br />
**Abstract:** <details><summary>原文: </summary>This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a self-supervised vision transformer, specifically DINO-ViT, to establish a robust and consistent facial structure representation across both real and style domains. Our stylization process begins by adapting the StyleGAN generator to be deformation-aware through the integration of spatial transformers (STN). We then introduce two innovative constraints for generator fine-tuning under the guidance of DINO semantics: i) a directional deformation loss that regulates directional vectors in DINO space, and ii) a relative structural consistency constraint based on DINO token self-similarities, ensuring diverse generation. Additionally, style-mixing is employed to align the color generation with the reference, minimizing inconsistent correspondences. This framework delivers enhanced deformability for general one-shot face stylization, achieving notable efficiency with a fine-tuning duration of approximately 10 minutes. Extensive qualitative and quantitative comparisons demonstrate our superiority over state-of-the-art one-shot face stylization methods. Code is available at \url{https://github.com/zichongc/DoesFS}.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文解决了一次性面部风格化的复杂问题，重点是同时考虑外观和结构，这是以前的方法所无法做到的。我们探索与传统的单图像风格参考不同的变形感知面部风格化，而是选择真实风格的图像对。我们方法的基石是利用自监督视觉转换器，特别是 DINO-ViT，在真实和风格领域建立强大且一致的面部结构表示。我们的风格化过程首先通过集成空间变换器（STN）使 StyleGAN 生成器具有变形感知能力。然后，我们在 DINO 语义的指导下引入了两个用于生成器微调的创新约束：i）调节 DINO 空间中的方向向量的方向变形损失，以及 ii）基于 DINO 令牌自相似性的相对结构一致性约束，确保多样性一代。此外，还采用风格混合来使颜色生成与参考保持一致，从而最大限度地减少不一致的对应关系。该框架为一般的一次性面部风格化提供了增强的可变形性，在大约 10 分钟的微调持续时间内实现了显着的效率。广泛的定性和定量比较证明了我们相对于最先进的一次性面部风格化方法的优越性。代码可在 \url{https://github.com/zichongc/DoesFS} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00459v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **An Ordinal Diffusion Model for Generating Medical Images with Different Severity Levels**<br />
**Title_cn:** 生成不同严重程度的医学图像的序数扩散模型<br />
**Authors:** Shumpei Takezaki, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models have recently been used for medical image generation because of their high image quality. In this study, we focus on generating medical images with ordinal classes, which have ordinal relationships, such as severity levels. We propose an Ordinal Diffusion Model (ODM) that controls the ordinal relationships of the estimated noise images among the classes. Our model was evaluated experimentally by generating retinal and endoscopic images of multiple severity classes. ODM achieved higher performance than conventional generative models by generating realistic images, especially in high-severity classes with fewer training samples.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型由于其高图像质量而最近被用于医学图像生成。在本研究中，我们专注于生成具有序数类的医学图像，这些图像具有序数关系，例如严重程度。我们提出了一种序数扩散模型（ODM），它控制类之间估计噪声图像的序数关系。我们的模型通过生成多个严重级别的视网膜和内窥镜图像进行了实验评估。 ODM 通过生成逼真的图像，实现了比传统生成模型更高的性能，特别是在训练样本较少的高严重性类别中。</details>
**PDF:** <http://arxiv.org/pdf/2403.00452v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **LoMOE: Localized Multi-Object Editing via Multi-Diffusion**<br />
**Title_cn:** LoMOE：通过多重扩散进行本地化多对象编辑<br />
**Authors:** Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, Prathosh AP<br />
**Abstract:** <details><summary>原文: </summary>Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text prompts that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and background preservation losses within the latent space ensures that the characteristics of the object being edited are preserved while simultaneously achieving a high-quality, seamless reconstruction of the background with fewer artifacts compared to the current methods. We also curate and release a dataset dedicated to multi-object editing, named $\texttt{LoMOE}$-Bench. Our experiments against existing state-of-the-art methods demonstrate the improved effectiveness of our approach in terms of both image editing quality and inference speed.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型领域的最新发展已经证明了生成高质量即时条件图像编辑的卓越能力。然而，以前的方法主要依赖于图像编辑的文本提示，当对包含单个/多个对象的场景中的特定对象或细粒度区域进行精确编辑时，其效果往往较差。我们引入了一种通过多重扩散过程进行零镜头局部多对象编辑的新颖框架，以克服这一挑战。该框架使用户能够对图像中的对象执行各种操作，例如在复杂场景$\textbf{一次性}$中添加、替换或编辑$\textbf{many}$对象。我们的方法利用前景蒙版和相应的简单文本提示，对目标区域施加局部影响，从而实现高保真图像编辑。潜在空间内的交叉​​注意力和背景保留损失的结合确保了正在编辑的对象的特征得到保留，同时实现高质量、无缝的背景重建，与当前方法相比，伪影更少。我们还策划并发布了一个专用于多对象编辑的数据集，名为 $\texttt{LoMOE}$-Bench。我们针对现有最先进方法的实验证明了我们的方法在图像编辑质量和推理速度方面的有效性的提高。</details>
**PDF:** <http://arxiv.org/pdf/2403.00437v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Abductive Ego-View Accident Video Understanding for Safe Driving Perception**<br />
**Title_cn:** 溯因式自我观看事故视频理解以实现安全驾驶感知<br />
**Authors:** Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, Tat-Seng Chua<br />
**Abstract:** <details><summary>原文: </summary>We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the causal region learning while fixing the content of the original frame background in video generation, to find the dominant cause-effect chain for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art diffusion models. Additionally, we provide careful benchmark evaluations for object detection and accident reason answering since AdVersa-SD relies on precise object and accident reason information.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了 MM-AU，一个用于多模态事故视频理解的新颖数据集。 MM-AU 包含 11,727 个野外自我视图事故视频，每个视频都有时间对齐的文本描述。我们注释了超过 223 万个对象框和 58,650 对基于视频的事故原因，涵盖 58 个事故类别。 MM-AU 支持各种事故理解任务，特别是多模态视频扩散，以了解事故因果链以实现安全驾驶。通过 MM-AU，我们提出了用于安全驾驶感知的绑架事故视频理解框架 (AdVersa-SD)。 AdVersa-SD 通过以对象为中心的视频扩散 (OAVD) 方法执行视频扩散，该方法由溯因 CLIP 模型驱动。该模型涉及对比交互损失，以学习正常、接近事故、事故框架与相应文本描述的配对共现，例如事故原因、预防建议和事故类别。 OAVD在视频生成中固定原始帧背景内容的同时强制进行因果区域学习，以找到某些事故的主导因果链。大量实验验证了 AdVersa-SD 的溯因能力以及 OAVD 相对于最先进的扩散模型的优越性。此外，由于 AdVersa-SD 依赖于精确的物体和事故原因信息，我们还为物体检测和事故原因解答提供仔细的基准评估。</details>
**PDF:** <http://arxiv.org/pdf/2403.00436v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation**<br />
**Title_cn:** HyperSDFusion：桥接语言和几何中的层次结构以增强 3D Text2Shape 生成<br />
**Authors:** Zhiying Leng, Tolga Birdal, Xiaohui Liang, Federico Tombari<br />
**Abstract:** <details><summary>原文: </summary>3D shape generation from text is a fundamental task in 3D representation learning. The text-shape pairs exhibit a hierarchical structure, where a general text like "chair" covers all 3D shapes of the chair, while more detailed prompts refer to more specific shapes. Furthermore, both text and 3D shapes are inherently hierarchical structures. However, existing Text2Shape methods, such as SDFusion, do not exploit that. In this work, we propose HyperSDFusion, a dual-branch diffusion model that generates 3D shapes from a given text. Since hyperbolic space is suitable for handling hierarchical data, we propose to learn the hierarchical representations of text and 3D shapes in hyperbolic space. First, we introduce a hyperbolic text-image encoder to learn the sequential and multi-modal hierarchical features of text in hyperbolic space. In addition, we design a hyperbolic text-graph convolution module to learn the hierarchical features of text in hyperbolic space. In order to fully utilize these text features, we introduce a dual-branch structure to embed text features in 3D feature space. At last, to endow the generated 3D shapes with a hierarchical structure, we devise a hyperbolic hierarchical loss. Our method is the first to explore the hyperbolic hierarchical representation for text-to-shape generation. Experimental results on the existing text-to-shape paired dataset, Text2Shape, achieved state-of-the-art results.</details>
**Abstract_cn:** <details><summary>译文: </summary>从文本生成 3D 形状是 3D 表示学习中的一项基本任务。文本-​​形状对呈现出层次结构，其中像“椅子”这样的一般文本涵盖了椅子的所有 3D 形状，而更详细的提示则指的是更具体的形状。此外，文本和 3D 形状本质上都是分层结构。然而，现有的 Text2Shape 方法（例如 SDFusion）并没有利用这一点。在这项工作中，我们提出了 HyperSDFusion，这是一种双分支扩散模型，可以根据给定文本生成 3D 形状。由于双曲空间适合处理分层数据，因此我们建议学习双曲空间中文本和 3D 形状的分层表示。首先，我们引入双曲文本图像编码器来学习双曲空间中文本的顺序和多模态层次特征。此外，我们设计了一个双曲文本图卷积模块来学习双曲空间中文本的层次特征。为了充分利用这些文本特征，我们引入了双分支结构将文本特征嵌入到 3D 特征空间中。最后，为了赋予生成的 3D 形状分层结构，我们设计了双曲分层损失。我们的方法是第一个探索文本到形状生成的双曲层次表示的方法。现有文本到形状配对数据集 Text2Shape 的实验结果取得了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.00372v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Few-Shot Relation Extraction with Hybrid Visual Evidence**<br />
**Title_cn:** 具有混合视觉证据的少样本关系提取<br />
**Authors:** Jiaying Gong, Hoda Eldardiry<br />
**Abstract:** <details><summary>原文: </summary>The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attention, and hybrid feature attention to fully capture the semantic interaction between visual regions of images and relevant texts. Extensive experiments conducted on two public datasets demonstrate that semantic visual information significantly improves the performance of few-shot relation prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>少镜头关系提取的目标是当只有少数标记实例可用于训练时预测句子中名称实体之间的关系。现有的少样本关系提取方法主要关注单模态信息，例如仅文本。当文本中描述的名称实体之间没有明确的上下文时，这会降低性能。我们提出了一种多模态少镜头关系提取模型（MFS-HVE），该模型利用文本和视觉语义信息来联合学习多模态表示。 MFS-HVE 包括语义特征提取器和多模态融合组件。 MFS-HVE 语义特征提取器被开发用于提取文本和视觉特征。视觉特征包括全局图像特征和图像内的局部对象特征。 MFS-HVE 多模态融合单元使用图像引导注意力、对象引导注意力和混合特征注意力来集成来自各种模态的信息，以充分捕获图像视觉区域和相关文本之间的语义交互。在两个公共数据集上进行的大量实验表明，语义视觉信息显着提高了少样本关系预测的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.00724v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding**<br />
**Title_cn:** HALC：通过自适应焦点对比度解码减少物体幻觉<br />
**Authors:** Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei Zhou<br />
**Abstract:** <details><summary>原文: </summary>While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然大型视觉语言模型 (LVLM) 在解释多模态上下文方面表现出了令人印象深刻的能力，但它们总是会遭受物体幻觉 (OH) 的困扰。我们引入 HALC，这是一种新颖的解码算法，旨在减轻 LVLM 中的 OH。 HALC 在视觉语言任务中利用独特的细粒度最佳视觉信息，并同时在本地和全局上下文中运行。具体来说，HALC 集成了强大的自动聚焦接地机制（本地）来即时纠正幻觉标记，并集成了专门的波束搜索算法（全局）来显着减少 OH，同时保持文本生成质量。此外，HALC 可以作为即插即用模块集成到任何 LVLM 中，无需额外培训。广泛的实验研究证明了 HALC 在减少 OH 方面的有效性，在四个基准方面均优于最先进的技术。</details>
**PDF:** <http://arxiv.org/pdf/2403.00425v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks**<br />
**Title_cn:** 通过使用多模态测量探索认知负荷和情绪唤醒的动态相互作用：情绪参与任务中瞳孔直径和情绪唤醒的相关性<br />
**Authors:** C. Kosel, S. Michel, T. Seidel, M. Foerster<br />
**Abstract:** <details><summary>原文: </summary>Multimodal data analysis and validation based on streams from state-of-the-art sensor technology such as eye-tracking or emotion recognition using the Facial Action Coding System (FACTs) with deep learning allows educational researchers to study multifaceted learning and problem-solving processes and to improve educational experiences. This study aims to investigate the correlation between two continuous sensor streams, pupil diameter as an indicator of cognitive workload and FACTs with deep learning as an indicator of emotional arousal (RQ 1a), specifically for epochs of high, medium, and low arousal (RQ 1b). Furthermore, the time lag between emotional arousal and pupil diameter data will be analyzed (RQ 2). 28 participants worked on three cognitively demanding and emotionally engaging everyday moral dilemmas while eye-tracking and emotion recognition data were collected. The data were pre-processed in Phyton (synchronization, blink control, downsampling) and analyzed using correlation analysis and Granger causality tests. The results show negative and statistically significant correlations between the data streams for emotional arousal and pupil diameter. However, the correlation is negative and significant only for epochs of high arousal, while positive but non-significant relationships were found for epochs of medium or low arousal. The average time lag for the relationship between arousal and pupil diameter was 2.8 ms. In contrast to previous findings without a multimodal approach suggesting a positive correlation between the constructs, the results contribute to the state of research by highlighting the importance of multimodal data validation and research on convergent vagility. Future research should consider emotional regulation strategies and emotional valence.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于最先进传感器技术流的多模态数据分析和验证，例如使用面部动作编码系统 (FACT) 和深度学习进行眼动追踪或情绪识别，使教育研究人员能够研究多方面的学习和解决问题的过程并改善教育体验。本研究旨在调查两个连续传感器流、作为认知工作量指标的瞳孔直径和作为情绪唤醒指标的深度学习 FACT 之间的相关性 (RQ 1a)，特别是针对高、中和低唤醒时期 (RQ 1a) 1b).此外，还将分析情绪唤醒和瞳孔直径数据之间的时间滞后（RQ 2）。 28 名参与者研究了三个认知要求高且情感参与的日常道德困境，同时收集了眼球追踪和情绪识别数据。数据在 Phyton 中进行预处理（同步、眨眼控制、下采样），并使用相关分析和格兰杰因果关系检验进行分析。结果显示，情绪唤醒和瞳孔直径的数据流之间存在负相关且具有统计显着性的相关性。然而，这种相关性是负相关的，并且仅对于高唤醒时期才显着，而对于中或低唤醒时期则存在正相关但不显着的关系。觉醒和瞳孔直径之间关系的平均时滞为 2.8 毫秒。与之前没有采用多模态方法表明结构之间存在正相关性的研究结果相反，这些结果通过强调多模态数据验证和收敛波动性研究的重要性，对研究状态做出了贡献。未来的研究应该考虑情绪调节策略和情绪效价。</details>
**PDF:** <http://arxiv.org/pdf/2403.00366v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes**<br />
**Title_cn:** MS-Net：多场景运动预测的多路径稀疏模型<br />
**Authors:** Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, Yafeng Guo<br />
**Abstract:** <details><summary>原文: </summary>The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted as a multi-task learning problem, an evolutionary algorithm is designed to encourage the network search of the optimal parameters for each scene while sharing common knowledge between different scenes. Our experiment results show that with substantially reduced parameters, MS-Net outperforms existing state-of-the-art methods on well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and ranks the 2nd place on the INTERACTION challenge.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类行为的多模态和随机特征使得运动预测成为一项极具挑战性的任务，这对于自动驾驶至关重要。尽管深度学习方法已经在该领域展示了其巨大潜力，但在多个驾驶场景（例如并道、环岛、交叉口）和深度学习模型的设计之间建立联系仍然悬而未决。当前基于学习的方法通常使用一种统一的模型来预测不同场景中的轨迹，这可能会导致单个场景的次优结果。为了解决这个问题，我们提出了多场景网络（又名 MS-Net），它是一种通过进化过程训练的多路径稀疏模型。 MS-Net 在推理阶段选择性地激活其参数的子集，以生成每个场景的预测结果。在训练阶段，区分场景下的运动预测任务被抽象为多任务学习问题，设计了一种进化算法来鼓励网络搜索每个场景的最优参数，同时共享不同场景之间的公共知识。我们的实验结果表明，通过大幅减少参数，MS-Net 在成熟的行人运动预测数据集（例如 ETH 和 UCY）上优于现有最先进的方法，并在交互挑战中排名第二。</details>
**PDF:** <http://arxiv.org/pdf/2403.00353v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models**<br />
**Title_cn:** 多模态 ArXiv：用于提高大型视觉语言模型科学理解的数据集<br />
**Authors:** Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu<br />
**Abstract:** <details><summary>原文: </summary>Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, with domain-specific training yielding substantial performance gains. Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.</details>
**Abstract_cn:** <details><summary>译文: </summary>以 GPT-4V 为代表的大型视觉语言模型 (LVLM) 在涉及自然场景中的具体图像的各种任务中表现出色。然而，由于科学领域训练数据集的缺乏，他们解释抽象图形（例如几何形状和科学绘图）的能力仍然有限。为了填补这一空白，我们引入了由 ArXivCap 和 ArXivQA 组成的 Multimodal ArXiv，用于增强 LVLM 的科学理解。 ArXivCap 是一个图形标题数据集，包含 640 万张图像和 390 万张标题，这些图像和标题源自跨越各个科学领域的 572K ArXiv 论文。借鉴ArXivCap，我们引入了ArXivQA，这是一个基于科学数据提示GPT-4V生成的问答数据集。 ArXivQA 极大地增强了 LVLM 的数学推理能力，在多模态数学推理基准上实现了 10.4% 的绝对精度增益。此外，我们利用 ArXivCap 设计了四个视觉到文本任务来对 LVLM 进行基准测试。最先进的 LVLM 的评估结果突显了他们与学术人物微妙语义的斗争，而特定领域的培训带来了显着的性能提升。我们的错误分析揭示了当前 LVLM 对视觉上下文的误解、识别错误以及过度简化的字幕生成，为未来的改进提供了线索。</details>
**PDF:** <http://arxiv.org/pdf/2403.00231v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Multi-modal Attribute Prompting for Vision-Language Models**<br />
**Title_cn:** 视觉语言模型的多模态属性提示<br />
**Authors:** Xin Liu, Jiamin Wu, Tianzhu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image representations, yet overlooking multi-modal attribute characteristics. This limitation hinders the model's ability to perceive fine-grained visual details and restricts its generalization ability to a broader range of unseen classes. To address this issue, we propose a Multi-modal Attribute Prompting method (MAP) by jointly exploring textual attribute prompting, visual attribute prompting, and attribute-level alignment. The proposed MAP enjoys several merits. First, we introduce learnable visual attribute prompts enhanced by textual attribute semantics to adaptively capture visual attributes for images from unknown categories, boosting fine-grained visual perception capabilities for CLIP. Second, the proposed attribute-level alignment complements the global alignment to enhance the robustness of cross-modal alignment for open-vocabulary objects. To our knowledge, this is the first work to establish cross-modal attribute-level alignment for CLIP-based few-shot adaptation. Extensive experimental results on 11 datasets demonstrate that our method performs favorably against state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型预训练视觉语言模型 (VLM)，例如 CLIP，对下游任务表现出强大的泛化能力，但在少量场景中表现不佳。现有的提示技术主要关注全局文本和图像表示，而忽视了多模态属性特征。这种限制阻碍了模型感知细粒度视觉细节的能力，并限制了其对更广泛的看不见的类的泛化能力。为了解决这个问题，我们通过共同探索文本属性提示、视觉属性提示和属性级别对齐，提出了一种多模态属性提示方法（MAP）。拟议的 MAP 有几个优点。首先，我们引入了通过文本属性语义增强的可学习视觉属性提示，以自适应地捕获来自未知类别的图像的视觉属性，从而增强 CLIP 的细粒度视觉感知能力。其次，所提出的属性级对齐补充了全局对齐，以增强开放词汇对象的跨模态对齐的鲁棒性。据我们所知，这是第一个为基于 CLIP 的小样本适应建立跨模式属性级对齐的工作。对 11 个数据集的广泛实验结果表明，我们的方法优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.00219v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **DISORF: A Distributed Online NeRF Training and Rendering Framework for Mobile Robots**<br />
**Title_cn:** DISORF：用于移动机器人的分布式在线 NeRF 训练和渲染框架<br />
**Authors:** Chunlin Li, Ruofan Liang, Hanrui Fan, Zhengen Zhang, Sankeerth Durvasula, Nandita Vijaykumar<br />
**Abstract:** <details><summary>原文: </summary>We present a framework, DISORF, to enable online 3D reconstruction and visualization of scenes captured by resource-constrained mobile robots and edge devices. To address the limited compute capabilities of edge devices and potentially limited network availability, we design a framework that efficiently distributes computation between the edge device and remote server. We leverage on-device SLAM systems to generate posed keyframes and transmit them to remote servers that can perform high quality 3D reconstruction and visualization at runtime by leveraging NeRF models. We identify a key challenge with online NeRF training where naive image sampling strategies can lead to significant degradation in rendering quality. We propose a novel shifted exponential frame sampling method that addresses this challenge for online NeRF training. We demonstrate the effectiveness of our framework in enabling high-quality real-time reconstruction and visualization of unknown scenes as they are captured and streamed from cameras in mobile robots and edge devices.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个框架 DISORF，可以对资源有限的移动机器人和边缘设备捕获的场景进行在线 3D 重建和可视化。为了解决边缘设备有限的计算能力和潜在的有限网络可用性，我们设计了一个框架，可以在边缘设备和远程服务器之间有效地分配计算。我们利用设备上的 SLAM 系统生成姿势关键帧并将其传输到远程服务器，远程服务器可以利用 NeRF 模型在运行时执行高质量的 3D 重建和可视化。我们发现了在线 NeRF 训练的一个关键挑战，即简单的图像采样策略可能会导致渲染质量显着下降。我们提出了一种新颖的移位指数帧采样方法来解决在线 NeRF 训练的这一挑战。我们展示了我们的框架在实现未知场景的高质量实时重建和可视化方面的有效性，因为这些场景是从移动机器人和边缘设备的摄像头捕获和流式传输的。</details>
**PDF:** <http://arxiv.org/pdf/2403.00228v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Learning Causal Features for Incremental Object Detection**<br />
**Title_cn:** 学习增量对象检测的因果特征<br />
**Authors:** Zhenwei He, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Object detection limits its recognizable categories during the training phase, in which it can not cover all objects of interest for users. To satisfy the practical necessity, the incremental learning ability of the detector becomes a critical factor for real-world applications. Unfortunately, neural networks unavoidably meet catastrophic forgetting problem when it is implemented on a new task. To this end, many incremental object detection models preserve the knowledge of previous tasks by replaying samples or distillation from previous models. However, they ignore an important factor that the performance of the model mostly depends on its feature. These models try to rouse the memory of the neural network with previous samples but not to prevent forgetting. To this end, in this paper, we propose an incremental causal object detection (ICOD) model by learning causal features, which can adapt to more tasks. Traditional object detection models, unavoidably depend on the data-bias or data-specific features to get the detection results, which can not adapt to the new task. When the model meets the requirements of incremental learning, the data-bias information is not beneficial to the new task, and the incremental learning may eliminate these features and lead to forgetting. To this end, our ICOD is introduced to learn the causal features, rather than the data-bias features when training the detector. Thus, when the model is implemented to a new task, the causal features of the old task can aid the incremental learning process to alleviate the catastrophic forgetting problem. We conduct our model on several experiments, which shows a causal feature without data-bias can make the model adapt to new tasks better. \keywords{Object detection, incremental learning, causal feature.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象检测在训练阶段限制了其可识别类别，无法覆盖用户感兴趣的所有对象。为了满足实际需要，检测器的增量学习能力成为实际应用的关键因素。不幸的是，神经网络在执行新任务时不可避免地会遇到灾难性的遗忘问题。为此，许多增量对象检测模型通过重放样本或从先前模型中进行蒸馏来保留先前任务的知识。然而，他们忽略了一个重要因素，即模型的性能主要取决于其特征。这些模型试图用先前的样本唤醒神经网络的记忆，但不能防止遗忘。为此，在本文中，我们通过学习因果特征提出了一种增量因果对象检测（ICOD）模型，该模型可以适应更多任务。传统的目标检测模型不可避免地依赖于数据偏差或数据特定的特征来获得检测结果，这不能适应新的任务。当模型满足增量学习的要求时，数据偏差信息对新任务不利，增量学习可能会消除这些特征并导致遗忘。为此，我们引入了 ICOD 来学习因果特征，而不是训练检测器时的数据偏差特征。因此，当模型应用于新任务时，旧任务的因果特征可以帮助增量学习过程缓解灾难性遗忘问题。我们对我们的模型进行了多次实验，结果表明没有数据偏差的因果特征可以使模型更好地适应新任务。 \keywords{对象检测、增量学习、因果特征。</details>
**PDF:** <http://arxiv.org/pdf/2403.00591v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Data-efficient Event Camera Pre-training via Disentangled Masked Modeling**<br />
**Title_cn:** 通过解缠屏蔽建模进行数据高效的事件相机预训练<br />
**Authors:** Zhenpeng Huang, Chao Li, Hao Chen, Yongjian Deng, Yifeng Geng, Limin Wang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a new data-efficient voxel-based self-supervised learning method for event cameras. Our pre-training overcomes the limitations of previous methods, which either sacrifice temporal information by converting event sequences into 2D images for utilizing pre-trained image models or directly employ paired image data for knowledge distillation to enhance the learning of event streams. In order to make our pre-training data-efficient, we first design a semantic-uniform masking method to address the learning imbalance caused by the varying reconstruction difficulties of different regions in non-uniform data when using random masking. In addition, we ease the traditional hybrid masked modeling process by explicitly decomposing it into two branches, namely local spatio-temporal reconstruction and global semantic reconstruction to encourage the encoder to capture local correlations and global semantics, respectively. This decomposition allows our selfsupervised learning method to converge faster with minimal pre-training data. Compared to previous approaches, our self-supervised learning method does not rely on paired RGB images, yet enables simultaneous exploration of spatial and temporal cues in multiple scales. It exhibits excellent generalization performance and demonstrates significant improvements across various tasks with fewer parameters and lower computational costs.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新的数据高效的基于体素的事件摄像机自监督学习方法。我们的预训练克服了以前方法的局限性，这些方法要么通过将事件序列转换为二维图像来利用预训练的图像模型来牺牲时间信息，要么直接使用成对的图像数据进行知识蒸馏以增强事件流的学习。为了使我们的预训练数据高效，我们首先设计了一种语义均匀掩码方法，以解决使用随机掩码时由于非均匀数据中不同区域的重建难度不同而导致的学习不平衡问题。此外，我们通过将传统的混合掩模建模过程明确地分解为两个分支（即局部时空重建和全局语义重建）来简化传统的混合掩模建模过程，以鼓励编码器分别捕获局部相关性和全局语义。这种分解使我们的自监督学习方法能够使用最少的预训练数据更快地收敛。与以前的方法相比，我们的自监督学习方法不依赖于成对的 RGB 图像，而是能够同时探索多个尺度的空间和时间线索。它表现出出色的泛化性能，并以更少的参数和更低的计算成本在各种任务中展现出显着的改进。</details>
**PDF:** <http://arxiv.org/pdf/2403.00416v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor**<br />
**Title_cn:** 相机和全局位姿传感器的联合时空校准<br />
**Authors:** Junlin Song, Antoine Richard, Miguel Olivares-Mendez<br />
**Abstract:** <details><summary>原文: </summary>In robotics, motion capture systems have been widely used to measure the accuracy of localization algorithms. Moreover, this infrastructure can also be used for other computer vision tasks, such as the evaluation of Visual (-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic annotation. Yet, to work optimally, these functionalities require having accurate and reliable spatial-temporal calibration parameters between the camera and the global pose sensor. In this study, we provide two novel solutions to estimate these calibration parameters. Firstly, we design an offline target-based method with high accuracy and consistency. Spatial-temporal parameters, camera intrinsic, and trajectory are optimized simultaneously. Then, we propose an online target-less method, eliminating the need for a calibration target and enabling the estimation of time-varying spatial-temporal parameters. Additionally, we perform detailed observability analysis for the target-less method. Our theoretical findings regarding observability are validated by simulation experiments and provide explainable guidelines for calibration. Finally, the accuracy and consistency of two proposed methods are evaluated with hand-held real-world datasets where traditional hand-eye calibration method do not work.</details>
**Abstract_cn:** <details><summary>译文: </summary>在机器人技术中，运动捕捉系统已被广泛用于测量定位算法的准确性。此外，该基础设施还可以用于其他计算机视觉任务，例如视觉（惯性）SLAM动态初始化、多目标跟踪或自动注释的评估。然而，为了最佳地工作，这些功能需要在相机和全局姿态传感器之间具有准确可靠的时空校准参数。在本研究中，我们提供了两种新颖的解决方案来估计这些校准参数。首先，我们设计了一种具有高精度和一致性的基于离线目标的方法。时空参数、相机固有参数和轨迹同时优化。然后，我们提出了一种在线无目标方法，消除了对校准目标的需要，并能够估计随时间变化的时空参数。此外，我们对无目标方法进行了详细的可观测性分析。我们关于可观测性的理论发现通过模拟实验得到了验证，并为校准提供了可解释的指南。最后，使用手持式真实世界数据集评估了两种提出的方​​法的准确性和一致性，而传统的手眼校准方法不起作用。</details>
**PDF:** <http://arxiv.org/pdf/2403.00976v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Can Transformers Capture Spatial Relations between Objects?**<br />
**Title_cn:** 变形金刚可以捕捉物体之间的空间关系吗？<br />
**Authors:** Chuan Wen, Dinesh Jayaraman, Yang Gao<br />
**Abstract:** <details><summary>原文: </summary>Spatial relationships between objects represent key scene information for humans to understand and interact with the world. To study the capability of current computer vision systems to recognize physically grounded spatial relations, we start by proposing precise relation definitions that permit consistently annotating a benchmark dataset. Despite the apparent simplicity of this task relative to others in the recognition literature, we observe that existing approaches perform poorly on this benchmark. We propose new approaches exploiting the long-range attention capabilities of transformers for this task, and evaluating key design principles. We identify a simple "RelatiViT" architecture and demonstrate that it outperforms all current approaches. To our knowledge, this is the first method to convincingly outperform naive baselines on spatial relation prediction in in-the-wild settings. The code and datasets are available in \url{https://sites.google.com/view/spatial-relation}.</details>
**Abstract_cn:** <details><summary>译文: </summary>物体之间的空间关系代表了人类理解世界并与世界互动的关键场景信息。为了研究当前计算机视觉系统识别物理基础空间关系的能力，我们首先提出精确的关系定义，以允许一致地注释基准数据集。尽管与识别文献中的其他任务相比，该任务明显简单，但我们观察到现有方法在此基准上表现不佳。我们提出了利用变压器的远程注意力能力来完成这项任务的新方法，并评估关键设计原则。我们确定了一个简单的“RelatiViT”架构，并证明它优于当前的所有方法。据我们所知，这是第一种在野外环境中的空间关系预测方面令人信服地优于朴素基线的方法。代码和数据集可在 \url{https://sites.google.com/view/spatial-relation} 中找到。</details>
**PDF:** <http://arxiv.org/pdf/2403.00729v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **COLON: The largest COlonoscopy LONg sequence public database**<br />
**Title_cn:** COLON：最大的结肠镜检查长序列公共数据库<br />
**Authors:** Lina Ruiz, Franklin Sierra-Jerez, Jair Ruiz, Fabio Martinez<br />
**Abstract:** <details><summary>原文: </summary>Colorectal cancer is the third most aggressive cancer worldwide. Polyps, as the main biomarker of the disease, are detected, localized, and characterized through colonoscopy procedures. Nonetheless, during the examination, up to 25% of polyps are missed, because of challenging conditions (camera movements, lighting changes), and the close similarity of polyps and intestinal folds. Besides, there is a remarked subjectivity and expert dependency to observe and detect abnormal regions along the intestinal tract. Currently, publicly available polyp datasets have allowed significant advances in computational strategies dedicated to characterizing non-parametric polyp shapes. These computational strategies have achieved remarkable scores of up to 90% in segmentation tasks. Nonetheless, these strategies operate on cropped and expert-selected frames that always observe polyps. In consequence, these computational approximations are far from clinical scenarios and real applications, where colonoscopies are redundant on intestinal background with high textural variability. In fact, the polyps typically represent less than 1% of total observations in a complete colonoscopy record. This work introduces COLON: the largest COlonoscopy LONg sequence dataset with around of 30 thousand polyp labeled frames and 400 thousand background frames. The dataset was collected from a total of 30 complete colonoscopies with polyps at different stages, variations in preparation procedures, and some cases the observation of surgical instrumentation. Additionally, 10 full intestinal background video control colonoscopies were integrated in order to achieve a robust polyp-background frame differentiation. The COLON dataset is open to the scientific community to bring new scenarios to propose computational tools dedicated to polyp detection and segmentation over long sequences, being closer to real colonoscopy scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>结直肠癌是全球第三大侵袭性癌症。息肉作为该疾病的主要生物标志物，可通过结肠镜检查程序进行检测、定位和表征。尽管如此，在检查过程中，由于条件困难（相机移动、照明变化）以及息肉和肠皱襞的密切相似性，高达 25% 的息肉被遗漏。此外，观察和检测肠道异常区域存在明显的主观性和专家依赖性。目前，公开可用的息肉数据集使得致力于表征非参数息肉形状的计算策略取得了重大进展。这些计算策略在分割任务中取得了高达 90% 的出色成绩。尽管如此，这些策略在始终观察息肉的裁剪和专家选择的帧上运行。因此，这些计算近似值与临床场景和实际应用相距甚远，在临床场景和实际应用中，结肠镜检查在具有高结构变异性的肠道背景上是多余的。事实上，在完整的结肠镜检查记录中，息肉通常只占总观察结果的不到 1%。这项工作介绍了 COLON：最大的结肠镜检查长序列数据集，包含大约 3 万个息肉标记帧和 40 万个背景帧。该数据集收集自总共 30 例不同阶段息肉的完整结肠镜检查、准备程序的变化以及某些病例对手术器械的观察。此外，还集成了 10 个完整肠道背景视频控制结肠镜检查，以实现稳健的息肉背景帧区分。 COLON 数据集向科学界开放，以带来新的场景，提出专门用于长序列息肉检测和分割的计算工具，更接近真实的结肠镜检查场景。</details>
**PDF:** <http://arxiv.org/pdf/2403.00663v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Region-Adaptive Transform with Segmentation Prior for Image Compression**<br />
**Title_cn:** 用于图像压缩的具有分割先验的区域自适应变换<br />
**Authors:** Yuxi Liu, Wenhan Yang, Huihui Bai, Yunchao Wei, Yao Zhao<br />
**Abstract:** <details><summary>原文: </summary>Learned Image Compression (LIC) has shown remarkable progress in recent years. Existing works commonly employ CNN-based or self-attention-based modules as transform methods for compression. However, there is no prior research on neural transform that focuses on specific regions. In response, we introduce the class-agnostic segmentation masks (i.e. semantic masks without category labels) for extracting region-adaptive contextual information. Our proposed module, Region-Adaptive Transform, applies adaptive convolutions on different regions guided by the masks. Additionally, we introduce a plug-and-play module named Scale Affine Layer to incorporate rich contexts from various regions. While there have been prior image compression efforts that involve segmentation masks as additional intermediate inputs, our approach differs significantly from them. Our advantages lie in that, to avoid extra bitrate overhead, we treat these masks as privilege information, which is accessible during the model training stage but not required during the inference phase. To the best of our knowledge, we are the first to employ class-agnostic masks as privilege information and achieve superior performance in pixel-fidelity metrics, such as Peak Signal to Noise Ratio (PSNR). The experimental results demonstrate our improvement compared to previously well-performing methods, with about 8.2% bitrate saving compared to VTM-17.0. The code will be released at https://github.com/GityuxiLiu/Region-Adaptive-Transform-with-Segmentation-Prior-for-Image-Compression.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习图像压缩（LIC）近年来取得了显着的进步。现有的工作通常采用基于 CNN 或基于自注意力的模块作为压缩的变换方法。然而，之前还没有针对特定区域的神经变换的研究。作为回应，我们引入了与类无关的分割掩码（即没有类别标签的语义掩码）来提取区域自适应上下文信息。我们提出的模块“区域自适应变换”在掩模引导的不同区域上应用自适应卷积。此外，我们引入了一个名为 Scale Affine Layer 的即插即用模块，以整合来自不同地区的丰富上下文。虽然之前的图像压缩工作涉及分割掩模作为额外的中间输入，但我们的方法与它们有很大不同。我们的优势在于，为了避免额外的比特率开销，我们将这些掩码视为特权信息，可以在模型训练阶段访问，但在推理阶段不需要。据我们所知，我们是第一个采用与类别无关的掩码作为特权信息的公司，并在像素保真度指标（例如峰值信噪比（PSNR））方面实现了卓越的性能。实验结果表明，与之前表现良好的方法相比，我们取得了进步，与 VTM-17.0 相比，比特率节省了约 8.2%。该代码将在 https://github.com/GityuxiLiu/Region-Adaptive-Transform-with-Segmentation-Prior-for-Image-Compression 发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.00628v1><br />
**Code:** <https://github.com/GityuxiLiu/Region-Adaptive-Transform-with-Segmentation-Prior-for-Image-Compression>**<br />
>>**index:** 5<br />
**Title:** **IDTrust: Deep Identity Document Quality Detection with Bandpass Filtering**<br />
**Title_cn:** IDTrust：使用带通滤波进行深度身份文档质量检测<br />
**Authors:** Musab Al-Ghadi, Joris Voerman, Souhail Bakkali, Mickaël Coustaty, Nicolas Sidere, Xavier St-Georges<br />
**Abstract:** <details><summary>原文: </summary>The increasing use of digital technologies and mobile-based registration procedures highlights the vital role of personal identity documents (IDs) in verifying users and safeguarding sensitive information. However, the rise in counterfeit ID production poses a significant challenge, necessitating the development of reliable and efficient automated verification methods. This paper introduces IDTrust, a deep-learning framework for assessing the quality of IDs. IDTrust is a system that enhances the quality of identification documents by using a deep learning-based approach. This method eliminates the need for relying on original document patterns for quality checks and pre-processing steps for alignment. As a result, it offers significant improvements in terms of dataset applicability. By utilizing a bandpass filtering-based method, the system aims to effectively detect and differentiate ID quality. Comprehensive experiments on the MIDV-2020 and L3i-ID datasets identify optimal parameters, significantly improving discrimination performance and effectively distinguishing between original and scanned ID documents.</details>
**Abstract_cn:** <details><summary>译文: </summary>数字技术和基于移动设备的注册程序的使用越来越多，凸显了个人身份证件 (ID) 在验证用户和保护敏感信息方面的重要作用。然而，伪造 ID 产量的增加带来了重大挑战，需要开发可靠且高效的自动化验证方法。本文介绍了 IDTrust，一个用于评估 ID 质量的深度学习框架。 IDTrust 是一个通过使用基于深度学习的方法来提高身份证明文件质量的系统。该方法无需依赖原始文档图案进行质量检查和对齐预处理步骤。因此，它在数据集适用性方面提供了显着改进。通过利用基于带通滤波的方法，该系统旨在有效地检测和区分ID质量。在 MIDV-2020 和 L3i-ID 数据集上进行的综合实验确定了最佳参数，显着提高了辨别性能，并有效区分原始和扫描的身份证件。</details>
**PDF:** <http://arxiv.org/pdf/2403.00573v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Lincoln's Annotated Spatio-Temporal Strawberry Dataset (LAST-Straw)**<br />
**Title_cn:** 林肯带注释的时空草莓数据集 (LAST-Straw)<br />
**Authors:** Katherine Margaret Frances James, Karoline Heiwolt, Daniel James Sargent, Grzegorz Cielniak<br />
**Abstract:** <details><summary>原文: </summary>Automated phenotyping of plants for breeding and plant studies promises to provide quantitative metrics on plant traits at a previously unattainable observation frequency. Developers of tools for performing high-throughput phenotyping are, however, constrained by the availability of relevant datasets on which to perform validation. To this end, we present a spatio-temporal dataset of 3D point clouds of strawberry plants for two varieties, totalling 84 individual point clouds. We focus on the end use of such tools - the extraction of biologically relevant phenotypes - and demonstrate a phenotyping pipeline on the dataset. This comprises of the steps, including; segmentation, skeletonisation and tracking, and we detail how each stage facilitates the extraction of different phenotypes or provision of data insights. We particularly note that assessment is focused on the validation of phenotypes, extracted from the representations acquired at each step of the pipeline, rather than singularly focusing on assessing the representation itself. Therefore, where possible, we provide \textit{in silico} ground truth baselines for the phenotypes extracted at each step and introduce methodology for the quantitative assessment of skeletonisation and the length trait extracted thereof. This dataset contributes to the corpus of freely available agricultural/horticultural spatio-temporal data for the development of next-generation phenotyping tools, increasing the number of plant varieties available for research in this field and providing a basis for genuine comparison of new phenotyping methodology.</details>
**Abstract_cn:** <details><summary>译文: </summary>用于育种和植物研究的植物自动表型分析有望以以前无法达到的观察频率提供植物性状的定量指标。然而，用于执行高通量表型分析的工具的开发人员受到执行验证的相关数据集的可用性的限制。为此，我们提出了两个品种草莓植株 3D 点云的时空数据集，总共 84 个单独的点云。我们专注于此类工具的最终用途 - 提取生物学相关的表型 - 并在数据集上演示表型分析流程。这包括以下步骤：细分、骨架化和跟踪，我们详细介绍了每个阶段如何促进不同表型的提取或提供数据见解。我们特别注意到，评估的重点是表型的验证，从管道每个步骤获取的表示中提取，而不是仅仅专注于评估表示本身。因此，在可能的情况下，我们为每个步骤提取的表型提供 \textit{in silico} 真实基线，并引入用于定量评估骨架化及其提取的长度特征的方法。该数据集有助于免费提供农业/园艺时空数据的语料库，用于开发下一代表型分析工具，增加可用于该领域研究的植物品种数量，并为新表型分析方法的真正比较提供基础。</details>
**PDF:** <http://arxiv.org/pdf/2403.00566v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **SURE: SUrvey REcipes for building reliable and robust deep networks**<br />
**Title_cn:** 当然：构建可靠且强大的深度网络的调查秘诀<br />
**Authors:** Yuting Li, Yingyi Chen, Xuanlong Yu, Dexiong Chen, Xi Shen<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques--spanning model regularization, classifier and optimization--substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the benchmark of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new benchmark for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \url{https://yutingli0606.github.io/SURE/}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们重新审视深度神经网络中的不确定性估计技术，并整合了一套技术以增强其可靠性。我们的研究表明，多种技术（包括模型正则化、分类器和优化）的综合应用可以显着提高图像分类任务中不确定性预测的准确性。这些技术的协同效应在我们新颖的 SURE 方法中达到了顶峰。我们根据故障预测基准严格评估 SURE，故障预测基准是不确定性估计功效的关键测试平台。我们的结果表明，在各种数据集和模型架构中，比单独部署每种技术的模型始终具有更好的性能。当应用于现实世界的挑战时，例如数据损坏、标签噪声和长尾类分布，SURE 表现出卓越的鲁棒性，提供优于或与当前最先进的专业方法相当的结果。特别是在 Animal-10N 和 Food-101N 上使用噪声标签进行学习时，SURE 无需任何特定于任务的调整即可实现最先进的性能。这项工作不仅为稳健的不确定性估计树立了新的基准，而且还为其在可靠性至关重要的各种现实场景中的应用铺平了道路。我们的代码可在 \url{https://yutingli0606.github.io/SURE/} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00543v1><br />
**Code:** <https://github.com/YutingLi0606/SURE>**<br />
>>**index:** 8<br />
**Title:** **Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction**<br />
**Title_cn:** 用于同时命名实体提取和拼写纠正的大型语言模型<br />
**Authors:** Edward Whittaker, Ikuo Kitagishi<br />
**Abstract:** <details><summary>原文: </summary>Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.   In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.   We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.   We show that the best fine-tuned LLM performs as well as, or slightly better than, the best fine-tuned BERT LM, although the differences are not significant. However, the best LLM is also shown to correct OCR errors in some cases, as initially hypothesised.</details>
**Abstract_cn:** <details><summary>译文: </summary>事实证明，BERT 等语言模型 (LM) 在识别文本中的命名实体 (NE) 任务方面表现良好。 BERT LM 通常用作分类器，对输入文本中的各个 token 进行分类，或者对 token 范围进行分类，使其属于一组可能的 NE 类别之一。在本文中，我们假设仅解码器的大型语言模型（LLM）也可以用于生成性地提取 NE，并有可能恢复 NE 的正确表面形式，其中输入中存在的任何拼写错误文本会自动更正。我们微调了两个 BERT LM 作为基线，以及八个开源 LLM，其任务是从通过对日本商店收据图像应用光学字符识别 (OCR) 获得的文本生成 NE；在这项工作中，我们不尝试查找或评估文本中 NE 的位置。我们表明，最佳微调的 LLM 的性能与最佳微调的 BERT LM 一样或稍好，尽管差异并不显着。然而，正如最初假设的那样，在某些情况下，最好的法学硕士也可以纠正 OCR 错误。</details>
**PDF:** <http://arxiv.org/pdf/2403.00528v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation**<br />
**Title_cn:** GLFNET：用于高效医学图像分割的全局-局部（频率）滤波器网络<br />
**Authors:** Athanasios Tragakis, Qianying Liu, Chaitanya Kaul, Swalpa Kumar Roy, Hang Dai, Fani Deligianni, Roderick Murray-Smith, Daniele Faccio<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel transformer-style architecture called Global-Local Filter Network (GLFNet) for medical image segmentation and demonstrate its state-of-the-art performance. We replace the self-attention mechanism with a combination of global-local filter blocks to optimize model efficiency. The global filters extract features from the whole feature map whereas the local filters are being adaptively created as 4x4 patches of the same feature map and add restricted scale information. In particular, the feature extraction takes place in the frequency domain rather than the commonly used spatial (image) domain to facilitate faster computations. The fusion of information from both spatial and frequency spaces creates an efficient model with regards to complexity, required data and performance. We test GLFNet on three benchmark datasets achieving state-of-the-art performance on all of them while being almost twice as efficient in terms of GFLOP operations.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种新颖的变压器式架构，称为全局局部过滤网络（GLFNet），用于医学图像分割，并展示了其最先进的性能。我们用全局-局部过滤器块的组合取代自注意力机制，以优化模型效率。全局滤波器从整个特征图中提取特征，而局部滤波器则自适应地创建为同一特征图的 4x4 块，并添加受限尺度信息。特别是，特征提取发生在频域而不是常用的空间（图像）域中，以促进更快的计算。来自空间和频率空间的信息的融合创建了一个关于复杂性、所需数据和性能的有效模型。我们在三个基准数据集上测试 GLFNet，在所有数据集上都实现了最先进的性能，同时在 GFLOP 操作方面的效率几乎提高了一倍。</details>
**PDF:** <http://arxiv.org/pdf/2403.00396v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Invariant Test-Time Adaptation for Vision-Language Model Generalization**<br />
**Title_cn:** 视觉语言模型泛化的不变测试时间适应<br />
**Authors:** Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu<br />
**Abstract:** <details><summary>原文: </summary>Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while disregarding decision shortcuts during the inference phase. The proposed method effectively alleviates excessive dependence on potentially misleading, task-irrelevant contextual information, while concurrently emphasizing critical, task-related visual cues. We conduct comparative analysis of the proposed method against various approaches which validates its effectiveness.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言基础模型由于其在广泛的图像文本配对数据集上的可扩展性，在众多下游任务中表现出了显着的成功。然而，这些模型在应用于细粒度图像分类等长尾任务时表现出明显的局限性，因为“决策捷径”阻碍了它们的泛化能力。在这项工作中，我们发现 CLIP 模型拥有丰富的特征，包括 \textit{期望的不变因果特征} 和 \textit{不期望的决策捷径}。此外，CLIP在下游任务上表现不佳源于其无法根据特定任务需求有效利用预训练的特征。为了应对这一挑战，本文引入了一种测试时提示调整范例，该范例可以优化可学习的提示，从而迫使模型利用真正的因果不变特征，同时在推理阶段忽略决策捷径。所提出的方法有效地减轻了对潜在误导性的、与任务无关的上下文信息的过度依赖，同时强调了关键的、与任务相关的视觉线索。我们对所提出的方法与各种方法进行了比较分析，验证了其有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00376v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **DAMS-DETR: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion**<br />
**Title_cn:** DAMS-DETR：具有竞争性查询选择和自适应特征融合的动态自适应多光谱检测变压器<br />
**Authors:** Guo Junjie, Gao Chenqiang, Liu Fangcen, Meng Deyu<br />
**Abstract:** <details><summary>原文: </summary>Infrared-visible object detection aims to achieve robust even full-day object detection by fusing the complementary information of infrared and visible images. However, highly dynamically variable complementary characteristics and commonly existing modality misalignment make the fusion of complementary information difficult. In this paper, we propose a Dynamic Adaptive Multispectral Detection Transformer (DAMS-DETR) based on DETR to simultaneously address these two challenges. Specifically, we propose a Modality Competitive Query Selection strategy to provide useful prior information. This strategy can dynamically select basic salient modality feature representation for each object. To effectively mine the complementary information and adapt to misalignment situations, we propose a Multispectral Deformable Cross-attention module to adaptively sample and aggregate multi-semantic level features of infrared and visible images for each object. In addition, we further adopt the cascade structure of DETR to better mine complementary information. Experiments on four public datasets of different scenes demonstrate significant improvements compared to other state-of-the-art methods. The code will be released at https://github.com/gjj45/DAMS-DETR.</details>
**Abstract_cn:** <details><summary>译文: </summary>红外-可见光目标检测旨在通过融合红外和可见光图像的互补信息来实现鲁棒的全天目标检测。然而，高度动态可变的互补特性和普遍存在的模态错位使得互补信息的融合变得困难。在本文中，我们提出了一种基于DETR的动态自适应多光谱检测变压器（DAMS-DETR）来同时解决这两个挑战。具体来说，我们提出了一种模态竞争查询选择策略来提供有用的先验信息。该策略可以动态地为每个对象选择基本显着模态特征表示。为了有效地挖掘互补信息并适应未对准情况，我们提出了一种多光谱可变形交叉注意模块来自适应采样和聚合每个对象的红外和可见图像的多语义级别特征。此外，我们进一步采用DETR的级联结构来更好地挖掘互补信息。对不同场景的四个公共数据集的实验表明，与其他最先进的方法相比，有显着的改进。代码将在 https://github.com/gjj45/DAMS-DETR 发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.00326v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Small, Versatile and Mighty: A Range-View Perception Framework**<br />
**Title_cn:** 小巧、多功能、强大：范围-视图感知框架<br />
**Authors:** Qiang Meng, Xiao Wang, JiaBao Wang, Liujiang Yan, Ke Wang<br />
**Abstract:** <details><summary>原文: </summary>Despite its compactness and information integrity, the range view representation of LiDAR data rarely occurs as the first choice for 3D perception tasks. In this work, we further push the envelop of the range-view representation with a novel multi-task framework, achieving unprecedented 3D detection performances. Our proposed Small, Versatile, and Mighty (SVM) network utilizes a pure convolutional architecture to fully unleash the efficiency and multi-tasking potentials of the range view representation. To boost detection performances, we first propose a range-view specific Perspective Centric Label Assignment (PCLA) strategy, and a novel View Adaptive Regression (VAR) module to further refine hard-to-predict box properties. In addition, our framework seamlessly integrates semantic segmentation and panoptic segmentation tasks for the LiDAR point cloud, without extra modules. Among range-view-based methods, our model achieves new state-of-the-art detection performances on the Waymo Open Dataset. Especially, over 10 mAP improvement over convolutional counterparts can be obtained on the vehicle class. Our presented results for other tasks further reveal the multi-task capabilities of the proposed small but mighty framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管 LiDAR 数据的范围视图表示形式紧凑且信息完整，但它很少作为 3D 感知任务的首选。在这项工作中，我们利用新颖的多任务框架进一步突破了范围视图表示的界限，实现了前所未有的 3D 检测性能。我们提出的小、多功能、强大（SVM）网络利用纯卷积架构来充分释放范围视图表示的效率和多任务潜力。为了提高检测性能，我们首先提出了一种范围视图特定的透视中心标签分配（PCLA）策略，以及一种新颖的视图自适应回归（VAR）模块来进一步细化难以预测的框属性。此外，我们的框架无缝集成了激光雷达点云的语义分割和全景分割任务，无需额外的模块。在基于范围视图的方法中，我们的模型在 Waymo 开放数据集上实现了最先进的检测性能。特别是，在车辆类别上可以获得超过 10 mAP 的卷积提升。我们提出的其他任务的结果进一步揭示了所提出的小而强大的框架的多任务能力。</details>
**PDF:** <http://arxiv.org/pdf/2403.00325v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Embedded Multi-label Feature Selection via Orthogonal Regression**<br />
**Title_cn:** 通过正交回归进行嵌入式多标签特征选择<br />
**Authors:** Xueyuan Xu, Fulin Wei, Tianyuan Jia, Li Zhuo, Feiping Nie, Xia Wu<br />
**Abstract:** <details><summary>原文: </summary>In the last decade, embedded multi-label feature selection methods, incorporating the search for feature subsets into model optimization, have attracted considerable attention in accurately evaluating the importance of features in multi-label classification tasks. Nevertheless, the state-of-the-art embedded multi-label feature selection algorithms based on least square regression usually cannot preserve sufficient discriminative information in multi-label data. To tackle the aforementioned challenge, a novel embedded multi-label feature selection method, termed global redundancy and relevance optimization in orthogonal regression (GRROOR), is proposed to facilitate the multi-label feature selection. The method employs orthogonal regression with feature weighting to retain sufficient statistical and structural information related to local label correlations of the multi-label data in the feature learning process. Additionally, both global feature redundancy and global label relevancy information have been considered in the orthogonal regression model, which could contribute to the search for discriminative and non-redundant feature subsets in the multi-label data. The cost function of GRROOR is an unbalanced orthogonal Procrustes problem on the Stiefel manifold. A simple yet effective scheme is utilized to obtain an optimal solution. Extensive experimental results on ten multi-label data sets demonstrate the effectiveness of GRROOR.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年中，嵌入式多标签特征选择方法，将特征子集的搜索纳入模型优化中，在准确评估多标签分类任务中特征的重要性方面引起了相当大的关注。然而，基于最小二乘回归的最先进的嵌入式多标签特征选择算法通常无法在多标签数据中保留足够的判别信息。为了解决上述挑战，提出了一种新颖的嵌入式多标签特征选择方法，称为正交回归中的全局冗余和相关性优化（GRROOR），以促进多标签特征选择。该方法采用具有特征加权的正交回归，以在特征学习过程中保留与多标签数据的局部标签相关性相关的足够的统计和结构信息。此外，正交回归模型中考虑了全局特征冗余和全局标签相关性信息，这有助于在多标签数据中搜索有区别的和非冗余的特征子集。 GRROOR 的成本函数是 Stiefel 流形上的不平衡正交 Procrustes 问题。利用简单而有效的方案来获得最佳解决方案。十个多标签数据集的广泛实验结果证明了 GRROOR 的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00307v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting**<br />
**Title_cn:** ODM：一种用于场景文本检测和定位的文本-图像进一步对齐预训练方法<br />
**Authors:** Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei<br />
**Abstract:** <details><summary>原文: </summary>In recent years, text-image joint pre-training techniques have shown promising results in various tasks. However, in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and OCR-Text (referring to the text in images as OCR-Text to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called OCR-Text Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text prompt. With ODM, we achieve better alignment between text and OCR-Text and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in OCR tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at {https://github.com/PriNing/ODM}.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，文本-图像联合预训练技术在各种任务中都显示出了可喜的结果。然而，在光学字符识别（OCR）任务中，将文本实例与图像中相应的文本区域对齐是一个挑战，因为它需要文本和 OCR 文本之间的有效对齐（将图像中的文本称为 OCR 文本以区分自然语言的文本）而不是对整体图像内容的整体理解。在本文中，我们提出了一种称为 OCR 文本去风格化建模（ODM）的新预训练方法，该方法可根据文本提示将图像中发现的不同风格的文本转换为统一的风格。通过 ODM，我们可以实现文本和 OCR 文本之间更好的对齐，并使预训练模型能够适应场景文本检测和识别任务的复杂且多样化的风格。此外，我们还专门为 ODM 设计了一种新的标签生成方法，并将其与我们提出的 Text-Controller 模块相结合，以解决 OCR 任务中注释成本的挑战，允许更大量的未标签数据参与预训练。对多个公共数据集的广泛实验表明，我们的方法显着提高了性能，并且在场景文本检测和识别任务中优于当前的预训练方法。代码可在 {https://github.com/PriNing/ODM} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00303v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation**<br />
**Title_cn:** CustomListener：文本引导的响应式交互，用于用户友好的聆听头生成<br />
**Authors:** Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, Pengfei Yan<br />
**Abstract:** <details><summary>原文: </summary>Listening head generation aims to synthesize a non-verbal responsive listener head by modeling the correlation between the speaker and the listener in dynamic conversion.The applications of listener agent generation in virtual interaction have promoted many works achieving the diverse and fine-grained motion generation. However, they can only manipulate motions through simple emotional labels, but cannot freely control the listener's motions. Since listener agents should have human-like attributes (e.g. identity, personality) which can be freely customized by users, this limits their realism. In this paper, we propose a user-friendly framework called CustomListener to realize the free-form text prior guided listener generation. To achieve speaker-listener coordination, we design a Static to Dynamic Portrait module (SDP), which interacts with speaker information to transform static text into dynamic portrait token with completion rhythm and amplitude information. To achieve coherence between segments, we design a Past Guided Generation Module (PGG) to maintain the consistency of customized listener attributes through the motion prior, and utilize a diffusion-based structure conditioned on the portrait token and the motion prior to realize the controllable generation. To train and evaluate our model, we have constructed two text-annotated listening head datasets based on ViCo and RealTalk, which provide text-video paired labels. Extensive experiments have verified the effectiveness of our model.</details>
**Abstract_cn:** <details><summary>译文: </summary>听者头部生成旨在通过对说话者和听者之间动态转换的相关性进行建模来合成非语言响应的听者头部。听者代理生成在虚拟交互中的应用推动了许多实现多样化、细粒度运动生成的工作。然而，他们只能通过简单的情感标签来操纵动作，而无法自由地控制听者的动作。由于监听代理应该具有可以由用户自由定制的类似人类的属性（例如身份、个性），这限制了它们的真实性。在本文中，我们提出了一个名为 CustomListener 的用户友好框架，以实现自由格式文本的引导侦听器生成。为了实现说话者与听众的协调，我们设计了一个静态到动态肖像模块（SDP），该模块与说话者信息交互，将静态文本转换为具有完成节奏和幅度信息的动态肖像标记。为了实现片段之间的连贯性，我们设计了过去引导生成模块（PGG），通过运动先验来保持定制听众属性的一致性，并利用基于肖像令牌和运动先验的基于扩散的结构来实现可控生成。为了训练和评估我们的模型，我们构建了两个基于 ViCo 和 RealTalk 的文本注释听力头数据集，它们提供文本-视频配对标签。大量的实验验证了我们模型的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00274v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval**<br />
**Title_cn:** 双姿势不变嵌入：用于识别和检索的学习类别和特定于对象的判别表示<br />
**Authors:** Rohan Sarkar, Avinash Kak<br />
**Abstract:** <details><summary>原文: </summary>In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class distances and maximize the inter-class distances in the dual representation spaces. We demonstrate the power of our approach with three challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With our dual approach, for single-view object recognition, we outperform the previous best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On the other hand, for single-view object retrieval, we outperform the previous best by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.</details>
**Abstract_cn:** <details><summary>译文: </summary>在姿势不变的对象识别和检索的背景下，我们证明，如果在训练期间同时学习基于类别和基于对象身份的嵌入，则可以实现性能的显着提高。事后看来，这听起来很直观，因为了解类别比了解与这些类别相对应的单个对象更为基础。然而，据我们所知，姿势不变学习的先前工作还没有证明这种效果。本文提出了一种基于注意力的双编码器架构，具有专门设计的损失函数，可以在两个不同的嵌入空间中同时优化类间和类内距离，一个用于类别嵌入，另一个用于对象级嵌入。我们提出的损失函数是姿势不变的排名损失，旨在最小化双表示空间中的类内距离并最大化类间距离。我们通过三个具有挑战性的多视图数据集 ModelNet-40、ObjectPI 和 FG3D 展示了我们方法的强大功能。通过我们的双方法，对于单视图对象识别，我们在 ModelNet40 上比之前的最佳表现高出 20.0%，在 ObjectPI 上比之前的最佳成绩高出 2.0%，在 FG3D 上比之前的最佳成绩高出 46.5%。另一方面，对于单视图对象检索，我们在 ModelNet40 上比之前的最好成绩高出 33.7%，在 ObjectPI 上比之前的最好成绩高出 18.8%，在 FG3D 上比之前的最好成绩高出 56.9%。</details>
**PDF:** <http://arxiv.org/pdf/2403.00272v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies**<br />
**Title_cn:** 使用挤压和激励卷积神经网络对放射性肺气肿亚型进行稳健深度标记：MESA Lung 和 SPIROMICS 研究<br />
**Authors:** Artur Wysoczanski, Nabil Ettehadi, Soroush Arabshahi, Yifei Sun, Karen Hinkley Stukovsky, Karol E. Watson, MeiLan K. Han, Erin D Michos, Alejandro P. Comellas, Eric A. Hoffman, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is conventionally categorized into three subtypes identifiable on pathology and on lung computed tomography (CT) images. Recent work has led to the unsupervised learning of ten spatially-informed lung texture patterns (sLTPs) on lung CT, representing distinct patterns of emphysematous lung parenchyma based on both textural appearance and spatial location within the lung, and which aggregate into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods for sLTP segmentation, however, are slow and highly sensitive to changes in CT acquisition protocol. In this work, we present a robust 3-D squeeze-and-excitation CNN for supervised classification of sLTPs and CTES on lung CT. Our results demonstrate that this model achieves accurate and reproducible sLTP segmentation on lung CTscans, across two independent cohorts and independently of scanner manufacturer and model.</details>
**Abstract_cn:** <details><summary>译文: </summary>肺气肿是一种进行性、不可逆的肺组织损失，通常分为可通过病理学和肺部计算机断层扫描 (CT) 图像识别的三种亚型。最近的工作导致了对肺部 CT 上 10 个空间信息肺纹理模式 (sLTP) 的无监督学习，这些模式代表基于肺内纹理外观和空间位置的肺气肿肺实质的不同模式，并聚合成 6 个稳健且可重复的模式CT 肺气肿亚型 (CTES)。然而，现有的 sLTP 分割方法速度缓慢，并且对 CT 采集协议的变化高度敏感。在这项工作中，我们提出了一种强大的 3D 挤压和激励 CNN，用于肺部 CT 上 sLTP 和 CTES 的监督分类。我们的结果表明，该模型在肺部 CT 扫描上实现了准确且可重复的 sLTP 分割，跨越两个独立队列，并且独立于扫描仪制造商和型号。</details>
**PDF:** <http://arxiv.org/pdf/2403.00257v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Cloud-based Federated Learning Framework for MRI Segmentation**<br />
**Title_cn:** 基于云的 MRI 分割联合学习框架<br />
**Authors:** Rukesh Prajapati, Amr S. El-Wakeel<br />
**Abstract:** <details><summary>原文: </summary>In contemporary rural healthcare settings, the principal challenge in diagnosing brain images is the scarcity of available data, given that most of the existing deep learning models demand extensive training data to optimize their performance, necessitating centralized processing methods that potentially compromise data privacy. This paper proposes a novel framework tailored for brain tissue segmentation in rural healthcare facilities. The framework employs a deep reinforcement learning (DRL) environment in tandem with a refinement model (RM) deployed locally at rural healthcare sites. The proposed DRL model has a reduced parameter count and practicality for implementation across distributed rural sites. To uphold data privacy and enhance model generalization without transgressing privacy constraints, we employ federated learning (FL) for cooperative model training. We demonstrate the efficacy of our approach by training the network with a limited data set and observing a substantial performance enhancement, mitigating inaccuracies and irregularities in segmentation across diverse sites. Remarkably, the DRL model attains an accuracy of up to 80%, surpassing the capabilities of conventional convolutional neural networks when confronted with data insufficiency. Incorporating our RM results in an additional accuracy improvement of at least 10%, while FL contributes to a further accuracy enhancement of up to 5%. Collectively, the framework achieves an average 92% accuracy rate within rural healthcare settings characterized by data constraints.</details>
**Abstract_cn:** <details><summary>译文: </summary>在当代农村医疗保健环境中，诊断大脑图像的主要挑战是可用数据的稀缺，因为大多数现有的深度学习模型需要大量的训练数据来优化其性能，从而需要集中处理方法，这可能会损害数据隐私。本文提出了一种适合农村医疗机构脑组织分割的新颖框架。该框架采用深度强化学习 (DRL) 环境以及在农村医疗机构本地部署的细化模型 (RM)。所提出的 DRL 模型减少了参数数量，并且对于跨分布式农村站点实施具有实用性。为了在不违反隐私限制的情况下维护数据隐私并增强模型泛化能力，我们采用联邦学习（FL）进行协作模型训练。我们通过使用有限的数据集训练网络并观察性能的显着提高，减少不同站点之间分割的不准确性和不规则性，证明了我们方法的有效性。值得注意的是，DRL 模型的准确率高达 80%，在数据不足的情况下超越了传统卷积神经网络的能力。结合我们的 RM 可以使准确度额外提高至少 10%，而 FL 则有助于进一步提高高达 5% 的准确度。总的来说，该框架在数据有限的农村医疗保健环境中实现了平均 92% 的准确率。</details>
**PDF:** <http://arxiv.org/pdf/2403.00254v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach**<br />
**Title_cn:** 重新思考长尾识别中的分类器重新训练：一种简单的 Logits 重定向方法<br />
**Authors:** Han Lu, Siyu Sun, Yichen Xie, Liqing Zhang, Xiaokang Yang, Junchi Yan<br />
**Abstract:** <details><summary>原文: </summary>In the long-tailed recognition field, the Decoupled Training paradigm has demonstrated remarkable capabilities among various methods. This paradigm decouples the training process into separate representation learning and classifier re-training. Previous works have attempted to improve both stages simultaneously, making it difficult to isolate the effect of classifier re-training. Furthermore, recent empirical studies have demonstrated that simple regularization can yield strong feature representations, emphasizing the need to reassess existing classifier re-training methods. In this study, we revisit classifier re-training methods based on a unified feature representation and re-evaluate their performances. We propose a new metric called Logits Magnitude as a superior measure of model performance, replacing the commonly used Weight Norm. However, since it is hard to directly optimize the new metric during training, we introduce a suitable approximate invariant called Regularized Standard Deviation. Based on the two newly proposed metrics, we prove that reducing the absolute value of Logits Magnitude when it is nearly balanced can effectively decrease errors and disturbances during training, leading to better model performance. Motivated by these findings, we develop a simple logits retargeting approach (LORT) without the requirement of prior knowledge of the number of samples per class. LORT divides the original one-hot label into small true label probabilities and large negative label probabilities distributed across each class. Our method achieves state-of-the-art performance on various imbalanced datasets, including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.</details>
**Abstract_cn:** <details><summary>译文: </summary>在长尾识别领域，解耦训练范式在各种方法中表现出了卓越的能力。这种范例将训练过程解耦为单独的表示学习和分类器重新训练。以前的工作试图同时改进这两个阶段，这使得很难隔离分类器重新训练的效果。此外，最近的实证研究表明，简单的正则化可以产生强大的特征表示，强调需要重新评估现有的分类器重新训练方法。在本研究中，我们重新审视基于统一特征表示的分类器重新训练方法，并重新评估其性能。我们提出了一种称为 Logits Magnitude 的新指标，作为模型性能的高级衡量指标，取代常用的权重范数。然而，由于在训练过程中很难直接优化新指标，因此我们引入了一种合适的近似不变量，称为正则化标准差。基于这两个新提出的指标，我们证明在接近平衡时降低 Logits Magnitude 的绝对值可以有效减少训练过程中的错误和干扰，从而获得更好的模型性能。受这些发现的启发，我们开发了一种简单的逻辑重定向方法（LORT），无需事先了解每类样本的数量。 LORT 将原始单热标签分为分布在每个类别中的较小的真实标签概率和较大的负标签概率。我们的方法在各种不平衡数据集上实现了最先进的性能，包括 CIFAR100-LT、ImageNet-LT 和 iNaturalist2018。</details>
**PDF:** <http://arxiv.org/pdf/2403.00250v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **YOLO-MED : Multi-Task Interaction Network for Biomedical Images**<br />
**Title_cn:** YOLO-MED：生物医学图像的多任务交互网络<br />
**Authors:** Suizhi Huang, Shalayiding Sirejiding, Yuxiang Lu, Yue Ding, Leheng Liu, Hui Zhou, Hongtao Lu<br />
**Abstract:** <details><summary>原文: </summary>Object detection and semantic segmentation are pivotal components in biomedical image analysis. Current single-task networks exhibit promising outcomes in both detection and segmentation tasks. Multi-task networks have gained prominence due to their capability to simultaneously tackle segmentation and detection tasks, while also accelerating the segmentation inference. Nevertheless, recent multi-task networks confront distinct limitations such as the difficulty in striking a balance between accuracy and inference speed. Additionally, they often overlook the integration of cross-scale features, which is especially important for biomedical image analysis. In this study, we propose an efficient end-to-end multi-task network capable of concurrently performing object detection and semantic segmentation called YOLO-Med. Our model employs a backbone and a neck for multi-scale feature extraction, complemented by the inclusion of two task-specific decoders. A cross-scale task-interaction module is employed in order to facilitate information fusion between various tasks. Our model exhibits promising results in balancing accuracy and speed when evaluated on the Kvasir-seg dataset and a private biomedical image dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>对象检测和语义分割是生物医学图像分析的关键组成部分。当前的单任务网络在检测和分割任务中都表现出了有希望的结果。多任务网络因其同时处理分割和检测任务的能力而受到关注，同时还加速了分割推理。然而，最近的多任务网络面临着明显的局限性，例如难以在准确性和推理速度之间取得平衡。此外，他们经常忽视跨尺度特征的整合，这对于生物医学图像分析尤其重要。在本研究中，我们提出了一种高效的端到端多任务网络，能够同时执行对象检测和语义分割，称为 YOLO-Med。我们的模型采用主干和颈部进行多尺度特征提取，并辅以两个特定于任务的解码器。采用跨尺度任务交互模块以促进各种任务之间的信息融合。在 Kvasir-seg 数据集和私人生物医学图像数据集上进行评估时，我们的模型在平衡准确性和速度方面表现出了有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.00245v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Trustworthy Self-Attention: Enabling the Network to Focus Only on the Most Relevant References**<br />
**Title_cn:** 值得信赖的自我关注：使网络仅关注最相关的参考文献<br />
**Authors:** Yu Jing, Tan Yujuan, Ren Ao, Liu Duo<br />
**Abstract:** <details><summary>原文: </summary>The prediction of optical flow for occluded points is still a difficult problem that has not yet been solved. Recent methods use self-attention to find relevant non-occluded points as references for estimating the optical flow of occluded points based on the assumption of self-similarity. However, they rely on visual features of a single image and weak constraints, which are not sufficient to constrain the trained network to focus on erroneous and weakly relevant reference points. We make full use of online occlusion recognition information to construct occlusion extended visual features and two strong constraints, allowing the network to learn to focus only on the most relevant references without requiring occlusion ground truth to participate in the training of the network. Our method adds very few network parameters to the original framework, making it very lightweight. Extensive experiments show that our model has the greatest cross-dataset generalization. Our method achieves much greater error reduction, 18.6%, 16.2%, and 20.1% for all points, non-occluded points, and occluded points respectively from the state-of-the-art GMA-base method, MATCHFlow(GMA), on Sintel Albedo pass. Furthermore, our model achieves state-of-the-art performance on the Sintel bench-marks, ranking \#1 among all published methods on Sintel clean pass. The code will be open-source.</details>
**Abstract_cn:** <details><summary>译文: </summary>遮挡点的光流预测仍然是一个尚未解决的难题。最近的方法使用自注意力来寻找相关的非遮挡点，作为基于自相似性假设估计遮挡点的光流的参考。然而，它们依赖于单个图像的视觉特征和弱约束，这不足以约束训练后的网络关注错误和弱相关的参考点。我们充分利用在线遮挡识别信息来构造遮挡扩展视觉特征和两个强约束，使网络能够学习仅关注最相关的参考，而不需要遮挡地面实况参与网络的训练。我们的方法在原始框架上添加了很少的网络参数，使其非常轻量级。大量的实验表明，我们的模型具有最大的跨数据集泛化能力。与最先进的基于 GMA 的方法 MATCHFlow(GMA) 相比，我们的方法在所有点、非遮挡点和遮挡点上实现了更大的误差减少，分别为 18.6%、16.2% 和 20.1%。辛特尔反照率山口。此外，我们的模型在 Sintel 基准上实现了最先进的性能，在 Sintel clean pass 上所有已发布的方法中排名第一。该代码将是开源的。</details>
**PDF:** <http://arxiv.org/pdf/2403.00211v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local Reference Frames for Rotation-invariant 3D Point Set Analysis**<br />
**Title_cn:** MaskLRF：通过局部参考系的屏蔽自动编码进行自监督预训练，用于旋转不变的 3D 点集分析<br />
**Authors:** Takahiko Furuya<br />
**Abstract:** <details><summary>原文: </summary>Following the successes in the fields of vision and language, self-supervised pretraining via masked autoencoding of 3D point set data, or Masked Point Modeling (MPM), has achieved state-of-the-art accuracy in various downstream tasks. However, current MPM methods lack a property essential for 3D point set analysis, namely, invariance against rotation of 3D objects/scenes. Existing MPM methods are thus not necessarily suitable for real-world applications where 3D point sets may have inconsistent orientations. This paper develops, for the first time, a rotation-invariant self-supervised pretraining framework for practical 3D point set analysis. The proposed algorithm, called MaskLRF, learns rotation-invariant and highly generalizable latent features via masked autoencoding of 3D points within Local Reference Frames (LRFs), which are not affected by rotation of 3D point sets. MaskLRF enhances the quality of latent features by integrating feature refinement using relative pose encoding and feature reconstruction using low-level but rich 3D geometry. The efficacy of MaskLRF is validated via extensive experiments on diverse downstream tasks including classification, segmentation, registration, and domain adaptation. I confirm that MaskLRF achieves new state-of-the-art accuracies in analyzing 3D point sets having inconsistent orientations. Code will be available at: https://github.com/takahikof/MaskLRF</details>
**Abstract_cn:** <details><summary>译文: </summary>继视觉和语言领域取得成功之后，通过 3D 点集数据的屏蔽自动编码或屏蔽点建模 (MPM) 进行的自我监督预训练，已在各种下游任务中实现了最先进的准确性。然而，当前的 MPM 方法缺乏 3D 点集分析所必需的属性，即 3D 对象/场景旋转的不变性。因此，现有的 MPM 方法不一定适合 3D 点集可能具有不一致方向的现实应用。本文首次开发了一种用于实际 3D 点集分析的旋转不变自监督预训练框架。所提出的算法称为 MaskLRF，通过局部参考框架 (LRF) 内的 3D 点的掩码自动编码来学习旋转不变和高度通用的潜在特征，这些特征不受 3D 点集旋转的影响。 MaskLRF 通过集成使用相对姿态编码的特征细化和使用低级但丰富的 3D 几何的特征重建来增强潜在特征的质量。 MaskLRF 的功效通过对各种下游任务（包括分类、分割、注册和域适应）的广泛实验得到验证。我确认 MaskLRF 在分析方向不一致的 3D 点集方面达到了最先进的新精度。代码可在以下位置获取：https://github.com/takahikof/MaskLRF</details>
**PDF:** <http://arxiv.org/pdf/2403.00206v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Rethinking Inductive Biases for Surface Normal Estimation**<br />
**Title_cn:** 重新思考表面法线估计的归纳偏差<br />
**Authors:** Gwangbin Bae, Andrew J. Davison<br />
**Abstract:** <details><summary>原文: </summary>Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) encode the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp - yet, piecewise smooth - predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state-of-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset. The code is available at https://github.com/baegwangbin/DSINE.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管对准确的表面法线估计模型的需求不断增长，但现有方法使用通用密集预测模型，采用与其他任务相同的归纳偏差。在本文中，我们讨论了表面法线估计所需的归纳偏差，并提出（1）利用每像素光线方向和（2）通过学习相邻表面法线的相对旋转来编码它们之间的关系。所提出的方法可以针对任意分辨率和纵横比的具有挑战性的野外图像生成清晰且分段平滑的预测。与最近基于 ViT 的最先进模型相比，我们的方法显示出更强的泛化能力，尽管是在小几个数量级的数据集上进行训练。代码可在 https://github.com/baegwangbin/DSINE 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00712v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Rethinking The Uniformity Metric in Self-Supervised Learning**<br />
**Title_cn:** 重新思考自我监督学习中的均匀性指标<br />
**Authors:** Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang<br />
**Abstract:** <details><summary>原文: </summary>Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various established self-supervised methods, our proposed uniformity metric consistently enhances their performance in downstream tasks.Our code was released at https://github.com/sunset-clouds/WassersteinUniformityMetric.</details>
**Abstract_cn:** <details><summary>译文: </summary>一致性在学习表征的评估中起着至关重要的作用，有助于更深入地理解自我监督学习。 \citet{Wang2020UnderstandingCR} 的开创性工作引入了一种均匀性度量，可以定量测量学习表示的崩溃程度。事实证明，直接优化该指标并结合对齐可以有效防止持续崩溃。然而，我们提出的理论和经验证据表明，该指标对维度崩溃缺乏敏感性，凸显了其局限性。为了解决这一限制并设计更有效的均匀性度量，本文确定了五个基本属性，其中一些属性是现有均匀性度量无法满足的。随后，我们引入了一种新颖的均匀性度量，它满足所有这些需求并表现出对维度崩溃的敏感性。当在各种已建立的自监督方法中用作辅助损失时，我们提出的均匀性度量持续增强了它们在下游任务中的性能。我们的代码发布于https://github.com/sunset-clouds/WassersteinUniformityMetric。</details>
**PDF:** <http://arxiv.org/pdf/2403.00642v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Improving Acne Image Grading with Label Distribution Smoothing**<br />
**Title_cn:** 通过标签分布平滑改进痤疮图像分级<br />
**Authors:** Kirill Prokhorov, Alexandr A. Kalinin<br />
**Abstract:** <details><summary>原文: </summary>Acne, a prevalent skin condition, necessitates precise severity assessment for effective treatment. Acne severity grading typically involves lesion counting and global assessment. However, manual grading suffers from variability and inefficiency, highlighting the need for automated tools. Recently, label distribution learning (LDL) was proposed as an effective framework for acne image grading, but its effectiveness is hindered by severity scales that assign varying numbers of lesions to different severity grades. Addressing these limitations, we proposed to incorporate severity scale information into lesion counting by combining LDL with label smoothing, and to decouple if from global assessment. A novel weighting scheme in our approach adjusts the degree of label smoothing based on the severity grading scale. This method helped to effectively manage label uncertainty without compromising class distinctiveness. Applied to the benchmark ACNE04 dataset, our model demonstrated improved performance in automated acne grading, showcasing its potential in enhancing acne diagnostics. The source code is publicly available at http://github.com/openface-io/acne-lds.</details>
**Abstract_cn:** <details><summary>译文: </summary>痤疮是一种常见的皮肤病，需要精确的严重程度评估才能有效治疗。痤疮严重程度分级通常涉及病变计数和整体评估。然而，手动评分存在可变性和低效率，凸显了对自动化工具的需求。最近，标签分布学习（LDL）被提出作为痤疮图像分级的有效框架，但其有效性受到严重程度的阻碍，该严重程度将不同数量的病变分配给不同的严重程度等级。为了解决这些限制，我们建议通过将 LDL 与标签平滑相结合，将严重程度信息纳入病变计数，并将 if 与全局评估分离。我们的方法中的一种新颖的加权方案根据严重程度分级量表调整标签平滑程度。这种方法有助于有效管理标签的不确定性，同时又不影响类别的独特性。应用于基准 ACNE04 数据集时，我们的模型在自动痤疮分级方面表现出了改进的性能，展示了其在增强痤疮诊断方面的潜力。源代码可在 http://github.com/openface-io/acne-lds 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00268v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **TempCompass: Do Video LLMs Really Understand Videos?**<br />
**Title_cn:** TempCompass：视频法学硕士真的了解视频吗？<br />
**Authors:** Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, Lu Hou<br />
**Abstract:** <details><summary>原文: </summary>Recently, there is a surge in interest surrounding video large language models (Video LLMs). However, existing benchmarks fail to provide a comprehensive feedback on the temporal perception ability of Video LLMs. On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice QA), which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the \textbf{TempCompass} benchmark, which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video LLMs from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an LLM generates the instruction. We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs. Based on TempCompass, we comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, and reveal the discerning fact that these models exhibit notably poor temporal perception ability. Our data will be available at \url{https://github.com/llyx97/TempCompass}.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，人们对视频大语言模型（视频 LLM）的兴趣激增。然而，现有的基准测试无法提供关于视频法学硕士时间感知能力的全面反馈。一方面，它们中的大多数无法区分不同的时间方面（例如速度、方向），因此无法反映这些特定方面的细微差别表现。另一方面，它们受到任务格式多样性的限制（例如，只有多选 QA），这阻碍了对不同类型任务的时间感知性能如何变化的理解。受这两个问题的推动，我们提出了 \textbf{TempCompass} 基准，它引入了时间方面和任务格式的多样性。为了收集高质量的测试数据，我们设计了两种新颖的策略：（1）在视频收集中，我们构建共享相同静态内容但在特定时间方面不同的冲突视频，这防止视频法学硕士利用单帧偏差或语言先验。 (2) 为了收集任务指令，我们提出了一种范例，其中人类首先注释视频的元信息，然后法学硕士生成指令。我们还设计了一种基于法学硕士的方法来自动准确地评估视频法学硕士的回答。基于 TempCompass，我们综合评估了 8 个最先进的（SOTA）视频 LLM 和 3 个图像 LLM，并揭示了这些模型表现出明显较差的时间感知能力的明显事实。我们的数据可在 \url{https://github.com/llyx97/TempCompass} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00476v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Tri-Modal Motion Retrieval by Learning a Joint Embedding Space**<br />
**Title_cn:** 通过学习联合嵌入空间进行三模态运动检索<br />
**Authors:** Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian<br />
**Abstract:** <details><summary>原文: </summary>Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.</details>
**Abstract_cn:** <details><summary>译文: </summary>信息检索是一个不断发展且至关重要的研究领域。对高质量人体运动数据尤其是在线获取的巨大需求导致了人体运动研究工作的激增。先前的工作主要集中在双模态学习，例如文本和动作任务，但很少探索三模态学习。直观上，引入额外的模态可以丰富模型的应用场景，更重要的是，适当选择额外的模态还可以充当中介并增强其他两种不同模态之间的一致性。在这项工作中，我们引入了 LAVIMO（语言-视频-运动对齐），这是一种新的三模态学习框架，它将以人为中心的视频作为附加模态，从而有效地弥合了文本和运动之间的差距。此外，我们的方法利用专门设计的注意力机制来促进文本、视频和运动模式之间增强的对齐和协同效应。根据经验，我们在 HumanML3D 和 KIT-ML 数据集上的结果表明，LAVIMO 在各种与运动相关的跨模态检索任务中实现了最先进的性能，包括文本到运动、运动到文本、视频-到运动和运动到视频。</details>
**PDF:** <http://arxiv.org/pdf/2403.00691v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks**<br />
**Title_cn:** VisionLLaMA：用于视觉任务的统一 LLaMA 接口<br />
**Authors:** Xiangxiang Chu, Jianlin Su, Bo Zhang, Chunhua Shen<br />
**Abstract:** <details><summary>原文: </summary>Large language models are built on top of a transformer-based architecture to process textual inputs. For example, the LLaMA stands out among many open-source implementations. Can the same transformer be used to process 2D images? In this paper, we answer this question by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA, which is tailored for this purpose. VisionLLaMA is a unified and generic modelling framework for solving most vision tasks. We extensively evaluate its effectiveness using typical pre-training paradigms in a good portion of downstream tasks of image perception and especially image generation. In many cases, VisionLLaMA have exhibited substantial gains over the previous state-of-the-art vision transformers. We believe that VisionLLaMA can serve as a strong new baseline model for vision generation and understanding. Our code will be released at https://github.com/Meituan-AutoML/VisionLLaMA.</details>
**Abstract_cn:** <details><summary>译文: </summary>大型语言模型构建在基于变压器的架构之上来处理文本输入。例如，LLaMA 在众多开源实现中脱颖而出。同一个变压器可以用来处理 2D 图像吗？在本文中，我们通过推出一种类似 LLaMA 的平面和金字塔形式的视觉转换器来回答这个问题，称为 VisionLLaMA，它是为此目的量身定制的。 VisionLLaMA 是一个统一的通用建模框架，用于解决大多数视觉任务。我们在图像感知（尤其是图像生成）的大部分下游任务中使用典型的预训练范例来广泛评估其有效性。在许多情况下，VisionLLaMA 比以前最先进的视觉转换器表现出了巨大的进步。我们相信 VisionLLaMA 可以作为视觉生成和理解的强大新基线模型。我们的代码将在 https://github.com/Meituan-AutoML/VisionLLaMA 发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.00522v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching**<br />
**Title_cn:** 选择性立体声：用于立体声匹配的自适应频率信息选择<br />
**Authors:** Xianqi Wang, Gangwei Xu, Hao Jia, Xin Yang<br />
**Abstract:** <details><summary>原文: </summary>Stereo matching methods based on iterative optimization, like RAFT-Stereo and IGEV-Stereo, have evolved into a cornerstone in the field of stereo matching. However, these methods struggle to simultaneously capture high-frequency information in edges and low-frequency information in smooth regions due to the fixed receptive field. As a result, they tend to lose details, blur edges, and produce false matches in textureless areas. In this paper, we propose Selective Recurrent Unit (SRU), a novel iterative update operator for stereo matching. The SRU module can adaptively fuse hidden disparity information at multiple frequencies for edge and smooth regions. To perform adaptive fusion, we introduce a new Contextual Spatial Attention (CSA) module to generate attention maps as fusion weights. The SRU empowers the network to aggregate hidden disparity information across multiple frequencies, mitigating the risk of vital hidden disparity information loss during iterative processes. To verify SRU's universality, we apply it to representative iterative stereo matching methods, collectively referred to as Selective-Stereo. Our Selective-Stereo ranks $1^{st}$ on KITTI 2012, KITTI 2015, ETH3D, and Middlebury leaderboards among all published methods. Code is available at https://github.com/Windsrain/Selective-Stereo.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于迭代优化的立体匹配方法，如RAFT-Stereo和IGEV-Stereo，已经发展成为立体匹配领域的基石。然而，由于感受野固定，这些方法很难同时捕获边缘的高频信息和平滑区域的低频信息。因此，它们往往会丢失细节、模糊边缘并在无纹理区域产生错误匹配。在本文中，我们提出了选择性循环单元（SRU），一种用于立体匹配的新型迭代更新算子。 SRU模块可以自适应地融合边缘和平滑区域的多个频率下的隐藏视差信息。为了执行自适应融合，我们引入了一个新的上下文空间注意力（CSA）模块来生成注意力图作为融合权重。 SRU 使网络能够跨多个频率聚合隐藏视差信息，从而降低迭代过程中重要的隐藏视差信息丢失的风险。为了验证SRU的普适性，我们将其应用于代表性的迭代立体匹配方法，统称为选择性立体。在所有已发布的方法中，我们的 Selective-Stereo 在 KITTI 2012、KITTI 2015、ETH3D 和 Middlebury 排行榜上排名 $1^{st}$。代码可在 https://github.com/Windsrain/Selective-Stereo 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00486v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization**<br />
**Title_cn:** RealCustom：缩小真实文本字的范围，实现实时开放域文本到图像的定制<br />
**Authors:** Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image customization, which aims to synthesize text-driven images for the given subjects, has recently revolutionized content creation. Existing works follow the pseudo-word paradigm, i.e., represent the given subjects as pseudo-words and then compose them with the given text. However, the inherent entangled influence scope of pseudo-words with the given text results in a dual-optimum paradox, i.e., the similarity of the given subjects and the controllability of the given text could not be optimal simultaneously. We present RealCustom that, for the first time, disentangles similarity from controllability by precisely limiting subject influence to relevant parts only, achieved by gradually narrowing real text word from its general connotation to the specific subject and using its cross-attention to distinguish relevance. Specifically, RealCustom introduces a novel "train-inference" decoupled framework: (1) during training, RealCustom learns general alignment between visual conditions to original textual conditions by a novel adaptive scoring module to adaptively modulate influence quantity; (2) during inference, a novel adaptive mask guidance strategy is proposed to iteratively update the influence scope and influence quantity of the given subjects to gradually narrow the generation of the real text word. Comprehensive experiments demonstrate the superior real-time customization ability of RealCustom in the open domain, achieving both unprecedented similarity of the given subjects and controllability of the given text for the first time. The project page is https://corleone-huang.github.io/realcustom/.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像定制旨在为给定主题合成文本驱动的图像，最近彻底改变了内容创建。现有的作品遵循伪词范式，即将给定的主题表示为伪词，然后将它们与给定的文本组合起来。然而，伪词与给定文本固有的影响范围纠缠导致了双优悖论，即给定主体的相似性和给定文本的可控性不可能同时最优。我们提出的 RealCustom 首次通过精确地将主题影响仅限于相关部分来将相似性与可控性分开，这是通过逐渐将真实文本单词从其一般含义缩小到特定主题并利用其交叉注意力来区分相关性来实现的。具体来说，RealCustom引入了一种新颖的“训练-推理”解耦框架：（1）在训练过程中，RealCustom通过新颖的自适应评分模块学习视觉条件与原始文本条件之间的一般对齐，以自适应地调节影响量； （2）在推理过程中，提出了一种新颖的自适应掩模引导策略，迭代更新给定主体的影响范围和影响量，以逐渐缩小真实文本词的生成范围。综合实验证明了RealCustom在开放域中优越的实时定制能力，首次实现了给定主题前所未有的相似性和给定文本的可控性。项目页面为https://corleone-huang.github.io/realcustom/。</details>
**PDF:** <http://arxiv.org/pdf/2403.00483v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Task Indicating Transformer for Task-conditional Dense Predictions**<br />
**Title_cn:** 用于任务条件密集预测的任务指示变压器<br />
**Authors:** Yuxiang Lu, Shalayiding Sirejiding, Bayram Bayramli, Suizhi Huang, Yue Ding, Hongtao Lu<br />
**Abstract:** <details><summary>原文: </summary>The task-conditional model is a distinctive stream for efficient multi-task learning. Existing works encounter a critical limitation in learning task-agnostic and task-specific representations, primarily due to shortcomings in global context modeling arising from CNN-based architectures, as well as a deficiency in multi-scale feature interaction within the decoder. In this paper, we introduce a novel task-conditional framework called Task Indicating Transformer (TIT) to tackle this challenge. Our approach designs a Mix Task Adapter module within the transformer block, which incorporates a Task Indicating Matrix through matrix decomposition, thereby enhancing long-range dependency modeling and parameter-efficient feature adaptation by capturing intra- and inter-task features. Moreover, we propose a Task Gate Decoder module that harnesses a Task Indicating Vector and gating mechanism to facilitate adaptive multi-scale feature refinement guided by task embeddings. Experiments on two public multi-task dense prediction benchmarks, NYUD-v2 and PASCAL-Context, demonstrate that our approach surpasses state-of-the-art task-conditional methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>任务条件模型是高效多任务学习的独特流。现有的工作在学习与任务无关和特定于任务的表示方面遇到了严重的限制，这主要是由于基于 CNN 的架构产生的全局上下文建模的缺陷，以及解码器内多尺度特征交互的缺陷。在本文中，我们引入了一种称为任务指示变压器（TIT）的新型任务条件框架来应对这一挑战。我们的方法在变压器块内设计了一个混合任务适配器模块，它通过矩阵分解合并了一个任务指示矩阵，从而通过捕获任务内和任务间特征来增强远程依赖建模和参数高效的特征适应。此外，我们提出了一个任务门解码器模块，它利用任务指示向量和门控机制来促进由任务嵌入引导的自适应多尺度特征细化。对两个公共多任务密集预测基准 NYUD-v2 和 PASCAL-Context 的实验表明，我们的方法超越了最先进的任务条件方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.00327v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **G3DR: Generative 3D Reconstruction in ImageNet**<br />
**Title_cn:** G3DR：ImageNet 中的生成 3D 重建<br />
**Authors:** Pradyumna Reddy, Ismail Elezi, Jiankang Deng<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel 3D generative method, Generative 3D Reconstruction (G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects from single images, addressing the limitations of existing methods. At the heart of our framework is a novel depth regularization technique that enables the generation of scenes with high-geometric fidelity. G3DR also leverages a pretrained language-vision model, such as CLIP, to enable reconstruction in novel views and improve the visual realism of generations. Additionally, G3DR designs a simple but effective sampling procedure to further improve the quality of generations. G3DR offers diverse and efficient 3D asset generation based on class or text conditioning. Despite its simplicity, G3DR is able to beat state-of-theart methods, improving over them by up to 22% in perceptual metrics and 90% in geometry scores, while needing only half of the training time. Code is available at https://github.com/preddy5/G3DR</details>
**Abstract_cn:** <details><summary>译文: </summary>我们在 ImageNet 中引入了一种新颖的 3D 生成方法，即生成 3D 重建（G3DR），能够从单个图像生成多样化且高质量的 3D 对象，解决了现有方法的局限性。我们框架的核心是一种新颖的深度正则化技术，可以生成具有高几何保真度的场景。 G3DR 还利用预训练的语言视觉模型（例如 CLIP）来实现新视图中的重建并提高几代人的视觉真实感。此外，G3DR 设计了一个简单但有效的采样程序，以进一步提高世代质量。 G3DR 基于类或文本条件提供多样化且高效的 3D 资产生成。尽管很简单，但 G3DR 能够击败最先进的方法，在感知指标上比它们提高了 22%，在几何分数上比它们提高了 90%，而只需要一半的训练时间。代码可在 https://github.com/preddy5/G3DR 获取</details>
**PDF:** <http://arxiv.org/pdf/2403.00939v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Point Could Mamba: Point Cloud Learning via State Space Model**<br />
**Title_cn:** Point Could Mamba：通过状态空间模型进行点云学习<br />
**Authors:** Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, Shuicheng Yan<br />
**Abstract:** <details><summary>原文: </summary>In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>在这项工作中，我们首次证明基于 Mamba 的点云方法可以优于基于点的方法。 Mamba 展现出强大的全局建模能力和线性计算复杂性，使其对于点云分析极具吸引力。为了使 Mamba 能够更有效地处理 3D 点云数据，我们提出了一种新颖的一致遍历序列化，将点云转换为 1D 点序列，同时确保序列中的相邻点在空间上相邻。一致遍历序列化通过排列 x、y、z 坐标的顺序产生六种变体，这些变体的协同使用有助于 Mamba 全面观察点云数据。此外，为了帮助 Mamba 更有效地处理不同顺序的点序列，我们引入了点提示来告知 Mamba 序列的排列规则。最后，我们提出基于空间坐标映射的位置编码，以更好地将位置信息注入点云序列。基于这些改进，我们构建了一个名为 Point Cloud Mamba 的点云网络，它结合了局部和全局建模。点云 Mamba 超越了基于 SOTA 点的方法 PointNeXt，并在 ScanObjectNN、ModelNet40 和 ShapeNetPart 数据集上实现了新的 SOTA 性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.00762v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Learning and Leveraging World Models in Visual Representation Learning**<br />
**Title_cn:** 在视觉表示学习中学习和利用世界模型<br />
**Authors:** Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, Yann LeCun<br />
**Abstract:** <details><summary>原文: </summary>Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.</details>
**Abstract_cn:** <details><summary>译文: </summary>联合嵌入预测架构 (JEPA) 已成为一种有前途的自我监督方法，它通过利用世界模型进行学习。虽然之前仅限于预测输入的缺失部分，但我们探索如何将 JEPA 预测任务推广到更广泛的损坏集。我们引入图像世界模型，这种方法超越了蒙版图像建模，并学习预测潜在空间中全局光度变换的效果。我们研究了学习绩效 IWM 的秘诀，并表明它依赖于三个关键方面：调节、预测难度和能力。此外，我们还表明，通过 IWM 学习的预测世界模型可以通过微调来适应不同的任务；经过微调的 IWM 世界模型可以匹配或超越以前的自监督方法的性能。最后，我们表明，使用 IWM 进行学习可以控制学习表示的抽象级别，学习不变表示（例如对比方法）或等变表示（例如掩模图像建模）。</details>
**PDF:** <http://arxiv.org/pdf/2403.00504v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training**<br />
**Title_cn:** 用于视觉语言预训练的语义增强跨模态掩模图像建模<br />
**Authors:** Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In vision-language pre-training (VLP), masked image modeling (MIM) has recently been introduced for fine-grained cross-modal alignment. However, in most existing methods, the reconstruction targets for MIM lack high-level semantics, and text is not sufficiently involved in masked modeling. These two drawbacks limit the effect of MIM in facilitating cross-modal semantic alignment. In this work, we propose a semantics-enhanced cross-modal MIM framework (SemMIM) for vision-language representation learning. Specifically, to provide more semantically meaningful supervision for MIM, we propose a local semantics enhancing approach, which harvest high-level semantics from global image features via self-supervised agreement learning and transfer them to local patch encodings by sharing the encoding space. Moreover, to achieve deep involvement of text during the entire MIM process, we propose a text-guided masking strategy and devise an efficient way of injecting textual information in both masked modeling and reconstruction target acquisition. Experimental results validate that our method improves the effectiveness of the MIM task in facilitating cross-modal semantic alignment. Compared to previous VLP models with similar model size and data scale, our SemMIM model achieves state-of-the-art or competitive performance on multiple downstream vision-language tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>在视觉语言预训练（VLP）中，最近引入了掩模图像建模（MIM）来进行细粒度的跨模态对齐。然而，在大多数现有方法中，MIM 的重建目标缺乏高级语义，并且文本没有充分参与掩模建模。这两个缺点限制了 MIM 在促进跨模态语义对齐方面的效果。在这项工作中，我们提出了一种用于视觉语言表示学习的语义增强的跨模态 MIM 框架（SemMIM）。具体来说，为了为 MIM 提供更多语义上有意义的监督，我们提出了一种局部语义增强方法，该方法通过自监督协议学习从全局图像特征中获取高级语义，并通过共享编码空间将它们转移到局部补丁编码。此外，为了在整个 MIM 过程中实现文本的深度参与，我们提出了一种文本引导的掩蔽策略，并设计了一种在掩蔽建模和重建目标获取中注入文本信息的有效方法。实验结果验证了我们的方法提高了 MIM 任务在促进跨模态语义对齐方面的有效性。与之前具有相似模型大小和数据规模的 VLP 模型相比，我们的 SemMIM 模型在多个下游视觉语言任务上实现了最先进的或有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.00249v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction**<br />
**Title_cn:** VisRec：无线电干涉数据重建的半监督方法<br />
**Authors:** Ruoqi Wang, Haitao Wang, Qiong Luo, Feng Wang, Hejun Wu<br />
**Abstract:** <details><summary>原文: </summary>Radio telescopes produce visibility data about celestial objects, but these data are sparse and noisy. As a result, images created on raw visibility data are of low quality. Recent studies have used deep learning models to reconstruct visibility data to get cleaner images. However, these methods rely on a substantial amount of labeled training data, which requires significant labeling effort from radio astronomers. Addressing this challenge, we propose VisRec, a model-agnostic semi-supervised learning approach to the reconstruction of visibility data. Specifically, VisRec consists of both a supervised learning module and an unsupervised learning module. In the supervised learning module, we introduce a set of data augmentation functions to produce diverse training examples. In comparison, the unsupervised learning module in VisRec augments unlabeled data and uses reconstructions from non-augmented visibility data as pseudo-labels for training. This hybrid approach allows VisRec to effectively leverage both labeled and unlabeled data. This way, VisRec performs well even when labeled data is scarce. Our evaluation results show that VisRec outperforms all baseline methods in reconstruction quality, robustness against common observation perturbation, and generalizability to different telescope configurations.</details>
**Abstract_cn:** <details><summary>译文: </summary>射电望远镜产生有关天体的可见度数据，但这些数据稀疏且嘈杂。因此，根据原始可见性数据创建的图像质量较低。最近的研究使用深度学习模型来重建可见性数据以获得更清晰的图像。然而，这些方法依赖于大量标记的训练数据，这需要射电天文学家进行大量的标记工作。为了应对这一挑战，我们提出了 VisRec，一种与模型无关的半监督学习方法，用于重建可见性数据。具体来说，VisRec由监督学习模块和无监督学习模块组成。在监督学习模块中，我们引入了一组数据增强函数来生成不同的训练示例。相比之下，VisRec 中的无监督学习模块增强了未标记数据，并使用非增强可见性数据的重建作为训练的伪标签。这种混合方法使 VisRec 能够有效地利用标记和未标记数据。这样，即使标记数据稀缺，VisRec 也能表现良好。我们的评估结果表明，VisRec 在重建质量、针对常见观测扰动的鲁棒性以及对不同望远镜配置的通用性方面优于所有基线方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.00897v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning**<br />
**Title_cn:** 展平远程损失景观以实现跨域小样本学习<br />
**Authors:** Yixiong Zou, Yicong Liu, Yiman Hu, Yuhua Li, Ruixuan Li<br />
**Abstract:** <details><summary>原文: </summary>Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve long-range flattening of the minima in the loss landscape. This approach considers representations that are differently normalized as minima in the loss landscape and flattens the high-loss region in the middle by randomly sampling interpolated representations. We implement this method as a new normalization layer that replaces the original one in both CNNs and ViTs. This layer is simple and lightweight, introducing only a minimal number of additional parameters. Experimental results on 8 datasets demonstrate that our approach outperforms state-of-the-art methods in terms of average accuracy. Moreover, our method achieves performance improvements of up to 9\% compared to the current best approaches on individual datasets. Our code will be released.</details>
**Abstract_cn:** <details><summary>译文: </summary>跨域少样本学习（CDFSL）旨在通过利用从具有丰富训练样本的源域转移的先验知识，从目标域的有限训练数据中获取知识。 CDFSL 在跨不同领域转移知识以及使用有限的训练数据微调模型方面面临着挑战。为了应对这些挑战，我们首先将损失景观的分析从参数空间扩展到表示空间，这使我们能够同时解释 CDFSL 模型的转移和微调困难。我们观察到表示空间的损失景观中的尖锐极小值导致难以转移和微调的表示。此外，现有的基于平坦度的方法由于其短程平坦度而导致泛化能力有限。为了增强可转移性并促进微调，我们引入了一种简单而有效的方法来实现损失景观中最小值的远程平坦化。这种方法将不同归一化的表示视为损失景观中的最小值，并通过随机采样插值表示来展平中间的高损失区域。我们将此方法实现为一个新的归一化层，取代了 CNN 和 ViT 中的原始归一化层。该层简单且轻量级，仅引入最少数量的附加参数。 8 个数据集的实验结果表明，我们的方法在平均准确度方面优于最先进的方法。此外，与当前单个数据集上的最佳方法相比，我们的方法实现了高达 9% 的性能提升。我们的代码将被发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.00567v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Spatial Cascaded Clustering and Weighted Memory for Unsupervised Person Re-identification**<br />
**Title_cn:** 用于无监督人员重新识别的空间级联聚类和加权记忆<br />
**Authors:** Jiahao Hong, Jialong Zuo, Chuchu Han, Ruochen Zheng, Ming Tian, Changxin Gao, Nong Sang<br />
**Abstract:** <details><summary>原文: </summary>Recent unsupervised person re-identification (re-ID) methods achieve high performance by leveraging fine-grained local context. These methods are referred to as part-based methods. However, most part-based methods obtain local contexts through horizontal division, which suffer from misalignment due to various human poses. Additionally, the misalignment of semantic information in part features restricts the use of metric learning, thus affecting the effectiveness of part-based methods. The two issues mentioned above result in the under-utilization of part features in part-based methods. We introduce the Spatial Cascaded Clustering and Weighted Memory (SCWM) method to address these challenges. SCWM aims to parse and align more accurate local contexts for different human body parts while allowing the memory module to balance hard example mining and noise suppression. Specifically, we first analyze the foreground omissions and spatial confusions issues in the previous method. Then, we propose foreground and space corrections to enhance the completeness and reasonableness of the human parsing results. Next, we introduce a weighted memory and utilize two weighting strategies. These strategies address hard sample mining for global features and enhance noise resistance for part features, which enables better utilization of both global and part features. Extensive experiments on Market-1501 and MSMT17 validate the proposed method's effectiveness over many state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的无监督人员重新识别（re-ID）方法通过利用细粒度的本地上下文实现了高性能。这些方法称为基于部分的方法。然而，大多数基于部位的方法通过水平划分来获取局部上下文，这会由于各种人体姿势而出现错位。此外，零件特征中语义信息的错位限制了度量学习的使用，从而影响了基于零件的方法的有效性。上述两个问题导致基于零件的方法中零件特征的利用不足。我们引入空间级联聚类和加权内存（SCWM）方法来应对这些挑战。 SCWM 旨在解析和对齐不同人体部位的更准确的局部上下文，同时允许内存模块平衡困难示例挖掘和噪声抑制。具体来说，我们首先分析之前方法中的前景遗漏和空间混乱问题。然后，我们提出前景和空间校正，以增强人体解析结果的完整性和合理性。接下来，我们引入加权内存并利用两种加权策略。这些策略解决了全局特征的硬样本挖掘问题，并增强了部分特征的抗噪性，从而可以更好地利用全局特征和部分特征。 Market-1501 和 MSMT17 上的大量实验验证了所提出的方法相对于许多最先进方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00261v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation**<br />
**Title_cn:** SELFI：通过强化学习进行社交导航的自主自我提升<br />
**Authors:** Noriaki Hirose, Dhruv Shah, Kyle Stachowicz, Ajay Sridhar, Sergey Levine<br />
**Abstract:** <details><summary>原文: </summary>Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-emptive behavior for the pedestrians, collision avoidance for small and transparent objects, and avoiding travel on uneven floor surfaces. We provide supplementary videos to demonstrate the performance of our fine-tuned policy on our project page.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过经验进行交互和改进的自主自我改进机器人是机器人系统在现实世界中部署的关键。在本文中，我们提出了一种在线学习方法 SELFI，它利用在线机器人经验来快速有效地调整预训练的控制策略。 SELFI 在基于离线模型的学习之上应用在线无模型强化学习，以发挥两种学习范式的最佳部分。具体来说，SELFI 通过将离线预训练中相同的基于模型的学习目标合并到通过在线无模型强化学习学习到的 Q 值中来稳定在线学习过程。我们在多个现实环境中评估 SELFI，并报告在避免碰撞方面的改进，以及通过人类用户研究衡量的更符合社会规范的行为。 SELFI 使我们能够以较少的人为干预快速学习有用的机器人行为，例如行人的先发制人行为、避免小型和透明物体的碰撞以及避免在不平坦的地板表面上行驶。我们在项目页面上提供补充视频来展示我们微调政策的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.00991v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Fine-tuning with Very Large Dropout**<br />
**Title_cn:** 使用非常大的压差进行微调<br />
**Authors:** Jianyu Zhang, Léon Bottou<br />
**Abstract:** <details><summary>原文: </summary>It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods such as model soups. This result has practical significance because the importance of the fine-tuning scenario has considerably grown in recent years. This result also provides interesting insights on the nature of rich representations and on the intrinsically linear nature of fine-tuning a large network using a comparatively small dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>今天不可能假装机器学习的实践与训练和测试数据遵循相同分布的想法兼容。几位作者最近使用集成技术来展示涉及多个数据分布的场景如何最好地通过表示来提供服务，这些表示既比通过正则化获得最佳分布内性能所获得的表示更丰富，又比在隐式稀疏偏差影响下获得的表示更丰富常见的随机梯度程序。该贡献研究了使用非常高的辍学率而不是集成来获得如此丰富的表示。尽管使用这样的丢失率从头开始训练深度网络实际上是不可能的，但在这种条件下对大型预训练模型进行微调不仅是可能的，而且还可以实现超过集成和权重平均方法的分布外性能比如模型汤。这一结果具有实际意义，因为近年来微调场景的重要性显着增长。这一结果还提供了关于丰富表示的性质以及使用相对较小的数据集微调大型网络的本质线性性质的有趣见解。</details>
**PDF:** <http://arxiv.org/pdf/2403.00946v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning**<br />
**Title_cn:** 基于遗传编程的损失函数学习的快速高效的局部搜索<br />
**Authors:** Christian Raymond, Qi Chen, Bing Xue, Mengjie Zhang<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we develop upon the topic of loss function learning, an emergent meta-learning paradigm that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of supervised learning tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们以损失函数学习为主题，这是一种新兴的元学习范式，旨在学习损失函数，从而显着提高在其下训练的模型的性能。具体来说，我们提出了一种新的元学习框架，用于通过混合搜索方法进行任务和模型无关的损失函数学习。该框架首先使用遗传编程来查找一组符号损失函数。其次，学习到的损失函数集随后通过展开微分进行参数化和优化。所提出框架的多功能性和性能在一组不同的监督学习任务上得到了实证验证。结果表明，使用各种特定于任务的神经网络架构，学习的损失函数可以提高表格、计算机视觉和自然语言处理问题的收敛性、样本效率和推理性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.00865v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Hydra: Computer Vision for Data Quality Monitoring**<br />
**Title_cn:** Hydra：用于数据质量监控的计算机视觉<br />
**Authors:** Thomas Britton, Torri Jeske, David Lawrence, Kishansingh Rajput<br />
**Abstract:** <details><summary>原文: </summary>Hydra is a system which utilizes computer vision to perform near real time data quality management, initially developed for Hall-D in 2019. Since then, it has been deployed across all experimental halls at Jefferson Lab, with the CLAS12 collaboration in Hall-B being the first outside of GlueX to fully utilize Hydra. The system comprises back end processes that manage the models, their inferences, and the data flow. The front-end components, accessible via web pages, allow detector experts and shift crews to view and interact with the system. This talk will give an overview of the Hydra system as well as highlight significant developments in Hydra's feature set, acute challenges with operating Hydra in all halls, and lessons learned along the way.</details>
**Abstract_cn:** <details><summary>译文: </summary>Hydra 是一个利用计算机视觉执行近实时数据质量管理的系统，最初于 2019 年为 Hall-D 开发。此后，它已部署在杰斐逊实验室的所有实验大厅，其中 B 厅的 CLAS12 协作正在GlueX 之外第一个充分利用 Hydra 的产品。该系统包括管理模型、模型推理和数据流的后端进程。前端组件可通过网页访问，允许探测器专家和值班人员查看系统并与之交互。本次演讲将概述 Hydra 系统，并重点介绍 Hydra 功能集的重大发展、在所有大厅运行 Hydra 的严峻挑战以及在此过程中吸取的经验教训。</details>
**PDF:** <http://arxiv.org/pdf/2403.00689v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Advancing dermatological diagnosis: Development of a hyperspectral dermatoscope for enhanced skin imaging**<br />
**Title_cn:** 推进皮肤病诊断：开发用于增强皮肤成像的高光谱皮肤镜<br />
**Authors:** Martin J. Hetz, Carina Nogueira Garcia, Sarah Haggenmüller, Titus J. Brinker<br />
**Abstract:** <details><summary>原文: </summary>Clinical dermatology necessitates precision and innovation for efficient diagnosis and treatment of various skin conditions. This paper introduces the development of a cutting-edge hyperspectral dermatoscope (the Hyperscope) tailored for human skin analysis. We detail the requirements to such a device and the design considerations, from optical configurations to sensor selection, necessary to capture a wide spectral range with high fidelity. Preliminary results from 15 individuals and 160 recorded skin images demonstrate the potential of the Hyperscope in identifying and characterizing various skin conditions, offering a promising avenue for non-invasive skin evaluation and a platform for future research in dermatology-related hyperspectral imaging.</details>
**Abstract_cn:** <details><summary>译文: </summary>临床皮肤病学需要精确和创新，以有效诊断和治疗各种皮肤病。本文介绍了专为人体皮肤分析而定制的尖端高光谱皮肤镜（Hyperscope）的开发。我们详细介绍了此类设备的要求以及从光学配置到传感器选择的设计注意事项，这些都是以高保真度捕获宽光谱范围所必需的。来自 15 个人和 160 个记录的皮肤图像的初步结果表明，Hyperscope 在识别和表征各种皮肤状况方面具有潜力，为非侵入性皮肤评估提供了一条有前途的途径，并为皮肤病学相关高光谱成像的未来研究提供了平台。</details>
**PDF:** <http://arxiv.org/pdf/2403.00612v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Flattening Singular Values of Factorized Convolution for Medical Images**<br />
**Title_cn:** 医学图像因式分解卷积的奇异值展平<br />
**Authors:** Zexin Feng, Na Zeng, Jiansheng Fang, Xingyue Wang, Xiaoxi Lu, Heng Meng, Jiang Liu<br />
**Abstract:** <details><summary>原文: </summary>Convolutional neural networks (CNNs) have long been the paradigm of choice for robust medical image processing (MIP). Therefore, it is crucial to effectively and efficiently deploy CNNs on devices with different computing capabilities to support computer-aided diagnosis. Many methods employ factorized convolutional layers to alleviate the burden of limited computational resources at the expense of expressiveness. To this end, given weak medical image-driven CNN model optimization, a Singular value equalization generalizer-induced Factorized Convolution (SFConv) is proposed to improve the expressive power of factorized convolutions in MIP models. We first decompose the weight matrix of convolutional filters into two low-rank matrices to achieve model reduction. Then minimize the KL divergence between the two low-rank weight matrices and the uniform distribution, thereby reducing the number of singular value directions with significant variance. Extensive experiments on fundus and OCTA datasets demonstrate that our SFConv yields competitive expressiveness over vanilla convolutions while reducing complexity.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络 (CNN) 长期以来一直是稳健的医学图像处理 (MIP) 的首选范例。因此，在具有不同计算能力的设备上有效且高效地部署CNN以支持计算机辅助诊断至关重要。许多方法采用分解卷积层来减轻有限计算资源的负担，但以牺牲表达能力为代价。为此，针对医学图像驱动的CNN模型优化较弱的情况，提出了奇异值均衡泛化器诱导的因式分解卷积（SFConv）来提高MIP模型中因式分解卷积的表达能力。我们首先将卷积滤波器的权重矩阵分解为两个低秩矩阵以实现模型缩减。然后最小化两个低秩权重矩阵和均匀分布之间的KL散度，从而减少方差显着的奇异值方向的数量。对眼底和 OCTA 数据集的大量实验表明，我们的 SFConv 比普通卷积具有有竞争力的表达能力，同时降低了复杂性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00606v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation**<br />
**Title_cn:** 多任务学习利用不确定性来权衡异质人脸属性估计的损失<br />
**Authors:** Huaqing Yuan, Yi He, Peng Du, Lu Song<br />
**Abstract:** <details><summary>原文: </summary>Face images contain a wide variety of attribute information. In this paper, we propose a generalized framework for joint estimation of ordinal and nominal attributes based on information sharing. We tackle the correlation problem between heterogeneous attributes using hard parameter sharing of shallow features, and trade-off multiple loss functions by considering homoskedastic uncertainty for each attribute estimation task. This leads to optimal estimation of multiple attributes of the face and reduces the training cost of multitask learning. Experimental results on benchmarks with multiple face attributes show that the proposed approach has superior performance compared to state of the art. Finally, we discuss the bias issues arising from the proposed approach in face attribute estimation and validate its feasibility on edge systems.</details>
**Abstract_cn:** <details><summary>译文: </summary>人脸图像包含多种属性信息。在本文中，我们提出了一种基于信息共享的序数和名义属性联合估计的通用框架。我们使用浅层特征的硬参数共享来解决异构属性之间的相关性问题，并通过考虑每个属性估计任务的同方差不确定性来权衡多个损失函数。这导致了人脸多个属性的最优估计，并降低了多任务学习的训练成本。具有多个面部属性的基准的实验结果表明，与现有技术相比，所提出的方法具有优越的性能。最后，我们讨论了所提出的人脸属性估计方法所产生的偏差问题，并验证了其在边缘系统上的可行性。</details>
**PDF:** <http://arxiv.org/pdf/2403.00561v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction**<br />
**Title_cn:** 松弛测量引导定量心脏磁共振图像重建<br />
**Authors:** Yidong Zhao, Yi Zhang, Qian Tao<br />
**Abstract:** <details><summary>原文: </summary>Deep learning-based methods have achieved prestigious performance for magnetic resonance imaging (MRI) reconstruction, enabling fast imaging for many clinical applications. Previous methods employ convolutional networks to learn the image prior as the regularization term. In quantitative MRI, the physical model of nuclear magnetic resonance relaxometry is known, providing additional prior knowledge for image reconstruction. However, traditional reconstruction networks are limited to learning the spatial domain prior knowledge, ignoring the relaxometry prior. Therefore, we propose a relaxometry-guided quantitative MRI reconstruction framework to learn the spatial prior from data and the relaxometry prior from MRI physics. Additionally, we also evaluated the performance of two popular reconstruction backbones, namely, recurrent variational networks (RVN) and variational networks (VN) with U- Net. Experiments demonstrate that the proposed method achieves highly promising results in quantitative MRI reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习的方法在磁共振成像 (MRI) 重建方面取得了享有盛誉的性能，为许多临床应用提供了快速成像。以前的方法采用卷积网络来学习图像先验作为正则化项。在定量 MRI 中，核磁共振弛豫测量的物理模型是已知的，为图像重建提供了额外的先验知识。然而，传统的重建网络仅限于学习空间域先验知识，忽略了松弛测量先验。因此，我们提出了一种松弛测量引导的定量 MRI 重建框架，以从数据中学习空间先验，并从 MRI 物理学中学习松弛测量先验。此外，我们还评估了两种流行的重建主干网络的性能，即循环变分网络（RVN）和带有 U-Net 的变分网络（VN）。实验表明，该方法在定量 MRI 重建方面取得了非常有前景的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.00549v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability**<br />
**Title_cn:** 当 ControlNet 遇到不明确的掩模时：ControlNet 轮廓跟踪能力案例研究<br />
**Authors:** Wenjie Xuan, Yufei Xu, Shanshan Zhao, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao<br />
**Abstract:** <details><summary>原文: </summary>ControlNet excels at creating content that closely matches precise contours in user-provided masks. However, when these masks contain noise, as a frequent occurrence with non-expert users, the output would include unwanted artifacts. This paper first highlights the crucial role of controlling the impact of these inexplicit masks with diverse deterioration levels through in-depth analysis. Subsequently, to enhance controllability with inexplicit masks, an advanced Shape-aware ControlNet consisting of a deterioration estimator and a shape-prior modulation block is devised. The deterioration estimator assesses the deterioration factor of the provided masks. Then this factor is utilized in the modulation block to adaptively modulate the model's contour-following ability, which helps it dismiss the noise part in the inexplicit masks. Extensive experiments prove its effectiveness in encouraging ControlNet to interpret inaccurate spatial conditions robustly rather than blindly following the given contours. We showcase application scenarios like modifying shape priors and composable shape-controllable generation. Codes are soon available.</details>
**Abstract_cn:** <details><summary>译文: </summary>ControlNet 擅长创建与用户提供的蒙版中的精确轮廓紧密匹配的内容。然而，当这些蒙版包含噪声时（对于非专家用户来说经常发生这种情况），输出将包含不需要的伪影。本文首先通过深入分析强调了控制这些具有不同劣化程度的不明掩模的影响的关键作用。随后，为了增强隐式掩模的可控性，设计了一种由恶化估计器和形状先验调制块组成的先进形状感知控制网络。劣化估计器评估所提供的面罩的劣化因子。然后，在调制模块中利用该因子来自适应调制模型的轮廓跟踪能力，这有助于消除隐式掩模中的噪声部分。大量的实验证明，它可以有效地鼓励 ControlNet 稳健地解释不准确的空间条件，而不是盲目地遵循给定的轮廓。我们展示了修改形状先验和可组合形状可控生成等应用场景。代码很快就会可用。</details>
**PDF:** <http://arxiv.org/pdf/2403.00467v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Deep Learning Computed Tomography based on the Defrise and Clack Algorithm**<br />
**Title_cn:** 基于 Defrise 和 Clack 算法的深度学习计算机断层扫描<br />
**Authors:** Chengze Ye, Linda-Sophie Schneider, Yipeng Sun, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>This study presents a novel approach for reconstructing cone beam computed tomography (CBCT) for specific orbits using known operator learning. Unlike traditional methods, this technique employs a filtered backprojection type (FBP-type) algorithm, which integrates a unique, adaptive filtering process. This process involves a series of operations, including weightings, differentiations, the 2D Radon transform, and backprojection. The filter is designed for a specific orbit geometry and is obtained using a data-driven approach based on deep learning. The approach efficiently learns and optimizes the orbit-related component of the filter. The method has demonstrated its ability through experimentation by successfully learning parameters from circular orbit projection data. Subsequently, the optimized parameters are used to reconstruct images, resulting in outcomes that closely resemble the analytical solution. This demonstrates the potential of the method to learn appropriate parameters from any specific orbit projection data and achieve reconstruction. The algorithm has demonstrated improvement, particularly in enhancing reconstruction speed and reducing memory usage for handling specific orbit reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究提出了一种使用已知算子学习来重建特定轨道的锥形束计算机断层扫描 (CBCT) 的新方法。与传统方法不同，该技术采用滤波反投影型（FBP 型）算法，该算法集成了独特的自适应滤波过程。这个过程涉及一系列操作，包括加权、微分、2D Radon 变换和反投影。该滤波器是针对特定轨道几何形状而设计的，并使用基于深度学习的数据驱动方法获得。该方法有效地学习和优化滤波器的轨道相关组件。该方法通过实验成功地从圆形轨道投影数据中学习参数，证明了其能力。随后，优化的参数用于重建图像，产生与解析解非常相似的结果。这证明了该方法从任何特定轨道投影数据中学习适当参数并实现重建的潜力。该算法已显示出改进，特别是在提高重建速度和减少处理特定轨道重建的内存使用量方面。</details>
**PDF:** <http://arxiv.org/pdf/2403.00426v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Spatio-temporal reconstruction of substance dynamics using compressed sensing in multi-spectral magnetic resonance spectroscopic imaging**<br />
**Title_cn:** 多光谱磁共振波谱成像中利用压缩感知对物质动力学进行时空重建<br />
**Authors:** Utako Yamamoto, Hirohiko Imai, Kei Sano, Masayuki Ohzeki, Tetsuya Matsuda, Toshiyuki Tanaka<br />
**Abstract:** <details><summary>原文: </summary>The objective of our study is to observe dynamics of multiple substances in vivo with high temporal resolution from multi-spectral magnetic resonance spectroscopic imaging (MRSI) data. The multi-spectral MRSI can effectively separate spectral peaks of multiple substances and is useful to measure spatial distributions of substances. However it is difficult to measure time-varying substance distributions directly by ordinary full sampling because the measurement requires a significantly long time. In this study, we propose a novel method to reconstruct the spatio-temporal distributions of substances from randomly undersampled multi-spectral MRSI data on the basis of compressed sensing (CS) and the partially separable function model with base spectra of substances. In our method, we have employed spatio-temporal sparsity and temporal smoothness of the substance distributions as prior knowledge to perform CS. The effectiveness of our method has been evaluated using phantom data sets of glass tubes filled with glucose or lactate solution in increasing amounts over time and animal data sets of a tumor-bearing mouse to observe the metabolic dynamics involved in the Warburg effect in vivo. The reconstructed results are consistent with the expected behaviors, showing that our method can reconstruct the spatio-temporal distribution of substances with a temporal resolution of four seconds which is extremely short time scale compared with that of full sampling. Since this method utilizes only prior knowledge naturally assumed for the spatio-temporal distributions of substances and is independent of the number of the spectral and spatial dimensions or the acquisition sequence of MRSI, it is expected to contribute to revealing the underlying substance dynamics in MRSI data already acquired or to be acquired in the future.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究的目的是通过多光谱磁共振波谱成像（MRSI）数据以高时间分辨率观察体内多种物质的动态。多光谱MRSI可以有效分离多种物质的光谱峰，有助于测量物质的空间分布。然而，通过普通的全采样直接测量随时间变化的物质分布是困难的，因为测量需要相当长的时间。在这项研究中，我们提出了一种基于压缩感知（CS）和具有物质基础光谱的部分可分离函数模型的随机欠采样多光谱MRSI数据重建物质时空分布的新方法。在我们的方法中，我们利用物质分布的时空稀疏性和时间平滑性作为执行 CS 的先验知识。我们的方法的有效性已经使用填充有随时间增加的葡萄糖或乳酸溶液的玻璃管的幻影数据集和荷瘤小鼠的动物数据集进行了评估，以观察体内瓦尔堡效应所涉及的代谢动态。重建结果与预期行为一致，表明我们的方法可以以四秒的时间分辨率重建物质的时空分布，与全采样相比，时间尺度极短。由于该方法仅利用物质时空分布自然假设的先验知识，并且独立于光谱和空间维度的数量或 MRSI 的采集序列，因此有望有助于揭示 MRSI 数据中潜在的物质动态已经获得或将来将获得的。</details>
**PDF:** <http://arxiv.org/pdf/2403.00402v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **List-Mode PET Image Reconstruction Using Dykstra-Like Splitting**<br />
**Title_cn:** 使用 Dykstra 类分割的列表模式 PET 图像重建<br />
**Authors:** Kibo Ote, Fumio Hashimoto, Yuya Onishi, Yasuomi Ouchi<br />
**Abstract:** <details><summary>原文: </summary>To converge the block iterative method in image reconstruction for positron emission tomography (PET), careful control of relaxation parameters is required, which is a challenging task. The automatic determination of relaxation parameters for list-mode reconstructions also remains challenging. Therefore, a different approach than controlling relaxation parameters would be desired by list-mode PET reconstruction. In this study, we propose a list-mode maximum likelihood Dykstra-like splitting PET reconstruction (LM-MLDS). LM-MLDS converges the list-mode block iterative method by adding the distance from an initial image as a penalty term into an objective function. LM-MLDS takes a two-step approach because its performance depends on the quality of the initial image. The first step uses a uniform image as the initial image, and then the second step uses a reconstructed image after one main iteration as the initial image. We evaluated LM-MLDS using simulation and clinical data. LM-MLDS provided a higher peak signal-to-noise ratio and suppressed an oscillation of tradeoff curves between noise and contrast than the other block iterative methods. In a clinical study, LM-MLDS removed the false hotspots at the edge of the axial field of view and improved the image quality of slices covering the top of the head to the cerebellum. LM-MLDS showed different noise properties than the other methods due to Gaussian denoising induced by the proximity operator. The list-mode proximal splitting PET reconstruction is useful not only for optimizing nondifferentiable functions such as total variation but also for converging block iterative methods without controlling relaxation parameters.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了在正电子发射断层扫描（PET）图像重建中收敛块迭代方法，需要仔细控制弛豫参数，这是一项具有挑战性的任务。自动确定列表模式重建的松弛参数仍然具有挑战性。因此，列表模式 PET 重建需要一种不同于控制松弛参数的方法。在本研究中，我们提出了一种列表模式最大似然 Dykstra 式分裂 PET 重建（LM-MLDS）。 LM-MLDS 通过将距初始图像的距离作为惩罚项添加到目标函数中来收敛列表模式块迭代方法。 LM-MLDS 采用两步方法，因为其性能取决于初始图像的质量。第一步使用均匀图像作为初始图像，然后第二步使用一次主迭代后的重建图像作为初始图像。我们使用模拟和临床数据评估了 LM-MLDS。与其他块迭代方法相比，LM-MLDS 提供了更高的峰值信噪比，并抑制了噪声和对比度之间的权衡曲线的振荡。在一项临床研究中，LM-MLDS 消除了轴向视场边缘的假热点，并提高了覆盖头顶到小脑的切片的图像质量。由于邻近算子引起的高斯去噪，LM-MLDS 显示出与其他方法不同的噪声特性。列表模式近端分裂 PET 重建不仅可用于优化不可微函数（例如总变差），而且还可用于在不控制松弛参数的情况下收敛块迭代方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.00394v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning**<br />
**Title_cn:** 重新审视下游任务中的解缠结：对其抽象视觉推理必要性的研究<br />
**Authors:** Ruiqian Nai, Zixin Wen, Ji Li, Yuanzhi Li, Yang Gao<br />
**Abstract:** <details><summary>原文: </summary>In representation learning, a disentangled representation is highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled representations to complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled representation in downstream applications. Specifically, we show that dimension-wise disentangled representations are unnecessary on a fundamental downstream task, abstract visual reasoning. We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, representation learning methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of representations is a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement explains the claimed usefulness of disentangled representations in previous works. The source code is available at https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>在表示学习中，解开的表示是非常理想的，因为它以可分离且紧凑的模式对数据的生成因素进行编码。研究人员主张利用解开的表征来完成具有令人鼓舞的经验证据的下游任务。本文进一步研究了下游应用中解缠结表示的必要性。具体来说，我们表明，在基本的下游任务（抽象视觉推理）中，维度上的解缠结表示是不必要的。我们提供了广泛的经验证据来证明解开的必要性，涵盖多个数据集、表示学习方法和下游网络架构。此外，我们的研究结果表明，表征的信息量是比解开更好的下游绩效指标。最后，信息性和解缠结之间的正相关性解释了先前作品中所声称的解缠结表示的有用性。源代码可在 https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.00352v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Event-Driven Learning for Spiking Neural Networks**<br />
**Title_cn:** 尖峰神经网络的事件驱动学习<br />
**Authors:** Wenjie Wei, Malu Zhang, Jilin Zhang, Ammar Belatreche, Jibin Wu, Zijing Xu, Xuerui Qiu, Hong Chen, Yang Yang, Haizhou Li<br />
**Abstract:** <details><summary>原文: </summary>Brain-inspired spiking neural networks (SNNs) have gained prominence in the field of neuromorphic computing owing to their low energy consumption during feedforward inference on neuromorphic hardware. However, it remains an open challenge how to effectively benefit from the sparse event-driven property of SNNs to minimize backpropagation learning costs. In this paper, we conduct a comprehensive examination of the existing event-driven learning algorithms, reveal their limitations, and propose novel solutions to overcome them. Specifically, we introduce two novel event-driven learning methods: the spike-timing-dependent event-driven (STD-ED) and membrane-potential-dependent event-driven (MPD-ED) algorithms. These proposed algorithms leverage precise neuronal spike timing and membrane potential, respectively, for effective learning. The two methods are extensively evaluated on static and neuromorphic datasets to confirm their superior performance. They outperform existing event-driven counterparts by up to 2.51% for STD-ED and 6.79% for MPD-ED on the CIFAR-100 dataset. In addition, we theoretically and experimentally validate the energy efficiency of our methods on neuromorphic hardware. On-chip learning experiments achieved a remarkable 30-fold reduction in energy consumption over time-step-based surrogate gradient methods. The demonstrated efficiency and efficacy of the proposed event-driven learning methods emphasize their potential to significantly advance the fields of neuromorphic computing, offering promising avenues for energy-efficiency applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>受大脑启发的尖峰神经网络（SNN）因其在神经形态硬件上的前馈推理过程中的低能耗而在神经形态计算领域获得了突出的地位。然而，如何有效地受益于 SNN 的稀疏事件驱动特性以最小化反向传播学习成本仍然是一个开放的挑战。在本文中，我们对现有的事件驱动学习算法进行了全面检查，揭示了它们的局限性，并提出了克服它们的新解决方案。具体来说，我们介绍了两种新颖的事件驱动学习方法：尖峰时序相关事件驱动（STD-ED）和膜电位相关事件驱动（MPD-ED）算法。这些提出的算法分别利用精确的神经元尖峰计时和膜电位来进行有效的学习。这两种方法在静态和神经形态数据集上进行了广泛的评估，以确认其优越的性能。在 CIFAR-100 数据集上，它们的 STD-ED 性能比现有的事件驱动同类产品高出 2.51%，MPD-ED 性能高出 6.79%。此外，我们从理论上和实验上验证了我们的方法在神经形态硬件上的能量效率。与基于时间步长的替代梯度方法相比，片上学习实验的能耗显着降低了 30 倍。所提出的事件驱动学习方法所展示的效率和功效强调了它们显着推进神经形态计算领域的潜力，为能源效率应用提供了有希望的途径。</details>
**PDF:** <http://arxiv.org/pdf/2403.00270v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **Parameter-Efficient Tuning of Large Convolutional Models**<br />
**Title_cn:** 大型卷积模型的参数高效调整<br />
**Authors:** Wei Chen, Zichen Miao, Qiang Qiu<br />
**Abstract:** <details><summary>原文: </summary>To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The fine-tuning of filter atoms reshapes the filter subspace, enabling convolutional layers to adapt to diverse downstream tasks efficiently. Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks. Our approach can potentially be complementary to many existing fine-tuning methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决与微调大型预训练模型相关的高计算和参数复杂性，研究人员开发了参数高效的方法，其中仅针对下游任务更新部分参数。然而，这些工作经常忽视卷积核的独特属性，而这些属性仍然是许多大型模型（例如稳定扩散）中的基本元素。在本研究中，我们首先通过将每个网络层内的卷积核分解到一小组过滤器子空间元素（称为过滤器原子）上来引入过滤器子空间。然后，我们通过仅调整过滤器原子（通常是数百个参数）来微调这些模型，以提取特定于任务的表示。为了潜在地扩展用于调整的参数空间，我们进一步展示了一种简单的方法，通过在另一组过滤器原子上递归分解每个过滤器原子来生成过完备过滤器子空间。滤波器原子的微调重塑了滤波器子空间，使卷积层能够有效地适应不同的下游任务。大量的实验表明，这样一个简单的方案超越了之前的判别任务和生成任务的调整基线。我们的方法可以补充许多现有的微调方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.00269v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART**<br />
**Title_cn:** 在自定义数据集和 mBART 上使用微调的 XLSR Wav2Vec2 进行视频转录和翻译<br />
**Authors:** Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare, Anirban C. Mitra<br />
**Abstract:** <details><summary>原文: </summary>This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究解决了用最少的数据训练个性化语音 ASR 模型的挑战。我们仅利用 YouTube 视频中 14 分钟的自定义音频，采用基于检索的语音转换 (RVC) 来创建自定义 Common Voice 16.0 语料库。随后，在此数据集上微调跨语言自监督表示 (XLSR) Wav2Vec2 模型。开发的基于网络的 GUI 可以有效地转录和翻译输入的印地语视频。通过集成 XLSR Wav2Vec2 和 mBART，该系统将翻译文本与视频时间轴对齐，为多语言视频内容转录和个性化语音翻译提供易于访问的解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2403.00212v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **ChartReformer: Natural Language-Driven Chart Image Editing**<br />
**Title_cn:** ChartReformer：自然语言驱动的图表图像编辑<br />
**Authors:** Pengyu Yan, Mahesh Bhosale, Jay Lal, Bikhyat Adhikari, David Doermann<br />
**Abstract:** <details><summary>原文: </summary>Chart visualizations are essential for data interpretation and communication; however, most charts are only accessible in image format and lack the corresponding data tables and supplementary information, making it difficult to alter their appearance for different application scenarios. To eliminate the need for original underlying data and information to perform chart editing, we propose ChartReformer, a natural language-driven chart image editing solution that directly edits the charts from the input images with the given instruction prompts. The key in this method is that we allow the model to comprehend the chart and reason over the prompt to generate the corresponding underlying data table and visual attributes for new charts, enabling precise edits. Additionally, to generalize ChartReformer, we define and standardize various types of chart editing, covering style, layout, format, and data-centric edits. The experiments show promising results for the natural language-driven chart image editing.</details>
**Abstract_cn:** <details><summary>译文: </summary>图表可视化对于数据解释和交流至关重要；然而，大多数图表只能以图像格式访问，缺乏相应的数据表和补充信息，因此很难根据不同的应用场景改变其外观。为了消除对原始基础数据和信息进行图表编辑的需要，我们提出了ChartReformer，这是一种自然语言驱动的图表图像编辑解决方案，可以根据给定的指令提示直接从输入图像编辑图表。该方法的关键是让模型理解图表并根据提示进行推理，为新图表生成相应的底层数据表和视觉属性，从而实现精确编辑。此外，为了概括 ChartReformer，我们定义并标准化了各种类型的图表编辑，包括样式、布局、格式和以数据为中心的编辑。这些实验显示了自然语言驱动的图表图像编辑的有希望的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.00209v1><br />
**Code:** null<br />

