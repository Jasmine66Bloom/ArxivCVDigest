## [UPDATED!] **2024-03-02** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code**<br />
**Title_cn:** SceneCraft：用于将 3D 场景合成为 Blender 代码的 LLM 代理<br />
**Authors:** Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 SceneCraft，这是一种大型语言模型 (LLM) 代理，可将文本描述转换为 Blender 可执行的 Python 脚本，从而渲染具有多达一百个 3D 资源的复杂场景。这个过程需要复杂的空间规划和布置。我们通过结合先进的抽象、战略规划和图书馆学习来应对这些挑战。 SceneCraft 首先将场景图建模为蓝图，详细说明场景中资源之间的空间关系。然后，SceneCraft 根据该图编写 Python 脚本，将关系转换为资产布局的数字约束。接下来，SceneCraft 利用 GPT-V 等视觉语言基础模型的感知优势来分析渲染图像并迭代地细化场景。在此过程之上，SceneCraft 具有库学习机制，可将常见脚本函数编译为可重用库，从而促进持续自我改进，而无需昂贵的 LLM 参数调整。我们的评估表明，SceneCraft 在渲染复杂场景方面超越了现有的基于 LLM 的代理，这从它遵守约束和有利的人类评估中可以看出。我们还通过重建 Sintel 电影中的详细 3D 场景并以生成的场景作为中间控制信号来指导视频生成模型，展示了 SceneCraft 更广泛的应用潜力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01248v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction**<br />
**Title_cn:** DiffSal：用于扩散显着性预测的联合音频和视频学习<br />
**Authors:** Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha<br />
**Abstract:** <details><summary>原文: </summary>Audio-visual saliency prediction can draw support from diverse modality complements, but further performance enhancement is still challenged by customized architectures as well as task-specific loss functions. In recent studies, denoising diffusion models have shown more promising in unifying task frameworks owing to their inherent ability of generalization. Following this motivation, a novel Diffusion architecture for generalized audio-visual Saliency prediction (DiffSal) is proposed in this work, which formulates the prediction problem as a conditional generative task of the saliency map by utilizing input audio and video as the conditions. Based on the spatio-temporal audio-visual features, an extra network Saliency-UNet is designed to perform multi-modal attention modulation for progressive refinement of the ground-truth saliency map from the noisy map. Extensive experiments demonstrate that the proposed DiffSal can achieve excellent performance across six challenging audio-visual benchmarks, with an average relative improvement of 6.3\% over the previous state-of-the-art results by six metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>视听显着性预测可以得到多种模态补充的支持，但进一步的性能增强仍然受到定制架构和特定于任务的损失函数的挑战。在最近的研究中，去噪扩散模型由于其固有的泛化能力，在统一任务框架方面表现出了更大的前景。遵循这一动机，本文提出了一种用于广义视听显着性预测（DiffSal）的新颖扩散架构，该架构利用输入音频和视频作为条件，将预测问题表述为显着性图的条件生成任务。基于时空视听特征，设计了一个额外的网络 Saliency-UNet 来执行多模态注意力调制，以从噪声图中逐步细化地面实况显着图。大量实验表明，所提出的 DiffSal 可以在六个具有挑战性的视听基准测试中实现出色的性能，与之前最先进的结果相比，六个指标的平均相对改进为 6.3%。</details>
**PDF:** <http://arxiv.org/pdf/2403.01226v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion**<br />
**Title_cn:** TCIG：两阶段控制图像生成，通过扩散增强质量<br />
**Authors:** Salaheldin Mohamed<br />
**Abstract:** <details><summary>原文: </summary>In recent years, significant progress has been made in the development of text-to-image generation models. However, these models still face limitations when it comes to achieving full controllability during the generation process. Often, specific training or the use of limited models is required, and even then, they have certain restrictions. To address these challenges, A two-stage method that effectively combines controllability and high quality in the generation of images is proposed. This approach leverages the expertise of pre-trained models to achieve precise control over the generated images, while also harnessing the power of diffusion models to achieve state-of-the-art quality. By separating controllability from high quality, This method achieves outstanding results. It is compatible with both latent and image space diffusion models, ensuring versatility and flexibility. Moreover, This approach consistently produces comparable outcomes to the current state-of-the-art methods in the field. Overall, This proposed method represents a significant advancement in text-to-image generation, enabling improved controllability without compromising on the quality of the generated images.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，文本到图像生成模型的发展取得了重大进展。然而，这些模型在实现生成过程的完全可控性方面仍然面临局限性。通常，需要进行特定的训练或使用有限的模型，即使这样，它们也有一定的限制。为了解决这些挑战，提出了一种有效结合图像生成的可控性和高质量的两阶段方法。这种方法利用预训练模型的专业知识来实现​​对生成图像的精确控制，同时还利用扩散模型的力量来实现最先进的质量。通过将可控性与高质量分开，该方法取得了出色的结果。它与潜在模型和图像空间扩散模型兼容，确保了多功能性和灵活性。此外，这种方法始终能产生与该领域当前最先进方法相当的结果。总体而言，所提出的方法代表了文本到图像生成方面的重大进步，能够在不影响生成图像质量的情况下提高可控性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01212v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Training Unbiased Diffusion Models From Biased Dataset**<br />
**Title_cn:** 从有偏数据集训练无偏扩散模型<br />
**Authors:** Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, Wanmo Kang, Il-Chul Moon<br />
**Abstract:** <details><summary>原文: </summary>With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着扩散模型的显着进步，解决数据集偏差的潜在风险变得越来越重要。由于生成的输出直接受到数据集偏差的影响，因此减轻潜在偏差成为提高样本质量和比例的关键因素。本文提出了与时间相关的重要性重新加权，以减轻扩散模型的偏差。我们证明，与时间相关的密度比变得比以前的方法更精确，从而最大限度地减少生成学习中的错误传播。虽然直接将其应用于分数匹配很困难，但我们发现使用时间相关的密度比进行重新加权和分数校正可以产生目标函数的易于处理的形式，以重新生成无偏数据密度。此外，我们从理论上建立了与传统分数匹配的联系，并证明了其收敛到无偏分布。实验证据支持所提出方法的实用性，该方法优于基线，包括在具有各种偏差设置的 CIFAR-10、CIFAR-100、FFHQ 和 CelebA 上进行与时间无关的重要性重新加权。我们的代码可从 https://github.com/alsdudrla10/TIW-DSM 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01189v1><br />
**Code:** <https://github.com/alsdudrla10/tiw-dsm>**<br />
>>**index:** 5<br />
**Title:** **Dynamic 3D Point Cloud Sequences as 2D Videos**<br />
**Title_cn:** 作为 2D 视频的动态 3D 点云序列<br />
**Authors:** Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang<br />
**Abstract:** <details><summary>原文: </summary>Dynamic 3D point cloud sequences serve as one of the most common and practical representation modalities of dynamic real-world environments. However, their unstructured nature in both spatial and temporal domains poses significant challenges to effective and efficient processing. Existing deep point cloud sequence modeling approaches imitate the mature 2D video learning mechanisms by developing complex spatio-temporal point neighbor grouping and feature aggregation schemes, often resulting in methods lacking effectiveness, efficiency, and expressive power. In this paper, we propose a novel generic representation called \textit{Structured Point Cloud Videos} (SPCVs). Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2D manifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial smoothness and temporal consistency, where the pixel values correspond to the 3D coordinates of points. The structured nature of our SPCV representation allows for the seamless adaptation of well-established 2D image/video techniques, enabling efficient and effective processing and analysis of 3D point cloud sequences. To achieve such re-organization, we design a self-supervised learning pipeline that is geometrically regularized and driven by self-reconstructive and deformation field learning objectives. Additionally, we construct SPCV-based frameworks for both low-level and high-level 3D point cloud sequence processing and analysis tasks, including action recognition, temporal interpolation, and compression. Extensive experiments demonstrate the versatility and superiority of the proposed SPCV, which has the potential to offer new possibilities for deep learning on unstructured 3D point cloud sequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</details>
**Abstract_cn:** <details><summary>译文: </summary>动态 3D 点云序列是动态现实环境最常见、最实用的表示方式之一。然而，它们在空间和时间域上的非结构化性质对有效和高效的处理提出了重大挑战。现有的深度点云序列建模方法通过开发复杂的时空点邻居分组和特征聚合方案来模仿成熟的2D视频学习机制，通常导致方法缺乏有效性、效率和表达能力。在本文中，我们提出了一种新颖的通用表示形式，称为 \textit{结构化点云视频} (SPCV)。直观地说，利用 3D 几何形状本质上是 2D 流形这一事实，SPCV 将点云序列重新组织为具有空间平滑性和时间一致性的 2D 视频，其中像素值对应于点的 3D 坐标。我们的 SPCV 表示的结构化性质允许无缝适应成熟的 2D 图像/视频技术，从而实现 3D 点云序列的高效且有效的处理和分析。为了实现这种重组，我们设计了一个自我监督的学习管道，该管道是几何正则化的，并由自我重建和变形场学习目标驱动。此外，我们还为低级和高级 3D 点云序列处理和分析任务构建了基于 SPCV 的框架，包括动作识别、时间插值和压缩。大量的实验证明了所提出的 SPCV 的多功能性和优越性，它有可能为非结构化 3D 点云序列的深度学习提供新的可能性。代码将在 https://github.com/ZENGYIMING-EAMON/SPCV 发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.01129v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Text-guided Explorable Image Super-resolution**<br />
**Title_cn:** 文本引导的可探索图像超分辨率<br />
**Authors:** Kanchana Vaishnavi Gandikota, Paramanand Chandramouli<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce the problem of zero-shot text-guided exploration of the solutions to open-domain image super-resolution. Our goal is to allow users to explore diverse, semantically accurate reconstructions that preserve data consistency with the low-resolution inputs for different large downsampling factors without explicitly training for these specific degradations. We propose two approaches for zero-shot text-guided super-resolution - i) modifying the generative process of text-to-image \textit{T2I} diffusion models to promote consistency with low-resolution inputs, and ii) incorporating language guidance into zero-shot diffusion-based restoration methods. We show that the proposed approaches result in diverse solutions that match the semantic meaning provided by the text prompt while preserving data consistency with the degraded inputs. We evaluate the proposed baselines for the task of extreme super-resolution and demonstrate advantages in terms of restoration quality, diversity, and explorability of solutions.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了开放域图像超分辨率解决方案的零样本文本引导探索问题。我们的目标是允许用户探索多样化的、语义上准确的重建，从而保持数据与不同大下采样因子的低分辨率输入的一致性，而无需针对这些特定的退化进行明确的训练。我们提出了两种零样本文本引导超分辨率方法 - i）修改文本到图像 \textit{T2I} 扩散模型的生成过程以促进与低分辨率输入的一致性，以及 ii）将语言指导纳入基于零样本扩散的恢复方法。我们表明，所提出的方法产生了多种解决方案，这些解决方案与文本提示提供的语义相匹配，同时保持数据与降级输入的一致性。我们评估了极端超分辨率任务所提出的基线，并展示了在恢复质量、多样性和解决方案的可探索性方面的优势。</details>
**PDF:** <http://arxiv.org/pdf/2403.01124v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Face Swap via Diffusion Model**<br />
**Title_cn:** 通过扩散模型进行面部交换<br />
**Authors:** Feifei Wang<br />
**Abstract:** <details><summary>原文: </summary>This technical report presents a diffusion model based framework for face swapping between two portrait images. The basic framework consists of three components, i.e., IP-Adapter, ControlNet, and Stable Diffusion's inpainting pipeline, for face feature encoding, multi-conditional generation, and face inpainting respectively. Besides, I introduce facial guidance optimization and CodeFormer based blending to further improve the generation quality.   Specifically, we engage a recent light-weighted customization method (i.e., DreamBooth-LoRA), to guarantee the identity consistency by 1) using a rare identifier "sks" to represent the source identity, and 2) injecting the image features of source portrait into each cross-attention layer like the text features. Then I resort to the strong inpainting ability of Stable Diffusion, and utilize canny image and face detection annotation of the target portrait as the conditions, to guide ContorlNet's generation and align source portrait with the target portrait. To further correct face alignment, we add the facial guidance loss to optimize the text embedding during the sample generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>该技术报告提出了一种基于扩散模型的框架，用于两个肖像图像之间的面部交换。基本框架由三个组件组成，即IP-Adapter、ControlNet和Stable Diffusion的修复管道，分别用于人脸特征编码、多条件生成和人脸修复。此外，我还引入了面部引导优化和基于 CodeFormer 的混合，以进一步提高生成质量。具体来说，我们采用了最近的轻量级定制方法（即DreamBooth-LoRA），通过以下方式保证身份一致性：1）使用罕见的标识符“sks”来表示源身份，2）注入源肖像的图像特征像文本特征一样进入每个交叉注意力层。然后借助Stable Diffusion强大的修复能力，以目标人像的canny图像和人脸检测标注为条件，指导ContorlNet的生成，并将源人像与目标人像对齐。为了进一步纠正面部对齐，我们添加面部引导损失以优化样本生成过程中的文本嵌入。</details>
**PDF:** <http://arxiv.org/pdf/2403.01108v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions**<br />
**Title_cn:** DNA 系列：通过块级监督增强权重共享 NAS<br />
**Authors:** Guangrun Wang, Changlin Li, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian, Xiaodan Liang, Xiaojun Chang, Liang Lin<br />
**Abstract:** <details><summary>原文: </summary>Neural Architecture Search (NAS), aiming at automatically designing neural architectures by machines, has been considered a key step toward automatic machine learning. One notable NAS branch is the weight-sharing NAS, which significantly improves search efficiency and allows NAS algorithms to run on ordinary computers. Despite receiving high expectations, this category of methods suffers from low search effectiveness. By employing a generalization boundedness tool, we demonstrate that the devil behind this drawback is the untrustworthy architecture rating with the oversized search space of the possible architectures. Addressing this problem, we modularize a large search space into blocks with small search spaces and develop a family of models with the distilling neural architecture (DNA) techniques. These proposed models, namely a DNA family, are capable of resolving multiple dilemmas of the weight-sharing NAS, such as scalability, efficiency, and multi-modal compatibility. Our proposed DNA models can rate all architecture candidates, as opposed to previous works that can only access a subsearch space using heuristic algorithms. Moreover, under a certain computational complexity constraint, our method can seek architectures with different depths and widths. Extensive experimental evaluations show that our models achieve state-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile convolutional network and a small vision transformer, respectively. Additionally, we provide in-depth empirical analysis and insights into neural architecture ratings. Codes available: \url{https://github.com/changlin31/DNA}.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经架构搜索（NAS）旨在由机器自动设计神经架构，被认为是自动机器学习的关键一步。一个值得注意的 NAS 分支是权重共享 NAS，它显着提高了搜索效率，并允许 NAS 算法在普通计算机上运行。尽管受到很高的期望，此类方法的搜索效率较低。通过使用泛化有界工具，我们证明了这个缺点背后的魔鬼是不可信的架构评级以及可能架构的过大搜索空间。为了解决这个问题，我们将大型搜索空间模块化为具有小型搜索空间的块，并使用蒸馏神经架构（DNA）技术开发了一系列模型。这些提出的模型，即 DNA 系列，能够解决权重共享 NAS 的多种困境，例如可扩展性、效率和多模式兼容性。我们提出的 DNA 模型可以对所有候选架构进行评级，而之前的工作只能使用启发式算法访问子搜索空间。此外，在一定的计算复杂度约束下，我们的方法可以寻找具有不同深度和宽度的架构。大量的实验评估表明，我们的模型在 ImageNet 上的移动卷积网络和小型视觉变换器上分别实现了最先进的 top-1 准确率 78.9% 和 83.6%。此外，我们还提供对神经架构评级的深入实证分析和见解。可用代码：\url{https://github.com/changlin31/DNA}。</details>
**PDF:** <http://arxiv.org/pdf/2403.01326v1><br />
**Code:** <https://github.com/changlin31/DNA>**<br />
>>**index:** 2<br />
**Title:** **TUMTraf V2X Cooperative Perception Dataset**<br />
**Title_cn:** TUMTraf V2X 协作感知数据集<br />
**Authors:** Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois C. Knoll<br />
**Abstract:** <details><summary>原文: </summary>Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.</details>
**Abstract_cn:** <details><summary>译文: </summary>合作感知为增强自动驾驶汽车的能力和改善道路安全提供了多种好处。除车载传感器外，使用路边传感器可提高可靠性并扩展传感器范围。外部传感器为自动驾驶车辆提供更高的态势感知并防止遮挡。我们提出了 CoopDet3D（一种协作多模态融合模型）和 TUMTraf-V2X（感知数据集），用于协作 3D 对象检测和跟踪任务。我们的数据集包含来自五个路边和四个车载传感器的 2,000 个标记点云和 5,000 个标记图像。它包括 30k 个 3D 框，带有轨道 ID 以及精确的 GPS 和 IMU 数据。我们标记了八个类别，并涵盖了具有挑战性驾驶操作的遮挡场景，例如交通违规、未遂事件、超车和掉头。通过多次实验，我们表明，与车载相机-LiDAR 融合模型相比，我们的 CoopDet3D 相机-LiDAR 融合模型实现了+14.36 3D mAP 的增加。最后，我们在我们的网站上公开提供我们的数据集、模型、标记工具和开发工具包：https://tum-traffic-dataset.github.io/tumtraf-v2x。</details>
**PDF:** <http://arxiv.org/pdf/2403.01316v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation**<br />
**Title_cn:** ICC：量化多模态数据集管理的图像描述的具体性<br />
**Authors:** Moran Yanuka, Morris Alper, Hadar Averbuch-Elor, Raja Giryes<br />
**Abstract:** <details><summary>原文: </summary>Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>对文本-图像配对数据进行网络规模的训练对于多模态学习变得越来越重要，但受到野外数据集的高噪声性质的挑战。标准数据过滤方法成功地删除了不匹配的文本图像对，但允许语义相关但高度抽象或主观的文本。这些方法缺乏细粒度的能力来隔离最具体的样本，而这些样本为在嘈杂的数据集中提供最强的学习信号。在这项工作中，我们提出了一种新的指标，即图像标题具体性，它可以在没有图像参考的情况下评估标题文本，以衡量其在多模态学习中使用的具体性和相关性。我们的方法利用强大的基础模型来测量多模态表示中的视觉语义信息丢失。我们证明，这与人类对单词和句子级文本的具体性评估密切相关。此外，我们表明使用 ICC 进行管理是对现有方法的补充：它成功地从多模式网络规模数据集中选择最高质量的样本，以便在资源有限的环境中进行有效的培训。</details>
**PDF:** <http://arxiv.org/pdf/2403.01306v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild**<br />
**Title_cn:** REWIND 数据集：根据野外多模态身体运动信号进行隐私保护的说话状态分割<br />
**Authors:** Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung<br />
**Abstract:** <details><summary>原文: </summary>Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking status estimation by presenting the first publicly available multimodal dataset with high-quality individual speech recordings of 33 subjects in a professional networking event. We present three baselines for no-audio speaking status segmentation: a) from video, b) from body acceleration (chest-worn accelerometer), c) from body pose tracks. In all cases we predict a 20Hz binary speaking status signal extracted from the audio, a time resolution not available in previous datasets. In addition to providing the signals and ground truth necessary to evaluate a wide range of speaking status detection methods, the availability of audio in REWIND makes it suitable for cross-modality studies not feasible with previous mingling datasets. Finally, our flexible data consent setup creates new challenges for multimodal systems under missing modalities.</details>
**Abstract_cn:** <details><summary>译文: </summary>识别人类说话是理解社交互动的核心任务。理想情况下，可以从个人录音中检测讲话，就像之前在会议场景中所做的那样。然而，由于成本、物流和隐私问题，在野外很难获得个人语音记录，尤其是在拥挤的混合场景中。作为替代方案，基于视频和可穿戴传感器数据训练的机器学习模型可以通过以不引人注目、保护隐私的方式检测相关手势来识别语音。理想情况下，这些模型本身应该使用从语音信号获得的标签进行训练。然而，现有的混合数据集不包含高质量的录音。相反，说话状态注释通常是由人类注释者从视频中推断出来的，而没有根据基于音频的基本事实验证这种方法。在本文中，我们通过展示第一个公开可用的多模态数据集，其中包含专业社交活动中 33 名受试者的高质量个人语音录音，重新审视无音频讲话状态估计。我们提出了无音频说话状态分割的三个基线：a）来自视频，b）来自身体加速度（胸戴式加速度计），c）来自身体姿势轨迹。在所有情况下，我们都会预测从音频中提取的 20Hz 二进制说话状态信号，这是以前数据集中不可用的时间分辨率。除了提供评估各种说话状态检测方法所需的信号和基本事实之外，REWIND 中音频的可用性使其适合跨模态研究，而这对于以前的混合数据集来说是不可行的。最后，我们灵活的数据同意设置为缺少模式的多模式系统带来了新的挑战。</details>
**PDF:** <http://arxiv.org/pdf/2403.01229v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Adversarial Testing for Visual Grounding via Image-Aware Property Reduction**<br />
**Title_cn:** 通过图像感知属性减少进行视觉接地的对抗性测试<br />
**Authors:** Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Boyu Wu, Fanjiang Xu, Qing Wang<br />
**Abstract:** <details><summary>原文: </summary>Due to the advantages of fusing information from various modalities, multimodal learning is gaining increasing attention. Being a fundamental task of multimodal learning, Visual Grounding (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the black box scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-related information in the original expression meanwhile ensuring the reduced expression can still uniquely describe the original object in the image. To achieve this, PEELING first conducts the object and properties extraction and recombination to generate candidate property reduction expressions. It then selects the satisfied expressions that accurately describe the original object while ensuring no other objects in the image fulfill the expression, through querying the image with a visual understanding technique. We evaluate PEELING on the state-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets. Results show that the adversarial tests generated by PEELING achieves 21.4% in MultiModal Impact score (MMI), and outperforms state-of-the-art baselines for images and texts by 8.2%--15.1%.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于融合来自不同模态的信息的优势，多模态学习越来越受到关注。视觉接地（VG）是多模态学习的一项基本任务，旨在通过自然语言表达来定位图像中的对象。由于任务的复杂性，确保 VG 模型的质量面临着巨大的挑战。在黑盒场景中，现有的对抗性测试技术通常无法充分利用这两种信息模式的潜力。他们通常仅根据图像或文本信息应用扰动，而忽略两种模式之间的关键相关性，这将导致测试预言失败或无法有效挑战 VG 模型。为此，我们提出了 PEELING，一种通过图像感知属性减少来进行 VG 模型的对抗性测试的文本扰动方法。其核心思想是减少原始表达式中与属性相关的信息，同时保证减少后的表达式仍然能够唯一地描述图像中的原始对象。为了实现这一点，PEELING 首先进行对象和属性提取和重组，以生成候选属性约简表达式。然后，通过使用视觉理解技术查询图像，选择准确描述原始对象的满意表达式，同时确保图像中没有其他对象满足该表达式。我们在最先进的 VG 模型（即 OFA-VG）上评估 PEELING，涉及三个常用数据集。结果显示，PEELING 生成的对抗性测试在多模式影响评分 (MMI) 中达到了 21.4%，比图像和文本的最新基线高出 8.2%--15.1%。</details>
**PDF:** <http://arxiv.org/pdf/2403.01118v1><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning**<br />
**Title_cn:** NeRF-VPT：通过视图提示调整学习具有神经辐射场的新颖视图表示<br />
**Authors:** Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr<br />
**Abstract:** <details><summary>原文: </summary>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{https://github.com/Freedomcls/NeRF-VPT}.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经辐射场（NeRF）在新颖的视图合成方面取得了显着的成功。尽管如此，为新颖的视角生成高质量图像的任务仍然是一项严峻的挑战。虽然现有的努力已经取得了值得称赞的进展，但捕获复杂的细节、增强纹理并实现卓越的峰值信噪比 (PSNR) 指标值得进一步关注和进步。在这项工作中，我们提出了 NeRF-VPT，一种新颖的视图合成创新方法来应对这些挑战。我们提出的 NeRF-VPT 采用级联视图提示调整范例，其中从先前渲染结果获得的 RGB 信息作为后续渲染阶段的指导性视觉提示，希望嵌入在提示中的先验知识可以促进渲染图像的逐步增强质量。 NeRF-VPT 只需要在每个训练阶段从前一阶段渲染中采样 RGB 数据作为先验，无需依赖额外的指导或复杂的技术。因此，我们的 NeRF-VPT 是即插即用的，可以轻松集成到现有方法中。通过对我们的 NeRF-VPT 与几种基于 NeRF 的方法在要求严格的真实场景基准上进行比较分析，例如 Realistic Synthetic 360、Real Forward-Facing、Replica 数据集和用户捕获的数据集，我们证实了我们的 NeRF-VPT与所有比较的最先进方法相比，显着提高了基线性能，并能熟练地生成更高质量的新颖视图图像。此外，NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。源代码和数据集可在 \url{https://github.com/Freedomcls/NeRF-VPT} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01325v1><br />
**Code:** <https://github.com/freedomcls/nerf-vpt>**<br />
>>**index:** 2<br />
**Title:** **Neural radiance fields-based holography [Invited]**<br />
**Title_cn:** 基于神经辐射场的全息术 [邀请]<br />
**Authors:** Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba<br />
**Abstract:** <details><summary>原文: </summary>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究提出了一种基于神经辐射场（NeRF）技术生成全息图的新方法。在全息图计算中生成三维（3D）数据很困难。 NeRF 是一种基于体渲染从 2D 图像重建 3D 光场的最先进技术。 NeRF 可以快速预测不包含训练数据集的新视图图像。在这项研究中，我们直接根据 NeRF 从 2D 图像生成的 3D 光场构建了渲染管道，以便在合理的时间内使用深度神经网络生成全息图。该管道包含三个主要组件：NeRF、深度预测器和全息图生成器，所有组件均使用深度神经网络构建。该管道不包括任何物理计算。使用所提出的管道计算从任何方向观看的 3D 场景的预测全息图。给出了模拟和实验结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.01137v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Neural Field Classifiers via Target Encoding and Classification Loss**<br />
**Title_cn:** 通过目标编码和分类损失的神经场分类器<br />
**Authors:** Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun<br />
**Abstract:** <details><summary>原文: </summary>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经场方法在计算机视觉和计算机图形学的各种长期任务中取得了巨大进展，包括新颖的视图合成和几何重建。由于现有的神经场方法试图预测一些基于坐标的连续目标值，例如神经辐射场（NeRF）的RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。然而，回归模型真的比神经场方法的分类模型更好吗？在这项工作中，我们尝试从机器学习的角度探讨神经领域这个非常基本但被忽视的问题。我们成功提出了一种新颖的神经场分类器（NFC）框架，它将现有的神经场方法制定为分类任务而不是回归任务。通过采用新颖的目标编码模块并优化分类损失，所提出的 NFC 可以轻松地将任意神经场回归器 (NFR) 转换为其分类变体。通过将连续回归目标编码为高维离散编码，我们自然地制定了多标签分类任务。大量的实验证明了 NFC 的有效性令人印象深刻，并且几乎不需要额外的计算成本。此外，NFC 还表现出对稀疏输入、损坏图像和动态场景的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01058v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models**<br />
**Title_cn:** 用于扩散和流动模型快速采样的定制非平稳求解器<br />
**Authors:** Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($<$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了定制非平稳 (BNS) 求解器，这是一种求解器蒸馏方法，可提高扩散和流动模型的样本效率。 BNS 求解器基于一系列非平稳求解器，这些求解器可证明包含现有的数值 ODE 求解器，因此在样本近似 (PSNR) 方面比这些基线表现出显着改进。与模型蒸馏相比，BNS 求解器受益于微小的参数空间（$<200 参数）、快速优化（快两个数量级）、保持样本的多样性，并且与之前的求解器蒸馏方法相比，几乎缩小了与标准蒸馏的差距方法，例如低-中 NFE 体系中的渐进蒸馏。例如，BNS 求解器在类条件 ImageNet-64 中使用 16 NFE 实现 45 PSNR / 1.76 FID。我们尝试使用 BNS 求解器进行条件图像生成、文本到图像生成和文本 2 音频生成，结果显示样本近似 (PSNR) 总体上有显着改善。</details>
**PDF:** <http://arxiv.org/pdf/2403.01329v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving**<br />
**Title_cn:** 走向便携性：压缩自动驾驶端到端运动规划器<br />
**Authors:** Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang<br />
**Abstract:** <details><summary>原文: </summary>End-to-end motion planning models equipped with deep neural networks have shown great potential for enabling full autonomous driving. However, the oversized neural networks render them impractical for deployment on resource-constrained systems, which unavoidably requires more computational time and resources during reference.To handle this, knowledge distillation offers a promising approach that compresses models by enabling a smaller student model to learn from a larger teacher model. Nevertheless, how to apply knowledge distillation to compress motion planners has not been explored so far. In this paper, we propose PlanKD, the first knowledge distillation framework tailored for compressing end-to-end motion planners. First, considering that driving scenes are inherently complex, often containing planning-irrelevant or even noisy information, transferring such information is not beneficial for the student planner. Thus, we design an information bottleneck based strategy to only distill planning-relevant information, rather than transfer all information indiscriminately. Second, different waypoints in an output planned trajectory may hold varying degrees of importance for motion planning, where a slight deviation in certain crucial waypoints might lead to a collision. Therefore, we devise a safety-aware waypoint-attentive distillation module that assigns adaptive weights to different waypoints based on the importance, to encourage the student to accurately mimic more crucial waypoints, thereby improving overall safety. Experiments demonstrate that our PlanKD can boost the performance of smaller planners by a large margin, and significantly reduce their reference time.</details>
**Abstract_cn:** <details><summary>译文: </summary>配备深度神经网络的端到端运动规划模型已显示出实现完全自动驾驶的巨大潜力。然而，过大的神经网络使得它们在资源受限的系统上部署不切实际，这在参考过程中不可避免地需要更多的计算时间和资源。为了解决这个问题，知识蒸馏提供了一种有前途的方法，通过使较小的学生模型能够学习来压缩模型更大的教师模型。然而，到目前为止，如何应用知识蒸馏来压缩运动规划器尚未被探索。在本文中，我们提出了 PlanKD，这是第一个为压缩端到端运动规划器而定制的知识蒸馏框架。首先，考虑到驾驶场景本质上很复杂，通常包含与规划无关甚至嘈杂的信息，传输此类信息对学生规划者来说并没有好处。因此，我们设计了一种基于信息瓶颈的策略，仅提取与规划相关的信息，而不是不加区别地传输所有信息。其次，输出规划轨迹中的不同航路点对于运动规划可能具有不同程度的重要性，其中某些关键航路点的轻微偏差可能会导致碰撞。因此，我们设计了一个安全意识航路点关注蒸馏模块，根据重要性为不同航路点分配自适应权重，以鼓励学生准确模仿更关键的航路点，从而提高整体安全性。实验表明，我们的 PlanKD 可以大幅提高小型规划器的性能，并显着减少其参考时间。</details>
**PDF:** <http://arxiv.org/pdf/2403.01238v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection**<br />
**Title_cn:** 通过不确定性量化从量化网络中提取可用预测以进行 OOD 检测<br />
**Authors:** Rishi Singhal, Srinath Srinivasan<br />
**Abstract:** <details><summary>原文: </summary>OOD detection has become more pertinent with advances in network design and increased task complexity. Identifying which parts of the data a given network is misclassifying has become as valuable as the network's overall performance. We can compress the model with quantization, but it suffers minor performance loss. The loss of performance further necessitates the need to derive the confidence estimate of the network's predictions. In line with this thinking, we introduce an Uncertainty Quantification(UQ) technique to quantify the uncertainty in the predictions from a pre-trained vision model. We subsequently leverage this information to extract valuable predictions while ignoring the non-confident predictions. We observe that our technique saves up to 80% of ignored samples from being misclassified. The code for the same is available here.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着网络设计的进步和​​任务复杂性的增加，OOD 检测变得更加相关。识别给定网络的数据的哪些部分被错误分类已经变得与网络的整体性能一样有价值。我们可以通过量化来压缩模型，但它的性能损失很小。性能损失进一步需要导出网络预测的置信度估计。根据这种想法，我们引入了不确定性量化（UQ）技术来量化预训练视觉模型的预测的不确定性。随后，我们利用这些信息来提取有价值的预测，同时忽略不可信的预测。我们观察到，我们的技术可以避免高达 80% 的被忽略样本被错误分类。此处提供了相同的代码。</details>
**PDF:** <http://arxiv.org/pdf/2403.01076v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System**<br />
**Title_cn:** 基于图像的饮食评估：健康饮食餐盘估计系统<br />
**Authors:** Assylzhan Izbassar, Pakizar Shamoi<br />
**Abstract:** <details><summary>原文: </summary>The nutritional quality of diets has significantly deteriorated over the past two to three decades, a decline often underestimated by the people. This deterioration, coupled with a hectic lifestyle, has contributed to escalating health concerns. Recognizing this issue, researchers at Harvard have advocated for a balanced nutritional plate model to promote health. Inspired by this research, our paper introduces an innovative Image-Based Dietary Assessment system aimed at evaluating the healthiness of meals through image analysis. Our system employs advanced image segmentation and classification techniques to analyze food items on a plate, assess their proportions, and calculate meal adherence to Harvard's healthy eating recommendations. This approach leverages machine learning and nutritional science to empower individuals with actionable insights for healthier eating choices. Our four-step framework involves segmenting the image, classifying the items, conducting a nutritional assessment based on the Harvard Healthy Eating Plate research, and offering tailored recommendations. The prototype system has shown promising results in promoting healthier eating habits by providing an accessible, evidence-based tool for dietary assessment.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的两到三十年里，饮食的营养质量显着恶化，人们常常低估了这种下降。这种恶化，加上忙碌的生活方式，导致健康问题不断升级。认识到这个问题，哈佛大学的研究人员提倡采用均衡营养餐盘模式来促进健康。受这项研究的启发，我们的论文介绍了一种创新的基于图像的饮食评估系统，旨在通过图像分析评估膳食的健康程度。我们的系统采用先进的图像分割和分类技术来分析盘子上的食物，评估其比例，并计算膳食是否符合哈佛大学的健康饮食建议。这种方法利用机器学习和营养科学，为个人提供可行的见解，以做出更健康的饮食选择。我们的四步框架包括分割图像、对项目进行分类、根据哈佛健康饮食餐盘研究进行营养评估，并提供量身定制的建议。该原型系统通过提供一种易于使用、基于证据的饮食评估工具，在促进更健康的饮食习惯方面取得了可喜的成果。</details>
**PDF:** <http://arxiv.org/pdf/2403.01310v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection**<br />
**Title_cn:** 因果模式多路复用器：无偏多光谱行人检测的新颖框架<br />
**Authors:** Taeheon Kim, Sebin Shin, Youngjoon Yu, Hak Gu Kim, Yong Man Ro<br />
**Abstract:** <details><summary>原文: </summary>RGBT multispectral pedestrian detection has emerged as a promising solution for safety-critical applications that require day/night operations. However, the modality bias problem remains unsolved as multispectral pedestrian detectors learn the statistical bias in datasets. Specifically, datasets in multispectral pedestrian detection mainly distribute between ROTO (day) and RXTO (night) data; the majority of the pedestrian labels statistically co-occur with their thermal features. As a result, multispectral pedestrian detectors show poor generalization ability on examples beyond this statistical correlation, such as ROTX data. To address this problem, we propose a novel Causal Mode Multiplexer (CMM) framework that effectively learns the causalities between multispectral inputs and predictions. Moreover, we construct a new dataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian detection. ROTX-MP mainly includes ROTX examples not presented in previous datasets. Extensive experiments demonstrate that our proposed CMM framework generalizes well on existing datasets (KAIST, CVC-14, FLIR) and the new ROTX-MP. We will release our new dataset to the public for future research.</details>
**Abstract_cn:** <details><summary>译文: </summary>RGBT 多光谱行人检测已成为需要白天/夜间操作的安全关键型应用的一种有前景的解决方案。然而，随着多光谱行人检测器学习数据集中的统计偏差，模态偏差问题仍未解决。具体来说，多光谱行人检测中的数据集主要分布在ROTO（白天）和RXTO（夜间）数据之间；大多数行人标签在统计上与其热特征同时出现。因此，多光谱行人检测器在超出这种统计相关性的示例（例如 ROTX 数据）上表现出较差的泛化能力。为了解决这个问题，我们提出了一种新颖的因果模式复用器（CMM）框架，可以有效地学习多光谱输入和预测之间的因果关系。此外，我们构建了一个新的数据集（ROTX-MP）来评估多光谱行人检测中的模态偏差。 ROTX-MP 主要包括以前数据集中未出现的 ROTX 示例。大量实验表明，我们提出的 CMM 框架可以很好地推广现有数据集（KAIST、CVC-14、FLIR）和新的 ROTX-MP。我们将向公众发布我们的新数据集以供未来研究。</details>
**PDF:** <http://arxiv.org/pdf/2403.01300v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Fast Low-parameter Video Activity Localization in Collaborative Learning Environments**<br />
**Title_cn:** 协作学习环境中的快速低参数视频活动定位<br />
**Authors:** Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon Pattichis, Marios S. Patticis<br />
**Abstract:** <details><summary>原文: </summary>Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring transfer learning from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频活动检测的研究主要集中在识别短视频片段中明确定义的人类活动。视频活动识别的大部分研究都集中在需要对大型视频数据集进行训练的大参数系统的开发上。本文开发了一种具有快速推理能力的低参数模块化系统，可以完全在有限的数据集上进行训练，而不需要从大参数系统进行迁移学习。该系统可以准确地检测特定活动并将其与现实课堂视频中执行活动的学生关联起来。此外，本文还开发了一种基于网络的交互式应用程序，可以通过长长的现实课堂视频可视化人类活动地图。</details>
**PDF:** <http://arxiv.org/pdf/2403.01281v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Benchmarking Segmentation Models with Mask-Preserved Attribute Editing**<br />
**Title_cn:** 使用掩模保留属性编辑对分割模型进行基准测试<br />
**Authors:** Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo<br />
**Abstract:** <details><summary>原文: </summary>When deploying segmentation models in practice, it is critical to evaluate their behaviors in varied and complex scenes. Different from the previous evaluation paradigms only in consideration of global attribute variations (e.g. adverse weather), we investigate both local and global attribute variations for robustness evaluation. To achieve this, we construct a mask-preserved attribute editing pipeline to edit visual attributes of real images with precise control of structural information. Therefore, the original segmentation labels can be reused for the edited images. Using our pipeline, we construct a benchmark covering both object and image attributes (e.g. color, material, pattern, style). We evaluate a broad variety of semantic segmentation models, spanning from conventional close-set models to recent open-vocabulary large models on their robustness to different types of variations. We find that both local and global attribute variations affect segmentation performances, and the sensitivity of models diverges across different variation types. We argue that local attributes have the same importance as global attributes, and should be considered in the robustness evaluation of segmentation models. Code: https://github.com/PRIS-CV/Pascal-EA.</details>
**Abstract_cn:** <details><summary>译文: </summary>在实践中部署分割模型时，评估其在各种复杂场景中的行为至关重要。与之前仅考虑全局属性变化（例如恶劣天气）的评估范式不同，我们同时研究局部和全局属性变化以进行鲁棒性评估。为了实现这一目标，我们构建了一个保留掩模的属性编辑管道，通过精确控制结构信息来编辑真实图像的视觉属性。因此，原始分割标签可以重新用于编辑后的图像。使用我们的管道，我们构建了一个涵盖对象和图像属性（例如颜色、材料、图案、风格）的基准。我们评估了各种语义分割模型，从传统的封闭集模型到最近的开放词汇大型模型，评估它们对不同类型变化的鲁棒性。我们发现局部和全局属性变化都会影响分割性能，并且模型的敏感性在不同变化类型之间存在差异。我们认为局部属性与全局属性具有相同的重要性，并且应该在分割模型的鲁棒性评估中考虑。代码：https://github.com/PRIS-CV/Pascal-EA。</details>
**PDF:** <http://arxiv.org/pdf/2403.01231v1><br />
**Code:** <https://github.com/pris-cv/pascal-ea>**<br />
>>**index:** 5<br />
**Title:** **Boosting Box-supervised Instance Segmentation with Pseudo Depth**<br />
**Title_cn:** 使用伪深度增强盒监督实例分割<br />
**Authors:** Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu, Linlin Ou<br />
**Abstract:** <details><summary>原文: </summary>The realm of Weakly Supervised Instance Segmentation (WSIS) under box supervision has garnered substantial attention, showcasing remarkable advancements in recent years. However, the limitations of box supervision become apparent in its inability to furnish effective information for distinguishing foreground from background within the specified target box. This research addresses this challenge by introducing pseudo-depth maps into the training process of the instance segmentation network, thereby boosting its performance by capturing depth differences between instances. These pseudo-depth maps are generated using a readily available depth predictor and are not necessary during the inference stage. To enable the network to discern depth features when predicting masks, we integrate a depth prediction layer into the mask prediction head. This innovative approach empowers the network to simultaneously predict masks and depth, enhancing its ability to capture nuanced depth-related information during the instance segmentation process. We further utilize the mask generated in the training process as supervision to distinguish the foreground from the background. When selecting the best mask for each box through the Hungarian algorithm, we use depth consistency as one calculation cost item. The proposed method achieves significant improvements on Cityscapes and COCO dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>盒子监督下的弱监督实例分割（WSIS）领域引起了广泛关注，近年来取得了显着进步。然而，框监督的局限性变得显而易见，因为它无法提供有效的信息来区分指定目标框中的前景和背景。这项研究通过将伪深度图引入实例分割网络的训练过程来解决这一挑战，从而通过捕获实例之间的深度差异来提高其性能。这些伪深度图是使用现成的深度预测器生成的，并且在推理阶段不是必需的。为了使网络在预测掩模时能够识别深度特征，我们将深度预测层集成到掩模预测头中。这种创新方法使网络能够同时预测掩模和深度，从而增强其在实例分割过程中捕获与深度相关的细微信息的能力。我们进一步利用训练过程中生成的掩模作为监督来区分前景和背景。在通过匈牙利算法为每个框选择最佳掩模时，我们使用深度一致性作为一项计算成本项。所提出的方法在 Cityscapes 和 COCO 数据集上取得了显着的改进。</details>
**PDF:** <http://arxiv.org/pdf/2403.01214v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters**<br />
**Title_cn:** SAR-AE-SFP：具有目标散射特征参数的真实物理域中的 SAR 图像对抗示例<br />
**Authors:** Jiahao Cui, Jiale Duan, Binyan Luo, Hang Cao, Wang Guo, Haifeng Li<br />
**Abstract:** <details><summary>原文: </summary>Deep neural network-based Synthetic Aperture Radar (SAR) target recognition models are susceptible to adversarial examples. Current adversarial example generation methods for SAR imagery primarily operate in the 2D digital domain, known as image adversarial examples. Recent work, while considering SAR imaging scatter mechanisms, fails to account for the actual imaging process, rendering attacks in the three-dimensional physical domain infeasible, termed pseudo physics adversarial examples. To address these challenges, this paper proposes SAR-AE-SFP-Attack, a method to generate real physics adversarial examples by altering the scattering feature parameters of target objects. Specifically, we iteratively optimize the coherent energy accumulation of the target echo by perturbing the reflection coefficient and scattering coefficient in the scattering feature parameters of the three-dimensional target object, and obtain the adversarial example after echo signal processing and imaging processing in the RaySAR simulator. Experimental results show that compared to digital adversarial attack methods, SAR-AE-SFP Attack significantly improves attack efficiency on CNN-based models (over 30\%) and Transformer-based models (over 13\%), demonstrating significant transferability of attack effects across different models and perspectives.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度神经网络的合成孔径雷达 (SAR) 目标识别模型容易受到对抗性示例的影响。当前 SAR 图像的对抗性示例生成方法主要在 2D 数字域中运行，称为图像对抗性示例。最近的工作虽然考虑了 SAR 成像散射机制，但未能考虑实际的成像过程，导致三维物理域中的攻击不可行，称为伪物理对抗示例。为了应对这些挑战，本文提出了 SAR-AE-SFP-Attack，一种通过改变目标物体的散射特征参数来生成真实物理对抗示例的方法。具体来说，我们通过扰动三维目标物体散射特征参数中的反射系数和散射系数来迭代优化目标回波的相干能量积累，并在RaySAR模拟器中经过回波信号处理和成像处理后得到对抗样本。实验结果表明，与数字对抗攻击方法相比，SAR-AE-SFP Attack显着提高了基于CNN的模型（超过30%）和基于Transformer的模型（超过13%）的攻击效率，表现出攻击效果的显着可转移性跨越不同的模型和视角。</details>
**PDF:** <http://arxiv.org/pdf/2403.01210v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning**<br />
**Title_cn:** 通过 LLM 支持的即时调整进行无数据多标签图像识别<br />
**Authors:** Shuo Yang, Zirui Shang, Yongqi Wang, Derong Deng, Hongwei Chen, Qiyuan Cheng, Xinxiao Wu<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification. Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens are shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than the state-of-the-art methods, especially outperforming the zero-shot multi-label recognition methods by 4.7% in mAP on MS-COCO.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种无需任何训练数据的多标签图像识别新框架，称为无数据框架，该框架使用预训练大语言模型（LLM）的知识来学习提示以适应预训练视觉语言模型（VLM），例如CLIP 到多标签分类。通过精心设计的问题向LLM提出问题，我们获得了有关物体特征和上下文的全面知识，这为学习提示提供了有价值的文本描述。然后，我们通过考虑多标签依赖性提出了一种分层提示学习方法，其中当相应的对象表现出相似的属性或更可能同时出现时，共享特定于类别的提示标记的子集。受益于 CLIP 的视觉和语言语义之间的显着一致性，从文本描述中学习到的分层提示可用于在推理过程中对图像进行分类。我们的框架提出了一种新方法来探索多个预训练模型之间的协同作用以进行新类别识别。对三个公共数据集（MS-COCO、VOC2007 和 NUS-WIDE）的大量实验表明，我们的方法比最先进的方法取得了更好的结果，特别是比零样本多标签识别方法高出 4.7%在 MS-COCO 上的 mAP 中。</details>
**PDF:** <http://arxiv.org/pdf/2403.01209v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery**<br />
**Title_cn:** 利用自我监督学习进行儿童性虐待图像的场景识别<br />
**Authors:** Pedro H. V. Valois, João Macedo, Leo S. F. Ribeiro, Jefersson A. dos Santos, Sandra Avila<br />
**Abstract:** <details><summary>原文: </summary>Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing & Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive material. The scarcity and limitations of working with child sexual abuse images lead to self-supervised learning, a machine-learning methodology that leverages unlabeled data to produce powerful representations that can be more easily transferred to target tasks. This work shows that self-supervised deep learning models pre-trained on scene-centric data can reach 71.6% balanced accuracy on our indoor scene classification task and, on average, 2.2 percentage points better performance than a fully supervised version. We cooperate with Brazilian Federal Police experts to evaluate our indoor classification model on actual child abuse material. The results demonstrate a notable discrepancy between the features observed in widely used scene datasets and those depicted on sensitive materials.</details>
**Abstract_cn:** <details><summary>译文: </summary>21世纪的犯罪分为虚拟世界和现实世界。然而，前者已成为对后者人民福祉和安全的全球性威胁。必须通过全球统一合作来应对它带来的挑战，我们必须比以往任何时候都更加依赖自动化但值得信赖的工具来打击日益增长的网络犯罪。每年有超过 1000 万份儿童性虐待报告提交给美国国家失踪与受虐儿童中心，其中超过 80% 来自网络来源。因此，调查中心和信息交换所无法手动处理和正确调查所有图像。有鉴于此，能够安全有效地处理这些数据的可靠自动化工具至关重要。从这个意义上说，场景识别任务寻找环境中的上下文线索，能够对儿童性虐待数据进行分组和分类，而无需接受敏感材料的培训。处理儿童性虐待图像的稀缺性和局限性导致了自我监督学习，这是一种机器学习方法，利用未标记的数据来产生强大的表示，可以更容易地转移到目标任务。这项工作表明，在以场景为中心的数据上预训练的自监督深度学习模型在我们的室内场景分类任务上可以达到 71.6% 的平衡精度，平均比完全监督版本的性能提高 2.2 个百分点。我们与巴西联邦警察专家合作，评估我们针对实际虐待儿童材料的室内分类模型。结果表明，在广泛使用的场景数据集中观察到的特征与敏感材料上描绘的特征之间存在显着差异。</details>
**PDF:** <http://arxiv.org/pdf/2403.01183v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations**<br />
**Title_cn:** 使用学习表示对自动驾驶系统中的 2D 对象检测进行运行时自省<br />
**Authors:** Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman<br />
**Abstract:** <details><summary>原文: </summary>Reliable detection of various objects and road users in the surrounding environment is crucial for the safe operation of automated driving systems (ADS). Despite recent progresses in developing highly accurate object detectors based on Deep Neural Networks (DNNs), they still remain prone to detection errors, which can lead to fatal consequences in safety-critical applications such as ADS. An effective remedy to this problem is to equip the system with run-time monitoring, named as introspection in the context of autonomous systems. Motivated by this, we introduce a novel introspection solution, which operates at the frame level for DNN-based 2D object detection and leverages neural network activation patterns. The proposed approach pre-processes the neural activation patterns of the object detector's backbone using several different modes. To provide extensive comparative analysis and fair comparison, we also adapt and implement several state-of-the-art (SOTA) introspection mechanisms for error detection in 2D object detection, using one-stage and two-stage object detectors evaluated on KITTI and BDD datasets. We compare the performance of the proposed solution in terms of error detection, adaptability to dataset shift, and, computational and memory resource requirements. Our performance evaluation shows that the proposed introspection solution outperforms SOTA methods, achieving an absolute reduction in the missed error ratio of 9% to 17% in the BDD dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>可靠地检测周围环境中的各种物体和道路使用者对于自动驾驶系统（ADS）的安全运行至关重要。尽管最近在开发基于深度神经网络 (DNN) 的高精度物体检测器方面取得了进展，但它们仍然容易出现检测错误，这可能会在 ADS 等安全关键型应用中导致致命后果。解决这个问题的有效方法是为系统配备运行时监控，在自治系统的背景下称为内省。受此启发，我们引入了一种新颖的自省解决方案，该解决方案在帧级别运行，用于基于 DNN 的 2D 对象检测，并利用神经网络激活模式。所提出的方法使用几种不同的模式预处理对象检测器主干的神经激活模式。为了提供广泛的比较分析和公平比较，我们还使用在 KITTI 和 BDD 上评估的一级和两级目标检测器，采用并实现了几种最先进的 (SOTA) 内省机制，用于 2D 目标检测中的错误检测数据集。我们比较了所提出的解决方案在错误检测、数据集转换的适应性以及计算和内存资源要求方面的性能。我们的性能评估表明，所提出的内省解决方案优于 SOTA 方法，在 BDD 数据集中实现了 9% 至 17% 的遗漏错误率的绝对减少。</details>
**PDF:** <http://arxiv.org/pdf/2403.01172v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection**<br />
**Title_cn:** 从事件提示中了解可疑异常情况以进行视频异常检测<br />
**Authors:** Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian<br />
**Abstract:** <details><summary>原文: </summary>Most models for weakly supervised video anomaly detection (WS-VAD) rely on multiple instance learning, aiming to distinguish normal and abnormal snippets without specifying the type of anomaly. The ambiguous nature of anomaly definitions across contexts introduces bias in detecting abnormal and normal snippets within the abnormal bag. Taking the first step to show the model why it is anomalous, a novel framework is proposed to guide the learning of suspected anomalies from event prompts. Given a textual prompt dictionary of potential anomaly events and the captions generated from anomaly videos, the semantic anomaly similarity between them could be calculated to identify the suspected anomalous events for each video snippet. It enables a new multi-prompt learning process to constrain the visual-semantic features across all videos, as well as provides a new way to label pseudo anomalies for self-training. To demonstrate effectiveness, comprehensive experiments and detailed ablation studies are conducted on four datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most state-of-the-art methods in terms of AP or AUC (82.6\%, 87.7\%, 93.1\%, and 97.4\%). Furthermore, it shows promising performance in open-set and cross-dataset cases.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数弱监督视频异常检测（WS-VAD）模型都依赖于多实例学习，旨在区分正常和异常片段，而不指定异常类型。跨上下文的异常定义的模糊性会在检测异常包内的异常和正常片段时引入偏差。第一步向模型展示其异常的原因，提出了一个新颖的框架来指导从事件提示中学习可疑异常。给定潜在异常事件的文本提示字典和异常视频生成的字幕，可以计算它们之间的语义异常相似度，以识别每个视频片段的可疑异常事件。它支持新的多提示学习过程来约束所有视频的视觉语义特征，并提供一种标记伪异常以进行自我训练的新方法。为了证明有效性，在 XD-Violence、UCF-Crime、TAD 和 ShanghaiTech 四个数据集上进行了全面的实验和详细的消融研究。我们提出的模型在 AP 或 AUC 方面优于大多数最先进的方法（82.6%、87.7%、93.1% 和 97.4%）。此外，它在开放集和跨数据集情况下表现出了良好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01169v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation**<br />
**Title_cn:** 辅助任务增强弱监督语义分割的双亲和力学习<br />
**Authors:** Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous Sohel, Dan Xu<br />
**Abstract:** <details><summary>原文: </summary>Most existing weakly supervised semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.</details>
**Abstract_cn:** <details><summary>译文: </summary>大多数现有的弱监督语义分割（WSSS）方法依赖类激活映射（CAM）来使用图像级标签提取粗略的类特定定位图。先前的工作通常使用离线启发式阈值处理，将 CAM 地图与由通用预训练显着性模型生成的现成显着性地图相结合，以产生更准确的伪分割标签。我们提出了 AuxSegNet+，一个弱监督的辅助学习框架，用于探索这些显着性图中的丰富信息以及显着性检测和语义分割之间的显着任务间相关性。在提出的 AuxSegNet+ 中，显着性检测和多标签图像分类被用作辅助任务，以改进仅使用图像级真实标签的语义分割的主要任务。我们还提出了一种跨任务亲和力学习机制，用于从显着性和分割特征图中学习像素级亲和力。特别是，我们提出了一个跨任务双亲和力学习模块来学习成对和一元亲和力，它们用于通过聚合查询相关和查询无关的全局上下文来增强特定于任务的特征和预测以进行显着性检测和语义分割。学习到的跨任务成对亲和力还可以用于细化和传播 CAM 映射，以为这两个任务提供更好的伪标签。通过跨任务亲和性学习和伪标签更新来实现分割性能的迭代改进。大量实验证明了所提出的方法的有效性，并在具有挑战性的 PASCAL VOC 和 MS COCO 基准上获得了最先进的 WSSS 结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.01156v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **ELA: Efficient Local Attention for Deep Convolutional Neural Networks**<br />
**Title_cn:** ELA：深度卷积神经网络的高效局部注意力<br />
**Authors:** Wei Xu, Yi Wan<br />
**Abstract:** <details><summary>原文: </summary>The attention mechanism has gained significant recognition in the field of computer vision due to its ability to effectively enhance the performance of deep neural networks. However, existing methods often struggle to effectively utilize spatial information or, if they do, they come at the cost of reducing channel dimensions or increasing the complexity of neural networks. In order to address these limitations, this paper introduces an Efficient Local Attention (ELA) method that achieves substantial performance improvements with a simple structure. By analyzing the limitations of the Coordinate Attention method, we identify the lack of generalization ability in Batch Normalization, the adverse effects of dimension reduction on channel attention, and the complexity of attention generation process. To overcome these challenges, we propose the incorporation of 1D convolution and Group Normalization feature enhancement techniques. This approach enables accurate localization of regions of interest by efficiently encoding two 1D positional feature maps without the need for dimension reduction, while allowing for a lightweight implementation. We carefully design three hyperparameters in ELA, resulting in four different versions: ELA-T, ELA-B, ELA-S, and ELA-L, to cater to the specific requirements of different visual tasks such as image classification, object detection and sementic segmentation. ELA can be seamlessly integrated into deep CNN networks such as ResNet, MobileNet, and DeepLab. Extensive evaluations on the ImageNet, MSCOCO, and Pascal VOC datasets demonstrate the superiority of the proposed ELA module over current state-of-the-art methods in all three aforementioned visual tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>注意力机制由于能够有效增强深度神经网络的性能而在计算机视觉领域获得了显着的认可。然而，现有的方法通常难以有效地利用空间信息，或者即使有效利用空间信息，也会以减少通道维度或增加神经网络的复杂性为代价。为了解决这些限制，本文引入了一种高效局部注意力（ELA）方法，该方法以简单的结构实现了显着的性能提升。通过分析坐标注意力方法的局限性，我们发现了Batch Normalization缺乏泛化能力、降维对通道注意力的不利影响以及注意力生成过程的复杂性。为了克服这些挑战，我们建议结合一维卷积和组归一化特征增强技术。该方法通过有效地编码两个一维位置特征图来实现感兴趣区域的精确定位，无需降维，同时允许轻量级实现。我们在ELA中精心设计了三个超参数，产生了四个不同的版本：ELA-T、ELA-B、ELA-S和ELA-L，以满足图像分类、目标检测和语义等不同视觉任务的具体要求。分割。 ELA可以无缝集成到ResNet、MobileNet和DeepLab等深度CNN网络中。对 ImageNet、MSCOCO 和 Pascal VOC 数据集的广泛评估证明了所提出的 ELA 模块在上述所有三个视觉任务中相对于当前最先进的方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01123v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images**<br />
**Title_cn:** 超越夜间能见度：红外和可见光图像的自适应多尺度融合<br />
**Authors:** Shufan Pei, Junhong Lin, Wenxi Liu, Tiesong Zhao, Chia-Wen Lin<br />
**Abstract:** <details><summary>原文: </summary>In addition to low light, night images suffer degradation from light effects (e.g., glare, floodlight, etc). However, existing nighttime visibility enhancement methods generally focus on low-light regions, which neglects, or even amplifies the light effects. To address this issue, we propose an Adaptive Multi-scale Fusion network (AMFusion) with infrared and visible images, which designs fusion rules according to different illumination regions. First, we separately fuse spatial and semantic features from infrared and visible images, where the former are used for the adjustment of light distribution and the latter are used for the improvement of detection accuracy. Thereby, we obtain an image free of low light and light effects, which improves the performance of nighttime object detection. Second, we utilize detection features extracted by a pre-trained backbone that guide the fusion of semantic features. Hereby, we design a Detection-guided Semantic Fusion Module (DSFM) to bridge the domain gap between detection and semantic features. Third, we propose a new illumination loss to constrain fusion image with normal light intensity. Experimental results demonstrate the superiority of AMFusion with better visual quality and detection accuracy. The source code will be released after the peer review process.</details>
**Abstract_cn:** <details><summary>译文: </summary>除了弱光之外，夜间图像还会因光效应（例如眩光、泛光灯等）而降低质量。然而，现有的夜间能见度增强方法普遍关注弱光区域，忽略甚至放大了光效应。为了解决这个问题，我们提出了一种具有红外和可见光图像的自适应多尺度融合网络（AMFusion），它根据不同的照明区域设计融合规则。首先，我们分别融合红外和可见光图像的空间特征和语义特征，前者用于调整光分布，后者用于提高检测精度。由此，我们获得了没有弱光和光效应的图像，从而提高了夜间物体检测的性能。其次，我们利用预先训练的主干提取的检测特征来指导语义特征的融合。因此，我们设计了一个检测引导的语义融合模块（DSFM）来弥合检测和语义特征之间的领域差距。第三，我们提出了一种新的照明损失来约束具有正常光强度的融合图像。实验结果证明了 AMFusion 的优越性，具有更好的视觉质量和检测精度。源代码将在同行评审过程后发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.01083v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing**<br />
**Title_cn:** 深度信息辅助单幅图像去雾协作互促网络<br />
**Authors:** Yafei Zhang, Shen Zhou, Huafeng Li<br />
**Abstract:** <details><summary>原文: </summary>Recovering a clear image from a single hazy image is an open inverse problem. Although significant research progress has been made, most existing methods ignore the effect that downstream tasks play in promoting upstream dehazing. From the perspective of the haze generation mechanism, there is a potential relationship between the depth information of the scene and the hazy image. Based on this, we propose a dual-task collaborative mutual promotion framework to achieve the dehazing of a single image. This framework integrates depth estimation and dehazing by a dual-task interaction mechanism and achieves mutual enhancement of their performance. To realize the joint optimization of the two tasks, an alternative implementation mechanism with the difference perception is developed. On the one hand, the difference perception between the depth maps of the dehazing result and the ideal image is proposed to promote the dehazing network to pay attention to the non-ideal areas of the dehazing. On the other hand, by improving the depth estimation performance in the difficult-to-recover areas of the hazy image, the dehazing network can explicitly use the depth information of the hazy image to assist the clear image recovery. To promote the depth estimation, we propose to use the difference between the dehazed image and the ground truth to guide the depth estimation network to focus on the dehazed unideal areas. It allows dehazing and depth estimation to leverage their strengths in a mutually reinforcing manner. Experimental results show that the proposed method can achieve better performance than that of the state-of-the-art approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>从单个模糊图像中恢复清晰图像是一个开放的逆问题。尽管已经取得了重大的研究进展，但大多数现有方法忽略了下游任务在促进上游去雾中所发挥的作用。从雾霾生成机制的角度来看，场景的深度信息与雾霾图像之间存在潜在的关系。基于此，我们提出了一种双任务协同相互促进框架来实现单个图像的去雾。该框架通过双任务交互机制将深度估计和去雾集成在一起，实现了两者性能的相互增强。为了实现这两个任务的联合优化，开发了一种具有差异感知的替代执行机制。一方面，提出了去雾结果的深度图与理想图像之间的差异感知，以促进去雾网络关注去雾的非理想区域。另一方面，通过提高有雾图像难以恢复区域的深度估计性能，去雾网络可以明确地使用有雾图像的深度信息来辅助清晰图像恢复。为了促进深度估计，我们建议利用去雾图像与地面实况之间的差异来引导深度估计网络聚焦于去雾的不理想区域。它允许去雾和深度估计以相辅相成的方式发挥各自的优势。实验结果表明，所提出的方法可以实现比最先进的方法更好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01105v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation**<br />
**Title_cn:** 基于双图注意力的解缠多实例学习用于脑年龄估计<br />
**Authors:** Fanzhe Yan, Gang Yang, Yu Li, Aiping Liu, Xun Chen<br />
**Abstract:** <details><summary>原文: </summary>Deep learning techniques have demonstrated great potential for accurately estimating brain age by analyzing Magnetic Resonance Imaging (MRI) data from healthy individuals. However, current methods for brain age estimation often directly utilize whole input images, overlooking two important considerations: 1) the heterogeneous nature of brain aging, where different brain regions may degenerate at different rates, and 2) the existence of age-independent redundancies in brain structure. To overcome these limitations, we propose a Dual Graph Attention based Disentanglement Multi-instance Learning (DGA-DMIL) framework for improving brain age estimation. Specifically, the 3D MRI data, treated as a bag of instances, is fed into a 2D convolutional neural network backbone, to capture the unique aging patterns in MRI. A dual graph attention aggregator is then proposed to learn the backbone features by exploiting the intra- and inter-instance relationships. Furthermore, a disentanglement branch is introduced to separate age-related features from age-independent structural representations to ameliorate the interference of redundant information on age prediction. To verify the effectiveness of the proposed framework, we evaluate it on two datasets, UK Biobank and ADNI, containing a total of 35,388 healthy individuals. Our proposed model demonstrates exceptional accuracy in estimating brain age, achieving a remarkable mean absolute error of 2.12 years in the UK Biobank. The results establish our approach as state-of-the-art compared to other competing brain age estimation models. In addition, the instance contribution scores identify the varied importance of brain areas for aging prediction, which provides deeper insights into the understanding of brain aging.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度学习技术已显示出通过分析健康个体的磁共振成像 (MRI) 数据来准确估计大脑年龄的巨大潜力。然而，当前的大脑年龄估计方法通常直接利用整个输入图像，忽略了两个重要的考虑因素：1）大脑衰老的异质性，其中不同的大脑区域可能以不同的速度退化，2）大脑老化中存在与年龄无关的冗余。大脑结构。为了克服这些限制，我们提出了一种基于双图注意力的解缠多实例学习（DGA-DMIL）框架来改进大脑年龄估计。具体来说，3D MRI 数据被视为实例包，被输入到 2D 卷积神经网络主干中，以捕获 MRI 中独特的老化模式。然后提出了双图注意力聚合器，通过利用实例内和实例间关系来学习主干特征。此外，引入解缠结分支将与年龄相关的特征与与年龄无关的结构表示分离，以改善冗余信息对年龄预测的干扰。为了验证所提出框架的有效性，我们在 UK Biobank 和 ADNI 两个数据集上对其进行了评估，总共包含 35,388 名健康个体。我们提出的模型在估计大脑年龄方面表现出极高的准确性，在英国生物银行中实现了 2.12 年的显着平均绝对误差。结果表明，与其他竞争性大脑年龄估计模型相比，我们的方法是最先进的。此外，实例贡献分数确定了大脑区域对于衰老预测的不同重要性，这为理解大脑衰老提供了更深入的见解。</details>
**PDF:** <http://arxiv.org/pdf/2403.01246v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling**<br />
**Title_cn:** 看到看不见的东西：通过几何约束概率建模发现新的生物医学概念<br />
**Authors:** Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai<br />
**Abstract:** <details><summary>原文: </summary>Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent bias. Then, we incorporate a suite of critical geometric properties to impose proper constraints on the layout of constructed embedding space, which in turn minimizes the uncontrollable risk for unknown class learning and structuring. Furthermore, a spectral graph-theoretic method is devised to estimate the number of potential novel classes. It inherits two intriguing merits compared to existent approaches, namely high computational efficiency and flexibility for taxonomy-adaptive estimation. Extensive experiments across various biomedical scenarios substantiate the effectiveness and general applicability of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习凭借其数据驱动的性质，为改变科学发现的基本实践带来了巨大的希望。随着研究数据收集的不断增加，从观察数据中自主探索模式和见解以发现新的表型和概念类别将很有吸引力。然而，在生物医学领域，累积数据本身存在一些挑战，阻碍了新类别发现的进展。非独立同分布数据分布伴随着不同类组之间的严重不平衡，本质上导致了语义表示的模糊性和偏差。在这项工作中，我们提出了一种几何约束的概率建模处理来解决已识别的问题。首先，我们建议将实例嵌入的近似后验参数化为边际 von MisesFisher 分布，以考虑分布潜在偏差的干扰。然后，我们结合了一套关键的几何属性，对构建的嵌入空间的布局施加适当的约束，从而最大限度地减少未知类学习和结构化的不可控风险。此外，设计了谱图理论方法来估计潜在新类别的数量。与现有方法相比，它继承了两个有趣的优点，即高计算效率和分类自适应估计的灵活性。跨各种生物医学场景的广泛实验证实了我们方法的有效性和普遍适用性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01053v2><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation**<br />
**Title_cn:** ShapeBoost：通过基于部位的参数化和服装保留增强来促进人体形状估计<br />
**Authors:** Siyuan Bian, Jiefeng Li, Jiasheng Tang, Cewu Lu<br />
**Abstract:** <details><summary>原文: </summary>Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations.</details>
**Abstract_cn:** <details><summary>译文: </summary>从单目 RGB 图像中准确恢复人体形状是一项具有挑战性的任务，因为人类具有不同的形状和大小并且穿着不同的衣服。在本文中，我们提出了 ShapeBoost，这是一种新的人体形状恢复框架，即使对于罕见的体形也能实现像素级对齐，并且对于穿着不同类型衣服的人也能实现高精度。与之前依赖使用基于 PCA 的形状系数的方法不同，我们采用了一种新的人体形状参数化，将人体形状分解为骨骼长度和每个部分切片的平均宽度。这种基于零件的参数化技术使用半解析形状重建算法实现了灵活性和有效性之间的平衡。基于这种新的参数化，提出了一种服装保护数据增强模块，以生成具有不同身体形状和准确注释的逼真图像。实验结果表明，我们的方法在不同的体型情况以及不同的服装情况下优于其他最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01345v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Mitigating the Bias in the Model for Continual Test-Time Adaptation**<br />
**Title_cn:** 减轻持续测试时间适应模型中的偏差<br />
**Authors:** Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak<br />
**Abstract:** <details><summary>原文: </summary>Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype. With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead.</details>
**Abstract_cn:** <details><summary>译文: </summary>持续测试时间适应 (CTA) 是一项具有挑战性的任务，旨在使源预训练模型适应不断变化的目标领域。在 CTA 设置中，模型不知道目标域何时发生变化，因此在测试期间面临流输入分布的巨大变化。关键的挑战是以在线方式不断调整模型以适应不断变化的目标领域。我们发现模型显示出高度偏差的预测，因为它不断适应目标数据的链接分布。它比其他类别更频繁地预测某些类别，从而做出不准确的过度自信的预测。本文缓解了这个问题，以提高 CTA 场景中的性能。为了缓解偏差问题，我们使用可靠的目标样本制作了类别指数移动平均目标原型，并利用它们对目标特征进行类别聚类。此外，我们的目标是通过将目标特征锚定到其相应的源原型来将目标分布与源分布对齐。通过大量的实验，我们提出的方法在应用于现有 CTA 方法之上时实现了显着的性能增益，而无需大量的适应时间开销。</details>
**PDF:** <http://arxiv.org/pdf/2403.01344v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Single-image camera calibration with model-free distortion correction**<br />
**Title_cn:** 具有无模型畸变校正的单图像相机校准<br />
**Authors:** Katia Genovese<br />
**Abstract:** <details><summary>原文: </summary>Camera calibration is a process of paramount importance in computer vision applications that require accurate quantitative measurements. The popular method developed by Zhang relies on the use of a large number of images of a planar grid of fiducial points captured in multiple poses. Although flexible and easy to implement, Zhang's method has some limitations. The simultaneous optimization of the entire parameter set, including the coefficients of a predefined distortion model, may result in poor distortion correction at the image boundaries or in miscalculation of the intrinsic parameters, even with a reasonably small reprojection error. Indeed, applications involving image stitching (e.g. multi-camera systems) require accurate mapping of distortion up to the outermost regions of the image. Moreover, intrinsic parameters affect the accuracy of camera pose estimation, which is fundamental for applications such as vision servoing in robot navigation and automated assembly. This paper proposes a method for estimating the complete set of calibration parameters from a single image of a planar speckle pattern covering the entire sensor. The correspondence between image points and physical points on the calibration target is obtained using Digital Image Correlation. The effective focal length and the extrinsic parameters are calculated separately after a prior evaluation of the principal point. At the end of the procedure, a dense and uniform model-free distortion map is obtained over the entire image. Synthetic data with different noise levels were used to test the feasibility of the proposed method and to compare its metrological performance with Zhang's method. Real-world tests demonstrate the potential of the developed method to reveal aspects of the image formation that are hidden by averaging over multiple images.</details>
**Abstract_cn:** <details><summary>译文: </summary>相机校准是需要精确定量测量的计算机视觉应用中至关重要的过程。张开发的流行方法依赖于使用以多种姿势捕获的基准点平面网格的大量图像。虽然灵活且易于实施，但张的方法有一些局限性。即使重投影误差相当小，同时优化整个参数集（包括预定义失真模型的系数）也可能导致图像边界处的失真校正效果不佳或内在参数计算错误。事实上，涉及图像拼接的应用（例如多摄像头系统）需要将畸变精确映射到图像的最外层区域。此外，内在参数会影响相机位姿估计的准确性，这对于机器人导航和自动化装配中的视觉伺服等应用至关重要。本文提出了一种从覆盖整个传感器的平面散斑图案的单个图像估计整套校准参数的方法。使用数字图像相关获得图像点和校准目标上的物理点之间的对应关系。在预先评估主点后，分别计算有效焦距和外在参数。在该过程结束时，在整个图像上获得密集且均匀的无模型畸变图。使用不同噪声水平的合成数据来测试该方法的可行性，并将其计量性能与张的方法进行比较。现实世界的测试证明了所开发的方法有潜力揭示通过对多个图像进行平均而隐藏的图像形成的各个方面。</details>
**PDF:** <http://arxiv.org/pdf/2403.01263v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Consistent and Asymptotically Statistically-Efficient Solution to Camera Motion Estimation**<br />
**Title_cn:** 相机运动估计的一致且渐近统计有效的解决方案<br />
**Authors:** Guangyang Zeng, Qingcheng Zeng, Xinghan Li, Biqiang Mu, Jiming Chen, Ling Shi, Junfeng Wu<br />
**Abstract:** <details><summary>原文: </summary>Given 2D point correspondences between an image pair, inferring the camera motion is a fundamental issue in the computer vision community. The existing works generally set out from the epipolar constraint and estimate the essential matrix, which is not optimal in the maximum likelihood (ML) sense. In this paper, we dive into the original measurement model with respect to the rotation matrix and normalized translation vector and formulate the ML problem. We then propose a two-step algorithm to solve it: In the first step, we estimate the variance of measurement noises and devise a consistent estimator based on bias elimination; In the second step, we execute a one-step Gauss-Newton iteration on manifold to refine the consistent estimate. We prove that the proposed estimate owns the same asymptotic statistical properties as the ML estimate: The first is consistency, i.e., the estimate converges to the ground truth as the point number increases; The second is asymptotic efficiency, i.e., the mean squared error of the estimate converges to the theoretical lower bound -- Cramer-Rao bound. In addition, we show that our algorithm has linear time complexity. These appealing characteristics endow our estimator with a great advantage in the case of dense point correspondences. Experiments on both synthetic data and real images demonstrate that when the point number reaches the order of hundreds, our estimator outperforms the state-of-the-art ones in terms of estimation accuracy and CPU time.</details>
**Abstract_cn:** <details><summary>译文: </summary>给定图像对之间的 2D 点对应关系，推断相机运动是计算机视觉社区中的一个基本问题。现有的工作一般从极线约束出发，估计本质矩阵，这在最大似然（ML）意义上并不是最优的。在本文中，我们深入研究了关于旋转矩阵和归一化平移向量的原始测量模型，并制定了机器学习问题。然后，我们提出了一个两步算法来解决这个问题：第一步，我们估计测量噪声的方差，并设计一个基于偏差消除的一致估计器；在第二步中，我们在流形上执行一步高斯-牛顿迭代以细化一致的估计。我们证明了所提出的估计具有与 ML 估计相同的渐近统计特性：首先是一致性，即随着点数的增加，估计收敛到真实值；第二个是渐近效率，即估计的均方误差收敛到理论下界——Cramer-Rao 界。此外，我们表明我们的算法具有线性时间复杂度。这些吸引人的特征使我们的估计器在密集点对应的情况下具有很大的优势。对合成数据和真实图像的实验表明，当点数达到数百个数量级时，我们的估计器在估计精度和 CPU 时间方面优于最先进的估计器。</details>
**PDF:** <http://arxiv.org/pdf/2403.01174v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Edge-guided Low-light Image Enhancement with Inertial Bregman Alternating Linearized Minimization**<br />
**Title_cn:** 采用惯性 Bregman 交替线性化最小化的边缘引导低光图像增强<br />
**Authors:** Chaoyan Huang, Zhongming Wu, Tieyong Zeng<br />
**Abstract:** <details><summary>原文: </summary>Prior-based methods for low-light image enhancement often face challenges in extracting available prior information from dim images. To overcome this limitation, we introduce a simple yet effective Retinex model with the proposed edge extraction prior. More specifically, we design an edge extraction network to capture the fine edge features from the low-light image directly. Building upon the Retinex theory, we decompose the low-light image into its illumination and reflectance components and introduce an edge-guided Retinex model for enhancing low-light images. To solve the proposed model, we propose a novel inertial Bregman alternating linearized minimization algorithm. This algorithm addresses the optimization problem associated with the edge-guided Retinex model, enabling effective enhancement of low-light images. Through rigorous theoretical analysis, we establish the convergence properties of the algorithm. Besides, we prove that the proposed algorithm converges to a stationary point of the problem through nonconvex optimization theory. Furthermore, extensive experiments are conducted on multiple real-world low-light image datasets to demonstrate the efficiency and superiority of the proposed scheme.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于先验的低光图像增强方法通常面临着从暗淡图像中提取可用先验信息的挑战。为了克服这个限制，我们引入了一个简单而有效的 Retinex 模型，该模型具有所提出的边缘提取先验。更具体地说，我们设计了一个边缘提取网络来直接从低光图像中捕获精细边缘特征。基于 Retinex 理论，我们将低光图像分解为其照明和反射分量，并引入边缘引导的 Retinex 模型来增强低光图像。为了求解所提出的模型，我们提出了一种新颖的惯性 Bregman 交替线性化最小化算法。该算法解决了与边缘引导 Retinex 模型相关的优化问题，能够有效增强低光图像。通过严格的理论分析，我们建立了算法的收敛特性。此外，我们通过非凸优化理论证明了所提出的算法收敛到问题的驻点。此外，在多个真实世界的低光图像数据集上进行了广泛的实验，以证明该方案的效率和优越性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01142v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Towards Accurate Lip-to-Speech Synthesis in-the-Wild**<br />
**Title_cn:** 实现野外准确的唇语合成<br />
**Authors:** Sindhu Hegde, Rudrabha Mukhopadhyay, C. V. Jawahar, Vinay Namboodiri<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we introduce a novel approach to address the task of synthesizing speech from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating speech from lip videos faces the challenge of not being able to learn a robust language model from speech alone, resulting in unsatisfactory outcomes. To overcome this issue, we propose incorporating noisy text supervision using a state-of-the-art lip-to-text network that instills language information into our model. The noisy text is generated using a pre-trained lip-to-text model, enabling our approach to work without text annotations during inference. We design a visual text-to-speech network that utilizes the visual stream to generate accurate speech, which is in-sync with the silent input video. We perform extensive experiments and ablation studies, demonstrating our approach's superiority over the current state-of-the-art methods on various benchmark datasets. Further, we demonstrate an essential practical application of our method in assistive technology by generating speech for an ALS patient who has lost the voice but can make mouth movements. Our demo video, code, and additional details can be found at \url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们介绍了一种新颖的方法来解决仅基于嘴唇运动从任何野外说话者的无声视频中合成语音的任务。直接从唇形视频生成语音的传统方法面临着无法仅从语音中学习鲁棒语言模型的挑战，导致结果不令人满意。为了克服这个问题，我们建议使用最先进的唇语到文本网络将噪声文本监督融入到我们的模型中。嘈杂的文本是使用预先训练的唇语到文本模型生成的，使我们的方法在推理过程中无需文本注释即可工作。我们设计了一个视觉文本到语音网络，利用视觉流生成准确的语音，该语音与无声输入视频同步。我们进行了广泛的实验和消融研究，证明了我们的方法在各种基准数据集上优于当前最先进的方法。此外，我们通过为失去声音但可以进行嘴巴运动的 ALS 患者生成语音来展示我们的方法在辅助技术中的重要实际应用。我们的演示视频、代码和其他详细信息可以在 \url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw} 中找到。</details>
**PDF:** <http://arxiv.org/pdf/2403.01087v1><br />
**Code:** null<br />

