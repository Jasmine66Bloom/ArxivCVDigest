## [UPDATED!] **2024-03-03** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Critical windows: non-asymptotic theory for feature emergence in diffusion models**<br />
**Title_cn:** 临界窗口：扩散模型中特征出现的非渐近理论<br />
**Authors:** Marvin Li, Sitan Chen<br />
**Abstract:** <details><summary>原文: </summary>We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively "decide" output features over a discrete sequence of times. We validate our bounds with synthetic experiments. Additionally, preliminary experiments on Stable Diffusion suggest critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们发展理论来理解图像生成扩散模型的一个有趣的特性，我们称之为临界窗口。根据经验，已经观察到采样中存在很窄的时间间隔，在此期间最终图像的特定特征出现，例如图像类别或背景颜色（Ho et al., 2020b；Georgiev et al., 2023；Raya & Ambrogioni, 2023；Sclocchi et al., 2024；Biroli et al., 2024）。虽然这对于可解释性是有利的，因为它意味着可以将生成的属性定位到轨迹的一小部分，但这似乎与扩散的连续性质不一致。我们提出了一个研究这些窗口的正式框架，并表明，对于来自强对数凹密度混合的数据，这些窗口可以根据组间和组内分离的某些度量来证明有界。我们还为具体示例（例如条件良好的高斯混合）实例化了这些边界。最后，我们使用边界对扩散模型进行严格解释，将其作为分层采样器，在离散的时间序列上逐步“决定”输出特征。我们通过综合实验验证我们的界限。此外，稳定扩散的初步实验表明，关键窗口可以作为诊断现实世界扩散模型中的公平性和隐私侵犯的有用工具。</details>
**PDF:** <http://arxiv.org/pdf/2403.01633v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation**<br />
**Title_cn:** SCott：通过随机一致性蒸馏加速扩散模型<br />
**Authors:** Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng-jun Zha, Haonan Lu<br />
**Abstract:** <details><summary>原文: </summary>The iterative sampling procedure employed by diffusion models (DMs) often leads to significant inference latency. To address this, we propose Stochastic Consistency Distillation (SCott) to enable accelerated text-to-image generation, where high-quality generations can be achieved with just 1-2 sampling steps, and further improvements can be obtained by adding additional steps. In contrast to vanilla consistency distillation (CD) which distills the ordinary differential equation solvers-based sampling process of a pretrained teacher model into a student, SCott explores the possibility and validates the efficacy of integrating stochastic differential equation (SDE) solvers into CD to fully unleash the potential of the teacher. SCott is augmented with elaborate strategies to control the noise strength and sampling process of the SDE solver. An adversarial loss is further incorporated to strengthen the sample quality with rare sampling steps. Empirically, on the MSCOCO-2017 5K dataset with a Stable Diffusion-V1.5 teacher, SCott achieves an FID (Frechet Inceptio Distance) of 22.1, surpassing that (23.4) of the 1-step InstaFlow (Liu et al., 2023) and matching that of 4-step UFOGen (Xue et al., 2023b). Moreover, SCott can yield more diverse samples than other consistency models for high-resolution image generation (Luo et al., 2023a), with up to 16% improvement in a qualified metric. The code and checkpoints are coming soon.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型 (DM) 采用的迭代采样过程通常会导致显着的推理延迟。为了解决这个问题，我们提出随机一致性蒸馏（SCott）来加速文本到图像的生成，只需 1-2 个采样步骤即可实现高质量的生成，并且可以通过添加额外的步骤来获得进一步的改进。与普通的一致性蒸馏 (CD) 将预训练教师模型的基于常微分方程求解器的采样过程蒸馏到学生中相比，SCott 探索了将随机微分方程 (SDE) 求解器集成到 CD 中的可能性并验证了其有效性。发挥教师的潜力。 SCott 通过精心设计的策略来控制 SDE 求解器的噪声强度和采样过程。进一步纳入对抗性损失，以通过罕见的采样步骤来增强样本质量。根据经验，在具有 Stable Diffusion-V1.5 教师的 MSCOCO-2017 5K 数据集上，SCott 实现了 22.1 的 FID（Frechet Inceptio Distance），超过了 1-step InstaFlow 的 (23.4)（Liu 等人，2023）并与 4 步 UFOGen 相匹配（Xue 等人，2023b）。此外，与用于高分辨率图像生成的其他一致性模型相比，SCott 可以生成更多样的样本（Luo 等人，2023a），合格指标提高高达 16%。代码和检查点即将推出。</details>
**PDF:** <http://arxiv.org/pdf/2403.01505v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement**<br />
**Title_cn:** 学习基于 Transformer 的物理感知扩散模型，用于水下图像增强<br />
**Authors:** Chen Zhao, Chenyu Dong, Weiling Cai<br />
**Abstract:** <details><summary>原文: </summary>Underwater visuals undergo various complex degradations, inevitably influencing the efficiency of underwater vision tasks. Recently, diffusion models were employed to underwater image enhancement (UIE) tasks, and gained SOTA performance. However, these methods fail to consider the physical properties and underwater imaging mechanisms in the diffusion process, limiting information completion capacity of diffusion models. In this paper, we introduce a novel UIE framework, named PA-Diff, designed to exploiting the knowledge of physics to guide the diffusion process.   PA-Diff consists of Physics Prior Generation (PPG) Branch and Physics-aware Diffusion Transformer (PDT) Branch. Our designed PPG branch is a plug-and-play network to produce the physics prior, which can be integrated into any deep framework. With utilizing the physics prior knowledge to guide the diffusion process, PDT branch can obtain underwater-aware ability and model the complex distribution in real-world underwater scenes. Extensive experiments prove that our method achieves best performance on UIE tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>水下视觉经历各种复杂的退化，不可避免地影响水下视觉任务的效率。最近，扩散模型被应用于水下图像增强（UIE）任务，并获得了 SOTA 性能。然而，这些方法未能考虑扩散过程中的物理特性和水下成像机制，限制了扩散模型的信息补全能力。在本文中，我们介绍了一种新颖的 UIE 框架，名为 PA-Diff，旨在利用物理学知识来指导扩散过程。 PA-Diff 由物理前代 (PPG) 分支和物理感知扩散变压器 (PDT) 分支组成。我们设计的 PPG 分支是一个即插即用的网络，用于产生物理先验，可以集成到任何深层框架中。利用物理先验知识指导扩散过程，PDT分支可以获得水下感知能力并对现实水下场景中的复杂分布进行建模。大量实验证明我们的方法在 UIE 任务上取得了最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01497v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models**<br />
**Title_cn:** 基于再生的文本到图像生成模型生成的假图像的免训练归因<br />
**Authors:** Meiling Li, Zhenxing Qian, Xinpeng Zhang<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像生成模型最近引起了广泛关注，因为它们能够根据提示描述生成图像。虽然这些模型表现出了良好的性能，但人们仍然担心生成的假图像可能被滥用。针对这一点，我们提出了一种简单而有效的免训练方法，将文本到图像模型生成的假图像归因于其源模型。给定要归因的测试图像，我们首先反转图像的文本提示，然后将重构的提示放入不同的候选模型中以重新生成候选假图像。通过计算测试图像与候选图像的相似度并进行排序，可以确定图像的来源。这种归因允许模型所有者对其模型的任何滥用负责。请注意，我们的方法不限制候选文本到图像生成模型的数量。综合实验表明：（1）我们的方法可以有效地将假图像归因于其源模型，实现了与最先进的方法相当的归因性能； （2）我们的方法具有很高的可扩展性，能够很好地适应现实世界的归因场景。 (3)所提出的方法对高斯模糊、JPEG 压缩和调整大小等常见攻击具有令人满意的鲁棒性。我们还分析了影响归因性能的因素，并探讨了所提出的方法作为插件所带来的提升，以提高现有SOTA的性能。我们希望我们的工作能够为解决人工智能生成图像的来源以及防止滥用文本到图像生成模型提供一些解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2403.01489v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection**<br />
**Title_cn:** 用于分布外检测的深度生成模型的 Fisher 信息度量的近似<br />
**Authors:** Sam Dauncey, Chris Holmes, Christopher Williams, Fabian Falck<br />
**Abstract:** <details><summary>原文: </summary>Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute diagonal values, motivating the use of chi-square distributed, layer-wise gradient norms as features. We combine these features to make a simple, model-agnostic and hyperparameter-free method for OOD detection which estimates the joint density of the layer-wise gradient norms for a given data point. We find that these layer-wise gradient norms are weakly correlated, rendering their combined usage informative, and prove that the layer-wise gradient norms satisfy the principle of (data representation) invariance. Our empirical results indicate that this method outperforms the Typicality test for most deep generative models and image dataset pairings.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于似然的深度生成模型（例如基于分数的扩散模型和变分自动编码器）是最先进的机器学习模型，可逼近图像、文本或音频等数据的高维分布。它们可以自然地应用到的许多下游任务之一是分布外（OOD）检测。然而，Nalisnick 等人的开创性工作。我们重现的结果表明，深度生成模型始终推断出 OOD 数据比它们所训练的数据更高的对数似然，这标志着一个悬而未决的问题。在这项工作中，我们基于 OOD 数据应具有比训练数据更大的梯度范数的简单直觉，使用数据点相对于 OOD 检测深度生成模型参数的梯度进行分析。我们将梯度大小的测量形式化为费舍尔信息度量的近似值。我们证明费舍尔信息矩阵（FIM）具有较大的绝对对角线值，从而激发了使用卡方分布、逐层梯度范数作为特征。我们结合这些特征，为 OOD 检测创建了一种简单的、与模型无关且无超参数的方法，该方法估计给定数据点的分层梯度范数的联合密度。我们发现这些分层梯度范数是弱相关的，使得它们的组合使用信息丰富，并证明分层梯度范数满足（数据表示）不变性原则。我们的实证结果表明，该方法优于大多数深度生成模型和图像数据集配对的典型性测试。</details>
**PDF:** <http://arxiv.org/pdf/2403.01485v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding**<br />
**Title_cn:** InfiMM-HD：高分辨率多模态理解的飞跃<br />
**Authors:** Haogeng Liu, Quanzeng You, Xiaotian Han, Yiqi Wang, Bohan Zhai, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have experienced significant advancements recently. Nevertheless, challenges persist in the accurate recognition and comprehension of intricate details within high-resolution images. Despite being indispensable for the development of robust MLLMs, this area remains underinvestigated. To tackle this challenge, our work introduces InfiMM-HD, a novel architecture specifically designed for processing images of different resolutions with low computational overhead. This innovation facilitates the enlargement of MLLMs to higher-resolution capabilities. InfiMM-HD incorporates a cross-attention module and visual windows to reduce computation costs. By integrating this architectural design with a four-stage training pipeline, our model attains improved visual perception efficiently and cost-effectively. Empirical study underscores the robustness and effectiveness of InfiMM-HD, opening new avenues for exploration in related areas. Codes and models can be found at https://huggingface.co/Infi-MM/infimm-hd</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型（MLLM）最近取得了重大进展。然而，准确识别和理解高分辨率图像中的复杂细节仍然存在挑战。尽管对于发展强大的 MLLM 来说是不可或缺的，但这一领域的研究仍然不足。为了应对这一挑战，我们的工作引入了 InfiMM-HD，这是一种专门为以较低的计算开销处理不同分辨率的图像而设计的新颖架构。这项创新有助于将 MLLM 扩展到更高分辨率。 InfiMM-HD 结合了交叉注意力模块和视觉窗口来降低计算成本。通过将此架构设计与四阶段训练流程相集成，我们的模型有效且经济高效地获得了改进的视觉感知。实证研究强调了InfiMM-HD的稳健性和有效性，为相关领域的探索开辟了新途径。代码和模型可以在 https://huggingface.co/Infi-MM/infimm-hd 找到</details>
**PDF:** <http://arxiv.org/pdf/2403.01487v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies**<br />
**Title_cn:** MovieLLM：通过人工智能生成的电影增强长视频理解<br />
**Authors:** Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, Tao Chen<br />
**Abstract:** <details><summary>原文: </summary>The development of multimodal models has marked a significant step forward in how machines understand videos. These models have shown promise in analyzing short video clips. However, when it comes to longer formats like movies, they often fall short. The main hurdles are the lack of high-quality, diverse video data and the intensive work required to collect or annotate such data. In the face of these challenges, we propose MovieLLM, a novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of GPT-4 and text-to-image models to generate detailed scripts and corresponding visuals. Our approach stands out for its flexibility and scalability, making it a superior alternative to traditional data collection methods. Our extensive experiments validate that the data produced by MovieLLM significantly improves the performance of multimodal models in understanding complex video narratives, overcoming the limitations of existing datasets regarding scarcity and bias.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态模型的发展标志着机器理解视频的方式向前迈出了重要一步。这些模型在分析短视频剪辑方面显示出了前景。然而，当涉及电影等较长格式时，它们往往会出现不足。主要障碍是缺乏高质量、多样化的视频数据以及收集或注释此类数据所需的大量工作。面对这些挑战，我们提出了 MovieLLM，这是一种新颖的框架，旨在为长视频创建合成的高质量数据。该框架利用 GPT-4 和文本到图像模型的强大功能来生成详细的脚本和相应的视觉效果。我们的方法以其灵活性和可扩展性而著称，使其成为传统数据收集方法的绝佳替代方案。我们的大量实验验证了 MovieLLM 生成的数据显着提高了多模态模型在理解复杂视频叙述方面的性能，克服了现有数据集在稀缺性和偏差方面的局限性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01422v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval**<br />
**Title_cn:** 基于Image2Sentence的非对称零样本合成图像检索<br />
**Authors:** Yongchao Du, Min Wang, Wengang Zhou, Shuping Hui, Houqiang Li<br />
**Abstract:** <details><summary>原文: </summary>The task of composed image retrieval (CIR) aims to retrieve images based on the query image and the text describing the users' intent. Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large vision-language model. To tackle the above problems, we propose Image2Sentence based Asymmetric zero-shot composed image retrieval (ISA), which takes advantage of the VL model and only relies on unlabeled images for composition learning. In the framework, we propose a new adaptive token learner that maps an image to a sentence in the word embedding space of VL model. The sentence adaptively captures discriminative visual information and is further integrated with the text modifier. An asymmetric structure is devised for flexible deployment, in which the lightweight model is adopted for the query side while the large VL model is deployed on the gallery side. The global contrastive distillation and the local alignment regularization are adopted for the alignment between the light model and the VL model for CIR task. Our experiments demonstrate that the proposed ISA could better cope with the real retrieval scenarios and further improve retrieval accuracy and efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>组合图像检索（CIR）任务旨在根据查询图像和描述用户意图的文本来检索图像。现有的方法在 CIR 任务中利用先进的大型视觉语言（VL）模型取得了很大的进步，但是它们普遍存在两个主要问题：缺乏用于模型训练的标记三元组以及部署模型时在资源有限的环境中部署困难。大视觉语言模型。为了解决上述问题，我们提出了基于 Image2Sentence 的非对称零样本合成图像检索（ISA），它利用 VL 模型，仅依赖于未标记图像进行合成学习。在该框架中，我们提出了一种新的自适应标记学习器，它将图像映射到 VL 模型的词嵌入空间中的句子。该句子自适应地捕获有区别的视觉信息，并进一步与文本修饰符集成。为了灵活部署，设计了非对称结构，查询侧采用轻量级模型，图库侧部署大型VL模型。采用全局对比蒸馏和局部对齐正则化来进行 CIR 任务的光模型和 VL 模型之间的对齐。我们的实验表明，所提出的ISA能够更好地应对真实的检索场景，并进一步提高检索的准确性和效率。</details>
**PDF:** <http://arxiv.org/pdf/2403.01431v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Logit Standardization in Knowledge Distillation**<br />
**Title_cn:** 知识蒸馏中的 Logit 标准化<br />
**Authors:** Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, Xiaochun Cao<br />
**Abstract:** <details><summary>原文: </summary>Knowledge distillation involves transferring soft labels from a teacher to a student using a shared temperature-based softmax function. However, the assumption of a shared temperature between teacher and student implies a mandatory exact match between their logits in terms of logit range and variance. This side-effect limits the performance of student, considering the capacity discrepancy between them and the finding that the innate logit relations of teacher are sufficient for student to learn. To address this issue, we propose setting the temperature as the weighted standard deviation of logit and performing a plug-and-play Z-score pre-process of logit standardization before applying softmax and Kullback-Leibler divergence. Our pre-process enables student to focus on essential logit relations from teacher rather than requiring a magnitude match, and can improve the performance of existing logit-based distillation methods. We also show a typical case where the conventional setting of sharing temperature between teacher and student cannot reliably yield the authentic distillation evaluation; nonetheless, this challenge is successfully alleviated by our Z-score. We extensively evaluate our method for various student and teacher models on CIFAR-100 and ImageNet, showing its significant superiority. The vanilla knowledge distillation powered by our pre-process can achieve favorable performance against state-of-the-art methods, and other distillation variants can obtain considerable gain with the assistance of our pre-process.</details>
**Abstract_cn:** <details><summary>译文: </summary>知识蒸馏涉及使用基于温度的共享 softmax 函数将软标签从教师转移到学生。然而，教师和学生之间共享温度的假设意味着他们的逻辑在逻辑范围和方差方面必须完全匹配。考虑到学生之间的能力差异以及教师固有的逻辑关系足以让学生学习，这种副作用限制了学生的表现。为了解决这个问题，我们建议将温度设置为 logit 的加权标准差，并在应用 softmax 和 Kullback-Leibler 散度之前执行即插即用的 logit 标准化 Z 分数预处理。我们的预处理使学生能够专注于教师提供的基本 Logit 关系，而不是要求大小匹配，并且可以提高现有基于 Logit 的蒸馏方法的性能。我们还展示了一个典型的案例，即教师和学生之间共享温度的传统设置无法可靠地产生真实的蒸馏评估；尽管如此，我们的 Z 分数成功缓解了这一挑战。我们在 CIFAR-100 和 ImageNet 上对各种学生和教师模型广泛评估了我们的方法，显示出其显着的优越性。由我们的预处理提供支持的普通知识蒸馏可以相对于最先进的方法获得良好的性能，并且其他蒸馏变体可以在我们的预处理的帮助下获得可观的收益。</details>
**PDF:** <http://arxiv.org/pdf/2403.01427v1><br />
**Code:** <https://github.com/sunshangquan/logit-standardardization-kd>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **AIO2: Online Correction of Object Labels for Deep Learning with Incomplete Annotation in Remote Sensing Image Segmentation**<br />
**Title_cn:** AIO2：遥感图像分割中标注不完整的深度学习对象标签在线校正<br />
**Authors:** Chenying Liu, Conrad M Albrecht, Yi Wang, Qingyu Li, Xiao Xiang Zhu<br />
**Abstract:** <details><summary>原文: </summary>While the volume of remote sensing data is increasing daily, deep learning in Earth Observation faces lack of accurate annotations for supervised optimization. Crowdsourcing projects such as OpenStreetMap distribute the annotation load to their community. However, such annotation inevitably generates noise due to insufficient control of the label quality, lack of annotators, frequent changes of the Earth's surface as a result of natural disasters and urban development, among many other factors. We present Adaptively trIggered Online Object-wise correction (AIO2) to address annotation noise induced by incomplete label sets. AIO2 features an Adaptive Correction Trigger (ACT) module that avoids label correction when the model training under- or overfits, and an Online Object-wise Correction (O2C) methodology that employs spatial information for automated label modification. AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch. We validate our approach on two building footprint segmentation datasets with different spatial resolutions. Experimental results with varying degrees of building label noise demonstrate the robustness of AIO2. Source code will be available at https://github.com/zhu-xlab/AIO2.git.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管遥感数据量每天都在增加，但地球观测中的深度学习面临着缺乏监督优化的准确注释。 OpenStreetMap 等众包项目将注释负载分配给其社区。然而，由于标签质量控制不够、标注者缺乏、自然灾害和城市发展导致地球表面频繁变化等因素，这种标注不可避免地会产生噪音。我们提出了自适应触发在线对象明智校正（AIO2）来解决由不完整标签集引起的注释噪声。 AIO2 具有自适应校正触发 (ACT) 模块，可在模型训练欠拟合或过拟合时避免标签校正，以及在线对象明智校正 (O2C) 方法，可利用空间信息进行自动标签修改。 AIO2利用均值教师模型来增强带有噪声标签的训练鲁棒性，以稳定ACT中拟合的训练精度曲线，并为O2C中的校正提供伪标签。此外，O2C是在线实现的，无需在每个训练周期存储更新的标签。我们在两个具有不同空间分辨率的建筑足迹分割数据集上验证了我们的方法。不同程度的建筑标签噪声的实验结果证明了 AIO2 的稳健性。源代码可在 https://github.com/zhu-xlab/AIO2.git 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01641v1><br />
**Code:** <https://github.com/zhu-xlab/aio2>**<br />
>>**index:** 2<br />
**Title:** **A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation**<br />
**Title_cn:** 基于谱聚类的运动分割统一模型选择技术<br />
**Authors:** Yuxiang Huang, John Zelek<br />
**Abstract:** <details><summary>原文: </summary>Motion segmentation is a fundamental problem in computer vision and is crucial in various applications such as robotics, autonomous driving and action recognition. Recently, spectral clustering based methods have shown impressive results on motion segmentation in dynamic environments. These methods perform spectral clustering on motion affinity matrices to cluster objects or point trajectories in the scene into different motion groups. However, existing methods often need the number of motions present in the scene to be known, which significantly reduces their practicality. In this paper, we propose a unified model selection technique to automatically infer the number of motion groups for spectral clustering based motion segmentation methods by combining different existing model selection techniques together. We evaluate our method on the KT3DMoSeg dataset and achieve competitve results comparing to the baseline where the number of clusters is given as ground truth information.</details>
**Abstract_cn:** <details><summary>译文: </summary>运动分割是计算机视觉中的一个基本问题，在机器人、自动驾驶和动作识别等各种应用中至关重要。最近，基于谱聚类的方法在动态环境中的运动分割方面显示出了令人印象深刻的结果。这些方法对运动亲和力矩阵执行谱聚类，将场景中的对象或点轨迹聚类到不同的运动组中。然而，现有方法通常需要知道场景中存在的运动数量，这大大降低了它们的实用性。在本文中，我们提出了一种统一的模型选择技术，通过将不同的现有模型选择技术结合在一起，自动推断基于谱聚类的运动分割方法的运动组的数量。我们在 KT3DMoSeg 数据集上评估我们的方法，并与以集群数量作为地面真实信息给出的基线相比，获得了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.01606v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition**<br />
**Title_cn:** 重新思考跨域开放词汇动作识别中基于 CLIP 的视频学习器<br />
**Authors:** Kun-Yu Lin, Henghui Ding, Jiaming Zhou, Yi-Xing Peng, Zhilin Zhao, Chen Change Loy, Wei-Shi Zheng<br />
**Abstract:** <details><summary>原文: </summary>Contrastive Language-Image Pretraining (CLIP) has shown remarkable open-vocabulary abilities across various image understanding tasks. Building upon this impressive success, recent pioneer works have proposed to adapt the powerful CLIP to video data, leading to efficient and effective video learners for open-vocabulary action recognition. Inspired by the fact that humans perform actions in diverse environments, our work delves into an intriguing question: Can CLIP-based video learners effectively generalize to video domains they have not encountered during training? To answer this, we establish a CROSS-domain Open-Vocabulary Action recognition benchmark named XOV-Action, and conduct a comprehensive evaluation of five state-of-the-art CLIP-based video learners under various types of domain gaps. Our evaluation demonstrates that previous methods exhibit limited action recognition performance in unseen video domains, revealing potential challenges of the cross-domain open-vocabulary action recognition task. To address this task, our work focuses on a critical challenge, namely scene bias, and we accordingly contribute a novel scene-aware video-text alignment method. Our key idea is to distinguish video representations apart from scene-encoded text representations, aiming to learn scene-agnostic video representations for recognizing actions across domains. Extensive experimental results demonstrate the effectiveness of our method. The benchmark and code will be available at https://github.com/KunyuLin/XOV-Action/.</details>
**Abstract_cn:** <details><summary>译文: </summary>对比语言图像预训练（CLIP）在各种图像理解任务中表现出了卓越的开放词汇能力。基于这一令人印象深刻的成功，最近的先驱工作提出将强大的 CLIP 应用于视频数据，从而实现高效且有效的视频学习者进行开放词汇动作识别。受人类在不同环境中执行动作这一事实的启发，我们的工作深入研究了一个有趣的问题：基于 CLIP 的视频学习器能否有效地推广到他们在训练期间未遇到的视频领域？为了回答这个问题，我们建立了一个名为 XOV-Action 的跨域开放词汇动作识别基准，并对五种最先进的基于 CLIP 的视频学习器在各种类型的领域差距下进行了综合评估。我们的评估表明，以前的方法在看不见的视频域中表现出有限的动作识别性能，揭示了跨域开放词汇动作识别任务的潜在挑战。为了解决这项任务，我们的工作重点关注一个关键挑战，即场景偏差，因此我们贡献了一种新颖的场景感知视频文本对齐方法。我们的关键思想是将视频表示与场景编码的文本表示区分开来，旨在学习与场景无关的视频表示以识别跨域的动作。大量的实验结果证明了我们方法的有效性。基准测试和代码将在 https://github.com/KunyuLin/XOV-Action/ 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2403.01560v1><br />
**Code:** <https://github.com/kunyulin/xov-action>**<br />
>>**index:** 4<br />
**Title:** **Self-Supervised Representation Learning with Meta Comprehensive Regularization**<br />
**Title_cn:** 具有元综合正则化的自监督表示学习<br />
**Authors:** Huijie Guo, Ying Ba, Jie Hu, Lingyu Si, Wenwen Qiang, Lei Shi<br />
**Abstract:** <details><summary>原文: </summary>Self-Supervised Learning (SSL) methods harness the concept of semantic invariance by utilizing data augmentation strategies to produce similar representations for different deformations of the same input. Essentially, the model captures the shared information among multiple augmented views of samples, while disregarding the non-shared information that may be beneficial for downstream tasks. To address this issue, we introduce a module called CompMod with Meta Comprehensive Regularization (MCR), embedded into existing self-supervised frameworks, to make the learned representations more comprehensive. Specifically, we update our proposed model through a bi-level optimization mechanism, enabling it to capture comprehensive features. Additionally, guided by the constrained extraction of features using maximum entropy coding, the self-supervised learning model learns more comprehensive features on top of learning consistent features. In addition, we provide theoretical support for our proposed method from information theory and causal counterfactual perspective. Experimental results show that our method achieves significant improvement in classification, object detection and instance segmentation tasks on multiple benchmark datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>自监督学习（SSL）方法通过利用数据增强策略来利用语义不变性的概念，为同一输入的不同变形生成相似的表示。本质上，该模型捕获样本的多个增强视图之间的共享信息，同时忽略可能对下游任务有益的非共享信息。为了解决这个问题，我们引入了一个名为 CompMod 的模块，该模块具有元综合正则化（MCR），嵌入到现有的自监督框架中，以使学习到的表示更加全面。具体来说，我们通过双层优化机制更新我们提出的模型，使其能够捕获全面的特征。此外，在使用最大熵编码约束提取特征的指导下，自监督学习模型在学习一致特征的基础上学习更全面的特征。此外，我们从信息论和因果反事实的角度为我们提出的方法提供理论支持。实验结果表明，我们的方法在多个基准数据集上的分类、对象检测和实例分割任务上取得了显着的改进。</details>
**PDF:** <http://arxiv.org/pdf/2403.01549v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge Detection and Dual-Path SENet Feature Fusion**<br />
**Title_cn:** CDSE-UNet：通过 Canny 边缘检测和双路径 SENet 特征融合增强 COVID-19 CT 图像分割<br />
**Authors:** Jiao Ding, Jie Chang, Renrui Han, Li Yang<br />
**Abstract:** <details><summary>原文: </summary>Accurate segmentation of COVID-19 CT images is crucial for reducing the severity and mortality rates associated with COVID-19 infections. In response to blurred boundaries and high variability characteristic of lesion areas in COVID-19 CT images, we introduce CDSE-UNet: a novel UNet-based segmentation model that integrates Canny operator edge detection and a dual-path SENet feature fusion mechanism. This model enhances the standard UNet architecture by employing the Canny operator for edge detection in sample images, paralleling this with a similar network structure for semantic feature extraction. A key innovation is the Double SENet Feature Fusion Block, applied across corresponding network layers to effectively combine features from both image paths. Moreover, we have developed a Multiscale Convolution approach, replacing the standard Convolution in UNet, to adapt to the varied lesion sizes and shapes. This addition not only aids in accurately classifying lesion edge pixels but also significantly improves channel differentiation and expands the capacity of the model. Our evaluations on public datasets demonstrate CDSE-UNet's superior performance over other leading models, particularly in segmenting large and small lesion areas, accurately delineating lesion edges, and effectively suppressing noise</details>
**Abstract_cn:** <details><summary>译文: </summary>COVID-19 CT 图像的准确分割对于降低与 COVID-19 感染相关的严重程度和死亡率至关重要。针对COVID-19 CT图像中病灶区域边界模糊和高变异性的特点，我们引入了CDSE-UNet：一种基于UNet的新型分割模型，集成了Canny算子边缘检测和双路径SENet特征融合机制。该模型通过使用 Canny 算子在样本图像中进行边缘检测来增强标准 UNet 架构，并将其与用于语义特征提取的类似网络结构并行。一项关键创新是双 SENet 特征融合块，它应用于相应的网络层，以有效地组合来自两个图像路径的特征。此外，我们开发了一种多尺度卷积方法，取代 UNet 中的标准卷积，以​​适应不同的病变大小和形状。这一添加不仅有助于准确地对病变边缘像素进行分类，而且还显着提高了通道区分度并扩展了模型的容量。我们对公共数据集的评估表明，CDSE-UNet 比其他领先模型具有优越的性能，特别是在分割大小病变区域、准确描绘病变边缘和有效抑制噪声方面</details>
**PDF:** <http://arxiv.org/pdf/2403.01513v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **End-to-End Human Instance Matting**<br />
**Title_cn:** 端到端人体实例抠图<br />
**Authors:** Qinglin Liu, Shengping Zhang, Quanling Meng, Bineng Zhong, Peiqiang Liu, Hongxun Yao<br />
**Abstract:** <details><summary>原文: </summary>Human instance matting aims to estimate an alpha matte for each human instance in an image, which is extremely challenging and has rarely been studied so far. Despite some efforts to use instance segmentation to generate a trimap for each instance and apply trimap-based matting methods, the resulting alpha mattes are often inaccurate due to inaccurate segmentation. In addition, this approach is computationally inefficient due to multiple executions of the matting method. To address these problems, this paper proposes a novel End-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple instance matting in a more efficient manner. Specifically, a general perception network first extracts image features and decodes instance contexts into latent codes. Then, a united guidance network exploits spatial attention and semantics embedding to generate united semantics guidance, which encodes the locations and semantic correspondences of all instances. Finally, an instance matting network decodes the image features and united semantics guidance to predict all instance-level alpha mattes. In addition, we construct a large-scale human instance matting dataset (HIM-100K) comprising over 100,000 human images with instance alpha matte labels. Experiments on HIM-100K demonstrate the proposed E2E-HIM outperforms the existing methods on human instance matting with 50% lower errors and 5X faster speed (6 instances in a 640X640 image). Experiments on the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also achieves competitive performance on traditional human matting.</details>
**Abstract_cn:** <details><summary>译文: </summary>人体实例抠图的目的是估计图像中每个人体实例的 alpha 遮罩，这是极具挑战性的，并且迄今为止很少被研究。尽管做出了一些努力，使用实例分割为每个实例生成三元图并应用基于三元图的抠图方法，但由于分割不准确，生成的 alpha 遮罩通常不准确。此外，由于抠图方法的多次执行，这种方法的计算效率较低。为了解决这些问题，本文提出了一种新颖的端到端人类实例抠图（E2E-HIM）框架，以更有效的方式同时进行多个实例抠图。具体来说，通用感知网络首先提取图像特征并将实例上下文解码为潜在代码。然后，联合引导网络利用空间注意力和语义嵌入来生成联合语义指导，对所有实例的位置和语义对应进行编码。最后，实例抠图网络对图像特征和统一语义指导进行解码，以预测所有实例级 alpha 遮罩。此外，我们还构建了一个大规模人体实例抠图数据集 (HIM-100K)，其中包含超过 100,000 张带有实例 alpha 抠图标签的人类图像。 HIM-100K 上的实验表明，所提出的 E2E-HIM 在人体实例抠图方面优于现有方法，错误率降低了 50%，速度提高了 5 倍（640X640 图像中的 6 个实例）。在 PPM-100、RWP-636 和 P3M 数据集上的实验表明，E2E-HIM 在传统人体抠图上也取得了有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01510v1><br />
**Code:** <https://github.com/qlyoo/e2e-him>**<br />
>>**index:** 7<br />
**Title:** **EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation**<br />
**Title_cn:** EAGLE：以对象为中心的无监督语义分割的特征聚合学习<br />
**Authors:** Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation has innately relied on extensive pixel-level labeled annotated data, leading to the emergence of unsupervised methodologies. Among them, leveraging self-supervised Vision Transformers for unsupervised semantic segmentation (USS) has been making steady progress with expressive deep features. Yet, for semantically segmenting images with complex objects, a predominant challenge remains: the lack of explicit object-level semantic encoding in patch-level features. This technical limitation often leads to inadequate segmentation of complex objects with diverse structures. To address this gap, we present a novel approach, EAGLE, which emphasizes object-centric representation learning for unsupervised semantic segmentation. Specifically, we introduce EiCue, a spectral technique providing semantic and structural cues through an eigenbasis derived from the semantic similarity matrix of deep image features and color affinity from an image. Further, by incorporating our object-centric contrastive loss with EiCue, we guide our model to learn object-level representations with intra- and inter-image object-feature consistency, thereby enhancing semantic accuracy. Extensive experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art USS results of EAGLE with accurate and consistent semantic segmentation across complex scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割本质上依赖于广泛的像素级标记注释数据，导致无监督方法的出现。其中，利用自监督视觉变换器进行无监督语义分割（USS）一直在表达深度特征方面取得稳步进展。然而，对于具有复杂对象的图像进行语义分割，仍然存在一个主要挑战：补丁级特征中缺乏明确的对象级语义编码。这种技术限制通常会导致对具有不同结构的复杂对象的分割不充分。为了解决这一差距，我们提出了一种新方法 EAGLE，它强调以对象为中心的表示学习以实现无监督语义分割。具体来说，我们介绍了 EiCue，一种光谱技术，通过从深层图像特征的语义相似性矩阵和图像的颜色亲和力导出的特征基提供语义和结构线索。此外，通过将以对象为中心的对比损失与 EiCue 相结合，我们引导模型学习具有图像内和图像间对象特征一致性的对象级表示，从而提高语义准确性。在 COCO-Stuff、Cityscapes 和 Potsdam-3 数据集上进行的大量实验证明了 EAGLE 最先进的 USS 结果，在复杂场景中具有准确且一致的语义分割。</details>
**PDF:** <http://arxiv.org/pdf/2403.01482v1><br />
**Code:** <https://github.com/MICV-yonsei/EAGLE>**<br />
>>**index:** 8<br />
**Title:** **CCC: Color Classified Colorization**<br />
**Title_cn:** CCC：颜色分类着色<br />
**Authors:** Mrityunjoy Gain, Avi Deb Raha, Rameswar Debnath<br />
**Abstract:** <details><summary>原文: </summary>Automatic colorization of gray images with objects of different colors and sizes is challenging due to inter- and intra-object color variation and the small area of the main objects due to extensive backgrounds. The learning process often favors dominant features, resulting in a biased model. In this paper, we formulate the colorization problem into a multinomial classification problem and then apply a weighted function to classes. We propose a set of formulas to transform color values into color classes and vice versa. Class optimization and balancing feature distribution are the keys for good performance. Observing class appearance on various extremely large-scale real-time images in practice, we propose 215 color classes for our colorization task. During training, we propose a class-weighted function based on true class appearance in each batch to ensure proper color saturation of individual objects. We establish a trade-off between major and minor classes to provide orthodox class prediction by eliminating major classes' dominance over minor classes. As we apply regularization to enhance the stability of the minor class, occasional minor noise may appear at the object's edges. We propose a novel object-selective color harmonization method empowered by the SAM to refine and enhance these edges. We propose a new color image evaluation metric, the Chromatic Number Ratio (CNR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using five different datasets: ADE, Celeba, COCO, Oxford 102 Flower, and ImageNet, in both qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization and CNR measurement criteria while maintaining satisfactory performance in regression (MSE, PSNR), similarity (SSIM, LPIPS, UIQI), and generative criteria (FID).</details>
**Abstract_cn:** <details><summary>译文: </summary>由于对象间和对象内的颜色变化以及由于广泛的背景而导致主要对象的面积较小，因此对具有不同颜色和大小的对象的灰度图像进行自动着色具有挑战性。学习过程通常偏向于主导特征，从而导致模型有偏差。在本文中，我们将着色问题转化为多项分类问题，然后将加权函数应用于类别。我们提出了一组公式来将颜色值转换为颜色类别，反之亦然。类优化和平衡特征分布是良好性能的关键。在实践中观察各种超大规模实时图像的类外观，我们为我们的着色任务提出了 215 个颜色类。在训练过程中，我们根据每批中的真实类别外观提出一个类别加权函数，以确保各个对象的适当颜色饱和度。我们在主要类别和次要类别之间建立权衡，通过消除主要类别对次要类别的主导地位来提供正统的类别预测。当我们应用正则化来增强次要类的稳定性时，对象的边缘可能会偶尔出现次要噪声。我们提出了一种由 SAM 支持的新颖的对象选择性颜色协调方法，以细化和增强这些边缘。我们提出了一种新的彩色图像评估指标，即色数比（CNR），以量化颜色成分的丰富度。我们使用五个不同的数据集（ADE、Celeba、COCO、Oxford 102 Flower 和 ImageNet）以定性和定量方法将我们提出的模型与最先进的模型进行比较。实验结果表明，我们提出的模型在可视化和 CNR 测量标准方面优于其他模型，同时在回归（MSE、PSNR）、相似性（SSIM、LPIPS、UIQI）和生成标准（FID）方面保持令人满意的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01476v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Is in-domain data beneficial in transfer learning for landmarks detection in x-ray images?**<br />
**Title_cn:** 域内数据对于 X 射线图像中地标检测的迁移学习是否有益？<br />
**Authors:** Roberto Di Via, Matteo Santacesaria, Francesca Odone, Vito Paolo Pastore<br />
**Abstract:** <details><summary>原文: </summary>In recent years, deep learning has emerged as a promising technique for medical image analysis. However, this application domain is likely to suffer from a limited availability of large public datasets and annotations. A common solution to these challenges in deep learning is the usage of a transfer learning framework, typically with a fine-tuning protocol, where a large-scale source dataset is used to pre-train a model, further fine-tuned on the target dataset. In this paper, we present a systematic study analyzing whether the usage of small-scale in-domain x-ray image datasets may provide any improvement for landmark detection over models pre-trained on large natural image datasets only. We focus on the multi-landmark localization task for three datasets, including chest, head, and hand x-ray images. Our results show that using in-domain source datasets brings marginal or no benefit with respect to an ImageNet out-of-domain pre-training. Our findings can provide an indication for the development of robust landmark detection systems in medical images when no large annotated dataset is available.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，深度学习已成为医学图像分析的一种有前景的技术。然而，该应用程序领域可能会受到大型公共数据集和注释的可用性有限的影响。深度学习中应对这些挑战的常见解决方案是使用迁移学习框架，通常带有微调协议，其中使用大规模源数据集来预训练模型，并在目标数据集上进一步微调。在本文中，我们提出了一项系统研究，分析与仅在大型自然图像数据集上预训练的模型相比，使用小规模域内 X 射线图像数据集是否可以为地标检测提供任何改进。我们专注于三个数据集的多地标定位任务，包括胸部、头部和手部 X 射线图像。我们的结果表明，使用域内源数据集相对于 ImageNet 域外预训练带来的好处微乎其微，甚至没有任何好处。当没有大型注释数据集可用时，我们的研究结果可以为医学图像中稳健的地标检测系统的开发提供指示。</details>
**PDF:** <http://arxiv.org/pdf/2403.01470v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Multiview Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks**<br />
**Title_cn:** 基于图卷积网络的高光谱图像多视图子空间聚类<br />
**Authors:** Xianju Li, Renxiang Guan, Zihao Li, Hao Liu, Jing Yang<br />
**Abstract:** <details><summary>原文: </summary>High-dimensional and complex spectral structures make clustering of hy-perspectral images (HSI) a challenging task. Subspace clustering has been shown to be an effective approach for addressing this problem. However, current subspace clustering algorithms are mainly designed for a single view and do not fully exploit spatial or texture feature information in HSI. This study proposed a multiview subspace clustering of HSI based on graph convolutional networks. (1) This paper uses the powerful classification ability of graph convolutional network and the learning ability of topologi-cal relationships between nodes to analyze and express the spatial relation-ship of HSI. (2) Pixel texture and pixel neighbor spatial-spectral infor-mation were sent to construct two graph convolutional subspaces. (3) An attention-based fusion module was used to adaptively construct a more discriminative feature map. The model was evaluated on three popular HSI datasets, including Indian Pines, Pavia University, and Houston. It achieved overall accuracies of 92.38%, 93.43%, and 83.82%, respectively and significantly outperformed the state-of-the-art clustering methods. In conclusion, the proposed model can effectively improve the clustering ac-curacy of HSI.</details>
**Abstract_cn:** <details><summary>译文: </summary>高维和复杂的光谱结构使得高光谱图像（HSI）的聚类成为一项具有挑战性的任务。子空间聚类已被证明是解决该问题的有效方法。然而，当前的子空间聚类算法主要是针对单视图设计的，并没有充分利用HSI中的空间或纹理特征信息。本研究提出了一种基于图卷积网络的HSI多视图子空间聚类。 (1)本文利用图卷积网络强大的分类能力和节点间拓扑关系的学习能力来分析和表达HSI的空间关系。 (2)发送像素纹理和像素邻居空间光谱信息来构造两个图卷积子空间。 （3）使用基于注意力的融合模块自适应地构建更具辨别力的特征图。该模型在三个流行的 HSI 数据集（包括 Indian Pines、Pavia University 和 Houston）上进行了评估。它的总体准确率分别为 92.38%、93.43% 和 83.82%，显着优于最先进的聚类方法。综上所述，该模型能够有效提高HSI的聚类精度。</details>
**PDF:** <http://arxiv.org/pdf/2403.01465v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **GuardT2I: Defending Text-to-Image Models from Adversarial Prompts**<br />
**Title_cn:** GuardT2I：保护文本到图像模型免受对抗性提示的影响<br />
**Authors:** Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像（T2I）模型的最新进展引起了人们对它们可能被滥用以生成不适当或不安全工作（NSFW）内容的严重安全担忧，尽管现有的对策如 NSFW 分类器或模型微调删除不适当的概念。为了应对这一挑战，我们的研究推出了 GuardT2I，这是一种新颖的调节框架，它采用生成方法来增强 T2I 模型针对对抗性提示的鲁棒性。 GuardT2I 没有进行二元分类，而是利用大型语言模型 (LLM) 有条件地将 T2I 模型中的文本指导嵌入转换为自然语言，以进行有效的对抗性提示检测，而不会影响模型的固有性能。我们广泛的实验表明，GuardT2I 在不同的对抗场景中明显优于 OpenAI-Moderation 和 Microsoft Azure Moderator 等领先的商业解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2403.01446v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features**<br />
**Title_cn:** GPTSee：通过基于描述的相似性特征增强时刻检索和亮点检测<br />
**Authors:** Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, Sidan Du<br />
**Abstract:** <details><summary>原文: </summary>Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\&HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using only span anchors and similarity scores as outputs, positioning accuracy outperforms traditional methods, like Moment-DETR.</details>
**Abstract_cn:** <details><summary>译文: </summary>时刻检索（MR）和亮点检测（HD）旨在从相应的自然语言查询中识别视频中的相关时刻和亮点。大型语言模型 (LLM) 已表现出对各种计算机视觉任务的熟练程度。然而，现有的 MR\&HD 方法尚未与法学硕士集成。在这封信中，我们提出了一种新颖的两级模型，它将 LLM 的输出作为第二级变压器编码器-解码器的输入。首先，采用 MiniGPT-4 生成视频帧的详细描述并重写查询语句，作为新特征馈入编码器。然后，计算生成的描述和重写的查询之间的语义相似度。最后，连续的高相似度视频帧被转换为跨度锚点，作为解码器的先验位置信息。实验表明，我们的方法取得了最先进的结果，并且仅使用跨度锚点和相似度分数作为输出，定位精度优于 Moment-DETR 等传统方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01437v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **A Simple-but-effective Baseline for Training-free Class-Agnostic Counting**<br />
**Title_cn:** 简单但有效的免训练与类别无关计数的基线<br />
**Authors:** Yuhao Lin, Haiming Xu, Lingqiao Liu, Javen Qinfeng Shi<br />
**Abstract:** <details><summary>原文: </summary>Class-Agnostic Counting (CAC) seeks to accurately count objects in a given image with only a few reference examples. While previous methods achieving this relied on additional training, recent efforts have shown that it's possible to accomplish this without training by utilizing pre-existing foundation models, particularly the Segment Anything Model (SAM), for counting via instance-level segmentation. Although promising, current training-free methods still lag behind their training-based counterparts in terms of performance. In this research, we present a straightforward training-free solution that effectively bridges this performance gap, serving as a strong baseline. The primary contribution of our work lies in the discovery of four key technologies that can enhance performance. Specifically, we suggest employing a superpixel algorithm to generate more precise initial point prompts, utilizing an image encoder with richer semantic knowledge to replace the SAM encoder for representing candidate objects, and adopting a multiscale mechanism and a transductive prototype scheme to update the representation of reference examples. By combining these four technologies, our approach achieves significant improvements over existing training-free methods and delivers performance on par with training-based ones.</details>
**Abstract_cn:** <details><summary>译文: </summary>类别无关计数 (CAC) 旨在仅通过几个参考示例来准确计数给定图像中的对象。虽然以前实现这一目标的方法依赖于额外的训练，但最近的努力表明，通过利用预先存在的基础模型，特别是通过实例级分割进行计数的分段任意模型 (SAM)，无需训练即可实现这一目标。尽管很有希望，但目前的免训练方法在性能方面仍然落后于基于训练的方法。在这项研究中，我们提出了一种简单的免培训解决方案，可以有效地弥合这种性能差距，并作为强有力的基线。我们工作的主要贡献在于发现了四种可以提高性能的关键技术。具体来说，我们建议采用超像素算法来生成更精确的初始点提示，利用具有更丰富语义知识的图像编码器来代替SAM编码器来表示候选对象，并采用多尺度机制和转导原型方案来更新参考的表示例子。通过结合这四种技术，我们的方法比现有的免培训方法取得了显着改进，并提供了与基于培训的方法相当的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01418v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition**<br />
**Title_cn:** LUM-ViT：用于带宽有限光信号采集的可学习欠采样掩模视觉变压器<br />
**Authors:** Lingfeng Liu, Dong Ni, Hangjie Yuan<br />
**Abstract:** <details><summary>原文: </summary>Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality. Code will be available at https://github.com/MaxLLF/LUM-ViT.</details>
**Abstract_cn:** <details><summary>译文: </summary>信号采集期间的带宽限制经常阻碍实时检测应用。高光谱数据就是一个值得注意的例子，其庞大的数据量影响了实时高光谱检测。为了解决这个障碍，我们引入了一种利用预采集调制来减少采集量的新颖方法。该调制过程由深度学习模型利用先验信息控制。我们方法的核心是 LUM-ViT，一种 Vision Transformer 变体。独特的是，LUM-ViT 结合了专为预采集调制而定制的可学习欠采样掩模。为了进一步优化光学计算，我们提出了内核级权重二值化技术和三阶段微调策略。我们的评估表明，通过仅采样 10% 的原始图像像素，LUM-ViT 在 ImageNet 分类任务上将精度损失保持在 1.8% 以内。该方法在现实世界的光学硬件上实现时保持了接近原始的精度，证明了其实用性。代码可在 https://github.com/MaxLLF/LUM-ViT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01412v1><br />
**Code:** <https://github.com/maxllf/lum-vit>**<br />
>>**index:** 15<br />
**Title:** **Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation**<br />
**Title_cn:** Region-Transformer：基于自注意力区域的与类无关的点云分割<br />
**Authors:** Dipesh Gyawali, Jian Zhang, BB Karki<br />
**Abstract:** <details><summary>原文: </summary>Point cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based transformer model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and self-attention mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a self-attention mechanism in a region-growth approach. With the introduction of self-attention to region-growth that can utilize local contextual information of neighborhood points, our experiments demonstrate that the Region-Transformer model outperforms previous class-agnostic and class-specific methods on indoor datasets regarding clustering metrics. The model generalizes well to large-scale scenes. Key advantages include capturing long-range dependencies through self-attention, avoiding the need for semantic labels during training, and applicability to a variable number of objects. The Region-Transformer model represents a promising approach for flexible point cloud segmentation with applications in robotics, digital twinning, and autonomous vehicles.</details>
**Abstract_cn:** <details><summary>译文: </summary>点云分割可以帮助我们了解特定结构和对象的环境，可以以特定于类和与类无关的方式执行。我们提出了一种称为 Region-Transformer 的新颖的基于区域的变压器模型，用于执行与类无关的点云分割。该模型利用区域增长方法和自注意力机制，通过添加或删除点来迭代扩展或收缩区域。它仅在具有实例标签的模拟点云上进行训练，避免了语义标签。基于注意力的网络在许多以前执行点云分割的方法中取得了成功。然而，基于注意力的网络的区域增长方法尚未用于探索其性能增益。据我们所知，我们是第一个在区域增长方法中使用自我关注机制的人。通过引入可以利用邻域点的局部上下文信息的区域增长自注意力，我们的实验表明，区域变换器模型在有关聚类指标的室内数据集上优于先前的类不可知和类特定方法。该模型可以很好地推广到大型场景。主要优点包括通过自注意力捕获远程依赖关系，避免在训练期间需要语义标签，以及适用于可变数量的对象。 Region-Transformer 模型代表了一种有前途的灵活点云分割方法，可应用于机器人、数字孪生和自动驾驶汽车。</details>
**PDF:** <http://arxiv.org/pdf/2403.01407v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model**<br />
**Title_cn:** 通过新颖设计的两路交互式融合模块模型增强图像中的视网膜血管结构分割<br />
**Authors:** Rui Yang, Shunpu Zhang<br />
**Abstract:** <details><summary>原文: </summary>Precision in identifying and differentiating micro and macro blood vessels in the retina is crucial for the diagnosis of retinal diseases, although it poses a significant challenge. Current autoencoding-based segmentation approaches encounter limitations as they are constrained by the encoder and undergo a reduction in resolution during the encoding stage. The inability to recover lost information in the decoding phase further impedes these approaches. Consequently, their capacity to extract the retinal microvascular structure is restricted. To address this issue, we introduce Swin-Res-Net, a specialized module designed to enhance the precision of retinal vessel segmentation. Swin-Res-Net utilizes the Swin transformer which uses shifted windows with displacement for partitioning, to reduce network complexity and accelerate model convergence. Additionally, the model incorporates interactive fusion with a functional module in the Res2Net architecture. The Res2Net leverages multi-scale techniques to enlarge the receptive field of the convolutional kernel, enabling the extraction of additional semantic information from the image. This combination creates a new module that enhances the localization and separation of micro vessels in the retina. To improve the efficiency of processing vascular information, we've added a module to eliminate redundant information between the encoding and decoding steps.   Our proposed architecture produces outstanding results, either meeting or surpassing those of other published models. The AUC reflects significant enhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise segmentation of retinal vessels across three widely utilized datasets: CHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms alternative architectures, demonstrating superior performance in both IOU and F1 measure metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>精确识别和区分视网膜中的微观和宏观血管对于视网膜疾病的诊断至关重要，尽管它提出了重大挑战。当前基于自动编码的分割方法遇到限制，因为它们受到编码器的约束并且在编码阶段经历分辨率的降低。无法在解码阶段恢复丢失的信息进一步阻碍了这些方法。因此，它们提取视网膜微血管结构的能力受到限制。为了解决这个问题，我们引入了 Swin-Res-Net，这是一个专门用于提高视网膜血管分割精度的模块。 Swin-Res-Net 利用 Swin 变压器，使用带有位移的移位窗口进行分区，以降低网络复杂性并加速模型收敛。此外，该模型将交互式融合与 Res2Net 架构中的功能模块相结合。 Res2Net 利用多尺度技术来扩大卷积核的感受野，从而能够从图像中提取额外的语义信息。这种组合创建了一个新模块，可以增强视网膜中微血管的定位和分离。为了提高处理血管信息的效率，我们添加了一个模块来消除编码和解码步骤之间的冗余信息。我们提出的架构产生了出色的结果，达到或超过了其他已发布模型的结果。 AUC 反映了显着的增强，在三个广泛使用的数据集（CHASE-DB1、DRIVE 和 STARE）中的视网膜血管像素级分割中分别实现了 0.9956、0.9931 和 0.9946 的值。此外，Swin-Res-Net 的性能优于替代架构，在 IOU 和 F1 测量指标上都表现出卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01362v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction**<br />
**Title_cn:** OccFusion：用于 3D 占用预测的简单有效的多传感器融合框架<br />
**Authors:** Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall<br />
**Abstract:** <details><summary>原文: </summary>This paper introduces OccFusion, a straightforward and efficient sensor fusion framework for predicting 3D occupancy. A comprehensive understanding of 3D scenes is crucial in autonomous driving, and recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, existing methods for 3D occupancy prediction heavily rely on surround-view camera images, making them susceptible to changes in lighting and weather conditions. By integrating features from additional sensors, such as lidar and surround view radars, our framework enhances the accuracy and robustness of occupancy prediction, resulting in top-tier performance on the nuScenes benchmark. Furthermore, extensive experiments conducted on the nuScenes dataset, including challenging night and rainy scenarios, confirm the superior performance of our sensor fusion strategy across various perception ranges. The code for this framework will be made available at https://github.com/DanielMing123/OCCFusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 OccFusion，这是一种简单高效的传感器融合框架，用于预测 3D 占用情况。对 3D 场景的全面理解对于自动驾驶至关重要，最近的 3D 语义占用预测模型已经成功解决了描述具有不同形状和类别的现实世界对象的挑战。然而，现有的 3D 占用预测方法严重依赖环视摄像机图像，因此容易受到照明和天气条件变化的影响。通过集成激光雷达和环视雷达等其他传感器的功能，我们的框架提高了占用预测的准确性和稳健性，从而在 nuScenes 基准测试中实现顶级性能。此外，在 nuScenes 数据集上进行的大量实验（包括具有挑战性的夜间和雨天场景）证实了我们的传感器融合策略在各种感知范围内的卓越性能。该框架的代码将在 https://github.com/DanielMing123/OCCFusion 上提供。</details>
**PDF:** <http://arxiv.org/pdf/2403.01644v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV**<br />
**Title_cn:** Kick Back & Relax++：利用 SlowTV 和 CribsTV 超越真实深度<br />
**Authors:** Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised learning is the key to unlocking generic computer vision systems. By eliminating the reliance on ground-truth annotations, it allows scaling to much larger data quantities. Unfortunately, self-supervised monocular depth estimation (SS-MDE) has been limited by the absence of diverse training data. Existing datasets have focused exclusively on urban driving in densely populated cities, resulting in models that fail to generalize beyond this domain.   To address these limitations, this paper proposes two novel datasets: SlowTV and CribsTV. These are large-scale datasets curated from publicly available YouTube videos, containing a total of 2M training frames. They offer an incredibly diverse set of environments, ranging from snowy forests to coastal roads, luxury mansions and even underwater coral reefs. We leverage these datasets to tackle the challenging task of zero-shot generalization, outperforming every existing SS-MDE approach and even some state-of-the-art supervised methods.   The generalization capabilities of our models are further enhanced by a range of components and contributions: 1) learning the camera intrinsics, 2) a stronger augmentation regime targeting aspect ratio changes, 3) support frame randomization, 4) flexible motion estimation, 5) a modern transformer-based architecture. We demonstrate the effectiveness of each component in extensive ablation experiments. To facilitate the development of future research, we make the datasets, code and pretrained models available to the public at https://github.com/jspenmar/slowtv_monodepth.</details>
**Abstract_cn:** <details><summary>译文: </summary>自监督学习是解锁通用计算机视觉系统的关键。通过消除对真实注释的依赖，它允许扩展到更大的数据量。不幸的是，自监督单目深度估计（SS-MDE）由于缺乏多样化的训练数据而受到限制。现有数据集仅关注人口稠密城市的城市驾驶，导致模型无法推广到该领域之外。为了解决这些限制，本文提出了两个新颖的数据集：SlowTV 和 CribsTV。这些是根据公开的 YouTube 视频整理的大型数据集，总共包含 200 万个训练帧。它们提供了极其多样化的环境，从白雪皑皑的森林到沿海道路、豪华豪宅，甚至水下珊瑚礁。我们利用这些数据集来解决零样本泛化这一具有挑战性的任务，其性能优于所有现有的 SS-MDE 方法，甚至优于一些最先进的监督方法。我们模型的泛化能力通过一系列组件和贡献进一步增强：1）学习相机内在特性，2）针对长宽比变化的更强的增强机制，3）支持帧随机化，4）灵活的运动估计，5）基于现代变压器的架构。我们在广泛的消融实验中证明了每个组件的有效性。为了促进未来研究的发展，我们在 https://github.com/jspenmar/slowtv_monodepth 上向公众提供数据集、代码和预训练模型。</details>
**PDF:** <http://arxiv.org/pdf/2403.01569v1><br />
**Code:** <https://github.com/jspenmar/slowtv_monodepth>**<br />
>>**index:** 3<br />
**Title:** **Pyramid Feature Attention Network for Monocular Depth Prediction**<br />
**Title_cn:** 用于单目深度预测的金字塔特征注意网络<br />
**Authors:** Yifang Xu, Chenglei Peng, Ming Li, Yang Li, Sidan Du<br />
**Abstract:** <details><summary>原文: </summary>Deep convolutional neural networks (DCNNs) have achieved great success in monocular depth estimation (MDE). However, few existing works take the contributions for MDE of different levels feature maps into account, leading to inaccurate spatial layout, ambiguous boundaries and discontinuous object surface in the prediction. To better tackle these problems, we propose a Pyramid Feature Attention Network (PFANet) to improve the high-level context features and low-level spatial features. In the proposed PFANet, we design a Dual-scale Channel Attention Module (DCAM) to employ channel attention in different scales, which aggregate global context and local information from the high-level feature maps. To exploit the spatial relationship of visual features, we design a Spatial Pyramid Attention Module (SPAM) which can guide the network attention to multi-scale detailed information in the low-level feature maps. Finally, we introduce scale-invariant gradient loss to increase the penalty on errors in depth-wise discontinuous regions. Experimental results show that our method outperforms state-of-the-art methods on the KITTI dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度卷积神经网络（DCNN）在单目深度估计（MDE）方面取得了巨大成功。然而，现有的工作很少考虑不同级别特征图对 MDE 的贡献，从而导致预测中空间布局不准确、边界模糊和物体表面不连续。为了更好地解决这些问题，我们提出了金字塔特征注意网络（PFANet）来改进高层上下文特征和低层空间特征。在所提出的 PFANet 中，我们设计了一个双尺度通道注意力模块（DCAM）来采用不同尺度的通道注意力，它聚合来自高级特征映射的全局上下文和局部信息。为了利用视觉特征的空间关系，我们设计了一个空间金字塔注意力模块（SPAM），它可以引导网络关注低级特征图中的多尺度详细信息。最后，我们引入尺度不变梯度损失来增加深度不连续区域中错误的惩罚。实验结果表明，我们的方法在 KITTI 数据集上优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01440v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion**<br />
**Title_cn:** 基于Transformer-Encoder和特征融合的深度估计算法<br />
**Authors:** Linhan Xia, Junbang Liu, Tong Wu<br />
**Abstract:** <details><summary>原文: </summary>This research presents a novel depth estimation algorithm based on a Transformer-encoder architecture, tailored for the NYU and KITTI Depth Dataset. This research adopts a transformer model, initially renowned for its success in natural language processing, to capture intricate spatial relationships in visual data for depth estimation tasks. A significant innovation of the research is the integration of a composite loss function that combines Structural Similarity Index Measure (SSIM) with Mean Squared Error (MSE). This combined loss function is designed to ensure the structural integrity of the predicted depth maps relative to the original images (via SSIM) while minimizing pixel-wise estimation errors (via MSE). This research approach addresses the challenges of over-smoothing often seen in MSE-based losses and enhances the model's ability to predict depth maps that are not only accurate but also maintain structural coherence with the input images. Through rigorous training and evaluation using the NYU Depth Dataset, the model demonstrates superior performance, marking a significant advancement in single-image depth estimation, particularly in complex indoor and traffic environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项研究提出了一种基于 Transformer-编码器架构的新型深度估计算法，专为 NYU 和 KITTI 深度数据集量身定制。这项研究采用了最初因其在自然语言处理方面的成功而闻名的变压器模型来捕获视觉数据中复杂的空间关系，以执行深度估计任务。该研究的一个重大创新是集成了将结构相似性指数测量（SSIM）与均方误差（MSE）相结合的复合损失函数。这种组合损失函数旨在确保预测深度图相对于原始图像（通过 SSIM）的结构完整性，同时最小化像素级估计误差（通过 MSE）。这种研究方法解决了基于 MSE 的损失中常见的过度平滑的挑战，并增强了模型预测深度图的能力，该深度图不仅准确，而且保持与输入图像的结构一致性。通过使用纽约大学深度数据集的严格训练和评估，该模型表现出了卓越的性能，标志着单图像深度估计的显着进步，特别是在复杂的室内和交通环境中。</details>
**PDF:** <http://arxiv.org/pdf/2403.01370v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos**<br />
**Title_cn:** 模式：状态变化对于教学视频中的程序规划很重要<br />
**Authors:** Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, Shih-Fu Chang<br />
**Abstract:** <details><summary>原文: </summary>We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting. For state change tracking, we align visual state observations with language state descriptions via cross-modal contrastive learning, and explicitly model the intermediate states of the procedure using LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV benchmark datasets demonstrate that our proposed SCHEMA model achieves state-of-the-art performance and obtains explainable visualizations.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们研究教学视频中的程序规划问题，其目的是在给定部分视觉状态观察的情况下制定以目标为导向的行动步骤序列。这个问题的动机是学习一个结构化的、可规划的状态和动作空间。最近的工作成功地对步骤进行了序列建模，在训练期间只能访问序列级注释，这忽略了状态在过程中的作用。在这项工作中，我们指出状态变化对于教学视频中的程序规划很重要（SCHEMA）。我们的目标是通过研究程序中步骤和状态之间的因果关系来建立一个更加结构化的状态空间。具体来说，我们明确地将每个步骤表示为状态变化，并跟踪过程中的状态变化。对于步骤表示，我们利用大型语言模型（LLM）中的常识知识，通过我们设计的思维链提示来描述步骤的状态变化。对于状态变化跟踪，我们通过跨模态对比学习将视觉状态观察与语言状态描述对齐，并使用 LLM 生成的状态描述对过程的中间状态进行显式建模。 CrossTask、COIN 和 NIV 基准数据集上的实验表明，我们提出的 SCHEMA 模型实现了最先进的性能并获得了可解释的可视化。</details>
**PDF:** <http://arxiv.org/pdf/2403.01599v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **You Need to Pay Better Attention**<br />
**Title_cn:** 你需要更加注意<br />
**Authors:** Mehran Hosseini, Peyman Hosseini<br />
**Abstract:** <details><summary>原文: </summary>We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了三种新的注意力机制，它们在效率和学习能力方面优于标准多头注意力，从而提高了 Transformer 模型的性能和更广泛的可部署性。我们的第一个贡献是优化注意力，它的表现与标准注意力类似，但参数数量是标准注意力的 3/4，并且每个头的矩阵乘法更少。接下来，我们介绍 Efficient Attention，它的性能与标准 Attention 相当，参数数量仅为标准 Attention 的 1/2，每个头的矩阵乘法次数更少，速度高达标准 Attention 的两倍。最后，我们介绍了超级注意力，它在视觉和自然语言处理任务上都远远超过了标准注意力，同时参数和矩阵乘法更少。除了提供严格的数学比较之外，我们还评估了 MNIST、CIFAR100、IMDB 电影评论和亚马逊评论数据集上提出的注意力机制。</details>
**PDF:** <http://arxiv.org/pdf/2403.01643v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **APISR: Anime Production Inspired Real-World Anime Super-Resolution**<br />
**Title_cn:** APISR：动漫制作启发现实世界动漫超分辨率<br />
**Authors:** Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao<br />
**Abstract:** <details><summary>原文: </summary>While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we introduce the balanced twin perceptual loss combining both anime and photorealistic high-level features to mitigate unwanted color artifacts and increase visual clarity. We evaluate our method through extensive experiments on the public benchmark, showing our method outperforms state-of-the-art approaches by a large margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>虽然现实世界的动漫超分辨率 (SR) 在 SR 社区中获得了越来越多的关注，但现有方法仍然采用真实感领域的技术。在本文中，我们分析了动漫制作流程，并重新思考如何利用其特点来实现现实世界的动漫SR。首先，我们认为由于手绘帧的重复使用，视频网络和数据集对于动漫 SR 来说不是必需的。相反，我们通过从视频源中选择压缩最少且信息最多的帧来提出动漫图像收集管道。基于这个管道，我们引入了面向动漫制作的图像（API）数据集。此外，我们还发现了两个动漫特有的挑战，即扭曲和微弱的手绘线条以及不需要的色彩伪影。我们通过在图像退化模型中引入面向预测的压缩模块和具有增强手绘线的伪地面实况准备来解决第一个问题。此外，我们引入了平衡双感知损失，结合了动画和真实感高级功能，以减轻不需要的色彩伪影并提高视觉清晰度。我们通过对公共基准的大量实验来评估我们的方法，表明我们的方法大大优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01598v1><br />
**Code:** <https://github.com/kiteretsu77/apisr>**<br />
>>**index:** 3<br />
**Title:** **MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images**<br />
**Title_cn:** MatchU：匹配看不见的对象以从 RGB-D 图像进行 6D 姿势估计<br />
**Authors:** Junwen Huang, Hao Yu, Kuan-Ting Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam<br />
**Abstract:** <details><summary>原文: </summary>Recent learning methods for object pose estimation require resource-intensive training for each individual object instance or category, hampering their scalability in real applications when confronted with previously unseen objects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for 6D pose estimation from RGB-D images. MatchU is a generic approach that fuses 2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We rely on learning geometric 3D descriptors that are rotation-invariant by design. By encoding pose-agnostic geometry, the learned descriptors naturally generalize to unseen objects and capture symmetries. To tackle ambiguous associations using 3D geometry only, we fuse additional RGB information into our descriptor. This is achieved through a novel attention-based mechanism that fuses cross-modal information, together with a matching loss that leverages the latent space learned from RGB data to guide the descriptor learning process. Extensive experiments reveal the generalizability of both the RGB-D fusion strategy as well as the descriptor efficacy. Benefiting from the novel designs, MatchU surpasses all existing methods by a significant margin in terms of both accuracy and speed, even without the requirement of expensive re-training or rendering.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的物体姿态估计学习方法需要对每个单独的物体实例或类别进行资源密集型训练，当面对以前未见过的物体时，阻碍了它们在实际应用中的可扩展性。在本文中，我们提出了 MatchU，一种用于从 RGB-D 图像进行 6D 姿态估计的融合-描述-匹配策略。 MatchU 是一种融合 2D 纹理和 3D 几何线索的通用方法，用于对不可见物体进行 6D 姿态预测。我们依赖于学习几何 3D 描述符，这些描述符在设计上是旋转不变的。通过编码与姿态无关的几何形状，学习到的描述符自然地推广到看不见的物体并捕获对称性。为了仅使用 3D 几何来解决模糊关联，我们将额外的 RGB 信息融合到描述符中。这是通过一种新颖的基于注意力的机制来实现的，该机制融合了跨模态信息，以及利用从 RGB 数据学习的潜在空间来指导描述符学习过程的匹配损失。大量实验揭示了 RGB-D 融合策略以及描述符功效的通用性。受益于新颖的设计，MatchU 在准确性和速度方面都远远超过了所有现有方法，甚至不需要昂贵的重新训练或渲染。</details>
**PDF:** <http://arxiv.org/pdf/2403.01517v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **Spectrum AUC Difference (SAUCD): Human-aligned 3D Shape Evaluation**<br />
**Title_cn:** 频谱 AUC 差异 (SAUCD)：人体对齐 3D 形状评估<br />
**Authors:** Tianyu Luan, Zhong Li, Lele Chen, Xuan Gong, Lichang Chen, Yi Xu, Junsong Yuan<br />
**Abstract:** <details><summary>原文: </summary>Existing 3D mesh shape evaluation metrics mainly focus on the overall shape but are usually less sensitive to local details. This makes them inconsistent with human evaluation, as human perception cares about both overall and detailed shape. In this paper, we propose an analytic metric named Spectrum Area Under the Curve Difference (SAUCD) that demonstrates better consistency with human evaluation. To compare the difference between two shapes, we first transform the 3D mesh to the spectrum domain using the discrete Laplace-Beltrami operator and Fourier transform. Then, we calculate the Area Under the Curve (AUC) difference between the two spectrums, so that each frequency band that captures either the overall or detailed shape is equitably considered. Taking human sensitivity across frequency bands into account, we further extend our metric by learning suitable weights for each frequency band which better aligns with human perception. To measure the performance of SAUCD, we build a 3D mesh evaluation dataset called Shape Grading, along with manual annotations from more than 800 subjects. By measuring the correlation between our metric and human evaluation, we demonstrate that SAUCD is well aligned with human evaluation, and outperforms previous 3D mesh metrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的3D网格形状评估指标主要关注整体形状，但通常对局部细节不太敏感。这使得它们与人类的评估不一致，因为人类的感知既关心整体形状又关心细节形状。在本文中，我们提出了一种名为曲线下频谱面积差异（SAUCD）的分析指标，它与人类评估具有更好的一致性。为了比较两种形状之间的差异，我们首先使用离散 Laplace-Beltrami 算子和傅里叶变换将 3D 网格变换到谱域。然后，我们计算两个频谱之间的曲线下面积 (AUC) 差异，以便公平地考虑捕获整体或详细形状的每个频带。考虑到人类跨频段的敏感性，我们通过学习每个频段的合适权重来进一步扩展我们的指标，从而更好地符合人类感知。为了衡量 SAUCD 的性能，我们构建了一个名为“形状分级”的 3D 网格评估数据集，以及来自 800 多个受试者的手动注释。通过测量我们的指标和人类评估之间的相关性，我们证明 SAUCD 与人类评估非常一致，并且优于之前的 3D 网格指标。</details>
**PDF:** <http://arxiv.org/pdf/2403.01619v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos**<br />
**Title_cn:** 3DGStream：3D 高斯的动态训练，用于高效流式传输逼真的自由视点视频<br />
**Authors:** Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing<br />
**Abstract:** <details><summary>原文: </summary>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\"ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>从多视图视频构建动态场景的逼真自由视点视频 (FVV) 仍然是一项具有挑战性的工作。尽管当前的神经渲染技术取得了显着的进步，但这些方法通常需要完整的视频序列进行离线训练，并且无法实时渲染。为了解决这些限制，我们引入了 3DGStream，这是一种专为现实世界动态场景的高效 FVV 流式传输而设计的方法。我们的方法实现了 12 秒内快速每帧重建和 200 FPS 的实时渲染。具体来说，我们利用 3D 高斯 (3DG) 来表示场景。我们没有采用直接优化每帧 3DG 的简单方法，而是采用紧凑的神经变换缓存 (NTC) 来对 3DG 的平移和旋转进行建模，显着减少了每个 FVV 帧所需的训练时间和存储空间。此外，我们提出了一种自适应 3DG 添加策略来处理动态场景中的新兴对象。实验表明，与最先进的方法相比，3DGStream 在渲染速度、图像质量、训练时间和模型存储方面实现了具有竞争力的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01444v2><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes**<br />
**Title_cn:** 无符号正交距离场：各种 3D 形状的准确神经隐式表示<br />
**Authors:** Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, Lin Gao<br />
**Abstract:** <details><summary>原文: </summary>Neural implicit representation of geometric shapes has witnessed considerable advancements in recent years. However, common distance field based implicit representations, specifically signed distance field (SDF) for watertight shapes or unsigned distance field (UDF) for arbitrary shapes, routinely suffer from degradation of reconstruction accuracy when converting to explicit surface points and meshes. In this paper, we introduce a novel neural implicit representation based on unsigned orthogonal distance fields (UODFs). In UODFs, the minimal unsigned distance from any spatial point to the shape surface is defined solely in one orthogonal direction, contrasting with the multi-directional determination made by SDF and UDF. Consequently, every point in the 3D UODFs can directly access its closest surface points along three orthogonal directions. This distinctive feature leverages the accurate reconstruction of surface points without interpolation errors. We verify the effectiveness of UODFs through a range of reconstruction examples, extending from simple watertight or non-watertight shapes to complex shapes that include hollows, internal or assembling structures.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，几何形状的神经隐式表示取得了长足的进步。然而，基于隐式表示的常见距离场，特别是用于防水形状的有符号距离场（SDF）或用于任意形状的无符号距离场（UDF），在转换为显式表面点和网格时通常会遭受重建精度下降的影响。在本文中，我们介绍了一种基于无符号正交距离场（UODF）的新型神经隐式表示。在 UODF 中，从任何空间点到形状表面的最小无符号距离仅在一个正交方向上定义，这与 SDF 和 UDF 进行的多方向确定形成对比。因此，3D UODF 中的每个点都可以沿三个正交方向直接访问其最近的表面点。这一独特的功能利用了表面点的精确重建，没有插值误差。我们通过一系列重建示例验证了 UODF 的有效性，从简单的水密或非水密形状到包括中空、内部或组装结构的复杂形状。</details>
**PDF:** <http://arxiv.org/pdf/2403.01414v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Efficient Action Counting with Dynamic Queries**<br />
**Title_cn:** 通过动态查询进行高效的操作计数<br />
**Authors:** Zishi Li, Xiaoxuan Ma, Qiuyan Shang, Wentao Zhu, Hai Ci, Yu Qiao, Yizhou Wang<br />
**Abstract:** <details><summary>原文: </summary>Temporal repetition counting aims to quantify the repeated action cycles within a video. The majority of existing methods rely on the similarity correlation matrix to characterize the repetitiveness of actions, but their scalability is hindered due to the quadratic computational complexity. In this work, we introduce a novel approach that employs an action query representation to localize repeated action cycles with linear computational complexity. Based on this representation, we further develop two key components to tackle the essential challenges of temporal repetition counting. Firstly, to facilitate open-set action counting, we propose the dynamic update scheme on action queries. Unlike static action queries, this approach dynamically embeds video features into action queries, offering a more flexible and generalizable representation. Secondly, to distinguish between actions of interest and background noise actions, we incorporate inter-query contrastive learning to regularize the video representations corresponding to different action queries. As a result, our method significantly outperforms previous works, particularly in terms of long video sequences, unseen actions, and actions at various speeds. On the challenging RepCountA benchmark, we outperform the state-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean error decrease and 94.1% computational burden reduction. Code is available at https://github.com/lizishi/DeTRC.</details>
**Abstract_cn:** <details><summary>译文: </summary>时间重复计数旨在量化视频中重复的动作周期。大多数现有方法依靠相似性相关矩阵来表征动作的重复性，但由于二次计算复杂性，其可扩展性受到阻碍。在这项工作中，我们引入了一种新颖的方法，该方法采用动作查询表示来以线性计算复杂度来定位重复的动作循环。基于这种表示，我们进一步开发了两个关键组件来解决时间重复计数的基本挑战。首先，为了促进开放集动作计数，我们提出了动作查询的动态更新方案。与静态动作查询不同，这种方法将视频特征动态嵌入到动作查询中，提供更灵活和更通用的表示。其次，为了区分感兴趣的动作和背景噪声动作，我们结合查询间对比学习来规范与不同动作查询相对应的视频表示。因此，我们的方法显着优于以前的工作，特别是在长视频序列、看不见的动作和不同速度的动作方面。在具有挑战性的 RepCountA 基准测试中，我们的 OBO 准确度比最先进的方法 TransRAC 高出 26.5%，平均误差降低了 22.7%，计算负担降低了 94.1%。代码可在 https://github.com/lizishi/DeTRC 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01543v2><br />
**Code:** <https://github.com/lizishi/detrc>**<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **DUFOMap: Efficient Dynamic Awareness Mapping**<br />
**Title_cn:** DUFOMap：高效的动态感知绘图<br />
**Authors:** Daniel Duberg, Qingwen Zhang, MingKai Jia, Patric Jensfelt<br />
**Abstract:** <details><summary>原文: </summary>The dynamic nature of the real world is one of the main challenges in robotics. The first step in dealing with it is to detect which parts of the world are dynamic. A typical benchmark task is to create a map that contains only the static part of the world to support, for example, localization and planning. Current solutions are often applied in post-processing, where parameter tuning allows the user to adjust the setting for a specific dataset. In this paper, we propose DUFOMap, a novel dynamic awareness mapping framework designed for efficient online processing. Despite having the same parameter settings for all scenarios, it performs better or is on par with state-of-the-art methods. Ray casting is utilized to identify and classify fully observed empty regions. Since these regions have been observed empty, it follows that anything inside them at another time must be dynamic. Evaluation is carried out in various scenarios, including outdoor environments in KITTI and Argoverse 2, open areas on the KTH campus, and with different sensor types. DUFOMap outperforms the state of the art in terms of accuracy and computational efficiency. The source code, benchmarks, and links to the datasets utilized are provided. See https://kin-zhang.github.io/dufomap for more details.</details>
**Abstract_cn:** <details><summary>译文: </summary>现实世界的动态本质是机器人技术的主要挑战之一。处理这个问题的第一步是检测世界的哪些部分是动态的。典型的基准任务是创建一个仅包含世界静态部分的地图，以支持本地化和规划等工作。当前的解决方案通常应用于后处理，其中参数调整允许用户调整特定数据集的设置。在本文中，我们提出了 DUFOMap，这是一种专为高效在线处理而设计的新型动态感知映射框架。尽管所有场景都具有相同的参数设置，但它的性能更好或与最先进的方法相当。利用射线投射来识别和分类完全观察到的空白区域。由于观察到这些区域是空的，因此在其他时间它们内部的任何东西都必须是动态的。评估在各种场景中进行，包括 KITTI 和 Argoverse 2 的室外环境、KTH 校园的开放区域以及不同的传感器类型。 DUFOMap 在准确性和计算效率方面优于最先进的技术。提供了源代码、基准测试和所使用的数据集的链接。请参阅 https://kin-zhang.github.io/dufomap 了解更多详细信息。</details>
**PDF:** <http://arxiv.org/pdf/2403.01449v1><br />
**Code:** <https://github.com/kth-rpl/dufomap>**<br />
>>**index:** 2<br />
**Title:** **Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis**<br />
**Title_cn:** 动态适配器满足快速调整：用于点云分析的参数高效迁移学习<br />
**Authors:** Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai<br />
**Abstract:** <details><summary>原文: </summary>Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal trade-off between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过迁移点云预训练模型，点云分析取得了出色的性能。然而，现有的模型自适应方法通常会更新所有模型参数，即完全微调范式，这种方法效率低下，因为它依赖于高计算成本（例如训练GPU内存）和海量存储空间。在本文中，我们的目标是研究用于点云分析的参数高效迁移学习，在任务性能和参数效率之间实现理想的权衡。为了实现这一目标，我们冻结默认预训练模型的参数，然后提出动态适配器，考虑到令牌对下游任务的重要性，它为每个令牌生成动态比例。我们通过构建内部提示，进一步将动态适配器与提示调整（DAPT）无缝集成，捕获特定于实例的交互功能。在五个具有挑战性的数据集上进行的大量实验表明，与完全微调的对应数据相比，所提出的 DAPT 实现了卓越的性能，同时将可训练参数和训练 GPU 内存分别显着减少了 95% 和 35%。代码可在 https://github.com/LMD0311/DAPT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01439v1><br />
**Code:** <https://github.com/lmd0311/dapt>**<br />
>>**index:** 3<br />
**Title:** **SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images**<br />
**Title_cn:** SA-MixNet：遥感图像中涂鸦监督道路提取的结构感知混合和不变学习<br />
**Authors:** Jie Feng, Hao Huang, Junpeng Zhang, Weisheng Dong, Dingwen Zhang, Licheng Jiao<br />
**Abstract:** <details><summary>原文: </summary>Mainstreamed weakly supervised road extractors rely on highly confident pseudo-labels propagated from scribbles, and their performance often degrades gradually as the image scenes tend various. We argue that such degradation is due to the poor model's invariance to scenes with different complexities, whereas existing solutions to this problem are commonly based on crafted priors that cannot be derived from scribbles. To eliminate the reliance on such priors, we propose a novel Structure-aware Mixup and Invariance Learning framework (SA-MixNet) for weakly supervised road extraction that improves the model invariance in a data-driven manner. Specifically, we design a structure-aware Mixup scheme to paste road regions from one image onto another for creating an image scene with increased complexity while preserving the road's structural integrity. Then an invariance regularization is imposed on the predictions of constructed and origin images to minimize their conflicts, which thus forces the model to behave consistently on various scenes. Moreover, a discriminator-based regularization is designed for enhancing the connectivity meanwhile preserving the structure of roads. Combining these designs, our framework demonstrates superior performance on the DeepGlobe, Wuhan, and Massachusetts datasets outperforming the state-of-the-art techniques by 1.47%, 2.12%, 4.09% respectively in IoU metrics, and showing its potential of plug-and-play. The code will be made publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>主流的弱监督道路提取器依赖于从涂鸦中传播的高度置信的伪标签，并且随着图像场景的变化，它们的性能通常会逐渐下降。我们认为这种退化是由于较差的模型对不同复杂度的场景的不变性造成的，而该问题的现有解决方案通常基于精心设计的先验，而这些先验不能从涂鸦中得出。为了消除对此类先验的依赖，我们提出了一种新颖的结构感知混合和不变学习框架（SA-MixNet），用于弱监督道路提取，以数据驱动的方式提高模型不变性。具体来说，我们设计了一种结构感知混合方案，将一张图像中的道路区域粘贴到另一张图像上，以创建复杂性更高的图像场景，同时保留道路的结构完整性。然后对构建图像和原始图像的预测进行不变性正则化，以尽量减少它们的冲突，从而迫使模型在各种场景上表现一致。此外，基于判别器的正则化旨在增强连通性，同时保留道路结构。结合这些设计，我们的框架在 DeepGlobe、武汉和马萨诸塞州数据集上展示了卓越的性能，在 IoU 指标上分别比最先进的技术高出 1.47%、2.12%、4.09%，并展示了其即插即用的潜力。 -玩。该代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2403.01381v1><br />
**Code:** null<br />

