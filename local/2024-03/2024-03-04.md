## [UPDATED!] **2024-03-04** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Gradient Correlation Subspace Learning against Catastrophic Forgetting**<br />
**Title_cn:** 针对灾难性遗忘的梯度相关子空间学习<br />
**Authors:** Tammuz Dubnov, Vishal Thengane<br />
**Abstract:** <details><summary>原文: </summary>Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}</details>
**Abstract_cn:** <details><summary>译文: </summary>高效的持续学习技术一直是过去几年中重要研究的主题。这种学习的一个基本问题是先前学习任务的表现严重下降，也称为灾难性遗忘。本文介绍了一种在增量课堂学习背景下减少灾难性遗忘的新方法，称为梯度相关子空间学习（GCSL）。该方法检测受先前任务影响最小的权重子空间，并将用于训练新任务的权重投影到所述子空间中。该方法可以应用于给定网络架构的一层或多层，并且所使用的子空间的大小可以随着层和任务的不同而改变。代码可在 \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL} 获取</details>
**PDF:** <http://arxiv.org/pdf/2403.02334v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control**<br />
**Title_cn:** UniCtrl：通过免训练统一注意力控制提高文本到视频扩散模型的时空一致性<br />
**Authors:** Xuweiyi Chen, Tian Xia, Sihan Xu<br />
**Abstract:** <details><summary>原文: </summary>Video Diffusion Models have been developed for video generation, usually integrating text and image conditioning to enhance control over the generated content. Despite the progress, ensuring consistency across frames remains a challenge, particularly when using text prompts as control conditions. To address this problem, we introduce UniCtrl, a novel, plug-and-play method that is universally applicable to improve the spatiotemporal consistency and motion diversity of videos generated by text-to-video models without additional training. UniCtrl ensures semantic consistency across different frames through cross-frame self-attention control, and meanwhile, enhances the motion quality and spatiotemporal consistency through motion injection and spatiotemporal synchronization. Our experimental results demonstrate UniCtrl's efficacy in enhancing various text-to-video models, confirming its effectiveness and universality.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频扩散模型是为视频生成而开发的，通常集成文本和图像调节以增强对生成内容的控制。尽管取得了进展，但确保跨框架的一致性仍然是一个挑战，特别是在使用文本提示作为控制条件时。为了解决这个问题，我们引入了 UniCtrl，这是一种新颖的即插即用方法，普遍适用于提高文本到视频模型生成的视频的时空一致性和运动多样性，无需额外训练。 UniCtrl通过跨帧自注意力控制确保不同帧之间的语义一致性，同时通过运动注入和时空同步增强运动质量和时空一致性。我们的实验结果证明了 UniCtrl 在增强各种文本到视频模型方面的功效，证实了其有效性和通用性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02332v2><br />
**Code:** <https://github.com/XuweiyiChen/UniCtrl>**<br />
>>**index:** 3<br />
**Title:** **3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors**<br />
**Title_cn:** 3DTopia：具有混合扩散先验的大型文本到 3D 生成模型<br />
**Authors:** Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, Ziwei Liu<br />
**Abstract:** <details><summary>原文: </summary>We present a two-stage text-to-3D generation system, namely 3DTopia, which generates high-quality general 3D assets within 5 minutes using hybrid diffusion priors. The first stage samples from a 3D diffusion prior directly learned from 3D data. Specifically, it is powered by a text-conditioned tri-plane latent diffusion model, which quickly generates coarse 3D samples for fast prototyping. The second stage utilizes 2D diffusion priors to further refine the texture of coarse 3D models from the first stage. The refinement consists of both latent and pixel space optimization for high-quality texture generation. To facilitate the training of the proposed system, we clean and caption the largest open-source 3D dataset, Objaverse, by combining the power of vision language models and large language models. Experiment results are reported qualitatively and quantitatively to show the performance of the proposed system. Our codes and models are available at https://github.com/3DTopia/3DTopia</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一个两阶段文本到 3D 生成系统，即 3DTopia，它使用混合扩散先验在 5 分钟内生成高质量的通用 3D 资产。第一阶段从直接从 3D 数据学习的 3D 扩散先验中进行采样。具体来说，它由文本条件三平面潜在扩散模型提供支持，该模型可以快速生成粗糙的 3D 样本以进行快速原型设计。第二阶段利用 2D 扩散先验进一步细化第一阶段的粗糙 3D 模型的纹理。细化包括潜在和像素空间优化，以生成高质量的纹理。为了促进所提出系统的训练，我们通过结合视觉语言模型和大型语言模型的力量，对最大的开源 3D 数据集 Objaverse 进行清理和描述。定性和定量地报告实验结果，以显示所提出系统的性能。我们的代码和模型可在 https://github.com/3DTopia/3DTopia 获取</details>
**PDF:** <http://arxiv.org/pdf/2403.02234v1><br />
**Code:** <https://github.com/3dtopia/3dtopia>**<br />
>>**index:** 4<br />
**Title:** **DragTex: Generative Point-Based Texture Editing on 3D Mesh**<br />
**Title_cn:** DragTex：3D 网格上基于点的生成纹理编辑<br />
**Authors:** Yudi Zhang, Qi Xu, Lei Zhang<br />
**Abstract:** <details><summary>原文: </summary>Creating 3D textured meshes using generative artificial intelligence has garnered significant attention recently. While existing methods support text-based generative texture generation or editing on 3D meshes, they often struggle to precisely control pixels of texture images through more intuitive interaction. While 2D images can be edited generatively using drag interaction, applying this type of methods directly to 3D mesh textures still leads to issues such as the lack of local consistency among multiple views, error accumulation and long training times. To address these challenges, we propose a generative point-based 3D mesh texture editing method called DragTex. This method utilizes a diffusion model to blend locally inconsistent textures in the region near the deformed silhouette between different views, enabling locally consistent texture editing. Besides, we fine-tune a decoder to reduce reconstruction errors in the non-drag region, thereby mitigating overall error accumulation. Moreover, we train LoRA using multi-view images instead of training each view individually, which significantly shortens the training time. The experimental results show that our method effectively achieves dragging textures on 3D meshes and generates plausible textures that align with the desired intent of drag interaction.</details>
**Abstract_cn:** <details><summary>译文: </summary>使用生成人工智能创建 3D 纹理网格最近引起了广泛关注。虽然现有方法支持基于文本的生成纹理生成或在 3D 网格上编辑，但它们通常难以通过更直观的交互来精确控制纹理图像的像素。虽然可以使用拖动交互来生成编辑 2D 图像，但将此类方法直接应用于 3D 网格纹理仍然会导致多个视图之间缺乏局部一致性、错误累积和训练时间长等问题。为了解决这些挑战，我们提出了一种名为 DragTex 的基于生成点的 3D 网格纹理编辑方法。该方法利用扩散模型来混合不同视图之间变形轮廓附近区域中局部不一致的纹理，从而实现局部一致的纹理编辑。此外，我们微调解码器以减少非拖动区域的重建错误，从而减轻整体错误累积。此外，我们使用多视图图像来训练 LoRA，而不是单独训练每个视图，这显着缩短了训练时间。实验结果表明，我们的方法有效地实现了在 3D 网格上拖动纹理，并生成与拖动交互的预期意图一致的合理纹理。</details>
**PDF:** <http://arxiv.org/pdf/2403.02217v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans**<br />
**Title_cn:** 医学图像分析 AI 的领域适应、可解释性和公平性：基于 3D 胸部 CT 扫描的 COVID-19 诊断<br />
**Authors:** Dimitrios Kollias, Anastasios Arsenos, Stefanos Kollias<br />
**Abstract:** <details><summary>原文: </summary>The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the 'Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference. The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain Adaptation. The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series. Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700. Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges. The paper presents the baseline models used in the Challenges and the performance which was obtained respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文介绍了 DEF-AI-MIA COV19D 竞赛，该竞赛是在 2024 年计算机视觉和模式识别的“人工智能医学图像分析领域适应、可解释性和公平性 (DEF-AI-MIA)”研讨会的框架下组织的（CVPR）会议。此次比赛是该系列比赛的第四次，前三届比赛分别在 ICCV 2021、ECCV 2022 和 ICASSP 2023 国际会议框架下举办。它包括两项挑战：i) Covid-19 检测和 ii) Covid-19 域适应。比赛使用论文中描述的COV19-CT-DB数据库的数据，该数据库包含大量胸部CT扫描系列。每个胸部 CT 扫描系列由一系列二维 CT 切片组成，数量在 50 到 700 之间。训练、验证和测试数据集已从 COV19-CT-DB 中提取，并提供给两项挑战赛的参与者。本文介绍了挑战赛中使用的基线模型以及分别获得的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02192v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Point2Building: Reconstructing Buildings from Airborne LiDAR Point Clouds**<br />
**Title_cn:** Point2Building：利用机载 LiDAR 点云重建建筑物<br />
**Authors:** Yujia Liu, Anton Obukhov, Jan Dirk Wegner, Konrad Schindler<br />
**Abstract:** <details><summary>原文: </summary>We present a learning-based approach to reconstruct buildings as 3D polygonal meshes from airborne LiDAR point clouds. What makes 3D building reconstruction from airborne LiDAR hard is the large diversity of building designs and especially roof shapes, the low and varying point density across the scene, and the often incomplete coverage of building facades due to occlusions by vegetation or to the viewing angle of the sensor. To cope with the diversity of shapes and inhomogeneous and incomplete object coverage, we introduce a generative model that directly predicts 3D polygonal meshes from input point clouds. Our autoregressive model, called Point2Building, iteratively builds up the mesh by generating sequences of vertices and faces. This approach enables our model to adapt flexibly to diverse geometries and building structures. Unlike many existing methods that rely heavily on pre-processing steps like exhaustive plane detection, our model learns directly from the point cloud data, thereby reducing error propagation and increasing the fidelity of the reconstruction. We experimentally validate our method on a collection of airborne LiDAR data of Zurich, Berlin and Tallinn. Our method shows good generalization to diverse urban styles.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种基于学习的方法，从机载 LiDAR 点云将建筑物重建为 3D 多边形网格。机载 LiDAR 进行 3D 建筑重建之所以困难，是因为建筑设计的多样性，特别是屋顶形状，整个场景中的点密度低且变化多端，以及由于植被遮挡或视角的影响，建筑立面通常覆盖不完整。传感器。为了应对形状的多样性以及不均匀和不完整的对象覆盖，我们引入了一种生成模型，可以直接从输入点云预测 3D 多边形网格。我们的自回归模型称为 Point2Building，通过生成顶点和面序列来迭代构建网格。这种方法使我们的模型能够灵活地适应不同的几何形状和建筑结构。与许多严重依赖于详尽平面检测等预处理步骤的现有方法不同，我们的模型直接从点云数据中学习，从而减少错误传播并提高重建的保真度。我们在苏黎世、柏林和塔林的机载 LiDAR 数据集合上进行了实验验证我们的方法。我们的方法显示出对不同城市风格的良好概括。</details>
**PDF:** <http://arxiv.org/pdf/2403.02136v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models**<br />
**Title_cn:** ResAdapter：扩散模型的域一致分辨率适配器<br />
**Authors:** Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia Li, Xuefeng Xiao, Min Zheng, Lean Fu<br />
**Abstract:** <details><summary>原文: </summary>Recent advancement in text-to-image models (e.g., Stable Diffusion) and corresponding personalized technologies (e.g., DreamBooth and LoRA) enables individuals to generate high-quality and imaginative images. However, they often suffer from limitations when generating images with resolutions outside of their trained domain. To overcome this limitation, we present the Resolution Adapter (ResAdapter), a domain-consistent adapter designed for diffusion models to generate images with unrestricted resolutions and aspect ratios. Unlike other multi-resolution generation methods that process images of static resolution with complex post-process operations, ResAdapter directly generates images with the dynamical resolution. Especially, after learning a deep understanding of pure resolution priors, ResAdapter trained on the general dataset, generates resolution-free images with personalized diffusion models while preserving their original style domain. Comprehensive experiments demonstrate that ResAdapter with only 0.5M can process images with flexible resolutions for arbitrary diffusion models. More extended experiments demonstrate that ResAdapter is compatible with other modules (e.g., ControlNet, IP-Adapter and LCM-LoRA) for image generation across a broad range of resolutions, and can be integrated into other multi-resolution model (e.g., ElasticDiffusion) for efficiently generating higher-resolution images. Project link is https://res-adapter.github.io</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像模型（例如稳定扩散）和相应的个性化技术（例如 DreamBooth 和 LoRA）的最新进展使个人能够生成高质量且富有想象力的图像。然而，在生成分辨率超出训练范围的图像时，它们经常受到限制。为了克服这一限制，我们提出了分辨率适配器（ResAdapter），这是一种域一致的适配器，专为扩散模型而设计，可生成分辨率和长宽比不受限制的图像。与其他通过复杂的后处理操作处理静态分辨率图像的多分辨率生成方法不同，ResAdapter 直接生成动态分辨率的图像。特别是，在深入了解纯分辨率先验之后，ResAdapter 在通用数据集上进行训练，使用个性化扩散模型生成无分辨率图像，同时保留其原始风格域。综合实验表明，仅0.5M的ResAdapter就可以处理任意扩散模型的灵活分辨率的图像。更扩展的实验表明，ResAdapter 与其他模块（例如 ControlNet、IP-Adapter 和 LCM-LoRA）兼容，可生成各种分辨率的图像，并且可以集成到其他多分辨率模型（例如 ElasticDiffusion）中有效地生成更高分辨率的图像。项目链接为https://res-adapter.github.io</details>
**PDF:** <http://arxiv.org/pdf/2403.02084v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey**<br />
**Title_cn:** 基于伪标签的半监督语义分割：一项调查<br />
**Authors:** Lingyan Ran, Yali Li, Guoqiang Liang, Yanning Zhang<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.</details>
**Abstract_cn:** <details><summary>译文: </summary>语义分割是计算机视觉中一个重要且流行的研究领域，重点是根据语义对图像中的像素进行分类。然而，有监督深度学习需要大量数据来训练模型，并且逐像素标记图像的过程既费时又费力。本综述旨在对半监督语义分割领域伪标签方法的最新研究成果进行首次全面、有组织的概述，我们从不同角度对其进行分类，并针对具体应用提出具体方法地区。此外，我们还探索了伪标签技术在医学和遥感图像分割中的应用。最后，我们还提出了一些可行的未来研究方向来应对现有的挑战。</details>
**PDF:** <http://arxiv.org/pdf/2403.01909v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio**<br />
**Title_cn:** FaceChain-ImagineID：从解开的音频中自由制作高保真多样化的说话面孔<br />
**Authors:** Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, et.al.<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at https://github.com/modelscope/facechain.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们将人们听到语音、提取有意义的线索以及创建各种动态音频一致的说话面孔（称为“聆听和想象”）的过程抽象为从单个音频生成高保真多样化说话面孔的任务。具体来说，它涉及两个关键挑战：一是有效地将身份、内容和情感与纠缠音频解耦，二是保持视频内的多样性和视频间的一致性。为了解决这些问题，我们首先挖掘面部因素之间错综复杂的关系，并简化解耦过程，定制渐进式音频解缠以实现准确的面部几何和语义学习，其中每个阶段都包含负责特定因素的定制训练模块。其次，为了仅通过单个模型中的输入音频实现视觉多样化和音频同步的动画，我们引入了可控相干帧生成，其中涉及三个可训练适配器与冻结潜在扩散模型（LDM）的灵活集成，以专注于保持面部几何和语义，以及帧之间的纹理和时间连贯性。通过这种方式，我们继承了LDM的高质量多样化生成，同时以较低的训练成本显着提高了其可控性。大量的实验证明了我们的方法在处理这种范式方面的灵活性和有效性。代码将在 https://github.com/modelscope/facechain 发布。</details>
**PDF:** <http://arxiv.org/pdf/2403.01901v1><br />
**Code:** <https://github.com/modelscope/facechain>**<br />
>>**index:** 10<br />
**Title:** **ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models**<br />
**Title_cn:** ViewDiff：使用文本到图像模型生成 3D 一致图像<br />
**Authors:** Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, Matthias Nießner<br />
**Abstract:** <details><summary>原文: </summary>3D asset generation is getting massive amounts of attention, inspired by the recent success of text-guided 2D content creation. Existing text-to-3D methods use pretrained text-to-image diffusion models in an optimization problem or fine-tune them on synthetic data, which often results in non-photorealistic 3D objects without backgrounds. In this paper, we present a method that leverages pretrained text-to-image models as a prior, and learn to generate multi-view images in a single denoising process from real-world data. Concretely, we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the text-to-image model. Moreover, we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods, the results generated by our method are consistent, and have favorable visual quality (-30% FID, -37% KID).</details>
**Abstract_cn:** <details><summary>译文: </summary>受最近成功的文本引导 2D 内容创建的启发，3D 资产生成正在受到广泛关注。现有的文本到 3D 方法在优化问题中使用预训练的文本到图像扩散模型，或在合成数据上对其进行微调，这通常会导致没有背景的非真实感 3D 对象。在本文中，我们提出了一种利用预训练的文本到图像模型作为先验的方法，并学习在单个去噪过程中从真实世界数据生成多视图图像。具体来说，我们建议将 3D 体积渲染和跨帧注意层集成到文本到图像模型现有 U-Net 网络的每个块中。此外，我们设计了一种自回归生成，可以在任何视点渲染更多 3D 一致的图像。我们在现实世界的对象数据集上训练我们的模型，并展示其在真实环境中生成具有各种高质量形状和纹理的实例的能力。与现有方法相比，我们的方法生成的结果是一致的，并且具有良好的视觉质量（-30% FID，-37% KID）。</details>
**PDF:** <http://arxiv.org/pdf/2403.01807v1><br />
**Code:** <https://github.com/facebookresearch/viewdiff>**<br />
>>**index:** 11<br />
**Title:** **OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on**<br />
**Title_cn:** OOTDiffusion：基于舾装融合的潜在扩散，用于可控虚拟试穿<br />
**Authors:** Yuhao Xu, Tao Gu, Weifeng Chen, Chengcai Chen<br />
**Abstract:** <details><summary>原文: </summary>Image-based virtual try-on (VTON), which aims to generate an outfitted image of a target human wearing an in-shop garment, is a challenging image-synthesis task calling for not only high fidelity of the outfitted human but also full preservation of garment details. To tackle this issue, we propose Outfitting over Try-on Diffusion (OOTDiffusion), leveraging the power of pretrained latent diffusion models and designing a novel network architecture for realistic and controllable virtual try-on. Without an explicit warping process, we propose an outfitting UNet to learn the garment detail features, and merge them with the target human body via our proposed outfitting fusion in the denoising process of diffusion models. In order to further enhance the controllability of our outfitting UNet, we introduce outfitting dropout to the training process, which enables us to adjust the strength of garment features through classifier-free guidance. Our comprehensive experiments on the VITON-HD and Dress Code datasets demonstrate that OOTDiffusion efficiently generates high-quality outfitted images for arbitrary human and garment images, which outperforms other VTON methods in both fidelity and controllability, indicating an impressive breakthrough in virtual try-on. Our source code is available at https://github.com/levihsu/OOTDiffusion.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于图像的虚拟试穿（VTON）旨在生成穿着店内服装的目标人的着装图像，是一项具有挑战性的图像合成任务，不仅要求着装人的高保真度，而且要求完整的保存的服装细节。为了解决这个问题，我们提出了 Outfitting over Try-on Diffusion (OOTDiffusion)，利用预训练的潜在扩散模型的力量，并设计一种新颖的网络架构来实现真实且可控的虚拟试穿。在没有显式变形过程的情况下，我们提出了一种服装 UNet 来学习服装细节特征，并通过我们在扩散模型的去噪过程中提出的服装融合将它们与目标人体合并。为了进一步增强我们的服装UNet的可控性，我们在训练过程中引入了服装dropout，这使得我们能够通过无分类器的指导来调整服装特征的强度。我们对 VITON-HD 和 Dress Code 数据集的综合实验表明，OOTDiffusion 可以有效地为任意人体和服装图像生成高质量的着装图像，在保真度和可控性方面优于其他 VTON 方法，这表明虚拟试穿方面取得了令人印象深刻的突破。我们的源代码可在 https://github.com/levihsu/OOTDiffusion 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01779v1><br />
**Code:** <https://github.com/levihsu/ootdiffusion>**<br />
>>**index:** 12<br />
**Title:** **AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network**<br />
**Title_cn:** AFBT GAN：通过反事实生成对抗网络增强认知能力下降的可解释性和诊断性能<br />
**Authors:** Xiongri Shen, Zhenxi Song, Zhiguo Zhang<br />
**Abstract:** <details><summary>原文: </summary>Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transformer generative adversarial network (AFBT GAN), which is specifically designed by network property in FC and inverse patch embedding operation in the transformer. The specific design can make the model focus more on the current network correlation and employ the global insight of the transformer to reconstruct FC, which both help the generation of high-quality target label FC. The validation experiments are conducted on both clinical and public datasets, the generated attention map are both vital correlated to cognitive function and the diagnostic performance is also significant. The code is available at https://github.com/SXR3015/AFBT-GAN.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的功能连接（FC）解释结果通常是利用分类结果标签和皮尔逊相关或梯度向后等相关分析方法生成的。然而，诊断模型仍然是在黑盒模型上训练的，并且在训练过程中可能缺乏重要区域FC的关注。为了增强可解释性并提高诊断性能，为诊断模型提供健康受试者（HC）发展为受试者认知衰退（SCD）和轻度认知障碍（MCI）时神经退行性变相关区域的先验知识是关键步骤。为了更好地确定与神经退行性变相关的区域，我们采用反事实推理来生成从源标签 FC 派生的目标标签 FC 矩阵，然后用目标标签 FC 减去源标签 FC。反事实推理架构由自适应前向和后向变压器生成对抗网络（AFBT GAN）构建，该网络是根据 FC 中的网络特性和变压器中的逆补丁嵌入操作专门设计的。具体的设计可以使模型更加关注当前的网络相关性，并利用变压器的全局洞察力来重构FC，这两者都有助于生成高质量的目标标签FC。验证实验在临床和公共数据集上进行，生成的注意力图与认知功能密切相关，诊断性能也很重要。代码可在 https://github.com/SXR3015/AFBT-GAN 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01758v1><br />
**Code:** <https://github.com/sxr3015/afbt-gan>**<br />
>>**index:** 13<br />
**Title:** **HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances**<br />
**Title_cn:** HanDiffuser：具有逼真手部外观的文本到图像生成<br />
**Authors:** Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita Dasgupta, Saayan Mitra, Minh Hoai<br />
**Abstract:** <details><summary>原文: </summary>Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We conduct extensive quantitative and qualitative experiments and perform user studies to demonstrate the efficacy of our method in generating images with high-quality hands.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本到图像的生成模型可以生成高质量的人类，但生成手时会失去真实感。常见的伪影包括不规则的手部姿势、形状、不正确的手指数量以及物理上不可信的手指方向。为了生成具有真实手部的图像，我们提出了一种名为 HanDiffuser 的新颖的基于扩散的架构，它通过在生成过程中注入手部嵌入来实现真实感。 HanDiffuser 由两个组件组成：一个 Text-to-Hand-Params 扩散模型，用于根据输入文本提示生成 SMPL-Body 和 MANO-Hand 参数；以及一个 Text-Guided Hand-Params-to-Image 扩散模型，用于通过条件合成图像上一个组件生成的提示和手部参数。我们整合了手部表示的多个方面，包括 3D 形状和关节级手指位置、方向和关节，以在推理过程中实现稳健的学习和可靠的性能。我们进行了广泛的定量和定性实验，并进行了用户研究，以证明我们的方法在用高质量的手生成图像方面的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01693v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Improving Adversarial Energy-Based Model via Diffusion Process**<br />
**Title_cn:** 通过扩散过程改进基于能量的对抗模型<br />
**Authors:** Cong Geng, Tian Han, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Søren Hauberg, Bo Li<br />
**Abstract:** <details><summary>原文: </summary>Generative models have shown strong generation ability while efficient likelihood estimation is less explored. Energy-based models~(EBMs) define a flexible energy function to parameterize unnormalized densities efficiently but are notorious for being difficult to train. Adversarial EBMs introduce a generator to form a minimax training game to avoid expensive MCMC sampling used in traditional EBMs, but a noticeable gap between adversarial EBMs and other strong generative models still exists. Inspired by diffusion-based models, we embedded EBMs into each denoising step to split a long-generated process into several smaller steps. Besides, we employ a symmetric Jeffrey divergence and introduce a variational posterior distribution for the generator's training to address the main challenges that exist in adversarial EBMs. Our experiments show significant improvement in generation compared to existing adversarial EBMs, while also providing a useful energy function for efficient density estimation.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型已显示出强大的生成能力，而有效的似然估计则较少被探索。基于能量的模型（EBM）定义了灵活的能量函数来有效地参数化非标准化密度，但因难以训练而臭名昭著。对抗性 EBM 引入生成器来形成极小极大训练游戏，以避免传统 EBM 中使用昂贵的 MCMC 采样，但对抗性 EBM 与其他强生成模型之间仍然存在明显的差距。受基于扩散的模型的启发，我们将 EBM 嵌入到每个去噪步骤中，将长时间生成的过程分成几个较小的步骤。此外，我们采用对称 Jeffrey 散度，并为生成器的训练引入变分后验分布，以解决对抗性 EBM 中存在的主要挑战。我们的实验表明，与现有的对抗性 EBM 相比，生成量显着提高，同时还为有效的密度估计提供了有用的能量函数。</details>
**PDF:** <http://arxiv.org/pdf/2403.01666v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Density-based Isometric Mapping**<br />
**Title_cn:** 基于密度的等轴测图<br />
**Authors:** Bardia Yousefi, Mélina Khansari, Ryan Trask, Patrick Tallon, Carina Carino, Arman Afrasiyabi, Vikas Kundra, Lan Ma, Lei Ren, Keyvan Farahani, et.al.<br />
**Abstract:** <details><summary>原文: </summary>The isometric mapping method employs the shortest path algorithm to estimate the Euclidean distance between points on High dimensional (HD) manifolds. This may not be sufficient for weakly uniformed HD data as it could lead to overestimating distances between far neighboring points, resulting in inconsistencies between the intrinsic (local) and extrinsic (global) distances during the projection. To address this issue, we modify the shortest path algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR) window, which helps to maintain the uniformity of the constructed shortest-path graph in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000 MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC CT/PET datasets with a total of 640 lung cancer patients, were used to benchmark and validate PR-Isomap. 431 imaging biomarkers were extracted from each modality. Our results indicate that PR-Isomap projects HD attributes into a lower-dimensional (LD) space while preserving information, visualized by the MNIST dataset indicating the maintaining local and global distances. PR-Isomap achieved the highest comparative accuracies of 80.9% (STD:5.8) for pneumonia and 78.5% (STD:4.4), 88.4% (STD:1.4), and 61.4% (STD:11.4) for three NSCLC datasets, with a confidence interval of 95% for outcome prediction. Similarly, the multivariate Cox model showed higher overall survival, measured with c-statistics and log-likelihood test, of PR-Isomap compared to other dimensionality reduction methods. Kaplan Meier survival curve also signifies the notable ability of PR-Isomap to distinguish between high-risk and low-risk patients using multimodal imaging biomarkers preserving HD imaging characteristics for precision medicine.</details>
**Abstract_cn:** <details><summary>译文: </summary>等距映射方法采用最短路径算法来估计高维（HD）流形上点之间的欧几里德距离。这对于弱均匀的高清数据来说可能还不够，因为它可能导致高估远邻点之间的距离，从而导致投影期间内在（局部）和外在（全局）距离之间的不一致。为了解决这个问题，我们通过添加受 Parzen-Rosenblatt (PR) 窗口启发的新约束来修改最短路径算法，这有助于保持 Isomap 中构造的最短路径图的一致性。使用包含 72,236 例病例的多个成像数据集、70,000 个 MINST 数据、来自多个胸部 X 射线肺炎数据集的 1596 个数据集以及包含总共 640 名肺癌患者的三个 NSCLC CT/PET 数据集来对 PR-Isomap 进行基准测试和验证。从每种模式中提取了 431 个成像生物标志物。我们的结果表明，PR-Isomap 将 HD 属性投影到低维 (LD) 空间，同时保留信息，通过 MNIST 数据集可视化，指示保持局部和全局距离。 PR-Isomap 对肺炎的比较准确度最高，为 80.9% (STD:5.8)，对三个 NSCLC 数据集的比较准确度为 78.5% (STD:4.4)、88.4% (STD:1.4) 和 61.4% (STD:11.4)，结果预测的置信区间为 95%。同样，与其他降维方法相比，通过 c 统计和对数似然检验测量，多变量 Cox 模型显示 PR-Isomap 具有更高的总体生存率。 Kaplan Meier 生存曲线还表明 PR-Isomap 具有使用多模态成像生物标志物区分高风险和低风险患者的显着能力，保留了精准医疗的高清成像特征。</details>
**PDF:** <http://arxiv.org/pdf/2403.02531v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Differentially Private Representation Learning via Image Captioning**<br />
**Title_cn:** 通过图像字幕进行差分私人表示学习<br />
**Authors:** Tom Sander, Yaodong Yu, Maziar Sanjabi, Alain Durmus, Yi Ma, Kamalika Chaudhuri, Chuan Guo<br />
**Abstract:** <details><summary>原文: </summary>Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For example, under a privacy budget of $\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA of 56.5%. Our work challenges the prevailing sentiment that high-utility DP representation learning cannot be achieved by training from scratch.</details>
**Abstract_cn:** <details><summary>译文: </summary>差分隐私 (DP) 机器学习被认为是从敏感数据训练模型同时仍然保护隐私的黄金标准解决方案。然而，实现这一理想的一个主要障碍是其次优的隐私准确性权衡，这在 DP 表示学习中尤其明显。具体来说，事实证明，在适度的隐私预算下，大多数模型学习的表示并不比手工制作的特征好得多。在这项工作中，我们证明了可以通过图像字幕和扩展到互联网规模的多模态数据集来完成有效的 DP 表示学习。通过一系列的工程技巧，我们使用合理的计算量从头开始在 LAION-2B 的 233M 子集上成功训练了 DP 图像字幕器（DP-Cap），并获得了前所未有的高质量图像特征，可用于各种下游视觉和视觉语言任务。例如，在隐私预算为 $\varepsilon=8$ 的情况下，基于学习的 DP-Cap 特征训练的线性分类器在 ImageNet-1K 上达到了 65.8% 的准确率，大大提高了之前 56.5% 的 SOTA。我们的工作挑战了普遍的观点，即高实用性的 DP 表示学习无法通过从头开始训练来实现。</details>
**PDF:** <http://arxiv.org/pdf/2403.02506v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review**<br />
**Title_cn:** 用于医疗报告生成和视觉问答的视觉语言模型：综述<br />
**Authors:** Iryna Hartsock, Ghulam Rasool<br />
**Abstract:** <details><summary>原文: </summary>Medical vision-language models (VLMs) combine computer vision and natural language processing to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and visual question answering. We provide background on natural language processing and computer vision, explaining how techniques from both fields are integrated into VLMs to enable learning from multimodal data. Key areas we address include the exploration of medical vision-language datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and visual question answering. We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing patient privacy concerns. Overall, our review summarizes recent progress in developing VLMs to harness multimodal medical data for improved healthcare applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学视觉语言模型 (VLM) 结合计算机视觉和自然语言处理来分析视觉和文本医学数据。我们的论文回顾了开发专门用于医疗保健的 VLM 的最新进展，重点关注为医疗报告生成和视觉问答而设计的模型。我们提供自然语言处理和计算机视觉的背景知识，解释如何将这两个领域的技术集成到 VLM 中以实现从多模式数据中学习。我们讨论的关键领域包括医学视觉语言数据集的探索，对最近值得注意的医学VLM中采用的架构和预训练策略的深入分析，以及对评估VLM在医学报告生成和视觉问题方面的性能的评估指标的全面讨论回答。我们还强调当前的挑战并提出未来的方向，包括提高临床有效性和解决患者隐私问题。总的来说，我们的综述总结了开发 VLM 以利用多模式医疗数据改进医疗保健应用的最新进展。</details>
**PDF:** <http://arxiv.org/pdf/2403.02469v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks**<br />
**Title_cn:** COMMIT：证明多传感器融合系统针对语义攻击的鲁棒性<br />
**Authors:** Zijian Huang, Wenda Chu, Linyi Li, Chejian Xu, Bo Li<br />
**Abstract:** <details><summary>原文: </summary>Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and performs a grid-based splitting method to characterize complex semantic transformations. We also propose efficient algorithms to compute the certification in terms of object detection accuracy and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive benchmark of certified robustness for different MSF models using the CARLA simulation platform. We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models. We believe our certification framework and benchmark will contribute an important step towards certifiably robust AVs in practice.</details>
**Abstract_cn:** <details><summary>译文: </summary>多传感器融合系统（MSF）作为现代自动驾驶汽车（AV）中的感知模块发挥着至关重要的作用。因此，确保它们对常见和现实的对抗性语义转换（例如物理世界中的旋转和移位）的鲁棒性对于自动驾驶汽车的安全至关重要。虽然经验证据表明，与单模态模型相比，MSF 表现出更高的鲁棒性，但它们仍然容易受到对抗性语义转换的影响。尽管提出了经验防御的建议，但一些研究表明这些防御可以再次受到新的自适应攻击的攻击。到目前为止，还没有为无国界医生提出经过认证的防御措施。在这项工作中，我们提出了第一个鲁棒性认证框架 COMMIT，证明多传感器融合系统针对语义攻击的鲁棒性。特别是，我们提出了一种实用的各向异性噪声机制，该机制利用多模态数据的随机平滑，并执行基于网格的分割方法来表征复杂的语义转换。我们还提出了有效的算法来计算大规模 MSF 模型的对象检测精度和 IoU 方面的认证。根据经验，我们评估了 COMMIT 在不同设置中的功效，并使用 CARLA 模拟平台为不同的 MSF 模型提供了经认证的稳健性的综合基准。我们发现MSF模型的认证率比单模态模型最多高出48.39%，这验证了MSF模型的优势。我们相信，我们的认证框架和基准将为实践中可验证的强大自动驾驶汽车迈出重要一步。</details>
**PDF:** <http://arxiv.org/pdf/2403.02329v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation**<br />
**Title_cn:** 超越专业化：评估 MLLM 在年龄和性别估计方面的能力<br />
**Authors:** Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune the ShareGPT4V model for this specific task, aiming to achieve state-of-the-art results in this particular challenge. Although such a model would not be practical in production, as it is incredibly expensive compared to a specialized model like MiVOLO, it could be very useful in some tasks, like data annotation.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大型语言模型 (MLLM) 最近非常受欢迎。 ChatGPT-4V 和 Gemini 等强大的商业模型以及 LLaVA 等开源模型本质上都是通用模型，可用于解决各种任务，包括计算机视觉领域的任务。这些神经网络拥有很强的常识和推理能力，事实证明它们甚至能够完成未经专门训练的任务。我们将迄今为止最强大的 MLLM 的功能进行了比较：ShareGPT4V、ChatGPT、LLaVA-Next 在年龄和性别估计的专门任务中与我们最先进的专业模型 MiVOLO 的功能。我们还更新了 MiVOLO 并在本文中提供了详细信息和新指标。这种比较产生了一些有趣的结果，并深入了解了参与模型的优缺点。此外，我们尝试了各种方法来针对此特定任务微调 ShareGPT4V 模型，旨在在此特定挑战中取得最先进的结果。尽管这样的模型在生产中并不实用，因为与 MiVOLO 等专用模型相比它非常昂贵，但它在某些任务中可能非常有用，例如数据注释。</details>
**PDF:** <http://arxiv.org/pdf/2403.02302v1><br />
**Code:** <https://github.com/wildchlamydia/mivolo>**<br />
>>**index:** 6<br />
**Title:** **A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter**<br />
**Title_cn:** 微笑和笑声检测的新视角：强度水平很重要<br />
**Authors:** Hugo Bohy, Kevin El Haddad, Thierry Dutoit<br />
**Abstract:** <details><summary>原文: </summary>Smiles and laughs detection systems have attracted a lot of attention in the past decade contributing to the improvement of human-agent interaction systems. But very few considered these expressions as distinct, although no prior work clearly proves them to belong to the same category or not. In this work, we present a deep learning-based multimodal smile and laugh classification system, considering them as two different entities. We compare the use of audio and vision-based models as well as a fusion approach. We show that, as expected, the fusion leads to a better generalization on unseen data. We also present an in-depth analysis of the behavior of these models on the smiles and laughs intensity levels. The analyses on the intensity levels show that the relationship between smiles and laughs might not be as simple as a binary one or even grouping them in a single category, and so, a more complex approach should be taken when dealing with them. We also tackle the problem of limited resources by showing that transfer learning allows the models to improve the detection of confusing intensity levels.</details>
**Abstract_cn:** <details><summary>译文: </summary>在过去的十年中，微笑和大笑检测系统引起了很多关注，为人机交互系统的改进做出了贡献。但很少有人认为这些表达是不同的，尽管之前的工作没有明确证明它们属于同一类别或不同类别。在这项工作中，我们提出了一种基于深度学习的多模态微笑和大笑分类系统，将它们视为两个不同的实体。我们比较了基于音频和视觉的模型以及融合方法的使用。我们表明，正如预期的那样，融合可以更好地概括未见过的数据。我们还对这些模型在微笑和笑声强度级别上的行为进行了深入分析。对强度水平的分析表明，微笑和大笑之间的关系可能并不像二元关系那么简单，甚至不会将它们归为一个类别，因此，在处理它们时应该采取更复杂的方法。我们还通过证明迁移学习允许模型改进对令人困惑的强度级别的检测来解决资源有限的问题。</details>
**PDF:** <http://arxiv.org/pdf/2403.02112v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations**<br />
**Title_cn:** 多模式社交互动建模：具有密集对齐表示的新挑战和基线<br />
**Authors:** Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James M. Rehg<br />
**Abstract:** <details><summary>原文: </summary>Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates concurrently capturing verbal and non-verbal cues pertinent to social reasoning. Experiments demonstrate the effectiveness of the proposed approach with densely aligned multimodal representations in modeling social interactions. We will release our benchmarks and source code to facilitate further research.</details>
**Abstract_cn:** <details><summary>译文: </summary>了解涉及言语和非言语线索的社交互动对于有效解释社交情境至关重要。然而，大多数关于多模式社交线索的先前研究主要集中在单人行为上，或者依赖于与多方环境中的话语并不紧密一致的整体视觉表征。它们在模拟多方交互的复杂动态方面受到限制。在本文中，我们引入了三个新的具有挑战性的任务来模拟多人之间的细粒度动态：说话目标识别、代词共指解析和提到的玩家预测。我们提供广泛的数据注释来应对社交演绎游戏设置中的这些新挑战。此外，我们提出了一种新颖的多模态基线，通过将视觉特征与其相应的话语同步来利用密集对齐的语言视觉表示。这有助于同时捕捉与社会推理相关的言语和非言语线索。实验证明了所提出的方法在社交交互建模中具有密集对齐的多模态表示的有效性。我们将发布我们的基准测试和源代码以促进进一步的研究。</details>
**PDF:** <http://arxiv.org/pdf/2403.02090v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation**<br />
**Title_cn:** 用于多模态脑肿瘤分割的模态感知和移位混合器<br />
**Authors:** Zhongzhen Huang, Linda Wei, Shaoting Zhang, Xiaofan Zhang<br />
**Abstract:** <details><summary>原文: </summary>Combining images from multi-modalities is beneficial to explore various information in computer vision, especially in the medical domain. As an essential part of clinical diagnosis, multi-modal brain tumor segmentation aims to delineate the malignant entity involving multiple modalities. Although existing methods have shown remarkable performance in the task, the information exchange for cross-scale and high-level representations fusion in spatial and modality are limited in these methods. In this paper, we present a novel Modality Aware and Shift Mixer that integrates intra-modality and inter-modality dependencies of multi-modal images for effective and robust brain tumor segmentation. Specifically, we introduce a Modality-Aware module according to neuroimaging studies for modeling the specific modality pair relationships at low levels, and a Modality-Shift module with specific mosaic patterns is developed to explore the complex relationships across modalities at high levels via the self-attention. Experimentally, we outperform previous state-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021 segmentation) dataset. Further qualitative experiments demonstrate the efficacy and robustness of MASM.</details>
**Abstract_cn:** <details><summary>译文: </summary>组合多模态图像有利于探索计算机视觉中的各种信息，特别是在医学领域。作为临床诊断的重要组成部分，多模态脑肿瘤分割旨在描绘涉及多种模态的恶性实体。尽管现有方法在该任务中表现出了显着的性能，但这些方法在空间和模态方面的跨尺度和高级表示融合的信息交换受到限制。在本文中，我们提出了一种新颖的模态感知和移位混合器，它集成了多模态图像的模态内和模态间依赖性，以实现有效且稳健的脑肿瘤分割。具体来说，我们根据神经影像学研究引入了模态感知模块，用于对低水平的特定模态对关系进行建模，并开发了具有特定马赛克模式的模态转换模块，以通过自学习探索高水平跨模态的复杂关系。注意力。在实验上，我们在公共脑肿瘤分割（BraTS 2021 分割）数据集上优于之前最先进的方法。进一步的定性实验证明了 MASM 的有效性和稳健性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02074v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification**<br />
**Title_cn:** TNF：用于多模式医疗数据分类的三分支神经融合<br />
**Authors:** Tong Zheng, Shusaku Sone, Yoshitaka Ushiku, Yuki Oba, Jiaxin Ma<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a Tri-branch Neural Fusion (TNF) approach designed for classifying multimodal medical images and tabular data. It also introduces two solutions to address the challenge of label inconsistency in multimodal classification. Traditional methods in multi-modality medical data classification often rely on single-label approaches, typically merging features from two distinct input modalities. This becomes problematic when features are mutually exclusive or labels differ across modalities, leading to reduced accuracy. To overcome this, our TNF approach implements a tri-branch framework that manages three separate outputs: one for image modality, another for tabular modality, and a third hybrid output that fuses both image and tabular data. The final decision is made through an ensemble method that integrates likelihoods from all three branches. We validate the effectiveness of TNF through extensive experiments, which illustrate its superiority over traditional fusion and ensemble methods in various convolutional neural networks and transformer-based architectures across multiple datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种三分支神经融合（TNF）方法，旨在对多模态医学图像和表格数据进行分类。它还引入了两种解决方案来解决多模态分类中标签不一致的挑战。多模态医疗数据分类中的传统方法通常依赖于单标签方法，通常合并来自两种不同输入模态的特征。当特征相互排斥或标签在不同模式下不同时，这就会成为问题，从而导致准确性降低。为了克服这个问题，我们的 TNF 方法实现了一个三分支框架，该框架管理三个独立的输出：一个用于图像模态，另一个用于表格模态，以及第三个融合图像和表格数据的混合输出。最终决策是通过集成所有三个分支的可能性的集成方法做出的。我们通过广泛的实验验证了 TNF 的有效性，这说明了它在跨多个数据集的各种卷积神经网络和基于 Transformer 的架构中相对于传统融合和集成方法的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2403.01802v2><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models**<br />
**Title_cn:** NPHardEval4V：多模态大语言模型的动态推理基准<br />
**Authors:** Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Understanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations. Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles, including visual, text, and combined visual and text prompts, on the reasoning abilities of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks, which focus primarily on static evaluations, our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and fine-grained evaluation of the models. We believe that this benchmark can aid in understanding and guide the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf/NPHardEval4V</details>
**Abstract_cn:** <details><summary>译文: </summary>了解多模态大型语言模型 (MLLM) 的推理能力是一个重要的研究领域。在本研究中，我们引入了一个动态基准 NPHardEval4V，旨在解决评估 MLLM 纯推理能力方面的现有差距。我们的基准测试旨在提供一个场所，将图像识别和指令遵循等各种因素的影响与模型的整体性能分开，使我们能够专注于评估它们的推理能力。它是通过将问题的文本描述从 NPHardEval 转换为图像表示来构建的。我们的研究结果揭示了不同模型的推理能力存在显着差异，并强调了 MLLM 与 LLM 相比在推理方面的表现相对较弱。我们还研究了不同提示风格（包括视觉、文本以及视觉和文本组合提示）对 MLLM 推理能力的影响，展示了多模式输入对模型性能的不同影响。与主要侧重于静态评估的传统基准不同，我们的基准将每月更新一次，以防止过度拟合并确保对模型进行更真实、更细粒度的评估。我们相信这个基准可以帮助理解和指导 MLLM 推理能力的进一步发展。基准数据集和代码可在 https://github.com/lizhouf/NPHardEval4V 获取</details>
**PDF:** <http://arxiv.org/pdf/2403.01777v2><br />
**Code:** null<br />

>## **Nerf**
>---
>>**index:** 1<br />
**Title:** **DaReNeRF: Direction-aware Representation for Dynamic Scenes**<br />
**Title_cn:** DaReNeRF：动态场景的方向感知表示<br />
**Authors:** Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu<br />
**Abstract:** <details><summary>原文: </summary>Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了解决建模和重新渲染动态场景的复杂挑战，最新的方法试图使用基于平面的显式表示来简化这些复杂性，克服与神经辐射场 (NeRF) 和隐式表示等方法相关的训练时间慢的问题。然而，事实证明，将 4D 动态场景直接分解为多个基于平面的 2D 表示不足以重新渲染具有复杂运动的高保真场景。作为回应，我们提出了一种新颖的方向感知表示（DaRe）方法，可以从六个不同方向捕获场景动态。这种学习到的表示经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。 DaReNeRF 通过融合来自这些恢复平面的向量来计算每个时空点的特征。将 DaReNeRF 与用于颜色回归的微型 MLP 相结合，并在训练中利用体积渲染，在复杂动态场景的新颖视图合成中产生最先进的性能。值得注意的是，为了解决六个实数和六个虚数方向感知小波系数引入的冗余，我们引入了可训练的掩蔽方法，在不显着性能下降的情况下减轻存储问题。此外，与现有技术相比，DaReNeRF 的训练时间减少了 2 倍，同时提供了卓越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02265v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views**<br />
**Title_cn:** 用于稀疏输入视图的深度引导鲁棒快速点云融合 NeRF<br />
**Authors:** Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song<br />
**Abstract:** <details><summary>原文: </summary>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>具有稀疏输入视图的新颖视图合成对于 AR/VR 和自动驾驶等现实世界应用非常重要。最近的方法已将深度信息集成到 NeRF 中以进行稀疏输入合成，利用深度先验进行几何和空间理解。然而，大多数现有工作往往会忽视深度图中的不准确性，并且时间效率较低。为了解决这些问题，我们提出了一种用于稀疏输入的深度引导的鲁棒且快速的点云融合 NeRF。我们将辐射场视为明确的体素特征网格。为每个输入视图构建点云，使用矩阵和向量在体素网格内进行表征。我们累积每个输入视图的点云来构建整个场景的融合点云。每个体素通过参考整个场景的点云来确定其密度和外观。通过点云融合和体素网格微调，深度值的不准确被细化或被其他视图的不准确取代。此外，我们的方法可以通过有效的向量矩阵分解实现更快的重建和更大的紧凑性。实验结果强调了我们的方法与最先进的基线相比具有卓越的性能和时间效率。</details>
**PDF:** <http://arxiv.org/pdf/2403.02063v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **Encodings for Prediction-based Neural Architecture Search**<br />
**Title_cn:** 基于预测的神经架构搜索的编码<br />
**Authors:** Yash Akhauri, Mohamed S. Abdelfattah<br />
**Abstract:** <details><summary>原文: </summary>Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present our predictor \textbf{FLAN}: \textbf{Fl}ow \textbf{A}ttention for \textbf{N}AS. FLAN integrates critical insights on predictor design, transfer learning, and \textit{unified encodings} to enable more than an order of magnitude cost reduction for training NAS accuracy predictors. Our implementation and encodings for all neural networks are open-sourced at \href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\_nas}.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于预测器的方法极大地增强了神经架构搜索（NAS）优化。这些预测器的功效在很大程度上受到神经网络架构编码方法的影响。虽然传统编码使用描述神经网络图结构的邻接矩阵，但新颖的编码采用了多种方法，从潜在表示的无监督预训练到零成本代理向量。在本文中，我们对三种主要类型的神经编码进行分类和研究：结构型、学习型和基于分数的。此外，我们扩展了这些编码并引入了 \textit{统一编码}，将 NAS 预测器扩展到多个搜索空间。我们的分析基于在 NAS 空间（例如 NASBench-101 (NB101)、NB201、NB301、网络设计空间 (NDS) 和 TransNASBench-101）上对超过 150 万个神经网络架构进行的实验。基于我们的研究，我们提出了我们的预测器 \textbf{FLAN}：\textbf{Fl}ow \textbf{A}ttention for \textbf{N}AS。 FLAN 集成了关于预测器设计、迁移学习和 \textit{统一编码} 的重要见解，可以将训练 NAS 准确性预测器的成本降低一个数量级以上。我们所有神经网络的实现和编码均在 \href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\_nas} 上开源。</details>
**PDF:** <http://arxiv.org/pdf/2403.02484v1><br />
**Code:** <https://github.com/abdelfattah-lab/flan_nas>**<br />
>>**index:** 2<br />
**Title:** **On Latency Predictors for Neural Architecture Search**<br />
**Title_cn:** 神经架构搜索的延迟预测器<br />
**Authors:** Yash Akhauri, Mohamed S. Abdelfattah<br />
**Abstract:** <details><summary>原文: </summary>Efficient deployment of neural networks (NN) requires the co-optimization of accuracy and latency. For example, hardware-aware neural architecture search has been used to automatically find NN architectures that satisfy a latency constraint on a specific hardware device. Central to these search algorithms is a prediction model that is designed to provide a hardware latency estimate for a candidate NN architecture. Recent research has shown that the sample efficiency of these predictive models can be greatly improved through pre-training on some \textit{training} devices with many samples, and then transferring the predictor on the \textit{test} (target) device. Transfer learning and meta-learning methods have been used for this, but often exhibit significant performance variability. Additionally, the evaluation of existing latency predictors has been largely done on hand-crafted training/test device sets, making it difficult to ascertain design features that compose a robust and general latency predictor. To address these issues, we introduce a comprehensive suite of latency prediction tasks obtained in a principled way through automated partitioning of hardware device sets. We then design a general latency predictor to comprehensively study (1) the predictor architecture, (2) NN sample selection methods, (3) hardware device representations, and (4) NN operation encoding schemes. Building on conclusions from our study, we present an end-to-end latency predictor training strategy that outperforms existing methods on 11 out of 12 difficult latency prediction tasks, improving latency prediction by 22.5\% on average, and up to to 87.6\% on the hardest tasks. Focusing on latency prediction, our HW-Aware NAS reports a $5.8\times$ speedup in wall-clock time. Our code is available on \href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\_latency}.</details>
**Abstract_cn:** <details><summary>译文: </summary>神经网络 (NN) 的高效部署需要准确性和延迟的共同优化。例如，硬件感知神经架构搜索已用于自动查找满足特定硬件设备上的延迟约束的神经网络架构。这些搜索算法的核心是预测模型，旨在为候选神经网络架构提供硬件延迟估计。最近的研究表明，通过在一些具有许多样本的 \textit{training} 设备上进行预训练，然后将预测器转移到 \textit{test} （目标）设备上，可以大大提高这些预测模型的样本效率。迁移学习和元学习方法已用于此目的，但通常表现出显着的性能变化。此外，现有延迟预测器的评估主要是在手工制作的训练/测试设备集上完成的，这使得很难确定构成稳健且通用的延迟预测器的设计特征。为了解决这些问题，我们引入了一套全面的延迟预测任务，这些任务是通过硬件设备集的自动分区以原则性的方式获得的。然后，我们设计一个通用延迟预测器来全面研究（1）预测器架构，（2）NN样本选择方法，（3）硬件设备表示，以及（4）NN操作编码方案。基于我们研究的结论，我们提出了一种端到端延迟预测器训练策略，该策略在 12 个困难延迟预测任务中的 11 个上优于现有方法，延迟预测平均提高了 22.5%，最高可达 87.6%在最艰巨的任务上。我们的 HW-Aware NAS 专注于延迟预测，报告显示挂钟时间加速了 5.8 美元\倍$。我们的代码可以在 \href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\_latency} 上找到。</details>
**PDF:** <http://arxiv.org/pdf/2403.02446v1><br />
**Code:** <https://github.com/abdelfattah-lab/nasflat_latency>**<br />
>>**index:** 3<br />
**Title:** **UB-FineNet: Urban Building Fine-grained Classification Network for Open-access Satellite Images**<br />
**Title_cn:** UB-FineNet：开放获取卫星图像的城市建筑细粒度分类网络<br />
**Authors:** Zhiyi He, Wei Yao, Jie Shao, Puzuo Wang<br />
**Abstract:** <details><summary>原文: </summary>Fine classification of city-scale buildings from satellite remote sensing imagery is a crucial research area with significant implications for urban planning, infrastructure development, and population distribution analysis. However, the task faces big challenges due to low-resolution overhead images acquired from high altitude space-borne platforms and the long-tail sample distribution of fine-grained urban building categories, leading to severe class imbalance problem. To address these issues, we propose a deep network approach to fine-grained classification of urban buildings using open-access satellite images. A Denoising Diffusion Probabilistic Model (DDPM) based super-resolution method is first introduced to enhance the spatial resolution of satellite images, which benefits from domain-adaptive knowledge distillation. Then, a new fine-grained classification network with Category Information Balancing Module (CIBM) and Contrastive Supervision (CS) technique is proposed to mitigate the problem of class imbalance and improve the classification robustness and accuracy. Experiments on Hong Kong data set with 11 fine building types revealed promising classification results with a mean Top-1 accuracy of 60.45\%, which is on par with street-view image based approaches. Extensive ablation study shows that CIBM and CS improve Top-1 accuracy by 2.6\% and 3.5\% compared to the baseline method, respectively. And both modules can be easily inserted into other classification networks and similar enhancements have been achieved. Our research contributes to the field of urban analysis by providing a practical solution for fine classification of buildings in challenging mega city scenarios solely using open-access satellite images. The proposed method can serve as a valuable tool for urban planners, aiding in the understanding of economic, industrial, and population distribution.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用卫星遥感图像对城市规模建筑进行精细分类是一个重要的研究领域，对城市规划、基础设施发展和人口分布分析具有重要意义。然而，由于高空星载平台获取的低分辨率俯视图像以及细粒度城市建筑类别的长尾样本分布，导致严重的类别不平衡问题，该任务面临巨大挑战。为了解决这些问题，我们提出了一种深度网络方法，利用开放卫星图像对城市建筑进行细粒度分类。首次引入基于去噪扩散概率模型（DDPM）的超分辨率方法来增强卫星图像的空间分辨率，这得益于领域自适应知识蒸馏。然后，提出了一种具有类别信息平衡模块（CIBM）和对比监督（CS）技术的新细粒度分类网络，以减轻类别不平衡问题并提高分类的鲁棒性和准确性。在具有 11 种精细建筑类型的香港数据集上进行的实验显示了有希望的分类结果，平均 Top-1 准确度为 60.45%，与基于街景图像的方法相当。广泛的消融研究表明，与基线方法相比，CIBM 和 CS 将 Top-1 准确率分别提高了 2.6% 和 3.5%。这两个模块都可以轻松插入到其他分类网络中，并且已经实现了类似的增强。我们的研究仅使用开放卫星图像，为具有挑战性的大城市场景中的建筑物精细分类提供了实用的解决方案，为城市分析领域做出了贡献。所提出的方法可以作为城市规划者的宝贵工具，帮助了解经济、工业和人口分布。</details>
**PDF:** <http://arxiv.org/pdf/2403.02132v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **CSE: Surface Anomaly Detection with Contrastively Selected Embedding**<br />
**Title_cn:** CSE：通过对比选择嵌入进行表面异常检测<br />
**Authors:** Simon Thomine, Hichem Snoussi<br />
**Abstract:** <details><summary>原文: </summary>Detecting surface anomalies of industrial materials poses a significant challenge within a myriad of industrial manufacturing processes. In recent times, various methodologies have emerged, capitalizing on the advantages of employing a network pre-trained on natural images for the extraction of representative features. Subsequently, these features are subjected to processing through a diverse range of techniques including memory banks, normalizing flow, and knowledge distillation, which have exhibited exceptional accuracy. This paper revisits approaches based on pre-trained features by introducing a novel method centered on target-specific embedding. To capture the most representative features of the texture under consideration, we employ a variant of a contrastive training procedure that incorporates both artificially generated defective samples and anomaly-free samples during training. Exploiting the intrinsic properties of surfaces, we derived a meaningful representation from the defect-free samples during training, facilitating a straightforward yet effective calculation of anomaly scores. The experiments conducted on the MVTEC AD and TILDA datasets demonstrate the competitiveness of our approach compared to state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测工业材料的表面异常在众多工业制造过程中提出了重大挑战。近年来，出现了各种方法，利用在自然图像上预先训练的网络来提取代表性特征的优势。随后，这些特征通过各种技术进行处理，包括记忆库、规范化流和知识蒸馏，这些技术表现出了卓越的准确性。本文通过引入一种以目标特定嵌入为中心的新方法，重新审视基于预训练特征的方法。为了捕获所考虑纹理的最具代表性的特征，我们采用了对比训练程序的变体，该程序在训练期间结合了人工生成的有缺陷样本和无异常样本。利用表面的内在特性，我们在训练期间从无缺陷样本中得出了有意义的表示，从而促进了异常分数的简单而有效的计算。在 MVTEC AD 和 TILDA 数据集上进行的实验证明了我们的方法与最先进的方法相比的竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01859v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models**<br />
**Title_cn:** NASH：硬件优化机器学习模型的神经架构搜索<br />
**Authors:** Mengfei Ji, Zaid Al-Ars<br />
**Abstract:** <details><summary>原文: </summary>As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the ImageNet data set. We also integrated this approach into the FINN hardware model synthesis tool to automate the application of our approach and the generation of the hardware model. Results show that using FINN can achieve a maximum throughput of 324.5 fps. In addition, NASH models can also result in a better trade-off between accuracy and hardware resource utilization. The accuracy-hardware (HW) Pareto curve shows that the models with the four NASH versions represent the best trade-offs achieving the highest accuracy for a given HW utilization. The code for our implementation is open-source and publicly available on GitHub at https://github.com/MFJI/NASH.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着机器学习 (ML) 算法在越来越多的应用中部署，这些算法需要在高精度、高吞吐量和低延迟之间实现更好的权衡。本文介绍了 NASH，一种将神经架构搜索应用于机器学习硬件的新颖方法。使用NASH，硬件设计不仅可以实现高吞吐量和低延迟，而且可以实现卓越的准确性性能。我们在本文中提出了 NASH 策略的四个版本，所有版本都比原始模型具有更高的准确性。该策略可以应用于各种卷积神经网络，从众多模型操作中选择特定的模型操作来指导训练过程向更高的精度迈进。实验结果表明，在 ImageNet 数据集上测试时，与非 NASH 版本相比，在 ResNet18 或 ResNet34 上应用 NASH 实现了 top 1 准确率提升高达 3.1%，top 5 准确率提升高达 2.2%。我们还将这种方法集成到 FINN 硬件模型综合工具中，以自动应用我们的方法并生成硬件模型。结果表明，使用 FINN 可以实现 324.5 fps 的最大吞吐量。此外，NASH模型还可以在准确性和硬件资源利用率之间实现更好的权衡。准确性硬件 (HW) Pareto 曲线显示，具有四个 NASH 版本的模型代表了在给定硬件利用率下实现最高准确性的最佳权衡。我们的实现代码是开源的，可在 GitHub 上公开获取：https://github.com/MFJI/NASH。</details>
**PDF:** <http://arxiv.org/pdf/2403.01845v1><br />
**Code:** <https://github.com/mfji/nash>**<br />
>>**index:** 6<br />
**Title:** **Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy Image Compression in JPEG 2000**<br />
**Title_cn:** 用于改进 JPEG 2000 中完全可扩展有损图像压缩的神经网络辅助提升步骤<br />
**Authors:** Xinyue Li, Aous Naman, David Taubman<br />
**Abstract:** <details><summary>原文: </summary>This work proposes to augment the lifting steps of the conventional wavelet transform with additional neural network assisted lifting steps. These additional steps reduce residual redundancy (notably aliasing information) amongst the wavelet subbands, and also improve the visual quality of reconstructed images at reduced resolutions. The proposed approach involves two steps, a high-to-low step followed by a low-to-high step. The high-to-low step suppresses aliasing in the low-pass band by using the detail bands at the same resolution, while the low-to-high step aims to further remove redundancy from detail bands, so as to achieve higher energy compaction. The proposed two lifting steps are trained in an end-to-end fashion; we employ a backward annealing approach to overcome the non-differentiability of the quantization and cost functions during back-propagation. Importantly, the networks employed in this paper are compact and with limited non-linearities, allowing a fully scalable system; one pair of trained network parameters are applied for all levels of decomposition and for all bit-rates of interest. By employing the proposed approach within the JPEG 2000 image coding standard, our method can achieve up to 17.4% average BD bit-rate saving over a wide range of bit-rates, while retaining quality and resolution scalability features of JPEG 2000.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作提出用额外的神经网络辅助提升步骤来增强传统小波变换的提升步骤。这些额外的步骤减少了小波子带之间的残余冗余（特别是混叠信息），并且还提高了在降低的分辨率下重建图像的视觉质量。所提出的方法涉及两个步骤，先是从高到低的步骤，然后是从低到高的步骤。从高到低的步长通过使用相同分辨率的细节频带来抑制低通带中的混叠，而从低到高的步长旨在进一步去除细节频带的冗余，从而实现更高的能量压缩。所提出的两个提升步骤以端到端的方式进行训练；我们采用向后退火方法来克服反向传播过程中量化和成本函数的不可微性。重要的是，本文中使用的网络是紧凑的并且具有有限的非线性，允许完全可扩展的系统；一对经过训练的网络参数适用于所有级别的分解和所有感兴趣的比特率。通过在 JPEG 2000 图像编码标准中采用所提出的方法，我们的方法可以在各种比特率上实现高达 17.4% 的平均 BD 比特率节省，同时保留 JPEG 2000 的质量和分辨率可扩展性特征。</details>
**PDF:** <http://arxiv.org/pdf/2403.01647v1><br />
**Code:** <https://github.com/xinyue-li3/hybrid-lifting-structure>**<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Coronary artery segmentation in non-contrast calcium scoring CT images using deep learning**<br />
**Title_cn:** 使用深度学习进行非造影钙评分 CT 图像中的冠状动脉分割<br />
**Authors:** Mariusz Bujny, Katarzyna Jesionek, Jakub Nalepa, Karol Miszalski-Jamka, Katarzyna Widawka-Żak, Sabina Wolny, Marcin Kostur<br />
**Abstract:** <details><summary>原文: </summary>Precise localization of coronary arteries in Computed Tomography (CT) scans is critical from the perspective of medical assessment of coronary artery disease. Although various methods exist that offer high-quality segmentation of coronary arteries in cardiac contrast-enhanced CT scans, the potential of less invasive, non-contrast CT in this area is still not fully exploited. Since such fine anatomical structures are hardly visible in this type of medical images, the existing methods are characterized by high recall and low precision, and are used mainly for filtering of atherosclerotic plaques in the context of calcium scoring. In this paper, we address this research gap and introduce a deep learning algorithm for segmenting coronary arteries in multi-vendor ECG-gated non-contrast cardiac CT images which benefits from a novel framework for semi-automatic generation of Ground Truth (GT) via image registration. We hypothesize that the proposed GT generation process is much more efficient in this case than manual segmentation, since it allows for a fast generation of large volumes of diverse data, which leads to well-generalizing models. To investigate and thoroughly evaluate the segmentation quality based on such an approach, we propose a novel method for manual mesh-to-image registration, which is used to create our test-GT. The experimental study shows that the trained model has significantly higher accuracy than the GT used for training, and leads to the Dice and clDice metrics close to the interrater variability.</details>
**Abstract_cn:** <details><summary>译文: </summary>从冠状动脉疾病的医学评估角度来看，计算机断层扫描 (CT) 扫描中冠状动脉的精确定位至关重要。尽管存在多种方法可以在心脏造影增强 CT 扫描中提供高质量的冠状动脉分割，但微创、非造影 CT 在该领域的潜力仍未得到充分利用。由于此类精细的解剖结构在此类医学图像中几乎不可见，因此现有方法具有高召回率和低精度的特点，并且主要用于钙评分背景下的动脉粥样硬化斑块的过滤。在本文中，我们解决了这一研究空白，并介绍了一种用于在多供应商心电门控非对比心脏 CT 图像中分割冠状动脉的深度学习算法，该算法受益于通过半自动生成地面真相（GT）的新颖框架图像注册。我们假设所提出的 GT 生成过程在这种情况下比手动分割更有效，因为它允许快速生成大量不同的数据，从而产生良好的泛化模型。为了研究并彻底评估基于这种方法的分割质量，我们提出了一种手动网格到图像配准的新方法，用于创建我们的测试 GT。实验研究表明，训练后的模型比用于训练的 GT 具有显着更高的准确性，并且导致 Dice 和 clDice 指标接近人间变异性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02544v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **When do Convolutional Neural Networks Stop Learning?**<br />
**Title_cn:** 卷积神经网络什么时候停止学习？<br />
**Authors:** Sahan Ahmad, Gabriel Trahan, Aminul Islam<br />
**Abstract:** <details><summary>原文: </summary>Convolutional Neural Networks (CNNs) have demonstrated outstanding performance in computer vision tasks such as image classification, detection, segmentation, and medical image analysis. In general, an arbitrary number of epochs is used to train such neural networks. In a single epoch, the entire training data -- divided by batch size -- are fed to the network. In practice, validation error with training loss is used to estimate the neural network's generalization, which indicates the optimal learning capacity of the network. Current practice is to stop training when the training loss decreases and the gap between training and validation error increases (i.e., the generalization gap) to avoid overfitting. However, this is a trial-and-error-based approach which raises a critical question: Is it possible to estimate when neural networks stop learning based on training data? This research work introduces a hypothesis that analyzes the data variation across all the layers of a CNN variant to anticipate its near-optimal learning capacity. In the training phase, we use our hypothesis to anticipate the near-optimal learning capacity of a CNN variant without using any validation data. Our hypothesis can be deployed as a plug-and-play to any existing CNN variant without introducing additional trainable parameters to the network. We test our hypothesis on six different CNN variants and three different general image datasets (CIFAR10, CIFAR100, and SVHN). The result based on these CNN variants and datasets shows that our hypothesis saves 58.49\% of computational time (on average) in training. We further conduct our hypothesis on ten medical image datasets and compared with the MedMNIST-V2 benchmark. Based on our experimental result, we save $\approx$ 44.1\% of computational time without losing accuracy against the MedMNIST-V2 benchmark.</details>
**Abstract_cn:** <details><summary>译文: </summary>卷积神经网络（CNN）在图像分类、检测、分割和医学图像分析等计算机视觉任务中表现出了出色的性能。一般来说，使用任意数量的纪元来训练此类神经网络。在一个时期内，整个训练数据（除以批量大小）都会被输入到网络中。在实践中，带有训练损失的验证误差用于估计神经网络的泛化能力，这表明网络的最佳学习能力。目前的做法是当训练损失减少并且训练和验证误差之间的差距增大（即泛化差距）时停止训练，以避免过度拟合。然而，这是一种基于试错的方法，提出了一个关键问题：是否可以根据训练数据来估计神经网络何时停止学习？这项研究工作引入了一个假设，该假设分析 CNN 变体所有层的数据变化，以预测其接近最佳的学习能力。在训练阶段，我们使用我们的假设来预测 CNN 变体的接近最优的学习能力，而不使用任何验证数据。我们的假设可以作为即插即用的方式部署到任何现有的 CNN 变体，而无需向网络引入额外的可训练参数。我们在六种不同的 CNN 变体和三个不同的通用图像数据集（CIFAR10、CIFAR100 和 SVHN）上测试我们的假设。基于这些 CNN 变体和数据集的结果表明，我们的假设在训练中节省了 58.49% 的计算时间（平均）。我们进一步对十个医学图像数据集进行假设，并与 MedMNIST-V2 基准进行比较。根据我们的实验结果，我们节省了 $\approx$ 44.1\% 的计算时间，并且相对于 MedMNIST-V2 基准没有损失准确性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02473v1><br />
**Code:** <https://github.com/paperunderreviewdeeplearning/optimization>**<br />
>>**index:** 3<br />
**Title:** **Anatomically Constrained Tractography of the Fetal Brain**<br />
**Title_cn:** 胎儿大脑的解剖学约束纤维束成像<br />
**Authors:** Camilo Calixto, Camilo Jaimes, Matheus D. Soldatelli, Simon K. Warfield, Ali Gholipour, Davood Karimi<br />
**Abstract:** <details><summary>原文: </summary>Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero. An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment. However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results. They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts. In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space. We develop a deep learning method to compute the segmentation automatically. Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tractography results. It enables the reconstruction of highly curved tracts such as optic radiations. Importantly, our method infers the tissue segmentation and streamline propagation direction from a diffusion tensor fit to the dMRI data, making it applicable to routine fetal dMRI scans. The proposed method can lead to significant improvements in the accuracy and reproducibility of quantitative assessment of the fetal brain with dMRI.</details>
**Abstract_cn:** <details><summary>译文: </summary>弥散加权磁共振成像 (dMRI) 越来越多地用于研究子宫内胎儿的大脑。 dMRI 实现的一项重要计算是流线纤维束成像，它具有独特的应用，例如大脑白质的纤维束特异性分析和结构连接评估。然而，由于胎儿 dMRI 数据质量低以及纤维束成像的挑战性，现有方法往往会产生高度不准确的结果。它们产生许多错误的流线，同时无法重建构成主要白质束的流线。在本文中，我们主张基于直接在 dMRI 空间中准确分割胎儿脑组织的解剖学约束纤维束成像。我们开发了一种深度学习方法来自动计算分割。独立测试数据的实验表明，该方法可以准确分割胎儿脑组织并极大地改善纤维束成像结果。它能够重建高度弯曲的束，例如视辐射。重要的是，我们的方法从适合 dMRI 数据的扩散张量推断出组织分割和流线型传播方向，使其适用于常规胎儿 dMRI 扫描。所提出的方法可以显着提高 dMRI 胎儿大脑定量评估的准确性和可重复性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02444v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function**<br />
**Title_cn:** NiNformer：具有令牌混合生成门控功能的网络变压器中的网络<br />
**Authors:** Abdullah Nazhat Abdullah, Tarkan Aydin<br />
**Abstract:** <details><summary>原文: </summary>The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an alternative to the standard ViT block that reduces the compute burdens by replacing the normal Attention layers with a Network in Network structure that enhances the static approach of the MLP Mixer with a dynamic system of learning an element-wise gating function by a token mixing process. Extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the vision domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>注意力机制是 Transformer 架构的主要组成部分，自推出以来，它在跨越多个领域和多个任务的深度学习领域取得了重大进展。注意力机制在计算机视觉中被用作视觉变压器ViT，其用途已扩展到视觉领域的许多任务，例如分类、分割、对象检测和图像生成。虽然这种机制非常具有表现力和能力，但它的缺点是计算量大，并且需要相当大的数据集才能有效优化。为了解决这些缺点，文献中提出了许多设计来减少计算负担并减轻数据大小要求。视觉领域中此类尝试的例子有 MLP-Mixer、Conv-Mixer、Perciver-IO 等等。本文介绍了一种新的计算块作为标准 ViT 块的替代品，通过用网络结构中的网络替换普通的注意力层来减少计算负担，该网络结构通过学习元素的动态系统增强了 MLP 混合器的静态方法 -通过令牌混合过程实现明智的门控功能。大量实验表明，所提出的设计在视觉领域图像分类任务中应用的多个数据集上提供了比基线架构更好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02411v1><br />
**Code:** <https://github.com/Abdullah-88/NiNformer>**<br />
>>**index:** 5<br />
**Title:** **Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis**<br />
**Title_cn:** 包装中的品牌可见度：用于徽标检测、显着图预测和徽标放置分析的深度学习方法<br />
**Authors:** Alireza Hosseini, Kiana Hooshanfar, Pouria Omrani, Reza Toosi, Ramin Toosi, Zahra Ebrahimian, Mohammad Ali Akhaee<br />
**Abstract:** <details><summary>原文: </summary>In the highly competitive area of product marketing, the visibility of brand logos on packaging plays a crucial role in shaping consumer perception, directly influencing the success of the product. This paper introduces a comprehensive framework to measure the brand logo's attention on a packaging design. The proposed method consists of three steps. The first step leverages YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500 and LogoDet-3K. The second step involves modeling the user's visual attention with a novel saliency prediction model tailored for the packaging context. The proposed saliency model combines the visual elements with text maps employing a transformers-based architecture to predict user attention maps. In the third step, by integrating logo detection with a saliency map generation, the framework provides a comprehensive brand attention score. The effectiveness of the proposed method is assessed module by module, ensuring a thorough evaluation of each component. Comparing logo detection and saliency map prediction with state-of-the-art models shows the superiority of the proposed methods. To investigate the robustness of the proposed brand attention score, we collected a unique dataset to examine previous psychophysical hypotheses related to brand visibility. the results show that the brand attention score is in line with all previous studies. Also, we introduced seven new hypotheses to check the impact of position, orientation, presence of person, and other visual elements on brand attention. This research marks a significant stride in the intersection of cognitive psychology, computer vision, and marketing, paving the way for advanced, consumer-centric packaging designs.</details>
**Abstract_cn:** <details><summary>译文: </summary>在竞争激烈的产品营销领域，包装上品牌标志的可见度对于塑造消费者认知起着至关重要的作用，直接影响产品的成功。本文介绍了一个综合框架来衡量品牌标志对包装设计的关注度。所提出的方法包括三个步骤。第一步利用 YOLOv8 在重要数据集 FoodLogoDet-1500 和 LogoDet-3K 中进行精确的徽标检测。第二步涉及使用针对包装环境量身定制的新颖显着性预测模型来对用户的视觉注意力进行建模。所提出的显着性模型将视觉元素与文本图结合起来，采用基于变压器的架构来预测用户注意力图。第三步，通过将徽标检测与显着图生成相结合，该框架提供了全面的品牌关注度得分。所提出方法的有效性是逐个模块评估的，确保对每个组件进行彻底评估。将徽标检测和显着性图预测与最先进的模型进行比较，显示了所提出方法的优越性。为了调查所提出的品牌注意力评分的稳健性，我们收集了一个独特的数据集来检验之前与品牌知名度相关的心理物理学假设。结果表明，品牌关注度得分与之前的所有研究结果一致。此外，我们引入了七个新假设来检查位置、方向、人物存在和其他视觉元素对品牌注意力的影响。这项研究标志着认知心理学、计算机视觉和营销交叉领域的重大进步，为先进的、以消费者为中心的包装设计铺平了道路。</details>
**PDF:** <http://arxiv.org/pdf/2403.02336v1><br />
**Code:** <https://github.com/Arhosseini77/Brand_Attention>**<br />
>>**index:** 6<br />
**Title:** **RegionGPT: Towards Region Understanding Vision Language Model**<br />
**Title_cn:** RegionGPT：迈向区域理解视觉语言模型<br />
**Authors:** Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu<br />
**Abstract:** <details><summary>原文: </summary>Vision language models (VLMs) have experienced rapid advancements through the integration of large language models (LLMs) with image-text pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model's versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, reasoning, object classification, and referring expressions comprehension.</details>
**Abstract_cn:** <details><summary>译文: </summary>通过将大型语言模型 (LLM) 与图像文本对集成，视觉语言模型 (VLM) 经历了快速的进步，但由于视觉编码器的空间意识有限以及粗略的使用，它们在详细的区域视觉理解方面遇到了困难。缺乏详细的、特定于区域的说明的粒度训练数据。为了解决这个问题，我们引入了 RegionGPT（简称 RGPT），这是一种专为复杂区域级字幕和理解而设计的新颖框架。 RGPT 通过对 VLM 中现有视觉编码器进行简单而有效的修改，增强了区域表示的空间意识。我们通过在训练和推理阶段集成任务引导的指令提示，进一步提高需要特定输出范围的任务的性能，同时保持模型对通用任务的多功能性。此外，我们开发了一个自动区域标题数据生成管道，通过详细的区域级标题丰富了训练集。我们证明了通用的 RGPT 模型可以有效地应用并显着提高一系列区域级任务的性能，包括但不限于复杂的区域描述、推理、对象分类和指代表达理解。</details>
**PDF:** <http://arxiv.org/pdf/2403.02330v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**<br />
**Title_cn:** 对比区域指导：无需训练即可改善视觉语言模型的基础<br />
**Authors:** David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal<br />
**Abstract:** <details><summary>原文: </summary>Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a "visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship reasoning. We also show CRG's applicability to spatial reasoning, with 10% improvement on What'sUp, as well as to compositional generalization -- improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe -- and to image-text alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy. Our analysis explores alternative masking strategies for CRG, quantifies CRG's probability shift, and evaluates the role of region guidance strength, empirically validating CRG's design choices.</details>
**Abstract_cn:** <details><summary>译文: </summary>突出显示图像中特别相关的区域可以引导模型更密切地关注这些感兴趣的区域，从而提高视觉语言模型 (VLM) 在各种视觉语言 (VL) 任务中的性能。例如，VLM 可以被给予“视觉提示”，其中诸如边界框之类的视觉标记描绘出关键图像区域。然而，当前可以结合视觉引导的 VLM 要么是专有的且昂贵，要么需要对包含视觉提示的精选数据进行昂贵的培训。我们引入了对比区域引导（CRG），这是一种无需培训的引导方法，使开源 VLM 能够响应视觉提示。 CRG 对比有视觉提示和没有视觉提示时生成的模型输出，排除模型在没有生成正确答案所需信息（即模型的先验）的情况下回答时所揭示的偏差。 CRG 在各种 VL 任务中实现了实质性改进：当提供区域注释时，CRG 在 ViP-Bench 上将绝对准确度提高了 11.1%，ViP-Bench 是六种不同的基于区域的任务的集合，例如识别、数学和对象关系推理。我们还展示了 CRG 对空间推理的适用性，与 What'sUp 相比提高了 10%，以及对组合泛化的适用性（在 SugarCrepe 的两个具有挑战性的分割上将准确性提高了 11.5% 和 7.5%），以及生成的图像文本对齐图像，我们在 SeeTRUE 上提高了高达 8.4 AUROC 和 6.8 F1 点。当参考区域不存在时，CRG 允许我们在参考表达理解和短语基础基准（如 RefCOCO/+/g 和 Flickr30K Entities）中对建议区域进行重新排名，平均准确度提高 3.2%。我们的分析探索了 CRG 的替代掩蔽策略，量化了 CRG 的概率变化，并评估了区域引导强度的作用，从经验上验证了 CRG 的设计选择。</details>
**PDF:** <http://arxiv.org/pdf/2403.02325v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications to Cardiac MRI Segmentation**<br />
**Title_cn:** 哈密​​顿蒙特卡罗贝叶斯不确定性估计：在心脏 MRI 分割中的应用<br />
**Authors:** Yidong Zhao, Joao Tourais, Iain Pierce, Christian Nitsche, Thomas A. Treibel, Sebastian Weingärtner, Artur M. Schweidtmann, Qian Tao<br />
**Abstract:** <details><summary>原文: </summary>Deep learning (DL)-based methods have achieved state-of-the-art performance for a wide range of medical image segmentation tasks. Nevertheless, recent studies show that deep neural networks (DNNs) can be miscalibrated and overconfident, leading to "silent failures" that are risky} for clinical applications. Bayesian statistics provide an intuitive approach to DL failure detection, based on posterior probability estimation. However, Bayesian DL, and in particular the posterior estimation, is intractable for large medical image segmentation DNNs. To tackle this challenge, we propose a Bayesian learning framework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC computation, we further propose a cyclical annealing strategy, which captures both local and global geometries of the posterior distribution, enabling highly efficient Bayesian DNN training with the same computational budget requirements as training a single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along with the segmentation uncertainty. We evaluate the proposed HMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation, using in-domain steady-state free precession (SSFP) cine images as well as out-of-domain datasets of quantitative $T_1$ and $T_2$ mapping.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于深度学习 (DL) 的方法已在各种医学图像分割任务中实现了最先进的性能。然而，最近的研究表明，深度神经网络（DNN）可能会被错误校准和过度自信，从而导致“无声的失败”，这对临床应用来说是有风险的。贝叶斯统计提供了一种基于后验概率估计的直观的深度学习故障检测方法。然而，贝叶斯深度学习，特别是后验估计，对于大型医学图像分割 DNN 来说很棘手。为了应对这一挑战，我们提出了一种由哈密顿蒙特卡罗 (HMC) 构建的贝叶斯学习框架，并通过冷后验 (CP) 进行调整以适应医疗数据增强，称为 HMC-CP。对于 HMC 计算，我们进一步提出了一种循环退火策略，该策略捕获后验分布的局部和全局几何形状，从而实现高效的贝叶斯 DNN 训练，其计算预算要求与训练单个 DNN 相同。由此产生的贝叶斯 DNN 输出整体分割以及分割不确定性。我们使用域内稳态自由进动 (SSFP) 电影图像以及定量 $T_1$ 和 $T_2$ 的域外数据集，在心脏磁共振图像 (MRI) 分割上广泛评估所提出的 HMC-CP映射。</details>
**PDF:** <http://arxiv.org/pdf/2403.02311v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures**<br />
**Title_cn:** Vision-RWKV：使用类似 RWKV 的架构实现高效且可扩展的视觉感知<br />
**Authors:** Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang<br />
**Abstract:** <details><summary>原文: </summary>Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations in image classification demonstrate that VRWKV matches ViT's classification performance with significantly faster speeds and lower memory usage. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at \url{https://github.com/OpenGVLab/Vision-RWKV}.</details>
**Abstract_cn:** <details><summary>译文: </summary>Transformer 彻底改变了计算机视觉和自然语言处理，但其高计算复杂性限制了其在高分辨率图像处理和长上下文分析中的应用。本文介绍了 Vision-RWKV（VRWKV），该模型改编自 NLP 领域使用的 RWKV 模型，并针对视觉任务进行了必要的修改。与 Vision Transformer (ViT) 类似，我们的模型旨在有效处理稀疏输入并展示强大的全局处理能力，同时还可以有效扩展，容纳大规模参数和广泛的数据集。其独特的优势在于降低了空间聚合复杂性，这使得它非常擅长无缝处理高分辨率图像，从而消除了窗口操作的必要性。我们对图像分类的评估表明，VRWKV 的分类性能与 ViT 的分类性能相当，速度明显更快，内存使用量更低。在密集的预测任务中，它的性能优于基于窗口的模型，保持了相当的速度。这些结果凸显了 VRWKV 作为视觉感知任务的更有效替代方案的潜力。代码发布于 \url{https://github.com/OpenGVLab/Vision-RWKV}。</details>
**PDF:** <http://arxiv.org/pdf/2403.02308v1><br />
**Code:** <https://github.com/OpenGVLab/Vision-RWKV>**<br />
>>**index:** 10<br />
**Title:** **Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection**<br />
**Title_cn:** 通过群体水平背景利用组内变异进行病理检测<br />
**Authors:** P. Bilha Githinji, Xi Yuan, Zhenglin Chen, Ijaz Gul, Dingqi Shang, Wen Liang, Jianming Deng, Dan Zeng, Dongmei yu, Chenggang Yan, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection convolutional models. Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images. This study introduces the notion of a population-level context for pathology detection and employs a graph theoretic approach to model and incorporate it into the latent code of an autoencoder via a refinement module we term PopuSense. PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the convolutional model might miss or smooth out. Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input. Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model.</details>
**Abstract_cn:** <details><summary>译文: </summary>实现健康样本和病理样本分布之间的足够可分离性是病理检测卷积模型的关键障碍。此外，这些模型对基于对比度的图像表现出偏差，而在基于纹理的医学图像上的性能下降。这项研究引入了用于病理检测的群体水平背景的概念，并采用图论方法进行建模，并通过我们称为 PopuSense 的细化模块将其合并到自动编码器的潜在代码中。 PopuSense 试图捕获生物医学数据中固有的额外组内变化，而卷积模型的局部或全局背景可能会错过或消除这些变化。基于对比度和基于纹理的图像的实验，在最小的适应下，遇到了对基于强度的输入的现有偏好。尽管如此，PopuSense 展示了基于对比度的图像中改进的可分离性，为细化模型学习的表示提供了额外的途径。</details>
**PDF:** <http://arxiv.org/pdf/2403.02307v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **REAL-Colon: A dataset for developing real-world AI applications in colonoscopy**<br />
**Title_cn:** REAL-Colon：用于开发结肠镜检查中真实人工智能应用的数据集<br />
**Authors:** Carlo Biffi, Giulio Antonelli, Sebastian Bernhofer, Cesare Hassan, Daizen Hirata, Mineo Iwatate, Andreas Maieron, Pietro Salvagnini, Andrea Cherubini<br />
**Abstract:** <details><summary>原文: </summary>Detection and diagnosis of colon polyps are key to preventing colorectal cancer. Recent evidence suggests that AI-based computer-aided detection (CADe) and computer-aided diagnosis (CADx) systems can enhance endoscopists' performance and boost colonoscopy effectiveness. However, most available public datasets primarily consist of still images or video clips, often at a down-sampled resolution, and do not accurately represent real-world colonoscopy procedures. We introduce the REAL-Colon (Real-world multi-center Endoscopy Annotated video Library) dataset: a compilation of 2.7M native video frames from sixty full-resolution, real-world colonoscopy recordings across multiple centers. The dataset contains 350k bounding-box annotations, each created under the supervision of expert gastroenterologists. Comprehensive patient clinical data, colonoscopy acquisition information, and polyp histopathological information are also included in each video. With its unprecedented size, quality, and heterogeneity, the REAL-Colon dataset is a unique resource for researchers and developers aiming to advance AI research in colonoscopy. Its openness and transparency facilitate rigorous and reproducible research, fostering the development and benchmarking of more accurate and reliable colonoscopy-related algorithms and models.</details>
**Abstract_cn:** <details><summary>译文: </summary>结肠息肉的检测和诊断是预防结直肠癌的关键。最近的证据表明，基于人工智能的计算机辅助检测（CADe）和计算机辅助诊断（CADx）系统可以提高内窥镜医生的表现并提高结肠镜检查的有效性。然而，大多数可用的公共数据集主要由静态图像或视频剪辑组成，通常采用下采样分辨率，并且不能准确代表现实世界的结肠镜检查程序。我们介绍 REAL-Colon（真实世界多中心内窥镜注释视频库）数据集：来自多个中心的 60 个全分辨率真实结肠镜检查记录的 270 万个本地视频帧的汇编。该数据集包含 35 万个边界框注释，每个注释都是在胃肠病专家的监督下创建的。每个视频中还包含全面的患者临床数据、结肠镜检查采集信息和息肉组织病理学信息。凭借其前所未有的规模、质量和异质性，REAL-Colon 数据集对于旨在推进结肠镜检查人工智能研究的研究人员和开发人员来说是一种独特的资源。其开放性和透明度促进了严格和可重复的研究，促进了更准确和可靠的结肠镜检查相关算法和模型的开发和基准测试。</details>
**PDF:** <http://arxiv.org/pdf/2403.02163v1><br />
**Code:** <https://github.com/cosmoimd/real-colon-dataset>**<br />
>>**index:** 12<br />
**Title:** **MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection**<br />
**Title_cn:** MiM-ISTD：用于高效红外小目标检测的 Mamba-in-Mamba<br />
**Authors:** Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu, Jieping Ye, Nenghai Yu<br />
**Abstract:** <details><summary>原文: </summary>Thanks to the development of basic models, infrared small target detection (ISTD) algorithms have made significant progress. Specifically, the structures combining convolutional networks with transformers can well extract both local and global features. At the same time, they also inherit defects from the basic model, e.g., the quadratic computational complexity of transformers, which impacts efficiency. Inspired by a recent basic model with linear complexity for long-distance modeling, called Mamba, we explore the potential of this state space model in ISTD in this paper. However, direct application is unsuitable since local features, which are critical to detecting small targets, cannot be fully exploited. Instead, we tailor a Mamba-in-Mamba (MiM-ISTD) structure for efficient ISTD. For example, we treat the local patches as "visual sentences" and further decompose them into sub-patches as "visual words" to further explore the locality. The interactions among each word in a given visual sentence will be calculated with negligible computational costs. By aggregating the word and sentence features, the representation ability of MiM-ISTD can be significantly bolstered. Experiments on NUAA-SIRST and IRSTD-1k prove the superior accuracy and efficiency of our method. Specifically, MiM-ISTD is $10 \times$ faster than the SOTA and reduces GPU memory usage by 73.4$\%$ per $2048 \times 2048$ image during inference, overcoming the computation$\&$memory constraints on performing Mamba-based understanding on high-resolution infrared images.Source code is available at https://github.com/txchen-USTC/MiM-ISTD.</details>
**Abstract_cn:** <details><summary>译文: </summary>得益于基础模型的发展，红外小目标检测（ISTD）算法取得了重大进展。具体来说，卷积网络与变压器相结合的结构可以很好地提取局部和全局特征。同时，它们也继承了基本模型的缺陷，例如变压器的二次计算复杂性，这影响了效率。受到最近用于长距离建模的线性复杂性基本模型 Mamba 的启发，我们在本文中探讨了这种状态空间模型在 ISTD 中的潜力。然而，直接应用是不合适的，因为对于检测小目标至关重要的局部特征无法被充分利用。相反，我们定制了 Mamba-in-Mamba (MiM-ISTD) 结构以实现高效的 ISTD。例如，我们将局部补丁视为“视觉句子”，并进一步将其分解为子补丁作为“视觉单词”，以进一步探索局部性。给定视觉句子中每个单词之间的交互将以可忽略的计算成本进行计算。通过聚合单词和句子特征，可以显着增强 MiM-ISTD 的表示能力。在 NUAA-SIRST 和 IRSTD-1k 上的实验证明了我们方法的卓越准确性和效率。具体来说，MiM-ISTD 比 SOTA 快 $10 \times$，并且在推理期间每 $2048 \times 2048$ 图像减少 GPU 内存使用 73.4$\%$，克服了执行基于 Mamba 的理解时的计算 $\&$内存限制高分辨率红外图像。源代码可在 https://github.com/txchen-USTC/MiM-ISTD 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02148v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Self-Supervised Facial Representation Learning with Facial Region Awareness**<br />
**Title_cn:** 具有面部区域意识的自监督面部表征学习<br />
**Authors:** Zheng Gao, Ioannis Patras<br />
**Abstract:** <details><summary>原文: </summary>Self-supervised pre-training has been proved to be effective in learning transferable representations that benefit various visual tasks. This paper asks this question: can self-supervised pre-training learn general facial representations for various facial analysis tasks? Recent efforts toward this goal are limited to treating each face image as a whole, i.e., learning consistent facial representations at the image-level, which overlooks the consistency of local facial representations (i.e., facial regions like eyes, nose, etc). In this work, we make a first attempt to propose a novel self-supervised facial representation learning framework to learn consistent global and local facial representations, Facial Region Awareness (FRA). Specifically, we explicitly enforce the consistency of facial regions by matching the local facial representations across views, which are extracted with learned heatmaps highlighting the facial regions. Inspired by the mask prediction in supervised semantic segmentation, we obtain the heatmaps via cosine similarity between the per-pixel projection of feature maps and facial mask embeddings computed from learnable positional embeddings, which leverage the attention mechanism to globally look up the facial image for facial regions. To learn such heatmaps, we formulate the learning of facial mask embeddings as a deep clustering problem by assigning the pixel features from the feature maps to them. The transfer learning results on facial classification and regression tasks show that our FRA outperforms previous pre-trained models and more importantly, using ResNet as the unified backbone for various tasks, our FRA achieves comparable or even better performance compared with SOTA methods in facial analysis tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>事实证明，自监督预训练在学习有利于各种视觉任务的可转移表示方面是有效的。本文提出了这个问题：自监督预训练能否学习各种面部分析任务的一般面部表征？最近实现这一目标的努力仅限于将每个面部图像视为一个整体，即在图像级别学习一致的面部表征，这忽略了局部面部表征（即眼睛、鼻子等面部区域）的一致性。在这项工作中，我们首次尝试提出一种新颖的自监督面部表征学习框架来学习一致的全局和局部面部表征，即面部区域感知（FRA）。具体来说，我们通过匹配视图中的局部面部表示来明确强制面部区域的一致性，这些视图是通过学习的突出面部区域的热图提取的。受监督语义分割中掩模预测的启发，我们通过特征图的每像素投影和从可学习的位置嵌入计算出的面部掩模嵌入之间的余弦相似性来获得热图，它利用注意力机制来全局查找面部图像中的面部地区。为了学习这样的热图，我们通过将特征图中的像素特征分配给它们，将面部掩模嵌入的学习公式化为深度聚类问题。面部分类和回归任务上的迁移学习结果表明，我们的 FRA 优于之前的预训练模型，更重要的是，使用 ResNet 作为各种任务的统一主干，我们的 FRA 在面部分析任务中与 SOTA 方法相比取得了相当甚至更好的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02138v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **LOCR: Location-Guided Transformer for Optical Character Recognition**<br />
**Title_cn:** LOCR：用于光学字符识别的位置引导变压器<br />
**Authors:** Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-Sen Zhong<br />
**Abstract:** <details><summary>原文: </summary>Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset, from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in OOD marketing documents. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human.</details>
**Abstract_cn:** <details><summary>译文: </summary>学术文档充满了文本、方程式、表格和图形，需要全面理解才能进行准确的光学字符识别 (OCR)。虽然端到端 OCR 方法比基于布局的方法提供了更高的准确性，但它们通常会解决严重的重复问题，尤其是域外 (OOD) 文档中的复杂布局。为了解决这个问题，我们提出了 LOCR，一种在自回归期间将位置引导集成到变压器架构中的模型。我们在一个数据集上训练模型，该数据集包含来自 125K 学术文档页面的超过 7700 万个文本位置对，其中包括单词、表格和数学符号的边界框。 LOCR 熟练地处理各种格式元素并以 Markdown 语言生成内容。根据编辑距离、BLEU、METEOR 和 F-measure 进行测量，它优于我们从 arXiv 构建的测试集中的所有现有方法。LOCR 还将 arXiv 数据集中的页面重复频率从 4.4% 降低到 0.5%，从 13.2% 降低到 1.3 OOD 量子物理文档中的百分比以及 OOD 营销文档中的 8.1% 至 1.8%。此外，LOCR还具有交互式OCR模式，通过人工的一些位置提示，可以方便地生成复杂的文档。</details>
**PDF:** <http://arxiv.org/pdf/2403.02127v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT**<br />
**Title_cn:** VTG-GPT：使用 GPT 的免调整零镜头视频临时接地<br />
**Authors:** Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, Sidan Du<br />
**Abstract:** <details><summary>原文: </summary>Video temporal grounding (VTG) aims to locate specific temporal segments from an untrimmed video based on a linguistic query. Most existing VTG models are trained on extensive annotated video-text pairs, a process that not only introduces human biases from the queries but also incurs significant computational costs. To tackle these challenges, we propose VTG-GPT, a GPT-based method for zero-shot VTG without training or fine-tuning. To reduce prejudice in the original query, we employ Baichuan2 to generate debiased queries. To lessen redundant information in videos, we apply MiniGPT-v2 to transform visual content into more precise captions. Finally, we devise the proposal generator and post-processing to produce accurate segments from debiased queries and image captions. Extensive experiments demonstrate that VTG-GPT significantly outperforms SOTA methods in zero-shot settings and surpasses unsupervised approaches. More notably, it achieves competitive performance comparable to supervised methods. The code is available on https://github.com/YoucanBaby/VTG-GPT</details>
**Abstract_cn:** <details><summary>译文: </summary>视频时间接地（VTG）旨在根据语言查询从未修剪的视频中定位特定的时间片段。大多数现有的 VTG 模型都是在大量带注释的视频文本对上进行训练的，这个过程不仅会引入查询中的人类偏见，还会产生大量的计算成本。为了应对这些挑战，我们提出了 VTG-GPT，这是一种基于 GPT 的零样本 VTG 方法，无需训练或微调。为了减少原始查询中的偏见，我们使用Baichuan2来生成去偏见查询。为了减少视频中的冗余信息，我们应用 MiniGPT-v2 将视觉内容转换为更精确的字幕。最后，我们设计了提案生成器和后处理，以根据去偏查询和图像标题生成准确的片段。大量实验表明，VTG-GPT 在零样本设置中显着优于 SOTA 方法，并超越无监督方法。更值得注意的是，它实现了与监督方法相当的竞争性能。该代码可在 https://github.com/YoucanBaby/VTG-GPT 上获取</details>
**PDF:** <http://arxiv.org/pdf/2403.02076v1><br />
**Code:** <https://github.com/YoucanBaby/VTG-GPT>**<br />
>>**index:** 16<br />
**Title:** **DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction**<br />
**Title_cn:** DiffMOT：具有非线性预测的基于扩散的实时多目标跟踪器<br />
**Authors:** Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng<br />
**Abstract:** <details><summary>原文: </summary>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) with Kalman Filter motion prediction works well in pedestrian-dominant scenarios but falls short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much less sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with 63.4 and 76.2 in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.</details>
**Abstract_cn:** <details><summary>译文: </summary>在多对象跟踪中，对象经常表现出加速和减速的非线性运动，并且方向变化不规则。采用卡尔曼滤波器运动预测的检测跟踪 (TBD) 在行人占主导地位的场景中效果很好，但在多个对象同时执行非线性和多样化运动的复杂情况下效果不佳。为了解决复杂的非线性运动，我们提出了一种基于实时扩散的 MOT 方法，名为 DiffMOT。具体来说，对于运动预测器组件，我们提出了一种新颖的基于解耦扩散的运动预测器（D MP）。它对数据呈现的各种运动的整个分布进行建模。它还根据个体的历史运动信息来预测个体对象的运动调节。此外，它还以更少的采样步骤优化了扩散过程。作为 MOT 跟踪器，DiffMOT 的实时速度为 22.7FPS，并且在 HOTA 指标方面也优于 DanceTrack 和 SportsMOT 数据集的最新水平，分别为 63.4 和 76.2。据我们所知，DiffMOT 是第一个将扩散概率模型引入 MOT 来解决非线性运动预测的模型。</details>
**PDF:** <http://arxiv.org/pdf/2403.02075v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **HyperPredict: Estimating Hyperparameter Effects for Instance-Specific Regularization in Deformable Image Registration**<br />
**Title_cn:** HyperPredict：估计可变形图像配准中实例特定正则化的超参数效应<br />
**Authors:** Aisha L. Shuaibu, Ivor J. A. Simpson<br />
**Abstract:** <details><summary>原文: </summary>Methods for medical image registration infer geometric transformations that align pairs/groups of images by maximising an image similarity metric. This problem is ill-posed as several solutions may have equivalent likelihoods, also optimising purely for image similarity can yield implausible transformations. For these reasons regularization terms are essential to obtain meaningful registration results. However, this requires the introduction of at least one hyperparameter often termed {\lambda}, that serves as a tradeoff between loss terms. In some situations, the quality of the estimated transformation greatly depends on hyperparameter choice, and different choices may be required depending on the characteristics of the data. Analyzing the effect of these hyperparameters requires labelled data, which is not commonly available at test-time. In this paper, we propose a method for evaluating the influence of hyperparameters and subsequently selecting an optimal value for given image pairs. Our approach which we call HyperPredict, implements a Multi-Layer Perceptron that learns the effect of selecting particular hyperparameters for registering an image pair by predicting the resulting segmentation overlap and measure of deformation smoothness. This approach enables us to select optimal hyperparameters at test time without requiring labelled data, removing the need for a one-size-fits-all cross-validation approach. Furthermore, the criteria used to define optimal hyperparameter is flexible post-training, allowing us to efficiently choose specific properties. We evaluate our proposed method on the OASIS brain MR dataset using a recent deep learning approach(cLapIRN) and an algorithmic method(Niftyreg). Our results demonstrate good performance in predicting the effects of regularization hyperparameters and highlight the benefits of our image-pair specific approach to hyperparameter selection.</details>
**Abstract_cn:** <details><summary>译文: </summary>医学图像配准方法推断通过最大化图像相似性度量来对齐图像对/组的几何变换。这个问题是不适定的，因为几个解决方案可能具有相同的可能性，而且纯粹针对图像相似性进行优化可能会产生令人难以置信的变换。由于这些原因，正则化条款对于获得有意义的注册结果至关重要。然而，这需要引入至少一个超参数（通常称为 {\lambda}），作为损失项之间的权衡。在某些情况下，估计变换的质量很大程度上取决于超参数的选择，并且根据数据的特征可能需要不同的选择。分析这些超参数的影响需要标记数据，而这些数据在测试时通常不可用。在本文中，我们提出了一种评估超参数影响并随后为给定图像对选择最佳值的方法。我们称为 HyperPredict 的方法实现了一个多层感知器，该感知器通过预测生成的分割重叠和变形平滑度的度量来学习选择特定超参数来注册图像对的效果。这种方法使我们能够在测试时选择最佳超参数，而无需标记数据，从而无需采用一刀切的交叉验证方法。此外，用于定义最佳超参数的标准是灵活的训练后，使我们能够有效地选择特定属性。我们使用最新的深度学习方法（cLapIRN）和算法方法（Niftyreg）在 OASIS 大脑 MR 数据集上评估我们提出的方法。我们的结果证明了在预测正则化超参数效果方面的良好性能，并强调了我们的图像对特定方法对超参数选择的好处。</details>
**PDF:** <http://arxiv.org/pdf/2403.02069v1><br />
**Code:** <https://github.com/aisha-lawal/hyperpredict>**<br />
>>**index:** 18<br />
**Title:** **A Generative Approach for Wikipedia-Scale Visual Entity Recognition**<br />
**Title_cn:** 维基百科规模视觉实体识别的生成方法<br />
**Authors:** Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we address web-scale visual entity recognition, specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual-encoder models (eg CLIP), where all the entity names and query images are embedded into a unified space, paving the way for an approximate k-NN search. Alternatively, it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast, we introduce a novel Generative Entity Recognition (GER) framework, which given an input image learns to auto-regressively decode a semantic and discriminative ``code'' identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm, showcasing state-of-the-art performance on the challenging OVEN benchmark. GER surpasses strong captioning, dual-encoder, visual matching and hierarchical classification baselines, affirming its advantage in tackling the complexities of web-scale recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们解决了网络规模的视觉实体识别问题，特别是将给定的查询图像映射到维基百科中 600 万个现有实体之一的任务。解决如此规模问题的一种方法是使用双编码器模型（例如 CLIP），其中所有实体名称和查询图像都嵌入到统一空间中，为近似 k-NN 搜索铺平道路。或者，也可以重新利用字幕模型来直接生成给定图像的实体名称。相比之下，我们引入了一种新颖的生成实体识别（GER）框架，该框架给定输入图像，学习自动回归解码识别目标实体的语义和判别性“代码”。我们的实验证明了这种 GER 范式的有效性，在具有挑战性的 OVEN 基准测试中展示了最先进的性能。 GER 超越了强大的字幕、双编码器、视觉匹配和分层分类基线，证实了其在解决网络规模识别的复杂性方面的优势。</details>
**PDF:** <http://arxiv.org/pdf/2403.02041v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation for Autonomous Driving**<br />
**Title_cn:** 适用于自动驾驶的可扩展视觉 3D 物体检测和单目深度估计<br />
**Authors:** Yuxuan Liu<br />
**Abstract:** <details><summary>原文: </summary>This dissertation is a multifaceted contribution to the advancement of vision-based 3D perception technologies. In the first segment, the thesis introduces structural enhancements to both monocular and stereo 3D object detection algorithms. By integrating ground-referenced geometric priors into monocular detection models, this research achieves unparalleled accuracy in benchmark evaluations for monocular 3D detection. Concurrently, the work refines stereo 3D detection paradigms by incorporating insights and inferential structures gleaned from monocular networks, thereby augmenting the operational efficiency of stereo detection systems. The second segment is devoted to data-driven strategies and their real-world applications in 3D vision detection. A novel training regimen is introduced that amalgamates datasets annotated with either 2D or 3D labels. This approach not only augments the detection models through the utilization of a substantially expanded dataset but also facilitates economical model deployment in real-world scenarios where only 2D annotations are readily available. Lastly, the dissertation presents an innovative pipeline tailored for unsupervised depth estimation in autonomous driving contexts. Extensive empirical analyses affirm the robustness and efficacy of this newly proposed pipeline. Collectively, these contributions lay a robust foundation for the widespread adoption of vision-based 3D perception technologies in autonomous driving applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>本论文对基于视觉的 3D 感知技术的进步做出了多方面的贡献。在第一部分中，本文介绍了单目和立体 3D 对象检测算法的结构增强。通过将地面参考几何先验集成到单目检测模型中，这项研究在单目 3D 检测基准评估中实现了无与伦比的准确性。同时，这项工作通过结合从单目网络收集的见解和推理结构来完善立体 3D 检测范例，从而提高立体检测系统的运行效率。第二部分致力于数据驱动策略及其在 3D 视觉检测中的实际应用。引入了一种新颖的训练方案，该方案合并了用 2D 或 3D 标签注释的数据集。这种方法不仅通过利用大幅扩展的数据集来增强检测模型，而且还有助于在只有 2D 注释可用的现实场景中经济地部署模型。最后，论文提出了一种专为自动驾驶环境中无监督深度估计而定制的创新流程。广泛的实证分析证实了这一新提出的管道的稳健性和有效性。总的来说，这些贡献为自动驾驶应用中基于视觉的 3D 感知技术的广泛采用奠定了坚实的基础。</details>
**PDF:** <http://arxiv.org/pdf/2403.02037v1><br />
**Code:** <https://github.com/owen-liuyuxuan/visionfactory>**<br />
>>**index:** 20<br />
**Title:** **Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted Sample Selection**<br />
**Title_cn:** 通过点辅助样本选择利用基于锚点的 LiDAR 3D 物体检测<br />
**Authors:** Shitao Chen, Haolin Zhang, Nanning Zheng<br />
**Abstract:** <details><summary>原文: </summary>3D object detection based on LiDAR point cloud and prior anchor boxes is a critical technology for autonomous driving environment perception and understanding. Nevertheless, an overlooked practical issue in existing methods is the ambiguity in training sample allocation based on box Intersection over Union (IoU_box). This problem impedes further enhancements in the performance of anchor-based LiDAR 3D object detectors. To tackle this challenge, this paper introduces a new training sample selection method that utilizes point cloud distribution for anchor sample quality measurement, named Point Assisted Sample Selection (PASS). This method has undergone rigorous evaluation on two widely utilized datasets. Experimental results demonstrate that the application of PASS elevates the average precision of anchor-based LiDAR 3D object detectors to a novel state-of-the-art, thereby proving the effectiveness of the proposed approach. The codes will be made available at https://github.com/XJTU-Haolin/Point_Assisted_Sample_Selection.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于LiDAR点云和先验锚框的3D物体检测是自动驾驶环境感知和理解的关键技术。然而，现有方法中一个被忽视的实际问题是基于框交集（IoU_box）的训练样本分配的模糊性。这个问题阻碍了基于锚点的 LiDAR 3D 物体检测器性能的进一步增强。为了应对这一挑战，本文引入了一种新的训练样本选择方法，该方法利用点云分布进行锚定样本质量测量，称为点辅助样本选择（PASS）。该方法已经在两个广泛使用的数据集上经过了严格的评估。实验结果表明，PASS 的应用将基于锚点的 LiDAR 3D 物体检测器的平均精度提升到了最先进的水平，从而证明了该方法的有效性。代码将在 https://github.com/XJTU-Haolin/Point_Assisted_Sample_Selection 提供。</details>
**PDF:** <http://arxiv.org/pdf/2403.01978v1><br />
**Code:** <https://github.com/xjtu-haolin/point_assisted_sample_selection>**<br />
>>**index:** 21<br />
**Title:** **Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection**<br />
**Title_cn:** 用于视频伪装物体检测的显式运动处理和交互式提示<br />
**Authors:** Xin Zhang, Tao Xiao, Gepeng Ji, Xuan Wu, Keren Fu, Qijun Zhao<br />
**Abstract:** <details><summary>原文: </summary>Camouflage poses challenges in distinguishing a static target, whereas any movement of the target can break this disguise. Existing video camouflaged object detection (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive prompting way that is inspired by emerging visual prompt learning. Two learnable modules, i.e. the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation prompts, respectively, and enhance outputs of the both streams. The prompt fed to the motion stream is learned by supervising optical flow in a self-supervised manner. Furthermore, we show that long-term historical information can also be incorporated as a prompt into EMIP and achieve more robust results with temporal consistency. Experimental results demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD benchmarks. The code will be publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>伪装对区分静态目标提出了挑战，而目标的任何移动都可以打破这种伪装。现有的视频伪装目标检测（VCOD）方法将噪声运动估计作为输入或隐式地建模运动，限制了复杂动态场景中的检测性能。在本文中，我们提出了一种新颖的 VCOD 显式运动处理和交互式提示框架，称为 EMIP，它使用冻结的预训练光流基本模型显式处理运动提示。 EMIP 的特点是采用双流架构，可同时进行伪装分割和光流估计。受新兴视觉提示学习的启发，双流之间的交互以交互式提示方式实现。两个可学习模块，即伪装馈线和运动收集器，分别设计用于合并分割到运动和运动到分割提示，并增强两个流的输出。馈送到运动流的提示是通过以自监督方式监督光流来学习的。此外，我们还表明，长期历史信息也可以作为提示纳入 EMIP 中，并获得具有时间一致性的更稳健的结果。实验结果表明，我们的 EMIP 在流行的 VCOD 基准测试中取得了新的最先进记录。该代码将公开。</details>
**PDF:** <http://arxiv.org/pdf/2403.01968v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning**<br />
**Title_cn:** 通过距离感知对比学习增强信息最大化，实现无源跨域少样本学习<br />
**Authors:** Huali Xu, Li Liu, Shuaifeng Zhi, Shaojing Fu, Zhuo Su, Ming-Ming Cheng, Yongxiang Liu<br />
**Abstract:** <details><summary>原文: </summary>Existing Cross-Domain Few-Shot Learning (CDFSL) methods require access to source domain data to train a model in the pre-training phase. However, due to increasing concerns about data privacy and the desire to reduce data transmission and training costs, it is necessary to develop a CDFSL solution without accessing source data. For this reason, this paper explores a Source-Free CDFSL (SF-CDFSL) problem, in which CDFSL is addressed through the use of existing pretrained models instead of training a model with source data, avoiding accessing source data. This paper proposes an Enhanced Information Maximization with Distance-Aware Contrastive Learning (IM-DCL) method to address these challenges. Firstly, we introduce the transductive mechanism for learning the query set. Secondly, information maximization (IM) is explored to map target samples into both individual certainty and global diversity predictions, helping the source model better fit the target data distribution. However, IM fails to learn the decision boundary of the target task. This motivates us to introduce a novel approach called Distance-Aware Contrastive Learning (DCL), in which we consider the entire feature set as both positive and negative sets, akin to Schrodinger's concept of a dual state. Instead of a rigid separation between positive and negative sets, we employ a weighted distance calculation among features to establish a soft classification of the positive and negative sets for the entire feature set. Furthermore, we address issues related to IM by incorporating contrastive constraints between object features and their corresponding positive and negative sets. Evaluations of the 4 datasets in the BSCD-FSL benchmark indicate that the proposed IM-DCL, without accessing the source domain, demonstrates superiority over existing methods, especially in the distant domain task.</details>
**Abstract_cn:** <details><summary>译文: </summary>现有的跨域少样本学习（CDFSL）方法需要访问源域数据以在预训练阶段训练模型。然而，由于对数据隐私的日益关注以及降低数据传输和训练​​成本的愿望，有必要开发一种无需访问源数据的CDFSL解决方案。为此，本文探讨了无源 CDFSL（SF-CDFSL）问题，其中通过使用现有的预训练模型而不是用源数据训练模型来解决 CDFSL，从而避免访问源数据。本文提出了一种通过距离感知对比学习增强信息最大化（IM-DCL）方法来应对这些挑战。首先，我们介绍了学习查询集的转导机制。其次，探索信息最大化（IM），将目标样本映射到个体确定性和全局多样性预测，帮助源模型更好地拟合目标数据分布。然而，IM 无法学习目标任务的决策边界。这促使我们引入一种称为距离感知对比学习（DCL）的新颖方法，在该方法中，我们将整个特征集视为正集和负集，类似于薛定谔的对偶状态概念。我们没有严格区分正集和负集，而是采用特征之间的加权距离计算来为整个特征集建立正集和负集的软分类。此外，我们通过结合对象特征及其相应的正负集之间的对比约束来解决与 IM 相关的问题。对 BSCD-FSL 基准中的 4 个数据集的评估表明，所提出的 IM-DCL 在不访问源域的情况下表现出优于现有方法的优越性，尤其是在远程域任务中。</details>
**PDF:** <http://arxiv.org/pdf/2403.01966v1><br />
**Code:** <https://github.com/xuhuali-mxj/im-dcl>**<br />
>>**index:** 23<br />
**Title:** **Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification**<br />
**Title_cn:** 弥合增强差距的傅里叶基函数：重新思考图像分类中的频率增强<br />
**Authors:** Puru Vaish, Shunxin Wang, Nicola Strisciuglio<br />
**Abstract:** <details><summary>原文: </summary>Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing perturbations, with negligible deficit to the standard performance of models. It can be seamlessly integrated with other augmentation techniques to further boost performance. Code and models can be found at: https://github.com/nis-research/afa-augment</details>
**Abstract_cn:** <details><summary>译文: </summary>由于训练期间未考虑到输入的意外变化，计算机视觉模型在部署到现实场景中时通常会出现性能下降。数据增强通常用于解决这个问题，因为它的目的是增加数据多样性并减少训练数据和测试数据之间的分布差距。然而，常见的视觉增强可能无法保证计算机视觉模型的广泛鲁棒性。在本文中，我们提出了辅助傅里叶基增强（AFA），这是一种针对频域增强并填补视觉增强留下的增强间隙的补充技术。我们在简单而高效的对抗环境中展示了通过傅立叶基加性噪声进行增强的实用性。我们的结果表明，AFA 有益于模型针对常见损坏的鲁棒性、OOD 泛化以及模型针对不断增加的扰动的性能一致性，而对模型标准性能的缺陷可以忽略不计。它可以与其他增强技术无缝集成，以进一步提高性能。代码和模型可以在以下位置找到：https://github.com/nis-research/afa-augment</details>
**PDF:** <http://arxiv.org/pdf/2403.01944v2><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **xT: Nested Tokenization for Larger Context in Large Images**<br />
**Title_cn:** xT：大图像中更大上下文的嵌套标记化<br />
**Authors:** Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell, Karttikeya Mangalam<br />
**Abstract:** <details><summary>原文: </summary>Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. By introducing a nested tokenization scheme for large images in conjunction with long-sequence length models normally used for natural language processing, we are able to increase accuracy by up to 8.6% on challenging classification tasks and $F_1$ score by 11.6 on context-dependent segmentation in large images.</details>
**Abstract_cn:** <details><summary>译文: </summary>现代计算机视觉管道以两种次优方式之一处理大图像：下采样或裁剪。这两种方法会导致图像中存在的信息量和上下文显着损失。在许多下游应用中，全局背景与高频细节一样重要，例如在现实世界的卫星图像中；在这种情况下，研究人员不得不做出令人不安的选择，决定丢弃哪些信息。我们引入了 xT，这是一个简单的视觉转换器框架，它可以有效地将全局上下文与局部细节聚合在一起，并可以在当代 GPU 上对大型图像进行端到端建模。我们在经典视觉任务中选择了一组基准数据集，这些数据集准确地反映了视觉模型理解真正大图像并在大范围内合并精细细节的能力，并评估我们的方法对其的改进。通过引入大图像的嵌套标记化方案以及通常用于自然语言处理的长序列长度模型，我们能够将具有挑战性的分类任务的准确性提高高达 8.6%，并将上下文相关的 $F_1$ 得分提高 11.6大图像分割。</details>
**PDF:** <http://arxiv.org/pdf/2403.01915v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Map-aided annotation for pole base detection**<br />
**Title_cn:** 用于杆基检测的地图辅助注释<br />
**Authors:** Benjamin Missaoui, Maxime Noizet, Philippe Xu<br />
**Abstract:** <details><summary>原文: </summary>For autonomous navigation, high definition maps are a widely used source of information. Pole-like features encoded in HD maps such as traffic signs, traffic lights or street lights can be used as landmarks for localization. For this purpose, they first need to be detected by the vehicle using its embedded sensors. While geometric models can be used to process 3D point clouds retrieved by lidar sensors, modern image-based approaches rely on deep neural network and therefore heavily depend on annotated training data. In this paper, a 2D HD map is used to automatically annotate pole-like features in images. In the absence of height information, the map features are represented as pole bases at the ground level. We show how an additional lidar sensor can be used to filter out occluded features and refine the ground projection. We also demonstrate how an object detector can be trained to detect a pole base. To evaluate our methodology, it is first validated with data manually annotated from semantic segmentation and then compared to our own automatically generated annotated data recorded in the city of Compi{\`e}gne, France. Erratum: In the original version [1], an error occurred in the accuracy evaluation of the different models studied and the evaluation method applied on the detection results was not clearly defined. In this revision, we offer a rectification to this segment, presenting updated results, especially in terms of Mean Absolute Errors (MAE).</details>
**Abstract_cn:** <details><summary>译文: </summary>对于自主导航，高清地图是广泛使用的信息源。高清地图中编码的杆状特征（例如交通标志、交通灯或路灯）可以用作定位的地标。为此，车辆首先需要使用其嵌入式传感器来检测它们。虽然几何模型可用于处理激光雷达传感器检索到的 3D 点云，但现代基于图像的方法依赖于深度神经网络，因此严重依赖于带注释的训练数据。在本文中，使用二维高清地图来自动注释图像中的类似杆的特征。在没有高度信息的情况下，地图特征被表示为地面的极基。我们展示了如何使用额外的激光雷达传感器来过滤掉被遮挡的特征并细化地面投影。我们还演示了如何训练对象检测器来检测杆基。为了评估我们的方法，首先使用语义分割手动注释的数据进行验证，然后与我们在法国贡比涅市记录的自动生成的注释数据进行比较。勘误：在原始版本[1]中，对所研究的不同模型的精度评估出现了错误，并且对检测结果应用的评估方法没有明确定义。在本次修订中，我们对此部分进行了修正，提供了更新的结果，特别是在平均绝对误差 (MAE) 方面。</details>
**PDF:** <http://arxiv.org/pdf/2403.01868v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **FreeA: Human-object Interaction Detection using Free Annotation Labels**<br />
**Title_cn:** FreeA：使用免费注释标签进行人机交互检测<br />
**Authors:** Yuxiao Wang, Zhenao Wei, Xinyu Jiang, Yu Lei, Weiying Xue, Jinxiu Liu, Qi Liu<br />
**Abstract:** <details><summary>原文: </summary>Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing and classifying the interactive actions than the newest weakly model, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively. Code will be available at https://drliuqi.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的人机交互（HOI）检测方法依赖于高昂的人力成本，并且需要全面的带注释的图像数据集。在本文中，我们提出了一种新颖的自适应语言驱动的 HOI 检测方法，称为 FreeA，利用 CLIP 的适应性生成潜在 HOI 标签，无需标签。具体来说，FreeA 将人-物体对的图像特征与 HOI 文本模板进行匹配，并开发了基于先验知识的掩模方法来抑制不可能的交互。此外，FreeA利用所提出的交互相关性匹配方法来增强与指定动作相关的动作的可能性，进一步细化生成的HOI标签。对两个基准数据集的实验表明，FreeA 在弱监督 HOI 模型中实现了最先进的性能。我们的方法在 HICO-DET 上的平均平均精度 (mAP) 为 +8.58，在 V-COCO 上的平均平均精度为 +1.23 mAP，比最新的弱模型在本地化和分类交互动作方面更准确，并且比最新的弱模型+1.68 mAP 和 +7.28 mAP+模型，分别。代码可在 https://drliuqi.github.io/ 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01840v1><br />
**Code:** null<br />
>>**index:** 27<br />
**Title:** **AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation**<br />
**Title_cn:** AllSpark：从 Transformer 中未标记的特征中重生，用于半监督语义分割<br />
**Authors:** Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li<br />
**Abstract:** <details><summary>原文: </summary>Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general transformer-based segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and model weights are available at: https://github.com/xmed-lab/AllSpark.</details>
**Abstract_cn:** <details><summary>译文: </summary>半监督语义分割（SSSS）被提出来减轻耗时的像素级手动标记的负担，它利用有限的标记数据和大量未标记数据。当前最先进的方法使用真实值训练标记数据，使用伪标签训练未标记数据。然而，两个训练流程是分开的，这使得标记数据主导训练过程，导致低质量的伪标签，从而导致次优结果。为了缓解这个问题，我们提出了 AllSpark，它通过通道交叉注意力机制从未标记的特征中重生标记的特征。我们进一步引入语义记忆和通道语义分组策略，以确保未标记的特征充分表示标记的特征。 AllSpark为SSSS的架构级设计而非框架级设计提供了新的思路，从而避免了日益复杂的训练管道设计。它也可以被视为一个灵活的瓶颈模块，可以无缝集成到基于通用变压器的分割模型中。所提出的 AllSpark 在 Pascal、Cityscapes 和 COCO 基准的所有评估协议中都优于现有方法，没有任何附加功能。代码和模型权重可在以下位置获取：https://github.com/xmed-lab/AllSpark。</details>
**PDF:** <http://arxiv.org/pdf/2403.01818v1><br />
**Code:** <https://github.com/xmed-lab/AllSpark>**<br />
>>**index:** 28<br />
**Title:** **A Simple Baseline for Efficient Hand Mesh Reconstruction**<br />
**Title_cn:** 高效手部网格重建的简单基线<br />
**Authors:** Zhishan Zhou, Shihao. zhou, Zhi Lv, Minqiang Zou, Yao Tang, Jiajun Liang<br />
**Abstract:** <details><summary>原文: </summary>3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks. As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods. In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency. To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures. A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities. Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models. Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 手势估计在手势识别和人机交互任务等领域有着广泛的应用。随着性能的提高，系统的复杂性也随之增加，这会限制这些方法的比较分析和实际实现。在本文中，我们提出了一个简单而有效的基线，它不仅超越了最先进的（SOTA）方法，而且还证明了计算效率。为了建立这个基线，我们将现有工作抽象为两个组件：令牌生成器和网格回归器，然后检查它们的核心结构。在这种情况下，核心结构是指能够实现内在功能、带来显着改进并在没有不必要的复杂性的情况下实现卓越性能的结构。我们提出的方法与对主干的任何修改无关，使其适用于任何现代模型。我们的方法优于现有解决方案，在多个数据集上实现了最先进的 (SOTA) 结果。在 FreiHAND 数据集上，我们的方法生成了 5.7 毫米的 PA-MPJPE 和 6.0 毫米的 PA-MPVPE。同样，在 Dexycb 数据集上，我们观察到 PA-MPJPE 为 5.5mm，PA-MPVPE 为 5.0mm。至于性能速度，我们的方法在使用 HRNet 时达到每秒 33 帧 (fps)，在使用 FastViT-MA36 时达到每秒 70 帧 (fps)</details>
**PDF:** <http://arxiv.org/pdf/2403.01813v1><br />
**Code:** null<br />
>>**index:** 29<br />
**Title:** **PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using Local-Global Features**<br />
**Title_cn:** PointCore：使用局部-全局特征的高效无监督点云异常检测器<br />
**Authors:** Baozhu Zhao, Qiwei Xiong, Xiaohan Zhang, Jingfeng Guo, Qi Liu, Xiaofen Xing, Xiangmin Xu<br />
**Abstract:** <details><summary>原文: </summary>Three-dimensional point cloud anomaly detection that aims to detect anomaly data points from a training set serves as the foundation for a variety of applications, including industrial inspection and autonomous driving. However, existing point cloud anomaly detection methods often incorporate multiple feature memory banks to fully preserve local and global representations, which comes at the high cost of computational complexity and mismatches between features. To address that, we propose an unsupervised point cloud anomaly detection framework based on joint local-global features, termed PointCore. To be specific, PointCore only requires a single memory bank to store local (coordinate) and global (PointMAE) representations and different priorities are assigned to these local-global features, thereby reducing the computational cost and mismatching disturbance in inference. Furthermore, to robust against the outliers, a normalization ranking method is introduced to not only adjust values of different scales to a notionally common scale, but also transform densely-distributed data into a uniform distribution. Extensive experiments on Real3D-AD dataset demonstrate that PointCore achieves competitive inference time and the best performance in both detection and localization as compared to the state-of-the-art Reg3D-AD approach and several competitors.</details>
**Abstract_cn:** <details><summary>译文: </summary>三维点云异常检测旨在从训练集中检测异常数据点，是工业检测和自动驾驶等各种应用的基础。然而，现有的点云异常检测方法通常结合多个特征存储库来完全保留局部和全局表示，这是以计算复杂性和特征之间的不匹配为代价的。为了解决这个问题，我们提出了一种基于联合局部全局特征的无监督点云异常检测框架，称为 PointCore。具体来说，PointCore只需要一个存储体来存储局部（坐标）和全局（PointMAE）表示，并且为这些局部-全局特征分配不同的优先级，从而减少推理中的计算成本和不匹配干扰。此外，为了对异常值具有鲁棒性，引入了归一化排序方法，不仅将不同尺度的值调整到名义上的公共尺度，而且将密集分布的数据转换为均匀分布。对 Real3D-AD 数据集的大量实验表明，与最先进的 Reg3D-AD 方法和几个竞争对手相比，PointCore 在检测和定位方面实现了有竞争力的推理时间和最佳性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01804v1><br />
**Code:** null<br />
>>**index:** 30<br />
**Title:** **RankED: Addressing Imbalance and Uncertainty in Edge Detection Using Ranking-based Losses**<br />
**Title_cn:** RankED：使用基于排名的损失解决边缘检测中的不平衡和不确定性<br />
**Authors:** Bedrettin Cetinkaya, Sinan Kalkan, Emre Akbas<br />
**Abstract:** <details><summary>原文: </summary>Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement between different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annotators. In this paper, we propose RankED, a unified ranking-based approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high confidence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUD-v2, BSDS500 and Multi-cue datasets. Code is available at https://ranked-cvpr24.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测图像中的边缘存在以下问题：（P1）正类和负类之间严重不平衡，以及（P2）由于不同注释者之间的分歧而导致标签不确定性。现有的解决方案使用类平衡交叉熵损失和骰子损失来解决 P1 问题，通过仅预测大多数注释者同意的边缘来解决 P2 问题。在本文中，我们提出了 RankED，一种基于统一排名的方法，可以解决不平衡问题 (P1) 和不确定性问题 (P2)。 RankED 通过两个组件解决这两个问题：第一个组件将正像素排在负像素之上，第二个组件促进高置信度边缘像素具有更高的标签确定性。我们证明 RankED 优于之前的研究，并在 NYUD-v2、BSDS500 和 Multi-cue 数据集上树立了新的最先进水平。代码可在 https://ranked-cvpr24.github.io 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01795v1><br />
**Code:** null<br />
>>**index:** 31<br />
**Title:** **Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection**<br />
**Title_cn:** 揭露欺骗：发现更多用于 Deepfake 检测的伪造线索<br />
**Authors:** Zhongjie Ba, Qingyu Liu, Zhenguang Liu, Shuang Wu, Feng Lin, Li Lu, Kui Ren<br />
**Abstract:** <details><summary>原文: </summary>Deepfake technology has given rise to a spectrum of novel and compelling applications. Unfortunately, the widespread proliferation of high-fidelity fake videos has led to pervasive confusion and deception, shattering our faith that seeing is believing. One aspect that has been overlooked so far is that current deepfake detection approaches may easily fall into the trap of overfitting, focusing only on forgery clues within one or a few local regions. Moreover, existing works heavily rely on neural networks to extract forgery features, lacking theoretical constraints guaranteeing that sufficient forgery clues are extracted and superfluous features are eliminated. These deficiencies culminate in unsatisfactory accuracy and limited generalizability in real-life scenarios.   In this paper, we try to tackle these challenges through three designs: (1) We present a novel framework to capture broader forgery clues by extracting multiple non-overlapping local representations and fusing them into a global semantic-rich feature. (2) Based on the information bottleneck theory, we derive Local Information Loss to guarantee the orthogonality of local representations while preserving comprehensive task-relevant information. (3) Further, to fuse the local representations and remove task-irrelevant information, we arrive at a Global Information Loss through the theoretical analysis of mutual information. Empirically, our method achieves state-of-the-art performance on five benchmark datasets.Our code is available at \url{https://github.com/QingyuLiu/Exposing-the-Deception}, hoping to inspire researchers.</details>
**Abstract_cn:** <details><summary>译文: </summary>Deepfake 技术催生了一系列新颖且引人注目的应用。不幸的是，高保真假视频的广泛传播导致了普遍的混乱和欺骗，粉碎了我们眼见为实的信念。迄今为止被忽视的一个方面是，当前的深度伪造检测方法很容易陷入过度拟合的陷阱，仅关注一个或几个局部区域内的伪造线索。此外，现有的工作严重依赖神经网络来提取伪造特征，缺乏理论约束来保证提取足够的伪造线索并消除多余的特征。这些缺陷最终导致现实生活场景中的准确性不令人满意且普遍性有限。在本文中，我们尝试通过三种设计来应对这些挑战：（1）我们提出了一种新颖的框架，通过提取多个不重叠的局部表示并将它们融合成全局语义丰富的特征来捕获更广泛的伪造线索。 （2）基于信息瓶颈理论，我们推导出局部信息损失，以保证局部表示的正交性，同时保留全面的任务相关信息。 (3)进一步，为了融合局部表示并去除与任务无关的信息，我们通过互信息的理论分析得出全局信息损失。根据经验，我们的方法在五个基准数据集上实现了最先进的性能。我们的代码可在 \url{https://github.com/QingyuLiu/Exusing-the-Deception} 获取，希望对研究人员有所启发。</details>
**PDF:** <http://arxiv.org/pdf/2403.01786v1><br />
**Code:** <https://github.com/qingyuliu/exposing-the-deception>**<br />
>>**index:** 32<br />
**Title:** **Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning**<br />
**Title_cn:** 集成高效的最优传输和功能图以实现无监督形状对应学习<br />
**Authors:** Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie<br />
**Abstract:** <details><summary>原文: </summary>In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.</details>
**Abstract_cn:** <details><summary>译文: </summary>在计算机视觉和图形领域，准确建立 3D 几何形状之间的对应关系对于对象跟踪、配准、纹理传输和统计形状分析等应用至关重要。超越传统的手工制作和数据驱动的特征学习方法，我们将光谱方法与深度学习相结合，重点关注功能图（FM）和最佳传输（OT）。传统的基于 OT 的方法通常依赖于基于学习的框架中的熵正则化 OT，由于其二次成本而面临计算挑战。我们的主要贡献是将切片 Wasserstein 距离 (SWD) 用于 OT，这是无监督形状匹配框架中的有效快速最优传输度量。这种无监督框架将功能图正则化器与源自 SWD 的新型基于 OT 的损失集成在一起，增强了被视为离散概率度量的形状之间的特征对齐。我们还引入了利用熵正则化 OT 的自适应细化过程，进一步细化特征对齐以实现准确的点对点对应。我们的方法在非刚性形状匹配（包括近等距和非等距场景）中表现出卓越的性能，并且在分割传输等下游任务中表现出色。不同数据集的实证结果凸显了我们框架的有效性和泛化能力，通过高效的 OT 指标和自适应细化模块为非刚性形状匹配设定了新标准。</details>
**PDF:** <http://arxiv.org/pdf/2403.01781v1><br />
**Code:** null<br />
>>**index:** 33<br />
**Title:** **Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition**<br />
**Title_cn:** 手写数学表达式识别的注意力引导机制<br />
**Authors:** Yutian Liu, Wenjun Ke, Jianguo Wei<br />
**Abstract:** <details><summary>原文: </summary>Handwritten mathematical expression recognition (HMER) is challenging in image-to-text tasks due to the complex layouts of mathematical expressions and suffers from problems including over-parsing and under-parsing. To solve these, previous HMER methods improve the attention mechanism by utilizing historical alignment information. However, this approach has limitations in addressing under-parsing since it cannot correct the erroneous attention on image areas that should be parsed at subsequent decoding steps. This faulty attention causes the attention module to incorporate future context into the current decoding step, thereby confusing the alignment process. To address this issue, we propose an attention guidance mechanism to explicitly suppress attention weights in irrelevant areas and enhance the appropriate ones, thereby inhibiting access to information outside the intended context. Depending on the type of attention guidance, we devise two complementary approaches to refine attention weights: self-guidance that coordinates attention of multiple heads and neighbor-guidance that integrates attention from adjacent time steps. Experiments show that our method outperforms existing state-of-the-art methods, achieving expression recognition rates of 60.75% / 61.81% / 63.30% on the CROHME 2014/ 2016/ 2019 datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于数学表达式布局复杂，手写数学表达式识别（HMER）在图像到文本任务中具有挑战性，并且存在过度解析和解析不足等问题。为了解决这些问题，之前的 HMER 方法通过利用历史对齐信息改进了注意力机制。然而，这种方法在解决解析不足方面存在局限性，因为它无法纠正对应在后续解码步骤中解析的图像区域的错误关注。这种错误的注意力会导致注意力模块将未来的上下文合并到当前的解码步骤中，从而混淆对齐过程。为了解决这个问题，我们提出了一种注意力引导机制，以明确抑制不相关领域的注意力权重并增强适当的注意力权重，从而抑制对预期上下文之外的信息的访问。根据注意力引导的类型，我们设计了两种互补的方法来细化注意力权重：协调多个头注意力的自我引导和整合相邻时间步骤注意力的邻居引导。实验表明，我们的方法优于现有的最先进方法，在 CROHME 2014/2016/2019 数据集上实现了 60.75% / 61.81% / 63.30% 的表情识别率。</details>
**PDF:** <http://arxiv.org/pdf/2403.01756v2><br />
**Code:** null<br />
>>**index:** 34<br />
**Title:** **Training-Free Pretrained Model Merging**<br />
**Title_cn:** 免训练预训练模型合并<br />
**Authors:** Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song<br />
**Abstract:** <details><summary>原文: </summary>Recently, model merging techniques have surfaced as a solution to combine multiple single-talent models into a single multi-talent model. However, previous endeavors in this field have either necessitated additional training or fine-tuning processes, or require that the models possess the same pre-trained initialization. In this work, we identify a common drawback in prior works w.r.t. the inconsistency of unit similarity in the weight space and the activation space. To address this inconsistency, we propose an innovative model merging framework, coined as merging under dual-space constraints (MuDSC). Specifically, instead of solely maximizing the objective of a single space, we advocate for the exploration of permutation matrices situated in a region with a unified high similarity in the dual space, achieved through the linear combination of activation and weight similarity matrices. In order to enhance usability, we have also incorporated adaptations for group structure, including Multi-Head Attention and Group Normalization. Comprehensive experimental comparisons demonstrate that MuDSC can significantly boost the performance of merged models with various task combinations and architectures. Furthermore, the visualization of the merged model within the multi-task loss landscape reveals that MuDSC enables the merged model to reside in the overlapping segment, featuring a unified lower loss for each task. Our code is publicly available at https://github.com/zju-vipa/training_free_model_merging.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，模型合并技术作为将多个单人才模型组合成单个多人才模型的解决方案而出现。然而，该领域之前的努力要么需要额外的训练或微调过程，要么要求模型具有相同的预训练初始化。在这项工作中，我们发现了先前工作中的一个常见缺点。权重空间和激活空间中单元相似度的不一致。为了解决这种不一致问题，我们提出了一种创新的模型合并框架，称为双空间约束下的合并（MuDSC）。具体来说，我们主张探索位于对偶空间中具有统一高相似度的区域中的置换矩阵，而不是仅仅最大化单个空间的目标，这是通过激活和权重相似矩阵的线性组合来实现的。为了增强可用性，我们还纳入了对组结构的调整，包括多头注意力和组标准化。全面的实验比较表明，MuDSC 可以显着提高具有各种任务组合和架构的合并模型的性能。此外，多任务损失环境中合并模型的可视化表明，MuDSC 使合并模型能够驻留在重叠部分，从而为每个任务提供统一的较低损失。我们的代码可在 https://github.com/zju-vipa/training_free_model_merging 上公开获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01753v1><br />
**Code:** <https://github.com/zju-vipa/training_free_model_merging>**<br />
>>**index:** 35<br />
**Title:** **Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer**<br />
**Title_cn:** 轻量级目标检测：基于YOLOv7结合ShuffleNetv2和Vision Transformer的研究<br />
**Authors:** Wenkai Gong<br />
**Abstract:** <details><summary>原文: </summary>As mobile computing technology rapidly evolves, deploying efficient object detection algorithms on mobile devices emerges as a pivotal research area in computer vision. This study zeroes in on optimizing the YOLOv7 algorithm to boost its operational efficiency and speed on mobile platforms while ensuring high accuracy. Leveraging a synergy of advanced techniques such as Group Convolution, ShuffleNetV2, and Vision Transformer, this research has effectively minimized the model's parameter count and memory usage, streamlined the network architecture, and fortified the real-time object detection proficiency on resource-constrained devices. The experimental outcomes reveal that the refined YOLO model demonstrates exceptional performance, markedly enhancing processing velocity while sustaining superior detection accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>随着移动计算技术的快速发展，在移动设备上部署高效的目标检测算法成为计算机视觉的关键研究领域。本研究的重点是优化YOLOv7算法，以提高其在移动平台上的运行效率和速度，同时确保高精度。该研究利用Group Convolution、ShuffleNetV2和Vision Transformer等先进技术的协同作用，有效减少了模型的参数数量和内存使用，简化了网络架构，并增强了资源受限设备上的实时目标检测能力。实验结果表明，改进后的 YOLO 模型表现出卓越的性能，显着提高了处理速度，同时保持了卓越的检测精度。</details>
**PDF:** <http://arxiv.org/pdf/2403.01736v1><br />
**Code:** null<br />
>>**index:** 36<br />
**Title:** **RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features**<br />
**Title_cn:** RISeg：通过身体框架不变特征进行机器人交互式对象分割<br />
**Authors:** Howard H. Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad Khargonkar, Yu Xiang, Kaiyu Hang<br />
**Abstract:** <details><summary>原文: </summary>In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>为了在新环境中成功执行抓取等操作任务，机器人必须精通从背景和/或其他物体中分割出看不见的物体。之前的工作通过在大规模数据上训练深度神经网络来学习 RGB/RGB-D 特征嵌入来执行看不见的对象实例分割（UOIS），其中杂乱的环境通常会导致分割不准确。我们在这些方法的基础上引入了一种新颖的方法，通过使用机器人交互和设计的身体框架不变特征来纠正基于静态图像的 UOIS 掩模的不准确分割，例如分割不足。我们证明，由于机器人交互而随机附着到刚体的框架的相对线性和旋转速度可用于识别对象并累积校正的对象级分割掩模。通过将运动引入分割不确定性区域，我们能够以不确定性驱动的方式，以最少的、无破坏性的交互（每个场景约 2-3 个）大幅提高分割精度。我们展示了我们提出的交互式感知管道在准确分割杂乱场景方面的有效性，平均对象分割准确率达到 80.7%，与其他最先进的 UOIS 方法相比提高了 28.2%。</details>
**PDF:** <http://arxiv.org/pdf/2403.01731v1><br />
**Code:** null<br />
>>**index:** 37<br />
**Title:** **MCA: Moment Channel Attention Networks**<br />
**Title_cn:** MCA：时刻通道注意力网络<br />
**Authors:** Yangbo Jiang, Zhiwei Jiang, Le Han, Zenan Huang, Nenggan Zheng<br />
**Abstract:** <details><summary>原文: </summary>Channel attention mechanisms endeavor to recalibrate channel weights to enhance representation abilities of networks. However, mainstream methods often rely solely on global average pooling as the feature squeezer, which significantly limits the overall potential of models. In this paper, we investigate the statistical moments of feature maps within a neural network. Our findings highlight the critical role of high-order moments in enhancing model capacity. Consequently, we introduce a flexible and comprehensive mechanism termed Extensive Moment Aggregation (EMA) to capture the global spatial context. Building upon this mechanism, we propose the Moment Channel Attention (MCA) framework, which efficiently incorporates multiple levels of moment-based information while minimizing additional computation costs through our Cross Moment Convolution (CMC) module. The CMC module via channel-wise convolution layer to capture multiple order moment information as well as cross channel features. The MCA block is designed to be lightweight and easily integrated into a variety of neural network architectures. Experimental results on classical image classification, object detection, and instance segmentation tasks demonstrate that our proposed method achieves state-of-the-art results, outperforming existing channel attention methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>通道注意力机制致力于重新校准通道权重以增强网络的表示能力。然而，主流方法通常仅仅依赖全局平均池化作为特征压缩器，这极大地限制了模型的整体潜力。在本文中，我们研究了神经网络中特征图的统计矩。我们的研究结果强调了高阶矩在增强模型能力方面的关键作用。因此，我们引入了一种灵活而全面的机制，称为广泛矩聚合（EMA）来捕获全球空间背景。在此机制的基础上，我们提出了矩通道注意（MCA）框架，该框架有效地整合了多个级别的基于矩的信息，同时通过我们的交叉矩卷积（CMC）模块最大限度地减少了额外的计算成本。 CMC模块通过通道卷积层来捕获多阶矩信息以及跨通道特征。 MCA 模块设计轻巧，易于集成到各种神经网络架构中。经典图像分类、对象检测和实例分割任务的实验结果表明，我们提出的方法取得了最先进的结果，优于现有的通道注意方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01713v1><br />
**Code:** null<br />
>>**index:** 38<br />
**Title:** **PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for Astronomical Image Deconvolution**<br />
**Title_cn:** PI-AstroDeconv：一种基于物理的天文图像反卷积无监督学习方法<br />
**Authors:** Shulei Ni, Yisheng Qiu, Yunchun Chen, Zihao Song, Hao Chen, Xuejian Jiang, Huaxi Chen<br />
**Abstract:** <details><summary>原文: </summary>In the imaging process of an astronomical telescope, the deconvolution of its beam or Point Spread Function (PSF) is a crucial task. However, deconvolution presents a classical and challenging inverse computation problem. In scenarios where the beam or PSF is complex or inaccurately measured, such as in interferometric arrays and certain radio telescopes, the resultant blurry images are often challenging to interpret visually or analyze using traditional physical detection methods. We argue that traditional methods frequently lack specific prior knowledge, thereby leading to suboptimal performance. To address this issue and achieve image deconvolution and reconstruction, we propose an unsupervised network architecture that incorporates prior physical information. The network adopts an encoder-decoder structure while leveraging the telescope's PSF as prior knowledge. During network training, we introduced accelerated Fast Fourier Transform (FFT) convolution to enable efficient processing of high-resolution input images and PSFs. We explored various classic regression networks, including autoencoder (AE) and U-Net, and conducted a comprehensive performance evaluation through comparative analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>在天文望远镜的成像过程中，其光束或点扩散函数（PSF）的反卷积是一项至关重要的任务。然而，反卷积提出了一个经典且具有挑战性的逆计算问题。在光束或 PSF 复杂或测量不准确的情况下，例如在干涉阵列和某些射电望远镜中，所产生的模糊图像通常难以用传统的物理检测方法进行视觉解释或分析。我们认为传统方法经常缺乏特定的先验知识，从而导致性能不佳。为了解决这个问题并实现图像反卷积和重建，我们提出了一种结合先验物理信息的无监督网络架构。该网络采用编码器-解码器结构，同时利用望远镜的PSF作为先验知识。在网络训练期间，我们引入了加速快速傅立叶变换 (FFT) 卷积，以实现高分辨率输入图像和 PSF 的高效处理。我们探索了各种经典的回归网络，包括自动编码器（AE）和U-Net，并通过比较分析进行了全面的性能评估。</details>
**PDF:** <http://arxiv.org/pdf/2403.01692v1><br />
**Code:** null<br />
>>**index:** 39<br />
**Title:** **Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection**<br />
**Title_cn:** 用于视觉语言目标检测的零样本可推广增量学习<br />
**Authors:** Jieren Deng, Haojian Zhang, Kun Ding, Jianhua Hu, Xingxuan Zhang, Yunkuan Wang<br />
**Abstract:** <details><summary>原文: </summary>This paper presents Incremental Vision-Language Object Detection (IVLOD), a novel learning task designed to incrementally adapt pre-trained Vision-Language Object Detection Models (VLODMs) to various specialized domains, while simultaneously preserving their zero-shot generalization capabilities for the generalized domain. To address this new challenge, we present the Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that introduces Zero-interference Loss and reparameterization techniques to tackle IVLOD without incurring additional inference costs or a significant increase in memory usage. Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the zero-shot generalization ability of VLODMs while continuously adapting to new tasks. Specifically, after training on ODinW-13 datasets, ZiRa exhibits superior performance compared to CL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and 8.71 AP, respectively.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了增量视觉语言目标检测（IVLOD），这是一种新颖的学习任务，旨在增量地将预训练的视觉语言目标检测模型（VLODM）适应各种专业领域，同时保留其零样本泛化能力领域。为了应对这一新挑战，我们提出了零干扰可重参数化适应（ZiRa），这是一种引入零干扰损失和重参数化技术来解决 IVLOD 的新颖方法，而不会产生额外的推理成本或内存使用量的显着增加。在 COCO 和 ODinW-13 数据集上的综合实验表明，ZiRa 在不断适应新任务的同时，有效保障了 VLODM 的零样本泛化能力。具体来说，在 ODinW-13 数据集上进行训练后，与 CL-DETR 和 iDETR 相比，ZiRa 表现出卓越的性能，将零样本泛化能力分别提高了 13.91 和 8.71 AP。</details>
**PDF:** <http://arxiv.org/pdf/2403.01680v1><br />
**Code:** null<br />
>>**index:** 40<br />
**Title:** **PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network**<br />
**Title_cn:** PillarGen：通过基于 Pillar 的点生成网络增强雷达点云密度和质量<br />
**Authors:** Jisong Kim, Geonho Bang, Kwangjin Choi, Minjae Seong, Jaechang Yoo, Eunjong Pyo, Jun Won Choi<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into bird's eye view object detection, a significant improvement in detection accuracy is achieved.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们提出了一种新颖的点生成模型，称为基于支柱的点生成网络（PillarGen），它有助于点云从一个域到另一个域的转换。 PillarGen 可以根据提供的输入点云生成具有增强密度和质量的合成点云。 PillarGen 模型执行以下三个步骤：1) 支柱编码，2) 占用支柱预测 (OPP)，以及 3) 支柱到点生成 (PPG)。使用柱网格结构对输入点云进行编码以生成柱特征。然后，OPP 确定用于点生成的活动支柱，并预测点的中心以及为每个活动支柱生成的点的数量。 PPG 根据 OPP 提供的信息为每个活动支柱生成综合点。我们使用我们专有的雷达数据集评估 PillarGen 的性能，重点是使用远程雷达数据作为监督来提高短程雷达数据的密度和质量。我们的实验表明，PillarGen 在定量和定性测量方面优于传统的点上采样方法。我们还确认，当将 PillarGen 纳入鸟瞰目标检测时，检测精度得到显着提高。</details>
**PDF:** <http://arxiv.org/pdf/2403.01663v1><br />
**Code:** null<br />

>## **图像理解**
>---
>>**index:** 1<br />
**Title:** **Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues**<br />
**Title_cn:** 使用 4D 几何线索进行迭代遮挡感知光场深度估计<br />
**Authors:** Rui Lourenço, Lucas Thomaz, Eduardo A. B. Silva, Sergio M. M. Faria<br />
**Abstract:** <details><summary>原文: </summary>Light field cameras and multi-camera arrays have emerged as promising solutions for accurately estimating depth by passively capturing light information. This is possible because the 3D information of a scene is embedded in the 4D light field geometry. Commonly, depth estimation methods extract this information relying on gradient information, heuristic-based optimisation models, or learning-based approaches. This paper focuses mainly on explicitly understanding and exploiting 4D geometrical cues for light field depth estimation. Thus, a novel method is proposed, based on a non-learning-based optimisation approach for depth estimation that explicitly considers surface normal accuracy and occlusion regions by utilising a fully explainable 4D geometric model of the light field. The 4D model performs depth/disparity estimation by determining the orientations and analysing the intersections of key 2D planes in 4D space, which are the images of 3D-space points in the 4D light field. Experimental results show that the proposed method outperforms both learning-based and non-learning-based state-of-the-art methods in terms of surface normal angle accuracy, achieving a Median Angle Error on planar surfaces, on average, 26.3\% lower than the state-of-the-art, and still being competitive with state-of-the-art methods in terms of Mean Squared Error $\vc{\times}$ 100 and Badpix 0.07.</details>
**Abstract_cn:** <details><summary>译文: </summary>光场相机和多相机阵列已成为通过被动捕获光信息来准确估计深度的有前景的解决方案。这是可能的，因为场景的 3D 信息嵌入在 4D 光场几何结构中。通常，深度估计方法依赖于梯度信息、基于启发式的优化模型或基于学习的方法来提取此信息。本文主要侧重于明确理解和利用 4D 几何线索进行光场深度估计。因此，提出了一种基于非学习的深度估计优化方法的新颖方法，该方法利用完全可解释的光场 4D 几何模型明确考虑表面法线精度和遮挡区域。 4D 模型通过确定方向并分析 4D 空间中关键 2D 平面（即 4D 光场中 3D 空间点的图像）的交集来执行深度/视差估计。实验结果表明，该方法在表面法线角度精度方面优于基于学习和非学习的最先进方法，在平面上实现中值角度误差平均降低 26.3%比最先进的方法，并且在均方误差 $\vc{\times}$ 100 和 Badpix 0.07 方面仍然与最先进的方法具有竞争力。</details>
**PDF:** <http://arxiv.org/pdf/2403.02043v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually Navigated Bronchoscopy**<br />
**Title_cn:** DD-VNB：基于深度的实时视觉导航支气管镜双环框架<br />
**Authors:** Qingyao Tian, Huai Liao, Xinyan Huang, Jian Chen, Zihui Zhang, Bingyu Yang, Sebastien Ourselin, Hongbin Liu<br />
**Abstract:** <details><summary>原文: </summary>Real-time 6 DOF localization of bronchoscopes is crucial for enhancing intervention quality. However, current vision-based technologies struggle to balance between generalization to unseen data and computational speed. In this study, we propose a Depth-based Dual-Loop framework for real-time Visually Navigated Bronchoscopy (DD-VNB) that can generalize across patient cases without the need of re-training. The DD-VNB framework integrates two key modules: depth estimation and dual-loop localization. To address the domain gap among patients, we propose a knowledge-embedded depth estimation network that maps endoscope frames to depth, ensuring generalization by eliminating patient-specific textures. The network embeds view synthesis knowledge into a cycle adversarial architecture for scale-constrained monocular depth estimation. For real-time performance, our localization module embeds a fast ego-motion estimation network into the loop of depth registration. The ego-motion inference network estimates the pose change of the bronchoscope in high frequency while depth registration against the pre-operative 3D model provides absolute pose periodically. Specifically, the relative pose changes are fed into the registration process as the initial guess to boost its accuracy and speed. Experiments on phantom and in-vivo data from patients demonstrate the effectiveness of our framework: 1) monocular depth estimation outperforms SOTA, 2) localization achieves an accuracy of Absolute Tracking Error (ATE) of 4.7 $\pm$ 3.17 mm in phantom and 6.49 $\pm$ 3.88 mm in patient data, 3) with a frame-rate approaching video capture speed, 4) without the necessity of case-wise network retraining. The framework's superior speed and accuracy demonstrate its promising clinical potential for real-time bronchoscopic navigation.</details>
**Abstract_cn:** <details><summary>译文: </summary>支气管镜的实时 6 DOF 定位对于提高干预质量至关重要。然而，当前基于视觉的技术很难在看不见的数据的泛化和计算速度之间取得平衡。在这项研究中，我们提出了一种用于实时视觉导航支气管镜检查（DD-VNB）的基于深度的双环框架，该框架可以在患者病例中进行推广，而无需重新训练。 DD-VNB框架集成了两个关键模块：深度估计和双环定位。为了解决患者之间的领域差距，我们提出了一种嵌入知识的深度估计网络，将内窥镜帧映射到深度，通过消除患者特定的纹理来确保泛化。该网络将视图合成知识嵌入到循环对抗架构中，以进行尺度受限的单目深度估计。为了实现实时性能，我们的定位模块将快速自我运动估计网络嵌入到深度配准循环中。自我运动推理网络以高频方式估计支气管镜的姿势变化，而针对术前 3D 模型的深度配准定期提供绝对姿势。具体来说，相对姿势的变化被输入配准过程作为初始猜测，以提高其准确性和速度。对来自患者的体模和体内数据的实验证明了我们框架的有效性：1）单目深度估计优于 SOTA，2）定位的绝对跟踪误差 (ATE) 精度在体模中达到 4.7 $\pm$ 3.17 mm，在体模中达到 6.49 mm $\pm$ 患者数据为 3.88 mm，3) 帧速率接近视频捕获速度，4) 无需根据具体情况进行网络重新训练。该框架卓越的速度和准确性证明了其在实时支气管镜导航方面具有广阔的临床潜力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01683v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement**<br />
**Title_cn:** 用于低光视频增强的时空对齐 SUNet 模型<br />
**Authors:** Ruirui Lin, Nantheera Anantrasirichai, Alexandra Malyugina, David Bull<br />
**Abstract:** <details><summary>原文: </summary>Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks. The restoration and enhancement have proven to be highly beneficial. However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin Transformer as a backbone to capture low light video features and exploit their spatio-temporal correlations. The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions. It is further analysed comparatively against various other models over three test datasets. The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values. It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results.</details>
**Abstract_cn:** <details><summary>译文: </summary>弱光条件引起的失真不仅在视觉上令人不愉快，而且还会降低计算机视觉任务的性能。事实证明，修复和增强是非常有益的。然而，针对低光条件下采集的视频明确设计的增强方法数量有限。我们提出了一种时空对齐 SUNet (STA-SUNet) 模型，使用 Swin Transformer 作为骨干来捕获低光视频特征并利用其时空相关性。 STA-SUNet 模型在一个新颖的、完全注册的数据集 (BVI) 上进行训练，该数据集包含在不同光照条件下捕获的动态场景。在三个测试数据集上与各种其他模型进行进一步比较分析。该模型在所有数据集上表现出卓越的适应性，获得最高的 PSNR 和 SSIM 值。它在极端低光条件下特别有效，可产生相当好的可视化结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.02408v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Neural Redshift: Random Networks are not Random Functions**<br />
**Title_cn:** 神经红移：随机网络不是随机函数<br />
**Authors:** Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad<br />
**Abstract:** <details><summary>原文: </summary>Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inherit all these properties from their building blocks.   Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们对神经网络（NN）泛化能力的理解仍然不完整。流行的解释基于梯度下降（GD）的隐式偏差，但它们无法解释无梯度方法模型的功能，也无法解释最近在未经训练的网络中观察到的简单性偏差。本文寻求神经网络泛化的其他来源。发现。为了理解独立于 GD 的架构提供的归纳偏差，我们检查了未经训练的随机权重网络。即使是简单的 MLP 也会表现出很强的归纳偏差：权重空间中的均匀采样会在复杂性方面产生非常有偏差的函数分布。但与常识不同的是，神经网络不存在固有的“简单性偏差”。此属性取决于 ReLU、残差连接和层归一化等组件。可以针对任何复杂程度来构建替代架构。变形金刚也从其构建块继承了所有这些属性。影响。我们为独立于梯度训练的深度学习的成功提供了全新的解释。它指出了控制经过训练的模型实施的解决方案的有希望的途径。</details>
**PDF:** <http://arxiv.org/pdf/2403.02241v2><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **TripoSR: Fast 3D Object Reconstruction from a Single Image**<br />
**Title_cn:** TripoSR：从单个图像快速重建 3D 对象<br />
**Authors:** Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao<br />
**Abstract:** <details><summary>原文: </summary>This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.</details>
**Abstract_cn:** <details><summary>译文: </summary>本技术报告介绍了 TripoSR，这是一种利用 Transformer 架构进行快速前馈 3D 生成的 3D 重建模型，可在 0.5 秒内从单个图像生成 3D 网格。 TripoSR 以 LRM 网络架构为基础，集成了数据处理、模型设计和训练技术方面的重大改进。对公共数据集的评估表明，与其他开源替代品相比，TripoSR 在数量和质量上都表现出卓越的性能。 TripoSR 在 MIT 许可下发布，旨在为研究人员、开发人员和创意人员提供 3D 生成 AI 的最新进展。</details>
**PDF:** <http://arxiv.org/pdf/2403.02151v1><br />
**Code:** <https://github.com/vast-ai-research/triposr>**<br />
>>**index:** 4<br />
**Title:** **Position Paper: Towards Implicit Prompt For Text-To-Image Models**<br />
**Title_cn:** 立场文件：走向文本到图像模型的隐式提示<br />
**Authors:** Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo<br />
**Abstract:** <details><summary>原文: </summary>Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit prompts. We call for increased attention to the potential and risks of implicit prompts in the T2I community and further investigation into the capabilities and impacts of implicit prompts, advocating for a balanced approach that harnesses their benefits while mitigating their risks.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的文本到图像（T2I）模型取得了巨大成功，并且提出了许多基准来评估其性能和安全性。然而，他们只考虑显式提示，而忽略隐式提示（暗示目标但未明确提及）。这些提示可能会摆脱安全约束，并对这些模型的应用构成潜在威胁。本立场文件强调了 T2I 模型在隐式提示方面的当前状态。我们提出了一个名为 ImplicitBench 的基准测试，并使用流行的 T2I 模型对隐式提示的性能和影响进行了调查。具体来说，我们设计并收集了通用符号、名人隐私和工作不安全（NSFW）问题三个方面的 2000 多个隐式提示，并评估了六种知名 T2I 模型在这些隐式提示下的能力。实验结果表明：（1）T2I模型能够准确创建隐式提示所指示的各种目标符号； (2)隐式提示给T2I模型带来潜在的隐私泄露风险。 (3) 大多数评估的 T2I 模型中 NSFW 的约束可以通过隐式提示绕过。我们呼吁在 T2I 社区中更多地关注隐式提示的潜力和风险，并进一步调查隐式提示的功能和影响，倡导采取平衡的方法，在利用其好处的同时降低其风险。</details>
**PDF:** <http://arxiv.org/pdf/2403.02118v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **DEMOS: Dynamic Environment Motion Synthesis in 3D Scenes via Local Spherical-BEV Perception**<br />
**Title_cn:** 演示：通过局部球形 BEV 感知在 3D 场景中进行动态环境运动合成<br />
**Authors:** Jingyu Gong, Min Wang, Wentao Liu, Chen Qian, Zhizhong Zhang, Yuan Xie, Lizhuang Ma<br />
**Abstract:** <details><summary>原文: </summary>Motion synthesis in real-world 3D scenes has recently attracted much attention. However, the static environment assumption made by most current methods usually cannot be satisfied especially for real-time motion synthesis in scanned point cloud scenes, if multiple dynamic objects exist, e.g., moving persons or vehicles. To handle this problem, we propose the first Dynamic Environment MOtion Synthesis framework (DEMOS) to predict future motion instantly according to the current scene, and use it to dynamically update the latent motion for final motion synthesis. Concretely, we propose a Spherical-BEV perception method to extract local scene features that are specifically designed for instant scene-aware motion prediction. Then, we design a time-variant motion blending to fuse the new predicted motions into the latent motion, and the final motion is derived from the updated latent motions, benefitting both from motion-prior and iterative methods. We unify the data format of two prevailing datasets, PROX and GTA-IM, and take them for motion synthesis evaluation in 3D scenes. We also assess the effectiveness of the proposed method in dynamic environments from GTA-IM and Semantic3D to check the responsiveness. The results show our method outperforms previous works significantly and has great performance in handling dynamic environments.</details>
**Abstract_cn:** <details><summary>译文: </summary>现实世界 3D 场景中的运动合成最近引起了广泛关注。然而，如果存在多个动态物体，例如移动的人或车辆，则大多数当前方法所做的静态环境假设通常不能满足，特别是对于扫描点云场景中的实时运动合成。为了解决这个问题，我们提出了第一个动态环境运动合成框架（DEMOS），可以根据当前场景即时预测未来的运动，并用它来动态更新潜在运动以进行最终的运动合成。具体来说，我们提出了一种 Spherical-BEV 感知方法来提取专为即时场景感知运动预测而设计的局部场景特征。然后，我们设计了一种时变运动混合，将新的预测运动融合到潜在运动中，最终的运动是从更新的潜在运动中导出的，受益于运动先验和迭代方法。我们统一了两个流行数据集PROX和GTA-IM的数据格式，并将它们用于3D场景中的运动合成评估。我们还评估了所提出的方法在 GTA-IM 和 Semantic3D 动态环境中的有效性，以检查响应能力。结果表明，我们的方法显着优于以前的工作，并且在处理动态环境方面具有出色的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.01740v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by Prior Knowledge for Hand-Object Interaction Scenario**<br />
**Title_cn:** 在手-物体交互场景的先验知识的指导下，通过聚合帧内图和帧间图来重建 3D 手部<br />
**Authors:** Feng Shuang, Wenbo He, Shaodong Li<br />
**Abstract:** <details><summary>原文: </summary>Recently, 3D hand reconstruction has gained more attention in human-computer cooperation, especially for hand-object interaction scenario. However, it still remains huge challenge due to severe hand-occlusion caused by interaction, which contain the balance of accuracy and physical plausibility, highly nonlinear mapping of model parameters and occlusion feature enhancement. To overcome these issues, we propose a 3D hand reconstruction network combining the benefits of model-based and model-free approaches to balance accuracy and physical plausibility for hand-object interaction scenario. Firstly, we present a novel MANO pose parameters regression module from 2D joints directly, which avoids the process of highly nonlinear mapping from abstract image feature and no longer depends on accurate 3D joints. Moreover, we further propose a vertex-joint mutual graph-attention model guided by MANO to jointly refine hand meshes and joints, which model the dependencies of vertex-vertex and joint-joint and capture the correlation of vertex-joint for aggregating intra-graph and inter-graph node features respectively. The experimental results demonstrate that our method achieves a competitive performance on recently benchmark datasets HO3DV2 and Dex-YCB, and outperforms all only model-base approaches and model-free approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，3D手部重建在人机协作中受到越来越多的关注，特别是对于手部物体交互场景。然而，由于交互引起的严重的手部遮挡，它仍然仍然是巨大的挑战，其中包括精度和物理合理性的平衡、模型参数的高度非线性映射和遮挡特征增强。为了克服这些问题，我们提出了一种 3D 手部重建网络，结合了基于模型和无模型方法的优点，以平衡手部物体交互场景的准确性和物理合理性。首先，我们提出了一种直接来自 2D 关节的新型 MANO 位姿参数回归模块，它避免了从抽象图像特征进行高度非线性映射的过程，并且不再依赖于精确的 3D 关节。此外，我们进一步提出了一种由 MANO 引导的顶点-关节相互图注意模型来联合细化手部网格和关节，该模型对顶点-顶点和关节-关节的依赖关系进行建模，并捕获顶点-关节的相关性以聚合图内和图间节点特征。实验结果表明，我们的方法在最近的基准数据集 HO3DV2 和 Dex-YCB 上实现了具有竞争力的性能，并且优于所有仅基于模型的方法和无模型方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.01733v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **A dataset of over one thousand computed tomography scans of battery cells**<br />
**Title_cn:** 超过一千次电池计算机断层扫描的数据集<br />
**Authors:** Amariah Condon, Bailey Buscarino, Eric Moch, William J. Sehnert, Owen Miles, Patrick K. Herring, Peter M. Attia<br />
**Abstract:** <details><summary>原文: </summary>Battery technology is increasingly important for global electrification efforts. However, batteries are highly sensitive to small manufacturing variations that can induce reliability or safety issues. An important technology for battery quality control is computed tomography (CT) scanning, which is widely used for non-destructive 3D inspection across a variety of clinical and industrial applications. Historically, however, the utility of CT scanning for high-volume manufacturing has been limited by its low throughput as well as the difficulty of handling its large file sizes. In this work, we present a dataset of over one thousand CT scans of as-produced commercially available batteries. The dataset spans various chemistries (lithium-ion and sodium-ion) as well as various battery form factors (cylindrical, pouch, and prismatic). We evaluate seven different battery types in total. The manufacturing variability and the presence of battery defects can be observed via this dataset. This dataset may be of interest to scientists and engineers working on battery technology, computer vision, or both.</details>
**Abstract_cn:** <details><summary>译文: </summary>电池技术对于全球电气化工作越来越重要。然而，电池对微小的制造变化非常敏感，这可能会引发可靠性或安全问题。电池质量控制的一项重要技术是计算机断层扫描 (CT) 扫描，该技术广泛用于各种临床和工业应用的无损 3D 检查。然而，从历史上看，CT 扫描在大批量制造中的实用性一直受到其低吞吐量以及处理大文件大小的困难的限制。在这项工作中，我们提供了超过一千张商用电池 CT 扫描数据集。该数据集涵盖各种化学成分（锂离子和钠离子）以及各种电池形状因素（圆柱形、袋装和棱柱形）。我们总共评估了七种不同的电池类型。通过该数据集可以观察制造的可变性和电池缺陷的存在。研究电池技术、计算机视觉或两者的科学家和工程师可能会对这个数据集感兴趣。</details>
**PDF:** <http://arxiv.org/pdf/2403.02527v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Twisting Lids Off with Two Hands**<br />
**Title_cn:** 用两只手拧开盖子<br />
**Authors:** Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, Jitendra Malik<br />
**Abstract:** <details><summary>原文: </summary>Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.</details>
**Abstract_cn:** <details><summary>译文: </summary>用两只多指手操纵物体一直是机器人技术中长期存在的挑战，这归因于许多操纵任务的接触丰富性以及协调高维双手系统固有的复杂性。在这项工作中，我们考虑用两只手扭转各种瓶状物体的盖子的问题，并证明使用深度强化学习进行模拟训练的策略可以有效地转移到现实世界。凭借对物理建模、实时感知和奖励设计的新颖工程见解，该策略展示了跨各种看不见的物体的泛化能力，展示了动态和灵巧的行为。我们的研究结果是令人信服的证据，表明深度强化学习与模拟到真实的迁移相结合仍然是解决前所未有的复杂性操纵问题的有前途的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.02338v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction**<br />
**Title_cn:** 用于时间分辨血管造影造影剂浓度重建的物理知情学习<br />
**Authors:** Noah Maul, Annette Birkhold, Fabian Wagner, Mareike Thies, Maximilian Rohleder, Philipp Berg, Markus Kowarschik, Andreas Maier<br />
**Abstract:** <details><summary>原文: </summary>Three-dimensional Digital Subtraction Angiography (3D-DSA) is a well-established X-ray-based technique for visualizing vascular anatomy. Recently, four-dimensional DSA (4D-DSA) reconstruction algorithms have been developed to enable the visualization of volumetric contrast flow dynamics through time-series of volumes. . This reconstruction problem is ill-posed mainly due to vessel overlap in the projection direction and geometric vessel foreshortening, which leads to information loss in the recorded projection images. However, knowledge about the underlying fluid dynamics can be leveraged to constrain the solution space. In our work, we implicitly include this information in a neural network-based model that is trained on a dataset of image-based blood flow simulations. The model predicts the spatially averaged contrast agent concentration for each centerline point of the vasculature over time, lowering the overall computational demand. The trained network enables the reconstruction of relative contrast agent concentrations with a mean absolute error of 0.02 $\pm$ 0.02 and a mean absolute percentage error of 5.31 % $\pm$ 9.25 %. Moreover, the network is robust to varying degrees of vessel overlap and vessel foreshortening. Our approach demonstrates the potential of the integration of machine learning and blood flow simulations in time-resolved angiographic flow reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>三维数字减影血管造影 (3D-DSA) 是一种成熟的基于 X 射线的血管解剖可视化技术。最近，四维 DSA (4D-DSA) 重建算法已被开发出来，可以通过体积的时间序列实现体积对比流动力学的可视化。 。该重建问题的不适定性主要是由于投影方向上的血管重叠和几何血管透视缩短，导致记录的投影图像中的信息丢失。然而，可以利用有关基础流体动力学的知识来限制解决方案空间。在我们的工作中，我们隐式地将这些信息包含在基于神经网络的模型中，该模型是在基于图像的血流模拟数据集上进行训练的。该模型预测脉管系统每个中心线点随时间的空间平均造影剂浓度，从而降低总体计算需求。经过训练的网络能够重建相对造影剂浓度，平均绝对误差为 0.02 $\pm$ 0.02，平均绝对百分比误差为 5.31 % $\pm$ 9.25 %。此外，该网络对于不同程度的船舶重叠和船舶缩短具有鲁棒性。我们的方法展示了机器学习和血流模拟在时间分辨血管造影血流重建中整合的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01993v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Tree Counting by Bridging 3D Point Clouds with Imagery**<br />
**Title_cn:** 通过将 3D 点云与图像桥接来进行树木计数<br />
**Authors:** Lei Li, Tianfang Zhang, Zhongyu Jiang, Cheng-Yen Yang, Jenq-Neng Hwang, Stefan Oehmcke, Dimitri Pierre Johannes Gominski, Fabian Gieseke, Christian Igel<br />
**Abstract:** <details><summary>原文: </summary>Accurate and consistent methods for counting trees based on remote sensing data are needed to support sustainable forest management, assess climate change mitigation strategies, and build trust in tree carbon credits. Two-dimensional remote sensing imagery primarily shows overstory canopy, and it does not facilitate easy differentiation of individual trees in areas with a dense canopy and does not allow for easy separation of trees when the canopy is dense. We leverage the fusion of three-dimensional LiDAR measurements and 2D imagery to facilitate the accurate counting of trees. We compare a deep learning approach to counting trees in forests using 3D airborne LiDAR data and 2D imagery. The approach is compared with state-of-the-art algorithms, like operating on 3D point cloud and 2D imagery. We empirically evaluate the different methods on the NeonTreeCount data set, which we use to define a tree-counting benchmark. The experiments show that FuseCountNet yields more accurate tree counts.</details>
**Abstract_cn:** <details><summary>译文: </summary>需要基于遥感数据的准确、一致的树木计数方法，以支持可持续森林管理、评估气候变化减缓战略并建立对树木碳信用额的信任。二维遥感影像主要显示林间树冠，在树冠茂密的地区不易区分单株树木，在树冠茂密时也不易区分树木。我们利用三维 LiDAR 测量和 2D 图像的融合来促进树木的准确计数。我们使用 3D 机载 LiDAR 数据和 2D 图像来比较深度学习方法来计算森林中的树木数量。该方法与最先进的算法（例如在 3D 点云和 2D 图像上运行）进行了比较。我们根据经验评估了 NeonTreeCount 数据集上的不同方法，并用它来定义树计数基准。实验表明 FuseCountNet 可以产生更准确的树木计数。</details>
**PDF:** <http://arxiv.org/pdf/2403.01932v2><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes**<br />
**Title_cn:** AiSDF：室内场景中的结构感知神经符号距离场<br />
**Authors:** Jaehoon Jang, Inha Lee, Minje Kim, Kyungdon Joo<br />
**Abstract:** <details><summary>原文: </summary>Indoor scenes we are living in are visually homogenous or textureless, while they inherently have structural forms and provide enough structural priors for 3D scene reconstruction. Motivated by this fact, we propose a structure-aware online signed distance fields (SDF) reconstruction framework in indoor scenes, especially under the Atlanta world (AW) assumption. Thus, we dub this incremental SDF reconstruction for AW as AiSDF. Within the online framework, we infer the underlying Atlanta structure of a given scene and then estimate planar surfel regions supporting the Atlanta structure. This Atlanta-aware surfel representation provides an explicit planar map for a given scene. In addition, based on these Atlanta planar surfel regions, we adaptively sample and constrain the structural regularity in the SDF reconstruction, which enables us to improve the reconstruction quality by maintaining a high-level structure while enhancing the details of a given scene. We evaluate the proposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate that the proposed framework is capable of reconstructing fine details of objects implicitly, as well as structures explicitly in room-scale scenes.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们生活的室内场景在视觉上是同质的或无纹理的，但它们本质上具有结构形式，并为 3D 场景重建提供足够的结构先验。受此事实的启发，我们提出了一种室内场景中的结构感知在线符号距离场（SDF）重建框架，特别是在亚特兰大世界（AW）假设下。因此，我们将 AW 的增量 SDF 重建称为 AiSDF。在在线框架内，我们推断给定场景的底层亚特兰大结构，然后估计支持亚特兰大结构的平面面元区域。这种亚特兰大感知的面元表示为给定场景提供了明确的平面地图。此外，基于这些亚特兰大平面面元区域，我们自适应采样并约束 SDF 重建中的结构规律，这使我们能够通过保持高级结构同时增强给定场景的细节来提高重建质量。我们在 ScanNet 和 ReplicaCAD 数据集上评估了所提出的 AiSDF，证明所提出的框架能够隐式重建对象的精细细节，以及房间尺度场景中显式的结构。</details>
**PDF:** <http://arxiv.org/pdf/2403.01861v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Superpixel Graph Contrastive Clustering with Semantic-Invariant Augmentations for Hyperspectral Images**<br />
**Title_cn:** 高光谱图像的超像素图对比聚类与语义不变增强<br />
**Authors:** Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou<br />
**Abstract:** <details><summary>原文: </summary>Hyperspectral images (HSI) clustering is an important but challenging task. The state-of-the-art (SOTA) methods usually rely on superpixels, however, they do not fully utilize the spatial and spectral information in HSI 3-D structure, and their optimization targets are not clustering-oriented. In this work, we first use 3-D and 2-D hybrid convolutional neural networks to extract the high-order spatial and spectral features of HSI through pre-training, and then design a superpixel graph contrastive clustering (SPGCC) model to learn discriminative superpixel representations. Reasonable augmented views are crucial for contrastive clustering, and conventional contrastive learning may hurt the cluster structure since different samples are pushed away in the embedding space even if they belong to the same class. In SPGCC, we design two semantic-invariant data augmentations for HSI superpixels: pixel sampling augmentation and model weight augmentation. Then sample-level alignment and clustering-center-level contrast are performed for better intra-class similarity and inter-class dissimilarity of superpixel embeddings. We perform clustering and network optimization alternatively. Experimental results on several HSI datasets verify the advantages of the proposed method, e.g., on India Pines, our model improves the clustering accuracy from 58.79% to 67.59% compared to the SOTA method.</details>
**Abstract_cn:** <details><summary>译文: </summary>高光谱图像（HSI）聚类是一项重要但具有挑战性的任务。最先进的（SOTA）方法通常依赖于超像素，但是它们没有充分利用 HSI 3-D 结构中的空间和光谱信息，并且它们的优化目标不是面向聚类的。在这项工作中，我们首先使用3D和2D混合卷积神经网络通过预训练提取HSI的高阶空间和光谱特征，然后设计一个超像素图对比聚类（SPGCC）模型来学习判别性超像素表示。合理的增强视图对于对比聚类至关重要，而传统的对比学习可能会损害聚类结构，因为不同的样本即使属于同一类，也会被推到嵌入空间中。在 SPGCC 中，我们为 HSI 超像素设计了两种语义不变的数据增强：像素采样增强和模型权重增强。然后进行样本级对齐和聚类中心级对比，以获得更好的超像素嵌入的类内相似性和类间相异性。我们交替执行聚类和网络优化。在多个 HSI 数据集上的实验结果验证了该方法的优势，例如，在 India Pines 上，与 SOTA 方法相比，我们的模型将聚类精度从 58.79% 提高到 67.59%。</details>
**PDF:** <http://arxiv.org/pdf/2403.01799v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Open-world Machine Learning: A Review and New Outlooks**<br />
**Title_cn:** 开放世界机器学习：回顾与新展望<br />
**Authors:** Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Cheng-Lin Liu<br />
**Abstract:** <details><summary>原文: </summary>Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a comprehensive introduction to the emerging open-world machine learning paradigm, to help researchers build more powerful AI systems in their respective fields, and to promote the development of artificial general intelligence.</details>
**Abstract_cn:** <details><summary>译文: </summary>机器学习在许多应用中取得了显着的成功。然而，现有的研究很大程度上基于封闭世界假设，即假设环境是静止的，模型一旦部署就固定下来。在许多实际应用中，这种基本且相当幼稚的假设可能不成立，因为开放环境是复杂的、动态的且充满未知数。在这种情况下，拒绝未知，发现新奇事物，然后逐步学习它们，可以使模型变得安全并像生物系统一样不断进化。本文通过研究统一范式中的未知拒绝、新类发现和类增量学习，提供了开放世界机器学习的整体视图。详细讨论了当前方法的挑战、原则和局限性。最后，我们讨论了未来研究的几个潜在方向。本文旨在全面介绍新兴的开放世界机器学习范式，帮助研究人员在各自领域构建更强大的人工智能系统，推动通用人工智能的发展。</details>
**PDF:** <http://arxiv.org/pdf/2403.01759v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **Towards Calibrated Deep Clustering Network**<br />
**Title_cn:** 迈向校准深度聚类网络<br />
**Authors:** Yuheng Jia, Jianhong Cheng, Hui Liu, Junhui Hou<br />
**Abstract:** <details><summary>原文: </summary>Deep clustering has exhibited remarkable performance; however, the overconfidence problem, i.e., the estimated confidence for a sample belonging to a particular cluster greatly exceeds its actual prediction accuracy, has been overlooked in prior research. To tackle this critical issue, we pioneer the development of a calibrated deep clustering framework. Specifically, we propose a novel dual-head deep clustering pipeline that can effectively calibrate the estimated confidence and the actual accuracy. The calibration head adjusts the overconfident predictions of the clustering head using regularization methods, generating prediction confidence and pseudo-labels that match the model learning status. This calibration process also guides the clustering head in dynamically selecting reliable high-confidence samples for training. Additionally, we introduce an effective network initialization strategy that enhances both training speed and network robustness. Extensive experiments demonstrate the proposed calibrated deep clustering framework not only surpasses state-of-the-art deep clustering methods by approximately 10 times in terms of expected calibration error but also significantly outperforms them in terms of clustering accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>深度聚类表现出了显着的性能；然而，过度自信问题，即属于特定簇的样本的估计置信度大大超过其实际预测精度，在先前的研究中被忽视了。为了解决这个关键问题，我们率先开发了经过校准的深度聚类框架。具体来说，我们提出了一种新颖的双头深度聚类管道，可以有效地校准估计的置信度和实际的准确性。校准头使用正则化方法调整聚类头的过度自信预测，生成与模型学习状态匹配的预测置信度和伪标签。该校准过程还指导聚类头动态选择可靠的高置信度样本进行训练。此外，我们引入了一种有效的网络初始化策略，可以提高训练速度和网络鲁棒性。大量实验表明，所提出的校准深度聚类框架不仅在预期校准误差方面超过了最先进的深度聚类方法约 10 倍，而且在聚类精度方面也显着优于它们。</details>
**PDF:** <http://arxiv.org/pdf/2403.02998v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging**<br />
**Title_cn:** 优化双曝光 HDR 成像中的光源估计<br />
**Authors:** Mahmoud Afifi, Zhenhua Hu, Liang Liang<br />
**Abstract:** <details><summary>原文: </summary>High dynamic range (HDR) imaging involves capturing a series of frames of the same scene, each with different exposure settings, to broaden the dynamic range of light. This can be achieved through burst capturing or using staggered HDR sensors that capture long and short exposures simultaneously in the camera image signal processor (ISP). Within camera ISP pipeline, illuminant estimation is a crucial step aiming to estimate the color of the global illuminant in the scene. This estimation is used in camera ISP white-balance module to remove undesirable color cast in the final image. Despite the multiple frames captured in the HDR pipeline, conventional illuminant estimation methods often rely only on a single frame of the scene. In this paper, we explore leveraging information from frames captured with different exposure times. Specifically, we introduce a simple feature extracted from dual-exposure images to guide illuminant estimators, referred to as the dual-exposure feature (DEF). To validate the efficiency of DEF, we employed two illuminant estimators using the proposed DEF: 1) a multilayer perceptron network (MLP), referred to as exposure-based MLP (EMLP), and 2) a modified version of the convolutional color constancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC achieve promising results, in some cases surpassing prior methods that require hundreds of thousands or millions of parameters, with only a few hundred parameters for EMLP and a few thousand parameters for ECCC.</details>
**Abstract_cn:** <details><summary>译文: </summary>高动态范围 (HDR) 成像涉及捕获同一场景的一系列帧，每个帧具有不同的曝光设置，以拓宽光线的动态范围。这可以通过连拍或使用交错的 HDR 传感器来实现，这些传感器在相机图像信号处理器 (ISP) 中同时捕获长曝光和短曝光。在相机 ISP 管道中，光源估计是关键的一步，旨在估计场景中全局光源的颜色。该估计用于相机 ISP 白平衡模块，以消除最终图像中不需要的色偏。尽管 HDR 管道中捕获了多个帧，但传统的光源估计方法通常仅依赖于场景的单个帧。在本文中，我们探索利用不同曝光时间捕获的帧中的信息。具体来说，我们引入了从双重曝光图像中提取的一个简单特征来指导光源估计器，称为双重曝光特征（DEF）。为了验证 DEF 的效率，我们使用了所提出的 DEF 的两个光源估计器：1）多层感知器网络（MLP），称为基于曝光的 MLP（EMLP），以及 2）卷积颜色恒常性的修改版本（ CCC）来集成我们的 DEF，我们称之为 ECCC。 EMLP 和 ECCC 都取得了可喜的结果，在某些情况下超越了先前需要数十万或数百万参数的方法，而 EMLP 只需要几百个参数，ECCC 只需要几千个参数。</details>
**PDF:** <http://arxiv.org/pdf/2403.02449v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Non-autoregressive Sequence-to-Sequence Vision-Language Models**<br />
**Title_cn:** 非自回归序列到序列视觉语言模型<br />
**Authors:** Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto<br />
**Abstract:** <details><summary>原文: </summary>Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.</details>
**Abstract_cn:** <details><summary>译文: </summary>序列到序列视觉语言模型显示出了希望，但由于其生成预测的自回归方式，其适用性受到推理延迟的限制。我们提出了一种并行解码序列到序列视觉语言模型，使用 Query-CTC 损失进行训练，该模型在解码器中的多个推理路径上边缘化。这使我们能够对令牌的联合分布进行建模，而不是像自回归模型那样限制于条件分布。由此产生的模型 NARVL 实现了与最先进的自回归模型相当的性能，但推理时间更快，从与令牌的顺序生成相关的线性复杂性降低到恒定时间联合推理的范例。</details>
**PDF:** <http://arxiv.org/pdf/2403.02249v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Interpretable Models for Detecting and Monitoring Elevated Intracranial Pressure**<br />
**Title_cn:** 用于检测和监测颅内压升高的可解释模型<br />
**Authors:** Darryl Hannan, Steven C. Nesbit, Ximing Wen, Glen Smith, Qiao Zhang, Alberto Goffi, Vincent Chan, Michael J. Morris, John C. Hunninghake, Nicholas E. Villalobos, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Detecting elevated intracranial pressure (ICP) is crucial in diagnosing and managing various neurological conditions. These fluctuations in pressure are transmitted to the optic nerve sheath (ONS), resulting in changes to its diameter, which can then be detected using ultrasound imaging devices. However, interpreting sonographic images of the ONS can be challenging. In this work, we propose two systems that actively monitor the ONS diameter throughout an ultrasound video and make a final prediction as to whether ICP is elevated. To construct our systems, we leverage subject matter expert (SME) guidance, structuring our processing pipeline according to their collection procedure, while also prioritizing interpretability and computational efficiency. We conduct a number of experiments, demonstrating that our proposed systems are able to outperform various baselines. One of our SMEs then manually validates our top system's performance, lending further credibility to our approach while demonstrating its potential utility in a clinical setting.</details>
**Abstract_cn:** <details><summary>译文: </summary>检测颅内压（ICP）升高对于诊断和治疗各种神经系统疾病至关重要。这些压力波动会传输到视神经鞘 (ONS)，导致其直径发生变化，然后可以使用超声成像设备进行检测。然而，解读 ONS 的超声图像可能具有挑战性。在这项工作中，我们提出了两个系统，可以在整个超声视频中主动监测 ONS 直径，并最终预测 ICP 是否升高。为了构建我们的系统，我们利用主题专家 (SME) 的指导，根据他们的收集程序构建我们的处理管道，同时还优先考虑可解释性和计算效率。我们进行了许多实验，证明我们提出的系统能够超越各种基线。然后，我们的一家中小企业手动验证了我们顶级系统的性能，进一步增强了我们方法的可信度，同时展示了其在临床环境中的潜在实用性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02236v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Perceptive self-supervised learning network for noisy image watermark removal**<br />
**Title_cn:** 用于去除噪声图像水印的感知自监督学习网络<br />
**Authors:** Chunwei Tian, Menghua Zheng, Bo Li, Yanning Zhang, Shichao Zhang, David Zhang<br />
**Abstract:** <details><summary>原文: </summary>Popular methods usually use a degradation model in a supervised way to learn a watermark removal model. However, it is true that reference images are difficult to obtain in the real world, as well as collected images by cameras suffer from noise. To overcome these drawbacks, we propose a perceptive self-supervised learning network for noisy image watermark removal (PSLNet) in this paper. PSLNet depends on a parallel network to remove noise and watermarks. The upper network uses task decomposition ideas to remove noise and watermarks in sequence. The lower network utilizes the degradation model idea to simultaneously remove noise and watermarks. Specifically, mentioned paired watermark images are obtained in a self supervised way, and paired noisy images (i.e., noisy and reference images) are obtained in a supervised way. To enhance the clarity of obtained images, interacting two sub-networks and fusing obtained clean images are used to improve the effects of image watermark removal in terms of structural information and pixel enhancement. Taking into texture information account, a mixed loss uses obtained images and features to achieve a robust model of noisy image watermark removal. Comprehensive experiments show that our proposed method is very effective in comparison with popular convolutional neural networks (CNNs) for noisy image watermark removal. Codes can be obtained at https://github.com/hellloxiaotian/PSLNet.</details>
**Abstract_cn:** <details><summary>译文: </summary>流行的方法通常以监督的方式使用退化模型来学习水印去除模型。然而，现实世界中参考图像确实很难获得，而且相机采集的图像也存在噪声。为了克服这些缺点，我们在本文中提出了一种用于噪声图像水印去除的感知自监督学习网络（PSLNet）。 PSLNet 依靠并行网络来去除噪声和水印。上层网络采用任务分解思想，依次去除噪声和水印。下层网络利用退化模型思想同时去除噪声和水印。具体地，以自监督方式获得上述配对水印图像，并以监督方式获得配对噪声图像（即，噪声图像和参考图像）。为了增强获得的图像的清晰度，通过两个子网络交互并融合获得的干净图像，从结构信息和像素增强方面提高图像水印去除的效果。考虑到纹理信息，混合损失使用获得的图像和特征来实现噪声图像水印去除的鲁棒模型。综合实验表明，与流行的卷积神经网络（CNN）相比，我们提出的方法在去除噪声图像水印方面非常有效。代码可以在https://github.com/helloxiaotian/PSLNet获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02211v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models**<br />
**Title_cn:** 使用地理空间基础模型的多光谱遥感图像检索<br />
**Authors:** Benedikt Blumenstiel, Viktoria Moor, Romeo Kienzler, Thomas Brunschwiler<br />
**Abstract:** <details><summary>原文: </summary>Image retrieval enables an efficient search through vast amounts of satellite imagery and returns similar images to a query. Deep learning models can identify images across various semantic concepts without the need for annotations. This work proposes to use Geospatial Foundation Models, like Prithvi, for remote sensing image retrieval with multiple benefits: i) the models encode multi-spectral satellite data and ii) generalize without further fine-tuning. We introduce two datasets to the retrieval task and observe a strong performance: Prithvi processes six bands and achieves a mean Average Precision of 97.62\% on BigEarthNet-43 and 44.51\% on ForestNet-12, outperforming other RGB-based models. Further, we evaluate three compression methods with binarized embeddings balancing retrieval speed and accuracy. They match the retrieval speed of much shorter hash codes while maintaining the same accuracy as floating-point embeddings but with a 32-fold compression. The code is available at https://github.com/IBM/remote-sensing-image-retrieval.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像检索可以对大量卫星图像进行高效搜索，并返回相似的图像进行查询。深度学习模型可以跨各种语义概念识别图像，而无需注释。这项工作建议使用像 Prithvi 这样的地理空间基础模型进行遥感图像检索，具有多种优点：i）模型编码多光谱卫星数据，ii）无需进一步微调即可进行概括。我们在检索任务中引入了两个数据集，并观察到了强大的性能：Prithvi 处理了 6 个波段，在 BigEarthNet-43 上实现了 97.62% 的平均精度，在 ForestNet-12 上实现了 44.51% 的平均精度，优于其他基于 RGB 的模型。此外，我们评估了三种具有二值化嵌入的压缩方法，平衡了检索速度和准确性。它们与更短的哈希码的检索速度相匹配，同时保持与浮点嵌入相同的精度，但压缩率为 32 倍。该代码可从 https://github.com/IBM/remote-sensing-image-retrieval 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02059v1><br />
**Code:** <https://github.com/ibm/remote-sensing-image-retrieval>**<br />
>>**index:** 7<br />
**Title:** **TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions**<br />
**Title_cn:** TTA-Nav：视觉损坏下点目标导航的测试时自适应重建<br />
**Authors:** Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan<br />
**Abstract:** <details><summary>原文: </summary>Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our "plug-and-play" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance across almost all visual corruptions in our benchmarks. Our method improves the success rate of point-goal navigation from the state-of-the-art result of 46% to 94% on the most severe corruption. This suggests its potential for broader application in robotic visual navigation.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉损坏下的机器人导航提出了巨大的挑战。为了解决这个问题，我们提出了一种测试时间适应（TTA）方法，称为 TTA-Nav，用于视觉损坏下的点目标导航。我们的“即插即用”方法将自上而下的解码器合并到预先训练的导航模型中。首先，预训练的导航模型获取损坏的图像并提取特征。其次，自上而下的解码器根据预训练模型提取的高级特征进行重建。然后，它将损坏图像的重建反馈给预先训练的模型。最后，预训练模型再次前向传递以输出动作。尽管仅在干净图像上进行训练，但自上而下的解码器可以从损坏的图像中重建更干净的图像，而不需要基于梯度的适应。带有自上而下解码器的预训练导航模型显着增强了基准测试中几乎所有视觉损坏的导航性能。对于最严重的腐败，我们的方法将点目标导航的成功率从最先进的 46% 提高到 94%。这表明它在机器人视觉导航中具有更广泛应用的潜力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01977v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Advancing Gene Selection in Oncology: A Fusion of Deep Learning and Sparsity for Precision Gene Selection**<br />
**Title_cn:** 推进肿瘤学基因选择：深度学习和稀疏性的融合实现精准基因选择<br />
**Authors:** Akhila Krishna, Ravi Kant Gupta, Pranav Jeevan, Amit Sethi<br />
**Abstract:** <details><summary>原文: </summary>Gene selection plays a pivotal role in oncology research for improving outcome prediction accuracy and facilitating cost-effective genomic profiling for cancer patients. This paper introduces two gene selection strategies for deep learning-based survival prediction models. The first strategy uses a sparsity-inducing method while the second one uses importance based gene selection for identifying relevant genes. Our overall approach leverages the power of deep learning to model complex biological data structures, while sparsity-inducing methods ensure the selection process focuses on the most informative genes, minimizing noise and redundancy. Through comprehensive experimentation on diverse genomic and survival datasets, we demonstrate that our strategy not only identifies gene signatures with high predictive power for survival outcomes but can also streamlines the process for low-cost genomic profiling. The implications of this research are profound as it offers a scalable and effective tool for advancing personalized medicine and targeted cancer therapies. By pushing the boundaries of gene selection methodologies, our work contributes significantly to the ongoing efforts in cancer genomics, promising improved diagnostic and prognostic capabilities in clinical settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>基因选择在肿瘤学研究中发挥着关键作用，可提高结果预测的准确性并促进癌症患者的经济高效的基因组分析。本文介绍了基于深度学习的生存预测模型的两种基因选择策略。第一个策略使用稀疏诱导方法，而第二个策略使用基于重要性的基因选择来识别相关基因。我们的整体方法利用深度学习的力量来建模复杂的生物数据结构，而稀疏性诱导方法确保选择过程专注于信息最丰富的基因，最大限度地减少噪音和冗余。通过对不同基因组和生存数据集的全面实验，我们证明我们的策略不仅可以识别对生存结果具有高预测能力的基因特征，而且还可以简化低成本基因组分析的过程。这项研究的意义深远，因为它为推进个性化医疗和靶向癌症治疗提供了可扩展且有效的工具。通过突破基因选择方法的界限，我们的工作为癌症基因组学的持续努力做出了重大贡献，有望提高临床环境中的诊断和预后能力。</details>
**PDF:** <http://arxiv.org/pdf/2403.01927v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Revisiting Learning-based Video Motion Magnification for Real-time Processing**<br />
**Title_cn:** 重新审视基于学习的视频运动放大以进行实时处理<br />
**Authors:** Hyunwoo Ha, Oh Hyun-Bin, Kim Jun-Seong, Kwon Byung-Ki, Kim Sung-Bin, Linh-Tam Tran, Ji-Yun Kim, Sung-Ho Bae, Tae-Hyun Oh<br />
**Abstract:** <details><summary>原文: </summary>Video motion magnification is a technique to capture and amplify subtle motion in a video that is invisible to the naked eye. The deep learning-based prior work successfully demonstrates the modelling of the motion magnification problem with outstanding quality compared to conventional signal processing-based ones. However, it still lags behind real-time performance, which prevents it from being extended to various online applications. In this paper, we investigate an efficient deep learning-based motion magnification model that runs in real time for full-HD resolution videos. Due to the specified network design of the prior art, i.e. inhomogeneous architecture, the direct application of existing neural architecture search methods is complicated. Instead of automatic search, we carefully investigate the architecture module by module for its role and importance in the motion magnification task. Two key findings are 1) Reducing the spatial resolution of the latent motion representation in the decoder provides a good trade-off between computational efficiency and task quality, and 2) surprisingly, only a single linear layer and a single branch in the encoder are sufficient for the motion magnification task. Based on these findings, we introduce a real-time deep learning-based motion magnification model with4.2X fewer FLOPs and is 2.7X faster than the prior art while maintaining comparable quality.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频运动放大是一种捕捉和放大视频中肉眼不可见的细微运动的技术。基于深度学习的先前工作成功地证明了运动放大问题的建模，与传统的基于信号处理的建模相比，具有出色的质量。然而，它仍然落后于实时性能，这阻碍了它扩展到各种在线应用程序。在本文中，我们研究了一种高效的基于深度学习的运动放大模型，该模型可以实时运行全高清分辨率视频。由于现有技术的特定网络设计，即非同质架构，现有神经架构搜索方法的直接应用是复杂的。我们没有自动搜索，而是逐个模块地仔细研究架构，了解其在运动放大任务中的作用和重要性。两个关键发现是 1) 降低解码器中潜在运动表示的空间分辨率在计算效率和任务质量之间提供了良好的权衡，2) 令人惊讶的是，编码器中仅单个线性层和单个分支就足够了用于运动放大任务。基于这些发现，我们引入了一种基于实时深度学习的运动放大模型，其 FLOP 次数减少了 4.2 倍，速度比现有技术快 2.7 倍，同时保持了相当的质量。</details>
**PDF:** <http://arxiv.org/pdf/2403.01898v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis**<br />
**Title_cn:** PLACE：用于语义图像合成的自适应布局-语义融合<br />
**Authors:** Zhengyao Lv, Yuxiang Wei, Wangmeng Zuo, Kwan-Yee K. Wong<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in large-scale pre-trained text-to-image models have led to remarkable progress in semantic image synthesis. Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge. In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues. Specifically, we first employ the layout control map to faithfully represent layouts in the feature space. Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details. During fine-tuning, we propose the Semantic Alignment (SA) loss to further enhance layout alignment. Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images. Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment. The source code and model are available at https://github.com/cszy98/PLACE/tree/main.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模预训练文本到图像模型的最新进展导致语义图像合成取得了显着进展。然而，合成具有一致语义和布局的高质量图像仍然是一个挑战。在本文中，我们提出了自适应布局语义融合模块（PLACE），它利用预先训练的模型来缓解上述问题。具体来说，我们首先使用布局控制图来忠实地表示特征空间中的布局。随后，我们以时间步自适应的方式结合布局和语义特征来合成具有真实细节的图像。在微调过程中，我们提出语义对齐（SA）损失以进一步增强布局对齐。此外，我们引入了无布局先验保留（LFP）损失，它利用未标记的数据来维护预训练模型的先验，从而提高合成图像的视觉质量和语义一致性。大量的实验表明，我们的方法在视觉质量、语义一致性和布局对齐方面表现良好。源代码和模型可在 https://github.com/cszy98/PLACE/tree/main 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01852v1><br />
**Code:** <https://github.com/cszy98/place>**<br />
>>**index:** 11<br />
**Title:** **One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models**<br />
**Title_cn:** 一个提示词足以提高预训练视觉语言模型的对抗鲁棒性<br />
**Authors:** Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling<br />
**Abstract:** <details><summary>原文: </summary>Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.</details>
**Abstract_cn:** <details><summary>译文: </summary>像 CLIP 这样的大型预训练视觉语言模型 (VLM)，尽管具有出色的泛化能力，但很容易受到对抗性示例的影响。这项工作从文本提示的新颖角度而不是广泛研究的模型权重（在这项工作中冻结）来研究 VLM 的对抗鲁棒性。我们首先表明，对抗性攻击和防御的有效性对所使用的文本提示都很敏感。受此启发，我们提出了一种方法，通过学习 VLM 的强大文本提示来提高对抗攻击的弹性。所提出的方法称为对抗性提示调整（APT），在计算和数据效率方面都很有效。在 15 个数据集和 4 个数据稀疏方案（从 1-shot 到完整训练数据设置）中进行了大量实验，以显示 APT 相对于手工设计的提示和其他最先进的适应方法的优越性。 APT 在分布内性能和输入分布变化下以及跨数据集的泛化方面表现出了出色的能力。令人惊讶的是，通过简单地将一个学到的单词添加到提示中，APT 可以比手工设计的提示显着提高准确性和鲁棒性 (epsilon=4/255)，平均分别提高 +13% 和 +8.5%。在我们最有效的设置中，准确性进一步提高到 +26.4%，鲁棒性提高到 +16.7%。代码可在 https://github.com/TreeLLi/APT 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.01849v1><br />
**Code:** <https://github.com/treelli/apt>**<br />
>>**index:** 12<br />
**Title:** **AtomoVideo: High Fidelity Image-to-Video Generation**<br />
**Title_cn:** AtomoVideo：高保真图像到视频生成<br />
**Authors:** Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge, Bo Zheng<br />
**Abstract:** <details><summary>原文: </summary>Recently, video generation has achieved significant rapid development based on superior text-to-image generation techniques. In this work, we propose a high fidelity framework for image-to-video generation, named AtomoVideo. Based on multi-granularity image injection, we achieve higher fidelity of the generated video to the given image. In addition, thanks to high quality datasets and training strategies, we achieve greater motion intensity while maintaining superior temporal consistency and stability. Our architecture extends flexibly to the video frame prediction task, enabling long sequence prediction through iterative generation. Furthermore, due to the design of adapter training, our approach can be well combined with existing personalized models and controllable modules. By quantitatively and qualitatively evaluation, AtomoVideo achieves superior results compared to popular methods, more examples can be found on our project website: https://atomo-video.github.io/.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近，基于卓越的文本到图像生成技术，视频生成取得了显着的快速发展。在这项工作中，我们提出了一种用于图像到视频生成的高保真框架，名为 AtomoVideo。基于多粒度图像注入，我们实现了生成的视频对给定图像的更高保真度。此外，得益于高质量的数据集和训练策略，我们在保持卓越的时间一致性和稳定性的同时实现了更大的运动强度。我们的架构灵活地扩展到视频帧预测任务，通过迭代生成实现长序列预测。此外，由于适配器训练的设计，我们的方法可以很好地与现有的个性化模型和可控模块结合。通过定量和定性评估，AtomoVideo 与流行的方法相比取得了优异的结果，更多示例可以在我们的项目网站上找到：https://atomo-video.github.io/。</details>
**PDF:** <http://arxiv.org/pdf/2403.01800v2><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Improving Visual Perception of a Social Robot for Controlled and In-the-wild Human-robot Interaction**<br />
**Title_cn:** 改善社交机器人的视觉感知，以实现受控和野外人机交互<br />
**Authors:** Wangjie Zhong, Leimin Tian, Duy Tho Le, Hamid Rezatofighi<br />
**Abstract:** <details><summary>原文: </summary>Social robots often rely on visual perception to understand their users and the environment. Recent advancements in data-driven approaches for computer vision have demonstrated great potentials for applying deep-learning models to enhance a social robot's visual perception. However, the high computational demands of deep-learning methods, as opposed to the more resource-efficient shallow-learning models, bring up important questions regarding their effects on real-world interaction and user experience. It is unclear how will the objective interaction performance and subjective user experience be influenced when a social robot adopts a deep-learning based visual perception model. We employed state-of-the-art human perception and tracking models to improve the visual perception function of the Pepper robot and conducted a controlled lab study and an in-the-wild human-robot interaction study to evaluate this novel perception function for following a specific user with other people present in the scene.</details>
**Abstract_cn:** <details><summary>译文: </summary>社交机器人通常依靠视觉感知来了解用户和环境。计算机视觉数据驱动方法的最新进展表明，应用深度学习模型来增强社交机器人的视觉感知具有巨大潜力。然而，与资源效率更高的浅层学习模型相比，深度学习方法的高计算要求提出了有关其对现实世界交互和用户体验的影响的重要问题。目前尚不清楚当社交机器人采用基于深度学习的视觉感知模型时，将如何影响客观交互性能和主观用户体验。我们采用最先进的人类感知和跟踪模型来改善 Pepper 机器人的视觉感知功能，并进行了受控实验室研究和野外人机交互研究，以评估这种新颖的感知功能，以用于以下目的特定用户与场景中存在的其他人。</details>
**PDF:** <http://arxiv.org/pdf/2403.01766v1><br />
**Code:** null<br />

