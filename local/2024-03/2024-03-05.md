## [UPDATED!] **2024-03-05** (Publish Time)

>## **生成模型**
>---
>>**index:** 1<br />
**Title:** **Scaling Rectified Flow Transformers for High-Resolution Image Synthesis**<br />
**Title_cn:** 缩放整流流量变压器以实现高分辨率图像合成<br />
**Authors:** Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.</details>
**Abstract_cn:** <details><summary>译文: </summary>扩散模型通过将数据的前向路径反转为噪声来从噪声中创建数据，并且已成为一种强大的生成建模技术，适用于图像和视频等高维感知数据。整流流是一种最新的生成模型公式，它将数据和噪声以直线连接。尽管其具有更好的理论特性和概念简单性，但它尚未被明确确立为标准实践。在这项工作中，我们改进了现有的噪声采样技术，通过将修正流模型偏向感知相关的尺度来训练它们。通过大规模研究，我们证明了与用于高分辨率文本到图像合成的现有扩散公式相比，这种方法具有优越的性能。此外，我们提出了一种新颖的基于变压器的文本到图像生成架构，该架构对两种模式使用单独的权重，并实现图像和文本标记之间的双向信息流，从而改善文本理解、排版和人类偏好评级。我们证明，该架构遵循可预测的扩展趋势，并将较低的验证损失与通过各种指标和人工评估衡量的改进的文本到图像合成相关联。我们最大的模型优于最先进的模型，我们将公开我们的实验数据、代码和模型权重。</details>
**PDF:** <http://arxiv.org/pdf/2403.03206v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process**<br />
**Title_cn:** Triple-CFN：重构概念空间以增强抽象推理过程<br />
**Authors:** Ruizhuo Song, Beiming Yuan<br />
**Abstract:** <details><summary>原文: </summary>Abstract reasoning problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks. This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly reorganizing the concept space of conflicting instances. Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results. To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns. The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing reasoning information. Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN. This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract reasoning problems, paving the way for further breakthroughs in this domain.</details>
**Abstract_cn:** <details><summary>译文: </summary>抽象推理问题对人工智能算法提出了重大挑战，要求的认知能力超出了感知任务所需的能力。本研究引入了 Triple-CFN 方法来解决 Bongard-Logo 问题，通过隐式重组冲突实例的概念空间来实现显着的推理准确性。此外，事实证明，经过必要的修改，Triple-CFN 范式对于 RPM 问题是有效的，并产生了有竞争力的结果。为了进一步提高 RPM 问题的性能，我们开发了 Meta Triple-CFN 网络，该网络明确地构建问题空间，同时保持渐进模式的可解释性。 Meta Triple-CFN 的成功归因于其概念空间建模范式，相当于规范化推理信息。基于这种思想，我们引入了Re-space层，增强了Meta Triple-CFN和Triple-CFN的性能。本文旨在通过探索解决抽象推理问题的创新网络设计，为机器智能的进步做出贡献，为该领域的进一步突破铺平道路。</details>
**PDF:** <http://arxiv.org/pdf/2403.03190v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors**<br />
**Title_cn:** NRDF：用于学习铰接姿势先验的神经黎曼距离场<br />
**Authors:** Yannan He, Garvita Tiwari, Tolga Birdal, Jan Eric Lenssen, Gerard Pons-Moll<br />
**Abstract:** <details><summary>原文: </summary>Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.</details>
**Abstract_cn:** <details><summary>译文: </summary>忠实地对关节空间进行建模是一项至关重要的任务，它允许恢复和生成逼真的姿势，并且仍然是一个臭名昭著的挑战。为此，我们引入神经黎曼距离场（NRDF），数据驱动的先验模型对合理的关节空间进行建模，表示为高维乘积四元数空间中神经场的零水平集。为了仅在正面示例上训练 NRDF，我们引入了一种新的采样算法，确保测地距离遵循所需的分布，从而产生有原则的距离场学习范例。然后，我们设计了一种投影算法，通过自适应步长黎曼优化器将任何随机姿态映射到水平集上，始终遵循关节旋转的乘积流形。 NRDF 可以通过反向传播和数学类比来计算黎曼梯度，与黎曼流匹配（一种最新的生成模型）相关。我们在各种下游任务（即姿态生成、基于图像的姿态估计和求解逆运动学）中对 NRDF 与其他姿态先验进行了综合评估，突出了 NRDF 的优越性能。除了人类之外，NRDF 的多功能性还延伸到手部和动物姿势，因为它可以有效地代表任何关节。</details>
**PDF:** <http://arxiv.org/pdf/2403.03122v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Doubly Abductive Counterfactual Inference for Text-based Image Editing**<br />
**Title_cn:** 基于文本的图像编辑的双重溯因反事实推理<br />
**Authors:** Xue Song, Jiequan Cui, Hanwang Zhang, Jingjing Chen, Richang Hong, Yu-Gang Jiang<br />
**Abstract:** <details><summary>原文: </summary>We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning. To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in https://github.com/xuesong39/DAC.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们通过反事实推理研究单个图像的基于文本的图像编辑（TBIE），因为它是精确满足要求的优雅公式：编辑后的图像应保留原始图像的保真度。通过公式的镜头，我们发现TBIE的症结在于现有技术很难在可编辑性和保真度之间实现良好的权衡，这主要是由于单图像微调的过度拟合。为此，我们提出了双重溯因反事实推理框架（DAC）。我们首先将一个外生变量参数化为 UNet LoRA，它的溯因可以对所有图像细节进行编码。其次，我们绑架了另一个由文本编码器 LoRA 参数化的外生变量，它恢复了因第一次绑架过度拟合而导致的可编辑性丢失。由于第二次溯因专门编码了从后期编辑到预编辑的视觉转换，其反转（减去 LoRA）有效地将预编辑恢复到后期编辑，从而完成编辑。通过大量的实验，我们的 DAC 在可编辑性和保真度之间实现了良好的权衡。因此，我们可以支持广泛的用户编辑意图，包括添加、删除、操纵、替换、风格转移和面部变化，这些意图在定性和定量评估中得到了广泛的验证。代码位于 https://github.com/xuesong39/DAC。</details>
**PDF:** <http://arxiv.org/pdf/2403.02981v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity**<br />
**Title_cn:** 具有文本引导编码的神经图像压缩，可实现像素级和感知保真度<br />
**Authors:** Hagyeong Lee, Minkyu Kim, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho Lee<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions.</details>
**Abstract_cn:** <details><summary>译文: </summary>文本引导图像压缩的最新进展显示出增强重建图像感知质量的巨大潜力。然而，这些方法往往会显着降低像素保真度，从而限制了它们的实用性。为了填补这一空白，我们开发了一种新的文本引导图像压缩算法，该算法可实现高感知和像素级保真度。特别是，我们提出了一种压缩框架，主要通过文本自适应编码和联合图像文本损失训练来利用文本信息。通过这样做，我们避免了基于文本引导的生成模型（以高生成多样性而闻名）的解码，并在全球范围内有效地利用文本的语义信息。各种数据集的实验结果表明，我们的方法可以通过人类或机器生成的字幕实现高像素级和感知质量。特别是，我们的方法在 LPIPS 方面优于所有基线，当我们使用更仔细生成的字幕时，还有进一步改进的空间。</details>
**PDF:** <http://arxiv.org/pdf/2403.02944v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Cross-Domain Image Conversion by CycleDM**<br />
**Title_cn:** CycleDM 的跨域图像转换<br />
**Authors:** Sho Shimotsumagari, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida<br />
**Abstract:** <details><summary>原文: </summary>The purpose of this paper is to enable the conversion between machine-printed character images (i.e., font images) and handwritten character images through machine learning. For this purpose, we propose a novel unpaired image-to-image domain conversion method, CycleDM, which incorporates the concept of CycleGAN into the diffusion model. Specifically, CycleDM has two internal conversion models that bridge the denoising processes of two image domains. These conversion models are efficiently trained without explicit correspondence between the domains. By applying machine-printed and handwritten character images to the two modalities, CycleDM realizes the conversion between them. Our experiments for evaluating the converted images quantitatively and qualitatively found that ours performs better than other comparable approaches.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文的目的是通过机器学习实现机器打印的字符图像（即字体图像）和手写字符图像之间的转换。为此，我们提出了一种新颖的不成对图像到图像域转换方法 CycleDM，它将 CycleGAN 的概念融入到扩散模型中。具体来说，CycleDM 有两个内部转换模型，可以桥接两个图像域的去噪过程。这些转换模型可以有效地进行训练，而无需域之间的明确对应。 CycleDM通过将机器打印和手写字符图像应用于这两种模式，实现了它们之间的转换。我们对转换图像进行定量和定性评估的实验发现，我们的方法比其他类似方法表现更好。</details>
**PDF:** <http://arxiv.org/pdf/2403.02919v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders**<br />
**Title_cn:** 使用条件扩散解码器增强学习图像编解码器的率失真感知灵活性<br />
**Authors:** Daniele Mari, Simone Milani<br />
**Abstract:** <details><summary>原文: </summary>Learned image compression codecs have recently achieved impressive compression performances surpassing the most efficient image coding architectures. However, most approaches are trained to minimize rate and distortion which often leads to unsatisfactory visual results at low bitrates since perceptual metrics are not taken into account. In this paper, we show that conditional diffusion models can lead to promising results in the generative compression task when used as a decoder, and that, given a compressed representation, they allow creating new tradeoff points between distortion and perception at the decoder side based on the sampling method.</details>
**Abstract_cn:** <details><summary>译文: </summary>学习的图像压缩编解码器最近取得了令人印象深刻的压缩性能，超越了最有效的图像编码架构。然而，大多数方法都经过训练以最小化速率和失真，这通常会导致低比特率下的视觉结果不令人满意，因为没有考虑感知指标。在本文中，我们表明，当用作解码器时，条件扩散模型可以在生成压缩任务中带来有希望的结果，并且，在给定压缩表示的情况下，它们允许在解码器端的失真和感知之间创建新的权衡点抽样方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.02887v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement**<br />
**Title_cn:** Zero-LED：用于低光图像增强的零参考照明估计扩散模型<br />
**Authors:** Jinhong He, Minglong Xue, Zhipu Liu, Chengyun Song, Senming Zhong<br />
**Abstract:** <details><summary>原文: </summary>Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper.</details>
**Abstract_cn:** <details><summary>译文: </summary>基于扩散模型的低光图像增强方法严重依赖配对训练数据，导致其广泛应用受到限制。同时，现有的无监督方法缺乏针对未知降解的有效桥接能力。为了解决这些限制，我们提出了一种用于低光图像增强的新型零参考照明估计扩散模型，称为零 LED。它利用扩散模型的稳定收敛能力来弥合弱光域和真实正常光域之间的差距，并通过零参考学习成功减轻对成对训练数据的依赖。具体来说，我们首先设计初始优化网络对输入图像进行预处理，并通过多个目标函数实现扩散模型和初始优化网络之间的双向约束。随后，迭代优化真实场景的退化因子，以实现有效的光增强。此外，我们探索了一种基于频域和语义引导的外观重建模块，该模块鼓励在细粒度级别上恢复图像的特征对齐并满足主观期望。最后，大量的实验证明了我们的方法相对于其他最先进的方法的优越性和更显着的泛化能力。论文被接受后，我们将开放源代码。</details>
**PDF:** <http://arxiv.org/pdf/2403.02879v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation**<br />
**Title_cn:** 用于高保真图像到视频生成的免调谐噪声校正<br />
**Authors:** Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, Bo Zheng<br />
**Abstract:** <details><summary>原文: </summary>Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: https://noise-rectification.github.io.</details>
**Abstract_cn:** <details><summary>译文: </summary>图像到视频（I2V）生成任务总是受到在开放域中保持高保真度的困扰。传统的图像动画技术主要关注特定领域，例如面部或人体姿势，这使得它们很难推广到开放领域。最近的几个基于扩散模型的 I2V 框架可以为开放域图像生成动态内容，但无法保持保真度。我们发现低保真度的两个主要因素是图像细节的丢失和去噪过程中的噪声预测偏差。为此，我们提出了一种可应用于主流视频扩散模型的有效方法。该方法通过补充更精确的图像信息和噪声校正来实现高保真度。具体来说，给定指定的图像，我们的方法首先向输入图像潜在添加噪声以保留更多细节，然后通过适当的校正对噪声潜在进行去噪以减轻噪声预测偏差。我们的方法是免调整且即插即用的。实验结果证明了我们的方法在提高生成视频的保真度方面的有效性。更多图像转视频生成结果请参考项目网站：https://noise-rectification.github.io。</details>
**PDF:** <http://arxiv.org/pdf/2403.02827v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models**<br />
**Title_cn:** 使用生成基础模型对地球系统模型场进行快速、尺度自适应和不确定性感知缩减<br />
**Authors:** Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers<br />
**Abstract:** <details><summary>原文: </summary>Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task. Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确和高分辨率的地球系统模型（ESM）模拟对于评估人为气候变化的生态和社会经济影响至关重要，但计算成本太高。最近的机器学习方法在缩小 ESM 模拟方面显示出了有希望的结果，优于最先进的统计方法。然而，现有方法需要对每个 ESM 进行计算成本高昂的再训练，并且很难推断出训练期间未见过的气候。我们通过学习一致性模型（CM）来解决这些缺点，该模型可以有效、准确地缩小任意 ESM 模拟的规模，而无需以零样本方式进行重新训练。我们的基础模型方法产生的概率缩小场的分辨率仅受观测参考数据的限制。我们表明，CM 以一小部分计算成本优于最先进的扩散模型，同时保持降尺度任务的高度可控性。此外，我们的方法可以推广到训练期间未见的气候状态，而无需明确制定物理约束。</details>
**PDF:** <http://arxiv.org/pdf/2403.02774v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Few-shot Learner Parameterization by Diffusion Time-steps**<br />
**Title_cn:** 通过扩散时间步长进行少样本学习器参数化<br />
**Authors:** Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun<br />
**Abstract:** <details><summary>原文: </summary>Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif.</details>
**Abstract_cn:** <details><summary>译文: </summary>即使使用大型多模态基础模型，小样本学习仍然具有挑战性——如果没有适当的归纳偏差，几乎不可能保留细致入微的类属性，同时删除与类标签虚假相关的视觉突出属性。为此，我们发现了一个归纳偏差，即扩散模型（DM）的时间步可以隔离细微的类属性，即，当前向扩散在每个时间步向图像添加噪声时，细微的属性通常会丢失比视觉上突出的虚假属性更早的时间步长。在此基础上，我们提出了时间步少样本（TiF）学习器。我们为文本条件 DM 训练特定于类别的低阶适配器，以弥补丢失的属性，以便在给出提示的情况下可以根据噪声图像准确地重建图像。因此，在较小的时间步长内，适配器和提示本质上只是细微差别的类属性的参数化。对于测试图像，我们可以使用参数化来仅提取细微的类属性以进行分类。 TiF 学习器在各种细粒度和定制的小样本学习任务上显着优于 OpenCLIP 及其适配器。代码位于https://github.com/yue-zhongqi/tif。</details>
**PDF:** <http://arxiv.org/pdf/2403.02649v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning**<br />
**Title_cn:** 通过概率感知学习增强弱监督 3D 医学图像分割<br />
**Authors:** Zhaoxin Fan, Runmin Jiang, Junhao Wu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu<br />
**Abstract:** <details><summary>原文: </summary>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware weakly supervised learning pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a probability-based pseudo-label generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head Self-Attention network for robust feature extraction within our Probabilistic Transformer Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully supervised methods but also surpasses existing weakly supervised methods in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at https://github.com/runminjiang/PW4MedSeg.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 医学图像分割是一项具有挑战性的任务，对疾病诊断和治疗计划具有重要意义。深度学习的最新进展显着增强了完全监督的医学图像分割。然而，这种方法严重依赖于劳动密集型且耗时的完全注释的地面实况标签，特别是对于 3D 体积。为了克服这一限制，我们提出了一种新颖的概率感知弱监督学习流程，专为 3D 医学成像而设计。我们的管道集成了三个创新组件：基于概率的伪标签生成技术，用于从稀疏注释合成密集分割掩模，概率多头自注意力网络，用于在概率变换器网络中提取稳健的特征，以及概率通知分割损失函数通过注释置信度来增强训练。我们的方法展示了显着的进步，不仅可以与完全监督方法的性能相媲美，而且还超越了 CT 和 MRI 数据集中现有的弱监督方法，在某些器官的 Dice 评分上实现了高达 18.1% 的改进。代码可在 https://github.com/runminjian/PW4MedSeg 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02566v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **Semantic Human Mesh Reconstruction with Textures**<br />
**Title_cn:** 使用纹理进行语义人体网格重建<br />
**Authors:** Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang<br />
**Abstract:** <details><summary>原文: </summary>The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，3D 详细人体网格重建领域取得了重大进展。然而，由于结果不稳定、网格质量低以及缺乏 UV 展开和蒙皮权重，当前方法在工业应用中仍然面临挑战。在本文中，我们提出了 SHERT，这是一种新颖的管道，可以利用纹理和高精度细节重建语义人体网格。 SHERT 在详细表面（例如网格和 SDF）与相应的 SMPL-X 模型之间应用基于语义和法线的采样，以获得部分采样的语义网格，然后通过我们专门设计的自监督完成和细化生成完整的语义网格网络。以完整的语义网格为基础，我们采用纹理扩散模型来创建由图像和文本驱动的人体纹理。我们重建的网格具有稳定的 UV 展开、高质量的三角形网格和一致的语义信息。给定的 SMPL-X 模型提供了语义信息和形状先验，使得 SHERT 即使在输入不正确和不完整的情况下也能表现良好。语义信息还可以轻松地替换和动画不同的身体部位，例如面部、身体和手。定量和定性实验表明，SHERT 能够生成高保真且稳健的语义网格，其性能优于最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.02561v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research**<br />
**Title_cn:** 更新生成建模研究的临床人工智能 (MI-CLAIM) 最低信息清单<br />
**Authors:** Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, Jaysón Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte, Madhumita Sushil<br />
**Abstract:** <details><summary>原文: </summary>Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data ("zero-" or "few-shot" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the "Minimum information about clinical artificial intelligence modeling" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards.</details>
**Abstract_cn:** <details><summary>译文: </summary>生成模型（包括大语言模型（LLM）、视觉语言模型（VLM）和扩散模型）的最新进展加速了医学中自然语言和图像处理领域的发展，并标志着生物医学模型开发方式的重大范式转变并部署。虽然这些模型高度适应新任务，但扩展和评估它们的使用提出了先前框架中未解决的新挑战。特别是，这些模型能够在几乎不需要专门训练数据（“零”或“少样本”方法）的情况下产生有用的输出，以及其输出的开放性，因此需要开发更新的模型使用和评估这些模型的指南。为了应对美国第 141103 号行政命令和几个新兴国家临床人工智能评估网络确定的临床人工智能工具开发标准和最佳实践方面的差距，我们开始在“有关临床人工智能的最低信息”的基础上正式制定其中一些指南。情报建模”（MI-CLAIM）清单。 MI-CLAIM 清单最初于 2020 年制定，提供了一组六个步骤，并提供了鼓励透明、可重复的医学人工智能 (AI) 研究所需的最低限度信息的指南。在这里，我们建议对原始清单进行修改，突出显示生成模型与用于临床研究的传统人工智能模型相比在训练、评估、可解释性和可重复性方面的差异。此更新的清单还旨在澄清队列选择报告，并添加与道德标准保持一致的其他项目。</details>
**PDF:** <http://arxiv.org/pdf/2403.02558v1><br />
**Code:** null<br />

>## **多模态**
>---
>>**index:** 1<br />
**Title:** **Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion**<br />
**Title_cn:** 具有多模态注意力融合的自监督 3D 患者建模<br />
**Authors:** Meng Zheng, Benjamin Planche, Xuan Gong, Fan Yang, Terrence Chen, Ziyan Wu<br />
**Abstract:** <details><summary>原文: </summary>3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms. Existing CNN-based end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly. To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment. We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and clinical data. Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios.</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 患者身体建模对于智能医疗扫描和手术室自动患者定位的成功至关重要。现有的基于 CNN 的端到端患者建模解决方案通常需要 a) 定制的网络设计，需要大量相关训练数据，涵盖广泛的现实临床场景（例如，被床单覆盖的患者），这导致实际部署中的通用性不佳， b) 昂贵的 3D 人体模型注释，即需要大量的手动工作，导致系统扩展性较差。为了解决这些问题，我们提出了一种通用的模块化 3D 患者建模方法，包括 (a) 多模态关键点检测模块，具有用于 2D 患者关节定位的注意融合，以学习互补的跨模态患者身体信息，从而改进关键点定位在各种成像（例如 CT、MRI 等）和临床场景（例如严重遮挡）中的稳健性和普遍性； (b) 自监督 3D 网格回归模块，不需要昂贵的 3D 网格参数注释来进行训练，为临床部署带来直接的成本效益。我们通过对公共和临床数据进行广泛的患者定位实验来证明所提出方法的有效性。我们的评估结果在真实临床场景中的各种成像模式中实现了卓越的患者定位性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.03217v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Design2Code: How Far Are We From Automating Front-End Engineering?**<br />
**Title_cn:** Design2Code：我们距离自动化前端工程还有多远？<br />
**Authors:** Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang<br />
**Abstract:** <details><summary>原文: </summary>Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V performs the best on this task compared to other models. Moreover, annotators think GPT-4V generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages are considered better than the original reference webpages. Our fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.</details>
**Abstract_cn:** <details><summary>译文: </summary>近年来，生成式人工智能取得了快速发展，在多模式理解和代码生成方面实现了前所未有的能力。这可以实现前端开发的新范例，其中多模式法学硕士可以直接将视觉设计转换为代码实现。在这项工作中，我们将其形式化为 Design2Code 任务并进行全面的基准测试。具体来说，我们手动策划了 484 个不同的现实世界网页作为测试用例的基准，并开发了一组自动评估指标，以评估当前的多模式 LLM 生成直接渲染到给定参考网页的代码实现的能力，给定的屏幕截图如下输入。我们还通过全面的人工评估来补充自动指标。我们开发了一套多模式提示方法，并在 GPT-4V 和 Gemini Pro Vision 上展示了它们的有效性。我们进一步微调开源 Design2Code-18B 模型，成功匹配 Gemini Pro Vision 的性能。人工评估和自动指标都表明，与其他模型相比，GPT-4V 在这项任务上表现最好。此外，注释者认为 GPT-4V 生成的网页在视觉外观和内容方面可以在 49% 的情况下取代原始参考网页；也许令人惊讶的是，在 64% 的情况下，GPT-4V 生成的网页被认为比原始参考网页更好。我们的细粒度细分指标表明，开源模型在从输入网页调用视觉元素和生成正确的布局设计方面大多滞后，而文本内容和着色等方面可以通过适当的微调得到显着改善。</details>
**PDF:** <http://arxiv.org/pdf/2403.03163v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models**<br />
**Title_cn:** 大饱眼福：多模态大语言模型的混合分辨率自适应<br />
**Authors:** Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji<br />
**Abstract:** <details><summary>原文: </summary>Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\times$ inference speed than LLaVA-1.5. Source codes are released at: https://github.com/luogen1996/LLaVA-HR.</details>
**Abstract_cn:** <details><summary>译文: </summary>尽管取得了显着的进步，现有的多模态大语言模型（MLLM）在粒度视觉识别方面仍然较差。与之前的工作相反，我们从图像分辨率的角度研究了这个问题，并揭示了低分辨率和高分辨率视觉特征的结合可以有效地缓解这一缺点。基于这一观察，我们提出了一种新颖且有效的 MLLM 方法，称为混合分辨率自适应（MRA）。特别是，MRA针对不同分辨率的图像采用两种视觉通路，其中高分辨率视觉信息通过新型分辨率混合适配器（MR-Adapters）嵌入到低分辨率通路中。这种设计还大大减少了 MLLM 的输入序列长度。为了验证 MRA，我们将其应用于最近的称为 LLaVA 的 MLLM，并将新模型称为 LLaVA-HR。我们对 11 项视觉语言 (VL) 任务进行了广泛的实验，结果表明 LLaVA-HR 在 8 项 VL 任务上优于现有的 MLLM，例如在 TextVQA 上+9.4%。更重要的是，LLaVA-HR 的训练和推理在 MRA 下仍然保持高效，例如，比 LLaVA-1.5 训练 20 小时，推理速度提高 3 倍。源代码发布于：https://github.com/luogen1996/LLaVA-HR。</details>
**PDF:** <http://arxiv.org/pdf/2403.03003v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer**<br />
**Title_cn:** MADTP：多模态对齐引导的动态令牌修剪，用于加速视觉语言变压器<br />
**Authors:** Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen<br />
**Abstract:** <details><summary>原文: </summary>Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉语言转换器（VLT）最近取得了巨大的成功，但同时也伴随着沉重的计算成本，其中一个主要原因可归因于大量的视觉和语言标记。现有的用于压缩VLT的令牌剪枝研究主要遵循基于单一模态的方案，但却忽略了对齐不同模态对于指导令牌剪枝过程的关键作用，导致一种模态的重要令牌在另一个模态分支中被错误地剪枝。同时，现有的VLT剪枝工作也缺乏根据不同输入样本动态压缩每一层的灵活性。为此，我们提出了一种名为多模态对齐引导动态令牌修剪（MADTP）的新颖框架，用于加速各种 VLT。具体来说，我们首先引入一个精心设计的多模态对齐指导（MAG）模块，该模块可以对齐来自不同模态的相同语义概念的特征，以确保修剪后的标记对于所有模态来说都不那么重要。我们进一步设计了一种新颖的动态令牌修剪（DTP）模块，它可以根据不同的输入实例自适应地调整每层的令牌压缩比。对各种基准的大量实验表明，MADTP 显着降低了各种多模态模型的计算复杂性，同时保持了竞争性能。值得注意的是，当应用于 NLVR2 数据集中的 BLIP 模型时，MADTP 可以将 GFLOP 降低 80%，而性能下降不到 4%。</details>
**PDF:** <http://arxiv.org/pdf/2403.02991v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception**<br />
**Title_cn:** 具有细粒度视觉感知的多模式指令调整法学硕士<br />
**Authors:** Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie<br />
**Abstract:** <details><summary>原文: </summary>Multimodal Large Language Model (MLLMs) leverages Large Language Models as a cognitive framework for diverse visual-language tasks. Recent efforts have been made to equip MLLMs with visual perceiving and grounding capabilities. However, there still remains a gap in providing fine-grained pixel-level perceptions and extending interactions beyond text-specific inputs. In this work, we propose {\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object perceptions and natural language descriptions from multi-modality references, such as texts, boxes, images, or audio. This innovation empowers users with greater flexibility to engage with the model beyond textual and regional prompts, without modality-specific designs. Through our proposed refocusing mechanism, the generated grounding output is guided to better focus on the referenced object, implicitly incorporating additional pixel-level supervision. This simple modification utilizes attention scores generated during the inference of LLM, eliminating the need for extra computations while exhibiting performance enhancements in both grounding masks and referring expressions. With only publicly available training data, our model achieves state-of-the-art results across multiple benchmarks, including diverse modality referring segmentation and region-level referring expression generation.</details>
**Abstract_cn:** <details><summary>译文: </summary>多模态大语言模型 (MLLM) 利用大语言模型作为各种视觉语言任务的认知框架。最近人们努力为 MLLM 配备视觉感知和接地能力。然而，在提供细粒度的像素级感知和将交互扩展到文本特定输入之外仍然存在差距。在这项工作中，我们提出了 {\bf{AnyRef}}，一种通用的 MLLM 模型，它可以从多模态参考（例如文本、框、图像或音频）生成像素级对象感知和自然语言描述。这项创新使用户能够更加灵活地参与模型，超越文本和区域提示，而无需特定于模态的设计。通过我们提出的重新聚焦机制，生成的接地输出被引导更好地聚焦于参考对象，隐式地结合了额外的像素级监督。这个简单的修改利用了 LLM 推理过程中生成的注意力分数，消除了额外计算的需要，同时在接地掩模和引用表达式方面表现出性能增强。仅利用公开可用的训练数据，我们的模型就在多个基准上实现了最先进的结果，包括不同模态的引用分割和区域级引用表达生成。</details>
**PDF:** <http://arxiv.org/pdf/2403.02969v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples**<br />
**Title_cn:** 通过硬负样本增强多模态对比学习中的概念理解<br />
**Authors:** Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický<br />
**Abstract:** <details><summary>原文: </summary>Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show significant improvements in fine-grained concept understanding across a wide range of vision-language datasets, including our InpaintCOCO dataset.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前利用对比学习的多模态模型在发展细粒度概念理解方面通常面临局限性。这是由于预训练期间的随机负样本，导致在损失函数中几乎完全比较非常不同的概念。因此，这些模型难以应对细粒度的语义差异。为了解决这个问题，我们引入了一种新颖的预训练方法，结合了合成的硬负文本示例。硬底片排列了与视觉概念相对应的术语，从而导致更细粒度的视觉和文本概念对齐。此外，我们还引入了 InpaintCOCO，这是一个新的具有挑战性的数据集，用于评估视觉语言模型中颜色、对象和大小的细粒度对齐。我们通过更改视觉概念，使用 COCO 图像的生成修复来创建数据集，以便图像不再与其原始标题匹配。我们的结果表明，在广泛的视觉语言数据集（包括我们的 InpaintCOCO 数据集）中，细粒度概念理解有了显着改进。</details>
**PDF:** <http://arxiv.org/pdf/2403.02875v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation**<br />
**Title_cn:** 通过梯度引导模型扰动增强医学视觉问答任务的泛化<br />
**Authors:** Gang Liu, Hongyang Li, Zerui He, Shenjun Zhong<br />
**Abstract:** <details><summary>原文: </summary>Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications. However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization. Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives. Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples. In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks. The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates. It is subsequently injected into the model's visual encoder. The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>利用预先训练的视觉语言模型已成为提高下游视觉问答 (VQA) 应用程序性能的广泛采用的方法。然而，在医学 VQA 专业领域，可用数据的稀缺对实现可靠的模型泛化构成了重大障碍。人们提出了许多方法来增强模型的泛化性，从以数据为中心和以模型为中心的角度解决这个问题。数据增强技术通常用于丰富数据集，而各种正则化方法旨在防止模型过度拟合，特别是在有限数据样本上进行训练时。在本文中，我们介绍了一种在预训练和微调阶段将梯度引导参数扰动合并到多模态模型的视觉编码器中的方法，以提高下游医疗 VQA 任务的模型泛化能力。小扰动是通过与优化景观中移动平均梯度的方向对齐来自适应生成的，该方向与优化器的历史更新的方向相反。随后将其注入模型的视觉编码器中。结果表明，即使使用明显较小的预训练图像描述数据集，我们的方法在 VQA-RAD 和 SLAKE 数据集上都取得了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.02707v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters**<br />
**Title_cn:** 微调的多模态语言模型是高质量的图像文本数据过滤器<br />
**Authors:** Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, Heng Wang<br />
**Abstract:** <details><summary>原文: </summary>We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们提出了一种利用微调多模态语言模型（MLM）来过滤图像文本数据的新颖框架。通过集成 MLM 的最新进展，我们的方法优于主要的过滤方法（例如 CLIPScore）。我们设计了四个不同但互补的指标来全面衡量图像文本数据的质量。建立了一个新的管道来构建高质量的指令数据，用于微调 MLM 作为数据过滤器。与 CLIPScore 相比，我们的 MLM 过滤器可产生更精确、更全面的分数，可直接提高过滤数据的质量并提高预训练模型的性能。我们在流行的基础模型（即 CLIP 和 BLIP2）和各种下游任务上实现了比 CLIPScore 显着的改进。我们的 MLM 过滤器可以推广到不同的模型和任务，并可用作 CLIPScore 的直接替代品。提供了额外的消融研究来验证我们对 MLM 滤波器的设计选择。</details>
**PDF:** <http://arxiv.org/pdf/2403.02677v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **Interactive Continual Learning: Fast and Slow Thinking**<br />
**Title_cn:** 交互式持续学习：快思考和慢思考<br />
**Authors:** Biqing Qi, Xingquan Chen, Junqi Gao, Jianxing Liu, Ligang Wu, Bowen Zhou<br />
**Abstract:** <details><summary>原文: </summary>Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>高级生命形式由神经认知机制的协同相互作用维持，在其一生中不断获取和转移知识。相比之下，当代机器学习范式在模拟持续学习（CL）方面表现出局限性。尽管如此，大型语言模型 (LLM) 的出现为通过与这些模型的交互来实现 CL 提供了有希望的途径。本文借鉴互补学习系统理论，提出了一种新颖的交互式持续学习（ICL）框架，该框架通过各种规模模型之间的协作交互来实现。具体来说，我们将 ViT 模型指定为 System1，将多模态 LLM 指定为 System2。为了使记忆模块能够从类信息中推断出任务并增强 Set2Set 检索，我们提出了类知识任务多头注意力（CKT-MHA）。此外，为了通过增强几何表示来改进 System1 中的内存检索，我们引入了基于 von Mises-Fisher (vMF) 分布的 CL-vMF 机制。同时，我们引入了 von Mises-Fisher 离群点检测和交互（vMF-ODI）策略来识别困难示例，从而增强 System1 和 System2 之间的协作以实现复杂的推理。对我们提出的 ICL 的综合评估表明，相对于现有方法，具有显着的抗遗忘能力和优越的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02628v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing**<br />
**Title_cn:** VEglue：通过对象对齐联合擦除测试视觉蕴涵系统<br />
**Authors:** Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Qing Wang<br />
**Abstract:** <details><summary>原文: </summary>Visual entailment (VE) is a multimodal reasoning task consisting of image-sentence pairs whereby a promise is defined by an image, and a hypothesis is described by a sentence. The goal is to predict whether the image semantically entails the sentence. VE systems have been widely adopted in many downstream tasks. Metamorphic testing is the commonest technique for AI algorithms, but it poses a significant challenge for VE testing. They either only consider perturbations on single modality which would result in ineffective tests due to the destruction of the relationship of image-text pair, or just conduct shallow perturbations on the inputs which can hardly detect the decision error made by VE systems. Motivated by the fact that objects in the image are the fundamental element for reasoning, we propose VEglue, an object-aligned joint erasing approach for VE systems testing. It first aligns the object regions in the premise and object descriptions in the hypothesis to identify linked and un-linked objects. Then, based on the alignment information, three Metamorphic Relations are designed to jointly erase the objects of the two modalities. We evaluate VEglue on four widely-used VE systems involving two public datasets. Results show that VEglue could detect 11,609 issues on average, which is 194%-2,846% more than the baselines. In addition, VEglue could reach 52.5% Issue Finding Rate (IFR) on average, and significantly outperform the baselines by 17.1%-38.2%. Furthermore, we leverage the tests generated by VEglue to retrain the VE systems, which largely improves model performance (50.8% increase in accuracy) on newly generated tests without sacrificing the accuracy on the original test set.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉蕴涵（VE）是一种由图像-句子对组成的多模态推理任务，其中承诺由图像定义，假设由句子描述。目标是预测图像在语义上是否包含句子。 VE系统已广泛应用于许多下游任务中。变形测试是 AI 算法最常见的技术，但它对 VE 测试提出了重大挑战。他们要么只考虑单一模态的扰动，这会由于图文对关系的破坏而导致测试无效，要么只是对输入进行浅层扰动，很难检测到VE系统做出的决策错误。由于图像中的对象是推理的基本元素，我们提出了 VEglue，一种用于 VE 系统测试的对象对齐联合擦除方法。它首先对齐前提中的对象区域和假设中的对象描述，以识别链接和未链接的对象。然后，根据对齐信息，设计三个变形关系来共同擦除两种模态的对象。我们在涉及两个公共数据集的四个广泛使用的 VE 系统上评估 VEglue。结果显示，VEglue 平均可以检测到 11,609 个问题，比基线多 194%-2,846%。此外，VEglue 平均问题发现率 (IFR) 可达 52.5%，明显优于基线 17.1%-38.2%。此外，我们利用 VEglue 生成的测试来重新训练 VE 系统，这大大提高了新生成的测试的模型性能（准确率提高了 50.8%），而无需牺牲原始测试集的准确率。</details>
**PDF:** <http://arxiv.org/pdf/2403.02581v1><br />
**Code:** null<br />

>## **模型压缩/优化**
>---
>>**index:** 1<br />
**Title:** **PromptKD: Unsupervised Prompt Distillation for Vision-Language Models**<br />
**Title_cn:** PromptKD：视觉语言模型的无监督快速蒸馏<br />
**Authors:** Zheng Li, Xiang Li, Xinyi Fu, Xing Zhang, Weiqiang Wang, Jian Yang<br />
**Abstract:** <details><summary>原文: </summary>Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method.</details>
**Abstract_cn:** <details><summary>译文: </summary>即时学习已成为增强视觉语言模型 (VLM)（例如针对特定领域下游任务的 CLIP）的一种有价值的技术。现有的工作主要集中在设计各种学习形式的提示，忽略了提示作为从更大的教师模型中学习的有效蒸馏器的潜力。在本文中，我们介绍了一种无监督域提示蒸馏框架，其目的是通过使用未标记域图像的提示驱动模仿，将较大教师模型的知识转移到轻量级目标模型。具体来说，我们的框架由两个不同的阶段组成。在初始阶段，我们使用域（少样本）标签预训练大型 CLIP 教师模型。预训练后，我们利用 CLIP 独特的解耦模态特征，通过教师文本编码器将文本特征预计算并存储为类向量一次。在后续阶段，存储的类向量在教师和学生图像编码器之间共享，以计算预测逻辑。此外，我们通过 KL 散度对齐教师和学生模型的逻辑，鼓励学生图像编码器通过可学习的提示生成与教师相似的概率分布。所提出的即时蒸馏过程消除了对标记数据的依赖，使算法能够利用域内大量未标记的图像。最后，利用训练有素的学生图像编码器和预存储的文本特征（类向量）进行推理。据我们所知，我们是第一个（1）为 CLIP 执行无监督的特定领域提示驱动知识蒸馏，以及（2）建立一种实用的文本特征预存储机制，作为教师和学生之间共享的类向量。对 11 个数据集的广泛实验证明了我们方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02781v1><br />
**Code:** null<br />

>## **分类/检测/识别/分割/...**
>---
>>**index:** 1<br />
**Title:** **Solving the bongard-logo problem by modeling a probabilistic model**<br />
**Title_cn:** 通过建立概率模型来解决 boongard-logo 问题<br />
**Authors:** Ruizhuo Song, Beiming Yuan<br />
**Abstract:** <details><summary>原文: </summary>Abstract reasoning problems challenge the perceptual and cognitive abilities of AI algorithms, demanding deeper pattern discernment and inductive reasoning beyond explicit image features. This study introduces PMoC, a tailored probability model for the Bongard-Logo problem, achieving high reasoning accuracy by constructing independent probability models. Additionally, we present Pose-Transformer, an enhanced Transformer-Encoder designed for complex abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM. Pose-Transformer incorporates positional information learning, inspired by capsule networks' pose matrices, enhancing its focus on local positional relationships in image data processing. When integrated with PMoC, it further improves reasoning accuracy. Our approach effectively addresses reasoning difficulties associated with abstract entities' positional changes, outperforming previous models on the OIG, D3$\times$3 subsets of RAVEN, and PGM databases. This research contributes to advancing AI's capabilities in abstract reasoning and cognitive pattern recognition.</details>
**Abstract_cn:** <details><summary>译文: </summary>抽象推理问题挑战人工智能算法的感知和认知能力，需要超越显式图像特征的更深入的模式辨别和归纳推理。本研究引入了 PMoC，这是一种针对 Bongard-Logo 问题量身定制的概率模型，通过构建独立的概率模型来实现高推理精度。此外，我们还推出了 Pose-Transformer，这是一种增强的 Transformer-Encoder，专为复杂的抽象推理任务而设计，包括 Bongard-Logo、RAVEN、I-RAVEN 和 PGM。 Pose-Transformer 受胶囊网络姿势矩阵的启发，结合了位置信息学习，增强了其对图像数据处理中局部位置关系的关注。与PMoC集成时，进一步提高推理精度。我们的方法有效地解决了与抽象实体位置变化相关的推理困难，优于 OIG、RAVEN 的 D3$\times$3 子集和 PGM 数据库上的先前模型。这项研究有助于提高人工智能在抽象推理和认知模式识别方面的能力。</details>
**PDF:** <http://arxiv.org/pdf/2403.03173v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning**<br />
**Title_cn:** PalmProbNet：通过迁移学习了解厄瓜多尔热带森林棕榈分布的概率方法<br />
**Authors:** Kangning Cui, Zishan Shao, Gregory Larsen, Victor Pauca, Sarra Alqahtani, David Segurado, João Pinheiro, Manqi Wang, David Lutz, Robert Plemmons, et.al.<br />
**Abstract:** <details><summary>原文: </summary>Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes. These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters. Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap. This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities. Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.</details>
**Abstract_cn:** <details><summary>译文: </summary>棕榈在热带森林中发挥着巨大作用，是人类和野生动物的重要资源。热带生态系统的一个核心问题是了解棕榈的分布和丰度。然而，由于混合森林景观中茂密的植被、重叠的树冠和可变的照明条件，在地理空间图像中准确识别和定位棕榈树提出了重大挑战。为了解决这个问题，我们引入了 PalmProbNet，这是一种概率方法，利用迁移学习来分析高分辨率无人机衍生的正射马赛克图像，从而能够检测厄瓜多尔雨林茂密树冠内的棕榈树。这种方法代表了自动棕榈检测的重大进步，可以有效地确定混合热带雨林中棕榈的存在和位置。我们的过程首先从无人机图像生成正射马赛克图像，我们从中提取并标记两种不同尺寸的手掌和非手掌图像块。然后，这些补丁用于训练具有相同架构的模型，该架构由未改变的预训练 ResNet-18 和具有专门训练参数的多层感知器 (MLP) 组成。随后，PalmProbNet 在景观正射马赛克上采用滑动窗口技术，使用小和大窗口尺寸来生成概率热图。该热图有效地可视化了棕榈树的分布，展示了我们的方法在各种森林密度下的可扩展性和适应性。尽管地形具有挑战性，我们的方法还是表现出了卓越的性能，在测试中达到了 97.32% 的准确率和 94.59% 的 Cohen's kappa 值。</details>
**PDF:** <http://arxiv.org/pdf/2403.03161v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Simplicity in Complexity**<br />
**Title_cn:** 复杂中的简单<br />
**Authors:** Kevin Shen, Surabhi S Nath, Aenne Brielmann, Peter Dayan<br />
**Abstract:** <details><summary>原文: </summary>The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple.</details>
**Abstract_cn:** <details><summary>译文: </summary>视觉刺激的复杂性在许多认知现象中发挥着重要作用，包括注意力、参与度、记忆力、时间感知和审美评价。尽管复杂性很重要，但人们对其知之甚少，具有讽刺意味的是，以前的图像复杂性模型相当\textit{复杂}。人们曾多次尝试寻找能够解释复杂性的手工特征，但这些特征通常是特定于数据集的，因此无法泛化。另一方面，最近的工作采用深度神经网络来预测复杂性，但这些模型仍然难以解释，并且不能指导对问题的理论理解。在这里，我们建议使用基于分段的图像表示来建模复杂性。我们使用最先进的分割模型 SAM 和 FC-CLIP 来分别量化多粒度的分割数量以及图像中的类别数量。我们发现，简单的线性模型可以很好地解释复杂性，该模型具有跨自然场景和艺术图像的六个不同图像集的这两个特征。这表明图像的复杂性可以非常简单。</details>
**PDF:** <http://arxiv.org/pdf/2403.03134v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation**<br />
**Title_cn:** 运动校正移动平均：包括事后时间信息以改进视频分割<br />
**Authors:** Robert Mendel, Tobias Rueckert, Dirk Wilhelm, Daniel Rueckert, Christoph Palm<br />
**Abstract:** <details><summary>原文: </summary>Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions. Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise. Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset. This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling. With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions. Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame. The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach. We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach.</details>
**Abstract_cn:** <details><summary>译文: </summary>实时计算速度和高精度是计算机辅助干预的要求。将分割网络应用于医疗视频处理任务可能会引入显着的帧间预测噪声。现有方法可以通过包含时间信息来减少不一致，但通常会对架构或数据集提出要求。本文提出了一种在任何分割模型中包含时间信息的方法，从而提供了一种无需在训练或附加标签期间进行更改即可提高视频分割性能的技术。通过运动校正移动平均线，我们可以细化当前和先前预测之间的指数移动平均线。使用光流来估计连续帧之间的移动，我们可以移动移动平均计算中的先验项以与当前帧的几何形状对齐。光流计算不需要模型的输出，因此可以并行执行，从而不会对我们的方法造成显着的运行时间损失。我们在两个公开可用的分割数据集和两个专有的内窥镜数据集上评估我们的方法，并显示相对于基线方法的改进。</details>
**PDF:** <http://arxiv.org/pdf/2403.03120v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection**<br />
**Title_cn:** 使用深度语义分割和新颖的异常值检测改进 LiDAR 里程计和地图绘制<br />
**Authors:** Mohamed Afifi, Mohamed ElHelw<br />
**Abstract:** <details><summary>原文: </summary>Perception is a key element for enabling intelligent autonomous navigation. Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks. Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms. In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms. Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data. We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class. To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process. In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion. Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms.</details>
**Abstract_cn:** <details><summary>译文: </summary>感知是实现智能自主导航的关键要素。了解周围环境的语义和准确的车辆姿态估计是自动驾驶汽车的基本能力，包括自动驾驶汽车和执行复杂任务的移动机器人。自动驾驶汽车等快速移动平台对定位和地图算法提出了严峻的挑战。在这项工作中，我们提出了一种基于 LOAM 架构的快速移动平台实时 LiDAR 测距和测绘的新颖框架。我们的框架利用深度学习模型生成的语义信息来改进 LiDAR 扫描之间的点对线和点对平面匹配，并构建环境语义图，从而使用 LiDAR 数据进行更准确的运动估计。我们观察到，在匹配过程中包含语义信息会为该过程引入一种新型异常匹配，其中匹配发生在同一语义类的不同对象之间。为此，我们提出了一种新颖的算法，可以在匹配过程中明确识别和丢弃潜在的异常值。在我们的实验中，我们研究了改进匹配过程对 LiDAR 里程计针对高速运动的鲁棒性的影响。我们对 KITTI 数据集的实验评估表明，当扫描采集姿势之间存在较大间隙（这对于快速移动平台来说是典型的）时，利用语义信息并拒绝异常值可显着增强 LiDAR 里程计和绘图的鲁棒性。</details>
**PDF:** <http://arxiv.org/pdf/2403.03111v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding**<br />
**Title_cn:** MiKASA：用于 3D 视觉基础的多键锚点和场景感知变压器<br />
**Authors:** Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker<br />
**Abstract:** <details><summary>原文: </summary>3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.   The source code and additional resources for this project are available on GitHub: https://github.com/birdy666/MiKASA-3DVG</details>
**Abstract_cn:** <details><summary>译文: </summary>3D 视觉基础涉及将自然语言描述与其在 3D 空间中的相应对象进行匹配。现有方法经常面临对象识别准确性的挑战，并且难以解释复杂的语言查询，特别是涉及多个锚点或依赖于视图的描述。作为回应，我们推出了 MiKASA（多键锚点场景感知）变压器。我们新颖的端到端训练模型集成了基于自注意力的场景感知对象编码器和原始的多键锚定技术，提高了对象识别的准确性和对空间关系的理解。此外，MiKASA 提高了决策的可解释性，有利于错误诊断。我们的模型在 Sr3D 和 Nr3D 数据集的 Referit3D 挑战赛中实现了最高的整体准确度，特别是在需要视点相关描述的类别中大幅领先。该项目的源代码和其他资源可在 GitHub 上找到：https://github.com/birdy666/MiKASA-3DVG</details>
**PDF:** <http://arxiv.org/pdf/2403.03077v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections**<br />
**Title_cn:** CrackNex：基于 Retinex 理论的无人机检测少镜头微光裂纹分割模型<br />
**Authors:** Zhen Yao, Jiawei Xu, Shuhang Hou, Mooi Choo Chuah<br />
**Abstract:** <details><summary>原文: </summary>Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex.</details>
**Abstract_cn:** <details><summary>译文: </summary>对混凝土结构进行例行目视检查对于维护关键基础设施的安全性和完整性至关重要。这种目视检查有时会在弱光条件下进行，例如检查桥梁的健康状况。由于裂缝与其周围环境之间的对比度较差，因此在这种条件下进行裂缝分割具有挑战性。然而，大多数深度学习方法都是针对照明良好的裂纹图像而设计的，因此它们的性能在弱光场景中会急剧下降。此外，传统方法需要许多带注释的低光裂纹图像，这非常耗时。在本文中，我们通过提出 CrackNex 来解决这些挑战，这是一个利用基于 Retinex 理论的反射信息来帮助模型学习统一的光照不变表示的框架。此外，我们利用少样本分割来解决训练数据效率低下的问题。在CrackNex中，支撑原型和反射原型都是从支撑集中提取的。然后，设计一个原型融合模块来集成两个原型的功能。 CrackNex 在多个数据集上优于 SOTA 方法。此外，我们还提出了第一个用于低光裂纹分割的基准数据集 LCSD。康文署包括 102 张光线充足的裂缝图像和 41 张光线较暗的裂缝图像。数据集和代码可在 https://github.com/zy1296/CrackNex 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.03063v1><br />
**Code:** null<br />
>>**index:** 8<br />
**Title:** **ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities**<br />
**Title_cn:** ChatGPT 和生物识别：面部识别、性别检测和年龄估计能力的评估<br />
**Authors:** Ahmad Hassanpour, Yasamin Kowsari, Hatef Otroshi Shahreza, Bian Yang, Sebastien Marcel<br />
**Abstract:** <details><summary>原文: </summary>This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文探讨了 ChatGPT 等大型语言模型 (LLM) 在生物识别任务中的应用。我们专门研究了 ChatGPT 在执行生物识别相关任务方面的能力，重点是人脸识别、性别检测和年龄估计。由于生物识别被视为敏感信息，ChatGPT 避免回答直接提示，因此我们制定了一个提示策略来绕过其保护并评估生物识别任务的能力。我们的研究表明，ChatGPT 可以相当准确地识别面部身份并区分两张面部图像。此外，实验结果表明在性别检测方面具有出色的性能，并且在年龄估计任务方面具有合理的准确性。我们的研究结果揭示了法学硕士和生物识别基础模型应用的巨大潜力。</details>
**PDF:** <http://arxiv.org/pdf/2403.02965v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **XAI-Based Detection of Adversarial Attacks on Deepfake Detectors**<br />
**Title_cn:** 基于 XAI 的 Deepfake 探测器对抗性攻击检测<br />
**Authors:** Ben Pinhasov, Raz Lapid, Rony Ohayon, Moshe Sipper, Yehudit Aperstein<br />
**Abstract:** <details><summary>原文: </summary>We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI). In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems. However, these systems are frequently targeted by adversarial attacks that inhibit their performance. We address this gap, developing a defensible deepfake detector by leveraging the power of XAI. The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models. We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image. The feature embeddings extracted from this process are then used for training a simple yet effective classifier. Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities. Furthermore, this approach does not change the performance of the deepfake detector. The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms. We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们引入了一种使用 eXplainable 人工智能 (XAI) 识别对 Deepfake 探测器的对抗性攻击的新颖方法。在数字化进步的时代，深度造假已成为一种强大的工具，创造了对高效检测系统的需求。然而，这些系统经常成为对抗性攻击的目标，从而抑制其性能。我们解决了这一差距，利用 XAI 的强大功能开发了一种可防御的深度造假检测器。所提出的方法使用 XAI 为给定方法生成可解释性图，从而提供 AI 模型中决策因素的明确可视化。随后，我们采用预训练的特征提取器来处理输入图像及其相应的 XAI 图像。然后，从此过程中提取的特征嵌入用于训练简单而有效的分类器。我们的方法不仅有助于检测深度伪造品，还可以增强对可能的对抗性攻击的理解，查明潜在的漏洞。此外，这种方法不会改变 Deepfake 检测器的性能。该论文展示了有希望的结果，为未来的深度伪造检测机制提供了潜在的途径。我们相信这项研究将为社区做出宝贵贡献，引发有关保护深度伪造探测器的急需讨论。</details>
**PDF:** <http://arxiv.org/pdf/2403.02955v1><br />
**Code:** null<br />
>>**index:** 10<br />
**Title:** **Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects**<br />
**Title_cn:** 用于研究和自然保护的公民科学和机器学习：欧亚山猫、自由放养的啮齿动物和昆虫的案例<br />
**Authors:** Kinga Skorupska, Rafał Stryjek, Izabela Wierzbowska, Piotr Bebas, Maciej Grzeszczuk, Piotr Gago, Jarosław Kowalski, Maciej Krzywicki, Jagoda Lazarek, Wiesław Kopeć<br />
**Abstract:** <details><summary>原文: </summary>Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.</details>
**Abstract_cn:** <details><summary>译文: </summary>世界各地的自然保护区和国家公园越来越多地使用技术来支持保护工作。濒临灭绝的物种，例如欧亚山猫（Lynx lynx），受到自动照片陷阱网络的监控。然而，这种方法会产生大量数据，需要准备、分析和解释。因此，从事该领域工作的研究人员越来越需要支持来处理这些传入信息。一个机会是寻求志愿公民科学家的支持，他们可以帮助标记数据，但是，保持他们的兴趣具有挑战性。另一种方法是使用卷积神经网络通过图像识别来自动化该过程。在小组会议期间，我们将讨论与自然研究和保护相关的考虑因素，以及使用公民科学和机器学习来加快数据准备、标记和分析过程的机会。</details>
**PDF:** <http://arxiv.org/pdf/2403.02906v1><br />
**Code:** null<br />
>>**index:** 11<br />
**Title:** **Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams**<br />
**Title_cn:** 使用全局、局部身体部位和头部流增强长期人员重新识别<br />
**Authors:** Duy Tran Thanh, Yeejin Lee, Byeongkeun Kang<br />
**Abstract:** <details><summary>原文: </summary>This work addresses the task of long-term person re-identification. Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios. To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios. In this paper, we propose a novel framework that effectively learns and utilizes both global and local information. The proposed framework consists of three streams: global, local body part, and head streams. The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively. Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling. The local body part stream extracts identity-related information for each body part, allowing it to be compared with the same body part from another image. Since body part annotations are not available in re-identification datasets, pseudo-labels are generated using clustering. These labels are then utilized to train a body part segmentation head in the local body part stream. The proposed framework is trained by backpropagating the weighted summation of the identity classification loss, the pair-based loss, and the pseudo body part segmentation loss. To demonstrate the effectiveness of the proposed method, we conducted experiments on three publicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art method.</details>
**Abstract_cn:** <details><summary>译文: </summary>这项工作解决了长期人员重新识别的任务。通常，人员重新识别假设人们不更换衣服，这限制了其应用在短期场景。为了克服这一限制，我们研究了长期人员重新识别，其中考虑了换衣服和衣服一致的场景。在本文中，我们提出了一种新颖的框架，可以有效地学习和利用全球和本地信息。所提出的框架由三个流组成：全局流、局部身体部分流和头部流。全局流和头部流分别对来自整个图像和头部区域的裁剪图像的身份相关信息进行编码。两个流都使用对抗性擦除、最大池化和平均池化的组合来编码最明显、不太明显和平均的特征。本地身体部位流提取每个身体部位的身份相关信息，使其可以与另一幅图像中的相同身体部位进行比较。由于身体部位注释在重新识别数据集中不可用，因此使用聚类生成伪标签。然后利用这些标签来训练本地身体部位流中的身体部位分割头。所提出的框架是通过反向传播身份分类损失、基于对的损失和伪身体部位分割损失的加权和来训练的。为了证明所提出方法的有效性，我们在三个公开可用的数据集（Celeb-reID、PRCC 和 VC-Clothes）上进行了实验。实验结果表明，所提出的方法优于以前的最先进方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.02892v1><br />
**Code:** null<br />
>>**index:** 12<br />
**Title:** **Revisiting Confidence Estimation: Towards Reliable Failure Prediction**<br />
**Title_cn:** 重新审视置信度估计：实现可靠的故障预测<br />
**Authors:** Fei Zhu, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu<br />
**Abstract:** <details><summary>原文: </summary>Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios. Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction. The code is available at \url{https://github.com/Impression2805/FMFP}.</details>
**Abstract_cn:** <details><summary>译文: </summary>在许多风险敏感的应用中，可靠的置信估计是一项具有挑战性但又基本的要求。然而，现代深度神经网络常常对其不正确的预测过于自信，即来自已知类别的错误分类样本和来自未知类别的分布外（OOD）样本。近年来，发展了许多置信度校准和OOD检测方法。在本文中，我们发现了一个普遍存在但实际上被忽视的现象，即大多数置信估计方法对于检测错误分类错误都是有害的。我们研究了这个问题，并发现流行的校准和 OOD 检测方法通常会导致正确分类和错误分类示例之间的置信度分离较差，从而很难决定是否信任预测。最后，我们建议通过寻找平坦最小值来扩大置信差距，这在包括平衡、长尾和协变量移位分类场景在内的各种设置下产生最先进的故障预测性能。我们的研究不仅为可靠的置信度估计提供了强有力的基线，而且还充当了理解校准、OOD 检测和故障预测之间的桥梁。该代码可在 \url{https://github.com/Impression2805/FMFP} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02886v1><br />
**Code:** null<br />
>>**index:** 13<br />
**Title:** **ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving**<br />
**Title_cn:** ActiveAD：面向规划的端到端自动驾驶主动学习<br />
**Authors:** Han Lu, Xiaosong Jia, Yichen Xie, Wenlong Liao, Xiaokang Yang, Junchi Yan<br />
**Abstract:** <details><summary>原文: </summary>End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin. Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data. We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts.</details>
**Abstract_cn:** <details><summary>译文: </summary>自动驾驶（AD）的端到端可微学习最近已成为一个突出的范例。一个主要瓶颈在于其对高质量标记数据的贪婪需求，例如3D 边界框和语义分割的手动注释成本非常高。由于 AD 样本中的行为经常受到长尾分布的影响，这一问题变得更加突出。换句话说，收集到的大部分数据可能是微不足道的（例如，简单地在笔直的道路上行驶），只有少数情况是安全关键的。在本文中，我们探讨了一个实际重要但尚未充分探索的问题，即如何实现端到端 AD 的样本和标签效率。具体来说，我们设计了一种面向规划的主动学习方法，根据提出的规划路线的多样性和有用性标准，逐步注释部分收集的原始数据。根据经验，我们表明我们的以计划为导向的方法可以大大优于一般的主动学习方法。值得注意的是，我们的方法仅使用 30% 的 nuScenes 数据，就实现了与最先进的端到端 AD 方法相当的性能。我们希望我们的工作能够启发未来的工作，除了方法论方面的努力之外，还可以从以数据为中心的角度探索端到端的AD。</details>
**PDF:** <http://arxiv.org/pdf/2403.02877v1><br />
**Code:** null<br />
>>**index:** 14<br />
**Title:** **Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?**<br />
**Title_cn:** 从点云进行 3D 物体检测是否始终需要密集标签？<br />
**Authors:** Chenqiang Gao, Chuandong Liu, Jun Shu, Fangcen Liu, Jiang Liu, Luyu Yang, Xinbo Gao, Deyu Meng<br />
**Abstract:** <details><summary>原文: </summary>Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at https://github.com/gaocq/SS3D2.</details>
**Abstract_cn:** <details><summary>译文: </summary>当前最先进的 (SOTA) 3D 对象检测方法通常需要大量 3D 边界框注释进行训练。然而，收集如此大规模的密集监督数据集的成本是众所周知的。为了减少繁琐的数据标注过程，我们提出了一种新颖的稀疏标注框架，其中我们只为每个场景标注一个 3D 对象。这种稀疏标注策略可以显着减轻繁重的标注负担，而不精确和不完整的稀疏监督可能会严重恶化检测性能。为了解决这个问题，我们开发了 SS3D++ 方法，该方法可以在统一的学习方案中改进 3D 检测器训练和自信的全注释场景生成。使用稀疏注释作为种子，我们基于设计缺失注释实例挖掘模块和可靠的背景挖掘模块，逐步生成置信的完全注释场景。与使用相同甚至更多注释成本的 SOTA 弱监督方法相比，我们提出的方法产生了有竞争力的结果。此外，与 SOTA 全监督方法相比，我们在 KITTI 数据集上实现了同等甚至更好的性能，注释成本降低了约 5 倍，在 Waymo 数据集上实现了 90% 的性能，注释成本降低了约 15 倍。额外的未标记训练场景可以进一步提高性能。该代码可在 https://github.com/gaocq/SS3D2 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02818v1><br />
**Code:** null<br />
>>**index:** 15<br />
**Title:** **DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation**<br />
**Title_cn:** DDF：一种新型双域图像融合策略，用于具有无监督域适应的遥感图像语义分割<br />
**Authors:** Lingyan Ran, Lushuang Wang, Tao Zhuo, Yinghui Xing<br />
**Abstract:** <details><summary>原文: </summary>Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data. Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain. However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome. This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information. Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy. The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于大量未标记数据，遥感图像的语义分割是一个具有挑战性和热点的问题。无监督域适应（UDA）已被证明在合并来自目标域的未分类信息方面具有优势。然而，在源域和目标域上独立微调 UDA 模型对结果的影响有限。本文提出了一种混合训练策略以及一种有效利用原始图像、变换图像和中间域信息的新型双域图像融合策略。此外，为了提高伪标签的精度，我们提出了伪标签区域特定的权重策略。我们的方法的有效性得到了对 ISPRS Vaihingen 和 Potsdam 数据集进行的广泛基准实验和消融研究的证实。</details>
**PDF:** <http://arxiv.org/pdf/2403.02784v1><br />
**Code:** null<br />
>>**index:** 16<br />
**Title:** **HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes**<br />
**Title_cn:** HUNTER：通过将合成实例的知识转移到真实场景进行无监督的以人为中心的 3D 检测<br />
**Authors:** Yichen Yao, Zimo Jiang, Yujing Sun, Zhencai Zhu, Xinge Zhu, Runnan Chen, Yuexin Ma<br />
**Abstract:** <details><summary>原文: </summary>Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving a substantial 87.8\% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife.</details>
**Abstract_cn:** <details><summary>译文: </summary>由于其对机器人技术的关键影响，以人为中心的 3D 场景理解最近引起了越来越多的关注。然而，以人为中心的现实生活场景极其多样和复杂，人类有着复杂的运动和交互。由于标记数据有限，监督方法很难推广到一般场景，阻碍了现实生活中的应用。模仿人类智能，我们通过将合成人类实例的知识转移到真实场景，提出了一种针对以人为中心的场景的无监督 3D 检测方法。为了弥合合成模型和真实点云的不同数据表示和特征分布之间的差距，我们引入了用于有效实例到场景表示传输和合成到真实特征对齐的新颖模块。值得注意的是，与当前最先进的技术相比，我们的方法表现出了卓越的性能，在 HuCenLife 上实现了 mAP 87.8% 的大幅提升，并且非常接近完全监督方法的性能（62.15 mAP 与 69.02 mAP）。</details>
**PDF:** <http://arxiv.org/pdf/2403.02769v1><br />
**Code:** null<br />
>>**index:** 17<br />
**Title:** **DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking**<br />
**Title_cn:** DeconfuseTrack：处理多目标跟踪的混乱<br />
**Authors:** Cheng Huang, Shoudong Han, Mengyu He, Wenbo Zheng, Yuhao Wei<br />
**Abstract:** <details><summary>原文: </summary>Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.</details>
**Abstract_cn:** <details><summary>译文: </summary>准确的数据关联对于减少多目标跟踪 (MOT) 中的混乱（例如 ID 切换和分配错误）至关重要。然而，现有的先进方法常常忽视轨迹之间的多样性以及运动和外观线索中存在的模糊性和冲突，导致在执行简单的全局数据关联时检测、轨迹和关联之间的混乱。为了解决这个问题，我们提出了一种简单、通用且高度可解释的数据关联方法，称为分解数据关联（DDA）。 DDA使用一系列非学习模块将传统的关联问题分解为多个子问题，并通过结合新线索的有针对性的利用来选择性地解决每个子问题中的混乱。此外，我们引入了遮挡感知非极大值抑制（ONMS）来保留更多遮挡检测，从而增加与轨迹关联的机会，并间接减少因漏检而造成的混乱。最后，基于DDA和ONMS，我们设计了一个强大的多目标跟踪器，名为DeconfuseTrack，专门致力于解决MOT中的混乱问题。在 MOT17 和 MOT20 数据集上进行的大量实验表明，我们提出的 DDA 和 ONMS 显着提高了几种流行跟踪器的性能。此外，DeconfuseTrack 在 MOT17 和 MOT20 测试集上实现了最先进的性能，在 HOTA、IDF1、AssA 等指标上显着优于基线跟踪器 ByteTrack。这验证了我们的跟踪设计有效地减少了简单的全局关联引起的混乱。</details>
**PDF:** <http://arxiv.org/pdf/2403.02767v1><br />
**Code:** null<br />
>>**index:** 18<br />
**Title:** **Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels**<br />
**Title_cn:** 在没有确切指导的情况下学习：根据低分辨率历史标签更新大规模高分辨率土地覆盖图<br />
**Authors:** Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang<br />
**Abstract:** <details><summary>原文: </summary>Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.</details>
**Abstract_cn:** <details><summary>译文: </summary>大规模高分辨率（HR）土地覆盖测绘是调查地球表面和解决人类面临的许多挑战的一项重要任务。然而，由于复杂的地面细节、各种地形以及大跨度地理区域缺乏准确的训练标签，这仍然是一项艰巨的任务。在本文中，我们提出了一种高效的弱监督框架（Paraformer），又名低到高网络（L2HNet）V2，以通过易于访问的低分辨率历史土地覆盖数据来指导大规模HR土地覆盖制图（LR）。具体来说，现有的土地覆盖测绘方法揭示了 CNN 在保留局部地面细节方面的主导地位，但仍然存在各种地形的全局建模不足的问题。因此，我们在 Paraformer 中设计了一个并行的 CNN-Transformer 特征提取器，由一个免下采样的 CNN 分支和一个 Transformer 分支组成，以共同捕获局部和全局上下文信息。此外，面对训练数据的空间不匹配，采用伪标签辅助训练（PLAT）模块合理细化LR标签，用于HR图像的弱监督语义分割。对两个大型数据集的实验证明了 Paraformer 相对于其他最先进的方法在从 LR 历史标签自动更新 HR 土地覆盖图方面的优越性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02746v1><br />
**Code:** null<br />
>>**index:** 19<br />
**Title:** **Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery**<br />
**Title_cn:** 在高分辨率卫星图像中引导稀有物体检测<br />
**Authors:** Akram Zaytar, Caleb Robinson, Gilles Q. Hacheme, Girmaw A. Tadesse, Rahul Dodhia, Juan M. Lavista Ferres, Lacey F. Hughey, Jared A. Stabach, Irene Amoke<br />
**Abstract:** <details><summary>原文: </summary>Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches.</details>
**Abstract_cn:** <details><summary>译文: </summary>稀有物体检测是应用地理空间机器学习中的一项基本任务，但由于大量的高分辨率卫星或航空图像以及很少或根本没有标记的阳性样本，因此通常具有挑战性。本文解决了假设没有标记数据并且感兴趣区域没有空间先验的情况下引导这种罕见物体检测任务的问题。我们提出了新颖的基于离线和在线集群的方法来对补丁进行采样，在将正样本暴露给人类注释者方面，这些方法比随机采样更有效。我们应用我们的方法来识别肯尼亚和坦桑尼亚塞伦盖蒂马拉地区的博马（bomas）或牧群动物的小型围栏。我们证明了检测效率的显着提高，实现了正采样率从 2%（随机）增加到 30%。这一进步即使在标签预算极小的情况下也能实现有效的机器学习映射，例如，在总预算为 300 个补丁的情况下，boma 检测任务的 F1 分数为 0.51。</details>
**PDF:** <http://arxiv.org/pdf/2403.02736v1><br />
**Code:** null<br />
>>**index:** 20<br />
**Title:** **FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View**<br />
**Title_cn:** FastOcc：通过融合 2D 鸟瞰图和透视图加速 3D 占用预测<br />
**Authors:** Jiawei Hou, Xiaoyan Li, Wenhao Guan, Gang Zhang, Di Feng, Yuheng Du, Xiangyang Xue, Jian Pu<br />
**Abstract:** <details><summary>原文: </summary>In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird's-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed.</details>
**Abstract_cn:** <details><summary>译文: </summary>在自动驾驶中，与传统感知任务（例如 3D 对象检测和鸟瞰图 (BEV) 语义分割）相比，3D 占用预测可输出体素状态和语义标签，以便更全面地理解 3D 场景。最近的研究人员广泛探索了这项任务的各个方面，包括视图转换技术、真实标签生成和精心设计的网络设计，旨在实现卓越的性能。然而，对于自动驾驶车辆运行至关重要的推理速度却被忽略了。为此，提出了一种称为 FastOcc 的新方法。通过仔细分析输入图像分辨率、图像主干、视图变换和占用预测头四个部分的网络效应和延迟，发现占用预测头在加速模型同时保持其准确性方面具有相当大的潜力。为了改进这个组件，耗时的 3D 卷积网络被一种新颖的类残差架构取代，其中特征主要由轻量级 2D BEV 卷积网络消化，并通过集成从原始图像特征插值的 3D 体素特征进行补偿。 Occ3D-nuScenes 基准测试表明，我们的 FastOcc 以快速推理速度实现了最先进的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.02710v1><br />
**Code:** null<br />
>>**index:** 21<br />
**Title:** **Deep Common Feature Mining for Efficient Video Semantic Segmentation**<br />
**Title_cn:** 深度共同特征挖掘实现高效视频语义分割<br />
**Authors:** Yaoyan Zheng, Hongyu Yang, Di Huang<br />
**Abstract:** <details><summary>原文: </summary>Recent advancements in video semantic segmentation have made substantial progress by exploiting temporal correlations. Nevertheless, persistent challenges, including redundant computation and the reliability of the feature propagation process, underscore the need for further innovation. In response, we present Deep Common Feature Mining (DCFM), a novel approach strategically designed to address these challenges by leveraging the concept of feature sharing. DCFM explicitly decomposes features into two complementary components. The common representation extracted from a key-frame furnishes essential high-level information to neighboring non-key frames, allowing for direct re-utilization without feature propagation. Simultaneously, the independent feature, derived from each video frame, captures rapidly changing information, providing frame-specific clues crucial for segmentation. To achieve such decomposition, we employ a symmetric training strategy tailored for sparsely annotated data, empowering the backbone to learn a robust high-level representation enriched with common information. Additionally, we incorporate a self-supervised loss function to reinforce intra-class feature similarity and enhance temporal consistency. Experimental evaluations on the VSPW and Cityscapes datasets demonstrate the effectiveness of our method, showing a superior balance between accuracy and efficiency.</details>
**Abstract_cn:** <details><summary>译文: </summary>视频语义分割的最新进展通过利用时间相关性取得了实质性进展。然而，持续存在的挑战，包括冗余计算和特征传播过程的可靠性，强调了进一步创新的必要性。作为回应，我们提出了深度共同特征挖掘（DCFM），这是一种战略性设计的新颖方法，旨在通过利用特征共享的概念来应对这些挑战。 DCFM 显式地将特征分解为两个互补的组件。从关键帧提取的通用表示为相邻的非关键帧提供了必要的高级信息，允许直接重新利用而无需特征传播。同时，从每个视频帧派生的独立特征捕获快速变化的信息，提供对于分割至关重要的特定于帧的线索。为了实现这种分解，我们采用了针对稀疏注释数据量身定制的对称训练策略，使骨干网能够学习富含公共信息的鲁棒高级表示。此外，我们还采用了自监督损失函数来增强类内特征相似性并增强时间一致性。对 VSPW 和 Cityscapes 数据集的实验评估证明了我们方法的有效性，显示了准确性和效率之间的卓越平衡。</details>
**PDF:** <http://arxiv.org/pdf/2403.02689v1><br />
**Code:** null<br />
>>**index:** 22<br />
**Title:** **UFO: Uncertainty-aware LiDAR-image Fusion for Off-road Semantic Terrain Map Estimation**<br />
**Title_cn:** UFO：用于越野语义地形图估计的不确定性激光雷达图像融合<br />
**Authors:** Ohn Kim, Junwon Seo, Seongyong Ahn, Chong Hui Kim<br />
**Abstract:** <details><summary>原文: </summary>Autonomous off-road navigation requires an accurate semantic understanding of the environment, often converted into a bird's-eye view (BEV) representation for various downstream tasks. While learning-based methods have shown success in generating local semantic terrain maps directly from sensor data, their efficacy in off-road environments is hindered by challenges in accurately representing uncertain terrain features. This paper presents a learning-based fusion method for generating dense terrain classification maps in BEV. By performing LiDAR-image fusion at multiple scales, our approach enhances the accuracy of semantic maps generated from an RGB image and a single-sweep LiDAR scan. Utilizing uncertainty-aware pseudo-labels further enhances the network's ability to learn reliably in off-road environments without requiring precise 3D annotations. By conducting thorough experiments using off-road driving datasets, we demonstrate that our method can improve accuracy in off-road terrains, validating its efficacy in facilitating reliable and safe autonomous navigation in challenging off-road settings.</details>
**Abstract_cn:** <details><summary>译文: </summary>自主越野导航需要对环境进行准确的语义理解，通常将其转换为各种下游任务的鸟瞰图 (BEV) 表示。虽然基于学习的方法已在直接从传感器数据生成本地语义地形地图方面取得了成功，但其在越野环境中的功效受到准确表示不确定地形特征的挑战的阻碍。本文提出了一种基于学习的融合方法，用于在 BEV 中生成密集地形分类图。通过在多个尺度上执行 LiDAR 图像融合，我们的方法提高了从 RGB 图像和单次扫描 LiDAR 扫描生成的语义图的准确性。利用不确定性感知伪标签进一步增强了网络在越野环境中可靠学习的能力，而无需精确的 3D 注释。通过使用越野驾驶数据集进行彻底的实验，我们证明了我们的方法可以提高越野地形的准确性，验证其在具有挑战性的越野环境中促进可靠和安全的自主导航的功效。</details>
**PDF:** <http://arxiv.org/pdf/2403.02642v1><br />
**Code:** null<br />
>>**index:** 23<br />
**Title:** **False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy**<br />
**Title_cn:** 基于误报采样的数据增强可增强 3D 对象检测的准确性<br />
**Authors:** Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee<br />
**Abstract:** <details><summary>原文: </summary>Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampling and propose a technique that applies the concept of curriculum learning to the sampling strategy that encompasses both false-positive and ground-truth sampling techniques. Our experiments demonstrate that models utilizing false-positive sampling show a reduction in false positives and exhibit improved object detection performance. On the KITTI and Waymo Open datasets, models with false-positive sampling surpass the baseline models by a large margin.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的研究重点是增强 3D 对象检测模型的性能。在各种方法中，地面实况采样已被提议作为一种增强技术，以解决有限的地面实况数据带来的挑战。然而，真实采样的一个固有问题是它会增加误报。因此，本研究旨在通过开发一种称为假阳性采样的新增强技术来克服地面实况采样的局限性并提高 3D 对象检测模型的性能。误报采样涉及使用在模型预测中被识别为误报的点云重新训练模型。我们提出了一种利用真实采样和假阳性采样的算法以及一种用于构建假阳性样本数据库的算法。此外，我们分析了假阳性采样带来的性能提升背后的原理，并提出了一种将课程学习的概念应用于包含假阳性和真实采样技术的采样策略的技术。我们的实验表明，利用误报采样的模型显示误报减少并表现出改进的对象检测性能。在 KITTI 和 Waymo Open 数据集上，具有误报采样的模型大大超过了基线模型。</details>
**PDF:** <http://arxiv.org/pdf/2403.02639v1><br />
**Code:** null<br />
>>**index:** 24<br />
**Title:** **BSDP: Brain-inspired Streaming Dual-level Perturbations for Online Open World Object Detection**<br />
**Title_cn:** BSDP：用于在线开放世界对象检测的受大脑启发的流式双级扰动<br />
**Authors:** Yu Chen, Liyan Ma, Liping Jing, Jian Yu<br />
**Abstract:** <details><summary>原文: </summary>Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object. Hence, we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as OnLine Open World Object Detection(OLOWOD). Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called Brain-inspired Streaming Dual-level Perturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类可以很容易地区分已知和未知的类别，并且可以通过学习一次来识别未知的对象，而不是重复多次而不会忘记所学的对象。因此，我们的目标是让深度学习模型模拟人们的学习方式。我们将这种学习方式称为在线开放世界对象检测（OLOWOD）。现有的OWOD方法更注重未知类别的识别，而增量学习部分也非常重要。此外，一些神经科学研究表明，特定的噪音可以让大脑形成新的连接和神经通路，从而提高学习速度和效率。在本文中，我们将旧样本的双级信息作为对新样本的扰动，使模型善于学习新知识而不忘记旧知识。因此，我们提出了一种简单的即插即用方法，称为脑启发流双级扰动（BSDP），来解决 OLOWOD 问题。具体来说，（1）我们首先计算之前类别的原型，并使用样本与原型之间的距离作为样本选择策略，选择旧样本进行重放； （2）然后将原型作为新样本的流式特征级扰动，通过重温旧知识来提高模型的可塑性； （3）并且还利用旧类别样本的特征分布以流的形式生成对抗性数据作为数据级扰动，以增强模型对新类别的鲁棒性。我们在 PASCAL VOC 和 MS-COCO 上对 BSDP 进行了实证评估，优异的结果证明了我们提出的方法和学习方式的良好性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02637v1><br />
**Code:** null<br />
>>**index:** 25<br />
**Title:** **Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use**<br />
**Title_cn:** 建模协作者：通过 LLM 工具使用以最少的人力实现主观视觉分类<br />
**Authors:** Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, et.al.<br />
**Abstract:** <details><summary>原文: </summary>From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points. Most importantly, our framework eliminates the need for crowd-sourced annotations. Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios. Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art zero-shot classification models like ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.</details>
**Abstract_cn:** <details><summary>译文: </summary>从内容审核到野生动物保护，需要模型识别细微差别或主观视觉概念的应用程序数量正在不断增加。传统上，为此类概念开发分类器需要大量的手动工作（以小时、天甚至月为单位）来识别和注释训练所需的数据。即使使用最近提出的敏捷建模技术，可以快速引导图像分类器，用户仍然需要花费 30 分钟或更长时间的单调、重复的数据标记来训练单个分类器。借鉴 Fiske 的认知守财奴理论，我们提出了一个新的框架，通过用自然语言交互代替人类标记来减轻人工工作量，从而将定义概念所需的总工作量减少一个数量级：从标记 2,000 张图像到仅标记 100 张加上一些自然图像语言互动。我们的框架利用基础模型（大型语言模型和视觉语言模型）的最新进展，通过对话和自动标记训练数据点来开拓概念空间。最重要的是，我们的框架消除了对众包注释的需要。此外，我们的框架最终产生可部署在成本敏感场景中的轻量级分类模型。在 15 个主观概念和 2 个公共图像分类数据集中，我们训练的模型优于传统的敏捷建模以及最先进的零样本分类模型（如 ALIGN、CLIP、CuPL）和大型视觉问答模型（如 PaLI） -X。</details>
**PDF:** <http://arxiv.org/pdf/2403.02626v1><br />
**Code:** null<br />
>>**index:** 26<br />
**Title:** **Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas**<br />
**Title_cn:** 手语人工智能研究中的系统性偏见：聋人主导的重新评估研究议程的呼吁<br />
**Authors:** Aashaka Desai, Maartje De Meulder, Julie A. Hochgesang, Annemarie Kocab, Alex X. Lu<br />
**Abstract:** <details><summary>原文: </summary>Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by what decisions are the most convenient or perceived as important to hearing researchers. We end with a call to action: the field must make space for Deaf researchers to lead the conversation in sign language AI.</details>
**Abstract_cn:** <details><summary>译文: </summary>手语识别、生成和翻译人工智能方面的研究不断发展，人们呼吁此类技术的道德发展。虽然这些工作对于帮助个体研究人员做得更好至关重要，但明显缺乏对系统偏见的讨论或对影响该领域研究问题和方法的修辞分析，特别是因为该领域仍然由听力非签名研究人员主导。因此，我们对手语人工智能领域的 101 篇最新论文进行了系统回顾。我们的分析发现了手语人工智能研究现状中的重大偏见，包括过度关注解决感知到的沟通障碍、缺乏代表性数据集的使用、使用缺乏语言基础的注释以及开发基于有缺陷的模型的方法。我们的立场是，该领域缺乏聋人利益相关者的有意义的投入，而是由对听力研究人员来说最方便或认为重要的决策来驱动。最后，我们呼吁采取行动：该领域必须为聋人研究人员腾出空间，以引导手语人工智能的对话。</details>
**PDF:** <http://arxiv.org/pdf/2403.02563v1><br />
**Code:** null<br />

>## **LLM**
>---
>>**index:** 1<br />
**Title:** **ImgTrojan: Jailbreaking Vision-Language Models with ONE Image**<br />
**Title_cn:** ImgTrojan：使用一张图像越狱视觉语言模型<br />
**Authors:** Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong<br />
**Abstract:** <details><summary>原文: </summary>There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>人们越来越关注大型语言模型（LLM）与人类价值观的一致性。然而，它们与视觉模块或视觉语言模型 (VLM) 集成的安全问题仍然相对未得到充分研究。在本文中，我们提出了一种针对 VLM 的新型越狱攻击，旨在当用户输入有害指令时绕过其安全屏障。假设我们的中毒（图像、文本）数据对包含在训练数据中的情况。通过用恶意越狱提示替换原始文本标题，我们的方法可以利用中毒图像进行越狱攻击。此外，我们分析了毒物比率和可训练参数的位置对攻击成功率的影响。为了进行评估，我们设计了两个指标来量化攻击的成功率和隐蔽性。与精选的有害指令列表一起，提供了衡量攻击效力的基准。我们通过与基线方法进行比较来证明我们的攻击的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02910v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Android in the Zoo: Chain-of-Action-Thought for GUI Agents**<br />
**Title_cn:** 动物园里的 Android：GUI 代理的行动链思想<br />
**Authors:** Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang<br />
**Abstract:** <details><summary>原文: </summary>Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.</details>
**Abstract_cn:** <details><summary>译文: </summary>大语言模型（LLM）导致智能手机自主GUI代理的激增，它们通过预测API的一系列动作来完成由自然语言触发的任务。尽管该任务高度依赖于过去的动作和视觉观察，但现有的研究通常很少考虑中间屏幕截图和屏幕操作所执行的语义信息。为了解决这个问题，这项工作提出了行动思想链（称为 CoAT），它描述了先前的行动、当前的屏幕，更重要的是，行动思考应该执行哪些行动以及由当前行动导致的结果。选择的行动。我们证明，在现成的法学硕士的零样本设置中，与标准上下文建模相比，CoAT 显着提高了目标进度。为了进一步促进这方面的研究，我们构建了一个基准 Android-In-The-Zoo (AitZ)，其中包含 18,643 个屏幕动作对以及动作思想注释链。实验表明，在 AitZ 数据集上微调 200M 模型可达到与 CogAgent-Chat-18B 相当的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.02713v1><br />
**Code:** null<br />

>## **Transformer**
>---
>>**index:** 1<br />
**Title:** **FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation**<br />
**Title_cn:** FAR：灵活、准确且鲁棒的 6DoF 相对相机姿态估计<br />
**Authors:** Chris Rockwell, Nilesh Kulkarni, Linyi Jin, Jeong Joon Park, Justin Johnson, David F. Fouhey<br />
**Abstract:** <details><summary>原文: </summary>Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.</details>
**Abstract_cn:** <details><summary>译文: </summary>估计图像之间的相对相机姿势一直是计算机视觉中的核心问题。在大多数情况下，找到对应关系并求解基本矩阵的方法可以提供高精度。相反，直接使用神经网络预测姿势的方法对于有限的重叠更加稳健，并且可以推断绝对平移比例，但代价是精度降低。我们展示了如何结合这两种方法的优点；我们的方法产生的结果既精确又稳健，同时还准确地推断出平移尺度。我们模型的核心是一个 Transformer，它 (1) 学习在求解的姿态估计和学习的姿态估计之间进行平衡，(2) 提供先验信息来指导求解器。全面的分析支持我们的设计选择，并证明我们的方法可以灵活地适应各种特征提取器和对应估计器，在 Matterport3D、InteriorNet、StreetLearn 和无地图重定位上的 6DoF 姿态估计中显示出最先进的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.03221v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning**<br />
**Title_cn:** 具有多金字塔变压器和对比学习的显微镜散焦去模糊统一框架<br />
**Authors:** Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin Cheng<br />
**Abstract:** <details><summary>原文: </summary>Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data. Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.</details>
**Abstract_cn:** <details><summary>译文: </summary>散焦模糊是显微镜成像中长期存在的问题，对细胞显微镜和显微镜手术的病理学解释和医疗干预造成损害。为了解决这个问题，提出了一个包括多金字塔变换器（MPT）和扩展频率对比正则化（EFCR）的统一框架，以解决显微镜去模糊中的两个突出挑战：更长的注意力广度和特征缺陷。 MPT在每个网络阶段采用显式金字塔结构，集成跨尺度窗口注意力（CSWA）、内尺度通道注意力（ISCA）和特征增强前馈网络（FEFN）来捕获长程跨尺度空间互动和全球渠道环境。 EFCR 通过探索来自不同频段的潜在去模糊信号来解决特征缺陷问题。它还支持去模糊知识转移，以从额外数据中学习跨域信息，从而提高标记和未标记数据的去模糊性能。广泛的实验和下游任务验证表明该框架在多个数据集上实现了最先进的性能。项目页面：https://github.com/PieceZhang/MPT-CataBlur。</details>
**PDF:** <http://arxiv.org/pdf/2403.02611v1><br />
**Code:** null<br />

>## **3D/CG**
>---
>>**index:** 1<br />
**Title:** **HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative**<br />
**Title_cn:** HoloVIC：多传感器全息交叉口和车路协同的大规模数据集和基准<br />
**Authors:** Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, Wei Wu<br />
**Abstract:** <details><summary>原文: </summary>Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.</details>
**Abstract_cn:** <details><summary>译文: </summary>车联网（V2X）是近年来自动驾驶领域的热门话题。车辆与基础设施合作（VIC）成为重要的研究领域之一。由于盲点、遮挡等交通状况的复杂性，极大限制了单视路侧传感系统的感知能力。为了进一步提高路侧感知的准确性，为车辆侧提供更好的信息，本文构建了各种布局的全息路口，构建了大规模多传感器全息车路协同数据集，称为HoloVIC。我们的数据集包括 3 种不同类型的传感器（相机、激光雷达、鱼眼），并根据不同的交叉点采用 4 种传感器布局。每个路口配备6-18个传感器，捕捉同步数据。当自动驾驶车辆经过这些路口时，会收集 VIC 数据。 HoloVIC 总共包含来自不同传感器的 100k+ 同步帧。此外，我们还基于相机、鱼眼和激光雷达注释了 3D 边界框。我们还将不同设备和连续帧中相同对象的 ID 按顺序关联起来。基于HoloVIC，我们制定了四项任务来促进相关研究的发展。我们还为这些任务提供基准。</details>
**PDF:** <http://arxiv.org/pdf/2403.02640v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Towards Geometric-Photometric Joint Alignment for Facial Mesh Registration**<br />
**Title_cn:** 面向面部网格配准的几何光度联合对准<br />
**Authors:** Xizhi Wang, Yaxiong Wang, Mengjian Li<br />
**Abstract:** <details><summary>原文: </summary>This paper presents a Geometric-Photometric Joint Alignment(GPJA) method, for accurately aligning human expressions by combining geometry and photometric information. Common practices for registering human heads typically involve aligning landmarks with facial template meshes using geometry processing approaches, but often overlook photometric consistency. GPJA overcomes this limitation by leveraging differentiable rendering to align vertices with target expressions, achieving joint alignment in geometry and photometric appearances automatically, without the need for semantic annotation or aligned meshes for training. It features a holistic rendering alignment strategy and a multiscale regularized optimization for robust and fast convergence. The method utilizes derivatives at vertex positions for supervision and employs a gradient-based algorithm which guarantees smoothness and avoids topological defects during the geometry evolution. Experimental results demonstrate faithful alignment under various expressions, surpassing the conventional ICP-based methods and the state-of-the-art deep learning based method. In practical, our method enhances the efficiency of obtaining topology-consistent face models from multi-view stereo facial scanning.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了一种几何光度联合对齐（GPJA）方法，通过结合几何和光度信息来精确对齐人体表情。注册人体头部的常见做法通常涉及使用几何处理方法将地标与面部模板网格对齐，但常常忽略光度一致性。 GPJA 通过利用可微分渲染将顶点与目标表达式对齐，自动实现几何和光度外观的联合对齐，而不需要语义注释或对齐网格进行训练，从而克服了这一限制。它具有整体渲染对齐策略和多尺度正则化优化，以实现稳健和快速的收敛。该方法利用顶点位置的导数进行监督，并采用基于梯度的算法，保证几何演化过程中的平滑性并避免拓扑缺陷。实验结果表明，在各种表达下均能忠实对齐，超越了传统的基于 ICP 的方法和最先进的基于深度学习的方法。实际上，我们的方法提高了从多视图立体面部扫描中获取拓扑一致的面部模型的效率。</details>
**PDF:** <http://arxiv.org/pdf/2403.02629v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning**<br />
**Title_cn:** 低分辨率引领潮流：通过自监督学习提高超分辨率的泛化能力<br />
**Authors:** Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Haoze Sun, Xueyi Zou, Zhensong Zhang, Youliang Yan, Lei Zhu<br />
**Abstract:** <details><summary>原文: </summary>For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel "Low-Res Leads the Way" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于图像超分辨率（SR）来说，弥合合成数据集性能与现实世界退化场景之间的差距仍然是一个挑战。这项工作引入了一种新颖的“低分辨率引领之路”（LWay）训练框架，将监督预训练与自监督学习相结合，以增强 SR 模型对现实世界图像的适应性。我们的方法利用低分辨率 (LR) 重建网络从 LR 图像中提取退化嵌入，将它们与超分辨率输出合并以进行 LR 重建。利用看不见的 LR 图像进行自我监督学习，引导模型将其建模空间适应目标域，从而促进 SR 模型的微调，而无需配对高分辨率 (HR) 图像。离散小波变换 (DWT) 的集成进一步细化了对高频细节的关注。广泛的评估表明，我们的方法显着提高了 SR 模型在未见过的现实数据集上的泛化和细节恢复能力，优于现有方法。我们的训练机制具有普遍兼容性，无需修改网络架构，使其成为现实世界 SR 应用的实用解决方案。</details>
**PDF:** <http://arxiv.org/pdf/2403.02601v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Pooling Image Datasets With Multiple Covariate Shift and Imbalance**<br />
**Title_cn:** 具有多个协变量偏移和不平衡的池化图像数据集<br />
**Authors:** Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Peiran Li, Vikas Singh<br />
**Abstract:** <details><summary>原文: </summary>Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effectiveness of this approach via extensive experiments on real datasets. Further, we discuss how this style of formulation offers a unified perspective on at least 5+ distinct problem settings, from self-supervised learning to matching problems in 3D reconstruction.</details>
**Abstract_cn:** <details><summary>译文: </summary>小样本量在许多学科中很常见，这需要在多个机构中汇集大致相似的数据集，以研究图像与疾病结果之间微弱但相关的关联。此类数据通常表现出协变量（即次要非成像数据）的偏移/不平衡。控制此类有害变量在标准统计分析中很常见，但这些想法并不直接适用于过度参数化模型。因此，最近的工作表明，不变表示学习的策略如何提供了一个有意义的起点，但当前的方法仅限于一次仅考虑几个协变量的变化/不平衡。在本文中，我们展示了如何从类别论的角度看待这个问题，提供了一种简单而有效的解决方案，完全避免了原本需要的复杂的多阶段训练流程。我们通过对真实数据集的大量实验展示了这种方法的有效性。此外，我们还讨论了这种表述风格如何为至少 5 个以上不同的问题设置提供统一的视角，从自我监督学习到 3D 重建中的匹配问题。</details>
**PDF:** <http://arxiv.org/pdf/2403.02598v1><br />
**Code:** null<br />

>## **各类学习方式**
>---
>>**index:** 1<br />
**Title:** **Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization**<br />
**Title_cn:** 对偶平均教师：用于视听源定位的无偏半监督框架<br />
**Authors:** Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, Yun Zheng<br />
**Abstract:** <details><summary>原文: </summary>Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods respectively, given only 3% positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance.</details>
**Abstract_cn:** <details><summary>译文: </summary>视听源定位 (AVSL) 旨在根据配对的音频剪辑在视频帧内定位发声对象。现有方法主要依赖于视听对应的自监督对比学习。如果没有任何边界框注释，它们就很难实现精确定位，尤其是对于小物体，并且会遭受边界模糊和误报的困扰。此外，朴素的半监督方法无法充分利用大量未标记数据的信息。在本文中，我们提出了一种新颖的 AVSL 半监督学习框架，即对偶平均教师（DMT），由两个师生结构组成，以规避确认偏差问题。具体来说，两名教师在有限的标记数据上进行了预先训练，通过他们的预测之间的共识来过滤掉噪声样本，然后通过交叉他们的置信图来生成高质量的伪标签。对标记和未标记数据的充分利用以及所提出的无偏框架使 DMT 能够大幅优于当前最先进的方法，在 Flickr-SoundNet 和 VGG-Sound Source 上的 CIoU 分别为 90.4% 和 48.8%，仅给出 3% 的位置注释，分别比自监督和半监督方法提高了 8.9%、9.6% 和 4.6%、6.4%。我们还将我们的框架扩展到一些现有的 AVSL 方法，并不断提高它们的性能。</details>
**PDF:** <http://arxiv.org/pdf/2403.03145v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization**<br />
**Title_cn:** 用于半监督视听源定位的交叉伪标签<br />
**Authors:** Yuxin Guo, Shijie Ma, Yuhao Zhao, Hu Su, Wei Zou<br />
**Abstract:** <details><summary>原文: </summary>Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation. We equip XPL with two effective components. Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training. Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias. Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability.</details>
**Abstract_cn:** <details><summary>译文: </summary>视听源定位 (AVSL) 是根据给定的音频提示识别场景中特定发声对象的任务。在我们的工作中，我们专注于带有伪标签的半监督 AVSL。为了解决普通硬伪标签的问题，包括偏差累积、噪声敏感性和不稳定性，我们提出了一种名为交叉伪标签（XPL）的新方法，其中两个模型通过交叉细化机制相互学习以避免偏差积累。我们为 XPL 配备了两个有效的组件。首先，带有锐化的软伪标签和伪标签指数移动平均机制使模型能够实现逐步自我改进并保证稳定的训练。其次，课程数据选择模块在训练过程中自适应地选择高质量的伪标签，以减轻潜在的偏差。实验结果表明，XPL 显着优于现有方法，实现了最先进的性能，同时有效减轻了确认偏差并确保了训练稳定性。</details>
**PDF:** <http://arxiv.org/pdf/2403.03095v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Recall-Oriented Continual Learning with Generative Adversarial Meta-Model**<br />
**Title_cn:** 具有生成对抗性元模型的面向回忆的持续学习<br />
**Authors:** Haneol Kang, Dong-Wan Choi<br />
**Abstract:** <details><summary>原文: </summary>The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.</details>
**Abstract_cn:** <details><summary>译文: </summary>稳定性-可塑性困境是持续学习中的一个主要挑战，因为它涉及平衡保持先前任务表现和学习新任务的相互冲突的目标。在本文中，我们提出了面向回忆的持续学习框架来应对这一挑战。受人脑分离负责稳定性和可塑性机制的能力的启发，我们的框架由两层架构组成，其中推理网络有效地获取新知识，生成网络在必要时回忆过去的知识。特别是，为了最大限度地提高过去知识的稳定性，我们根据不同的表示研究知识的复杂性，从而引入生成对抗性元模型（GAMM），它增量学习特定于任务的参数而不是任务的输入数据样本。通过我们的实验，我们表明我们的框架不仅可以在没有任何干扰的情况下有效地学习新知识，而且可以在任务感知和任务无关的学习场景中实现先前知识的高度稳定性。我们的代码位于：https://github.com/bigdata-inha/recall-orient-cl-framework。</details>
**PDF:** <http://arxiv.org/pdf/2403.03082v1><br />
**Code:** <https://github.com/bigdata-inha/recall-oriented-cl-framework>**<br />
>>**index:** 4<br />
**Title:** **Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives**<br />
**Title_cn:** 通过硬阴性和软阴性的监督对比学习进行康复运动质量评估<br />
**Authors:** Mark Karlov, Ali Abedi, Shehroz S. Khan<br />
**Abstract:** <details><summary>原文: </summary>Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entire dataset to train a single model applicable to all exercise types. This model, with a Spatial-Temporal Graph Convolutional Network (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity. Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, the University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD), IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE), our method has shown to surpass existing methods, setting a new benchmark in rehabilitation exercise assessment accuracy.</details>
**Abstract_cn:** <details><summary>译文: </summary>事实证明，基于运动的康复计划可以有效提高生活质量、降低死亡率和再住院率。人工智能驱动的虚拟康复允许患者在家中独立完成锻炼，利用人工智能算法分析锻炼数据，向患者提供反馈并更新临床医生的进展情况。这些程序通常规定了各种运动类型，这给康复运动评估数据集带来了明显的挑战：虽然总体训练样本丰富，但这些数据集对于每种运动类型的样本数量通常有限。这种差异阻碍了现有方法在每次练习中使用如此小的样本量来训练可推广模型的能力。为了解决这个问题，我们的论文引入了一种新颖的监督对比学习框架，该框架具有硬负样本和软负样本，该框架有效地利用整个数据集来训练适用于所有运动类型的单一模型。该模型采用时空图卷积网络 (ST-GCN) 架构，证明了跨练习的通用性增强，整体复杂性降低。通过对三个公开的康复运动评估数据集、爱达荷大学物理康复运动数据 (UI-PRMD)、IntelliRehabDS (IRDS) 以及用于远程监测身体康复的运动和临床评分的 Kinematic 评估 (KIMORE) 进行广泛的实验，我们的方法超越了现有方法，为康复运动评估准确性树立了新的基准。</details>
**PDF:** <http://arxiv.org/pdf/2403.02772v1><br />
**Code:** null<br />

>## **其他**
>---
>>**index:** 1<br />
**Title:** **A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives**<br />
**Title_cn:** 充满技能的背包：以自我为中心的视频理解与多样化的任务视角<br />
**Authors:** Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta<br />
**Abstract:** <details><summary>原文: </summary>Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>人类对视频流的理解自然是广泛的：在几个瞬间，我们能够理解正在发生的事情、对象的相关性和关系，并预测在不久的将来会发生什么，一切都在同一时间。我们相信，为了有效地将这种整体感知转移到智能机器上，学习关联概念和抽象来自不同任务的知识，并在学习新技能时协同利用它们，发挥着重要作用。为了实现这一目标，我们寻求一种统一的视频理解方法，它将人类行为的共享时间建模与最小的开销相结合，以支持多个下游任务并在学习新技能时实现合作。然后，我们提出了 EgoPack，这是一种创建任务视角集合的解决方案，可以在下游任务中携带这些视角，并用作额外见解的潜在来源，就像机器人可以随身携带并在需要时使用的技能背包。我们在四个 Ego4D 基准测试上展示了我们方法的有效性和效率，优于当前最先进的方法。</details>
**PDF:** <http://arxiv.org/pdf/2403.03037v1><br />
**Code:** null<br />
>>**index:** 2<br />
**Title:** **Gaze-Vector Estimation in the Dark with Temporally Encoded Event-driven Neural Networks**<br />
**Title_cn:** 使用时间编码事件驱动神经网络在黑暗中进行注视矢量估计<br />
**Authors:** Abeer Banerjee, Naval K. Mehta, Shyam S. Prasad, Himanshu, Sumeet Saurav, Sanjay Singh<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems. Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture. The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network. This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions. The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions. Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们解决了注视矢量预测的复杂挑战，这是一项关键任务，其应用范围从人机交互到驾驶员监控系统。我们的创新方法专为极低光照条件下的苛刻设置而设计，利用新颖的时间事件编码方案和专用的神经网络架构。时间编码方法将动态视觉传感器 (DVS) 事件与灰度引导帧无缝集成，生成连续编码的图像以输入到我们的神经网络中。这种独特的解决方案不仅可以捕获活跃年龄段参与者的不同注视反应，还引入了针对弱光条件量身定制的精选数据集。与我们的网络配对的编码时间帧在其预测中展示了令人印象深刻的空间定位和可靠的注视方向。我们的研究实现了 100% 的惊人 100 像素精度，强调了我们的神经网络在具有挑战性的低光视频中处理时间连续编码图像以进行精确注视矢量预测的潜力，从而有助于注视预测技术的进步。</details>
**PDF:** <http://arxiv.org/pdf/2403.02909v1><br />
**Code:** null<br />
>>**index:** 3<br />
**Title:** **Towards Robust Federated Learning via Logits Calibration on Non-IID Data**<br />
**Title_cn:** 通过非 IID 数据的 Logits 校准实现稳健的联邦学习<br />
**Authors:** Yu Qiao, Apurba Adhikary, Chaoning Zhang, Choong Seon Hong<br />
**Abstract:** <details><summary>原文: </summary>Federated learning (FL) is a privacy-preserving distributed management framework based on collaborative model training of distributed devices in edge networks. However, recent studies have shown that FL is vulnerable to adversarial examples (AEs), leading to a significant drop in its performance. Meanwhile, the non-independent and identically distributed (non-IID) challenge of data distribution between edge devices can further degrade the performance of models. Consequently, both AEs and non-IID pose challenges to deploying robust learning models at the edge. In this work, we adopt the adversarial training (AT) framework to improve the robustness of FL models against adversarial example (AE) attacks, which can be termed as federated adversarial training (FAT). Moreover, we address the non-IID challenge by implementing a simple yet effective logits calibration strategy under the FAT framework, which can enhance the robustness of models when subjected to adversarial attacks. Specifically, we employ a direct strategy to adjust the logits output by assigning higher weights to classes with small samples during training. This approach effectively tackles the class imbalance in the training data, with the goal of mitigating biases between local and global models. Experimental results on three dataset benchmarks, MNIST, Fashion-MNIST, and CIFAR-10 show that our strategy achieves competitive results in natural and robust accuracy compared to several baselines.</details>
**Abstract_cn:** <details><summary>译文: </summary>联邦学习（FL）是一种基于边缘网络中分布式设备协作模型训练的隐私保护分布式管理框架。然而，最近的研究表明，FL 很容易受到对抗性例子（AE）的影响，导致其性能显着下降。同时，边缘设备之间数据分布的非独立同分布（non-IID）挑战可能会进一步降低模型的性能。因此，AE 和非 IID 都对在边缘部署稳健的学习模型提出了挑战。在这项工作中，我们采用对抗性训练（AT）框架来提高 FL 模型针对对抗性示例（AE）攻击的鲁棒性，这可以称为联合对抗性训练（FAT）。此外，我们通过在 FAT 框架下实施简单而有效的 logits 校准策略来解决非 IID 挑战，这可以增强模型在遭受对抗性攻击时的鲁棒性。具体来说，我们采用直接策略通过在训练期间为小样本的类分配更高的权重来调整逻辑输出。这种方法有效地解决了训练数据中的类别不平衡问题，目标是减轻本地模型和全局模型之间的偏差。三个数据集基准（MNIST、Fashion-MNIST 和 CIFAR-10）的实验结果表明，与多个基准相比，我们的策略在自然和鲁棒的准确性方面取得了有竞争力的结果。</details>
**PDF:** <http://arxiv.org/pdf/2403.02803v1><br />
**Code:** null<br />
>>**index:** 4<br />
**Title:** **Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos**<br />
**Title_cn:** 为什么不使用你的教科书？教学视频的知识增强程序规划<br />
**Authors:** Kumaranage Ravindu Yasas Nagasinghe, Honglu Zhou, Malitha Gunawardhana, Martin Renqiang Min, Daniel Harari, Muhammad Haris Khan<br />
**Abstract:** <details><summary>原文: </summary>In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan. This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos. Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals. However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans. To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural knowledge. This knowledge, sourced from training procedure plans and structured as a directed weighted graph, equips the agent to better navigate the complexities of step sequencing and its potential variations. We coin our approach KEPP, a novel Knowledge-Enhanced Procedure Planning system, which harnesses a probabilistic procedural knowledge graph extracted from training data, effectively acting as a comprehensive textbook for the training domain. Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision.</details>
**Abstract_cn:** <details><summary>译文: </summary>在本文中，我们探讨了智能体构建行动步骤逻辑序列的能力，从而制定战略程序计划。该计划对于从最初的视觉观察到目标视觉结果至关重要，如现实教学视频中所示。现有的工作通过广泛利用数据集中可用的各种信息源（例如大量中间视觉观察、程序名称或自然语言逐步指令）来获得特征或监督信号，从而取得了部分成功。然而，由于步骤顺序中隐含的因果约束以及多个可行计划固有的可变性，这项任务仍然艰巨。为了解决以前的努力忽视的这些复杂问题，我们建议通过向代理注入程序知识来增强代理的能力。这些知识源自训练程序计划并构造为有向加权图，使代理能够更好地驾驭步骤排序的复杂性及其潜在变化。我们创造了我们的方法 KEPP，这是一种新颖的知识增强程序规划系统，它利用从训练数据中提取的概率程序知识图，有效地充当训练领域的综合教科书。在不同复杂度的设置下对三个广泛使用的数据集进行的实验评估表明，KEPP 可以获得卓越的、最先进的结果，同时只需要最少的监督。</details>
**PDF:** <http://arxiv.org/pdf/2403.02782v1><br />
**Code:** null<br />
>>**index:** 5<br />
**Title:** **Learning Group Activity Features Through Person Attribute Prediction**<br />
**Title_cn:** 通过人员属性预测学习群体活动特征<br />
**Authors:** Chihiro Nakatani, Hiroaki Kawashima, Norimichi Ukita<br />
**Abstract:** <details><summary>原文: </summary>This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.</details>
**Abstract_cn:** <details><summary>译文: </summary>本文提出了群体活动特征（GAF）学习，其中多人活动的特征被学习为紧凑的潜在向量。与监督学习需要手动注释群体活动的先前工作不同，我们的方法通过人员属性预测来学习 GAF，而不需要群体活动注释。通过以端到端的方式学习整个网络，从而需要GAF来预测群体中的人的属性，将GAF训练为多人活动的特征。作为人物属性，我们建议使用人物的动作类和外观特征，因为前者由于其简单而易于注释，而后者不需要手动注释。此外，我们引入了位置引导的属性预测来解开复杂的 GAF，以正确提取每个目标人的特征。各种实验结果验证了我们的方法在两个公共数据集上定量和定性地优于 SOTA 方法。我们的 GAF 的可视化还表明，我们的方法学习了代表细粒度小组活动类别的 GAF。代码：https://github.com/chihina/GAFL-CVPR2024。</details>
**PDF:** <http://arxiv.org/pdf/2403.02753v1><br />
**Code:** null<br />
>>**index:** 6<br />
**Title:** **DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization**<br />
**Title_cn:** DomainVerse：免调整自适应域泛化的现实世界分布转变的基准<br />
**Authors:** Feng Hou, Jin Yuan, Ying Yang, Yang Liu, Yang Zhang, Cheng Zhong, Zhongchao Shi, Jianping Fan, Yong Rui, Zhiqiang He<br />
**Abstract:** <details><summary>原文: </summary>Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.</details>
**Abstract_cn:** <details><summary>译文: </summary>传统的跨域任务，包括域适应和域泛化，严重依赖于源域数据的训练模型。随着视觉语言模型（VLM）（被视为自然源模型）的最新进展，跨域任务发生变化，直接将预先训练的源模型适应配备先验领域知识的任意目标域，我们将此任务命名为“自适应”域泛化（ADG）。然而，当前的跨域数据集存在许多局限性，例如域不切实际、域定义不明确以及无法细粒度域分解，这促使我们为 ADG 建立一个新颖的数据集 DomainVerse。受益于域转移的分层定义，DomainVerse 由来自 390 个细粒度现实域的约 50 万张图像组成。借助构建的 DomainVerse 和 VLM，我们提出了两种称为 Domain CLIP 和 Domain++ CLIP 的方法，用于免调整自适应域泛化。广泛而全面的实验证明了该数据集的重要性和所提出方法的有效性。</details>
**PDF:** <http://arxiv.org/pdf/2403.02714v1><br />
**Code:** null<br />
>>**index:** 7<br />
**Title:** **Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning**<br />
**Title_cn:** 通过转移矩阵进行基于狄利克雷的每样本加权，用于噪声标签学习<br />
**Authors:** HeeSun Bae, Seungjae Shin, Byeonghu Na, Il-Chul Moon<br />
**Abstract:** <details><summary>原文: </summary>For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets. Our code is available at \url{https://github.com/BaeHeeSun/RENT}.</details>
**Abstract_cn:** <details><summary>译文: </summary>对于使用噪声标签进行学习，转移矩阵明确地模拟了噪声标签分布和干净标签分布之间的关系，已被用来实现分类器或风险的统计一致性。以往的研究更多地关注于如何很好地估计这个转移矩阵，而不是如何利用它。我们提出良好的利用转移矩阵至关重要，并提出了一种基于重采样的新利用方法，即 RENT。具体来说，我们首先证明当前的利用率可能对实施有潜在的限制。作为重新加权的扩展，我们建议使用基于狄利克雷分布的每样本权重采样（DWS）框架，并比较 DWS 框架下的重新加权和重采样。通过DWS的分析，我们提出了RENT，一种带有噪声转移矩阵的重采样方法。根据经验，RENT 在各种基准数据集上始终优于现有的转移矩阵利用方法（包括重新加权）。我们的代码可在 \url{https://github.com/BaeHeeSun/RENT} 获取。</details>
**PDF:** <http://arxiv.org/pdf/2403.02690v1><br />
**Code:** <https://github.com/BaeHeeSun/RENT>**<br />
>>**index:** 8<br />
**Title:** **What do we learn from inverting CLIP models?**<br />
**Title_cn:** 我们从反演 CLIP 模型中学到了什么？<br />
**Authors:** Hamid Kazemi, Atoosa Chegini, Jonas Geiping, Soheil Feizi, Tom Goldstein<br />
**Abstract:** <details><summary>原文: </summary>We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts. We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous prompts, like "a beautiful landscape," as well as for prompts involving the names of celebrities.</details>
**Abstract_cn:** <details><summary>译文: </summary>我们采用基于反演的方法来检查 CLIP 模型。我们的研究表明，反转 CLIP 模型会生成与指定目标提示语义一致的图像。我们利用这些倒置图像来深入了解 CLIP 模型的各个方面，例如它们融合概念和包容性别偏见的能力。我们在模型反演过程中特别观察到 NSFW（工作不安全）图像的实例。即使对于语义上无害的提示（例如“美丽的风景”）以及涉及名人姓名的提示，也会发生这种现象。</details>
**PDF:** <http://arxiv.org/pdf/2403.02580v1><br />
**Code:** null<br />
>>**index:** 9<br />
**Title:** **DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training**<br />
**Title_cn:** DPAdapter：通过噪声容忍预训练改进差分隐私深度学习<br />
**Authors:** Zihao Wang, Rui Zhu, Dongruo Zhou, Zhikun Zhang, John Mitchell, Haixu Tang, XiaoFeng Wang<br />
**Abstract:** <details><summary>原文: </summary>Recent developments have underscored the critical role of \textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML algorithms by enhancing parameter robustness. The fundamental intuition behind this strategy is that models with robust parameters are inherently more resistant to the noise introduced by DP, thereby retaining better performance despite the perturbations. DPAdapter modifies and enhances the sharpness-aware minimization (SAM) technique, utilizing a two-batch strategy to provide a more accurate perturbation estimate and an efficient gradient descent, thereby improving parameter robustness against noise. Notably, DPAdapter can act as a plug-and-play component and be combined with existing DPML algorithms to further improve their performance. Our experiments show that DPAdapter vastly enhances state-of-the-art DPML algorithms, increasing average accuracy from 72.92\% to 77.09\% with a privacy budget of $\epsilon=4$.</details>
**Abstract_cn:** <details><summary>译文: </summary>最近的发展强调了 \textit{差分隐私} (DP) 在保护训练机器学习模型的个人数据方面的关键作用。然而，由于训练过程中引入扰动，集成DP常常会导致模型性能显着下降，这在{差分隐私机器学习}（DPML）领域提出了巨大的挑战。为此，人们提出了一些缓解措施，通常围绕制定新的 DPML 算法或放宽 DP 定义以与不同的上下文相协调。尽管采取了这些举措，DP 对模型（尤其是大型模型）造成的影响仍然很大，因此需要一种创新的解决方案来巧妙地避免模型效用的相应损害。为此，我们引入了 DPAdapter，这是一项开创性技术，旨在通过增强参数鲁棒性来增强 DPML 算法的模型性能。该策略背后的基本直觉是，具有鲁棒参数的模型本质上更能抵抗动态规划引入的噪声，从而在存在扰动的情况下保持更好的性能。 DPAdapter修改并增强了锐度感知最小化（SAM）技术，利用两批策略提供更准确的扰动估计和高效的梯度下降，从而提高参数对噪声的鲁棒性。值得注意的是，DPAdapter 可以充当即插即用组件，并与现有的 DPML 算法相结合，以进一步提高其性能。我们的实验表明，DPAdapter 极大地增强了最先进的 DPML 算法，将平均准确度从 72.92% 提高到 77.09%，隐私预算为 $\epsilon=4$。</details>
**PDF:** <http://arxiv.org/pdf/2403.02571v1><br />
**Code:** null<br />

