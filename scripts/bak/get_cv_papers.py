#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CVè®ºæ–‡è·å–ä¸åˆ†ç±»ç¨‹åº

ä» arXiv è·å–æœ€æ–°CVè®ºæ–‡å¹¶ä½¿ç”¨ChatGLMæ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚

ä½œè€…: Jasmine Bloom
æ—¥æœŸ: 2025-06
"""

import os
import re
import math
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from collections import defaultdict
from categories_config import CATEGORY_DISPLAY_ORDER, CATEGORY_THRESHOLDS, CATEGORY_KEYWORDS
from chatglm_helper import ChatGLMHelper
from typing import Dict, List, Tuple, Optional
import traceback
import arxiv
import random

from chatglm_helper import ChatGLMHelper
chatglm = ChatGLMHelper()

# ä½¿ç”¨logger_configæ¨¡å—é…ç½®æ—¥å¿—è®°å½•å™¨
from logger_config import setup_logger
# ä½¿ç”¨ç®€åŒ–æ ¼å¼ï¼Œä¸æ˜¾ç¤ºæ—¶é—´ã€çº§åˆ«å’Œæ—¥å¿—å
logger = setup_logger(name='cv_papers', level='info', simple_format=True)

# æŸ¥è¯¢å‚æ•°
QUERY_DAYS_AGO = 3          # ç›®æ ‡æ—¥æœŸåç§»é‡
MAX_RESULTS = 300           # æœ€å¤§è®ºæ–‡æ•°
MAX_WORKERS = 8            # å¹¶è¡Œçº¿ç¨‹æ•°

def get_related_category_pairs():
    """
    æ ¹æ®æœ€æ–°çš„ç±»åˆ«å®šä¹‰ç”Ÿæˆç›¸å…³ç±»åˆ«å¯¹
    è¿™äº›å¯¹åœ¨è®¡ç®—ç±»åˆ«ç›¸ä¼¼åº¦æ—¶ä½¿ç”¨
    
    Returns:
        List[Tuple[str, str]]: ç›¸å…³ç±»åˆ«å¯¹åˆ—è¡¨
    """
    # ä½¿ç”¨åŸºç¡€ç ”ç©¶ç±»åˆ«ï¼ˆCATEGORY_DISPLAY_ORDERçš„å‰10ä¸ªï¼‰
    basic_research_categories = CATEGORY_DISPLAY_ORDER[:10]
    
    # åˆ›å»ºç›¸å…³ç±»åˆ«å¯¹
    # å°†æ¯ä¸ªç±»åˆ«åç§°çš„ä¸­æ–‡éƒ¨åˆ†æå–å‡ºæ¥ï¼ˆå»é™¤è‹±æ–‡éƒ¨åˆ†ï¼‰
    related_pairs = [
        # è§†è§‰è¡¨å¾å­¦ä¹ ä¸å¤§æ¨¡å‹ å’Œ é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ
        (basic_research_categories[0].split(' (')[0], basic_research_categories[1].split(' (')[0]),
        # é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ å’Œ ä¸‰ç»´é‡å»ºä¸å‡ ä½•æ„ŸçŸ¥
        (basic_research_categories[1].split(' (')[0], basic_research_categories[3].split(' (')[0]),
        # ç”Ÿæˆå¼è§†è§‰ä¸å†…å®¹åˆ›å»º å’Œ å¤šæ¨¡æ€è§†è§‰ä¸è·¨æ¨¡æ€å­¦ä¹ 
        (basic_research_categories[2].split(' (')[0], basic_research_categories[5].split(' (')[0]),
        # åŠ¨æ€è§†è§‰ä¸æ—¶åºå»ºæ¨¡ å’Œ æ„ŸçŸ¥-åŠ¨ä½œæ™ºèƒ½ä¸ä¸»åŠ¨è§†è§‰
        (basic_research_categories[4].split(' (')[0], basic_research_categories[8].split(' (')[0]),
        # æ¨¡å‹ä¼˜åŒ–ä¸ç³»ç»Ÿé²æ£’æ€§ å’Œ æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½
        (basic_research_categories[6].split(' (')[0], basic_research_categories[7].split(' (')[0]),
        # æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½ å’Œ å‰æ²¿è§†è§‰ç†è®ºä¸è·¨å­¦ç§‘èåˆ
        (basic_research_categories[7].split(' (')[0], basic_research_categories[9].split(' (')[0])
    ]
    
    return related_pairs

# å¯¼å…¥NLTKåº“ç”¨äºæ–‡æœ¬é¢„å¤„ç†
try:
    import nltk
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    
    # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè®°å½•NLTKæ•°æ®æ˜¯å¦å·²ä¸‹è½½
    nltk_flag_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.nltk_data_downloaded')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡NLTKæ•°æ®
    if os.path.exists(nltk_flag_file):
        # å·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥ä½¿ç”¨
        logger.debug("NLTKæ•°æ®å·²ä¸‹è½½è¿‡ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
        NLTK_AVAILABLE = True
    else:
        logger.info("é¦–æ¬¡è¿è¡Œï¼Œæ£€æŸ¥NLTKæ•°æ®æ˜¯å¦å·²ä¸‹è½½")
        # æ£€æŸ¥å¿…è¦çš„NLTKæ•°æ®æ˜¯å¦å·²ä¸‹è½½
        needed_data = []
        for data_name in ['punkt', 'wordnet', 'stopwords']:
            try:
                path = f"{'tokenizers/' if data_name == 'punkt' else 'corpora/'}{data_name}"
                nltk.data.find(path)
                logger.debug(f"NLTKæ•°æ® '{data_name}' å·²å­˜åœ¨äº: {path}")
            except LookupError:
                needed_data.append(data_name)
                logger.info(f"NLTKæ•°æ® '{data_name}' ä¸å­˜åœ¨ï¼Œå°†ä¸‹è½½")
        
        # åªä¸‹è½½ç¼ºå¤±çš„æ•°æ®ï¼Œé¿å…é‡å¤ä¸‹è½½
        if needed_data:
            logger.info(f"æ­£åœ¨ä¸‹è½½ç¼ºå¤±çš„NLTKæ•°æ®æ–‡ä»¶: {', '.join(needed_data)}")
            for data_name in needed_data:
                logger.info(f"å¼€å§‹ä¸‹è½½ '{data_name}'...")
                download_result = nltk.download(data_name, quiet=False)
                logger.info(f"ä¸‹è½½ '{data_name}' ç»“æœ: {download_result}")
            logger.info("NLTKæ•°æ®æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        
        # ç‰¹åˆ«å¤„ç†punkt_tabï¼ˆæŸäº›NLTKåŠŸèƒ½éœ€è¦æ­¤æ•°æ®ï¼‰
        try:
            nltk.data.find('tokenizers/punkt_tab')
            logger.debug("NLTKæ•°æ® 'punkt_tab' å·²å­˜åœ¨")
        except LookupError:
            logger.info("å¼€å§‹ä¸‹è½½ 'punkt_tab'...")
            # é‡æ–°ä¸‹è½½punktå¯èƒ½ä¼šåŒ…å«punkt_tab
            download_result = nltk.download('punkt', quiet=False)
            logger.info(f"ä¸‹è½½ 'punkt' ç»“æœ: {download_result}")
        
        # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è¡¨ç¤ºæ•°æ®å·²ä¸‹è½½ï¼Œé¿å…ä¸‹æ¬¡é‡å¤ä¸‹è½½
        with open(nltk_flag_file, 'w') as f:
            f.write(f"NLTK data downloaded at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        logger.info("NLTKåˆå§‹åŒ–å®Œæˆ")
        NLTK_AVAILABLE = True
    
    NLTK_AVAILABLE = True
except ImportError:
    logger.warning("NLTKåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºæœ¬æ–‡æœ¬å¤„ç†æ–¹æ³•")
    NLTK_AVAILABLE = False

def extract_github_link(text, paper_url=None, title=None, authors=None, pdf_url=None):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥

    Args:
        text: è®ºæ–‡æ‘˜è¦æ–‡æœ¬
        paper_url: è®ºæ–‡URL
        title: è®ºæ–‡æ ‡é¢˜
        authors: ä½œè€…åˆ—è¡¨
        pdf_url: PDFæ–‡ä»¶URL

    Returns:
        str: GitHubé“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # ä»æ‘˜è¦ä¸­æŸ¥æ‰¾
    for pattern in github_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            url = match.group(0)
            if not url.startswith('http'):
                url = 'https://' + url
            return url

    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç /è´¡çŒ®']
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
        # åªè¾“å‡ºä¸€æ¬¡ä¸»ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        papers_by_subcategory = defaultdict(list)
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if subcategory and subcategory != "æœªæŒ‡å®š":
                papers_by_subcategory[subcategory].append(paper)
            elif category == "å…¶ä»– (Others)":
                papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
        if not papers_by_subcategory and category != "å…¶ä»– (Others)":
            continue
        for subcategory, papers in papers_by_subcategory.items():
            markdown += f"\n### {subcategory}\n\n"
            markdown += "|" + "|".join(headers) + "|\n"
            markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"
            for paper in papers:
                if paper['is_updated']:
                    status = f"ğŸ“ æ›´æ–°"
                else:
                    status = f"ğŸ†• å‘å¸ƒ"
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")]
                    else:
                        items = [core_contribution.strip()]
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    items = items[:2]
                    items = [(i[:50] + ("..." if len(i) > 50 else "")) for i in items]
                    return items
                contrib_list = []
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                if paper['github_url'] != 'None':
                    code_and_contribution = f"[ä»£ç ]({paper['github_url']})"
                    if contrib_list:
                        code_and_contribution += "; " + "; ".join(contrib_list)
                elif contrib_list:
                    code_and_contribution = "; ".join(contrib_list)
                else:
                    code_and_contribution = 'æ— '
                values = [
                    status,
                    paper['title'],
                    paper.get('title_zh', ''),
                    paper['authors'],
                    f"<{paper['pdf_url']}>",
                    code_and_contribution,
                ]
                values = [str(v).replace('\n', ' ').replace('|', '&#124;') for v in values]
                markdown += "|" + "|".join(values) + "|\n"
            markdown += "\n"
    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
            
        # æ·»åŠ ä¸€çº§ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        
        # æŒ‰å­ç±»åˆ«ç»„ç»‡è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        
        # å°†æ‰€æœ‰è®ºæ–‡åˆ†é…åˆ°å­ç±»åˆ«
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if subcategory and subcategory != "æœªæŒ‡å®š":
                papers_by_subcategory[subcategory].append(paper)
            elif category == "å…¶ä»– (Others)":
                # å¯¹äº"å…¶ä»–"ç±»åˆ«ï¼Œæ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç›´æ¥æ˜¾ç¤ºåœ¨ä¸»ç±»åˆ«ä¸‹
                papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
        
        # å¦‚æœå½“å‰ç±»åˆ«ä¸‹æ²¡æœ‰å¸¦å­ç±»åˆ«çš„è®ºæ–‡ï¼Œè·³è¿‡
        if not papers_by_subcategory and category != "å…¶ä»– (Others)":
            continue
            
        # å¤„ç†æ¯ä¸ªå­ç±»åˆ«
        for subcategory, papers in papers_by_subcategory.items():
            # æ·»åŠ äºŒçº§ç±»åˆ«æ ‡é¢˜
            markdown += f"\n### {subcategory}\n\n"
            
            # æ·»åŠ è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            for idx, paper in enumerate(papers, 1):
                # å¼•ç”¨ç¼–å·
                markdown += f'**index:** {idx}<br />\n'
                # æ—¥æœŸ
                markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
                # è‹±æ–‡æ ‡é¢˜
                markdown += f'**Title:** {paper["title"]}<br />\n'
                # ä¸­æ–‡æ ‡é¢˜
                markdown += f'**Title_cn:** {paper.get("title_zh", "")}<br />\n'
                # ä½œè€…ï¼ˆå·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²ï¼‰
                markdown += f'**Authors:** {paper["authors"]}<br />\n'
                # PDFé“¾æ¥
                markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

                # åˆå¹¶ä»£ç é“¾æ¥å’Œç²¾ç®€åçš„æ ¸å¿ƒè´¡çŒ®
                markdown += '**Code/Contribution:**\n'
                
                # ç²¾ç®€æ ¸å¿ƒè´¡çŒ®å†…å®¹
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    # åˆ†å‰²ä¸ºå¤šæ¡
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")] 
                    else:
                        items = [core_contribution.strip()]
                    # å»é™¤æ¨¡æ¿åŒ–å†…å®¹
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    # åªä¿ç•™å‰ä¸‰æ¡
                    items = items[:3]
                    return items
                
                # å¤„ç†æ ¸å¿ƒè´¡çŒ®
                contrib_list = []
                if "æ ¸å¿ƒé—®é¢˜" in paper:
                    markdown += f'é—®é¢˜ï¼š{paper["æ ¸å¿ƒé—®é¢˜"]}\n'
                
                if "æ ¸å¿ƒæ–¹æ³•" in paper:
                    markdown += f'æ–¹æ³•ï¼š{paper["æ ¸å¿ƒæ–¹æ³•"]}\n'
                
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                    if contrib_list:
                        markdown += f'{", ".join(contrib_list)}\n'
                
                # å¤„ç†ä»£ç é“¾æ¥
                if paper['github_url'] != 'None':
                    markdown += f'[ä»£ç ]({paper["github_url"]})\n'
                
                # æ·»åŠ ç©ºè¡Œ
                markdown += '\n'

    return markdown


def preprocess_text(text: str) -> str:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å°å†™è½¬æ¢ã€åˆ†è¯ã€å»åœç”¨è¯ã€è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: é¢„å¤„ç†åçš„æ–‡æœ¬
    """
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # åŸºæœ¬æ–‡æœ¬å¤„ç†ï¼šå…ˆå»é™¤ç‰¹æ®Šå­—ç¬¦
    basic_processed = re.sub(r'[^\w\s]', ' ', text)
    
    # å¦‚æœNLTKä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸºæœ¬å¤„ç†ç»“æœ
    if not NLTK_AVAILABLE:
        return basic_processed
    
    # å°è¯•ä½¿ç”¨NLTKè¿›è¡Œé«˜çº§å¤„ç†
    try:
        # åˆ†è¯ - å…ˆä½¿ç”¨åŸºæœ¬åˆ†è¯ä½œä¸ºå¤‡é€‰
        try:
            tokens = word_tokenize(text)
        except Exception:
            # å¦‚æœé«˜çº§åˆ†è¯å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯
            tokens = basic_processed.split()
        
        # å»é™¤åœç”¨è¯
        try:
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        except Exception:
            # å¦‚æœåœç”¨è¯å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åœç”¨è¯åˆ—è¡¨
            basic_stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by'}
            tokens = [token for token in tokens if token not in basic_stop_words and len(token) > 2]
        
        # è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ - å¯é€‰åŠŸèƒ½
        try:
            stemmer = PorterStemmer()
            stemmed_tokens = [stemmer.stem(token) for token in tokens]
            
            lemmatizer = WordNetLemmatizer()
            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]
            
            # é‡æ–°ç»„åˆæˆæ–‡æœ¬
            return " ".join(lemmatized_tokens)
        except Exception:
            # å¦‚æœè¯å¹²æå–æˆ–è¯å½¢è¿˜åŸå¤±è´¥ï¼Œåªè¿”å›åˆ†è¯å’Œå»åœç”¨è¯çš„ç»“æœ
            return " ".join(tokens)
    
    except Exception as e:
        logger.error(f"NLTKå¤„ç†æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
        # å¦‚æœæ‰€æœ‰NLTKå¤„ç†éƒ½å¤±è´¥ï¼Œå›é€€åˆ°åŸºæœ¬å¤„ç†
        return basic_processed


def get_category_by_keywords(title: str, abstract: str, categories_config: Dict) -> List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]:
    """
    æ‰§è¡ŒåŸºäºå…³é”®è¯åŒ¹é…å’Œä¼˜å…ˆçº§è§„åˆ™çš„å±‚æ¬¡åŒ–è®ºæ–‡åˆ†ç±»ï¼Œå¸¦æœ‰å¢å¼ºçš„æ–‡æœ¬å¤„ç†å’Œç½®ä¿¡åº¦è¯„åˆ†ã€‚
    
    Args:
        title (str): è®ºæ–‡æ ‡é¢˜ï¼Œç”¨äºä¸»è¦ä¸Šä¸‹æ–‡åˆ†æ
        abstract (str): è®ºæ–‡æ‘˜è¦ï¼Œç”¨äºå…¨é¢å†…å®¹åˆ†æ
        categories_config (Dict): åŒ…å«ç±»åˆ«å®šä¹‰ã€å…³é”®è¯ã€æƒé‡å’Œä¼˜å…ˆçº§çš„é…ç½®å­—å…¸
    
    å®ç°ç»†èŠ‚:
        1. å¢å¼ºæ–‡æœ¬é¢„å¤„ç†ï¼š
           - å¤§å°å†™æ ‡å‡†åŒ–å’Œæ ‡å‡†åŒ–å¤„ç†
           - æ ‡é¢˜å’Œæ‘˜è¦çš„ç»„åˆåˆ†æï¼Œä½¿ç”¨å·®å¼‚åŒ–æƒé‡
           - é«˜çº§åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
           - å¤šçº§è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
           - N-gramåˆ†æï¼Œæé«˜çŸ­è¯­åŒ¹é…å‡†ç¡®æ€§
        
        2. ä¼˜åŒ–è¯„åˆ†æœºåˆ¶ï¼š
           - ä¸»è¦å¾—åˆ†ï¼šåŠ æƒå…³é”®è¯åŒ¹é…ï¼ˆåŠ¨æ€åŸºç¡€æƒé‡ï¼‰
           - æ ‡é¢˜åŠ æˆï¼šæ ‡é¢˜åŒ¹é…çš„é¢å¤–æƒé‡ï¼ˆä¼˜åŒ–åŠ æƒï¼‰
           - ç²¾ç¡®åŒ¹é…åŠ æˆï¼šå®Œæ•´çŸ­è¯­åŒ¹é…çš„é¢å¤–æƒé‡
           - ä¼˜å…ˆçº§ä¹˜æ•°ï¼šç±»åˆ«ç‰¹å®šé‡è¦æ€§ç¼©æ”¾
           - è´Ÿé¢å…³é”®è¯æƒ©ç½šï¼šä½¿ç”¨æ”¹è¿›çš„é€»è¾‘å‡½æ•°å¹³æ»‘æƒ©ç½š
           - ç±»åˆ«ç›¸å…³æ€§åˆ¤æ–­ï¼šè€ƒè™‘ç±»åˆ«é—´çš„ç›¸å…³æ€§
        
        3. æ™ºèƒ½åˆ†ç±»é€»è¾‘ï¼š
           - ä½¿ç”¨ç±»åˆ«è‡ªå®šä¹‰é˜ˆå€¼ä¸åŠ¨æ€é˜ˆå€¼è°ƒæ•´
           - å¢å¼ºçš„å­ç±»åˆ«åˆ†ç±»
           - ä¼˜å…ˆç±»åˆ«çš„å±‚æ¬¡åŒ–å¤„ç†
           - æ™ºèƒ½å›é€€æœºåˆ¶ï¼Œè€ƒè™‘ç±»åˆ«ç›¸å…³æ€§
           - ç½®ä¿¡åº¦è¯„åˆ†å’Œåˆ†ç±»è§£é‡Š
    
    Returns:
        List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]: æŒ‰ç½®ä¿¡åº¦é™åºæ’åºçš„ 
        (ç±»åˆ«, ç½®ä¿¡åº¦åˆ†æ•°, å­ç±»åˆ«ä¿¡æ¯, åˆ†ç±»è§£é‡Š) å…ƒç»„åˆ—è¡¨
    """
    # æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    
    # ä½¿ç”¨é«˜çº§æ–‡æœ¬é¢„å¤„ç†
    processed_title = preprocess_text(title)
    processed_abstract = preprocess_text(abstract)
    processed_combined = processed_title + " " + processed_abstract
    
    # ç§»é™¤å¸¸è§çš„åœç”¨è¯ï¼Œæé«˜åŒ¹é…è´¨é‡
    stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by', 
                 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 
                 'does', 'did', 'but', 'if', 'then', 'else', 'when', 'up', 'down', 'this', 'that'}
    
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    title_words = set(w for w in title_lower.split() if w not in stop_words)
    abstract_words = set(w for w in abstract_lower.split() if w not in stop_words)
    
    # ç»„åˆæ–‡æœ¬ç”¨äºåŒ¹é…
    combined_text = title_lower + " " + abstract_lower
    
    # åˆå§‹åŒ–å¾—åˆ†ç´¯åŠ å™¨å’ŒåŒ¹é…è®°å½•
    scores = defaultdict(float)
    match_details = defaultdict(list)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¾—åˆ†
    for category, config in categories_config.items():
        score = 0.0
        matches = []
        
        # 1. æ­£å‘å…³é”®è¯åŒ¹é…
        for keyword, weight in config["keywords"]:
            keyword_lower = keyword.lower()
            keyword_words = set(w for w in keyword_lower.split() if w not in stop_words)
            
            # å¯¹å…³é”®è¯ä¹Ÿè¿›è¡Œé¢„å¤„ç†
            processed_keyword = preprocess_text(keyword)
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if keyword_lower in title_lower:
                match_score = weight * 0.25  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æœ€é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif keyword_lower in abstract_lower:
                match_score = weight * 0.15  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æ¬¡ä¹‹
                score += match_score
                matches.append(f"æ‘˜è¦ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒåŒ¹é…ï¼ˆæé«˜å‡†ç¡®æ€§ï¼‰
            elif processed_keyword in processed_title:
                match_score = weight * 0.22  # é¢„å¤„ç†æ ‡é¢˜ä¸­çš„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif processed_keyword in processed_abstract:
                match_score = weight * 0.14  # é¢„å¤„ç†æ‘˜è¦ä¸­çš„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(title_words):
                match_score = weight * 0.18  # æ ‡é¢˜ä¸­çš„è¯ç»„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ‘˜è¦ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(abstract_words):
                match_score = weight * 0.12  # æ‘˜è¦ä¸­çš„è¯ç»„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # å•è¯åŒ¹é…ï¼ˆä½æƒé‡ï¼‰
            else:
                # å°†å…³é”®è¯æ‹†åˆ†ä¸ºå•è¯è¿›è¡ŒåŒ¹é…
                word_matches = 0
                title_match_bonus = 0
                
                # åˆ†åˆ«å¤„ç†åŸå§‹æ–‡æœ¬å’Œé¢„å¤„ç†æ–‡æœ¬çš„åŒ¹é…
                for word in keyword_words:
                    if len(word) <= 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                        continue
                        
                    if word in title_words:
                        word_matches += 1
                        title_match_bonus += 1  # æ ‡é¢˜åŒ¹é…é¢å¤–åŠ åˆ†
                    elif word in abstract_words:
                        word_matches += 0.6  # æ‘˜è¦åŒ¹é…çš„æƒé‡ä½äºæ ‡é¢˜
                
                # å¤„ç†é¢„å¤„ç†æ–‡æœ¬ä¸­çš„åŒ¹é…
                processed_keyword_words = processed_keyword.split()
                for word in processed_keyword_words:
                    if len(word) <= 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                        continue
                        
                    if word in processed_title:
                        word_matches += 0.8  # é¢„å¤„ç†æ–‡æœ¬ä¸­çš„åŒ¹é…æƒé‡ç¨ä½
                        title_match_bonus += 0.8
                    elif word in processed_abstract:
                        word_matches += 0.5
                
                # åªæœ‰å½“åŒ¹é…åˆ°è¶³å¤Ÿå¤šçš„å•è¯æ—¶æ‰è®¡ç®—å¾—åˆ†
                if word_matches > 0 and len(keyword_words) > 0:
                    # è®¡ç®—åŒ¹é…æ¯”ä¾‹
                    match_ratio = word_matches / (len(keyword_words) + len(processed_keyword_words) / 2)
                    if match_ratio >= 0.4:  # é™ä½é˜ˆå€¼ä»¥å¢åŠ çµæ´»æ€§
                        match_score = weight * 0.08 * match_ratio  # åŸºç¡€åˆ†
                        title_bonus = weight * 0.04 * (title_match_bonus / (len(keyword_words) + len(processed_keyword_words) / 2))  # æ ‡é¢˜åŠ åˆ†
                        
                        total_score = match_score + title_bonus
                        score += total_score
                        matches.append(f"å•è¯åŒ¹é… [{keyword}]: +{total_score:.2f} (åŒ¹é…ç‡: {match_ratio:.1f})")
            
            # å•è¯é¢‘ç‡åŠ æˆï¼ˆå¯¹äºé‡å¤å‡ºç°çš„å…³é”®è¯ç»™äºˆé¢å¤–åŠ æˆï¼‰
            if len(keyword_words) == 1 and keyword_lower in combined_text:
                # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
                frequency = combined_text.count(keyword_lower)
                if frequency > 1:
                    # é¢‘ç‡åŠ æˆï¼Œä½†æœ‰ä¸Šé™
                    freq_bonus = min(frequency * 0.02, 0.1) * weight
                    score += freq_bonus
                    matches.append(f"é¢‘ç‡åŠ æˆ [{keyword}] (x{frequency}): +{freq_bonus:.2f}")
        
        # 2. è´Ÿå‘å…³é”®è¯æƒ©ç½šï¼ˆæ›´ä¸¥æ ¼çš„æƒ©ç½šæœºåˆ¶ï¼‰
        if "negative_keywords" in config:
            negative_score = 0
            for keyword_tuple in config["negative_keywords"]:
                if isinstance(keyword_tuple, tuple) and len(keyword_tuple) >= 1:
                    keyword = keyword_tuple[0].lower()
                    neg_weight = keyword_tuple[1] if len(keyword_tuple) > 1 else 1.0
                else:
                    # å…¼å®¹å­—ç¬¦ä¸²æ ¼å¼
                    keyword = str(keyword_tuple).lower()
                    neg_weight = 1.0
                
                # æ£€æŸ¥è´Ÿå‘å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æœ¬ä¸­
                if keyword in combined_text:
                    # è®¡ç®—æƒ©ç½šåˆ†æ•°
                    penalty = neg_weight * 1.2
                    negative_score += penalty
                    matches.append(f"è´Ÿå‘åŒ¹é… [{keyword}]: -{penalty:.2f}")
            
            # ä½¿ç”¨æ›´å¹³æ»‘çš„æƒ©ç½šå‡½æ•°
            if negative_score > 0:
                original_score = score
                # ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è´Ÿå‘å…³é”®è¯å¤„ç†
                # æ£€æŸ¥è´Ÿå‘å…³é”®è¯çš„ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å¦å®šè¯æˆ–å¯¹ç«‹è¯
                context_adjustment = 1.0
                
                # æ£€æŸ¥å¦å®šè¯å’Œå¯¹ç«‹è¯
                negation_words = ["not", "without", "no", "non", "instead of", "rather than", "unlike"]
                opposition_words = ["but", "however", "although", "despite", "contrary to"]
                
                # å¦‚æœå­˜åœ¨å¦å®šè¯æˆ–å¯¹ç«‹è¯ï¼Œå‡å°‘æƒ©ç½š
                for neg_word in negation_words:
                    if neg_word + " " + keyword_lower in combined_text or neg_word + "-" + keyword_lower in combined_text:
                        context_adjustment = 0.5  # å¤§å¹…å‡å°‘æƒ©ç½š
                        matches.append(f"æ£€æµ‹åˆ°å¦å®šä¸Šä¸‹æ–‡: '{neg_word} {keyword_lower}', æƒ©ç½šå‡å°‘")
                        break
                
                for opp_word in opposition_words:
                    if opp_word in combined_text and combined_text.find(opp_word) < combined_text.find(keyword_lower):
                        # å¦‚æœå¯¹ç«‹è¯åœ¨å…³é”®è¯ä¹‹å‰ï¼Œå‡å°‘æƒ©ç½š
                        context_adjustment = 0.7
                        matches.append(f"æ£€æµ‹åˆ°å¯¹ç«‹ä¸Šä¸‹æ–‡: '{opp_word}... {keyword_lower}', æƒ©ç½šéƒ¨åˆ†å‡å°‘")
                        break
                
                # åº”ç”¨ä¸Šä¸‹æ–‡è°ƒæ•´
                negative_score *= context_adjustment
                
                # ä½¿ç”¨æ”¹è¿›çš„æƒ©ç½šå‡½æ•°
                # å¯¹äºè¾ƒå°çš„è´Ÿå‘åˆ†æ•°ä½¿ç”¨çº¿æ€§æƒ©ç½šï¼Œå¯¹äºè¾ƒå¤§çš„è´Ÿå‘åˆ†æ•°ä½¿ç”¨æŒ‡æ•°æƒ©ç½š
                if negative_score < 0.5:
                    # è½»å¾®è´Ÿå‘åˆ†æ•°ä½¿ç”¨çº¿æ€§æƒ©ç½š
                    penalty_factor = 1 - negative_score * 0.3
                else:
                    # è¾ƒå¤§è´Ÿå‘åˆ†æ•°ä½¿ç”¨æŒ‡æ•°æƒ©ç½š
                    penalty_factor = math.exp(-negative_score * 0.8)
                
                score *= penalty_factor
                penalty = original_score - score
                matches.append(f"è´Ÿå‘æƒ©ç½šæ€»è®¡: -{penalty:.2f} (å› å­: {penalty_factor:.2f}, ä¸Šä¸‹æ–‡è°ƒæ•´: {context_adjustment:.1f})")
        
        # 3. åº”ç”¨ç±»åˆ«ä¼˜å…ˆçº§ç¼©æ”¾
        priority = config.get("priority", 0)
        if priority > 0:
            priority_bonus = score * (priority * 0.12)  # ä¼˜å…ˆçº§åŠ æˆæ›´æ˜æ˜¾
            score += priority_bonus
            matches.append(f"ä¼˜å…ˆçº§åŠ æˆ (çº§åˆ« {priority}): +{priority_bonus:.2f}")
        
        # è®°å½•å¾—åˆ†å’ŒåŒ¹é…è¯¦æƒ…
        if score > 0:
            scores[category] = score
            match_details[category] = matches
    
    # 4. åˆ†ç±»å†³ç­–é€»è¾‘
    # éªŒè¯æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
    max_score = max(scores.values()) if scores else 0
    if max_score < 0.05:
        return []
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å¼ºä¿¡å·å…³é”®è¯ï¼Œç›´æ¥åˆ†ç±»æŸäº›æ˜ç¡®çš„è®ºæ–‡
    combined_text = (title + " " + abstract).lower()
    
    # å¼ºä¿¡å·å…³é”®è¯æ£€æµ‹ - ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·æŒ‡å®šçš„åŸºç¡€ç ”ç©¶ç±»åˆ«
    strong_signals = {
        "ç”Ÿæˆå¼è§†è§‰ä¸å†…å®¹åˆ›å»º (Generative Vision & Content Creation)": [
            "diffusion", "generative", "gan", "vae", "synthesis", "text-to-image", "image generation", 
            "stable diffusion", "latent diffusion", "text to image", "image to image", "style transfer",
            "æ‰©æ•£æ¨¡å‹", "ç”Ÿæˆæ¨¡å‹", "å›¾åƒç”Ÿæˆ", "æ–‡æœ¬åˆ°å›¾åƒ", "å›¾åƒåˆ°å›¾åƒ", "é£æ ¼è¿ç§»"
        ],
        "ä¸‰ç»´é‡å»ºä¸å‡ ä½•æ„ŸçŸ¥ (3D Reconstruction & Geometric Perception)": [
            "3d reconstruction", "point cloud", "depth estimation", "nerf", "3d vision", "geometry", "3d shape", 
            "novel view synthesis", "multi-view", "stereo matching", "structure from motion", "sfm", "slam",
            "ä¸‰ç»´é‡å»º", "ç‚¹äº‘", "æ·±åº¦ä¼°è®¡", "ä¸‰ç»´è§†è§‰", "å‡ ä½•", "æ–°è§†è§’åˆæˆ", "å¤šè§†è§’", "ç«‹ä½“åŒ¹é…", "è¿åŠ¨ç»“æ„"
        ],
        "åŠ¨æ€è§†è§‰ä¸æ—¶åºå»ºæ¨¡ (Dynamic Vision & Temporal Modeling)": [
            "video", "temporal", "motion", "tracking", "optical flow", "action recognition", "trajectory", 
            "sequence", "dynamic", "temporal consistency", "video generation", "video prediction",
            "è§†é¢‘", "æ—¶åº", "è¿åŠ¨", "è·Ÿè¸ª", "å…‰æµ", "åŠ¨ä½œè¯†åˆ«", "è½¨è¿¹", "åºåˆ—", "åŠ¨æ€", "æ—¶é—´ä¸€è‡´æ€§"
        ],
        "å¤šæ¨¡æ€è§†è§‰ä¸è·¨æ¨¡æ€å­¦ä¹  (Multimodal Vision & Cross-modal Learning)": [
            "multimodal", "cross-modal", "vision-language", "text-vision", "audio-visual", "multi-modal", 
            "vision and language", "clip", "align", "å¤šæ¨¡æ€", "è·¨æ¨¡æ€", "è§†è§‰è¯­è¨€", "æ–‡æœ¬è§†è§‰", "éŸ³è§†é¢‘", "è§†è§‰ä¸è¯­è¨€"
        ],
        "æ¨¡å‹ä¼˜åŒ–ä¸ç³»ç»Ÿé²æ£’æ€§ (Model Optimization & System Robustness)": [
            "adversarial", "robustness", "optimization", "quantization", "pruning", "compression", "distillation", 
            "knowledge distillation", "model compression", "efficient inference", "low-rank", "sparsity",
            "å¯¹æŠ—", "é²æ£’æ€§", "ä¼˜åŒ–", "é‡åŒ–", "å‰ªæ", "å‹ç¼©", "è’¸é¦", "çŸ¥è¯†è’¸é¦", "æ¨¡å‹å‹ç¼©", "é«˜æ•ˆæ¨ç†", "ä½ç§©", "ç¨€ç–æ€§"
        ],
        "æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½ (Efficient Learning & Adaptive Intelligence)": [
            "few-shot", "zero-shot", "meta-learning", "transfer learning", "domain adaptation", "continual learning", 
            "lifelong learning", "incremental learning", "curriculum learning", "active learning", "self-supervised",
            "å°‘æ ·æœ¬", "é›¶æ ·æœ¬", "å…ƒå­¦ä¹ ", "è¿ç§»å­¦ä¹ ", "åŸŸé€‚åº”", "æŒç»­å­¦ä¹ ", "ç»ˆèº«å­¦ä¹ ", "å¢é‡å­¦ä¹ ", "è¯¾ç¨‹å­¦ä¹ ", "ä¸»åŠ¨å­¦ä¹ ", "è‡ªç›‘ç£"
        ],
        "é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ (Static Image Understanding & Semantic Analysis)": [
            "object detection", "semantic segmentation", "instance segmentation", "scene understanding",
            "visual reasoning", "object recognition", "image classification", "scene parsing", "panoptic segmentation",
            "ç›®æ ‡æ£€æµ‹", "è¯­ä¹‰åˆ†å‰²", "å®ä¾‹åˆ†å‰²", "åœºæ™¯ç†è§£", "è§†è§‰æ¨ç†", "ç›®æ ‡è¯†åˆ«", "å›¾åƒåˆ†ç±»", "å…¨æ™¯åˆ†å‰²"
        ]
    }
    
    # æ£€æŸ¥å¼ºä¿¡å·å…³é”®è¯
    for category, keywords in strong_signals.items():
        matches = sum(1 for kw in keywords if kw in combined_text)
        if matches >= 2 and category in scores and scores[category] >= 0.05:
            subcategory = get_subcategory(title, abstract, category, scores[category])
            return [(category, scores[category], subcategory)]
    
    # ç¬¬äºŒæ­¥ï¼šæ”¶é›†æ‰€æœ‰æ½œåœ¨çš„å€™é€‰ç±»åˆ«
    all_candidates = []
    
    # å®šä¹‰æ‰€æœ‰ç±»åˆ«åŠå…¶åŸºç¡€é˜ˆå€¼ - æŒ‰ç…§ç”¨æˆ·æŒ‡å®šçš„ä¼˜å…ˆçº§æ’åº
    all_categories = {
        # åŸºç¡€ç ”ç©¶ç±»åˆ« - ä¼˜å…ˆçº§æœ€é«˜çš„ç±»åˆ«
        "ç”Ÿæˆå¼è§†è§‰ä¸å†…å®¹åˆ›å»º (Generative Vision & Content Creation)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "ä¸‰ç»´é‡å»ºä¸å‡ ä½•æ„ŸçŸ¥ (3D Reconstruction & Geometric Perception)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "åŠ¨æ€è§†è§‰ä¸æ—¶åºå»ºæ¨¡ (Dynamic Vision & Temporal Modeling)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "å¤šæ¨¡æ€è§†è§‰ä¸è·¨æ¨¡æ€å­¦ä¹  (Multimodal Vision & Cross-modal Learning)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "æ¨¡å‹ä¼˜åŒ–ä¸ç³»ç»Ÿé²æ£’æ€§ (Model Optimization & System Robustness)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½ (Efficient Learning & Adaptive Intelligence)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        "é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ (Static Image Understanding & Semantic Analysis)": 0.06,  # é™ä½é˜ˆå€¼ï¼Œé¼“åŠ±åˆ†ç±»
        
        # å…¶ä»–åŸºç¡€ç ”ç©¶ç±»åˆ«
        "è§†è§‰è¡¨å¾å­¦ä¹ ä¸å¤§æ¨¡å‹ (Visual Representation Learning & Foundation Models)": 0.25,  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘è¿‡åº¦åˆ†ç±»
        "æ„ŸçŸ¥-åŠ¨ä½œæ™ºèƒ½ä¸ä¸»åŠ¨è§†è§‰ (Perception-Action Intelligence & Active Vision)": 0.10,
        "å‰æ²¿è§†è§‰ç†è®ºä¸è·¨å­¦ç§‘èåˆ (Advanced Vision Theory & Interdisciplinary Integration)": 0.10,
        
        # åº”ç”¨ç±»åˆ« - éœ€è¦æ›´å¼ºçš„ä¿¡å·
        "ç”Ÿç‰©åŒ»å­¦å½±åƒè®¡ç®— (Biomedical Image Computing)": 3.0,  # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼
        "æ™ºèƒ½äº¤é€šä¸è‡ªä¸»ç³»ç»Ÿ (Intelligent Transportation & Autonomous Systems)": 3.0,  # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼
        "å·¥ä¸šè§†è§‰ä¸é¥æ„Ÿæ„ŸçŸ¥ (Industrial Vision & Remote Sensing)": 3.0,  # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼
        "äº¤äº’åª’ä½“ä¸è™šæ‹Ÿç°å®æŠ€æœ¯ (Interactive Media & Extended Reality)": 3.0   # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼
    }
    
    # æ”¶é›†æ‰€æœ‰ç¬¦åˆé˜ˆå€¼çš„å€™é€‰ç±»åˆ«
    for category, threshold in all_categories.items():
        if category in scores and scores[category] >= threshold:
            subcategory = get_subcategory(title, abstract, category, scores[category])
            all_candidates.append((category, scores[category], subcategory))
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•å€™é€‰ç±»åˆ«ï¼Œè¿”å›ç©º
    if not all_candidates:
        return []
    
    # ç¬¬äºŒæ­¥ï¼šå¯¹å€™é€‰ç±»åˆ«è¿›è¡Œç­›é€‰å’Œä¼˜åŒ–    
    # å¦‚æœåªæœ‰ä¸€ä¸ªå€™é€‰ç±»åˆ«ï¼Œç›´æ¥è¿”å›
    if len(all_candidates) == 1:
        # print(f"âœ… å”¯ä¸€å€™é€‰ï¼Œç›´æ¥åˆ†ç±»: {all_candidates[0][0]}")
        return all_candidates
    
    # ç¬¬ä¸‰æ­¥ï¼šå¤šå€™é€‰ç±»åˆ«çš„æ™ºèƒ½ç­›é€‰
    # 3.1 ä¼˜å…ˆé€‰æ‹©åŸºç¡€ç ”ç©¶ç±»åˆ«ï¼ˆé™¤éåº”ç”¨ç±»åˆ«å¾—åˆ†æ˜æ˜¾æ›´é«˜ï¼‰
    basic_research_candidates = [
        cat for cat in all_candidates 
        if not any(app in cat[0] for app in ["ç”Ÿç‰©åŒ»å­¦", "æ™ºèƒ½äº¤é€š", "å·¥ä¸šè§†è§‰", "äº¤äº’åª’ä½“"])
    ]
    
    application_candidates = [
        cat for cat in all_candidates 
        if any(app in cat[0] for app in ["ç”Ÿç‰©åŒ»å­¦", "æ™ºèƒ½äº¤é€š", "å·¥ä¸šè§†è§‰", "äº¤äº’åª’ä½“"])
    ]
    
    # 3.2 æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦æ˜ç¡®æåˆ°åº”ç”¨åœºæ™¯
    title_lower = title.lower()
    has_application_in_title = any(term in title_lower for term in [
        "medical", "healthcare", "clinical", "autonomous", "driving", "vehicle", 
        "industrial", "inspection", "remote sensing", "satellite", "aerial", "drone",
        "ar", "vr", "augmented reality", "virtual reality", "mixed reality", "xr",
        "åŒ»å­¦", "åŒ»ç–—", "è‡ªä¸»", "é©¾é©¶", "è½¦è¾†", "å·¥ä¸š", "æ£€æµ‹", "é¥æ„Ÿ", "å«æ˜Ÿ", "èˆªç©º", "æ— äººæœº",
        "å¢å¼ºç°å®", "è™šæ‹Ÿç°å®", "æ··åˆç°å®"
    ])
    
    # 3.3 æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ä¼˜å…ˆçº§ç±»åˆ«è¿›è¡Œç­›é€‰
    priority_categories = [
        "ç”Ÿæˆå¼è§†è§‰ä¸å†…å®¹åˆ›å»º", "ä¸‰ç»´é‡å»ºä¸å‡ ä½•æ„ŸçŸ¥", "åŠ¨æ€è§†è§‰ä¸æ—¶åºå»ºæ¨¡",
        "å¤šæ¨¡æ€è§†è§‰ä¸è·¨æ¨¡æ€å­¦ä¹ ", "æ¨¡å‹ä¼˜åŒ–ä¸ç³»ç»Ÿé²æ£’æ€§", "æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½",
        "é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ"
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜å…ˆçº§ç±»åˆ«
    priority_candidates = [cat for cat in basic_research_candidates if any(pc in cat[0] for pc in priority_categories)]
    
    # 3.4 å†³ç­–é€»è¾‘
    if priority_candidates:
        # å¦‚æœæœ‰ä¼˜å…ˆçº§ç±»åˆ«ï¼Œé€‰æ‹©è¿™äº›ç±»åˆ«
        candidates_to_consider = priority_candidates
        # print(f"ğŸ” é€‰æ‹©ä¼˜å…ˆçº§åŸºç¡€ç ”ç©¶ç±»åˆ«")
    elif basic_research_candidates and application_candidates:
        max_basic_score = max(basic_research_candidates, key=lambda x: x[1])[1]
        max_app_score = max(application_candidates, key=lambda x: x[1])[1]
        
        # å¦‚æœæ ‡é¢˜ä¸­æ˜ç¡®æåˆ°åº”ç”¨åœºæ™¯ä¸”åº”ç”¨ç±»åˆ«å¾—åˆ†è¶³å¤Ÿé«˜
        if has_application_in_title and max_app_score > max_basic_score * 1.5:
            candidates_to_consider = application_candidates
            # print(f"ğŸ­ æ ‡é¢˜ä¸­æœ‰åº”ç”¨åœºæ™¯ä¸”å¾—åˆ†è¾ƒé«˜ ({max_app_score:.3f} vs {max_basic_score:.3f})")
        # å¦‚æœåº”ç”¨ç±»åˆ«å¾—åˆ†å‹å€’æ€§ä¼˜åŠ¿
        elif max_app_score > max_basic_score * 3.0:
            candidates_to_consider = application_candidates
            # print(f"ğŸ­ åº”ç”¨ç±»åˆ«å¾—åˆ†å‹å€’æ€§ä¼˜åŠ¿ ({max_app_score:.3f} vs {max_basic_score:.3f})")
        else:
            candidates_to_consider = basic_research_candidates
            # print(f"ğŸ”¬ ä¼˜å…ˆåŸºç¡€ç ”ç©¶ç±»åˆ« ({max_basic_score:.3f} vs {max_app_score:.3f})")
    elif basic_research_candidates:
        candidates_to_consider = basic_research_candidates
        # print(f"ğŸ”¬ ä»…æœ‰åŸºç¡€ç ”ç©¶å€™é€‰")
    else:
        candidates_to_consider = application_candidates
        # print(f"ğŸ­ ä»…æœ‰åº”ç”¨ç±»åˆ«å€™é€‰")
    
    # 3.3 åœ¨é€‰å®šçš„å€™é€‰ç±»åˆ«ä¸­è¿›è¡Œæœ€ç»ˆé€‰æ‹©
    if len(candidates_to_consider) == 1:
        # print(f"âœ… ç­›é€‰åå”¯ä¸€å€™é€‰: {candidates_to_consider[0][0]}")
        return candidates_to_consider
    
    # 3.4 å¤šä¸ªå€™é€‰ç±»åˆ«æ—¶ï¼Œä½¿ç”¨æ›´æ™ºèƒ½çš„å†³ç­–é€»è¾‘
    try:
        # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„å…³é”®è¯ï¼Œæ›´ç²¾ç¡®åœ°åˆ¤æ–­è®ºæ–‡çš„çœŸæ­£ç±»åˆ«
        combined_text = (title + " " + abstract).lower()
        
        # ç”¨æˆ·æŒ‡å®šçš„ä¼˜å…ˆçº§ç±»åˆ«å…³é”®è¯
        priority_keywords = {
            "ç”Ÿæˆå¼è§†è§‰ä¸å†…å®¹åˆ›å»º": ["diffusion", "generative", "gan", "vae", "synthesis", "text-to-image", "image generation"],
            "ä¸‰ç»´é‡å»ºä¸å‡ ä½•æ„ŸçŸ¥": ["3d", "point cloud", "depth", "nerf", "geometry", "stereo", "reconstruction"],
            "åŠ¨æ€è§†è§‰ä¸æ—¶åºå»ºæ¨¡": ["video", "temporal", "motion", "tracking", "optical flow", "action"],
            "å¤šæ¨¡æ€è§†è§‰ä¸è·¨æ¨¡æ€å­¦ä¹ ": ["multimodal", "cross-modal", "vision-language", "text-vision", "audio-visual"],
            "æ¨¡å‹ä¼˜åŒ–ä¸ç³»ç»Ÿé²æ£’æ€§": ["adversarial", "robustness", "optimization", "quantization", "pruning", "compression"],
            "æ•ˆç‡å­¦ä¹ ä¸é€‚åº”æ€§æ™ºèƒ½": ["few-shot", "zero-shot", "meta-learning", "transfer", "domain adaptation", "continual"],
            "é™æ€å›¾åƒç†è§£ä¸è¯­ä¹‰è§£æ": ["detection", "segmentation", "recognition", "classification", "scene understanding"]
        }
        
        # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å…³é”®è¯åŒ¹é…
        for cat in candidates_to_consider:
            category_name = cat[0].split(" (")[0]  # å»é™¤è‹±æ–‡éƒ¨åˆ†
            if category_name in priority_keywords:
                keywords = priority_keywords[category_name]
                matches = sum(1 for kw in keywords if kw in combined_text)
                if matches >= 3:  # å¦‚æœæ ‡é¢˜å’Œæ‘˜è¦ä¸­æœ‰å¤šä¸ªå…³é”®è¯åŒ¹é…ï¼Œç›´æ¥é€‰æ‹©è¯¥ç±»åˆ«
                    return [cat]
        
        # ä½¿ç”¨ChatGLMåšå†³ç­–
        final_category = chatglm.decide_category(title, abstract, candidates_to_consider)
        
        if final_category:
            # æ‰¾åˆ°å¯¹åº”çš„å®Œæ•´ä¿¡æ¯
            for cat, score, subcat in candidates_to_consider:
                if cat == final_category:
                    # å¦‚æœæ ‡é¢˜ä¸­æ˜ç¡®æåˆ°äº†è¿™ä¸ªç±»åˆ«çš„å…³é”®è¯ï¼Œå¢å¼ºè¿™ä¸ªå†³ç­–
                    category_name = cat.split(" (")[0]  # å»é™¤è‹±æ–‡éƒ¨åˆ†
                    if category_name in priority_keywords:
                        keywords = priority_keywords[category_name]
                        if any(kw in title.lower() for kw in keywords):
                            return [(cat, score, subcat)]
                    
                    # å¦‚æœChatGLMé€‰æ‹©äº†è§†è§‰è¡¨å¾å­¦ä¹ ç±»åˆ«ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ›´é€‚åˆçš„ä¼˜å…ˆçº§ç±»åˆ«
                    if "è§†è§‰è¡¨å¾å­¦ä¹ ä¸å¤§æ¨¡å‹" in cat:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜å…ˆçº§ç±»åˆ«çš„å€™é€‰
                        for priority_cat in candidates_to_consider:
                            priority_name = priority_cat[0].split(" (")[0]
                            if priority_name in priority_keywords and priority_name != "è§†è§‰è¡¨å¾å­¦ä¹ ä¸å¤§æ¨¡å‹":
                                # å¦‚æœæœ‰ä¼˜å…ˆçº§ç±»åˆ«ä¸”å¾—åˆ†ä¸ä½äºè§†è§‰è¡¨å¾å­¦ä¹ çš„70%
                                if priority_cat[1] >= cat[1] * 0.7:
                                    return [priority_cat]
                    
                    return [(cat, score, subcat)]
        
        # print(f"âš ï¸ ChatGLMå†³ç­–å¤±è´¥ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰")
    except Exception as e:
        print(f"âš ï¸ ChatGLMè°ƒç”¨å¤±è´¥: {e}ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰")
    
    # 3.5 å¦‚æœChatGLMå¤±è´¥ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰
    best_candidate = max(candidates_to_consider, key=lambda x: x[1])
    # print(f"ğŸ“Š æœ€é«˜å¾—åˆ†å€™é€‰: {best_candidate[0]} ({best_candidate[1]:.3f})")
    return [best_candidate]


def calculate_category_relation(category1, category2, categories_config):
    """
    è®¡ç®—ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§
    
    Args:
        category1: ç¬¬ä¸€ä¸ªç±»åˆ«åç§°
        category2: ç¬¬äºŒä¸ªç±»åˆ«åç§°
        categories_config: ç±»åˆ«é…ç½®å­—å…¸
        
    Returns:
        float: ç›¸å…³æ€§åˆ†æ•° (0-1)ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç›¸å…³
    """
    # å¦‚æœç±»åˆ«ç›¸åŒï¼Œç›¸å…³æ€§ä¸º1
    if category1 == category2:
        return 1.0
    
    # è·å–ä¸¤ä¸ªç±»åˆ«çš„å…³é”®è¯
    keywords1 = set()
    keywords2 = set()
    
    if category1 in categories_config and "keywords" in categories_config[category1]:
        keywords1 = {kw[0].lower() for kw in categories_config[category1]["keywords"] if isinstance(kw, tuple)}
    
    if category2 in categories_config and "keywords" in categories_config[category2]:
        keywords2 = {kw[0].lower() for kw in categories_config[category2]["keywords"] if isinstance(kw, tuple)}
    
    # å¦‚æœä»»ä¸€ç±»åˆ«æ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›0
    if not keywords1 or not keywords2:
        return 0.0
    
    # è®¡ç®—å…³é”®è¯é‡å 
    overlap = keywords1.intersection(keywords2)
    
    # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—ç›¸å…³æ€§
    similarity = len(overlap) / len(keywords1.union(keywords2))
    
    # ä½¿ç”¨æœ€æ–°çš„ç±»åˆ«å®šä¹‰ç”Ÿæˆç›¸å…³ç±»åˆ«å¯¹
    related_pairs = get_related_category_pairs()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    for pair in related_pairs:
        if (category1.startswith(pair[0]) and category2.startswith(pair[1])) or \
           (category1.startswith(pair[1]) and category2.startswith(pair[0])):
            # å¢åŠ ç›¸å…³æ€§åˆ†æ•°
            similarity += 0.2
            break
    
    return min(similarity, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated
        
        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None
            
        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)
        
        # åˆå§‹åŒ–é»˜è®¤å€¼ï¼Œé¿å…å¼‚å¸¸æ—¶æœªå®šä¹‰
        github_link = "None"
        category = "å…¶ä»– (Others)"  # ä¿®æ”¹é»˜è®¤å€¼ä¸ºå¸¦è‹±æ–‡çš„æ ¼å¼
        subcategory = "æœªæŒ‡å®š"
        title_cn = f"[ç¿»è¯‘å¤±è´¥] {title}"
        analysis = {}

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        try:
            with ThreadPoolExecutor(max_workers=2) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                github_future = executor.submit(extract_github_link, abstract)
                analysis_future = executor.submit(
                    glm_helper.analyze_paper_contribution, title, abstract)
                title_cn_future = executor.submit(
                    glm_helper.translate_title, title)

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                github_link = github_future.result() or "None"
                analysis = analysis_future.result() or {}
                title_cn = title_cn_future.result() or f"[ç¿»è¯‘å¤±è´¥] {title}"
        except Exception as e:
            logger.error(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            # ç»§ç»­å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        
        # ä½¿ç”¨åŸºäºå…³é”®è¯çš„åˆ†ç±»æ–¹æ³•
        try:
            # ä½¿ç”¨æ–°çš„åˆ†ç±»å‡½æ•°
            category_results = get_category_by_keywords(title, abstract, CATEGORY_KEYWORDS)
            
            if category_results:
                # è·å–ä¸»ç±»åˆ«å’Œå¾—åˆ†
                result_item = category_results[0]
                
                # å…¼å®¹å¤šç§è¿”å›æ ¼å¼ï¼š(category, score) æˆ– (category, score, subcategory) æˆ– (category, score, subcategory, explanation)
                if len(result_item) >= 4:  # æ–°æ ¼å¼ï¼ŒåŒ…å«è§£é‡Š
                    main_category, main_score, sub_category_tuple, explanation = result_item
                elif len(result_item) == 3:  # æ—§æ ¼å¼ï¼ŒåŒ…å«å­ç±»åˆ«
                    main_category, main_score, sub_category_tuple = result_item
                    explanation = None
                else:  # æœ€ç®€å•çš„æ ¼å¼
                    main_category, main_score = result_item
                    sub_category_tuple = None
                    explanation = None
                    
                category = main_category
                
                # å¤„ç†å­ç±»åˆ«
                if sub_category_tuple:
                    subcategory_name, subcategory_score = sub_category_tuple
                    subcategory = subcategory_name
                else:
                    subcategory = "æœªæŒ‡å®š"
                    
                # ä¸è¾“å‡ºåˆ†ç±»ç»“æœä¿¡æ¯ï¼Œå‡å°‘æ—¥å¿—å¹²æ‰°
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
                category = "å…¶ä»– (Others)"
                subcategory = "æœªæŒ‡å®š"
        except Exception as e:
            logger.error(f"åˆ†ç±»è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()
            category = "å…¶ä»– (Others)"
            subcategory = "æœªæŒ‡å®š"

        paper_info = {
            'title': title,
            'title_zh': title_cn,  # ä¿®æ”¹é”®åä¸º title_zh ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'abstract': abstract,
            'authors': authors_str,
            'pdf_url': pdf_url,
            'github_url': github_link,  # ä¿®æ”¹é”®åä¸º github_url ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'url': paper_url,  # æ·»åŠ  arxiv URL
            'category': category,
            'subcategory': subcategory,  # æ·»åŠ å­ç±»åˆ«ä¿¡æ¯
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

        # åˆå¹¶åˆ†æç»“æœ
        if analysis:
            paper_info.update(analysis)

        return paper_info

    except Exception as e:
        logger.error(f"\u5904\u7406\u8bba\u6587\u65f6\u51fa\u9519: {str(e)}")
        return None


def get_subcategory(title: str, abstract: str, main_category: str, main_score: float) -> Optional[Tuple[str, float]]:
    """
    åœ¨ç¡®å®šä¸»ç±»åˆ«åï¼Œè¿›ä¸€æ­¥ç¡®å®šå­ç±»åˆ«
    
    Args:
        title: è®ºæ–‡æ ‡é¢˜
        abstract: è®ºæ–‡æ‘˜è¦
        main_category: ä¸»ç±»åˆ«
        main_score: ä¸»ç±»åˆ«å¾—åˆ†
        
    Returns:
        Optional[Tuple[str, float]]: å­ç±»åˆ«åŠå…¶å¾—åˆ†ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›None
    """
    # å¢å¼ºçš„æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    combined_text = title_lower + " " + abstract_lower
    
    # ä½¿ç”¨NLTKè¿›è¡Œå¤šçº§æ–‡æœ¬é¢„å¤„ç†
    processed_title = preprocess_text(title)
    processed_abstract = preprocess_text(abstract)
    processed_combined = processed_title + " " + processed_abstract
    
    # åˆ›å»ºN-gramç‰ˆæœ¬çš„æ–‡æœ¬ç”¨äºçŸ­è¯­åŒ¹é…
    # è¿™æœ‰åŠ©äºæ•è·å¤šè¯çŸ­è¯­ï¼Œå³ä½¿å®ƒä»¬çš„é¡ºåºæˆ–å½¢å¼ç•¥æœ‰ä¸åŒ
    from nltk.util import ngrams
    import re
    
    # æ¸…ç†å¹¶æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºN-gramå¤„ç†
    clean_title = re.sub(r'[^\w\s]', ' ', title_lower)
    clean_abstract = re.sub(r'[^\w\s]', ' ', abstract_lower)
    
    # ç”Ÿæˆ2-gramå’Œ3-gram
    title_words = clean_title.split()
    abstract_words = clean_abstract.split()
    
    # ç”Ÿæˆ2-gram
    title_bigrams = [' '.join(ng) for ng in ngrams(title_words, 2)] if len(title_words) >= 2 else []
    abstract_bigrams = [' '.join(ng) for ng in ngrams(abstract_words, 2)] if len(abstract_words) >= 2 else []
    
    # ç”Ÿæˆ3-gram
    title_trigrams = [' '.join(ng) for ng in ngrams(title_words, 3)] if len(title_words) >= 3 else []
    abstract_trigrams = [' '.join(ng) for ng in ngrams(abstract_words, 3)] if len(abstract_words) >= 3 else []
    
    # åˆå¹¶æ‰€æœ‰N-gram
    all_ngrams = set(title_bigrams + abstract_bigrams + title_trigrams + abstract_trigrams)
    
    # æ£€æŸ¥ä¸»ç±»åˆ«æ˜¯å¦æœ‰å­ç±»åˆ«å®šä¹‰
    if main_category in CATEGORY_THRESHOLDS and "subcategories" in CATEGORY_THRESHOLDS[main_category]:
        subcategories = CATEGORY_THRESHOLDS[main_category]["subcategories"]
        
        # è®¡ç®—æ¯ä¸ªå­ç±»åˆ«çš„å¾—åˆ†
        subcategory_scores = {}
        for subcategory_name, subcategory_threshold in subcategories.items():
            # æå–å­ç±»åˆ«åç§°ä¸­çš„å…³é”®è¯
            subcategory_keywords = subcategory_name.lower().split()
            score = 0.0
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if subcategory_name.lower() in combined_text:
                score += 3.5  # å¤§å¹…å¢åŠ ç²¾ç¡®åŒ¹é…çš„æƒé‡ï¼Œä»2.5æé«˜åˆ°3.5
            elif subcategory_name.lower() in title_lower:
                score += 4.0  # å¦‚æœå­ç±»åˆ«åç§°ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç»™äºˆæ›´é«˜æƒé‡
            
            # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒåŒ¹é…
            processed_subcategory = preprocess_text(subcategory_name)
            if processed_subcategory in processed_combined:
                score += 2.5  # å¢åŠ é¢„å¤„ç†æ–‡æœ¬åŒ¹é…çš„æƒé‡ï¼Œä»1.8æé«˜åˆ°2.5
            elif processed_subcategory in processed_title:
                score += 3.0  # å¦‚æœé¢„å¤„ç†åçš„å­ç±»åˆ«åç§°å‡ºç°åœ¨æ ‡é¢˜ä¸­
            
            # å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
            for keyword in subcategory_keywords:
                if len(keyword) > 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                    # åŸå§‹æ–‡æœ¬åŒ¹é…
                    if keyword in title_lower:
                        score += 1.5  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…
                    elif keyword in abstract_lower:
                        score += 0.8  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…
                    
                    # é¢„å¤„ç†æ–‡æœ¬åŒ¹é…
                    processed_keyword = preprocess_text(keyword)
                    if processed_keyword in processed_title:
                        score += 1.2  # é¢„å¤„ç†åçš„æ ‡é¢˜åŒ¹é…
                    elif processed_keyword in processed_abstract:
                        score += 0.6  # é¢„å¤„ç†åçš„æ‘˜è¦åŒ¹é…
                    
                    # N-gramåŒ¹é…ï¼ˆæ•è·çŸ­è¯­å˜ä½“ï¼‰
                    for ngram in all_ngrams:
                        if keyword in ngram:
                            score += 0.4  # N-gramä¸­çš„å…³é”®è¯åŒ¹é…
                            break
                    
                    # è¯æ ¹åŒ¹é…ï¼ˆå¤„ç†è¯å½¢å˜åŒ–ï¼‰
                    keyword_root = preprocess_text(keyword)
                    for word in processed_title.split():
                        if keyword_root in word and len(keyword_root) > 4:  # ç¡®ä¿è¶³å¤Ÿé•¿ä»¥é¿å…è¯¯åŒ¹é…
                            score += 0.3
                            break
                    for word in processed_abstract.split():
                        if keyword_root in word and len(keyword_root) > 4:
                            score += 0.2
                            break
            
            # å¤§å¹…é™ä½å­ç±»åˆ«é˜ˆå€¼ï¼Œç¡®ä¿å¤§å¤šæ•°è®ºæ–‡èƒ½è¢«åˆ†é…åˆ°å­ç±»åˆ«
            if score > 0:
                # å­ç±»åˆ«å¾—åˆ†éœ€è¦è¾¾åˆ°ä¸»ç±»åˆ«å¾—åˆ†çš„ä¸€å®šæ¯”ä¾‹ï¼Œä½†å¤§å¹…é™ä½è¦æ±‚
                relative_threshold = main_score * 0.15 * subcategory_threshold  # ä»0.25é™ä½åˆ°0.15
                if score >= relative_threshold or score >= 0.5:  # æ·»åŠ ç»å¯¹åˆ†æ•°é˜ˆå€¼
                    subcategory_scores[subcategory_name] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„å­ç±»åˆ«
        if subcategory_scores:
            best_subcategory = max(subcategory_scores.items(), key=lambda x: x[1])
            return best_subcategory
    
    return None


def calculate_category_relation(category1, category2, categories_config):
    """
    è®¡ç®—ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§
    
    Args:
        category1: ç¬¬ä¸€ä¸ªç±»åˆ«åç§°
        category2: ç¬¬äºŒä¸ªç±»åˆ«åç§°
        categories_config: ç±»åˆ«é…ç½®å­—å…¸
        
    Returns:
        float: ç›¸å…³æ€§åˆ†æ•° (0-1)ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç›¸å…³
    """
    # å¦‚æœç±»åˆ«ç›¸åŒï¼Œç›¸å…³æ€§ä¸º1
    if category1 == category2:
        return 1.0
    
    # è·å–ä¸¤ä¸ªç±»åˆ«çš„å…³é”®è¯
    keywords1 = set()
    keywords2 = set()
    
    if category1 in categories_config and "keywords" in categories_config[category1]:
        keywords1 = {kw[0].lower() for kw in categories_config[category1]["keywords"] if isinstance(kw, tuple)}
    
    if category2 in categories_config and "keywords" in categories_config[category2]:
        keywords2 = {kw[0].lower() for kw in categories_config[category2]["keywords"] if isinstance(kw, tuple)}
    
    # å¦‚æœä»»ä¸€ç±»åˆ«æ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›0
    if not keywords1 or not keywords2:
        return 0.0
    
    # è®¡ç®—å…³é”®è¯é‡å 
    overlap = keywords1.intersection(keywords2)
    
    # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—ç›¸å…³æ€§
    similarity = len(overlap) / len(keywords1.union(keywords2))
    
    # ä½¿ç”¨æœ€æ–°çš„ç±»åˆ«å®šä¹‰ç”Ÿæˆç›¸å…³ç±»åˆ«å¯¹
    related_pairs = get_related_category_pairs()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    for pair in related_pairs:
        if (category1.startswith(pair[0]) and category2.startswith(pair[1])) or \
           (category1.startswith(pair[1]) and category2.startswith(pair[0])):
            # å¢åŠ ç›¸å…³æ€§åˆ†æ•°
            similarity += 0.2
            break
    
    return min(similarity, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated
        
        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None
            
        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)
        
        # åˆå§‹åŒ–é»˜è®¤å€¼ï¼Œé¿å…å¼‚å¸¸æ—¶æœªå®šä¹‰
        github_link = "None"
        category = "å…¶ä»– (Others)"  # ä¿®æ”¹é»˜è®¤å€¼ä¸ºå¸¦è‹±æ–‡çš„æ ¼å¼
        subcategory = "æœªæŒ‡å®š"
        title_cn = f"[ç¿»è¯‘å¤±è´¥] {title}"
        analysis = {}

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        try:
            with ThreadPoolExecutor(max_workers=2) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                github_future = executor.submit(extract_github_link, abstract)
                analysis_future = executor.submit(
                    glm_helper.analyze_paper_contribution, title, abstract)
                title_cn_future = executor.submit(
                    glm_helper.translate_title, title)

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                github_link = github_future.result() or "None"
                analysis = analysis_future.result() or {}
                title_cn = title_cn_future.result() or f"[ç¿»è¯‘å¤±è´¥] {title}"
        except Exception as e:
            logger.error(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            # ç»§ç»­å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        
        # ä½¿ç”¨åŸºäºå…³é”®è¯çš„åˆ†ç±»æ–¹æ³•
        try:
            # ä½¿ç”¨æ–°çš„åˆ†ç±»å‡½æ•°
            category_results = get_category_by_keywords(title, abstract, CATEGORY_KEYWORDS)
            
            if category_results:
                # è·å–ä¸»ç±»åˆ«å’Œå¾—åˆ†
                result_item = category_results[0]
                
                # å…¼å®¹å¤šç§è¿”å›æ ¼å¼ï¼š(category, score) æˆ– (category, score, subcategory) æˆ– (category, score, subcategory, explanation)
                if len(result_item) >= 4:  # æ–°æ ¼å¼ï¼ŒåŒ…å«è§£é‡Š
                    main_category, main_score, sub_category_tuple, explanation = result_item
                elif len(result_item) == 3:  # æ—§æ ¼å¼ï¼ŒåŒ…å«å­ç±»åˆ«
                    main_category, main_score, sub_category_tuple = result_item
                    explanation = None
                else:  # æœ€ç®€å•çš„æ ¼å¼
                    main_category, main_score = result_item
                    sub_category_tuple = None
                    explanation = None
                    
                category = main_category
                
                # å¤„ç†å­ç±»åˆ«
                if sub_category_tuple:
                    subcategory_name, subcategory_score = sub_category_tuple
                    subcategory = subcategory_name
                else:
                    subcategory = "æœªæŒ‡å®š"
                    
                # ä¸è¾“å‡ºåˆ†ç±»ç»“æœä¿¡æ¯ï¼Œå‡å°‘æ—¥å¿—å¹²æ‰°
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
                category = "å…¶ä»– (Others)"
                subcategory = "æœªæŒ‡å®š"
        except Exception as e:
            logger.error(f"åˆ†ç±»è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()
            category = "å…¶ä»– (Others)"
            subcategory = "æœªæŒ‡å®š"

        paper_info = {
            'title': title,
            'title_zh': title_cn,  # ä¿®æ”¹é”®åä¸º title_zh ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'abstract': abstract,
            'authors': authors_str,
            'pdf_url': pdf_url,
            'github_url': github_link,  # ä¿®æ”¹é”®åä¸º github_url ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'url': paper_url,  # æ·»åŠ  arxiv URL
            'category': category,
            'subcategory': subcategory,  # æ·»åŠ å­ç±»åˆ«ä¿¡æ¯
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

        # åˆå¹¶åˆ†æç»“æœ
        if analysis:
            paper_info.update(analysis)

        return paper_info

    except Exception as e:
        logger.error(f"\u5904\u7406\u8bba\u6587\u65f6\u51fa\u9519: {str(e)}")
        return None


def get_cv_papers():
    """
    è·å–å¹¶åˆ†ç±» CV è®ºæ–‡çš„ä¸»å‡½æ•°
    """
    logger.info("="*50)
    logger.info(f"å¼€å§‹è·å–CVè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*50)
    
    try:
        # è®¡ç®—ç›®æ ‡æ—¥æœŸ
        target_date = (datetime.now() - timedelta(days=QUERY_DAYS_AGO)).date()
        logger.info(f"\nğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        logger.info(f"ğŸ“Š è®ºæ–‡æ•°é‡: {MAX_RESULTS}")
        logger.info(f"ğŸ§µ çº¿ç¨‹æ•°é‡: {MAX_WORKERS}\n")

        # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
        logger.info("ğŸ¤– åˆå§‹åŒ–ChatGLMåŠ©æ‰‹...")
        glm_helper = ChatGLMHelper()

        # åˆå§‹åŒ–arxivå®¢æˆ·ç«¯
        logger.info("ğŸ”„ åˆå§‹åŒ–arXivå®¢æˆ·ç«¯...")
        client = arxiv.Client(
            page_size=100,
            delay_seconds=3,
            num_retries=5
        )

        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query='cat:cs.CV',
            max_results=MAX_RESULTS,
            sort_by=arxiv.SortCriterion.LastUpdatedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        # åˆ›å»ºçº¿ç¨‹æ± 
        total_papers = 0
        papers_by_category = defaultdict(list)
        
        # ç¡®ä¿"å…¶ä»– (Others)"ç±»åˆ«æ€»æ˜¯å­˜åœ¨
        papers_by_category["å…¶ä»– (Others)"]  # åˆå§‹åŒ–ç©ºåˆ—è¡¨

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # åˆ›å»ºè¿›åº¦æ¡å¹¶å¼€å§‹è·å–è®ºæ–‡
            logger.info("\nğŸ” å¼€å§‹è·å–è®ºæ–‡...")
            results = client.results(search)  # ä» arXiv API è·å–è®ºæ–‡ç»“æœ
            
            # åˆ›å»ºæ€»è¿›åº¦æ¡
            total_pbar = tqdm(
                total=MAX_RESULTS,
                desc="æ€»è¿›åº¦",
                unit="ç¯‡",
                position=0,
                leave=True
            )
            
            # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
            batch_pbar = tqdm(
                total=0,  # åˆå§‹å€¼ä¸º0ï¼Œåé¢ä¼šæ›´æ–°
                desc="å½“å‰æ‰¹æ¬¡",
                unit="ç¯‡",
                position=1,
                leave=True
            )
            
            # æ‰¹é‡å¤„ç†è®ºæ–‡
            batch_size = 10  # æ¯æ‰¹å¤„ç†10ç¯‡è®ºæ–‡
            papers = []
            futures = []
            
            for i, paper in enumerate(results):
                papers.append(paper)
                
                # å½“æ”¶é›†åˆ°ä¸€æ‰¹è®ºæ–‡æˆ–è¾¾åˆ°æœ€å¤§æ•°é‡æ—¶å¤„ç†
                if len(papers) >= batch_size or i >= MAX_RESULTS - 1:
                    batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                    batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                    
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    batch_futures = [
                        executor.submit(process_paper, paper, glm_helper, target_date)
                        for paper in papers
                    ]
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                    for future in as_completed(batch_futures):
                        paper_info = future.result()
                        if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                            total_papers += 1
                            category = paper_info['category']
                            papers_by_category[category].append(paper_info)
                            total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                        batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡
                    papers = []
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢è·å–
                if i >= MAX_RESULTS - 1:
                    break
            
            # å¤„ç†å‰©ä½™çš„è®ºæ–‡
            if papers:
                batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = [
                    executor.submit(process_paper, paper, glm_helper, target_date)
                    for paper in papers
                ]
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    paper_info = future.result()
                    if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                        total_papers += 1
                        category = paper_info['category']
                        papers_by_category[category].append(paper_info)
                        total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                    batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦

            # å…³é—­è¿›åº¦æ¡
            batch_pbar.close()
            total_pbar.close()

        if total_papers == 0:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°{target_date}å‘å¸ƒçš„è®ºæ–‡ã€‚")
            return

        # è¾“å‡ºè®ºæ–‡ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"\nğŸ“Š è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        logger.info(f"{'='*50}")
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
        sorted_categories = sorted(
            papers_by_category.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # ç»Ÿè®¡æ€»è®ºæ–‡æ•°
        total_papers = sum(len(papers) for papers in papers_by_category.values())
        
        # ä¸€çº§åˆ†ç±»å’Œå¯¹åº”çš„äºŒçº§åˆ†ç±»ç»Ÿè®¡
        for category, papers in sorted_categories:
            # å³ä½¿æ²¡æœ‰è®ºæ–‡ï¼Œä¹Ÿè¦æ‰“å°"å…¶ä»–"ç±»åˆ«
            if len(papers) == 0 and category != "å…¶ä»– (Others)":
                continue
                
            # è¾“å‡ºä¸€çº§åˆ†ç±»æ ‡é¢˜
            logger.info(f"\nã€{category}ã€‘")
            
            # å¦‚æœä¸æ˜¯"å…¶ä»–"ç±»åˆ«ï¼Œå°†æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç§»åŠ¨åˆ°"å…¶ä»–"ç±»åˆ«
            if category != "å…¶ä»– (Others)":
                # åˆ†ç¦»æœ‰å­ç±»åˆ«çš„è®ºæ–‡å’Œæ— å­ç±»åˆ«çš„è®ºæ–‡
                papers_with_subcategory = []
                papers_without_subcategory = []
                
                for paper in list(papers):  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨éå†æ—¶ä¿®æ”¹
                    subcategory = paper.get('subcategory', '')
                    if not subcategory or subcategory == "æœªæŒ‡å®š":
                        papers_without_subcategory.append(paper)
                        # å°†æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç§»åŠ¨åˆ°"å…¶ä»–"ç±»åˆ«
                        papers_by_category["å…¶ä»– (Others)"].append(paper)
                        papers.remove(paper)  # ä»å½“å‰ç±»åˆ«ä¸­ç§»é™¤
                    else:
                        papers_with_subcategory.append(paper)
            
            # å¦‚æœå½“å‰ç±»åˆ«ä¸‹æ²¡æœ‰è®ºæ–‡ï¼Œè·³è¿‡
            if len(papers) == 0 and category != "å…¶ä»– (Others)":
                continue
            
            # æŒ‰å­ç±»åˆ«åˆ†ç»„è®ºæ–‡
            papers_by_subcategory = defaultdict(list)
            for paper in papers:  # ä½¿ç”¨æ›´æ–°åçš„papers
                subcategory = paper.get('subcategory', '')
                if subcategory and subcategory != "æœªæŒ‡å®š":
                    papers_by_subcategory[subcategory].append(paper)
                else:
                    # å¯¹äºæ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ï¼Œå¦‚æœæ˜¯"å…¶ä»–"ç±»åˆ«ï¼Œæ˜¾ç¤ºä¸º"æœªåˆ†ç±»"
                    papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
            
            # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
            sorted_subcategories = sorted(
                papers_by_subcategory.items(),
                key=lambda x: len(x[1]),
                reverse=True
            )
            
            # æ‰“å°ä¸€çº§åˆ†ç±»æ€»æ•°
            num_new = sum(1 for p in papers if not p['is_updated'])
            num_updated = sum(1 for p in papers if p['is_updated'])
            logger.info(f"æ€»è®¡: {len(papers):3d} ç¯‡ (ğŸ”¥ {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
            
            # æ‰“å°å­ç±»åˆ«ç»Ÿè®¡
            for subcategory, subpapers in sorted_subcategories:
                num_new = sum(1 for p in subpapers if not p['is_updated'])
                num_updated = sum(1 for p in subpapers if p['is_updated'])
                logger.info(
                    f"â””â”€ {subcategory:15s}: {len(subpapers):3d} ç¯‡ (ğŸ”¥ {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
            
            # ä¸å†æ‰“å°"ç›´æ¥å½’ç±»"ï¼Œå› ä¸ºè¿™äº›è®ºæ–‡å·²ç»è¢«ç§»åŠ¨åˆ°"å…¶ä»– (Others)"ç±»åˆ«ä¸­
        
        logger.info(f"\n{'='*50}")
        logger.info(f"æ€»è®¡: {total_papers} ç¯‡")
        
        # ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶
        logger.info("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶...")
        save_papers_to_markdown(papers_by_category, target_date)
        
        logger.info("\n" + "="*50)
        logger.info(f"CVè®ºæ–‡è·å–å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*50 + "\n")

    except Exception as e:
        logger.error("\nâŒ å¤„ç†CVè®ºæ–‡æ—¶å‡ºé”™:")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        logger.error(f"å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        # è®°å½•å †æ ˆè·Ÿè¸ªä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
        logger.debug(traceback.format_exc())
        raise  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯


def save_papers_to_markdown(papers_by_category: dict, target_date):
    """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Markdownæ–‡ä»¶"""
    # ä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    filename = target_date.strftime("%Y-%m-%d") + ".md"
    year_month = target_date.strftime("%Y-%m")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•(scripts)çš„çˆ¶çº§è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    # è®¾ç½®åŸºç¡€ç›®å½•ï¼Œä¸scriptsåŒçº§
    data_base = os.path.join(parent_dir, 'data')
    local_base = os.path.join(parent_dir, 'local')

    # åˆ›å»ºå¹´æœˆå­ç›®å½•
    data_year_month = os.path.join(data_base, year_month)
    local_year_month = os.path.join(local_base, year_month)

    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)

    # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡åˆ—è¡¨\n\n")
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡è¯¦æƒ…\n\n")
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    logger.info(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    logger.info(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


def generate_statistics_markdown(papers_by_category: dict) -> str:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯çš„Markdownæ ¼å¼æ–‡æœ¬
    
    Args:
        papers_by_category: æŒ‰ç±»åˆ«ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        
    Returns:
        str: Markdownæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
    """
    markdown = "## ç»Ÿè®¡ä¿¡æ¯\n\n"
    
    # è®¡ç®—æ€»è®ºæ–‡æ•°
    total_papers = sum(len(papers) for papers in papers_by_category.values())
    markdown += f"**æ€»è®ºæ–‡æ•°**: {total_papers} ç¯‡\n\n"
    
    # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
    sorted_categories = sorted(
        papers_by_category.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )
    
    # ä¸€çº§åˆ†ç±»ç»Ÿè®¡
    markdown += "### ä¸€çº§åˆ†ç±»ç»Ÿè®¡\n\n"
    markdown += "| ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
    markdown += "|------|--------|--------|------|\n"
    
    for category, papers in sorted_categories:
        num_new = sum(1 for p in papers if not p['is_updated'])
        num_updated = sum(1 for p in papers if p['is_updated'])
        markdown += f"| {category} | {len(papers)} | {num_new} | {num_updated} |\n"
    
    # äºŒçº§åˆ†ç±»ç»Ÿè®¡
    markdown += "\n### äºŒçº§åˆ†ç±»ç»Ÿè®¡\n\n"
    
    for category, papers in sorted_categories:
        if len(papers) == 0:
            continue
            
        markdown += f"#### {category}\n\n"
        markdown += "| å­ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
        markdown += "|--------|--------|--------|------|\n"
        
        # å¦‚æœæ˜¯"å…¶ä»–"ç±»åˆ«ï¼Œæ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç›´æ¥æ˜¾ç¤ºåœ¨ä¸»ç±»åˆ«ä¸‹
        papers_with_subcategory = []
        
        if category != "å…¶ä»– (Others)":
            # åˆ†ç¦»æœ‰å­ç±»åˆ«çš„è®ºæ–‡å’Œæ— å­ç±»åˆ«çš„è®ºæ–‡
            for paper in papers:
                subcategory = paper.get('subcategory', '')
                if subcategory and subcategory != "æœªæŒ‡å®š":
                    papers_with_subcategory.append(paper)
                # æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡å·²ç»è¢«ç§»åŠ¨åˆ°"å…¶ä»– (Others)"ç±»åˆ«ä¸­
        else:
            # å¯¹äº"å…¶ä»–"ç±»åˆ«ï¼Œæ‰€æœ‰è®ºæ–‡éƒ½ç›´æ¥å¤„ç†
            papers_with_subcategory = papers
        
        # æŒ‰å­ç±»åˆ«åˆ†ç»„æœ‰å­ç±»åˆ«çš„è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        for paper in papers_with_subcategory:
            subcategory = paper.get('subcategory', '')
            papers_by_subcategory[subcategory].append(paper)
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
        sorted_subcategories = sorted(
            papers_by_subcategory.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # æ·»åŠ å­ç±»åˆ«ç»Ÿè®¡
        for subcategory, subpapers in sorted_subcategories:
            num_new = sum(1 for p in subpapers if not p['is_updated'])
            num_updated = sum(1 for p in subpapers if p['is_updated'])
            markdown += f"| {subcategory} | {len(subpapers)} | {num_new} | {num_updated} |\n"
        
        markdown += "\n"
    
    return markdown


if __name__ == "__main__":    
    # è¿è¡Œè®ºæ–‡è·å–ä¸åˆ†ç±»ç¨‹åº
    logger.info(f"CVè®ºæ–‡è·å–ç¨‹åºå¯åŠ¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    get_cv_papers()
