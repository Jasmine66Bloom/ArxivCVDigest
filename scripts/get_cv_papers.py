from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import io
import os
import re
import time
from typing import Any, Dict

import arxiv
import requests
from tqdm import tqdm

from chatglm_helper import ChatGLMHelper

# æŸ¥è¯¢å‚æ•°è®¾ç½®
QUERY_DAYS_AGO = 1          # æŸ¥è¯¢å‡ å¤©å‰çš„è®ºæ–‡ï¼Œ0=ä»Šå¤©ï¼Œ1=æ˜¨å¤©ï¼Œ2=å‰å¤©
MAX_RESULTS = 200           # æœ€å¤§è¿”å›è®ºæ–‡æ•°é‡
MAX_WORKERS = 5            # å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°


def extract_github_link(text, paper_url=None, title=None, authors=None, pdf_url=None):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥

    Args:
        text: è®ºæ–‡æ‘˜è¦æ–‡æœ¬
        paper_url: è®ºæ–‡URLï¼ˆæœªä½¿ç”¨ï¼‰
        title: è®ºæ–‡æ ‡é¢˜ï¼ˆæœªä½¿ç”¨ï¼‰
        authors: ä½œè€…åˆ—è¡¨ï¼ˆæœªä½¿ç”¨ï¼‰
        pdf_url: PDFæ–‡ä»¶URLï¼ˆæœªä½¿ç”¨ï¼‰

    Returns:
        str: GitHubé“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # ä»æ‘˜è¦ä¸­æŸ¥æ‰¾
    for pattern in github_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            url = match.group(0)
            if not url.startswith('http'):
                url = 'https://' + url
            return url

    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹"""
    markdown = ""

    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}

    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"

    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç é“¾æ¥']

    # ä¸ºæ¯ä¸ªæœ‰è®ºæ–‡çš„ç±»åˆ«åˆ›å»ºè¡¨æ ¼
    for category, papers in sorted(((k, v) for k, v in active_categories.items() if k != "å…¶ä»–")):
        # æ·»åŠ ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"

        # åˆ›å»ºè¡¨æ ¼å¤´
        markdown += "|" + "|".join(headers) + "|\n"
        markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"

        # æ·»åŠ è®ºæ–‡
        for paper in papers:
            # ç¡®å®šè®ºæ–‡çŠ¶æ€
            if paper['is_updated']:
                status = f"ğŸ“ æ›´æ–°"
            else:
                status = f"ğŸ†• å‘å¸ƒ"

            # å‡†å¤‡æ¯ä¸ªå­—æ®µçš„å€¼
            values = [
                status,
                paper['title'],
                paper['title_cn'],
                paper['authors'],  # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
                f"<{paper['pdf_url']}>",
                f"<{paper['github_link']}>" if paper['github_link'] != 'None' else 'None'
            ]

            # å¤„ç†ç‰¹æ®Šå­—ç¬¦
            values = [str(v).replace('\n', ' ').replace('|', '&#124;')
                      for v in values]

            # æ·»åŠ åˆ°è¡¨æ ¼
            markdown += "|" + "|".join(values) + "|\n"

        # åœ¨æ¯ä¸ªè¡¨æ ¼åæ·»åŠ ç©ºè¡Œ
        markdown += "\n"

    # å¤„ç†"å…¶ä»–"ç±»åˆ«
    if "å…¶ä»–" in active_categories:
        # æ·»åŠ ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## å…¶ä»–\n\n"

        # åˆ›å»ºè¡¨æ ¼å¤´
        markdown += "|" + "|".join(headers) + "|\n"
        markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"

        # æ·»åŠ è®ºæ–‡
        for paper in active_categories["å…¶ä»–"]:
            # ç¡®å®šè®ºæ–‡çŠ¶æ€
            if paper['is_updated']:
                status = f"ğŸ“ æ›´æ–°"
            else:
                status = f"ğŸ†• å‘å¸ƒ"

            # å‡†å¤‡æ¯ä¸ªå­—æ®µçš„å€¼
            values = [
                status,
                paper['title'],
                paper['title_cn'],
                paper['authors'],  # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
                f"<{paper['pdf_url']}>",
                f"<{paper['github_link']}>" if paper['github_link'] != 'None' else 'None'
            ]

            # å¤„ç†ç‰¹æ®Šå­—ç¬¦
            values = [str(v).replace('\n', ' ').replace('|', '&#124;')
                      for v in values]

            # æ·»åŠ åˆ°è¡¨æ ¼
            markdown += "|" + "|".join(values) + "|\n"

        # åœ¨æ¯ä¸ªè¡¨æ ¼åæ·»åŠ ç©ºè¡Œ
        markdown += "\n"

    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹"""
    markdown = ""

    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}

    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"

    # ä¸ºæ¯ä¸ªæœ‰è®ºæ–‡çš„ç±»åˆ«åˆ›å»ºè¯¦ç»†å†…å®¹
    for category, papers in sorted(((k, v) for k, v in active_categories.items() if k != "å…¶ä»–")):
        # æ·»åŠ ç±»åˆ«æ ‡é¢˜
        markdown += f"## **{category}**\n"
        markdown += "\n"

        # æ·»åŠ è®ºæ–‡
        for idx, paper in enumerate(papers, 1):
            markdown += f'**index:** {idx}<br />\n'
            markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
            markdown += f'**Title:** {paper["title"]}<br />\n'
            markdown += f'**Title_cn:** {paper["title_cn"]}<br />\n'
            # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
            markdown += f'**Authors:** {paper["authors"]}<br />\n'
            markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

            if paper["github_link"] != 'None':
                markdown += f'**Code:** <{paper["github_link"]}><br />\n'
            else:
                markdown += '**Code:** None<br />\n'

            if "æ ¸å¿ƒè´¡çŒ®" in paper:
                markdown += f'**Core Contribution:** {paper["æ ¸å¿ƒè´¡çŒ®"]}<br />\n'
            if "æ ¸å¿ƒé—®é¢˜" in paper:
                markdown += f'**Core Problem:** {paper["æ ¸å¿ƒé—®é¢˜"]}<br />\n'

            markdown += "\n"

        markdown += "\n"

    # å¤„ç†"å…¶ä»–"ç±»åˆ«
    if "å…¶ä»–" in active_categories:
        markdown += f"## **å…¶ä»–**\n"
        markdown += "\n"

        for idx, paper in enumerate(active_categories["å…¶ä»–"], 1):
            markdown += f'**index:** {idx}<br />\n'
            markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
            markdown += f'**Title:** {paper["title"]}<br />\n'
            markdown += f'**Title_cn:** {paper["title_cn"]}<br />\n'
            # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
            markdown += f'**Authors:** {paper["authors"]}<br />\n'
            markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

            if paper["github_link"] != 'None':
                markdown += f'**Code:** <{paper["github_link"]}><br />\n'
            else:
                markdown += '**Code:** None<br />\n'

            if "æ ¸å¿ƒè´¡çŒ®" in paper:
                markdown += f'**Core Contribution:** {paper["æ ¸å¿ƒè´¡çŒ®"]}<br />\n'
            if "æ ¸å¿ƒé—®é¢˜" in paper:
                markdown += f'**Core Problem:** {paper["æ ¸å¿ƒé—®é¢˜"]}<br />\n'

            markdown += "\n"

    return markdown


def save_papers_to_markdown(papers_by_category: dict, target_date):
    """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Markdownæ–‡ä»¶"""
    # ä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    filename = target_date.strftime("%Y-%m-%d") + ".md"
    year_month = target_date.strftime("%Y-%m")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•(scripts)çš„çˆ¶çº§è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    # è®¾ç½®åŸºç¡€ç›®å½•ï¼Œä¸scriptsåŒçº§
    data_base = os.path.join(parent_dir, 'data')
    local_base = os.path.join(parent_dir, 'local')

    # åˆ›å»ºå¹´æœˆå­ç›®å½•
    data_year_month = os.path.join(data_base, year_month)
    local_year_month = os.path.join(local_base, year_month)

    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)

    # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    print(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    print(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated

        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None

        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            github_future = executor.submit(extract_github_link, abstract)
            analysis_future = executor.submit(
                glm_helper.analyze_paper_contribution, title, abstract)
            category_future = executor.submit(
                glm_helper.categorize_paper, title, abstract)
            title_cn_future = executor.submit(
                glm_helper.translate_title, title)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            github_link = github_future.result() or "None"
            analysis = analysis_future.result()
            category = category_future.result()
            title_cn = title_cn_future.result()

        return {
            'title': title,
            'title_cn': title_cn,
            'authors': authors_str,
            'abstract': abstract,
            'paper_url': paper_url,
            'pdf_url': pdf_url,
            'github_link': github_link,
            'analysis_result': analysis,
            'category': category,
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

    except Exception as e:
        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return None


def get_cv_papers():
    """è·å–CVé¢†åŸŸè®ºæ–‡å¹¶ä¿å­˜ä¸ºMarkdown"""
    print("\n" + "="*50)
    print(f"å¼€å§‹è·å–CVè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # è·å–ç›®æ ‡æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
        target_date = (datetime.now() - timedelta(days=QUERY_DAYS_AGO)).date()
        print(f"\nğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"ğŸ“Š æœ€å¤§è®ºæ–‡æ•°: {MAX_RESULTS}")
        print(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}\n")

        # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
        print("ğŸ¤– åˆå§‹åŒ–ChatGLMåŠ©æ‰‹...")
        glm_helper = ChatGLMHelper()

        # åˆå§‹åŒ–arxivå®¢æˆ·ç«¯
        print("ğŸ”„ åˆå§‹åŒ–arXivå®¢æˆ·ç«¯...")
        client = arxiv.Client(
            page_size=100,  # æ¯é¡µè·å–100ç¯‡è®ºæ–‡
            delay_seconds=3,  # è¯·æ±‚é—´éš”3ç§’
            num_retries=5    # å¤±è´¥é‡è¯•5æ¬¡
        )

        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query='cat:cs.CV',  # è®¡ç®—æœºè§†è§‰ç±»åˆ«
            max_results=MAX_RESULTS,
            sort_by=arxiv.SortCriterion.LastUpdatedDate,
            sort_order=arxiv.SortOrder.Descending  # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åº
        )

        # åˆ›å»ºçº¿ç¨‹æ± 
        total_papers = 0
        papers_by_category = defaultdict(list)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # åˆ†æ‰¹è·å–å’Œå¤„ç†è®ºæ–‡
            batch_size = 30  # æ¯æ‰¹å¤„ç†30ç¯‡è®ºæ–‡
            futures = []
            processed_papers = set()  # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„è®ºæ–‡ID

            # åˆ›å»ºæ€»è¿›åº¦æ¡
            total_pbar = tqdm(total=MAX_RESULTS, desc="æ€»è¿›åº¦", unit="ç¯‡")
            # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
            batch_pbar = tqdm(total=batch_size, desc="å½“å‰æ‰¹æ¬¡",
                              unit="ç¯‡", leave=False)

            for paper in client.results(search):
                paper_id = paper.get_short_id()

                # è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡
                if paper_id in processed_papers:
                    continue
                processed_papers.add(paper_id)

                # æäº¤å¤„ç†ä»»åŠ¡
                future = executor.submit(
                    process_paper, paper, glm_helper, target_date)
                futures.append(future)

                # å½“ç´¯ç§¯äº†ä¸€æ‰¹è®ºæ–‡æ—¶ï¼Œç­‰å¾…å®ƒä»¬å¤„ç†å®Œæˆ
                if len(futures) >= batch_size:
                    batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                    for completed_future in as_completed(futures):
                        paper_info = completed_future.result()
                        if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                            total_papers += 1
                            category = paper_info['category']
                            papers_by_category[category].append(paper_info)
                            total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                        batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦
                    futures = []  # æ¸…ç©ºå·²å¤„ç†çš„futures

                # å¦‚æœå·²ç»å¤„ç†äº†è¶³å¤Ÿå¤šçš„è®ºæ–‡ï¼Œå°±åœæ­¢
                if total_papers >= MAX_RESULTS:
                    break

            # å¤„ç†å‰©ä½™çš„è®ºæ–‡
            if futures:
                batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                batch_pbar.total = len(futures)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                for future in as_completed(futures):
                    paper_info = future.result()
                    if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                        total_papers += 1
                        category = paper_info['category']
                        papers_by_category[category].append(paper_info)
                        total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                    batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦

            # å…³é—­è¿›åº¦æ¡
            batch_pbar.close()
            total_pbar.close()

        if total_papers == 0:
            print(f"æ²¡æœ‰æ‰¾åˆ°{target_date}å‘å¸ƒçš„è®ºæ–‡ã€‚")
            return

        # æŒ‰ç±»åˆ«å¯¹è®ºæ–‡è¿›è¡Œæ’åºï¼ˆæŒ‰å‘å¸ƒæ—¶é—´é™åºï¼‰
        for category in papers_by_category:
            papers_by_category[category].sort(
                key=lambda x: x.get('published', ''),
                reverse=True
            )

        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶...")
        save_papers_to_markdown(papers_by_category, target_date)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"{'='*30}")
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
        sorted_categories = sorted(
            papers_by_category.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        for category, papers in sorted_categories:
            num_new = sum(1 for p in papers if not p['is_updated'])
            num_updated = sum(1 for p in papers if p['is_updated'])
            print(
                f"{category:15s}: {len(papers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
        print(f"{'='*30}")
        print(f"æ€»è®¡: {total_papers} ç¯‡")
        
        print("\n" + "="*50)
        print(f"CVè®ºæ–‡è·å–å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50 + "\n")

    except Exception as e:
        print("\nâŒ å¤„ç†CVè®ºæ–‡æ—¶å‡ºé”™:")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        raise  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯


if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒæŸ¥è¯¢
    get_cv_papers()
