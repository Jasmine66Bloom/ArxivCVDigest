"""è·å–CVè®ºæ–‡"""
import os
import re
import json
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from collections import defaultdict
import categories_config
from categories_config import CATEGORY_DISPLAY_ORDER, CATEGORY_THRESHOLDS
from chatglm_helper import ChatGLMHelper
from typing import Dict, List, Tuple, Optional
import traceback
import arxiv

# æŸ¥è¯¢å‚æ•°è®¾ç½®
QUERY_DAYS_AGO = 1          # æŸ¥è¯¢å‡ å¤©å‰çš„è®ºæ–‡ï¼Œ0=ä»Šå¤©ï¼Œ1=æ˜¨å¤©ï¼Œ2=å‰å¤©
MAX_RESULTS = 200           # æœ€å¤§è¿”å›è®ºæ–‡æ•°é‡
MAX_WORKERS = 8            # å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°


def extract_github_link(text, paper_url=None, title=None, authors=None, pdf_url=None):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥

    Args:
        text: è®ºæ–‡æ‘˜è¦æ–‡æœ¬
        paper_url: è®ºæ–‡URLï¼ˆæœªä½¿ç”¨ï¼‰
        title: è®ºæ–‡æ ‡é¢˜ï¼ˆæœªä½¿ç”¨ï¼‰
        authors: ä½œè€…åˆ—è¡¨ï¼ˆæœªä½¿ç”¨ï¼‰
        pdf_url: PDFæ–‡ä»¶URLï¼ˆæœªä½¿ç”¨ï¼‰

    Returns:
        str: GitHubé“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # ä»æ‘˜è¦ä¸­æŸ¥æ‰¾
    for pattern in github_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            url = match.group(0)
            if not url.startswith('http'):
                url = 'https://' + url
            return url

    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç /è´¡çŒ®']
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
            
        # æ·»åŠ ä¸€çº§ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        
        # æŒ‰å­ç±»åˆ«ç»„ç»‡è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if not subcategory or subcategory == "æœªæŒ‡å®š":
                subcategory = "å…¶ä»–"
            papers_by_subcategory[subcategory].append(paper)
            
        # å¤„ç†æ¯ä¸ªå­ç±»åˆ«
        for subcategory, papers in papers_by_subcategory.items():
            # æ·»åŠ äºŒçº§ç±»åˆ«æ ‡é¢˜
            markdown += f"\n### {subcategory}\n\n"
            
            # åˆ›å»ºè¡¨æ ¼å¤´
            markdown += "|" + "|".join(headers) + "|\n"
            markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"
            
            # æ·»åŠ è®ºæ–‡
            for paper in papers:
                # ç¡®å®šè®ºæ–‡çŠ¶æ€
                if paper['is_updated']:
                    status = f"ğŸ“ æ›´æ–°"
                else:
                    status = f"ğŸ†• å‘å¸ƒ"
                
                # åˆå¹¶ä»£ç é“¾æ¥å’Œæ ¸å¿ƒè´¡çŒ®
                code_and_contribution = ""
                if paper['github_url'] != 'None':
                    code_and_contribution = f"[[ä»£ç ]](<{paper['github_url']}>)"
                    if "æ ¸å¿ƒè´¡çŒ®" in paper:
                        code_and_contribution += "<br />"
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    core_contribution = paper["æ ¸å¿ƒè´¡çŒ®"]
                    if " | " in core_contribution:
                        items = core_contribution.split(" | ")
                        code_and_contribution += "<br />".join([f"- {item.strip()}" for item in items])
                    else:
                        code_and_contribution += core_contribution
                if not code_and_contribution:
                    code_and_contribution = 'None'
                
                # å‡†å¤‡æ¯ä¸ªå­—æ®µçš„å€¼
                values = [
                    status,
                    paper['title'],
                    paper.get('title_zh', ''),
                    paper['authors'],  # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
                    f"<{paper['pdf_url']}>",
                    code_and_contribution,
                ]
                
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦
                values = [str(v).replace('\n', ' ').replace('|', '&#124;')
                          for v in values]
                
                # æ·»åŠ åˆ°è¡¨æ ¼
                markdown += "|" + "|".join(values) + "|\n"
            
            # åœ¨æ¯ä¸ªè¡¨æ ¼åæ·»åŠ ç©ºè¡Œ
            markdown += "\n"
    
    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
            
        # æ·»åŠ ä¸€çº§ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        
        # æŒ‰å­ç±»åˆ«ç»„ç»‡è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if not subcategory or subcategory == "æœªæŒ‡å®š":
                subcategory = "å…¶ä»–"
            papers_by_subcategory[subcategory].append(paper)
        
        # å¤„ç†æ¯ä¸ªå­ç±»åˆ«
        for subcategory, papers in papers_by_subcategory.items():
            # æ·»åŠ äºŒçº§ç±»åˆ«æ ‡é¢˜
            markdown += f"\n### {subcategory}\n\n"
            
            # æ·»åŠ è®ºæ–‡
            for idx, paper in enumerate(papers, 1):
                markdown += f'**index:** {idx}<br />\n'
                markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
                markdown += f'**Title:** {paper["title"]}<br />\n'
                markdown += f'**Title_cn:** {paper.get("title_zh", "")}<br />\n'
                # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
                markdown += f'**Authors:** {paper["authors"]}<br />\n'
                markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

                # åˆå¹¶ä»£ç é“¾æ¥å’Œæ ¸å¿ƒè´¡çŒ®
                markdown += '**Code/Contribution:**<br />\n'
                if paper["github_url"] != 'None':
                    markdown += f'- [[ä»£ç ]](<{paper["github_url"]}>)<br />\n'
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    core_contribution = paper["æ ¸å¿ƒè´¡çŒ®"]
                    if " | " in core_contribution:
                        for item in core_contribution.split(" | "):
                            markdown += f'- {item.strip()}<br />\n'
                    else:
                        markdown += f'- {core_contribution}<br />\n'
                elif paper["github_url"] == 'None':
                    markdown += 'None<br />\n'

                if "æ ¸å¿ƒé—®é¢˜" in paper:
                    markdown += f'**Core Problem:** {paper["æ ¸å¿ƒé—®é¢˜"]}<br />\n'

                markdown += "\n"

    return markdown


def get_category_by_keywords(title: str, abstract: str, categories_config: Dict) -> List[Tuple[str, float]]:
    """
    æ‰§è¡ŒåŸºäºå…³é”®è¯åŒ¹é…å’Œä¼˜å…ˆçº§è§„åˆ™çš„å±‚æ¬¡åŒ–è®ºæ–‡åˆ†ç±»ã€‚
    
    Args:
        title (str): è®ºæ–‡æ ‡é¢˜ï¼Œç”¨äºä¸»è¦ä¸Šä¸‹æ–‡åˆ†æ
        abstract (str): è®ºæ–‡æ‘˜è¦ï¼Œç”¨äºå…¨é¢å†…å®¹åˆ†æ
        categories_config (Dict): åŒ…å«ç±»åˆ«å®šä¹‰ã€å…³é”®è¯ã€æƒé‡å’Œä¼˜å…ˆçº§çš„é…ç½®å­—å…¸
    
    å®ç°ç»†èŠ‚:
        1. æ–‡æœ¬é¢„å¤„ç†:
           - å¤§å°å†™æ ‡å‡†åŒ–ï¼Œç¡®ä¿åŒ¹é…ç¨³å¥æ€§
           - æ ‡é¢˜å’Œæ‘˜è¦çš„ç»„åˆåˆ†æï¼Œä½¿ç”¨å·®å¼‚åŒ–æƒé‡
           - åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤ï¼Œæé«˜åŒ¹é…è´¨é‡
        
        2. è¯„åˆ†æœºåˆ¶:
           - ä¸»è¦å¾—åˆ†: åŠ æƒå…³é”®è¯åŒ¹é… (åŸºç¡€æƒé‡ 0.15)
           - æ ‡é¢˜åŠ æˆ: æ ‡é¢˜åŒ¹é…çš„é¢å¤–æƒé‡ (0.08 æƒé‡)
           - ç²¾ç¡®åŒ¹é…åŠ æˆ: å®Œæ•´çŸ­è¯­åŒ¹é…çš„é¢å¤–æƒé‡
           - ä¼˜å…ˆçº§ä¹˜æ•°: ç±»åˆ«ç‰¹å®šé‡è¦æ€§ç¼©æ”¾
           - è´Ÿé¢å…³é”®è¯æƒ©ç½š: æŒ‡æ•°çº§åˆ†æ•°å‡å°‘
        
        3. åˆ†ç±»é€»è¾‘:
           - æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼: 0.25
           - ä¼˜å…ˆç±»åˆ«é˜ˆå€¼: æœ€é«˜åˆ†çš„ 65%
           - ä¸€èˆ¬ç±»åˆ«é˜ˆå€¼: æœ€é«˜åˆ†çš„ 45%
           - ä¼˜å…ˆç±»åˆ«çš„å±‚æ¬¡åŒ–å¤„ç†
    
    Returns:
        List[Tuple[str, float]]: æŒ‰ç½®ä¿¡åº¦é™åºæ’åºçš„ (ç±»åˆ«, ç½®ä¿¡åº¦åˆ†æ•°) å¯¹åˆ—è¡¨
    """
    # æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    
    # ç§»é™¤å¸¸è§çš„åœç”¨è¯ï¼Œæé«˜åŒ¹é…è´¨é‡
    stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by', 
                 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 
                 'does', 'did', 'but', 'if', 'then', 'else', 'when', 'up', 'down', 'this', 'that'}
    
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    title_words = set(w for w in title_lower.split() if w not in stop_words)
    abstract_words = set(w for w in abstract_lower.split() if w not in stop_words)
    
    # ç»„åˆæ–‡æœ¬ç”¨äºåŒ¹é…
    combined_text = title_lower + " " + abstract_lower
    
    # åˆå§‹åŒ–å¾—åˆ†ç´¯åŠ å™¨å’ŒåŒ¹é…è®°å½•
    scores = defaultdict(float)
    match_details = defaultdict(list)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¾—åˆ†
    for category, config in categories_config.items():
        score = 0.0
        matches = []
        
        # 1. æ­£å‘å…³é”®è¯åŒ¹é…
        for keyword, weight in config["keywords"]:
            keyword_lower = keyword.lower()
            keyword_words = set(w for w in keyword_lower.split() if w not in stop_words)
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if keyword_lower in title_lower:
                match_score = weight * 0.25  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æœ€é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif keyword_lower in abstract_lower:
                match_score = weight * 0.15  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æ¬¡ä¹‹
                score += match_score
                matches.append(f"æ‘˜è¦ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(title_words):
                match_score = weight * 0.18  # æ ‡é¢˜ä¸­çš„è¯ç»„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ‘˜è¦ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(abstract_words):
                match_score = weight * 0.12  # æ‘˜è¦ä¸­çš„è¯ç»„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # éƒ¨åˆ†å…³é”®è¯åŒ¹é…ï¼ˆè¾ƒä½æƒé‡ï¼Œä½†è€ƒè™‘åŒ¹é…æ¯”ä¾‹ï¼‰
            else:
                # è®¡ç®—åœ¨æ ‡é¢˜ä¸­åŒ¹é…çš„å•è¯
                title_matched = keyword_words & title_words
                if title_matched:
                    # åŒ¹é…æ¯”ä¾‹å½±å“å¾—åˆ†
                    match_ratio = len(title_matched) / len(keyword_words)
                    # åŒ¹é…æ¯”ä¾‹è¶Šé«˜ï¼Œå¾—åˆ†è¶Šé«˜
                    match_score = weight * 0.08 * match_ratio * (1 + match_ratio)
                    score += match_score
                    matches.append(f"æ ‡é¢˜éƒ¨åˆ†åŒ¹é… [{keyword}] ({match_ratio:.1%}): +{match_score:.2f}")
                
                # è®¡ç®—åœ¨æ‘˜è¦ä¸­åŒ¹é…çš„å•è¯
                abstract_matched = keyword_words & abstract_words
                if abstract_matched and abstract_matched != title_matched:
                    # åŒ¹é…æ¯”ä¾‹å½±å“å¾—åˆ†
                    match_ratio = len(abstract_matched) / len(keyword_words)
                    # åŒ¹é…æ¯”ä¾‹è¶Šé«˜ï¼Œå¾—åˆ†è¶Šé«˜
                    match_score = weight * 0.05 * match_ratio * (1 + match_ratio)
                    score += match_score
                    matches.append(f"æ‘˜è¦éƒ¨åˆ†åŒ¹é… [{keyword}] ({match_ratio:.1%}): +{match_score:.2f}")
            
            # å•è¯é¢‘ç‡åŠ æˆï¼ˆå¯¹äºé‡å¤å‡ºç°çš„å…³é”®è¯ç»™äºˆé¢å¤–åŠ æˆï¼‰
            if len(keyword_words) == 1 and keyword_lower in combined_text:
                # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
                frequency = combined_text.count(keyword_lower)
                if frequency > 1:
                    # é¢‘ç‡åŠ æˆï¼Œä½†æœ‰ä¸Šé™
                    freq_bonus = min(frequency * 0.02, 0.1) * weight
                    score += freq_bonus
                    matches.append(f"é¢‘ç‡åŠ æˆ [{keyword}] (x{frequency}): +{freq_bonus:.2f}")
        
        # 2. è´Ÿå‘å…³é”®è¯æƒ©ç½šï¼ˆæ›´ä¸¥æ ¼çš„æƒ©ç½šæœºåˆ¶ï¼‰
        if "negative_keywords" in config:
            negative_score = 0
            for keyword_tuple in config["negative_keywords"]:
                if isinstance(keyword_tuple, tuple) and len(keyword_tuple) >= 1:
                    keyword = keyword_tuple[0].lower()
                    neg_weight = keyword_tuple[1] if len(keyword_tuple) > 1 else 1.0
                else:
                    # å…¼å®¹å­—ç¬¦ä¸²æ ¼å¼
                    keyword = str(keyword_tuple).lower()
                    neg_weight = 1.0
                
                # æ£€æŸ¥è´Ÿå‘å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æœ¬ä¸­
                if keyword in combined_text:
                    # è®¡ç®—æƒ©ç½šåˆ†æ•°
                    penalty = neg_weight * 1.2
                    negative_score += penalty
                    matches.append(f"è´Ÿå‘åŒ¹é… [{keyword}]: -{penalty:.2f}")
            
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡è¿›è¡Œæƒ©ç½šï¼Œæƒ©ç½šåŠ›åº¦æ›´å¤§
            if negative_score > 0:
                original_score = score
                score *= math.exp(-negative_score * 1.2)
                penalty = original_score - score
                matches.append(f"è´Ÿå‘æƒ©ç½šæ€»è®¡: -{penalty:.2f}")
        
        # 3. åº”ç”¨ç±»åˆ«ä¼˜å…ˆçº§ç¼©æ”¾
        priority = config.get("priority", 0)
        if priority > 0:
            priority_bonus = score * (priority * 0.12)  # ä¼˜å…ˆçº§åŠ æˆæ›´æ˜æ˜¾
            score += priority_bonus
            matches.append(f"ä¼˜å…ˆçº§åŠ æˆ (çº§åˆ« {priority}): +{priority_bonus:.2f}")
        
        # è®°å½•å¾—åˆ†å’ŒåŒ¹é…è¯¦æƒ…
        if score > 0:
            scores[category] = score
            match_details[category] = matches
    
    # 4. åˆ†ç±»å†³ç­–é€»è¾‘
    # éªŒè¯æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
    max_score = max(scores.values()) if scores else 0
    if max_score < 0.25:  # æé«˜æœ€ä½ç½®ä¿¡åº¦è¦æ±‚
        return []
    
    # å¤„ç†é«˜ä¼˜å…ˆçº§ç±»åˆ«
    high_priority_categories = ["åŸºç¡€æ™ºèƒ½ä¸è®¤çŸ¥", "ç”Ÿæˆå¼å»ºæ¨¡", "å¤šæ¨¡æ€å­¦ä¹ ", "æ„ŸçŸ¥ä¸è¯†åˆ«", "åŒ»å­¦å½±åƒä¸åˆ†æ"]
    for category in high_priority_categories:
        if category in scores:
            category_score = scores[category]
            # å¯¹é«˜ä¼˜å…ˆçº§ç±»åˆ«ä½¿ç”¨è¾ƒä½çš„ç›¸å¯¹é˜ˆå€¼
            if category_score >= max_score * 0.65 and category_score >= 0.25:
                return [(category, category_score)]
    
    # å¤„ç†ä¸€èˆ¬ç±»åˆ«
    significant_categories = [
        (category, score) 
        for category, score in scores.items() 
        if score >= max_score * 0.45  # é™ä½ç›¸å¯¹é˜ˆå€¼ï¼Œæ•è·æ›´å¤šç›¸å…³ç±»åˆ«
    ]
    
    # æŒ‰ç½®ä¿¡åº¦é™åºæ’åºè¿”å›ç»“æœ
    return sorted(significant_categories, key=lambda x: x[1], reverse=True)


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated

        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None

        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)
        
        # åˆå§‹åŒ–é»˜è®¤å€¼ï¼Œé¿å…å¼‚å¸¸æ—¶æœªå®šä¹‰
        github_link = "None"
        category = "å…¶ä»–"
        subcategory = "æœªæŒ‡å®š"
        title_cn = f"[ç¿»è¯‘å¤±è´¥] {title}"
        analysis = {}

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        try:
            with ThreadPoolExecutor(max_workers=3) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                github_future = executor.submit(extract_github_link, abstract)
                analysis_future = executor.submit(
                    glm_helper.analyze_paper_contribution, title, abstract)
                category_future = executor.submit(
                    glm_helper.categorize_paper, title, abstract)
                title_cn_future = executor.submit(
                    glm_helper.translate_title, title)

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                github_link = github_future.result() or "None"
                analysis = analysis_future.result() or {}
                
                # ç¡®ä¿åˆ†ç±»ç»“æœæ˜¯æœ‰æ•ˆçš„ç±»åˆ«
                category_result = category_future.result()
                if category_result and category_result in CATEGORY_THRESHOLDS:
                    category = category_result
                else:
                    print(f"è­¦å‘Š: æ— æ•ˆçš„åˆ†ç±»ç»“æœ '{category_result}'ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»'å…¶ä»–'")
                    category = "å…¶ä»–"
                    
                title_cn = title_cn_future.result() or f"[ç¿»è¯‘å¤±è´¥] {title}"
        except Exception as e:
            print(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            # ç»§ç»­å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        
        # è·å–å­ç±»åˆ«ä¿¡æ¯
        try:
            subcategory = glm_helper.determine_subcategory(title, abstract, category)
        except Exception as e:
            print(f"è·å–å­ç±»åˆ«æ—¶å‡ºé”™: {str(e)}")
            subcategory = "æœªæŒ‡å®š"

        paper_info = {
            'title': title,
            'title_zh': title_cn,  # ä¿®æ”¹é”®åä¸º title_zh ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'abstract': abstract,
            'authors': authors_str,
            'pdf_url': pdf_url,
            'github_url': github_link,  # ä¿®æ”¹é”®åä¸º github_url ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'url': paper_url,  # æ·»åŠ  arxiv URL
            'category': category,
            'subcategory': subcategory,  # æ·»åŠ å­ç±»åˆ«ä¿¡æ¯
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

        # åˆå¹¶åˆ†æç»“æœ
        if analysis:
            paper_info.update(analysis)

        return paper_info

    except Exception as e:
        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return None


def get_cv_papers():
    """è·å–CVé¢†åŸŸè®ºæ–‡å¹¶ä¿å­˜ä¸ºMarkdown"""
    print("\n" + "="*50)
    print(f"å¼€å§‹è·å–CVè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # è·å–ç›®æ ‡æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
        target_date = (datetime.now() - timedelta(days=QUERY_DAYS_AGO)).date()
        print(f"\nğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"ğŸ“Š æœ€å¤§è®ºæ–‡æ•°: {MAX_RESULTS}")
        print(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}\n")

        # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
        print("ğŸ¤– åˆå§‹åŒ–ChatGLMåŠ©æ‰‹...")
        glm_helper = ChatGLMHelper()

        # åˆå§‹åŒ–arxivå®¢æˆ·ç«¯
        print("ğŸ”„ åˆå§‹åŒ–arXivå®¢æˆ·ç«¯...")
        client = arxiv.Client(
            page_size=100,  # æ¯é¡µè·å–100ç¯‡è®ºæ–‡
            delay_seconds=3,  # è¯·æ±‚é—´éš”3ç§’
            num_retries=5    # å¤±è´¥é‡è¯•5æ¬¡
        )

        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query='cat:cs.CV',  # è®¡ç®—æœºè§†è§‰ç±»åˆ«
            max_results=MAX_RESULTS,
            sort_by=arxiv.SortCriterion.LastUpdatedDate,
            sort_order=arxiv.SortOrder.Descending  # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åº
        )

        # åˆ›å»ºçº¿ç¨‹æ± 
        total_papers = 0
        papers_by_category = defaultdict(list)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # åˆ›å»ºè¿›åº¦æ¡
            print("\nğŸ” å¼€å§‹è·å–è®ºæ–‡...")
            results = client.results(search)
            
            # åˆ›å»ºæ€»è¿›åº¦æ¡
            total_pbar = tqdm(
                total=MAX_RESULTS,
                desc="æ€»è¿›åº¦",
                unit="ç¯‡",
                position=0,
                leave=True
            )
            
            # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
            batch_pbar = tqdm(
                total=0,  # åˆå§‹å€¼ä¸º0ï¼Œåé¢ä¼šæ›´æ–°
                desc="å½“å‰æ‰¹æ¬¡",
                unit="ç¯‡",
                position=1,
                leave=True
            )
            
            # æ‰¹é‡å¤„ç†è®ºæ–‡
            batch_size = 10  # æ¯æ‰¹å¤„ç†10ç¯‡è®ºæ–‡
            papers = []
            futures = []
            
            for i, paper in enumerate(results):
                papers.append(paper)
                
                # å½“æ”¶é›†åˆ°ä¸€æ‰¹è®ºæ–‡æˆ–è¾¾åˆ°æœ€å¤§æ•°é‡æ—¶å¤„ç†
                if len(papers) >= batch_size or i >= MAX_RESULTS - 1:
                    batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                    batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                    
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    batch_futures = [
                        executor.submit(process_paper, paper, glm_helper, target_date)
                        for paper in papers
                    ]
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                    for future in as_completed(batch_futures):
                        paper_info = future.result()
                        if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                            total_papers += 1
                            category = paper_info['category']
                            papers_by_category[category].append(paper_info)
                            total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                        batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡
                    papers = []
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢è·å–
                if i >= MAX_RESULTS - 1:
                    break
            
            # å¤„ç†å‰©ä½™çš„è®ºæ–‡
            if papers:
                batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = [
                    executor.submit(process_paper, paper, glm_helper, target_date)
                    for paper in papers
                ]
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    paper_info = future.result()
                    if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                        total_papers += 1
                        category = paper_info['category']
                        papers_by_category[category].append(paper_info)
                        total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                    batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦

            # å…³é—­è¿›åº¦æ¡
            batch_pbar.close()
            total_pbar.close()

        if total_papers == 0:
            print(f"æ²¡æœ‰æ‰¾åˆ°{target_date}å‘å¸ƒçš„è®ºæ–‡ã€‚")
            return

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"{'='*50}")
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
        sorted_categories = sorted(
            papers_by_category.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # ç»Ÿè®¡æ€»è®ºæ–‡æ•°
        total_papers = sum(len(papers) for papers in papers_by_category.values())
        
        # ä¸€çº§åˆ†ç±»å’Œå¯¹åº”çš„äºŒçº§åˆ†ç±»ç»Ÿè®¡
        for category, papers in sorted_categories:
            if len(papers) == 0:
                continue
                
            # æ‰“å°ä¸€çº§åˆ†ç±»æ ‡é¢˜
            print(f"\nã€{category}ã€‘")
            
            # æŒ‰å­ç±»åˆ«åˆ†ç»„
            papers_by_subcategory = defaultdict(list)
            for paper in papers:
                subcategory = paper.get('subcategory', '')
                if not subcategory or subcategory == "æœªæŒ‡å®š":
                    subcategory = "å…¶ä»–"
                papers_by_subcategory[subcategory].append(paper)
            
            # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
            sorted_subcategories = sorted(
                papers_by_subcategory.items(),
                key=lambda x: len(x[1]),
                reverse=True
            )
            
            # æ‰“å°ä¸€çº§åˆ†ç±»æ€»æ•°
            num_new = sum(1 for p in papers if not p['is_updated'])
            num_updated = sum(1 for p in papers if p['is_updated'])
            print(f"æ€»è®¡: {len(papers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
            
            # æ‰“å°å­ç±»åˆ«ç»Ÿè®¡
            for subcategory, subpapers in sorted_subcategories:
                num_new = sum(1 for p in subpapers if not p['is_updated'])
                num_updated = sum(1 for p in subpapers if p['is_updated'])
                print(
                    f"â””â”€ {subcategory:15s}: {len(subpapers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
        
        print(f"\n{'='*50}")
        print(f"æ€»è®¡: {total_papers} ç¯‡")
        
        # ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶...")
        save_papers_to_markdown(papers_by_category, target_date)
        
        print("\n" + "="*50)
        print(f"CVè®ºæ–‡è·å–å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50 + "\n")

    except Exception as e:
        print("\nâŒ å¤„ç†CVè®ºæ–‡æ—¶å‡ºé”™:")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        raise  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯


def save_papers_to_markdown(papers_by_category: dict, target_date):
    """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Markdownæ–‡ä»¶"""
    # ä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    filename = target_date.strftime("%Y-%m-%d") + ".md"
    year_month = target_date.strftime("%Y-%m")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•(scripts)çš„çˆ¶çº§è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    # è®¾ç½®åŸºç¡€ç›®å½•ï¼Œä¸scriptsåŒçº§
    data_base = os.path.join(parent_dir, 'data')
    local_base = os.path.join(parent_dir, 'local')

    # åˆ›å»ºå¹´æœˆå­ç›®å½•
    data_year_month = os.path.join(data_base, year_month)
    local_year_month = os.path.join(local_base, year_month)

    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)

    # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡åˆ—è¡¨\n\n")
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡è¯¦æƒ…\n\n")
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    print(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    print(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


def generate_statistics_markdown(papers_by_category: dict) -> str:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯çš„Markdownæ ¼å¼æ–‡æœ¬
    
    Args:
        papers_by_category: æŒ‰ç±»åˆ«ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        
    Returns:
        str: Markdownæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
    """
    markdown = "## ç»Ÿè®¡ä¿¡æ¯\n\n"
    
    # è®¡ç®—æ€»è®ºæ–‡æ•°
    total_papers = sum(len(papers) for papers in papers_by_category.values())
    markdown += f"**æ€»è®ºæ–‡æ•°**: {total_papers} ç¯‡\n\n"
    
    # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
    sorted_categories = sorted(
        papers_by_category.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )
    
    # ä¸€çº§åˆ†ç±»ç»Ÿè®¡
    markdown += "### ä¸€çº§åˆ†ç±»ç»Ÿè®¡\n\n"
    markdown += "| ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
    markdown += "|------|--------|--------|------|\n"
    
    for category, papers in sorted_categories:
        num_new = sum(1 for p in papers if not p['is_updated'])
        num_updated = sum(1 for p in papers if p['is_updated'])
        markdown += f"| {category} | {len(papers)} | {num_new} | {num_updated} |\n"
    
    # äºŒçº§åˆ†ç±»ç»Ÿè®¡
    markdown += "\n### äºŒçº§åˆ†ç±»ç»Ÿè®¡\n\n"
    
    for category, papers in sorted_categories:
        if len(papers) == 0:
            continue
            
        markdown += f"#### {category}\n\n"
        markdown += "| å­ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
        markdown += "|--------|--------|--------|------|\n"
        
        # æŒ‰å­ç±»åˆ«åˆ†ç»„
        papers_by_subcategory = defaultdict(list)
        for paper in papers:
            subcategory = paper.get('subcategory', '')
            if not subcategory or subcategory == "æœªæŒ‡å®š":
                subcategory = "å…¶ä»–"
            papers_by_subcategory[subcategory].append(paper)
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
        sorted_subcategories = sorted(
            papers_by_subcategory.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # æ·»åŠ å­ç±»åˆ«ç»Ÿè®¡
        for subcategory, subpapers in sorted_subcategories:
            num_new = sum(1 for p in subpapers if not p['is_updated'])
            num_updated = sum(1 for p in subpapers if p['is_updated'])
            markdown += f"| {subcategory} | {len(subpapers)} | {num_new} | {num_updated} |\n"
        
        markdown += "\n"
    
    return markdown


if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒæŸ¥è¯¢
    get_cv_papers()
