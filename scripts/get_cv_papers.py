"""è·å–CVè®ºæ–‡"""
import os
import re
import math
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from collections import defaultdict
from categories_config import CATEGORY_DISPLAY_ORDER, CATEGORY_THRESHOLDS
from chatglm_helper import ChatGLMHelper
from typing import Dict, List, Tuple, Optional
import traceback
import arxiv

# æŸ¥è¯¢å‚æ•°è®¾ç½®
QUERY_DAYS_AGO = 1          # æŸ¥è¯¢å‡ å¤©å‰çš„è®ºæ–‡ï¼Œ0=ä»Šå¤©ï¼Œ1=æ˜¨å¤©ï¼Œ2=å‰å¤©
MAX_RESULTS = 300           # æœ€å¤§è¿”å›è®ºæ–‡æ•°é‡
MAX_WORKERS = 1            # å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°

# å¯¼å…¥NLTKåº“ç”¨äºæ–‡æœ¬é¢„å¤„ç†
try:
    import nltk
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    
    # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è·¯å¾„
    nltk_flag_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.nltk_data_downloaded')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡NLTKæ•°æ®
    if os.path.exists(nltk_flag_file):
        # å·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥ä½¿ç”¨
        NLTK_AVAILABLE = True
    else:
        # æ£€æŸ¥å¿…è¦çš„NLTKæ•°æ®æ˜¯å¦å·²ä¸‹è½½
        needed_data = []
        for data_name in ['punkt', 'wordnet', 'stopwords']:
            try:
                path = f"{'tokenizers/' if data_name == 'punkt' else 'corpora/'}{data_name}"
                nltk.data.find(path)
                print(f"NLTKæ•°æ® '{data_name}' å·²å­˜åœ¨äº: {path}")
            except LookupError:
                needed_data.append(data_name)
                print(f"NLTKæ•°æ® '{data_name}' ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½")
        
        # åªä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        if needed_data:
            print(f"æ­£åœ¨ä¸‹è½½ç¼ºå¤±çš„NLTKæ•°æ®æ–‡ä»¶: {', '.join(needed_data)}")
            for data_name in needed_data:
                print(f"å¼€å§‹ä¸‹è½½ '{data_name}'...")
                download_result = nltk.download(data_name, quiet=False)
                print(f"ä¸‹è½½ '{data_name}' ç»“æœ: {download_result}")
            print("NLTKæ•°æ®æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        
        # ç‰¹åˆ«å¤„ç†punkt_tab
        try:
            nltk.data.find('tokenizers/punkt_tab')
            print("NLTKæ•°æ® 'punkt_tab' å·²å­˜åœ¨")
        except LookupError:
            print("å¼€å§‹ä¸‹è½½ 'punkt_tab'...")
            download_result = nltk.download('punkt', quiet=False)  # é‡æ–°ä¸‹è½½ punktå¯èƒ½ä¼šåŒ…å«punkt_tab
            print(f"ä¸‹è½½ 'punkt' ç»“æœ: {download_result}")
        
        # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è¡¨ç¤ºæ•°æ®å·²ä¸‹è½½
        with open(nltk_flag_file, 'w') as f:
            f.write(f"NLTK data downloaded at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        NLTK_AVAILABLE = True
    
    NLTK_AVAILABLE = True
except ImportError:
    print("NLTKåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºæœ¬æ–‡æœ¬å¤„ç†")
    NLTK_AVAILABLE = False

def extract_github_link(text, paper_url=None, title=None, authors=None, pdf_url=None):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥

    Args:
        text: è®ºæ–‡æ‘˜è¦æ–‡æœ¬
        paper_url: è®ºæ–‡URL
        title: è®ºæ–‡æ ‡é¢˜
        authors: ä½œè€…åˆ—è¡¨
        pdf_url: PDFæ–‡ä»¶URL

    Returns:
        str: GitHubé“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # ä»æ‘˜è¦ä¸­æŸ¥æ‰¾
    for pattern in github_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            url = match.group(0)
            if not url.startswith('http'):
                url = 'https://' + url
            return url

    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç /è´¡çŒ®']
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
        # åªè¾“å‡ºä¸€æ¬¡ä¸»ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        papers_by_subcategory = defaultdict(list)
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if subcategory and subcategory != "æœªæŒ‡å®š":
                papers_by_subcategory[subcategory].append(paper)
            elif category == "å…¶ä»– (Others)":
                papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
        if not papers_by_subcategory and category != "å…¶ä»– (Others)":
            continue
        for subcategory, papers in papers_by_subcategory.items():
            markdown += f"\n### {subcategory}\n\n"
            markdown += "|" + "|".join(headers) + "|\n"
            markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"
            for paper in papers:
                if paper['is_updated']:
                    status = f"ğŸ“ æ›´æ–°"
                else:
                    status = f"ğŸ†• å‘å¸ƒ"
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")]
                    else:
                        items = [core_contribution.strip()]
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    items = items[:2]
                    items = [(i[:50] + ("..." if len(i) > 50 else "")) for i in items]
                    return items
                contrib_list = []
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                if paper['github_url'] != 'None':
                    code_and_contribution = f"[ä»£ç ]({paper['github_url']})"
                    if contrib_list:
                        code_and_contribution += "; " + "; ".join(contrib_list)
                elif contrib_list:
                    code_and_contribution = "; ".join(contrib_list)
                else:
                    code_and_contribution = 'æ— '
                values = [
                    status,
                    paper['title'],
                    paper.get('title_zh', ''),
                    paper['authors'],
                    f"<{paper['pdf_url']}>",
                    code_and_contribution,
                ]
                values = [str(v).replace('\n', ' ').replace('|', '&#124;') for v in values]
                markdown += "|" + "|".join(values) + "|\n"
            markdown += "\n"
    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
            
        # æ·»åŠ ä¸€çº§ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        
        # æŒ‰å­ç±»åˆ«ç»„ç»‡è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        
        # å°†æ‰€æœ‰è®ºæ–‡åˆ†é…åˆ°å­ç±»åˆ«
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            if subcategory and subcategory != "æœªæŒ‡å®š":
                papers_by_subcategory[subcategory].append(paper)
            elif category == "å…¶ä»– (Others)":
                # å¯¹äº"å…¶ä»–"ç±»åˆ«ï¼Œæ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç›´æ¥æ˜¾ç¤ºåœ¨ä¸»ç±»åˆ«ä¸‹
                papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
        
        # å¦‚æœå½“å‰ç±»åˆ«ä¸‹æ²¡æœ‰å¸¦å­ç±»åˆ«çš„è®ºæ–‡ï¼Œè·³è¿‡
        if not papers_by_subcategory and category != "å…¶ä»– (Others)":
            continue
            
        # å¤„ç†æ¯ä¸ªå­ç±»åˆ«
        for subcategory, papers in papers_by_subcategory.items():
            # æ·»åŠ äºŒçº§ç±»åˆ«æ ‡é¢˜
            markdown += f"\n### {subcategory}\n\n"
            
            # æ·»åŠ è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            for idx, paper in enumerate(papers, 1):
                # å¼•ç”¨ç¼–å·
                markdown += f'**index:** {idx}<br />\n'
                # æ—¥æœŸ
                markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
                # è‹±æ–‡æ ‡é¢˜
                markdown += f'**Title:** {paper["title"]}<br />\n'
                # ä¸­æ–‡æ ‡é¢˜
                markdown += f'**Title_cn:** {paper.get("title_zh", "")}<br />\n'
                # ä½œè€…ï¼ˆå·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²ï¼‰
                markdown += f'**Authors:** {paper["authors"]}<br />\n'
                # PDFé“¾æ¥
                markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

                # åˆå¹¶ä»£ç é“¾æ¥å’Œç²¾ç®€åçš„æ ¸å¿ƒè´¡çŒ®
                markdown += '**Code/Contribution:**\n'
                
                # ç²¾ç®€æ ¸å¿ƒè´¡çŒ®å†…å®¹
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    # åˆ†å‰²ä¸ºå¤šæ¡
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")] 
                    else:
                        items = [core_contribution.strip()]
                    # å»é™¤æ¨¡æ¿åŒ–å†…å®¹
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    # åªä¿ç•™å‰ä¸‰æ¡
                    items = items[:3]
                    return items
                
                # å¤„ç†æ ¸å¿ƒè´¡çŒ®
                contrib_list = []
                if "æ ¸å¿ƒé—®é¢˜" in paper:
                    markdown += f'é—®é¢˜ï¼š{paper["æ ¸å¿ƒé—®é¢˜"]}\n'
                
                if "æ ¸å¿ƒæ–¹æ³•" in paper:
                    markdown += f'æ–¹æ³•ï¼š{paper["æ ¸å¿ƒæ–¹æ³•"]}\n'
                
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                    if contrib_list:
                        markdown += f'{", ".join(contrib_list)}\n'
                
                # å¤„ç†ä»£ç é“¾æ¥
                if paper['github_url'] != 'None':
                    markdown += f'[ä»£ç ]({paper["github_url"]})\n'
                
                # æ·»åŠ ç©ºè¡Œ
                markdown += '\n'

    return markdown


def preprocess_text(text: str) -> str:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å°å†™è½¬æ¢ã€åˆ†è¯ã€å»åœç”¨è¯ã€è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: é¢„å¤„ç†åçš„æ–‡æœ¬
    """
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # åŸºæœ¬æ–‡æœ¬å¤„ç†ï¼šå…ˆå»é™¤ç‰¹æ®Šå­—ç¬¦
    basic_processed = re.sub(r'[^\w\s]', ' ', text)
    
    # å¦‚æœNLTKä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸºæœ¬å¤„ç†ç»“æœ
    if not NLTK_AVAILABLE:
        return basic_processed
    
    # å°è¯•ä½¿ç”¨NLTKè¿›è¡Œé«˜çº§å¤„ç†
    try:
        # åˆ†è¯ - å…ˆä½¿ç”¨åŸºæœ¬åˆ†è¯ä½œä¸ºå¤‡é€‰
        try:
            tokens = word_tokenize(text)
        except Exception:
            # å¦‚æœé«˜çº§åˆ†è¯å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯
            tokens = basic_processed.split()
        
        # å»é™¤åœç”¨è¯
        try:
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        except Exception:
            # å¦‚æœåœç”¨è¯å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åœç”¨è¯åˆ—è¡¨
            basic_stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by'}
            tokens = [token for token in tokens if token not in basic_stop_words and len(token) > 2]
        
        # è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ - å¯é€‰åŠŸèƒ½
        try:
            stemmer = PorterStemmer()
            stemmed_tokens = [stemmer.stem(token) for token in tokens]
            
            lemmatizer = WordNetLemmatizer()
            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]
            
            # é‡æ–°ç»„åˆæˆæ–‡æœ¬
            return " ".join(lemmatized_tokens)
        except Exception:
            # å¦‚æœè¯å¹²æå–æˆ–è¯å½¢è¿˜åŸå¤±è´¥ï¼Œåªè¿”å›åˆ†è¯å’Œå»åœç”¨è¯çš„ç»“æœ
            return " ".join(tokens)
    
    except Exception as e:
        print(f"NLTKå¤„ç†æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
        # å¦‚æœæ‰€æœ‰NLTKå¤„ç†éƒ½å¤±è´¥ï¼Œå›é€€åˆ°åŸºæœ¬å¤„ç†
        return basic_processed


def get_category_by_keywords(title: str, abstract: str, categories_config: Dict) -> List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]:
    """
    æ‰§è¡ŒåŸºäºå…³é”®è¯åŒ¹é…å’Œä¼˜å…ˆçº§è§„åˆ™çš„å±‚æ¬¡åŒ–è®ºæ–‡åˆ†ç±»ï¼Œå¸¦æœ‰å¢å¼ºçš„æ–‡æœ¬å¤„ç†å’Œç½®ä¿¡åº¦è¯„åˆ†ã€‚
    
    Args:
        title (str): è®ºæ–‡æ ‡é¢˜ï¼Œç”¨äºä¸»è¦ä¸Šä¸‹æ–‡åˆ†æ
        abstract (str): è®ºæ–‡æ‘˜è¦ï¼Œç”¨äºå…¨é¢å†…å®¹åˆ†æ
        categories_config (Dict): åŒ…å«ç±»åˆ«å®šä¹‰ã€å…³é”®è¯ã€æƒé‡å’Œä¼˜å…ˆçº§çš„é…ç½®å­—å…¸
    
    å®ç°ç»†èŠ‚:
        1. å¢å¼ºæ–‡æœ¬é¢„å¤„ç†:
           - å¤§å°å†™æ ‡å‡†åŒ–å’Œæ ‡å‡†åŒ–å¤„ç†
           - æ ‡é¢˜å’Œæ‘˜è¦çš„ç»„åˆåˆ†æï¼Œä½¿ç”¨å·®å¼‚åŒ–æƒé‡
           - é«˜çº§åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
           - å¤šçº§è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
           - N-gramåˆ†æï¼Œæé«˜çŸ­è¯­åŒ¹é…å‡†ç¡®æ€§
        
        2. ä¼˜åŒ–è¯„åˆ†æœºåˆ¶:
           - ä¸»è¦å¾—åˆ†: åŠ æƒå…³é”®è¯åŒ¹é… (åŠ¨æ€åŸºç¡€æƒé‡)
           - æ ‡é¢˜åŠ æˆ: æ ‡é¢˜åŒ¹é…çš„é¢å¤–æƒé‡ (ä¼˜åŒ–åŠ æƒ)
           - ç²¾ç¡®åŒ¹é…åŠ æˆ: å®Œæ•´çŸ­è¯­åŒ¹é…çš„é¢å¤–æƒé‡
           - ä¼˜å…ˆçº§ä¹˜æ•°: ç±»åˆ«ç‰¹å®šé‡è¦æ€§ç¼©æ”¾
           - è´Ÿé¢å…³é”®è¯æƒ©ç½š: ä½¿ç”¨æ”¹è¿›çš„é€»è¾‘å‡½æ•°å¹³æ»‘æƒ©ç½š
           - ç±»åˆ«ç›¸å…³æ€§åˆ¤æ–­: è€ƒè™‘ç±»åˆ«é—´çš„ç›¸å…³æ€§
        
        3. æ™ºèƒ½åˆ†ç±»é€»è¾‘:
           - ä½¿ç”¨ç±»åˆ«è‡ªå®šä¹‰é˜ˆå€¼ä¸åŠ¨æ€é˜ˆå€¼è°ƒæ•´
           - å¢å¼ºçš„å­ç±»åˆ«åˆ†ç±»
           - ä¼˜å…ˆç±»åˆ«çš„å±‚æ¬¡åŒ–å¤„ç†
           - æ™ºèƒ½å›é€€æœºåˆ¶ï¼Œè€ƒè™‘ç±»åˆ«ç›¸å…³æ€§
           - ç½®ä¿¡åº¦è¯„åˆ†å’Œåˆ†ç±»è§£é‡Š
    
    Returns:
        List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]: æŒ‰ç½®ä¿¡åº¦é™åºæ’åºçš„ 
        (ç±»åˆ«, ç½®ä¿¡åº¦åˆ†æ•°, å­ç±»åˆ«ä¿¡æ¯, åˆ†ç±»è§£é‡Š) å…ƒç»„åˆ—è¡¨
    """
    # æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    
    # ä½¿ç”¨é«˜çº§æ–‡æœ¬é¢„å¤„ç†
    processed_title = preprocess_text(title)
    processed_abstract = preprocess_text(abstract)
    processed_combined = processed_title + " " + processed_abstract
    
    # ç§»é™¤å¸¸è§çš„åœç”¨è¯ï¼Œæé«˜åŒ¹é…è´¨é‡
    stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by', 
                 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 
                 'does', 'did', 'but', 'if', 'then', 'else', 'when', 'up', 'down', 'this', 'that'}
    
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    title_words = set(w for w in title_lower.split() if w not in stop_words)
    abstract_words = set(w for w in abstract_lower.split() if w not in stop_words)
    
    # ç»„åˆæ–‡æœ¬ç”¨äºåŒ¹é…
    combined_text = title_lower + " " + abstract_lower
    
    # åˆå§‹åŒ–å¾—åˆ†ç´¯åŠ å™¨å’ŒåŒ¹é…è®°å½•
    scores = defaultdict(float)
    match_details = defaultdict(list)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¾—åˆ†
    for category, config in categories_config.items():
        score = 0.0
        matches = []
        
        # 1. æ­£å‘å…³é”®è¯åŒ¹é…
        for keyword, weight in config["keywords"]:
            keyword_lower = keyword.lower()
            keyword_words = set(w for w in keyword_lower.split() if w not in stop_words)
            
            # å¯¹å…³é”®è¯ä¹Ÿè¿›è¡Œé¢„å¤„ç†
            processed_keyword = preprocess_text(keyword)
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if keyword_lower in title_lower:
                match_score = weight * 0.25  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æœ€é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif keyword_lower in abstract_lower:
                match_score = weight * 0.15  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æ¬¡ä¹‹
                score += match_score
                matches.append(f"æ‘˜è¦ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒåŒ¹é…ï¼ˆæé«˜å‡†ç¡®æ€§ï¼‰
            elif processed_keyword in processed_title:
                match_score = weight * 0.22  # é¢„å¤„ç†æ ‡é¢˜ä¸­çš„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif processed_keyword in processed_abstract:
                match_score = weight * 0.14  # é¢„å¤„ç†æ‘˜è¦ä¸­çš„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(title_words):
                match_score = weight * 0.18  # æ ‡é¢˜ä¸­çš„è¯ç»„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ‘˜è¦ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(abstract_words):
                match_score = weight * 0.12  # æ‘˜è¦ä¸­çš„è¯ç»„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # å•è¯åŒ¹é…ï¼ˆä½æƒé‡ï¼‰
            else:
                # å°†å…³é”®è¯æ‹†åˆ†ä¸ºå•è¯è¿›è¡ŒåŒ¹é…
                word_matches = 0
                title_match_bonus = 0
                
                # åˆ†åˆ«å¤„ç†åŸå§‹æ–‡æœ¬å’Œé¢„å¤„ç†æ–‡æœ¬çš„åŒ¹é…
                for word in keyword_words:
                    if len(word) <= 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                        continue
                        
                    if word in title_words:
                        word_matches += 1
                        title_match_bonus += 1  # æ ‡é¢˜åŒ¹é…é¢å¤–åŠ åˆ†
                    elif word in abstract_words:
                        word_matches += 0.6  # æ‘˜è¦åŒ¹é…çš„æƒé‡ä½äºæ ‡é¢˜
                
                # å¤„ç†é¢„å¤„ç†æ–‡æœ¬ä¸­çš„åŒ¹é…
                processed_keyword_words = processed_keyword.split()
                for word in processed_keyword_words:
                    if len(word) <= 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                        continue
                        
                    if word in processed_title:
                        word_matches += 0.8  # é¢„å¤„ç†æ–‡æœ¬ä¸­çš„åŒ¹é…æƒé‡ç¨ä½
                        title_match_bonus += 0.8
                    elif word in processed_abstract:
                        word_matches += 0.5
                
                # åªæœ‰å½“åŒ¹é…åˆ°è¶³å¤Ÿå¤šçš„å•è¯æ—¶æ‰è®¡ç®—å¾—åˆ†
                if word_matches > 0 and len(keyword_words) > 0:
                    # è®¡ç®—åŒ¹é…æ¯”ä¾‹
                    match_ratio = word_matches / (len(keyword_words) + len(processed_keyword_words) / 2)
                    if match_ratio >= 0.4:  # é™ä½é˜ˆå€¼ä»¥å¢åŠ çµæ´»æ€§
                        match_score = weight * 0.08 * match_ratio  # åŸºç¡€åˆ†
                        title_bonus = weight * 0.04 * (title_match_bonus / (len(keyword_words) + len(processed_keyword_words) / 2))  # æ ‡é¢˜åŠ åˆ†
                        
                        total_score = match_score + title_bonus
                        score += total_score
                        matches.append(f"å•è¯åŒ¹é… [{keyword}]: +{total_score:.2f} (åŒ¹é…ç‡: {match_ratio:.1f})")
            
            # å•è¯é¢‘ç‡åŠ æˆï¼ˆå¯¹äºé‡å¤å‡ºç°çš„å…³é”®è¯ç»™äºˆé¢å¤–åŠ æˆï¼‰
            if len(keyword_words) == 1 and keyword_lower in combined_text:
                # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
                frequency = combined_text.count(keyword_lower)
                if frequency > 1:
                    # é¢‘ç‡åŠ æˆï¼Œä½†æœ‰ä¸Šé™
                    freq_bonus = min(frequency * 0.02, 0.1) * weight
                    score += freq_bonus
                    matches.append(f"é¢‘ç‡åŠ æˆ [{keyword}] (x{frequency}): +{freq_bonus:.2f}")
        
        # 2. è´Ÿå‘å…³é”®è¯æƒ©ç½šï¼ˆæ›´ä¸¥æ ¼çš„æƒ©ç½šæœºåˆ¶ï¼‰
        if "negative_keywords" in config:
            negative_score = 0
            for keyword_tuple in config["negative_keywords"]:
                if isinstance(keyword_tuple, tuple) and len(keyword_tuple) >= 1:
                    keyword = keyword_tuple[0].lower()
                    neg_weight = keyword_tuple[1] if len(keyword_tuple) > 1 else 1.0
                else:
                    # å…¼å®¹å­—ç¬¦ä¸²æ ¼å¼
                    keyword = str(keyword_tuple).lower()
                    neg_weight = 1.0
                
                # æ£€æŸ¥è´Ÿå‘å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æœ¬ä¸­
                if keyword in combined_text:
                    # è®¡ç®—æƒ©ç½šåˆ†æ•°
                    penalty = neg_weight * 1.2
                    negative_score += penalty
                    matches.append(f"è´Ÿå‘åŒ¹é… [{keyword}]: -{penalty:.2f}")
            
            # ä½¿ç”¨æ›´å¹³æ»‘çš„æƒ©ç½šå‡½æ•°
            if negative_score > 0:
                original_score = score
                # ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è´Ÿå‘å…³é”®è¯å¤„ç†
                # æ£€æŸ¥è´Ÿå‘å…³é”®è¯çš„ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å¦å®šè¯æˆ–å¯¹ç«‹è¯
                context_adjustment = 1.0
                
                # æ£€æŸ¥å¦å®šè¯å’Œå¯¹ç«‹è¯
                negation_words = ["not", "without", "no", "non", "instead of", "rather than", "unlike"]
                opposition_words = ["but", "however", "although", "despite", "contrary to"]
                
                # å¦‚æœå­˜åœ¨å¦å®šè¯æˆ–å¯¹ç«‹è¯ï¼Œå‡å°‘æƒ©ç½š
                for neg_word in negation_words:
                    if neg_word + " " + keyword_lower in combined_text or neg_word + "-" + keyword_lower in combined_text:
                        context_adjustment = 0.5  # å¤§å¹…å‡å°‘æƒ©ç½š
                        matches.append(f"æ£€æµ‹åˆ°å¦å®šä¸Šä¸‹æ–‡: '{neg_word} {keyword_lower}', æƒ©ç½šå‡å°‘")
                        break
                
                for opp_word in opposition_words:
                    if opp_word in combined_text and combined_text.find(opp_word) < combined_text.find(keyword_lower):
                        # å¦‚æœå¯¹ç«‹è¯åœ¨å…³é”®è¯ä¹‹å‰ï¼Œå‡å°‘æƒ©ç½š
                        context_adjustment = 0.7
                        matches.append(f"æ£€æµ‹åˆ°å¯¹ç«‹ä¸Šä¸‹æ–‡: '{opp_word}... {keyword_lower}', æƒ©ç½šéƒ¨åˆ†å‡å°‘")
                        break
                
                # åº”ç”¨ä¸Šä¸‹æ–‡è°ƒæ•´
                negative_score *= context_adjustment
                
                # ä½¿ç”¨æ”¹è¿›çš„æƒ©ç½šå‡½æ•°
                # å¯¹äºè¾ƒå°çš„è´Ÿå‘åˆ†æ•°ä½¿ç”¨çº¿æ€§æƒ©ç½šï¼Œå¯¹äºè¾ƒå¤§çš„è´Ÿå‘åˆ†æ•°ä½¿ç”¨æŒ‡æ•°æƒ©ç½š
                if negative_score < 0.5:
                    # è½»å¾®è´Ÿå‘åˆ†æ•°ä½¿ç”¨çº¿æ€§æƒ©ç½š
                    penalty_factor = 1 - negative_score * 0.3
                else:
                    # è¾ƒå¤§è´Ÿå‘åˆ†æ•°ä½¿ç”¨æŒ‡æ•°æƒ©ç½š
                    penalty_factor = math.exp(-negative_score * 0.8)
                
                score *= penalty_factor
                penalty = original_score - score
                matches.append(f"è´Ÿå‘æƒ©ç½šæ€»è®¡: -{penalty:.2f} (å› å­: {penalty_factor:.2f}, ä¸Šä¸‹æ–‡è°ƒæ•´: {context_adjustment:.1f})")
        
        # 3. åº”ç”¨ç±»åˆ«ä¼˜å…ˆçº§ç¼©æ”¾
        priority = config.get("priority", 0)
        if priority > 0:
            priority_bonus = score * (priority * 0.12)  # ä¼˜å…ˆçº§åŠ æˆæ›´æ˜æ˜¾
            score += priority_bonus
            matches.append(f"ä¼˜å…ˆçº§åŠ æˆ (çº§åˆ« {priority}): +{priority_bonus:.2f}")
        
        # è®°å½•å¾—åˆ†å’ŒåŒ¹é…è¯¦æƒ…
        if score > 0:
            scores[category] = score
            match_details[category] = matches
    
    # 4. åˆ†ç±»å†³ç­–é€»è¾‘
    # éªŒè¯æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
    max_score = max(scores.values()) if scores else 0
    if max_score < 0.05:  # è¿›ä¸€æ­¥é™ä½æœ€ä½ç½®ä¿¡åº¦è¦æ±‚ï¼Œä»0.08é™ä½åˆ°0.05
        return []
    
    # ä½¿ç”¨ç±»åˆ«è‡ªå®šä¹‰é˜ˆå€¼è¿›è¡Œåˆ†ç±»
    from categories_config import CATEGORY_THRESHOLDS
    
    # å¤„ç†é«˜ä¼˜å…ˆçº§ç±»åˆ«ï¼ˆåŒ…å«æ‰€æœ‰ä¸»è¦ç±»åˆ«ï¼‰
    high_priority_categories = [
        "è§†è§‰è¡¨å¾ä¸åŸºç¡€æ¨¡å‹ (Visual Representation & Foundation Models)",
        "ç”Ÿæˆå¼è§†è§‰æ¨¡å‹ (Generative Visual Modeling)",
        "è§†è§‰-è¯­è¨€ååŒç†è§£ (Vision-Language Joint Understanding)",
        "è§†è§‰è¯†åˆ«ä¸ç†è§£ (Visual Recognition & Understanding)",
        "é¢†åŸŸç‰¹å®šè§†è§‰åº”ç”¨ (Domain-specific Visual Applications)",
        "ä¸‰ç»´è§†è§‰ä¸å‡ ä½•æ¨ç† (3D Vision & Geometric Reasoning)",
        "æ—¶åºè§†è§‰åˆ†æ (Temporal Visual Analysis)",
        "è‡ªç›‘ç£ä¸è¡¨å¾å­¦ä¹  (Self-supervised & Representation Learning)",
        "è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ– (Computational Efficiency & Model Optimization)",
        "é²æ£’æ€§ä¸å¯é æ€§ (Robustness & Reliability)",
        "ä½èµ„æºä¸é«˜æ•ˆå­¦ä¹  (Low-resource & Efficient Learning)",
        "å…·èº«æ™ºèƒ½ä¸äº¤äº’è§†è§‰ (Embodied Intelligence & Interactive Vision)",
        "æ–°å…´ç†è®ºä¸è·¨å­¦ç§‘æ–¹å‘ (Emerging Theory & Interdisciplinary Directions)"
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åº”ç”¨ç±»åˆ«çš„ç‰¹å¾
    application_category = "é¢†åŸŸç‰¹å®šè§†è§‰åº”ç”¨ (Domain-specific Visual Applications)"
    has_application_features = False
    application_score = 0
    application_subcategory = None
    
    # å¦‚æœåº”ç”¨ç±»åˆ«æœ‰è¶³å¤Ÿçš„å¾—åˆ†ï¼Œåˆ™è®¤ä¸ºæœ‰åº”ç”¨ç‰¹å¾ - è°ƒæ•´é˜ˆå€¼ä¸º0.35ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œè¦†ç›–ç‡
    if application_category in scores and scores[application_category] >= 0.35:
        has_application_features = True
        application_score = scores[application_category]
        # å°è¯•è·å–åº”ç”¨ç±»åˆ«çš„å­ç±»åˆ«
        application_subcategory = get_subcategory(title, abstract, application_category, application_score)
        
        # åˆ›å»ºåˆ†ç±»è§£é‡Š
        explanation = {
            "reason": "è¯¥è®ºæ–‡å…·æœ‰æ˜æ˜¾çš„åº”ç”¨ç‰¹å¾",
            "score": round(application_score, 4),
            "threshold": 0.35,
            "key_matches": match_details.get(application_category, [])[:5],
            "decision_method": "åº”ç”¨ç±»åˆ«å¼ºåˆ¶åˆ¤æ–­"
        }
        
        # å¦‚æœæœ‰åº”ç”¨ç‰¹å¾ï¼Œç›´æ¥è¿”å›åº”ç”¨ç±»åˆ«åŠè§£é‡Š
        return [(application_category, application_score, application_subcategory, explanation)]
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨é«˜ä¼˜å…ˆçº§ç±»åˆ«ï¼ˆå¤§å¹…é™ä½é˜ˆå€¼ï¼‰
    result_with_subcategories = []
    
    for category in high_priority_categories:
        if category in scores and category in CATEGORY_THRESHOLDS:
            category_score = scores[category]
            threshold = CATEGORY_THRESHOLDS[category]["threshold"]
            # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå¤æ‚åº¦è°ƒæ•´é˜ˆå€¼
            # è®¡ç®—æ–‡æœ¬å¤æ‚åº¦å› å­
            text_length = len(title) + len(abstract)
            complexity_factor = 1.0
            
            # è¾ƒçŸ­æ–‡æœ¬éœ€è¦æ›´é«˜çš„é˜ˆå€¼ï¼ˆå› ä¸ºå…³é”®è¯å¯†åº¦æ›´é«˜ï¼‰
            if text_length < 500:
                complexity_factor = 1.2
            elif text_length > 2000:
                complexity_factor = 0.9  # è¾ƒé•¿æ–‡æœ¬éœ€è¦æ›´å®½æ¾çš„é˜ˆå€¼
            
            # è®¡ç®—å…³é”®è¯å¯†åº¦ï¼ˆåŒ¹é…çš„å…³é”®è¯æ•°é‡é™¤ä»¥æ–‡æœ¬é•¿åº¦ï¼‰
            keyword_density = len(match_details.get(category, [])) / (text_length / 100) if text_length > 0 else 0
            density_factor = 1.0
            
            if keyword_density > 1.5:  # å…³é”®è¯å¯†åº¦é«˜
                density_factor = 0.9  # é™ä½é˜ˆå€¼è¦æ±‚
            elif keyword_density < 0.5:  # å…³é”®è¯å¯†åº¦ä½
                density_factor = 1.1  # æé«˜é˜ˆå€¼è¦æ±‚
            
            # è®¡ç®—åŠ¨æ€é˜ˆå€¼ç³»æ•°
            dynamic_threshold_factor = 0.35 * complexity_factor * density_factor
            
            # åº”ç”¨åŠ¨æ€é˜ˆå€¼
            if category_score >= threshold * dynamic_threshold_factor and category_score >= 0.10:
                # å°è¯•è·å–å­ç±»åˆ«
                subcategory = get_subcategory(title, abstract, category, category_score)
                # ä¼˜å…ˆè¿”å›æœ‰å­ç±»åˆ«çš„ç»“æœ
                if subcategory:
                    return [(category, category_score, subcategory)]
                # å¦‚æœæ²¡æœ‰å­ç±»åˆ«ï¼Œå…ˆä¿å­˜ç»“æœï¼Œç»§ç»­å¯»æ‰¾å…¶ä»–å¯èƒ½æœ‰å­ç±»åˆ«çš„ç±»åˆ«
                result_with_subcategories.append((category, category_score, None))
    
    # æ”¶é›†å€™é€‰ç±»åˆ«
    candidate_categories = []
    
    # å°†é«˜ä¼˜å…ˆçº§ç±»åˆ«çš„ç»“æœæ·»åŠ åˆ°å€™é€‰ç±»åˆ«ä¸­
    if result_with_subcategories:
        candidate_categories.extend(result_with_subcategories)
    
    # å¤„ç†æ‰€æœ‰ç±»åˆ«ï¼Œæ”¶é›†å€™é€‰ç±»åˆ«
    for category, score in scores.items():
        # è·³è¿‡åº”ç”¨ç±»åˆ«ï¼Œå› ä¸ºå®ƒå·²ç»åœ¨å‰é¢å¤„ç†è¿‡äº†
        if category == application_category:
            continue
            
        if category in CATEGORY_THRESHOLDS:
            threshold = CATEGORY_THRESHOLDS[category]["threshold"]
            # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼æ”¶é›†å€™é€‰ç±»åˆ«
            if score >= threshold * 0.3:  
                # å°è¯•è·å–å­ç±»åˆ«
                subcategory = get_subcategory(title, abstract, category, score)
                candidate_categories.append((category, score, subcategory))
        else:
            # å¯¹äºæ²¡æœ‰å®šä¹‰é˜ˆå€¼çš„ç±»åˆ«ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç›¸å¯¹é˜ˆå€¼
            if score >= max_score * 0.2:  
                # å°è¯•è·å–å­ç±»åˆ«
                subcategory = get_subcategory(title, abstract, category, score)
                candidate_categories.append((category, score, subcategory))
    
    # å¦‚æœæœ‰å€™é€‰ç±»åˆ«ï¼Œä½¿ç”¨ChatGLMåšå‡ºæœ€ç»ˆå†³ç­–
    if candidate_categories:
        # æŒ‰å¾—åˆ†é™åºæ’åºå€™é€‰ç±»åˆ«
        sorted_candidates = sorted(candidate_categories, key=lambda x: x[1], reverse=True)
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå€™é€‰ç±»åˆ«ï¼Œç›´æ¥è¿”å›
        if len(sorted_candidates) == 1:
            return [sorted_candidates[0]]
        
        # å¦‚æœæœ‰å¤šä¸ªå€™é€‰ç±»åˆ«ï¼Œä½¿ç”¨ChatGLMåšå‡ºå†³ç­–
        try:
            from chatglm_helper import ChatGLMHelper
            chatglm_helper = ChatGLMHelper()
            
            # ä½¿ç”¨ChatGLMå†³ç­–æœ€ç»ˆç±»åˆ«
            final_category = chatglm_helper.decide_category(title, abstract, sorted_candidates)
            
            # æ‰¾åˆ°å¯¹åº”çš„å€™é€‰ç±»åˆ«å…ƒç»„
            for candidate in sorted_candidates:
                if candidate[0] == final_category:
                    return [candidate]
            
            # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„å€™é€‰ç±»åˆ«ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„
            return [sorted_candidates[0]]
        except Exception as e:
            print(f"ChatGLMå†³ç­–åˆ†ç±»å‡ºé”™: {str(e)}")
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„å€™é€‰ç±»åˆ«
            return [sorted_candidates[0]]
    
    # å¦‚æœæ²¡æœ‰å€™é€‰ç±»åˆ«ï¼Œä½¿ç”¨æœ€ç®€å•çš„å›é€€æœºåˆ¶
    if scores:
        # æŒ‰å¾—åˆ†é™åºæ’åºæ‰€æœ‰ç±»åˆ«
        all_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        top_category, top_score = all_categories[0]
        
        # å¦‚æœæœ€é«˜å¾—åˆ†è¶…è¿‡ä¸€ä¸ªæœ€ä½é˜ˆå€¼
        if top_score >= 0.15:
            # å°è¯•è·å–å­ç±»åˆ«
            subcategory = get_subcategory(title, abstract, top_category, top_score)
            
            # åˆ›å»ºåˆ†ç±»è§£é‡Š
            explanation = {
                "reason": "æ²¡æœ‰åŒ¹é…åˆ°æ˜¾è‘—ç±»åˆ«ï¼Œä½¿ç”¨å¾—åˆ†æœ€é«˜çš„ç±»åˆ«",
                "score": round(top_score, 4),
                "threshold": 0.15,
                "key_matches": match_details.get(top_category, [])[:5],
                "decision_method": "å›é€€åˆ†ç±»æœºåˆ¶"
            }
            
            return [(top_category, top_score, subcategory, explanation)]
    
    # å¦‚æœæœ‰å€™é€‰ç±»åˆ«ï¼Œä½¿ç”¨ChatGLMåšå‡ºæœ€ç»ˆå†³ç­–
    if candidate_categories:
        # æŒ‰å¾—åˆ†é™åºæ’åºå€™é€‰ç±»åˆ«
        sorted_candidates = sorted(candidate_categories, key=lambda x: x[1], reverse=True)
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå€™é€‰ç±»åˆ«ï¼Œç›´æ¥è¿”å›
        if len(sorted_candidates) == 1:
            return [sorted_candidates[0]]
        
        # å¦‚æœæœ‰å¤šä¸ªå€™é€‰ç±»åˆ«ï¼Œä½¿ç”¨ChatGLMåšå‡ºå†³ç­–
        try:
            from chatglm_helper import ChatGLMHelper
            chatglm_helper = ChatGLMHelper()
            
            # ä½¿ç”¨ChatGLMå†³ç­–æœ€ç»ˆç±»åˆ«
            final_category = chatglm_helper.decide_category(title, abstract, sorted_candidates)
            
            # æ‰¾åˆ°å¯¹åº”çš„å€™é€‰ç±»åˆ«å…ƒç»„
            for candidate in sorted_candidates:
                if candidate[0] == final_category:
                    return [candidate]
            
            # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„å€™é€‰ç±»åˆ«ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„
            return [sorted_candidates[0]]
        except Exception as e:
            print(f"ChatGLMå†³ç­–åˆ†ç±»å‡ºé”™: {str(e)}")
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„å€™é€‰ç±»åˆ«
            return [sorted_candidates[0]]
        
    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
    return []


def calculate_category_relation(category1, category2, categories_config):
    """
    è®¡ç®—ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§
    
    Args:
        category1: ç¬¬ä¸€ä¸ªç±»åˆ«åç§°
        category2: ç¬¬äºŒä¸ªç±»åˆ«åç§°
        categories_config: ç±»åˆ«é…ç½®å­—å…¸
        
    Returns:
        float: ç›¸å…³æ€§åˆ†æ•° (0-1)ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç›¸å…³
    """
    # å¦‚æœç±»åˆ«ç›¸åŒï¼Œç›¸å…³æ€§ä¸º1
    if category1 == category2:
        return 1.0
    
    # è·å–ä¸¤ä¸ªç±»åˆ«çš„å…³é”®è¯
    keywords1 = set()
    keywords2 = set()
    
    if category1 in categories_config and "keywords" in categories_config[category1]:
        keywords1 = {kw[0].lower() for kw in categories_config[category1]["keywords"] if isinstance(kw, tuple)}
    
    if category2 in categories_config and "keywords" in categories_config[category2]:
        keywords2 = {kw[0].lower() for kw in categories_config[category2]["keywords"] if isinstance(kw, tuple)}
    
    # å¦‚æœä»»ä¸€ç±»åˆ«æ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›0
    if not keywords1 or not keywords2:
        return 0.0
    
    # è®¡ç®—å…³é”®è¯é‡å 
    overlap = keywords1.intersection(keywords2)
    
    # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—ç›¸å…³æ€§
    similarity = len(overlap) / len(keywords1.union(keywords2))
    
    # é¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    related_pairs = [
        ("è§†è§‰è¡¨å¾ä¸åŸºç¡€æ¨¡å‹", "è‡ªç›‘ç£ä¸è¡¨å¾å­¦ä¹ "),
        ("è§†è§‰è¯†åˆ«ä¸ç†è§£", "ä¸‰ç»´è§†è§‰ä¸å‡ ä½•æ¨ç†"),
        ("ç”Ÿæˆå¼è§†è§‰æ¨¡å‹", "è§†è§‰-è¯­è¨€ååŒç†è§£"),
        ("æ—¶åºè§†è§‰åˆ†æ", "å…·èº«æ™ºèƒ½ä¸äº¤äº’è§†è§‰"),
        ("è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ–", "é²æ£’æ€§ä¸å¯é æ€§"),
        ("ä½èµ„æºä¸é«˜æ•ˆå­¦ä¹ ", "è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ–")
    ]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    for pair in related_pairs:
        if (category1.startswith(pair[0]) and category2.startswith(pair[1])) or \
           (category1.startswith(pair[1]) and category2.startswith(pair[0])):
            # å¢åŠ ç›¸å…³æ€§åˆ†æ•°
            similarity += 0.2
            break
    
    return min(similarity, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1


def get_subcategory(title: str, abstract: str, main_category: str, main_score: float) -> Optional[Tuple[str, float]]:
    """
    åœ¨ç¡®å®šä¸»ç±»åˆ«åï¼Œè¿›ä¸€æ­¥ç¡®å®šå­ç±»åˆ«
    
    Args:
        title: è®ºæ–‡æ ‡é¢˜
        abstract: è®ºæ–‡æ‘˜è¦
        main_category: ä¸»ç±»åˆ«
        main_score: ä¸»ç±»åˆ«å¾—åˆ†
        
    Returns:
        Optional[Tuple[str, float]]: å­ç±»åˆ«åŠå…¶å¾—åˆ†ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›None
    """
    # å¢å¼ºçš„æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    combined_text = title_lower + " " + abstract_lower
    
    # ä½¿ç”¨NLTKè¿›è¡Œå¤šçº§æ–‡æœ¬é¢„å¤„ç†
    processed_title = preprocess_text(title)
    processed_abstract = preprocess_text(abstract)
    processed_combined = processed_title + " " + processed_abstract
    
    # åˆ›å»ºN-gramç‰ˆæœ¬çš„æ–‡æœ¬ç”¨äºçŸ­è¯­åŒ¹é…
    # è¿™æœ‰åŠ©äºæ•è·å¤šè¯çŸ­è¯­ï¼Œå³ä½¿å®ƒä»¬çš„é¡ºåºæˆ–å½¢å¼ç•¥æœ‰ä¸åŒ
    from nltk.util import ngrams
    import re
    
    # æ¸…ç†å¹¶æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºN-gramå¤„ç†
    clean_title = re.sub(r'[^\w\s]', ' ', title_lower)
    clean_abstract = re.sub(r'[^\w\s]', ' ', abstract_lower)
    
    # ç”Ÿæˆ2-gramå’Œ3-gram
    title_words = clean_title.split()
    abstract_words = clean_abstract.split()
    
    # ç”Ÿæˆ2-gram
    title_bigrams = [' '.join(ng) for ng in ngrams(title_words, 2)] if len(title_words) >= 2 else []
    abstract_bigrams = [' '.join(ng) for ng in ngrams(abstract_words, 2)] if len(abstract_words) >= 2 else []
    
    # ç”Ÿæˆ3-gram
    title_trigrams = [' '.join(ng) for ng in ngrams(title_words, 3)] if len(title_words) >= 3 else []
    abstract_trigrams = [' '.join(ng) for ng in ngrams(abstract_words, 3)] if len(abstract_words) >= 3 else []
    
    # åˆå¹¶æ‰€æœ‰N-gram
    all_ngrams = set(title_bigrams + abstract_bigrams + title_trigrams + abstract_trigrams)
    
    # æ£€æŸ¥ä¸»ç±»åˆ«æ˜¯å¦æœ‰å­ç±»åˆ«å®šä¹‰
    if main_category in CATEGORY_THRESHOLDS and "subcategories" in CATEGORY_THRESHOLDS[main_category]:
        subcategories = CATEGORY_THRESHOLDS[main_category]["subcategories"]
        
        # è®¡ç®—æ¯ä¸ªå­ç±»åˆ«çš„å¾—åˆ†
        subcategory_scores = {}
        for subcategory_name, subcategory_threshold in subcategories.items():
            # æå–å­ç±»åˆ«åç§°ä¸­çš„å…³é”®è¯
            subcategory_keywords = subcategory_name.lower().split()
            score = 0.0
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if subcategory_name.lower() in combined_text:
                score += 3.5  # å¤§å¹…å¢åŠ ç²¾ç¡®åŒ¹é…çš„æƒé‡ï¼Œä»2.5æé«˜åˆ°3.5
            elif subcategory_name.lower() in title_lower:
                score += 4.0  # å¦‚æœå­ç±»åˆ«åç§°ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç»™äºˆæ›´é«˜æƒé‡
            
            # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒåŒ¹é…
            processed_subcategory = preprocess_text(subcategory_name)
            if processed_subcategory in processed_combined:
                score += 2.5  # å¢åŠ é¢„å¤„ç†æ–‡æœ¬åŒ¹é…çš„æƒé‡ï¼Œä»1.8æé«˜åˆ°2.5
            elif processed_subcategory in processed_title:
                score += 3.0  # å¦‚æœé¢„å¤„ç†åçš„å­ç±»åˆ«åç§°å‡ºç°åœ¨æ ‡é¢˜ä¸­
            
            # å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
            for keyword in subcategory_keywords:
                if len(keyword) > 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                    # åŸå§‹æ–‡æœ¬åŒ¹é…
                    if keyword in title_lower:
                        score += 1.5  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…
                    elif keyword in abstract_lower:
                        score += 0.8  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…
                    
                    # é¢„å¤„ç†æ–‡æœ¬åŒ¹é…
                    processed_keyword = preprocess_text(keyword)
                    if processed_keyword in processed_title:
                        score += 1.2  # é¢„å¤„ç†åçš„æ ‡é¢˜åŒ¹é…
                    elif processed_keyword in processed_abstract:
                        score += 0.6  # é¢„å¤„ç†åçš„æ‘˜è¦åŒ¹é…
                    
                    # N-gramåŒ¹é…ï¼ˆæ•è·çŸ­è¯­å˜ä½“ï¼‰
                    for ngram in all_ngrams:
                        if keyword in ngram:
                            score += 0.4  # N-gramä¸­çš„å…³é”®è¯åŒ¹é…
                            break
                    
                    # è¯æ ¹åŒ¹é…ï¼ˆå¤„ç†è¯å½¢å˜åŒ–ï¼‰
                    keyword_root = preprocess_text(keyword)
                    for word in processed_title.split():
                        if keyword_root in word and len(keyword_root) > 4:  # ç¡®ä¿è¶³å¤Ÿé•¿ä»¥é¿å…è¯¯åŒ¹é…
                            score += 0.3
                            break
                    for word in processed_abstract.split():
                        if keyword_root in word and len(keyword_root) > 4:
                            score += 0.2
                            break
            
            # å¤§å¹…é™ä½å­ç±»åˆ«é˜ˆå€¼ï¼Œç¡®ä¿å¤§å¤šæ•°è®ºæ–‡èƒ½è¢«åˆ†é…åˆ°å­ç±»åˆ«
            if score > 0:
                # å­ç±»åˆ«å¾—åˆ†éœ€è¦è¾¾åˆ°ä¸»ç±»åˆ«å¾—åˆ†çš„ä¸€å®šæ¯”ä¾‹ï¼Œä½†å¤§å¹…é™ä½è¦æ±‚
                relative_threshold = main_score * 0.15 * subcategory_threshold  # ä»0.25é™ä½åˆ°0.15
                if score >= relative_threshold or score >= 0.5:  # æ·»åŠ ç»å¯¹åˆ†æ•°é˜ˆå€¼
                    subcategory_scores[subcategory_name] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„å­ç±»åˆ«
        if subcategory_scores:
            best_subcategory = max(subcategory_scores.items(), key=lambda x: x[1])
            return best_subcategory
    
    return None


def calculate_category_relation(category1, category2, categories_config):
    """
    è®¡ç®—ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§
    
    Args:
        category1: ç¬¬ä¸€ä¸ªç±»åˆ«åç§°
        category2: ç¬¬äºŒä¸ªç±»åˆ«åç§°
        categories_config: ç±»åˆ«é…ç½®å­—å…¸
        
    Returns:
        float: ç›¸å…³æ€§åˆ†æ•° (0-1)ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç›¸å…³
    """
    # å¦‚æœç±»åˆ«ç›¸åŒï¼Œç›¸å…³æ€§ä¸º1
    if category1 == category2:
        return 1.0
    
    # è·å–ä¸¤ä¸ªç±»åˆ«çš„å…³é”®è¯
    keywords1 = set()
    keywords2 = set()
    
    if category1 in categories_config and "keywords" in categories_config[category1]:
        keywords1 = {kw[0].lower() for kw in categories_config[category1]["keywords"] if isinstance(kw, tuple)}
    
    if category2 in categories_config and "keywords" in categories_config[category2]:
        keywords2 = {kw[0].lower() for kw in categories_config[category2]["keywords"] if isinstance(kw, tuple)}
    
    # å¦‚æœä»»ä¸€ç±»åˆ«æ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›0
    if not keywords1 or not keywords2:
        return 0.0
    
    # è®¡ç®—å…³é”®è¯é‡å 
    overlap = keywords1.intersection(keywords2)
    
    # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—ç›¸å…³æ€§
    similarity = len(overlap) / len(keywords1.union(keywords2))
    
    # é¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    related_pairs = [
        ("è§†è§‰è¡¨å¾ä¸åŸºç¡€æ¨¡å‹", "è‡ªç›‘ç£ä¸è¡¨å¾å­¦ä¹ "),
        ("è§†è§‰è¯†åˆ«ä¸ç†è§£", "ä¸‰ç»´è§†è§‰ä¸å‡ ä½•æ¨ç†"),
        ("ç”Ÿæˆå¼è§†è§‰æ¨¡å‹", "è§†è§‰-è¯­è¨€ååŒç†è§£"),
        ("æ—¶åºè§†è§‰åˆ†æ", "å…·èº«æ™ºèƒ½ä¸äº¤äº’è§†è§‰"),
        ("è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ–", "é²æ£’æ€§ä¸å¯é æ€§"),
        ("ä½èµ„æºä¸é«˜æ•ˆå­¦ä¹ ", "è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹ä¼˜åŒ–")
    ]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„å®šä¹‰çš„ç›¸å…³ç±»åˆ«å¯¹
    for pair in related_pairs:
        if (category1.startswith(pair[0]) and category2.startswith(pair[1])) or \
           (category1.startswith(pair[1]) and category2.startswith(pair[0])):
            # å¢åŠ ç›¸å…³æ€§åˆ†æ•°
            similarity += 0.2
            break
    
    return min(similarity, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated

        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None

        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)
        
        # åˆå§‹åŒ–é»˜è®¤å€¼ï¼Œé¿å…å¼‚å¸¸æ—¶æœªå®šä¹‰
        github_link = "None"
        category = "å…¶ä»– (Others)"  # ä¿®æ”¹é»˜è®¤å€¼ä¸ºå¸¦è‹±æ–‡çš„æ ¼å¼
        subcategory = "æœªæŒ‡å®š"
        title_cn = f"[ç¿»è¯‘å¤±è´¥] {title}"
        analysis = {}

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        try:
            with ThreadPoolExecutor(max_workers=2) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                github_future = executor.submit(extract_github_link, abstract)
                analysis_future = executor.submit(
                    glm_helper.analyze_paper_contribution, title, abstract)
                title_cn_future = executor.submit(
                    glm_helper.translate_title, title)

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                github_link = github_future.result() or "None"
                analysis = analysis_future.result() or {}
                title_cn = title_cn_future.result() or f"[ç¿»è¯‘å¤±è´¥] {title}"
        except Exception as e:
            print(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            # ç»§ç»­å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        
        # ä½¿ç”¨åŸºäºå…³é”®è¯çš„åˆ†ç±»æ–¹æ³•
        try:
            # ä½¿ç”¨æ–°çš„åˆ†ç±»å‡½æ•°
            from categories_config import CATEGORY_KEYWORDS
            category_results = get_category_by_keywords(title, abstract, CATEGORY_KEYWORDS)
            
            if category_results:
                # è·å–ä¸»ç±»åˆ«å’Œå¾—åˆ†
                result_item = category_results[0]
                
                # å…¼å®¹å¤šç§è¿”å›æ ¼å¼ï¼š(category, score) æˆ– (category, score, subcategory) æˆ– (category, score, subcategory, explanation)
                if len(result_item) >= 4:  # æ–°æ ¼å¼ï¼ŒåŒ…å«è§£é‡Š
                    main_category, main_score, sub_category_tuple, explanation = result_item
                elif len(result_item) == 3:  # æ—§æ ¼å¼ï¼ŒåŒ…å«å­ç±»åˆ«
                    main_category, main_score, sub_category_tuple = result_item
                    explanation = None
                else:  # æœ€ç®€å•çš„æ ¼å¼
                    main_category, main_score = result_item
                    sub_category_tuple = None
                    explanation = None
                    
                category = main_category
                
                # å¤„ç†å­ç±»åˆ«
                if sub_category_tuple:
                    subcategory_name, subcategory_score = sub_category_tuple
                    subcategory = subcategory_name
                else:
                    subcategory = "æœªæŒ‡å®š"
                    
                # ä¸è¾“å‡ºåˆ†ç±»ç»“æœä¿¡æ¯ï¼Œå‡å°‘æ—¥å¿—å¹²æ‰°
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
                category = "å…¶ä»– (Others)"
                subcategory = "æœªæŒ‡å®š"
        except Exception as e:
            print(f"åˆ†ç±»è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()
            category = "å…¶ä»– (Others)"
            subcategory = "æœªæŒ‡å®š"

        paper_info = {
            'title': title,
            'title_zh': title_cn,  # ä¿®æ”¹é”®åä¸º title_zh ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'abstract': abstract,
            'authors': authors_str,
            'pdf_url': pdf_url,
            'github_url': github_link,  # ä¿®æ”¹é”®åä¸º github_url ä»¥åŒ¹é…å…¶ä»–å‡½æ•°
            'url': paper_url,  # æ·»åŠ  arxiv URL
            'category': category,
            'subcategory': subcategory,  # æ·»åŠ å­ç±»åˆ«ä¿¡æ¯
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

        # åˆå¹¶åˆ†æç»“æœ
        if analysis:
            paper_info.update(analysis)

        return paper_info

    except Exception as e:
        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return None


def get_cv_papers():
    """è·å–CVé¢†åŸŸè®ºæ–‡å¹¶ä¿å­˜ä¸ºMarkdown"""
    print("\n" + "="*50)
    print(f"å¼€å§‹è·å–CVè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # è·å–ç›®æ ‡æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
        target_date = (datetime.now() - timedelta(days=QUERY_DAYS_AGO)).date()
        print(f"\nğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"ğŸ“Š æœ€å¤§è®ºæ–‡æ•°: {MAX_RESULTS}")
        print(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}\n")

        # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
        print("ğŸ¤– åˆå§‹åŒ–ChatGLMåŠ©æ‰‹...")
        glm_helper = ChatGLMHelper()

        # åˆå§‹åŒ–arxivå®¢æˆ·ç«¯
        print("ğŸ”„ åˆå§‹åŒ–arXivå®¢æˆ·ç«¯...")
        client = arxiv.Client(
            page_size=100,  # æ¯é¡µè·å–100ç¯‡è®ºæ–‡
            delay_seconds=3,  # è¯·æ±‚é—´éš”3ç§’
            num_retries=5    # å¤±è´¥é‡è¯•5æ¬¡
        )

        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query='cat:cs.CV',  # è®¡ç®—æœºè§†è§‰ç±»åˆ«
            max_results=MAX_RESULTS,
            sort_by=arxiv.SortCriterion.LastUpdatedDate,
            sort_order=arxiv.SortOrder.Descending  # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åº
        )

        # åˆ›å»ºçº¿ç¨‹æ± 
        total_papers = 0
        papers_by_category = defaultdict(list)
        
        # ç¡®ä¿"å…¶ä»– (Others)"ç±»åˆ«æ€»æ˜¯å­˜åœ¨
        papers_by_category["å…¶ä»– (Others)"]  # åˆå§‹åŒ–ç©ºåˆ—è¡¨

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # åˆ›å»ºè¿›åº¦æ¡
            print("\nğŸ” å¼€å§‹è·å–è®ºæ–‡...")
            results = client.results(search)
            
            # åˆ›å»ºæ€»è¿›åº¦æ¡
            total_pbar = tqdm(
                total=MAX_RESULTS,
                desc="æ€»è¿›åº¦",
                unit="ç¯‡",
                position=0,
                leave=True
            )
            
            # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
            batch_pbar = tqdm(
                total=0,  # åˆå§‹å€¼ä¸º0ï¼Œåé¢ä¼šæ›´æ–°
                desc="å½“å‰æ‰¹æ¬¡",
                unit="ç¯‡",
                position=1,
                leave=True
            )
            
            # æ‰¹é‡å¤„ç†è®ºæ–‡
            batch_size = 10  # æ¯æ‰¹å¤„ç†10ç¯‡è®ºæ–‡
            papers = []
            futures = []
            
            for i, paper in enumerate(results):
                papers.append(paper)
                
                # å½“æ”¶é›†åˆ°ä¸€æ‰¹è®ºæ–‡æˆ–è¾¾åˆ°æœ€å¤§æ•°é‡æ—¶å¤„ç†
                if len(papers) >= batch_size or i >= MAX_RESULTS - 1:
                    batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                    batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                    
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    batch_futures = [
                        executor.submit(process_paper, paper, glm_helper, target_date)
                        for paper in papers
                    ]
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                    for future in as_completed(batch_futures):
                        paper_info = future.result()
                        if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                            total_papers += 1
                            category = paper_info['category']
                            papers_by_category[category].append(paper_info)
                            total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                        batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡
                    papers = []
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢è·å–
                if i >= MAX_RESULTS - 1:
                    break
            
            # å¤„ç†å‰©ä½™çš„è®ºæ–‡
            if papers:
                batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                batch_pbar.total = len(papers)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = [
                    executor.submit(process_paper, paper, glm_helper, target_date)
                    for paper in papers
                ]
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    paper_info = future.result()
                    if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                        total_papers += 1
                        category = paper_info['category']
                        papers_by_category[category].append(paper_info)
                        total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                    batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦

            # å…³é—­è¿›åº¦æ¡
            batch_pbar.close()
            total_pbar.close()

        if total_papers == 0:
            print(f"æ²¡æœ‰æ‰¾åˆ°{target_date}å‘å¸ƒçš„è®ºæ–‡ã€‚")
            return

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"{'='*50}")
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
        sorted_categories = sorted(
            papers_by_category.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # ç»Ÿè®¡æ€»è®ºæ–‡æ•°
        total_papers = sum(len(papers) for papers in papers_by_category.values())
        
        # ä¸€çº§åˆ†ç±»å’Œå¯¹åº”çš„äºŒçº§åˆ†ç±»ç»Ÿè®¡
        for category, papers in sorted_categories:
            # å³ä½¿æ²¡æœ‰è®ºæ–‡ï¼Œä¹Ÿè¦æ‰“å°"å…¶ä»–"ç±»åˆ«
            if len(papers) == 0 and category != "å…¶ä»– (Others)":
                continue
                
            # æ‰“å°ä¸€çº§åˆ†ç±»æ ‡é¢˜
            print(f"\nã€{category}ã€‘")
            
            # å¦‚æœä¸æ˜¯"å…¶ä»–"ç±»åˆ«ï¼Œå°†æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç§»åŠ¨åˆ°"å…¶ä»–"ç±»åˆ«
            if category != "å…¶ä»– (Others)":
                # åˆ†ç¦»æœ‰å­ç±»åˆ«çš„è®ºæ–‡å’Œæ— å­ç±»åˆ«çš„è®ºæ–‡
                papers_with_subcategory = []
                papers_without_subcategory = []
                
                for paper in list(papers):  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨éå†æ—¶ä¿®æ”¹
                    subcategory = paper.get('subcategory', '')
                    if not subcategory or subcategory == "æœªæŒ‡å®š":
                        papers_without_subcategory.append(paper)
                        # å°†æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç§»åŠ¨åˆ°"å…¶ä»–"ç±»åˆ«
                        papers_by_category["å…¶ä»– (Others)"].append(paper)
                        papers.remove(paper)  # ä»å½“å‰ç±»åˆ«ä¸­ç§»é™¤
                    else:
                        papers_with_subcategory.append(paper)
            
            # å¦‚æœå½“å‰ç±»åˆ«ä¸‹æ²¡æœ‰è®ºæ–‡ï¼Œè·³è¿‡
            if len(papers) == 0 and category != "å…¶ä»– (Others)":
                continue
            
            # æŒ‰å­ç±»åˆ«åˆ†ç»„è®ºæ–‡
            papers_by_subcategory = defaultdict(list)
            for paper in papers:  # ä½¿ç”¨æ›´æ–°åçš„papers
                subcategory = paper.get('subcategory', '')
                if subcategory and subcategory != "æœªæŒ‡å®š":
                    papers_by_subcategory[subcategory].append(paper)
                else:
                    # å¯¹äºæ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ï¼Œå¦‚æœæ˜¯"å…¶ä»–"ç±»åˆ«ï¼Œæ˜¾ç¤ºä¸º"æœªåˆ†ç±»"
                    papers_by_subcategory["æœªåˆ†ç±»"].append(paper)
            
            # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
            sorted_subcategories = sorted(
                papers_by_subcategory.items(),
                key=lambda x: len(x[1]),
                reverse=True
            )
            
            # æ‰“å°ä¸€çº§åˆ†ç±»æ€»æ•°
            num_new = sum(1 for p in papers if not p['is_updated'])
            num_updated = sum(1 for p in papers if p['is_updated'])
            print(f"æ€»è®¡: {len(papers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
            
            # æ‰“å°å­ç±»åˆ«ç»Ÿè®¡
            for subcategory, subpapers in sorted_subcategories:
                num_new = sum(1 for p in subpapers if not p['is_updated'])
                num_updated = sum(1 for p in subpapers if p['is_updated'])
                print(
                    f"â””â”€ {subcategory:15s}: {len(subpapers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
            
            # ä¸å†æ‰“å°"ç›´æ¥å½’ç±»"ï¼Œå› ä¸ºè¿™äº›è®ºæ–‡å·²ç»è¢«ç§»åŠ¨åˆ°"å…¶ä»– (Others)"ç±»åˆ«ä¸­
        
        print(f"\n{'='*50}")
        print(f"æ€»è®¡: {total_papers} ç¯‡")
        
        # ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶...")
        save_papers_to_markdown(papers_by_category, target_date)
        
        print("\n" + "="*50)
        print(f"CVè®ºæ–‡è·å–å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50 + "\n")

    except Exception as e:
        print("\nâŒ å¤„ç†CVè®ºæ–‡æ—¶å‡ºé”™:")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        raise  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯


def save_papers_to_markdown(papers_by_category: dict, target_date):
    """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Markdownæ–‡ä»¶"""
    # ä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    filename = target_date.strftime("%Y-%m-%d") + ".md"
    year_month = target_date.strftime("%Y-%m")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•(scripts)çš„çˆ¶çº§è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    # è®¾ç½®åŸºç¡€ç›®å½•ï¼Œä¸scriptsåŒçº§
    data_base = os.path.join(parent_dir, 'data')
    local_base = os.path.join(parent_dir, 'local')

    # åˆ›å»ºå¹´æœˆå­ç›®å½•
    data_year_month = os.path.join(data_base, year_month)
    local_year_month = os.path.join(local_base, year_month)

    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)

    # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡åˆ—è¡¨\n\n")
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        # f.write("\n## è®ºæ–‡è¯¦æƒ…\n\n")
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    print(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    print(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


def generate_statistics_markdown(papers_by_category: dict) -> str:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯çš„Markdownæ ¼å¼æ–‡æœ¬
    
    Args:
        papers_by_category: æŒ‰ç±»åˆ«ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        
    Returns:
        str: Markdownæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
    """
    markdown = "## ç»Ÿè®¡ä¿¡æ¯\n\n"
    
    # è®¡ç®—æ€»è®ºæ–‡æ•°
    total_papers = sum(len(papers) for papers in papers_by_category.values())
    markdown += f"**æ€»è®ºæ–‡æ•°**: {total_papers} ç¯‡\n\n"
    
    # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
    sorted_categories = sorted(
        papers_by_category.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )
    
    # ä¸€çº§åˆ†ç±»ç»Ÿè®¡
    markdown += "### ä¸€çº§åˆ†ç±»ç»Ÿè®¡\n\n"
    markdown += "| ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
    markdown += "|------|--------|--------|------|\n"
    
    for category, papers in sorted_categories:
        num_new = sum(1 for p in papers if not p['is_updated'])
        num_updated = sum(1 for p in papers if p['is_updated'])
        markdown += f"| {category} | {len(papers)} | {num_new} | {num_updated} |\n"
    
    # äºŒçº§åˆ†ç±»ç»Ÿè®¡
    markdown += "\n### äºŒçº§åˆ†ç±»ç»Ÿè®¡\n\n"
    
    for category, papers in sorted_categories:
        if len(papers) == 0:
            continue
            
        markdown += f"#### {category}\n\n"
        markdown += "| å­ç±»åˆ« | è®ºæ–‡æ•° | æ–°å‘å¸ƒ | æ›´æ–° |\n"
        markdown += "|--------|--------|--------|------|\n"
        
        # å¦‚æœæ˜¯"å…¶ä»– (Others)"ç±»åˆ«ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰è®ºæ–‡
        # å¯¹äºå…¶ä»–ç±»åˆ«ï¼Œå°†æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡ç§»åŠ¨åˆ°"å…¶ä»– (Others)"ç±»åˆ«ä¸­
        papers_with_subcategory = []
        
        if category != "å…¶ä»– (Others)":
            # åˆ†ç¦»æœ‰å­ç±»åˆ«çš„è®ºæ–‡å’Œæ— å­ç±»åˆ«çš„è®ºæ–‡
            for paper in papers:
                subcategory = paper.get('subcategory', '')
                if subcategory and subcategory != "æœªæŒ‡å®š":
                    papers_with_subcategory.append(paper)
                # æ²¡æœ‰å­ç±»åˆ«çš„è®ºæ–‡å·²ç»è¢«ç§»åŠ¨åˆ°"å…¶ä»– (Others)"ç±»åˆ«ä¸­
        else:
            # å¯¹äº"å…¶ä»– (Others)"ç±»åˆ«ï¼Œæ‰€æœ‰è®ºæ–‡éƒ½ç›´æ¥å¤„ç†
            papers_with_subcategory = papers
        
        # æŒ‰å­ç±»åˆ«åˆ†ç»„æœ‰å­ç±»åˆ«çš„è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        for paper in papers_with_subcategory:
            subcategory = paper.get('subcategory', '')
            papers_by_subcategory[subcategory].append(paper)
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºå­ç±»åˆ«
        sorted_subcategories = sorted(
            papers_by_subcategory.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        # æ·»åŠ å­ç±»åˆ«ç»Ÿè®¡
        for subcategory, subpapers in sorted_subcategories:
            num_new = sum(1 for p in subpapers if not p['is_updated'])
            num_updated = sum(1 for p in subpapers if p['is_updated'])
            markdown += f"| {subcategory} | {len(subpapers)} | {num_new} | {num_updated} |\n"
        
        markdown += "\n"
    
    return markdown


if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒæŸ¥è¯¢
    get_cv_papers()
