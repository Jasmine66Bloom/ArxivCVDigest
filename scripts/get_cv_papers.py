from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import io
import os
import re
import time
from typing import Any, Dict, List, Tuple
import math

import arxiv
import requests
from tqdm import tqdm

from chatglm_helper import ChatGLMHelper
from categories_config import CATEGORY_DISPLAY_ORDER  # å¯¼å…¥ç±»åˆ«æ˜¾ç¤ºé¡ºåº

# æŸ¥è¯¢å‚æ•°è®¾ç½®
QUERY_DAYS_AGO = 2          # æŸ¥è¯¢å‡ å¤©å‰çš„è®ºæ–‡ï¼Œ0=ä»Šå¤©ï¼Œ1=æ˜¨å¤©ï¼Œ2=å‰å¤©
MAX_RESULTS = 600           # æœ€å¤§è¿”å›è®ºæ–‡æ•°é‡
MAX_WORKERS = 8            # å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°


def extract_github_link(text, paper_url=None, title=None, authors=None, pdf_url=None):
    """ä»æ–‡æœ¬ä¸­æå–GitHubé“¾æ¥

    Args:
        text: è®ºæ–‡æ‘˜è¦æ–‡æœ¬
        paper_url: è®ºæ–‡URLï¼ˆæœªä½¿ç”¨ï¼‰
        title: è®ºæ–‡æ ‡é¢˜ï¼ˆæœªä½¿ç”¨ï¼‰
        authors: ä½œè€…åˆ—è¡¨ï¼ˆæœªä½¿ç”¨ï¼‰
        pdf_url: PDFæ–‡ä»¶URLï¼ˆæœªä½¿ç”¨ï¼‰

    Returns:
        str: GitHubé“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # ä»æ‘˜è¦ä¸­æŸ¥æ‰¾
    for pattern in github_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            url = match.group(0)
            if not url.startswith('http'):
                url = 'https://' + url
            return url

    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹"""
    markdown = ""

    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}

    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"

    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç é“¾æ¥']

    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue

        # æ·»åŠ ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"

        # åˆ›å»ºè¡¨æ ¼å¤´
        markdown += "|" + "|".join(headers) + "|\n"
        markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"

        # æ·»åŠ è®ºæ–‡
        for paper in active_categories[category]:
            # ç¡®å®šè®ºæ–‡çŠ¶æ€
            if paper['is_updated']:
                status = f"ğŸ“ æ›´æ–°"
            else:
                status = f"ğŸ†• å‘å¸ƒ"

            # å‡†å¤‡æ¯ä¸ªå­—æ®µçš„å€¼
            values = [
                status,
                paper['title'],
                paper['title_cn'],
                paper['authors'],  # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
                f"<{paper['pdf_url']}>",
                f"<{paper['github_link']}>" if paper['github_link'] != 'None' else 'None',
            ]

            # å¤„ç†ç‰¹æ®Šå­—ç¬¦
            values = [str(v).replace('\n', ' ').replace('|', '&#124;')
                      for v in values]

            # æ·»åŠ åˆ°è¡¨æ ¼
            markdown += "|" + "|".join(values) + "|\n"

        # åœ¨æ¯ä¸ªè¡¨æ ¼åæ·»åŠ ç©ºè¡Œ
        markdown += "\n"

    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹"""
    markdown = ""

    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}

    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"

    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue

        # æ·»åŠ ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"

        # æ·»åŠ è®ºæ–‡
        for idx, paper in enumerate(active_categories[category], 1):
            markdown += f'**index:** {idx}<br />\n'
            markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
            markdown += f'**Title:** {paper["title"]}<br />\n'
            markdown += f'**Title_cn:** {paper["title_cn"]}<br />\n'
            # å·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²
            markdown += f'**Authors:** {paper["authors"]}<br />\n'
            markdown += f'**PDF:** <{paper["pdf_url"]}><br />\n'

            if paper["github_link"] != 'None':
                markdown += f'**Code:** <{paper["github_link"]}><br />\n'
            else:
                markdown += '**Code:** None<br />\n'

            if "æ ¸å¿ƒè´¡çŒ®" in paper:
                markdown += f'**Core Contribution:** {paper["æ ¸å¿ƒè´¡çŒ®"]}<br />\n'
            if "æ ¸å¿ƒé—®é¢˜" in paper:
                markdown += f'**Core Problem:** {paper["æ ¸å¿ƒé—®é¢˜"]}<br />\n'

            markdown += "\n"

    return markdown


def get_category_by_keywords(title: str, abstract: str, categories_config: Dict) -> List[Tuple[str, float]]:
    """
    Perform hierarchical classification of papers based on keyword matching and priority rules.
    
    Args:
        title (str): Paper title for primary context analysis
        abstract (str): Paper abstract for comprehensive content analysis
        categories_config (Dict): Configuration dictionary containing category definitions,
                                keywords, weights, and priority levels
    
    Implementation Details:
        1. Text Preprocessing:
           - Case normalization for robust matching
           - Combined analysis of title and abstract with differential weighting
        
        2. Scoring Mechanism:
           - Primary score: Weighted keyword matches (0.15 base weight)
           - Title bonus: Additional weight for title matches (0.05 weight)
           - Priority multiplier: Category-specific importance scaling
           - Negative keyword penalty: Exponential score reduction
        
        3. Classification Logic:
           - Minimum confidence threshold: 0.2
           - Priority category threshold: 60% of max score
           - General category threshold: 40% of max score
           - Hierarchical processing of priority categories
    
    Returns:
        List[Tuple[str, float]]: List of (category, confidence_score) pairs,
                                sorted by confidence in descending order
    """
    # Normalize input text for consistent matching
    text = (title + " " + abstract).lower()
    
    # Initialize score accumulator
    scores = defaultdict(float)
    
    # Compute category scores with priority weighting
    for category, config in categories_config.items():
        score = 0.0
        
        # Primary keyword matching with base weight
        for keyword, weight in config["keywords"]:
            keyword = keyword.lower()
            if keyword in text:
                score += weight * 0.15  # Base confidence weight
            
            # Additional weight for title matches
            if keyword in title.lower():
                score += weight * 0.05  # Title significance bonus
        
        # Apply negative keyword penalties
        if "negative_keywords" in config:
            negative_score = 0
            for keyword in config["negative_keywords"]:
                keyword = keyword.lower()
                if keyword in text:
                    negative_score += 1
            
            # Exponential penalty for negative matches
            if negative_score > 0:
                score *= math.exp(-negative_score)
        
        # Apply category priority scaling
        priority = config.get("priority", 0)
        if priority > 0:
            score *= (1 + priority * 0.1)  # Priority-based confidence adjustment
        
        scores[category] = score
    
    # Validate against minimum confidence threshold
    max_score = max(scores.values()) if scores else 0
    if max_score < 0.2:  # Minimum confidence requirement
        return []
    
    # Priority category processing
    high_priority_categories = ["æ‰©æ•£æ¡¥", "å…·èº«æ™ºèƒ½", "æµæ¨¡å‹"]
    for category in high_priority_categories:
        if category in scores:
            category_score = scores[category]
            if category_score >= max_score * 0.6 and category_score >= 0.2:
                return [(category, category_score)]
    
    # General category processing
    significant_categories = [
        (category, score) 
        for category, score in scores.items() 
        if score >= max_score * 0.4  # Significance threshold
    ]
    
    return sorted(significant_categories, key=lambda x: x[1], reverse=True)


def process_paper(paper, glm_helper, target_date):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰åˆ†æä»»åŠ¡

    Args:
        paper: ArXivè®ºæ–‡å¯¹è±¡
        glm_helper: ChatGLMåŠ©æ‰‹å®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        Dict: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœè®ºæ–‡ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚åˆ™è¿”å›None
    """
    try:
        # è·å–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        abstract = paper.summary
        paper_url = paper.entry_id
        author_list = paper.authors
        authors = [author.name for author in author_list]
        authors_str = ', '.join(authors[:8]) + (' .etc.' if len(authors) > 8 else '')  # é™åˆ¶ä½œè€…æ˜¾ç¤ºæ•°é‡ï¼Œè¶…è¿‡8ä¸ªæ˜¾ç¤ºetc.
        published = paper.published
        updated = paper.updated

        # æ£€æŸ¥å‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸæ˜¯å¦åŒ¹é…ç›®æ ‡æ—¥æœŸ
        published_date = published.date()
        updated_date = updated.date()
        if published_date != target_date and updated_date != target_date:
            return None

        # è·å–PDFé“¾æ¥
        pdf_url = next(
            (link.href for link in paper.links if link.title == "pdf"), None)

        # å¹¶è¡Œæ‰§è¡Œè€—æ—¶ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            github_future = executor.submit(extract_github_link, abstract)
            analysis_future = executor.submit(
                glm_helper.analyze_paper_contribution, title, abstract)
            category_future = executor.submit(
                glm_helper.categorize_paper, title, abstract)
            title_cn_future = executor.submit(
                glm_helper.translate_title, title)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            github_link = github_future.result() or "None"
            analysis = analysis_future.result()
            category = category_future.result()
            title_cn = title_cn_future.result()

        paper_info = {
            'title': title,
            'title_cn': title_cn,
            'abstract': abstract,
            'authors': authors_str,
            'pdf_url': pdf_url,
            'github_link': github_link,
            'category': category,
            'published': published,
            'updated': updated,
            'is_updated': updated_date == target_date and published_date != target_date
        }

        return paper_info

    except Exception as e:
        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return None


def get_cv_papers():
    """è·å–CVé¢†åŸŸè®ºæ–‡å¹¶ä¿å­˜ä¸ºMarkdown"""
    print("\n" + "="*50)
    print(f"å¼€å§‹è·å–CVè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # è·å–ç›®æ ‡æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
        target_date = (datetime.now() - timedelta(days=QUERY_DAYS_AGO)).date()
        print(f"\nğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"ğŸ“Š æœ€å¤§è®ºæ–‡æ•°: {MAX_RESULTS}")
        print(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {MAX_WORKERS}\n")

        # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
        print("ğŸ¤– åˆå§‹åŒ–ChatGLMåŠ©æ‰‹...")
        glm_helper = ChatGLMHelper()

        # åˆå§‹åŒ–arxivå®¢æˆ·ç«¯
        print("ğŸ”„ åˆå§‹åŒ–arXivå®¢æˆ·ç«¯...")
        client = arxiv.Client(
            page_size=100,  # æ¯é¡µè·å–100ç¯‡è®ºæ–‡
            delay_seconds=3,  # è¯·æ±‚é—´éš”3ç§’
            num_retries=5    # å¤±è´¥é‡è¯•5æ¬¡
        )

        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query='cat:cs.CV',  # è®¡ç®—æœºè§†è§‰ç±»åˆ«
            max_results=MAX_RESULTS,
            sort_by=arxiv.SortCriterion.LastUpdatedDate,
            sort_order=arxiv.SortOrder.Descending  # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åº
        )

        # åˆ›å»ºçº¿ç¨‹æ± 
        total_papers = 0
        papers_by_category = defaultdict(list)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # åˆ†æ‰¹è·å–å’Œå¤„ç†è®ºæ–‡
            batch_size = 30  # æ¯æ‰¹å¤„ç†30ç¯‡è®ºæ–‡
            futures = []
            processed_papers = set()  # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„è®ºæ–‡ID

            # åˆ›å»ºæ€»è¿›åº¦æ¡
            total_pbar = tqdm(total=MAX_RESULTS, desc="æ€»è¿›åº¦", unit="ç¯‡")
            # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
            batch_pbar = tqdm(total=batch_size, desc="å½“å‰æ‰¹æ¬¡",
                              unit="ç¯‡", leave=False)

            for paper in client.results(search):
                paper_id = paper.get_short_id()

                # è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡
                if paper_id in processed_papers:
                    continue
                processed_papers.add(paper_id)

                # æäº¤å¤„ç†ä»»åŠ¡
                future = executor.submit(
                    process_paper, paper, glm_helper, target_date)
                futures.append(future)

                # å½“ç´¯ç§¯äº†ä¸€æ‰¹è®ºæ–‡æ—¶ï¼Œç­‰å¾…å®ƒä»¬å¤„ç†å®Œæˆ
                if len(futures) >= batch_size:
                    batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                    for completed_future in as_completed(futures):
                        paper_info = completed_future.result()
                        if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                            total_papers += 1
                            category = paper_info['category']
                            papers_by_category[category].append(paper_info)
                            total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                        batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦
                    futures = []  # æ¸…ç©ºå·²å¤„ç†çš„futures

                # å¦‚æœå·²ç»å¤„ç†äº†è¶³å¤Ÿå¤šçš„è®ºæ–‡ï¼Œå°±åœæ­¢
                if total_papers >= MAX_RESULTS:
                    break

            # å¤„ç†å‰©ä½™çš„è®ºæ–‡
            if futures:
                batch_pbar.reset()  # é‡ç½®æ‰¹å¤„ç†è¿›åº¦æ¡
                batch_pbar.total = len(futures)  # è®¾ç½®æ­£ç¡®çš„æ€»æ•°
                for future in as_completed(futures):
                    paper_info = future.result()
                    if paper_info:  # å¦‚æœè®ºæ–‡ç¬¦åˆæ—¥æœŸè¦æ±‚
                        total_papers += 1
                        category = paper_info['category']
                        papers_by_category[category].append(paper_info)
                        total_pbar.update(1)  # æ›´æ–°æ€»è¿›åº¦
                    batch_pbar.update(1)  # æ›´æ–°æ‰¹å¤„ç†è¿›åº¦

            # å…³é—­è¿›åº¦æ¡
            batch_pbar.close()
            total_pbar.close()

        if total_papers == 0:
            print(f"æ²¡æœ‰æ‰¾åˆ°{target_date}å‘å¸ƒçš„è®ºæ–‡ã€‚")
            return

        # æŒ‰ç±»åˆ«å¯¹è®ºæ–‡è¿›è¡Œæ’åºï¼ˆæŒ‰å‘å¸ƒæ—¶é—´é™åºï¼‰
        for category in papers_by_category:
            papers_by_category[category].sort(
                key=lambda x: x.get('published', ''),
                reverse=True
            )

        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°Markdownæ–‡ä»¶...")
        save_papers_to_markdown(papers_by_category, target_date)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è®ºæ–‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"{'='*30}")
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åºç±»åˆ«
        sorted_categories = sorted(
            papers_by_category.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        for category, papers in sorted_categories:
            num_new = sum(1 for p in papers if not p['is_updated'])
            num_updated = sum(1 for p in papers if p['is_updated'])
            print(
                f"{category:15s}: {len(papers):3d} ç¯‡ (ğŸ†• {num_new:3d} æ–°å‘å¸ƒ, ğŸ“ {num_updated:3d} æ›´æ–°)")
        print(f"{'='*30}")
        print(f"æ€»è®¡: {total_papers} ç¯‡")
        
        print("\n" + "="*50)
        print(f"CVè®ºæ–‡è·å–å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50 + "\n")

    except Exception as e:
        print("\nâŒ å¤„ç†CVè®ºæ–‡æ—¶å‡ºé”™:")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"å‘ç”Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        raise  # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯


def save_papers_to_markdown(papers_by_category: dict, target_date):
    """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Markdownæ–‡ä»¶"""
    # ä½¿ç”¨ç›®æ ‡æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    filename = target_date.strftime("%Y-%m-%d") + ".md"
    year_month = target_date.strftime("%Y-%m")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•(scripts)çš„çˆ¶çº§è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    # è®¾ç½®åŸºç¡€ç›®å½•ï¼Œä¸scriptsåŒçº§
    data_base = os.path.join(parent_dir, 'data')
    local_base = os.path.join(parent_dir, 'local')

    # åˆ›å»ºå¹´æœˆå­ç›®å½•
    data_year_month = os.path.join(data_base, year_month)
    local_year_month = os.path.join(local_base, year_month)

    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)

    # ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    print(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    print(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒæŸ¥è¯¢
    get_cv_papers()
