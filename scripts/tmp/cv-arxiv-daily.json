{"\u751f\u6210\u6a21\u578b": {"2402.10210": "|**2024-02-15**|**Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation**|\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6269\u6563\u6a21\u578b\u7684\u81ea\u73a9\u5fae\u8c03|Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu|Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.|\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ecd\u7136\u662f\u751f\u6210\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u9886\u57df\u5c1a\u672a\u5f00\u53d1\u7684\u524d\u6cbf\u9886\u57df\uff0c\u7279\u522b\u662f\u4e0e\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65b9\u9762\u53d6\u5f97\u7684\u663e\u7740\u8fdb\u5c55\u76f8\u6bd4\u3002\u867d\u7136\u7a33\u5b9a\u6269\u6563 (SD) \u548c SDXL \u7b49\u5c16\u7aef\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u5728\u770b\u5230\u4e00\u5b9a\u91cf\u7684\u6570\u636e\u540e\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u8d8b\u4e8e\u7a33\u5b9a\u3002\u6700\u8fd1\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u88ab\u7528\u6765\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6765\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u4f46\u6bcf\u4e2a\u6587\u672c\u63d0\u793a\u81f3\u5c11\u9700\u8981\u4e24\u5f20\u56fe\u50cf\uff08\u201c\u83b7\u80dc\u8005\u201d\u548c\u201c\u5931\u8d25\u8005\u201d\u56fe\u50cf\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u6269\u6563\u6a21\u578b\u81ea\u6211\u8c03\u6574\u5fae\u8c03\uff08SPIN-Diffusion\uff09\u7684\u521b\u65b0\u6280\u672f\uff0c\u5176\u4e2d\u6269\u6563\u6a21\u578b\u4e0e\u5176\u65e9\u671f\u7248\u672c\u8fdb\u884c\u7ade\u4e89\uff0c\u4fc3\u8fdb\u8fed\u4ee3\u7684\u81ea\u6211\u6539\u8fdb\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728 Pick-a-Pic \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPIN-Diffusion \u4ece\u7b2c\u4e00\u6b21\u8fed\u4ee3\u8d77\u5c31\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u548c\u89c6\u89c9\u5438\u5f15\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002\u5230\u7b2c\u4e8c\u6b21\u8fed\u4ee3\u65f6\uff0c\u5b83\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u8d85\u8fc7\u4e86\u57fa\u4e8e RLHF \u7684\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7528\u66f4\u5c11\u7684\u6570\u636e\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u7ed3\u679c\u3002|[2402.10210v1](http://arxiv.org/pdf/2402.10210v1)|null|\n", "2402.10208": "|**2024-02-15**|**Recovering the Pre-Fine-Tuning Weights of Generative Models**|\u6062\u590d\u751f\u6210\u6a21\u578b\u7684\u9884\u5fae\u8c03\u6743\u91cd|Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen|The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.|\u751f\u6210\u5efa\u6a21\u7684\u4e3b\u5bfc\u8303\u5f0f\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1ai\uff09\u5728\u5927\u89c4\u6a21\u4f46\u4e0d\u5b89\u5168\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0cii\uff09\u901a\u8fc7\u5fae\u8c03\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u3002\u8fd9\u79cd\u505a\u6cd5\u88ab\u8ba4\u4e3a\u662f\u5b89\u5168\u7684\uff0c\u56e0\u4e3a\u5f53\u524d\u6ca1\u6709\u65b9\u6cd5\u53ef\u4ee5\u6062\u590d\u4e0d\u5b89\u5168\u7684\u9884\u5fae\u8c03\u6a21\u578b\u6743\u91cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u8fd9\u79cd\u5047\u8bbe\u901a\u5e38\u662f\u9519\u8bef\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Spectral DeTuning\uff0c\u4e00\u79cd\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e9b\u4f4e\u79e9\uff08LoRA\uff09\u5fae\u8c03\u6a21\u578b\u6765\u6062\u590d\u9884\u5fae\u8c03\u6a21\u578b\u6743\u91cd\u7684\u65b9\u6cd5\u3002\u4e0e\u4e4b\u524d\u5c1d\u8bd5\u6062\u590d\u9884\u5fae\u8c03\u80fd\u529b\u7684\u653b\u51fb\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u6062\u590d\u7cbe\u786e\u7684\u9884\u5fae\u8c03\u6743\u91cd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9488\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\uff08\u4f8b\u5982\u4e2a\u6027\u5316\u7a33\u5b9a\u6269\u6563\u548c\u5bf9\u9f50\u7684\u7c73\u65af\u7279\u62c9\u5c14\uff09\u5229\u7528\u4e86\u8fd9\u4e00\u65b0\u6f0f\u6d1e\u3002|[2402.10208v1](http://arxiv.org/pdf/2402.10208v1)|null|\n", "2402.10204": "|**2024-02-15**|**Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model**|\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u5c04\u7535\u5929\u6587\u56fe\u50cf\u91cd\u5efa|Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer, Slava Voloshynovskiy|Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model's performance across different water vapor levels. Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies. While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research.|\u4ece\u810f\u5c04\u7535\u56fe\u50cf\u91cd\u5efa\u5929\u7a7a\u6a21\u578b\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u6e90\u5b9a\u4f4d\u548c\u901a\u91cf\u4f30\u8ba1\u5bf9\u4e8e\u7814\u7a76\u9ad8\u7ea2\u79fb\u4e0b\u7684\u661f\u7cfb\u6f14\u5316\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u963f\u5854\u5361\u9a6c\u5927\u578b\u6beb\u7c73\u6ce2\u9635\u5217\uff08ALMA\uff09\u7b49\u4eea\u5668\u7684\u6df1\u573a\u4e2d\u3002\u968f\u7740\u5e73\u65b9\u516c\u91cc\u9635\u5217 (SKA) \u7b49\u65b0\u9879\u76ee\u7684\u5f00\u5c55\uff0c\u5bf9\u66f4\u597d\u7684\u6e90\u63d0\u53d6\u65b9\u6cd5\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u5f53\u524d\u7684\u6280\u672f\uff0c\u4f8b\u5982 CLEAN \u548c PyBDSF\uff0c\u901a\u5e38\u65e0\u6cd5\u68c0\u6d4b\u5fae\u5f31\u7684\u5149\u6e90\uff0c\u8fd9\u51f8\u663e\u4e86\u5bf9\u66f4\u51c6\u786e\u65b9\u6cd5\u7684\u9700\u6c42\u3002\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u4ece\u810f\u56fe\u50cf\u91cd\u5efa\u5929\u7a7a\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u65e0\u7ebf\u7535\u6e90\u5e76\u6d4b\u91cf\u5176\u5177\u6709\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u7684\u901a\u91cf\uff0c\u8fd9\u6807\u5fd7\u7740\u65e0\u7ebf\u7535\u6e90\u8868\u5f81\u7684\u6f5c\u5728\u6539\u8fdb\u3002\u6211\u4eec\u57fa\u4e8e ALMA \u7684 Cycle 5.3 \u5929\u7ebf\u8bbe\u7f6e\uff0c\u5728\u4f7f\u7528 CASA \u5de5\u5177 simalma \u6a21\u62df\u7684 10164 \u4e2a\u56fe\u50cf\u4e0a\u6d4b\u8bd5\u4e86\u8fd9\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u5e94\u7528\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM) \u8fdb\u884c\u5929\u7a7a\u6a21\u578b\u91cd\u5efa\uff0c\u7136\u540e\u4f7f\u7528 Photutils \u786e\u5b9a\u6e90\u5750\u6807\u548c\u901a\u91cf\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u6c34\u6c7d\u6c34\u5e73\u4e0b\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6e90\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u4fe1\u566a\u6bd4 (SNR) \u4f4e\u81f3 2 \u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86 90% \u4ee5\u4e0a\u7684\u5b8c\u6574\u6027\u3002\u5b83\u5728\u901a\u91cf\u4f30\u8ba1\u65b9\u9762\u4e5f\u8d85\u8d8a\u4e86 PyBDSF\uff0c\u51c6\u786e\u8bc6\u522b\u4e86\u6d4b\u8bd5\u96c6\u4e2d 96% \u7684\u6e90\u7684\u901a\u91cf\uff0c\u6bd4 CLEAN+ PyBDSF \u7684 57% \u663e\u7740\u63d0\u9ad8\u3002\u6761\u4ef6 DDPM \u662f\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u53ef\u4ee5\u51c6\u786e\u3001\u7a33\u5065\u5730\u8868\u5f81\u65e0\u7ebf\u7535\u6e90\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u867d\u7136\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u5176\u5728\u5c04\u7535\u5929\u6587\u5b66\u4e2d\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u6211\u4eec\u4e5f\u627f\u8ba4\u5176\u4f7f\u7528\u6240\u4f34\u968f\u7684\u67d0\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u5b8c\u5584\u548c\u7814\u7a76\u7684\u65b9\u5411\u3002|[2402.10204v1](http://arxiv.org/pdf/2402.10204v1)|null|\n", "2402.10055": "|**2024-02-15**|**Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network**|\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u795e\u7ecf\u7f51\u7edc\u5728\u4eba\u7c7b\u89c6\u7f51\u819c\u56fe\u50cf\u4e2d\u8fdb\u884c\u9c81\u68d2\u7684\u534a\u81ea\u52a8\u8840\u7ba1\u8ffd\u8e2a|Siyi Chen, Amir H. Kashani, Ji Yi|The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map. We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net. We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information. InSegNN paves a way for any subsequent morphological analysis of vascular morphology in relation to retinal diseases.|\u8840\u7ba1\u7cfb\u7edf\u7684\u5f62\u6001\u548c\u5c42\u6b21\u5bf9\u4e8e\u652f\u6301\u65b0\u9648\u4ee3\u8c22\u7684\u704c\u6ce8\u81f3\u5173\u91cd\u8981\u3002\u4eba\u7c7b\u89c6\u7f51\u819c\u662f\u6700\u9700\u8981\u80fd\u91cf\u7684\u5668\u5b98\u4e4b\u4e00\uff0c\u89c6\u7f51\u819c\u5faa\u73af\u901a\u8fc7\u5728\u89c6\u795e\u7ecf\u4e73\u5934 (ONH) \u5904\u51fa\u73b0\u548c\u91cd\u65b0\u51fa\u73b0\u7684\u590d\u6742\u8109\u7ba1\u7cfb\u7edf\u6765\u6ecb\u517b\u6574\u4e2a\u89c6\u7f51\u819c\u5185\u5c42\u3002\u56e0\u6b64\uff0c\u901a\u8fc7\u8840\u7ba1\u6811\u8ffd\u8e2a\u4ece ONH \u5f00\u59cb\u7684\u8840\u7ba1\u5206\u652f\u53ef\u4ee5\u8bf4\u660e\u8840\u7ba1\u5c42\u6b21\u7ed3\u6784\u5e76\u5141\u8bb8\u8be6\u7ec6\u7684\u5f62\u6001\u5b66\u91cf\u5316\uff0c\u4f46\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u795e\u7ecf\u7f51\u7edc\uff08InSegNN\uff09\u5bf9\u4eba\u4f53\u773c\u5e95\u56fe\u50cf\u8fdb\u884c\u9c81\u68d2\u534a\u81ea\u52a8\u8840\u7ba1\u8ffd\u8e2a\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\u3002\u4e0e\u8bed\u4e49\u5206\u5272\u4e0d\u540c\uff0cInSegNN \u5355\u72ec\u5206\u79bb\u548c\u6807\u8bb0\u4e0d\u540c\u7684\u8840\u7ba1\u6811\uff0c\u56e0\u6b64\u80fd\u591f\u8ddf\u8e2a\u6bcf\u68f5\u6811\u7684\u6574\u4e2a\u5206\u652f\u3002\u6211\u4eec\u5185\u7f6e\u4e86\u4e09\u79cd\u7b56\u7565\u6765\u901a\u8fc7\u65f6\u95f4\u5b66\u4e60\u3001\u7a7a\u95f4\u591a\u91cd\u91c7\u6837\u548c\u52a8\u6001\u6982\u7387\u56fe\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u4e0e\u6587\u732e\u76f8\u6bd4\uff0c\u6211\u4eec\u5728\u5bf9\u79f0\u6700\u4f73\u9ab0\u5b50 (SBD) \u65b9\u9762\u5b9e\u73b0\u4e86 83% \u7684\u7279\u5f02\u6027\u548c 50% \u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u4f18\u4e8e\u57fa\u7ebf U-net\u3002\u6211\u4eec\u5df2\u7ecf\u6f14\u793a\u4e86\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u8ffd\u8e2a\u5355\u4e2a\u8840\u7ba1\u6811\uff0c\u5e76\u540c\u65f6\u4fdd\u7559\u8840\u7ba1\u5c42\u6b21\u7ed3\u6784\u4fe1\u606f\u3002 InSegNN \u4e3a\u4e0e\u89c6\u7f51\u819c\u75be\u75c5\u76f8\u5173\u7684\u8840\u7ba1\u5f62\u6001\u7684\u540e\u7eed\u5f62\u6001\u5b66\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.10055v1](http://arxiv.org/pdf/2402.10055v1)|null|\n", "2402.09982": "|**2024-02-15**|**Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition**|\u5e94\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6570\u636e\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5|Enrico Randellini, Leonardo Rigutini, Claudio Sacca'|The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model.|\u5f53\u6211\u4eec\u60f3\u8981\u4e86\u89e3\u4e00\u4e2a\u4eba\u7684\u5fc3\u7406\u72b6\u6001\u65f6\uff0c\u9996\u5148\u8981\u5173\u6ce8\u7684\u662f\u9762\u90e8\u8868\u60c5\u3002\u56e0\u6b64\uff0c\u81ea\u52a8\u8bc6\u522b\u9762\u90e8\u8868\u60c5\u7684\u80fd\u529b\u662f\u4e00\u4e2a\u975e\u5e38\u6709\u8da3\u7684\u7814\u7a76\u9886\u57df\u3002\u5728\u672c\u6587\u4e2d\uff0c\u7531\u4e8e\u53ef\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u89c4\u6a21\u8f83\u5c0f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6211\u4eec\u5e94\u7528\u51e0\u4f55\u53d8\u6362\u5e76\u4ece\u5934\u5f00\u59cb\u6784\u5efa GAN \u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u6bcf\u79cd\u60c5\u611f\u7c7b\u578b\u751f\u6210\u65b0\u7684\u5408\u6210\u56fe\u50cf\u3002\u56e0\u6b64\uff0c\u5728\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u5bf9\u5177\u6709\u4e0d\u540c\u67b6\u6784\u7684\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u3002\u4e3a\u4e86\u8861\u91cf\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u6570\u636e\u5e93\u5916\u534f\u8bae\u65b9\u6cd5\uff0c\u5373\u6211\u4eec\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u589e\u5f3a\u7248\u672c\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u5e93\u4e0a\u6d4b\u8bd5\u5b83\u4eec\u3002\u8fd9\u4e9b\u6280\u672f\u7684\u7ed3\u5408\u4f7f\u5f97 InceptionResNetV2 \u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u5230 85% \u5de6\u53f3\u3002|[2402.09982v1](http://arxiv.org/pdf/2402.09982v1)|null|\n", "2402.09966": "|**2024-02-15**|**Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation**|\u6587\u672c\u672c\u5730\u5316\uff1a\u5206\u89e3\u591a\u6982\u5ff5\u56fe\u50cf\u4ee5\u751f\u6210\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf|Junjie Shentu, Matthew Watson, Noura Al Moubayed|Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images. During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt. Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.|\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f7f\u7528\u6237\u80fd\u591f\u4f7f\u7528\u4e00\u4e9b\u793a\u4f8b\u56fe\u50cf\u6839\u636e\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7f3a\u5c11\u7684\u65b0\u6982\u5ff5\u5b9a\u5236\u6a21\u578b\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684\u4e3b\u9898\u9a71\u52a8\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5355\u6982\u5ff5\u8f93\u5165\u56fe\u50cf\uff0c\u5728\u5904\u7406\u591a\u6982\u5ff5\u8f93\u5165\u56fe\u50cf\u65f6\u9762\u4e34\u7740\u6307\u5b9a\u76ee\u6807\u6982\u5ff5\u7684\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6587\u672c\u672c\u5730\u5316\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08Texual Localization\uff09\u6765\u5904\u7406\u591a\u6982\u5ff5\u8f93\u5165\u56fe\u50cf\u3002\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u53c9\u6ce8\u610f\u6307\u5bfc\u6765\u5206\u89e3\u591a\u4e2a\u6982\u5ff5\uff0c\u5728\u76ee\u6807\u6982\u5ff5\u7684\u89c6\u89c9\u8868\u793a\u548c\u6587\u672c\u63d0\u793a\u4e2d\u7684\u6807\u8bc6\u7b26\u6807\u8bb0\u4e4b\u95f4\u5efa\u7acb\u660e\u663e\u7684\u8054\u7cfb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u6982\u5ff5\u8f93\u5165\u56fe\u50cf\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u6216\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u3002\u4e0e\u81ea\u5b9a\u4e49\u6269\u6563\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u786c\u6307\u5bfc\u65b9\u6cd5\u5728\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u751f\u6210\u65b9\u9762\u7684 CLIP-I \u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e86 7.04%\u30018.13%\uff0cCLIP-T \u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e86 2.22%\u30015.85%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u4e0e\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u6982\u5ff5\u4e00\u81f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u8fd9\u662f\u73b0\u6709\u6a21\u578b\u4e2d\u4e0d\u5177\u5907\u7684\u529f\u80fd\u3002|[2402.09966v1](http://arxiv.org/pdf/2402.09966v1)|null|\n", "2402.09883": "|**2024-02-15**|**Lester: rotoscope animation through video object segmentation and tracking**|Lester\uff1a\u901a\u8fc7\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u548c\u8ddf\u8e2a\u5236\u4f5c\u8f6c\u63cf\u52a8\u753b|Ruben Tous|This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.|\u672c\u6587\u4ecb\u7ecd\u4e86 Lester\uff0c\u4e00\u79cd\u4ece\u89c6\u9891\u4e2d\u81ea\u52a8\u5408\u6210\u590d\u53e4\u98ce\u683c 2D \u52a8\u753b\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e3b\u8981\u5c06\u6311\u6218\u89c6\u4e3a\u5bf9\u8c61\u5206\u5272\u548c\u8ddf\u8e2a\u95ee\u9898\u3002\u89c6\u9891\u5e27\u4f7f\u7528\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM) \u8fdb\u884c\u5904\u7406\uff0c\u5e76\u4f7f\u7528 DeAOT\uff08\u4e00\u79cd\u534a\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u5206\u5c42\u4f20\u64ad\u65b9\u6cd5\uff09\u901a\u8fc7\u540e\u7eed\u5e27\u8ddf\u8e2a\u751f\u6210\u7684\u63a9\u6a21\u3002\u4f7f\u7528 Douglas-Peucker \u7b97\u6cd5\u7b80\u5316\u4e86\u63a9\u6a21\u8f6e\u5ed3\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u6700\u540e\uff0c\u53ef\u4ee5\u9009\u62e9\u6dfb\u52a0\u9762\u90e8\u7279\u5f81\u3001\u50cf\u7d20\u5316\u548c\u57fa\u672c\u9634\u5f71\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u826f\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u80fd\u591f\u6b63\u786e\u5904\u7406\u4e0d\u540c\u59ff\u52bf\u548c\u5916\u89c2\u3001\u52a8\u6001\u955c\u5934\u3001\u5c40\u90e8\u955c\u5934\u548c\u4e0d\u540c\u80cc\u666f\u7684\u89c6\u9891\u3002\u4e0e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u7ba1\u9053\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u548c\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u540e\u8005\u5b58\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u5e76\u4e14\u4e0d\u80fd\u5f88\u597d\u5730\u5904\u7406\u50cf\u7d20\u5316\u548c\u793a\u610f\u6027\u8f93\u51fa\u3002\u8be5\u65b9\u6cd5\u4e5f\u6bd4\u57fa\u4e8e 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u7684\u6280\u672f\u66f4\u5b9e\u7528\uff0c\u540e\u8005\u9700\u8981\u5b9a\u5236\u7684\u624b\u5de5 3D \u6a21\u578b\uff0c\u5e76\u4e14\u5b83\u4eec\u53ef\u4ee5\u5904\u7406\u7684\u573a\u666f\u7c7b\u578b\u975e\u5e38\u6709\u9650\u3002|[2402.09883v1](http://arxiv.org/pdf/2402.09883v1)|null|\n", "2402.09812": "|**2024-02-15**|**DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization**|DreamMatcher\uff1a\u5916\u89c2\u5339\u914d\u81ea\u6211\u5173\u6ce8\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u7684\u6587\u672c\u5230\u56fe\u50cf\u4e2a\u6027\u5316|Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, Seunggyu Chang|The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.|\u6587\u672c\u5230\u56fe\u50cf (T2I) \u4e2a\u6027\u5316\u7684\u76ee\u6807\u662f\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u53c2\u8003\u6982\u5ff5\u5b9a\u5236\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u4e0e\u76ee\u6807\u63d0\u793a\u4e00\u81f4\u7684\u6982\u5ff5\u7684\u4e0d\u540c\u56fe\u50cf\u3002\u4f7f\u7528\u72ec\u7279\u7684\u6587\u672c\u5d4c\u5165\u6765\u8868\u793a\u53c2\u8003\u6982\u5ff5\u7684\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u51c6\u786e\u6a21\u4eff\u53c2\u8003\u7684\u5916\u89c2\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u660e\u786e\u5730\u5c06\u53c2\u8003\u56fe\u50cf\u8c03\u8282\u5230\u76ee\u6807\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u79f0\u4e3a\u952e\u503c\u66ff\u6362\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u5de5\u4f5c\u4ec5\u9650\u4e8e\u672c\u5730\u7f16\u8f91\uff0c\u56e0\u4e3a\u5b83\u4eec\u7834\u574f\u4e86\u9884\u8bad\u7ec3 T2I \u6a21\u578b\u7684\u7ed3\u6784\u8def\u5f84\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d2\u4ef6\u65b9\u6cd5\uff0c\u79f0\u4e3a DreamMatcher\uff0c\u5b83\u5c06 T2I \u4e2a\u6027\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u8bed\u4e49\u5339\u914d\u3002\u5177\u4f53\u6765\u8bf4\uff0cDreamMatcher \u5c06\u76ee\u6807\u503c\u66ff\u6362\u4e3a\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u5bf9\u9f50\u7684\u53c2\u8003\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u8def\u5f84\u4e0d\u53d8\uff0c\u4ee5\u4fdd\u7559\u9884\u8bad\u7ec3 T2I \u6a21\u578b\u751f\u6210\u4e0d\u540c\u7ed3\u6784\u7684\u591a\u529f\u80fd\u80fd\u529b\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u8bed\u4e49\u4e00\u81f4\u7684\u5c4f\u853d\u7b56\u7565\uff0c\u5c06\u4e2a\u6027\u5316\u6982\u5ff5\u4e0e\u76ee\u6807\u63d0\u793a\u5f15\u5165\u7684\u4e0d\u76f8\u5173\u533a\u57df\u9694\u79bb\u5f00\u6765\u3002 DreamMatcher \u4e0e\u73b0\u6709 T2I \u6a21\u578b\u517c\u5bb9\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u663e\u793a\u51fa\u663e\u7740\u6539\u8fdb\u3002\u6df1\u5165\u7684\u5206\u6790\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.09812v1](http://arxiv.org/pdf/2402.09812v1)|null|\n", "2402.09786": "|**2024-02-15**|**Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model**|\u68c0\u67e5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u9274\u522b\u5668\u4e2d\u7684\u75c5\u7406\u504f\u5dee\uff1aStyleGAN3 \u6a21\u578b\u7684\u6848\u4f8b\u7814\u7a76|Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter|Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.|\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u903c\u771f\u7684\u9762\u5b54\uff0c\u4eba\u7c7b\u901a\u5e38\u65e0\u6cd5\u5c06\u5176\u4e0e\u771f\u5b9e\u9762\u5b54\u533a\u5206\u5f00\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u9884\u5148\u8bad\u7ec3\u7684 StyleGAN3 \u6a21\u578b\uff08\u4e00\u79cd\u6d41\u884c\u7684 GAN \u7f51\u7edc\uff09\u4e2d\u7684\u5224\u522b\u5668\u4f1a\u6839\u636e\u56fe\u50cf\u548c\u9762\u90e8\u8d28\u91cf\u5bf9\u5206\u6570\u8fdb\u884c\u7cfb\u7edf\u5206\u5c42\uff0c\u8fd9\u5bf9\u8de8\u6027\u522b\u3001\u79cd\u65cf\u548c\u5176\u4ed6\u7c7b\u522b\u7684\u56fe\u50cf\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002\u6211\u4eec\u68c0\u67e5\u4e86\u9274\u522b\u5668\u5728\u611f\u77e5\u79cd\u65cf\u548c\u6027\u522b\u7684\u8f74\u4e0a\u5bf9\u989c\u8272\u548c\u4eae\u5ea6\u7684\u504f\u89c1\uff1b\u7136\u540e\u6211\u4eec\u7814\u7a76\u793e\u4f1a\u5fc3\u7406\u5b66\u523b\u677f\u5370\u8c61\u7814\u7a76\u4e2d\u5e38\u89c1\u7684\u8f74\u3002|[2402.09786v1](http://arxiv.org/pdf/2402.09786v1)|null|\n", "2402.09712": "|**2024-02-15**|**Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement**|\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89e3\u5f00\u7684\u5f52\u7eb3\u504f\u5dee|Tao Yang, Cuiling Lan, Yan Lu, Nanning zheng|Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.|\u89e3\u7f20\u7ed3\u8868\u793a\u5b66\u4e60\u81f4\u529b\u4e8e\u63d0\u53d6\u89c2\u5bdf\u6570\u636e\u4e2d\u7684\u5185\u5728\u56e0\u7d20\u3002\u4ee5\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u5206\u89e3\u8fd9\u4e9b\u8868\u793a\u662f\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u901a\u5e38\u9700\u8981\u5b9a\u5236\u7684\u635f\u5931\u51fd\u6570\u6216\u7279\u5b9a\u7684\u7ed3\u6784\u8bbe\u8ba1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u548c\u6846\u67b6\uff0c\u8bc1\u660e\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u5dee\u6765\u4fc3\u8fdb\u89e3\u7f20\u7ed3\u8868\u793a\u7684\u5b66\u4e60\u3002\u6211\u4eec\u5efa\u8bae\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u4e00\u7ec4\u6982\u5ff5\u6807\u8bb0\uff0c\u5e76\u5c06\u5b83\u4eec\u89c6\u4e3a\u56fe\u50cf\u91cd\u5efa\u7684\u6f5c\u5728\u6269\u6563\u7684\u6761\u4ef6\uff0c\u5176\u4e2d\u5bf9\u6982\u5ff5\u6807\u8bb0\u7684\u4ea4\u53c9\u5173\u6ce8\u7528\u4e8e\u6865\u63a5\u7f16\u7801\u5668\u548c\u6269\u6563\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u5728\u6ca1\u6709\u4efb\u4f55\u989d\u5916\u7684\u6b63\u5219\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u6846\u67b6\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89e3\u7f20\u7ed3\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4ee5\u524d\u6240\u6709\u5177\u6709\u590d\u6742\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u548c\u53ef\u89c6\u5316\u5206\u6790\uff0c\u9610\u660e\u4e86\u8be5\u6a21\u578b\u7684\u529f\u80fd\u3002\u8fd9\u662f\u7b2c\u4e00\u9879\u63ed\u793a\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u89e3\u7f20\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u4e0d\u9700\u8981\u590d\u6742\u7684\u8bbe\u8ba1\u3002\u6211\u4eec\u9884\u8ba1\u6211\u4eec\u7684\u53d1\u73b0\u5c06\u6fc0\u53d1\u66f4\u591a\u7684\u7814\u7a76\uff0c\u63a2\u7d22\u89e3\u7ea0\u7f20\u8868\u793a\u5b66\u4e60\u7684\u6269\u6563\uff0c\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u6790\u548c\u7406\u89e3\u3002|[2402.09712v1](http://arxiv.org/pdf/2402.09712v1)|null|\n", "2402.09677": "|**2024-02-15**|**Prompt-based Personalized Federated Learning for Medical Visual Question Answering**|\u57fa\u4e8e\u63d0\u793a\u7684\u4e2a\u6027\u5316\u8054\u5408\u5b66\u4e60\u533b\u5b66\u89c6\u89c9\u95ee\u7b54|He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama|We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods. Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client. To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters. In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients. Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u4e2a\u6027\u5316\u8054\u5408\u5b66\u4e60\uff08pFL\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u65b9\u6cd5\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u9690\u79c1\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u6765\u81ea\u4e0d\u540c\u5668\u5b98\u7684\u533b\u5b66\u6570\u636e\u96c6\u89c6\u4e3a\u5ba2\u6237\u7aef\uff0c\u5e76\u4f7f\u7528 pFL \u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8bad\u7ec3\u57fa\u4e8e Transformer \u7684\u4e2a\u6027\u5316 VQA \u6a21\u578b\u3002\u4e3a\u4e86\u89e3\u51b3\u4ee5\u524d\u7684 pFL \u65b9\u6cd5\u4e2d\u5ba2\u6237\u7aef\u5230\u5ba2\u6237\u7aef\u901a\u4fe1\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u5c0f\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u63d0\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u6d01\u7684\u4fe1\u606f\u5171\u4eab\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u53ef\u9760\u6027\u53c2\u6570\uff0c\u4ee5\u9632\u6b62\u4f4e\u6027\u80fd\u548c\u4e0d\u76f8\u5173\u5ba2\u6237\u7aef\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u6700\u540e\uff0c\u5bf9\u5404\u79cd\u5f02\u6784\u533b\u5b66\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.09677v1](http://arxiv.org/pdf/2402.09677v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.10002": "|**2024-02-15**|**MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding**|MM-Point\uff1a\u591a\u89c6\u56fe\u4fe1\u606f\u589e\u5f3a\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763 3D \u70b9\u4e91\u7406\u89e3|Hai-Tao Yu, Mofei Song|In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation.|\u5728\u611f\u77e5\u4e2d\uff0c\u96c6\u6210\u591a\u79cd\u611f\u5b98\u4fe1\u606f\uff0c\u5c06\u89c6\u89c9\u4fe1\u606f\u4ece 2D \u89c6\u56fe\u6620\u5c04\u5230 3D \u7269\u4f53\uff0c\u8fd9\u6709\u5229\u4e8e 3D \u73af\u5883\u4e2d\u7684\u7406\u89e3\u3002\u4f46\u5bf9\u4e8e\u4ece\u4e0d\u540c\u89d2\u5ea6\u6e32\u67d3\u7684\u5355\u4e2a2D\u89c6\u56fe\u800c\u8a00\uff0c\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u7684\u90e8\u5206\u4fe1\u606f\u3002\u591a\u89c6\u56fe2D\u4fe1\u606f\u7684\u4e30\u5bcc\u6027\u548c\u4ef7\u503c\u53ef\u4ee5\u4e3a3D\u5bf9\u8c61\u63d0\u4f9b\u4f18\u8d8a\u7684\u81ea\u76d1\u7763\u4fe1\u53f7\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u65b9\u6cd5MM-Point\uff0c\u8be5\u65b9\u6cd5\u7531\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u76f8\u4f3c\u6027\u76ee\u6807\u9a71\u52a8\u3002 MM-Point\u7684\u6838\u5fc3\u5728\u4e8e3D\u5bf9\u8c61\u4e0e\u591a\u4e2a2D\u89c6\u56fe\u540c\u65f6\u8fdb\u884c\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u4f20\u8f93\u3002\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u540c\u65f6\u6267\u884c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684 2D \u591a\u89c6\u56fe\u4fe1\u606f\u7684\u4e00\u81f4\u8de8\u6a21\u6001\u76ee\u6807\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86 Multi-MLP \u548c Multi-level Augmentation \u7b56\u7565\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d8\u6362\u7b56\u7565\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5b66\u4e602D\u591a\u89c6\u56fe\u4e2d\u7684\u591a\u7ea7\u4e0d\u53d8\u6027\u3002 MM-Point \u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5b83\u5728\u5408\u6210\u6570\u636e\u96c6 ModelNet40 \u4e0a\u5b9e\u73b0\u4e86 92.4% \u7684\u5cf0\u503c\u51c6\u786e\u7387\uff0c\u5728\u73b0\u5b9e\u6570\u636e\u96c6 ScanObjectNN \u4e0a\u5b9e\u73b0\u4e86 87.8% \u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4e0e\u5b8c\u5168\u76d1\u7763\u7684\u65b9\u6cd5\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u5c11\u955c\u5934\u5206\u7c7b\u30013D \u96f6\u4ef6\u5206\u5272\u548c 3D \u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002|[2402.10002v1](http://arxiv.org/pdf/2402.10002v1)|null|\n", "2402.09989": "|**2024-02-15**|**LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition**|\u6cd5\u5b66\u7855\u58eb\u4f5c\u4e3a\u6865\u6881\uff1a\u91cd\u65b0\u5236\u5b9a\u624e\u6839\u591a\u6a21\u6001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b|Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan|Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.|\u624e\u6839\u591a\u6a21\u6001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08GMNER\uff09\u662f\u4e00\u9879\u65b0\u5174\u7684\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u65e8\u5728\u8bc6\u522b\u547d\u540d\u5b9e\u4f53\u3001\u5b9e\u4f53\u7c7b\u578b\u53ca\u5176\u76f8\u5e94\u7684\u89c6\u89c9\u533a\u57df\u3002 GMNER \u4efb\u52a1\u8868\u73b0\u51fa\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7279\u6027\uff1a1\uff09\u793e\u4ea4\u5a92\u4f53\u4e2d\u56fe\u50cf\u6587\u672c\u5bf9\u4e4b\u95f4\u7684\u5f31\u76f8\u5173\u6027\u5bfc\u81f4\u5f88\u5927\u4e00\u90e8\u5206\u547d\u540d\u5b9e\u4f53\u65e0\u6cd5\u63a5\u5730\u3002 2\uff09\u7c7b\u4f3c\u4efb\u52a1\u4e2d\u5e38\u7528\u7684\u7c97\u7c92\u5ea6\u6307\u79f0\u8868\u8fbe\u5f0f\uff08\u4f8b\u5982\uff0c\u77ed\u8bed\u672c\u5730\u5316\u3001\u6307\u79f0\u8868\u8fbe\u5f0f\u7406\u89e3\uff09\u548c\u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u4e4b\u95f4\u5b58\u5728\u533a\u522b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RiVEG\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u8fde\u63a5\u6865\u6881\uff0c\u5c06 GMNER \u91cd\u65b0\u6784\u5efa\u4e3a\u8054\u5408 MNER-VE-VG \u4efb\u52a1\u3002\u8fd9\u79cd\u91cd\u65b0\u8868\u8ff0\u5e26\u6765\u4e86\u4e24\u4e2a\u597d\u5904\uff1a1\uff09\u5b83\u4fdd\u6301\u4e86\u6700\u4f73\u7684 MNER \u6027\u80fd\uff0c\u5e76\u4e14\u65e0\u9700\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u6765\u9884\u5148\u63d0\u53d6\u533a\u57df\u7279\u5f81\uff0c\u4ece\u800c\u81ea\u7136\u5730\u89e3\u51b3\u4e86\u73b0\u6709 GMNER \u65b9\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\u3002 2\uff09\u5f15\u5165\u5b9e\u4f53\u6269\u5c55\u8868\u8fbe\u548c\u89c6\u89c9\u8574\u6db5\uff08VE\uff09\u6a21\u5757\uff0c\u7edf\u4e00\u4e86\u89c6\u89c9\u57fa\u7840\uff08VG\uff09\u548c\u5b9e\u4f53\u57fa\u7840\uff08EG\uff09\u3002\u5b83\u4f7f RiVEG \u80fd\u591f\u8f7b\u677e\u7ee7\u627f\u4efb\u4f55\u5f53\u524d\u6216\u672a\u6765\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u89c6\u89c9\u8574\u6db5\u548c\u89c6\u89c9\u63a5\u5730\u529f\u80fd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRiVEG \u5728\u73b0\u6709 GMNER \u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6240\u6709\u4e09\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86 10.65%\u30016.21% \u548c 8.83% \u7684\u7edd\u5bf9\u9886\u5148\u3002|[2402.09989v1](http://arxiv.org/pdf/2402.09989v1)|null|\n", "2402.09801": "|**2024-02-15**|**EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models**|EFUF\uff1a\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u9057\u5fd8\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u8f7b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9|Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai|Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u8fc7\u53bb\u51e0\u5e74\u4e2d\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u53ef\u80fd\u751f\u6210\u5305\u542b\u76f8\u5e94\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u7269\u4f53\u7684\u63cf\u8ff0\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u7269\u4f53\u5e7b\u89c9\u3002\u4e3a\u4e86\u6d88\u9664\u5e7b\u89c9\uff0c\u73b0\u6709\u65b9\u6cd5\u624b\u52a8\u6ce8\u91ca\u6709\u5e7b\u89c9\u548c\u6ca1\u6709\u5e7b\u89c9\u7684\u914d\u5bf9\u54cd\u5e94\uff0c\u7136\u540e\u91c7\u7528\u5404\u79cd\u5bf9\u9f50\u7b97\u6cd5\u6765\u63d0\u9ad8\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5bf9\u9f50\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u4ec5\u5728\u5fae\u8c03\u9636\u6bb5\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u4e14\u8fd8\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6ce8\u91ca\u6765\u6784\u5efa\u6bd4\u5bf9\u7b97\u6cd5\u6240\u9700\u7684\u914d\u5bf9\u6570\u636e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u501f\u9274\u4e86\u9057\u5fd8\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7ec6\u7c92\u5ea6\u9057\u5fd8\u6846\u67b6\uff08EFUF\uff09\uff0c\u5b83\u53ef\u4ee5\u6d88\u9664\u5e7b\u89c9\uff0c\u800c\u4e0d\u9700\u8981\u914d\u5bf9\u6570\u636e\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u59cb\u7ec8\u5982\u4e00\u5730\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u4ee5\u9002\u5ea6\u7684\u8ba1\u7b97\u5f00\u9500\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002|[2402.09801v1](http://arxiv.org/pdf/2402.09801v1)|null|\n", "2402.09717": "|**2024-02-15**|**Visually Dehallucinative Instruction Generation: Know What You Don't Know**|\u89c6\u89c9\u53bb\u5e7b\u89c9\u6307\u4ee4\u751f\u6210\uff1a\u77e5\u9053\u4f60\u4e0d\u77e5\u9053\u7684\u4e1c\u897f|Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang|\"When did the emperor Napoleon invented iPhone?\" Such hallucination-inducing question is well known challenge in generative language modeling. In this study, we present an innovative concept of visual hallucination, referred to as \"I Know (IK)\" hallucination, to address scenarios where \"I Don't Know\" is the desired response. To effectively tackle this issue, we propose the VQAv2-IDK benchmark, the subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators. Stepping further, we present the visually dehallucinative instruction generation method for IK hallucination and introduce the IDK-Instructions visual instruction database. Our experiments show that current methods struggle with IK hallucination. Yet, our approach effectively reduces these hallucinations, proving its versatility across different frameworks and datasets.|\u201c\u62ff\u7834\u4ed1\u7687\u5e1d\u4ec0\u4e48\u65f6\u5019\u53d1\u660e\u4e86iPhone\uff1f\u201d\u8fd9\u79cd\u5f15\u8d77\u5e7b\u89c9\u7684\u95ee\u9898\u662f\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u4e2d\u4f17\u6240\u5468\u77e5\u7684\u6311\u6218\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u89c6\u5e7b\u89c9\u6982\u5ff5\uff0c\u79f0\u4e3a\u201c\u6211\u77e5\u9053\uff08IK\uff09\u201d\u5e7b\u89c9\uff0c\u4ee5\u89e3\u51b3\u201c\u6211\u4e0d\u77e5\u9053\u201d\u662f\u6240\u9700\u53cd\u5e94\u7684\u573a\u666f\u3002\u4e3a\u4e86\u6709\u6548\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VQAv2-IDK \u57fa\u51c6\uff0cVQAv2 \u7684\u5b50\u96c6\u5305\u542b\u7531\u4eba\u7c7b\u6ce8\u91ca\u8005\u786e\u5b9a\u7684\u65e0\u6cd5\u56de\u7b54\u7684\u56fe\u50cf-\u95ee\u9898\u5bf9\u3002\u66f4\u8fdb\u4e00\u6b65\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 IK \u5e7b\u89c9\u7684\u89c6\u89c9\u53bb\u5e7b\u89c9\u6307\u4ee4\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86 IDK-Instructions \u89c6\u89c9\u6307\u4ee4\u6570\u636e\u5e93\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5f88\u96be\u5e94\u5bf9 IK \u5e7b\u89c9\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u5c11\u4e86\u8fd9\u4e9b\u5e7b\u89c9\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u4e0d\u540c\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e0a\u7684\u591a\u529f\u80fd\u6027\u3002|[2402.09717v1](http://arxiv.org/pdf/2402.09717v1)|null|\n", "2402.09671": "|**2024-02-15**|**Exploiting Alpha Transparency In Language And Vision-Based AI Systems**|\u5728\u57fa\u4e8e\u8bed\u8a00\u548c\u89c6\u89c9\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u5229\u7528 Alpha \u900f\u660e\u5ea6|David Noever, Forrest McKee|This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack's potential breadth. This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or updates. Instead, they require retraining and architectural changes, indicating a persistent hole in multimodal technologies without some future adversarial hardening against such vision-language exploits.|\u8fd9\u9879\u8c03\u67e5\u63ed\u793a\u4e86\u4e00\u79cd\u6e90\u81ea PNG \u56fe\u50cf\u6587\u4ef6\u683c\u5f0f\u7684\u65b0\u9896\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5176 alpha \u900f\u660e\u5c42\uff0c\u53ca\u5176\u6b3a\u9a97\u591a\u4e2a AI \u89c6\u89c9\u7cfb\u7edf\u7684\u6f5c\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8fd9\u4e2a alpha \u5c42\u4f5c\u4e3a\u4eba\u7c7b\u89c2\u5bdf\u8005\u770b\u4e0d\u89c1\u7684\u79d8\u5bc6\u901a\u9053\uff0c\u4f46\u4eba\u5de5\u667a\u80fd\u56fe\u50cf\u5904\u7406\u5668\u5b8c\u5168\u53ef\u4ee5\u64cd\u4f5c\u3002\u8be5\u6f0f\u6d1e\u7684\u6d4b\u8bd5\u8303\u56f4\u6db5\u76d6\u4e86\u82f9\u679c\u3001\u5fae\u8f6f\u3001\u8c37\u6b4c\u3001Salesforce\u3001Nvidia \u548c Facebook \u7b49\u4ee3\u8868\u6027\u89c6\u89c9\u7cfb\u7edf\uff0c\u51f8\u663e\u4e86\u8be5\u653b\u51fb\u7684\u6f5c\u5728\u5e7f\u5ea6\u3002\u8be5\u6f0f\u6d1e\u5bf9\u73b0\u6709\u548c\u73b0\u573a\u89c6\u89c9\u7cfb\u7edf\uff08\u4ece\u533b\u5b66\u6210\u50cf\u5230\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\uff09\u7684\u5b89\u5168\u534f\u8bae\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u53d7\u5f71\u54cd\u7684\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6216\u6700\u65b0\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u6cd5\u901a\u8fc7\u7b80\u5355\u7684\u8865\u4e01\u6216\u66f4\u65b0\u5feb\u901f\u7f13\u89e3\u8fd9\u4e9b\u6f0f\u6d1e\u3002\u76f8\u53cd\uff0c\u5b83\u4eec\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u548c\u67b6\u6784\u66f4\u6539\uff0c\u8fd9\u8868\u660e\u591a\u6a21\u5f0f\u6280\u672f\u4e2d\u5b58\u5728\u6301\u7eed\u7684\u6f0f\u6d1e\uff0c\u800c\u672a\u6765\u6ca1\u6709\u9488\u5bf9\u6b64\u7c7b\u89c6\u89c9\u8bed\u8a00\u6f0f\u6d1e\u7684\u5bf9\u6297\u6027\u5f3a\u5316\u3002|[2402.09671v1](http://arxiv.org/pdf/2402.09671v1)|null|\n", "2402.09635": "|**2024-02-15**|**VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs**|VisIRNet\uff1a\u65e0\u4eba\u673a\u62cd\u6444\u7684\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u5bf9\u7684\u6df1\u5ea6\u56fe\u50cf\u5bf9\u9f50|Sedat Ozer, Alain P. Ndigande|This paper proposes a deep learning based solution for multi-modal image alignment regarding UAV-taken images. Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment. However, we show that we can achieve state of the art results without using LK-based methods. Our approach carefully utilizes a two-branch based convolutional neural network (CNN) based on feature embedding blocks. We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly. Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment. We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u62cd\u6444\u56fe\u50cf\u7684\u591a\u6a21\u6001\u56fe\u50cf\u5bf9\u9f50\u3002\u8bb8\u591a\u6700\u8fd1\u63d0\u51fa\u7684\u6700\u5148\u8fdb\u7684\u5bf9\u9f50\u6280\u672f\u4f9d\u8d56\u4e8e\u4f7f\u7528\u57fa\u4e8e Lucas-Kanade (LK) \u7684\u89e3\u51b3\u65b9\u6848\u6765\u6210\u529f\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u57fa\u4e8e LK \u7684\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8c28\u614e\u5730\u5229\u7528\u4e86\u57fa\u4e8e\u7279\u5f81\u5d4c\u5165\u5757\u7684\u4e24\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4e24\u79cd\u53d8\u4f53\uff0c\u5176\u4e2d\u5728\u7b2c\u4e00\u4e2a\u53d8\u4f53\uff08\u6a21\u578bA\uff09\u4e2d\uff0c\u6211\u4eec\u76f4\u63a5\u9884\u6d4b\u4ec5\u8981\u5bf9\u9f50\u7684\u56fe\u50cf\u56db\u4e2a\u89d2\u7684\u65b0\u5750\u6807\uff1b\u5728\u7b2c\u4e8c\u4e2a\u6a21\u578b\uff08ModelB\uff09\u4e2d\uff0c\u6211\u4eec\u76f4\u63a5\u9884\u6d4b\u5355\u5e94\u6027\u77e9\u9635\u3002\u5728\u56fe\u50cf\u89d2\u4e0a\u5e94\u7528\u5bf9\u9f50\u4f1a\u5f3a\u5236\u7b97\u6cd5\u4ec5\u5339\u914d\u8fd9\u56db\u4e2a\u89d2\uff0c\u800c\u4e0d\u662f\u8ba1\u7b97\u548c\u5339\u914d\u8bb8\u591a\uff08\u5173\u952e\uff09\u70b9\uff0c\u56e0\u4e3a\u540e\u8005\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8bb8\u591a\u5f02\u5e38\u503c\uff0c\u4ece\u800c\u4ea7\u751f\u4e0d\u592a\u51c6\u786e\u7684\u5bf9\u9f50\u3002\u4e0e\u73b0\u6709\u7684\u6700\u65b0\u57fa\u4e8e\u6df1\u5ea6 LK \u7684\u67b6\u6784\u76f8\u6bd4\uff0c\u6211\u4eec\u5728\u56db\u4e2a\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002|[2402.09635v1](http://arxiv.org/pdf/2402.09635v1)|null|\n"}, "Nerf": {}, "3DGS": {"2402.10128": "|**2024-02-15**|**GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering**|GES\uff1a\u7528\u4e8e\u9ad8\u6548\u8f90\u5c04\u573a\u6e32\u67d3\u7684\u5e7f\u4e49\u6307\u6570\u6cfc\u6e85|Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi|Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .|3D \u9ad8\u65af\u55b7\u5c04\u6280\u672f\u7684\u8fdb\u6b65\u663e\u7740\u52a0\u901f\u4e86 3D \u91cd\u5efa\u548c\u751f\u6210\u3002\u7136\u800c\uff0c\u5b83\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u9ad8\u65af\uff0c\u8fd9\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u5185\u5b58\u5360\u7528\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 GES\uff08\u5e7f\u4e49\u6307\u6570\u5206\u5e03\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u91c7\u7528\u5e7f\u4e49\u6307\u6570\u51fd\u6570 (GEF) \u6765\u5efa\u6a21 3D \u573a\u666f\u7684\u65b0\u9896\u8868\u793a\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u5c11\u7684\u7c92\u5b50\u6765\u8868\u793a\u573a\u666f\uff0c\u56e0\u6b64\u5728\u5373\u63d2\u5373\u7528\u7684\u6548\u7387\u4e0a\u663e\u7740\u4f18\u4e8e\u9ad8\u65af\u5206\u5e03\u65b9\u6cd5\u57fa\u4e8e\u9ad8\u65af\u7684\u516c\u7528\u4e8b\u4e1a\u7684\u66ff\u4ee3\u80fd\u529b\u3002 GES \u5728\u539f\u5219\u6027 1D \u8bbe\u7f6e\u548c\u73b0\u5b9e 3D \u573a\u666f\u4e2d\u90fd\u7ecf\u8fc7\u4e86\u7406\u8bba\u548c\u7ecf\u9a8c\u9a8c\u8bc1\u3002\u5b83\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8868\u793a\u5177\u6709\u9510\u5229\u8fb9\u7f18\u7684\u4fe1\u53f7\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u65af\u51fd\u6570\u6765\u8bf4\u901a\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5176\u56fa\u6709\u7684\u4f4e\u901a\u7279\u6027\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0cGEF \u5728\u62df\u5408\u81ea\u7136\u53d1\u751f\u7684\u4fe1\u53f7\uff08\u4f8b\u5982\u6b63\u65b9\u5f62\u3001\u4e09\u89d2\u5f62\u548c\u629b\u7269\u7ebf\u4fe1\u53f7\uff09\u65b9\u9762\u4f18\u4e8e\u9ad8\u65af\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u5206\u88c2\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u4f1a\u589e\u52a0\u9ad8\u65af\u5206\u5e03\u7684\u5185\u5b58\u5360\u7528\u3002\u501f\u52a9\u8c03\u9891\u635f\u8017\uff0cGES \u5728\u65b0\u9896\u89c6\u56fe\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6240\u9700\u5185\u5b58\u5b58\u50a8\u91cf\u4e0d\u5230 Gaussian Splatting \u7684\u4e00\u534a\uff0c\u5e76\u5c06\u6e32\u67d3\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe 39%\u3002\u8be5\u4ee3\u7801\u53ef\u5728\u9879\u76ee\u7f51\u7ad9 https://abdullahamdi.com/ges \u4e0a\u83b7\u53d6\u3002|[2402.10128v1](http://arxiv.org/pdf/2402.10128v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.10026": "|**2024-02-15**|**Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification**|\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u6df7\u5408 CNN Bi-LSTM \u795e\u7ecf\u7f51\u7edc|Alok Ranjan Sahoo, Pavan Chakraborty|Hyper spectral images have drawn the attention of the researchers for its complexity to classify. It has nonlinear relation between the materials and the spectral information provided by the HSI image. Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods. Use of 3-D CNN along with 2-D CNN have shown great success for learning spatial and spectral features. However, it uses comparatively large number of parameters. Moreover, it is not effective to learn inter layer information. Hence, this paper proposes a neural network combining 3-D CNN, 2-D CNN and Bi-LSTM. The performance of this model has been tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data sets. The results are compared with the state of-the-art deep learning-based models. This model performed better in all three datasets. It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively.|\u9ad8\u5149\u8c31\u56fe\u50cf\u56e0\u5176\u5206\u7c7b\u7684\u590d\u6742\u6027\u800c\u5f15\u8d77\u4e86\u7814\u7a76\u4eba\u5458\u7684\u5173\u6ce8\u3002\u6750\u6599\u4e0e HSI \u56fe\u50cf\u63d0\u4f9b\u7684\u5149\u8c31\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u975e\u7ebf\u6027\u5173\u7cfb\u3002\u4e0e\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5b66\u4e60\u8fd9\u79cd\u975e\u7ebf\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\u3002 3-D CNN \u548c 2-D CNN \u7684\u4f7f\u7528\u5728\u5b66\u4e60\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u4f7f\u7528\u76f8\u5bf9\u8f83\u591a\u7684\u53c2\u6570\u3002\u800c\u4e14\uff0c\u5b66\u4e60\u5c42\u95f4\u4fe1\u606f\u5e76\u4e0d\u6709\u6548\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408 3-D CNN\u30012-D CNN \u548c Bi-LSTM \u7684\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u6a21\u578b\u7684\u6027\u80fd\u5df2\u7ecf\u5728 Indian Pines(IP)\u3001University of Pavia(PU) \u548c Salinas Scene(SA) \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u7ed3\u679c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u8be5\u6a21\u578b\u5728\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u90fd\u8868\u73b0\u66f4\u597d\u3002\u4ec5\u4f7f\u7528 IP\u3001PU \u548c SA \u6570\u636e\u96c6\u4e2d\u6700\u5148\u8fdb\u6a21\u578b\u7684 30% \u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5b83\u5c31\u53ef\u4ee5\u5206\u522b\u8fbe\u5230 99.83%\u300199.98% \u548c 100% \u7684\u51c6\u786e\u7387\u3002|[2402.10026v1](http://arxiv.org/pdf/2402.10026v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.10130": "|**2024-02-15**|**Is Continual Learning Ready for Real-world Challenges?**|\u6301\u7eed\u5b66\u4e60\u51c6\u5907\u597d\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\u4e86\u5417\uff1f|Theodora Kontogianni, Yuanwen Yue, Siyu Tang, Konrad Schindler|Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in realistic settings. Our paper aims to initiate a paradigm shift, advocating for the adoption of continual learning methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field.|\u5c3d\u7ba1\u6301\u7eed\u5b66\u4e60\u6709\u7740\u60a0\u4e45\u800c\u5b8c\u5584\u7684\u5b66\u672f\u5386\u53f2\uff0c\u4f46\u5b83\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u76f8\u5f53\u6709\u9650\u3002\u672c\u6587\u8ba4\u4e3a\uff0c\u8fd9\u79cd\u5dee\u8ddd\u5f52\u56e0\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u5b9e\u9645\u6311\u6218\u4e0e\u6240\u4f7f\u7528\u7684\u8bc4\u4f30\u534f\u8bae\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u4f7f\u7528\u65b0\u7684 3D \u8bed\u4e49\u5206\u5272\u57fa\u51c6 OCL-3DSS \u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u5047\u8bbe\u5e76\u8bc4\u4f30\u4e86\u8fc4\u4eca\u4e3a\u6b62\u7684\u8fdb\u5c55\u3002\u6211\u4eec\u901a\u8fc7\u5229\u7528\u66f4\u73b0\u5b9e\u7684\u534f\u8bae\u6765\u7814\u7a76\u6587\u732e\u4e2d\u7684\u5404\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6848\uff0c\u8fd9\u4e9b\u534f\u8bae\u9700\u8981\u9488\u5bf9\u52a8\u6001\u3001\u771f\u5b9e\u573a\u666f\uff08\u4f8b\u5982\uff0c\u5728\u673a\u5668\u4eba\u548c 3D \u89c6\u89c9\u5e94\u7528\u4e2d\uff09\u8fdb\u884c\u5728\u7ebf\u548c\u6301\u7eed\u5b66\u4e60\u3002\u7ed3\u679c\u53d1\u4eba\u6df1\u7701\uff1a\u6240\u6709\u8003\u8651\u7684\u65b9\u6cd5\u90fd\u8868\u73b0\u4e0d\u4f73\uff0c\u660e\u663e\u504f\u79bb\u8054\u5408\u79bb\u7ebf\u8bad\u7ec3\u7684\u4e0a\u9650\u3002\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u7684\u95ee\u9898\u3002\u6211\u4eec\u7684\u8bba\u6587\u65e8\u5728\u53d1\u8d77\u8303\u5f0f\u8f6c\u53d8\uff0c\u5021\u5bfc\u901a\u8fc7\u65b0\u7684\u5b9e\u9a8c\u534f\u8bae\u91c7\u7528\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u66f4\u597d\u5730\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u6761\u4ef6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7a81\u7834\u3002|[2402.10130v1](http://arxiv.org/pdf/2402.10130v1)|null|\n", "2402.10093": "|**2024-02-15**|**MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations**|MIM-Refiner\uff1a\u4e2d\u95f4\u9884\u8bad\u7ec3\u8868\u793a\u7684\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347|Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter|We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner|\u6211\u4eec\u5f15\u5165\u4e86 MIM\uff08\u63a9\u6a21\u56fe\u50cf\u5efa\u6a21\uff09-Refiner\uff0c\u5b83\u662f\u9884\u8bad\u7ec3 MIM \u6a21\u578b\u7684\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u5de5\u5177\u3002 MIM-Refiner \u80cc\u540e\u7684\u52a8\u673a\u690d\u6839\u4e8e\u8fd9\u6837\u7684\u89c1\u89e3\uff1aMIM \u6a21\u578b\u4e2d\u7684\u6700\u4f73\u8868\u793a\u901a\u5e38\u4f4d\u4e8e\u4e2d\u95f4\u5c42\u3002\u56e0\u6b64\uff0cMIM-Refiner \u5229\u7528\u8fde\u63a5\u5230\u4e0d\u540c\u4e2d\u95f4\u5c42\u7684\u591a\u4e2a\u5bf9\u6bd4\u5934\u3002\u5728\u6bcf\u4e2a\u5934\u4e2d\uff0c\u4fee\u6539\u540e\u7684\u6700\u8fd1\u90bb\u76ee\u6807\u6709\u52a9\u4e8e\u6784\u5efa\u5404\u81ea\u7684\u8bed\u4e49\u7c07\u3002\u7ec6\u5316\u8fc7\u7a0b\u867d\u7136\u77ed\u6682\u4f46\u6709\u6548\u3002\u5728\u51e0\u4e2a\u65f6\u671f\u5185\uff0c\u6211\u4eec\u5c06 MIM \u6a21\u578b\u7684\u529f\u80fd\u4ece\u4f4e\u4e8e\u6807\u51c6\u7684\u529f\u80fd\u6539\u8fdb\u4e3a\u6700\u5148\u8fdb\u7684\u73b0\u6210\u529f\u80fd\u3002\u6539\u8fdb\u5728 ImageNet-1K \u4e0a\u4f7f\u7528 data2vec 2.0 \u8fdb\u884c\u9884\u8bad\u7ec3\u7684 ViT-H\uff0c\u5728 ImageNet-1K \u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\u7684\u7ebf\u6027\u63a2\u6d4b (84.7%) \u548c\u4f4e\u6837\u672c\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5728 ImageNet-1K 1-shot \u5206\u7c7b\u4e2d\uff0cMIM-Refiner \u521b\u4e0b\u4e86 64.2% \u7684\u65b0\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f18\u4e8e\u4f7f\u7528\u591a\u8fbe 2000 \u500d\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u5927\u578b\u6a21\u578b\uff0c\u4f8b\u5982 DINOv2-g\u3001OpenCLIP-G \u548c MAWS\u3002 6.5B\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://ml-jku.github.io/MIM-Refiner|[2402.10093v1](http://arxiv.org/pdf/2402.10093v1)|null|\n", "2402.10035": "|**2024-02-15**|**Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity**|\u5177\u6709\u7edf\u8ba1\u5f02\u8d28\u6027\u7684\u89c6\u7f51\u819c\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u5206\u7c7b\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u7814\u7a76|Sanskar Amgain, Prashant Shrestha, Sophia Bano, Ignacio del Valle Torres, Michael Cunniffe, Victor Hernandez, Phil Beales, Binod Bhattarai|Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings.   Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.|\u76ee\u7684\uff1a\u6211\u4eec\u5e94\u7528\u8054\u90a6\u5b66\u4e60\u6765\u8bad\u7ec3 OCT \u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u6a21\u62df\u5177\u6709\u591a\u4e2a\u5ba2\u6237\u7aef\u548c\u7edf\u8ba1\u5f02\u6784\u6570\u636e\u5206\u5e03\u7684\u73b0\u5b9e\u573a\u666f\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u4e2d\u7684\u6570\u636e\u5b8c\u5168\u7f3a\u4e4f\u67d0\u4e9b\u7c7b\u522b\u7684\u6837\u672c\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u7814\u7a76\u4e86 FedAvg \u548c FedProx \u4ee5\u5206\u6563\u65b9\u5f0f\u8bad\u7ec3 OCT \u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u4e0e\u96c6\u4e2d\u6570\u636e\u76f8\u5173\u7684\u9690\u79c1\u95ee\u9898\u3002\u6211\u4eec\u5728 IID \u548c\u975e IID \u8bbe\u7f6e\u4e0b\u5c06\u516c\u5f00\u53ef\u7528\u7684 OCT \u6570\u636e\u96c6\u5212\u5206\u5230\u591a\u4e2a\u5ba2\u6237\u7aef\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u5b50\u96c6\u8fdb\u884c\u672c\u5730\u8bad\u7ec3\u3002\u6211\u4eec\u9488\u5bf9\u8fd9\u4e9b\u8bbe\u7f6e\u8bc4\u4f30\u4e86\u4e24\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5 FedAvg \u548c FedProx\u3002\u7ed3\u679c\uff1a\u6211\u4eec\u5bf9\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 IID \u8bbe\u7f6e\u4e0b\uff0c\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u5728\u4e2d\u592e\u6570\u636e\u6c60\u4e0a\u7684\u8bad\u7ec3\u76f8\u5f53\u3002\u7136\u800c\uff0c\u968f\u7740\u6211\u4eec\u589e\u52a0\u5ba2\u6237\u7aef\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u4e24\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u90fd\u4f1a\u4e0b\u964d\uff0c\u800c\u5728\u5f02\u8d28\u6027\u589e\u52a0\u7684\u8bbe\u7f6e\u4e2d\uff0cFedProx \u59cb\u7ec8\u6bd4 FedAvg \u8868\u73b0\u66f4\u597d\u3002\u7ed3\u8bba\uff1a\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\u5728\u8de8\u591a\u4e2a\u533b\u7597\u673a\u6784\u7684\u79c1\u6709\u6570\u636e\u5229\u7528\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u5927\u91cf\u7684\u5ba2\u6237\u7aef\u548c\u6807\u7b7e\u7684\u5f02\u6784\u5206\u5e03\u964d\u4f4e\u4e86\u4e24\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cFedProx \u4f3c\u4e4e\u5bf9\u589e\u52a0\u7684\u5f02\u8d28\u6027\u66f4\u52a0\u7a33\u5065\u3002|[2402.10035v1](http://arxiv.org/pdf/2402.10035v1)|null|\n", "2402.10021": "|**2024-02-15**|**SAWEC: Sensing-Assisted Wireless Edge Computing**|SAWEC\uff1a\u4f20\u611f\u8f85\u52a9\u65e0\u7ebf\u8fb9\u7f18\u8ba1\u7b97|Khandaker Foysal Haque, Francesca Meneghello, Md. Ebtidaul Karim, Francesco Restuccia|Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link. In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service. Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided. Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics. Hence, only the part of the frames where any environmental change is detected is transmitted and processed. We evaluated SAWEC by using a 10K 360$^{\\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking. We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups. Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches. For reproducibility purposes, we pledge to share our whole dataset and code repository.|\u65b0\u5174\u7684\u79fb\u52a8\u865a\u62df\u73b0\u5b9e (VR) \u7cfb\u7edf\u9700\u8981\u901a\u8fc7\u6267\u884c\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u7684\u7b97\u6cd5\uff0c\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u5e27\u4e0a\u8fde\u7eed\u6267\u884c\u590d\u6742\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u7531\u4e8e\u6700\u5148\u8fdb\u7684 DNN \u9700\u8981\u7684\u8ba1\u7b97\u80fd\u529b\u5bf9\u4e8e\u79fb\u52a8\u8bbe\u5907\u800c\u8a00\u8fc7\u4e8e\u5e9e\u5927\uff0c\u56e0\u6b64\u6700\u8fd1\u63d0\u51fa\u4e86\u57fa\u4e8e\u65e0\u7ebf\u8fb9\u7f18\u8ba1\u7b97 (WEC) \u7684\u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684WEC\u65b9\u6cd5\u9700\u8981\u4f20\u8f93\u548c\u5904\u7406\u5927\u91cf\u89c6\u9891\u6570\u636e\uff0c\u8fd9\u6700\u7ec8\u53ef\u80fd\u4f7f\u65e0\u7ebf\u94fe\u8def\u9971\u548c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f20\u611f\u8f85\u52a9\u65e0\u7ebf\u8fb9\u7f18\u8ba1\u7b97\uff08SAWEC\uff09\u8303\u4f8b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002 SAWEC \u5229\u7528\u6709\u5173\u7269\u7406\u73af\u5883\u7684\u77e5\u8bc6\uff0c\u4ec5\u5c06\u4e0e\u670d\u52a1\u4ea4\u4ed8\u76f8\u5173\u7684\u6570\u636e\u4f20\u8f93\u5230\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u4ece\u800c\u51cf\u5c11\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u6574\u4f53\u8ba1\u7b97\u8d1f\u62c5\u3002\u6211\u4eec\u7684\u76f4\u89c9\u662f\uff0c\u53ef\u4ee5\u907f\u514d\u4f20\u8f93\u76f8\u5bf9\u4e8e\u5148\u524d\u5e27\u6ca1\u6709\u53d8\u5316\u7684\u89c6\u9891\u5e27\u90e8\u5206\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u65e0\u7ebf\u4f20\u611f\u6280\u672f\u6765\u4f30\u8ba1\u73af\u5883\u4e2d\u7269\u4f53\u7684\u4f4d\u7f6e\u5e76\u83b7\u5f97\u6709\u5173\u73af\u5883\u52a8\u6001\u7684\u89c1\u89e3\u3002\u56e0\u6b64\uff0c\u4ec5\u4f20\u8f93\u548c\u5904\u7406\u68c0\u6d4b\u5230\u4efb\u4f55\u73af\u5883\u53d8\u5316\u7684\u5e27\u90e8\u5206\u3002\u6211\u4eec\u4f7f\u7528 10K 360$^{\\circ}$ \u76f8\u673a\u548c Wi-Fi 6 \u4f20\u611f\u7cfb\u7edf\uff08\u4ee5 160 MHz \u8fd0\u884c\u5e76\u6267\u884c\u5b9a\u4f4d\u548c\u8ddf\u8e2a\uff09\u6765\u8bc4\u4f30 SAWEC\u3002\u6211\u4eec\u5728\u6d88\u58f0\u5ba4\u548c\u5927\u5385\u623f\u95f4\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5176\u4e2d\u6709\u4e24\u540d\u53d7\u8bd5\u8005\u5728\u516d\u79cd\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684WEC\u65b9\u6cd5\u76f8\u6bd4\uff0cSAWEC\u5c06\u4fe1\u9053\u5360\u7528\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u522b\u964d\u4f4e\u4e8693.81%\u548c96.19%\uff0c\u540c\u65f6\u5c06\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\u63d0\u9ad8\u4e8646.98%\u3002\u51fa\u4e8e\u53ef\u91cd\u590d\u6027\u7684\u76ee\u7684\uff0c\u6211\u4eec\u627f\u8bfa\u5171\u4eab\u6211\u4eec\u7684\u6574\u4e2a\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5b58\u50a8\u5e93\u3002|[2402.10021v1](http://arxiv.org/pdf/2402.10021v1)|null|\n", "2402.09990": "|**2024-02-15**|**TIAViz: A Browser-based Visualization Tool for Computational Pathology Models**|TIAViz\uff1a\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u8ba1\u7b97\u75c5\u7406\u5b66\u6a21\u578b\u53ef\u89c6\u5316\u5de5\u5177|Mark Eastwood, John Pocock, Mostafa Jahanifar, Adam Shephard, Skiros Habib, Ethar Alzaid, Abdullah Alsalemi, Jan Lukas Robertus, Nasir Rajpoot, Shan Raza, et.al.|Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos. This tool is open source and is made available at: https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox.|\u6570\u5b57\u75c5\u7406\u5b66\u5728\u73b0\u4ee3\u533b\u7597\u4fdd\u5065\u7cfb\u7edf\u4e2d\u83b7\u5f97\u4e86\u5de8\u5927\u7684\u5173\u6ce8\u3002\u4ece\u5149\u5b66\u663e\u5fae\u955c\u5230\u6570\u5b57\u56fe\u50cf\u7684\u8f6c\u53d8\u5e26\u6765\u4e86\u6539\u8fdb\u8bca\u65ad\u3001\u63d0\u9ad8\u6548\u7387\u4ee5\u53ca\u5c06\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u96c6\u6210\u5230\u75c5\u7406\u5b66\u5bb6\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u53ef\u89c6\u5316\u3002\u5728\u6570\u5b57\u75c5\u7406\u5b66\u673a\u5668\u5b66\u4e60 (ML) \u6a21\u578b\u7684\u6574\u4e2a\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u62e5\u6709\u7075\u6d3b\u3001\u5f00\u653e\u7684\u5de5\u5177\u6765\u53ef\u89c6\u5316\u6a21\u578b\uff08\u4ece\u8f93\u51fa\u548c\u9884\u6d4b\u5230\u7528\u4e8e\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6a21\u578b\u7684\u5e95\u5c42\u6ce8\u91ca\u548c\u56fe\u50cf\uff09\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63a8\u51fa\u4e86 TIAViz\uff0c\u8fd9\u662f\u4e00\u79cd\u5185\u7f6e\u4e8e TIAToolbox \u4e2d\u7684\u57fa\u4e8e Python \u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5b83\u5141\u8bb8\u5c06\u5404\u79cd\u4fe1\u606f\u7075\u6d3b\u3001\u4ea4\u4e92\u5f0f\u3001\u5b8c\u5168\u53ef\u7f29\u653e\u5730\u53e0\u52a0\u5230\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u4e0a\uff0c\u5305\u62ec\u56fe\u5f62\u3001\u70ed\u56fe\u3001\u5206\u5272\u3001\u6ce8\u91ca\u548c\u5176\u4ed6 WSI\u3002 UI \u57fa\u4e8e\u6d4f\u89c8\u5668\uff0c\u5141\u8bb8\u5728\u672c\u5730\u3001\u8fdc\u7a0b\u8ba1\u7b97\u673a\u6216\u670d\u52a1\u5668\u4e0a\u4f7f\u7528\u4ee5\u63d0\u4f9b\u516c\u5f00\u53ef\u7528\u7684\u6f14\u793a\u3002\u8be5\u5de5\u5177\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u4ece\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u53d6\uff1ahttps://github.com/TissueImageAnalytics/tiatoolbox\uff0c\u5e76\u901a\u8fc7 pip \u5b89\u88c5 (pip install tiatoolbox) \u548c conda \u4f5c\u4e3a TIAToolbox \u7684\u4e00\u90e8\u5206\u3002|[2402.09990v1](http://arxiv.org/pdf/2402.09990v1)|null|\n", "2402.09975": "|**2024-02-15**|**Current and future roles of artificial intelligence in retinopathy of prematurity**|\u4eba\u5de5\u667a\u80fd\u5f53\u524d\u548c\u672a\u6765\u5728\u65e9\u4ea7\u513f\u89c6\u7f51\u819c\u75c5\u53d8\u4e2d\u7684\u4f5c\u7528|Ali Jafarizadeh, Shadi Farabi Maleki, Parnia Pouya, Navid Sobhi, Mirsaeed Abdollahi, Siamak Pedrammehr, Chee Peng Lim, Houshyar Asadi, Roohallah Alizadehsani, Ru-San Tan, et.al.|Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI's potential in ROP detection, classification, diagnosis, and prognosis.|\u65e9\u4ea7\u513f\u89c6\u7f51\u819c\u75c5\u53d8 (ROP) \u662f\u4e00\u79cd\u5f71\u54cd\u65e9\u4ea7\u513f\u7684\u4e25\u91cd\u75be\u75c5\uff0c\u5bfc\u81f4\u89c6\u7f51\u819c\u8840\u7ba1\u751f\u957f\u5f02\u5e38\u3001\u89c6\u7f51\u819c\u8131\u79bb\u548c\u6f5c\u5728\u5931\u660e\u3002\u867d\u7136\u8fc7\u53bb\u5df2\u4f7f\u7528\u534a\u81ea\u52a8\u5316\u7cfb\u7edf\u901a\u8fc7\u91cf\u5316\u89c6\u7f51\u819c\u8840\u7ba1\u7279\u5f81\u6765\u8bca\u65ad ROP \u76f8\u5173\u75be\u75c5\uff0c\u4f46\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60 (ML) \u6a21\u578b\u9762\u4e34\u51c6\u786e\u6027\u548c\u8fc7\u5ea6\u62df\u5408\u7b49\u6311\u6218\u3002\u6df1\u5ea6\u5b66\u4e60 (DL) \u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u6700\u65b0\u8fdb\u5c55\u663e\u7740\u6539\u8fdb\u4e86 ROP \u68c0\u6d4b\u548c\u5206\u7c7b\u3002 i-ROP \u6df1\u5ea6\u5b66\u4e60 (i-ROP-DL) \u7cfb\u7edf\u5728\u68c0\u6d4b\u9644\u52a0\u75be\u75c5\u65b9\u9762\u4e5f\u663e\u793a\u51fa\u524d\u666f\uff0c\u63d0\u4f9b\u53ef\u9760\u7684 ROP \u8bca\u65ad\u6f5c\u529b\u3002\u8fd9\u9879\u7814\u7a76\u5168\u9762\u63a2\u8ba8\u4e86\u4f7f\u7528\u89c6\u7f51\u819c\u6210\u50cf\u548c\u4eba\u5de5\u667a\u80fd (AI) \u68c0\u6d4b ROP \u7684\u5f53\u4ee3\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u53ef\u4ee5\u6307\u5bfc\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u57fa\u4e8e\u8be5\u9886\u57df\u7684 89 \u9879\u539f\u59cb\u7814\u7a76\uff08\u7efc\u5408\u5ba1\u67e5\u4e86 1487 \u9879\u7814\u7a76\uff09\uff0c\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u4f20\u7edf\u7684 ROP \u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u4eba\u5de5\u5206\u6790\uff0c\u5bfc\u81f4\u4e34\u5e8a\u51b3\u7b56\u4e0d\u4e00\u81f4\u3002\u4eba\u5de5\u667a\u80fd\u5bf9\u4e8e\u6539\u5584 ROP \u7ba1\u7406\u6709\u7740\u5de8\u5927\u7684\u524d\u666f\u3002\u672c\u7efc\u8ff0\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728 ROP \u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u8bca\u65ad\u548c\u9884\u540e\u65b9\u9762\u7684\u6f5c\u529b\u3002|[2402.09975v1](http://arxiv.org/pdf/2402.09975v1)|null|\n", "2402.09962": "|**2024-02-15**|**ViGEO: an Assessment of Vision GNNs in Earth Observation**|ViGEO\uff1a\u5bf9\u5730\u7403\u89c2\u6d4b\u4e2d\u89c6\u89c9 GNN \u7684\u8bc4\u4f30|Luca Colomba, Paolo Garza|Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs). Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks.|\u536b\u661f\u4efb\u52a1\u548c\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u7cfb\u7edf\u662f\u73af\u5883\u76d1\u6d4b\u548c\u53ca\u65f6\u8bc6\u522b\u707e\u96be\u6027\u4e8b\u4ef6\u3001\u5bf9\u81ea\u7136\u8d44\u6e90\u548c\u4eba\u9020\u8d44\u4ea7\uff08\u4f8b\u5982\u690d\u88ab\u3001\u6c34\u4f53\u3001\u68ee\u6797\u548c\u5efa\u7b51\u7269\uff09\u8fdb\u884c\u957f\u671f\u76d1\u6d4b\u7684\u57fa\u672c\u8d44\u4ea7\u3002\u4e0d\u540c\u7684 EO \u4efb\u52a1\u53ef\u4ee5\u6536\u96c6\u591a\u79cd\u5149\u8c31\u5e26\u5bbd\u7684\u4fe1\u606f\uff0c\u4f8b\u5982 MODIS\u3001Sentinel-1 \u548c Sentinel-2\u3002\u56e0\u6b64\uff0c\u9274\u4e8e\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6807\u8bb0\u6570\u636e\u7684\u53ef\u7528\u6027\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bc1\u660e\u4e86\u571f\u5730\u5229\u7528\u76d1\u6d4b\u7cfb\u7edf\u548c\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u7684\u53ef\u884c\u6027\u548c\u7cbe\u5ea6\u3002\u6b64\u7c7b\u7cfb\u7edf\u53ef\u4ee5\u5e2e\u52a9\u9886\u57df\u4e13\u5bb6\u548c\u653f\u5e9c\u8fdb\u884c\u6301\u7eed\u7684\u73af\u5883\u76d1\u6d4b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u53d1\u751f\u707e\u96be\u6027\u4e8b\u4ef6\uff08\u4f8b\u5982\u504f\u8fdc\u5730\u533a\u7684\u68ee\u6797\u91ce\u706b\uff09\u65f6\u53ca\u65f6\u8fdb\u884c\u5e72\u9884\u3002\u5c3d\u7ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8bb8\u591a\u5de5\u4f5c\u5c06\u5176\u5206\u6790\u9650\u5236\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4ee5\u53ca\u6700\u8fd1\u7684\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u4e0a\u3002\u9274\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u6700\u8fd1\u5728\u975e\u56fe\u6570\u636e\uff08\u4f8b\u5982\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\uff09\u4e0a\u53d6\u5f97\u7684\u6210\u529f\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5e94\u7528\u4e8e\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u7684\u6700\u65b0 Vision GNN \u67b6\u6784 (ViG) \u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cViG \u5728\u591a\u7c7b\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86 ViT \u548c ResNet\u3002|[2402.09962v1](http://arxiv.org/pdf/2402.09962v1)|null|\n", "2402.09872": "|**2024-02-15**|**Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community**|\u793e\u4f1a\u5956\u52b1\uff1a\u901a\u8fc7\u5728\u7ebf\u521b\u610f\u793e\u533a\u7684\u6570\u767e\u4e07\u7528\u6237\u53cd\u9988\u8bc4\u4f30\u548c\u589e\u5f3a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd|Arman Isajanyan, Artur Shatveryan, David Kocharyan, Zhangyang Wang, Humphrey Shi|Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art. Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward|\u793e\u4f1a\u5956\u52b1\u4f5c\u4e3a\u793e\u533a\u8ba4\u53ef\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4e3a\u5728\u7ebf\u5e73\u53f0\u7684\u7528\u6237\u53c2\u4e0e\u548c\u8d21\u732e\u5185\u5bb9\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u52a8\u529b\u6765\u6e90\u3002\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u5408\u6210\u7684\u6700\u65b0\u8fdb\u5c55\u5f00\u521b\u4e86\u4e00\u4e2a\u534f\u4f5c\u65f6\u4ee3\uff0c\u4eba\u5de5\u667a\u80fd\u4f7f\u7528\u6237\u80fd\u591f\u5236\u4f5c\u539f\u521b\u89c6\u89c9\u827a\u672f\u4f5c\u54c1\uff0c\u5bfb\u6c42\u793e\u533a\u9a8c\u8bc1\u3002\u7136\u800c\uff0c\u5728\u96c6\u4f53\u793e\u533a\u504f\u597d\u7684\u80cc\u666f\u4e0b\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u4f1a\u5e26\u6765\u660e\u663e\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4ee5\u56fe\u50cf\u8d28\u91cf\u548c\u5373\u65f6\u5bf9\u9f50\u4e3a\u6307\u5bfc\u7684\u6709\u9650\u89c4\u6a21\u7684\u7528\u6237\u7814\u7a76\u4e0a\u3002\u8fd9\u9879\u5de5\u4f5c\u5f00\u521b\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u63ed\u793a\u4e86\u793e\u4ea4\u5956\u52b1\u2014\u2014\u4e00\u79cd\u521b\u65b0\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u53c2\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u521b\u610f\u7f16\u8f91\u7684\u793e\u4ea4\u7f51\u7edc\u7528\u6237\u7684\u9690\u5f0f\u53cd\u9988\u3002\u6211\u4eec\u501f\u9274\u5728\u7ebf\u89c6\u89c9\u521b\u4f5c\u548c\u7f16\u8f91\u5e73\u53f0 Picsart\uff0c\u5f00\u59cb\u4e86\u6570\u636e\u96c6\u7ba1\u7406\u548c\u7ec6\u5316\u7684\u5e7f\u6cdb\u65c5\u7a0b\uff0c\u4ea7\u751f\u4e86\u7b2c\u4e00\u4e2a\u767e\u4e07\u7528\u6237\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u7528\u6237\u751f\u6210\u7684\u89c6\u89c9\u827a\u672f\u7684\u9690\u6027\u4eba\u7c7b\u504f\u597d\uff0c\u540d\u4e3a Picsart Image-Social\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6307\u6807\u5728\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8f93\u51fa\u7684\u793e\u533a\u521b\u610f\u504f\u597d\u8fdb\u884c\u5efa\u6a21\u65b9\u9762\u7684\u7f3a\u70b9\uff0c\u8feb\u4f7f\u6211\u4eec\u5f15\u5165\u4e00\u79cd\u4e13\u95e8\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u800c\u5b9a\u5236\u7684\u65b0\u9896\u7684\u9884\u6d4b\u6a21\u578b\u3002\u4e25\u683c\u7684\u5b9a\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684\u793e\u4ea4\u5956\u52b1\u6a21\u578b\u6bd4\u73b0\u6709\u6307\u6807\u66f4\u7b26\u5408\u793e\u4ea4\u6d41\u884c\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u793e\u4ea4\u5956\u52b1\u6765\u5fae\u8c03\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4e0d\u4ec5\u66f4\u53d7\u793e\u4ea4\u5956\u52b1\u7684\u9752\u7750\uff0c\u800c\u4e14\u4e5f\u66f4\u53d7\u5176\u4ed6\u65e2\u5b9a\u6307\u6807\u7684\u9752\u7750\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u793e\u4f1a\u5956\u52b1\u5728\u8bc4\u4f30\u793e\u533a\u5bf9\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u827a\u672f\u4f5c\u54c1\u7684\u6b23\u8d4f\u65b9\u9762\u7684\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\uff0c\u4e0e\u7528\u6237\u7684\u521b\u4f5c\u76ee\u6807\uff08\u521b\u9020\u6d41\u884c\u7684\u89c6\u89c9\u827a\u672f\uff09\u5efa\u7acb\u4e86\u66f4\u7d27\u5bc6\u7684\u4e00\u81f4\u6027\u3002\u4ee3\u7801\u53ef\u4ee5\u8bbf\u95ee https://github.com/Picsart-AI-Research/Social-Reward|[2402.09872v1](http://arxiv.org/pdf/2402.09872v1)|null|\n", "2402.09867": "|**2024-02-15**|**Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs**|\u8868\u5f81\u5d4c\u5165\u5f0f HMP \u4e0a EEG \u5e94\u7528\u7684\u51c6\u786e\u6027\u6743\u8861|Zain Taufique, Muhammad Awais Bin Altaf, Antonio Miele, Pasi Liljeberg, Anil Kanduri|Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detection on the real-world embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial evaluation of power-performance-accuracy trade-offs of EEG applications at different approximation, power, and performance levels to provide insights into the disciplined tuning of approximation in EEG applications on embedded platforms.|\u4f7f\u7528\u7535\u6c60\u4f9b\u7535\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u5206\u6790\u8111\u7535\u56fe (EEG) \u8bb0\u5f55\uff0c\u4ee5\u76d1\u6d4b\u5927\u8111\u6d3b\u52a8\u548c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u3002\u8fd9\u4e9b\u5e94\u7528\u9700\u8981\u957f\u671f\u4e14\u8fde\u7eed\u7684\u5904\u7406\u624d\u80fd\u4ea7\u751f\u53ef\u200b\u200b\u884c\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u5c3a\u5bf8\u8f83\u5c0f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u7528\u4f8b\uff0c\u56e0\u6b64\u5176\u80fd\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3002\u5d4c\u5165\u5f0f\u5f02\u6784\u591a\u6838\u5e73\u53f0\uff08HMP\uff09\u53ef\u4ee5\u5728\u6709\u9650\u7684\u80fd\u91cf\u9884\u7b97\u5185\u4e3a\u8111\u7535\u56fe\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5229\u7528 EEG \u5e94\u7528\u7a0b\u5e8f\u7ba1\u9053\u7684\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8 HMP \u7684\u6027\u80fd\u548c\u80fd\u91cf\u589e\u76ca\u3002\u7136\u800c\uff0c\u5bf9\u5d4c\u5165\u5f0f HMP \u8fd1\u4f3c\u503c\u8fdb\u884c\u4e25\u683c\u7684\u8c03\u6574\u9700\u8981\u5f7b\u5e95\u63a2\u7d22\u7cbe\u5ea6-\u6027\u80fd-\u529f\u8017\u7684\u6743\u8861\u7a7a\u95f4\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5728 Odroid XU3 \u5e73\u53f0\u7684\u771f\u5b9e\u5d4c\u5165\u5f0f HMP \u6d4b\u8bd5\u53f0\u4e0a\u63cf\u8ff0\u4e86\u4e09\u79cd EEG \u5e94\u7528\u7a0b\u5e8f\u7684\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u5305\u62ec\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u3001\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u548c\u538b\u529b\u68c0\u6d4b\u3002\u6211\u4eec\u5bf9\u8111\u7535\u56fe\u5e94\u7528\u7a0b\u5e8f\u5728\u4e0d\u540c\u8fd1\u4f3c\u3001\u529f\u7387\u548c\u6027\u80fd\u6c34\u5e73\u4e0b\u7684\u529f\u7387-\u6027\u80fd-\u7cbe\u5ea6\u6743\u8861\u8fdb\u884c\u4e86\u7ec4\u5408\u8bc4\u4f30\uff0c\u4ee5\u63d0\u4f9b\u5bf9\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8111\u7535\u56fe\u5e94\u7528\u7a0b\u5e8f\u8fd1\u4f3c\u503c\u7684\u4e25\u683c\u8c03\u6574\u7684\u89c1\u89e3\u3002|[2402.09867v1](http://arxiv.org/pdf/2402.09867v1)|null|\n", "2402.09865": "|**2024-02-15**|**Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking**|\u8d85\u8d8a\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff1a\u7528\u4e8e\u6539\u8fdb\u5bf9\u8c61\u8ddf\u8e2a\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ee4\u6ce2\u5668|Momir Ad\u017eemovi\u0107, Predrag Tadi\u0107, Andrija Petrovi\u0107, Mladen Nikoli\u0107|Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.|\u4f20\u7edf\u7684\u68c0\u6d4b\u8ddf\u8e2a\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08KF\uff09\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u3002\u7136\u800c\uff0cKF \u9700\u8981\u7279\u5b9a\u9886\u57df\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u4e14\u4e0d\u9002\u5408\u5904\u7406\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u521b\u65b0\u7684\u6570\u636e\u9a71\u52a8\u8fc7\u6ee4\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7b2c\u4e00\u79cd\u65b9\u6cd5\u91c7\u7528\u5e26\u6709\u53ef\u8bad\u7ec3\u8fd0\u52a8\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u6765\u9884\u6d4b\u5bf9\u8c61\u7684\u672a\u6765\u4f4d\u7f6e\uff0c\u5e76\u5c06\u5176\u9884\u6d4b\u4e0e\u4ece\u5bf9\u8c61\u68c0\u6d4b\u5668\u83b7\u5f97\u7684\u89c2\u5bdf\u7ed3\u679c\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u8fb9\u754c\u6846\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5b83\u7701\u53bb\u4e86 KF \u6240\u7279\u6709\u7684\u5927\u591a\u6570\u7279\u5b9a\u9886\u57df\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662f\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6ee4\u6ce2\u5668\uff0c\u5b83\u66f4\u8fdb\u4e00\u6b65\uff0c\u901a\u8fc7\u5b66\u4e60\u7ea0\u6b63\u68c0\u6d4b\u5668\u9519\u8bef\uff0c\u8fdb\u4e00\u6b65\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5bf9\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3001\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u548c\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\u7684\u8fd0\u52a8\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u4e0e\u6240\u63d0\u51fa\u7684\u6ee4\u6ce2\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6ee4\u6ce2\u5668\u5728\u5bf9\u8c61\u8ddf\u8e2a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684 KF\uff0c\u7279\u522b\u662f\u5728\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u7684\u60c5\u51b5\u4e0b\u2014\u2014\u6211\u4eec\u7684\u6ee4\u6ce2\u5668\u6700\u9002\u5408\u7684\u7528\u4f8b\u3002\u6211\u4eec\u8fd8\u5bf9\u6ee4\u6ce2\u5668\u8fdb\u884c\u566a\u58f0\u9c81\u68d2\u6027\u5206\u6790\uff0c\u5e76\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u79ef\u6781\u7ed3\u679c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6210\u672c\u51fd\u6570\uff0c\u7528\u4e8e\u5c06\u89c2\u6d4b\u503c\u4e0e\u8f68\u8ff9\u76f8\u5173\u8054\u3002\u6839\u636e\u8fd0\u52a8\u4e30\u5bcc\u7684 DanceTrack \u548c SportsMOT \u6570\u636e\u96c6\u4e0a\u7684\u591a\u4e2a\u6307\u6807\uff0c\u6211\u4eec\u7684\u8ddf\u8e2a\u5668\u5c06\u8fd9\u79cd\u65b0\u7684\u5173\u8054\u6210\u672c\u4e0e\u6211\u4eec\u63d0\u51fa\u7684\u8fc7\u6ee4\u5668\u76f8\u7ed3\u5408\uff0c\u5728\u591a\u5bf9\u8c61\u8ddf\u8e2a\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684 SORT \u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u4e8e\u8fd0\u52a8\u7684\u8ddf\u8e2a\u5668\u3002|[2402.09865v1](http://arxiv.org/pdf/2402.09865v1)|null|\n", "2402.09816": "|**2024-02-15**|**Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment**|\u6ce8\u610f\u6a21\u6001\u5dee\u8ddd\uff1a\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u5b9e\u73b0\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Angelos Zavras, Dimitrios Michail, Beg\u00fcm Demir, Ioannis Papoutsis|Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.|\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6df1\u5ea6\u5b66\u4e60 (DL) \u6b63\u5728\u7ecf\u5386\u8303\u5f0f\u8f6c\u53d8\uff0c\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u56e0\u5176\u5173\u952e\u4f46\u4e0d\u5b8c\u6574\u7684\u6027\u8d28\u800c\u6070\u5982\u5176\u5206\u5730\u547d\u540d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5f00\u653e\u8bcd\u6c47\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u5728\u8bb8\u591a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u901a\u5e38\u53ef\u4ee5\u4e0e\u672a\u7ecf\u663e\u5f0f\u8bad\u7ec3\u7684\u5b8c\u5168\u76d1\u7763\u57fa\u7ebf\u7ade\u4e89\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u5728\u67d0\u4e9b\u9886\u57df\uff0c\u96f6\u6837\u672c CLIP \u6027\u80fd\u4ecd\u8fdc\u672a\u8fbe\u5230\u6700\u4f73\uff0c\u4f8b\u5982\u9065\u611f (RS) \u548c\u533b\u5b66\u56fe\u50cf\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u9886\u57df\u4e0d\u4ec5\u8868\u73b0\u51fa\u6839\u672c\u4e0d\u540c\u7684\u5206\u5e03\uff0c\u800c\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e RGB \u4e4b\u5916\u7684\u4e92\u8865\u6a21\u5f0f\u6765\u83b7\u5f97\u6709\u610f\u4e49\u7684\u89c1\u89e3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u4e0d\u540c\u7684 RS \u56fe\u50cf\u6a21\u5f0f\u4e0e CLIP \u7684\u89c6\u89c9\u548c\u6587\u672c\u6a21\u5f0f\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b\u5305\u62ec\u5f3a\u5927\u7684\u5fae\u8c03 CLIP\uff0c\u4ee5\u5904\u7406\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u4f34\u968f RS \u6a21\u6001\u7f16\u7801\u5668\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4ee5\u52aa\u529b\u6269\u5c55 CLIP \u7684\u96f6\u6837\u672c\u529f\u80fd\u3002\u6211\u4eec\u6700\u7ec8\u5728\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u51ed\u7ecf\u9a8c\u8bc1\u660e\uff0c\u5728\u591a\u4e2a RS \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u7a33\u5065\u7684\u5fae\u8c03\u548c\u8de8\u6a21\u5f0f\u5bf9\u9f50\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e9b\u589e\u5f3a\u662f\u5728\u4e0d\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7684\uff0c\u65e0\u9700\u5f15\u5165\u4efb\u4f55\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u53c2\u6570\uff0c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u4e5f\u4e0d\u4f1a\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002|[2402.09816v1](http://arxiv.org/pdf/2402.09816v1)|null|\n", "2402.09811": "|**2024-02-15**|**TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming**|TEXTRON\uff1a\u901a\u8fc7\u6570\u636e\u7f16\u7a0b\u8fdb\u884c\u5f31\u76d1\u7763\u591a\u8bed\u8a00\u6587\u672c\u68c0\u6d4b|Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan|Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON|\u6700\u8fd1\u7684\u51e0\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7684\u6280\u672f\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u591a\u8bed\u8a00\u6587\u672c\u68c0\u6d4b\u4e0a\u8868\u73b0\u5f97\u76f8\u5f53\u597d\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u8d28\u91cf\u3002\u9875\u9762\u7ea7\u6587\u6863\u56fe\u50cf\u6709\u591a\u79cd\u7c7b\u578b\uff0c\u7531\u591a\u79cd\u6a21\u5f0f\u3001\u8bed\u8a00\u3001\u5b57\u4f53\u548c\u5e03\u5c40\u7684\u4fe1\u606f\u7ec4\u6210\u3002\u8fd9\u4f7f\u5f97\u6587\u672c\u68c0\u6d4b\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u9886\u57df\u7684\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d44\u6e90\u532e\u4e4f\u6216\u624b\u5199\u8bed\u8a00\u3002\u6b64\u5916\uff0c\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u5b57\u7ea7\u6807\u8bb0\u6570\u636e\u5f88\u7f3a\u4e4f\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u591a\u8bed\u8a00\u8bbe\u7f6e\u548c\u5305\u542b\u5370\u5237\u548c\u624b\u5199\u6587\u672c\u7684\u5370\u5ea6\u6587\u5b57\u3002\u4f20\u7edf\u4e0a\uff0c\u5370\u5ea6\u6587\u5b57\u6587\u672c\u68c0\u6d4b\u9700\u8981\u5728\u5927\u91cf\u6807\u8bb0\u6570\u636e\u4e0a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f46\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6ca1\u6709\u76f8\u5173\u7684\u6570\u636e\u96c6\u53ef\u7528\u3002\u6b64\u7c7b\u6570\u636e\u7684\u624b\u52a8\u6ce8\u91ca\u9700\u8981\u5927\u91cf\u65f6\u95f4\u3001\u7cbe\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TEXTRON\uff0c\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7f16\u7a0b\u7684\u65b9\u6cd5\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u5404\u79cd\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u63d2\u5165\u5230\u57fa\u4e8e\u5f31\u76d1\u7763\u7684\u5b66\u4e60\u6846\u67b6\u4e2d\u3002\u4eba\u4eec\u53ef\u4ee5\u5c06\u8fd9\u79cd\u591a\u8bed\u8a00\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u89c6\u4e3a\u4e0d\u540c\u7684\u57fa\u4e8e CV \u7684\u6280\u672f\u548c DL \u65b9\u6cd5\u7684\u96c6\u5408\u3002 TEXTRON \u53ef\u4ee5\u5229\u7528\u5728\u5927\u91cf\u8bed\u8a00\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684 DL \u6a21\u578b\u7684\u9884\u6d4b\u4ee5\u53ca\u57fa\u4e8e CV \u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u5176\u4ed6\u8bed\u8a00\u7684\u6587\u672c\u68c0\u6d4b\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5c3d\u7ba1\u7f3a\u4e4f\u76f8\u5e94\u7684\u6807\u8bb0\u6570\u636e\uff0cTEXTRON \u4ecd\u53ef\u4ee5\u63d0\u9ad8\u7528\u5370\u5ea6\u8bed\u8a00\u7f16\u5199\u7684\u6587\u6863\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6a21\u578b\u6240\u5e26\u6765\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u624b\u5199\u68b5\u6587\u6587\u672c\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5728 https://github.com/IITB-LEAP-OCR/TEXTRON \u4e0a\u63d0\u4f9b|[2402.09811v1](http://arxiv.org/pdf/2402.09811v1)|null|\n", "2402.09781": "|**2024-02-15**|**A Comprehensive Review on Computer Vision Analysis of Aerial Data**|\u822a\u7a7a\u6570\u636e\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u6790\u7684\u7efc\u5408\u7efc\u8ff0|Vivek Tetarwal, Sandeep Kumar|With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.|\u968f\u7740\u673a\u8f7d\u5e73\u53f0\u548c\u6210\u50cf\u4f20\u611f\u5668\u9886\u57df\u65b0\u6280\u672f\u7684\u51fa\u73b0\uff0c\u822a\u7a7a\u6570\u636e\u5206\u6790\u5229\u7528\u5176\u76f8\u5bf9\u4e8e\u9646\u5730\u6570\u636e\u7684\u4f18\u52bf\u53d8\u5f97\u975e\u5e38\u6d41\u884c\u3002\u672c\u6587\u5bf9\u822a\u7a7a\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\u3002\u5728\u89e3\u51b3\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7b49\u57fa\u672c\u65b9\u9762\u65f6\uff0c\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u53d8\u5316\u68c0\u6d4b\u3001\u5bf9\u8c61\u5206\u5272\u548c\u573a\u666f\u7ea7\u5206\u6790\u7b49\u5173\u952e\u4efb\u52a1\u3002\u8be5\u8bba\u6587\u5bf9\u4e0d\u540c\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u5404\u79cd\u8d85\u53c2\u6570\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5176\u4e2d\u5f88\u5927\u4e00\u90e8\u5206\u81f4\u529b\u4e8e\u6df1\u5165\u8ba8\u8bba\u56fe\u4e66\u9986\u3001\u5b83\u4eec\u7684\u5206\u7c7b\u4ee5\u53ca\u5b83\u4eec\u4e0e\u4e0d\u540c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u76f8\u5173\u6027\u3002\u8be5\u8bba\u6587\u6db5\u76d6\u4e86\u822a\u7a7a\u6570\u636e\u96c6\u3001\u91c7\u7528\u7684\u67b6\u6784\u7ec6\u5fae\u5dee\u522b\u4ee5\u53ca\u4e0e\u822a\u7a7a\u6570\u636e\u5206\u6790\u4e2d\u6240\u6709\u4efb\u52a1\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u3002\u63a2\u8ba8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u5728\u4e0d\u540c\u9886\u57df\u822a\u7a7a\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7684\u89c1\u89e3\u3002\u672c\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u822a\u7a7a\u6570\u636e\u5206\u6790\u56fa\u6709\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8fd8\u786e\u5b9a\u4e86\u5c1a\u672a\u89e3\u51b3\u7684\u91cd\u8981\u95ee\u9898\uff0c\u4e3a\u822a\u7a7a\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.09781v1](http://arxiv.org/pdf/2402.09781v1)|null|\n", "2402.09747": "|**2024-02-15**|**Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources**|\u5c11\u5373\u662f\u591a\uff1a\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u89c6\u7f51\u819c\u75be\u75c5\u8bc6\u522b\u96c6\u6210\u5b66\u4e60|Jiahao Wang, Hong Peng, Shengchao Chen, Sufen Ren|Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries. This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation). The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images. This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch. Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets. Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in low-resource scenarios.|\u89c6\u7f51\u819c\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf (OCT) \u56fe\u50cf\u4e3a\u4e86\u89e3\u773c\u540e\u6bb5\u7684\u5065\u5eb7\u72b6\u51b5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002\u56e0\u6b64\uff0c\u81ea\u52a8\u5316\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\u7684\u8fdb\u6b65\u52bf\u5728\u5fc5\u884c\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5b9a\u91cf\u6570\u636e\uff0c\u4ece\u800c\u4fc3\u8fdb\u660e\u667a\u7684\u51b3\u7b56\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60 (DL) \u7684\u65b9\u6cd5\u7684\u5e94\u7528\u5728\u6267\u884c\u8fd9\u4e9b\u5206\u6790\u4efb\u52a1\u65b9\u9762\u83b7\u5f97\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\uff0c\u4e0e\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u624b\u52a8\u5206\u6790\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u89c6\u7f51\u819c OCT \u56fe\u50cf\u7684\u83b7\u53d6\u901a\u5e38\u4f1a\u5e26\u6765\u6765\u81ea\u9690\u79c1\u95ee\u9898\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u6807\u8bb0\u7a0b\u5e8f\u7684\u6311\u6218\uff0c\u8fd9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u624d\u80fd\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u7684\u666e\u904d\u89c2\u5ff5\u76f8\u77db\u76fe\u3002\u6b64\u5916\uff0c\u53ef\u7528\u8ba1\u7b97\u8d44\u6e90\u7684\u9650\u5236\u9650\u5236\u4e86\u9ad8\u6027\u80fd\u533b\u7597\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u5728\u6b20\u53d1\u8fbe\u5730\u533a\u548c\u56fd\u5bb6\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96c6\u6210\u5b66\u4e60\u673a\u5236\uff0c\u65e8\u5728\u5728\u6709\u9650\u8d44\u6e90\uff08\u4f8b\u5982\u6570\u636e\u3001\u8ba1\u7b97\uff09\u4e0b\u8bc6\u522b\u89c6\u7f51\u819c\u75be\u75c5\u3002\u8be5\u673a\u5236\u5229\u7528\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u89c1\u89e3\uff0c\u4fc3\u8fdb\u5c06\u5176\u77e5\u8bc6\u8f6c\u79fb\u548c\u9002\u5e94\u89c6\u7f51\u819c OCT \u56fe\u50cf\u3002\u5373\u4f7f\u9762\u5bf9\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e5f\u80fd\u5efa\u7acb\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u6240\u9700\u7684\u5927\u91cf\u53c2\u6570\u7684\u9700\u8981\u3002\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5904\u7406\u6781\u5176\u53d7\u9650\u7684\u6807\u8bb0\u6570\u636e\u96c6\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u5728\u8bc6\u522b\u89c6\u7f51\u819c OCT \u56fe\u50cf\u65b9\u9762\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5b66\u4e60\u5927\u89c4\u6a21\u53c2\u6570\u7684\u5fc5\u8981\u6027\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u573a\u666f\u4e2d\u90e8\u7f72\u3002|[2402.09747v1](http://arxiv.org/pdf/2402.09747v1)|null|\n", "2402.09724": "|**2024-02-15**|**Region Feature Descriptor Adapted to High Affine Transformations**|\u9002\u5e94\u9ad8\u4eff\u5c04\u53d8\u6362\u7684\u533a\u57df\u7279\u5f81\u63cf\u8ff0\u7b26|Shaojie Zhang, Yinghui Wang, Peixuan Liu, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang|To address the issue of feature descriptors being ineffective in representing grayscale feature information when images undergo high affine transformations, leading to a rapid decline in feature matching accuracy, this paper proposes a region feature descriptor based on simulating affine transformations using classification. The proposed method initially categorizes images with different affine degrees to simulate affine transformations and generate a new set of images. Subsequently, it calculates neighborhood information for feature points on this new image set. Finally, the descriptor is generated by combining the grayscale histogram of the maximum stable extremal region to which the feature point belongs and the normalized position relative to the grayscale centroid of the feature point's region. Experimental results, comparing feature matching metrics under affine transformation scenarios, demonstrate that the proposed descriptor exhibits higher precision and robustness compared to existing classical descriptors. Additionally, it shows robustness when integrated with other descriptors.|\u9488\u5bf9\u56fe\u50cf\u8fdb\u884c\u9ad8\u4eff\u5c04\u53d8\u6362\u65f6\u7279\u5f81\u63cf\u8ff0\u5b50\u65e0\u6cd5\u6709\u6548\u8868\u793a\u7070\u5ea6\u7279\u5f81\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u5339\u914d\u7cbe\u5ea6\u8fc5\u901f\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u6a21\u62df\u4eff\u5c04\u53d8\u6362\u7684\u533a\u57df\u7279\u5f81\u63cf\u8ff0\u5b50\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5bf9\u5177\u6709\u4e0d\u540c\u4eff\u5c04\u5ea6\u7684\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u6a21\u62df\u4eff\u5c04\u53d8\u6362\u5e76\u751f\u6210\u4e00\u7ec4\u65b0\u7684\u56fe\u50cf\u3002\u968f\u540e\uff0c\u5b83\u8ba1\u7b97\u8fd9\u4e2a\u65b0\u56fe\u50cf\u96c6\u4e0a\u7279\u5f81\u70b9\u7684\u90bb\u57df\u4fe1\u606f\u3002\u6700\u540e\uff0c\u7ed3\u5408\u7279\u5f81\u70b9\u6240\u5c5e\u7684\u6700\u5927\u7a33\u5b9a\u6781\u503c\u533a\u57df\u7684\u7070\u5ea6\u76f4\u65b9\u56fe\u548c\u76f8\u5bf9\u4e8e\u7279\u5f81\u70b9\u533a\u57df\u7684\u7070\u5ea6\u8d28\u5fc3\u7684\u5f52\u4e00\u5316\u4f4d\u7f6e\u6765\u751f\u6210\u63cf\u8ff0\u7b26\u3002\u5b9e\u9a8c\u7ed3\u679c\u6bd4\u8f83\u4e86\u4eff\u5c04\u53d8\u6362\u573a\u666f\u4e0b\u7684\u7279\u5f81\u5339\u914d\u6307\u6807\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u63cf\u8ff0\u7b26\u4e0e\u73b0\u6709\u7684\u7ecf\u5178\u63cf\u8ff0\u7b26\u76f8\u6bd4\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u5b83\u5728\u4e0e\u5176\u4ed6\u63cf\u8ff0\u7b26\u96c6\u6210\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002|[2402.09724v1](http://arxiv.org/pdf/2402.09724v1)|null|\n", "2402.09663": "|**2024-02-15**|**Hand Shape and Gesture Recognition using Multiscale Template Matching, Background Subtraction and Binary Image Analysis**|\u4f7f\u7528\u591a\u5c3a\u5ea6\u6a21\u677f\u5339\u914d\u3001\u80cc\u666f\u6263\u9664\u548c\u4e8c\u503c\u56fe\u50cf\u5206\u6790\u8fdb\u884c\u624b\u5f62\u548c\u624b\u52bf\u8bc6\u522b|Ketan Suhaas Saichandran|This paper presents a hand shape classification approach employing multiscale template matching. The integration of background subtraction is utilized to derive a binary image of the hand object, enabling the extraction of key features such as centroid and bounding box. The methodology, while simple, demonstrates effectiveness in basic hand shape classification tasks, laying the foundation for potential applications in straightforward human-computer interaction scenarios. Experimental results highlight the system's capability in controlled environments.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u591a\u5c3a\u5ea6\u6a21\u677f\u5339\u914d\u7684\u624b\u5f62\u5206\u7c7b\u65b9\u6cd5\u3002\u5229\u7528\u80cc\u666f\u51cf\u6cd5\u7684\u96c6\u6210\u6765\u5bfc\u51fa\u624b\u90e8\u7269\u4f53\u7684\u4e8c\u503c\u56fe\u50cf\uff0c\u4ece\u800c\u80fd\u591f\u63d0\u53d6\u8d28\u5fc3\u548c\u8fb9\u754c\u6846\u7b49\u5173\u952e\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u867d\u7136\u7b80\u5355\uff0c\u4f46\u5c55\u793a\u4e86\u57fa\u672c\u624b\u5f62\u5206\u7c7b\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7b80\u5355\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5b9e\u9a8c\u7ed3\u679c\u7a81\u51fa\u4e86\u7cfb\u7edf\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002|[2402.09663v1](http://arxiv.org/pdf/2402.09663v1)|null|\n", "2402.09636": "|**2024-02-15**|**Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography**|\u6570\u5b57\u51cf\u5f71\u8840\u7ba1\u9020\u5f71\u4e2d\u52a8\u9759\u8109\u7578\u5f62\u7684\u65f6\u7a7a\u89e3\u7f20|Kathleen Baur, Xin Xiong, Erickson Torio, Rose Du, Parikshit Juvekar, Reuben Dorent, Alexandra Golby, Sarah Frisken, Nazim Haouchine|Although Digital Subtraction Angiography (DSA) is the most important imaging for visualizing cerebrovascular anatomy, its interpretation by clinicians remains difficult. This is particularly true when treating arteriovenous malformations (AVMs), where entangled vasculature connecting arteries and veins needs to be carefully identified.The presented method aims to enhance DSA image series by highlighting critical information via automatic classification of vessels using a combination of two learning models: An unsupervised machine learning method based on Independent Component Analysis that decomposes the phases of flow and a convolutional neural network that automatically delineates the vessels in image space. The proposed method was tested on clinical DSA images series and demonstrated efficient differentiation between arteries and veins that provides a viable solution to enhance visualizations for clinical use.|\u5c3d\u7ba1\u6570\u5b57\u51cf\u5f71\u8840\u7ba1\u9020\u5f71 (DSA) \u662f\u8111\u8840\u7ba1\u89e3\u5256\u53ef\u89c6\u5316\u6700\u91cd\u8981\u7684\u6210\u50cf\u6280\u672f\uff0c\u4f46\u4e34\u5e8a\u533b\u751f\u5bf9\u5176\u8fdb\u884c\u89e3\u91ca\u4ecd\u7136\u5f88\u56f0\u96be\u3002\u5728\u6cbb\u7597\u52a8\u9759\u8109\u7578\u5f62 (AVM) \u65f6\u5c24\u5176\u5982\u6b64\uff0c\u9700\u8981\u4ed4\u7ec6\u8bc6\u522b\u8fde\u63a5\u52a8\u8109\u548c\u9759\u8109\u7684\u7f20\u7ed3\u8109\u7ba1\u7cfb\u7edf\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4f7f\u7528\u4e24\u79cd\u5b66\u4e60\u6a21\u578b\u5bf9\u8840\u7ba1\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b\u6765\u7a81\u51fa\u5173\u952e\u4fe1\u606f\uff0c\u4ece\u800c\u589e\u5f3a DSA \u56fe\u50cf\u7cfb\u5217\uff1a\u4e00\u79cd\u57fa\u4e8e\u72ec\u7acb\u6210\u5206\u5206\u6790\u7684\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u5206\u89e3\u6d41\u52a8\u7684\u9636\u6bb5\uff0c\u4ee5\u53ca\u81ea\u52a8\u63cf\u7ed8\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u8840\u7ba1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e34\u5e8a DSA \u56fe\u50cf\u7cfb\u5217\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u8bc1\u660e\u4e86\u52a8\u8109\u548c\u9759\u8109\u4e4b\u95f4\u7684\u6709\u6548\u533a\u5206\uff0c\u4e3a\u589e\u5f3a\u4e34\u5e8a\u4f7f\u7528\u7684\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.09636v1](http://arxiv.org/pdf/2402.09636v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.10061": "|**2024-02-15**|**X-maps: Direct Depth Lookup for Event-based Structured Light Systems**|X-maps\uff1a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u7ed3\u6784\u5149\u7cfb\u7edf\u7684\u76f4\u63a5\u6df1\u5ea6\u67e5\u627e|Wieland Morgenstern, Niklas Gard, Simon Baumann, Anna Hilsmann, Peter Eisert|We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u76f4\u63a5\u4f30\u8ba1\u7a7a\u95f4\u589e\u5f3a\u73b0\u5b9e (SAR) \u5e94\u7528\u6df1\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002\u8fd9\u4e9b\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u975e\u5e38\u9002\u5408\u4e0e\u6fc0\u5149\u6295\u5f71\u4eea\u914d\u5408\u4f7f\u7528\uff0c\u4ee5\u5728\u7ed3\u6784\u5149\u65b9\u6cd5\u4e2d\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u5c06\u6295\u5f71\u4eea\u65f6\u95f4\u56fe\u8f6c\u6362\u4e3a\u6821\u6b63\u7684 X \u56fe\u3001\u6355\u83b7\u4f20\u5165\u4e8b\u4ef6\u7684 x \u8f74\u5bf9\u5e94\u5173\u7cfb\u4ee5\u53ca\u65e0\u9700\u4efb\u4f55\u989d\u5916\u641c\u7d22\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u89c6\u5dee\u67e5\u627e\u3002\u4e0e\u4ee5\u524d\u7684\u5b9e\u73b0\u76f8\u6bd4\uff0c\u8fd9\u663e\u7740\u7b80\u5316\u4e86\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f7f\u5176\u66f4\u52a0\u9ad8\u6548\uff0c\u800c\u51c6\u786e\u6027\u4e0e\u57fa\u4e8e\u65f6\u95f4\u56fe\u7684\u8fc7\u7a0b\u7c7b\u4f3c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u7b80\u5355\u7684\u65f6\u95f4\u56fe\u6821\u51c6\u6765\u8865\u507f\u5ec9\u4ef7\u6fc0\u5149\u6295\u5f71\u4eea\u7684\u975e\u7ebf\u6027\u65f6\u95f4\u884c\u4e3a\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u5e76\u63d0\u9ad8\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002\u7531\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u4ec5\u901a\u8fc7\u4e24\u6b21\u67e5\u627e\u6765\u6267\u884c\uff0c\u56e0\u6b64\u5b83\u51e0\u4e4e\u53ef\u4ee5\u7acb\u5373\u9488\u5bf9\u4f20\u5165\u4e8b\u4ef6\u6267\u884c\uff08\u4f7f\u7528 Python \u5b9e\u73b0\u6bcf\u5e27\u4e0d\u5230 3 \u6beb\u79d2\uff09\u3002\u8fd9\u5141\u8bb8\u5b9e\u65f6\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u6027\uff0c\u8fd9\u4f7f\u5f97\u6211\u4eec\u7684\u65b9\u6cd5\u7279\u522b\u9002\u5408\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u5e27\u901f\u7387\u548c\u76f4\u63a5\u53cd\u9988\u81f3\u5173\u91cd\u8981\u7684 SAR \u4f53\u9a8c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4ece\u8f6c\u6362\u4e3a X \u5730\u56fe\u7684\u6570\u636e\u4e2d\u83b7\u5f97\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u6839\u636e\u57fa\u4e8e\u65f6\u95f4\u56fe\u7684\u6700\u65b0\u7ed3\u679c\u8bc4\u4f30\u89c6\u5dee\u4f30\u8ba1\u7684\u6df1\u5ea6\u3002\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4e0a\u63d0\u4f9b\u4e86\u5176\u4ed6\u7ed3\u679c\u548c\u4ee3\u7801\uff1ahttps://fraunhoferhhi.github.io/X-maps/|[2402.10061v1](http://arxiv.org/pdf/2402.10061v1)|null|\n"}, "LLM": {"2402.10038": "|**2024-02-15**|**RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models**|RS-DPO\uff1a\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u6df7\u5408\u62d2\u7edd\u91c7\u6837\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5|Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra|Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.|\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60 (RLHF) \u5df2\u88ab\u5e7f\u6cdb\u7528\u4e8e\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7528\u6237\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u7136\u800c\uff0c\u57fa\u4e8e RLHF \u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316 (PPO) \u5076\u5c14\u4f1a\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u5927\u91cf\u7684\u8d85\u53c2\u6570\u5fae\u8c03\uff0c\u5e76\u4e14\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u6700\u5927\u5316\u4f30\u8ba1\u5956\u52b1\u7684\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002\u6700\u8fd1\uff0c\u63d0\u51fa\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u7136\u800c\uff0cDPO \u4f9d\u8d56\u4e8e\u4eba\u7c7b\u6ce8\u91ca\u8005\u548c\u66ff\u4ee3 LLM \u751f\u6210\u7684\u5bf9\u6bd4\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u653f\u7b56\u6a21\u578b\uff0c\u9650\u5236\u4e86 RLHF \u7684\u6709\u6548\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u7cfb\u7edf\u5730\u7ed3\u5408\u62d2\u7edd\u91c7\u6837 (RS) \u548c DPO \u6765\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5 RS-DPO \u59cb\u4e8e\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u6a21\u578b (SFT) \u7684\u5f00\u53d1\u3002\u6bcf\u4e2a\u63d0\u793a\u7684\u4e00\u7ec4\u4e0d\u540c\u7684 k \u4e2a\u54cd\u5e94\u662f\u76f4\u63a5\u4ece SFT \u6a21\u578b\u4e2d\u91c7\u6837\u7684\u3002 RS-DPO \u6839\u636e\u5956\u52b1\u5206\u5e03\u8bc6\u522b\u5bf9\u6bd4\u6837\u672c\u5bf9\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06 DPO \u4e0e\u5bf9\u6bd4\u6837\u672c\u4e00\u8d77\u5e94\u7528\uff0c\u4ee5\u4f7f\u6a21\u578b\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5728\u6709\u9650\u7684\u8d44\u6e90\u73af\u5883\u4e0b\u5fae\u8c03 LLM\uff0c\u4ece\u800c\u63d0\u9ad8\u4e0e\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5b83\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec RS\u3001PPO \u548c DPO\u3002|[2402.10038v1](http://arxiv.org/pdf/2402.10038v1)|null|\n"}, "Transformer": {"2402.10099": "|**2024-02-15**|**Any-Shift Prompting for Generalization over Distributions**|Any-Shift \u63d0\u793a\u5bf9\u5206\u5e03\u7684\u6cdb\u5316|Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek|Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.|\u5177\u6709\u5373\u65f6\u5b66\u4e60\u529f\u80fd\u7684\u56fe\u50cf\u8bed\u8a00\u6a21\u578b\u5728\u4f17\u591a\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u7740\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5373\u65f6\u5b66\u4e60\u65b9\u6cd5\u8fc7\u5ea6\u62df\u5408\u5176\u8bad\u7ec3\u5206\u5e03\u5e76\u5931\u53bb\u4e86\u6d4b\u8bd5\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u63d0\u9ad8\u5404\u79cd\u5206\u5e03\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4efb\u610f\u53d8\u5316\u63d0\u793a\uff1a\u4e00\u4e2a\u901a\u7528\u7684\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u8003\u8651\u63d0\u793a\u5b66\u4e60\u671f\u95f4\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6211\u4eec\u901a\u8fc7\u5728\u5206\u5c42\u67b6\u6784\u4e2d\u6784\u5efa\u8bad\u7ec3\u548c\u6d4b\u8bd5\u63d0\u793a\u6765\u660e\u786e\u8fde\u63a5\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u6d4b\u8bd5\u63d0\u793a\u5229\u7528\u5206\u5e03\u5173\u7cfb\u6765\u6307\u5bfc CLIP \u56fe\u50cf\u8bed\u8a00\u6a21\u578b\u4ece\u8bad\u7ec3\u5230\u4efb\u4f55\u6d4b\u8bd5\u5206\u5e03\u7684\u6cdb\u5316\u3002\u4e3a\u4e86\u6709\u6548\u5730\u7f16\u7801\u5206\u5e03\u4fe1\u606f\u53ca\u5176\u5173\u7cfb\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u5177\u6709\u4f2a\u79fb\u4f4d\u8bad\u7ec3\u673a\u5236\u7684\u53d8\u538b\u5668\u63a8\u7406\u7f51\u7edc\u3002\u8be5\u7f51\u7edc\u5728\u524d\u9988\u4f20\u9012\u4e2d\u751f\u6210\u5305\u542b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4fe1\u606f\u7684\u5b9a\u5236\u6d4b\u8bd5\u63d0\u793a\uff0c\u907f\u514d\u4e86\u6d4b\u8bd5\u65f6\u7684\u989d\u5916\u8bad\u7ec3\u6210\u672c\u3002\u5bf9 23 \u4e2a\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4efb\u4f55\u8f6c\u53d8\u63d0\u793a\u5bf9\u5404\u79cd\u5206\u5e03\u8f6c\u53d8\u7684\u6cdb\u5316\u7684\u6709\u6548\u6027\u3002|[2402.10099v1](http://arxiv.org/pdf/2402.10099v1)|null|\n", "2402.10066": "|**2024-02-15**|**NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction**|NYCTALE\uff1a\u7528\u4e8e\u81ea\u9002\u5e94\u548c\u4e2a\u6027\u5316\u80ba\u7ed3\u8282\u4fb5\u88ad\u6027\u9884\u6d4b\u7684\u795e\u7ecf\u8bc1\u636e\u53d8\u538b\u5668|Sadaf Khademi, Anastasia Oikonomou, Konstantinos N. Plataniotis, Arash Mohammadi|Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset.|\u672c\u6587\u4ece\u7075\u957f\u7c7b\u52a8\u7269\u5927\u8111\u6709\u8da3\u7684\u8bc1\u636e\u79ef\u7d2f\u8fc7\u7a0b\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5e76\u4ee5\u8ba4\u77e5\u5fc3\u7406\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u6a21\u578b\u4e3a\u6307\u5bfc\uff0c\u4ecb\u7ecd\u4e86 NYCTALE \u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u795e\u7ecf\u542f\u53d1\u3001\u57fa\u4e8e\u8bc1\u636e\u79ef\u7d2f\u7684 Transformer \u67b6\u6784\u3002\u6240\u63d0\u51fa\u7684\u53d7\u795e\u7ecf\u542f\u53d1\u7684 NYCTALE \u4e3a\u80ba\u764c\u8bca\u65ad\u7684\u4e2a\u6027\u5316\u533b\u7597 (PM) \u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\u3002\u5728\u81ea\u7136\u754c\u4e2d\uff0c\u591c\u732b\u5934\u9e70\u662f\u4e00\u79cd\u4ee5\u591c\u95f4\u6d3b\u52a8\u800c\u95fb\u540d\u7684\u5c0f\u578b\u732b\u5934\u9e70\uff0c\u4e3b\u8981\u5728\u591c\u95f4\u6355\u730e\u3002 NYCTALE \u4ee5\u7c7b\u4f3c\u7684\u8b66\u60d5\u65b9\u5f0f\u8fd0\u4f5c\uff0c\u5373\u4ee5\u57fa\u4e8e\u8bc1\u636e\u7684\u65b9\u5f0f\u5904\u7406\u6570\u636e\u5e76\u52a8\u6001/\u81ea\u9002\u5e94\u5730\u505a\u51fa\u9884\u6d4b\u3002\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u7684\u6df1\u5ea6\u5b66\u4e60 (DL) \u6a21\u578b\u4e0d\u540c\uff0cNYCTALE \u4ec5\u5728\u79ef\u7d2f\u4e86\u8db3\u591f\u7684\u8bc1\u636e\u65f6\u624d\u8fdb\u884c\u9884\u6d4b\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u4e0d\u662f\u5904\u7406\u6240\u6709\u6216\u9884\u5b9a\u4e49\u7684 CT \u5207\u7247\u5b50\u96c6\uff0c\u800c\u662f\u4e3a\u6bcf\u4e2a\u4eba\u4e00\u6b21\u63d0\u4f9b\u4e00\u4e2a\u5207\u7247\u3002\u7136\u540e\uff0cNYCTALE \u6846\u67b6\u8ba1\u7b97\u4e0e\u6bcf\u4e2a\u65b0 CT \u56fe\u50cf\u7684\u8d21\u732e\u76f8\u5173\u7684\u8bc1\u636e\u5411\u91cf\u3002\u4e00\u65e6\u7d2f\u79ef\u7684\u8bc1\u636e\u603b\u6570\u8d85\u8fc7\u7279\u5b9a\u9608\u503c\uff0c\u5c31\u4f1a\u505a\u51fa\u51b3\u5b9a\u3002\u4f7f\u7528\u5305\u542b 114 \u540d\u53d7\u8bd5\u8005\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u5185\u90e8\u6570\u636e\u96c6\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\u5206\u6790\u3002\u7ed3\u679c\u503c\u5f97\u6ce8\u610f\uff0c\u8868\u660e NYCTALE \u5728\u8fd9\u4e2a\u8981\u6c42\u8f83\u9ad8\u7684\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c11\u4e86\u5927\u7ea6 60%\uff0c\u4f46\u4ecd\u4f18\u4e8e\u57fa\u51c6\u7cbe\u5ea6\u3002|[2402.10066v1](http://arxiv.org/pdf/2402.10066v1)|null|\n", "2402.10039": "|**2024-02-15**|**Feature Accentuation: Revealing 'What' Features Respond to in Natural Images**|\u7279\u5f81\u5f3a\u8c03\uff1a\u63ed\u793a\u81ea\u7136\u56fe\u50cf\u4e2d\u201c\u4ec0\u4e48\u201d\u7279\u5f81\u7684\u53cd\u5e94|Chris Hamblin, Thomas Fel, Srijani Saha, Talia Konkle, George Alvarez|Efforts to decode neural network vision models necessitate a comprehensive grasp of both the spatial and semantic facets governing feature responses within images. Most research has primarily centered around attribution methods, which provide explanations in the form of heatmaps, showing where the model directs its attention for a given feature. However, grasping 'where' alone falls short, as numerous studies have highlighted the limitations of those methods and the necessity to understand 'what' the model has recognized at the focal point of its attention. In parallel, 'Feature visualization' offers another avenue for interpreting neural network features. This approach synthesizes an optimal image through gradient ascent, providing clearer insights into 'what' features respond to. However, feature visualizations only provide one global explanation per feature; they do not explain why features activate for particular images. In this work, we introduce a new method to the interpretability tool-kit, 'feature accentuation', which is capable of conveying both where and what in arbitrary input images induces a feature's response. At its core, feature accentuation is image-seeded (rather than noise-seeded) feature visualization. We find a particular combination of parameterization, augmentation, and regularization yields naturalistic visualizations that resemble the seed image and target feature simultaneously. Furthermore, we validate these accentuations are processed along a natural circuit by the model. We make our precise implementation of feature accentuation available to the community as the Faccent library, an extension of Lucent.|\u89e3\u7801\u795e\u7ecf\u7f51\u7edc\u89c6\u89c9\u6a21\u578b\u7684\u52aa\u529b\u9700\u8981\u5168\u9762\u638c\u63e1\u63a7\u5236\u56fe\u50cf\u5185\u7279\u5f81\u54cd\u5e94\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u65b9\u9762\u3002\u5927\u591a\u6570\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5f52\u56e0\u65b9\u6cd5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ee5\u70ed\u56fe\u7684\u5f62\u5f0f\u63d0\u4f9b\u89e3\u91ca\uff0c\u663e\u793a\u6a21\u578b\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u7ed9\u5b9a\u7279\u5f81\u7684\u4f4d\u7f6e\u3002\u7136\u800c\uff0c\u4ec5\u4ec5\u638c\u63e1\u201c\u54ea\u91cc\u201d\u662f\u4e0d\u591f\u7684\uff0c\u56e0\u4e3a\u5927\u91cf\u7814\u7a76\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4e86\u89e3\u6a21\u578b\u5728\u5176\u5173\u6ce8\u7126\u70b9\u4e0a\u8bc6\u522b\u201c\u4ec0\u4e48\u201d\u7684\u5fc5\u8981\u6027\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u201c\u7279\u5f81\u53ef\u89c6\u5316\u201d\u4e3a\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u9014\u5f84\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u5408\u6210\u6700\u4f73\u56fe\u50cf\uff0c\u4ece\u800c\u66f4\u6e05\u6670\u5730\u4e86\u89e3\u7279\u5f81\u54cd\u5e94\u7684\u201c\u5185\u5bb9\u201d\u3002\u7136\u800c\uff0c\u7279\u5f81\u53ef\u89c6\u5316\u4ec5\u4e3a\u6bcf\u4e2a\u7279\u5f81\u63d0\u4f9b\u4e00\u4e2a\u5168\u5c40\u89e3\u91ca\uff1b\u4ed6\u4eec\u6ca1\u6709\u89e3\u91ca\u4e3a\u4ec0\u4e48\u7279\u5b9a\u56fe\u50cf\u4f1a\u6fc0\u6d3b\u529f\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5305\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u201c\u7279\u5f81\u5f3a\u8c03\u201d\uff0c\u5b83\u80fd\u591f\u4f20\u8fbe\u4efb\u610f\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u4f4d\u7f6e\u548c\u5185\u5bb9\u5f15\u8d77\u7279\u5f81\u54cd\u5e94\u3002\u7279\u5f81\u5f3a\u8c03\u7684\u6838\u5fc3\u662f\u56fe\u50cf\u79cd\u5b50\uff08\u800c\u4e0d\u662f\u566a\u58f0\u79cd\u5b50\uff09\u7279\u5f81\u53ef\u89c6\u5316\u3002\u6211\u4eec\u53d1\u73b0\u53c2\u6570\u5316\u3001\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7684\u7279\u5b9a\u7ec4\u5408\u53ef\u4ee5\u4ea7\u751f\u540c\u65f6\u7c7b\u4f3c\u4e8e\u79cd\u5b50\u56fe\u50cf\u548c\u76ee\u6807\u7279\u5f81\u7684\u81ea\u7136\u53ef\u89c6\u5316\u6548\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u6a21\u578b\u6cbf\u7740\u81ea\u7136\u56de\u8def\u5904\u7406\u8fd9\u4e9b\u5f3a\u8c03\u3002\u6211\u4eec\u5c06\u529f\u80fd\u5f3a\u8c03\u7684\u7cbe\u786e\u5b9e\u73b0\u4f5c\u4e3a Faccent \u5e93\uff08\u6717\u8baf\u79d1\u6280\u7684\u6269\u5c55\uff09\u63d0\u4f9b\u7ed9\u793e\u533a\u3002|[2402.10039v1](http://arxiv.org/pdf/2402.10039v1)|null|\n"}, "3D/CG": {"2402.09722": "|**2024-02-15**|**Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields**|Reg-NF\uff1a\u795e\u7ecf\u573a\u5185\u9690\u5f0f\u8868\u9762\u7684\u6709\u6548\u914d\u51c6|Stephen Hausler, David Hall, Sutharsan Mahendren, Peyman Moghadam|Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.|\u795e\u7ecf\u573a\uff08\u57fa\u4e8e\u5750\u6807\u7684\u795e\u7ecf\u7f51\u7edc\uff09\u6700\u8fd1\u56e0\u9690\u5f0f\u8868\u793a\u573a\u666f\u800c\u53d7\u5230\u6b22\u8fce\u3002\u4e0e\u57fa\u4e8e\u70b9\u4e91\u7b49\u663e\u5f0f\u8868\u793a\u7684\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\uff0c\u795e\u7ecf\u573a\u63d0\u4f9b\u4e86\u8fde\u7eed\u7684\u573a\u666f\u8868\u793a\uff0c\u80fd\u591f\u4ee5\u7d27\u51d1\u4e14\u9002\u5408\u673a\u5668\u4eba\u5e94\u7528\u7684\u65b9\u5f0f\u8868\u793a 3D \u51e0\u4f55\u548c\u5916\u89c2\u3002\u7136\u800c\uff0c\u6709\u9650\u7684\u73b0\u6709\u65b9\u6cd5\u5df2\u7ecf\u7814\u7a76\u4e86\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u8fd9\u4e9b\u8fde\u7eed\u9690\u5f0f\u8868\u793a\u6765\u6ce8\u518c\u591a\u4e2a\u795e\u7ecf\u573a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Reg-NF\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u914d\u51c6\uff0c\u5b83\u4f18\u5316\u4e24\u4e2a\u4efb\u610f\u795e\u7ecf\u573a\u4e4b\u95f4\u7684\u76f8\u5bf9 6-DoF \u53d8\u6362\uff0c\u5373\u4f7f\u8fd9\u4e24\u4e2a\u573a\u5177\u6709\u4e0d\u540c\u7684\u6bd4\u4f8b\u56e0\u5b50\u3002 Reg-NF \u7684\u5173\u952e\u7ec4\u4ef6\u5305\u62ec\u53cc\u5411\u914d\u51c6\u635f\u5931\u3001\u591a\u89c6\u56fe\u8868\u9762\u200b\u200b\u91c7\u6837\u4ee5\u53ca\u4f53\u79ef\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570 (SDF) \u7684\u5229\u7528\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u5728\u65b0\u7684\u795e\u7ecf\u573a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u914d\u51c6\u95ee\u9898\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u5957\u8be6\u5c3d\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u6765\u786e\u5b9a\u6211\u4eec\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u8ba8\u8bba\u4e86\u5c40\u9650\u6027\uff0c\u4ee5\u4fbf\u4e3a\u7814\u7a76\u754c\u5728\u4e0d\u53d7\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u5229\u7528\u795e\u7ecf\u573a\u7684\u5f00\u653e\u6311\u6218\u63d0\u4f9b\u672a\u6765\u65b9\u5411\u3002|[2402.09722v1](http://arxiv.org/pdf/2402.09722v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.09694": "|**2024-02-15**|**Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement**|\u4f7f\u7528\u51b7\u51bb\u53d1\u751f\u5668\u8fdb\u884c\u79cd\u5b50\u4f18\u5316\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u96f6\u6b21\u4f4e\u5149\u589e\u5f3a|Yuxuan Gu, Yi Jin, Ben Wang, Zhixiang Wei, Xiaoxiao Ma, Pengyang Ling, Haoxuan Wang, Huaian Chen, Enhong Chen|In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed. Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset. Extensive experiments on various benchmarks demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5728\u5927\u91cf\u81ea\u7136\u56fe\u50cf\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5668\u672c\u8d28\u4e0a\u5177\u6709\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u8fdb\u884c\u5353\u8d8a\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6709\u5e0c\u671b\u7684\u6f5c\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5668\u5d4c\u5165\u5230 Retinex \u6a21\u578b\u4e2d\u4ee5\u751f\u6210\u53cd\u5c04\u56fe\u5177\u6709\u589e\u5f3a\u7684\u7ec6\u8282\u548c\u751f\u52a8\u6027\uff0c\u4ece\u800c\u6062\u590d\u56e0\u4f4e\u5149\u6761\u4ef6\u800c\u9000\u5316\u7684\u7279\u5f81\u3002\u66f4\u8fdb\u4e00\u6b65\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5b83\u5c06\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u5230\u8f93\u5165\u79cd\u5b50\u800c\u4e0d\u662f\u4f4e\u5149\u589e\u5f3a\u6a21\u578b\u7684\u53c2\u6570\uff0c\u4ece\u800c\u5b8c\u6574\u5730\u4fdd\u7559\u4ece\u81ea\u7136\u56fe\u50cf\u4e2d\u5b66\u5230\u7684\u751f\u6210\u77e5\u8bc6\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002\u53d7\u76ca\u4e8e\u9884\u5148\u8bad\u7ec3\u7684\u77e5\u8bc6\u548c\u79cd\u5b50\u4f18\u5316\u7b56\u7565\uff0c\u4f4e\u5149\u589e\u5f3a\u6a21\u578b\u53ef\u4ee5\u663e\u7740\u89c4\u8303\u589e\u5f3a\u7ed3\u679c\u7684\u771f\u5b9e\u6027\u548c\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u5728\u4efb\u4f55\u4f4e\u5149\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5bf9\u5404\u79cd\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u4f17\u591a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2402.09694v1](http://arxiv.org/pdf/2402.09694v1)|null|\n"}, "\u5176\u4ed6": {"2402.10010": "|**2024-02-15**|**Enhancing signal detectability in learning-based CT reconstruction with a model observer inspired loss function**|\u5229\u7528\u6a21\u578b\u89c2\u5bdf\u8005\u542f\u53d1\u7684\u635f\u5931\u51fd\u6570\u589e\u5f3a\u57fa\u4e8e\u5b66\u4e60\u7684 CT \u91cd\u5efa\u4e2d\u7684\u4fe1\u53f7\u53ef\u68c0\u6d4b\u6027|Megan Lantz, Emil Y. Sidky, Ingrid S. Reiser, Xiaochuan Pan, Gregory Ongie|Deep neural networks used for reconstructing sparse-view CT data are typically trained by minimizing a pixel-wise mean-squared error or similar loss function over a set of training images. However, networks trained with such pixel-wise losses are prone to wipe out small, low-contrast features that are critical for screening and diagnosis. To remedy this issue, we introduce a novel training loss inspired by the model observer framework to enhance the detectability of weak signals in the reconstructions. We evaluate our approach on the reconstruction of synthetic sparse-view breast CT data, and demonstrate an improvement in signal detectability with the proposed loss.|\u7528\u4e8e\u91cd\u5efa\u7a00\u758f\u89c6\u56fe CT \u6570\u636e\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u901a\u8fc7\u6700\u5c0f\u5316\u4e00\u7ec4\u8bad\u7ec3\u56fe\u50cf\u4e0a\u7684\u50cf\u7d20\u5747\u65b9\u8bef\u5dee\u6216\u7c7b\u4f3c\u635f\u5931\u51fd\u6570\u6765\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u7ecf\u8fc7\u8fd9\u79cd\u50cf\u7d20\u7ea7\u635f\u5931\u8bad\u7ec3\u7684\u7f51\u7edc\u5f88\u5bb9\u6613\u6d88\u9664\u5bf9\u4e8e\u7b5b\u67e5\u548c\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u7684\u5c0f\u800c\u4f4e\u5bf9\u6bd4\u5ea6\u7684\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u53d7\u6a21\u578b\u89c2\u5bdf\u8005\u6846\u67b6\u542f\u53d1\u7684\u65b0\u578b\u8bad\u7ec3\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u91cd\u5efa\u4e2d\u5f31\u4fe1\u53f7\u7684\u53ef\u68c0\u6d4b\u6027\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u5408\u6210\u7a00\u758f\u89c6\u56fe\u4e73\u817a CT \u6570\u636e\u91cd\u5efa\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u635f\u5931\u5728\u4fe1\u53f7\u53ef\u68c0\u6d4b\u6027\u65b9\u9762\u7684\u6539\u8fdb\u3002|[2402.10010v1](http://arxiv.org/pdf/2402.10010v1)|null|\n", "2402.09731": "|**2024-02-15**|**POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge**|POBEVM\uff1a\u901a\u8fc7\u9010\u6b65\u4f18\u5316\u76ee\u6807\u4e3b\u4f53\u548c\u8fb9\u7f18\u8fdb\u884c\u5b9e\u65f6\u89c6\u9891\u62a0\u56fe|Jianming Xian|Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge. Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization.|\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u65b9\u6cd5\u5728\u89c6\u9891\u62a0\u56fe\u65b9\u9762\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u5176\u4e2d\u8bb8\u591a\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u76ee\u6807\u8eab\u4f53\u4ea7\u751f\u51c6\u786e\u7684 alpha \u4f30\u8ba1\uff0c\u4f46\u901a\u5e38\u4f1a\u4ea7\u751f\u6a21\u7cca\u6216\u4e0d\u6b63\u786e\u7684\u76ee\u6807\u8fb9\u7f18\u3002\u8fd9\u901a\u5e38\u662f\u7531\u4ee5\u4e0b\u539f\u56e0\u5f15\u8d77\u7684\uff1a 1\uff09\u5f53\u524d\u7684\u65b9\u6cd5\u603b\u662f\u4e0d\u52a0\u533a\u522b\u5730\u5bf9\u5f85\u76ee\u6807\u4f53\u548c\u8fb9\u7f18\uff1b 2) \u76ee\u6807\u4f53\u5360\u6574\u4e2a\u76ee\u6807\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4ec5\u5360\u76ee\u6807\u8fb9\u7f18\u5f88\u5c0f\u7684\u6bd4\u4f8b\u3002\u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e CNN \u7684\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5355\u72ec\u4f18\u5316\u62a0\u56fe\u76ee\u6807\u4e3b\u4f53\u548c\u8fb9\u7f18\uff08SOBE\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u65e0\u4e09\u5143\u56fe\u7684\u89c6\u9891\u62a0\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u4f18\u5316\u62a0\u56fe\u76ee\u6807\u4e3b\u4f53\u548c\u8fb9\u7f18\uff08POBEVM\uff09\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u8f7b\u5f97\u591a\uff0c\u5e76\u4e14\u5728\u9884\u6d4b\u76ee\u6807\u8fb9\u7f18\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002\u5bf9\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Edge-L1-Loss (ELL) \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5728\u62a0\u56fe\u76ee\u6807\u8fb9\u7f18\u4e0a\u5f3a\u5236\u6267\u884c\u6211\u4eec\u7684\u7f51\u7edc\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 Distinctions-646 (D646) \u548c VideoMatte240K(VM) \u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u7684\u65e0 trimap \u62a0\u56fe\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u4f18\u5316\u65b9\u9762\u3002|[2402.09731v1](http://arxiv.org/pdf/2402.09731v1)|null|\n", "2402.09658": "|**2024-02-15**|**Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm**|\u5b9e\u73b0\u6591\u9a6c\u9c7c\u7cbe\u5bc6\u5fc3\u8840\u7ba1\u5206\u6790\uff1aZACAF \u8303\u5f0f|Amir Mohammad Naderi, Jennifer G. Casey, Mao-Hsiang Huang, Rachelle Victorio, David Y. Chiang, Calum MacRae, Hung Cao, Vandana A. Gupta|Quantifying cardiovascular parameters like ejection fraction in zebrafish as a host of biological investigations has been extensively studied. Since current manual monitoring techniques are time-consuming and fallible, several image processing frameworks have been proposed to automate the process. Most of these works rely on supervised deep-learning architectures. However, supervised methods tend to be overfitted on their training dataset. This means that applying the same framework to new data with different imaging setups and mutant types can severely decrease performance. We have developed a Zebrafish Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac function in zebrafish. In this work, we further applied data augmentation, Transfer Learning (TL), and Test Time Augmentation (TTA) to ZACAF to improve the performance for the quantification of cardiovascular function quantification in zebrafish. This strategy can be integrated with the available frameworks to aid other researchers. We demonstrate that using TL, even with a constrained dataset, the model can be refined to accommodate a novel microscope setup, encompassing diverse mutant types and accommodating various video recording protocols. Additionally, as users engage in successive rounds of TL, the model is anticipated to undergo substantial enhancements in both generalizability and accuracy. Finally, we applied this approach to assess the cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy.|\u4f5c\u4e3a\u4e00\u7cfb\u5217\u751f\u7269\u5b66\u7814\u7a76\uff0c\u91cf\u5316\u6591\u9a6c\u9c7c\u5c04\u8840\u5206\u6570\u7b49\u5fc3\u8840\u7ba1\u53c2\u6570\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u7531\u4e8e\u5f53\u524d\u7684\u624b\u52a8\u76d1\u63a7\u6280\u672f\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u51e0\u79cd\u56fe\u50cf\u5904\u7406\u6846\u67b6\u6765\u81ea\u52a8\u5316\u8be5\u8fc7\u7a0b\u3002\u8fd9\u4e9b\u5de5\u4f5c\u5927\u90e8\u5206\u4f9d\u8d56\u4e8e\u76d1\u7763\u5f0f\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u7136\u800c\uff0c\u76d1\u7763\u65b9\u6cd5\u5f80\u5f80\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8fc7\u5ea6\u62df\u5408\u3002\u8fd9\u610f\u5473\u7740\u5c06\u76f8\u540c\u7684\u6846\u67b6\u5e94\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u6210\u50cf\u8bbe\u7f6e\u548c\u7a81\u53d8\u7c7b\u578b\u7684\u65b0\u6570\u636e\u53ef\u80fd\u4f1a\u4e25\u91cd\u964d\u4f4e\u6027\u80fd\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u6591\u9a6c\u9c7c\u81ea\u52a8\u5fc3\u8840\u7ba1\u8bc4\u4f30\u6846\u67b6\uff08ZACAF\uff09\u6765\u91cf\u5316\u6591\u9a6c\u9c7c\u7684\u5fc3\u810f\u529f\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u6570\u636e\u589e\u5f3a\u3001\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u548c\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\uff08TTA\uff09\u5e94\u7528\u4e8eZACAF\uff0c\u4ee5\u63d0\u9ad8\u6591\u9a6c\u9c7c\u5fc3\u8840\u7ba1\u529f\u80fd\u91cf\u5316\u7684\u6027\u80fd\u3002\u8be5\u7b56\u7565\u53ef\u4ee5\u4e0e\u53ef\u7528\u7684\u6846\u67b6\u96c6\u6210\u4ee5\u5e2e\u52a9\u5176\u4ed6\u7814\u7a76\u4eba\u5458\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4f7f\u7528 TL\uff0c\u5373\u4f7f\u6570\u636e\u96c6\u6709\u9650\uff0c\u4e5f\u53ef\u4ee5\u6539\u8fdb\u6a21\u578b\u4ee5\u9002\u5e94\u65b0\u9896\u7684\u663e\u5fae\u955c\u8bbe\u7f6e\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u7a81\u53d8\u4f53\u7c7b\u578b\u5e76\u9002\u5e94\u5404\u79cd\u89c6\u9891\u8bb0\u5f55\u534f\u8bae\u3002\u6b64\u5916\uff0c\u968f\u7740\u7528\u6237\u8fdb\u884c\u8fde\u7eed\u51e0\u8f6e\u7684 TL\uff0c\u9884\u8ba1\u8be5\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u5c06\u5f97\u5230\u663e\u7740\u589e\u5f3a\u3002\u6700\u540e\uff0c\u6211\u4eec\u5e94\u7528\u8fd9\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30 nrap \u7a81\u53d8\u6591\u9a6c\u9c7c\uff08\u4e00\u79cd\u5fc3\u808c\u75c5\u6a21\u578b\uff09\u7684\u5fc3\u8840\u7ba1\u529f\u80fd\u3002|[2402.09658v1](http://arxiv.org/pdf/2402.09658v1)|null|\n", "2402.09650": "|**2024-02-15**|**Foul prediction with estimated poses from soccer broadcast video**|\u6839\u636e\u8db3\u7403\u8f6c\u64ad\u89c6\u9891\u4e2d\u7684\u4f30\u8ba1\u59ff\u52bf\u8fdb\u884c\u72af\u89c4\u9884\u6d4b|Jiale Fang, Calvin Yeung, Keisuke Fujii|Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction. Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area.|\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u4f53\u80b2\u8fd0\u52a8\u5458\u7684\u8ddf\u8e2a\u548c\u59ff\u52bf\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5728\u4f53\u80b2\u8fd0\u52a8\u4e2d\u5229\u7528\u59ff\u52bf\u4f30\u8ba1\u8fdb\u884c\u884c\u4e3a\u9884\u6d4b\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u8db3\u7403\u72af\u89c4\u7684\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u7403\u5458\u7684\u56fe\u50cf\u5c3a\u5bf8\u8f83\u5c0f\uff0c\u5e76\u4e14\u96be\u4ee5\u4f7f\u7528\u4f8b\u5982\u7403\u548c\u59ff\u52bf\u4fe1\u606f\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u8db3\u7403\u72af\u89c4\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u7406\u65b0\u9896\u7684\u8db3\u7403\u72af\u89c4\u6570\u636e\u96c6\u6765\u96c6\u6210\u89c6\u9891\u6570\u636e\u3001\u8fb9\u754c\u6846\u4f4d\u7f6e\u3001\u56fe\u50cf\u7ec6\u8282\u548c\u59ff\u52bf\u4fe1\u606f\u3002\u6211\u4eec\u7684\u6a21\u578b\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08CNN \u548c RNN\uff09\u7684\u7ec4\u5408\u6765\u6709\u6548\u5730\u5408\u5e76\u6765\u81ea\u8fd9\u56db\u79cd\u6a21\u5f0f\u7684\u4fe1\u606f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5b8c\u6574\u6a21\u578b\u4f18\u4e8e\u6d88\u878d\u6a21\u578b\uff0c\u5e76\u4e14\u6240\u6709 RNN \u6a21\u5757\u3001\u8fb9\u754c\u6846\u4f4d\u7f6e\u548c\u56fe\u50cf\u4ee5\u53ca\u4f30\u8ba1\u59ff\u6001\u5bf9\u4e8e\u72af\u89c4\u9884\u6d4b\u90fd\u662f\u6709\u7528\u7684\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3\u8db3\u7403\u72af\u89c4\u884c\u4e3a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u53c2\u8003\u3002|[2402.09650v1](http://arxiv.org/pdf/2402.09650v1)|null|\n"}}