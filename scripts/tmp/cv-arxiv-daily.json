{"\u751f\u6210\u6a21\u578b": {"2402.18451": "|**2024-02-28**|**MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation**|MambaMIR\uff1a\u7528\u4e8e\u8054\u5408\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u4efb\u610f\u5c4f\u853d\u66fc\u5df4|Jiahao Huang, Liutao Yang, Fanwen Wang, Yinzhe Wu, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Sch\u00f6nlieb, Daoqiang Zhang, Guang Yang|The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at https://github.com/ayanglab/MambaMIR.|\u6700\u8fd1\u7684 Mamba \u6a21\u578b\u5728\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u9002\u5e94\u6027\uff0c\u5305\u62ec\u5728\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86 MambaMIR\uff08\u4e00\u79cd\u57fa\u4e8e Mamba \u7684\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u6a21\u578b\uff09\u53ca\u5176\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u53d8\u4f53 MambaMIR-GAN\u3002\u6211\u4eec\u63d0\u51fa\u7684 MambaMIR \u7ee7\u627f\u4e86\u539f\u59cb Mamba \u6a21\u578b\u7684\u51e0\u4e2a\u4f18\u70b9\uff0c\u4f8b\u5982\u7ebf\u6027\u590d\u6742\u6027\u3001\u5168\u5c40\u611f\u53d7\u91ce\u548c\u52a8\u6001\u6743\u91cd\u3002\u521b\u65b0\u7684\u4efb\u610f\u63a9\u6a21\u673a\u5236\u6709\u6548\u5730\u4f7f Mamba \u9002\u5e94\u6211\u4eec\u7684\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\uff0c\u4e3a\u540e\u7eed\u57fa\u4e8e\u8499\u7279\u5361\u7f57\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u968f\u673a\u6027\u3002\u5bf9\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\uff08\u5305\u62ec\u8986\u76d6\u819d\u76d6\u3001\u80f8\u90e8\u548c\u8179\u90e8\u7b49\u89e3\u5256\u533a\u57df\u7684\u5feb\u901f MRI \u548c SVCT\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMambaMIR \u548c MambaMIR-GAN \u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u6216\u66f4\u597d\u7684\u91cd\u5efa\u7ed3\u679c\u3002 -\u827a\u672f\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4e86\u89e3\u91cd\u5efa\u8d28\u91cf\u7684\u53ef\u9760\u6027\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/ayanglab/MambaMIR \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.18451v1](http://arxiv.org/pdf/2402.18451v1)|null|\n", "2402.18362": "|**2024-02-28**|**Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model**|\u4f7f\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53bb\u566a\u6269\u6563\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u5ba2\u89c2\u4e14\u53ef\u89e3\u91ca\u7684\u4e73\u623f\u7f8e\u5bb9\u8bc4\u4f30|Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun|As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.|\u968f\u7740\u4e73\u817a\u764c\u6cbb\u7597\u9886\u57df\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u672f\u540e\u7f8e\u5bb9\u6548\u679c\u7684\u8bc4\u4f30\u7531\u4e8e\u5176\u5bf9\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u7684\u91cd\u5927\u5f71\u54cd\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e13\u5bb6\u6807\u7b7e\u56fa\u6709\u7684\u4e3b\u89c2\u6027\uff0c\u8bc4\u4f30\u4e73\u623f\u7f8e\u5bb9\u9762\u4e34\u7740\u6311\u6218\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5373\u6ce8\u610f\u529b\u5f15\u5bfc\u53bb\u566a\u6269\u6563\u5f02\u5e38\u68c0\u6d4b\uff08AG-DDAD\uff09\uff0c\u65e8\u5728\u8bc4\u4f30\u624b\u672f\u540e\u7684\u4e73\u623f\u7f8e\u5bb9\u60c5\u51b5\uff0c\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u548c\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u65e0\u6807\u7b7e\u84b8\u998f\uff08DINO\uff09\u81ea\u76d1\u7763\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u91cd\u5efa\u548c\u5224\u522b\u533a\u57df\u7684\u7cbe\u786e\u53d8\u6362\u3002\u901a\u8fc7\u5728\u4e3b\u8981\u5305\u542b\u6b63\u5e38\u7f8e\u5bb9\u7684\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u91c7\u7528\u65e0\u76d1\u7763\u7684\u5f02\u5e38\u68c0\u6d4b\u89c6\u89d2\u6765\u81ea\u52a8\u5bf9\u7f8e\u5bb9\u8fdb\u884c\u8bc4\u5206\u3002\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7f8e\u5bb9\u8bc4\u4f30\u63d0\u4f9b\u4e86\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u8868\u793a\u548c\u53ef\u91cf\u5316\u7684\u5206\u6570\u3002\u4e0e\u5e38\u7528\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7a0b\u5e8f\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5168\u81ea\u52a8\u65b9\u6cd5\u6d88\u9664\u4e86\u624b\u52a8\u6ce8\u91ca\u7684\u9700\u8981\u5e76\u63d0\u4f9b\u5ba2\u89c2\u7684\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u8d85\u8d8a\u4e86\u4e73\u623f\u7f8e\u5bb9\u7684\u8303\u56f4\uff0c\u4ee3\u8868\u4e86\u533b\u5b66\u9886\u57df\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.18362v1](http://arxiv.org/pdf/2402.18362v1)|null|\n", "2402.18351": "|**2024-02-28**|**LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping**|LatentSwap\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u4eba\u8138\u4ea4\u6362\u6f5c\u5728\u4ee3\u7801\u6620\u5c04\u6846\u67b6|Changho Choi, Minho Kim, Junhyeok Lee, Hyoung-Kyu Song, Younggeun Kim, Seungryong Kim|We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator. Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward. The loss objective consists of only three terms, and can effectively control the face swap results between source and target images. By attaching a pre-trained GAN inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models. We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks. The source code and models can be found at \\url{https://github.com/usingcolor/LatentSwap}.|\u6211\u4eec\u63d0\u51fa LatentSwap\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u9762\u90e8\u4ea4\u6362\u6846\u67b6\uff0c\u751f\u6210\u7ed9\u5b9a\u751f\u6210\u5668\u7684\u9762\u90e8\u4ea4\u6362\u6f5c\u5728\u4ee3\u7801\u3002\u5229\u7528\u968f\u673a\u91c7\u6837\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5f88\u8f7b\uff0c\u9664\u4e86\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u4e4b\u5916\u4e0d\u9700\u8981\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e5f\u5feb\u901f\u800c\u7b80\u5355\u3002\u635f\u5931\u76ee\u6807\u4ec5\u7531\u4e09\u9879\u7ec4\u6210\uff0c\u53ef\u4ee5\u6709\u6548\u63a7\u5236\u6e90\u56fe\u50cf\u548c\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u4eba\u8138\u4ea4\u6362\u7ed3\u679c\u3002\u901a\u8fc7\u9644\u52a0\u72ec\u7acb\u4e8e\u6a21\u578b\u7684\u9884\u8bad\u7ec3 GAN \u53cd\u8f6c\u6a21\u578b\u5e76\u4f7f\u7528 StyleGAN2 \u751f\u6210\u5668\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u751f\u6210\u4e0e\u5176\u4ed6\u7ade\u4e89\u6027\u4eba\u8138\u4ea4\u6362\u6a21\u578b\u76f8\u5f53\u7684\u903c\u771f\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u3002\u6211\u4eec\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u9002\u7528\u4e8e\u5176\u4ed6\u751f\u6210\u5668\uff0c\u4f8b\u5982 StyleNeRF\uff0c\u4e3a 3D \u611f\u77e5\u9762\u90e8\u4ea4\u6362\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u4e14\u8fd8\u4e0e\u5176\u4ed6\u4e0b\u6e38 StyleGAN2 \u751f\u6210\u5668\u4efb\u52a1\u517c\u5bb9\u3002\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u4ee5\u5728 \\url{https://github.com/usingcolor/LatentSwap} \u627e\u5230\u3002|[2402.18351v1](http://arxiv.org/pdf/2402.18351v1)|null|\n", "2402.18331": "|**2024-02-28**|**FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes**|FineDiffusion\uff1a\u6269\u5c55\u6269\u6563\u6a21\u578b\u4ee5\u751f\u6210\u5177\u6709 10,000 \u4e2a\u7c7b\u522b\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf|Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai|The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7c7b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u56fe\u50cf\u800c\u95fb\u540d\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u5148\u524d\u7684\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u751f\u6210\u4e00\u822c\u7c7b\u522b\u7684\u56fe\u50cf\uff0c\u4f8b\u5982 ImageNet-1k \u4e2d\u7684 1000 \u4e2a\u7c7b\u522b\u3002\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u56fe\u50cf\u751f\u6210\uff0c\u4ecd\u7136\u662f\u6709\u5f85\u63a2\u7d22\u7684\u8fb9\u754c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a FineDiffusion \u7684\u53c2\u6570\u9ad8\u6548\u7b56\u7565\uff0c\u7528\u4e8e\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u751f\u6210\u5177\u6709 10,000 \u4e2a\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u56fe\u50cf\u3002 FineDiffusion \u4ec5\u901a\u8fc7\u5fae\u8c03\u5206\u5c42\u7c7b\u5d4c\u5165\u5668\u3001\u504f\u5dee\u9879\u548c\u5f52\u4e00\u5316\u5c42\u53c2\u6570\u5373\u53ef\u663e\u7740\u52a0\u901f\u8bad\u7ec3\u5e76\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u751f\u6210\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e13\u4e3a\u7ec6\u7c92\u5ea6\u7c7b\u522b\u5b9a\u5236\u7684\u8d85\u7c7b\u6761\u4ef6\u6307\u5bfc\u6765\u53d6\u4ee3\u4f20\u7edf\u7684\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u91c7\u6837\u3002\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u6bd4\uff0cFineDiffusion \u5b9e\u73b0\u4e86 1.56 \u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u5e76\u4e14\u4ec5\u9700\u8981\u5b58\u50a8\u603b\u6a21\u578b\u53c2\u6570\u7684 1.77%\uff0c\u540c\u65f6\u5728 10,000 \u4e2a\u7c7b\u522b\u7684\u56fe\u50cf\u751f\u6210\u4e0a\u5b9e\u73b0\u4e86 9.776 \u7684\u6700\u5148\u8fdb\u7684 FID\u3002\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5176\u4ed6\u53c2\u6570\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u548c\u66f4\u591a\u751f\u6210\u7684\u7ed3\u679c\u53ef\u5728\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9\u4e0a\u627e\u5230\uff1ahttps://finediffusion.github.io/\u3002|[2402.18331v1](http://arxiv.org/pdf/2402.18331v1)|null|\n", "2402.18206": "|**2024-02-28**|**Balancing Act: Distribution-Guided Debiasing in Diffusion Models**|\u5e73\u8861\u6cd5\uff1a\u6269\u6563\u6a21\u578b\u4e2d\u5206\u5e03\u5f15\u5bfc\u7684\u53bb\u504f|Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu|Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.|\u6269\u6563\u6a21\u578b (DM) \u5df2\u6210\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u5177\u6709\u524d\u6240\u672a\u6709\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u5e7f\u6cdb\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u521b\u610f\u5e94\u7528\u3002\u7136\u800c\uff0cDM \u53cd\u6620\u4e86\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u3002\u8fd9\u5728\u9762\u5b54\u7684\u80cc\u666f\u4e0b\u5c24\u5176\u4ee4\u4eba\u62c5\u5fe7\uff0c\u5176\u4e2d DM \u66f4\u559c\u6b22\u4e00\u4e2a\u4eba\u53e3\u7edf\u8ba1\u4e9a\u7ec4\u800c\u4e0d\u662f\u5176\u4ed6\u4e9a\u7ec4\uff08\u4f8b\u5982\u5973\u6027\u4e0e\u7537\u6027\uff09\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u6570\u636e\u6216\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6d88\u9664 DM \u504f\u5dee\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5e03\u6307\u5bfc\uff0c\u5b83\u5f3a\u5236\u751f\u6210\u7684\u56fe\u50cf\u9075\u5faa\u89c4\u5b9a\u7684\u5c5e\u6027\u5206\u5e03\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u57fa\u4e8e\u4ee5\u4e0b\u5173\u952e\u89c1\u89e3\uff1a\u53bb\u566a UNet \u7684\u6f5c\u5728\u7279\u5f81\u62e5\u6709\u4e30\u5bcc\u7684\u4eba\u53e3\u7edf\u8ba1\u8bed\u4e49\uff0c\u5e76\u4e14\u53ef\u4ee5\u5229\u7528\u76f8\u540c\u7684\u7279\u5f81\u6765\u6307\u5bfc\u53bb\u504f\u751f\u6210\u3002\u6211\u4eec\u8bad\u7ec3\u5c5e\u6027\u5206\u5e03\u9884\u6d4b\u5668 (ADP) - \u4e00\u4e2a\u5c0f\u578b mlp\uff0c\u5c06\u6f5c\u5728\u7279\u5f81\u6620\u5c04\u5230\u5c5e\u6027\u5206\u5e03\u3002 ADP \u4f7f\u7528\u73b0\u6709\u5c5e\u6027\u5206\u7c7b\u5668\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u3002\u62df\u8bae\u7684 ADP \u5206\u914d\u6307\u5357\u4f7f\u6211\u4eec\u80fd\u591f\u516c\u5e73\u751f\u6210\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u5355\u4e2a/\u591a\u4e2a\u5c5e\u6027\u7684\u504f\u5dee\uff0c\u5e76\u4e14\u5bf9\u4e8e\u65e0\u6761\u4ef6\u548c\u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u8bf4\uff0c\u5176\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u7684\u6570\u636e\u91cd\u65b0\u5e73\u8861\u8bad\u7ec3\u96c6\u6765\u8bad\u7ec3\u516c\u5e73\u5c5e\u6027\u5206\u7c7b\u5668\u7684\u4e0b\u6e38\u4efb\u52a1\u3002|[2402.18206v1](http://arxiv.org/pdf/2402.18206v1)|null|\n", "2402.18137": "|**2024-02-28**|**DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning**|DecisionNCE\uff1a\u901a\u8fc7\u9690\u5f0f\u504f\u597d\u5b66\u4e60\u4f53\u73b0\u591a\u6a21\u6001\u8868\u793a|Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, et.al.|Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning. Project Page: https://2toinf.github.io/DecisionNCE/|\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u5df2\u6210\u4e3a\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u8868\u5f81\u5b66\u4e60\u4e09\u5927\u76ee\u6807\u7684\u6709\u6548\u7b56\u7565\uff1a1\uff09\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u4efb\u52a1\u8fdb\u5c55\u4fe1\u606f\uff1b 2\uff09\u52a0\u5f3a\u89c6\u89c9\u8868\u73b0\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff1b 3\uff09\u6355\u83b7\u8f68\u8ff9\u7ea7\u8bed\u8a00\u57fa\u7840\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5355\u72ec\u7684\u76ee\u6807\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u4f1a\u8fbe\u5230\u6b21\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7edf\u4e00\u76ee\u6807\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u4ece\u56fe\u50cf\u5e8f\u5217\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4efb\u52a1\u8fdb\u5c55\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u4e0e\u8bed\u8a00\u6307\u4ee4\u65e0\u7f1d\u5bf9\u9f50\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u901a\u8fc7\u9690\u5f0f\u504f\u597d\uff0c\u89c6\u89c9\u8f68\u8ff9\u672c\u8d28\u4e0a\u4e0e\u5176\u76f8\u5e94\u7684\u8bed\u8a00\u6307\u4ee4\u6bd4\u4e0d\u5339\u914d\u7684\u914d\u5bf9\u66f4\u597d\u5730\u5bf9\u9f50\uff0c\u6d41\u884c\u7684 Bradley-Terry \u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u5956\u52b1\u91cd\u65b0\u53c2\u6570\u5316\u8f6c\u5316\u4e3a\u8868\u793a\u5b66\u4e60\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6846\u67b6 DecisionNCE \u53cd\u6620\u4e86 InfoNCE \u98ce\u683c\u7684\u76ee\u6807\uff0c\u4f46\u4e13\u95e8\u9488\u5bf9\u51b3\u7b56\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u4f18\u96c5\u5730\u63d0\u53d6\u672c\u5730\u548c\u5168\u5c40\u4efb\u52a1\u8fdb\u5c55\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9690\u5f0f\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u7f16\u7801\u786e\u4fdd\u8f68\u8ff9\u7ea7\u6307\u4ee4\u843d\u5730\u3002\u5bf9\u6a21\u62df\u673a\u5668\u4eba\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7684\u8bc4\u4f30\u8868\u660e\uff0cDecisionNCE \u6709\u6548\u5730\u4fc3\u8fdb\u4e86\u591a\u6837\u5316\u7684\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u4efb\u52a1\uff0c\u4e3a\u7edf\u4e00\u8868\u793a\u548c\u5956\u52b1\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://2toinf.github.io/DecisionNCE/|[2402.18137v1](http://arxiv.org/pdf/2402.18137v1)|null|\n", "2402.18092": "|**2024-02-28**|**Context-aware Talking Face Video Generation**|\u4e0a\u4e0b\u6587\u611f\u77e5\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u751f\u6210|Meidai Xuanyuan, Yuwang Wang, Honglei Guo, Qionghai Dai|In this paper, we consider a novel and practical case for talking face video generation. Specifically, we focus on the scenarios involving multi-people interactions, where the talking context, such as audience or surroundings, is present. In these situations, the video generation should take the context into consideration in order to generate video content naturally aligned with driving audios and spatially coherent to the context. To achieve this, we provide a two-stage and cross-modal controllable video generation pipeline, taking facial landmarks as an explicit and compact control signal to bridge the driving audio, talking context and generated videos. Inside this pipeline, we devise a 3D video diffusion model, allowing for efficient contort of both spatial conditions (landmarks and context video), as well as audio condition for temporally coherent generation. The experimental results verify the advantage of the proposed method over other baselines in terms of audio-video synchronization, video fidelity and frame consistency.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u5b9e\u7528\u7684\u4eba\u8138\u89c6\u9891\u751f\u6210\u6848\u4f8b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5173\u6ce8\u6d89\u53ca\u591a\u4eba\u4ea4\u4e92\u7684\u573a\u666f\uff0c\u5176\u4e2d\u5b58\u5728\u8c08\u8bdd\u4e0a\u4e0b\u6587\uff0c\u4f8b\u5982\u89c2\u4f17\u6216\u5468\u56f4\u73af\u5883\u3002\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u89c6\u9891\u751f\u6210\u5e94\u8003\u8651\u4e0a\u4e0b\u6587\uff0c\u4ee5\u4fbf\u751f\u6210\u4e0e\u9a7e\u9a76\u97f3\u9891\u81ea\u7136\u5bf9\u9f50\u5e76\u5728\u7a7a\u95f4\u4e0a\u4e0e\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u89c6\u9891\u5185\u5bb9\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u3001\u8de8\u6a21\u5f0f\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u9762\u90e8\u6807\u5fd7\u4f5c\u4e3a\u660e\u786e\u800c\u7d27\u51d1\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u4ee5\u6865\u63a5\u9a7e\u9a76\u97f3\u9891\u3001\u8c08\u8bdd\u4e0a\u4e0b\u6587\u548c\u751f\u6210\u7684\u89c6\u9891\u3002\u5728\u6b64\u7ba1\u9053\u5185\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a 3D \u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5141\u8bb8\u6709\u6548\u626d\u66f2\u7a7a\u95f4\u6761\u4ef6\uff08\u5730\u6807\u548c\u4e0a\u4e0b\u6587\u89c6\u9891\uff09\u4ee5\u53ca\u97f3\u9891\u6761\u4ef6\u4ee5\u5b9e\u73b0\u65f6\u95f4\u76f8\u5e72\u7684\u751f\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u97f3\u89c6\u9891\u540c\u6b65\u3001\u89c6\u9891\u4fdd\u771f\u5ea6\u548c\u5e27\u4e00\u81f4\u6027\u65b9\u9762\u76f8\u5bf9\u4e8e\u5176\u4ed6\u57fa\u7ebf\u7684\u4f18\u52bf\u3002|[2402.18092v1](http://arxiv.org/pdf/2402.18092v1)|null|\n", "2402.18078": "|**2024-02-28**|**Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis**|\u7528\u4e8e\u59ff\u52bf\u5f15\u5bfc\u4eba\u4f53\u56fe\u50cf\u5408\u6210\u7684\u4ece\u7c97\u5230\u7ec6\u7684\u6f5c\u5728\u6269\u6563|Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai|Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at https://github.com/YanzuoLu/CFLD.|\u6269\u6563\u6a21\u578b\u662f\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u5df2\u88ab\u7528\u4e8e\u59ff\u52bf\u5f15\u5bfc\u4eba\u4f53\u56fe\u50cf\u5408\u6210\uff08PGPIS\uff09\uff0c\u5177\u6709\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u53ea\u662f\u5c06\u4eba\u7684\u5916\u89c2\u4e0e\u76ee\u6807\u59ff\u52bf\u5bf9\u9f50\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u6e90\u4eba\u56fe\u50cf\u7684\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\uff0c\u5b83\u4eec\u5f88\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 PGPIS \u7c97\u5230\u7ec6\u6f5c\u5728\u6269\u6563\uff08CFLD\uff09\u65b9\u6cd5\u3002\u5728\u7f3a\u4e4f\u56fe\u50cf\u6807\u9898\u5bf9\u548c\u6587\u672c\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7eaf\u7cb9\u57fa\u4e8e\u56fe\u50cf\u7684\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u63a7\u5236\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u611f\u77e5\u7ec6\u5316\u89e3\u7801\u5668\u65e8\u5728\u9010\u6b65\u7ec6\u5316\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\uff0c\u5e76\u63d0\u53d6\u4eba\u7269\u56fe\u50cf\u7684\u8bed\u4e49\u7406\u89e3\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u7684\u63d0\u793a\u3002\u8fd9\u5141\u8bb8\u5728\u4e0d\u540c\u9636\u6bb5\u89e3\u8026\u7ec6\u7c92\u5ea6\u7684\u5916\u89c2\u548c\u59ff\u52bf\u4fe1\u606f\u63a7\u5236\uff0c\u4ece\u800c\u907f\u514d\u6f5c\u5728\u7684\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\u3002\u4e3a\u4e86\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\u7ec6\u8282\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7c92\u5ea6\u6ce8\u610f\u6a21\u5757\uff0c\u5c06\u591a\u5c3a\u5ea6\u7ec6\u7c92\u5ea6\u5916\u89c2\u7279\u5f81\u7f16\u7801\u4e3a\u504f\u5dee\u9879\uff0c\u4ee5\u589e\u5f3a\u7c97\u7c92\u5ea6\u63d0\u793a\u3002 DeepFashion \u57fa\u51c6\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u7ed3\u679c\u90fd\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e PGPIS \u7684\u6700\u65b0\u6280\u672f\u7684\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/YanzuoLu/CFLD \u83b7\u53d6\u3002|[2402.18078v1](http://arxiv.org/pdf/2402.18078v1)|null|\n", "2402.18068": "|**2024-02-28**|**SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model**|SynArtifact\uff1a\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\u8fdb\u884c\u5206\u7c7b\u548c\u6d88\u9664|Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao|In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.|\u5728\u5feb\u901f\u53d1\u5c55\u7684\u56fe\u50cf\u5408\u6210\u9886\u57df\uff0c\u4e00\u4e2a\u4e25\u5cfb\u7684\u6311\u6218\u662f\u5b58\u5728\u590d\u6742\u7684\u4f2a\u5f71\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u4f1a\u635f\u5bb3\u5408\u6210\u56fe\u50cf\u7684\u611f\u77e5\u771f\u5b9e\u611f\u3002\u4e3a\u4e86\u51cf\u5c11\u4f2a\u5f71\u5e76\u63d0\u9ad8\u5408\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u4f2a\u5f71\u5206\u7c7b\u5668\uff0c\u4ee5\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u5404\u79cd\u4f2a\u5f71\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u76d1\u7763\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4f2a\u5f71\u5206\u7c7b\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5e26\u6709\u4f2a\u5f71\u6ce8\u91ca\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fae\u8c03 VLM\uff0c\u540d\u4e3a SynArtifact-1K\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 VLM \u8868\u73b0\u51fa\u5353\u8d8a\u7684\u4f2a\u5f71\u8bc6\u522b\u80fd\u529b\uff0c\u6bd4\u57fa\u7ebf\u9ad8\u51fa 25.66%\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u6b21\u63d0\u51fa\u8fd9\u79cd\u7aef\u5230\u7aef\u7684\u5de5\u4ef6\u5206\u7c7b\u4efb\u52a1\u548c\u89e3\u51b3\u65b9\u6848\u3002\u6700\u540e\uff0c\u6211\u4eec\u5229\u7528 VLM \u7684\u8f93\u51fa\u4f5c\u4e3a\u53cd\u9988\u6765\u5b8c\u5584\u751f\u6210\u6a21\u578b\u4ee5\u51cf\u8f7b\u4f2a\u5f71\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7ec6\u5316\u6269\u6563\u6a21\u578b\u5408\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u5f97\u5230\u4e86\u660e\u663e\u63d0\u9ad8\u3002|[2402.18068v1](http://arxiv.org/pdf/2402.18068v1)|null|\n", "2402.18028": "|**2024-02-28**|**OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine**|OpenMEDLab\uff1a\u533b\u5b66\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5f00\u6e90\u5e73\u53f0|Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, et.al.|The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.|\u63a8\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u4f8b\u5982 GPTv4 \u548c Gemini\uff0c\u91cd\u5851\u4e86\u673a\u5668\u5b66\u4e60\u548c\u8bb8\u591a\u5176\u4ed6\u7814\u7a76\u9886\u57df\u7684\u7814\u7a76\uff08\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\uff09\u683c\u5c40\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u57fa\u7840\u6a21\u578b\u7684\u7279\u5b9a\u9886\u57df\u5e94\u7528\uff08\u4f8b\u5982\u5728\u533b\u5b66\u4e2d\uff09\u4ecd\u7136\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\u6216\u901a\u5e38\u5904\u4e8e\u975e\u5e38\u65e9\u671f\u7684\u9636\u6bb5\u3002\u5b83\u5c06\u9700\u8981\u4e00\u5957\u5355\u72ec\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u6a21\u578b\u9002\u5e94\u6280\u672f\uff0c\u901a\u8fc7\u8fdb\u4e00\u6b65\u6269\u5c55\u548c\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u548c\u6570\u636e\u8fd9\u4e9b\u6a21\u578b\u3002\u5982\u679c\u5c06\u6570\u636e\u3001\u7b97\u6cd5\u548c\u9884\u5148\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u96c6\u4e2d\u5728\u4e00\u8d77\u5e76\u4ee5\u6709\u7ec4\u7ec7\u7684\u65b9\u5f0f\u5f00\u6e90\uff0c\u5219\u53ef\u4ee5\u5927\u5927\u52a0\u901f\u6b64\u7c7b\u6280\u672f\u7684\u53d1\u5c55\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 OpenMEDLab\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u5f00\u6e90\u5e73\u53f0\u3002\u5b83\u4e0d\u4ec5\u56ca\u62ec\u4e86\u9488\u5bf9\u4e00\u7ebf\u4e34\u5e8a\u548c\u751f\u7269\u4fe1\u606f\u5b66\u5e94\u7528\u63d0\u793a\u548c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u7684\u5f00\u521b\u6027\u5c1d\u8bd5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e14\u8fd8\u5229\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u6784\u5efa\u7279\u5b9a\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u3002\u91cd\u8981\u7684\u662f\uff0c\u5b83\u4e3a\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u6a21\u5f0f\u3001\u4e34\u5e8a\u6587\u672c\u3001\u86cb\u767d\u8d28\u5de5\u7a0b\u7b49\u63d0\u4f9b\u4e86\u4e00\u7ec4\u9884\u5148\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u5404\u79cd\u57fa\u51c6\u4e2d\uff0c\u8fd8\u4e3a\u6bcf\u4e2a\u6536\u96c6\u7684\u65b9\u6cd5\u548c\u6a21\u578b\u5c55\u793a\u4e86\u9f13\u821e\u4eba\u5fc3\u4e14\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u6211\u4eec\u6b22\u8fce\u533b\u7597\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u4e0d\u65ad\u4e3aOpenMEDLab\u8d21\u732e\u524d\u6cbf\u7684\u65b9\u6cd5\u548c\u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7https://github.com/openmedlab\u8bbf\u95ee\u3002|[2402.18028v1](http://arxiv.org/pdf/2402.18028v1)|null|\n", "2402.18027": "|**2024-02-28**|**Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift**|\u6253\u7834\u9ed1\u5323\u5b50\uff1a\u9488\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u7f6e\u4fe1\u5f15\u5bfc\u6a21\u578b\u53cd\u8f6c\u653b\u51fb|Xinhao Liu, Yingzhao Jiang, Zetao Lin|Model inversion attacks (MIAs) seek to infer the private training data of a target classifier by generating synthetic images that reflect the characteristics of the target class through querying the model. However, prior studies have relied on full access to the target model, which is not practical in real-world scenarios. Additionally, existing black-box MIAs assume that the image prior and target model follow the same distribution. However, when confronted with diverse data distribution settings, these methods may result in suboptimal performance in conducting attacks. To address these limitations, this paper proposes a \\textbf{C}onfidence-\\textbf{G}uided \\textbf{M}odel \\textbf{I}nversion attack method called CG-MI, which utilizes the latent space of a pre-trained publicly available generative adversarial network (GAN) as prior information and gradient-free optimizer, enabling high-resolution MIAs across different data distributions in a black-box setting. Our experiments demonstrate that our method significantly \\textbf{outperforms the SOTA black-box MIA by more than 49\\% for Celeba and 58\\% for Facescrub in different distribution settings}. Furthermore, our method exhibits the ability to generate high-quality images \\textbf{comparable to those produced by white-box attacks}. Our method provides a practical and effective solution for black-box model inversion attacks.|\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\uff08MIA\uff09\u8bd5\u56fe\u901a\u8fc7\u67e5\u8be2\u6a21\u578b\u751f\u6210\u53cd\u6620\u76ee\u6807\u7c7b\u7279\u5f81\u7684\u5408\u6210\u56fe\u50cf\u6765\u63a8\u65ad\u76ee\u6807\u5206\u7c7b\u5668\u7684\u79c1\u6709\u8bad\u7ec3\u6570\u636e\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7814\u7a76\u4f9d\u8d56\u4e8e\u5bf9\u76ee\u6807\u6a21\u578b\u7684\u5b8c\u5168\u8bbf\u95ee\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e76\u4e0d\u5b9e\u7528\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u9ed1\u76d2 MIA \u5047\u8bbe\u56fe\u50cf\u5148\u9a8c\u548c\u76ee\u6807\u6a21\u578b\u9075\u5faa\u76f8\u540c\u7684\u5206\u5e03\u3002\u7136\u800c\uff0c\u5f53\u9762\u5bf9\u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u8bbe\u7f6e\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u653b\u51fb\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CG-MI \u7684 \\textbf{C}onfidence-\\textbf{G}uided \\textbf{M}odel \\textbf{I}nversion \u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u7f16\u7801\u7684\u6f5c\u5728\u7a7a\u95f4\u7ecf\u8fc7\u8bad\u7ec3\u7684\u516c\u5f00\u53ef\u7528\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4f5c\u4e3a\u5148\u9a8c\u4fe1\u606f\u548c\u65e0\u68af\u5ea6\u4f18\u5316\u5668\uff0c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u8de8\u4e0d\u540c\u6570\u636e\u5206\u5e03\u7684\u9ad8\u5206\u8fa8\u7387 MIA\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\\textbf{\u5728\u4e0d\u540c\u7684\u5206\u5e03\u8bbe\u7f6e\u4e2d\uff0cCeleba \u7684\u6027\u80fd\u4f18\u4e8e SOTA \u9ed1\u76d2 MIA \u8d85\u8fc7 49\\%\uff0cFacescrub \u7684\u6027\u80fd\u4f18\u4e8e 58\\%}\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\\textbf{\u53ef\u4e0e\u767d\u76d2\u653b\u51fb\u751f\u6210\u7684\u56fe\u50cf\u76f8\u5ab2\u7f8e}\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u9ed1\u76d2\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2402.18027v1](http://arxiv.org/pdf/2402.18027v1)|null|\n", "2402.17986": "|**2024-02-28**|**PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis**|PolyOculus\uff1a\u57fa\u4e8e\u56fe\u50cf\u7684\u540c\u65f6\u591a\u89c6\u56fe\u65b0\u9896\u89c6\u56fe\u5408\u6210|Jason J. Yu, Tristan Aumentado-Armstrong, Fereshteh Forghani, Konstantinos G. Derpanis, Marcus A. Brubaker|This paper considers the problem of generative novel view synthesis (GNVS), generating novel, plausible views of a scene given a limited number of known views. Here, we propose a set-based generative model that can simultaneously generate multiple, self-consistent new views, conditioned on any number of known views. Our approach is not limited to generating a single image at a time and can condition on zero, one, or more views. As a result, when generating a large number of views, our method is not restricted to a low-order autoregressive generation approach and is better able to maintain generated image quality over large sets of images. We evaluate the proposed model on standard NVS datasets and show that it outperforms the state-of-the-art image-based GNVS baselines. Further, we show that the model is capable of generating sets of camera views that have no natural sequential ordering, like loops and binocular trajectories, and significantly outperforms other methods on such tasks.|\u672c\u6587\u8003\u8651\u4e86\u751f\u6210\u65b0\u9896\u89c6\u56fe\u5408\u6210\uff08GNVS\uff09\u7684\u95ee\u9898\uff0c\u5373\u5728\u7ed9\u5b9a\u6709\u9650\u6570\u91cf\u7684\u5df2\u77e5\u89c6\u56fe\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u65b0\u9896\u7684\u3001\u5408\u7406\u7684\u573a\u666f\u89c6\u56fe\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u7684\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u751f\u6210\u591a\u4e2a\u3001\u81ea\u6d3d\u7684\u65b0\u89c6\u56fe\uff0c\u4ee5\u4efb\u610f\u6570\u91cf\u7684\u5df2\u77e5\u89c6\u56fe\u4e3a\u6761\u4ef6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9650\u4e8e\u4e00\u6b21\u751f\u6210\u5355\u4e2a\u56fe\u50cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u96f6\u4e2a\u3001\u4e00\u4e2a\u6216\u591a\u4e2a\u89c6\u56fe\u4e3a\u6761\u4ef6\u3002\u56e0\u6b64\uff0c\u5f53\u751f\u6210\u5927\u91cf\u89c6\u56fe\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9650\u4e8e\u4f4e\u9636\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u66f4\u597d\u5730\u5728\u5927\u91cf\u56fe\u50cf\u4e0a\u4fdd\u6301\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u3002\u6211\u4eec\u5728\u6807\u51c6 NVS \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u5e76\u8868\u660e\u5b83\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u50cf\u7684 GNVS \u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u6ca1\u6709\u81ea\u7136\u987a\u5e8f\u6392\u5e8f\u7684\u76f8\u673a\u89c6\u56fe\u96c6\uff0c\u4f8b\u5982\u5faa\u73af\u548c\u53cc\u76ee\u8f68\u8ff9\uff0c\u5e76\u4e14\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u663e\u7740\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002|[2402.17986v1](http://arxiv.org/pdf/2402.17986v1)|null|\n", "2402.17969": "|**2024-02-28**|**Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction**|\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5229\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u63d0\u53d6\u7684\u5b57\u5e55\u8bc4\u4f30\u65b9\u6cd5|Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki|Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.|\u9274\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u5efa\u6a21\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u5bf9\u673a\u5668\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u7684\u51c6\u786e\u8bc4\u4f30\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u504f\u597d\u5730\u8bc4\u4f30\u5b57\u5e55\uff0c\u6307\u6807\u9700\u8981\u533a\u5206\u4e0d\u540c\u8d28\u91cf\u548c\u5185\u5bb9\u7684\u5b57\u5e55\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6307\u6807\u65e0\u6cd5\u8d85\u8d8a\u5355\u8bcd\u7684\u8868\u9762\u5339\u914d\u6216\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u6bd4\u8f83\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u4ecd\u7136\u9700\u8981\u6539\u8fdb\u3002\u672c\u6587\u63d0\u51fa\u4e86VisCE$^2$\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b57\u5e55\u8bc4\u4f30\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u5b83\u6307\u7684\u662f\u56fe\u50cf\u7684\u8be6\u7ec6\u5185\u5bb9\uff0c\u5305\u62ec\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u3002\u901a\u8fc7\u63d0\u53d6\u5b83\u4eec\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u6211\u4eec\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u53d6\u4ee3\u4e86\u4eba\u5de5\u7f16\u5199\u7684\u53c2\u8003\u6587\u732e\uff0c\u5e76\u5e2e\u52a9 VLM \u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\uff0c\u4ece\u800c\u63d0\u9ad8\u8bc4\u4f30\u6027\u80fd\u3002\u901a\u8fc7\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5143\u8bc4\u4f30\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86 VisCE$^2$ \u5728\u6355\u83b7\u5b57\u5e55\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u9884\u8bad\u7ec3\u6307\u6807\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5353\u8d8a\u4e00\u81f4\u6027\u3002|[2402.17969v1](http://arxiv.org/pdf/2402.17969v1)|null|\n"}, "\u591a\u6a21\u6001": {"2402.18507": "|**2024-02-28**|**Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images**|\u591a\u6a21\u6001\u5b66\u4e60\u6539\u5584\u7535\u5f71 MR \u56fe\u50cf\u4e2d\u7684\u5fc3\u810f\u665a\u671f\u673a\u68b0\u6fc0\u6d3b\u68c0\u6d4b|Jiarui Xing, Nian Wu, Kenneth Bilchick, Frederick Epstein, Miaomiao Zhang|This paper presents a multimodal deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images. More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection. An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs. Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection. Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5148\u8fdb\u7684\u56fe\u50cf\u6280\u672f\u6765\u63d0\u9ad8\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5e38\u89c4\u91c7\u96c6\u7684\u6807\u51c6\u56fe\u50cf\u7684\u4e34\u5e8a\u5206\u6790\u7684\u6027\u80fd\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u8054\u5408\u5b66\u4e60\u7f51\u7edc\uff0c\u9996\u6b21\u5229\u7528\u53d7\u6fc0\u56de\u6ce2\u4f4d\u79fb\u7f16\u7801\uff08DENSE\uff09\u83b7\u5f97\u7684\u5fc3\u808c\u5e94\u53d8\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\u6765\u6307\u5bfc\u540e\u671f\u673a\u68b0\u6fc0\u6d3b\u4e2d\u7684\u7535\u5f71\u5fc3\u810f\u78c1\u5171\u632f\uff08CMR\uff09\u6210\u50cf\u5206\u6790\u3002 LMA\uff09\u68c0\u6d4b\u3002\u5229\u7528\u56fe\u50cf\u914d\u51c6\u7f51\u7edc\u4ece\u6807\u51c6\u7535\u5f71 CMR \u4e2d\u83b7\u53d6\u5fc3\u810f\u8fd0\u52a8\u7684\u77e5\u8bc6\uff0c\u8fd9\u662f\u5e94\u53d8\u503c\u7684\u91cd\u8981\u7279\u5f81\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u7684\u6846\u67b6\u7531\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\u7ec4\u6210\uff1a\uff08i\uff09\u4e00\u4e2a DENSE \u76d1\u7763\u5e94\u53d8\u7f51\u7edc\uff0c\u5229\u7528\u4ece\u914d\u51c6\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u8fd0\u52a8\u7279\u5f81\u6765\u9884\u6d4b\u5fc3\u808c\u5e94\u53d8\uff1b (ii) LMA \u7f51\u7edc\u5229\u7528\u9884\u6d4b\u5e94\u53d8\u8fdb\u884c\u6709\u6548\u7684 LMA \u68c0\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u5de5\u4f5c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u7535\u5f71 CMR \u56fe\u50cf\u7684\u5e94\u53d8\u5206\u6790\u548c LMA \u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e0e DENSE \u7684\u6210\u5c31\u66f4\u52a0\u4e00\u81f4\u3002|[2402.18507v1](http://arxiv.org/pdf/2402.18507v1)|null|\n", "2402.18490": "|**2024-02-28**|**TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding**|TAMM\uff1a\u7528\u4e8e 3D \u5f62\u72b6\u7406\u89e3\u7684 TriAdapter \u591a\u6a21\u6001\u5b66\u4e60|Zhihao Zhang, Shengcao Cao, Yu-Xiong Wang|The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \\url{https://alanzhangcs.github.io/tamm-page}.|\u5f53\u524d 3D \u5f62\u72b6\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u963b\u788d\u4e86 3D \u5f62\u72b6\u7406\u89e3\u7684\u8fdb\u6b65\uff0c\u5e76\u6fc0\u53d1\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u7684\u77e5\u8bc6\u4ece\u6570\u636e\u4e30\u5bcc\u7684 2D \u56fe\u50cf\u548c\u8bed\u8a00\u6a21\u6001\u8f6c\u79fb\u5230 3D \u5f62\u72b6\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u56fe\u50cf\u548c\u8bed\u8a00\u8868\u793a\u5df2\u7ecf\u901a\u8fc7 CLIP \u7b49\u8de8\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u9f50\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u5728\u73b0\u6709\u7684\u591a\u6a21\u6001 3D \u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u56fe\u50cf\u6a21\u6001\u7684\u8d21\u732e\u4e0d\u5982\u8bed\u8a00\u3002\u8fd9\u662f\u7531\u4e8e 2D \u56fe\u50cf\u4e2d\u7684\u57df\u504f\u79fb\u548c\u6bcf\u79cd\u6a21\u6001\u7684\u4e0d\u540c\u7126\u70b9\u3002\u4e3a\u4e86\u5728\u9884\u8bad\u7ec3\u4e2d\u66f4\u6709\u6548\u5730\u5229\u7528\u8fd9\u4e24\u79cd\u6a21\u5f0f\uff0c\u6211\u4eec\u5f15\u5165\u4e86 TriAdapter \u591a\u6a21\u5f0f\u5b66\u4e60\uff08TAMM\uff09\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u4e09\u4e2a\u534f\u540c\u9002\u914d\u5668\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684 CLIP \u56fe\u50cf\u9002\u914d\u5668\u901a\u8fc7\u8c03\u6574 CLIP \u7684\u89c6\u89c9\u8868\u793a\u6765\u5408\u6210\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u4ece\u800c\u7f29\u5c0f 3D \u6e32\u67d3\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u3002\u968f\u540e\uff0c\u6211\u4eec\u7684\u53cc\u9002\u914d\u5668\u5c06 3D \u5f62\u72b6\u8868\u793a\u7a7a\u95f4\u89e3\u8026\u4e3a\u4e24\u4e2a\u4e92\u8865\u7684\u5b50\u7a7a\u95f4\uff1a\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u89c6\u89c9\u5c5e\u6027\uff0c\u53e6\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bed\u4e49\u7406\u89e3\uff0c\u8fd9\u786e\u4fdd\u4e86\u66f4\u5168\u9762\u3001\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTAMM \u80fd\u591f\u6301\u7eed\u589e\u5f3a\u5404\u79cd 3D \u7f16\u7801\u5668\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e0b\u6e38\u4efb\u52a1\u7684 3D \u8868\u793a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5c06 Objaverse-LVIS \u4e0a\u7684\u96f6\u6837\u672c\u5206\u7c7b\u7cbe\u5ea6\u4ece 46.8 \u63d0\u9ad8\u5230 50.7\uff0c\u5e76\u5c06 ModelNet40 \u4e0a\u7684 5 \u8def 10 \u6837\u672c\u7ebf\u6027\u63a2\u6d4b\u5206\u7c7b\u7cbe\u5ea6\u4ece 96.1 \u63d0\u9ad8\u5230 99.0\u3002\u9879\u76ee\u9875\u9762\uff1a\\url{https://alanzhangcs.github.io/tamm-page}\u3002|[2402.18490v1](http://arxiv.org/pdf/2402.18490v1)|null|\n", "2402.18417": "|**2024-02-28**|**Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information**|\u4f7f\u7528 PET/CT \u653e\u5c04\u7ec4\u5b66\u548c\u4e34\u5e8a\u4fe1\u606f\u9884\u6d4b\u5934\u9888\u764c\u7684\u65e0\u590d\u53d1\u751f\u5b58\u671f|Mona Furukawa, Daniel R. McGowan, Bart\u0142omiej W. Papie\u017c|The 5-year survival rate of Head and Neck Cancer (HNC) has not improved over the past decade and one common cause of treatment failure is recurrence. In this paper, we built Cox proportional hazard (CoxPH) models that predict the recurrence free survival (RFS) of oropharyngeal HNC patients. Our models utilise both clinical information and multimodal radiomics features extracted from tumour regions in Computed Tomography (CT) and Positron Emission Tomography (PET). Furthermore, we were one of the first studies to explore the impact of segmentation accuracy on the predictive power of the extracted radiomics features, through under- and over-segmentation study. Our models were trained using the HEad and neCK TumOR (HECKTOR) challenge data, and the best performing model achieved a concordance index (C-index) of 0.74 for the model utilising clinical information and multimodal CT and PET radiomics features, which compares favourably with the model that only used clinical information (C-index of 0.67). Our under- and over-segmentation study confirms that segmentation accuracy affects radiomics extraction, however, it affects PET and CT differently.|\u5934\u9888\u764c (HNC) \u7684 5 \u5e74\u751f\u5b58\u7387\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u5e76\u672a\u63d0\u9ad8\uff0c\u6cbb\u7597\u5931\u8d25\u7684\u5e38\u89c1\u539f\u56e0\u4e4b\u4e00\u662f\u590d\u53d1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u7acb\u4e86 Cox \u6bd4\u4f8b\u98ce\u9669 (CoxPH) \u6a21\u578b\u6765\u9884\u6d4b\u53e3\u54bd\u90e8 HNC \u60a3\u8005\u7684\u65e0\u590d\u53d1\u751f\u5b58\u671f (RFS)\u3002\u6211\u4eec\u7684\u6a21\u578b\u5229\u7528\u4ece\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u548c\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf (PET) \u4e2d\u7684\u80bf\u7624\u533a\u57df\u63d0\u53d6\u7684\u4e34\u5e8a\u4fe1\u606f\u548c\u591a\u6a21\u6001\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u662f\u901a\u8fc7\u6b20\u5206\u5272\u548c\u8fc7\u5ea6\u5206\u5272\u7814\u7a76\u63a2\u7d22\u5206\u5272\u51c6\u786e\u6027\u5bf9\u63d0\u53d6\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u7684\u9884\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u7684\u9996\u6279\u7814\u7a76\u4e4b\u4e00\u3002\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u7528 HEad and neCK TumOR (HECKTOR) \u6311\u6218\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528\u4e34\u5e8a\u4fe1\u606f\u548c\u591a\u6a21\u6001 CT \u548c PET \u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u7684\u6a21\u578b\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u8fbe\u5230\u4e86 0.74 \u7684\u4e00\u81f4\u6027\u6307\u6570 (C \u6307\u6570)\uff0c\u8fd9\u4e0e\u4ec5\u4f7f\u7528\u4e34\u5e8a\u4fe1\u606f\u7684\u6a21\u578b\uff08C \u6307\u6570\u4e3a 0.67\uff09\u3002\u6211\u4eec\u7684\u6b20\u5206\u5272\u548c\u8fc7\u5ea6\u5206\u5272\u7814\u7a76\u8bc1\u5b9e\uff0c\u5206\u5272\u7cbe\u5ea6\u4f1a\u5f71\u54cd\u653e\u5c04\u7ec4\u5b66\u63d0\u53d6\uff0c\u4f46\u662f\uff0c\u5b83\u5bf9 PET \u548c CT \u7684\u5f71\u54cd\u4e0d\u540c\u3002|[2402.18417v1](http://arxiv.org/pdf/2402.18417v1)|null|\n", "2402.18319": "|**2024-02-28**|**A Multimodal Handover Failure Detection Dataset and Baselines**|\u591a\u6a21\u5f0f\u5207\u6362\u5931\u8d25\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u57fa\u7ebf|Santosh Thoduka, Nico Hochgeschwender, Juergen Gall, Paul G. Pl\u00f6ger|An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.|\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u7269\u4f53\u5207\u6362\u662f\u4e00\u79cd\u534f\u8c03\u52a8\u4f5c\uff0c\u5f88\u5bb9\u6613\u56e0\u6c9f\u901a\u4e0d\u7545\u3001\u52a8\u4f5c\u4e0d\u6b63\u786e\u548c\u610f\u5916\u7684\u7269\u4f53\u5c5e\u6027\u7b49\u539f\u56e0\u800c\u5931\u8d25\u3002\u73b0\u6709\u7684\u5207\u6362\u6545\u969c\u68c0\u6d4b\u548c\u9884\u9632\u5de5\u4f5c\u91cd\u70b9\u662f\u9632\u6b62\u7531\u4e8e\u7269\u4f53\u6ed1\u52a8\u6216\u5916\u90e8\u5e72\u6270\u800c\u5bfc\u81f4\u7684\u6545\u969c\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u8003\u8651\u4eba\u7c7b\u53c2\u4e0e\u8005\u9020\u6210\u7684\u4e0d\u53ef\u9884\u9632\u7684\u6545\u969c\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u6a21\u5f0f\u5207\u6362\u6545\u969c\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4eba\u7c7b\u53c2\u4e0e\u8005\u5f15\u8d77\u7684\u6545\u969c\uff0c\u4f8b\u5982\u5ffd\u7565\u673a\u5668\u4eba\u6216\u4e0d\u91ca\u653e\u7269\u4f53\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8e\u5207\u6362\u5931\u8d25\u68c0\u6d4b\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1a(i) \u4f7f\u7528 3D CNN \u7684\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\u548c (ii) \u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u52a8\u4f5c\u3001\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u52a8\u4f5c\u7684\u6574\u4f53\u7ed3\u679c\u8fdb\u884c\u8054\u5408\u5206\u7c7b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u9891\u662f\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u5f0f\uff0c\u4f46\u4f7f\u7528\u529b-\u626d\u77e9\u6570\u636e\u548c\u5939\u5177\u4f4d\u7f6e\u6709\u52a9\u4e8e\u63d0\u9ad8\u6545\u969c\u68c0\u6d4b\u548c\u52a8\u4f5c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002|[2402.18319v1](http://arxiv.org/pdf/2402.18319v1)|null|\n", "2402.18262": "|**2024-02-28**|**Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding**|\u7528\u4e8e\u89c6\u89c9\u4e30\u5bcc\u7f51\u9875\u7406\u89e3\u7684\u5206\u5c42\u591a\u6a21\u6001\u9884\u8bad\u7ec3|Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu, Kai Yu|The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.|\u7f51\u9875\u548c\u626b\u63cf/\u6570\u5b57\u751f\u6210\u7684\u6587\u6863\uff08\u56fe\u50cf\u3001PDF \u7b49\uff09\u7b49\u89c6\u89c9\u6548\u679c\u4e30\u5bcc\u7684\u6587\u6863\u65e5\u76ca\u6d41\u884c\uff0c\u5bfc\u81f4\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u5bf9\u81ea\u52a8\u6587\u6863\u7406\u89e3\u548c\u4fe1\u606f\u63d0\u53d6\u7684\u5174\u8da3\u65e5\u76ca\u6d53\u539a\u3002\u5c3d\u7ba1\u5404\u79cd\u6587\u6863\u6a21\u5f0f\uff08\u5305\u62ec\u56fe\u50cf\u3001\u6587\u672c\u3001\u5e03\u5c40\u548c\u7ed3\u6784\uff09\u6709\u52a9\u4e8e\u4eba\u7c7b\u4fe1\u606f\u68c0\u7d22\uff0c\u4f46\u8fd9\u4e9b\u6a21\u5f0f\u7684\u4e92\u8fde\u6027\u8d28\u7ed9\u795e\u7ecf\u7f51\u7edc\u5e26\u6765\u4e86\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 WebLM\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u6a21\u5f0f\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u65e8\u5728\u89e3\u51b3\u4ec5\u5bf9\u7f51\u9875\u4e2d\u7684 HTML \u6587\u672c\u548c\u7ed3\u6784\u6a21\u5f0f\u8fdb\u884c\u5efa\u6a21\u7684\u5c40\u9650\u6027\u3002 WebLM \u4e0d\u662f\u5c06\u6587\u6863\u56fe\u50cf\u5904\u7406\u4e3a\u7edf\u4e00\u7684\u81ea\u7136\u56fe\u50cf\uff0c\u800c\u662f\u96c6\u6210\u4e86\u6587\u6863\u56fe\u50cf\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u589e\u5f3a\u5bf9\u57fa\u4e8e\u6807\u8bb0\u8bed\u8a00\u7684\u6587\u6863\u7684\u7406\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1\u6765\u6709\u6548\u5730\u6a21\u62df\u6587\u672c\u3001\u7ed3\u6784\u548c\u56fe\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u7684 WebLM \u5728\u591a\u4e2a\u7f51\u9875\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u7740\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u4ee3\u7801\u53ef\u5728 https://github.com/X-LANCE/weblm \u4e0a\u83b7\u53d6\u3002|[2402.18262v1](http://arxiv.org/pdf/2402.18262v1)|null|\n", "2402.18091": "|**2024-02-28**|**Polos: Multimodal Metric Learning from Human Feedback for Image Captioning**|Polos\uff1a\u6839\u636e\u56fe\u50cf\u5b57\u5e55\u7684\u4eba\u7c7b\u53cd\u9988\u8fdb\u884c\u591a\u6a21\u6001\u5ea6\u91cf\u5b66\u4e60|Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura|Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.|\u5efa\u7acb\u4e0e\u4eba\u7c7b\u5224\u65ad\u7d27\u5bc6\u7ed3\u5408\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5bf9\u4e8e\u6709\u6548\u5f00\u53d1\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u6700\u8fd1\u7684\u6570\u636e\u9a71\u52a8\u6307\u6807\u8868\u660e\uff0c\u4e0e CIDEr \u7b49\u7ecf\u5178\u6307\u6807\u76f8\u6bd4\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u66f4\u5f3a\uff1b\u7136\u800c\uff0c\u5b83\u4eec\u7f3a\u4e4f\u8db3\u591f\u7684\u80fd\u529b\u6765\u5904\u7406\u5e7b\u89c9\u548c\u6cdb\u5316\u4e0d\u540c\u7684\u56fe\u50cf\u548c\u6587\u672c\uff0c\u90e8\u5206\u539f\u56e0\u662f\u5b83\u4eec\u4ec5\u4f7f\u7528\u4ece\u4e0e\u56fe\u50cf\u5b57\u5e55\u8bc4\u4f30\u65e0\u5173\u7684\u4efb\u52a1\u4e2d\u5b66\u4e60\u7684\u5d4c\u5165\u6765\u8ba1\u7b97\u6807\u91cf\u76f8\u4f3c\u6027\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Polos\uff0c\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u7684\u76d1\u7763\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002 Polos \u4f7f\u7528\u5e76\u884c\u7279\u5f81\u63d0\u53d6\u673a\u5236\u8ba1\u7b97\u591a\u6a21\u6001\u8f93\u5165\u7684\u5206\u6570\uff0c\u8be5\u673a\u5236\u5229\u7528\u901a\u8fc7\u5927\u89c4\u6a21\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u5d4c\u5165\u3002\u4e3a\u4e86\u8bad\u7ec3 Polos\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6765\u81ea\u4eba\u7c7b\u53cd\u9988\u7684\u591a\u6a21\u6001\u5ea6\u91cf\u5b66\u4e60 (M$^2$LHF)\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u5f00\u53d1\u5ea6\u91cf\u7684\u6846\u67b6\u3002\u6211\u4eec\u6784\u5efa\u4e86 Polaris \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea 550 \u540d\u8bc4\u4f30\u8005\u7684 131K \u6761\u4eba\u7c7b\u5224\u65ad\uff0c\u5927\u7ea6\u6bd4\u6807\u51c6\u6570\u636e\u96c6\u5927\u5341\u500d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 Composite\u3001Flickr8K-Expert\u3001Flickr8K-CF\u3001PASCAL-50S\u3001FOIL \u548c Polaris \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ece\u800c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002|[2402.18091v1](http://arxiv.org/pdf/2402.18091v1)|null|\n", "2402.17983": "|**2024-02-28**|**M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding**|M3-VRD\uff1a\u591a\u6a21\u6001\u591a\u4efb\u52a1\u591a\u6559\u5e08\u89c6\u89c9\u4e30\u5bcc\u5f62\u5f0f\u6587\u6863\u7406\u89e3|Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo Garza, Josiah Poon, Luca Cagliero|This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a81\u7834\u6027\u7684\u591a\u6a21\u5f0f\u3001\u591a\u4efb\u52a1\u3001\u591a\u6559\u5e08\u8054\u5408\u7c92\u5ea6\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u89c9\u4e30\u5bcc\u7684\u8868\u5355\u6587\u6863\u7406\u89e3\u3002\u8be5\u6a21\u578b\u65e8\u5728\u901a\u8fc7\u4fc3\u8fdb\u4ee4\u724c\u548c\u5b9e\u4f53\u8868\u793a\u4e4b\u95f4\u7684\u7ec6\u5fae\u5173\u8054\u6765\u5229\u7528\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u7ea7\u522b\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u89e3\u51b3\u8868\u5355\u6587\u6863\u4e2d\u56fa\u6709\u7684\u590d\u6742\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u7c92\u95f4\u548c\u8de8\u7c92\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u591a\u6837\u5316\u7684\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u5448\u73b0\u5206\u5e03\u5dee\u8ddd\u548c\u5bf9\u8868\u5355\u6587\u6863\u7684\u7edf\u4e00\u7406\u89e3\u3002\u901a\u8fc7\u5bf9\u516c\u5f00\u7684\u8868\u5355\u6587\u6863\u7406\u89e3\u6570\u636e\u96c6\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u89c6\u89c9\u4e0a\u590d\u6742\u7684\u8868\u5355\u6587\u6863\u7684\u590d\u6742\u7ed3\u6784\u548c\u5185\u5bb9\u65b9\u9762\u7684\u529f\u6548\u3002|[2402.17983v1](http://arxiv.org/pdf/2402.17983v1)|null|\n", "2402.17971": "|**2024-02-28**|**All in a Single Image: Large Multimodal Models are In-Image Learners**|\u4e00\u5207\u90fd\u5728\u4e00\u4e2a\u56fe\u50cf\u4e2d\uff1a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u662f\u56fe\u50cf\u5185\u5b66\u4e60\u5668|Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy Ka-Wei Lee, Ee-Peng Lim|This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVista and Hallusionbench to test the effectiveness of I$^2$L in complex multimodal reasoning tasks and mitigating language hallucination and visual illusion. Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of I$^2$L. Our code is publicly available at https://github.com/AGI-Edgerunners/IIL.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL) \u673a\u5236\uff0c\u79f0\u4e3a\u56fe\u50cf\u5185\u5b66\u4e60 (I$^2$L)\uff0c\u5b83\u5c06\u6f14\u793a\u793a\u4f8b\u3001\u89c6\u89c9\u63d0\u793a\u548c\u6307\u4ee4\u7ec4\u5408\u5230\u5355\u4e2a\u56fe\u50cf\u4e2d\uff0c\u4ee5\u589e\u5f3a GPT-4V \u7684\u529f\u80fd\u3002\u4e0e\u4e4b\u524d\u4f9d\u8d56\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u6587\u672c\u6216\u5c06\u89c6\u89c9\u8f93\u5165\u5408\u5e76\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cI$^2$L \u5c06\u6240\u6709\u4fe1\u606f\u6574\u5408\u5230\u4e00\u5f20\u56fe\u50cf\u4e2d\uff0c\u5e76\u4e3b\u8981\u5229\u7528\u56fe\u50cf\u5904\u7406\u3001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u6709\u51e0\u4e2a\u4f18\u70b9\uff1a\u5b83\u907f\u514d\u4e86\u5bf9\u590d\u6742\u56fe\u50cf\u7684\u4e0d\u51c6\u786e\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u4f9b\u4e86\u5b9a\u4f4d\u6f14\u793a\u793a\u4f8b\u7684\u7075\u6d3b\u6027\uff0c\u51cf\u5c11\u4e86\u8f93\u5165\u8d1f\u62c5\uff0c\u5e76\u901a\u8fc7\u6d88\u9664\u5bf9\u591a\u4e2a\u56fe\u50cf\u548c\u5197\u957f\u6587\u672c\u7684\u9700\u8981\u6765\u907f\u514d\u8d85\u51fa\u8f93\u5165\u9650\u5236\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e0d\u540c ICL \u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u52a8\u7b56\u7565\uff0c\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u793a\u4f8b\u9009\u62e9\u9002\u5f53\u7684 ICL \u65b9\u6cd5\u3002\u6211\u4eec\u5728 MathVista \u548c Hallusionbench \u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6d4b\u8bd5 I$^2$L \u5728\u590d\u6742\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4ee5\u53ca\u51cf\u8f7b\u8bed\u8a00\u5e7b\u89c9\u548c\u89c6\u9519\u89c9\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u6f14\u793a\u793a\u4f8b\u6570\u91cf\u53ca\u5176\u4f4d\u7f6e\u5bf9 I$^2$L \u6709\u6548\u6027\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/AGI-Edgerunners/IIL \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2402.17971v1](http://arxiv.org/pdf/2402.17971v1)|null|\n"}, "Nerf": {"2402.18196": "|**2024-02-28**|**NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images**|NToP\uff1aNeRF \u652f\u6301\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u751f\u6210\uff0c\u7528\u4e8e\u9876\u89c6\u56fe\u9c7c\u773c\u56fe\u50cf\u4e2d\u7684 2D \u548c 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1|Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz|Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain. However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective. Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after finetuning on our training set. A similarly finetuned HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.|\u4f7f\u7528\u9c7c\u773c\u76f8\u673a\u8fdb\u884c\u9876\u89c6\u56fe\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\u5448\u73b0\u51fa\u4e00\u4e2a\u6709\u524d\u9014\u4e14\u521b\u65b0\u7684\u5e94\u7528\u9886\u57df\u3002\u7136\u800c\uff0c\u6355\u6349\u8fd9\u4e00\u89c2\u70b9\u7684\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u6781\u5176\u6709\u9650\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u5177\u6709\u9ad8\u8d28\u91cf 2D \u548c 3D \u5173\u952e\u70b9\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u6280\u672f\u7684\u529f\u80fd\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u73b0\u6709 2D \u548c 3D \u6570\u636e\u96c6\u751f\u6210\u4eba\u4f53\u59ff\u52bf\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u9876\u89c6\u56fe\u9c7c\u773c\u89c6\u89d2\u5b9a\u5236\u3002\u901a\u8fc7\u8fd9\u4e2a\u6d41\u7a0b\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u96c6 NToP570K\uff08\u7531 NeRF \u9a71\u52a8\u7684\u9c7c\u773c\u76f8\u673a\u4fef\u89c6\u4eba\u4f53\u59ff\u52bf\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7 57 \u4e07\u5f20\u56fe\u50cf\uff09\uff0c\u5e76\u5bf9\u5176\u5728\u589e\u5f3a 2D \u548c 3D \u4fef\u89c6\u4eba\u4f53\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u7684\u529f\u6548\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u59ff\u6001\u4f30\u8ba1\u3002\u5728\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u9884\u8bad\u7ec3\u7684 ViTPose-B \u6a21\u578b\u5728 2D HPE \u9a8c\u8bc1\u96c6\u4e0a\u7684 AP \u63d0\u9ad8\u4e86 33.3%\u3002\u7ecf\u8fc7\u7c7b\u4f3c\u5fae\u8c03\u7684 HybrIK-Transformer \u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7684 3D HPE \u7684 PA-MPJPE \u51cf\u5c11\u4e86 53.7 \u6beb\u7c73\u3002|[2402.18196v1](http://arxiv.org/pdf/2402.18196v1)|null|\n"}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.18528": "|**2024-02-28**|**Gradient Reweighting: Towards Imbalanced Class-Incremental Learning**|\u68af\u5ea6\u91cd\u65b0\u52a0\u6743\uff1a\u8d70\u5411\u4e0d\u5e73\u8861\u7684\u73ed\u7ea7\u589e\u91cf\u5b66\u4e60|Jiangpeng He, Fengqing Zhu|Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.|\u7c7b\u522b\u589e\u91cf\u5b66\u4e60 (CIL) \u8bad\u7ec3\u6a21\u578b\u4e0d\u65ad\u4ece\u975e\u5e73\u7a33\u6570\u636e\u4e2d\u8bc6\u522b\u65b0\u7c7b\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u5b66\u5230\u7684\u77e5\u8bc6\u3002\u5f53\u5e94\u7528\u4e8e\u4ee5\u975e\u5747\u5300\u5206\u5e03\u4e3a\u7279\u5f81\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u65f6\uff0cCIL \u51fa\u73b0\u4e86\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5b83\u5f15\u5165\u4e86\u53cc\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6d89\u53ca\uff08i\uff09\u65e7\u4efb\u52a1\u7684\u5b58\u50a8\u6837\u672c\u548c\u65b0\u7c7b\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u5f02\uff08\u76f8\u95f4\u4e0d\u5e73\u8861\uff09\uff0c\u4ee5\u53ca\uff08 ii\uff09\u6bcf\u4e2a\u5355\u72ec\u4efb\u52a1\u4e2d\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\uff08\u9636\u6bb5\u5185\u4e0d\u5e73\u8861\uff09\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u79cd\u53cc\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\u4f1a\u5bfc\u81f4 FC \u5c42\u4e2d\u7684\u6743\u91cd\u51fa\u73b0\u504f\u5dee\uff0c\u5bfc\u81f4\u68af\u5ea6\u66f4\u65b0\u503e\u659c\uff0c\u4ece\u800c\u5bfc\u81f4 CI\u200b\u200bL \u4e2d\u7684\u8fc7\u5ea6/\u6b20\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u5e73\u8861\u4f18\u5316\u548c\u65e0\u504f\u5206\u7c7b\u5668\u5b66\u4e60\u7684\u68af\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e0d\u5e73\u8861\u7684\u9057\u5fd8\uff0c\u77db\u76fe\u7684\u662f\uff0c\u7531\u4e8e\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u5728\u540e\u7eed\u5b66\u4e60\u9636\u6bb5\u53d8\u5f97\u4e0d\u53ef\u7528\uff0c\u5b9e\u4f8b\u4e30\u5bcc\u7684\u7c7b\u5728 CIL \u671f\u95f4\u906d\u53d7\u66f4\u9ad8\u7684\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5e03\u611f\u77e5\u7684\u77e5\u8bc6\u84b8\u998f\u635f\u5931\uff0c\u901a\u8fc7\u5c06\u8f93\u51fa\u903b\u8f91\u4e0e\u4e22\u5931\u7684\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u6309\u6bd4\u4f8b\u5bf9\u9f50\u6765\u51cf\u5c11\u9057\u5fd8\u3002\u6211\u4eec\u5728 CIFAR-100\u3001ImageNetSubset \u548c Food101 \u4e0a\u8de8\u5404\u79cd\u8bc4\u4f30\u534f\u8bae\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u5de5\u4f5c\u76f8\u6bd4\u7684\u4e00\u81f4\u6539\u8fdb\uff0c\u663e\u793a\u51fa\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e94\u7528 CIL \u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002|[2402.18528v1](http://arxiv.org/pdf/2402.18528v1)|null|\n", "2402.18493": "|**2024-02-28**|**Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection**|\u9633\u5149\u660e\u5a9a\u5230\u66b4\u96e8\uff1a\u8de8\u5929\u6c14\u77e5\u8bc6\u84b8\u998f\uff0c\u5b9e\u73b0\u7a33\u5065\u7684 3D \u7269\u4f53\u68c0\u6d4b|Xun Huang, Hai Wu, Xin Li, Xiaoliang Fan, Chenglu Wen, Cheng Wang|LiDAR-based 3D object detection models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny|\u7531\u4e8e\u626b\u63cf\u4fe1\u53f7\u8d28\u91cf\u4e0b\u964d\u4e14\u566a\u58f0\u8f83\u5927\uff0c\u57fa\u4e8e LiDAR \u7684 3D \u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u4f20\u7edf\u4e0a\u5728\u96e8\u5929\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u5148\u524d\u7684\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u6a21\u62df\u96e8\u566a\u58f0\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u6a21\u62df\u548c\u5b9e\u9645\u53d7\u964d\u96e8\u5f71\u54cd\u7684\u6570\u636e\u70b9\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u964d\u96e8\u6a21\u62df\u65b9\u6cd5\uff0c\u79f0\u4e3a DRET\uff0c\u5b83\u7ed3\u5408\u4e86\u52a8\u529b\u5b66\u548c\u591a\u96e8\u73af\u5883\u7406\u8bba\uff0c\u4e3a\u6269\u5c55 3D \u68c0\u6d4b\u8bad\u7ec3\u7684\u53ef\u7528\u771f\u5b9e\u964d\u96e8\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6674\u5929\u5230\u96e8\u5929\u77e5\u8bc6\u84b8\u998f\uff08SRKD\uff09\u65b9\u6cd5\u6765\u589e\u5f3a\u96e8\u5929\u6761\u4ef6\u4e0b\u7684 3D \u68c0\u6d4b\u3002\u5728 WaymoOpenDataset \u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4e0e\u6700\u5148\u8fdb\u7684 DSVT \u6a21\u578b\u548c\u5176\u4ed6\u7ecf\u5178 3D \u68c0\u6d4b\u5668\u76f8\u7ed3\u5408\u65f6\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e0d\u635f\u5931\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u7684\u663e\u7740\u63d0\u9ad8\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6846\u67b6\u8fd8\u63d0\u9ad8\u4e86\u9633\u5149\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u56e0\u6b64\u65e0\u8bba\u5929\u6c14\u662f\u96e8\u5929\u8fd8\u662f\u6674\u5929\uff0c\u90fd\u53ef\u4ee5\u4e3a 3D \u68c0\u6d4b\u63d0\u4f9b\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848|[2402.18493v1](http://arxiv.org/pdf/2402.18493v1)|null|\n", "2402.18213": "|**2024-02-28**|**Multi-objective Differentiable Neural Architecture Search**|\u591a\u76ee\u6807\u53ef\u5fae\u795e\u7ecf\u67b6\u6784\u641c\u7d22|Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter|Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.|\u591a\u76ee\u6807\u4f18\u5316 (MOO) \u4e2d\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\uff08\u5373\u627e\u5230\u4e00\u7ec4\u4e0d\u540c\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff09\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7b49\u6602\u8d35\u7684\u76ee\u6807\u3002\u901a\u5e38\uff0c\u5728 MOO \u795e\u7ecf\u67b6\u6784\u641c\u7d22 (NAS) \u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8de8\u8bbe\u5907\u5e73\u8861\u6027\u80fd\u548c\u786c\u4ef6\u6307\u6807\u3002\u5148\u524d\u7684 NAS \u65b9\u6cd5\u901a\u8fc7\u5c06\u786c\u4ef6\u7ea6\u675f\u5408\u5e76\u5230\u76ee\u6807\u51fd\u6570\u4e2d\u6765\u7b80\u5316\u6b64\u4efb\u52a1\uff0c\u4f46\u5206\u6790 Pareto \u524d\u6cbf\u9700\u8981\u641c\u7d22\u6bcf\u4e2a\u7ea6\u675f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 NAS \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5bf9\u7528\u6237\u504f\u597d\u8fdb\u884c\u7f16\u7801\uff0c\u4ee5\u5b9e\u73b0\u6027\u80fd\u548c\u786c\u4ef6\u6307\u6807\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5728\u4e00\u6b21\u641c\u7d22\u8fd0\u884c\u4e2d\u4ea7\u751f\u8de8\u591a\u4e2a\u8bbe\u5907\u7684\u4ee3\u8868\u6027\u4e14\u591a\u6837\u5316\u7684\u67b6\u6784\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u8d85\u7f51\u7edc\u53c2\u6570\u5316\u8de8\u8bbe\u5907\u548c\u591a\u4e2a\u76ee\u6807\u7684\u8054\u5408\u67b6\u6784\u5206\u5e03\uff0c\u8be5\u8d85\u7f51\u7edc\u53ef\u4ee5\u6839\u636e\u786c\u4ef6\u529f\u80fd\u548c\u504f\u597d\u5411\u91cf\u8fdb\u884c\u8c03\u8282\uff0c\u4ece\u800c\u5b9e\u73b0\u5411\u65b0\u8bbe\u5907\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002\u4f7f\u7528\u591a\u8fbe 19 \u4e2a\u786c\u4ef6\u8bbe\u5907\u548c 3 \u4e2a\u7269\u955c\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5c55\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5728\u6ca1\u6709\u989d\u5916\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u8d28\u91cf\u7684\u641c\u7d22\u7a7a\u95f4\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684 MOO NAS \u65b9\u6cd5\uff0c\u5305\u62ec ImageNet-1k \u4e0a\u7684 MobileNetV3 \u548c\u673a\u5668\u7ffb\u8bd1\u4e0a\u7684 Transformer \u7a7a\u95f4\u3002|[2402.18213v1](http://arxiv.org/pdf/2402.18213v1)|null|\n", "2402.18181": "|**2024-02-28**|**CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation**|CFDNet\uff1a\u5177\u6709\u5bf9\u6bd4\u7279\u5f81\u84b8\u998f\u7684\u53ef\u63a8\u5e7f\u96fe\u7acb\u4f53\u5339\u914d\u7f51\u7edc|Zihua Liu, Yizhou Li, Masatoshi Okutomi|Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.|\u96fe\u5929\u573a\u666f\u4e0b\u7684\u7acb\u4f53\u5339\u914d\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u6563\u5c04\u6548\u5e94\u4f1a\u964d\u4f4e\u53ef\u89c1\u5ea6\u5e76\u5bfc\u81f4\u5bc6\u96c6\u5bf9\u5e94\u5339\u914d\u7684\u7279\u5f81\u4e0d\u592a\u660e\u663e\u3002\u867d\u7136\u4ee5\u524d\u7684\u4e00\u4e9b\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u96c6\u6210\u4e86\u7269\u7406\u6563\u5c04\u51fd\u6570\u4ee5\u540c\u65f6\u8fdb\u884c\u7acb\u4f53\u5339\u914d\u548c\u53bb\u96fe\uff0c\u4f46\u7b80\u5355\u5730\u53bb\u9664\u96fe\u53ef\u80fd\u65e0\u52a9\u4e8e\u6df1\u5ea6\u4f30\u8ba1\uff0c\u56e0\u4e3a\u96fe\u672c\u8eab\u53ef\u4ee5\u63d0\u4f9b\u5173\u952e\u7684\u6df1\u5ea6\u7ebf\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u6bd4\u7279\u5f81\u84b8\u998f\uff08CFD\uff09\u7684\u6846\u67b6\u3002\u8be5\u7b56\u7565\u5c06\u5408\u5e76\u7684\u5e72\u51c0\u96fe\u7279\u5f81\u7684\u7279\u5f81\u84b8\u998f\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u786e\u4fdd\u5bf9\u96fe\u6df1\u5ea6\u63d0\u793a\u548c\u5e72\u51c0\u5339\u914d\u7279\u5f81\u7684\u5e73\u8861\u4f9d\u8d56\u3002\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u589e\u5f3a\u6a21\u578b\u5728\u5e72\u51c0\u548c\u6709\u96fe\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5353\u8d8a\u5f3a\u5ea6\u548c\u9002\u5e94\u6027\u3002|[2402.18181v1](http://arxiv.org/pdf/2402.18181v1)|null|\n", "2402.18163": "|**2024-02-28**|**Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision**|Ef-QuantFace\uff1a\u5177\u6709\u5c0f\u6570\u636e\u548c\u4f4e\u4f4d\u7cbe\u5ea6\u7684\u7b80\u5316\u4eba\u8138\u8bc6\u522b|William Gazali, Jocelyn Michelle Kho, Joshua Santoso, Williem|In recent years, model quantization for face recognition has gained prominence. Traditionally, compressing models involved vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential. This paper addresses this by introducing an efficiency-driven approach, fine-tuning the model with just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective quantization is achievable with a smaller dataset, presenting a new paradigm. Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed model training for face recognition. The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach. This paper advances model quantization by highlighting the efficiency and optimal results with small data and training time.|\u8fd1\u5e74\u6765\uff0c\u4eba\u8138\u8bc6\u522b\u7684\u6a21\u578b\u91cf\u5316\u5f97\u5230\u4e86\u91cd\u89c6\u3002\u4f20\u7edf\u4e0a\uff0c\u538b\u7f29\u6a21\u578b\u6d89\u53ca\u5e9e\u5927\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982 580 \u4e07\u5f20\u56fe\u50cf\u7684 MS1M \u6570\u636e\u96c6\uff09\u4ee5\u53ca\u5927\u91cf\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u8fd9\u5c31\u63d0\u51fa\u4e86\u8fd9\u6837\u7684\u6570\u636e\u91cf\u662f\u5426\u5fc5\u8981\u7684\u95ee\u9898\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u6548\u7387\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ec5\u7528\u6700\u591a 14,000 \u5f20\u56fe\u50cf\uff08\u6bd4 MS1M \u5c0f 440 \u500d\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u6211\u4eec\u8bc1\u660e\u53ef\u4ee5\u4f7f\u7528\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u5b9e\u73b0\u6709\u6548\u7684\u91cf\u5316\uff0c\u4ece\u800c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u57fa\u4e8e\u8bc4\u4f30\u7684\u5ea6\u91cf\u635f\u5931\uff0c\u5e76\u5728 IJB-C \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u51fa\u8272\u7684 96.15% \u51c6\u786e\u7387\uff0c\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u5148\u8fdb\u7684\u4eba\u8138\u8bc6\u522b\u538b\u7f29\u6a21\u578b\u8bad\u7ec3\u3002\u968f\u540e\u7684\u5206\u6790\u6df1\u5165\u7814\u7a76\u4e86\u6f5c\u5728\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u53d8\u9769\u529b\u91cf\u3002\u672c\u6587\u901a\u8fc7\u5f3a\u8c03\u5c0f\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u6548\u7387\u548c\u6700\u4f73\u7ed3\u679c\u6765\u63a8\u8fdb\u6a21\u578b\u91cf\u5316\u3002|[2402.18163v1](http://arxiv.org/pdf/2402.18163v1)|null|\n", "2402.18147": "|**2024-02-28**|**A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction**|\u901a\u8fc7\u901a\u9053\u5148\u9a8c\u548c\u4f3d\u739b\u6821\u6b63\u7684\u8f7b\u91cf\u7ea7\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7f51\u7edc|Shyang-En Weng, Shaou-Gang Miaou, Ricky Christanto|Human vision relies heavily on available ambient light to perceive objects. Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts. Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario. We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory. This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction. The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria. Furthermore, we utilized knowledge distillation with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time. The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios.|\u4eba\u7c7b\u89c6\u89c9\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u53ef\u7528\u7684\u73af\u5883\u5149\u6765\u611f\u77e5\u7269\u4f53\u3002\u4f4e\u5149\u573a\u666f\u5e26\u6765\u4e24\u4e2a\u4e0d\u540c\u7684\u6311\u6218\uff1a\u7531\u4e8e\u7167\u660e\u4e0d\u8db3\u800c\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\u548c\u4e0d\u826f\u7684\u4eae\u5ea6\u53d8\u5316\u3002\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u662f\u6307\u4e3a\u5904\u7406\u8fd9\u79cd\u60c5\u51b5\u800c\u5b9a\u5236\u7684\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86 CPGA-Net\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684 LLIE \u7f51\u7edc\uff0c\u5b83\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5c06\u6697/\u4eae\u901a\u9053\u5148\u9a8c\u548c\u4f3d\u739b\u6821\u6b63\u7ed3\u5408\u8d77\u6765\uff0c\u5e76\u96c6\u6210\u4e86\u53d7\u5927\u6c14\u6563\u5c04\u6a21\u578b\u548c Retinex \u7406\u8bba\u542f\u53d1\u7684\u529f\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u4f7f\u7528\uff0c\u5728\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u67b6\u6784\u6846\u67b6\u5185\u8bbe\u8ba1\uff0c\u4e13\u6ce8\u4e8e\u57fa\u672c\u7279\u5f81\u63d0\u53d6\u3002\u7531\u6b64\u4ea7\u751f\u7684 CPGA-Net \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u53ea\u6709 0.25 \u4e07\u4e2a\u53c2\u6570\u548c 0.030 \u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u4f46\u5b83\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u6807\u51c6\u4e0a\u90fd\u6bd4\u73b0\u6709\u7684 LLIE \u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5177\u6709\u53ef\u89e3\u91ca\u56e0\u7d20\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7248\u672c\uff0c\u53ef\u5b9e\u73b0 0.18 \u4e07\u4e2a\u53c2\u6570\u548c 0.006 \u79d2\u7684\u63a8\u7406\u65f6\u95f4\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a LLIE \u6ce8\u5165\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u7406\u5ff5\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u5149\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u3002|[2402.18147v1](http://arxiv.org/pdf/2402.18147v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.18573": "|**2024-02-28**|**UniMODE: Unified Monocular 3D Object Detection**|UniMODE\uff1a\u7edf\u4e00\u5355\u76ee 3D \u7269\u4f53\u68c0\u6d4b|Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao|Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection.|\u5b9e\u73b0\u7edf\u4e00\u7684\u5355\u76ee 3D \u7269\u4f53\u68c0\u6d4b\uff08\u5305\u62ec\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\uff09\u5728\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u5e94\u7528\u4e2d\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u6d89\u53ca\u5404\u79cd\u6570\u636e\u573a\u666f\u6765\u8bad\u7ec3\u6a21\u578b\u4f1a\u5e26\u6765\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u7279\u5f81\u663e\u7740\u4e0d\u540c\uff0c\u4f8b\u5982\u4e0d\u540c\u7684\u51e0\u4f55\u5c5e\u6027\u548c\u5f02\u6784\u57df\u5206\u5e03\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u68c0\u6d4b\u8303\u4f8b\u7684\u68c0\u6d4b\u5668\uff0c\u5176\u4e2d\u663e\u5f0f\u7279\u5f81\u6295\u5f71\u6709\u5229\u4e8e\u89e3\u51b3\u4f7f\u7528\u591a\u79cd\u6570\u636e\u573a\u666f\u6765\u8bad\u7ec3\u68c0\u6d4b\u5668\u65f6\u7684\u51e0\u4f55\u5b66\u4e60\u6a21\u7cca\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u7ecf\u5178\u7684 BEV \u68c0\u6d4b\u67b6\u6784\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4e0d\u5747\u5300\u7684 BEV \u7f51\u683c\u8bbe\u8ba1\u6765\u5904\u7406\u4e0a\u8ff0\u6311\u6218\u5f15\u8d77\u7684\u6536\u655b\u4e0d\u7a33\u5b9a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7a00\u758f BEV \u7279\u5f81\u6295\u5f71\u7b56\u7565\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u57df\u5bf9\u9f50\u65b9\u6cd5\u6765\u5904\u7406\u5f02\u6784\u57df\u3002\u7ed3\u5408\u8fd9\u4e9b\u6280\u672f\uff0c\u5f97\u51fa\u4e86\u7edf\u4e00\u7684\u68c0\u6d4b\u5668 UniMODE\uff0c\u5b83\u5728\u5177\u6709\u6311\u6218\u6027\u7684 Omni3D \u6570\u636e\u96c6\uff08\u5305\u62ec\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff09\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f 4.9% AP_3D\uff0c\u63ed\u793a\u4e86\u9996\u6b21\u6210\u529f\u7684\u6cdb\u5316BEV \u63a2\u6d4b\u5668\u7edf\u4e00 3D \u7269\u4f53\u68c0\u6d4b\u3002|[2402.18573v1](http://arxiv.org/pdf/2402.18573v1)|null|\n", "2402.18527": "|**2024-02-28**|**Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures**|\u8f6e\u80ce X \u5c04\u7ebf\u56fe\u50cf\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\uff1a\u4f20\u7edf\u65b9\u6cd5\u4e0e\u6df1\u5c42\u7ed3\u6784\u7684\u7ed3\u5408|Andrei Cozma, Landon Harris, Hairong Qi, Ping Ji, Wenpeng Guo, Song Yuan|This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff08\u4f8b\u5982\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\uff08LBP\uff09\u548c\u7070\u5ea6\u5171\u751f\u77e9\u9635\uff08GLCM\uff09\u7279\u5f81\u4ee5\u53ca\u57fa\u4e8e\u5085\u7acb\u53f6\u548c\u5c0f\u6ce2\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff09\u5728\u8f6e\u80ce X \u5c04\u7ebf\u56fe\u50cf\u4e2d\u81ea\u52a8\u68c0\u6d4b\u7f3a\u9677\u7684\u9c81\u68d2\u65b9\u6cd5\u3002\u529f\u80fd\uff0c\u5e76\u8f85\u4ee5\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002\u8ba4\u8bc6\u5230\u8f6e\u80ce X \u5c04\u7ebf\u56fe\u50cf\u7684\u590d\u6742\u56fe\u6848\u548c\u7eb9\u7406\u6240\u56fa\u6709\u7684\u6311\u6218\uff0c\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7279\u5f81\u5de5\u7a0b\u5bf9\u4e8e\u589e\u5f3a\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u7279\u5f81\u7684\u7ec4\u5408\u4e0e\u968f\u673a\u68ee\u6797 (RF) \u5206\u7c7b\u5668\u7cbe\u5fc3\u96c6\u6210\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e YOLOv8 \u7b49\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u8be5\u7814\u7a76\u4e0d\u4ec5\u5bf9\u4f20\u7edf\u7279\u5f81\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u4e14\u8fd8\u63a2\u7d22\u4e86\u7ecf\u5178\u65b9\u6cd5\u548c\u73b0\u4ee3\u65b9\u6cd5\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u4f20\u7edf\u7279\u5f81\u7ecf\u8fc7\u5fae\u8c03\u5e76\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u8f6e\u80ce\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u65e8\u5728\u4e3a\u8f6e\u80ce\u5236\u9020\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\u6811\u7acb\u65b0\u6807\u51c6\u3002|[2402.18527v1](http://arxiv.org/pdf/2402.18527v1)|null|\n", "2402.18503": "|**2024-02-28**|**Detection of Micromobility Vehicles in Urban Traffic Videos**|\u57ce\u5e02\u4ea4\u901a\u89c6\u9891\u4e2d\u5fae\u578b\u8f66\u8f86\u7684\u68c0\u6d4b|Khalil Sabri, C\u00e9lia Djilali, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Wassim Bouachir|Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects. Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.|\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u7ed9\u7269\u4f53\u68c0\u6d4b\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u968f\u7740\u7535\u52a8\u6ed1\u677f\u8f66\u548c\u81ea\u884c\u8f66\u7b49\u5fae\u578b\u79fb\u52a8\u8f66\u8f86\u7684\u4e0d\u65ad\u589e\u52a0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5bf9\u8c61\u68c0\u6d4b\u95ee\u9898\uff0c\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u68c0\u6d4b\u6a21\u578b\uff0c\u5b83\u5c06\u5355\u5e27\u5bf9\u8c61\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0e\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u63d0\u4f9b\u7684\u66f4\u4e30\u5bcc\u7684\u529f\u80fd\u7ed3\u5408\u8d77\u6765\u3002\u8fd9\u662f\u901a\u8fc7\u5c06\u901a\u8fc7\u8fd0\u52a8\u6d41\u5904\u7406\u7684\u8fde\u7eed\u5e27\u7684\u805a\u5408\u7279\u5f81\u56fe\u5e94\u7528\u5230 YOLOX \u67b6\u6784\u6765\u5b8c\u6210\u7684\u3002\u8fd9\u79cd\u878d\u5408\u4e3a YOLOX \u68c0\u6d4b\u80fd\u529b\u5e26\u6765\u4e86\u65f6\u95f4\u89c6\u89d2\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u57ce\u5e02\u4ea4\u901a\u6a21\u5f0f\u5e76\u5927\u5e45\u63d0\u9ad8\u68c0\u6d4b\u53ef\u9760\u6027\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u9488\u5bf9\u57ce\u5e02\u5fae\u4ea4\u901a\u573a\u666f\u7684\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5bf9\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9e\u8d28\u6027\u6539\u8fdb\uff0c\u8868\u660e\u9700\u8981\u8003\u8651\u65f6\u7a7a\u4fe1\u606f\u6765\u68c0\u6d4b\u5982\u6b64\u5c0f\u800c\u8584\u7684\u7269\u4f53\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\uff0c\u5305\u62ec\u906e\u6321\u3001\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u5e76\u6709\u6548\u51cf\u8f7b\u8fd0\u52a8\u6a21\u7cca\u3002|[2402.18503v1](http://arxiv.org/pdf/2402.18503v1)|null|\n", "2402.18467": "|**2024-02-28**|**Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation**|\u5206\u79bb\u4e0e\u5f81\u670d\uff1a\u901a\u8fc7\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u5206\u89e3\u548c\u8868\u793a\u6765\u89e3\u8026\u5171\u73b0|Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song|Attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in weakly supervised semantic segmentation (WSSS). In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted to guarantee the correctness of knowledge and further facilitate the discrepancy among co-occurring objects. We streamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence without external supervision. Extensive experiments are conducted, validating the efficiency of our method tackling co-occurrence and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code will be available.|\u7531\u4e8e\u5171\u73b0\u5bf9\u8c61\u7684\u9891\u7e41\u8026\u5408\u548c\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u6709\u9650\u76d1\u7763\uff0c\u5177\u6709\u6311\u6218\u6027\u7684\u5171\u73b0\u95ee\u9898\u5e7f\u6cdb\u5b58\u5728\uff0c\u5e76\u5bfc\u81f4\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08WSSS\uff09\u4e2d\u5bf9\u8c61\u7684\u9519\u8bef\u6fc0\u6d3b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u201c\u5206\u79bb\u5e76\u5f81\u670d\u201d\u65b9\u6848 SeCo\uff0c\u4ece\u56fe\u50cf\u7a7a\u95f4\u548c\u7279\u5f81\u7a7a\u95f4\u7684\u7ef4\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5c06\u56fe\u50cf\u7ec6\u5206\u4e3a\u5757\u6765\u901a\u8fc7\u56fe\u50cf\u5206\u89e3\u6765\u201c\u5206\u79bb\u201d\u540c\u65f6\u51fa\u73b0\u7684\u5bf9\u8c61\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u4e3a\u6bcf\u4e2a\u8865\u4e01\u5206\u914d\u4e00\u4e2a\u6765\u81ea\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u8fd9\u5728\u7a7a\u95f4\u4e0a\u6709\u52a9\u4e8e\u6d88\u9664\u5171\u4e0a\u4e0b\u6587\u504f\u5dee\u5e76\u6307\u5bfc\u540e\u7eed\u8868\u793a\u3002\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5229\u7528\u591a\u7c92\u5ea6\u77e5\u8bc6\u5bf9\u6bd4\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6765\u201c\u514b\u670d\u201d\u9519\u8bef\u6fc0\u6d3b\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u53cc\u5e08\u5355\u5b66\u751f\u67b6\u6784\uff0c\u5e76\u8fdb\u884c\u6807\u7b7e\u5f15\u5bfc\u5bf9\u6bd4\uff0c\u4ee5\u4fdd\u8bc1\u77e5\u8bc6\u7684\u6b63\u786e\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u4fc3\u8fdb\u5171\u73b0\u5bf9\u8c61\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6211\u4eec\u7aef\u5230\u7aef\u5730\u7b80\u5316\u4e86\u591a\u9636\u6bb5 WSSS \u7ba1\u9053\uff0c\u5e76\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u4e86\u5171\u73b0\u95ee\u9898\u3002\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5904\u7406\u5171\u73b0\u7684\u6548\u7387\u4ee5\u53ca\u5728 PASCAL VOC \u548c MS COCO \u4e0a\u76f8\u5bf9\u4e8e\u4e4b\u524d\u7684\u5355\u9636\u6bb5\u751a\u81f3\u591a\u9636\u6bb5\u7ade\u4e89\u5bf9\u624b\u7684\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u5c06\u53ef\u7528\u3002|[2402.18467v1](http://arxiv.org/pdf/2402.18467v1)|null|\n", "2402.18447": "|**2024-02-28**|**Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization**|\u7528\u4e8e\u5355\u57df\u6cdb\u5316\u7684\u5feb\u901f\u9a71\u52a8\u7684\u52a8\u6001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60|Deng Li, Aming Wu, Yaowei Wang, Yahong Han|Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.|\u5355\u57df\u6cdb\u5316\u65e8\u5728\u4ece\u5355\u4e2a\u6e90\u57df\u6570\u636e\u4e2d\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u5728\u5176\u4ed6\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u4e0a\u5b9e\u73b0\u6cdb\u5316\u6027\u80fd\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u63d0\u9ad8\u9759\u6001\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u9759\u6001\u7f51\u7edc\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u56fe\u50cf\u573a\u666f\u7684\u591a\u6837\u5316\u53d8\u5316\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4e0d\u540c\u7684\u573a\u666f\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u5728\u8de8\u57df\u573a\u666f\u4e0b\u56fe\u50cf\u7684\u590d\u6742\u5ea6\u4e5f\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5373\u65f6\u5b66\u4e60\u7684\u52a8\u6001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u611f\u77e5\u7f51\u7edc\uff0c\u65e8\u5728\u9002\u5e94\u56fe\u50cf\u590d\u6742\u6027\u7684\u53d8\u5316\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u95e8\u63a7\u6a21\u5757\uff0c\u4ee5\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u5404\u79cd\u573a\u666f\u63d0\u793a\u5f15\u5bfc\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u4e0a\u3002\u7136\u540e\uff0c\u5229\u7528\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u95e8\u63a7\u63a9\u6a21\uff0c\u52a8\u6001\u9009\u62e9\u6a21\u5757\u5728\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u4e0a\u52a8\u6001\u9009\u62e9\u9ad8\u5ea6\u76f8\u5173\u7684\u7279\u5f81\u533a\u57df\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u611f\u77e5\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u76f8\u5173\u7279\u5f81\uff0c\u4ece\u800c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5355\u57df\u6cdb\u5316\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8fd9\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u603b\u4f53\u6027\u3002|[2402.18447v1](http://arxiv.org/pdf/2402.18447v1)|null|\n", "2402.18402": "|**2024-02-28**|**A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation**|\u901a\u8fc7\u6df1\u5ea6\u53c2\u6570\u4f30\u8ba1\u589e\u5f3a\u591a\u5a92\u4f53\u7406\u89e3\u7f51\u7edc\u9c81\u68d2\u6027\u7684\u6a21\u5757\u5316\u7cfb\u7edf|Francesco Barbato, Umberto Michieli, Mehmet Karim Yucel, Pietro Zanuttigh, Mete Ozay|In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.|\u5728\u591a\u5a92\u4f53\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u635f\u574f\u7684\u6837\u672c\u6784\u6210\u4e86\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5f53\u8f93\u5165\u5230\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u5b83\u4eec\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u8fc7\u53bb\uff0c\u5df2\u7ecf\u63d0\u51fa\u4e86\u4e09\u7ec4\u65b9\u6cd5\u6765\u5904\u7406\u566a\u58f0\u6570\u636e\uff1ai\uff09\u589e\u5f3a\u5668\u548c\u964d\u566a\u5668\u6a21\u5757\u4ee5\u63d0\u9ad8\u566a\u58f0\u6570\u636e\u7684\u8d28\u91cf\uff0cii\uff09\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u53ca iii\uff09\u57df\u9002\u5e94\u7b56\u7565\u3002\u6240\u6709\u4e0a\u8ff0\u65b9\u6cd5\u90fd\u5b58\u5728\u9650\u5236\u5176\u9002\u7528\u6027\u7684\u7f3a\u70b9\uff1b\u7b2c\u4e00\u4e2a\u5177\u6709\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u9700\u8981\u6210\u5bf9\u7684\u5e72\u51c0\u635f\u574f\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u5176\u4ed6\u7684\u4ec5\u5141\u8bb8\u90e8\u7f72\u5b83\u4eec\u6240\u8bad\u7ec3\u7684\u76f8\u540c\u4efb\u52a1/\u7f51\u7edc\uff08\u5373\uff0c\u5f53\u4e0a\u6e38\u548c\u4e0b\u6e38\u4efb\u52a1/\u7f51\u7edc\u76f8\u540c\u65f6\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa SyMPIE \u6765\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u70b9\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5c0f\u578b\u3001\u6a21\u5757\u5316\u548c\u9ad8\u6548\uff08\u4ec5 2GFLOPs \u5373\u53ef\u5904\u7406\u5168\u9ad8\u6e05\u56fe\u50cf\uff09\u7684\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u8f93\u5165\u6570\u636e\uff0c\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u5f3a\u5927\u7684\u4e0b\u6e38\u591a\u5a92\u4f53\u7406\u89e3\u3002\u6211\u4eec\u7684 SyMPIE \u5728\u4e0a\u6e38\u4efb\u52a1/\u7f51\u7edc\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u8be5\u4efb\u52a1/\u7f51\u7edc\u4e0d\u5e94\u4e0e\u4e0b\u6e38\u4efb\u52a1/\u7f51\u7edc\u5339\u914d\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u914d\u5bf9\u7684\u5e72\u51c0\u635f\u574f\u6837\u672c\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d1\u73b0\u7684\u5927\u591a\u6570\u8f93\u5165\u635f\u574f\u53ef\u4ee5\u901a\u8fc7\u5bf9\u56fe\u50cf\u989c\u8272\u901a\u9053\u6216\u5177\u6709\u5c0f\u5185\u6838\u7684\u7a7a\u95f4\u6ee4\u6ce2\u5668\u7684\u5168\u5c40\u64cd\u4f5c\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\uff08\u5728 ImageNetC\u3001ImageNetC-Bar\u3001VizWiz \u548c\u65b0\u63d0\u51fa\u7684\u540d\u4e3a ImageNetC-mixed \u7684\u6df7\u5408\u635f\u574f\u57fa\u51c6\uff09\u548c\u8bed\u4e49\u5206\u5272\uff08\u5728 Cityscapes\u3001ACDC \u548c DarkZurich\uff09\u5168\u9762\u63d0\u9ad8\u7ea6 5% \u7684\u76f8\u5bf9\u51c6\u786e\u5ea6\u589e\u76ca\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u4ee3\u7801\u548c\u65b0\u7684 ImageNetC \u6df7\u5408\u57fa\u51c6\u5c06\u5728\u53d1\u5e03\u540e\u63d0\u4f9b\u3002|[2402.18402v1](http://arxiv.org/pdf/2402.18402v1)|null|\n", "2402.18383": "|**2024-02-28**|**Robust Quantification of Percent Emphysema on CT via Domain Attention: the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study**|\u901a\u8fc7\u9886\u57df\u6ce8\u610f\u529b\u5bf9 CT \u4e0a\u7684\u80ba\u6c14\u80bf\u767e\u5206\u6bd4\u8fdb\u884c\u7a33\u5065\u91cf\u5316\uff1a\u52a8\u8109\u7ca5\u6837\u786c\u5316 (MESA) \u80ba\u7814\u7a76\u7684\u591a\u79cd\u65cf\u7814\u7a76|Xuzhe Zhang, Elsa D. Angelini, Eric A. Hoffman, Karol E. Watson, Benjamin M. Smith, R. Graham Barr, Andrew F. Laine|Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans. Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density. Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study. To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework. We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors. We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results.|\u5bf9\u4e8e\u6d89\u53ca\u4e0d\u540c\u626b\u63cf\u4eea\u7c7b\u578b\u626b\u63cf\u4ee5\u53ca\u8f6c\u5316\u4e3a\u4e34\u5e8a\u626b\u63cf\u7684\u5927\u89c4\u6a21\u7814\u7a76\u6765\u8bf4\uff0c\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u5bf9\u80ba\u6c14\u80bf\u7684\u7a33\u5065\u91cf\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u7684\u51e0\u4e2a\u65b9\u5411\uff0c\u5305\u62ec\u5bc6\u5ea6\u6821\u6b63\u3001\u566a\u58f0\u8fc7\u6ee4\u3001\u56de\u5f52\u3001\u57fa\u4e8e\u9690\u9a6c\u5c14\u53ef\u592b\u6d4b\u91cf\u573a\uff08HMMF\uff09\u6a21\u578b\u7684\u5206\u5272\u548c\u4f53\u79ef\u8c03\u6574\u7684\u80ba\u5bc6\u5ea6\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u8981\u4e48\u9700\u8981\u7e41\u7410\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8981\u4e48\u5bf9\u4e0b\u6e38\u80ba\u6c14\u80bf\u4e9a\u578b\u8fdb\u884c\u6709\u9650\u7684\u673a\u4f1a\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7814\u7a76\u7684\u6709\u6548\u9002\u5e94\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e00\u56f0\u5883\uff0c\u6211\u4eec\u57fa\u4e8e\u73b0\u6709\u7684 HMMF \u5206\u5272\u6846\u67b6\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\uff0c\u7531\u4e8e\u7f3a\u4e4f\u626b\u63cf\u4eea\u5148\u9a8c\uff0c\u5e38\u89c4 UNet \u65e0\u6cd5\u590d\u5236\u73b0\u6709\u7684 HMMF \u7ed3\u679c\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9886\u57df\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5c06\u56fe\u50cf\u7279\u5f81\u4e0e\u5b9a\u91cf\u626b\u63cf\u4eea\u5148\u9a8c\u878d\u5408\uff0c\u4ece\u800c\u663e\u7740\u6539\u5584\u7ed3\u679c\u3002|[2402.18383v1](http://arxiv.org/pdf/2402.18383v1)|null|\n", "2402.18309": "|**2024-02-28**|**Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis**|\u589e\u5f3a\u9053\u8def\u5b89\u5168\uff1a\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u6811\u6728\u95f4\u9699\u5206\u6790|Miriam Louise Carnot, Eric Peukert, Bogdan Franczyk|In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road. Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape. The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences. By automating this process, municipalities can address potential road space constraints, enhancing safety for all. They may also save valuable time by carrying out the inspections more systematically. Our open-source code gives communities inspiration on how to automate the process themselves.|\u5728\u786e\u4fdd\u9053\u8def\u5b89\u5168\u7684\u52aa\u529b\u4e2d\uff0c\u786e\u4fdd\u9053\u8def\u4e0a\u65b9\u6709\u8db3\u591f\u7684\u5782\u76f4\u95f4\u9699\u975e\u5e38\u91cd\u8981\u3002\u6811\u6728\u6216\u5176\u4ed6\u690d\u88ab\u7ecf\u5e38\u751f\u957f\u5728\u9053\u8def\u4e0a\u65b9\uff0c\u906e\u6321\u4ea4\u901a\u6807\u5fd7\u548c\u706f\u5149\uff0c\u5e76\u5bf9\u4ea4\u901a\u53c2\u4e0e\u8005\u6784\u6210\u5371\u9669\u3002\u7531\u4e8e\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u4ece\u7b80\u5355\u56fe\u50cf\u51c6\u786e\u4f30\u8ba1\u8fd9\u4e2a\u7a7a\u95f4\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u5c31\u662f\u6fc0\u5149\u96f7\u8fbe\u6280\u672f\u53d1\u6325\u4f5c\u7528\u7684\u5730\u65b9\uff0c\u5b83\u662f\u4e00\u79cd\u663e\u793a\u4e09\u7ef4\u89c6\u89d2\u7684\u6fc0\u5149\u626b\u63cf\u4f20\u611f\u5668\u3002\u76ee\u524d\uff0c\u8857\u9053\u7ea7\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4e3b\u8981\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u626b\u63cf\u4e5f\u4e3a\u57ce\u5e02\u7ba1\u7406\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u7b97\u6cd5\uff0c\u53ef\u4ee5\u81ea\u52a8\u68c0\u6d4b\u751f\u957f\u5728\u8857\u9053\u4e0a\u4e14\u9700\u8981\u4fee\u526a\u7684\u6811\u6728\u90e8\u5206\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u6765\u8fc7\u6ee4\u76f8\u5173\u70b9\u548c\u4e0b\u6e38\u5904\u7406\u6b65\u9aa4\uff0c\u4ee5\u521b\u5efa\u6240\u9700\u7684\u4f53\u79ef\u4ee5\u4fdd\u6301\u9053\u8def\u4e0a\u65b9\u7545\u901a\u3002\u6311\u6218\u5305\u62ec\u6a21\u7cca\u7684\u9053\u8def\u3001\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7684\u5608\u6742\u7684\u975e\u7ed3\u6784\u5316\u6027\u8d28\u4ee5\u53ca\u9053\u8def\u5f62\u72b6\u7684\u8bc4\u4f30\u3002\u6240\u8bc6\u522b\u7684\u4e0d\u5408\u89c4\u6811\u6728\u70b9\u53ef\u4ee5\u4ece\u70b9\u4e91\u6295\u5f71\u5230\u56fe\u50cf\u4e0a\uff0c\u4e3a\u5e02\u653f\u5f53\u5c40\u63d0\u4f9b\u5904\u7406\u6b64\u7c7b\u4e8b\u4ef6\u7684\u89c6\u89c9\u8f85\u52a9\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5e02\u653f\u5f53\u5c40\u53ef\u4ee5\u89e3\u51b3\u6f5c\u5728\u7684\u9053\u8def\u7a7a\u95f4\u9650\u5236\uff0c\u63d0\u9ad8\u6240\u6709\u4eba\u7684\u5b89\u5168\u3002\u4ed6\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7\u66f4\u7cfb\u7edf\u5730\u8fdb\u884c\u68c0\u67e5\u6765\u8282\u7701\u5b9d\u8d35\u7684\u65f6\u95f4\u3002\u6211\u4eec\u7684\u5f00\u6e90\u4ee3\u7801\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5982\u4f55\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u7075\u611f\u3002|[2402.18309v1](http://arxiv.org/pdf/2402.18309v1)|null|\n", "2402.18307": "|**2024-02-28**|**Feature Denoising For Low-Light Instance Segmentation Using Weighted Non-Local Blocks**|\u4f7f\u7528\u52a0\u6743\u975e\u5c40\u90e8\u5757\u8fdb\u884c\u4f4e\u5149\u5b9e\u4f8b\u5206\u5272\u7684\u7279\u5f81\u53bb\u566a|Joanne Lin, Nantheera Anantrasirichai, David Bull|Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0.|\u7531\u4e8e\u4f4e\u5149\u56fe\u50cf\u6240\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f8b\u5982\u7531\u4e8e\u4f4e\u5149\u5b50\u6570\u3001\u989c\u8272\u5931\u771f\u548c\u5bf9\u6bd4\u5ea6\u964d\u4f4e\u5bfc\u81f4\u7684\u6563\u7c92\u566a\u58f0\uff0c\u4f4e\u5149\u56fe\u50cf\u7684\u5b9e\u4f8b\u5206\u5272\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u57fa\u4e8eMask R-CNN\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u5668\u4e2d\u5b9e\u73b0\u4e86\u52a0\u6743\u975e\u5c40\u90e8\uff08NL\uff09\u5757\u3002\u8fd9\u79cd\u96c6\u6210\u53ef\u4ee5\u5728\u7279\u5f81\u7ea7\u522b\u5b9e\u73b0\u56fa\u6709\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6d88\u9664\u4e86\u8bad\u7ec3\u671f\u95f4\u5bf9\u9f50\u5730\u9762\u5b9e\u51b5\u56fe\u50cf\u7684\u9700\u8981\uff0c\u4ece\u800c\u652f\u6301\u5bf9\u73b0\u5b9e\u4e16\u754c\u4f4e\u5149\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u5728\u6bcf\u4e00\u5c42\u5f15\u5165\u989d\u5916\u7684\u53ef\u5b66\u4e60\u6743\u91cd\uff0c\u4ee5\u589e\u5f3a\u7f51\u7edc\u5bf9\u73b0\u5b9e\u4e16\u754c\u566a\u58f0\u7279\u5f81\u7684\u9002\u5e94\u6027\uff0c\u8fd9\u4e9b\u7279\u5f81\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u5f71\u54cd\u4e0d\u540c\u7684\u7279\u5f81\u5c3a\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u9884\u8bad\u7ec3\u7684 Mask R-CNN\uff0c\u5e73\u5747\u7cbe\u5ea6 (AP) \u63d0\u9ad8\u4e86 +10.0\uff0c\u5e76\u4e14\u5f15\u5165\u52a0\u6743 NL \u5757\u8fdb\u4e00\u6b65\u5c06 AP \u63d0\u9ad8\u4e86 +1.0\u3002|[2402.18307v1](http://arxiv.org/pdf/2402.18307v1)|null|\n", "2402.18302": "|**2024-02-28**|**EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving**|EchoTrack\uff1a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u542c\u89c9\u53c2\u8003\u591a\u76ee\u6807\u8ddf\u8e2a|Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang|This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack model and its components. The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack.|\u672c\u6587\u4ecb\u7ecd\u4e86\u542c\u89c9\u53c2\u8003\u591a\u5bf9\u8c61\u8ddf\u8e2a\uff08AR-MOT\uff09\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u57fa\u4e8e\u97f3\u9891\u8868\u8fbe\u52a8\u6001\u8ddf\u8e2a\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61\uff0c\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u7531\u4e8e\u7f3a\u4e4f\u97f3\u89c6\u9891\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\uff0c\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u4e8e\u6587\u672c\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u8fd9\u5f80\u5f80\u4ee5\u8ddf\u8e2a\u8d28\u91cf\u3001\u4ea4\u4e92\u6548\u7387\u751a\u81f3\u8f85\u52a9\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e3a\u4ee3\u4ef7\uff0c\u9650\u5236\u4e86\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u5e94\u7528\u3002\u6b64\u7c7b\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u97f3\u89c6\u9891\u878d\u5408\u548c\u97f3\u89c6\u9891\u8ddf\u8e2a\u7684\u89d2\u5ea6\u6df1\u5165\u7814\u7a76AR-MOT\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86 EchoTrack\uff0c\u4e00\u4e2a\u5e26\u6709\u53cc\u6d41\u89c6\u89c9\u8f6c\u6362\u5668\u7684\u7aef\u5230\u7aef AR-MOT \u6846\u67b6\u3002\u53cc\u6d41\u4e0e\u6211\u4eec\u7684\u53cc\u5411\u9891\u57df\u4ea4\u53c9\u6ce8\u610f\u878d\u5408\u6a21\u5757\uff08Bi-FCFM\uff09\u4ea4\u7ec7\u5728\u4e00\u8d77\uff0c\u8be5\u6a21\u5757\u53cc\u5411\u878d\u5408\u6765\u81ea\u9891\u57df\u548c\u65f6\u7a7a\u57df\u7684\u97f3\u9891\u548c\u89c6\u9891\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u542c\u5bf9\u6bd4\u8ddf\u8e2a\u5b66\u4e60\uff08ACTL\uff09\u673a\u5236\uff0c\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u4e0d\u540c\u97f3\u9891\u548c\u89c6\u9891\u5bf9\u8c61\u4e4b\u95f4\u7684\u540c\u8d28\u7279\u5f81\u6765\u63d0\u53d6\u8868\u8fbe\u548c\u89c6\u89c9\u5bf9\u8c61\u4e4b\u95f4\u7684\u540c\u8d28\u8bed\u4e49\u7279\u5f81\u3002\u9664\u4e86\u67b6\u6784\u8bbe\u8ba1\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5efa\u7acb\u4e86\u7b2c\u4e00\u5957\u5927\u89c4\u6a21 AR-MOT \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec Echo-KITTI\u3001Echo-KITTI+ \u548c Echo-BDD\u3002\u5bf9\u65e2\u5b9a\u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 EchoTrack \u6a21\u578b\u53ca\u5176\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 https://github.com/lab206/EchoTrack \u4e0a\u516c\u5f00\u63d0\u4f9b\u3002|[2402.18302v1](http://arxiv.org/pdf/2402.18302v1)|null|\n", "2402.18293": "|**2024-02-28**|**Grid-Based Continuous Normal Representation for Anomaly Detection**|\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u8fde\u7eed\u6b63\u6001\u8868\u793a|Joo Chan Lee, Taejune Kim, Eunbyung Park, Simon S. Woo, Jong Hwan Ko|There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing the input and the directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose GRAD, a novel anomaly detection method for representing normal features within a \"continuous\" feature space, enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that GRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, GRAD effectively handles diverse classes in a single model thanks to the high-granularity global representation. In an evaluation using the MVTec AD dataset, GRAD significantly outperforms the previous state-of-the-art method by reducing 65.0\\% of the error for multi-class unified anomaly detection. The project page is available at https://tae-mo.github.io/grad/.|\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u5df2\u7ecf\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5176\u4e2d\u53ea\u6709\u6b63\u5e38\u56fe\u50cf\u53ef\u7528\u4e8e\u8bad\u7ec3\u3002\u6700\u8fd1\u7684\u51e0\u79cd\u65b9\u6cd5\u65e8\u5728\u57fa\u4e8e\u5185\u5b58\u68c0\u6d4b\u5f02\u5e38\uff0c\u5c06\u8f93\u5165\u4e0e\u76f4\u63a5\u5b58\u50a8\u7684\u6b63\u5e38\u7279\u5f81\uff08\u6216\u8bad\u7ec3\u540e\u7684\u7279\u5f81\u4e0e\u6b63\u5e38\u56fe\u50cf\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u5728\u7531\u6700\u8fd1\u90bb\u5c45\u6216\u6ce8\u610f\u673a\u5236\u5b9e\u73b0\u7684\u79bb\u6563\u7279\u5f81\u7a7a\u95f4\u4e0a\u8fd0\u884c\uff0c\u5206\u522b\u906d\u53d7\u8f83\u5dee\u7684\u6cdb\u5316\u6216\u8f93\u51fa\u4e0e\u8f93\u5165\u76f8\u540c\u7684\u8eab\u4efd\u5feb\u6377\u65b9\u5f0f\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u65e8\u5728\u68c0\u6d4b\u5355\u7c7b\u5f02\u5e38\uff0c\u5bfc\u81f4\u5728\u5448\u73b0\u591a\u7c7b\u5bf9\u8c61\u65f6\u6027\u80fd\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6240\u6709\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GRAD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u201c\u8fde\u7eed\u201d\u7279\u5f81\u7a7a\u95f4\u5185\u7684\u6b63\u5e38\u7279\u5f81\uff0c\u901a\u8fc7\u5c06\u7a7a\u95f4\u7279\u5f81\u8f6c\u6362\u4e3a\u5750\u6807\u5e76\u5c06\u5176\u6620\u5c04\u5230\u8fde\u7eed\u7f51\u683c\u6765\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e3a\u5f02\u5e38\u68c0\u6d4b\u91cf\u8eab\u5b9a\u5236\u7684\u7f51\u683c\uff0c\u4ee3\u8868\u5c40\u90e8\u548c\u5168\u5c40\u6b63\u5e38\u7279\u5f81\u5e76\u6709\u6548\u5730\u878d\u5408\u5b83\u4eec\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRAD \u6210\u529f\u5730\u6982\u62ec\u4e86\u6b63\u5e38\u7279\u5f81\u5e76\u51cf\u8f7b\u4e86\u8eab\u4efd\u6377\u5f84\uff0c\u6b64\u5916\uff0c\u7531\u4e8e\u9ad8\u7c92\u5ea6\u7684\u5168\u5c40\u8868\u793a\uff0cGRAD \u53ef\u4ee5\u6709\u6548\u5730\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u5904\u7406\u4e0d\u540c\u7684\u7c7b\u522b\u3002\u5728\u4f7f\u7528 MVTec AD \u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\uff0cGRAD \u663e\u7740\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c06\u591a\u7c7b\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u7684\u8bef\u5dee\u964d\u4f4e\u4e86 65.0%\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8e https://tae-mo.github.io/grad/\u3002|[2402.18293v1](http://arxiv.org/pdf/2402.18293v1)|**[link](https://github.com/tae-mo/GRAD)**|\n", "2402.18292": "|**2024-02-28**|**FSL Model can Score Higher as It Is**|FSL \u6a21\u578b\u53ef\u4ee5\u5f97\u5206\u66f4\u9ad8|Yunwei Bai, Ying Kiat Tan, Tsuhan Chen|In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.|\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\uff0c\u6211\u4eec\u5f80\u5f80\u4f1a\u76f4\u89c6\u4eba\u8138\u8bc6\u522b\u673a\u5668\uff0c\u800c\u4e0d\u662f\u4fa7\u5411\u8138\u90e8\uff0c\u4ee5\u5448\u73b0\u6211\u4eec\u7684\u6b63\u9762\uff0c\u4ee5\u589e\u52a0\u88ab\u6b63\u786e\u8bc6\u522b\u7684\u673a\u4f1a\u3002\u5c11\u6837\u672c\u5b66\u4e60 (FSL) \u5206\u7c7b\u672c\u8eab\u5c31\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6a21\u578b\u5fc5\u987b\u8bc6\u522b\u5c5e\u4e8e\u8bad\u7ec3\u671f\u95f4\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\u7684\u56fe\u50cf\u3002\u56e0\u6b64\uff0c\u6d4b\u8bd5\u671f\u95f4\u626d\u66f2\u4e14\u975e\u5178\u578b\u7684\u67e5\u8be2\u6216\u652f\u6301\u56fe\u50cf\u53ef\u80fd\u4f1a\u4f7f\u6a21\u578b\u7684\u6b63\u786e\u9884\u6d4b\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\u3002\u5728\u6211\u4eec\u7684\u5de5\u4f5c\u4e2d\uff0c\u4e3a\u4e86\u589e\u52a0\u6d4b\u8bd5\u671f\u95f4\u6b63\u786e\u9884\u6d4b\u7684\u673a\u4f1a\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u751f\u6210\u6d4b\u8bd5\u7c7b\u7684\u65b0\u6837\u672c\uff0c\u4ece\u800c\u7ea0\u6b63\u7ecf\u8fc7\u8bad\u7ec3\u7684 FSL \u6a21\u578b\u7684\u6d4b\u8bd5\u8f93\u5165\u3002 FSL \u6a21\u578b\u901a\u5e38\u5728\u5177\u6709\u8db3\u591f\u6837\u672c\u7684\u7c7b\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u5177\u6709\u5c11\u91cf\u6837\u672c\u7684\u7c7b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u9996\u5148\u6355\u83b7\u6d4b\u8bd5\u56fe\u50cf\u7684\u98ce\u683c\u6216\u5f62\u72b6\uff0c\u7136\u540e\u8bc6\u522b\u5408\u9002\u7684\u8bad\u7ec3\u7c7b\u6837\u672c\u3002\u7136\u540e\uff0c\u5b83\u5c06\u6d4b\u8bd5\u56fe\u50cf\u7684\u6837\u5f0f\u6216\u5f62\u72b6\u4f20\u8f93\u5230\u8bad\u7ec3\u7c7b\u56fe\u50cf\uff0c\u4ee5\u751f\u6210\u66f4\u591a\u6d4b\u8bd5\u7c7b\u6837\u672c\uff0c\u7136\u540e\u57fa\u4e8e\u4e00\u7ec4\u751f\u6210\u7684\u6837\u672c\uff08\u800c\u4e0d\u662f\u4ec5\u4e00\u4e2a\u6837\u672c\uff09\u6267\u884c\u5206\u7c7b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6f5c\u529b\u4f7f\u7ecf\u8fc7\u8bad\u7ec3\u7684 FSL \u6a21\u578b\u5728\u6d4b\u8bd5\u9636\u6bb5\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u8bad\u7ec3\u6216\u6570\u636e\u96c6\u3002\u6839\u636e\u6211\u4eec\u7684\u5b9e\u9a8c\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528 1 \u4e2a\u989d\u5916\u751f\u6210\u7684\u6837\u672c\u6765\u589e\u5f3a\u652f\u6301\u96c6\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u7531\u52a8\u7269\u9762\u5b54\u6216\u4ea4\u901a\u6807\u5fd7\u7ec4\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 FSL \u6a21\u578b\u5b9e\u73b0\u7ea6 2% \u7684\u6539\u8fdb\u3002\u901a\u8fc7\u589e\u52a0\u652f\u6301\u96c6\u548c\u67e5\u8be2\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u73b0\u66f4\u591a\u7684\u6027\u80fd\u6539\u8fdb\u3002\u6211\u4eec\u7684 Github \u5b58\u50a8\u5e93\u662f\u516c\u5f00\u53ef\u7528\u7684\u3002|[2402.18292v1](http://arxiv.org/pdf/2402.18292v1)|null|\n", "2402.18286": "|**2024-02-28**|**Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis**|\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff1a\u5efa\u7acb\u9ad8\u7ea7\u56fe\u50cf\u5206\u6790\u7684\u57fa\u7840\u6a21\u578b|Bashir Kazimi, Karina Ruzaeva, Stefan Sandfeld|In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4ece\u672a\u6807\u8bb0\u7684\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u96c6\u8fdb\u884c\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u671d\u7740\u5efa\u7acb\u8be5\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002\u6211\u4eec\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5982\u4f55\u4fc3\u8fdb\u4e00\u7cfb\u5217\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u6548\u5fae\u8c03\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\u3001\u53bb\u566a\u3001\u566a\u58f0\u548c\u80cc\u666f\u53bb\u9664\u4ee5\u53ca\u8d85\u5206\u8fa8\u7387\u3002\u5bf9\u4e0d\u540c\u6a21\u578b\u590d\u6742\u6027\u548c\u611f\u53d7\u91ce\u5927\u5c0f\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e00\u4e2a\u663e\u7740\u7684\u73b0\u8c61\uff0c\u5373\u8f83\u4f4e\u590d\u6742\u6027\u7684\u5fae\u8c03\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5177\u6709\u968f\u673a\u6743\u91cd\u521d\u59cb\u5316\u7684\u66f4\u590d\u6742\u6a21\u578b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5728\u7535\u5b50\u663e\u5fae\u955c\u80cc\u666f\u4e0b\u8de8\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u591a\u529f\u80fd\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u50ac\u5316\u5242\uff0c\u5f53\u53ef\u7528\u7684\u6ce8\u91ca\u6570\u636e\u6709\u9650\u4e14\u8ba1\u7b97\u6210\u672c\u7684\u6709\u6548\u6269\u5c55\u5f88\u91cd\u8981\u65f6\uff0c\u5b83\u5c24\u5176\u6709\u5229\u3002|[2402.18286v1](http://arxiv.org/pdf/2402.18286v1)|null|\n", "2402.18278": "|**2024-02-28**|**EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods**|EAN-MapNet\uff1a\u5229\u7528\u951a\u70b9\u90bb\u57df\u6784\u5efa\u9ad8\u6548\u7684\u77e2\u91cf\u5316\u9ad8\u6e05\u5730\u56fe|Huiyuan Xiong, Jun Shen, Taohong Zhu, Yuelong Pan|High-definition (HD) map is crucial for autonomous driving systems. Most existing works design map elements detection heads based on the DETR decoder. However, the initial queries lack integration with the physical location feature of map elements, and vanilla self-attention entails high computational complexity. Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods. Firstly, we design query units based on the physical location feature of anchor neighborhoods. Non-neighborhood central anchors effectively assist the neighborhood central anchors in fitting to the target points, significantly improving the prediction accuracy. Then, we introduce grouped local self-attention (GL-SA), which innovatively utilizes local queries as the medium for feature interaction, thereby substantially reducing the computational complexity of self-attention while facilitating ample feature interaction among queries. On nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs. Furthermore, it considerably reduces memory consumption by 8198M compared to the baseline.|\u9ad8\u6e05\uff08HD\uff09\u5730\u56fe\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5927\u591a\u6570\u73b0\u6709\u4f5c\u54c1\u57fa\u4e8eDETR\u89e3\u7801\u5668\u8bbe\u8ba1\u5730\u56fe\u5143\u7d20\u68c0\u6d4b\u5934\u3002\u7136\u800c\uff0c\u521d\u59cb\u67e5\u8be2\u7f3a\u4e4f\u4e0e\u5730\u56fe\u5143\u7d20\u7684\u7269\u7406\u4f4d\u7f6e\u7279\u5f81\u7684\u96c6\u6210\uff0c\u5e76\u4e14\u666e\u901a\u7684\u81ea\u6ce8\u610f\u529b\u9700\u8981\u5f88\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EAN-MapNet\uff0c\u7528\u4e8e\u4f7f\u7528\u951a\u70b9\u90bb\u57df\u9ad8\u6548\u6784\u5efa\u9ad8\u6e05\u5730\u56fe\u3002\u9996\u5148\uff0c\u6211\u4eec\u6839\u636e\u951a\u90bb\u57df\u7684\u7269\u7406\u4f4d\u7f6e\u7279\u5f81\u8bbe\u8ba1\u67e5\u8be2\u5355\u5143\u3002\u975e\u90bb\u57df\u4e2d\u5fc3\u951a\u6709\u6548\u8f85\u52a9\u90bb\u57df\u4e2d\u5fc3\u951a\u62df\u5408\u76ee\u6807\u70b9\uff0c\u663e\u7740\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u7ec4\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\uff08GL-SA\uff09\uff0c\u5b83\u521b\u65b0\u6027\u5730\u5229\u7528\u5c40\u90e8\u67e5\u8be2\u4f5c\u4e3a\u7279\u5f81\u4ea4\u4e92\u7684\u5a92\u4ecb\uff0c\u4ece\u800c\u5927\u5927\u964d\u4f4e\u4e86\u81ea\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fc3\u8fdb\u67e5\u8be2\u4e4b\u95f4\u5145\u5206\u7684\u7279\u5f81\u4ea4\u4e92\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\uff0cEAN-MapNet \u5728\u8bad\u7ec3 24 \u4e2a epoch \u540e\u5b9e\u73b0\u4e86 63.0 mAP \u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u663e\u7740\u51cf\u5c11\u4e86 8198M \u7684\u5185\u5b58\u6d88\u8017\u3002|[2402.18278v1](http://arxiv.org/pdf/2402.18278v1)|null|\n", "2402.18251": "|**2024-02-28**|**On the Accuracy of Edge Detectors in Number Plate Extraction**|\u8fb9\u7f18\u68c0\u6d4b\u5668\u5728\u8f66\u724c\u63d0\u53d6\u4e2d\u7684\u51c6\u786e\u6027\u7814\u7a76|Bashir Olaniyi Sadiq|Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system. This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle. As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection. This paper presents a method of number plate extraction using edge detection technique. Edges in number plates are identified with changes in the intensity of pixel values. Therefore, these edges are identified using a single based pixel or collection of pixel-based approach. The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented. Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric|\u8fb9\u7f18\u68c0\u6d4b\u4f5c\u4e3a\u9884\u5904\u7406\u9636\u6bb5\u662f\u8f66\u724c\u63d0\u53d6\u7cfb\u7edf\u7684\u57fa\u672c\u4e14\u91cd\u8981\u7684\u65b9\u9762\u3002\u8fd9\u662f\u56e0\u4e3a\u53ef\u4ee5\u4f7f\u7528\u8f66\u724c\u6765\u8bc6\u522b\u7279\u5b9a\u8f66\u8f86\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u8f66\u724c\u5bf9\u4e8e\u8f66\u8f86\u6765\u8bf4\u90fd\u662f\u552f\u4e00\u7684\u3002\u8fd9\u6837\uff0c\u53ef\u4ee5\u5229\u7528\u8fb9\u7f18\u68c0\u6d4b\u539f\u7406\u63d0\u53d6\u8f66\u724c\u7cfb\u7edf\u4e2d\u7ebf\u6761\u548c\u5f62\u72b6\u4e0d\u540c\u7684\u5b57\u7b26\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8fb9\u7f18\u68c0\u6d4b\u6280\u672f\u63d0\u53d6\u8f66\u724c\u7684\u65b9\u6cd5\u3002\u8f66\u724c\u4e2d\u7684\u8fb9\u7f18\u901a\u8fc7\u50cf\u7d20\u503c\u5f3a\u5ea6\u7684\u53d8\u5316\u6765\u8bc6\u522b\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u8fb9\u7f18\u662f\u4f7f\u7528\u57fa\u4e8e\u5355\u4e2a\u50cf\u7d20\u6216\u57fa\u4e8e\u50cf\u7d20\u7684\u96c6\u5408\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u7684\u3002\u5b9e\u9a8c\u4e86\u8fd9\u4e9b\u8fb9\u7f18\u68c0\u6d4b\u7b97\u6cd5\u5728\u566a\u58f0\u548c\u6e05\u6d01\u73af\u5883\u4e2d\u63d0\u53d6\u8f66\u724c\u7684\u6548\u7387\u3002\u4f7f\u7528 Pratt \u54c1\u8d28\u56e0\u6570 (PFOM) \u4f5c\u4e3a\u6027\u80fd\u6307\u6807\u5728 MATLAB 2017b \u4e2d\u83b7\u5f97\u5b9e\u9a8c\u7ed3\u679c|[2402.18251v1](http://arxiv.org/pdf/2402.18251v1)|null|\n", "2402.18236": "|**2024-02-28**|**Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data**|Image2Flow\uff1a\u6df7\u5408\u56fe\u50cf\u548c\u56fe\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u6839\u636e 3D \u5fc3\u810f MRI \u6570\u636e\u5feb\u901f\u8fdb\u884c\u60a3\u8005\u7279\u5b9a\u80ba\u52a8\u8109\u5206\u5272\u548c CFD \u6d41\u573a\u8ba1\u7b97|Tina Yao, Endrit Pajaziti, Michael Quail, Silvia Schievano, Jennifer A Steeden, Vivek Muthurangu|Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics. However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming simulation. This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields.   This study used 135 3D cardiac MRIs from both a public and private dataset. The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes. CFD simulations were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset. The dataset was split 85/10/15 for training, validation and testing. Image2Flow, a hybrid image and graph convolutional neural network, was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values. Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons. Centerline comparisons of Image2Flow and CFD simulations performed using machine learning segmentation were also performed.   Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92). The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively. Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes.   This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation. Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment.|\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u53ef\u7528\u4e8e\u8bc4\u4f30\u8840\u6d41\u52a8\u529b\u5b66\u3002\u7136\u800c\uff0c\u5176\u65e5\u5e38\u4f7f\u7528\u53d7\u5230\u52b3\u52a8\u5bc6\u96c6\u578b\u624b\u52a8\u5206\u5272\u3001CFD \u7f51\u683c\u521b\u5efa\u548c\u8017\u65f6\u6a21\u62df\u7684\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u6839\u636e 3D \u5fc3\u810f MRI \u6570\u636e\u751f\u6210\u60a3\u8005\u7279\u5b9a\u7684\u80ba\u52a8\u8109\u4f53\u79ef\u7f51\u683c\uff0c\u5e76\u76f4\u63a5\u4f30\u8ba1 CFD \u6d41\u573a\u3002\u672c\u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81ea\u516c\u5171\u548c\u79c1\u4eba\u6570\u636e\u96c6\u7684 135 \u4e2a 3D \u5fc3\u810f MRI\u3002 MRI \u4e2d\u7684\u80ba\u52a8\u8109\u88ab\u624b\u52a8\u5206\u5272\u5e76\u8f6c\u6362\u4e3a\u4f53\u79ef\u7f51\u683c\u3002 CFD \u6a21\u62df\u5728\u5730\u9762\u5b9e\u51b5\u7f51\u683c\u4e0a\u8fdb\u884c\uff0c\u5e76\u63d2\u503c\u5230\u70b9\u5bf9\u70b9\u5bf9\u5e94\u7684\u7f51\u683c\u4e0a\u4ee5\u521b\u5efa\u5730\u9762\u5b9e\u51b5\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u5206\u4e3a 85/10/15 \u90e8\u5206\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u3002 Image2Flow \u662f\u4e00\u79cd\u6df7\u5408\u56fe\u50cf\u548c\u56fe\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u53ef\u5c06\u80ba\u52a8\u8109\u6a21\u677f\u8f6c\u6362\u4e3a\u60a3\u8005\u7279\u5b9a\u7684\u89e3\u5256\u7ed3\u6784\u548c CFD \u503c\u3002 Image2Flow \u6839\u636e\u5206\u6bb5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u8282\u70b9\u6bd4\u8f83\u6765\u8bc4\u4f30 CFD \u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u8fd8\u5bf9\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u5272\u8fdb\u884c\u7684 Image2Flow \u548c CFD \u6a21\u62df\u8fdb\u884c\u4e86\u4e2d\u5fc3\u7ebf\u6bd4\u8f83\u3002 Image2Flow \u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u4e2d\u503c Dice \u5f97\u5206\u4e3a 0.9\uff08IQR\uff1a0.86-0.92\uff09\u3002\u538b\u529b\u548c\u901f\u5ea6\u5927\u5c0f\u7684\u4e2d\u4f4d\u8282\u70b9\u5f52\u4e00\u5316\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a 11.98% (IQR: 9.44-17.90%) \u548c 8.06% (IQR: 7.54-10.41)\u3002\u4e2d\u5fc3\u7ebf\u5206\u6790\u663e\u793a\uff0c\u5728\u673a\u5668\u5b66\u4e60\u751f\u6210\u7684\u4f53\u79ef\u7f51\u683c\u4e0a\u6a21\u62df\u7684 Image2Flow \u548c\u4f20\u7edf CFD \u4e4b\u95f4\u6ca1\u6709\u663e\u7740\u5dee\u5f02\u3002\u8fd9\u9879\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u57fa\u4e8e\u60a3\u8005\u7279\u5b9a\u4f53\u79ef\u7f51\u683c\u7684\u5206\u5272\u4ee5\u53ca\u538b\u529b\u548c\u6d41\u573a\u4f30\u8ba1\u3002 Image2Flow \u5728\u7ea6 205 \u6beb\u79d2\u5185\u5b8c\u6210\u5206\u5272\u548c CFD\uff0c\u6bd4\u624b\u52a8\u65b9\u6cd5\u5feb\u7ea6 7000 \u500d\uff0c\u4f7f\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u66f4\u52a0\u53ef\u884c\u3002|[2402.18236v1](http://arxiv.org/pdf/2402.18236v1)|null|\n", "2402.18233": "|**2024-02-28**|**Zero-Shot Aerial Object Detection with Visual Description Regularization**|\u5177\u6709\u89c6\u89c9\u63cf\u8ff0\u6b63\u5219\u5316\u7684\u96f6\u6837\u672c\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b|Zhengqing Zang, Chenyu Lin, Chenwei Tang, Tao Wang, Jiancheng Lv|Existing object detection models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient object detection methods on aerial images. In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture.|\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u5728\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u4e3a\u65b0\u9896\u7684\u7a7a\u4e2d\u7269\u4f53\u7c7b\u522b\u6ce8\u91ca\u6570\u636e\u975e\u5e38\u6602\u8d35\uff0c\u56e0\u4e3a\u5b83\u975e\u5e38\u8017\u65f6\u5e76\u4e14\u53ef\u80fd\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u822a\u7a7a\u56fe\u50cf\u4e0a\u7684\u6807\u7b7e\u6709\u6548\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u79f0\u4e3a\u89c6\u89c9\u63cf\u8ff0\u6b63\u5219\u5316\u6216 DescReg\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u7a7a\u4e2d\u7269\u4f53\u7684\u5f31\u8bed\u4e49\u89c6\u89c9\u76f8\u5173\u6027\uff0c\u5e76\u65e8\u5728\u901a\u8fc7\u9884\u5148\u63cf\u8ff0\u5176\u89c6\u89c9\u5916\u89c2\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002\u6211\u4eec\u5efa\u8bae\u5c06\u63cf\u8ff0\u4e2d\u4f20\u8fbe\u7684\u5148\u524d\u7c7b\u95f4\u89c6\u89c9\u76f8\u4f3c\u6027\u6ce8\u5165\u5230\u5d4c\u5165\u5b66\u4e60\u4e2d\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5c06\u63cf\u8ff0\u7f16\u7801\u5230\u906d\u53d7\u8868\u793a\u5dee\u8ddd\u95ee\u9898\u7684\u7c7b\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u6ce8\u5165\u8fc7\u7a0b\u662f\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u4e09\u5143\u7ec4\u635f\u5931\u6765\u5b8c\u6210\u7684\uff0c\u8be5\u635f\u5931\u7ed3\u5408\u4e86\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u6b63\u5219\u5316\u3002\u6211\u4eec\u5bf9\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u96c6\uff08\u5305\u62ec DIOR\u3001xView \u548c DOTA\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u590d\u6742\u7684\u6295\u5f71\u8bbe\u8ba1\u548c\u751f\u6210\u6846\u67b6\uff0cDescReg \u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 ZSD \u65b9\u6cd5\uff0c\u4f8b\u5982\uff0c\u5728 DIOR \u4e0a\uff0cDESReg \u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684 mAP \u4f18\u4e8e\u6700\u4f73\u62a5\u544a\u7684 ZSD \u65b9\u6cd5 4.5 mAP\uff0c\u5728 HM \u4e0a\u4f18\u4e8e 8.1 mAP\u3002\u6211\u4eec\u901a\u8fc7\u5c06 DescReg \u96c6\u6210\u5230\u751f\u6210 ZSD \u65b9\u6cd5\u4ee5\u53ca\u6539\u53d8\u68c0\u6d4b\u67b6\u6784\u6765\u8fdb\u4e00\u6b65\u5c55\u793a DescReg \u7684\u901a\u7528\u6027\u3002|[2402.18233v1](http://arxiv.org/pdf/2402.18233v1)|null|\n", "2402.18202": "|**2024-02-28**|**Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments**|\u6ea2\u6cb9\u65e0\u4eba\u673a\uff1a\u65e0\u4eba\u673a\u6355\u83b7\u7684\u5206\u6bb5 RGB \u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6e2f\u53e3\u73af\u5883\u4e2d\u7684\u6ea2\u6cb9\u68c0\u6d4b|T. De Kerf, S. Sels, S. Samsonova, S. Vanlanduit|The high incidence of oil spills in port areas poses a serious threat to the environment, prompting the need for efficient detection mechanisms. Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection. Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents. Currently, there's a scarcity of datasets employing RGB images for oil spill detection in maritime settings. This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms. The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a convolutional neural network trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection. This underscores the dataset's practicality for real-world applications, offering crucial resources for environmental conservation in port environments.|\u6e2f\u53e3\u5730\u533a\u6f0f\u6cb9\u4e8b\u4ef6\u7684\u9ad8\u53d1\u5bf9\u73af\u5883\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\uff0c\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u673a\u5236\u3002\u4e3a\u6b64\u4f7f\u7528\u81ea\u52a8\u5316\u65e0\u4eba\u673a\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u6ea2\u6cb9\u68c0\u6d4b\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u8fdb\u6b65\u4e0d\u4ec5\u52a0\u5feb\u4e86\u6e05\u7406\u5de5\u4f5c\uff0c\u51cf\u5c11\u4e86\u73af\u5883\u5371\u5bb3\uff0c\u800c\u4e14\u8fd8\u52a0\u5f3a\u4e86\u6c61\u67d3\u8005\u7684\u8d23\u4efb\uff0c\u6709\u53ef\u80fd\u963b\u6b62\u672a\u6765\u53d1\u751f\u7c7b\u4f3c\u7684\u4e8b\u4ef6\u3002\u76ee\u524d\uff0c\u4f7f\u7528 RGB \u56fe\u50cf\u8fdb\u884c\u6d77\u4e0a\u6ea2\u6cb9\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u5f88\u7a00\u7f3a\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5728\u684c\u9762\u548c\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u4e0a\u8fdb\u884c\u5206\u6790\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u65e0\u4eba\u673a\u6355\u83b7\uff0c\u5305\u542b 1268 \u5f20\u56fe\u50cf\uff0c\u5206\u4e3a\u77f3\u6cb9\u3001\u6c34\u548c\u5176\u4ed6\u7c7b\u522b\uff0c\u4f7f\u7528 Unet \u6a21\u578b\u67b6\u6784\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u77f3\u6cb9\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86 0.71 \u7684 F1 \u5206\u6570\u3002\u8fd9\u5f3a\u8c03\u4e86\u8be5\u6570\u636e\u96c6\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u6e2f\u53e3\u73af\u5883\u7684\u73af\u5883\u4fdd\u62a4\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002|[2402.18202v1](http://arxiv.org/pdf/2402.18202v1)|null|\n", "2402.18162": "|**2024-02-28**|**Out-of-Distribution Detection using Neural Activation Prior**|\u4f7f\u7528\u795e\u7ecf\u6fc0\u6d3b\u5148\u9a8c\u8fdb\u884c\u5206\u5e03\u5916\u68c0\u6d4b|Weilin Wan, Weizhong Zhang, Cheng Jin|Out-of-distribution detection is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios.In this paper, we propose a simple but effective Neural Activation Prior (NAP) for out-of-distribution detection (OOD). Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few of its neurons being activated with a larger response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is each channel in a model fully trained on ID dataset would play a role in detecting a certain pattern in the samples within the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Thus, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. This approach is plug-and-play and does not lead to any performance degradation on in-distribution data classification and requires no extra training or statistics from training or external datasets. Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator. Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications. Experimental results show that our method achieves the state-of-the-art performance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates the power of the proposed prior.|\u5206\u5e03\u5916\u68c0\u6d4b\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5904\u7406\u672a\u89c1\u8fc7\u7684\u573a\u666f\u7684\u5173\u952e\u6280\u672f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u795e\u7ecf\u6fc0\u6d3b\u5148\u9a8c\uff08NAP\uff09\uff0c\u7528\u4e8e\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09 \uff09\u3002\u6211\u4eec\u7684\u795e\u7ecf\u6fc0\u6d3b\u5148\u9a8c\u57fa\u4e8e\u4e00\u4e2a\u5173\u952e\u89c2\u5bdf\uff0c\u5373\u5bf9\u4e8e\u7ecf\u8fc7\u5145\u5206\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u5168\u5c40\u6c60\u5316\u5c42\u4e4b\u524d\u7684\u901a\u9053\uff0c\u5176\u4e00\u4e9b\u795e\u7ecf\u5143\u88ab\u5206\u5e03\u5185 (ID) \u6837\u672c\u6fc0\u6d3b\u4e3a\u5177\u6709\u66f4\u5927\u54cd\u5e94\u7684\u6982\u7387\u660e\u663e\u9ad8\u4e8e OOD \u6837\u672c\u3002\u76f4\u89c2\u7684\u89e3\u91ca\u662f\uff0c\u5728 ID \u6570\u636e\u96c6\u4e0a\u5145\u5206\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\u7684\u6bcf\u4e2a\u901a\u9053\u90fd\u5c06\u5728\u68c0\u6d4b ID \u6570\u636e\u96c6\u4e2d\u6837\u672c\u4e2d\u7684\u67d0\u79cd\u6a21\u5f0f\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u5e76\u4e14\u5f53\u5728\u8f93\u5165\u4e2d\u68c0\u6d4b\u5230\u8be5\u6a21\u5f0f\u65f6\uff0c\u53ef\u4ee5\u6fc0\u6d3b\u4e00\u4e9b\u795e\u7ecf\u5143\u5e76\u4ea7\u751f\u8f83\u5927\u7684\u54cd\u5e94\u6837\u672c\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b64\u5148\u9a8c\u7684\u65b0\u8bc4\u5206\u51fd\u6570\uff0c\u4ee5\u5f3a\u8c03\u8fd9\u4e9b\u5f3a\u70c8\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u5728 OOD \u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002\u8fd9\u79cd\u65b9\u6cd5\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u4e0d\u4f1a\u5bfc\u81f4\u5206\u5e03\u5185\u6570\u636e\u5206\u7c7b\u7684\u4efb\u4f55\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u6765\u81ea\u8bad\u7ec3\u6216\u5916\u90e8\u6570\u636e\u96c6\u7684\u989d\u5916\u8bad\u7ec3\u6216\u7edf\u8ba1\u3002\u8bf7\u6ce8\u610f\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u540e\u5168\u5c40\u6c60\u5316\u7279\u5f81\uff0c\u800c\u6211\u4eec\u5229\u7528\u7684\u901a\u9053\u5185\u5206\u5e03\u4fe1\u606f\u5c06\u88ab\u5168\u5c40\u6c60\u5316\u7b97\u5b50\u4e22\u5f03\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u65b9\u6cd5\u6b63\u4ea4\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u4e0e\u5b83\u4eec\u6709\u6548\u5730\u7ed3\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd9\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u5148\u9a8c\u7684\u5f3a\u5927\u529f\u80fd\u3002|[2402.18162v1](http://arxiv.org/pdf/2402.18162v1)|null|\n", "2402.18140": "|**2024-02-28**|**OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction**|OccTransformer\uff1a\u6539\u8fdb BEVFormer\uff0c\u4ee5\u5b9e\u73b0\u4ec5 3D \u76f8\u673a\u7684\u5360\u7528\u9884\u6d4b|Jian Liu, Sipeng Zhang, Chuixin Kong, Wenyuan Zhang, Yuhang Wu, Yikang Ding, Borun Xu, Ruibo Ming, Donglai Wei, Xianming Liu|This technical report presents our solution, \"occTransformer\" for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023. Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques. Firstly, we employed data augmentation to increase the diversity of the training data and improve the model's generalization ability. Secondly, we used a strong image backbone to extract more informative features from the input data. Thirdly, we incorporated a 3D unet head to better capture the spatial information of the scene. Fourthly, we added more loss functions to better optimize the model. Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance. Most importantly, we integrated 3D detection model StreamPETR to enhance the model's ability to detect objects in the scene. Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge.|\u672c\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86\u6211\u4eec\u5728 CVPR 2023 \u7684\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u4e2d\u7528\u4e8e 3D \u5360\u7528\u9884\u6d4b\u8d5b\u9053\u7684\u89e3\u51b3\u65b9\u6848\u201coccTransformer\u201d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5efa\u7acb\u5728\u5f3a\u5927\u7684\u57fa\u7ebf BEVFormer \u7684\u57fa\u7840\u4e0a\uff0c\u5e76\u901a\u8fc7\u51e0\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\u63d0\u9ad8\u4e86\u5176\u6027\u80fd\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6765\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5176\u6b21\uff0c\u6211\u4eec\u4f7f\u7528\u5f3a\u5927\u7684\u56fe\u50cf\u4e3b\u5e72\u4ece\u8f93\u5165\u6570\u636e\u4e2d\u63d0\u53d6\u66f4\u591a\u4fe1\u606f\u7279\u5f81\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u52a0\u5165\u4e86 3Dunet \u5934\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u573a\u666f\u7684\u7a7a\u95f4\u4fe1\u606f\u3002\u7b2c\u56db\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u66f4\u591a\u635f\u5931\u51fd\u6570\u4ee5\u66f4\u597d\u5730\u4f18\u5316\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u4e86 Occ \u6a21\u578b BevDet \u548c SurroundOcc \u7684\u96c6\u6210\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u96c6\u6210\u4e86 3D \u68c0\u6d4b\u6a21\u578b StreamPETR\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u68c0\u6d4b\u573a\u666f\u4e2d\u7269\u4f53\u7684\u80fd\u529b\u3002\u4f7f\u7528\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5728\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u7684 3D \u5360\u7528\u9884\u6d4b\u8d5b\u9053\u4e0a\u53d6\u5f97\u4e86 49.23 miou \u7684\u6210\u7ee9\u3002|[2402.18140v1](http://arxiv.org/pdf/2402.18140v1)|null|\n", "2402.18133": "|**2024-02-28**|**Classes Are Not Equal: An Empirical Study on Image Recognition Fairness**|\u7c7b\u4e0d\u5e73\u7b49\uff1a\u56fe\u50cf\u8bc6\u522b\u516c\u5e73\u6027\u7684\u5b9e\u8bc1\u7814\u7a76|Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang|In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5173\u4e8e\u56fe\u50cf\u8bc6\u522b\u516c\u5e73\u6027\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5373 ImageNet \u7b49\u5e73\u8861\u6570\u636e\u4e0a\u7684\u6781\u7aef\u7c7b\u522b\u51c6\u786e\u5ea6\u5dee\u5f02\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7c7b\u522b\u4e0d\u76f8\u7b49\uff0c\u5e76\u4e14\u516c\u5e73\u6027\u95ee\u9898\u5728\u5404\u79cd\u6570\u636e\u96c6\u3001\u7f51\u7edc\u67b6\u6784\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u516c\u5e73\u6027\u7684\u51e0\u4e2a\u6709\u8da3\u7684\u7279\u6027\u3002\u9996\u5148\uff0c\u4e0d\u516c\u5e73\u5728\u4e8e\u6709\u95ee\u9898\u7684\u8868\u793a\u800c\u4e0d\u662f\u5206\u7c7b\u5668\u504f\u5dee\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u6a21\u578b\u9884\u6d4b\u504f\u5dee\u6982\u5ff5\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4f18\u5316\u671f\u95f4\u6709\u95ee\u9898\u7684\u8868\u793a\u7684\u6839\u6e90\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5f80\u5f80\u5bf9\u66f4\u96be\u4ee5\u8bc6\u522b\u7684\u7c7b\u522b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u9884\u6d4b\u504f\u5dee\u3002\u8fd9\u610f\u5473\u7740\u66f4\u591a\u5176\u4ed6\u8bfe\u7a0b\u5c06\u4e0e\u66f4\u96be\u7684\u8bfe\u7a0b\u6df7\u6dc6\u3002\u90a3\u4e48\u8bef\u62a5\uff08FP\uff09\u5c06\u4e3b\u5bfc\u4f18\u5316\u4e2d\u7684\u5b66\u4e60\uff0c\u4ece\u800c\u5bfc\u81f4\u5176\u51c6\u786e\u6027\u8f83\u5dee\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f97\u51fa\u7684\u7ed3\u8bba\u662f\uff0c\u6570\u636e\u589e\u5f3a\u548c\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u901a\u8fc7\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4fc3\u8fdb\u56fe\u50cf\u5206\u7c7b\u7684\u516c\u5e73\u6027\u6765\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\u3002|[2402.18133v1](http://arxiv.org/pdf/2402.18133v1)|null|\n", "2402.18132": "|**2024-02-28**|**Understanding the Role of Pathways in a Deep Neural Network**|\u4e86\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u8def\u5f84\u7684\u4f5c\u7528|Lei Lyu, Chen Pang, Jihua Wang|Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding adversarial attacks, object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7684\u4e0d\u900f\u660e\u662f\u5176\u5e94\u7528\u4e2d\u7684\u4e00\u5927\u7f3a\u9677\u3002\u6d41\u884c\u7684\u57fa\u4e8e\u5355\u5143\u7684\u89e3\u91ca\u662f\u5bf9\u523a\u6fc0-\u53cd\u5e94\u6570\u636e\u7684\u7edf\u8ba1\u89c2\u5bdf\uff0c\u5b83\u65e0\u6cd5\u663e\u793a\u795e\u7ecf\u7f51\u7edc\u56fa\u6709\u673a\u5236\u7684\u8be6\u7ec6\u5185\u90e8\u8fc7\u7a0b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\u6765\u63d0\u53d6\u5355\u4e2a\u50cf\u7d20\u7684\u6269\u6563\u8def\u5f84\uff0c\u4ee5\u8bc6\u522b\u4e0e\u5bf9\u8c61\u7c7b\u76f8\u5173\u7684\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u4f4d\u7f6e\u3002\u8fd9\u4e9b\u8def\u5f84\u4f7f\u6211\u4eec\u80fd\u591f\u6d4b\u8bd5\u5bf9\u5206\u7c7b\u5f88\u91cd\u8981\u7684\u56e0\u679c\u6210\u5206\uff0c\u5e76\u4e14\u57fa\u4e8e\u8def\u5f84\u7684\u8868\u793a\u5728\u7c7b\u522b\u4e4b\u95f4\u53ef\u4ee5\u6e05\u695a\u5730\u533a\u5206\u3002\u6211\u4eec\u53d1\u73b0\u56fe\u50cf\u4e2d\u5355\u4e2a\u50cf\u7d20\u7684\u51e0\u4e2a\u6700\u5927\u8def\u5f84\u5f80\u5f80\u4f1a\u7a7f\u8fc7\u5bf9\u5206\u7c7b\u5f88\u91cd\u8981\u7684\u6bcf\u4e00\u5c42\u4e2d\u7684\u7279\u5f81\u56fe\u3002\u5e76\u4e14\u540c\u4e00\u7c7b\u522b\u56fe\u50cf\u7684\u5927\u8def\u5f84\u6bd4\u4e0d\u540c\u7c7b\u522b\u56fe\u50cf\u7684\u5927\u8def\u5f84\u8d8b\u52bf\u66f4\u52a0\u4e00\u81f4\u3002\u6211\u4eec\u8fd8\u5e94\u7528\u8fd9\u4e9b\u9014\u5f84\u6765\u7406\u89e3\u5bf9\u6297\u6027\u653b\u51fb\u3001\u5bf9\u8c61\u5b8c\u6210\u548c\u8fd0\u52a8\u611f\u77e5\u3002\u6b64\u5916\uff0c\u6240\u6709\u5c42\u4e2d\u7279\u5f81\u56fe\u4e0a\u7684\u8def\u5f84\u603b\u6570\u53ef\u4ee5\u6e05\u695a\u5730\u533a\u5206\u539f\u59cb\u6837\u672c\u3001\u53d8\u5f62\u6837\u672c\u548c\u76ee\u6807\u6837\u672c\u3002|[2402.18132v1](http://arxiv.org/pdf/2402.18132v1)|null|\n", "2402.18117": "|**2024-02-28**|**PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation**|PRCL\uff1a\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6982\u7387\u8868\u793a\u5bf9\u6bd4\u5b66\u4e60|Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu, Baigui Sun|Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.|\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08S4\uff09\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u7a81\u7834\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6ce8\u91ca\u6709\u9650\uff0c\u5bf9\u672a\u6807\u8bb0\u56fe\u50cf\u7684\u6307\u5bfc\u662f\u7531\u6a21\u578b\u672c\u8eab\u751f\u6210\u7684\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u5b58\u5728\u566a\u58f0\u5e76\u5e72\u6270\u65e0\u76d1\u7763\u8bad\u7ec3\u8fc7\u7a0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u57fa\u4e8e\u5bf9\u6bd4\u7684 S4 \u6846\u67b6\uff0c\u79f0\u4e3a\u6982\u7387\u8868\u793a\u5bf9\u6bd4\u5b66\u4e60\uff08PRCL\uff09\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u65e0\u76d1\u7763\u8bad\u7ec3\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u901a\u8fc7\u591a\u5143\u9ad8\u65af\u5206\u5e03\u5c06\u50cf\u7d20\u7ea7\u8868\u793a\u5efa\u6a21\u4e3a\u6982\u7387\u8868\u793a\uff08PR\uff09\uff0c\u5e76\u8c03\u6574\u6a21\u7cca\u8868\u793a\u7684\u8d21\u732e\u4ee5\u5bb9\u5fcd\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u6307\u5bfc\u4e0d\u51c6\u786e\u7684\u98ce\u9669\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u6536\u96c6\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6240\u6709 PR \u6765\u5f15\u5165\u5168\u7403\u5206\u5e03\u539f\u578b (GDP)\u3002\u7531\u4e8eGDP\u5305\u542b\u540c\u4e00\u7c7b\u7684\u6240\u6709\u8868\u793a\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u5b83\u5bf9\u8868\u793a\u4e2d\u7684\u77ac\u65f6\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u627f\u53d7\u8868\u793a\u7684\u7c7b\u5185\u65b9\u5dee\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6839\u636e GDP \u751f\u6210\u865a\u62df\u8d1f\u6570\uff08VN\uff09\u4ee5\u53c2\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u8fc7\u7a0b\u3002\u5bf9\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 PRCL \u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002|[2402.18117v1](http://arxiv.org/pdf/2402.18117v1)|null|\n", "2402.18115": "|**2024-02-28**|**UniVS: Unified and Universal Video Segmentation with Prompts as Queries**|UniVS\uff1a\u4ee5\u63d0\u793a\u4f5c\u4e3a\u67e5\u8be2\u7684\u7edf\u4e00\u901a\u7528\u89c6\u9891\u5206\u5272|Minghan Li, Shuai Li, Xindong Zhang, Lei Zhang|Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \\url{https://github.com/MinghanLi/UniVS}.|\u5c3d\u7ba1\u7edf\u4e00\u56fe\u50cf\u5206\u5272\uff08IS\uff09\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5f00\u53d1\u7edf\u4e00\u89c6\u9891\u5206\u5272\uff08VS\uff09\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u901a\u7528\u7c7b\u522b\u6307\u5b9a\u7684 VS \u4efb\u52a1\u9700\u8981\u68c0\u6d4b\u6240\u6709\u5bf9\u8c61\u5e76\u5728\u8fde\u7eed\u5e27\u4e2d\u8ddf\u8e2a\u5b83\u4eec\uff0c\u800c\u63d0\u793a\u5f15\u5bfc\u7684 VS \u4efb\u52a1\u9700\u8981\u5728\u6574\u4e2a\u89c6\u9891\u4e2d\u901a\u8fc7\u89c6\u89c9/\u6587\u672c\u63d0\u793a\u91cd\u65b0\u8bc6\u522b\u76ee\u6807\uff0c\u8fd9\u4f7f\u5f97\u5904\u7406\u5177\u6709\u76f8\u540c\u67b6\u6784\u7684\u4e0d\u540c\u4efb\u52a1\u3002\u6211\u4eec\u5c1d\u8bd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u63d0\u793a\u4f5c\u4e3a\u67e5\u8be2\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00 VS \u67b6\u6784\uff0c\u5373 UniVS\u3002 UniVS \u5c06\u524d\u4e00\u5e27\u4e2d\u76ee\u6807\u7684\u63d0\u793a\u7279\u5f81\u5e73\u5747\u4f5c\u4e3a\u5176\u521d\u59cb\u67e5\u8be2\uff0c\u4ee5\u663e\u5f0f\u89e3\u7801\u63a9\u7801\uff0c\u5e76\u5728\u63a9\u7801\u89e3\u7801\u5668\u4e2d\u5f15\u5165\u76ee\u6807\u660e\u667a\u7684\u63d0\u793a\u4ea4\u53c9\u6ce8\u610f\u5c42\uff0c\u4ee5\u5c06\u63d0\u793a\u7279\u5f81\u96c6\u6210\u5230\u5185\u5b58\u6c60\u4e2d\u3002\u901a\u8fc7\u5c06\u524d\u4e00\u5e27\u4e2d\u5b9e\u4f53\u7684\u9884\u6d4b\u63a9\u6a21\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0cUniVS \u5c06\u4e0d\u540c\u7684 VS \u4efb\u52a1\u8f6c\u6362\u4e3a\u63d0\u793a\u5f15\u5bfc\u7684\u76ee\u6807\u5206\u5272\uff0c\u6d88\u9664\u4e86\u542f\u53d1\u5f0f\u5e27\u95f4\u5339\u914d\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u6846\u67b6\u4e0d\u4ec5\u7edf\u4e00\u4e86\u4e0d\u540c\u7684VS\u4efb\u52a1\uff0c\u800c\u4e14\u81ea\u7136\u5730\u5b9e\u73b0\u4e86\u901a\u7528\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u786e\u4fdd\u8de8\u4e0d\u540c\u573a\u666f\u7684\u7a33\u5065\u6027\u80fd\u3002 UniVS \u5728 10 \u4e2a\u5177\u6709\u6311\u6218\u6027\u7684 VS \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u4e86\u6027\u80fd\u548c\u901a\u7528\u6027\u4e4b\u95f4\u503c\u5f97\u79f0\u8d5e\u7684\u5e73\u8861\uff0c\u6db5\u76d6\u89c6\u9891\u5b9e\u4f8b\u3001\u8bed\u4e49\u3001\u5168\u666f\u3001\u5bf9\u8c61\u548c\u5f15\u7528\u5206\u5272\u4efb\u52a1\u3002\u4ee3\u7801\u53ef\u4ee5\u5728 \\url{https://github.com/MinghanLi/UniVS} \u627e\u5230\u3002|[2402.18115v1](http://arxiv.org/pdf/2402.18115v1)|null|\n", "2402.18109": "|**2024-02-28**|**Dual-Context Aggregation for Universal Image Matting**|\u7528\u4e8e\u901a\u7528\u56fe\u50cf\u62a0\u56fe\u7684\u53cc\u4e0a\u4e0b\u6587\u805a\u5408|Qinglin Liu, Xiaoqian Lv, Wei Yu, Changyong Guo, Shengping Zhang|Natural image matting aims to estimate the alpha matte of the foreground from a given image. Various approaches have been explored to address this problem, such as interactive matting methods that use guidance such as click or trimap, and automatic matting methods tailored to specific objects. However, existing matting methods are designed for specific objects or guidance, neglecting the common requirement of aggregating global and local contexts in image matting. As a result, these methods often encounter challenges in accurately identifying the foreground and generating precise boundaries, which limits their effectiveness in unforeseen scenarios. In this paper, we propose a simple and universal matting framework, named Dual-Context Aggregation Matting (DCAM), which enables robust image matting with arbitrary guidance or without guidance. Specifically, DCAM first adopts a semantic backbone network to extract low-level features and context features from the input image and guidance. Then, we introduce a dual-context aggregation network that incorporates global object aggregators and local appearance aggregators to iteratively refine the extracted context features. By performing both global contour segmentation and local boundary refinement, DCAM exhibits robustness to diverse types of guidance and objects. Finally, we adopt a matting decoder network to fuse the low-level features and the refined context features for alpha matte estimation. Experimental results on five matting datasets demonstrate that the proposed DCAM outperforms state-of-the-art matting methods in both automatic matting and interactive matting tasks, which highlights the strong universality and high performance of DCAM. The source code is available at \\url{https://github.com/Windaway/DCAM}.|\u81ea\u7136\u56fe\u50cf\u62a0\u56fe\u65e8\u5728\u4f30\u8ba1\u7ed9\u5b9a\u56fe\u50cf\u4e2d\u524d\u666f\u7684 alpha \u906e\u7f69\u3002\u4eba\u4eec\u5df2\u7ecf\u63a2\u7d22\u4e86\u5404\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f8b\u5982\u4f7f\u7528\u70b9\u51fb\u6216\u4e09\u5206\u56fe\u7b49\u6307\u5bfc\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u6cd5\uff0c\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u5bf9\u8c61\u5b9a\u5236\u7684\u81ea\u52a8\u62a0\u56fe\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u62a0\u56fe\u65b9\u6cd5\u662f\u9488\u5bf9\u7279\u5b9a\u5bf9\u8c61\u6216\u6307\u5bfc\u800c\u8bbe\u8ba1\u7684\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u62a0\u56fe\u4e2d\u805a\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7684\u5e38\u89c1\u8981\u6c42\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u51c6\u786e\u8bc6\u522b\u524d\u666f\u548c\u751f\u6210\u7cbe\u786e\u8fb9\u754c\u65b9\u9762\u7ecf\u5e38\u9047\u5230\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u53ef\u9884\u89c1\u7684\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u62a0\u56fe\u6846\u67b6\uff0c\u79f0\u4e3a\u53cc\u4e0a\u4e0b\u6587\u805a\u5408\u62a0\u56fe\uff08DCAM\uff09\uff0c\u5b83\u53ef\u4ee5\u5728\u4efb\u610f\u6307\u5bfc\u6216\u65e0\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u56fe\u50cf\u62a0\u56fe\u3002\u5177\u4f53\u6765\u8bf4\uff0cDCAM\u9996\u5148\u91c7\u7528\u8bed\u4e49\u9aa8\u5e72\u7f51\u7edc\u4ece\u8f93\u5165\u56fe\u50cf\u548c\u6307\u5bfc\u4e2d\u63d0\u53d6\u4f4e\u7ea7\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u4e0a\u4e0b\u6587\u805a\u5408\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u7ed3\u5408\u4e86\u5168\u5c40\u5bf9\u8c61\u805a\u5408\u5668\u548c\u5c40\u90e8\u5916\u89c2\u805a\u5408\u5668\u6765\u8fed\u4ee3\u5730\u7ec6\u5316\u63d0\u53d6\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u901a\u8fc7\u6267\u884c\u5168\u5c40\u8f6e\u5ed3\u5206\u5272\u548c\u5c40\u90e8\u8fb9\u754c\u7ec6\u5316\uff0cDCAM \u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u5f15\u5bfc\u548c\u5bf9\u8c61\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u62a0\u56fe\u89e3\u7801\u5668\u7f51\u7edc\u6765\u878d\u5408\u4f4e\u7ea7\u7279\u5f81\u548c\u7ec6\u5316\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u4ee5\u8fdb\u884c alpha \u62a0\u56fe\u4f30\u8ba1\u3002\u4e94\u4e2a\u62a0\u56fe\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 DCAM \u5728\u81ea\u52a8\u62a0\u56fe\u548c\u4ea4\u4e92\u5f0f\u62a0\u56fe\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u62a0\u56fe\u65b9\u6cd5\uff0c\u8fd9\u51f8\u663e\u4e86 DCAM \u7684\u5f3a\u5927\u901a\u7528\u6027\u548c\u9ad8\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/Windaway/DCAM} \u83b7\u53d6\u3002|[2402.18109v1](http://arxiv.org/pdf/2402.18109v1)|null|\n", "2402.18084": "|**2024-02-28**|**Spannotation: Enhancing Semantic Segmentation for Autonomous Navigation with Efficient Image Annotation**|Spanotation\uff1a\u901a\u8fc7\u9ad8\u6548\u7684\u56fe\u50cf\u6ce8\u91ca\u589e\u5f3a\u81ea\u4e3b\u5bfc\u822a\u7684\u8bed\u4e49\u5206\u5272|Samuel O. Folorunsho, William R. Norris|Spannotation is an open source user-friendly tool developed for image annotation for semantic segmentation specifically in autonomous navigation tasks. This study provides an evaluation of Spannotation, demonstrating its effectiveness in generating accurate segmentation masks for various environments like agricultural crop rows, off-road terrains and urban roads. Unlike other popular annotation tools that requires about 40 seconds to annotate an image for semantic segmentation in a typical navigation task, Spannotation achieves similar result in about 6.03 seconds. The tools utility was validated through the utilization of its generated masks to train a U-Net model which achieved a validation accuracy of 98.27% and mean Intersection Over Union (mIOU) of 96.66%. The accessibility, simple annotation process and no-cost features have all contributed to the adoption of Spannotation evident from its download count of 2098 (as of February 25, 2024) since its launch. Future enhancements of Spannotation aim to broaden its application to complex navigation scenarios and incorporate additional automation functionalities. Given its increasing popularity and promising potential, Spannotation stands as a valuable resource in autonomous navigation and semantic segmentation. For detailed information and access to Spannotation, readers are encouraged to visit the project's GitHub repository at https://github.com/sof-danny/spannotation|Spannotation \u662f\u4e00\u6b3e\u5f00\u6e90\u7528\u6237\u53cb\u597d\u5de5\u5177\uff0c\u4e13\u4e3a\u8bed\u4e49\u5206\u5272\u7684\u56fe\u50cf\u6ce8\u91ca\u800c\u5f00\u53d1\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u4e2d\u3002\u672c\u7814\u7a76\u5bf9 Spannotation \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e3a\u519c\u4f5c\u7269\u884c\u3001\u8d8a\u91ce\u5730\u5f62\u548c\u57ce\u5e02\u9053\u8def\u7b49\u5404\u79cd\u73af\u5883\u751f\u6210\u51c6\u786e\u5206\u5272\u63a9\u6a21\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4e0e\u5176\u4ed6\u6d41\u884c\u7684\u6ce8\u91ca\u5de5\u5177\u5728\u5178\u578b\u5bfc\u822a\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u7ea6 40 \u79d2\u6765\u6ce8\u91ca\u56fe\u50cf\u4ee5\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u4e0d\u540c\uff0cSpannotation \u5728\u5927\u7ea6 6.03 \u79d2\u5185\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u7684\u7ed3\u679c\u3002\u8be5\u5de5\u5177\u5b9e\u7528\u7a0b\u5e8f\u901a\u8fc7\u5229\u7528\u5176\u751f\u6210\u7684\u63a9\u7801\u6765\u8bad\u7ec3 U-Net \u6a21\u578b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u7684\u9a8c\u8bc1\u51c6\u786e\u5ea6\u4e3a 98.27%\uff0c\u5e73\u5747\u4ea4\u96c6\u6bd4 (mIOU) \u4e3a 96.66%\u3002\u53ef\u8bbf\u95ee\u6027\u3001\u7b80\u5355\u7684\u6ce8\u91ca\u8fc7\u7a0b\u548c\u514d\u8d39\u529f\u80fd\u90fd\u6709\u52a9\u4e8e Spannotation \u7684\u91c7\u7528\uff0c\u4ece\u5176\u63a8\u51fa\u4ee5\u6765\u7684\u4e0b\u8f7d\u91cf\u4e3a 2098 \u6b21\uff08\u622a\u81f3 2024 \u5e74 2 \u6708 25 \u65e5\uff09\u53ef\u89c1\u4e00\u6591\u3002 Spannotation \u7684\u672a\u6765\u589e\u5f3a\u65e8\u5728\u5c06\u5176\u5e94\u7528\u8303\u56f4\u6269\u5927\u5230\u590d\u6742\u7684\u5bfc\u822a\u573a\u666f\uff0c\u5e76\u7eb3\u5165\u989d\u5916\u7684\u81ea\u52a8\u5316\u529f\u80fd\u3002\u9274\u4e8e\u5176\u65e5\u76ca\u666e\u53ca\u548c\u5e7f\u9614\u7684\u6f5c\u529b\uff0cSpannotation \u6210\u4e3a\u81ea\u4e3b\u5bfc\u822a\u548c\u8bed\u4e49\u5206\u5272\u9886\u57df\u7684\u5b9d\u8d35\u8d44\u6e90\u3002\u6709\u5173 Spannotation \u7684\u8be6\u7ec6\u4fe1\u606f\u548c\u8bbf\u95ee\u6743\u9650\uff0c\u9f13\u52b1\u8bfb\u8005\u8bbf\u95ee\u8be5\u9879\u76ee\u7684 GitHub \u5b58\u50a8\u5e93\uff1ahttps://github.com/sof-danny/spannotation|[2402.18084v1](http://arxiv.org/pdf/2402.18084v1)|null|\n", "2402.18032": "|**2024-02-28**|**Human Shape and Clothing Estimation**|\u4eba\u4f53\u5f62\u72b6\u548c\u670d\u88c5\u4f30\u8ba1|Aayush Gupta, Aditya Gulati, Himanshu, Lakshya LNU|Human shape and clothing estimation has gained significant prominence in various domains, including online shopping, fashion retail, augmented reality (AR), virtual reality (VR), and gaming. The visual representation of human shape and clothing has become a focal point for computer vision researchers in recent years. This paper presents a comprehensive survey of the major works in the field, focusing on four key aspects: human shape estimation, fashion generation, landmark detection, and attribute recognition. For each of these tasks, the survey paper examines recent advancements, discusses their strengths and limitations, and qualitative differences in approaches and outcomes. By exploring the latest developments in human shape and clothing estimation, this survey aims to provide a comprehensive understanding of the field and inspire future research in this rapidly evolving domain.|\u4eba\u4f53\u5f62\u72b6\u548c\u670d\u88c5\u4f30\u8ba1\u5728\u5728\u7ebf\u8d2d\u7269\u3001\u65f6\u5c1a\u96f6\u552e\u3001\u589e\u5f3a\u73b0\u5b9e (AR)\u3001\u865a\u62df\u73b0\u5b9e (VR) \u548c\u6e38\u620f\u7b49\u5404\u4e2a\u9886\u57df\u90fd\u5f97\u5230\u4e86\u663e\u7740\u7684\u91cd\u89c6\u3002\u8fd1\u5e74\u6765\uff0c\u4eba\u4f53\u5f62\u72b6\u548c\u670d\u88c5\u7684\u89c6\u89c9\u8868\u793a\u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4eba\u5458\u5173\u6ce8\u7684\u7126\u70b9\u3002\u672c\u6587\u5bf9\u8be5\u9886\u57df\u7684\u4e3b\u8981\u5de5\u4f5c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u4eba\u4f53\u5f62\u72b6\u4f30\u8ba1\u3001\u65f6\u5c1a\u751f\u6210\u3001\u5730\u6807\u68c0\u6d4b\u548c\u5c5e\u6027\u8bc6\u522b\u3002\u5bf9\u4e8e\u6bcf\u4e00\u9879\u4efb\u52a1\uff0c\u8c03\u67e5\u8bba\u6587\u90fd\u68c0\u67e5\u4e86\u6700\u8fd1\u7684\u8fdb\u5c55\uff0c\u8ba8\u8bba\u4e86\u5b83\u4eec\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u65b9\u6cd5\u548c\u7ed3\u679c\u7684\u8d28\u91cf\u5dee\u5f02\u3002\u901a\u8fc7\u63a2\u7d22\u4eba\u4f53\u5f62\u72b6\u548c\u670d\u88c5\u4f30\u8ba1\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u672c\u6b21\u8c03\u67e5\u65e8\u5728\u63d0\u4f9b\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u4e86\u89e3\uff0c\u5e76\u542f\u53d1\u8be5\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002|[2402.18032v1](http://arxiv.org/pdf/2402.18032v1)|null|\n", "2402.17987": "|**2024-02-28**|**Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach**|\u98de\u884c\u5668\u591a\u57fa\u5730\u96f7\u8fbe RCS \u7b7e\u540d\u8bc6\u522b\uff1a\u8d1d\u53f6\u65af\u878d\u5408\u65b9\u6cd5|Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz Erdogmus, Tales Imbiriba|Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven drones, correlating target aspect angles to Radar Cross Section (RCS) measurements in an anechoic chamber. Comparing against single radar Automated Target Recognition (ATR) systems and suboptimal fusion methods, our empirical results demonstrate that the OBF method integrated with RBC significantly enhances classification accuracy compared to other fusion methods and single radar configurations.|\u65e0\u4eba\u673a (UAV) \u7684\u96f7\u8fbe\u81ea\u52a8\u76ee\u6807\u8bc6\u522b (RATR) \u6d89\u53ca\u53d1\u5c04\u7535\u78c1\u6ce2 (EMW) \u5e76\u6839\u636e\u63a5\u6536\u5230\u7684\u96f7\u8fbe\u56de\u6ce2\u6267\u884c\u76ee\u6807\u7c7b\u578b\u8bc6\u522b\uff0c\u8fd9\u5bf9\u4e8e\u56fd\u9632\u548c\u822a\u7a7a\u822a\u5929\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5148\u524d\u7684\u7814\u7a76\u5f3a\u8c03\u4e86 RATR \u4e2d\u591a\u57fa\u5730\u96f7\u8fbe\u914d\u7f6e\u76f8\u5bf9\u4e8e\u5355\u57fa\u5730\u96f7\u8fbe\u914d\u7f6e\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u591a\u57fa\u5730\u96f7\u8fbe\u914d\u7f6e\u4e2d\u7684\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u5728\u6982\u7387\u4e0a\u4e0d\u7406\u60f3\u5730\u7ec4\u5408\u6765\u81ea\u5404\u4e2a\u96f7\u8fbe\u7684\u5206\u7c7b\u5411\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u8d1d\u53f6\u65af RATR \u6846\u67b6\uff0c\u91c7\u7528\u6700\u4f73\u8d1d\u53f6\u65af\u878d\u5408 (OBF) \u6765\u805a\u5408\u6765\u81ea\u591a\u4e2a\u96f7\u8fbe\u7684\u5206\u7c7b\u6982\u7387\u5411\u91cf\u3002 OBF \u57fa\u4e8e\u9884\u671f\u7684 0-1 \u635f\u5931\uff0c\u66f4\u65b0\u76ee\u6807\u65e0\u4eba\u673a\u7c7b\u578b\u7684\u9012\u5f52\u8d1d\u53f6\u65af\u5206\u7c7b (RBC) \u540e\u9a8c\u5206\u5e03\uff0c\u4ee5\u591a\u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u5386\u53f2\u89c2\u5bdf\u7ed3\u679c\u4e3a\u6761\u4ef6\u3002\u6211\u4eec\u4f7f\u7528\u4e03\u67b6\u65e0\u4eba\u673a\u7684\u6a21\u62df\u968f\u673a\u6e38\u8d70\u8f68\u8ff9\u6765\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u65b9\u4f4d\u89d2\u4e0e\u6d88\u58f0\u5ba4\u4e2d\u7684\u96f7\u8fbe\u622a\u9762\uff08RCS\uff09\u6d4b\u91cf\u76f8\u5173\u8054\u3002\u4e0e\u5355\u96f7\u8fbe\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\uff08ATR\uff09\u7cfb\u7edf\u548c\u6b21\u4f18\u878d\u5408\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u878d\u5408\u65b9\u6cd5\u548c\u5355\u96f7\u8fbe\u914d\u7f6e\u76f8\u6bd4\uff0c\u4e0e RBC \u96c6\u6210\u7684 OBF \u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002|[2402.17987v1](http://arxiv.org/pdf/2402.17987v1)|null|\n", "2402.17976": "|**2024-02-28**|**Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks**|\u901a\u8fc7\u8f85\u52a9\u5bf9\u6297\u6027\u9632\u5fa1\u7f51\u7edc\u589e\u5f3a\u8ddf\u8e2a\u9c81\u68d2\u6027|Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou|Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. These attack methods have garnered considerable attention from researchers in recent years. However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking. To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process. DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments. We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability. Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead. We will make our code publicly available soon.|\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\u4e2d\u7684\u5bf9\u6297\u6027\u653b\u51fb\u901a\u8fc7\u5728\u56fe\u50cf\u4e2d\u5f15\u5165\u96be\u4ee5\u5bdf\u89c9\u7684\u6270\u52a8\uff0c\u663e\u7740\u964d\u4f4e\u4e86\u9ad8\u7ea7\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\u3002\u8fd1\u5e74\u6765\uff0c\u8fd9\u4e9b\u653b\u51fb\u65b9\u6cd5\u5f15\u8d77\u4e86\u7814\u7a76\u4eba\u5458\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u76ee\u524d\u4ecd\u7136\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u8bbe\u8ba1\u5bf9\u6297\u6027\u9632\u5fa1\u65b9\u6cd5\u7684\u7814\u7a76\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DuaLossDef \u7684\u6709\u6548\u9644\u52a0\u9884\u5904\u7406\u7f51\u7edc\uff0c\u5b83\u53ef\u4ee5\u6d88\u9664\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u6297\u6027\u6270\u52a8\u3002 DuaLossDef \u90e8\u7f72\u5728\u8ddf\u8e2a\u5668\u7684\u641c\u7d22\u5206\u652f\u6216\u6a21\u677f\u5206\u652f\u4e4b\u524d\uff0c\u4ee5\u5c06\u9632\u5fa1\u53d8\u6362\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u4e0e\u5176\u4ed6\u89c6\u89c9\u8ddf\u8e2a\u5668\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4efb\u4f55\u53c2\u6570\u8c03\u6574\u3002\u6211\u4eec\u4f7f\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u6765\u8bad\u7ec3 DuaLossDef\uff0c\u7279\u522b\u662f\u4f7f\u7528 Dua-Loss \u6765\u751f\u6210\u5bf9\u6297\u6027\u6837\u672c\uff0c\u540c\u65f6\u653b\u51fb\u8ddf\u8e2a\u5668\u7684\u5206\u7c7b\u548c\u56de\u5f52\u5206\u652f\u3002\u5728 OTB100\u3001LaSOT \u548c VOT2018 \u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDuaLossDef \u5728\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u653b\u51fb\u573a\u666f\u4e2d\u90fd\u9488\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u9632\u5fa1\u9c81\u68d2\u6027\u3002\u800c\u4e14\uff0c\u5f53\u5c06\u9632\u5fa1\u7f51\u7edc\u8f6c\u79fb\u5230\u5176\u4ed6\u8ddf\u8e2a\u5668\u65f6\uff0c\u5b83\u8868\u73b0\u51fa\u53ef\u9760\u7684\u53ef\u8f6c\u79fb\u6027\u3002\u6700\u540e\uff0cDuaLossDef \u7684\u5904\u7406\u65f6\u95f4\u9ad8\u8fbe 5ms/\u5e27\uff0c\u5141\u8bb8\u4e0e\u73b0\u6709\u9ad8\u901f\u8ddf\u8e2a\u5668\u65e0\u7f1d\u96c6\u6210\uff0c\u800c\u4e0d\u4f1a\u5f15\u5165\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\u3002\u6211\u4eec\u5c06\u5f88\u5feb\u516c\u5f00\u6211\u4eec\u7684\u4ee3\u7801\u3002|[2402.17976v1](http://arxiv.org/pdf/2402.17976v1)|null|\n", "2402.17972": "|**2024-02-28**|**From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments**|\u4ece\u6cdb\u5316\u5230\u7cbe\u786e\uff1a\u63a2\u7d22 SAM \u5728\u624b\u672f\u73af\u5883\u4e2d\u7684\u5de5\u5177\u5206\u5272|Kanyifeechukwu J. Oguine, Roger D. Soberanis-Mukul, Nathan Drenkow, Mathias Unberath|Purpose: Accurate tool segmentation is essential in computer-aided procedures. However, this task conveys challenges due to artifacts' presence and the limited training data in medical scenarios. Methods that generalize to unseen data represent an interesting venue, where zero-shot segmentation presents an option to account for data limitation. Initial exploratory works with the Segment Anything Model (SAM) show that bounding-box-based prompting presents notable zero-short generalization. However, point-based prompting leads to a degraded performance that further deteriorates under image corruption. We argue that SAM drastically over-segment images with high corruption levels, resulting in degraded performance when only a single segmentation mask is considered, while the combination of the masks overlapping the object of interest generates an accurate prediction. Method: We use SAM to generate the over-segmented prediction of endoscopic frames. Then, we employ the ground-truth tool mask to analyze the results of SAM when the best single mask is selected as prediction and when all the individual masks overlapping the object of interest are combined to obtain the final predicted mask. We analyze the Endovis18 and Endovis17 instrument segmentation datasets using synthetic corruptions of various strengths and an In-House dataset featuring counterfactually created real-world corruptions. Results: Combining the over-segmented masks contributes to improvements in the IoU. Furthermore, selecting the best single segmentation presents a competitive IoU score for clean images. Conclusions: Combined SAM predictions present improved results and robustness up to a certain corruption level. However, appropriate prompting strategies are fundamental for implementing these models in the medical domain.|\u76ee\u7684\uff1a\u51c6\u786e\u7684\u5de5\u5177\u5206\u5272\u5bf9\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u7a0b\u5e8f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u533b\u7597\u573a\u666f\u4e2d\u4f2a\u5f71\u7684\u5b58\u5728\u548c\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u9879\u4efb\u52a1\u5e26\u6765\u4e86\u6311\u6218\u3002\u63a8\u5e7f\u5230\u770b\u4e0d\u89c1\u7684\u6570\u636e\u7684\u65b9\u6cd5\u4ee3\u8868\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u573a\u6240\uff0c\u5176\u4e2d\u96f6\u6837\u672c\u5206\u5272\u63d0\u4f9b\u4e86\u89e3\u51b3\u6570\u636e\u9650\u5236\u7684\u9009\u9879\u3002\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM) \u7684\u521d\u6b65\u63a2\u7d22\u6027\u5de5\u4f5c\u8868\u660e\uff0c\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u63d0\u793a\u5448\u73b0\u51fa\u663e\u7740\u7684\u96f6\u77ed\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u70b9\u7684\u63d0\u793a\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u56fe\u50cf\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u6076\u5316\u3002\u6211\u4eec\u8ba4\u4e3a\uff0cSAM \u4e25\u91cd\u8fc7\u5ea6\u5206\u5272\u5177\u6709\u9ad8\u635f\u574f\u7ea7\u522b\u7684\u56fe\u50cf\uff0c\u5bfc\u81f4\u4ec5\u8003\u8651\u5355\u4e2a\u5206\u5272\u63a9\u6a21\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4e0e\u611f\u5174\u8da3\u5bf9\u8c61\u91cd\u53e0\u7684\u63a9\u6a21\u7ec4\u5408\u4f1a\u751f\u6210\u51c6\u786e\u7684\u9884\u6d4b\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u4f7f\u7528 SAM \u751f\u6210\u5185\u7aa5\u955c\u5e27\u7684\u8fc7\u5206\u5272\u9884\u6d4b\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528 ground-truth \u5de5\u5177 mask \u6765\u5206\u6790\u5f53\u9009\u62e9\u6700\u4f73\u5355\u4e2a mask \u4f5c\u4e3a\u9884\u6d4b\u65f6\u4ee5\u53ca\u5f53\u4e0e\u611f\u5174\u8da3\u5bf9\u8c61\u91cd\u53e0\u7684\u6240\u6709\u5355\u72ec mask \u7ec4\u5408\u4ee5\u83b7\u5f97\u6700\u7ec8\u9884\u6d4b mask \u65f6 SAM \u7684\u7ed3\u679c\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u5f3a\u5ea6\u7684\u5408\u6210\u635f\u574f\u548c\u5177\u6709\u53cd\u4e8b\u5b9e\u521b\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u635f\u574f\u7684\u5185\u90e8\u6570\u636e\u96c6\u6765\u5206\u6790 Endovis18 \u548c Endovis17 \u4eea\u5668\u5206\u5272\u6570\u636e\u96c6\u3002\u7ed3\u679c\uff1a\u7ed3\u5408\u8fc7\u5ea6\u5206\u5272\u7684\u63a9\u6a21\u6709\u52a9\u4e8e\u6539\u5584 IoU\u3002\u6b64\u5916\uff0c\u9009\u62e9\u6700\u4f73\u7684\u5355\u4e00\u5206\u5272\u53ef\u4ee5\u4e3a\u5e72\u51c0\u56fe\u50cf\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684 IoU \u5206\u6570\u3002\u7ed3\u8bba\uff1a\u7ec4\u5408 SAM \u9884\u6d4b\u5728\u4e00\u5b9a\u7684\u635f\u574f\u6c34\u5e73\u4e0b\u5448\u73b0\u51fa\u6539\u8fdb\u7684\u7ed3\u679c\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u9002\u5f53\u7684\u63d0\u793a\u7b56\u7565\u662f\u5728\u533b\u5b66\u9886\u57df\u5b9e\u65bd\u8fd9\u4e9b\u6a21\u578b\u7684\u57fa\u7840\u3002|[2402.17972v1](http://arxiv.org/pdf/2402.17972v1)|null|\n", "2402.17960": "|**2024-02-28**|**Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping**|\u5229\u7528\u7a00\u758f\u6570\u636e\u8fdb\u884c\u5feb\u901f\u9ad8\u5149\u8c31\u5149\u70ed\u4e2d\u7ea2\u5916\u5149\u8c31\u6210\u50cf\uff0c\u7528\u4e8e\u5987\u79d1\u764c\u75c7\u7ec4\u7ec7\u4e9a\u578b\u5206\u6790|Reza Reihanisaransari, Chalapathi Charan Gajjela, Xinyu Wu, Ragib Ishrak, Sara Corvigno, Yanping Zhong, Jinsong Liu, Anil K. Sood, David Mayerich, Sebastian Berisha, et.al.|Ovarian cancer detection has traditionally relied on a multi-step process that includes biopsy, tissue staining, and morphological analysis by experienced pathologists. While widely practiced, this conventional approach suffers from several drawbacks: it is qualitative, time-intensive, and heavily dependent on the quality of staining. Mid-infrared (MIR) hyperspectral photothermal imaging is a label-free, biochemically quantitative technology that, when combined with machine learning algorithms, can eliminate the need for staining and provide quantitative results comparable to traditional histology. However, this technology is slow. This work presents a novel approach to MIR photothermal imaging that enhances its speed by an order of magnitude. Our method significantly accelerates data collection by capturing a combination of high-resolution and interleaved, lower-resolution infrared band images and applying computational techniques for data interpolation. We effectively minimize data collection requirements by leveraging sparse data acquisition and employing curvelet-based reconstruction algorithms. This method enables the reconstruction of high-quality, high-resolution images from undersampled datasets and achieving a 10X improvement in data acquisition time. We assessed the performance of our sparse imaging methodology using a variety of quantitative metrics, including mean squared error (MSE), structural similarity index (SSIM), and tissue subtype classification accuracies, employing both random forest and convolutional neural network (CNN) models, accompanied by ROC curves. Our statistically robust analysis, based on data from 100 ovarian cancer patient samples and over 65 million data points, demonstrates the method's capability to produce superior image quality and accurately distinguish between different gynecological tissue types with segmentation accuracy exceeding 95%.|\u4f20\u7edf\u4e0a\uff0c\u5375\u5de2\u764c\u68c0\u6d4b\u4f9d\u8d56\u4e8e\u591a\u6b65\u9aa4\u8fc7\u7a0b\uff0c\u5305\u62ec\u7531\u7ecf\u9a8c\u4e30\u5bcc\u7684\u75c5\u7406\u5b66\u5bb6\u8fdb\u884c\u7684\u6d3b\u68c0\u3001\u7ec4\u7ec7\u67d3\u8272\u548c\u5f62\u6001\u5b66\u5206\u6790\u3002\u867d\u7136\u8fd9\u79cd\u4f20\u7edf\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u51e0\u4e2a\u7f3a\u70b9\uff1a\u5b83\u662f\u5b9a\u6027\u7684\u3001\u8017\u65f6\u7684\uff0c\u5e76\u4e14\u4e25\u91cd\u4f9d\u8d56\u4e8e\u67d3\u8272\u7684\u8d28\u91cf\u3002\u4e2d\u7ea2\u5916\uff08MIR\uff09\u9ad8\u5149\u8c31\u5149\u70ed\u6210\u50cf\u662f\u4e00\u79cd\u65e0\u6807\u8bb0\u7684\u751f\u5316\u5b9a\u91cf\u6280\u672f\uff0c\u4e0e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6d88\u9664\u67d3\u8272\u7684\u9700\u8981\uff0c\u5e76\u63d0\u4f9b\u4e0e\u4f20\u7edf\u7ec4\u7ec7\u5b66\u76f8\u5f53\u7684\u5b9a\u91cf\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u9879\u6280\u672f\u8fdb\u5c55\u7f13\u6162\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2d\u7ea2\u5916\u5149\u70ed\u6210\u50cf\u65b9\u6cd5\uff0c\u5c06\u5176\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u548c\u4ea4\u9519\u7684\u4f4e\u5206\u8fa8\u7387\u7ea2\u5916\u6ce2\u6bb5\u56fe\u50cf\u7684\u7ec4\u5408\u5e76\u5e94\u7528\u6570\u636e\u63d2\u503c\u8ba1\u7b97\u6280\u672f\uff0c\u663e\u7740\u52a0\u901f\u4e86\u6570\u636e\u6536\u96c6\u3002\u6211\u4eec\u901a\u8fc7\u5229\u7528\u7a00\u758f\u6570\u636e\u91c7\u96c6\u548c\u91c7\u7528\u57fa\u4e8e\u66f2\u7ebf\u7684\u91cd\u5efa\u7b97\u6cd5\uff0c\u6709\u6548\u5730\u6700\u5c0f\u5316\u6570\u636e\u6536\u96c6\u8981\u6c42\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6b20\u91c7\u6837\u6570\u636e\u96c6\u91cd\u5efa\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u5c06\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u7f29\u77ed 10 \u500d\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u7a00\u758f\u6210\u50cf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5747\u65b9\u8bef\u5dee (MSE)\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570 (SSIM) \u548c\u7ec4\u7ec7\u4e9a\u578b\u5206\u7c7b\u7cbe\u5ea6\uff0c\u540c\u65f6\u91c7\u7528\u968f\u673a\u68ee\u6797\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u6a21\u578b\uff0c\u5e76\u9644\u6709ROC\u66f2\u7ebf\u3002\u6211\u4eec\u57fa\u4e8e 100 \u4e2a\u5375\u5de2\u764c\u60a3\u8005\u6837\u672c\u548c\u8d85\u8fc7 6500 \u4e07\u4e2a\u6570\u636e\u70b9\u7684\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\u7a33\u5065\u5206\u6790\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u5353\u8d8a\u7684\u56fe\u50cf\u8d28\u91cf\u5e76\u51c6\u786e\u533a\u5206\u4e0d\u540c\u7684\u5987\u79d1\u7ec4\u7ec7\u7c7b\u578b\uff0c\u5206\u5272\u7cbe\u5ea6\u8d85\u8fc7 95%\u3002|[2402.17960v1](http://arxiv.org/pdf/2402.17960v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.18277": "|**2024-02-28**|**Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing**|\u7528\u4e8e\u591a\u5149\u6e90\u767d\u5e73\u8861\u7684\u5173\u6ce8\u7167\u660e\u5206\u89e3\u6a21\u578b|Dongyoung Kim, Jinwoo Kim, Junsang Yu, Seon Joo Kim|White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.|\u8bb8\u591a\u5546\u7528\u76f8\u673a\u4e2d\u7684\u767d\u5e73\u8861 (WB) \u7b97\u6cd5\u5047\u8bbe\u5355\u4e00\u4e14\u5747\u5300\u7684\u7167\u660e\uff0c\u5f53\u573a\u666f\u4e2d\u5b58\u5728\u5177\u6709\u4e0d\u540c\u8272\u5ea6\u7684\u591a\u4e2a\u5149\u6e90\u65f6\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u826f\u7ed3\u679c\u3002\u5148\u524d\u5bf9\u591a\u5149\u6e90\u767d\u5e73\u8861\u7684\u7814\u7a76\u901a\u5e38\u662f\u5728\u50cf\u7d20\u7ea7\u522b\u9884\u6d4b\u7167\u660e\uff0c\u800c\u6ca1\u6709\u5b8c\u5168\u638c\u63e1\u573a\u666f\u7684\u5b9e\u9645\u7167\u660e\u6761\u4ef6\uff0c\u5305\u62ec\u5149\u6e90\u7684\u6570\u91cf\u548c\u989c\u8272\u3002\u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e0d\u81ea\u7136\u7684\u7ed3\u679c\uff0c\u7f3a\u4e4f\u6574\u4f53\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u69fd\u4f4d\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u767d\u5e73\u8861\u6a21\u578b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u69fd\u4f4d\u8d1f\u8d23\u4ee3\u8868\u5404\u4e2a\u5149\u6e90\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u5404\u4e2a\u5149\u6e90\u7684\u8272\u5ea6\u548c\u6743\u91cd\u56fe\uff0c\u7136\u540e\u5c06\u5176\u878d\u5408\u4ee5\u7ec4\u6210\u6700\u7ec8\u7684\u7167\u660e\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8d28\u5fc3\u5339\u914d\u635f\u5931\uff0c\u5b83\u6839\u636e\u989c\u8272\u8303\u56f4\u8c03\u8282\u6bcf\u4e2a\u69fd\u7684\u6fc0\u6d3b\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u4ee5\u66f4\u6709\u6548\u5730\u5206\u79bb\u7167\u660e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5355\u5149\u6e90\u548c\u591a\u5149\u6e90 WB \u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8fd8\u63d0\u4f9b\u4e86\u9644\u52a0\u4fe1\u606f\uff0c\u4f8b\u5982\u573a\u666f\u4e2d\u7684\u5149\u6e90\u6570\u91cf\u53ca\u5176\u8272\u5ea6\u3002\u6b64\u529f\u80fd\u5141\u8bb8\u8fdb\u884c\u7167\u660e\u7f16\u8f91\uff0c\u8fd9\u662f\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u7684\u5e94\u7528\u3002|[2402.18277v1](http://arxiv.org/pdf/2402.18277v1)|null|\n", "2402.18175": "|**2024-02-28**|**Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus**|\u7528\u4e8e\u50cf\u5dee\u611f\u77e5\u6563\u7126\u6df1\u5ea6\u7684\u81ea\u76d1\u7763\u7a7a\u95f4\u53d8\u5f02 PSF \u4f30\u8ba1|Zhuofeng Wu, Yusuke Monno, Masatoshi Okutomi|In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u50cf\u5dee\u611f\u77e5\u79bb\u7126\u6df1\u5ea6\uff08DfD\uff09\u7684\u4efb\u52a1\uff0c\u5b83\u8003\u8651\u4e86\u771f\u5b9e\u76f8\u673a\u7684\u7a7a\u95f4\u53d8\u5316\u70b9\u6269\u6563\u51fd\u6570\uff08PSF\uff09\u3002\u4e3a\u4e86\u6709\u6548\u5730\u83b7\u5f97\u771f\u5b9e\u76f8\u673a\u7684\u7a7a\u95f4\u53d8\u5316 PSF\uff0c\u800c\u4e0d\u9700\u8981\u4efb\u4f55\u5730\u9762\u771f\u5b9e PSF\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e00\u5bf9\u771f\u5b9e\u7684\u6e05\u6670\u548c\u6a21\u7cca\u56fe\u50cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u6539\u53d8\u76f8\u673a\u7684\u5149\u5708\u8bbe\u7f6e\u6765\u8f7b\u677e\u6355\u83b7\u8fd9\u4e9b\u56fe\u50cf\u3002\u76f8\u673a\u3002\u5728\u6211\u4eec\u7684 PSF \u4f30\u8ba1\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u65cb\u8f6c\u5bf9\u79f0 PSF \u5e76\u5f15\u5165\u6781\u5750\u6807\u7cfb\u4ee5\u66f4\u51c6\u786e\u5730\u5b66\u4e60 PSF \u4f30\u8ba1\u7f51\u7edc\u3002\u6211\u4eec\u8fd8\u5904\u7406\u771f\u5b9e DfD \u60c5\u51b5\u4e0b\u53d1\u751f\u7684\u7126\u70b9\u547c\u5438\u73b0\u8c61\u3002\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728 PSF \u4f30\u8ba1\u548c\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.18175v1](http://arxiv.org/pdf/2402.18175v1)|null|\n", "2402.18172": "|**2024-02-28**|**NiteDR: Nighttime Image De-Raining with Cross-View Sensor Cooperative Learning for Dynamic Driving Scenes**|NiteDR\uff1a\u591c\u95f4\u56fe\u50cf\u9664\u96e8\u4e0e\u4ea4\u53c9\u89c6\u89d2\u4f20\u611f\u5668\u534f\u4f5c\u5b66\u4e60\u52a8\u6001\u9a7e\u9a76\u573a\u666f|Cidan Shi, Lihuang Fang, Han Wu, Xiaoyu Xian, Yukai Shi, Liang Lin|In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation. Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility. Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios. Additionally, driving assistance systems suffer from reduced capabilities in capturing and discerning the surrounding environment, posing a threat to driving safety. Single-view information captured by single-modal sensors cannot comprehensively depict the entire scene. To address these challenges, we developed an image de-raining framework tailored for rainy nighttime driving scenes. It aims to remove rain artifacts, enrich scene representation, and restore useful information. Specifically, we introduce cooperative learning between visible and infrared images captured by different sensors. By cross-view fusion of these multi-source data, the scene within the images gains richer texture details and enhanced contrast. We constructed an information cleaning module called CleanNet as the first stage of our framework. Moreover, we designed an information fusion module called FusionNet as the second stage to fuse the clean visible images with infrared images. Using this stage-by-stage learning strategy, we obtain de-rained fusion images with higher quality and better visual perception. Extensive experiments demonstrate the effectiveness of our proposed Cross-View Cooperative Learning (CVCL) in adverse driving scenarios in low-light rainy environments. The proposed approach addresses the gap in the utilization of existing rain removal algorithms in specific low-light conditions.|\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u5ba4\u5916\u6210\u50cf\u7cfb\u7edf\u7ecf\u5e38\u53d7\u5230\u96e8\u6c34\u9000\u5316\u7b49\u5e72\u6270\u7684\u5f71\u54cd\u3002\u5c24\u5176\u662f\u5728\u591c\u95f4\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u7167\u660e\u4e0d\u8db3\u4e14\u4e0d\u5747\u5300\uff0c\u4f7f\u573a\u666f\u7b3c\u7f69\u5728\u9ed1\u6697\u4e2d\uff0c\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u548c\u53ef\u89c6\u6027\u4e0b\u964d\u3002\u5c24\u5176\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0cRGB\u4f20\u611f\u5668\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u5728\u8fd9\u79cd\u6076\u52a3\u7684\u573a\u666f\u4e0b\u4f1a\u51fa\u73b0\u6025\u5267\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u6355\u6349\u548c\u8bc6\u522b\u5468\u56f4\u73af\u5883\u7684\u80fd\u529b\u4e0b\u964d\uff0c\u5bf9\u9a7e\u9a76\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u5355\u6a21\u6001\u4f20\u611f\u5668\u6355\u83b7\u7684\u5355\u89c6\u56fe\u4fe1\u606f\u65e0\u6cd5\u5168\u9762\u63cf\u8ff0\u6574\u4e2a\u573a\u666f\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u9488\u5bf9\u591c\u95f4\u96e8\u5929\u9a7e\u9a76\u573a\u666f\u7684\u56fe\u50cf\u53bb\u96e8\u6846\u67b6\u3002\u5b83\u7684\u76ee\u7684\u662f\u6d88\u9664\u96e8\u6c34\u4f2a\u5f71\uff0c\u4e30\u5bcc\u573a\u666f\u8868\u793a\uff0c\u5e76\u6062\u590d\u6709\u7528\u7684\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e0d\u540c\u4f20\u611f\u5668\u6355\u83b7\u7684\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u4e4b\u95f4\u7684\u534f\u4f5c\u5b66\u4e60\u3002\u901a\u8fc7\u8fd9\u4e9b\u591a\u6e90\u6570\u636e\u7684\u8de8\u89c6\u56fe\u878d\u5408\uff0c\u56fe\u50cf\u5185\u7684\u573a\u666f\u83b7\u5f97\u66f4\u4e30\u5bcc\u7684\u7eb9\u7406\u7ec6\u8282\u548c\u589e\u5f3a\u7684\u5bf9\u6bd4\u5ea6\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a CleanNet \u7684\u4fe1\u606f\u6e05\u7406\u6a21\u5757\u4f5c\u4e3a\u6846\u67b6\u7684\u7b2c\u4e00\u9636\u6bb5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3a FusionNet \u7684\u4fe1\u606f\u878d\u5408\u6a21\u5757\u4f5c\u4e3a\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5c06\u5e72\u51c0\u7684\u53ef\u89c1\u5149\u56fe\u50cf\u4e0e\u7ea2\u5916\u56fe\u50cf\u878d\u5408\u3002\u4f7f\u7528\u8fd9\u79cd\u9010\u6b65\u5b66\u4e60\u7b56\u7565\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5177\u6709\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u597d\u89c6\u89c9\u611f\u77e5\u7684\u53bb\u96e8\u878d\u5408\u56fe\u50cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u8de8\u89c6\u56fe\u5408\u4f5c\u5b66\u4e60\uff08CVCL\uff09\u5728\u5f31\u5149\u96e8\u5929\u73af\u5883\u4e2d\u4e0d\u5229\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5728\u7279\u5b9a\u4f4e\u5149\u6761\u4ef6\u4e0b\u5229\u7528\u73b0\u6709\u9664\u96e8\u7b97\u6cd5\u7684\u5dee\u8ddd\u3002|[2402.18172v1](http://arxiv.org/pdf/2402.18172v1)|null|\n", "2402.18102": "|**2024-02-28**|**Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging**|\u88ab\u52a8\u5feb\u7167\u7f16\u7801\u5b54\u5f84\u53cc\u50cf\u7d20 RGB-D \u6210\u50cf|Bhargav Ghanekar, Salman Siddique Khan, Vivek Boominathan, Pranav Sharma, Shreyas Singh, Kaushik Mitra, Ashok Veeraraghavan|Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.|\u65e0\u6e90\u3001\u7d27\u51d1\u3001\u5355\u6b21 3D \u4f20\u611f\u5728\u8bb8\u591a\u5e94\u7528\u9886\u57df\u975e\u5e38\u6709\u7528\uff0c\u4f8b\u5982\u663e\u5fae\u955c\u3001\u533b\u5b66\u200b\u200b\u6210\u50cf\u3001\u624b\u672f\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u53ef\u80fd\u5b58\u5728\u5916\u5f62\u5c3a\u5bf8\u3001\u65f6\u95f4\u548c\u529f\u7387\u9650\u5236\u7684\u5e94\u7528\u9886\u57df\u3002\u5728\u77ed\u6210\u50cf\u8ddd\u79bb\u5185\u3001\u4ee5\u8d85\u7d27\u51d1\u7684\u5916\u5f62\u5c3a\u5bf8\u4ee5\u53ca\u88ab\u52a8\u5feb\u7167\u65b9\u5f0f\u83b7\u53d6 RGB-D \u573a\u666f\u4fe1\u606f\u5177\u6709\u6311\u6218\u6027\u3002\u53cc\u50cf\u7d20 (DP) \u4f20\u611f\u5668\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002 DP \u4f20\u611f\u5668\u4ee5\u4e24\u4e2a\u4ea4\u9519\u7684\u50cf\u7d20\u9635\u5217\u6536\u96c6\u6765\u81ea\u955c\u5934\u4e24\u4e2a\u4e0d\u540c\u534a\u90e8\u7684\u5149\u7ebf\uff0c\u4ece\u800c\u6355\u83b7\u573a\u666f\u7684\u4e24\u4e2a\u7565\u6709\u4e0d\u540c\u7684\u89c6\u56fe\uff0c\u5c31\u50cf\u7acb\u4f53\u76f8\u673a\u7cfb\u7edf\u4e00\u6837\u3002\u7136\u800c\uff0c\u4f7f\u7528 DP \u4f20\u611f\u5668\u6210\u50cf\u610f\u5473\u7740\u6563\u7126\u6a21\u7cca\u5927\u5c0f\u4e0e\u89c6\u56fe\u4e4b\u95f4\u7684\u89c6\u5dee\u6210\u6b63\u6bd4\u3002\u8fd9\u5728\u89c6\u5dee\u4f30\u8ba1\u4e0e\u53bb\u6a21\u7cca\u7cbe\u5ea6\u4e4b\u95f4\u4ea7\u751f\u4e86\u6743\u8861\u3002\u4e3a\u4e86\u6539\u5584\u8fd9\u79cd\u6743\u8861\u6548\u679c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CADS\uff08\u7f16\u7801\u5b54\u5f84\u53cc\u50cf\u7d20\u4f20\u611f\uff09\uff0c\u5176\u4e2d\u6211\u4eec\u5728\u6210\u50cf\u955c\u5934\u4e2d\u4f7f\u7528\u7f16\u7801\u5b54\u5f84\u4ee5\u53caDP\u4f20\u611f\u5668\u3002\u5728\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u5728\u7aef\u5230\u7aef\u4f18\u5316\u8bbe\u7f6e\u4e2d\u5171\u540c\u5b66\u4e60\u6700\u4f73\u7f16\u7801\u6a21\u5f0f\u548c\u91cd\u5efa\u7b97\u6cd5\u3002\u6211\u4eec\u7684 CADS \u6210\u50cf\u7cfb\u7edf\u5728\u5404\u79cd\u5149\u5708\u8bbe\u7f6e\u4e0b\uff0c\u4e0e\u7b80\u5355\u7684 DP \u4f20\u611f\u76f8\u6bd4\uff0c\u5168\u7126\u70b9 (AIF) \u4f30\u8ba1\u7684 PSNR \u63d0\u9ad8\u4e86 1.5dB \u4ee5\u4e0a\uff0c\u6df1\u5ea6\u4f30\u8ba1\u8d28\u91cf\u63d0\u9ad8\u4e86 5-6%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a DSLR \u6444\u5f71\u8bbe\u7f6e\u4ee5\u53ca\u5185\u7aa5\u955c\u548c\u76ae\u80a4\u955c\u5916\u5f62\u6784\u5efa\u4e86\u62df\u8bae\u7684 CADS \u539f\u578b\u3002\u6211\u4eec\u65b0\u9896\u7684\u7f16\u7801\u53cc\u50cf\u7d20\u4f20\u611f\u65b9\u6cd5\u4ee5\u88ab\u52a8\u3001\u5feb\u7167\u548c\u7d27\u51d1\u7684\u65b9\u5f0f\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u51c6\u786e\u7684 RGB-D \u91cd\u5efa\u7ed3\u679c\u3002|[2402.18102v1](http://arxiv.org/pdf/2402.18102v1)|null|\n"}, "LLM": {"2402.18157": "|**2024-02-28**|**From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs**|\u4ece\u603b\u7ed3\u5230\u884c\u52a8\uff1a\u4f7f\u7528\u5f00\u653e\u4e16\u754c API \u589e\u5f3a\u590d\u6742\u4efb\u52a1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b|Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, Hang Xu|The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.|\u4eba\u7c7b\u4e0e\u52a8\u7269\u7684\u533a\u522b\u5728\u4e8e\u4eba\u7c7b\u4f7f\u7528\u548c\u521b\u9020\u5de5\u5177\u7684\u72ec\u7279\u80fd\u529b\u3002\u5de5\u5177\u4f7f\u4eba\u7c7b\u80fd\u591f\u514b\u670d\u751f\u7406\u9650\u5236\uff0c\u4fc3\u8fdb\u521b\u9020\u4f1f\u5927\u7684\u6587\u660e\u3002\u540c\u6837\uff0c\u542f\u7528\u5177\u6709\u5b66\u4e60\u5916\u90e8\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b49\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u5173\u952e\u4e00\u6b65\u3002\u8be5\u9886\u57df\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u91c7\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u6cd5\u5b66\u7855\u58eb\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u3002\u7b2c\u4e00\u79cd\u65b9\u6cd5\u5f3a\u8c03\u6784\u5efa\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u7684\u76f8\u5173\u6570\u636e\u96c6\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u60c5\u5883\u5b66\u4e60\u7b56\u7565\u5145\u5206\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u56fa\u6709\u7684\u63a8\u7406\u80fd\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5de5\u5177\u8c03\u7528\u7ba1\u9053\uff0c\u65e8\u5728\u63a7\u5236\u5927\u91cf\u73b0\u5b9e\u4e16\u754c\u7684 API\u3002\u8be5\u7ba1\u9053\u53cd\u6620\u4e86\u4eba\u5de5\u4efb\u52a1\u89e3\u51b3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u751f\u6d3b\u4e2d\u590d\u6742\u7684\u7528\u6237\u67e5\u8be2\u3002\u5728\u6bcf\u4e00\u6b65\u4e2d\uff0c\u6211\u4eec\u90fd\u4f1a\u6307\u5bfc\u6cd5\u5b66\u7855\u58eb\u603b\u7ed3\u6240\u53d6\u5f97\u7684\u6210\u679c\u5e76\u786e\u5b9a\u4e0b\u4e00\u6b65\u7684\u884c\u52a8\u65b9\u9488\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u7ba1\u9053\u79f0\u4e3a\u201c\u4ece\u6458\u8981\u5230\u884c\u52a8\u201d\uff0c\u7b80\u79f0 Sum2Act\u3002\u6211\u4eec\u5728 ToolBench \u57fa\u51c6\u4e0a\u5bf9 Sum2Act \u7ba1\u9053\u8fdb\u884c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u51fa\u663e\u7740\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u4f18\u4e8e ReAct \u548c DFSDT \u7b49\u65e2\u5b9a\u65b9\u6cd5\u3002\u8fd9\u51f8\u663e\u4e86 Sum2Act \u5728\u589e\u5f3a\u6cd5\u5b66\u7855\u58eb\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2402.18157v1](http://arxiv.org/pdf/2402.18157v1)|null|\n"}, "Transformer": {"2402.18330": "|**2024-02-28**|**Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting**|\u7528\u4e8e\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u70ed\u56fe\u5230 3D \u59ff\u52bf\u63d0\u5347\u7684\u6ce8\u610f\u529b\u4f20\u64ad\u7f51\u7edc|Taeho Kang, Youngki Lee|We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\\% reduction of error in an MPJPE metric. Our source code is available in GitHub.|\u6211\u4eec\u63d0\u51fa\u4e86 EgoTAP\uff0c\u4e00\u79cd\u70ed\u56fe\u5230 3D \u59ff\u6001\u63d0\u5347\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7acb\u4f53\u81ea\u6211\u4e2d\u5fc3 3D \u59ff\u6001\u4f30\u8ba1\u3002\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u76f8\u673a\u89c6\u56fe\u4e2d\u4e25\u91cd\u7684\u81ea\u906e\u6321\u548c\u89c6\u7ebf\u5916\u7684\u80a2\u4f53\u4f7f\u5f97\u51c6\u786e\u7684\u59ff\u52bf\u4f30\u8ba1\u6210\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u91c7\u7528\u8054\u5408\u70ed\u56fe-\u8eab\u4f53\u59ff\u52bf\u7684\u6982\u7387 2D \u8868\u793a\uff0c\u4f46\u70ed\u56fe\u5230 3D \u59ff\u52bf\u7684\u8f6c\u6362\u4ecd\u7136\u662f\u4e00\u4e2a\u4e0d\u51c6\u786e\u7684\u8fc7\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u7f51\u683c ViT \u7f16\u7801\u5668\u548c\u4f20\u64ad\u7f51\u7edc\u7ec4\u6210\u7684\u65b0\u9896\u7684\u70ed\u56fe\u5230 3D \u63d0\u5347\u65b9\u6cd5\u3002 Grid ViT \u7f16\u7801\u5668\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u5c06\u8054\u5408\u70ed\u56fe\u603b\u7ed3\u4e3a\u6709\u6548\u7684\u7279\u5f81\u5d4c\u5165\u3002\u7136\u540e\uff0c\u4f20\u64ad\u7f51\u7edc\u5229\u7528\u9aa8\u9abc\u4fe1\u606f\u6765\u4f30\u8ba1 3D \u59ff\u6001\uff0c\u4ee5\u66f4\u597d\u5730\u4f30\u8ba1\u6a21\u7cca\u5173\u8282\u7684\u4f4d\u7f6e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u663e\u7740\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0cMPJPE \u6307\u6807\u7684\u8bef\u5dee\u51cf\u5c11\u4e86 23.9%\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u4ee5\u5728 GitHub \u4e0a\u627e\u5230\u3002|[2402.18330v1](http://arxiv.org/pdf/2402.18330v1)|null|\n", "2402.18044": "|**2024-02-28**|**SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation**|SFTformer\uff1a\u7528\u4e8e\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u65f6\u7a7a\u76f8\u5173\u89e3\u8026\u53d8\u538b\u5668|Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Fanglong Yao, Xian Sun, Kun Fu|Extrapolating future weather radar echoes from past observations is a complex task vital for precipitation nowcasting. The spatial morphology and temporal evolution of radar echoes exhibit a certain degree of correlation, yet they also possess independent characteristics. {Existing methods learn unified spatial and temporal representations in a highly coupled feature space, emphasizing the correlation between spatial and temporal features but neglecting the explicit modeling of their independent characteristics, which may result in mutual interference between them.} To effectively model the spatiotemporal dynamics of radar echoes, we propose a Spatial-Frequency-Temporal correlation-decoupling Transformer (SFTformer). The model leverages stacked multiple SFT-Blocks to not only mine the correlation of the spatiotemporal dynamics of echo cells but also avoid the mutual interference between the temporal modeling and the spatial morphology refinement by decoupling them. Furthermore, inspired by the practice that weather forecast experts effectively review historical echo evolution to make accurate predictions, SFTfomer incorporates a joint training paradigm for historical echo sequence reconstruction and future echo sequence prediction. Experimental results on the HKO-7 dataset and ChinaNorth-2021 dataset demonstrate the superior performance of SFTfomer in short(1h), mid(2h), and long-term(3h) precipitation nowcasting.|\u6839\u636e\u8fc7\u53bb\u7684\u89c2\u6d4b\u63a8\u65ad\u672a\u6765\u5929\u6c14\u96f7\u8fbe\u56de\u6ce2\u662f\u4e00\u9879\u5bf9\u4e8e\u4e34\u8fd1\u964d\u6c34\u9884\u62a5\u81f3\u5173\u91cd\u8981\u7684\u590d\u6742\u4efb\u52a1\u3002\u96f7\u8fbe\u56de\u6ce2\u7684\u7a7a\u95f4\u5f62\u6001\u548c\u65f6\u95f4\u6f14\u5316\u8868\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e5f\u5177\u6709\u72ec\u7acb\u7684\u7279\u5f81\u3002 {\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u5ea6\u8026\u5408\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7edf\u4e00\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u5f3a\u8c03\u65f6\u7a7a\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u5bf9\u5176\u72ec\u7acb\u7279\u5f81\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5e72\u6270\u3002}\u9488\u5bf9\u96f7\u8fbe\u56de\u6ce2\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4-\u9891\u7387-\u65f6\u95f4\u76f8\u5173\u89e3\u8026\u53d8\u6362\u5668\uff08SFTformer\uff09\u3002\u8be5\u6a21\u578b\u5229\u7528\u5806\u53e0\u7684\u591a\u4e2aSFT-Blocks\u4e0d\u4ec5\u6316\u6398\u56de\u58f0\u5355\u5143\u65f6\u7a7a\u52a8\u6001\u7684\u76f8\u5173\u6027\uff0c\u800c\u4e14\u901a\u8fc7\u89e3\u8026\u907f\u514d\u65f6\u95f4\u5efa\u6a21\u548c\u7a7a\u95f4\u5f62\u6001\u7ec6\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u5e72\u6270\u3002\u6b64\u5916\uff0c\u53d7\u5929\u6c14\u9884\u62a5\u4e13\u5bb6\u6709\u6548\u56de\u987e\u5386\u53f2\u56de\u6ce2\u6f14\u5316\u4ee5\u505a\u51fa\u51c6\u786e\u9884\u6d4b\u7684\u5b9e\u8df5\u7684\u542f\u53d1\uff0cSFTfomer \u7ed3\u5408\u4e86\u5386\u53f2\u56de\u6ce2\u5e8f\u5217\u91cd\u5efa\u548c\u672a\u6765\u56de\u6ce2\u5e8f\u5217\u9884\u6d4b\u7684\u8054\u5408\u8bad\u7ec3\u8303\u4f8b\u3002\u5728HKO-7\u6570\u636e\u96c6\u548cChinaNorth-2021\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSFTfomer\u5728\u77ed\uff081h\uff09\u3001\u4e2d\uff082h\uff09\u548c\u957f\u671f\uff083h\uff09\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002|[2402.18044v1](http://arxiv.org/pdf/2402.18044v1)|null|\n", "2402.18011": "|**2024-02-28**|**Representing 3D sparse map points and lines for camera relocalization**|\u8868\u793a 3D \u7a00\u758f\u5730\u56fe\u70b9\u548c\u7ebf\u4ee5\u8fdb\u884c\u76f8\u673a\u91cd\u65b0\u5b9a\u4f4d|Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee|Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/|\u89c6\u89c9\u5b9a\u4f4d\u548c\u5730\u56fe\u7ed8\u5236\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u96c6\u6210\u70b9\u548c\u7ebf\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u6269\u5c55\u672c\u5730\u5316\u6846\u67b6\u4ee5\u5305\u542b\u989d\u5916\u7684\u6620\u5c04\u7ec4\u4ef6\u7ecf\u5e38\u4f1a\u5bfc\u81f4\u5bf9\u4e13\u7528\u4e8e\u5339\u914d\u4efb\u52a1\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u589e\u52a0\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5b66\u4e60\u8868\u793a 3D \u70b9\u548c\u7ebf\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u5b66\u4e60\u6620\u5c04\u7684\u529b\u91cf\u6765\u5c55\u793a\u9886\u5148\u7684\u59ff\u52bf\u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u5355\u4e2a\u8f6c\u6362\u5668\u5757\u6765\u7f16\u7801\u7ebf\u7279\u5f81\uff0c\u6709\u6548\u5730\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u72ec\u7279\u7684\u70b9\u72b6\u63cf\u8ff0\u7b26\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u70b9\u548c\u7ebf\u63cf\u8ff0\u7b26\u96c6\u89c6\u4e3a\u4e0d\u540c\u4f46\u76f8\u4e92\u5173\u8054\u7684\u7279\u5f81\u96c6\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u56fe\u5c42\u4e2d\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f7f\u7528\u4e24\u4e2a\u7b80\u5355\u7684 MLP \u56de\u5f52 3D \u5730\u56fe\u4e4b\u524d\u6709\u6548\u5730\u7ec6\u5316\u6bcf\u4e2a\u7279\u5f81\u3002\u5728\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u5ba4\u5185\u5b9a\u4f4d\u7ed3\u679c\u5728\u57fa\u4e8e\u70b9\u548c\u7ebf\u8f85\u52a9\u914d\u7f6e\u65b9\u9762\u5747\u8d85\u8fc7\u4e86 Hloc \u548c Limap\u3002\u6b64\u5916\uff0c\u5728\u6237\u5916\u573a\u666f\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u7684\u9886\u5148\u4f18\u52bf\uff0c\u6807\u5fd7\u7740\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u6bd4\u7684\u6700\u663e\u7740\u7684\u589e\u5f3a\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u6e90\u4ee3\u7801\u548c\u6f14\u793a\u89c6\u9891\u5df2\u516c\u5f00\uff1ahttps://thpjp.github.io/pl2map/|[2402.18011v1](http://arxiv.org/pdf/2402.18011v1)|null|\n"}, "3D/CG": {"2402.18553": "|**2024-02-28**|**Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture**|\u9009\u62e9\u9002\u5f53\u7684\u591a\u5149\u8c31\u76f8\u673a\u66dd\u5149\u8bbe\u7f6e\u548c\u8f90\u5c04\u6821\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u578b\u5206\u6790\u548c\u7cbe\u51c6\u519c\u4e1a|Vaishali Swaminathan, J. Alex Thomasson, Robert G. Hardin, Nithya Rajan|Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making. The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough. The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings. This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%). The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure. A simulation of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).|\u6570\u636e\u7684\u8f90\u5c04\u51c6\u786e\u6027\u5bf9\u4e8e\u5b9a\u91cf\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u4e3a\u5efa\u6a21\u548c\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u4e14\u53ef\u91cd\u590d\u7684\u6570\u636e\u3002\u66dd\u5149\u65f6\u95f4\u548c\u589e\u76ca\u8bbe\u7f6e\u5bf9\u591a\u5149\u8c31\u56fe\u50cf\u8f90\u5c04\u7cbe\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u7684\u76ee\u7684\u662f\u786e\u5b9a\u4e0e\u9ed8\u8ba4\u7684\u81ea\u52a8\u66dd\u5149 (AE) \u8bbe\u7f6e\u76f8\u6bd4\uff0c\u5728\u56fe\u50cf\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u91c7\u7528\u56fa\u5b9a\u66dd\u5149 (FE) \u65f6\u95f4\u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u56fe\u50cf\u7684\u8f90\u5c04\u7cbe\u5ea6\u3002\u8fd9\u6d89\u53ca\u91cf\u5316\u81ea\u52a8\u66dd\u5149\u7684\u8bef\u5dee\u5e76\u786e\u5b9a\u7406\u60f3\u7684\u66dd\u5149\u503c\uff0c\u5728\u8be5\u503c\u5185\u8f90\u5c04\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee (MAPE) \u6700\u5c0f (< 5%)\u3002\u7ed3\u679c\u8868\u660e\uff0cFE \u6b63\u5c04\u9a6c\u8d5b\u514b\u6bd4 AE \u6b63\u5c04\u9a6c\u8d5b\u514b\u66f4\u63a5\u8fd1\u5730\u9762\u5b9e\u51b5\uff08\u8f83\u9ad8\u7684 R2 \u548c\u8f83\u4f4e\u7684 MAPE\uff09\u3002\u786e\u5b9a\u4e86\u6355\u6349\u6811\u51a0\u548c\u571f\u58e4\u7269\u4f53\u7684\u7406\u60f3\u66dd\u5149\u8303\u56f4\uff0c\u4e0d\u4f1a\u56e0\u66dd\u5149\u4e0d\u8db3\u800c\u4e22\u5931\u4fe1\u606f\u6216\u56e0\u66dd\u5149\u8fc7\u5ea6\u800c\u5bfc\u81f4\u9971\u548c\u3002 AE \u8bef\u5dee\u6a21\u62df\u8868\u660e\uff0c\u5bf9\u4e8e\u786e\u5b9a\u7684\u7406\u60f3\u8303\u56f4\u5185\u7684\u66dd\u5149\u8bbe\u7f6e\uff0c\u84dd\u8272\u3001\u7eff\u8272\u3001\u7ea2\u8272\u548c\u8fd1\u7ea2\u5916\u6ce2\u6bb5\u7684 MAPE < 5%\uff0c\u7ea2\u8fb9\u6ce2\u6bb5\u7684 MAPE < 7%\uff0c\u5e76\u4e14\u5728\u8d85\u51fa\u7406\u60f3\u66dd\u5149\u4e0a\u9650\u65f6\u5448\u6307\u6570\u589e\u957f\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e0d\u540c\u751f\u957f\u5b63\u8282\u7684\u690d\u88ab\u6307\u6570 (VI) \u9884\u6d4b\u690d\u7269\u603b\u6c2e\u5438\u6536\u91cf (g/plant) \u66f4\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\uff08\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0cR2 > 0.40\uff0cMAPE = 12 \u81f3 14%\uff0cp < 0.05\uff09\uff1a\u4e0e AE \u56fe\u50cf\u7684\u9884\u6d4b\u76f8\u6bd4\uff0c\u4f7f\u7528 FE\uff08\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0cR2 < 0.13\uff0cMAPE = 15 \u81f3 18%\uff0cp >= 0.05\uff09\u3002|[2402.18553v1](http://arxiv.org/pdf/2402.18553v1)|null|\n", "2402.18287": "|**2024-02-28**|**Windowed-FourierMixer: Enhancing Clutter-Free Room Modeling with Fourier Transform**|Windowed-FourierMixer\uff1a\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u589e\u5f3a\u6574\u6d01\u7684\u623f\u95f4\u5efa\u6a21|Bruno Henriques, Benjamin Allaert, Jean-Philippe Vandeborre|With the growing demand for immersive digital applications, the need to understand and reconstruct 3D scenes has significantly increased. In this context, inpainting indoor environments from a single image plays a crucial role in modeling the internal structure of interior spaces as it enables the creation of textured and clutter-free reconstructions. While recent methods have shown significant progress in room modeling, they rely on constraining layout estimators to guide the reconstruction process. These methods are highly dependent on the performance of the structure estimator and its generative ability in heavily occluded environments. In response to these issues, we propose an innovative approach based on a U-Former architecture and a new Windowed-FourierMixer block, resulting in a unified, single-phase network capable of effectively handle human-made periodic structures such as indoor spaces. This new architecture proves advantageous for tasks involving indoor scenes where symmetry is prevalent, allowing the model to effectively capture features such as horizon/ceiling height lines and cuboid-shaped rooms. Experiments show the proposed approach outperforms current state-of-the-art methods on the Structured3D dataset demonstrating superior performance in both quantitative metrics and qualitative results. Code and models will be made publicly available.|\u968f\u7740\u6c89\u6d78\u5f0f\u6570\u5b57\u5e94\u7528\u9700\u6c42\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u7406\u89e3\u548c\u91cd\u5efa 3D \u573a\u666f\u7684\u9700\u6c42\u663e\u7740\u589e\u52a0\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4ece\u5355\u4e2a\u56fe\u50cf\u4fee\u590d\u5ba4\u5185\u73af\u5883\u5728\u5efa\u6a21\u5ba4\u5185\u7a7a\u95f4\u7684\u5185\u90e8\u7ed3\u6784\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u521b\u5efa\u6709\u7eb9\u7406\u4e14\u6574\u6d01\u7684\u91cd\u5efa\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u623f\u95f4\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7ea6\u675f\u5e03\u5c40\u4f30\u8ba1\u5668\u6765\u6307\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7ed3\u6784\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u53ca\u5176\u5728\u4e25\u91cd\u906e\u6321\u73af\u5883\u4e2d\u7684\u751f\u6210\u80fd\u529b\u3002\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e U-Former \u67b6\u6784\u548c\u65b0\u7684 Windowed-FourierMixer \u6a21\u5757\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u5355\u76f8\u7f51\u7edc\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5ba4\u5185\u7a7a\u95f4\u7b49\u4eba\u9020\u5468\u671f\u6027\u7ed3\u6784\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b0\u67b6\u6784\u5bf9\u4e8e\u6d89\u53ca\u5bf9\u79f0\u6027\u666e\u904d\u5b58\u5728\u7684\u5ba4\u5185\u573a\u666f\u7684\u4efb\u52a1\u662f\u6709\u5229\u7684\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u5730\u5e73\u7ebf/\u5929\u82b1\u677f\u9ad8\u5ea6\u7ebf\u548c\u957f\u65b9\u4f53\u5f62\u72b6\u7684\u623f\u95f4\u7b49\u7279\u5f81\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 Structured3D \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u7ed3\u679c\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002|[2402.18287v1](http://arxiv.org/pdf/2402.18287v1)|null|\n", "2402.18146": "|**2024-02-28**|**3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling**|3DSFLabelling\uff1a\u901a\u8fc7\u4f2a\u81ea\u52a8\u6807\u8bb0\u589e\u5f3a 3D \u573a\u666f\u6d41\u4f30\u8ba1|Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, Zhujin Liang, Yi Shan, Dalong Du|Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.|\u4ece LiDAR \u70b9\u4e91\u5b66\u4e60 3D \u573a\u666f\u6d41\u5b58\u5728\u5f88\u5927\u7684\u56f0\u96be\uff0c\u5305\u62ec\u4ece\u5408\u6210\u6570\u636e\u96c6\u5230\u771f\u5b9e\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u771f\u5b9e\u4e16\u754c 3D \u6807\u7b7e\u7684\u7a00\u7f3a\u4ee5\u53ca\u771f\u5b9e\u7a00\u758f LiDAR \u70b9\u4e91\u7684\u6027\u80fd\u4e0d\u4f73\u3002\u6211\u4eec\u4ece\u81ea\u52a8\u6807\u8bb0\u7684\u89d2\u5ea6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684 LiDAR \u70b9\u4e91\u751f\u6210\u5927\u91cf 3D \u573a\u666f\u6d41\u4f2a\u6807\u7b7e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u521a\u4f53\u8fd0\u52a8\u7684\u5047\u8bbe\u6765\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u6f5c\u5728\u7684\u7269\u4f53\u7ea7\u521a\u6027\u8fd0\u52a8\u3002\u901a\u8fc7\u66f4\u65b0\u591a\u4e2a\u951a\u6846\u7684\u4e0d\u540c\u8fd0\u52a8\u5c5e\u6027\uff0c\u83b7\u5f97\u6574\u4e2a\u573a\u666f\u7684\u521a\u6027\u8fd0\u52a8\u5206\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7528\u4e8e\u5168\u5c40\u548c\u5c40\u90e8\u8fd0\u52a8\u7684 3D \u573a\u666f\u6d41\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u901a\u8fc7\u57fa\u4e8e\u589e\u5f3a\u8fd0\u52a8\u53c2\u6570\u5b8c\u7f8e\u5408\u6210\u76ee\u6807\u70b9\u4e91\uff0c\u6211\u4eec\u53ef\u4ee5\u8f7b\u677e\u83b7\u5f97\u70b9\u4e91\u4e2d\u5927\u91cf\u4e0e\u771f\u5b9e\u573a\u666f\u9ad8\u5ea6\u4e00\u81f4\u76843D\u573a\u666f\u6d41\u6807\u7b7e\u3002\u5728\u5305\u62ec LiDAR KITTI\u3001nuScenes \u548c Argoverse \u5728\u5185\u7684\u591a\u4e2a\u73b0\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4e4b\u524d\u6240\u6709\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u624b\u52a8\u6807\u8bb0\u3002\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 LiDAR KITTI \u6570\u636e\u96c6\u4e0a\u5c06 EPE3D \u6307\u6807\u964d\u4f4e\u4e86\u5341\u500d\uff0c\u5c06\u8bef\u5dee\u4ece $0.190m$ \u51cf\u5c11\u5230\u4ec5\u4ec5 $0.008m$ \u3002|[2402.18146v1](http://arxiv.org/pdf/2402.18146v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.18411": "|**2024-02-28**|**Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport**|\u901a\u8fc7\u539f\u578b\u6700\u4f18\u4f20\u8f93\u8fdb\u884c\u65e0\u76d1\u7763\u8de8\u57df\u56fe\u50cf\u68c0\u7d22|Bin Li, Ye Shi, Qian Yu, Jingya Wang|Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT.|\u65e0\u76d1\u7763\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\uff08UCIR\uff09\u65e8\u5728\u68c0\u7d22\u8de8\u4e0d\u540c\u57df\u5171\u4eab\u540c\u4e00\u7c7b\u522b\u7684\u56fe\u50cf\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u6807\u8bb0\u6570\u636e\u3002\u5148\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u5c06 UCIR \u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\uff1a\u57df\u5185\u8868\u793a\u5b66\u4e60\u548c\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5206\u79bb\u7684\u7b56\u7565\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u4efb\u52a1\u4e4b\u95f4\u7684\u6f5c\u5728\u534f\u540c\u4f5c\u7528\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 ProtoOT\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a UCIR \u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b\u6700\u4f18\u4f20\u8f93\u516c\u5f0f\uff0c\u5b83\u5c06\u57df\u5185\u7279\u5f81\u8868\u793a\u5b66\u4e60\u548c\u8de8\u57df\u5bf9\u9f50\u96c6\u6210\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\u3002 ProtoOT \u5229\u7528 K \u5747\u503c\u805a\u7c7b\u65b9\u6cd5\u7684\u4f18\u52bf\u6765\u6709\u6548\u7ba1\u7406 UCIR \u56fa\u6709\u7684\u5206\u5e03\u4e0d\u5e73\u8861\u3002\u901a\u8fc7\u5229\u7528 K \u5747\u503c\u751f\u6210\u521d\u59cb\u539f\u578b\u5e76\u8fd1\u4f3c\u7c7b\u8fb9\u7f18\u5206\u5e03\uff0c\u6211\u4eec\u76f8\u5e94\u5730\u4fee\u6539\u4e86\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u7ea6\u675f\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u5176\u5728 UCIR \u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u5bf9\u6bd4\u5b66\u4e60\u7eb3\u5165 ProtoOT \u6846\u67b6\u4e2d\uff0c\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u3002\u8fd9\u9f13\u52b1\u5177\u6709\u76f8\u4f3c\u8bed\u4e49\u7684\u7279\u5f81\u4e4b\u95f4\u7684\u5c40\u90e8\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u8fd8\u660e\u786e\u5730\u5f3a\u5236\u7279\u5f81\u548c\u4e0d\u5339\u914d\u539f\u578b\u4e4b\u95f4\u7684\u5206\u79bb\uff0c\u4ece\u800c\u589e\u5f3a\u5168\u5c40\u8fa8\u522b\u529b\u3002 ProtoOT \u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u660e\u663e\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 DomainNet \u4e0a\uff0cProtoOT \u7684\u5e73\u5747 P@200 \u63d0\u5347\u4e86 24.44%\uff0c\u5728 Office-Home \u4e0a\uff0c\u5b83\u7684 P@15 \u63d0\u5347\u4e86 12.12%\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/HCVLAB/ProtoOT \u83b7\u53d6\u3002|[2402.18411v1](http://arxiv.org/pdf/2402.18411v1)|null|\n", "2402.18086": "|**2024-02-28**|**Generalizable Two-Branch Framework for Image Class-Incremental Learning**|\u56fe\u50cf\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u53ef\u63a8\u5e7f\u4e8c\u5206\u652f\u6846\u67b6|Chao Wu, Xiaobin Chang, Ruixuan Wang|Deep neural networks often severely forget previously learned knowledge when learning new knowledge. Various continual learning (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements.In this paper, a novel two-branch continual learning framework is proposed to further enhance most existing CL methods. Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight convolutional network. The output of each main branch block is modulated by the output of the corresponding side branch block. Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells.Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b66\u4e60\u65b0\u77e5\u8bc6\u65f6\u5e38\u5e38\u4f1a\u4e25\u91cd\u5fd8\u8bb0\u4ee5\u524d\u5b66\u8fc7\u7684\u77e5\u8bc6\u3002\u4eba\u4eec\u63d0\u51fa\u4e86\u5404\u79cd\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\u6765\u4ece\u4e0d\u540c\u89d2\u5ea6\u5904\u7406\u8fd9\u79cd\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u7684\u6539\u8fdb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u5206\u652f\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u5927\u591a\u6570\u73b0\u6709\u7684 CL \u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3b\u5206\u652f\u53ef\u4ee5\u662f\u4efb\u4f55\u73b0\u6709\u7684 CL \u6a21\u578b\uff0c\u65b0\u5f15\u5165\u7684\u4fa7\u5206\u652f\u662f\u8f7b\u91cf\u7ea7\u5377\u79ef\u7f51\u7edc\u3002\u6bcf\u4e2a\u4e3b\u5206\u652f\u5757\u7684\u8f93\u51fa\u7531\u76f8\u5e94\u4fa7\u5206\u652f\u5757\u7684\u8f93\u51fa\u8c03\u5236\u3002\u8fd9\u6837\u4e00\u4e2a\u7b80\u5355\u7684\u4e24\u5206\u652f\u6a21\u578b\u5c31\u53ef\u4ee5\u901a\u8fc7\u666e\u901a\u7684\u4f18\u5316\u8bbe\u7f6e\u8f7b\u677e\u5b9e\u73b0\u548c\u5b66\u4e60\uff0c\u65e0\u9700\u4efb\u4f55\u63d0\u793a\u3002\u5728\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5404\u79cd\u8bbe\u7f6e\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\u3002|[2402.18086v1](http://arxiv.org/pdf/2402.18086v1)|null|\n"}, "\u5176\u4ed6": {"2402.18476": "|**2024-02-28**|**IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding**|IBD\uff1a\u901a\u8fc7\u56fe\u50cf\u504f\u5411\u89e3\u7801\u51cf\u8f7b\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9|Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, Jun Liu|Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.|\u5c3d\u7ba1\u53d6\u5f97\u4e86\u5feb\u901f\u7684\u53d1\u5c55\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u9762\u4e34\u7740\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u4e25\u5cfb\u6311\u6218\u3002\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u5df2\u88ab\u8ba4\u4e3a\u662f\u5bfc\u81f4\u8fd9\u4e9b\u5e7b\u89c9\u7684\u5173\u952e\u56e0\u7d20\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u504f\u5411\u89e3\u7801\uff08IBD\uff09\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u4f20\u7edf LVLM \u7684\u9884\u6d4b\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684 LVLM \u7684\u9884\u6d4b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u51fa\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7684\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u653e\u5927\u4e0e\u56fe\u50cf\u5185\u5bb9\u9ad8\u5ea6\u76f8\u5173\u7684\u6b63\u786e\u4fe1\u606f\uff0c\u540c\u65f6\u51cf\u8f7b\u7531\u4e8e\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u800c\u5bfc\u81f4\u7684\u5e7b\u89c9\u9519\u8bef\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8fdb\u884c\u5168\u9762\u7684\u7edf\u8ba1\u5206\u6790\u4ee5\u9a8c\u8bc1\u6211\u4eec\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u4ee5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5065\u548c\u7075\u6d3b\u7684\u5904\u7406\u3002\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u4e0d\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u6570\u636e\u5e76\u4e14\u53ea\u9700\u8981\u6700\u5c0f\u9650\u5ea6\u5730\u589e\u52a0\u6a21\u578b\u53c2\u6570\uff0c\u5c31\u53ef\u4ee5\u663e\u7740\u51cf\u5c11 LVLM \u4e2d\u7684\u5e7b\u89c9\u5e76\u589e\u5f3a\u751f\u6210\u54cd\u5e94\u7684\u771f\u5b9e\u6027\u3002|[2402.18476v1](http://arxiv.org/pdf/2402.18476v1)|null|\n", "2402.18409": "|**2024-02-28**|**A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models**|\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56fe\u50cf\u63a8\u7406\u4e0e\u63cf\u8ff0\u7684\u8ba4\u77e5\u8bc4\u4f30\u57fa\u51c6|Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen|Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.|\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5c3d\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u8ba4\u77e5\u80fd\u529b\u5374\u5f88\u96be\u5f97\u5230\u5168\u9762\u6d4b\u8bd5\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u6d4b\u8bd5\u4e2d\u666e\u904d\u4f7f\u7528\u7684\u201cCookie Theft\u201d\u4efb\u52a1\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f7f\u7528\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u7684\u56fe\u50cf\u6765\u8bc4\u4f30 LVLM \u7684\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u3002\u5b83\u5b9a\u4e49\u4e86\u516b\u79cd\u63a8\u7406\u80fd\u529b\uff0c\u7531\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7ec4\u6210\u3002\u6211\u4eec\u5bf9\u8457\u540d\u7684 LVLM \u7684\u8bc4\u4f30\u8868\u660e\uff0cLVLM \u4e0e\u4eba\u7c7b\u7684\u8ba4\u77e5\u80fd\u529b\u4ecd\u7136\u5b58\u5728\u5f88\u5927\u5dee\u8ddd\u3002|[2402.18409v1](http://arxiv.org/pdf/2402.18409v1)|null|\n", "2402.18337": "|**2024-02-28**|**Probabilistic Bayesian optimal experimental design using conditional normalizing flows**|\u4f7f\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u7684\u6982\u7387\u8d1d\u53f6\u65af\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1|Rafael Orozco, Felix J. Herrmann, Peng Chen|Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations.|\u8d1d\u53f6\u65af\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1\uff08OED\uff09\u5bfb\u6c42\u5728\u9884\u7b97\u9650\u5236\u4e0b\u8fdb\u884c\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u5b9e\u9a8c\uff0c\u4ee5\u5c06\u7cfb\u7edf\u7684\u5148\u9a8c\u77e5\u8bc6\u4ece\u8d1d\u53f6\u65af\u6846\u67b6\u4e2d\u7684\u5b9e\u9a8c\u6570\u636e\u66f4\u65b0\u4e3a\u540e\u9a8c\u77e5\u8bc6\u3002\u6b64\u7c7b\u95ee\u9898\u5728\u8ba1\u7b97\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\uff081\uff09\u5bf9\u67d0\u4e9b\u6700\u4f18\u6027\u6807\u51c6\u8fdb\u884c\u6602\u8d35\u4e14\u91cd\u590d\u7684\u8bc4\u4f30\uff0c\u901a\u5e38\u6d89\u53ca\u7cfb\u7edf\u53c2\u6570\u548c\u5b9e\u9a8c\u6570\u636e\u7684\u53cc\u91cd\u79ef\u5206\uff0c\uff082\uff09\u5f53\u7cfb\u7edf\u53c2\u6570\u548c\u8bbe\u8ba1\u53d8\u91cf\u662f\u9ad8\u7ef4\u7684\uff0c(3) \u5982\u679c\u8bbe\u8ba1\u53d8\u91cf\u662f\u4e8c\u5143\u7684\uff0c\u5219\u4f18\u5316\u662f\u7ec4\u5408\u7684\u5e76\u4e14\u9ad8\u5ea6\u975e\u51f8\u7684\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u975e\u9c81\u68d2\u8bbe\u8ba1\u3002\u4e3a\u4e86\u4f7f\u8d1d\u53f6\u65af OED \u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u540c\u65f6\u6267\u884c (1) \u53ef\u6269\u5c55\u6761\u4ef6\u5f52\u4e00\u5316\u6d41 (CNF) \u7684\u8bad\u7ec3\uff0c\u4ee5\u6709\u6548\u5730\u6700\u5927\u5316\u8054\u5408\u5b66\u4e60\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u9884\u671f\u4fe1\u606f\u589e\u76ca (EIG) (2) \u4f7f\u7528\u4f2f\u52aa\u5229\u5206\u5e03\u4f18\u5316\u4e8c\u5143\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u6982\u7387\u516c\u5f0f\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u9645 MRI \u6570\u636e\u91c7\u96c6\u95ee\u9898\u4e2d\u7684\u6027\u80fd\uff0c\u8fd9\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u8d1d\u53f6\u65af OED \u95ee\u9898\u4e4b\u4e00\uff0c\u5728\u9ad8\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u5177\u6709\u9ad8\u7ef4\uff08320 $\\times$ 320\uff09\u53c2\u6570\uff0c\u9ad8\u7ef4\uff08640 $\\ times$ 386\uff09\u89c2\u6d4b\u503c\uff0c\u5e76\u91c7\u7528\u4e8c\u5143\u63a9\u6a21\u8bbe\u8ba1\u6765\u9009\u62e9\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u89c2\u6d4b\u503c\u3002|[2402.18337v1](http://arxiv.org/pdf/2402.18337v1)|null|\n", "2402.18320": "|**2024-02-28**|**Location-guided Head Pose Estimation for Fisheye Image**|\u9c7c\u773c\u56fe\u50cf\u7684\u4f4d\u7f6e\u5f15\u5bfc\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1|Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, Dah-Jye Lee|Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \\textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \\textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \\textcolor{blue}{a} fisheye-\\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.|\u5e26\u6709\u9c7c\u773c\u955c\u5934\u6216\u8d85\u5e7f\u89d2\u955c\u5934\u7684\u76f8\u673a\u53ef\u8986\u76d6\u900f\u89c6\u6295\u5f71\u65e0\u6cd5\u5efa\u6a21\u7684\u5e7f\u9614\u89c6\u91ce\u3002\u56fe\u50cf\u5916\u56f4\u533a\u57df\u4e25\u91cd\u7684\u9c7c\u773c\\textcolor{blue}{lens}\u5931\u771f\u5bfc\u81f4\u5728\u672a\u5931\u771f\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\\textcolor{blue}{\u73b0\u6709}\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1\u6a21\u578b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u4e2d\u5934\u90e8\u4f4d\u7f6e\u7684\u77e5\u8bc6\u6765\u51cf\u5c11\u9c7c\u773c\u5931\u771f\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u901a\u8fc7\u5934\u90e8\u59ff\u52bf\u548c\u5934\u90e8\u4f4d\u7f6e\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u4f30\u8ba1\u5934\u90e8\u59ff\u52bf\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7f51\u7edc\u76f4\u63a5\u4ece\u9c7c\u773c\u56fe\u50cf\u4f30\u8ba1\u5934\u90e8\u59ff\u52bf\uff0c\u65e0\u9700\u8fdb\u884c\u6821\u6b63\u6216\u6821\u51c6\u64cd\u4f5c\u3002\u6211\u4eec\u8fd8\u4e3a\u6211\u4eec\u7684\u5b9e\u9a8c\u521b\u5efa\u4e86\u4e09\u4e2a\u6d41\u884c\u7684\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1\u6570\u636e\u96c6 BIWI\u3001300W-LP \u548c AFLW2000 \u7684 \\textcolor{blue}{a} Fisheye-\\textcolor{blue}{ Distorted} \u7248\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u4e00\u9636\u6bb5\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u7f51\u7edc\u663e\u7740\u63d0\u9ad8\u4e86\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002|[2402.18320v1](http://arxiv.org/pdf/2402.18320v1)|null|\n", "2402.18305": "|**2024-02-28**|**NERV++: An Enhanced Implicit Neural Video Representation**|NERV++\uff1a\u589e\u5f3a\u7684\u9690\u5f0f\u795e\u7ecf\u89c6\u9891\u8868\u793a|Ahmed Ghorbel, Wassim Hamidouche, Luce Morin|Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint. Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability. Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks. We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation. NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs. This achievement narrows the gap to autoencoder-based video coding, marking a significant stride in INR-based video compression research.|\u795e\u7ecf\u573a\uff0c\u4e5f\u79f0\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a (INR)\uff0c\u5df2\u663e\u793a\u51fa\u8868\u793a\u3001\u751f\u6210\u548c\u64cd\u4f5c\u5404\u79cd\u6570\u636e\u7c7b\u578b\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5141\u8bb8\u4ee5\u4f4e\u5185\u5b58\u5360\u7528\u8fdb\u884c\u8fde\u7eed\u6570\u636e\u91cd\u5efa\u3002\u5c3d\u7ba1\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5e94\u7528\u4e8e\u89c6\u9891\u538b\u7f29\u7684 INR \u4ecd\u9700\u8981\u5927\u5e45\u63d0\u9ad8\u5176\u7387\u5931\u771f\u6027\u80fd\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u53c2\u6570\u548c\u957f\u65f6\u95f4\u8bad\u7ec3\u8fed\u4ee3\u6765\u6355\u83b7\u9ad8\u9891\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4ecd\u7136\u662f\u4e00\u9879\u76f8\u5f53\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8fd9\u5c06\u4f7f INR \u5728\u538b\u7f29\u4efb\u52a1\u4e2d\u66f4\u5bb9\u6613\u83b7\u5f97\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u89c6\u9891\u795e\u7ecf\u8868\u793a NeRV++\uff08\u4e00\u79cd\u589e\u5f3a\u7684\u9690\u5f0f\u795e\u7ecf\u89c6\u9891\u8868\u793a\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u5b83\u6bd4\u539f\u59cb NeRV \u89e3\u7801\u5668\u67b6\u6784\u66f4\u76f4\u63a5\u800c\u6709\u6548\u7684\u589e\u5f3a\uff0c\u5177\u6709\u5939\u7740\u4e0a\u91c7\u6837\u5757\u7684\u53ef\u5206\u79bb\u7684 conv2d \u6b8b\u5dee\u5757\uff08SCRB\uff09\uff08 UB\uff09\uff0c\u4ee5\u53ca\u7528\u4e8e\u6539\u8fdb\u7279\u5f81\u8868\u793a\u7684\u53cc\u7ebf\u6027\u63d2\u503c\u8df3\u8dc3\u5c42\u3002 NeRV++\u5141\u8bb8\u89c6\u9891\u76f4\u63a5\u8868\u793a\u4e3a\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7684\u51fd\u6570\uff0c\u5e76\u4e14\u663e\u7740\u589e\u5f3a\u4e86\u8868\u793a\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u57fa\u4e8eINR\u7684\u89c6\u9891\u7f16\u89e3\u7801\u5668\u3002\u6211\u4eec\u5728 UVG\u3001MCL JVC \u548c Bunny \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728\u4f7f\u7528 INR \u7684\u89c6\u9891\u538b\u7f29\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u8fd9\u4e00\u6210\u679c\u7f29\u5c0f\u4e86\u4e0e\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89c6\u9891\u7f16\u7801\u7684\u5dee\u8ddd\uff0c\u6807\u5fd7\u7740\u57fa\u4e8e INR \u7684\u89c6\u9891\u538b\u7f29\u7814\u7a76\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u6b65\u3002|[2402.18305v1](http://arxiv.org/pdf/2402.18305v1)|null|\n", "2402.18288": "|**2024-02-28**|**Development of Context-Sensitive Formulas to Obtain Constant Luminance Perception for a Foreground Object in Front of Backgrounds of Varying Luminance**|\u5f00\u53d1\u4e0a\u4e0b\u6587\u76f8\u5173\u516c\u5f0f\u4ee5\u83b7\u5f97\u53d8\u5316\u4eae\u5ea6\u80cc\u666f\u4e0b\u524d\u666f\u7269\u4f53\u7684\u6052\u5b9a\u4eae\u5ea6\u611f\u77e5|Ergun Akleman, Bekir Tevfik Akgun, Adil Alpkocak|In this article, we present a framework for developing context-sensitive luminance correction formulas that can produce constant luminance perception for foreground objects. Our formulas make the foreground object slightly translucent to mix with the blurred version of the background. This mix can quickly produce any desired illusion of luminance in foreground objects based on the luminance of the background. The translucency formula has only one parameter; the relative size of the foreground object, which is a number between zero and one. We have identified the general structure of the translucency formulas as a power function of the relative size of the foreground object. We have implemented a web-based interactive program in Shadertoy. Using this program, we determined the coefficients of the polynomial exponents of the power function. To intuitively control the coefficients of the polynomial functions, we have used a B\\'{e}zier form. Our final translucency formula uses a quadratic polynomial and requires only three coefficients. We also identified a simpler affine formula, which requires only two coefficients. We made our program publicly available in Shadertoy so that anyone can access and improve it. In this article, we also explain how to intuitively change the polynomial part of the formula. Using our explanation, users change the polynomial part of the formula to obtain their own perceptively constant luminance. This can be used as a crowd-sourcing experiment for further improvement of the formula.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u4e0a\u4e0b\u6587\u76f8\u5173\u4eae\u5ea6\u6821\u6b63\u516c\u5f0f\u7684\u6846\u67b6\uff0c\u8be5\u516c\u5f0f\u53ef\u4ee5\u4e3a\u524d\u666f\u5bf9\u8c61\u4ea7\u751f\u6052\u5b9a\u7684\u4eae\u5ea6\u611f\u77e5\u3002\u6211\u4eec\u7684\u516c\u5f0f\u4f7f\u524d\u666f\u5bf9\u8c61\u7a0d\u5fae\u534a\u900f\u660e\uff0c\u4ee5\u4e0e\u80cc\u666f\u7684\u6a21\u7cca\u7248\u672c\u6df7\u5408\u3002\u8fd9\u79cd\u6df7\u5408\u53ef\u4ee5\u6839\u636e\u80cc\u666f\u7684\u4eae\u5ea6\u5feb\u901f\u5728\u524d\u666f\u5bf9\u8c61\u4e2d\u4ea7\u751f\u4efb\u4f55\u6240\u9700\u7684\u4eae\u5ea6\u5e7b\u89c9\u3002\u534a\u900f\u660e\u5ea6\u516c\u5f0f\u53ea\u6709\u4e00\u4e2a\u53c2\u6570\uff1b\u524d\u666f\u5bf9\u8c61\u7684\u76f8\u5bf9\u5927\u5c0f\uff0c\u662f 0 \u5230 1 \u4e4b\u95f4\u7684\u6570\u5b57\u3002\u6211\u4eec\u5df2\u7ecf\u5c06\u534a\u900f\u660e\u5ea6\u516c\u5f0f\u7684\u4e00\u822c\u7ed3\u6784\u786e\u5b9a\u4e3a\u524d\u666f\u5bf9\u8c61\u76f8\u5bf9\u5927\u5c0f\u7684\u5e42\u51fd\u6570\u3002\u6211\u4eec\u5728 Shadertoy \u4e2d\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u4ea4\u4e92\u7a0b\u5e8f\u3002\u4f7f\u7528\u8be5\u7a0b\u5e8f\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5e42\u51fd\u6570\u591a\u9879\u5f0f\u6307\u6570\u7684\u7cfb\u6570\u3002\u4e3a\u4e86\u76f4\u89c2\u5730\u63a7\u5236\u591a\u9879\u5f0f\u51fd\u6570\u7684\u7cfb\u6570\uff0c\u6211\u4eec\u4f7f\u7528\u4e86 B\\'{e}zier \u5f62\u5f0f\u3002\u6211\u4eec\u6700\u7ec8\u7684\u534a\u900f\u660e\u5ea6\u516c\u5f0f\u4f7f\u7528\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u4ec5\u9700\u8981\u4e09\u4e2a\u7cfb\u6570\u3002\u6211\u4eec\u8fd8\u786e\u5b9a\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u4eff\u5c04\u516c\u5f0f\uff0c\u5b83\u53ea\u9700\u8981\u4e24\u4e2a\u7cfb\u6570\u3002\u6211\u4eec\u5728 Shadertoy \u4e2d\u516c\u5f00\u63d0\u4f9b\u6211\u4eec\u7684\u7a0b\u5e8f\uff0c\u4ee5\u4fbf\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u8bbf\u95ee\u548c\u6539\u8fdb\u5b83\u3002\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u8fd8\u89e3\u91ca\u4e86\u5982\u4f55\u76f4\u89c2\u5730\u6539\u53d8\u516c\u5f0f\u7684\u591a\u9879\u5f0f\u90e8\u5206\u3002\u4f7f\u7528\u6211\u4eec\u7684\u89e3\u91ca\uff0c\u7528\u6237\u66f4\u6539\u516c\u5f0f\u7684\u591a\u9879\u5f0f\u90e8\u5206\u4ee5\u83b7\u5f97\u4ed6\u4eec\u81ea\u5df1\u611f\u77e5\u7684\u6052\u5b9a\u4eae\u5ea6\u3002\u8fd9\u53ef\u4ee5\u4f5c\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u516c\u5f0f\u7684\u4f17\u5305\u5b9e\u9a8c\u3002|[2402.18288v1](http://arxiv.org/pdf/2402.18288v1)|null|\n", "2402.18217": "|**2024-02-28**|**Region-Aware Exposure Consistency Network for Mixed Exposure Correction**|\u7528\u4e8e\u6df7\u5408\u66dd\u5149\u6821\u6b63\u7684\u533a\u57df\u611f\u77e5\u66dd\u5149\u4e00\u81f4\u6027\u7f51\u7edc|Jin Liu, Huiyuan Fu, Chuanming Wang, Huadong Ma|Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet.|\u66dd\u5149\u6821\u6b63\u7684\u76ee\u7684\u662f\u589e\u5f3a\u56e0\u66dd\u5149\u4e0d\u5f53\u800c\u53d7\u5230\u5f71\u54cd\u7684\u56fe\u50cf\uff0c\u4ee5\u8fbe\u5230\u6ee1\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002\u5c3d\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ef\u4ee5\u51cf\u8f7b\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u8fc7\u5ea6\u66dd\u5149\u6216\u66dd\u5149\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u5904\u7406\u6df7\u5408\u66dd\u5149\u7684\u56fe\u50cf\uff0c\u5373\u4e00\u5f20\u56fe\u50cf\u540c\u65f6\u5305\u542b\u66dd\u5149\u8fc7\u5ea6\u548c\u66dd\u5149\u4e0d\u8db3\u7684\u533a\u57df\u3002\u6df7\u5408\u66dd\u5149\u5206\u5e03\u4e0d\u5747\u5300\u5e76\u5bfc\u81f4\u4e0d\u540c\u7684\u8868\u793a\uff0c\u8fd9\u4f7f\u5f97\u5728\u7edf\u4e00\u8fc7\u7a0b\u4e2d\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6709\u6548\u7684\u533a\u57df\u611f\u77e5\u66dd\u5149\u6821\u6b63\u7f51\u7edc\uff08RECNet\uff09\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u6865\u63a5\u4e0d\u540c\u533a\u57df\u66dd\u5149\u8868\u793a\u6765\u5904\u7406\u6df7\u5408\u66dd\u5149\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u89e3\u51b3\u6df7\u5408\u66dd\u5149\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u533a\u57df\u611f\u77e5\u53bb\u66dd\u5149\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u6df7\u5408\u66dd\u5149\u573a\u666f\u7684\u533a\u57df\u7279\u5f81\u8f6c\u6362\u4e3a\u66dd\u5149\u4e0d\u53d8\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u53bb\u66dd\u5149\u64cd\u4f5c\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u51cf\u5c11\u5224\u522b\u4fe1\u606f\uff0c\u56e0\u6b64\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u5c3a\u5ea6\u6062\u590d\u5355\u5143\uff0c\u8be5\u5355\u5143\u96c6\u6210\u4e86\u66dd\u5149\u4e0d\u53d8\u7279\u5f81\u548c\u672a\u5904\u7406\u7279\u5f81\u6765\u6062\u590d\u5c40\u90e8\u4fe1\u606f\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5b9e\u73b0\u5168\u5c40\u56fe\u50cf\u4e2d\u5747\u5300\u7684\u66dd\u5149\u5206\u5e03\uff0c\u6211\u4eec\u5728\u533a\u57df\u5185\u66dd\u5149\u4e00\u81f4\u6027\u548c\u533a\u57df\u95f4\u66dd\u5149\u8fde\u7eed\u6027\u7684\u7ea6\u675f\u4e0b\u63d0\u51fa\u4e86\u66dd\u5149\u5bf9\u6bd4\u6b63\u5219\u5316\u7b56\u7565\u3002\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u6027\u3002\u4ee3\u7801\u53d1\u5e03\u4e8e\uff1ahttps://github.com/kravrolens/RECNet\u3002|[2402.18217v1](http://arxiv.org/pdf/2402.18217v1)|null|\n", "2402.18201": "|**2024-02-28**|**Learning Invariant Inter-pixel Correlations for Superpixel Generation**|\u5b66\u4e60\u8d85\u50cf\u7d20\u751f\u6210\u7684\u4e0d\u53d8\u50cf\u7d20\u95f4\u76f8\u5173\u6027|Sen Xu, Shikui Wei, Tao Ruan, Lixin Liao|Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel.|\u901a\u8fc7\u7528\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u66ff\u4ee3\u624b\u5de5\u5236\u4f5c\u7684\u7279\u5f81\uff0c\u6df1\u5ea6\u8d85\u50cf\u7d20\u7b97\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u4f5c\u4e3a\u4e2d\u5c42\u8868\u793a\u64cd\u4f5c\u7684\u73b0\u6709\u6df1\u5ea6\u8d85\u50cf\u7d20\u65b9\u6cd5\u4ecd\u7136\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5d4c\u5165\u7684\u7edf\u8ba1\u5c5e\u6027\uff08\u4f8b\u5982\u989c\u8272\u5206\u5e03\u3001\u9ad8\u7ea7\u8bed\u4e49\uff09\u654f\u611f\u3002\u56e0\u6b64\uff0c\u53ef\u5b66\u4e60\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u53d7\u5230\u9650\u5236\uff0c\u5bfc\u81f4\u50cf\u7d20\u5206\u7ec4\u6027\u80fd\u4e0d\u7406\u60f3\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u8bad\u7ec3\u7684\u5e94\u7528\u573a\u666f\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5185\u5bb9\u89e3\u7f20\u8d85\u50cf\u7d20\uff08CDS\uff09\u7b97\u6cd5\u6765\u9009\u62e9\u6027\u5730\u5206\u79bb\u4e0d\u53d8\u7684\u50cf\u7d20\u95f4\u76f8\u5173\u6027\u548c\u7edf\u8ba1\u5c5e\u6027\uff0c\u5373\u98ce\u683c\u566a\u58f0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e0e\u539f\u59cb RGB \u56fe\u50cf\u540c\u6e90\u4f46\u5177\u6709\u663e\u7740\u98ce\u683c\u53d8\u5316\u7684\u8f85\u52a9\u6a21\u6001\u3002\u7136\u540e\uff0c\u5728\u4e92\u4fe1\u606f\u7684\u9a71\u52a8\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u8de8\u6a21\u6001\u7684\u5c40\u90e8\u7f51\u683c\u76f8\u5173\u6027\u5bf9\u9f50\uff0c\u4ee5\u51cf\u5c11\u81ea\u9002\u5e94\u9009\u62e9\u7684\u7279\u5f81\u7684\u5206\u5e03\u5dee\u5f02\u5e76\u5b66\u4e60\u4e0d\u53d8\u7684\u50cf\u7d20\u95f4\u76f8\u5173\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u6267\u884c\u5168\u5c40\u5f0f\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\uff0c\u4ee5\u5f3a\u5236\u4e0d\u53d8\u5185\u5bb9\u548c\u8bad\u7ec3\u6570\u636e\u6837\u5f0f\u7684\u5206\u79bb\u3002\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8fb9\u754c\u9075\u5b88\u3001\u6cdb\u5316\u548c\u6548\u7387\u65b9\u9762\u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/rookiie/CDSpixel \u83b7\u53d6\u3002|[2402.18201v1](http://arxiv.org/pdf/2402.18201v1)|null|\n", "2402.18192": "|**2024-02-28**|**Misalignment-Robust Frequency Distribution Loss for Image Transformation**|\u56fe\u50cf\u53d8\u6362\u7684\u5931\u51c6\u9c81\u68d2\u9891\u7387\u5206\u5e03\u635f\u5931|Zhangkai Ni, Juncheng Wu, Zian Wang, Wenhan Yang, Hanli Wang, Lin Ma|This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL|\u672c\u6587\u65e8\u5728\u89e3\u51b3\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff08\u4f8b\u5982\u56fe\u50cf\u589e\u5f3a\u548c\u8d85\u5206\u8fa8\u7387\uff09\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5177\u6709\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684\u7cbe\u786e\u5bf9\u9f50\u7684\u914d\u5bf9\u6570\u636e\u96c6\u3002\u7136\u800c\uff0c\u521b\u5efa\u7cbe\u786e\u5bf9\u9f50\u7684\u914d\u5bf9\u56fe\u50cf\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5e76\u963b\u788d\u4e86\u57fa\u4e8e\u6b64\u7c7b\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\u7684\u8fdb\u6b65\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7b80\u5355\u7684\u9891\u7387\u5206\u5e03\u635f\u5931\uff08FDL\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97\u9891\u57df\u5185\u7684\u5206\u5e03\u8ddd\u79bb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4f7f\u7528\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362\uff08DFT\uff09\u5c06\u56fe\u50cf\u7279\u5f81\u53d8\u6362\u5230\u9891\u57df\u3002\u968f\u540e\uff0c\u5206\u522b\u5904\u7406\u9891\u7387\u5206\u91cf\uff08\u5e45\u5ea6\u548c\u76f8\u4f4d\uff09\u4ee5\u5f62\u6210 FDL \u635f\u5931\u51fd\u6570\u3002\u7531\u4e8e\u9891\u57df\u4e2d\u5168\u5c40\u4fe1\u606f\u7684\u5468\u5230\u5229\u7528\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u88ab\u7ecf\u9a8c\u8bc1\u660e\u4f5c\u4e3a\u8bad\u7ec3\u7ea6\u675f\u662f\u6709\u6548\u7684\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff08\u91cd\u70b9\u5173\u6ce8\u56fe\u50cf\u589e\u5f3a\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff09\u8868\u660e FDL \u4f18\u4e8e\u73b0\u6709\u7684\u5931\u51c6\u9c81\u68d2\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86 FDL \u5728\u4ec5\u4f9d\u8d56\u4e8e\u5b8c\u5168\u9519\u4f4d\u6570\u636e\u7684\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u65b9\u9762\u7684\u6f5c\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/eezkni/FDL|[2402.18192v1](http://arxiv.org/pdf/2402.18192v1)|null|\n", "2402.18178": "|**2024-02-28**|**Reflection Removal Using Recurrent Polarization-to-Polarization Network**|\u4f7f\u7528\u5faa\u73af\u504f\u632f\u5230\u504f\u632f\u7f51\u7edc\u53bb\u9664\u53cd\u5c04|Wenjiao Bian, Yusuke Monno, Masatoshi Okutomi|This paper addresses reflection removal, which is the task of separating reflection components from a captured image and deriving the image with only transmission components. Considering that the existence of the reflection changes the polarization state of a scene, some existing methods have exploited polarized images for reflection removal. While these methods apply polarized images as the inputs, they predict the reflection and the transmission directly as non-polarized intensity images. In contrast, we propose a polarization-to-polarization approach that applies polarized images as the inputs and predicts \"polarized\" reflection and transmission images using two sequential networks to facilitate the separation task by utilizing the interrelated polarization information between the reflection and the transmission. We further adopt a recurrent framework, where the predicted reflection and transmission images are used to iteratively refine each other. Experimental results on a public dataset demonstrate that our method outperforms other state-of-the-art methods.|\u672c\u6587\u8ba8\u8bba\u4e86\u53cd\u5c04\u53bb\u9664\uff0c\u5373\u4ece\u6355\u83b7\u7684\u56fe\u50cf\u4e2d\u5206\u79bb\u53cd\u5c04\u5206\u91cf\u5e76\u5bfc\u51fa\u4ec5\u5305\u542b\u900f\u5c04\u5206\u91cf\u7684\u56fe\u50cf\u7684\u4efb\u52a1\u3002\u8003\u8651\u5230\u53cd\u5c04\u7684\u5b58\u5728\u4f1a\u6539\u53d8\u573a\u666f\u7684\u504f\u632f\u72b6\u6001\uff0c\u73b0\u6709\u7684\u4e00\u4e9b\u65b9\u6cd5\u5df2\u7ecf\u5229\u7528\u504f\u632f\u56fe\u50cf\u6765\u53bb\u9664\u53cd\u5c04\u3002\u867d\u7136\u8fd9\u4e9b\u65b9\u6cd5\u5e94\u7528\u504f\u632f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f46\u5b83\u4eec\u76f4\u63a5\u5c06\u53cd\u5c04\u548c\u900f\u5c04\u9884\u6d4b\u4e3a\u975e\u504f\u632f\u5f3a\u5ea6\u56fe\u50cf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u632f\u5230\u504f\u632f\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5e94\u7528\u504f\u632f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u4e24\u4e2a\u987a\u5e8f\u7f51\u7edc\u9884\u6d4b\u201c\u504f\u632f\u201d\u53cd\u5c04\u548c\u900f\u5c04\u56fe\u50cf\uff0c\u4ee5\u5229\u7528\u53cd\u5c04\u548c\u900f\u5c04\u4e4b\u95f4\u76f8\u4e92\u5173\u8054\u7684\u504f\u632f\u4fe1\u606f\u6765\u4fc3\u8fdb\u5206\u79bb\u4efb\u52a1\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u91c7\u7528\u5faa\u73af\u6846\u67b6\uff0c\u5176\u4e2d\u9884\u6d4b\u7684\u53cd\u5c04\u548c\u900f\u5c04\u56fe\u50cf\u7528\u4e8e\u8fed\u4ee3\u5730\u76f8\u4e92\u7ec6\u5316\u3002\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2402.18178v1](http://arxiv.org/pdf/2402.18178v1)|null|\n", "2402.18171": "|**2024-02-28**|**Digging Into Normal Incorporated Stereo Matching**|\u6df1\u5165\u7814\u7a76\u6b63\u5e38\u5408\u5e76\u7684\u7acb\u4f53\u5339\u914d|Zihua Liu, Songyan Zhang, Zhicheng Wang, Masatoshi Okutomi|Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, disparity estimation in low-texture, occluded, and bordered regions still remains a bottleneck that limits the performance. To tackle these challenges, geometric guidance like plane information is necessary as it provides intuitive guidance about disparity consistency and affinity similarity. In this paper, we propose a normal incorporated joint learning framework consisting of two specific modules named non-local disparity propagation(NDP) and affinity-aware residual learning(ARL). The estimated normal map is first utilized for calculating a non-local affinity matrix and a non-local offset to perform spatial propagation at the disparity level. To enhance geometric consistency, especially in low-texture regions, the estimated normal map is then leveraged to calculate a local affinity matrix, providing the residual learning with information about where the correction should refer and thus improving the residual learning efficiency. Extensive experiments on several public datasets including Scene Flow, KITTI 2015, and Middlebury 2014 validate the effectiveness of our proposed method. By the time we finished this work, our approach ranked 1st for stereo matching across foreground pixels on the KITTI 2015 dataset and 3rd on the Scene Flow dataset among all the published works.|\u5c3d\u7ba1\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\uff0c\u4f46\u4f4e\u7eb9\u7406\u3001\u906e\u6321\u548c\u8fb9\u6846\u533a\u57df\u7684\u89c6\u5dee\u4f30\u8ba1\u4ecd\u7136\u662f\u9650\u5236\u6027\u80fd\u7684\u74f6\u9888\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u50cf\u5e73\u9762\u4fe1\u606f\u8fd9\u6837\u7684\u51e0\u4f55\u6307\u5bfc\u662f\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u6709\u5173\u89c6\u5dee\u4e00\u81f4\u6027\u548c\u4eb2\u548c\u529b\u76f8\u4f3c\u6027\u7684\u76f4\u89c2\u6307\u5bfc\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b63\u5e38\u7684\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u7531\u4e24\u4e2a\u7279\u5b9a\u7684\u6a21\u5757\u7ec4\u6210\uff0c\u5373\u975e\u5c40\u90e8\u89c6\u5dee\u4f20\u64ad\uff08NDP\uff09\u548c\u4eb2\u548c\u529b\u611f\u77e5\u6b8b\u5dee\u5b66\u4e60\uff08ARL\uff09\u3002\u4f30\u8ba1\u7684\u6cd5\u7ebf\u56fe\u9996\u5148\u7528\u4e8e\u8ba1\u7b97\u975e\u5c40\u90e8\u4eb2\u548c\u529b\u77e9\u9635\u548c\u975e\u5c40\u90e8\u504f\u79fb\u4ee5\u5728\u89c6\u5dee\u7ea7\u522b\u6267\u884c\u7a7a\u95f4\u4f20\u64ad\u3002\u4e3a\u4e86\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7eb9\u7406\u533a\u57df\uff0c\u7136\u540e\u5229\u7528\u4f30\u8ba1\u7684\u6cd5\u7ebf\u56fe\u6765\u8ba1\u7b97\u5c40\u90e8\u4eb2\u548c\u529b\u77e9\u9635\uff0c\u4e3a\u6b8b\u5dee\u5b66\u4e60\u63d0\u4f9b\u6709\u5173\u6821\u6b63\u5e94\u8be5\u53c2\u8003\u7684\u4f4d\u7f6e\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u6b8b\u5dee\u5b66\u4e60\u6548\u7387\u3002\u5bf9\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08\u5305\u62ec Scene Flow\u3001KITTI 2015 \u548c Middlebury 2014\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5f53\u6211\u4eec\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u65f6\uff0c\u5728\u6240\u6709\u5df2\u53d1\u8868\u7684\u4f5c\u54c1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 KITTI 2015 \u6570\u636e\u96c6\u4e0a\u7684\u524d\u666f\u50cf\u7d20\u7acb\u4f53\u5339\u914d\u65b9\u9762\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u573a\u666f\u6d41\u6570\u636e\u96c6\u4e0a\u6392\u540d\u7b2c\u4e09\u3002|[2402.18171v1](http://arxiv.org/pdf/2402.18171v1)|null|\n", "2402.18152": "|**2024-02-28**|**Boosting Neural Representations for Videos with a Conditional Decoder**|\u4f7f\u7528\u6761\u4ef6\u89e3\u7801\u5668\u589e\u5f3a\u89c6\u9891\u7684\u795e\u7ecf\u8868\u793a|Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang, Hongwei Qin, Jun Zhang|Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs.|\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u89c6\u9891\u5b58\u50a8\u548c\u5904\u7406\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u89c6\u9891\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5353\u8d8a\u7684\u591a\u529f\u80fd\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u8868\u793a\u80fd\u529b\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u76ee\u6807\u5e27\u89e3\u7801\u8fc7\u7a0b\u4e2d\u4e2d\u95f4\u7279\u5f81\u5bf9\u9f50\u4e0d\u5145\u5206\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5f53\u524d\u9690\u5f0f\u89c6\u9891\u8868\u793a\u65b9\u6cd5\u7684\u901a\u7528\u589e\u5f3a\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u5e26\u6709\u65f6\u95f4\u611f\u77e5\u4eff\u5c04\u53d8\u6362\u6a21\u5757\u7684\u6761\u4ef6\u89e3\u7801\u5668\uff0c\u8be5\u6a21\u5757\u4f7f\u7528\u5e27\u7d22\u5f15\u4f5c\u4e3a\u5148\u9a8c\u6761\u4ef6\u6765\u6709\u6548\u5730\u5c06\u4e2d\u95f4\u7279\u5f81\u4e0e\u76ee\u6807\u5e27\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7c7b\u4f3c NeRV \u7684\u6b63\u5f26\u5757\u6765\u751f\u6210\u4e0d\u540c\u7684\u4e2d\u95f4\u7279\u5f81\u5e76\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u53c2\u6570\u5206\u5e03\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u5bb9\u91cf\u3002\u901a\u8fc7\u9ad8\u9891\u4fe1\u606f\u4fdd\u7559\u91cd\u5efa\u635f\u5931\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u89c6\u9891\u56de\u5f52\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u7684\u591a\u4e2a\u57fa\u7ebfINR\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u4fee\u590d\u548c\u63d2\u503c\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4e00\u81f4\u7684\u71b5\u6700\u5c0f\u5316\u6280\u672f\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u589e\u5f3a\u7684 INR \u5f00\u53d1\u89c6\u9891\u7f16\u89e3\u7801\u5668\u3002 UVG \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u6211\u4eec\u7684\u589e\u5f3a\u578b\u7f16\u89e3\u7801\u5668\u663e\u7740\u4f18\u4e8e\u57fa\u7ebf INR\uff0c\u5e76\u4e14\u4e0e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u901f\u7387\u5931\u771f\u6027\u80fd\u3002|[2402.18152v1](http://arxiv.org/pdf/2402.18152v1)|null|\n", "2402.18134": "|**2024-02-28**|**Learning to Deblur Polarized Images**|\u5b66\u4e60\u53bb\u6a21\u7cca\u504f\u632f\u56fe\u50cf|Chu Zhou, Minggui Teng, Xinyu Zhou, Chao Xu, Boxin Sh|A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.|\u504f\u632f\u76f8\u673a\u53ef\u4ee5\u5728\u4e00\u6b21\u62cd\u6444\u4e2d\u6355\u83b7\u5177\u6709\u4e0d\u540c\u504f\u632f\u5668\u89d2\u5ea6\u7684\u56db\u4e2a\u504f\u632f\u56fe\u50cf\uff0c\u8fd9\u5728\u57fa\u4e8e\u504f\u632f\u7684\u89c6\u89c9\u5e94\u7528\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u56e0\u4e3a\u53ef\u4ee5\u6839\u636e\u6355\u83b7\u7684\u504f\u632f\u76f4\u63a5\u8ba1\u7b97\u504f\u632f\u5ea6 (DoP) \u548c\u504f\u632f\u89d2 (AoP)\u56fe\u7247\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7247\u4e0a\u5fae\u504f\u5149\u7247\u963b\u6321\u4e86\u90e8\u5206\u5149\u7ebf\uff0c\u5bfc\u81f4\u4f20\u611f\u5668\u901a\u5e38\u9700\u8981\u8f83\u957f\u7684\u66dd\u5149\u65f6\u95f4\uff0c\u56e0\u6b64\u6355\u83b7\u7684\u504f\u5149\u56fe\u50cf\u5bb9\u6613\u51fa\u73b0\u76f8\u673a\u6296\u52a8\u5f15\u8d77\u7684\u8fd0\u52a8\u6a21\u7cca\uff0c\u5bfc\u81f4\u8ba1\u7b97\u7684 DoP \u548c AoP \u660e\u663e\u4e0b\u964d\u3002\u4f20\u7edf\u56fe\u50cf\u7684\u53bb\u6a21\u7cca\u65b9\u6cd5\u5728\u5904\u7406\u504f\u632f\u56fe\u50cf\u65f6\u901a\u5e38\u8868\u73b0\u51fa\u9000\u5316\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ea\u5173\u6ce8\u53bb\u6a21\u7cca\u800c\u4e0d\u8003\u8651\u504f\u632f\u7ea6\u675f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u632f\u56fe\u50cf\u53bb\u6a21\u7cca\u7ba1\u9053\uff0c\u901a\u8fc7\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u7b56\u7565\u4ee5\u504f\u632f\u611f\u77e5\u7684\u65b9\u5f0f\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u5c06\u95ee\u9898\u660e\u786e\u5730\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e0d\u9002\u5b9a\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u4e8c\u5143\u95ee\u9898\u3002\u9636\u6bb5\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u5904\u7406\u4e24\u4e2a\u5b50\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u504f\u632f\u7684\u89c6\u89c9\u5e94\u7528\uff08\u4f8b\u5982\u56fe\u50cf\u53bb\u96fe\u548c\u53cd\u5c04\u53bb\u9664\uff09\u7684\u6027\u80fd\u3002|[2402.18134v1](http://arxiv.org/pdf/2402.18134v1)|null|\n", "2402.18128": "|**2024-02-28**|**Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization**|\u4f7f\u7528\u591a\u7ea7\u4f18\u5316\u7684\u63a9\u853d\u81ea\u52a8\u7f16\u7801\u5668\u4e2d\u7684\u4e0b\u6e38\u4efb\u52a1\u5f15\u5bfc\u63a9\u853d\u5b66\u4e60|Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie|Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: https://github.com/Alexiland/MLOMAE|\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u662f\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u4e2d\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u4e00\u79cd\u8457\u540d\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u968f\u673a\u5c4f\u853d\u56fe\u50cf\u8865\u4e01\u5e76\u4f7f\u7528\u672a\u5c4f\u853d\u7684\u56fe\u50cf\u91cd\u5efa\u8fd9\u4e9b\u5c4f\u853d\u8865\u4e01\u6765\u8fdb\u884c\u64cd\u4f5c\u3002 MAE \u7684\u4e00\u4e2a\u5173\u952e\u9650\u5236\u5728\u4e8e\u5b83\u5ffd\u7565\u4e86\u4e0d\u540c\u8865\u4e01\u7684\u4e0d\u540c\u4fe1\u606f\u91cf\uff0c\u56e0\u4e3a\u5b83\u7edf\u4e00\u9009\u62e9\u8981\u5c4f\u853d\u7684\u8865\u4e01\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e00\u4e9b\u65b9\u6cd5\u63d0\u51fa\u4e86\u57fa\u4e8e\u8865\u4e01\u4fe1\u606f\u7684\u5c4f\u853d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4e0d\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7684\u5177\u4f53\u8981\u6c42\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fd9\u4e9b\u4efb\u52a1\u7684\u8868\u793a\u4e0d\u7406\u60f3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u7ea7\u4f18\u5316\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MLO-MAE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e0b\u6e38\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u53cd\u9988\u6765\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u6700\u4f73\u63a9\u7801\u7b56\u7565\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u51f8\u663e\u4e86 MLO-MAE \u5728\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u663e\u7740\u8fdb\u6b65\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u663e\u7740\u7684\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u548c\u6548\u7387\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/Alexiland/MLOMAE|[2402.18128v1](http://arxiv.org/pdf/2402.18128v1)|null|\n", "2402.18122": "|**2024-02-28**|**G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment**|G4G\uff1a\u5177\u6709\u7ec6\u7c92\u5ea6\u6a21\u5185\u5bf9\u9f50\u7684\u9ad8\u4fdd\u771f\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u7684\u901a\u7528\u6846\u67b6|Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu|Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G's success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods.|\u5c3d\u7ba1\u6709\u5927\u91cf\u5df2\u5b8c\u6210\u7684\u7814\u7a76\uff0c\u4f46\u901a\u8fc7\u4e0e\u4efb\u610f\u97f3\u9891\u76f8\u5bf9\u5e94\u7684\u9ad8\u5ea6\u540c\u6b65\u7684\u5634\u5507\u8fd0\u52a8\u6765\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8bf4\u8bdd\u9762\u90e8\u751f\u6210\u4ecd\u7136\u662f\u8be5\u9886\u57df\u7684\u91cd\u5927\u6311\u6218\u3002\u5df2\u53d1\u8868\u7814\u7a76\u7684\u7f3a\u70b9\u7ee7\u7eed\u8ba9\u8bb8\u591a\u7814\u7a76\u4eba\u5458\u611f\u5230\u56f0\u60d1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 G4G\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u7ec6\u7c92\u5ea6\u6a21\u5185\u5bf9\u9f50\u7684\u9ad8\u4fdd\u771f\u8bf4\u8bdd\u9762\u90e8\u751f\u6210\u7684\u901a\u7528\u6846\u67b6\u3002\u65e0\u8bba\u7ed9\u5b9a\u7684\u97f3\u8c03\u6216\u97f3\u91cf\u5982\u4f55\uff0cG4G \u90fd\u53ef\u4ee5\u91cd\u73b0\u539f\u59cb\u89c6\u9891\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4ea7\u751f\u9ad8\u5ea6\u540c\u6b65\u7684\u5634\u5507\u8fd0\u52a8\u3002 G4G\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u4f7f\u7528\u5bf9\u89d2\u77e9\u9635\u6765\u589e\u5f3a\u97f3\u9891\u56fe\u50cf\u6a21\u5185\u7279\u5f81\u7684\u666e\u901a\u5bf9\u9f50\uff0c\u8fd9\u663e\u7740\u589e\u52a0\u4e86\u6b63\u8d1f\u6837\u672c\u4e4b\u95f4\u7684\u6bd4\u8f83\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u591a\u5c3a\u5ea6\u76d1\u7763\u6a21\u5757\uff0c\u4ee5\u5168\u9762\u91cd\u73b0\u539f\u59cb\u89c6\u9891\u5728\u9762\u90e8\u533a\u57df\u7684\u611f\u77e5\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5f3a\u8c03\u5634\u5507\u8fd0\u52a8\u548c\u8f93\u5165\u97f3\u9891\u7684\u540c\u6b65\u3002\u7136\u540e\u4f7f\u7528\u878d\u5408\u7f51\u7edc\u8fdb\u4e00\u6b65\u878d\u5408\u9762\u90e8\u533a\u57df\u548c\u5176\u4f59\u533a\u57df\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u91cd\u73b0\u539f\u59cb\u89c6\u9891\u8d28\u91cf\u4ee5\u53ca\u9ad8\u5ea6\u540c\u6b65\u7684\u8bf4\u8bdd\u5634\u5507\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u6210\u5c31\u3002 G4G \u662f\u4e00\u4e2a\u6027\u80fd\u5353\u8d8a\u7684\u901a\u7528\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u751f\u6210\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u66f4\u63a5\u8fd1\u771f\u5b9e\u6c34\u5e73\u7684\u8c08\u8bdd\u89c6\u9891\u3002|[2402.18122v1](http://arxiv.org/pdf/2402.18122v1)|null|\n", "2402.18116": "|**2024-02-28**|**Block and Detail: Scaffolding Sketch-to-Image Generation**|\u5757\u548c\u7ec6\u8282\uff1a\u811a\u624b\u67b6\u8349\u56fe\u5230\u56fe\u50cf\u7684\u751f\u6210|Vishnu Sarukkai, Lu Yuan, Mia Tang, Maneesh Agrawala, Kayvon Fatahalian|We introduce a novel sketch-to-image tool that aligns with the iterative refinement process of artists. Our tool lets users sketch blocking strokes to coarsely represent the placement and form of objects and detail strokes to refine their shape and silhouettes. We develop a two-pass algorithm for generating high-fidelity images from such sketches at any point in the iterative process. In the first pass we use a ControlNet to generate an image that strictly follows all the strokes (blocking and detail) and in the second pass we add variation by renoising regions surrounding blocking strokes. We also present a dataset generation scheme that, when used to train a ControlNet architecture, allows regions that do not contain strokes to be interpreted as not-yet-specified regions rather than empty space. We show that this partial-sketch-aware ControlNet can generate coherent elements from partial sketches that only contain a small number of strokes. The high-fidelity images produced by our approach serve as scaffolds that can help the user adjust the shape and proportions of objects or add additional elements to the composition. We demonstrate the effectiveness of our approach with a variety of examples and evaluative comparisons.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8349\u56fe\u5230\u56fe\u50cf\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u4e0e\u827a\u672f\u5bb6\u7684\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\u76f8\u4e00\u81f4\u3002\u6211\u4eec\u7684\u5de5\u5177\u5141\u8bb8\u7528\u6237\u7ed8\u5236\u5757\u7b14\u5212\u4ee5\u7c97\u7565\u5730\u8868\u793a\u5bf9\u8c61\u7684\u4f4d\u7f6e\u548c\u5f62\u5f0f\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u7ed8\u5236\u7ec6\u8282\u7b14\u5212\u4ee5\u7ec6\u5316\u5176\u5f62\u72b6\u548c\u8f6e\u5ed3\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4e24\u904d\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u4efb\u4f55\u70b9\u4ece\u6b64\u7c7b\u8349\u56fe\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u3002\u5728\u7b2c\u4e00\u904d\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 ControlNet \u751f\u6210\u4e25\u683c\u9075\u5faa\u6240\u6709\u7b14\u5212\uff08\u5757\u548c\u7ec6\u8282\uff09\u7684\u56fe\u50cf\uff0c\u5728\u7b2c\u4e8c\u904d\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u5757\u7b14\u5212\u5468\u56f4\u7684\u533a\u57df\u8fdb\u884c\u518d\u566a\u5904\u7406\u6765\u6dfb\u52a0\u53d8\u5316\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u96c6\u751f\u6210\u65b9\u6848\uff0c\u5f53\u7528\u4e8e\u8bad\u7ec3 ControlNet \u67b6\u6784\u65f6\uff0c\u5141\u8bb8\u5c06\u4e0d\u5305\u542b\u7b14\u753b\u7684\u533a\u57df\u89e3\u91ca\u4e3a\u5c1a\u672a\u6307\u5b9a\u7684\u533a\u57df\u800c\u4e0d\u662f\u7a7a\u767d\u533a\u57df\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u90e8\u5206\u8349\u56fe\u611f\u77e5\u7684 ControlNet \u53ef\u4ee5\u4ece\u4ec5\u5305\u542b\u5c11\u91cf\u7b14\u753b\u7684\u90e8\u5206\u8349\u56fe\u751f\u6210\u8fde\u8d2f\u7684\u5143\u7d20\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u5145\u5f53\u652f\u67b6\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u8c03\u6574\u5bf9\u8c61\u7684\u5f62\u72b6\u548c\u6bd4\u4f8b\u6216\u5411\u6784\u56fe\u6dfb\u52a0\u5176\u4ed6\u5143\u7d20\u3002\u6211\u4eec\u901a\u8fc7\u5404\u79cd\u793a\u4f8b\u548c\u8bc4\u4f30\u6bd4\u8f83\u6765\u8bc1\u660e\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2402.18116v1](http://arxiv.org/pdf/2402.18116v1)|null|\n", "2402.18066": "|**2024-02-28**|**Six-Point Method for Multi-Camera Systems with Reduced Solution Space**|\u5177\u6709\u51cf\u5c11\u89e3\u7a7a\u95f4\u7684\u591a\u6444\u50cf\u673a\u7cfb\u7edf\u7684\u516d\u70b9\u6cd5|Banglei Guan, Ji Zhao, Laurent Kneip|Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of a multi-camera system, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated. Extensive experiments demonstrate that our solvers are more accurate than the state-of-the-art six-point methods, while achieving better performance in efficiency.|\u4f7f\u7528\u70b9\u5bf9\u5e94\uff08PC\uff09\u7684\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\u3002\u901a\u7528\u76f8\u673a\u6700\u5c11\u9700\u8981\u516d\u53f0 PC \u7684\u914d\u7f6e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u4f7f\u7528\u516d\u53f0 PC \u6765\u8ba1\u7b97\u591a\u76f8\u673a\u7cfb\u7edf\u7684 6DOF \u76f8\u5bf9\u4f4d\u59ff\u7684\u6700\u5c0f\u89e3\u7b97\u5668\uff0c\u5305\u62ec\u7528\u4e8e\u5e7f\u4e49\u76f8\u673a\u7684\u6700\u5c0f\u89e3\u7b97\u5668\u548c\u7528\u4e8e\u53cc\u76f8\u673a\u88c5\u5907\u5b9e\u9645\u914d\u7f6e\u7684\u4e24\u4e2a\u6700\u5c0f\u89e3\u7b97\u5668\u3002\u65b9\u7a0b\u6784\u9020\u57fa\u4e8e\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u89e3\u8026\u3002\u65cb\u8f6c\u7531\u51ef\u83b1\u6216\u56db\u5143\u6570\u53c2\u6570\u5316\u8868\u793a\uff0c\u5e73\u79fb\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u9690\u53d8\u91cf\u6280\u672f\u6765\u6d88\u9664\u3002\u5f53 PC \u7684\u5b50\u96c6\u5728\u4e24\u4e2a\u89c6\u56fe\u4e2d\u5173\u8054\u76f8\u540c\u7684\u76f8\u673a\u65f6\uff0c\u5c31\u4f1a\u53d1\u73b0\u5e76\u8bc1\u660e\u5149\u7ebf\u675f\u7ea6\u675f\u3002\u8fd9\u662f\u51cf\u5c11\u89e3\u6570\u548c\u751f\u6210\u6570\u503c\u7a33\u5b9a\u6c42\u89e3\u5668\u7684\u5173\u952e\u3002\u6b64\u5916\uff0c\u8fd8\u679a\u4e3e\u4e86\u591a\u6444\u50cf\u673a\u7cfb\u7edf\u516d\u70b9\u95ee\u9898\u7684\u6240\u6709\u914d\u7f6e\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6c42\u89e3\u5668\u6bd4\u6700\u5148\u8fdb\u7684\u516d\u70b9\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u540c\u65f6\u5728\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u8868\u73b0\u3002|[2402.18066v1](http://arxiv.org/pdf/2402.18066v1)|null|\n", "2402.18008": "|**2024-02-28**|**Fast and Interpretable 2D Homography Decomposition: Similarity-Kernel-Similarity and Affine-Core-Affine Transformations**|\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684 2D \u5355\u5e94\u6027\u5206\u89e3\uff1a\u76f8\u4f3c\u6027-\u6838-\u76f8\u4f3c\u6027\u548c\u4eff\u5c04-\u6838\u5fc3-\u4eff\u5c04\u53d8\u6362|Shen Cai, Zhanhao Wu, Lingxi Guo, Jiachun Wang, Siyu Zhang, Junchi Yan, Shuhan Shen|In this paper, we present two fast and interpretable decomposition methods for 2D homography, which are named Similarity-Kernel-Similarity (SKS) and Affine-Core-Affine (ACA) transformations respectively. Under the minimal $4$-point configuration, the first and the last similarity transformations in SKS are computed by two anchor points on target and source planes, respectively. Then, the other two point correspondences can be exploited to compute the middle kernel transformation with only four parameters. Furthermore, ACA uses three anchor points to compute the first and the last affine transformations, followed by computation of the middle core transformation utilizing the other one point correspondence. ACA can compute a homography up to a scale with only $85$ floating-point operations (FLOPs), without even any division operations. Therefore, as a plug-in module, ACA facilitates the traditional feature-based Random Sample Consensus (RANSAC) pipeline, as well as deep homography pipelines estimating $4$-point offsets. In addition to the advantages of geometric parameterization and computational efficiency, SKS and ACA can express each element of homography by a polynomial of input coordinates ($7$th degree to $9$th degree), extend the existing essential Similarity-Affine-Projective (SAP) decomposition and calculate 2D affine transformations in a unified way. Source codes are released in https://github.com/cscvlab/SKS-Homography.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\u4e8c\u7ef4\u5355\u5e94\u6027\u5206\u89e3\u65b9\u6cd5\uff0c\u5206\u522b\u79f0\u4e3a\u76f8\u4f3c\u6027-\u6838\u76f8\u4f3c\u6027\uff08SKS\uff09\u548c\u4eff\u5c04-\u6838\u5fc3-\u4eff\u5c04\uff08ACA\uff09\u53d8\u6362\u3002\u5728\u6700\u5c0f 4$ \u70b9\u914d\u7f6e\u4e0b\uff0cSKS \u4e2d\u7684\u7b2c\u4e00\u4e2a\u548c\u6700\u540e\u4e00\u4e2a\u76f8\u4f3c\u53d8\u6362\u5206\u522b\u7531\u76ee\u6807\u5e73\u9762\u548c\u6e90\u5e73\u9762\u4e0a\u7684\u4e24\u4e2a\u951a\u70b9\u8ba1\u7b97\u3002\u7136\u540e\uff0c\u53ef\u4ee5\u5229\u7528\u5176\u4ed6\u4e24\u70b9\u5bf9\u5e94\u5173\u7cfb\u6765\u8ba1\u7b97\u4ec5\u5177\u6709\u56db\u4e2a\u53c2\u6570\u7684\u4e2d\u95f4\u6838\u53d8\u6362\u3002\u6b64\u5916\uff0cACA \u4f7f\u7528\u4e09\u4e2a\u951a\u70b9\u6765\u8ba1\u7b97\u7b2c\u4e00\u4e2a\u548c\u6700\u540e\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362\uff0c\u7136\u540e\u5229\u7528\u53e6\u4e00\u4e2a\u70b9\u5bf9\u5e94\u5173\u7cfb\u8ba1\u7b97\u4e2d\u95f4\u6838\u5fc3\u53d8\u6362\u3002 ACA \u53ea\u9700 85 \u7f8e\u5143\u7684\u6d6e\u70b9\u8fd0\u7b97 (FLOP) \u5373\u53ef\u8ba1\u7b97\u51fa\u6700\u5927\u89c4\u6a21\u7684\u5355\u5e94\u6027\uff0c\u751a\u81f3\u4e0d\u9700\u8981\u4efb\u4f55\u9664\u6cd5\u8fd0\u7b97\u3002\u56e0\u6b64\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0cACA \u4fc3\u8fdb\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u968f\u673a\u6837\u672c\u5171\u8bc6 (RANSAC) \u7ba1\u9053\uff0c\u4ee5\u53ca\u4f30\u8ba1 4 \u7f8e\u5143\u70b9\u504f\u79fb\u7684\u6df1\u5ea6\u5355\u5e94\u6027\u7ba1\u9053\u3002\u9664\u4e86\u51e0\u4f55\u53c2\u6570\u5316\u548c\u8ba1\u7b97\u6548\u7387\u7684\u4f18\u70b9\u5916\uff0cSKS\u548cACA\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u5750\u6807\u7684\u591a\u9879\u5f0f\uff08$7$\u6b21\u5230$9$\u6b21\uff09\u6765\u8868\u8fbe\u5355\u5e94\u6027\u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u672c\u8d28\u7684\u76f8\u4f3c\u6027\u4eff\u5c04\u5c04\u5f71\uff08SAP \uff09\u7edf\u4e00\u5206\u89e3\u5e76\u8ba1\u7b97\u4e8c\u7ef4\u4eff\u5c04\u53d8\u6362\u3002\u6e90\u4ee3\u7801\u53d1\u5e03\u5728 https://github.com/cscvlab/SKS-Homography\u3002|[2402.18008v1](http://arxiv.org/pdf/2402.18008v1)|null|\n", "2402.17951": "|**2024-02-28**|**QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction**|QN-Mixer\uff1a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe CT \u91cd\u5efa\u7684\u62df\u725b\u987f MLP \u6df7\u5408\u5668\u6a21\u578b|Ishak Ayad, Nicolas Larue, Ma\u00ef K. Nguyen|Inverse problems span across diverse fields. In medical contexts, computed tomography (CT) plays a crucial role in reconstructing a patient's internal structure, presenting challenges due to artifacts caused by inherently ill-posed inverse problems. Previous research advanced image quality via post-processing and deep unrolling algorithms but faces challenges, such as extended convergence times with ultra-sparse data. Despite enhancements, resulting images often show significant artifacts, limiting their effectiveness for real-world diagnostic applications. We aim to explore deep second-order unrolling algorithms for solving imaging inverse problems, emphasizing their faster convergence and lower time complexity compared to common first-order methods like gradient descent. In this paper, we introduce QN-Mixer, an algorithm based on the quasi-Newton approach. We use learned parameters through the BFGS algorithm and introduce Incept-Mixer, an efficient neural architecture that serves as a non-local regularization term, capturing long-range dependencies within images. To address the computational demands typically associated with quasi-Newton algorithms that require full Hessian matrix computations, we present a memory-efficient alternative. Our approach intelligently downsamples gradient information, significantly reducing computational requirements while maintaining performance. The approach is validated through experiments on the sparse-view CT problem, involving various datasets and scanning protocols, and is compared with post-processing and deep unrolling state-of-the-art approaches. Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, all while reducing the number of unrolling iterations required.|\u9006\u95ee\u9898\u8de8\u8d8a\u4e0d\u540c\u7684\u9886\u57df\u3002\u5728\u533b\u5b66\u9886\u57df\uff0c\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u5728\u91cd\u5efa\u60a3\u8005\u5185\u90e8\u7ed3\u6784\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u7531\u4e8e\u56fa\u6709\u7684\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u9020\u6210\u7684\u4f2a\u5f71\u800c\u5e26\u6765\u4e86\u6311\u6218\u3002\u5148\u524d\u7684\u7814\u7a76\u901a\u8fc7\u540e\u5904\u7406\u548c\u6df1\u5ea6\u5c55\u5f00\u7b97\u6cd5\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u4f46\u9762\u4e34\u6311\u6218\uff0c\u4f8b\u5982\u8d85\u7a00\u758f\u6570\u636e\u7684\u6536\u655b\u65f6\u95f4\u5ef6\u957f\u3002\u5c3d\u7ba1\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u4f46\u751f\u6210\u7684\u56fe\u50cf\u901a\u5e38\u4f1a\u663e\u793a\u51fa\u660e\u663e\u7684\u4f2a\u5f71\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u8bca\u65ad\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u63a2\u7d22\u7528\u4e8e\u89e3\u51b3\u6210\u50cf\u9006\u95ee\u9898\u7684\u6df1\u5ea6\u4e8c\u9636\u5c55\u5f00\u7b97\u6cd5\uff0c\u5f3a\u8c03\u4e0e\u68af\u5ea6\u4e0b\u964d\u7b49\u5e38\u89c1\u7684\u4e00\u9636\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u4eec\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 QN-Mixer\uff0c\u4e00\u79cd\u57fa\u4e8e\u62df\u725b\u987f\u65b9\u6cd5\u7684\u7b97\u6cd5\u3002\u6211\u4eec\u901a\u8fc7 BFGS \u7b97\u6cd5\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u53c2\u6570\uff0c\u5e76\u5f15\u5165 Incept-Mixer\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u53ef\u7528\u4f5c\u975e\u5c40\u90e8\u6b63\u5219\u5316\u9879\uff0c\u6355\u83b7\u56fe\u50cf\u5185\u7684\u8fdc\u7a0b\u4f9d\u8d56\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u901a\u5e38\u4e0e\u9700\u8981\u5b8c\u6574 Hessian \u77e9\u9635\u8ba1\u7b97\u7684\u62df\u725b\u987f\u7b97\u6cd5\u76f8\u5173\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u667a\u80fd\u5730\u5bf9\u68af\u5ea6\u4fe1\u606f\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u7740\u964d\u4f4e\u8ba1\u7b97\u8981\u6c42\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u89c6\u56fe CT \u95ee\u9898\u7684\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u6d89\u53ca\u5404\u79cd\u6570\u636e\u96c6\u548c\u626b\u63cf\u534f\u8bae\uff0c\u5e76\u4e0e\u540e\u5904\u7406\u548c\u6df1\u5ea6\u5c55\u5f00\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728 SSIM \u548c PSNR \u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6240\u9700\u7684\u5c55\u5f00\u8fed\u4ee3\u6b21\u6570\u3002|[2402.17951v1](http://arxiv.org/pdf/2402.17951v1)|null|\n"}}