{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.06019": "|**2024-01-11**|**Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios**|\u4f7f\u7528\u6df7\u5408\u771f\u5b9e\u548c\u865a\u62df\u573a\u666f\u7684\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u81ea\u52a8\u673a\u573a\u8def\u9762\u68c0\u67e5|Pablo Alonso, Jon Ander I\u00f1iguez de Gordoa, Juan Diego Ortega, Sara Garc\u00eda, Francisco Javier Iriarte, Marcos Nieto|Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time. To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections. UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost. In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs. The proposed method is based on Deep Learning (DL) to segment defects in the image. The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation. To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets. We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios.|\u8dd1\u9053\u548c\u6ed1\u884c\u9053\u8def\u9762\u5728\u5176\u9884\u8ba1\u4f7f\u7528\u5bff\u547d\u671f\u95f4\u4f1a\u627f\u53d7\u9ad8\u538b\u529b\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5bfc\u81f4\u5176\u72b6\u51b5\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u6076\u5316\u3002\u4e3a\u4e86\u786e\u4fdd\u673a\u573a\u8def\u9762\u72b6\u51b5\u786e\u4fdd\u4e0d\u95f4\u65ad\u548c\u5f39\u6027\u8fd0\u884c\uff0c\u76d1\u6d4b\u5176\u72b6\u51b5\u5e76\u8fdb\u884c\u5b9a\u671f\u68c0\u67e5\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u68c0\u67e5\u7531\u4e8e\u5176\u5e7f\u6cdb\u7684\u76d1\u63a7\u80fd\u529b\u548c\u964d\u4f4e\u7684\u6210\u672c\u800c\u6700\u8fd1\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u65e0\u4eba\u673a\u6355\u83b7\u7684\u56fe\u50cf\u81ea\u52a8\u8bc6\u522b\u8def\u9762\u7834\u635f\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6765\u5206\u5272\u56fe\u50cf\u4e2d\u7684\u7f3a\u9677\u3002\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u901a\u8fc7\u4f7f\u7528 EfficientNet \u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u5206\u5272\u7684\u4f18\u5316\u5b9e\u73b0\uff0c\u5229\u7528\u4e86\u65e0\u4eba\u673a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u4f4e\u8ba1\u7b97\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8bad\u7ec3\u6240\u9700\u6ce8\u91ca\u6570\u636e\u7684\u7f3a\u4e4f\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u6765\u6269\u5c55\u53ef\u7528\u7684\u9047\u9669\u6570\u636e\u96c6\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u6d4b\u8bd5\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u7531\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u8bad\u7ec3\u56fe\u50cf\u7ec4\u6210\u7684\u6df7\u5408\u6570\u636e\u96c6\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u7ed3\u679c\u3002|[2401.06019v1](http://arxiv.org/pdf/2401.06019v1)|null|\n", "2401.06010": "|**2024-01-11**|**Attention to detail: inter-resolution knowledge distillation**|\u5173\u6ce8\u7ec6\u8282\uff1a\u5206\u8fa8\u7387\u95f4\u77e5\u8bc6\u84b8\u998f|Roc\u00edo del Amor, Julio Silva-Rodr\u00edguez, Adri\u00e1n Colomer, Valery Naranjo|The development of computer vision solutions for gigapixel images in digital pathology is hampered by significant computational limitations due to the large size of whole slide images. In particular, digitizing biopsies at high resolutions is a time-consuming process, which is necessary due to the worsening results from the decrease in image detail. To alleviate this issue, recent literature has proposed using knowledge distillation to enhance the model performance at reduced image resolutions. In particular, soft labels and features extracted at the highest magnification level are distilled into a model that takes lower-magnification images as input. However, this approach fails to transfer knowledge about the most discriminative image regions in the classification process, which may be lost when the resolution is decreased. In this work, we propose to distill this information by incorporating attention maps during training. In particular, our formulation leverages saliency maps of the target class via grad-CAMs, which guides the lower-resolution Student model to match the Teacher distribution by minimizing the l2 distance between them. Comprehensive experiments on prostate histology image grading demonstrate that the proposed approach substantially improves the model performance across different image resolutions compared to previous literature.|\u7531\u4e8e\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u5c3a\u5bf8\u8f83\u5927\uff0c\u8ba1\u7b97\u9650\u5236\u6781\u5927\uff0c\u963b\u788d\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u5341\u4ebf\u50cf\u7d20\u56fe\u50cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002\u7279\u522b\u662f\uff0c\u4ee5\u9ad8\u5206\u8fa8\u7387\u5bf9\u6d3b\u68c0\u8fdb\u884c\u6570\u5b57\u5316\u662f\u4e00\u4e2a\u8017\u65f6\u7684\u8fc7\u7a0b\uff0c\u7531\u4e8e\u56fe\u50cf\u7ec6\u8282\u7684\u51cf\u5c11\u4f1a\u5bfc\u81f4\u7ed3\u679c\u6076\u5316\uff0c\u56e0\u6b64\u8fd9\u662f\u5fc5\u8981\u7684\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u8fd1\u7684\u6587\u732e\u63d0\u51fa\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6765\u589e\u5f3a\u56fe\u50cf\u5206\u8fa8\u7387\u964d\u4f4e\u65f6\u7684\u6a21\u578b\u6027\u80fd\u3002\u7279\u522b\u662f\uff0c\u5728\u6700\u9ad8\u653e\u5927\u500d\u7387\u7ea7\u522b\u63d0\u53d6\u7684\u8f6f\u6807\u7b7e\u548c\u7279\u5f81\u88ab\u63d0\u70bc\u6210\u4ee5\u8f83\u4f4e\u653e\u5927\u500d\u7387\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u4f20\u9012\u6709\u5173\u5206\u7c7b\u8fc7\u7a0b\u4e2d\u6700\u5177\u8fa8\u522b\u529b\u7684\u56fe\u50cf\u533a\u57df\u7684\u77e5\u8bc6\uff0c\u5f53\u5206\u8fa8\u7387\u964d\u4f4e\u65f6\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u80fd\u4f1a\u4e22\u5931\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5408\u5e76\u6ce8\u610f\u529b\u56fe\u6765\u63d0\u53d6\u8fd9\u4e9b\u4fe1\u606f\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u516c\u5f0f\u901a\u8fc7 grad-CAM \u5229\u7528\u76ee\u6807\u7c7b\u7684\u663e\u7740\u6027\u56fe\uff0c\u5b83\u901a\u8fc7\u6700\u5c0f\u5316\u5b83\u4eec\u4e4b\u95f4\u7684 l2 \u8ddd\u79bb\u6765\u6307\u5bfc\u8f83\u4f4e\u5206\u8fa8\u7387\u7684\u5b66\u751f\u6a21\u578b\u5339\u914d\u6559\u5e08\u5206\u5e03\u3002\u524d\u5217\u817a\u7ec4\u7ec7\u5b66\u56fe\u50cf\u5206\u7ea7\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u6587\u732e\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5927\u5927\u63d0\u9ad8\u4e86\u4e0d\u540c\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002|[2401.06010v1](http://arxiv.org/pdf/2401.06010v1)|null|\n", "2401.06009": "|**2024-01-11**|**Sea ice detection using concurrent multispectral and synthetic aperture radar imagery**|\u4f7f\u7528\u5e76\u53d1\u591a\u5149\u8c31\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u8fdb\u884c\u6d77\u51b0\u63a2\u6d4b|Martin S J Rogers, Maria Fox, Andrew Fleming, Louisa van Zeeland, Jeremy Wilkinson, J. Scott Hosking|Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation. Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions.|\u5408\u6210\u5b54\u5f84\u96f7\u8fbe (SAR) \u56fe\u50cf\u662f\u7528\u4e8e\u6d77\u51b0\u6d4b\u7ed8\u7684\u4e3b\u8981\u6570\u636e\u7c7b\u578b\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u65f6\u7a7a\u8986\u76d6\u8303\u56f4\uff0c\u5e76\u4e14\u80fd\u591f\u72ec\u7acb\u4e8e\u4e91\u548c\u7167\u660e\u6761\u4ef6\u68c0\u6d4b\u6d77\u51b0\u3002\u7531\u4e8e\u56fe\u50cf\u4e2d\u5b58\u5728\u6a21\u7cca\u4fe1\u53f7\u548c\u566a\u58f0\uff0c\u4f7f\u7528 SAR \u56fe\u50cf\u8fdb\u884c\u81ea\u52a8\u6d77\u51b0\u68c0\u6d4b\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002\u76f8\u53cd\uff0c\u4f7f\u7528\u591a\u5149\u8c31\u56fe\u50cf (MSI) \u53ef\u4ee5\u8f7b\u677e\u533a\u5206\u51b0\u548c\u6c34\uff0c\u4f46\u5728\u6781\u5730\u5730\u533a\uff0c\u6d77\u6d0b\u8868\u9762\u7ecf\u5e38\u88ab\u4e91\u906e\u6321\uff0c\u6216\u8005\u592a\u9633\u53ef\u80fd\u597d\u51e0\u4e2a\u6708\u90fd\u4e0d\u4f1a\u51fa\u73b0\u5728\u5730\u5e73\u7ebf\u4e0a\u65b9\u3002\u4e3a\u4e86\u89e3\u51b3\u5176\u4e2d\u4e00\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5e76\u53d1\u591a\u5149\u8c31\u53ef\u89c1\u5149\u548c SAR \u56fe\u50cf\u8fdb\u884c\u6d77\u51b0\u68c0\u6d4b\u8bad\u7ec3\u7684\u65b0\u5de5\u5177 (ViSual\\_IceD)\u3002 ViSual\\_IceD \u662f\u4e00\u79cd\u57fa\u4e8e\u7ecf\u5178 U-Net \u67b6\u6784\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\uff0c\u5305\u542b\u4e24\u4e2a\u5e76\u884c\u7f16\u7801\u5668\u7ea7\uff0c\u80fd\u591f\u878d\u5408\u548c\u4e32\u8054\u5305\u542b\u4e0d\u540c\u7a7a\u95f4\u5206\u8fa8\u7387\u7684 MSI \u548c SAR \u56fe\u50cf\u3002\u5c06 ViSual\\_IceD \u7684\u6027\u80fd\u4e0e\u4f7f\u7528\u4e32\u8054 MSI \u548c SAR \u56fe\u50cf\u8bad\u7ec3\u7684 U-Net \u6a21\u578b\u4ee5\u53ca\u4ec5\u5728 MSI \u6216 SAR \u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002 ViSual\\_IceD \u4f18\u4e8e\u5176\u4ed6\u7f51\u7edc\uff0cF1 \u5206\u6570\u6bd4\u6b21\u4f18\u7f51\u7edc\u9ad8 1.60\\% \u4e2a\u767e\u5206\u70b9\uff0c\u7ed3\u679c\u8868\u660e ViSual\\_IceD \u5728\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u56fe\u50cf\u7c7b\u578b\u5177\u6709\u9009\u62e9\u6027\u3002 ViSualIceD \u7684\u8f93\u51fa\u4e0e AMSR2 \u65e0\u6e90\u5fae\u6ce2 (PMW) \u4f20\u611f\u5668\u7684\u6d77\u51b0\u6d53\u5ea6\u4ea7\u54c1\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u51f8\u663e\u4e86 ViSual_IceD \u662f\u5982\u4f55\u4e0e PMW \u6570\u636e\u7ed3\u5408\u4f7f\u7528\u7684\u6709\u7528\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u6cbf\u6d77\u5730\u533a\u3002\u968f\u7740 MSI \u548c SAR \u56fe\u50cf\u7684\u65f6\u7a7a\u8986\u76d6\u8303\u56f4\u4e0d\u65ad\u589e\u52a0\uff0cViSual\\_IceD \u4e3a\u6781\u5730\u5730\u533a\u7a33\u5065\u3001\u51c6\u786e\u7684\u6d77\u51b0\u8986\u76d6\u8303\u56f4\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002|[2401.06009v1](http://arxiv.org/pdf/2401.06009v1)|null|\n", "2401.06000": "|**2024-01-11**|**Body-Area Capacitive or Electric Field Sensing for Human Activity Recognition and Human-Computer Interaction: A Comprehensive Survey**|\u7528\u4e8e\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u8eab\u4f53\u533a\u57df\u7535\u5bb9\u6216\u7535\u573a\u611f\u5e94\uff1a\u7efc\u5408\u8c03\u67e5|Sizhen Bian, Mengxi Liu, Bo Zhou, Paul Lukowicz, Michele Magno|Due to the fact that roughly sixty percent of the human body is essentially composed of water, the human body is inherently a conductive object, being able to, firstly, form an inherent electric field from the body to the surroundings and secondly, deform the distribution of an existing electric field near the body. Body-area capacitive sensing, also called body-area electric field sensing, is becoming a promising alternative for wearable devices to accomplish certain tasks in human activity recognition and human-computer interaction. Over the last decade, researchers have explored plentiful novel sensing systems backed by the body-area electric field. On the other hand, despite the pervasive exploration of the body-area electric field, a comprehensive survey does not exist for an enlightening guideline. Moreover, the various hardware implementations, applied algorithms, and targeted applications result in a challenging task to achieve a systematic overview of the subject. This paper aims to fill in the gap by comprehensively summarizing the existing works on body-area capacitive sensing so that researchers can have a better view of the current exploration status. To this end, we first sorted the explorations into three domains according to the involved body forms: body-part electric field, whole-body electric field, and body-to-body electric field, and enumerated the state-of-art works in the domains with a detailed survey of the backed sensing tricks and targeted applications. We then summarized the three types of sensing frontends in circuit design, which is the most critical part in body-area capacitive sensing, and analyzed the data processing pipeline categorized into three kinds of approaches. Finally, we described the challenges and outlooks of body-area electric sensing.|\u7531\u4e8e\u4eba\u4f53\u5927\u7ea6\u767e\u5206\u4e4b\u516d\u5341\u7684\u6210\u5206\u57fa\u672c\u4e0a\u662f\u6c34\uff0c\u4eba\u4f53\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5bfc\u7535\u7269\u4f53\uff0c\u9996\u5148\u80fd\u591f\u4ece\u8eab\u4f53\u5230\u5468\u56f4\u73af\u5883\u5f62\u6210\u56fa\u6709\u7684\u7535\u573a\uff0c\u5176\u6b21\u4f7f\u5206\u5e03\u53d8\u5f62\u8eab\u4f53\u9644\u8fd1\u73b0\u6709\u7684\u7535\u573a\u3002\u8eab\u4f53\u533a\u57df\u7535\u5bb9\u4f20\u611f\uff0c\u4e5f\u79f0\u4e3a\u8eab\u4f53\u533a\u57df\u7535\u573a\u4f20\u611f\uff0c\u6b63\u5728\u6210\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u5b8c\u6210\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u67d0\u4e9b\u4efb\u52a1\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u4e86\u8bb8\u591a\u7531\u8eab\u4f53\u533a\u57df\u7535\u573a\u652f\u6301\u7684\u65b0\u578b\u4f20\u611f\u7cfb\u7edf\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5c3d\u7ba1\u5bf9\u8eab\u4f53\u533a\u57df\u7535\u573a\u7684\u63a2\u7d22\u666e\u904d\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u5177\u6709\u542f\u53d1\u6027\u7684\u6307\u5bfc\u65b9\u9488\u7684\u5168\u9762\u8c03\u67e5\u3002\u6b64\u5916\uff0c\u5404\u79cd\u786c\u4ef6\u5b9e\u73b0\u3001\u5e94\u7528\u7b97\u6cd5\u548c\u76ee\u6807\u5e94\u7528\u5bfc\u81f4\u5b9e\u73b0\u8be5\u4e3b\u9898\u7684\u7cfb\u7edf\u6982\u8ff0\u6210\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5168\u9762\u603b\u7ed3\u4eba\u4f53\u533a\u57df\u7535\u5bb9\u4f20\u611f\u7684\u73b0\u6709\u5de5\u4f5c\u6765\u586b\u8865\u7a7a\u767d\uff0c\u4ee5\u4fbf\u7814\u7a76\u4eba\u5458\u66f4\u597d\u5730\u4e86\u89e3\u5f53\u524d\u7684\u63a2\u7d22\u73b0\u72b6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u6839\u636e\u6240\u6d89\u53ca\u7684\u8eab\u4f53\u5f62\u5f0f\u5c06\u63a2\u7d22\u5206\u4e3a\u4e09\u4e2a\u9886\u57df\uff1a\u8eab\u4f53\u90e8\u4f4d\u7535\u573a\u3001\u5168\u8eab\u7535\u573a\u548c\u8eab\u4f53\u5bf9\u8eab\u4f53\u7535\u573a\uff0c\u5e76\u5217\u4e3e\u4e86\u8fd9\u4e9b\u9886\u57df\u7684\u6700\u65b0\u6210\u679c\u3002\u5bf9\u652f\u6301\u7684\u4f20\u611f\u6280\u5de7\u548c\u76ee\u6807\u5e94\u7528\u8fdb\u884c\u8be6\u7ec6\u8c03\u67e5\u3002\u7136\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u4e09\u79cd\u7c7b\u578b\u7684\u4f20\u611f\u524d\u7aef\uff0c\u8fd9\u662f\u4eba\u4f53\u533a\u57df\u7535\u5bb9\u4f20\u611f\u4e2d\u6700\u5173\u952e\u7684\u90e8\u5206\uff0c\u5e76\u5206\u6790\u4e86\u5206\u4e3a\u4e09\u79cd\u65b9\u6cd5\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3002\u6700\u540e\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u8eab\u4f53\u533a\u57df\u7535\u4f20\u611f\u7684\u6311\u6218\u548c\u524d\u666f\u3002|[2401.06000v1](http://arxiv.org/pdf/2401.06000v1)|null|\n", "2401.05925": "|**2024-01-11**|**CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians**|CoSSegGaussians\uff1a\u7d27\u51d1\u4e14\u5feb\u901f\u7684\u573a\u666f\u5206\u5272 3D \u9ad8\u65af|Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan|We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method firstly optimizes Gaussian points' position, convariance and color attributes under the supervision of RGB images. After Gaussian Locating, we distill multi-scale DINO features extracted from images through unprojection to each Gaussian, which is then incorporated with spatial features from the fast point features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is applied to the multi-scale fused features to obtain compact segmentation. Experimental results show that our model can perform high-quality zero-shot scene segmentation, as our model outperforms other segmentation methods on both semantic and panoptic segmentation task, meanwhile consumes approximately only 10% segmenting time compared to NeRF-based segmentation. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians|\u6211\u4eec\u63d0\u51fa\u4e86\u7d27\u51d1\u548c\u5feb\u901f\u5206\u5272 3D \u9ad8\u65af\uff08CoSSegGaussians\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528 RGB \u56fe\u50cf\u8f93\u5165\u4ee5\u5feb\u901f\u6e32\u67d3\u901f\u5ea6\u8fdb\u884c\u7d27\u51d1 3D \u4e00\u81f4\u573a\u666f\u5206\u5272\u7684\u65b9\u6cd5\u3002\u4ee5\u524d\u57fa\u4e8e NeRF \u7684 3D \u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9690\u5f0f\u6216\u4f53\u7d20\u795e\u7ecf\u573a\u666f\u8868\u793a\u548c\u5149\u7ebf\u884c\u8fdb\u4f53\u79ef\u6e32\u67d3\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u975e\u5e38\u8017\u65f6\u3002\u6700\u8fd1\u76843D Gaussian Splatting\u663e\u7740\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\uff0c\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u9ad8\u65af\u7684\u5206\u5272\u65b9\u6cd5\uff08\u4f8b\u5982\uff1a\u9ad8\u65af\u5206\u7ec4\uff09\u65e0\u6cd5\u63d0\u4f9b\u7d27\u51d1\u7684\u5206\u5272\u63a9\u6a21\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u5206\u5272\u4e2d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u76f4\u63a5\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u7d27\u51d1\u6027\u9020\u6210\u7684\u5f53\u9047\u5230\u4e0d\u4e00\u81f4\u7684 2D \u673a\u5668\u751f\u6210\u6807\u7b7e\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u5206\u914d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u6d45\u5c42\u89e3\u7801\u7f51\u7edc\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u70b9\u6620\u5c04\u878d\u5408\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5feb\u901f\u5b9e\u73b0\u7d27\u51d1\u4e14\u53ef\u9760\u7684\u96f6\u955c\u5934\u573a\u666f\u5206\u5272\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u5728 RGB \u56fe\u50cf\u7684\u76d1\u7763\u4e0b\u4f18\u5316\u9ad8\u65af\u70b9\u7684\u4f4d\u7f6e\u3001\u534f\u65b9\u5dee\u548c\u989c\u8272\u5c5e\u6027\u3002\u5728\u9ad8\u65af\u5b9a\u4f4d\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u901a\u8fc7\u975e\u6295\u5f71\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u591a\u5c3a\u5ea6 DINO \u7279\u5f81\u63d0\u53d6\u5230\u6bcf\u4e2a\u9ad8\u65af\uff0c\u7136\u540e\u5c06\u5176\u4e0e\u6765\u81ea\u5feb\u901f\u70b9\u7279\u5f81\u5904\u7406\u7f51\u7edc\uff08\u5373 RandLA-Net\uff09\u7684\u7a7a\u95f4\u7279\u5f81\u5408\u5e76\u3002\u7136\u540e\u5c06\u6d45\u5c42\u89e3\u7801MLP\u5e94\u7528\u4e8e\u591a\u5c3a\u5ea6\u878d\u5408\u7279\u5f81\u4ee5\u83b7\u5f97\u7d27\u51d1\u5206\u5272\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6267\u884c\u9ad8\u8d28\u91cf\u7684\u96f6\u955c\u5934\u573a\u666f\u5206\u5272\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u5206\u5272\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u57fa\u4e8e NeRF \u7684\u5206\u5272\u76f8\u6bd4\uff0c\u4ec5\u6d88\u8017\u5927\u7ea6 10% \u7684\u5206\u5272\u65f6\u95f4\u3002\u4ee3\u7801\u548c\u66f4\u591a\u7ed3\u679c\u5c06\u5728 https://David-Dou.github.io/CoSSegGaussians \u63d0\u4f9b|[2401.05925v1](http://arxiv.org/pdf/2401.05925v1)|null|\n", "2401.05906": "|**2024-01-11**|**PartSTAD: 2D-to-3D Part Segmentation Task Adaptation**|PartSTAD\uff1a2D \u5230 3D \u96f6\u4ef6\u5206\u5272\u4efb\u52a1\u9002\u914d|Hyunjin Kim, Minhyuk Sung|We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D segmentation lifting. Recent studies have highlighted the advantages of utilizing 2D segmentation models to achieve high-quality 3D segmentation through few-shot adaptation. However, previous approaches have focused on adapting 2D segmentation models for domain shift to rendered images and synthetic text descriptions, rather than optimizing the model specifically for 3D segmentation. Our proposed task adaptation method finetunes a 2D bounding box prediction model with an objective function for 3D segmentation. We introduce weights for 2D bounding boxes for adaptive merging and learn the weights using a small additional neural network. Additionally, we incorporate SAM, a foreground segmentation model on a bounding box, to improve the boundaries of 2D segments and consequently those of 3D segmentation. Our experiments on the PartNet-Mobility dataset show significant improvements with our task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p improvement in mAP_50 for semantic and instance segmentation compared to the SotA few-shot 3D segmentation model.|\u6211\u4eec\u4ecb\u7ecd\u4e86 PartSTAD\uff0c\u4e00\u79cd\u4e13\u4e3a 2D \u5230 3D \u5206\u5272\u63d0\u5347\u7684\u4efb\u52a1\u9002\u914d\u800c\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u5229\u7528 2D \u5206\u5272\u6a21\u578b\u901a\u8fc7\u5c11\u6837\u672c\u81ea\u9002\u5e94\u5b9e\u73b0\u9ad8\u8d28\u91cf 3D \u5206\u5272\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u8c03\u6574 2D \u5206\u5272\u6a21\u578b\u4ee5\u5c06\u57df\u8f6c\u79fb\u5230\u6e32\u67d3\u56fe\u50cf\u548c\u5408\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u800c\u4e0d\u662f\u4e13\u95e8\u9488\u5bf9 3D \u5206\u5272\u4f18\u5316\u6a21\u578b\u3002\u6211\u4eec\u63d0\u51fa\u7684\u4efb\u52a1\u9002\u5e94\u65b9\u6cd5\u4f7f\u7528 3D \u5206\u5272\u7684\u76ee\u6807\u51fd\u6570\u5fae\u8c03 2D \u8fb9\u754c\u6846\u9884\u6d4b\u6a21\u578b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u7528\u4e8e\u81ea\u9002\u5e94\u5408\u5e76\u7684 2D \u8fb9\u754c\u6846\u7684\u6743\u91cd\uff0c\u5e76\u4f7f\u7528\u5c0f\u578b\u9644\u52a0\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u6743\u91cd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86 SAM\uff08\u8fb9\u754c\u6846\u4e0a\u7684\u524d\u666f\u5206\u5272\u6a21\u578b\uff09\uff0c\u4ee5\u6539\u5584 2D \u5206\u5272\u7684\u8fb9\u754c\uff0c\u4ece\u800c\u6539\u5584 3D \u5206\u5272\u7684\u8fb9\u754c\u3002\u6211\u4eec\u5728 PartNet-Mobility \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u4efb\u52a1\u9002\u5e94\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\uff0c\u4e0e SotA \u5c11\u6837\u672c 3D \u5206\u5272\u6a21\u578b\u76f8\u6bd4\uff0c\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u7684 mIoU \u63d0\u9ad8\u4e86 7.0%p\uff0cmAP_50 \u63d0\u9ad8\u4e86 5.2%p\u3002|[2401.05906v1](http://arxiv.org/pdf/2401.05906v1)|null|\n", "2401.05820": "|**2024-01-11**|**Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification**|\u7535\u963b\u5b58\u50a8\u5668\u4e2d\u7684\u566a\u58f0\u5bf9\u56fe\u50cf\u5206\u7c7b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5f71\u54cd|Yannick Emonds, Kai Xi, Holger Fr\u00f6ning|Resistive memory is a promising alternative to SRAM, but is also an inherently unstable device that requires substantial effort to ensure correct read and write operations. To avoid the associated costs in terms of area, time and energy, the present work is concerned with exploring how much noise in memory operations can be tolerated by image classification tasks based on neural networks. We introduce a special noisy operator that mimics the noise in an exemplary resistive memory unit, explore the resilience of convolutional neural networks on the CIFAR-10 classification task, and discuss a couple of countermeasures to improve this resilience.|\u7535\u963b\u5f0f\u5b58\u50a8\u5668\u662f SRAM \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u54c1\uff0c\u4f46\u5b83\u672c\u8d28\u4e0a\u4e5f\u662f\u4e00\u79cd\u4e0d\u7a33\u5b9a\u7684\u8bbe\u5907\uff0c\u9700\u8981\u4ed8\u51fa\u5927\u91cf\u52aa\u529b\u624d\u80fd\u786e\u4fdd\u6b63\u786e\u7684\u8bfb\u5199\u64cd\u4f5c\u3002\u4e3a\u4e86\u907f\u514d\u5728\u9762\u79ef\u3001\u65f6\u95f4\u548c\u80fd\u6e90\u65b9\u9762\u7684\u76f8\u5173\u6210\u672c\uff0c\u76ee\u524d\u7684\u5de5\u4f5c\u81f4\u529b\u4e8e\u63a2\u7d22\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u53ef\u4ee5\u5bb9\u5fcd\u5185\u5b58\u64cd\u4f5c\u4e2d\u7684\u591a\u5c11\u566a\u58f0\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7279\u6b8a\u7684\u566a\u58f0\u7b97\u5b50\uff0c\u5b83\u6a21\u4eff\u793a\u4f8b\u6027\u7535\u963b\u5b58\u50a8\u5355\u5143\u4e2d\u7684\u566a\u58f0\uff0c\u63a2\u7d22\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728 CIFAR-10 \u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5f39\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e00\u4e9b\u63d0\u9ad8\u8fd9\u79cd\u5f39\u6027\u7684\u5bf9\u7b56\u3002|[2401.05820v1](http://arxiv.org/pdf/2401.05820v1)|null|\n", "2401.05771": "|**2024-01-11**|**Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image Classification**|\u5411 Zoom \u5b66\u4e60\uff1aWCE \u56fe\u50cf\u5206\u7c7b\u7684\u89e3\u8026\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60|Kunpeng Qiu, Zhiying Zhou, Yongxin Guo|Accurate lesion classification in Wireless Capsule Endoscopy (WCE) images is vital for early diagnosis and treatment of gastrointestinal (GI) cancers. However, this task is confronted with challenges like tiny lesions and background interference. Additionally, WCE images exhibit higher intra-class variance and inter-class similarities, adding complexity. To tackle these challenges, we propose Decoupled Supervised Contrastive Learning for WCE image classification, learning robust representations from zoomed-in WCE images generated by Saliency Augmentor. Specifically, We use uniformly down-sampled WCE images as anchors and WCE images from the same class, especially their zoomed-in images, as positives. This approach empowers the Feature Extractor to capture rich representations from various views of the same image, facilitated by Decoupled Supervised Contrastive Learning. Training a linear Classifier on these representations within 10 epochs yields an impressive 92.01% overall accuracy, surpassing the prior state-of-the-art (SOTA) by 0.72% on a blend of two publicly accessible WCE datasets. Code is available at: https://github.com/Qiukunpeng/DSCL.|\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c (WCE) \u56fe\u50cf\u4e2d\u51c6\u786e\u7684\u75c5\u53d8\u5206\u7c7b\u5bf9\u4e8e\u80c3\u80a0\u9053 (GI) \u764c\u75c7\u7684\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u9879\u4efb\u52a1\u9762\u4e34\u7740\u5fae\u5c0f\u75c5\u53d8\u548c\u80cc\u666f\u5e72\u6270\u7b49\u6311\u6218\u3002\u6b64\u5916\uff0cWCE \u56fe\u50cf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7c7b\u5185\u65b9\u5dee\u548c\u7c7b\u95f4\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u590d\u6742\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e WCE \u56fe\u50cf\u5206\u7c7b\u7684\u89e3\u8026\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ece\u663e\u7740\u6027\u589e\u5f3a\u5668\u751f\u6210\u7684\u653e\u5927 WCE \u56fe\u50cf\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4f7f\u7528\u7edf\u4e00\u4e0b\u91c7\u6837\u7684 WCE \u56fe\u50cf\u4f5c\u4e3a\u951a\u70b9\uff0c\u4f7f\u7528\u6765\u81ea\u540c\u4e00\u7c7b\u7684 WCE \u56fe\u50cf\uff0c\u7279\u522b\u662f\u5b83\u4eec\u7684\u653e\u5927\u56fe\u50cf\u4f5c\u4e3a\u6b63\u503c\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u591f\u5728\u89e3\u8026\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u4fc3\u8fdb\u4e0b\u4ece\u540c\u4e00\u56fe\u50cf\u7684\u4e0d\u540c\u89c6\u56fe\u4e2d\u6355\u83b7\u4e30\u5bcc\u7684\u8868\u793a\u3002\u5728 10 \u4e2a epoch \u5185\u5bf9\u8fd9\u4e9b\u8868\u793a\u8fdb\u884c\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u603b\u4f53\u51c6\u786e\u7387\u9ad8\u8fbe 92.01%\uff0c\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u5728\u4e24\u4e2a\u53ef\u516c\u5f00\u8bbf\u95ee\u7684 WCE \u6570\u636e\u96c6\u7684\u6df7\u5408\u4e0a\uff0c\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f (SOTA) \u63d0\u9ad8\u4e86 0.72%\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/Qiukunpeng/DSCL\u3002|[2401.05771v1](http://arxiv.org/pdf/2401.05771v1)|null|\n", "2401.05768": "|**2024-01-11**|**Evaluating Data Augmentation Techniques for Coffee Leaf Disease Classification**|\u8bc4\u4f30\u5496\u5561\u53f6\u75c5\u5206\u7c7b\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f|Adrian Gheorghiu, Iulian-Marius T\u0103iatu, Dumitru-Clementin Cercel, Iuliana Marin, Florin Pop|The detection and classification of diseases in Robusta coffee leaves are essential to ensure that plants are healthy and the crop yield is kept high. However, this job requires extensive botanical knowledge and much wasted time. Therefore, this task and others similar to it have been extensively researched subjects in image classification. Regarding leaf disease classification, most approaches have used the more popular PlantVillage dataset while completely disregarding other datasets, like the Robusta Coffee Leaf (RoCoLe) dataset. As the RoCoLe dataset is imbalanced and does not have many samples, fine-tuning of pre-trained models and multiple augmentation techniques need to be used. The current paper uses the RoCoLe dataset and approaches based on deep learning for classifying coffee leaf diseases from images, incorporating the pix2pix model for segmentation and cycle-generative adversarial network (CycleGAN) for augmentation. Our study demonstrates the effectiveness of Transformer-based models, online augmentations, and CycleGAN augmentation in improving leaf disease classification. While synthetic data has limitations, it complements real data, enhancing model performance. These findings contribute to developing robust techniques for plant disease detection and classification.|\u7f57\u5e03\u65af\u5854\u5496\u5561\u53f6\u75c5\u5bb3\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\u5bf9\u4e8e\u786e\u4fdd\u690d\u7269\u5065\u5eb7\u548c\u4fdd\u6301\u4f5c\u7269\u9ad8\u4ea7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u9879\u5de5\u4f5c\u9700\u8981\u5e7f\u6cdb\u7684\u690d\u7269\u77e5\u8bc6\u5e76\u4e14\u6d6a\u8d39\u5927\u91cf\u65f6\u95f4\u3002\u56e0\u6b64\uff0c\u8be5\u4efb\u52a1\u548c\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\u5df2\u6210\u4e3a\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u5e7f\u6cdb\u7814\u7a76\u7684\u8bfe\u9898\u3002\u5173\u4e8e\u53f6\u75c5\u5206\u7c7b\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u90fd\u4f7f\u7528\u66f4\u6d41\u884c\u7684 PlantVillage \u6570\u636e\u96c6\uff0c\u800c\u5b8c\u5168\u5ffd\u7565\u5176\u4ed6\u6570\u636e\u96c6\uff0c\u4f8b\u5982 Robusta Coffee Leaf (RoCoLe) \u6570\u636e\u96c6\u3002\u7531\u4e8eRoCoLe\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u4e14\u6837\u672c\u4e0d\u591a\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5fae\u8c03\u548c\u591a\u79cd\u589e\u5f3a\u6280\u672f\u3002\u5f53\u524d\u7684\u8bba\u6587\u4f7f\u7528 RoCoLe \u6570\u636e\u96c6\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5bf9\u56fe\u50cf\u4e2d\u7684\u5496\u5561\u53f6\u75c5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408\u7528\u4e8e\u5206\u5272\u7684 pix2pix \u6a21\u578b\u548c\u7528\u4e8e\u589e\u5f3a\u7684\u5faa\u73af\u751f\u6210\u5bf9\u6297\u7f51\u7edc (CycleGAN)\u3002\u6211\u4eec\u7684\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u3001\u5728\u7ebf\u589e\u5f3a\u548c CycleGAN \u589e\u5f3a\u5728\u6539\u5584\u53f6\u75c5\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u5408\u6210\u6570\u636e\u6709\u5c40\u9650\u6027\uff0c\u4f46\u5b83\u8865\u5145\u4e86\u771f\u5b9e\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u5f3a\u5927\u7684\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u548c\u5206\u7c7b\u6280\u672f\u3002|[2401.05768v1](http://arxiv.org/pdf/2401.05768v1)|null|\n", "2401.05738": "|**2024-01-11**|**LKCA: Large Kernel Convolutional Attention**|LKCA\uff1a\u5927\u6838\u5377\u79ef\u6ce8\u610f\u529b|Chenghao Li, Boheng Zeng, Yi Lu, Pengbo Shi, Qingzi Chen, Jirui Liu, Lingyun Zhu|We revisit the relationship between attention mechanisms and large kernel ConvNets in visual transformers and propose a new spatial attention named Large Kernel Convolutional Attention (LKCA). It simplifies the attention operation by replacing it with a single large kernel convolution. LKCA combines the advantages of convolutional neural networks and visual transformers, possessing a large receptive field, locality, and parameter sharing. We explained the superiority of LKCA from both convolution and attention perspectives, providing equivalent code implementations for each view. Experiments confirm that LKCA implemented from both the convolutional and attention perspectives exhibit equivalent performance. We extensively experimented with the LKCA variant of ViT in both classification and segmentation tasks. The experiments demonstrated that LKCA exhibits competitive performance in visual tasks. Our code will be made publicly available at https://github.com/CatworldLee/LKCA.|\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u89c6\u89c9 Transformer \u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5927\u6838\u5377\u79ef\u7f51\u7edc\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u79f0\u4e3a\u5927\u6838\u5377\u79ef\u6ce8\u610f\u529b\uff08LKCA\uff09\u3002\u5b83\u901a\u8fc7\u7528\u5355\u4e2a\u5927\u6838\u5377\u79ef\u66ff\u6362\u5b83\u6765\u7b80\u5316\u6ce8\u610f\u529b\u64cd\u4f5c\u3002 LKCA\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7684\u4f18\u70b9\uff0c\u62e5\u6709\u5927\u7684\u611f\u53d7\u91ce\u3001\u5c40\u90e8\u6027\u548c\u53c2\u6570\u5171\u4eab\u3002\u6211\u4eec\u4ece\u5377\u79ef\u548c\u6ce8\u610f\u529b\u4e24\u4e2a\u89d2\u5ea6\u89e3\u91ca\u4e86LKCA\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u6bcf\u4e2a\u89c6\u56fe\u63d0\u4f9b\u4e86\u7b49\u6548\u7684\u4ee3\u7801\u5b9e\u73b0\u3002\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u4ece\u5377\u79ef\u548c\u6ce8\u610f\u529b\u89d2\u5ea6\u5b9e\u73b0\u7684 LKCA \u8868\u73b0\u51fa\u540c\u7b49\u7684\u6027\u80fd\u3002\u6211\u4eec\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5bf9 ViT \u7684 LKCA \u53d8\u4f53\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLKCA \u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/CatworldLee/LKCA \u516c\u5f00\u53d1\u5e03\u3002|[2401.05738v1](http://arxiv.org/pdf/2401.05738v1)|null|\n", "2401.05702": "|**2024-01-11**|**Video Anomaly Detection and Explanation via Large Language Models**|\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u548c\u89e3\u91ca|Hui Lv, Qianru Sun|Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.|\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u65e8\u5728\u5b9a\u4f4d\u8fdc\u7a0b\u76d1\u63a7\u89c6\u9891\u65f6\u95f4\u8f74\u4e0a\u7684\u5f02\u5e38\u4e8b\u4ef6\u3002\u57fa\u4e8e\u5f02\u5e38\u8bc4\u5206\u7684\u65b9\u6cd5\u5df2\u7ecf\u6d41\u884c\u591a\u5e74\uff0c\u4f46\u5b58\u5728\u9608\u503c\u590d\u6742\u5ea6\u9ad8\u548c\u68c0\u6d4b\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u4f4e\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5728VAD\u6846\u67b6\u4e2d\u88c5\u5907\u57fa\u4e8e\u89c6\u9891\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u8fdb\u884c\u4e86\u5f00\u521b\u6027\u7684\u7814\u7a76\uff0c\u4f7fVAD\u6a21\u578b\u4e0d\u53d7\u9608\u503c\u9650\u5236\uff0c\u5e76\u4e14\u80fd\u591f\u89e3\u91ca\u68c0\u6d4b\u5230\u7684\u5f02\u5e38\u7684\u539f\u56e0\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u6a21\u5757\u957f\u671f\u4e0a\u4e0b\u6587\uff08LTC\uff09\u6765\u7f13\u89e3 VLLM \u5728\u8fdc\u7a0b\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u7684\u65e0\u80fd\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u5e45\u51cf\u5c11\u5bf9 VAD \u6570\u636e\u7684\u9700\u6c42\u5e76\u964d\u4f4e\u6ce8\u91ca\u6307\u4ee4\u8c03\u6574\u6570\u636e\u7684\u6210\u672c\u6765\u63d0\u9ad8\u5fae\u8c03 VLLM \u7684\u6548\u7387\u3002\u6211\u4eec\u8bad\u7ec3\u7684\u6a21\u578b\u5728 UCF-Crime \u548c TAD \u57fa\u51c6\u7684\u5f02\u5e38\u89c6\u9891\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0cAUC \u5206\u522b\u63d0\u9ad8\u4e86 +3.86\\% \u548c +4.96\\%\u3002\u66f4\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u68c0\u6d4b\u5230\u7684\u5f02\u5e38\u63d0\u4f9b\u6587\u672c\u89e3\u91ca\u3002|[2401.05702v1](http://arxiv.org/pdf/2401.05702v1)|null|\n", "2401.05698": "|**2024-01-11**|**HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition**|HiCMAE\uff1a\u7528\u4e8e\u81ea\u76d1\u7763\u89c6\u542c\u60c5\u611f\u8bc6\u522b\u7684\u5206\u5c42\u5bf9\u6bd4\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668|Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao|Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE.|\u8fd1\u5e74\u6765\uff0c\u89c6\u542c\u60c5\u611f\u8bc6\u522b\uff08AVER\uff09\u56e0\u5176\u5728\u521b\u5efa\u60c5\u611f\u611f\u77e5\u667a\u80fd\u673a\u5668\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u6b64\u524d\u8be5\u9886\u57df\u7684\u5de5\u4f5c\u4e3b\u8981\u4ee5\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u4e3a\u4e3b\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e AVER \u957f\u671f\u5b58\u5728\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u76d1\u7763\u5b66\u4e60\u6b63\u5728\u9047\u5230\u74f6\u9888\u3002\u53d7\u81ea\u76d1\u7763\u5b66\u4e60\u6700\u65b0\u8fdb\u5c55\u7684\u63a8\u52a8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u5bf9\u6bd4\u63a9\u6a21\u81ea\u52a8\u7f16\u7801\u5668\uff08HiCMAE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u5927\u91cf\u672a\u6807\u8bb0\u7684\u89c6\u542c\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6765\u4fc3\u8fdb AVER \u7684\u8fdb\u6b65\u3002\u9075\u5faa\u81ea\u76d1\u7763\u89c6\u542c\u8868\u793a\u5b66\u4e60\u7684\u73b0\u6709\u6280\u672f\uff0cHiCMAE \u91c7\u7528\u4e24\u79cd\u4e3b\u8981\u7684\u81ea\u76d1\u7763\u5f62\u5f0f\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5373\u63a9\u7801\u6570\u636e\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u3002\u4e0e\u53ea\u5173\u6ce8\u9876\u5c42\u8868\u793a\u800c\u5ffd\u7565\u4e2d\u95f4\u5c42\u7684\u663e\u5f0f\u6307\u5bfc\u4e0d\u540c\uff0cHiCMAE \u5f00\u53d1\u4e86\u4e00\u79cd\u4e09\u7ba1\u9f50\u4e0b\u7684\u7b56\u7565\u6765\u4fc3\u8fdb\u5206\u5c42\u89c6\u542c\u7279\u5f81\u5b66\u4e60\u5e76\u63d0\u9ad8\u5b66\u4e60\u8868\u793a\u7684\u6574\u4f53\u8d28\u91cf\u3002\u4e3a\u4e86\u9a8c\u8bc1 HiCMAE \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5bf9\u6db5\u76d6\u5206\u7c7b\u548c\u7ef4\u5ea6 AVER \u4efb\u52a1\u7684 9 \u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u548c\u81ea\u76d1\u7763\u89c6\u542c\u65b9\u6cd5\uff0c\u8fd9\u8868\u660e HiCMAE \u662f\u4e00\u79cd\u5f3a\u5927\u7684\u89c6\u542c\u60c5\u611f\u8868\u793a\u5b66\u4e60\u5668\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://github.com/sunlicai/HiCMAE \u4e0a\u516c\u5f00\u63d0\u4f9b\u3002|[2401.05698v1](http://arxiv.org/pdf/2401.05698v1)|null|\n", "2401.05676": "|**2024-01-11**|**Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection**|\u63a2\u7d22\u4eba\u673a\u4ea4\u4e92\u68c0\u6d4b\u7684\u81ea\u4e09\u5143\u7ec4\u76f8\u5173\u6027\u548c\u4e92\u4e09\u5143\u7ec4\u76f8\u5173\u6027|Weibo Jiang, Weihong Ren, Jiandong Tian, Liangqiong Qu, Zhiyong Wang, Honghai Liu|Human-Object Interaction (HOI) detection plays a vital role in scene understanding, which aims to predict the HOI triplet in the form of <human, object, action>. Existing methods mainly extract multi-modal features (e.g., appearance, object semantics, human pose) and then fuse them together to directly predict HOI triplets. However, most of these methods focus on seeking for self-triplet aggregation, but ignore the potential cross-triplet dependencies, resulting in ambiguity of action prediction. In this work, we propose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI detection. Specifically, we regard each triplet proposal as a graph where Human, Object represent nodes and Action indicates edge, to aggregate self-triplet correlation. Also, we try to explore cross-triplet dependencies by jointly considering instance-level, semantic-level, and layout-level relations. Besides, we leverage the CLIP model to assist our SCTC obtain interaction-aware feature by knowledge distillation, which provides useful action clues for HOI detection. Extensive experiments on HICO-DET and V-COCO datasets verify the effectiveness of our proposed SCTC.|\u4eba\u673a\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u5728\u573a\u666f\u7406\u89e3\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5176\u76ee\u7684\u662f\u9884\u6d4b<\u4eba\u3001\u7269\u4f53\u3001\u52a8\u4f5c>\u5f62\u5f0f\u7684HOI\u4e09\u5143\u7ec4\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81\uff08\u4f8b\u5982\u5916\u89c2\u3001\u5bf9\u8c61\u8bed\u4e49\u3001\u4eba\u4f53\u59ff\u52bf\uff09\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u878d\u5408\u5728\u4e00\u8d77\u4ee5\u76f4\u63a5\u9884\u6d4b HOI \u4e09\u5143\u7ec4\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u6570\u90fd\u4e13\u6ce8\u4e8e\u5bfb\u627e\u81ea\u4e09\u5143\u7ec4\u805a\u5408\uff0c\u800c\u5ffd\u7565\u4e86\u6f5c\u5728\u7684\u8de8\u4e09\u5143\u7ec4\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u52a8\u4f5c\u9884\u6d4b\u7684\u6a21\u7cca\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u63a2\u7d22\u7528\u4e8e HOI \u68c0\u6d4b\u7684\u81ea\u4e09\u91cd\u6001\u76f8\u5173\u6027\u548c\u4ea4\u53c9\u4e09\u91cd\u6001\u76f8\u5173\u6027 (SCTC)\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u4e09\u5143\u7ec4\u63d0\u6848\u89c6\u4e3a\u4e00\u4e2a\u56fe\uff0c\u5176\u4e2d\u4eba\u7c7b\u3001\u5bf9\u8c61\u4ee3\u8868\u8282\u70b9\uff0c\u52a8\u4f5c\u4ee3\u8868\u8fb9\u7f18\uff0c\u4ee5\u805a\u5408\u81ea\u4e09\u5143\u7ec4\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c1d\u8bd5\u901a\u8fc7\u5171\u540c\u8003\u8651\u5b9e\u4f8b\u7ea7\u3001\u8bed\u4e49\u7ea7\u548c\u5e03\u5c40\u7ea7\u5173\u7cfb\u6765\u63a2\u7d22\u8de8\u4e09\u5143\u7ec4\u4f9d\u8d56\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528 CLIP \u6a21\u578b\u6765\u5e2e\u52a9\u6211\u4eec\u7684 SCTC \u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u83b7\u5f97\u4ea4\u4e92\u611f\u77e5\u7279\u5f81\uff0c\u8fd9\u4e3a HOI \u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u884c\u52a8\u7ebf\u7d22\u3002 HICO-DET \u548c V-COCO \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684 SCTC \u7684\u6709\u6548\u6027\u3002|[2401.05676v1](http://arxiv.org/pdf/2401.05676v1)|null|\n", "2401.05646": "|**2024-01-11**|**Masked Attribute Description Embedding for Cloth-Changing Person Re-identification**|\u7528\u4e8e\u6362\u8863\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\u7684\u5c4f\u853d\u5c5e\u6027\u63cf\u8ff0\u5d4c\u5165|Chunlei Peng, Boyu Wang, Decheng Liu, Nannan Wang, Ruimin Hu, Xinbo Gao|Cloth-changing person re-identification (CC-ReID) aims to match persons who change clothes over long periods. The key challenge in CC-ReID is to extract clothing-independent features, such as face, hairstyle, body shape, and gait. Current research mainly focuses on modeling body shape using multi-modal biological features (such as silhouettes and sketches). However, it does not fully leverage the personal description information hidden in the original RGB image. Considering that there are certain attribute descriptions which remain unchanged after the changing of cloth, we propose a Masked Attribute Description Embedding (MADE) method that unifies personal visual appearance and attribute description for CC-ReID. Specifically, handling variable clothing-sensitive information, such as color and type, is challenging for effective modeling. To address this, we mask the clothing and color information in the personal attribute description extracted through an attribute detection model. The masked attribute description is then connected and embedded into Transformer blocks at various levels, fusing it with the low-level to high-level features of the image. This approach compels the model to discard clothing information. Experiments are conducted on several CC-ReID benchmarks, including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE effectively utilizes attribute description, enhancing cloth-changing person re-identification performance, and compares favorably with state-of-the-art methods. The code is available at https://github.com/moon-wh/MADE.|\u6362\u8863\u670d\u7684\u4eba\u91cd\u65b0\u8bc6\u522b\uff08CC-ReID\uff09\u65e8\u5728\u5339\u914d\u957f\u65f6\u95f4\u6362\u8863\u670d\u7684\u4eba\u3002 CC-ReID \u7684\u5173\u952e\u6311\u6218\u662f\u63d0\u53d6\u4e0e\u670d\u88c5\u65e0\u5173\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u9762\u90e8\u3001\u53d1\u578b\u3001\u4f53\u578b\u548c\u6b65\u6001\u3002\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\uff08\u4f8b\u5982\u8f6e\u5ed3\u548c\u8349\u56fe\uff09\u6765\u5efa\u6a21\u8eab\u4f53\u5f62\u72b6\u3002\u7136\u800c\uff0c\u5b83\u5e76\u6ca1\u6709\u5145\u5206\u5229\u7528\u9690\u85cf\u5728\u539f\u59cbRGB\u56fe\u50cf\u4e2d\u7684\u4e2a\u4eba\u63cf\u8ff0\u4fe1\u606f\u3002\u8003\u8651\u5230\u67d0\u4e9b\u5c5e\u6027\u63cf\u8ff0\u5728\u6362\u8863\u670d\u540e\u4fdd\u6301\u4e0d\u53d8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63a9\u853d\u5c5e\u6027\u63cf\u8ff0\u5d4c\u5165\uff08MADE\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u4e2a\u4eba\u89c6\u89c9\u5916\u89c2\u548c\u5c5e\u6027\u63cf\u8ff0\u7edf\u4e00\u5230CC-ReID\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5904\u7406\u53ef\u53d8\u7684\u670d\u88c5\u654f\u611f\u4fe1\u606f\uff08\u4f8b\u5982\u989c\u8272\u548c\u7c7b\u578b\uff09\u5bf9\u4e8e\u6709\u6548\u5efa\u6a21\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a9\u76d6\u4e86\u901a\u8fc7\u5c5e\u6027\u68c0\u6d4b\u6a21\u578b\u63d0\u53d6\u7684\u4e2a\u4eba\u5c5e\u6027\u63cf\u8ff0\u4e2d\u7684\u670d\u88c5\u548c\u989c\u8272\u4fe1\u606f\u3002\u7136\u540e\u5c06\u5c4f\u853d\u7684\u5c5e\u6027\u63cf\u8ff0\u8fde\u63a5\u5e76\u5d4c\u5165\u5230\u5404\u4e2a\u7ea7\u522b\u7684 Transformer \u5757\u4e2d\uff0c\u5c06\u5176\u4e0e\u56fe\u50cf\u7684\u4f4e\u7ea7\u5230\u9ad8\u7ea7\u7279\u5f81\u878d\u5408\u3002\u8fd9\u79cd\u65b9\u6cd5\u8feb\u4f7f\u6a21\u578b\u4e22\u5f03\u670d\u88c5\u4fe1\u606f\u3002\u5728\u591a\u4e2a CC-ReID \u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5305\u62ec PRCC\u3001LTCC\u3001Celeb-reID-light \u548c LaST\u3002\u7ed3\u679c\u8868\u660e\uff0cMADE \u6709\u6548\u5730\u5229\u7528\u4e86\u5c5e\u6027\u63cf\u8ff0\uff0c\u589e\u5f3a\u4e86\u6362\u8863\u670d\u7684\u4eba\u91cd\u65b0\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/moon-wh/MADE \u83b7\u53d6\u3002|[2401.05646v1](http://arxiv.org/pdf/2401.05646v1)|null|\n", "2401.05638": "|**2024-01-11**|**MatSAM: Efficient Materials Microstructure Extraction via Visual Large Model**|MatSAM\uff1a\u901a\u8fc7\u89c6\u89c9\u5927\u6a21\u578b\u9ad8\u6548\u63d0\u53d6\u6750\u6599\u5fae\u89c2\u7ed3\u6784|Changtai Li, Xu Han, Chao Yao, Xiaojuan Ban|Accurate and efficient extraction of microstructures in microscopic images of materials plays a critical role in the exploration of structure-property relationships and the optimization of process parameters. Deep learning-based image segmentation techniques that rely on manual annotation are time-consuming and labor-intensive and hardly meet the demand for model transferability and generalization. Segment Anything Model (SAM), a large visual model with powerful deep feature representation and zero-shot generalization capabilities, has provided new solutions for image segmentation. However, directly applying SAM to segmenting microstructures in microscopic images of materials without human annotation cannot achieve the expected results, as the difficulty of adapting its native prompt engineering to the dense and dispersed characteristics of key microstructures in materials microscopy images. In this paper, we propose MatSAM, a general and efficient microstructure extraction solution based on SAM. A new point-based prompts generation strategy is designed, grounded on the distribution and shape of materials microstructures. It generates prompts for different microscopic images, fuses the prompts of the region of interest (ROI) key points and grid key points, and integrates post-processing methods for quantitative characterization of materials microstructures. For common microstructures including grain boundary and phase, MatSAM achieves superior segmentation performance to conventional methods and is even preferable to supervised learning methods evaluated on 18 materials microstructures imaged by the optical microscope (OM) and scanning electron microscope (SEM). We believe that MatSAM can significantly reduce the cost of quantitative characterization of materials microstructures and accelerate the design of new materials.|\u51c6\u786e\u9ad8\u6548\u5730\u63d0\u53d6\u6750\u6599\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u5fae\u89c2\u7ed3\u6784\u5bf9\u4e8e\u63a2\u7d22\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u548c\u4f18\u5316\u5de5\u827a\u53c2\u6570\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u5206\u5272\u6280\u672f\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6807\u6ce8\uff0c\u8017\u65f6\u8017\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u6027\u7684\u9700\u6c42\u3002 Segment Anything Model\uff08SAM\uff09\u662f\u4e00\u79cd\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6df1\u5ea6\u7279\u5f81\u8868\u793a\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5728\u6ca1\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u76f4\u63a5\u5e94\u7528 SAM \u6765\u5206\u5272\u6750\u6599\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u5fae\u89c2\u7ed3\u6784\u65e0\u6cd5\u8fbe\u5230\u9884\u671f\u7684\u7ed3\u679c\uff0c\u56e0\u4e3a\u5176\u539f\u751f\u5373\u65f6\u5de5\u7a0b\u96be\u4ee5\u9002\u5e94\u6750\u6599\u663e\u5fae\u56fe\u50cf\u4e2d\u5173\u952e\u5fae\u89c2\u7ed3\u6784\u7684\u5bc6\u96c6\u548c\u5206\u6563\u7279\u5f81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MatSAM\uff0c\u4e00\u79cd\u57fa\u4e8e SAM \u7684\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5fae\u89c2\u7ed3\u6784\u63d0\u53d6\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e\u6750\u6599\u5fae\u89c2\u7ed3\u6784\u7684\u5206\u5e03\u548c\u5f62\u72b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u70b9\u7684\u63d0\u793a\u751f\u6210\u7b56\u7565\u3002\u5b83\u9488\u5bf9\u4e0d\u540c\u7684\u663e\u5fae\u56fe\u50cf\u751f\u6210\u63d0\u793a\uff0c\u878d\u5408\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u5173\u952e\u70b9\u548c\u7f51\u683c\u5173\u952e\u70b9\u7684\u63d0\u793a\uff0c\u5e76\u96c6\u6210\u7528\u4e8e\u6750\u6599\u5fae\u89c2\u7ed3\u6784\u5b9a\u91cf\u8868\u5f81\u7684\u540e\u5904\u7406\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5305\u62ec\u6676\u754c\u548c\u76f8\u5728\u5185\u7684\u5e38\u89c1\u5fae\u89c2\u7ed3\u6784\uff0cMatSAM \u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u5272\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5bf9\u5149\u5b66\u663e\u5fae\u955c (OM) \u548c\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c (SEM) \u6210\u50cf\u7684 18 \u79cd\u6750\u6599\u5fae\u89c2\u7ed3\u6784\u8fdb\u884c\u8bc4\u4f30\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u6211\u4eec\u76f8\u4fe1MatSAM\u53ef\u4ee5\u663e\u7740\u964d\u4f4e\u6750\u6599\u5fae\u89c2\u7ed3\u6784\u5b9a\u91cf\u8868\u5f81\u7684\u6210\u672c\u5e76\u52a0\u901f\u65b0\u6750\u6599\u7684\u8bbe\u8ba1\u3002|[2401.05638v1](http://arxiv.org/pdf/2401.05638v1)|null|\n", "2401.05604": "|**2024-01-11**|**REBUS: A Robust Evaluation Benchmark of Understanding Symbols**|REBUS\uff1a\u7406\u89e3\u7b26\u53f7\u7684\u7a33\u5065\u8bc4\u4f30\u57fa\u51c6|Andrew Gritsevskiy, Arjun Panickssery, Aaron Kirtland, Derik Kauffman, Hans Gundlach, Irina Gritsevskaya, Joe Cavanagh, Jonathan Chiang, Lydia La Roux, Michelle Hung|We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowledge and reasoning of multimodal large language models.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u753b\u753b\u8c1c\u9898\u4e0a\u7684\u6027\u80fd\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6 333 \u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684\u6587\u5b57\u6e38\u620f\u7684\u539f\u59cb\u793a\u4f8b\uff0c\u6db5\u76d6\u7535\u5f71\u3001\u4f5c\u66f2\u5bb6\u3001\u4e3b\u8981\u57ce\u5e02\u548c\u98df\u7269\u7b49 13 \u4e2a\u7c7b\u522b\u3002\u4e3a\u4e86\u5728\u8bc6\u522b\u7ebf\u7d22\u5355\u8bcd\u6216\u77ed\u8bed\u7684\u57fa\u51c6\u4e0a\u53d6\u5f97\u826f\u597d\u7684\u6027\u80fd\uff0c\u6a21\u578b\u5fc5\u987b\u5c06\u56fe\u50cf\u8bc6\u522b\u548c\u5b57\u7b26\u4e32\u64cd\u4f5c\u4e0e\u5047\u8bbe\u68c0\u9a8c\u3001\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u7406\u89e3\u7ed3\u5408\u8d77\u6765\uff0c\u4ece\u800c\u5bf9\u80fd\u529b\u8fdb\u884c\u590d\u6742\u7684\u591a\u6a21\u5f0f\u8bc4\u4f30\u3002\u6211\u4eec\u53d1\u73b0 GPT-4V \u548c Gemini Pro \u7b49\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u6240\u6709\u5176\u4ed6\u6d4b\u8bd5\u6a21\u578b\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u597d\u7684\u6a21\u578b\uff0c\u6700\u7ec8\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a 24%\uff0c\u8fd9\u51f8\u663e\u4e86\u63a8\u7406\u65b9\u9762\u9700\u8981\u5927\u5e45\u6539\u8fdb\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5f88\u5c11\u7406\u89e3\u8c1c\u9898\u7684\u6240\u6709\u90e8\u5206\uff0c\u5e76\u4e14\u51e0\u4e4e\u603b\u662f\u65e0\u6cd5\u8ffd\u6eaf\u89e3\u91ca\u6b63\u786e\u7684\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u53ef\u7528\u4e8e\u8bc6\u522b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u4e2d\u7684\u4e3b\u8981\u7f3a\u9677\u3002|[2401.05604v1](http://arxiv.org/pdf/2401.05604v1)|**[link](https://github.com/cvndsh/rebus)**|\n", "2401.05602": "|**2024-01-11**|**Nucleus subtype classification using inter-modality learning**|\u4f7f\u7528\u8de8\u6a21\u6001\u5b66\u4e60\u8fdb\u884c\u7ec6\u80de\u6838\u4e9a\u578b\u5206\u7c7b|Lucas W. Remedios, Shunxing Bao, Samuel W. Remedios, Ho Hin Lee, Leon Y. Cai, Thomas Li, Ruining Deng, Can Cui, Jia Li, Qi Liu, et.al.|Understanding the way cells communicate, co-locate, and interrelate is essential to understanding human physiology. Hematoxylin and eosin (H&E) staining is ubiquitously available both for clinical studies and research. The Colon Nucleus Identification and Classification (CoNIC) Challenge has recently innovated on robust artificial intelligence labeling of six cell types on H&E stains of the colon. However, this is a very small fraction of the number of potential cell classification types. Specifically, the CoNIC Challenge is unable to classify epithelial subtypes (progenitor, endocrine, goblet), lymphocyte subtypes (B, helper T, cytotoxic T), or connective subtypes (fibroblasts, stromal). In this paper, we propose to use inter-modality learning to label previously un-labelable cell types on virtual H&E. We leveraged multiplexed immunofluorescence (MxIF) histology imaging to identify 14 subclasses of cell types. We performed style transfer to synthesize virtual H&E from MxIF and transferred the higher density labels from MxIF to these virtual H&E images. We then evaluated the efficacy of learning in this approach. We identified helper T and progenitor nuclei with positive predictive values of $0.34 \\pm 0.15$ (prevalence $0.03 \\pm 0.01$) and $0.47 \\pm 0.1$ (prevalence $0.07 \\pm 0.02$) respectively on virtual H&E. This approach represents a promising step towards automating annotation in digital pathology.|\u4e86\u89e3\u7ec6\u80de\u901a\u4fe1\u3001\u5171\u5b9a\u4f4d\u548c\u76f8\u4e92\u5173\u8054\u7684\u65b9\u5f0f\u5bf9\u4e8e\u7406\u89e3\u4eba\u7c7b\u751f\u7406\u5b66\u81f3\u5173\u91cd\u8981\u3002\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2 (H&E) \u67d3\u8272\u666e\u904d\u53ef\u7528\u4e8e\u4e34\u5e8a\u7814\u7a76\u548c\u7814\u7a76\u3002\u7ed3\u80a0\u6838\u8bc6\u522b\u548c\u5206\u7c7b (CoNIC) \u6311\u6218\u8d5b\u6700\u8fd1\u5728\u7ed3\u80a0 H&E \u67d3\u8272\u4e0a\u5bf9\u516d\u79cd\u7ec6\u80de\u7c7b\u578b\u8fdb\u884c\u4e86\u5f3a\u5927\u7684\u4eba\u5de5\u667a\u80fd\u6807\u8bb0\u3002\u7136\u800c\uff0c\u8fd9\u53ea\u662f\u6f5c\u5728\u7ec6\u80de\u5206\u7c7b\u7c7b\u578b\u6570\u91cf\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u5177\u4f53\u6765\u8bf4\uff0cCoNIC \u6311\u6218\u65e0\u6cd5\u5bf9\u4e0a\u76ae\u4e9a\u578b\uff08\u7956\u7ec6\u80de\u3001\u5185\u5206\u6ccc\u7ec6\u80de\u3001\u676f\u72b6\u7ec6\u80de\uff09\u3001\u6dcb\u5df4\u7ec6\u80de\u4e9a\u578b\uff08B\u3001\u8f85\u52a9 T\u3001\u7ec6\u80de\u6bd2\u6027 T\uff09\u6216\u7ed3\u7f14\u7ec6\u80de\u4e9a\u578b\uff08\u6210\u7ea4\u7ef4\u7ec6\u80de\u3001\u57fa\u8d28\u7ec6\u80de\uff09\u8fdb\u884c\u5206\u7c7b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u8de8\u6a21\u6001\u5b66\u4e60\u6765\u6807\u8bb0\u865a\u62df H&E \u4e0a\u4ee5\u524d\u65e0\u6cd5\u6807\u8bb0\u7684\u7ec6\u80de\u7c7b\u578b\u3002\u6211\u4eec\u5229\u7528\u591a\u91cd\u514d\u75ab\u8367\u5149 (MxIF) \u7ec4\u7ec7\u5b66\u6210\u50cf\u6765\u8bc6\u522b 14 \u79cd\u7ec6\u80de\u7c7b\u578b\u4e9a\u7c7b\u3002\u6211\u4eec\u6267\u884c\u98ce\u683c\u8f6c\u79fb\u4ee5\u4ece MxIF \u5408\u6210\u865a\u62df H&E\uff0c\u5e76\u5c06\u66f4\u9ad8\u5bc6\u5ea6\u7684\u6807\u7b7e\u4ece MxIF \u8f6c\u79fb\u5230\u8fd9\u4e9b\u865a\u62df H&E \u56fe\u50cf\u3002\u7136\u540e\u6211\u4eec\u8bc4\u4f30\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u5b66\u4e60\u6548\u679c\u3002\u6211\u4eec\u5728\u865a\u62df H&E \u4e0a\u786e\u5b9a\u4e86\u8f85\u52a9 T \u548c\u7956\u7ec6\u80de\u6838\u7684\u9633\u6027\u9884\u6d4b\u503c\u5206\u522b\u4e3a $0.34 \\pm 0.15$\uff08\u60a3\u75c5\u7387 $0.03 \\pm 0.01$\uff09\u548c $0.47 \\pm 0.1$\uff08\u60a3\u75c5\u7387 $0.07 \\pm 0.02$\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u4ee3\u8868\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u81ea\u52a8\u5316\u6ce8\u91ca\u65b9\u9762\u8fc8\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002|[2401.05602v1](http://arxiv.org/pdf/2401.05602v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.06127": "|**2024-01-11**|**E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation**|E$^{2}$GAN\uff1a\u7528\u4e8e\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u7684\u9ad8\u6548 GAN \u7684\u9ad8\u6548\u8bad\u7ec3|Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, et.al.|One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.|\u5b9e\u73b0\u7075\u6d3b\u7684\u5b9e\u65f6\u8bbe\u5907\u56fe\u50cf\u7f16\u8f91\u7684\u4e00\u4e2a\u975e\u5e38\u6709\u524d\u9014\u7684\u65b9\u5411\u662f\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982\u7a33\u5b9a\u6269\u6563\uff09\u6765\u5229\u7528\u6570\u636e\u84b8\u998f\u6765\u751f\u6210\u7528\u4e8e\u8bad\u7ec3\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u914d\u5bf9\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u65b9\u6cd5\u663e\u7740\u7f13\u89e3\u4e86\u9ad8\u7aef\u5546\u7528 GPU \u901a\u5e38\u5bf9\u4f7f\u7528\u6269\u6563\u6a21\u578b\u6267\u884c\u56fe\u50cf\u7f16\u8f91\u63d0\u51fa\u7684\u4e25\u683c\u8981\u6c42\u3002\u7136\u800c\uff0c\u4e0e\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u4e0d\u540c\uff0c\u6bcf\u4e2a\u7cbe\u70bc\u7684 GAN \u4e13\u95e8\u7528\u4e8e\u7279\u5b9a\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u9700\u8981\u6602\u8d35\u7684\u8bad\u7ec3\u5de5\u4f5c\u624d\u80fd\u83b7\u5f97\u5404\u79cd\u6982\u5ff5\u7684\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6 GAN \u7684\u8fc7\u7a0b\u662f\u5426\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u6548\u7387\uff1f\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u521b\u65b0\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u4eec\u6784\u5efa\u4e00\u4e2a\u5177\u6709\u901a\u7528\u7279\u5f81\u7684\u57fa\u7840 GAN \u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u9002\u5e94\u4e0d\u540c\u7684\u6982\u5ff5\uff0c\u4ece\u800c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bc6\u522b\u57fa\u672c GAN \u6a21\u578b\u4e2d\u7684\u5173\u952e\u5c42\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u6392\u540d\u641c\u7d22\u8fc7\u7a0b\u91c7\u7528\u4f4e\u79e9\u9002\u5e94 (LoRA)\uff0c\u800c\u4e0d\u662f\u5fae\u8c03\u6574\u4e2a\u57fa\u672c\u6a21\u578b\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u7814\u7a76\u5fae\u8c03\u6240\u9700\u7684\u6700\u5c11\u91cf\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u603b\u4f53\u8bad\u7ec3\u65f6\u95f4\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u6709\u6548\u5730\u8d4b\u4e88 GAN \u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u5b9e\u65f6\u9ad8\u8d28\u91cf\u56fe\u50cf\u7f16\u8f91\u7684\u80fd\u529b\uff0c\u5e76\u663e\u7740\u964d\u4f4e\u6bcf\u4e2a\u6982\u5ff5\u7684\u8bad\u7ec3\u6210\u672c\u548c\u5b58\u50a8\u6210\u672c\u3002|[2401.06127v1](http://arxiv.org/pdf/2401.06127v1)|null|\n", "2401.06105": "|**2024-01-11**|**PALP: Prompt Aligned Personalization of Text-to-Image Models**|PALP\uff1a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5feb\u901f\u5bf9\u9f50\u4e2a\u6027\u5316|Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, Ariel Shamir|Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \\emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks. We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques.|\u5185\u5bb9\u521b\u5efa\u8005\u901a\u5e38\u65e8\u5728\u4f7f\u7528\u8d85\u51fa\u4f20\u7edf\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u529f\u80fd\u7684\u4e2a\u4eba\u4e3b\u9898\u6765\u521b\u5efa\u4e2a\u6027\u5316\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u53ef\u80fd\u5e0c\u671b\u751f\u6210\u7684\u56fe\u50cf\u5305\u542b\u7279\u5b9a\u7684\u4f4d\u7f6e\u3001\u98ce\u683c\u3001\u6c1b\u56f4\u7b49\u3002\u73b0\u6709\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u53ef\u80fd\u4f1a\u635f\u5bb3\u4e2a\u6027\u5316\u80fd\u529b\u6216\u4e0e\u590d\u6742\u6587\u672c\u63d0\u793a\u7684\u5bf9\u9f50\u3002\u8fd9\u79cd\u6743\u8861\u53ef\u80fd\u4f1a\u59a8\u788d\u7528\u6237\u63d0\u793a\u7684\u5b9e\u73b0\u548c\u4e3b\u9898\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u6ce8\u4e8e \\emph{single} \u63d0\u793a\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u7684\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a\u201c\u53ca\u65f6\u8c03\u6574\u4e2a\u6027\u5316\u201d\u3002\u867d\u7136\u8fd9\u770b\u8d77\u6765\u53ef\u80fd\u6709\u9650\u5236\uff0c\u4f46\u6211\u4eec\u7684\u65b9\u6cd5\u64c5\u957f\u6539\u8fdb\u6587\u672c\u5bf9\u9f50\uff0c\u80fd\u591f\u521b\u5efa\u5177\u6709\u590d\u6742\u63d0\u793a\u7684\u56fe\u50cf\uff0c\u8fd9\u53ef\u80fd\u5bf9\u5f53\u524d\u6280\u672f\u6784\u6210\u6311\u6218\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u9644\u52a0\u7684\u5206\u6570\u84b8\u998f\u91c7\u6837\u9879\u4f7f\u4e2a\u6027\u5316\u6a21\u578b\u4e0e\u76ee\u6807\u63d0\u793a\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u955c\u5934\u548c\u5355\u955c\u5934\u8bbe\u7f6e\u4e2d\u7684\u591a\u529f\u80fd\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u8868\u660e\u5b83\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e3b\u9898\u6216\u4f7f\u7528\u6765\u81ea\u53c2\u8003\u56fe\u50cf\uff08\u4f8b\u5982\u827a\u672f\u54c1\uff09\u7684\u7075\u611f\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u6280\u672f\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u3002|[2401.06105v1](http://arxiv.org/pdf/2401.06105v1)|null|\n", "2401.06003": "|**2024-01-11**|**TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering**|TRIPS\uff1a\u7528\u4e8e\u5b9e\u65f6\u8f90\u5c04\u573a\u6e32\u67d3\u7684\u4e09\u7ebf\u6027\u70b9\u6e85\u5c04|Linus Franke, Darius R\u00fcckert, Laura Fink, Marc Stamminger|Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage.|\u57fa\u4e8e\u70b9\u7684\u8f90\u5c04\u573a\u6e32\u67d3\u5728\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u65b9\u9762\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u7684\u7ed3\u5408\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7684\u6700\u65b0\u65b9\u6cd5\u4e5f\u5e76\u975e\u6ca1\u6709\u7f3a\u70b9\u3002 3D \u9ad8\u65af\u6cfc\u6e85 [Kerbl \u548c Kopanas \u7b49\u4eba\u3002 2023] \u7531\u4e8e\u6a21\u7cca\u548c\u6d51\u6d4a\u7684\u4f2a\u50cf\uff0c\u5728\u6e32\u67d3\u9ad8\u5ea6\u8be6\u7ec6\u7684\u573a\u666f\u65f6\u4f1a\u9047\u5230\u56f0\u96be\u3002\u53e6\u4e00\u65b9\u9762\uff0cADOP [R\\\"uckert et al. 2022] \u53ef\u4ee5\u5bb9\u7eb3\u66f4\u6e05\u6670\u7684\u56fe\u50cf\uff0c\u4f46\u795e\u7ecf\u91cd\u5efa\u7f51\u7edc\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u5b83\u4f1a\u89e3\u51b3\u65f6\u95f4\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5e76\u4e14\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u70b9\u4e91\u4e2d\u7684\u5927\u95f4\u9699\u3002\u5728\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TRIPS\uff08\u4e09\u7ebf\u6027\u70b9\u5206\u5e03\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u9ad8\u65af\u5206\u5e03\u548c ADOP \u601d\u60f3\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u65b0\u6280\u672f\u80cc\u540e\u7684\u57fa\u672c\u6982\u5ff5\u6d89\u53ca\u5c06\u70b9\u5149\u6805\u5316\u4e3a\u5c4f\u5e55\u7a7a\u95f4\u56fe\u50cf\u91d1\u5b57\u5854\uff0c\u91d1\u5b57\u5854\u5c42\u7684\u9009\u62e9\u7531\u4e0b\u5f0f\u786e\u5b9a\uff1a\u6295\u5f71\u70b9\u5927\u5c0f\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u4f7f\u7528\u5355\u4e2a\u4e09\u7ebf\u6027\u5199\u5165\u6e32\u67d3\u4efb\u610f\u5927\u7684\u70b9\u3002\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6765\u91cd\u5efa\u65e0\u5b54\u56fe\u50cf\uff0c\u5305\u62ec\u8d85\u51fasplat\u5206\u8fa8\u7387\u7684\u7ec6\u8282\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u6e32\u67d3\u7ba1\u9053\u662f\u5b8c\u5168\u53ef\u5fae\u7684\uff0c\u5141\u8bb8\u81ea\u52a8\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cTRIPS \u5728\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u73b0\u6210\u7684\u786c\u4ef6\u4e0a\u4fdd\u6301\u4e86\u6bcf\u79d2 60 \u5e27\u7684\u5b9e\u65f6\u5e27\u901f\u7387\u3002\u8fd9\u79cd\u6027\u80fd\u6269\u5c55\u5230\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u4f8b\u5982\u5177\u6709\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u3001\u5e7f\u9614\u666f\u89c2\u548c\u81ea\u52a8\u66dd\u5149\u955c\u5934\u7684\u573a\u666f\u3002|[2401.06003v1](http://arxiv.org/pdf/2401.06003v1)|null|\n", "2401.05968": "|**2024-01-11**|**A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd Counting**|\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u4eba\u7fa4\u8ba1\u6570\u7684\u8f7b\u91cf\u7ea7\u7279\u5f81\u878d\u5408\u67b6\u6784|Yashwardhan Chaudhuri, Ankit Kumar, Orchid Chetia Phukan, Arun Balaji Buduru|Crowd counting finds direct applications in real-world situations, making computational efficiency and performance crucial. However, most of the previous methods rely on a heavy backbone and a complex downstream architecture that restricts the deployment. To address this challenge and enhance the versatility of crowd-counting models, we introduce two lightweight models. These models maintain the same downstream architecture while incorporating two distinct backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to extract diverse scale features from a Pre-Trained Model (PTM) and subsequently combine these features seamlessly. This approach empowers our models to achieve improved performance while maintaining a compact and efficient design. With the comparison of our proposed models with previously available state-of-the-art (SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it achieves comparable results while being the most computationally efficient model. Finally, we present a comparative study, an extensive ablation study, along with pruning to show the effectiveness of our models.|\u4eba\u7fa4\u8ba1\u6570\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u627e\u5230\u4e86\u76f4\u63a5\u5e94\u7528\uff0c\u8fd9\u4f7f\u5f97\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e4b\u524d\u7684\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e8e\u6c89\u91cd\u7684\u4e3b\u5e72\u7f51\u548c\u590d\u6742\u7684\u4e0b\u6e38\u67b6\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u90e8\u7f72\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u5e76\u589e\u5f3a\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u4fdd\u6301\u76f8\u540c\u7684\u4e0b\u6e38\u67b6\u6784\uff0c\u540c\u65f6\u5408\u5e76\u4e24\u4e2a\u4e0d\u540c\u7684\u4e3b\u5e72\uff1aMobileNet \u548c MobileViT\u3002\u6211\u4eec\u5229\u7528\u76f8\u90bb\u7279\u5f81\u878d\u5408\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTM\uff09\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u5c3a\u5ea6\u7279\u5f81\uff0c\u7136\u540e\u65e0\u7f1d\u5730\u7ec4\u5408\u8fd9\u4e9b\u7279\u5f81\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7d27\u51d1\u548c\u9ad8\u6548\u7684\u8bbe\u8ba1\u3002\u901a\u8fc7\u5c06\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u4e0e ShanghaiTech-A\u3001ShanghaiTech-B \u548c UCF-CC-50 \u6570\u636e\u96c6\u4e0a\u73b0\u6709\u7684\u6700\u5148\u8fdb (SOTA) \u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b83\u53d6\u5f97\u4e86\u53ef\u6bd4\u8f83\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u6210\u4e3a\u8ba1\u7b97\u6548\u7387\u6700\u9ad8\u7684\u6a21\u578b\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u6bd4\u8f83\u7814\u7a76\uff0c\u4e00\u9879\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u53ca\u4fee\u526a\u4ee5\u663e\u793a\u6211\u4eec\u6a21\u578b\u7684\u6709\u6548\u6027\u3002|[2401.05968v1](http://arxiv.org/pdf/2401.05968v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.06035": "|**2024-01-11**|**RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks**|RAVEN\uff1a\u91cd\u65b0\u601d\u8003\u4f7f\u7528\u9ad8\u6548\u4e09\u5e73\u9762\u7f51\u7edc\u7684\u5bf9\u6297\u6027\u89c6\u9891\u751f\u6210|Partha Ghosh, Soubhik Sanyal, Cordelia Schmid, Bernhard Sch\u00f6lkopf|We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN) based generator architecture, thereby compensating for the constraints imposed by a smaller generator size. As a result, our model is capable of synthesizing high-fidelity video clips at a resolution of $256\\times256$ pixels, with durations extending to more than $5$ seconds at a frame rate of 30 fps. The efficacy and versatility of our approach are empirically validated through qualitative and quantitative assessments across three different datasets comprising both synthetic and real video clips.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u671f\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u4e3a\u4e86\u6355\u83b7\u8fd9\u4e9b\u4f9d\u8d56\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u6df7\u5408\u663e\u5f0f-\u9690\u5f0f\u4e09\u5e73\u9762\u8868\u793a\uff0c\u5176\u7075\u611f\u6765\u81ea\u4e3a\u4e09\u7ef4\u5bf9\u8c61\u8868\u793a\u5f00\u53d1\u7684 3D \u611f\u77e5\u751f\u6210\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u5355\u4e00\u6f5c\u5728\u4ee3\u7801\u6765\u5efa\u6a21\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u3002\u7136\u540e\u4ece\u4e2d\u95f4\u4e09\u5e73\u9762\u8868\u793a\u5408\u6210\u5404\u4e2a\u89c6\u9891\u5e27\uff0c\u8be5\u4e2d\u95f4\u4e09\u5e73\u9762\u8868\u793a\u672c\u8eab\u662f\u4ece\u4e3b\u8981\u6f5c\u5728\u4ee3\u7801\u5bfc\u51fa\u7684\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u7b56\u7565\u5c06\u8ba1\u7b97\u590d\u6742\u6027\u964d\u4f4e\u4e86 2 \u7f8e\u5143\uff08\u4ee5 FLOP \u4e3a\u5355\u4f4d\uff09\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u9ad8\u6548\u4e14\u65f6\u95f4\u8fde\u8d2f\u5730\u751f\u6210\u89c6\u9891\u3002\u6b64\u5916\uff0c\u4e0e\u81ea\u56de\u5f52\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u8054\u5408\u5e27\u5efa\u6a21\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u89c6\u89c9\u4f2a\u5f71\u7684\u4ea7\u751f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u57fa\u4e8e\u5149\u6d41\u7684\u6a21\u5757\u96c6\u6210\u5230\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u751f\u6210\u5668\u67b6\u6784\u4e2d\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u529f\u80fd\uff0c\u4ece\u800c\u8865\u507f\u4e86\u8f83\u5c0f\u751f\u6210\u5668\u5c3a\u5bf8\u6240\u5e26\u6765\u7684\u9650\u5236\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u4ee5 256\\times256$ \u50cf\u7d20\u7684\u5206\u8fa8\u7387\u5408\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u526a\u8f91\uff0c\u6301\u7eed\u65f6\u95f4\u5728 30 fps \u7684\u5e27\u901f\u7387\u4e0b\u5ef6\u957f\u5230\u8d85\u8fc7 5$ \u79d2\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u901a\u8fc7\u5bf9\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08\u5305\u62ec\u5408\u6210\u89c6\u9891\u526a\u8f91\u548c\u771f\u5b9e\u89c6\u9891\u526a\u8f91\uff09\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002|[2401.06035v1](http://arxiv.org/pdf/2401.06035v1)|null|\n", "2401.06031": "|**2024-01-11**|**GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model**|GE-AdvGAN\uff1a\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7f16\u8f91\u7684\u5bf9\u6297\u751f\u6210\u6a21\u578b\u63d0\u9ad8\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027|Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo|Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN|\u5bf9\u6297\u751f\u6210\u6a21\u578b\uff0c\u4f8b\u5982\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u751f\u6210\u5404\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u5373\u56fe\u50cf\u3001\u6587\u672c\u548c\u97f3\u9891\u3002\u56e0\u6b64\uff0c\u5176\u4ee4\u4eba\u9f13\u821e\u7684\u6027\u80fd\u50ac\u751f\u4e86\u767d\u76d2\u548c\u9ed1\u76d2\u653b\u51fb\u573a\u666f\u4e2d\u57fa\u4e8e GAN \u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u3002\u53ef\u8f6c\u79fb\u9ed1\u76d2\u653b\u51fb\u7684\u91cd\u8981\u6027\u5728\u4e8e\u5b83\u4eec\u80fd\u591f\u5728\u4e0d\u540c\u7684\u6a21\u578b\u548c\u8bbe\u7f6e\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u7a0b\u5e8f\u66f4\u52a0\u7d27\u5bc6\u5730\u7ed3\u5408\u3002\u7136\u800c\uff0c\u4fdd\u6301\u6b64\u7c7b\u65b9\u6cd5\u5728\u53ef\u8f6c\u79fb\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6027\u80fd\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u540c\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e00\u4e9b\u589e\u5f3a\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u9700\u8981\u66f4\u957f\u7684\u65f6\u95f4\u6765\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002\u56e0\u6b64\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GE-AdvGAN \u7684\u65b0\u9896\u7b97\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5bf9\u6297\u6027\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u7b97\u6cd5\u7684\u6548\u7387\u3002\u4e3b\u8981\u65b9\u6cd5\u662f\u901a\u8fc7\u4f18\u5316\u751f\u6210\u5668\u53c2\u6570\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u901a\u8fc7\u529f\u80fd\u548c\u7279\u5f81\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68af\u5ea6\u7f16\u8f91\uff08GE\uff09\u673a\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u751f\u6210\u53ef\u8f6c\u79fb\u6837\u672c\u7684\u53ef\u884c\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u63a2\u7d22\u9891\u57df\u4fe1\u606f\u6765\u786e\u5b9a\u68af\u5ea6\u7f16\u8f91\u65b9\u5411\uff0cGE-AdvGAN \u53ef\u4ee5\u751f\u6210\u9ad8\u5ea6\u53ef\u8f6c\u79fb\u7684\u5bf9\u6297\u6837\u672c\uff0c\u540c\u65f6\u4e0e\u6700\u5148\u8fdb\u7684\u53ef\u8f6c\u79fb\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u5bf9GE-AdvGAN\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/LMBTough/GE-advGAN|[2401.06031v1](http://arxiv.org/pdf/2401.06031v1)|null|\n", "2401.06005": "|**2024-01-11**|**How does the primate brain combine generative and discriminative computations in vision?**|\u7075\u957f\u7c7b\u52a8\u7269\u7684\u5927\u8111\u5982\u4f55\u5c06\u89c6\u89c9\u4e2d\u7684\u751f\u6210\u8ba1\u7b97\u548c\u5224\u522b\u8ba1\u7b97\u7ed3\u5408\u8d77\u6765\uff1f|Benjamin Peters, James J. DiCarlo, Todd Gureckis, Ralf Haefner, Leyla Isik, Joshua Tenenbaum, Talia Konkle, Thomas Naselaris, Kimberly Stachenfeld, Zenna Tavares, et.al.|Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of \"inference\" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmholtz's sense, where the sensory evidence is evaluated in the context of a generative model of the causal processes giving rise to it. In this conception, vision inverts a generative model through an interrogation of the evidence in a process often thought to involve top-down predictions of sensory data to evaluate the likelihood of alternative hypotheses. The authors include scientists rooted in roughly equal numbers in each of the conceptions and motivated to overcome what might be a false dichotomy between them and engage the other perspective in the realm of theory and experiment. The primate brain employs an unknown algorithm that may combine the advantages of both conceptions. We explain and clarify the terminology, review the key empirical evidence, and propose an empirical research program that transcends the dichotomy and sets the stage for revealing the mysterious hybrid algorithm of primate vision.|\u89c6\u89c9\u88ab\u5e7f\u6cdb\u7406\u89e3\u4e3a\u4e00\u4e2a\u63a8\u7406\u95ee\u9898\u3002\u7136\u800c\uff0c\u63a8\u7406\u8fc7\u7a0b\u7684\u4e24\u79cd\u622a\u7136\u4e0d\u540c\u7684\u6982\u5ff5\u90fd\u5bf9\u751f\u7269\u89c6\u89c9\u7814\u7a76\u4ee5\u53ca\u673a\u5668\u89c6\u89c9\u5de5\u7a0b\u4ea7\u751f\u4e86\u5f71\u54cd\u3002\u7b2c\u4e00\u4e2a\u5f3a\u8c03\u81ea\u4e0b\u800c\u4e0a\u7684\u4fe1\u53f7\u6d41\uff0c\u5c06\u89c6\u89c9\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u5f88\u5927\u7a0b\u5ea6\u4e0a\u524d\u9988\u3001\u5224\u522b\u6027\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b83\u8fc7\u6ee4\u548c\u8f6c\u6362\u89c6\u89c9\u4fe1\u606f\u4ee5\u6d88\u9664\u4e0d\u76f8\u5173\u7684\u53d8\u5316\uff0c\u5e76\u4ee5\u9002\u5408\u8ba4\u77e5\u548c\u884c\u4e3a\u63a7\u5236\u4e0b\u6e38\u529f\u80fd\u7684\u683c\u5f0f\u8868\u793a\u884c\u4e3a\u76f8\u5173\u4fe1\u606f\u3002\u5728\u8fd9\u4e2a\u6982\u5ff5\u4e2d\uff0c\u89c6\u89c9\u662f\u7531\u611f\u5b98\u6570\u636e\u9a71\u52a8\u7684\uff0c\u800c\u611f\u77e5\u662f\u76f4\u63a5\u7684\uff0c\u56e0\u4e3a\u5904\u7406\u4ece\u6570\u636e\u8fdb\u884c\u5230\u611f\u5174\u8da3\u7684\u6f5c\u5728\u53d8\u91cf\u3002\u8fd9\u4e2a\u6982\u5ff5\u4e2d\u7684\u201c\u63a8\u7406\u201d\u6982\u5ff5\u662f\u795e\u7ecf\u7f51\u7edc\u5de5\u7a0b\u6587\u732e\u4e2d\u7684\u6982\u5ff5\uff0c\u5176\u4e2d\u5904\u7406\u56fe\u50cf\u7684\u524d\u9988\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u88ab\u79f0\u4e3a\u6267\u884c\u63a8\u7406\u3002\u53e6\u4e00\u79cd\u6982\u5ff5\u662f\u5c06\u89c6\u89c9\u89c6\u4e3a\u4ea5\u59c6\u970d\u5179\u610f\u4e49\u4e0a\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5176\u4e2d\u611f\u5b98\u8bc1\u636e\u662f\u5728\u4ea7\u751f\u5b83\u7684\u56e0\u679c\u8fc7\u7a0b\u7684\u751f\u6210\u6a21\u578b\u7684\u80cc\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\u7684\u3002\u5728\u8fd9\u4e2a\u6982\u5ff5\u4e2d\uff0c\u89c6\u89c9\u901a\u8fc7\u8be2\u95ee\u8bc1\u636e\u6765\u53cd\u8f6c\u751f\u6210\u6a21\u578b\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u88ab\u8ba4\u4e3a\u6d89\u53ca\u81ea\u4e0a\u800c\u4e0b\u7684\u611f\u5b98\u6570\u636e\u9884\u6d4b\uff0c\u4ee5\u8bc4\u4f30\u66ff\u4ee3\u5047\u8bbe\u7684\u53ef\u80fd\u6027\u3002\u4f5c\u8005\u5305\u62ec\u5728\u6bcf\u4e2a\u6982\u5ff5\u4e2d\u690d\u6839\u4e8e\u5927\u81f4\u76f8\u7b49\u6570\u91cf\u7684\u79d1\u5b66\u5bb6\uff0c\u5e76\u81f4\u529b\u4e8e\u514b\u670d\u5b83\u4eec\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u7684\u9519\u8bef\u4e8c\u5206\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u9886\u57df\u4e2d\u91c7\u7528\u5176\u4ed6\u89c2\u70b9\u3002\u7075\u957f\u7c7b\u52a8\u7269\u7684\u5927\u8111\u91c7\u7528\u4e86\u4e00\u79cd\u672a\u77e5\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7ed3\u5408\u8fd9\u4e24\u79cd\u6982\u5ff5\u7684\u4f18\u70b9\u3002\u6211\u4eec\u89e3\u91ca\u548c\u6f84\u6e05\u4e86\u672f\u8bed\uff0c\u56de\u987e\u4e86\u5173\u952e\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d85\u8d8a\u4e8c\u5206\u6cd5\u7684\u5b9e\u8bc1\u7814\u7a76\u8ba1\u5212\uff0c\u5e76\u4e3a\u63ed\u793a\u7075\u957f\u7c7b\u52a8\u7269\u89c6\u89c9\u7684\u795e\u79d8\u6df7\u5408\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002|[2401.06005v1](http://arxiv.org/pdf/2401.06005v1)|null|\n", "2401.05964": "|**2024-01-11**|**An attempt to generate new bridge types from latent space of PixelCNN**|\u5c1d\u8bd5\u4ece PixelCNN \u7684\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u65b0\u7684\u6865\u7c7b\u578b|Hongjun Zhang|Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the way to achieve artificial general intelligence in the future.|\u5c1d\u8bd5\u4f7f\u7528\u751f\u6210\u4eba\u5de5\u667a\u80fd\u6280\u672f\u751f\u6210\u65b0\u7684\u6865\u6881\u7c7b\u578b\u3002\u5229\u7528\u4e09\u8de8\u6881\u6865\u3001\u62f1\u6865\u3001\u659c\u62c9\u6865\u3001\u60ac\u7d22\u6865\u7684\u5bf9\u79f0\u7ed3\u6784\u5316\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u57fa\u4e8ePython\u7f16\u7a0b\u8bed\u8a00\u3001TensorFlow\u548cKeras\u6df1\u5ea6\u5b66\u4e60\u5e73\u53f0\u6846\u67b6\uff0c\u6784\u5efa\u5e76\u8bad\u7ec3PixelCNN\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u6355\u83b7\u56fe\u50cf\u7684\u7edf\u8ba1\u7ed3\u6784\uff0c\u5e76\u5728\u7ed9\u5b9a\u5148\u524d\u50cf\u7d20\u7684\u60c5\u51b5\u4e0b\u8ba1\u7b97\u4e0b\u4e00\u4e2a\u50cf\u7d20\u7684\u6982\u7387\u5206\u5e03\u3002\u6839\u636e\u83b7\u5f97\u7684\u6f5c\u5728\u7a7a\u95f4\u91c7\u6837\uff0c\u53ef\u4ee5\u751f\u6210\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0d\u540c\u7684\u65b0\u6865\u6881\u7c7b\u578b\u3002 PixelCNN\u53ef\u4ee5\u5728\u4eba\u7c7b\u539f\u521b\u6865\u6881\u7c7b\u578b\u7684\u57fa\u7840\u4e0a\u6709\u673a\u5730\u7ec4\u5408\u4e0d\u540c\u7684\u7ed3\u6784\u7ec4\u4ef6\uff0c\u521b\u9020\u51fa\u5177\u6709\u4e00\u5b9a\u4eba\u7c7b\u539f\u521b\u80fd\u529b\u7684\u65b0\u6865\u6881\u7c7b\u578b\u3002\u81ea\u56de\u5f52\u6a21\u578b\u65e0\u6cd5\u7406\u89e3\u5e8f\u5217\u7684\u542b\u4e49\uff0c\u800c\u591a\u6a21\u6001\u6a21\u578b\u5219\u7ed3\u5408\u56de\u5f52\u548c\u81ea\u56de\u5f52\u6a21\u578b\u6765\u7406\u89e3\u5e8f\u5217\u3002\u591a\u6a21\u6001\u6a21\u578b\u5e94\u8be5\u662f\u672a\u6765\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u9014\u5f84\u3002|[2401.05964v1](http://arxiv.org/pdf/2401.05964v1)|**[link](https://github.com/QQ583304953/Bridge-PixelCNN)**|\n", "2401.05907": "|**2024-01-11**|**Efficient Image Deblurring Networks based on Diffusion Models**|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u56fe\u50cf\u53bb\u6a21\u7cca\u7f51\u7edc|Kang Chen, Yuanjie Liu|This article introduces a sliding window model for defocus deblurring that achieves the best performance to date with extremely low memory usage. Named Swintormer, the method utilizes a diffusion model to generate latent prior features that assist in restoring more detailed images. It also extends the sliding window strategy to specialized Transformer blocks for efficient inference. Additionally, we have further optimized Multiply-Accumulate operations (Macs). Compared to the currently top-performing GRL method, our Swintormer model drastically reduces computational complexity from 140.35 GMACs to 8.02 GMacs, while also improving the Signal-to-Noise Ratio (SNR) for defocus deblurring from 27.04 dB to 27.07 dB. This new method allows for the processing of higher resolution images on devices with limited memory, significantly expanding potential application scenarios. The article concludes with an ablation study that provides an in-depth analysis of the impact of each network module on final performance. The source code and model will be available at the following website: https://github.com/bnm6900030/swintormer.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u6563\u7126\u53bb\u6a21\u7cca\u7684\u6ed1\u52a8\u7a97\u53e3\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u6781\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u5b9e\u73b0\u4e86\u8fc4\u4eca\u4e3a\u6b62\u7684\u6700\u4f73\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u540d\u4e3a Swintormer\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6f5c\u5728\u7684\u5148\u9a8c\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u6062\u590d\u66f4\u8be6\u7ec6\u7684\u56fe\u50cf\u3002\u5b83\u8fd8\u5c06\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u6269\u5c55\u5230\u4e13\u95e8\u7684 Transformer \u5757\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u4e58\u6cd5\u7d2f\u52a0\u8fd0\u7b97 (Mac)\u3002\u4e0e\u76ee\u524d\u8868\u73b0\u6700\u597d\u7684 GRL \u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 Swintormer \u6a21\u578b\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece 140.35 GMAC \u5927\u5e45\u964d\u4f4e\u5230 8.02 GMac\uff0c\u540c\u65f6\u8fd8\u5c06\u6563\u7126\u53bb\u6a21\u7cca\u7684\u4fe1\u566a\u6bd4 (SNR) \u4ece 27.04 dB \u63d0\u9ad8\u5230 27.07 dB\u3002\u8fd9\u79cd\u65b0\u65b9\u6cd5\u5141\u8bb8\u5728\u5185\u5b58\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u5904\u7406\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u663e\u200b\u200b\u7740\u6269\u5c55\u4e86\u6f5c\u5728\u7684\u5e94\u7528\u573a\u666f\u3002\u672c\u6587\u6700\u540e\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\uff0c\u6df1\u5165\u5206\u6790\u4e86\u6bcf\u4e2a\u7f51\u7edc\u6a21\u5757\u5bf9\u6700\u7ec8\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728\u4ee5\u4e0b\u7f51\u7ad9\u83b7\u53d6\uff1ahttps://github.com/bnm6900030/swintormer\u3002|[2401.05907v1](http://arxiv.org/pdf/2401.05907v1)|null|\n", "2401.05870": "|**2024-01-11**|**HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models**|HiCAST\uff1a\u9ad8\u5ea6\u5b9a\u5236\u7684\u4efb\u610f\u98ce\u683c\u8f6c\u79fb\uff0c\u5e26\u6709\u9002\u914d\u5668\u589e\u5f3a\u6269\u6563\u6a21\u578b|Hanzhang Wang, Haoran Wang, Jinze Yang, Zhongrui Yu, Zeke Xie, Lei Tian, Xinyan Xiao, Junjun Jiang, Xianming Liu, Mingming Sun|The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \\textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for video diffusion model training, which significantly improve cross-frame temporal consistency in the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our HiCAST outperforms the existing SoTA methods in generating visually plausible stylization results.|\u4efb\u610f\u98ce\u683c\u8fc1\u79fb\uff08AST\uff09\u7684\u76ee\u6807\u662f\u5c06\u98ce\u683c\u53c2\u8003\u7684\u827a\u672f\u7279\u5f81\u6ce8\u5165\u7ed9\u5b9a\u7684\u56fe\u50cf/\u89c6\u9891\u4e2d\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u6ce8\u91cd\u8ffd\u6c42\u98ce\u683c\u548c\u5185\u5bb9\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u7075\u6d3b\u548c\u5b9a\u5236\u7684\u98ce\u683c\u5316\u7ed3\u679c\u7684\u5de8\u5927\u9700\u6c42\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 AST \u65b9\u6cd5\uff0c\u5373 HiCAST\uff0c\u5b83\u80fd\u591f\u6839\u636e\u5404\u79cd\u8bed\u4e49\u7ebf\u7d22\u6765\u6e90\u663e\u5f0f\u5b9a\u5236\u6837\u5f0f\u5316\u7ed3\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u662f\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u6784\u5efa\u7684\uff0c\u5e76\u7cbe\u5fc3\u8bbe\u8ba1\u4ee5\u5438\u6536\u5185\u5bb9\u548c\u98ce\u683c\u5b9e\u4f8b\u4f5c\u4e3aLDM\u7684\u6761\u4ef6\u3002\u5b83\u7684\u7279\u70b9\u662f\u5f15\u5165\u4e86\\textit{Style Adapter}\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u5bf9\u9f50LDM\u4e2d\u7684\u591a\u7ea7\u6837\u5f0f\u4fe1\u606f\u548c\u5185\u5728\u77e5\u8bc6\u6765\u7075\u6d3b\u5730\u64cd\u7eb5\u8f93\u51fa\u7ed3\u679c\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u6269\u5c55\u6211\u4eec\u7684\u6a21\u578b\u6765\u6267\u884c\u89c6\u9891 AST\u3002\u5229\u7528\u65b0\u9896\u7684\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u98ce\u683c\u5316\u5f3a\u5ea6\u7684\u524d\u63d0\u4e0b\u663e\u7740\u63d0\u9ad8\u8de8\u5e27\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u4ee5\u53ca\u5168\u9762\u7684\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684 HiCAST \u5728\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u98ce\u683c\u5316\u7ed3\u679c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684 SoTA \u65b9\u6cd5\u3002|[2401.05870v1](http://arxiv.org/pdf/2401.05870v1)|null|\n", "2401.05779": "|**2024-01-11**|**EraseDiff: Erasing Data Influence in Diffusion Models**|EraseDiff\uff1a\u6d88\u9664\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6570\u636e\u5f71\u54cd|Jing Wu, Trung Le, Munawar Hayat, Mehrtash Harandi|In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data. The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios. In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10.|\u4e3a\u4e86\u54cd\u5e94\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u548c\u201c\u88ab\u9057\u5fd8\u6743\u201d\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6269\u6563\u6a21\u578b\u7684\u9057\u5fd8\u7b97\u6cd5\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u914d\u5907\u4e86\u4e00\u79cd\u673a\u5236\uff0c\u4ee5\u51cf\u8f7b\u4e0e\u6570\u636e\u8bb0\u5fc6\u76f8\u5173\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u5c06\u9057\u5fd8\u95ee\u9898\u8868\u8ff0\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u5916\u90e8\u76ee\u6807\u662f\u4fdd\u7559\u6269\u6563\u6a21\u578b\u5bf9\u5269\u4f59\u6570\u636e\u7684\u6548\u7528\u3002\u5185\u90e8\u76ee\u6807\u65e8\u5728\u901a\u8fc7\u4f7f\u53ef\u5b66\u4e60\u7684\u751f\u6210\u8fc7\u7a0b\u504f\u79bb\u771f\u5b9e\u7684\u53bb\u566a\u8fc7\u7a0b\u6765\u6e05\u9664\u4e0e\u9057\u5fd8\u6570\u636e\u76f8\u5173\u7684\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u7531\u6b64\u4ea7\u751f\u7684\u53cc\u5c42\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u4e00\u9636\u65b9\u6cd5\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u5b9e\u7528\u6027\u80fd\uff0c\u540c\u65f6\u8b66\u60d5\u6269\u6563\u8fc7\u7a0b\u5e76\u89e3\u51b3\u5176\u4e2d\u7684\u53cc\u5c42\u95ee\u9898\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u7b97\u6cd5\u53ef\u4ee5\u4fdd\u7559\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3001\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u5728\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6269\u6563\u6a21\u578b\u4ee5\u53ca\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u573a\u666f\u4e2d\u8fdb\u884c\u5220\u9664\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u6f14\u793a\u4e86\u5982\u4f55\u4ece UTKFace\u3001CelebA\u3001CelebA-HQ \u548c CIFAR10 \u7b49\u4eba\u8138\u548c\u5bf9\u8c61\u6570\u636e\u96c6\u4e2d\u5fd8\u8bb0\u7c7b\u522b\u3001\u5c5e\u6027\u751a\u81f3\u79cd\u65cf\u3002|[2401.05779v1](http://arxiv.org/pdf/2401.05779v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.06071": "|**2024-01-11**|**LEGO:Language Enhanced Multi-modal Grounding Model**|\u4e50\u9ad8\uff1a\u8bed\u8a00\u589e\u5f3a\u591a\u6a21\u5f0f\u63a5\u5730\u6a21\u578b|Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et.al.|Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u6001\u7684\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5f3a\u8c03\u6355\u83b7\u6bcf\u79cd\u6a21\u6001\u5185\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u8de8\u6a21\u6001\u611f\u77e5\u5c40\u90e8\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7406\u89e3\u8f93\u5165\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u66f4\u7ec6\u81f4\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u591a\u79cd\u6a21\u5f0f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7406\u89e3\u7684\u6a21\u578b\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5bf9\u5e7f\u6cdb\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LEGO\uff0c\u4e00\u79cd\u8bed\u8a00\u589e\u5f3a\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002\u9664\u4e86\u50cf\u5176\u4ed6\u591a\u6a21\u6001\u6a21\u578b\u4e00\u6837\u6355\u83b7\u5168\u5c40\u4fe1\u606f\u4e4b\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u8fd8\u64c5\u957f\u6267\u884c\u9700\u8981\u8be6\u7ec6\u4e86\u89e3\u8f93\u5165\u4e2d\u7684\u672c\u5730\u4fe1\u606f\u7684\u4efb\u52a1\u3002\u5b83\u6f14\u793a\u4e86\u5bf9\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u7279\u5b9a\u533a\u57df\u7684\u7cbe\u786e\u8bc6\u522b\u548c\u5b9a\u4f4d\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u4ece\u800c\u4ea7\u751f\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u7684\u591a\u6a21\u5f0f\u3001\u591a\u7c92\u5ea6\u6570\u636e\u96c6\u3002\u6211\u4eec\u6a21\u578b\u7684\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6f14\u793a\u53ef\u4ee5\u5728 https://github.com/lzw-lzw/LEGO \u627e\u5230\u3002|[2401.06071v1](http://arxiv.org/pdf/2401.06071v1)|null|\n", "2401.05827": "|**2024-01-11**|**Hallucination Benchmark in Medical Visual Question Answering**|\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u57fa\u51c6|Jinge Wu, Yunsoo Kim, Honghan Wu|The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.|\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u65b9\u9762\u53d6\u5f97\u7684\u6210\u529f\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u533b\u5b66\uff08Med-VQA\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5df2\u7ecf\u663e\u793a\u51fa\u5b9e\u73b0\u6709\u6548\u7684\u533b\u7597\u4fdd\u5065\u89c6\u89c9\u52a9\u624b\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5e76\u672a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5bf9\u5e7b\u89c9\u73b0\u8c61\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d4b\u8bd5\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e0e\u95ee\u7b54\u96c6\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u7684\u5e7b\u89c9\u57fa\u51c6\uff0c\u5e76\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u8be5\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5404\u79cd\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002|[2401.05827v1](http://arxiv.org/pdf/2401.05827v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.05745": "|**2024-01-11**|**Surface Normal Estimation with Transformers**|\u4f7f\u7528 Transformer \u8fdb\u884c\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1|Barry Shichen Hu, Siyun Liang, Johannes Paetzold, Huy H. Nguyen, Isao Echizen, Jiapeng Tang|We propose the use of a Transformer to accurately predict normals from point clouds with noise and density variations. Previous learning-based methods utilize PointNet variants to explicitly extract multi-scale features at different input scales, then focus on a surface fitting method by which local point cloud neighborhoods are fitted to a geometric surface approximated by either a polynomial function or a multi-layer perceptron (MLP). However, fitting surfaces to fixed-order polynomial functions can suffer from overfitting or underfitting, and learning MLP-represented hyper-surfaces requires pre-generated per-point weights. To avoid these limitations, we first unify the design choices in previous works and then propose a simplified Transformer-based model to extract richer and more robust geometric features for the surface normal estimation task. Through extensive experiments, we demonstrate that our Transformer-based method achieves state-of-the-art performance on both the synthetic shape dataset PCPNet, and the real-world indoor scene dataset SceneNN, exhibiting more noise-resilient behavior and significantly faster inference. Most importantly, we demonstrate that the sophisticated hand-designed modules in existing works are not necessary to excel at the task of surface normal estimation.|\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 Transformer \u6765\u51c6\u786e\u9884\u6d4b\u5177\u6709\u566a\u58f0\u548c\u5bc6\u5ea6\u53d8\u5316\u7684\u70b9\u4e91\u7684\u6cd5\u7ebf\u3002\u4ee5\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5229\u7528 PointNet \u53d8\u4f53\u5728\u4e0d\u540c\u8f93\u5165\u5c3a\u5ea6\u4e0a\u663e\u5f0f\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u7136\u540e\u4e13\u6ce8\u4e8e\u8868\u9762\u62df\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u5c06\u5c40\u90e8\u70b9\u4e91\u90bb\u57df\u62df\u5408\u5230\u7531\u591a\u9879\u5f0f\u51fd\u6570\u6216\u591a\u5c42\u8fd1\u4f3c\u7684\u51e0\u4f55\u8868\u9762\u611f\u77e5\u5668\uff08MLP\uff09\u3002\u7136\u800c\uff0c\u5c06\u66f2\u9762\u62df\u5408\u5230\u56fa\u5b9a\u9636\u591a\u9879\u5f0f\u51fd\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5ea6\u62df\u5408\u6216\u6b20\u62df\u5408\u7684\u60c5\u51b5\uff0c\u5e76\u4e14\u5b66\u4e60 MLP \u8868\u793a\u7684\u8d85\u66f2\u9762\u9700\u8981\u9884\u5148\u751f\u6210\u7684\u6bcf\u70b9\u6743\u91cd\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u9996\u5148\u7edf\u4e00\u4ee5\u524d\u5de5\u4f5c\u4e2d\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u7b80\u5316\u6a21\u578b\uff0c\u4e3a\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u4efb\u52a1\u63d0\u53d6\u66f4\u4e30\u5bcc\u3001\u66f4\u9c81\u68d2\u7684\u51e0\u4f55\u7279\u5f81\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u5728\u5408\u6210\u5f62\u72b6\u6570\u636e\u96c6 PCPNet \u548c\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u6570\u636e\u96c6 SceneNN \u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6297\u566a\u884c\u4e3a\u548c\u663e\u7740\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u8bc1\u660e\u73b0\u6709\u4f5c\u54c1\u4e2d\u590d\u6742\u7684\u624b\u5de5\u8bbe\u8ba1\u6a21\u5757\u4e0d\u4e00\u5b9a\u80fd\u591f\u51fa\u8272\u5730\u5b8c\u6210\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u7684\u4efb\u52a1\u3002|[2401.05745v1](http://arxiv.org/pdf/2401.05745v1)|null|\n", "2401.05735": "|**2024-01-11**|**Object-Centric Diffusion for Efficient Video Editing**|\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u7f16\u8f91|Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki M. Asano, Amirhossein Habibian|Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, which reduces cost of cross-frame attention by fusing redundant tokens in unimportant background regions. Both techniques are readily applicable to a given video editing model \\textit{without} retraining, and can drastically reduce its memory and computational cost. We evaluate our proposals on inversion-based and control-signal-based editing pipelines, and show a latency reduction up to 10x for a comparable synthesis quality.|\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u7f16\u8f91\u5df2\u7ecf\u8fbe\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8d28\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u6309\u7167\u6587\u672c\u7f16\u8f91\u63d0\u793a\u8f6c\u6362\u7ed9\u5b9a\u89c6\u9891\u8f93\u5165\u7684\u5168\u5c40\u6837\u5f0f\u3001\u5c40\u90e8\u7ed3\u6784\u548c\u5c5e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u6837\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u6765\u751f\u6210\u65f6\u95f4\u76f8\u5e72\u7684\u5e27\uff0c\u65e0\u8bba\u662f\u91c7\u7528\u6269\u6563\u53cd\u8f6c\u548c/\u6216\u8de8\u5e27\u6ce8\u610f\u7684\u5f62\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u8fd9\u79cd\u4f4e\u6548\u7387\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u4fee\u6539\u5efa\u8bae\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u663e\u7740\u63d0\u9ad8\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\uff08\u88ab\u79f0\u4e3a OCD\uff09\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u66f4\u591a\u5730\u5206\u914d\u7ed9\u524d\u53f0\u7f16\u8f91\u533a\u57df\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u5ef6\u8fdf\uff0c\u8fd9\u5bf9\u4e8e\u611f\u77e5\u8d28\u91cf\u6765\u8bf4\u53ef\u80fd\u66f4\u91cd\u8981\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u65b0\u9896\u7684\u63d0\u8bae\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff1ai) \u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u91c7\u6837\uff0c\u89e3\u8026\u5728\u663e\u7740\u533a\u57df\u6216\u80cc\u666f\u4e0a\u82b1\u8d39\u7684\u6269\u6563\u6b65\u9aa4\uff0c\u5c06\u5927\u90e8\u5206\u6a21\u578b\u5bb9\u91cf\u5206\u914d\u7ed9\u524d\u8005\uff0c\u4ee5\u53ca ii) \u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684 3D \u4ee4\u724c\u5408\u5e76\uff0c\u8fd9\u964d\u4f4e\u4e86\u901a\u8fc7\u5728\u4e0d\u91cd\u8981\u7684\u80cc\u666f\u533a\u57df\u878d\u5408\u5197\u4f59\u6807\u8bb0\u6765\u5b9e\u73b0\u8de8\u5e27\u6ce8\u610f\u529b\u3002\u8fd9\u4e24\u79cd\u6280\u672f\u90fd\u5f88\u5bb9\u6613\u9002\u7528\u4e8e\u7ed9\u5b9a\u7684\u89c6\u9891\u7f16\u8f91\u6a21\u578b\\textit{\u65e0\u9700}\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u5927\u5927\u51cf\u5c11\u5176\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5173\u4e8e\u57fa\u4e8e\u53cd\u8f6c\u548c\u57fa\u4e8e\u63a7\u5236\u4fe1\u53f7\u7684\u7f16\u8f91\u7ba1\u9053\u7684\u5efa\u8bae\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u540c\u7b49\u5408\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5ef6\u8fdf\u51cf\u5c11\u4e86\u9ad8\u8fbe 10 \u500d\u3002|[2401.05735v1](http://arxiv.org/pdf/2401.05735v1)|null|\n", "2401.05633": "|**2024-01-11**|**Transforming Image Super-Resolution: A ConvFormer-based Efficient Approach**|\u53d8\u6362\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff1a\u4e00\u79cd\u57fa\u4e8e ConvFormer \u7684\u9ad8\u6548\u65b9\u6cd5|Gang Wu, Junjun Jiang, Junpeng Jiang, Xianming Liu|Recent progress in single-image super-resolution (SISR) has achieved remarkable performance, yet the computational costs of these methods remain a challenge for deployment on resource-constrained devices. Especially for transformer-based methods, the self-attention mechanism in such models brings great breakthroughs while incurring substantial computational costs. To tackle this issue, we introduce the Convolutional Transformer layer (ConvFormer) and the ConvFormer-based Super-Resolution network (CFSR), which offer an effective and efficient solution for lightweight image super-resolution tasks. In detail, CFSR leverages the large kernel convolution as the feature mixer to replace the self-attention module, efficiently modeling long-range dependencies and extensive receptive fields with a slight computational cost. Furthermore, we propose an edge-preserving feed-forward network, simplified as EFN, to obtain local feature aggregation and simultaneously preserve more high-frequency information. Extensive experiments demonstrate that CFSR can achieve an advanced trade-off between computational cost and performance when compared to existing lightweight SR methods. Compared to state-of-the-art methods, e.g. ShuffleMixer, the proposed CFSR achieves 0.39 dB gains on Urban100 dataset for x2 SR task while containing 26% and 31% fewer parameters and FLOPs, respectively. Code and pre-trained models are available at https://github.com/Aitical/CFSR.|\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\uff0c\u6b64\u7c7b\u6a21\u578b\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5e26\u6765\u4e86\u5de8\u5927\u7684\u7a81\u7834\uff0c\u540c\u65f6\u5e26\u6765\u4e86\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5377\u79ef\u53d8\u6362\u5c42\uff08ConvFormer\uff09\u548c\u57fa\u4e8eConvFormer\u7684\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff08CFSR\uff09\uff0c\u5b83\u4eec\u4e3a\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5177\u4f53\u6765\u8bf4\uff0cCFSR\u5229\u7528\u5927\u6838\u5377\u79ef\u4f5c\u4e3a\u7279\u5f81\u6df7\u5408\u5668\u6765\u53d6\u4ee3\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u5c11\u91cf\u7684\u8ba1\u7b97\u6210\u672c\u6709\u6548\u5730\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u6027\u548c\u5e7f\u6cdb\u7684\u611f\u53d7\u91ce\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u4fdd\u7559\u524d\u9988\u7f51\u7edc\uff0c\u7b80\u5316\u4e3a EFN\uff0c\u4ee5\u83b7\u5f97\u5c40\u90e8\u7279\u5f81\u805a\u5408\u5e76\u540c\u65f6\u4fdd\u7559\u66f4\u591a\u9ad8\u9891\u4fe1\u606f\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7 SR \u65b9\u6cd5\u76f8\u6bd4\uff0cCFSR \u53ef\u4ee5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u9ad8\u7ea7\u6743\u8861\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f8b\u5982ShuffleMixer \u4e2d\uff0c\u6240\u63d0\u51fa\u7684 CFSR \u5728 x2 SR \u4efb\u52a1\u7684 Urban100 \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 0.39 dB \u7684\u589e\u76ca\uff0c\u540c\u65f6\u53c2\u6570\u548c FLOP \u5206\u522b\u51cf\u5c11\u4e86 26% \u548c 31%\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5728 https://github.com/Atical/CFSR \u4e0a\u83b7\u53d6\u3002|[2401.05633v1](http://arxiv.org/pdf/2401.05633v1)|null|\n"}, "Nerf": {"2401.06052": "|**2024-01-11**|**Fast High Dynamic Range Radiance Fields for Dynamic Scenes**|\u9002\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u5feb\u901f\u9ad8\u52a8\u6001\u8303\u56f4\u8f90\u5c04\u573a|Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang|Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \\url{https://guanjunwu.github.io/HDR-HexPlane/}.|\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u53ca\u5176\u6269\u5c55\u5728\u8868\u793a 3D \u573a\u666f\u548c\u5408\u6210\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5927\u591a\u6570 NeRF \u65b9\u6cd5\u91c7\u7528\u4f4e\u52a8\u6001\u8303\u56f4 (LDR) \u56fe\u50cf\uff0c\u8fd9\u53ef\u80fd\u4f1a\u4e22\u5931\u7ec6\u8282\uff0c\u5c24\u5176\u662f\u5728\u7167\u660e\u4e0d\u5747\u5300\u7684\u60c5\u51b5\u4e0b\u3002\u4e4b\u524d\u7684\u4e00\u4e9b NeRF \u65b9\u6cd5\u5c1d\u8bd5\u5f15\u5165\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u6280\u672f\uff0c\u4f46\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u573a\u666f\u3002\u4e3a\u4e86\u5c06 HDR NeRF \u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001 HDR NeRF \u6846\u67b6\uff0c\u540d\u4e3a HDR-HexPlane\uff0c\u5b83\u53ef\u4ee5\u4ece\u4f7f\u7528\u5404\u79cd\u66dd\u5149\u6355\u83b7\u7684\u52a8\u6001 2D \u56fe\u50cf\u4e2d\u5b66\u4e60 3D \u573a\u666f\u3002\u6784\u5efa\u53ef\u5b66\u4e60\u7684\u66dd\u5149\u6620\u5c04\u51fd\u6570\u4ee5\u83b7\u5f97\u6bcf\u4e2a\u56fe\u50cf\u7684\u81ea\u9002\u5e94\u66dd\u5149\u503c\u3002\u57fa\u4e8e\u5355\u8c03\u9012\u589e\u5148\u9a8c\uff0c\u8bbe\u8ba1\u4e86\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60\u3002\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4ee5\u4efb\u4f55\u6240\u9700\u7684\u66dd\u5149\u5ea6\u6e32\u67d3\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f7f\u7528\u4e0d\u540c\u66dd\u5149\u6355\u83b7\u7684\u591a\u4e2a\u52a8\u6001\u573a\u666f\u4ee5\u8fdb\u884c\u8bc4\u4f30\u3002\u6240\u6709\u6570\u636e\u96c6\u548c\u4ee3\u7801\u90fd\u53ef\u4ee5\u5728 \\url{https://guanjunwu.github.io/HDR-HexPlane/} \u83b7\u53d6\u3002|[2401.06052v1](http://arxiv.org/pdf/2401.06052v1)|null|\n", "2401.05750": "|**2024-01-11**|**GO-NeRF: Generating Virtual Objects in Neural Radiance Fields**|GO-NeRF\uff1a\u5728\u795e\u7ecf\u8f90\u5c04\u573a\u4e2d\u751f\u6210\u865a\u62df\u5bf9\u8c61|Peng Dai, Feitong Tan, Xin Yu, Yinda Zhang, Xiaojuan Qi|Despite advances in 3D generation, the direct creation of 3D objects within an existing 3D scene represented as NeRF remains underexplored. This process requires not only high-quality 3D object generation but also seamless composition of the generated 3D content into the existing NeRF. To this end, we propose a new method, GO-NeRF, capable of utilizing scene context for high-quality and harmonious 3D object generation within an existing NeRF. Our method employs a compositional rendering formulation that allows the generated 3D objects to be seamlessly composited into the scene utilizing learned 3D-aware opacity maps without introducing unintended scene modification. Moreover, we also develop tailored optimization objectives and training strategies to enhance the model's ability to exploit scene context and mitigate artifacts, such as floaters, originating from 3D object generation within a scene. Extensive experiments on both feed-forward and $360^o$ scenes show the superior performance of our proposed GO-NeRF in generating objects harmoniously composited with surrounding scenes and synthesizing high-quality novel view images. Project page at {\\url{https://daipengwa.github.io/GO-NeRF/}.|\u5c3d\u7ba1 3D \u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5728\u4ee5 NeRF \u8868\u793a\u7684\u73b0\u6709 3D \u573a\u666f\u4e2d\u76f4\u63a5\u521b\u5efa 3D \u5bf9\u8c61\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e0d\u4ec5\u9700\u8981\u9ad8\u8d28\u91cf\u7684 3D \u5bf9\u8c61\u751f\u6210\uff0c\u8fd8\u9700\u8981\u5c06\u751f\u6210\u7684 3D \u5185\u5bb9\u65e0\u7f1d\u7ec4\u5408\u5230\u73b0\u6709\u7684 NeRF \u4e2d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5 GO-NeRF\uff0c\u80fd\u591f\u5229\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u5728\u73b0\u6709 NeRF \u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u548c\u8c10\u7684 3D \u5bf9\u8c61\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u5408\u6210\u6e32\u67d3\u516c\u5f0f\uff0c\u5141\u8bb8\u4f7f\u7528\u5b66\u4e60\u7684 3D \u611f\u77e5\u4e0d\u900f\u660e\u8d34\u56fe\u5c06\u751f\u6210\u7684 3D \u5bf9\u8c61\u65e0\u7f1d\u5408\u6210\u5230\u573a\u666f\u4e2d\uff0c\u800c\u4e0d\u4f1a\u5f15\u5165\u610f\u5916\u7684\u573a\u666f\u4fee\u6539\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u5b9a\u5236\u7684\u4f18\u5316\u76ee\u6807\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5229\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u51cf\u5c11\u6e90\u81ea\u573a\u666f\u5185 3D \u5bf9\u8c61\u751f\u6210\u7684\u4f2a\u50cf\uff08\u4f8b\u5982\u6f02\u6d6e\u7269\uff09\u7684\u80fd\u529b\u3002\u5bf9\u524d\u9988\u548c 360^o$ \u573a\u666f\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 GO-NeRF \u5728\u751f\u6210\u4e0e\u5468\u56f4\u573a\u666f\u548c\u8c10\u5408\u6210\u7684\u5bf9\u8c61\u4ee5\u53ca\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u56fe\u50cf\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8e {\\url{https://daipengwa.github.io/GO-NeRF/}\u3002|[2401.05750v1](http://arxiv.org/pdf/2401.05750v1)|null|\n"}, "3DGS": {"2401.06116": "|**2024-01-11**|**Gaussian Shadow Casting for Neural Characters**|\u795e\u7ecf\u89d2\u8272\u7684\u9ad8\u65af\u9634\u5f71\u6295\u5c04|Luis Bolanos, Shih-Yang Su, Helge Rhodin|Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.|\u795e\u7ecf\u89d2\u8272\u6a21\u578b\u73b0\u5728\u53ef\u4ee5\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u8be6\u7ec6\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u7eb9\u7406\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u660e\u786e\u7684\u9634\u5f71\u548c\u9634\u5f71\uff0c\u5bfc\u81f4\u5728\u751f\u6210\u65b0\u9896\u7684\u89c6\u56fe\u548c\u59ff\u52bf\u6216\u91cd\u65b0\u7167\u660e\u671f\u95f4\u51fa\u73b0\u4f2a\u5f71\u3002\u5305\u542b\u9634\u5f71\u7279\u522b\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u662f\u5168\u5c40\u6548\u679c\uff0c\u5e76\u4e14\u6240\u9700\u7684\u4e8c\u6b21\u5149\u7ebf\u6295\u5c04\u6210\u672c\u5f88\u9ad8\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u9ad8\u65af\u5bc6\u5ea6\u4ee3\u7406\u7684\u65b0\u9634\u5f71\u6a21\u578b\uff0c\u7528\u7b80\u5355\u7684\u5206\u6790\u516c\u5f0f\u4ee3\u66ff\u91c7\u6837\u3002\u5b83\u652f\u6301\u52a8\u6001\u8fd0\u52a8\u5e76\u4e13\u4e3a\u9634\u5f71\u8ba1\u7b97\u800c\u5b9a\u5236\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bc6\u5207\u76f8\u5173\u7684\u9ad8\u65af\u6cfc\u6e85\u6240\u9700\u7684\u4eff\u5c04\u6295\u5f71\u8fd1\u4f3c\u548c\u6392\u5e8f\u3002\u4e0e\u5ef6\u8fdf\u795e\u7ecf\u6e32\u67d3\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u9ad8\u65af\u9634\u5f71\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5f00\u9500\u5b9e\u73b0\u6717\u4f2f\u7740\u8272\u548c\u9634\u5f71\u6295\u5c04\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u91cd\u5efa\uff0c\u5728\u5177\u6709\u76f4\u5c04\u9633\u5149\u548c\u786c\u9634\u5f71\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u6237\u5916\u573a\u666f\u4e2d\u66f4\u597d\u5730\u5206\u79bb\u4e86\u53cd\u7167\u7387\u3001\u9634\u5f71\u548c\u9634\u5f71\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u4f18\u5316\u5149\u7ebf\u65b9\u5411\uff0c\u65e0\u9700\u7528\u6237\u8f93\u5165\u4efb\u4f55\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u9896\u7684\u59ff\u52bf\u5177\u6709\u66f4\u5c11\u7684\u9634\u5f71\u4f2a\u5f71\uff0c\u5e76\u4e14\u65b0\u9896\u573a\u666f\u4e2d\u7684\u91cd\u65b0\u7167\u660e\u66f4\u52a0\u771f\u5b9e\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u5728\u65b0\u9896\u73af\u5883\u4e2d\u6446\u51fa\u795e\u7ecf\u89d2\u8272\u59ff\u52bf\u7684\u65b0\u65b9\u6cd5\uff0c\u589e\u52a0\u4e86\u5176\u9002\u7528\u6027\u3002|[2401.06116v1](http://arxiv.org/pdf/2401.06116v1)|null|\n"}, "3D/CG": {"2401.06126": "|**2024-01-11**|**Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors**|\u9002\u5408\u6240\u6709\u4eba\u7684\u914d\u97f3\uff1a\u4f7f\u7528\u795e\u7ecf\u6e32\u67d3\u5148\u9a8c\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u7684\u89c6\u89c9\u914d\u97f3|Jack Saunders, Vinay Namboodiri|Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio. Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption. Existing methods are split into either person-generic or person-specific models. Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets. Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts. Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches. Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures. This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies. Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models. Our experiments on real-world, limited data scenarios find that our model is preferred over all others. The project page may be found at https://dubbingforeveryone.github.io/|\u89c6\u89c9\u914d\u97f3\u662f\u5728\u89c6\u9891\u4e2d\u751f\u6210\u6f14\u5458\u7684\u5634\u5507\u52a8\u4f5c\u4ee5\u4e0e\u7ed9\u5b9a\u97f3\u9891\u540c\u6b65\u7684\u8fc7\u7a0b\u3002\u6700\u8fd1\u7684\u8fdb\u5c55\u5728\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u672a\u80fd\u4ea7\u751f\u9002\u5408\u5927\u89c4\u6a21\u91c7\u7528\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5206\u4e3a\u4e2a\u4eba\u901a\u7528\u6a21\u578b\u6216\u4e2a\u4eba\u7279\u5b9a\u6a21\u578b\u3002\u7279\u5b9a\u4e8e\u4eba\u7684\u6a21\u578b\u4ea7\u751f\u7684\u7ed3\u679c\u51e0\u4e4e\u4e0e\u73b0\u5b9e\u6ca1\u6709\u533a\u522b\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4f7f\u7528\u5927\u578b\u5355\u4eba\u6570\u636e\u96c6\u7684\u957f\u65f6\u95f4\u8bad\u7ec3\u3002\u4eba\u7269\u901a\u7528\u4f5c\u54c1\u5141\u8bb8\u5c06\u4efb\u4f55\u89c6\u9891\u89c6\u89c9\u914d\u97f3\u4e3a\u4efb\u4f55\u97f3\u9891\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u57f9\u8bad\uff0c\u4f46\u8fd9\u4e9b\u4f5c\u54c1\u65e0\u6cd5\u6355\u6349\u7279\u5b9a\u4e8e\u4eba\u7269\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u4e14\u7ecf\u5e38\u53d7\u5230\u89c6\u89c9\u4f2a\u5f71\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u6570\u200b\u200b\u636e\u9ad8\u6548\u7684\u795e\u7ecf\u6e32\u67d3\u5148\u9a8c\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u5305\u62ec\u5b66\u4e60\u5ef6\u8fdf\u795e\u7ecf\u6e32\u67d3\u5148\u9a8c\u7f51\u7edc\u548c\u4f7f\u7528\u795e\u7ecf\u7eb9\u7406\u8fdb\u884c\u7279\u5b9a\u4e8e\u6f14\u5458\u7684\u9002\u5e94\u3002\u6b64\u65b9\u6cd5\u5141\u8bb8$\\textbf{\u53ea\u9700\u51e0\u79d2\u949f\u7684\u6570\u636e\u5373\u53ef\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u914d\u97f3}$\uff0c\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6f14\u5458\uff08\u4ece\u4e00\u7ebf\u540d\u4eba\u5230\u80cc\u666f\u6f14\u5458\uff09\u8fdb\u884c\u89c6\u9891\u914d\u97f3\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u9879\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002\u6211\u4eec\u4e4b\u524d\u7684\u5b66\u4e60\u548c\u9002\u5e94\u65b9\u6cd5 $\\textbf{\u6982\u62ec\u5230\u6709\u9650\u7684\u6570\u636e}$ \u6bd4\u73b0\u6709\u7684\u7279\u5b9a\u4e8e\u4e2a\u4eba\u7684\u6a21\u578b\u66f4\u597d\uff0c\u5e76\u4e14\u66f4 $\\textbf{\u53ef\u6269\u5c55}$\u3002\u6211\u4eec\u5bf9\u73b0\u5b9e\u4e16\u754c\u3001\u6709\u9650\u6570\u636e\u573a\u666f\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6bd4\u6240\u6709\u5176\u4ed6\u6a21\u578b\u66f4\u53d7\u9752\u7750\u3002\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728 https://dubbingforeveryone.github.io/ \u627e\u5230|[2401.06126v1](http://arxiv.org/pdf/2401.06126v1)|null|\n", "2401.06056": "|**2024-01-11**|**MatSynth: A Modern PBR Materials Dataset**|MatSynth\uff1a\u73b0\u4ee3 PBR \u6750\u6599\u6570\u636e\u96c6|Giuseppe Vecchio, Valentin Deschaintre|We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.|\u6211\u4eec\u4ecb\u7ecd MatSynth\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 4,000+$ CC0 \u8d85\u9ad8\u5206\u8fa8\u7387 PBR \u6750\u6599\u7684\u6570\u636e\u96c6\u3002\u6750\u8d28\u662f\u865a\u62df\u53ef\u91cd\u65b0\u7167\u660e\u8d44\u4ea7\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5b9a\u4e49\u4e86\u5149\u5728\u51e0\u4f55\u4f53\u8868\u9762\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u9274\u4e8e\u5b83\u4eec\u7684\u91cd\u8981\u6027\uff0c\u5927\u91cf\u7684\u7814\u7a76\u5de5\u4f5c\u81f4\u529b\u4e8e\u5b83\u4eec\u7684\u8868\u73b0\u3001\u521b\u9020\u548c\u83b7\u53d6\u3002\u7136\u800c\uff0c\u5728\u8fc7\u53bb\u7684\u516d\u5e74\u4e2d\uff0c\u5927\u591a\u6570\u6750\u6599\u83b7\u53d6\u6216\u751f\u6210\u7684\u7814\u7a76\u8981\u4e48\u4f9d\u8d56\u4e8e\u76f8\u540c\u7684\u72ec\u7279\u6570\u636e\u96c6\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u516c\u53f8\u62e5\u6709\u7684\u5e9e\u5927\u7684\u7a0b\u5e8f\u6750\u6599\u5e93\u3002\u901a\u8fc7\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6bd4\u4ee5\u524d\u516c\u5f00\u7684\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u3001\u5206\u8fa8\u7387\u66f4\u9ad8\u7684\u6750\u6599\u96c6\u3002\u6211\u4eec\u4ed4\u7ec6\u8ba8\u8bba\u4e86\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5728\u6750\u6599\u91c7\u96c6\u548c\u751f\u6210\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u4f18\u52bf\u3002\u5b8c\u6574\u7684\u6570\u636e\u8fd8\u5305\u542b\u5143\u6570\u636e\uff0c\u5176\u4e2d\u5305\u542b\u6bcf\u79cd\u6750\u8d28\u7684\u6765\u6e90\u3001\u8bb8\u53ef\u8bc1\u3001\u7c7b\u522b\u3001\u6807\u7b7e\u3001\u521b\u5efa\u65b9\u6cd5\u4ee5\u53ca\u53ef\u7528\u7684\u63cf\u8ff0\u548c\u7269\u7406\u5c3a\u5bf8\uff0c\u4ee5\u53ca\u5728\u5404\u79cd\u73af\u5883\u7167\u660e\u4e0b\u4ee5 1K \u8868\u793a\u7684\u589e\u5f3a\u6750\u8d28\u7684 3M+ \u6e32\u67d3\u3002 MatSynth \u6570\u636e\u96c6\u901a\u8fc7\u9879\u76ee\u9875\u9762\u53d1\u5e03\uff1ahttps://www.gvecchio.com/matsynth\u3002|[2401.06056v1](http://arxiv.org/pdf/2401.06056v1)|null|\n", "2401.06013": "|**2024-01-11**|**Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery**|Surgical-DINO\uff1a\u5185\u7aa5\u955c\u624b\u672f\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\u7684\u9002\u914d\u5668\u5b66\u4e60|Cui Beilei, Islam Mobarakol, Bai Long, Ren Hongliang|Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.|\u76ee\u7684\uff1a\u673a\u5668\u4eba\u624b\u672f\u4e2d\u7684\u6df1\u5ea6\u4f30\u8ba1\u5bf9\u4e8e 3D \u91cd\u5efa\u3001\u624b\u672f\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u53ef\u89c6\u5316\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u8bb8\u591a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6df1\u5ea6\u4f30\u8ba1\uff08\u4f8b\u5982 DINOv2\uff09\uff0c\u4f46\u6700\u8fd1\u7684\u5de5\u4f5c\u89c2\u5bdf\u5230\u5176\u5728\u533b\u7597\u548c\u5916\u79d1\u9886\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u7528\u4e8e\u624b\u672f\u6df1\u5ea6\u4f30\u8ba1\u7684\u57fa\u7840\u6a21\u578b\u7684\u4f4e\u9636\u9002\u5e94\uff08LoRA\uff09\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u79f0\u4e3a Surgical-DINO\uff0c\u662f DINOv2 \u7684\u4f4e\u9636\u6539\u7f16\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u624b\u672f\u4e2d\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002\u6211\u4eec\u6784\u5efa LoRA \u5c42\u5e76\u5c06\u5176\u96c6\u6210\u5230 DINO \u4e2d\uff0c\u4ee5\u9002\u5e94\u624b\u672f\u7279\u5b9a\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u5fae\u8c03\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u51bb\u7ed3\u4e86 DINO \u56fe\u50cf\u7f16\u7801\u5668\uff0c\u8be5\u7f16\u7801\u5668\u663e\u793a\u51fa\u51fa\u8272\u7684\u89c6\u89c9\u8868\u793a\u80fd\u529b\uff0c\u5e76\u4e14\u4ec5\u4f18\u5316 LoRA \u5c42\u548c\u6df1\u5ea6\u89e3\u7801\u5668\u4ee5\u96c6\u6210\u624b\u672f\u573a\u666f\u7684\u7279\u5f81\u3002\u7ed3\u679c\uff1a\u6211\u4eec\u7684\u6a21\u578b\u5728 SCARED \u7684 MICCAI \u6311\u6218\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u8be5\u6570\u636e\u96c6\u662f\u4ece\u8fbe\u82ac\u5947 Xi \u5185\u7aa5\u955c\u624b\u672f\u4e2d\u6536\u96c6\u7684\u3002\u6211\u4eec\u7684\u7ecf\u9a8c\u8868\u660e\uff0c\u5728\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0cSurgical-DINO \u663e\u7740\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u7684\u5206\u6790\u8bc1\u660e\u4e86\u6211\u4eec\u7684 LoRA \u5c42\u548c\u9002\u5e94\u7684\u663e\u7740\u6548\u679c\u3002\u7ed3\u8bba\uff1aSurgical-DINO \u4e3a\u5c06\u57fa\u7840\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8e\u624b\u672f\u9886\u57df\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e9b\u542f\u793a\u3002\u7ed3\u679c\u4e2d\u6709\u660e\u786e\u7684\u8bc1\u636e\u8868\u660e\uff0c\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u96f6\u6837\u672c\u9884\u6d4b\u6216\u6734\u7d20\u5fae\u8c03\u4e0d\u8db3\u4ee5\u76f4\u63a5\u5728\u5916\u79d1\u9886\u57df\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/BeileiCui/SurgicalDINO \u83b7\u53d6\u3002|[2401.06013v1](http://arxiv.org/pdf/2401.06013v1)|null|\n", "2401.05971": "|**2024-01-11**|**UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization**|UAVD4L\uff1a\u65e0\u4eba\u673a 6 \u81ea\u7531\u5ea6\u5b9a\u4f4d\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6|Rouwan Wu, Xiaoya Cheng, Juelin Zhu, Xuxiang Liu, Maojun Zhang, Shen Yan|Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. Code and dataset are available at https://github.com/RingoWRW/UAVD4L|\u5c3d\u7ba1\u5728\u6ca1\u6709 GPS \u7684\u73af\u5883\u4e2d\u65e0\u4eba\u673a (UAV) \u7684\u5168\u7403\u5b9a\u4f4d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u7136\u53d7\u5230\u6570\u636e\u96c6\u53ef\u7528\u6027\u7684\u9650\u5236\u3002\u5f53\u524d\u7684\u6570\u636e\u96c6\u901a\u5e38\u5173\u6ce8\u5c0f\u89c4\u6a21\u573a\u666f\uff0c\u7f3a\u4e4f\u89c6\u70b9\u53ef\u53d8\u6027\u3001\u51c6\u786e\u7684\u5730\u9762\u5b9e\u51b5 (GT) \u59ff\u6001\u548c\u65e0\u4eba\u673a\u5185\u7f6e\u4f20\u611f\u5668\u6570\u636e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7528\u4e8e\u5b9a\u4f4d\u7684\u5927\u89c4\u6a21\u516d\u81ea\u7531\u5ea6\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff08UAVD4L\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u516d\u81ea\u7531\u5ea6\u5b9a\u4f4d\u7ba1\u9053\uff08UAVLoc\uff09\uff0c\u5176\u4e2d\u5305\u62ec\u79bb\u7ebf\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5728\u7ebf\u89c6\u89c9\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u57fa\u4e8e 6-DoF \u4f30\u8ba1\u5668\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7528\u4e8e\u5728 3D \u7a7a\u95f4\u4e2d\u8ddf\u8e2a\u5730\u9762\u76ee\u6807\u7684\u5206\u5c42\u7cfb\u7edf\u3002\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u5728 https://github.com/RingoWRW/UAVD4L \u83b7\u53d6|[2401.05971v1](http://arxiv.org/pdf/2401.05971v1)|null|\n", "2401.05891": "|**2024-01-11**|**LiDAR data acquisition and processing for ecology applications**|\u7528\u4e8e\u751f\u6001\u5e94\u7528\u7684\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u91c7\u96c6\u548c\u5904\u7406|Ion Ciobotari, Adriana Pr\u00edncipe, Maria Alexandra Oliveira, Jo\u00e3o Nuno Silva|The collection of ecological data in the field is essential to diagnose, monitor and manage ecosystems in a sustainable way. Since acquisition of this information through traditional methods are generally time-consuming, due to the capability of recording large volumes of data in short time periods, automation of data acquisition sees a growing trend. Terrestrial laser scanners (TLS), particularly LiDAR sensors, have been used in ecology, allowing to reconstruct the 3D structure of vegetation, and thus, infer ecosystem characteristics based on the spatial variation of the density of points. However, the low amount of information obtained per beam, lack of data analysis tools and the high cost of the equipment limit their use. This way, a low-cost TLS (<10k$) was developed along with data acquisition and processing mechanisms applicable in two case studies: an urban garden and a target area for ecological restoration. The orientation of LiDAR was modified to make observations in the vertical plane and a motor was integrated for its rotation, enabling the acquisition of 360 degree data with high resolution. Motion and location sensors were also integrated for automatic error correction and georeferencing. From the data generated, histograms of point density variation along the vegetation height were created, where shrub stratum was easily distinguishable from tree stratum, and maximum tree height and shrub cover were calculated. These results agreed with the field data, whereby the developed TLS has proved to be effective in calculating metrics of structural complexity of vegetation.|\u73b0\u573a\u751f\u6001\u6570\u636e\u7684\u6536\u96c6\u5bf9\u4e8e\u4ee5\u53ef\u6301\u7eed\u7684\u65b9\u5f0f\u8bca\u65ad\u3001\u76d1\u6d4b\u548c\u7ba1\u7406\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u901a\u8fc7\u4f20\u7edf\u65b9\u6cd5\u83b7\u53d6\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u975e\u5e38\u8017\u65f6\uff0c\u5e76\u4e14\u7531\u4e8e\u80fd\u591f\u5728\u77ed\u65f6\u95f4\u5185\u8bb0\u5f55\u5927\u91cf\u6570\u636e\uff0c\u56e0\u6b64\u6570\u636e\u91c7\u96c6\u7684\u81ea\u52a8\u5316\u5448\u73b0\u51fa\u65e5\u76ca\u589e\u957f\u7684\u8d8b\u52bf\u3002\u5730\u9762\u6fc0\u5149\u626b\u63cf\u4eea (TLS)\uff0c\u7279\u522b\u662f LiDAR \u4f20\u611f\u5668\uff0c\u5df2\u5e94\u7528\u4e8e\u751f\u6001\u5b66\u4e2d\uff0c\u53ef\u4ee5\u91cd\u5efa\u690d\u88ab\u7684 3D \u7ed3\u6784\uff0c\u4ece\u800c\u6839\u636e\u70b9\u5bc6\u5ea6\u7684\u7a7a\u95f4\u53d8\u5316\u63a8\u65ad\u751f\u6001\u7cfb\u7edf\u7279\u5f81\u3002\u7136\u800c\uff0c\u6bcf\u675f\u83b7\u5f97\u7684\u4fe1\u606f\u91cf\u4f4e\u3001\u7f3a\u4e4f\u6570\u636e\u5206\u6790\u5de5\u5177\u4ee5\u53ca\u8bbe\u5907\u6210\u672c\u9ad8\u9650\u5236\u4e86\u5b83\u4eec\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5f00\u53d1\u4e86\u4f4e\u6210\u672c TLS\uff08<10k \u7f8e\u5143\uff09\u4ee5\u53ca\u9002\u7528\u4e8e\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u7684\u6570\u636e\u91c7\u96c6\u548c\u5904\u7406\u673a\u5236\uff1a\u57ce\u5e02\u82b1\u56ed\u548c\u751f\u6001\u6062\u590d\u76ee\u6807\u533a\u57df\u3002\u4fee\u6539\u6fc0\u5149\u96f7\u8fbe\u7684\u65b9\u5411\u4ee5\u5728\u5782\u76f4\u5e73\u9762\u5185\u8fdb\u884c\u89c2\u6d4b\uff0c\u5e76\u96c6\u6210\u7535\u673a\u7528\u4e8e\u5176\u65cb\u8f6c\uff0c\u4ece\u800c\u80fd\u591f\u91c7\u96c6\u9ad8\u5206\u8fa8\u7387\u7684 360 \u5ea6\u6570\u636e\u3002\u8fd8\u96c6\u6210\u4e86\u8fd0\u52a8\u548c\u4f4d\u7f6e\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u7ea0\u9519\u548c\u5730\u7406\u914d\u51c6\u3002\u6839\u636e\u751f\u6210\u7684\u6570\u636e\uff0c\u521b\u5efa\u4e86\u6cbf\u690d\u88ab\u9ad8\u5ea6\u7684\u70b9\u5bc6\u5ea6\u53d8\u5316\u76f4\u65b9\u56fe\uff0c\u5176\u4e2d\u704c\u6728\u5c42\u4e0e\u4e54\u6728\u5c42\u5f88\u5bb9\u6613\u533a\u5206\uff0c\u5e76\u8ba1\u7b97\u4e86\u6700\u5927\u6811\u6728\u9ad8\u5ea6\u548c\u704c\u6728\u8986\u76d6\u5ea6\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e0e\u73b0\u573a\u6570\u636e\u4e00\u81f4\uff0c\u8bc1\u660e\u6240\u5f00\u53d1\u7684 TLS \u5728\u8ba1\u7b97\u690d\u88ab\u7ed3\u6784\u590d\u6742\u6027\u6307\u6807\u65b9\u9762\u662f\u6709\u6548\u7684\u3002|[2401.05891v1](http://arxiv.org/pdf/2401.05891v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.06129": "|**2024-01-11**|**Distilling Vision-Language Models on Millions of Videos**|\u4ece\u6570\u767e\u4e07\u4e2a\u89c6\u9891\u4e2d\u63d0\u53d6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, et.al.|The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%.|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f52\u529f\u4e8e\u4e30\u5bcc\u7684\u56fe\u50cf\u6587\u672c\u6570\u636e\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5728\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u590d\u5236\u8fd9\u4e00\u6210\u529f\uff0c\u4f46\u6839\u672c\u6ca1\u6709\u8db3\u591f\u7684\u4eba\u5de5\u7ba1\u7406\u89c6\u9891\u6587\u672c\u6570\u636e\u53ef\u7528\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4f9d\u9760\u5f3a\u5927\u7684\u56fe\u50cf\u8bed\u8a00\u57fa\u7ebf\u548c\u5408\u6210\u7684\u6559\u5b66\u6570\u636e\u6765\u5fae\u8c03\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u3002\u7136\u540e\u4f7f\u7528\u751f\u6210\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6807\u8bb0\u6570\u767e\u4e07\u4e2a\u89c6\u9891\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6539\u7f16\u540e\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u89c6\u9891\u8bed\u8a00\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\u3002\u4f8b\u5982\uff0c\u5b83\u6bd4\u5f00\u653e\u5f0f NExT-QA \u7684\u6700\u4f73\u5148\u524d\u7ed3\u679c\u9ad8\u51fa 2.8%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e3a\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u89c6\u9891\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8fd9\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u66f4\u597d\u7684\u6587\u672c\u76d1\u7763\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8fd9\u4e9b\u81ea\u52a8\u751f\u6210\u7684\u5b57\u5e55\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u8bad\u7ec3\u7684\u89c6\u9891\u8bed\u8a00\u53cc\u7f16\u7801\u5668\u6a21\u578b\u6bd4\u540c\u6837\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u5f3a\u57fa\u7ebf\u8981\u597d 3.8%\u3002\u6211\u4eec\u6700\u597d\u7684\u6a21\u578b\u5728 MSR-VTT \u96f6\u6837\u672c\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u9762\u7684\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9ad8\u51fa 6%\u3002|[2401.06129v1](http://arxiv.org/pdf/2401.06129v1)|null|\n", "2401.05901": "|**2024-01-11**|**ConKeD: Multiview contrastive descriptor learning for keypoint-based retinal image registration**|ConKeD\uff1a\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u7684\u591a\u89c6\u56fe\u5bf9\u6bd4\u63cf\u8ff0\u7b26\u5b66\u4e60|David Rivas-Villar, \u00c1lvaro S. Hervella, Jos\u00e9 Rouco, Jorge Novo|Retinal image registration is of utmost importance due to its wide applications in medical practice. In this context, we propose ConKeD, a novel deep learning approach to learn descriptors for retinal image registration. In contrast to current registration methods, our approach employs a novel multi-positive multi-negative contrastive learning strategy that enables the utilization of additional information from the available training samples. This makes it possible to learn high quality descriptors from limited training data. To train and evaluate ConKeD, we combine these descriptors with domain-specific keypoints, particularly blood vessel bifurcations and crossovers, that are detected using a deep neural network. Our experimental results demonstrate the benefits of the novel multi-positive multi-negative strategy, as it outperforms the widely used triplet loss technique (single-positive and single-negative) as well as the single-positive multi-negative alternative. Additionally, the combination of ConKeD with the domain-specific keypoints produces comparable results to the state-of-the-art methods for retinal image registration, while offering important advantages such as avoiding pre-processing, utilizing fewer training samples, and requiring fewer detected keypoints, among others. Therefore, ConKeD shows a promising potential towards facilitating the development and application of deep learning-based methods for retinal image registration.|\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u7531\u4e8e\u5176\u5728\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ConKeD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u7684\u63cf\u8ff0\u7b26\u3002\u4e0e\u5f53\u524d\u7684\u914d\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6b63\u591a\u8d1f\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u53ef\u4ee5\u5229\u7528\u53ef\u7528\u8bad\u7ec3\u6837\u672c\u4e2d\u7684\u9644\u52a0\u4fe1\u606f\u3002\u8fd9\u4f7f\u5f97\u4ece\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u63cf\u8ff0\u7b26\u6210\u4e3a\u53ef\u80fd\u3002\u4e3a\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30 ConKeD\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u7b26\u4e0e\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u5230\u7684\u7279\u5b9a\u9886\u57df\u5173\u952e\u70b9\u7ed3\u5408\u8d77\u6765\uff0c\u7279\u522b\u662f\u8840\u7ba1\u5206\u53c9\u548c\u4ea4\u53c9\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u65b0\u578b\u591a\u6b63\u591a\u8d1f\u7b56\u7565\u7684\u597d\u5904\uff0c\u56e0\u4e3a\u5b83\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e09\u91cd\u6001\u635f\u5931\u6280\u672f\uff08\u5355\u6b63\u548c\u5355\u8d1f\uff09\u4ee5\u53ca\u5355\u6b63\u591a\u8d1f\u66ff\u4ee3\u65b9\u6848\u3002\u6b64\u5916\uff0cConKeD \u4e0e\u7279\u5b9a\u9886\u57df\u5173\u952e\u70b9\u7684\u7ed3\u5408\u4ea7\u751f\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u4f18\u52bf\uff0c\u4f8b\u5982\u907f\u514d\u9884\u5904\u7406\u3001\u4f7f\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u4ee5\u53ca\u9700\u8981\u66f4\u5c11\u7684\u68c0\u6d4b\u5173\u952e\u70b9\u7b49\u3002\u56e0\u6b64\uff0cConKeD \u5728\u4fc3\u8fdb\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u5e94\u7528\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002|[2401.05901v1](http://arxiv.org/pdf/2401.05901v1)|null|\n", "2401.05730": "|**2024-01-11**|**Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing**|\u901a\u8fc7\u6709\u6548\u7684\u7ec4\u5408\u6b63\u914d\u5bf9\u589e\u5f3a\u5bf9\u6bd4\u5b66\u4e60|Jaeill Kim, Duhun Hwang, Eunjung Lee, Jangwon Suh, Jimyeong Kim, Wonjong Rhee|In the past few years, contrastive learning has played a central role for the success of visual unsupervised representation learning. Around the same time, high-performance non-contrastive learning methods have been developed as well. While most of the works utilize only two views, we carefully review the existing multi-view methods and propose a general multi-view strategy that can improve learning speed and performance of any contrastive or non-contrastive method. We first analyze CMC's full-graph paradigm and empirically show that the learning speed of $K$-views can be increased by $_{K}\\mathrm{C}_{2}$ times for small learning rate and early training. Then, we upgrade CMC's full-graph by mixing views created by a crop-only augmentation, adopting small-size views as in SwAV multi-crop, and modifying the negative sampling. The resulting multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness of ECPP by applying it to SimCLR and assessing the linear evaluation performance for CIFAR-10 and ImageNet-100. For each benchmark, we achieve a state-of-the-art performance. In case of ImageNet-100, ECPP boosted SimCLR outperforms supervised learning.|\u5728\u8fc7\u53bb\u7684\u51e0\u5e74\u4e2d\uff0c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u4e8e\u89c6\u89c9\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u6210\u529f\u53d1\u6325\u4e86\u6838\u5fc3\u4f5c\u7528\u3002\u5927\u7ea6\u5728\u540c\u4e00\u65f6\u95f4\uff0c\u9ad8\u6027\u80fd\u7684\u975e\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4e5f\u88ab\u5f00\u53d1\u51fa\u6765\u3002\u867d\u7136\u5927\u591a\u6570\u4f5c\u54c1\u4ec5\u4f7f\u7528\u4e24\u4e2a\u89c6\u56fe\uff0c\u4f46\u6211\u4eec\u4ed4\u7ec6\u56de\u987e\u4e86\u73b0\u6709\u7684\u591a\u89c6\u56fe\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u591a\u89c6\u56fe\u7b56\u7565\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4efb\u4f55\u5bf9\u6bd4\u6216\u975e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u5b66\u4e60\u901f\u5ea6\u548c\u6027\u80fd\u3002\u6211\u4eec\u9996\u5148\u5206\u6790\u4e86 CMC \u7684\u5168\u56fe\u8303\u5f0f\uff0c\u5e76\u51ed\u7ecf\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u5c0f\u5b66\u4e60\u7387\u548c\u65e9\u671f\u8bad\u7ec3\uff0c$K$-views \u7684\u5b66\u4e60\u901f\u5ea6\u53ef\u4ee5\u63d0\u9ad8 $_{K}\\mathrm{C}_{2}$ \u500d\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u6df7\u5408\u4ec5\u88c1\u526a\u589e\u5f3a\u521b\u5efa\u7684\u89c6\u56fe\u3001\u91c7\u7528 SwAV \u591a\u88c1\u526a\u4e2d\u7684\u5c0f\u5c3a\u5bf8\u89c6\u56fe\u4ee5\u53ca\u4fee\u6539\u8d1f\u91c7\u6837\u6765\u5347\u7ea7 CMC \u7684\u5168\u56fe\u3002\u7531\u6b64\u4ea7\u751f\u7684\u591a\u89c6\u56fe\u7b56\u7565\u79f0\u4e3a ECPP\uff08\u9ad8\u6548\u7ec4\u5408\u6b63\u914d\u5bf9\uff09\u3002\u6211\u4eec\u901a\u8fc7\u5c06 ECPP \u5e94\u7528\u4e8e SimCLR \u5e76\u8bc4\u4f30 CIFAR-10 \u548c ImageNet-100 \u7684\u7ebf\u6027\u8bc4\u4f30\u6027\u80fd\u6765\u7814\u7a76 ECPP \u7684\u6709\u6548\u6027\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5c31 ImageNet-100 \u800c\u8a00\uff0cECPP \u63d0\u5347\u7684 SimCLR \u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u3002|[2401.05730v1](http://arxiv.org/pdf/2401.05730v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.06122": "|**2024-01-11**|**Manipulating Feature Visualizations with Gradient Slingshots**|\u4f7f\u7528\u6e10\u53d8\u5f39\u5f13\u64cd\u4f5c\u7279\u5f81\u53ef\u89c6\u5316|Dilyara Bareeva, Marina M. -C. H\u00f6hne, Alexander Warnecke, Lukas Pirch, Klaus-Robert M\u00fcller, Konrad Rieck, Kirill Bykov|Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which substantiates our findings.|\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u80fd\u591f\u5b66\u4e60\u590d\u6742\u4e14\u901a\u7528\u7684\u8868\u793a\uff0c\u7136\u800c\uff0c\u6240\u5b66\u4e60\u6982\u5ff5\u7684\u8bed\u4e49\u672c\u8d28\u4ecd\u7136\u672a\u77e5\u3002\u7528\u4e8e\u89e3\u91ca DNN \u5b66\u4e60\u6982\u5ff5\u7684\u5e38\u7528\u65b9\u6cd5\u662f\u6fc0\u6d3b\u6700\u5927\u5316 (AM)\uff0c\u5b83\u751f\u6210\u4e00\u4e2a\u5408\u6210\u8f93\u5165\u4fe1\u53f7\uff0c\u6700\u5927\u9650\u5ea6\u5730\u6fc0\u6d3b\u7f51\u7edc\u4e2d\u7684\u7279\u5b9a\u795e\u7ecf\u5143\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u5bf9\u6297\u6027\u6a21\u578b\u64cd\u4f5c\u7684\u8106\u5f31\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u663e\u7740\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u64cd\u4f5c\u7279\u5f81\u53ef\u89c6\u5316\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5728\u6a21\u578b\u5ba1\u6838\u671f\u95f4\u7528\u9009\u5b9a\u7684\u76ee\u6807\u89e3\u91ca\u63a9\u76d6\u795e\u7ecf\u5143\u7684\u539f\u59cb\u89e3\u91ca\u6765\u5c55\u793a\u5176\u9690\u85cf\u7279\u5b9a\u795e\u7ecf\u5143\u529f\u80fd\u7684\u80fd\u529b\u3002\u4f5c\u4e3a\u8865\u6551\u63aa\u65bd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9488\u5bf9\u6b64\u7c7b\u64cd\u7eb5\u7684\u4fdd\u62a4\u63aa\u65bd\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc1\u5b9e\u6211\u4eec\u53d1\u73b0\u7684\u5b9a\u91cf\u8bc1\u636e\u3002|[2401.06122v1](http://arxiv.org/pdf/2401.06122v1)|null|\n", "2401.05994": "|**2024-01-11**|**MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring**|MGARD\uff1a\u7528\u4e8e\u9ad8\u6027\u80fd\u3001\u9519\u8bef\u63a7\u5236\u6570\u636e\u538b\u7f29\u548c\u91cd\u6784\u7684\u591a\u91cd\u7f51\u683c\u6846\u67b6|Qian Gong, Jieyang Chen, Ben Whitney, Xin Liang, Viktor Reshniak, Tania Banerjee, Jaemoon Lee, Anand Rangarajan, Lipeng Wan, Nicolas Vidal, et.al.|We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids. With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis. It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures. MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations.|\u6211\u4eec\u63cf\u8ff0\u4e86 MGARD\uff0c\u8fd9\u662f\u4e00\u6b3e\u4e3a\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u7684\u6d6e\u70b9\u79d1\u5b66\u6570\u636e\u63d0\u4f9b MultiGrid Adaptive Reduction \u7684\u8f6f\u4ef6\u3002\u51ed\u501f\u5353\u8d8a\u7684\u6570\u636e\u538b\u7f29\u80fd\u529b\u548c\u7cbe\u786e\u7684\u9519\u8bef\u63a7\u5236\uff0cMGARD \u6ee1\u8db3\u4e86\u5e7f\u6cdb\u7684\u9700\u6c42\uff0c\u5305\u62ec\u5b58\u50a8\u51cf\u5c11\u3001\u9ad8\u6027\u80fd I/O \u548c\u73b0\u573a\u6570\u636e\u5206\u6790\u3002\u5b83\u5177\u6709\u7edf\u4e00\u7684\u5e94\u7528\u7a0b\u5e8f\u7f16\u7a0b\u63a5\u53e3 (API)\uff0c\u53ef\u4ee5\u8de8\u4e0d\u540c\u7684\u8ba1\u7b97\u67b6\u6784\u65e0\u7f1d\u8fd0\u884c\u3002 MGARD \u7ecf\u8fc7\u9ad8\u5ea6\u8c03\u4f18\u7684 GPU \u5185\u6838\u4ee5\u53ca\u9ad8\u6548\u7684\u5185\u5b58\u548c\u8bbe\u5907\u7ba1\u7406\u673a\u5236\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u786e\u4fdd\u4e86\u53ef\u6269\u5c55\u548c\u5feb\u901f\u7684\u64cd\u4f5c\u3002|[2401.05994v1](http://arxiv.org/pdf/2401.05994v1)|null|\n", "2401.05879": "|**2024-01-11**|**YOIO: You Only Iterate Once by mining and fusing multiple necessary global information in the optical flow estimation**|YOIO\uff1a\u901a\u8fc7\u5728\u5149\u6d41\u4f30\u8ba1\u4e2d\u6316\u6398\u548c\u878d\u5408\u591a\u4e2a\u5fc5\u8981\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u60a8\u53ea\u9700\u8fed\u4ee3\u4e00\u6b21|Yu Jing, Tan Yujuan, Ren Ao, Liu Duo|Occlusions pose a significant challenge to optical flow algorithms that even rely on global evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work only used the current frame as the only input, which could not guarantee providing correct global reference information for occluded points, and had problems such as long calculation time and poor accuracy in predicting optical flow at occluded points. To enable both high accuracy and efficiency, We fully mine and utilize the spatiotemporal information provided by the frame pair, design a loopback judgment algorithm to ensure that correct global reference information is obtained, mine multiple necessary global information, and design an efficient refinement module that fuses these global information. Specifically, we propose a YOIO framework, which consists of three main components: an initial flow estimator, a multiple global information extraction module, and a unified refinement module. We demonstrate that optical flow estimates in the occluded regions can be significantly improved in only one iteration without damaging the performance in non-occluded regions. Compared with GMA, the optical flow prediction accuracy of this method in the occluded area is improved by more than 10%, and the occ_out area exceeds 15%, while the calculation time is 27% shorter. This approach, running up to 18.9fps with 436*1024 image resolution, obtains new state-of-the-art results on the challenging Sintel dataset among all published and unpublished approaches that can run in real-time, suggesting a new paradigm for accurate and efficient optical flow estimation.|\u906e\u6321\u5bf9\u751a\u81f3\u4f9d\u8d56\u5168\u5c40\u8bc1\u636e\u7684\u5149\u6d41\u7b97\u6cd5\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u8ba4\u4e3a\u906e\u6321\u70b9\u662f\u5728\u53c2\u8003\u5e27\u4e2d\u6210\u50cf\u4f46\u4e0d\u5728\u4e0b\u4e00\u5e27\u4e2d\u6210\u50cf\u7684\u70b9\u3002\u4f30\u8ba1\u8fd9\u4e9b\u70b9\u7684\u8fd0\u52a8\u975e\u5e38\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4e24\u5e27\u8bbe\u7f6e\u4e2d\u3002\u4ee5\u5f80\u7684\u5de5\u4f5c\u4ec5\u4ee5\u5f53\u524d\u5e27\u4f5c\u4e3a\u552f\u4e00\u8f93\u5165\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u4e3a\u906e\u6321\u70b9\u63d0\u4f9b\u6b63\u786e\u7684\u5168\u5c40\u53c2\u8003\u4fe1\u606f\uff0c\u5e76\u4e14\u5b58\u5728\u8ba1\u7b97\u65f6\u95f4\u957f\u3001\u906e\u6321\u70b9\u5149\u6d41\u9884\u6d4b\u7cbe\u5ea6\u5dee\u7b49\u95ee\u9898\u3002\u4e3a\u4e86\u517c\u987e\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u5145\u5206\u6316\u6398\u548c\u5229\u7528\u5e27\u5bf9\u63d0\u4f9b\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u8bbe\u8ba1\u73af\u56de\u5224\u65ad\u7b97\u6cd5\u4ee5\u786e\u4fdd\u83b7\u5f97\u6b63\u786e\u7684\u5168\u5c40\u53c2\u8003\u4fe1\u606f\uff0c\u6316\u6398\u591a\u4e2a\u5fc5\u8981\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7684\u7ec6\u5316\u6a21\u5757\u878d\u5408\u8fd9\u4e9b\u5168\u5c40\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2aYOIO\u6846\u67b6\uff0c\u5b83\u7531\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u7ec4\u6210\uff1a\u521d\u59cb\u6d41\u91cf\u4f30\u8ba1\u5668\u3001\u591a\u4e2a\u5168\u5c40\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\u548c\u7edf\u4e00\u7ec6\u5316\u6a21\u5757\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u906e\u6321\u533a\u57df\u7684\u5149\u6d41\u4f30\u8ba1\u53ea\u9700\u4e00\u6b21\u8fed\u4ee3\u5373\u53ef\u663e\u7740\u6539\u5584\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u975e\u906e\u6321\u533a\u57df\u7684\u6027\u80fd\u3002\u4e0eGMA\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u906e\u6321\u533a\u57df\u7684\u5149\u6d41\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\uff0cocc_out\u533a\u57df\u8d85\u8fc715%\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u7f29\u77ed\u4e8627%\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u8fd0\u884c\u901f\u5ea6\u9ad8\u8fbe 18.9fps\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u4e3a 436*1024\uff0c\u5728\u6240\u6709\u5df2\u53d1\u8868\u548c\u672a\u53d1\u8868\u7684\u53ef\u5b9e\u65f6\u8fd0\u884c\u7684\u65b9\u6cd5\u4e2d\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684 Sintel \u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u6700\u65b0\u7684\u7ed3\u679c\uff0c\u8fd9\u4e3a\u7cbe\u786e\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u3002\u548c\u9ad8\u6548\u7684\u5149\u6d41\u4f30\u8ba1\u3002|[2401.05879v1](http://arxiv.org/pdf/2401.05879v1)|null|\n", "2401.05807": "|**2024-01-11**|**On the representation and methodology for wide and short range head pose estimation**|\u5bbd\u77ed\u7a0b\u5934\u90e8\u59ff\u6001\u4f30\u8ba1\u7684\u8868\u793a\u548c\u65b9\u6cd5|Alejandro Cobo, Roberto Valle, Jos\u00e9 M. Buenaposada, Luis Baumela|Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360{\\deg} rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles' gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP|Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set.|\u5934\u90e8\u59ff\u52bf\u4f30\u8ba1\uff08HPE\uff09\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u4e2a\u4ee4\u4eba\u611f\u5174\u8da3\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u534a\u6b63\u9762\u6216\u4fa7\u9762\u8bbe\u7f6e\u4e2d\u7684\u9762\u90e8\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6700\u8fd1\u7684\u5e94\u7528\u9700\u8981\u5728\u6574\u4e2a 360{\\deg} \u65cb\u8f6c\u8303\u56f4\u5185\u5206\u6790\u4eba\u8138\u3002\u89e3\u51b3\u534a\u6b63\u9762\u548c\u4fa7\u9762\u60c5\u51b5\u7684\u4f20\u7edf\u65b9\u6cd5\u5e76\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u5168\u65cb\u8f6c\u60c5\u51b5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u77ed\u671f\u548c\u5e7f\u6cdb HPE \u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u54ea\u4e9b\u8868\u793a\u548c\u6307\u6807\u9002\u5408\u6bcf\u79cd\u60c5\u51b5\u3002\u6211\u4eec\u8868\u660e\uff0c\u6d41\u884c\u7684\u6b27\u62c9\u89d2\u8868\u793a\u5bf9\u4e8e\u77ed\u7a0b HPE \u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u9009\u62e9\uff0c\u4f46\u4e0d\u9002\u7528\u4e8e\u6781\u7aef\u65cb\u8f6c\u3002\u7136\u800c\uff0c\u6b27\u62c9\u89d2\u7684\u4e07\u5411\u8282\u9501\u5b9a\u95ee\u9898\u963b\u6b62\u5b83\u4eec\u5728\u4efb\u4f55\u8bbe\u7f6e\u4e2d\u7528\u4f5c\u6709\u6548\u7684\u5ea6\u91cf\u3002\u6211\u4eec\u8fd8\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f53\u524d\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6ce8\u610f\u5230\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u53c2\u8003\u7cfb\u7edf\u4e4b\u95f4\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5bf9\u6587\u732e\u4e2d\u6240\u6709\u6587\u7ae0\u7684\u7ed3\u679c\u4ea7\u751f\u8d1f\u9762\u504f\u5dee\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u91cf\u5316\u8fd9\u79cd\u504f\u5dee\u7684\u7a0b\u5e8f\uff0c\u4ee5\u53ca\u4e00\u79cd\u7528\u4e8e\u8de8\u6570\u636e\u96c6 HPE \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e3a 300W-LP|Biwi \u57fa\u51c6\u5efa\u7acb\u4e86\u65b0\u7684\u3001\u66f4\u51c6\u786e\u7684 SOTA\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u6d4b\u5730\u89d2\u8ddd\u79bb\u5ea6\u91cf\u7684\u63a8\u5e7f\uff0c\u8be5\u5ea6\u91cf\u80fd\u591f\u6784\u5efa\u63a7\u5236\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u5bf9\u6a21\u578b\u4f18\u5316\u7684\u8d21\u732e\u7684\u635f\u5931\u3002\u6700\u540e\uff0c\u6211\u4eec\u4ecb\u7ecd\u57fa\u4e8e CMU Panoptic \u6570\u636e\u96c6\u7684\u5e7f\u6cdb HPE \u57fa\u51c6\u6d4b\u8bd5\u3002|[2401.05807v1](http://arxiv.org/pdf/2401.05807v1)|null|\n", "2401.05806": "|**2024-01-11**|**CLIP-Driven Semantic Discovery Network for Visible-Infrared Person Re-Identification**|CLIP \u9a71\u52a8\u7684\u8bed\u4e49\u53d1\u73b0\u7f51\u7edc\uff0c\u7528\u4e8e\u53ef\u89c1\u7ea2\u5916\u4eba\u5458\u91cd\u65b0\u8bc6\u522b|Xiaoyan Yu, Neng Dong, Liehuang Zhu, Hao Peng, Dapeng Tao|Visible-infrared person re-identification (VIReID) primarily deals with matching identities across person images from different modalities. Due to the modality gap between visible and infrared images, cross-modality identity matching poses significant challenges. Recognizing that high-level semantics of pedestrian appearance, such as gender, shape, and clothing style, remain consistent across modalities, this paper intends to bridge the modality gap by infusing visual features with high-level semantics. Given the capability of CLIP to sense high-level semantic information corresponding to visual representations, we explore the application of CLIP within the domain of VIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network (CSDN) that consists of Modality-specific Prompt Learner, Semantic Information Integration (SII), and High-level Semantic Embedding (HSE). Specifically, considering the diversity stemming from modality discrepancies in language descriptions, we devise bimodal learnable text tokens to capture modality-private semantic information for visible and infrared images, respectively. Additionally, acknowledging the complementary nature of semantic details across different modalities, we integrate text features from the bimodal language descriptions to achieve comprehensive semantics. Finally, we establish a connection between the integrated text features and the visual features across modalities. This process embed rich high-level semantic information into visual representations, thereby promoting the modality invariance of visual representations. The effectiveness and superiority of our proposed CSDN over existing methods have been substantiated through experimental evaluations on multiple widely used benchmarks. The code will be released at \\url{https://github.com/nengdong96/CSDN}.|\u53ef\u89c1\u7ea2\u5916\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\uff08VIReID\uff09\u4e3b\u8981\u5904\u7406\u6765\u81ea\u4e0d\u540c\u6a21\u5f0f\u7684\u4eba\u5458\u56fe\u50cf\u4e4b\u95f4\u7684\u8eab\u4efd\u5339\u914d\u3002\u7531\u4e8e\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u8de8\u6a21\u6001\u8eab\u4efd\u5339\u914d\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u8ba4\u8bc6\u5230\u884c\u4eba\u5916\u89c2\u7684\u9ad8\u7ea7\u8bed\u4e49\uff08\u4f8b\u5982\u6027\u522b\u3001\u5f62\u72b6\u548c\u670d\u88c5\u98ce\u683c\uff09\u5728\u4e0d\u540c\u6a21\u6001\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u6253\u7b97\u901a\u8fc7\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u76f8\u7ed3\u5408\u6765\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\u3002\u9274\u4e8e CLIP \u80fd\u591f\u611f\u77e5\u4e0e\u89c6\u89c9\u8868\u793a\u76f8\u5bf9\u5e94\u7684\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0c\u6211\u4eec\u63a2\u7d22\u4e86 CLIP \u5728 VIReID \u9886\u57df\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd CLIP \u9a71\u52a8\u7684\u8bed\u4e49\u53d1\u73b0\u7f51\u7edc\uff08CSDN\uff09\uff0c\u7531\u7279\u5b9a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u5668\u3001\u8bed\u4e49\u4fe1\u606f\u96c6\u6210\uff08SII\uff09\u548c\u9ad8\u7ea7\u8bed\u4e49\u5d4c\u5165\uff08HSE\uff09\u7ec4\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8003\u8651\u5230\u8bed\u8a00\u63cf\u8ff0\u4e2d\u6a21\u6001\u5dee\u5f02\u5e26\u6765\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u53cc\u6a21\u6001\u53ef\u5b66\u4e60\u6587\u672c\u6807\u8bb0\u6765\u5206\u522b\u6355\u83b7\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u7684\u6a21\u6001\u79c1\u6709\u8bed\u4e49\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8ba4\u8bc6\u5230\u4e0d\u540c\u6a21\u6001\u8bed\u4e49\u7ec6\u8282\u7684\u4e92\u8865\u6027\uff0c\u6211\u4eec\u6574\u5408\u53cc\u6a21\u6001\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u6587\u672c\u7279\u5f81\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u8bed\u4e49\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u96c6\u6210\u7684\u6587\u672c\u7279\u5f81\u548c\u8de8\u6a21\u6001\u7684\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u5c06\u4e30\u5bcc\u7684\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u5d4c\u5165\u5230\u89c6\u89c9\u8868\u793a\u4e2d\uff0c\u4ece\u800c\u4fc3\u8fdb\u89c6\u89c9\u8868\u793a\u7684\u6a21\u6001\u4e0d\u53d8\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684 CSDN \u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u5df2\u901a\u8fc7\u5bf9\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u7684\u5b9e\u9a8c\u8bc4\u4f30\u5f97\u5230\u8bc1\u5b9e\u3002\u4ee3\u7801\u5c06\u53d1\u5e03\u5728\\url{https://github.com/nengdong96/CSDN}\u3002|[2401.05806v1](http://arxiv.org/pdf/2401.05806v1)|null|\n", "2401.05772": "|**2024-01-11**|**Knowledge Translation: A New Pathway for Model Compression**|\u77e5\u8bc6\u7ffb\u8bd1\uff1a\u6a21\u578b\u538b\u7f29\u7684\u65b0\u9014\u5f84|Wujie Sun, Defang Chen, Jiawei Chen, Yan Feng, Chun Chen, Can Wang|Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategies to enhance model performance despite restricted training data, and successfully demonstrate the feasibility of KT on the MNIST dataset. Code is available at \\url{https://github.com/zju-SWJ/KT}.|\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u6b65\uff0c\u4f46\u4ee3\u4ef7\u662f\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u6a21\u578b\u5b58\u50a8\u5f00\u9500\u7684\u589e\u52a0\u3002\u867d\u7136\u73b0\u6709\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u52aa\u529b\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u7684\u6570\u91cf\uff0c\u4f46\u5b83\u4eec\u4e0d\u53ef\u907f\u514d\u5730\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u538b\u7f29\u6a21\u578b\u6216\u65bd\u52a0\u67b6\u6784\u7ea6\u675f\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u79f0\u4e3a \\textbf{K}nowledge \\textbf{T}translation (KT)\uff0c\u5176\u4e2d\u8bad\u7ec3\u201c\u7ffb\u8bd1\u201d\u6a21\u578b\u4ee5\u63a5\u6536\u66f4\u5927\u6a21\u578b\u7684\u53c2\u6570\u5e76\u751f\u6210\u538b\u7f29\u7684\u53c2\u6570\u3002 KT\u7684\u6982\u5ff5\u53d7\u5230\u8bed\u8a00\u7ffb\u8bd1\u7684\u542f\u53d1\uff0c\u5b83\u6709\u6548\u5730\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u8f6c\u6362\u4e0d\u540c\u7684\u8bed\u8a00\uff0c\u5e76\u4fdd\u6301\u76f8\u540c\u7684\u542b\u4e49\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u8f6c\u6362\u4e0d\u540c\u5927\u5c0f\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u529f\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684 KT \u6846\u67b6\uff0c\u5f15\u5165\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u5e76\u6210\u529f\u8bc1\u660e\u4e86 KT \u5728 MNIST \u6570\u636e\u96c6\u4e0a\u7684\u53ef\u884c\u6027\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/zju-SWJ/KT} \u83b7\u53d6\u3002|[2401.05772v1](http://arxiv.org/pdf/2401.05772v1)|null|\n", "2401.05752": "|**2024-01-11**|**Learning Generalizable Models via Disentangling Spurious and Enhancing Potential Correlations**|\u901a\u8fc7\u6d88\u9664\u865a\u5047\u548c\u589e\u5f3a\u6f5c\u5728\u76f8\u5173\u6027\u6765\u5b66\u4e60\u53ef\u63a8\u5e7f\u6a21\u578b|Na Wang, Lei Qi, Jintao Guo, Yinghuan Shi, Yang Gao|Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain. The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model. Adopting multiple perspectives, such as the sample and the feature, proves to be effective. The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features. In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations. 1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations. 2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model. The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG.|\u57df\u6cdb\u5316\uff08DG\uff09\u65e8\u5728\u5728\u591a\u4e2a\u6e90\u57df\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4efb\u610f\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u3002\u57df\u4e0d\u53d8\u8868\u793a\u7684\u83b7\u53d6\u5bf9\u4e8e DG \u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u6355\u83b7\u6570\u636e\u7684\u56fa\u6709\u8bed\u4e49\u4fe1\u606f\uff0c\u51cf\u8f7b\u57df\u8f6c\u79fb\u7684\u5f71\u54cd\u5e76\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u91c7\u7528\u6837\u672c\u548c\u7279\u5f81\u7b49\u591a\u4e2a\u89c6\u89d2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002\u6837\u672c\u89c6\u89d2\u901a\u8fc7\u6570\u636e\u64cd\u4f5c\u6280\u672f\u4fc3\u8fdb\u6570\u636e\u589e\u5f3a\uff0c\u800c\u7279\u5f81\u89c6\u89d2\u5219\u80fd\u591f\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6cdb\u5316\u7279\u5f81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u81f4\u529b\u4e8e\u901a\u8fc7\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u5e76\u589e\u5f3a\u6f5c\u5728\u76f8\u5173\u6027\uff0c\u8feb\u4f7f\u6a21\u578b\u4ece\u6837\u672c\u548c\u7279\u5f81\u89d2\u5ea6\u83b7\u53d6\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002 1\uff09\u4ece\u6837\u672c\u89d2\u5ea6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u9891\u7387\u9650\u5236\u6a21\u5757\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u5bf9\u8c61\u7279\u5f81\u548c\u6807\u7b7e\u4e4b\u95f4\u7684\u76f8\u5173\u76f8\u5173\u6027\uff0c\u4ece\u800c\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u3002 2\uff09\u4ece\u7279\u5f81\u89d2\u5ea6\u6765\u770b\uff0c\u7b80\u5355\u7684\u5c3e\u90e8\u4ea4\u4e92\u6a21\u5757\u9690\u5f0f\u589e\u5f3a\u4e86\u6765\u81ea\u6240\u6709\u6e90\u57df\u7684\u6240\u6709\u6837\u672c\u4e4b\u95f4\u7684\u6f5c\u5728\u76f8\u5173\u6027\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u83b7\u53d6\u8de8\u591a\u4e2a\u57df\u7684\u57df\u4e0d\u53d8\u8868\u793a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5165\u8fd9\u4e24\u4e2a\u6a21\u5757\u7684\u5177\u6709\u5f3a\u5927\u57fa\u7ebf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6216\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u53ef\u4ee5\u53d6\u5f97\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u4f8b\u5982\u5728 Digits-DG \u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u5230 92.30\uff05\u3002|[2401.05752v1](http://arxiv.org/pdf/2401.05752v1)|null|\n", "2401.05686": "|**2024-01-11**|**Self Expanding Convolutional Neural Networks**|\u81ea\u6269\u5c55\u5377\u79ef\u795e\u7ecf\u7f51\u7edc|Blaise Appolinary, Alex Deaconu, Sophia Yang|In this paper, we present a novel method for dynamically expanding Convolutional Neural Networks (CNNs) during training, aimed at meeting the increasing demand for efficient and sustainable deep learning models. Our approach, drawing from the seminal work on Self-Expanding Neural Networks (SENN), employs a natural expansion score as an expansion criteria to address the common issue of over-parameterization in deep convolutional neural networks, thereby ensuring that the model's complexity is finely tuned to the task's specific needs. A significant benefit of this method is its eco-friendly nature, as it obviates the necessity of training multiple models of different sizes. We employ a strategy where a single model is dynamically expanded, facilitating the extraction of checkpoints at various complexity levels, effectively reducing computational resource use and energy consumption while also expediting the development cycle by offering diverse model complexities from a single training session. We evaluate our method on the CIFAR-10 dataset and our experimental results validate this approach, demonstrating that dynamically adding layers not only maintains but also improves CNN performance, underscoring the effectiveness of our expansion criteria. This approach marks a considerable advancement in developing adaptive, scalable, and environmentally considerate neural network architectures, addressing key challenges in the field of deep learning.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6269\u5c55\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u6ee1\u8db3\u5bf9\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u501f\u9274\u4e86\u81ea\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\uff08SENN\uff09\u7684\u5f00\u521b\u6027\u5de5\u4f5c\uff0c\u91c7\u7528\u81ea\u7136\u6269\u5c55\u5206\u6570\u4f5c\u4e3a\u6269\u5c55\u6807\u51c6\u6765\u89e3\u51b3\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u4ece\u800c\u786e\u4fdd\u6a21\u578b\u7684\u590d\u6742\u6027\u5f88\u597d\u6839\u636e\u4efb\u52a1\u7684\u5177\u4f53\u9700\u6c42\u8fdb\u884c\u8c03\u6574\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u4e00\u4e2a\u663e\u7740\u597d\u5904\u662f\u5b83\u7684\u73af\u4fdd\u6027\u8d28\uff0c\u56e0\u4e3a\u5b83\u6d88\u9664\u4e86\u8bad\u7ec3\u4e0d\u540c\u5927\u5c0f\u7684\u591a\u4e2a\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002\u6211\u4eec\u91c7\u7528\u52a8\u6001\u6269\u5c55\u5355\u4e2a\u6a21\u578b\u7684\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u63d0\u53d6\u5404\u79cd\u590d\u6742\u7a0b\u5ea6\u7684\u68c0\u67e5\u70b9\uff0c\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u548c\u80fd\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u8fd8\u901a\u8fc7\u4ece\u5355\u4e2a\u8bad\u7ec3\u4f1a\u8bdd\u4e2d\u63d0\u4f9b\u4e0d\u540c\u7684\u6a21\u578b\u590d\u6742\u6027\u6765\u52a0\u5feb\u5f00\u53d1\u5468\u671f\u3002\u6211\u4eec\u5728 CIFAR-10 \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\uff0c\u8bc1\u660e\u52a8\u6001\u6dfb\u52a0\u5c42\u4e0d\u4ec5\u53ef\u4ee5\u4fdd\u6301\u800c\u4e14\u8fd8\u53ef\u4ee5\u63d0\u9ad8 CNN \u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u6211\u4eec\u6269\u5c55\u6807\u51c6\u7684\u6709\u6548\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u6807\u5fd7\u7740\u5728\u5f00\u53d1\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u548c\u73af\u5883\u53cb\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u65b9\u9762\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002|[2401.05686v1](http://arxiv.org/pdf/2401.05686v1)|null|\n", "2401.05675": "|**2024-01-11**|**Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation**|Parrot\uff1a\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6|Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et.al.|Recent works demonstrate that using reinforcement learning (RL) with quality rewards can enhance the quality of generated images in text-to-image (T2I) generation. However, a simple aggregation of multiple rewards may cause over-optimization in certain metrics and degradation in others, and it is challenging to manually find the optimal weights. An effective strategy to jointly optimize multiple rewards in RL for T2I generation is highly desirable. This paper introduces Parrot, a novel multi-reward RL framework for T2I generation. Through the use of the batch-wise Pareto optimal selection, Parrot automatically identifies the optimal trade-off among different rewards during the RL optimization of the T2I generation. Additionally, Parrot employs a joint optimization approach for the T2I model and the prompt expansion network, facilitating the generation of quality-aware text prompts, thus further enhancing the final image quality. To counteract the potential catastrophic forgetting of the original user prompt due to prompt expansion, we introduce original prompt centered guidance at inference time, ensuring that the generated image remains faithful to the user input. Extensive experiments and a user study demonstrate that Parrot outperforms several baseline methods across various quality criteria, including aesthetics, human preference, image sentiment, and text-image alignment.|\u6700\u8fd1\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u4f7f\u7528\u5177\u6709\u8d28\u91cf\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u4e2d\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u7136\u800c\uff0c\u591a\u4e2a\u5956\u52b1\u7684\u7b80\u5355\u805a\u5408\u53ef\u80fd\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u6307\u6807\u7684\u8fc7\u5ea6\u4f18\u5316\u548c\u5176\u4ed6\u6307\u6807\u7684\u9000\u5316\uff0c\u5e76\u4e14\u624b\u52a8\u627e\u5230\u6700\u4f73\u6743\u91cd\u5177\u6709\u6311\u6218\u6027\u3002\u975e\u5e38\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u6765\u8054\u5408\u4f18\u5316 RL \u4e2d\u7684\u591a\u79cd\u5956\u52b1\u4ee5\u751f\u6210 T2I\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Parrot\uff0c\u4e00\u79cd\u7528\u4e8e T2I \u751f\u6210\u7684\u65b0\u578b\u591a\u5956\u52b1 RL \u6846\u67b6\u3002\u901a\u8fc7\u4f7f\u7528\u6279\u91cf Pareto \u6700\u4f18\u9009\u62e9\uff0cParrot \u5728 T2I \u4e00\u4ee3\u7684 RL \u4f18\u5316\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8bc6\u522b\u4e0d\u540c\u5956\u52b1\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u3002\u6b64\u5916\uff0cParrot\u5bf9T2I\u6a21\u578b\u548c\u63d0\u793a\u6269\u5c55\u7f51\u7edc\u91c7\u7528\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u751f\u6210\u8d28\u91cf\u611f\u77e5\u7684\u6587\u672c\u63d0\u793a\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6700\u7ec8\u56fe\u50cf\u8d28\u91cf\u3002\u4e3a\u4e86\u62b5\u6d88\u7531\u4e8e\u63d0\u793a\u6269\u5c55\u800c\u5bfc\u81f4\u7684\u5bf9\u539f\u59cb\u7528\u6237\u63d0\u793a\u7684\u6f5c\u5728\u707e\u96be\u6027\u9057\u5fd8\uff0c\u6211\u4eec\u5728\u63a8\u7406\u65f6\u5f15\u5165\u4e86\u4ee5\u539f\u59cb\u63d0\u793a\u4e3a\u4e2d\u5fc3\u7684\u6307\u5bfc\uff0c\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u4fdd\u6301\u5fe0\u5b9e\u4e8e\u7528\u6237\u8f93\u5165\u3002\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cParrot \u5728\u5404\u79cd\u8d28\u91cf\u6807\u51c6\uff08\u5305\u62ec\u7f8e\u5b66\u3001\u4eba\u7c7b\u504f\u597d\u3001\u56fe\u50cf\u60c5\u611f\u548c\u6587\u672c\u56fe\u50cf\u5bf9\u9f50\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002|[2401.05675v1](http://arxiv.org/pdf/2401.05675v1)|null|\n", "2401.05625": "|**2024-01-11**|**Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle Dynamics in Videos**|Face-GPS\uff1a\u91cf\u5316\u89c6\u9891\u4e2d\u9762\u90e8\u808c\u8089\u52a8\u6001\u7684\u7efc\u5408\u6280\u672f|Juni Kim, Zhikang Dong, Pawel Polak|We introduce a novel method that combines differential geometry, kernels smoothing, and spectral analysis to quantify facial muscle activity from widely accessible video recordings, such as those captured on personal smartphones. Our approach emphasizes practicality and accessibility. It has significant potential for applications in national security and plastic surgery. Additionally, it offers remote diagnosis and monitoring for medical conditions such as stroke, Bell's palsy, and acoustic neuroma. Moreover, it is adept at detecting and classifying emotions, from the overt to the subtle. The proposed face muscle analysis technique is an explainable alternative to deep learning methods and a non-invasive substitute to facial electromyography (fEMG).|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5fae\u5206\u51e0\u4f55\u3001\u6838\u5e73\u6ed1\u548c\u5149\u8c31\u5206\u6790\uff0c\u4ece\u5e7f\u6cdb\u8bbf\u95ee\u7684\u89c6\u9891\u8bb0\u5f55\uff08\u4f8b\u5982\u4e2a\u4eba\u667a\u80fd\u624b\u673a\u4e0a\u6355\u83b7\u7684\u89c6\u9891\u8bb0\u5f55\uff09\u4e2d\u91cf\u5316\u9762\u90e8\u808c\u8089\u6d3b\u52a8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f3a\u8c03\u5b9e\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u5b83\u5728\u56fd\u5bb6\u5b89\u5168\u548c\u6574\u5f62\u5916\u79d1\u65b9\u9762\u5177\u6709\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e2d\u98ce\u3001\u8d1d\u5c14\u9ebb\u75f9\u548c\u542c\u795e\u7ecf\u7624\u7b49\u533b\u7597\u72b6\u51b5\u7684\u8fdc\u7a0b\u8bca\u65ad\u548c\u76d1\u6d4b\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u64c5\u957f\u68c0\u6d4b\u548c\u5206\u7c7b\u60c5\u7eea\uff0c\u4ece\u660e\u663e\u7684\u60c5\u7eea\u5230\u5fae\u5999\u7684\u60c5\u7eea\u3002\u6240\u63d0\u51fa\u7684\u9762\u90e8\u808c\u8089\u5206\u6790\u6280\u672f\u662f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4e5f\u662f\u9762\u90e8\u808c\u7535\u56fe\uff08fEMG\uff09\u7684\u975e\u4fb5\u5165\u6027\u66ff\u4ee3\u65b9\u6cd5\u3002|[2401.05625v1](http://arxiv.org/pdf/2401.05625v1)|null|\n"}}