{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.14405": "|**2024-01-25**|**Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities**|\u591a\u6a21\u6001\u8def\u5f84\uff1a\u5229\u7528\u5176\u4ed6\u6a21\u6001\u7684\u4e0d\u76f8\u5173\u6570\u636e\u6539\u8fdb Transformer|Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue|We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.|\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u6765\u81ea\u5176\u4ed6\u6a21\u6001\u7684\u4e0d\u76f8\u5173\u6570\u636e\u6765\u6539\u8fdb\u7279\u5b9a\u6a21\u6001\u7684\u8f6c\u6362\u5668\uff0c\u4f8b\u5982\uff0c\u4f7f\u7528\u97f3\u9891\u6216\u70b9\u4e91\u6570\u636e\u96c6\u6539\u8fdb ImageNet \u6a21\u578b\u3002\u6211\u4eec\u60f3\u5f3a\u8c03\u7684\u662f\uff0c\u76ee\u6807\u6a21\u6001\u7684\u6570\u636e\u6837\u672c\u4e0e\u5176\u4ed6\u6a21\u6001\u65e0\u5173\uff0c\u8fd9\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5229\u7528\u4e0d\u540c\u6a21\u6001\u7684\u914d\u5bf9\uff08\u4f8b\u5982 CLIP\uff09\u6216\u4ea4\u9519\u6570\u636e\u7684\u5176\u4ed6\u5de5\u4f5c\u533a\u5206\u5f00\u6765\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u6a21\u6001\u8def\u5f84\u7684\u65b9\u6cd5 - \u7ed9\u5b9a\u76ee\u6807\u6a21\u6001\u548c\u4e3a\u5176\u8bbe\u8ba1\u7684\u53d8\u538b\u5668\uff0c\u6211\u4eec\u4f7f\u7528\u7528\u53e6\u4e00\u79cd\u6a21\u6001\u7684\u6570\u636e\u8bad\u7ec3\u7684\u8f85\u52a9\u53d8\u538b\u5668\u5e76\u6784\u5efa\u8def\u5f84\u6765\u8fde\u63a5\u4e24\u4e2a\u6a21\u578b\u7684\u7ec4\u4ef6\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5904\u7406\u76ee\u6807\u6a21\u6001\u7684\u6570\u636e\u4e24\u79cd\u578b\u53f7\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u5229\u7528\u4ece\u4e24\u79cd\u6a21\u6001\u83b7\u5f97\u7684\u8f6c\u6362\u5668\u7684\u901a\u7528\u5e8f\u5217\u5230\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u3002\u4f5c\u4e3a\u5177\u4f53\u5b9e\u73b0\uff0c\u6211\u4eec\u50cf\u5f80\u5e38\u4e00\u6837\u4f7f\u7528\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u5206\u8bcd\u5668\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5934\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u540d\u4e3a\u201c\u8de8\u6a21\u6001\u91cd\u65b0\u53c2\u6570\u5316\u201d\u7684\u65b9\u6cd5\u5229\u7528\u8f85\u52a9\u6a21\u578b\u7684\u8f6c\u6362\u5668\u5757\uff0c\u8be5\u65b9\u6cd5\u5728\u6ca1\u6709\u4efb\u4f55\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u8f85\u52a9\u6743\u91cd\u3002\u5728\u56fe\u50cf\u3001\u70b9\u4e91\u3001\u89c6\u9891\u548c\u97f3\u9891\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6765\u81ea\u5176\u4ed6\u6a21\u5f0f\u7684\u4e0d\u76f8\u5173\u6570\u636e\u7684\u663e\u7740\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/AILab-CVC/M2PT \u83b7\u53d6\u3002|[2401.14405v1](http://arxiv.org/pdf/2401.14405v1)|**[link](https://github.com/ailab-cvc/m2pt)**|\n", "2401.14398": "|**2024-01-25**|**pix2gestalt: Amodal Segmentation by Synthesizing Wholes**|pix2gestalt\uff1a\u901a\u8fc7\u7efc\u5408\u6574\u4f53\u8fdb\u884c\u65e0\u6a21\u6001\u5206\u5272|Ege Ozguroglu, Ruoshi Liu, D\u00eddac Sur\u00eds, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick|We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.|\u6211\u4eec\u5f15\u5165\u4e86 pix2gestalt\uff0c\u8fd9\u662f\u4e00\u79cd\u96f6\u6837\u672c\u975e\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u5b83\u5b66\u4e60\u4f30\u8ba1\u5728\u906e\u6321\u540e\u4ec5\u90e8\u5206\u53ef\u89c1\u7684\u6574\u4e2a\u5bf9\u8c61\u7684\u5f62\u72b6\u548c\u5916\u89c2\u3002\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5e76\u5c06\u5176\u8868\u793a\u8f6c\u79fb\u5230\u6b64\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u4e00\u79cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u91cd\u5efa\u6574\u4e2a\u5bf9\u8c61\uff0c\u5305\u62ec\u6253\u7834\u81ea\u7136\u548c\u7269\u7406\u5148\u9a8c\u7684\u793a\u4f8b\uff0c\u4f8b\u5982\u827a\u672f\u3002\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u6211\u4eec\u4f7f\u7528\u7efc\u5408\u6574\u7406\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e0e\u5176\u6574\u4e2a\u5bf9\u5e94\u5bf9\u8c61\u914d\u5bf9\u7684\u906e\u6321\u5bf9\u8c61\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65e2\u5b9a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u53ef\u7528\u4e8e\u5728\u5b58\u5728\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u663e\u7740\u63d0\u9ad8\u73b0\u6709\u5bf9\u8c61\u8bc6\u522b\u548c 3D \u91cd\u5efa\u65b9\u6cd5\u7684\u6027\u80fd\u3002|[2401.14398v1](http://arxiv.org/pdf/2401.14398v1)|**[link](https://github.com/cvlab-columbia/pix2gestalt)**|\n", "2401.14391": "|**2024-01-25**|**Rethinking Patch Dependence for Masked Autoencoders**|\u91cd\u65b0\u601d\u8003\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668\u7684\u8865\u4e01\u4f9d\u8d56\u6027|Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei A. Efros, Ken Goldberg|In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io|\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u68c0\u67e5\u4e86\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u89e3\u7801\u673a\u5236\u4e2d\u7684\u8865\u4e01\u95f4\u4f9d\u8d56\u6027\u3002\u6211\u4eec\u5c06 MAE \u4e2d\u63a9\u6a21\u8865\u4e01\u91cd\u5efa\u7684\u89e3\u7801\u673a\u5236\u5206\u89e3\u4e3a\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u6211\u4eec\u7684\u8c03\u67e5\u8868\u660e\uff0c\u63a9\u6a21\u8865\u4e01\u4e4b\u95f4\u7684\u81ea\u6ce8\u610f\u529b\u5bf9\u4e8e\u5b66\u4e60\u826f\u597d\u7684\u8868\u793a\u5e76\u4e0d\u662f\u5fc5\u9700\u7684\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff1a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668\uff08CrossMAE\uff09\u3002 CrossMAE \u7684\u89e3\u7801\u5668\u4ec5\u5229\u7528\u5c4f\u853d\u6807\u8bb0\u548c\u53ef\u89c1\u6807\u8bb0\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u4e0d\u4f1a\u964d\u4f4e\u4e0b\u6e38\u6027\u80fd\u3002\u8fd9\u79cd\u8bbe\u8ba1\u8fd8\u53ef\u4ee5\u4ec5\u89e3\u7801\u63a9\u7801\u4ee4\u724c\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u6bcf\u4e2a\u89e3\u7801\u5668\u5757\u73b0\u5728\u53ef\u4ee5\u5229\u7528\u4e0d\u540c\u7684\u7f16\u7801\u5668\u529f\u80fd\uff0c\u4ece\u800c\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u3002 CrossMAE \u7684\u6027\u80fd\u4e0e MAE \u76f8\u5f53\uff0c\u4f46\u89e3\u7801\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86 2.5 \u81f3 3.7 \u7f8e\u5143\\\u500d$\u3002\u5728\u76f8\u540c\u8ba1\u7b97\u4e0b\uff0c\u5b83\u5728 ImageNet \u5206\u7c7b\u548c COCO \u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4e5f\u8d85\u8fc7\u4e86 MAE\u3002\u4ee3\u7801\u548c\u6a21\u578b\uff1ahttps://crossmae.github.io|[2401.14391v1](http://arxiv.org/pdf/2401.14391v1)|null|\n", "2401.14387": "|**2024-01-25**|**Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label Pairs**|\u4e0d\u4e00\u81f4\u63a9\u7801\uff1a\u6d88\u9664\u8f93\u5165\u4f2a\u6807\u7b7e\u5bf9\u7684\u4e0d\u786e\u5b9a\u6027|Michael R. H. Vorndran, Bernhard F. Roeck|Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks. Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models. We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques. By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data. Notably, three of our hybrid models outperform those trained on the fully labeled dataset. Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques. For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions. The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks|\u751f\u6210\u8db3\u591f\u7684\u6807\u8bb0\u6570\u636e\u662f\u6709\u6548\u6267\u884c\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u7684\u4e00\u4e2a\u91cd\u5927\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5206\u5272\u7684\u672a\u77e5\u9886\u57df\uff0c\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e0d\u540c\uff0c\u6807\u8bb0\u9700\u8981\u5927\u91cf\u65f6\u95f4\u3002\u6211\u4eec\u7684\u7814\u7a76\u9762\u4e34\u7740\u8fd9\u4e00\u6311\u6218\uff0c\u5728\u53d7\u786c\u4ef6\u8d44\u6e90\u6709\u9650\u4e14\u7f3a\u4e4f\u5e7f\u6cdb\u6570\u636e\u96c6\u6216\u9884\u8bad\u7ec3\u6a21\u578b\u9650\u5236\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e0d\u4e00\u81f4\u63a9\u6a21\uff08IM\uff09\u7684\u65b0\u9896\u7528\u9014\uff0c\u53ef\u4ee5\u6709\u6548\u8fc7\u6ee4\u56fe\u50cf\u4f2a\u6807\u7b7e\u5bf9\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6280\u672f\u3002\u901a\u8fc7\u5c06 IM \u4e0e\u5176\u4ed6\u65b9\u6cd5\u96c6\u6210\uff0c\u6211\u4eec\u5728 ISIC 2018 \u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u51fa\u8272\u7684\u4e8c\u5143\u5206\u5272\u6027\u80fd\uff0c\u4ece\u4ec5 10% \u7684\u6807\u8bb0\u6570\u636e\u5f00\u59cb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u4e09\u4e2a\u6df7\u5408\u6a21\u578b\u4f18\u4e8e\u5728\u5b8c\u5168\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53e6\u5916\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u4e0e\u5176\u4ed6\u6280\u672f\u76f8\u7ed3\u5408\u65f6\u663e\u793a\u51fa\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\u3002\u4e3a\u4e86\u8fdb\u884c\u5168\u9762\u548c\u7a33\u5065\u7684\u8bc4\u4f30\uff0c\u672c\u6587\u5bf9\u6d41\u884c\u7684\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5206\u6790\uff0c\u6240\u6709\u8fd9\u4e9b\u7b56\u7565\u90fd\u5728\u76f8\u540c\u7684\u8d77\u59cb\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002\u5b8c\u6574\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/MichaelVorndran/InconsistencyMasks|[2401.14387v1](http://arxiv.org/pdf/2401.14387v1)|**[link](https://github.com/michaelvorndran/inconsistencymasks)**|\n", "2401.14379": "|**2024-01-25**|**UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models**|UrbanGenAI\uff1a\u4f7f\u7528\u5168\u666f\u5206\u5272\u548c\u6269\u6563\u6a21\u578b\u91cd\u5efa\u57ce\u5e02\u666f\u89c2|Timo Kapsalis|In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design|\u5728\u5f53\u4ee3\u8bbe\u8ba1\u5b9e\u8df5\u4e2d\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u6210\u4eba\u5de5\u667a\u80fd\uff08genAI\uff09\u7684\u96c6\u6210\u4ee3\u8868\u4e86\u5411\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u5305\u5bb9\u6027\u6d41\u7a0b\u7684\u53d8\u9769\u6027\u8f6c\u53d8\u3002\u8fd9\u4e9b\u6280\u672f\u63d0\u4f9b\u4e86\u56fe\u50cf\u5206\u6790\u548c\u751f\u6210\u7684\u65b0\u7ef4\u5ea6\uff0c\u8fd9\u5728\u57ce\u5e02\u666f\u89c2\u91cd\u5efa\u7684\u80cc\u666f\u4e0b\u5c24\u5176\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c01\u88c5\u5728\u539f\u578b\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u65b0\u9896\u5de5\u4f5c\u6d41\u7a0b\uff0c\u65e8\u5728\u5229\u7528\u5148\u8fdb\u56fe\u50cf\u5206\u5272\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u6765\u5b9e\u73b0\u57ce\u5e02\u8bbe\u8ba1\u7684\u7efc\u5408\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u7528\u4e8e\u8be6\u7ec6\u56fe\u50cf\u5206\u5272\u7684 OneFormer \u6a21\u578b\u548c\u901a\u8fc7 ControlNet \u5b9e\u73b0\u7684\u7a33\u5b9a\u6269\u6563 XL (SDXL) \u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u63cf\u8ff0\u751f\u6210\u56fe\u50cf\u3002\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u539f\u578b\u5e94\u7528\u7a0b\u5e8f\u5177\u6709\u5f88\u9ad8\u7684\u6027\u80fd\uff0c\u5728\u5bf9\u8c61\u68c0\u6d4b\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u90fd\u663e\u793a\u51fa\u6781\u9ad8\u7684\u51c6\u786e\u6027\u3002\u5bf9\u5404\u7c7b\u57ce\u5e02\u666f\u89c2\u7279\u5f81\u7684\u8fed\u4ee3\u8bc4\u4f30\u4e2d\u4f18\u5f02\u7684\u4ea4\u96c6\u6bd4\u5e76\u96c6 (IoU) \u548c CLIP \u5206\u6570\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u521d\u6b65\u6d4b\u8bd5\u5305\u62ec\u5229\u7528 UrbanGenAI \u4f5c\u4e3a\u4e00\u79cd\u6559\u80b2\u5de5\u5177\uff0c\u589e\u5f3a\u8bbe\u8ba1\u6559\u5b66\u6cd5\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u5e76\u4f5c\u4e3a\u4e00\u79cd\u53c2\u4e0e\u6027\u5de5\u5177\uff0c\u4fc3\u8fdb\u793e\u533a\u9a71\u52a8\u7684\u57ce\u5e02\u89c4\u5212\u3002\u65e9\u671f\u7ed3\u679c\u8868\u660e\uff0cUrbanGenAI \u4e0d\u4ec5\u63a8\u8fdb\u4e86\u57ce\u5e02\u666f\u89c2\u91cd\u5efa\u7684\u6280\u672f\u524d\u6cbf\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u4e86\u663e\u7740\u7684\u6559\u5b66\u548c\u53c2\u4e0e\u6027\u89c4\u5212\u6548\u76ca\u3002 UrbanGenAI \u7684\u6301\u7eed\u5f00\u53d1\u65e8\u5728\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u96c6\u6210\u5b9e\u65f6\u53cd\u9988\u673a\u5236\u548c 3D \u5efa\u6a21\u529f\u80fd\u7b49\u9644\u52a0\u529f\u80fd\u3002\u5173\u952e\u8bcd\uff1a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff1b\u5168\u666f\u56fe\u50cf\u5206\u5272\uff1b\u6269\u6563\u6a21\u578b\uff1b\u57ce\u5e02\u666f\u89c2\u8bbe\u8ba1\uff1b\u8bbe\u8ba1\u6559\u5b66\u6cd5\uff1b\u534f\u540c\u8bbe\u8ba1|[2401.14379v1](http://arxiv.org/pdf/2401.14379v1)|null|\n", "2401.14336": "|**2024-01-25**|**Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition**|\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8f66\u8f86\u8bc6\u522b\u7684\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u6297\u566a\u58f0\u5b66\u4e60\u548c\u84b8\u998f\u6846\u67b6|Dichao Liu|Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network. Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks. The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR|\u7ec6\u7c92\u5ea6\u8f66\u8f86\u8bc6\u522b\uff08FGVR\uff09\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u5fc5\u4e0d\u53ef\u5c11\u7684\u57fa\u7840\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u5176\u56fa\u6709\u7684\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u5176\u96be\u5ea6\u975e\u5e38\u5927\u3002\u4ee5\u5f80\u7684FGVR\u7814\u7a76\u5927\u591a\u53ea\u5173\u6ce8\u4e0d\u540c\u62cd\u6444\u89d2\u5ea6\u3001\u4f4d\u7f6e\u7b49\u5f15\u8d77\u7684\u7c7b\u5185\u53d8\u5f02\uff0c\u800c\u56fe\u50cf\u566a\u58f0\u5f15\u8d77\u7684\u7c7b\u5185\u53d8\u5f02\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u6297\u566a\u58f0\u5b66\u4e60\uff08PMAL\uff09\u6846\u67b6\u548c\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u84b8\u998f\uff08PMD\uff09\u6846\u67b6\u6765\u89e3\u51b3FGVR\u4e2d\u7531\u4e8e\u56fe\u50cf\u566a\u58f0\u800c\u5bfc\u81f4\u7684\u7c7b\u5185\u53d8\u5f02\u95ee\u9898\u3002 PMAL \u6846\u67b6\u901a\u8fc7\u5c06\u56fe\u50cf\u53bb\u566a\u89c6\u4e3a\u56fe\u50cf\u8bc6\u522b\u4e2d\u7684\u9644\u52a0\u4efb\u52a1\u5e76\u9010\u6b65\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u566a\u58f0\u4e0d\u53d8\u6027\u6765\u5b9e\u73b0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002 PMD \u6846\u67b6\u5c06 PMAL \u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u539f\u59cb\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u751f\u6210\u7684\u6a21\u578b\u4e0e PMAL \u8bad\u7ec3\u6a21\u578b\u7684\u8bc6\u522b\u7cbe\u5ea6\u5927\u81f4\u76f8\u540c\uff0c\u4f46\u4e0e\u539f\u59cb\u9aa8\u5e72\u7f51\u7edc\u76f8\u6bd4\u6ca1\u6709\u4efb\u4f55\u989d\u5916\u5f00\u9500\u3002\u7ed3\u5408\u8fd9\u4e24\u4e2a\u6846\u67b6\uff0c\u6211\u4eec\u83b7\u5f97\u7684\u6a21\u578b\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6807\u51c6 FGVR \u6570\u636e\u96c6\uff08\u5373\u65af\u5766\u798f\u6c7d\u8f66\u548c CompCars\uff09\u4ee5\u53ca\u53e6\u5916\u4e09\u4e2a\u57fa\u4e8e\u76d1\u63a7\u56fe\u50cf\u7684\u8f66\u8f86\u4e0a\u7684\u8bc6\u522b\u7cbe\u5ea6\u663e\u7740\u8d85\u8fc7\u4e86\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7c7b\u578b\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5373\u5317\u4eac\u7406\u5de5\u5927\u5b66 (BIT)-\u8f66\u8f86\u3001\u8f66\u8f86\u7c7b\u578b\u56fe\u50cf\u6570\u636e 2 (VTID2) \u548c\u7528\u4e8e\u6a21\u578b\u8bc6\u522b\u7684\u8f66\u8f86\u56fe\u50cf\u6570\u636e\u96c6 (VIDMMR)\uff0c\u800c\u4e0d\u4f1a\u5bf9\u539f\u59cb\u9aa8\u5e72\u7f51\u7edc\u4ea7\u751f\u4efb\u4f55\u989d\u5916\u5f00\u9500\u3002\u6e90\u4ee3\u7801\u4f4d\u4e8e https://github.com/Dichao-Liu/Anti-noise_FGVR|[2401.14336v1](http://arxiv.org/pdf/2401.14336v1)|**[link](https://github.com/dichao-liu/anti-noise_fgvr)**|\n", "2401.14325": "|**2024-01-25**|**Unlocking Past Information: Temporal Embeddings in Cooperative Bird's Eye View Prediction**|\u89e3\u9501\u8fc7\u53bb\u7684\u4fe1\u606f\uff1a\u5408\u4f5c\u9e1f\u77b0\u9884\u6d4b\u4e2d\u7684\u65f6\u95f4\u5d4c\u5165|Dominik R\u00f6\u00dfle, Jeremias Gerner, Klaus Bogenberger, Daniel Cremers, Stefanie Schmidtner, Torsten Sch\u00f6n|Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub.|\u51c6\u786e\u3001\u5168\u9762\u7684\u9e1f\u77b0\u56fe (BEV) \u8bed\u4e49\u5206\u5272\u5bf9\u4e8e\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b89\u5168\u548c\u4e3b\u52a8\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u534f\u4f5c\u611f\u77e5\u5df2\u7ecf\u8d85\u8fc7\u4e86\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u534f\u4f5c\u611f\u77e5\u4e2d\u6d41\u884c\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u7b97\u6cd5\u5ffd\u7565\u4e86\u4ece\u5386\u53f2\u89c2\u5bdf\u4e2d\u83b7\u5f97\u7684\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u5728\u4f20\u611f\u5668\u6545\u969c\u6216\u901a\u4fe1\u95ee\u9898\u671f\u95f4\uff0c\u968f\u7740\u534f\u4f5c\u611f\u77e5\u6062\u590d\u4e3a\u5355\u4ee3\u7406\u611f\u77e5\uff0c\u8fd9\u79cd\u9650\u5236\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c BEV \u5206\u5272\u56fe\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 TempCoBEV\uff0c\u8fd9\u662f\u4e00\u4e2a\u65f6\u95f4\u6a21\u5757\uff0c\u65e8\u5728\u5c06\u5386\u53f2\u7ebf\u7d22\u878d\u5165\u5f53\u524d\u89c2\u6d4b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8 BEV \u5730\u56fe\u5206\u5272\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u8981\u6027\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u67b6\u6784\u6765\u6709\u6548\u5730\u6574\u5408\u65f6\u95f4\u4fe1\u606f\uff0c\u4f18\u5148\u8003\u8651 BEV \u5730\u56fe\u5206\u5272\u7684\u76f8\u5173\u5c5e\u6027\u3002 TempCoBEV \u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u65f6\u95f4\u6a21\u5757\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u534f\u4f5c\u611f\u77e5\u6a21\u578b\u4e2d\u3002\u6211\u4eec\u901a\u8fc7\u5bf9 OPV2V \u6570\u636e\u96c6\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cTempCoBEV \u5728\u9884\u6d4b\u5f53\u524d\u548c\u672a\u6765\u7684 BEV \u5730\u56fe\u5206\u5272\u65b9\u9762\u6bd4\u975e\u65f6\u95f4\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u901a\u4fe1\u6545\u969c\u7684\u573a\u666f\u4e2d\u3002\u6211\u4eec\u5c55\u793a\u4e86 TempCoBEV \u7684\u529f\u6548\u53ca\u5176\u5c06\u5386\u53f2\u7ebf\u7d22\u96c6\u6210\u5230\u5f53\u524d BEV \u5730\u56fe\u4e2d\u7684\u80fd\u529b\uff0c\u5c06\u6700\u4f73\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u63d0\u9ad8\u4e86\u9ad8\u8fbe 2%\uff0c\u5c06\u901a\u4fe1\u6545\u969c\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u63d0\u9ad8\u4e86\u9ad8\u8fbe 19%\u3002\u4ee3\u7801\u5c06\u53d1\u5e03\u5728 GitHub \u4e0a\u3002|[2401.14325v1](http://arxiv.org/pdf/2401.14325v1)|null|\n", "2401.14256": "|**2024-01-25**|**Producing Plankton Classifiers that are Robust to Dataset Shift**|\u751f\u6210\u5bf9\u6570\u636e\u96c6\u8f6c\u6362\u5177\u6709\u9c81\u68d2\u6027\u7684\u6d6e\u6e38\u751f\u7269\u5206\u7c7b\u5668|Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi|Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD degradation compared to nominal test performance, (ii) conducting a diagnostic analysis of degradation causes, and (iii) providing solutions. We find that ensembles of BEiT vision transformers, with targeted augmentations addressing OOD robustness, geometric ensembling, and rotation-based test-time augmentation, constitute the most robust model, which we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated on container classes. Moreover, it exhibits lower sensitivity to dataset shift, and reproduces well the plankton abundances. Our proposed pipeline is applicable to generic plankton classifiers, contingent on the availability of suitable test cells. By identifying critical shortcomings and offering practical procedures to fortify models against dataset shift, our study contributes to the development of more reliable plankton classification technologies.|\u73b0\u4ee3\u6d6e\u6e38\u751f\u7269\u9ad8\u901a\u91cf\u76d1\u6d4b\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u6765\u8bc6\u522b\u6c34\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u7269\u79cd\u3002\u5c3d\u7ba1\u6807\u79f0\u6027\u80fd\u4ee4\u4eba\u6ee1\u610f\uff0c\u4f46\u6570\u636e\u96c6\u79fb\u4f4d\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u90e8\u7f72\u671f\u95f4\u6027\u80fd\u4e0b\u964d\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5c06 ZooLake \u6570\u636e\u96c6\u4e0e 10 \u4e2a\u72ec\u7acb\u90e8\u7f72\u65e5\u7684\u624b\u52a8\u6ce8\u91ca\u56fe\u50cf\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u96c6\u5916 (OOD) \u6027\u80fd\u7684\u6d4b\u8bd5\u5355\u5143\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5206\u7c7b\u5668\u6700\u521d\u5728\u6570\u636e\u96c6\u4e2d\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9047\u5230\u663e\u7740\u5931\u8d25\u7684\u60c5\u51b5\u3002\u4f8b\u5982\uff0c\u6807\u79f0\u6d4b\u8bd5\u51c6\u786e\u5ea6\u4e3a 92% \u7684 MobileNet \u663e\u793a\u51fa 77% OOD \u51c6\u786e\u5ea6\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u8c03\u67e5\u5bfc\u81f4 OOD \u6027\u80fd\u4e0b\u964d\u7684\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5148\u53d1\u5236\u4eba\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u8bc6\u522b\u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u65f6\u7684\u6f5c\u5728\u9677\u9631\uff0c\u5e76\u67e5\u660e OOD \u56fe\u50cf\u4e2d\u5bf9\u5206\u7c7b\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u7684\u7279\u5f81\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u6b65\u6d41\u7a0b\uff1a(i) \u4e0e\u6807\u79f0\u6d4b\u8bd5\u6027\u80fd\u76f8\u6bd4\u8bc6\u522b OOD \u9000\u5316\uff0c(ii) \u5bf9\u9000\u5316\u539f\u56e0\u8fdb\u884c\u8bca\u65ad\u5206\u6790\uff0c\u4ee5\u53ca (iii) \u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u53d1\u73b0 BEiT \u89c6\u89c9\u53d8\u6362\u5668\u7684\u96c6\u5408\uff0c\u5177\u6709\u9488\u5bf9 OOD \u9c81\u68d2\u6027\u7684\u6709\u9488\u5bf9\u6027\u7684\u589e\u5f3a\u3001\u51e0\u4f55\u96c6\u6210\u548c\u57fa\u4e8e\u65cb\u8f6c\u7684\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\uff0c\u6784\u6210\u4e86\u6700\u9c81\u68d2\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a BEsT \u6a21\u578b\u3002\u5b83\u5b9e\u73b0\u4e86 83% \u7684 OOD \u51c6\u786e\u7387\uff0c\u9519\u8bef\u96c6\u4e2d\u5728\u5bb9\u5668\u7c7b\u4e0a\u3002\u6b64\u5916\uff0c\u5b83\u5bf9\u6570\u636e\u96c6\u53d8\u5316\u7684\u654f\u611f\u6027\u8f83\u4f4e\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u518d\u73b0\u6d6e\u6e38\u751f\u7269\u4e30\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7ba1\u9053\u9002\u7528\u4e8e\u901a\u7528\u6d6e\u6e38\u751f\u7269\u5206\u7c7b\u5668\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5408\u9002\u7684\u6d4b\u8bd5\u5355\u5143\u7684\u53ef\u7528\u6027\u3002\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u7f3a\u9677\u5e76\u63d0\u4f9b\u5b9e\u7528\u7a0b\u5e8f\u6765\u5f3a\u5316\u6a21\u578b\u4ee5\u5e94\u5bf9\u6570\u636e\u96c6\u53d8\u5316\uff0c\u6211\u4eec\u7684\u7814\u7a76\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u6d6e\u6e38\u751f\u7269\u5206\u7c7b\u6280\u672f\u3002|[2401.14256v1](http://arxiv.org/pdf/2401.14256v1)|null|\n", "2401.14248": "|**2024-01-25**|**On generalisability of segment anything model for nuclear instance segmentation in histology images**|\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u6838\u5b9e\u4f8b\u5206\u5272\u7684\u5206\u6bb5\u4efb\u610f\u6a21\u578b\u7684\u901a\u7528\u6027|Kesi Xu, Lea Goetz, Nasir Rajpoot|Pre-trained on a large and diverse dataset, the segment anything model (SAM) is the first promptable foundation model in computer vision aiming at object segmentation tasks. In this work, we evaluate SAM for the task of nuclear instance segmentation performance with zero-shot learning and finetuning. We compare SAM with other representative methods in nuclear instance segmentation, especially in the context of model generalisability. To achieve automatic nuclear instance segmentation, we propose using a nuclei detection model to provide bounding boxes or central points of nu-clei as visual prompts for SAM in generating nuclear instance masks from histology images.|\u5206\u6bb5\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u5728\u5927\u578b\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7b2c\u4e00\u4e2a\u9488\u5bf9\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u7684\u53ef\u63d0\u793a\u57fa\u7840\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5fae\u8c03\u6765\u8bc4\u4f30 SAM \u5728\u6838\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06 SAM \u4e0e\u6838\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u5176\u4ed6\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u901a\u7528\u6027\u65b9\u9762\u3002\u4e3a\u4e86\u5b9e\u73b0\u81ea\u52a8\u6838\u5b9e\u4f8b\u5206\u5272\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u6838\u68c0\u6d4b\u6a21\u578b\u6765\u63d0\u4f9b\u6838\u7684\u8fb9\u754c\u6846\u6216\u4e2d\u5fc3\u70b9\uff0c\u4f5c\u4e3a SAM \u4ece\u7ec4\u7ec7\u5b66\u56fe\u50cf\u751f\u6210\u6838\u5b9e\u4f8b\u63a9\u6a21\u65f6\u7684\u89c6\u89c9\u63d0\u793a\u3002|[2401.14248v1](http://arxiv.org/pdf/2401.14248v1)|null|\n", "2401.14236": "|**2024-01-25**|**Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification**|\u63a2\u7d22\u672a\u63a2\u7d22\u7684\u4e8b\u7269\uff1a\u4e86\u89e3\u56fe\u5c42\u8c03\u6574\u5bf9\u56fe\u50cf\u5206\u7c7b\u7684\u5f71\u54cd|Haixia Liu, Tim Brailsford, James Goulding, Gavin Smith, Larry Bull|This paper investigates how adjustments to deep learning architectures impact model performance in image classification. Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset. Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results. The choice and order of layers as well as filter placement significantly impact model performance. This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms.|\u672c\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u8c03\u6574\u5982\u4f55\u5f71\u54cd\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002\u5c3d\u7ba1\u89c2\u5bdf\u5230\u7684\u8d8b\u52bf\u4e0e\u6574\u4e2a\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\uff0c\u4f46\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u4ea7\u751f\u4e86\u521d\u6b65\u89c1\u89e3\u3002\u56fe\u50cf\u5904\u7406\u7ba1\u9053\u4e2d\u7684\u8fc7\u6ee4\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u5728\u9884\u5904\u7406\u4e4b\u524d\u8fdb\u884c\u56fe\u50cf\u8fc7\u6ee4\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u7ed3\u679c\u3002\u5c42\u7684\u9009\u62e9\u548c\u987a\u5e8f\u4ee5\u53ca\u8fc7\u6ee4\u5668\u7684\u653e\u7f6e\u4f1a\u663e\u7740\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u9014\u5f84\uff0c\u5305\u62ec\u534f\u4f5c\u5e73\u53f0\u3002|[2401.14236v1](http://arxiv.org/pdf/2401.14236v1)|null|\n", "2401.14193": "|**2024-01-25**|**Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study**|\u4eba\u5de5\u667a\u80fd\u4e34\u5e8a\u9ed1\u8272\u7d20\u7624\u8bca\u65ad\uff1a\u524d\u77bb\u6027\u591a\u4e2d\u5fc3\u7814\u7a76\u7684\u89c1\u89e3|Lukas Heinlein, Roman C. Maron, Achim Hekler, Sarah Haggenm\u00fcller, Christoph Wies, Jochen S. Utikal, Friedegund Meier, Sarah Hobelsberger, Frank F. Gellrich, Mildred Sergon, et.al.|Early detection of melanoma, a potentially lethal type of skin cancer with high prevalence worldwide, improves patient prognosis. In retrospective studies, artificial intelligence (AI) has proven to be helpful for enhancing melanoma detection. However, there are few prospective studies confirming these promising results. Existing studies are limited by low sample sizes, too homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing a fair and thorough evaluation of AI and its generalizability, a crucial aspect for its application in the clinical setting. Therefore, we assessed 'All Data are Ext' (ADAE), an established open-source ensemble algorithm for detecting melanomas, by comparing its diagnostic accuracy to that of dermatologists on a prospectively collected, external, heterogeneous test set comprising eight distinct hospitals, four different camera setups, rare melanoma subtypes, and special anatomical sites. We advanced the algorithm with real test-time augmentation (R-TTA, i.e. providing real photographs of lesions taken from multiple angles and averaging the predictions), and evaluated its generalization capabilities. Overall, the AI showed higher balanced accuracy than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781, 95% CI 0.760-0.802; p<0.001), obtaining a higher sensitivity (0.921, 95% CI 0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p<0.001) at the cost of a lower specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p<0.001). As the algorithm exhibited a significant performance advantage on our heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI may offer the potential to support dermatologists particularly in diagnosing challenging cases.|\u9ed1\u8272\u7d20\u7624\u662f\u4e00\u79cd\u6f5c\u5728\u81f4\u547d\u7684\u76ae\u80a4\u764c\uff0c\u5728\u5168\u7403\u8303\u56f4\u5185\u53d1\u75c5\u7387\u5f88\u9ad8\uff0c\u65e9\u671f\u53d1\u73b0\u53ef\u4ee5\u6539\u5584\u60a3\u8005\u7684\u9884\u540e\u3002\u56de\u987e\u6027\u7814\u7a76\u8868\u660e\uff0c\u4eba\u5de5\u667a\u80fd (AI) \u5df2\u88ab\u8bc1\u660e\u6709\u52a9\u4e8e\u589e\u5f3a\u9ed1\u8272\u7d20\u7624\u68c0\u6d4b\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u524d\u77bb\u6027\u7814\u7a76\u8bc1\u5b9e\u8fd9\u4e9b\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u73b0\u6709\u7684\u7814\u7a76\u53d7\u5230\u6837\u672c\u91cf\u5c0f\u3001\u6570\u636e\u96c6\u8fc7\u4e8e\u540c\u8d28\u6216\u7f3a\u4e4f\u7f55\u89c1\u9ed1\u8272\u7d20\u7624\u4e9a\u578b\u7684\u9650\u5236\uff0c\u963b\u788d\u4e86\u5bf9\u4eba\u5de5\u667a\u80fd\u53ca\u5176\u666e\u904d\u6027\u7684\u516c\u5e73\u548c\u5f7b\u5e95\u7684\u8bc4\u4f30\uff0c\u800c\u8fd9\u662f\u4eba\u5de5\u667a\u80fd\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5e94\u7528\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u201cAll Data are Ext\u201d\uff08ADAE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u9ed1\u8272\u7d20\u7624\u7684\u5df2\u5efa\u7acb\u7684\u5f00\u6e90\u96c6\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5176\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u76ae\u80a4\u79d1\u533b\u751f\u5728\u524d\u77bb\u6027\u6536\u96c6\u7684\u5916\u90e8\u5f02\u8d28\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8bca\u65ad\u51c6\u786e\u6027\u8fdb\u884c\u6bd4\u8f83\uff0c\u8be5\u6d4b\u8bd5\u96c6\u5305\u62ec\u516b\u5bb6\u4e0d\u540c\u7684\u533b\u9662\u3001\u56db\u5bb6\u533b\u9662\u4e0d\u540c\u7684\u76f8\u673a\u8bbe\u7f6e\u3001\u7f55\u89c1\u7684\u9ed1\u8272\u7d20\u7624\u4e9a\u578b\u548c\u7279\u6b8a\u7684\u89e3\u5256\u90e8\u4f4d\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u65f6\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\uff08R-TTA\uff0c\u5373\u63d0\u4f9b\u4ece\u591a\u4e2a\u89d2\u5ea6\u62cd\u6444\u7684\u75c5\u53d8\u7684\u771f\u5b9e\u7167\u7247\u5e76\u5bf9\u9884\u6d4b\u8fdb\u884c\u5e73\u5747\uff09\u6765\u6539\u8fdb\u8be5\u7b97\u6cd5\uff0c\u5e76\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002\u603b\u4f53\u800c\u8a00\uff0cAI \u663e\u793a\u51fa\u6bd4\u76ae\u80a4\u79d1\u533b\u751f\u66f4\u9ad8\u7684\u5e73\u8861\u51c6\u786e\u6027\uff080.798\uff0c95% CI 0.779-0.814 vs. 0.781\uff0c95% CI 0.760-0.802\uff1bp<0.001\uff09\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\uff080.921\uff0c95% CI 0.900\uff09 - 0.942 vs. 0.734\uff0c95% CI 0.701-0.770\uff1bp<0.001\uff09\uff0c\u4f46\u7279\u5f02\u6027\u8f83\u4f4e\uff080.673\uff0c95% CI 0.641-0.702 vs. 0.828\uff0c95% CI 0.804-0.852\uff1bp<0.001\uff09\u3002\u7531\u4e8e\u8be5\u7b97\u6cd5\u5728\u4ec5\u5305\u542b\u9ed1\u8272\u7d20\u7624\u53ef\u7591\u75c5\u53d8\u7684\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u7740\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u56e0\u6b64\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u4e3a\u76ae\u80a4\u79d1\u533b\u751f\u63d0\u4f9b\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u8bca\u65ad\u5177\u6709\u6311\u6218\u6027\u7684\u75c5\u4f8b\u65b9\u9762\u3002|[2401.14193v1](http://arxiv.org/pdf/2401.14193v1)|null|\n", "2401.14168": "|**2024-01-25**|**Vivim: a Video Vision Mamba for Medical Video Object Segmentation**|Vivim\uff1a\u7528\u4e8e\u533b\u7597\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u89c6\u9891\u89c6\u89c9 Mamba|Yijun Yang, Zhaohu Xing, Lei Zhu|Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks. Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks. To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block. Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance. Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim. The code for Vivim is available at: https://github.com/scott-yjyang/Vivim.|\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u611f\u53d7\u91ce\u6709\u9650\uff0c\u800c\u4ece\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f51\u7edc\u5728\u6784\u5efa\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u5e73\u5e73\u3002\u5728\u89c6\u9891\u5206\u6790\u4efb\u52a1\u4e2d\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u65f6\uff0c\u8fd9\u79cd\u74f6\u9888\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\uff0c\u4ee5 Mamba \u7740\u79f0\u7684\u5177\u6709\u9ad8\u6548\u786c\u4ef6\u611f\u77e5\u8bbe\u8ba1\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u5c31\uff0c\u8fd9\u4fc3\u8fdb\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bb8\u591a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u53d1\u5c55\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6355\u83b7\u89c6\u9891\u5e27\u4e2d\u7684\u53ef\u7528\u7ebf\u7d22\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u89c6\u89c9 Mamba \u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\uff0c\u540d\u4e3a Vivim\u3002\u6211\u4eec\u7684 Vivim \u53ef\u4ee5\u901a\u8fc7\u6211\u4eec\u8bbe\u8ba1\u7684\u65f6\u7a7a\u66fc\u5df4\u5757\u6709\u6548\u5730\u5c06\u957f\u671f\u65f6\u7a7a\u8868\u793a\u538b\u7f29\u4e3a\u4e0d\u540c\u5c3a\u5ea6\u7684\u5e8f\u5217\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e Transformer \u7684\u89c6\u9891\u7ea7\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u5206\u5272\u7ed3\u679c\u548c\u66f4\u597d\u7684\u901f\u5ea6\u6027\u80fd\u3002\u5bf9\u7f8e\u56fd\u4e73\u623f\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec Vivim \u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002 Vivim \u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/scott-yjyang/Vivim\u3002|[2401.14168v1](http://arxiv.org/pdf/2401.14168v1)|null|\n", "2401.14159": "|**2024-01-25**|**Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks**|\u624e\u6839 SAM\uff1a\u4e3a\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u7ec4\u88c5\u5f00\u653e\u4e16\u754c\u6a21\u578b|Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et.al.|We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.|\u6211\u4eec\u5f15\u5165\u4e86 Grounded SAM\uff0c\u5b83\u4f7f\u7528 Grounding DINO \u4f5c\u4e3a\u5f00\u653e\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4e0e\u5206\u6bb5\u4efb\u4f55\u6a21\u578b (SAM) \u76f8\u7ed3\u5408\u3002\u8fd9\u79cd\u96c6\u6210\u80fd\u591f\u57fa\u4e8e\u4efb\u610f\u6587\u672c\u8f93\u5165\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u4f55\u533a\u57df\uff0c\u5e76\u4e3a\u8fde\u63a5\u5404\u79cd\u89c6\u89c9\u6a21\u578b\u6253\u5f00\u4e86\u4e00\u6247\u95e8\u3002\u5982\u56fe 1 \u6240\u793a\uff0c\u901a\u8fc7\u4f7f\u7528\u591a\u529f\u80fd\u63a5\u5730 SAM \u7ba1\u9053\u53ef\u4ee5\u5b9e\u73b0\u5e7f\u6cdb\u7684\u89c6\u89c9\u4efb\u52a1\u3002\u4f8b\u5982\uff0c\u4ec5\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u7684\u81ea\u52a8\u6ce8\u91ca\u7ba1\u9053\u53ef\u4ee5\u901a\u8fc7\u5408\u5e76 BLIP \u548c Recognize Anything \u7b49\u6a21\u578b\u6765\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u7a33\u5b9a\u6269\u6563\u53ef\u4ee5\u5b9e\u73b0\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u800c OSX \u7684\u96c6\u6210\u5219\u6709\u52a9\u4e8e\u5feb\u901f\u8fdb\u884c 3D \u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u3002 Grounded SAM \u5728\u5f00\u653e\u8bcd\u6c47\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u7ed3\u5408 Grounding DINO-Base \u548c SAM-Huge \u6a21\u578b\uff0c\u5728 SegInW\uff08\u91ce\u5916\u5206\u5272\uff09\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 48.7 \u7684\u5e73\u5747 AP\u3002|[2401.14159v1](http://arxiv.org/pdf/2401.14159v1)|null|\n", "2401.14136": "|**2024-01-25**|**Expression-aware video inpainting for HMD removal in XR applications**|\u7528\u4e8e\u5728 XR \u5e94\u7528\u7a0b\u5e8f\u4e2d\u79fb\u9664 HMD \u7684\u8868\u60c5\u611f\u77e5\u89c6\u9891\u4fee\u590d|Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr|Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content. However, HMDs present an obstacle to external recording techniques as they block the upper face of the user. This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience. In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs). Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user. The framework and its components ensure the preservation of the user's identity across frames using the reference frame. To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation. Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity. Moreover, the outputs exhibit temporal consistency along the inpainted frames. This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware.|\u5934\u6234\u5f0f\u663e\u793a\u5668 (HMD) \u662f\u89c2\u5bdf\u6269\u5c55\u73b0\u5b9e (XR) \u73af\u5883\u548c\u865a\u62df\u5185\u5bb9\u4e0d\u53ef\u6216\u7f3a\u7684\u8bbe\u5907\u3002\u7136\u800c\uff0c\u5934\u6234\u5f0f\u663e\u793a\u5668\u7ed9\u5916\u90e8\u8bb0\u5f55\u6280\u672f\u5e26\u6765\u4e86\u969c\u788d\uff0c\u56e0\u4e3a\u5b83\u4eec\u6321\u4f4f\u4e86\u7528\u6237\u7684\u4e0a\u8138\u3002\u8fd9\u79cd\u9650\u5236\u6781\u5927\u5730\u5f71\u54cd\u4e86\u793e\u4ea4 XR \u5e94\u7528\u7a0b\u5e8f\uff0c\u7279\u522b\u662f\u7535\u8bdd\u4f1a\u8bae\uff0c\u5176\u4e2d\u9762\u90e8\u7279\u5f81\u548c\u773c\u775b\u6ce8\u89c6\u4fe1\u606f\u5728\u521b\u5efa\u6c89\u6d78\u5f0f\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u7528\u4e8e\u53bb\u9664 HMD \u7684\u8868\u8fbe\u611f\u77e5\u89c6\u9891\u4fee\u590d\u7684\u65b0\u7f51\u7edc\uff08EVI-HRnet\uff09\u3002\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u586b\u5145\u4e86\u6709\u5173\u9762\u90e8\u6807\u5fd7\u548c\u7528\u6237\u7684\u5355\u4e2a\u65e0\u906e\u6321\u53c2\u8003\u56fe\u50cf\u7684\u7f3a\u5931\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u53ca\u5176\u7ec4\u4ef6\u786e\u4fdd\u4f7f\u7528\u53c2\u8003\u6846\u67b6\u8de8\u6846\u67b6\u4fdd\u5b58\u7528\u6237\u7684\u8eab\u4efd\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4fee\u590d\u8f93\u51fa\u7684\u771f\u5b9e\u611f\u6c34\u5e73\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u635f\u5931\u51fd\u6570\u6765\u4fdd\u5b58\u60c5\u611f\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u4ece\u9762\u90e8\u89c6\u9891\u4e2d\u53bb\u9664\u5934\u6234\u5f0f\u663e\u793a\u5668\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u7684\u9762\u90e8\u8868\u60c5\u548c\u8eab\u4efd\u3002\u6b64\u5916\uff0c\u8f93\u51fa\u6cbf\u7740\u4fee\u590d\u5e27\u8868\u73b0\u51fa\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u8fd9\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53bb\u9664 HMD \u906e\u6321\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5177\u6709\u589e\u5f3a\u5404\u79cd\u534f\u4f5c XR \u5e94\u7528\u7a0b\u5e8f\u7684\u6f5c\u529b\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u786c\u4ef6\u3002|[2401.14136v1](http://arxiv.org/pdf/2401.14136v1)|null|\n", "2401.14130": "|**2024-01-25**|**Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease**|\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5 3D MRI \u56fe\u50cf\u9ad8\u6548\u5206\u7c7b|Yihao Lin, Ximeng Li, Yan Zhang, Jinshan Tang|Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy.|\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\uff08AD\uff09\u7684\u65e9\u671f\u8bca\u65ad\u56e0\u5176\u5fae\u5999\u800c\u590d\u6742\u7684\u4e34\u5e8a\u75c7\u72b6\u800c\u6210\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5229\u7528\u56fe\u50cf\u8bc6\u522b\u6280\u672f\u7684\u6df1\u5ea6\u5b66\u4e60\u8f85\u52a9\u533b\u5b66\u8bca\u65ad\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u8bfe\u9898\u3002\u8fd9\u4e9b\u7279\u5f81\u5fc5\u987b\u51c6\u786e\u6355\u6349\u5927\u8111\u89e3\u5256\u7ed3\u6784\u7684\u4e3b\u8981\u53d8\u5316\u3002\u7136\u800c\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u975e\u5e38\u8017\u65f6\u4e14\u6602\u8d35\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684 ResNet \u7f51\u7edc\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u7ed3\u5408\u4e86 3D \u533b\u5b66\u56fe\u50cf\u7684\u540e\u878d\u5408\u7b97\u6cd5\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u91c7\u7528\u7684\u4e8c\u7ef4\u878d\u5408\u7b97\u6cd5\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u5f00\u9500\u3002\u5e76\u4e14\u5f15\u5165\u7684\u6ce8\u610f\u529b\u673a\u5236\u51c6\u786e\u5730\u5bf9\u56fe\u50cf\u4e2d\u7684\u91cd\u8981\u533a\u57df\u8fdb\u884c\u52a0\u6743\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002|[2401.14130v1](http://arxiv.org/pdf/2401.14130v1)|null|\n", "2401.14115": "|**2024-01-25**|**MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver Activity Recognition**|MIFI\uff1a\u7528\u4e8e Roust 3D \u5206\u5fc3\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u7684\u591a\u6444\u50cf\u5934\u529f\u80fd\u96c6\u6210|Jian Kuang, Wenjing Li, Fang Li, Jun Zhang, Zhongcheng Wu|Distracted driver activity recognition plays a critical role in risk aversion-particularly beneficial in intelligent transportation systems. However, most existing methods make use of only the video from a single view and the difficulty-inconsistent issue is neglected. Different from them, in this work, we propose a novel MultI-camera Feature Integration (MIFI) approach for 3D distracted driver activity recognition by jointly modeling the data from different camera views and explicitly re-weighting examples based on their degree of difficulty. Our contributions are two-fold: (1) We propose a simple but effective multi-camera feature integration framework and provide three types of feature fusion techniques. (2) To address the difficulty-inconsistent problem in distracted driver activity recognition, a periodic learning method, named example re-weighting that can jointly learn the easy and hard samples, is presented. The experimental results on the 3MDAD dataset demonstrate that the proposed MIFI can consistently boost performance compared to single-view models.|\u5206\u5fc3\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u5728\u89c4\u907f\u98ce\u9669\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5c24\u5176\u6709\u76ca\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6765\u81ea\u5355\u4e2a\u89c6\u56fe\u7684\u89c6\u9891\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u96be\u5ea6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u4e0e\u5b83\u4eec\u4e0d\u540c\u7684\u662f\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6444\u50cf\u5934\u7279\u5f81\u96c6\u6210\uff08MIFI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6765\u81ea\u4e0d\u540c\u6444\u50cf\u5934\u89c6\u56fe\u7684\u6570\u636e\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u5e76\u6839\u636e\u793a\u4f8b\u7684\u96be\u5ea6\u7a0b\u5ea6\u663e\u5f0f\u5730\u91cd\u65b0\u52a0\u6743\uff0c\u6765\u8bc6\u522b 3D \u5206\u5fc3\u9a7e\u9a76\u5458\u6d3b\u52a8\u3002\u6211\u4eec\u7684\u8d21\u732e\u6709\u4e24\u4e2a\uff1a\uff081\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u591a\u76f8\u673a\u7279\u5f81\u96c6\u6210\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u7279\u5f81\u878d\u5408\u6280\u672f\u3002 (2)\u9488\u5bf9\u5206\u5fc3\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u4e2d\u96be\u5ea6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8054\u5408\u5b66\u4e60\u6613\u6837\u672c\u548c\u96be\u6837\u672c\u7684\u5468\u671f\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5b9e\u4f8b\u91cd\u52a0\u6743\u3002 3MDAD \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5355\u89c6\u56fe\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 MIFI \u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\u3002|[2401.14115v1](http://arxiv.org/pdf/2401.14115v1)|**[link](https://github.com/john828/mifi)**|\n", "2401.14088": "|**2024-01-25**|**Double Trouble? Impact and Detection of Duplicates in Face Image Datasets**|\u53cc\u91cd\u9ebb\u70e6\uff1f\u4eba\u8138\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u91cd\u590d\u9879\u7684\u5f71\u54cd\u548c\u68c0\u6d4b|Torsten Schlett, Christian Rathgeb, Juan Tapia, Christoph Busch|Various face image datasets intended for facial biometrics research were created via web-scraping, i.e. the collection of images publicly available on the internet. This work presents an approach to detect both exactly and nearly identical face image duplicates, using file and image hashes. The approach is extended through the use of face image preprocessing. Additional steps based on face recognition and face image quality assessment models reduce false positives, and facilitate the deduplication of the face images both for intra- and inter-subject duplicate sets. The presented approach is applied to five datasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a cleaned MS-Celeb-1M variant). Duplicates are detected within every dataset, with hundreds to hundreds of thousands of duplicates for all except LFW. Face recognition and quality assessment experiments indicate a minor impact on the results through the duplicate removal. The final deduplication data is publicly available.|\u7528\u4e8e\u9762\u90e8\u751f\u7269\u8bc6\u522b\u7814\u7a76\u7684\u5404\u79cd\u9762\u90e8\u56fe\u50cf\u6570\u636e\u96c6\u662f\u901a\u8fc7\u7f51\u7edc\u6293\u53d6\u521b\u5efa\u7684\uff0c\u5373\u4e92\u8054\u7f51\u4e0a\u516c\u5f00\u63d0\u4f9b\u7684\u56fe\u50cf\u96c6\u5408\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6587\u4ef6\u548c\u56fe\u50cf\u54c8\u5e0c\u6765\u68c0\u6d4b\u5b8c\u5168\u76f8\u540c\u548c\u51e0\u4e4e\u76f8\u540c\u7684\u9762\u90e8\u56fe\u50cf\u526f\u672c\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u4eba\u8138\u56fe\u50cf\u9884\u5904\u7406\u8fdb\u884c\u4e86\u6269\u5c55\u3002\u57fa\u4e8e\u4eba\u8138\u8bc6\u522b\u548c\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u9644\u52a0\u6b65\u9aa4\u51cf\u5c11\u4e86\u8bef\u62a5\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u8c61\u5185\u548c\u5bf9\u8c61\u95f4\u91cd\u590d\u96c6\u7684\u4eba\u8138\u56fe\u50cf\u7684\u91cd\u590d\u6570\u636e\u5220\u9664\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u4e94\u4e2a\u6570\u636e\u96c6\uff0c\u5373 LFW\u3001TinyFace\u3001Adience\u3001CASIA-WebFace \u548c C-MS-Celeb\uff08\u7ecf\u8fc7\u6e05\u7406\u7684 MS-Celeb-1M \u53d8\u4f53\uff09\u3002\u6bcf\u4e2a\u6570\u636e\u96c6\u4e2d\u90fd\u4f1a\u68c0\u6d4b\u5230\u91cd\u590d\u9879\uff0c\u9664\u4e86 LFW \u4e4b\u5916\uff0c\u6240\u6709\u6570\u636e\u96c6\u4e2d\u90fd\u6709\u6570\u767e\u5230\u6570\u5341\u4e07\u4e2a\u91cd\u590d\u9879\u3002\u4eba\u8138\u8bc6\u522b\u548c\u8d28\u91cf\u8bc4\u4f30\u5b9e\u9a8c\u8868\u660e\uff0c\u91cd\u590d\u53bb\u9664\u5bf9\u7ed3\u679c\u5f71\u54cd\u8f83\u5c0f\u3002\u6700\u7ec8\u7684\u91cd\u590d\u6570\u636e\u5220\u9664\u6570\u636e\u662f\u516c\u5f00\u7684\u3002|[2401.14088v1](http://arxiv.org/pdf/2401.14088v1)|**[link](https://github.com/dasec/dataset-duplicates)**|\n", "2401.14074": "|**2024-01-25**|**ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation**|ProCNS\uff1a\u5f31\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6e10\u8fdb\u5f0f\u539f\u578b\u6821\u51c6\u548c\u566a\u58f0\u6291\u5236|Y. Liu, L. Lin, K. K. Y. Wong, X. Tang|Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on three medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods|\u5f31\u76d1\u7763\u5206\u5272\uff08WSS\uff09\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u91c7\u7528\u7a00\u758f\u6ce8\u91ca\u683c\u5f0f\uff08\u4f8b\u5982\u70b9\u3001\u6d82\u9e26\u3001\u5757\u7b49\uff09\u6765\u7f13\u89e3\u6ce8\u91ca\u6210\u672c\u548c\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u5178\u578b\u7684\u65b9\u6cd5\u5c1d\u8bd5\u5229\u7528\u89e3\u5256\u5b66\u548c\u62d3\u6251\u5148\u9a8c\u5c06\u7a00\u758f\u6ce8\u91ca\u76f4\u63a5\u6269\u5c55\u4e3a\u4f2a\u6807\u7b7e\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u533b\u5b66\u56fe\u50cf\u4e2d\u6a21\u7cca\u8fb9\u7f18\u7684\u5173\u6ce8\u4ee5\u53ca\u5bf9\u7a00\u758f\u76d1\u7763\u7684\u63a2\u7d22\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f1a\u5728\u566a\u58f0\u533a\u57df\u4e2d\u751f\u6210\u9519\u8bef\u4e14\u8fc7\u5ea6\u81ea\u4fe1\u7684\u4f2a\u5efa\u8bae\uff0c\u5bfc\u81f4\u7d2f\u79ef\u6a21\u578b\u9519\u8bef\u548c\u6027\u80fd\u4e0b\u964d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 WSS \u65b9\u6cd5\uff0c\u540d\u4e3a ProCNS\uff0c\u5305\u542b\u6839\u636e\u6e10\u8fdb\u5f0f\u539f\u578b\u6821\u51c6\u548c\u566a\u58f0\u6291\u5236\u539f\u7406\u8bbe\u8ba1\u7684\u4e24\u4e2a\u534f\u540c\u6a21\u5757\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u7684\u533a\u57df\u7a7a\u95f4\u4eb2\u548c\u529b\uff08PRSA\uff09\u635f\u5931\uff0c\u4ee5\u6700\u5927\u5316\u7a7a\u95f4\u548c\u8bed\u4e49\u5143\u7d20\u4e4b\u95f4\u7684\u6210\u5bf9\u4eb2\u548c\u529b\uff0c\u4e3a\u6211\u4eec\u611f\u5174\u8da3\u7684\u6a21\u578b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6307\u5bfc\u3002\u76f8\u4f3c\u5ea6\u662f\u4ece\u8f93\u5165\u56fe\u50cf\u548c\u539f\u578b\u6539\u8fdb\u7684\u9884\u6d4b\u4e2d\u5f97\u51fa\u7684\u3002\u540c\u65f6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u566a\u58f0\u611f\u77e5\u548c\u63a9\u853d\uff08ANPM\uff09\u6a21\u5757\u6765\u83b7\u5f97\u66f4\u4e30\u5bcc\u548c\u66f4\u5177\u4ee3\u8868\u6027\u7684\u539f\u578b\u8868\u793a\uff0c\u5b83\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u548c\u63a9\u853d\u4f2a\u63d0\u6848\u4e2d\u7684\u566a\u58f0\u533a\u57df\uff0c\u51cf\u5c11\u539f\u578b\u8ba1\u7b97\u671f\u95f4\u6f5c\u5728\u7684\u9519\u8bef\u5e72\u6270\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a ANPM \u8bc6\u522b\u7684\u566a\u58f0\u533a\u57df\u751f\u6210\u4e13\u95e8\u7684\u8f6f\u4f2a\u6807\u7b7e\uff0c\u63d0\u4f9b\u8865\u5145\u76d1\u7763\u3002\u5bf9\u6d89\u53ca\u4e0d\u540c\u6a21\u6001\u7684\u4e09\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u7740\u4f18\u4e8e\u4ee3\u8868\u6027\u7684\u6700\u5148\u8fdb\u65b9\u6cd5|[2401.14074v1](http://arxiv.org/pdf/2401.14074v1)|**[link](https://github.com/lyxdlii/procns)**|\n", "2401.14034": "|**2024-01-25**|**Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition**|\u7528\u4e8e\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u65f6\u7a7a\u7279\u5f81\u4e30\u5bcc\u548c\u4fdd\u771f\u5ea6\u7f51\u7edc|Chuankun Li, Shuai Li, Yanbo Gao, Ping Chen, Jian Li, Wanqing Li|Unsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods suffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability. To address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is first investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the same manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to produce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve this problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-FEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature transformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent unit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to generate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton sequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations. Experimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning methods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based action recognition.|\u57fa\u4e8e\u65e0\u76d1\u7763\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u6700\u8fd1\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u5c55\u3002\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u56e0\u6b64\u4f7f\u7528\u5c0f\u578b\u7f51\u7edc\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8868\u793a\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9996\u5148\u7814\u7a76\u4e86\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u80cc\u540e\u7684\u8fc7\u62df\u5408\u673a\u5236\u3002\u53ef\u4ee5\u770b\u51fa\uff0c\u9aa8\u67b6\u5df2\u7ecf\u662f\u4e00\u4e2a\u76f8\u5bf9\u9ad8\u7ea7\u548c\u4f4e\u7ef4\u7684\u7279\u5f81\uff0c\u4f46\u4e0e\u52a8\u4f5c\u8bc6\u522b\u7684\u7279\u5f81\u4e0d\u5728\u540c\u4e00\u6d41\u5f62\u4e2d\u3002\u7b80\u5355\u5730\u5e94\u7528\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u4f1a\u4ea7\u751f\u533a\u5206\u4e0d\u540c\u6837\u672c\u800c\u4e0d\u662f\u52a8\u4f5c\u7c7b\u522b\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5bfc\u81f4\u8fc7\u62df\u5408\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65f6\u7a7a\u7279\u5f81\u4e30\u5bcc\u548c\u4fdd\u771f\u5ea6\u6846\u67b6\uff08U-FEFP\uff09\u6765\u751f\u6210\u5305\u542b\u9aa8\u67b6\u5e8f\u5217\u6240\u6709\u4fe1\u606f\u7684\u4e30\u5bcc\u5206\u5e03\u5f0f\u7279\u5f81\u3002\u4f7f\u7528\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u56fe\u5377\u79ef\u95e8\u5faa\u73af\u5355\u5143\u7f51\u7edc\u4f5c\u4e3a\u57fa\u672c\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u5f00\u53d1\u4e86\u65f6\u7a7a\u7279\u5f81\u8f6c\u6362\u5b50\u7f51\u7edc\u3002\u57fa\u4e8e\u65e0\u76d1\u7763\u7684 Bootstrap Your Own Latent \u5b66\u4e60\u7528\u4e8e\u751f\u6210\u4e30\u5bcc\u7684\u5206\u5e03\u5f0f\u7279\u5f81\uff0c\u57fa\u4e8e\u65e0\u76d1\u7763\u501f\u53e3\u4efb\u52a1\u7684\u5b66\u4e60\u7528\u4e8e\u4fdd\u7559\u9aa8\u67b6\u5e8f\u5217\u7684\u4fe1\u606f\u3002\u8fd9\u4e24\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f5c\u4e3a U-FEFP \u8fdb\u884c\u534f\u4f5c\uff0c\u4ee5\u4ea7\u751f\u7a33\u5065\u4e14\u6709\u533a\u522b\u7684\u8868\u793a\u3002\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\uff08\u5373 NTU-RGB+D-60\u3001NTU-RGB+D-120 \u548c PKU-MMD \u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 U-FEFP \u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002\u827a\u672f\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002 t-SNE \u63d2\u56fe\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86 U-FEFP \u53ef\u4ee5\u4e3a\u57fa\u4e8e\u65e0\u76d1\u7763\u9aa8\u67b6\u7684\u52a8\u4f5c\u8bc6\u522b\u5b66\u4e60\u66f4\u591a\u5224\u522b\u6027\u7279\u5f81\u3002|[2401.14034v1](http://arxiv.org/pdf/2401.14034v1)|null|\n", "2401.14024": "|**2024-01-25**|**PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction in High-definition Maps**|PLCNet\uff1a\u7528\u4e8e\u9ad8\u6e05\u5730\u56fe\u4e2d\u81ea\u52a8\u8f66\u9053\u6821\u6b63\u7684\u5206\u7247\u8f66\u9053\u6821\u6b63\u7f51\u7edc|Haiyang Peng, Yi Zhan, Benkang Wang, Hongtao Zhang|In High-definition (HD) maps, lane elements constitute the majority of components and demand stringent localization requirements to ensure safe vehicle navigation. Vision lane detection with LiDAR position assignment is a prevalent method to acquire initial lanes for HD maps. However, due to incorrect vision detection and coarse camera-LiDAR calibration, initial lanes may deviate from their true positions within an uncertain range. To mitigate the need for manual lane correction, we propose a patch-wise lane correction network (PLCNet) to automatically correct the positions of initial lane points in local LiDAR images that are transformed from point clouds. PLCNet first extracts multi-scale image features and crops patch (ROI) features centered at each initial lane point. By applying ROIAlign, the fix-sized ROI features are flattened into 1D features. Then, a 1D lane attention module is devised to compute instance-level lane features with adaptive weights. Finally, lane correction offsets are inferred by a multi-layer perceptron and used to correct the initial lane positions. Considering practical applications, our automatic method supports merging local corrected lanes into global corrected lanes. Through extensive experiments on a self-built dataset, we demonstrate that PLCNet achieves fast and effective initial lane correction.|\u5728\u9ad8\u6e05\uff08HD\uff09\u5730\u56fe\u4e2d\uff0c\u8f66\u9053\u5143\u7d20\u6784\u6210\u4e86\u5927\u90e8\u5206\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u4e25\u683c\u7684\u5b9a\u4f4d\u8981\u6c42\u4ee5\u786e\u4fdd\u8f66\u8f86\u5bfc\u822a\u5b89\u5168\u3002\u4f7f\u7528 LiDAR \u4f4d\u7f6e\u5206\u914d\u7684\u89c6\u89c9\u8f66\u9053\u68c0\u6d4b\u662f\u83b7\u53d6\u9ad8\u6e05\u5730\u56fe\u521d\u59cb\u8f66\u9053\u7684\u5e38\u7528\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e0d\u6b63\u786e\u7684\u89c6\u89c9\u68c0\u6d4b\u548c\u7c97\u7565\u7684\u76f8\u673a\u6fc0\u5149\u96f7\u8fbe\u6821\u51c6\uff0c\u521d\u59cb\u8f66\u9053\u53ef\u80fd\u4f1a\u5728\u4e0d\u786e\u5b9a\u7684\u8303\u56f4\u5185\u504f\u79bb\u5176\u771f\u5b9e\u4f4d\u7f6e\u3002\u4e3a\u4e86\u51cf\u8f7b\u624b\u52a8\u8f66\u9053\u6821\u6b63\u7684\u9700\u8981\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5757\u8f66\u9053\u6821\u6b63\u7f51\u7edc\uff08PLCNet\uff09\u6765\u81ea\u52a8\u6821\u6b63\u4ece\u70b9\u4e91\u8f6c\u6362\u7684\u672c\u5730 LiDAR \u56fe\u50cf\u4e2d\u521d\u59cb\u8f66\u9053\u70b9\u7684\u4f4d\u7f6e\u3002 PLCNet \u9996\u5148\u63d0\u53d6\u4ee5\u6bcf\u4e2a\u521d\u59cb\u8f66\u9053\u70b9\u4e3a\u4e2d\u5fc3\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u548c\u88c1\u526a\u5757 (ROI) \u7279\u5f81\u3002\u901a\u8fc7\u5e94\u7528 ROIAlign\uff0c\u56fa\u5b9a\u5927\u5c0f\u7684 ROI \u7279\u5f81\u88ab\u5c55\u5e73\u4e3a\u4e00\u7ef4\u7279\u5f81\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e00\u7ef4\u8f66\u9053\u6ce8\u610f\u6a21\u5757\u6765\u8ba1\u7b97\u5177\u6709\u81ea\u9002\u5e94\u6743\u91cd\u7684\u5b9e\u4f8b\u7ea7\u8f66\u9053\u7279\u5f81\u3002\u6700\u540e\uff0c\u7531\u591a\u5c42\u611f\u77e5\u5668\u63a8\u65ad\u8f66\u9053\u6821\u6b63\u504f\u79fb\u5e76\u7528\u4e8e\u6821\u6b63\u521d\u59cb\u8f66\u9053\u4f4d\u7f6e\u3002\u8003\u8651\u5230\u5b9e\u9645\u5e94\u7528\uff0c\u6211\u4eec\u7684\u81ea\u52a8\u65b9\u6cd5\u652f\u6301\u5c06\u5c40\u90e8\u6821\u6b63\u8f66\u9053\u5408\u5e76\u5230\u5168\u5c40\u6821\u6b63\u8f66\u9053\u4e2d\u3002\u901a\u8fc7\u5bf9\u81ea\u5efa\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 PLCNet \u5b9e\u73b0\u4e86\u5feb\u901f\u6709\u6548\u7684\u521d\u59cb\u8f66\u9053\u6821\u6b63\u3002|[2401.14024v1](http://arxiv.org/pdf/2401.14024v1)|null|\n", "2401.13998": "|**2024-01-25**|**WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification**|WAL-Net\uff1a\u7528\u4e8e\u9888\u52a8\u8109\u6591\u5757\u5206\u7c7b\u7684\u5f31\u76d1\u7763\u8f85\u52a9\u4efb\u52a1\u5b66\u4e60\u7f51\u7edc|Haitao Gan, Lingchao Fu, Ran Zhou, Weiyan Gan, Furong Wang, Xiaoyan Wu, Zhi Yang, Zhongwei Huang|The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. This paper proposes a novel weakly supervised auxiliary task learning network model (WAL-Net) to explore the interdependence between carotid plaque classification and segmentation tasks. The plaque classification task is primary task, while the plaque segmentation task serves as an auxiliary task, providing valuable information to enhance the performance of the primary task. Weakly supervised learning is adopted in the auxiliary task to completely break away from the dependence on segmentation annotations. Experiments and evaluations are conducted on a dataset comprising 1270 carotid plaque ultrasound images from Wuhan University Zhongnan Hospital. Results indicate that the proposed method achieved an approximately 1.3% improvement in carotid plaque classification accuracy compared to the baseline network. Specifically, the accuracy of mixed-echoic plaques classification increased by approximately 3.3%, demonstrating the effectiveness of our approach.|\u9888\u52a8\u8109\u8d85\u58f0\u56fe\u50cf\u7684\u5206\u7c7b\u662f\u8bca\u65ad\u9888\u52a8\u8109\u6591\u5757\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u5bf9\u4e8e\u9884\u6d4b\u4e2d\u98ce\u98ce\u9669\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u610f\u4e49\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u6591\u5757\u5206\u5272\u4f5c\u4e3a\u5206\u7c7b\u7684\u8f85\u52a9\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u63d0\u9ad8\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u83b7\u53d6\u5927\u91cf\u96be\u4ee5\u83b7\u53d6\u7684\u5206\u5272\u6ce8\u91ca\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u8f85\u52a9\u4efb\u52a1\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\uff08WAL-Net\uff09\u6765\u63a2\u7d22\u9888\u52a8\u8109\u6591\u5757\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u6591\u5757\u5206\u7c7b\u4efb\u52a1\u662f\u4e3b\u8981\u4efb\u52a1\uff0c\u800c\u6591\u5757\u5206\u5272\u4efb\u52a1\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u4e3a\u63d0\u9ad8\u4e3b\u8981\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u8f85\u52a9\u4efb\u52a1\u4e2d\u91c7\u7528\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u5f7b\u5e95\u6446\u8131\u5bf9\u5206\u5272\u6807\u6ce8\u7684\u4f9d\u8d56\u3002\u5728\u6b66\u6c49\u5927\u5b66\u4e2d\u5357\u533b\u9662\u5305\u542b 1270 \u5f20\u9888\u52a8\u8109\u6591\u5757\u8d85\u58f0\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u548c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u7f51\u7edc\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9888\u52a8\u8109\u6591\u5757\u5206\u7c7b\u51c6\u786e\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u7ea6 1.3%\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6df7\u5408\u56de\u58f0\u6591\u5757\u5206\u7c7b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86\u7ea6 3.3%\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.13998v1](http://arxiv.org/pdf/2401.13998v1)|null|\n", "2401.13990": "|**2024-01-25**|**Deep Learning Innovations in Diagnosing Diabetic Retinopathy: The Potential of Transfer Learning and the DiaCNN Model**|\u8bca\u65ad\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7684\u6df1\u5ea6\u5b66\u4e60\u521b\u65b0\uff1a\u8fc1\u79fb\u5b66\u4e60\u548c DiaCNN \u6a21\u578b\u7684\u6f5c\u529b|Mohamed R. Shoaib, Heba M. Emara, Jun Zhao, Walid El-Shafai, Naglaa F. Soliman, Ahmed S. Mubarak, Osama A. Omer, Fathi E. Abd El-Samie, Hamada Esmaiel|Diabetic retinopathy (DR) is a significant cause of vision impairment, emphasizing the critical need for early detection and timely intervention to avert visual deterioration. Diagnosing DR is inherently complex, as it necessitates the meticulous examination of intricate retinal images by experienced specialists. This makes the early diagnosis of DR essential for effective treatment and the prevention of eventual blindness. Traditional diagnostic methods, relying on human interpretation of these medical images, face challenges in terms of accuracy and efficiency. In the present research, we introduce a novel method that offers superior precision in DR diagnosis, compared to these traditional methods, by employing advanced deep learning techniques. Central to this approach is the concept of transfer learning. This entails using pre-existing, well-established models, specifically InceptionResNetv2 and Inceptionv3, to extract features and fine-tune select layers to cater to the unique requirements of this specific diagnostic task. Concurrently, we also present a newly devised model, DiaCNN, which is tailored for the classification of eye diseases. To validate the efficacy of the proposed methodology, we leveraged the Ocular Disease Intelligent Recognition (ODIR) dataset, which comprises eight different eye disease categories. The results were promising. The InceptionResNetv2 model, incorporating transfer learning, registered an impressive 97.5% accuracy in both the training and testing phases. Its counterpart, the Inceptionv3 model, achieved an even more commendable 99.7% accuracy during training, and 97.5% during testing. Remarkably, the DiaCNN model showcased unparalleled precision, achieving 100% accuracy in training and 98.3\\% in testing.|\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u89c6\u529b\u635f\u5bb3\u7684\u4e00\u4e2a\u91cd\u8981\u539f\u56e0\uff0c\u5f3a\u8c03\u65e9\u671f\u53d1\u73b0\u548c\u53ca\u65f6\u5e72\u9884\u4ee5\u907f\u514d\u89c6\u529b\u6076\u5316\u7684\u8feb\u5207\u9700\u8981\u3002\u8bca\u65ad DR \u672c\u8d28\u4e0a\u662f\u590d\u6742\u7684\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4e13\u5bb6\u5bf9\u590d\u6742\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u8fdb\u884c\u7ec6\u81f4\u7684\u68c0\u67e5\u3002\u8fd9\u4f7f\u5f97 DR \u7684\u65e9\u671f\u8bca\u65ad\u5bf9\u4e8e\u6709\u6548\u6cbb\u7597\u548c\u9884\u9632\u6700\u7ec8\u5931\u660e\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u5bf9\u8fd9\u4e9b\u533b\u5b66\u56fe\u50cf\u7684\u89e3\u91ca\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u4e0e\u8fd9\u4e9b\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728 DR \u8bca\u65ad\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u8fc1\u79fb\u5b66\u4e60\u7684\u6982\u5ff5\u3002\u8fd9\u9700\u8981\u4f7f\u7528\u9884\u5148\u5b58\u5728\u7684\u3001\u5b8c\u5584\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f InceptionResNetv2 \u548c Inceptionv3\uff0c\u6765\u63d0\u53d6\u7279\u5f81\u5e76\u5fae\u8c03\u9009\u62e9\u7684\u5c42\uff0c\u4ee5\u6ee1\u8db3\u6b64\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u7684\u72ec\u7279\u8981\u6c42\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u8bbe\u8ba1\u7684\u6a21\u578bDiaCNN\uff0c\u5b83\u662f\u4e3a\u773c\u90e8\u75be\u75c5\u7684\u5206\u7c7b\u91cf\u8eab\u5b9a\u5236\u7684\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5229\u7528\u4e86\u773c\u90e8\u75be\u75c5\u667a\u80fd\u8bc6\u522b\uff08ODIR\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u516b\u79cd\u4e0d\u540c\u7684\u773c\u90e8\u75be\u75c5\u7c7b\u522b\u3002\u7ed3\u679c\u662f\u6709\u5e0c\u671b\u7684\u3002 InceptionResNetv2 \u6a21\u578b\u7ed3\u5408\u4e86\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u7684\u51c6\u786e\u7387\u8fbe\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 97.5%\u3002\u4e0e\u4e4b\u5bf9\u5e94\u7684 Inceptionv3 \u6a21\u578b\u5728\u8bad\u7ec3\u671f\u95f4\u8fbe\u5230\u4e86 99.7% \u7684\u51c6\u786e\u7387\uff0c\u5728\u6d4b\u8bd5\u671f\u95f4\u8fbe\u5230\u4e86 97.5% \u7684\u51c6\u786e\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDiaCNN \u6a21\u578b\u5c55\u73b0\u4e86\u65e0\u4e0e\u4f26\u6bd4\u7684\u7cbe\u5ea6\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe\u5230 100%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u5230 98.3%\u3002|[2401.13990v1](http://arxiv.org/pdf/2401.13990v1)|null|\n", "2401.13974": "|**2024-01-25**|**BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models**|BootPIG\uff1a\u5728\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u5f15\u5bfc\u96f6\u6837\u672c\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u529f\u80fd|Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik|Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts.|\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u5fe0\u5b9e\u9075\u5faa\u8f93\u5165\u63d0\u793a\u7684\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u96be\u4ee5\u7f6e\u4fe1\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u4f7f\u7528\u6587\u5b57\u6765\u63cf\u8ff0\u6240\u9700\u6982\u5ff5\u7684\u8981\u6c42\u63d0\u4f9b\u4e86\u5bf9\u6240\u751f\u6210\u6982\u5ff5\u7684\u5916\u89c2\u7684\u6709\u9650\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5728\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u542f\u7528\u4e2a\u6027\u5316\u529f\u80fd\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u7f3a\u70b9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff08BootPIG\uff09\uff0c\u5b83\u5141\u8bb8\u7528\u6237\u63d0\u4f9b\u5bf9\u8c61\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u4ee5\u6307\u5bfc\u751f\u6210\u56fe\u50cf\u4e2d\u6982\u5ff5\u7684\u51fa\u73b0\u3002\u6240\u63d0\u51fa\u7684 BootPIG \u67b6\u6784\u5bf9\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4e86\u6700\u5c0f\u7684\u4fee\u6539\uff0c\u5e76\u5229\u7528\u5355\u72ec\u7684 UNet \u6a21\u578b\u6765\u5f15\u5bfc\u5404\u4ee3\u4eba\u83b7\u5f97\u6240\u9700\u7684\u5916\u89c2\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5141\u8bb8\u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3001LLM \u804a\u5929\u4ee3\u7406\u548c\u56fe\u50cf\u5206\u5272\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u6765\u5f15\u5bfc BootPIG \u67b6\u6784\u4e2d\u7684\u4e2a\u6027\u5316\u529f\u80fd\u3002\u4e0e\u9700\u8981\u51e0\u5929\u9884\u8bad\u7ec3\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cBootPIG \u67b6\u6784\u53ef\u4ee5\u5728\u5927\u7ea6 1 \u5c0f\u65f6\u5185\u5b8c\u6210\u8bad\u7ec3\u3002 DreamBooth \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBootPIG \u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u6d4b\u8bd5\u65f6\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86 BootPIG \u751f\u6210\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u504f\u597d\uff0c\u65e0\u8bba\u662f\u5728\u4fdd\u6301\u53c2\u8003\u5bf9\u8c61\u5916\u89c2\u7684\u4fdd\u771f\u5ea6\u8fd8\u662f\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u65b9\u9762\u3002|[2401.13974v1](http://arxiv.org/pdf/2401.13974v1)|null|\n", "2401.13965": "|**2024-01-25**|**Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization**|\u6539\u8fdb\u4f2a\u6807\u7b7e\u5e76\u589e\u5f3a\u534a\u76d1\u7763\u57df\u6cdb\u5316\u7684\u9c81\u68d2\u6027|Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan|Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: https://github.com/Adnan-Khan7/UPLM|\u9664\u4e86\u5b9e\u73b0\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u4e4b\u5916\uff0c\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u8fd8\u5e94\u8be5\u901a\u8fc7\u5229\u7528\u6709\u9650\u7684\u6807\u7b7e\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u3002\u6211\u4eec\u7814\u7a76\u534a\u76d1\u7763\u57df\u6cdb\u5316\uff08SSDG\uff09\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u5316\u533b\u7597\u4fdd\u5065\u7b49\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u7ed9\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\u4ec5\u90e8\u5206\u6807\u8bb0\u65f6\uff0cSSDG \u9700\u8981\u5b66\u4e60\u8de8\u57df\u53ef\u63a8\u5e7f\u6a21\u578b\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cDG \u65b9\u6cd5\u5728 SSDG \u8bbe\u7f6e\u4e2d\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\u3002\u4e0e\u5b8c\u5168\u76d1\u7763\u5b66\u4e60\u76f8\u6bd4\uff0c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u663e\u793a\u51fa\u6539\u8fdb\u4f46\u4ecd\u7136\u8f83\u5dee\u7684\u7ed3\u679c\u3002\u6027\u80fd\u6700\u4f73\u7684\u57fa\u4e8e SSL \u7684 SSDG \u65b9\u6cd5\u9762\u4e34\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u5728\u591a\u4e2a\u57df\u8f6c\u6362\u4e0b\u9009\u62e9\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\uff0c\u5e76\u51cf\u5c11\u5728\u6709\u9650\u6807\u7b7e\u4e0b\u5bf9\u6e90\u57df\u7684\u8fc7\u5ea6\u62df\u5408\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684 SSDG \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f2a\u6807\u7b7e\u6a21\u578b\u5e73\u5747\uff08UPLM\uff09\u3002\u6211\u4eec\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f2a\u6807\u8bb0\uff08UPL\uff09\u4f7f\u7528\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u4f2a\u6807\u8bb0\u9009\u62e9\uff0c\u89e3\u51b3\u591a\u6e90\u672a\u6807\u8bb0\u6570\u636e\u4e0b\u7684\u4e0d\u826f\u6a21\u578b\u6821\u51c6\u95ee\u9898\u3002 UPL \u6280\u672f\u901a\u8fc7\u6211\u4eec\u65b0\u9896\u7684\u6a21\u578b\u5e73\u5747 (MA) \u7b56\u7565\u5f97\u5230\u589e\u5f3a\uff0c\u53ef\u4ee5\u51cf\u8f7b\u5bf9\u6807\u7b7e\u6709\u9650\u7684\u6e90\u57df\u7684\u8fc7\u5ea6\u62df\u5408\u3002\u5bf9\u5173\u952e\u4ee3\u8868\u6027 DG \u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u9009\u62e9\u7684\u6807\u8bb0\u6570\u636e\u79cd\u5b50\u53ef\u5728 GitHub \u4e0a\u627e\u5230\uff1ahttps://github.com/Adnan-Khan7/UPLM|[2401.13965v1](http://arxiv.org/pdf/2401.13965v1)|**[link](https://github.com/adnan-khan7/uplm)**|\n", "2401.13961": "|**2024-01-25**|**TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images**|TriSAM\uff1a\u7528\u4e8e VEM \u56fe\u50cf\u4e2d\u96f6\u6b21\u76ae\u8d28\u8840\u7ba1\u5206\u5272\u7684\u4e09\u5e73\u9762 SAM|Jia Wan, Wanhua Li, Atmadeep Banerjee, Jason Ken Adhinarta, Evelina Sjostedt, Jingpeng Wu, Jeff Lichtman, Hanspeter Pfister, Donglai Wei|In this paper, we address a significant gap in the field of neuroimaging by introducing the largest-to-date public benchmark, BvEM, designed specifically for cortical blood vessel segmentation in Volume Electron Microscopy (VEM) images. The intricate relationship between cerebral blood vessels and neural function underscores the vital role of vascular analysis in understanding brain health. While imaging techniques at macro and mesoscales have garnered substantial attention and resources, the microscale VEM imaging, capable of revealing intricate vascular details, has lacked the necessary benchmarking infrastructure. As researchers delve deeper into the microscale intricacies of cerebral vasculature, our BvEM benchmark represents a critical step toward unraveling the mysteries of neurovascular coupling and its impact on brain function and pathology. The BvEM dataset is based on VEM image volumes from three mammal species: adult mouse, macaque, and human. We standardized the resolution, addressed imaging variations, and meticulously annotated blood vessels through semi-automatic, manual, and quality control processes, ensuring high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical blood vessel segmentation method named TriSAM, which leverages the powerful segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to 3D volume segmentation, TriSAM employs a multi-seed tracking framework, leveraging the reliability of certain image planes for tracking while using others to identify potential turning points. This approach, consisting of Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively achieves long-term 3D blood vessel segmentation without model training or fine-tuning. Experimental results show that TriSAM achieved superior performances on the BvEM benchmark across three species.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5171\u57fa\u51c6 BvEM \u6765\u89e3\u51b3\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u7684\u91cd\u5927\u5dee\u8ddd\uff0cBvEM \u4e13\u4e3a\u4f53\u79ef\u7535\u5b50\u663e\u5fae\u955c (VEM) \u56fe\u50cf\u4e2d\u7684\u76ae\u8d28\u8840\u7ba1\u5206\u5272\u800c\u8bbe\u8ba1\u3002\u8111\u8840\u7ba1\u548c\u795e\u7ecf\u529f\u80fd\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u5f3a\u8c03\u4e86\u8840\u7ba1\u5206\u6790\u5728\u4e86\u89e3\u5927\u8111\u5065\u5eb7\u65b9\u9762\u7684\u91cd\u8981\u4f5c\u7528\u3002\u867d\u7136\u5b8f\u89c2\u548c\u4e2d\u5c3a\u5ea6\u6210\u50cf\u6280\u672f\u5df2\u7ecf\u83b7\u5f97\u4e86\u5927\u91cf\u5173\u6ce8\u548c\u8d44\u6e90\uff0c\u4f46\u80fd\u591f\u63ed\u793a\u590d\u6742\u8840\u7ba1\u7ec6\u8282\u7684\u5fae\u5c3a\u5ea6 VEM \u6210\u50cf\u5374\u7f3a\u4e4f\u5fc5\u8981\u7684\u57fa\u51c6\u57fa\u7840\u8bbe\u65bd\u3002\u968f\u7740\u7814\u7a76\u4eba\u5458\u6df1\u5165\u7814\u7a76\u8111\u8840\u7ba1\u7cfb\u7edf\u7684\u5fae\u89c2\u590d\u6742\u6027\uff0c\u6211\u4eec\u7684 BvEM \u57fa\u51c6\u4ee3\u8868\u4e86\u63ed\u5f00\u795e\u7ecf\u8840\u7ba1\u8026\u5408\u4e4b\u8c1c\u53ca\u5176\u5bf9\u5927\u8111\u529f\u80fd\u548c\u75c5\u7406\u5b66\u5f71\u54cd\u7684\u5173\u952e\u4e00\u6b65\u3002 BvEM \u6570\u636e\u96c6\u57fa\u4e8e\u6765\u81ea\u4e09\u79cd\u54fa\u4e73\u52a8\u7269\u7684 VEM \u56fe\u50cf\u5377\uff1a\u6210\u5e74\u5c0f\u9f20\u3001\u7315\u7334\u548c\u4eba\u7c7b\u3002\u6211\u4eec\u901a\u8fc7\u534a\u81ea\u52a8\u3001\u624b\u52a8\u548c\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u6807\u51c6\u5316\u4e86\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86\u6210\u50cf\u53d8\u5316\uff0c\u5e76\u4ed4\u7ec6\u6ce8\u91ca\u4e86\u8840\u7ba1\uff0c\u786e\u4fdd\u4e86\u9ad8\u8d28\u91cf\u7684 3D \u5206\u5272\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a TriSAM \u7684\u96f6\u6837\u672c\u76ae\u8d28\u8840\u7ba1\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5f3a\u5927\u7684\u5206\u5272\u6a21\u578b SAM \u8fdb\u884c 3D \u5206\u5272\u3002\u4e3a\u4e86\u5c06 SAM \u4ece 2D \u5206\u5272\u63d0\u5347\u5230 3D \u4f53\u79ef\u5206\u5272\uff0cTriSAM \u91c7\u7528\u4e86\u591a\u79cd\u5b50\u8ddf\u8e2a\u6846\u67b6\uff0c\u5229\u7528\u67d0\u4e9b\u56fe\u50cf\u5e73\u9762\u7684\u53ef\u9760\u6027\u8fdb\u884c\u8ddf\u8e2a\uff0c\u540c\u65f6\u4f7f\u7528\u5176\u4ed6\u56fe\u50cf\u5e73\u9762\u6765\u8bc6\u522b\u6f5c\u5728\u7684\u8f6c\u6298\u70b9\u3002\u8be5\u65b9\u6cd5\u7531\u4e09\u5e73\u9762\u9009\u62e9\u3001\u57fa\u4e8e SAM \u7684\u8ddf\u8e2a\u548c\u9012\u5f52\u91cd\u5b9a\u5411\u7ec4\u6210\uff0c\u53ef\u6709\u6548\u5b9e\u73b0\u957f\u671f 3D \u8840\u7ba1\u5206\u5272\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTriSAM \u5728\u4e09\u4e2a\u7269\u79cd\u7684 BvEM \u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002|[2401.13961v1](http://arxiv.org/pdf/2401.13961v1)|null|\n", "2401.13956": "|**2024-01-25**|**A New Image Quality Database for Multiple Industrial Processes**|\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u6d41\u7a0b\u7684\u65b0\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93|Xuanchao Ma, Zehan Wu, Hongyan Liu, Chengxu Zhou, Ke Gu|Recent years have witnessed a broader range of applications of image processing technologies in multiple industrial processes, such as smoke detection, security monitoring, and workpiece inspection. Different kinds of distortion types and levels must be introduced into an image during the processes of acquisition, compression, transmission, storage, and display, which might heavily degrade the image quality and thus strongly reduce the final display effect and clarity. To verify the reliability of existing image quality assessment methods, we establish a new industrial process image database (IPID), which contains 3000 distorted images generated by applying different levels of distortion types to each of the 50 source images. We conduct the subjective test on the aforementioned 3000 images to collect their subjective quality ratings in a well-suited laboratory environment. Finally, we perform comparison experiments on IPID database to investigate the performance of some objective image quality assessment algorithms. The experimental results show that the state-of-the-art image quality assessment methods have difficulty in predicting the quality of images that contain multiple distortion types.|\u8fd1\u5e74\u6765\uff0c\u56fe\u50cf\u5904\u7406\u6280\u672f\u5728\u70df\u96fe\u68c0\u6d4b\u3001\u5b89\u5168\u76d1\u63a7\u3001\u5de5\u4ef6\u68c0\u6d4b\u7b49\u591a\u4e2a\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u5f97\u5230\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u56fe\u50cf\u5728\u91c7\u96c6\u3001\u538b\u7f29\u3001\u4f20\u8f93\u3001\u5b58\u50a8\u3001\u663e\u793a\u7b49\u8fc7\u7a0b\u4e2d\u5fc5\u7136\u4f1a\u5f15\u5165\u4e0d\u540c\u7c7b\u578b\u548c\u7a0b\u5ea6\u7684\u7578\u53d8\uff0c\u8fd9\u53ef\u80fd\u4f1a\u4e25\u91cd\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u4e25\u91cd\u964d\u4f4e\u6700\u7ec8\u7684\u663e\u793a\u6548\u679c\u548c\u6e05\u6670\u5ea6\u3002\u4e3a\u4e86\u9a8c\u8bc1\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u5de5\u4e1a\u8fc7\u7a0b\u56fe\u50cf\u6570\u636e\u5e93\uff08IPID\uff09\uff0c\u5176\u4e2d\u5305\u542b\u901a\u8fc7\u5bf9 50 \u4e2a\u6e90\u56fe\u50cf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5e94\u7528\u4e0d\u540c\u7ea7\u522b\u7684\u5931\u771f\u7c7b\u578b\u800c\u751f\u6210\u7684 3000 \u4e2a\u5931\u771f\u56fe\u50cf\u3002\u6211\u4eec\u5728\u5408\u9002\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u5bf9\u4e0a\u8ff0 3000 \u5f20\u56fe\u50cf\u8fdb\u884c\u4e3b\u89c2\u6d4b\u8bd5\uff0c\u6536\u96c6\u5176\u4e3b\u89c2\u8d28\u91cf\u8bc4\u7ea7\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728IPID\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4ee5\u7814\u7a76\u4e00\u4e9b\u5ba2\u89c2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u9884\u6d4b\u5305\u542b\u591a\u79cd\u5931\u771f\u7c7b\u578b\u7684\u56fe\u50cf\u7684\u8d28\u91cf\u3002|[2401.13956v1](http://arxiv.org/pdf/2401.13956v1)|null|\n", "2401.13950": "|**2024-01-25**|**AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking**|AM-SORT\uff1a\u5177\u6709\u5386\u53f2\u8f68\u8ff9\u5d4c\u5165\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u591a\u5bf9\u8c61\u8ddf\u8e2a|Vitaliy Kim, Gunho Jung, Seong-Whan Lee|Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions.|\u8bb8\u591a\u591a\u76ee\u6807\u8ddf\u8e2a (MOT) \u65b9\u6cd5\u91c7\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u5047\u8bbe\u901f\u5ea6\u6052\u5b9a\u4e14\u6ee4\u6ce2\u566a\u58f0\u5448\u9ad8\u65af\u5206\u5e03\u3002\u8fd9\u4e9b\u5047\u8bbe\u4f7f\u5f97\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u8ddf\u8e2a\u5668\u5728\u7ebf\u6027\u8fd0\u52a8\u573a\u666f\u4e2d\u6709\u6548\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u975e\u7ebf\u6027\u8fd0\u52a8\u548c\u906e\u6321\u7684\u573a\u666f\u4e2d\u4f30\u8ba1\u672a\u6765\u5bf9\u8c61\u4f4d\u7f6e\u65f6\uff0c\u8fd9\u4e9b\u7ebf\u6027\u5047\u8bbe\u662f\u4e00\u4e2a\u5173\u952e\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u7684 MOT \u65b9\u6cd5\uff0c\u5177\u6709\u81ea\u9002\u5e94\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u79f0\u4e3a AM-SORT\uff0c\u5b83\u9002\u5408\u4f30\u8ba1\u975e\u7ebf\u6027\u4e0d\u786e\u5b9a\u6027\u3002 AM-SORT \u662f SORT \u7cfb\u5217\u8ddf\u8e2a\u5668\u7684\u65b0\u9896\u6269\u5c55\uff0c\u5b83\u4ee5\u53d8\u538b\u5668\u67b6\u6784\u53d6\u4ee3\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u8fd0\u52a8\u9884\u6d4b\u5668\u3002\u6211\u4eec\u5f15\u5165\u4e86\u5386\u53f2\u8f68\u8ff9\u5d4c\u5165\uff0c\u4f7f\u8f6c\u6362\u5668\u80fd\u591f\u4ece\u4e00\u7cfb\u5217\u8fb9\u754c\u6846\u4e2d\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u3002\u4e0e DanceTrack \u4e0a\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\u76f8\u6bd4\uff0cAM-SORT \u7684\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\uff0cIDF1 \u4e3a 56.3\uff0cHOTA \u4e3a 55.6\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u906e\u6321\u4e0b\u975e\u7ebf\u6027\u8fd0\u52a8\u65b9\u9762\u7684\u6709\u6548\u6027\u3002|[2401.13950v1](http://arxiv.org/pdf/2401.13950v1)|null|\n", "2401.13937": "|**2024-01-25**|**Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention**|\u5229\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u7684\u84b8\u998f\u5b66\u4e60\u8fdb\u884c\u81ea\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272|Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung|Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.|\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u7814\u7a76\u95ee\u9898\u3002\u6700\u8fd1\u7684\u6280\u672f\u7ecf\u5e38\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u4e8e\u4ece\u89c6\u9891\u5e8f\u5217\u4e2d\u5b66\u4e60\u5bf9\u8c61\u8868\u793a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u89c6\u9891\u6570\u636e\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u6ce8\u610f\u529b\u56fe\u53ef\u80fd\u65e0\u6cd5\u4e0e\u89c6\u9891\u5e27\u4e2d\u7684\u611f\u5174\u8da3\u5bf9\u8c61\u5f88\u597d\u5730\u5bf9\u9f50\uff0c\u4ece\u800c\u5bfc\u81f4\u957f\u671f\u89c6\u9891\u5904\u7406\u4e2d\u7684\u7d2f\u79ef\u9519\u8bef\u3002\u6b64\u5916\uff0c\u73b0\u6709\u6280\u672f\u5229\u7528\u4e86\u590d\u6742\u7684\u67b6\u6784\uff0c\u9700\u8981\u9ad8\u5ea6\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9650\u5236\u4e86\u5c06\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u96c6\u6210\u5230\u4f4e\u529f\u7387\u8bbe\u5907\u4e2d\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u84b8\u998f\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u65b0\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u9002\u5e94\u65f6\u95f4\u53d8\u5316\u3002\u8fd9\u662f\u901a\u8fc7\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7684\uff0c\u5176\u4e2d\u6355\u83b7\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u89c6\u9891\u5e8f\u5217\u5185\u5b58\u7684\u952e\u548c\u503c\u5177\u6709\u8de8\u5e27\u66f4\u65b0\u7684\u7075\u6d3b\u4f4d\u7f6e\u3002\u56e0\u6b64\uff0c\u5b66\u4e60\u5230\u7684\u5bf9\u8c61\u8868\u793a\u9002\u5e94\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u3002\u6211\u4eec\u901a\u8fc7\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u8bad\u7ec3\u6240\u63d0\u51fa\u7684\u67b6\u6784\uff0c\u5176\u4e2d\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u56fe\u88ab\u96c6\u6210\u5230\u84b8\u998f\u635f\u5931\u4e2d\u3002\u6211\u4eec\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ec DAVIS 2016/2017 \u548c YouTube-VOS 2018/2019\uff09\u4e0a\u7684\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u901a\u8fc7\u5176\u5b9e\u73b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u6700\u4f73\u7684\u5185\u5b58\u4f7f\u7528\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2401.13937v1](http://arxiv.org/pdf/2401.13937v1)|null|\n", "2401.13877": "|**2024-01-25**|**AscDAMs: Advanced SLAM-based channel detection and mapping system**|AscDAMs\uff1a\u57fa\u4e8e SLAM \u7684\u9ad8\u7ea7\u901a\u9053\u68c0\u6d4b\u548c\u6620\u5c04\u7cfb\u7edf|Tengfei Wang, Fucheng Lu, Jintao Qin, Taosheng Huang, Hui Kong, Ping Shen|Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.|\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u3001\u51c6\u786e\u7684\u6cb3\u9053\u5730\u5f62\u548c\u6c89\u79ef\u6761\u4ef6\u662f\u6e20\u9053\u5316\u6ce5\u77f3\u6d41\u7814\u7a76\u7684\u9996\u8981\u6311\u6218\u3002\u76ee\u524d\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u536b\u661f\u6210\u50cf\u548c\u65e0\u4eba\u673a\u6444\u5f71\u6d4b\u91cf\u7b49\u6d4b\u7ed8\u6280\u672f\u96be\u4ee5\u7cbe\u786e\u89c2\u6d4b\u5c71\u5730\u957f\u6df1\u6c9f\u6e20\u7684\u6cb3\u9053\u5185\u90e8\u72b6\u51b5\uff0c\u7279\u522b\u662f\u5728\u6c76\u5ddd\u5730\u9707\u5730\u533a\u3002 SLAM \u662f\u4e00\u79cd\u65b0\u5174\u7684 3D \u5730\u56fe\u6280\u672f\uff1b\u7136\u800c\uff0c\u5373\u4f7f\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684SLAM\u6765\u8bf4\uff0c\u957f\u800c\u6df1\u7684\u6c9f\u58d1\u4e2d\u6781\u5176\u6076\u52a3\u7684\u73af\u5883\u4e5f\u5e26\u6765\u4e86\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\uff081\uff09\u975e\u5178\u578b\u7279\u5f81\uff1b (2)\u4f20\u611f\u5668\u5267\u70c8\u6643\u52a8\u3001\u632f\u8361\u3002\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4 SLAM \u7ed3\u679c\u5b58\u5728\u8f83\u5927\u504f\u5dee\u548c\u5927\u91cf\u566a\u58f0\u3002\u4e3a\u4e86\u6539\u5584\u6b64\u7c7b\u73af\u5883\u4e2d\u7684 SLAM \u6620\u5c04\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u57fa\u4e8e SLAM \u7684\u901a\u9053\u68c0\u6d4b\u548c\u6620\u5c04\u7cfb\u7edf\uff0c\u5373 AscDAM\u3002\u5b83\u5bf9SLAM\u7ed3\u679c\u7684\u540e\u5904\u7406\u4e3b\u8981\u6709\u4e09\u4e2a\u589e\u5f3a\uff1a\uff081\uff09\u6570\u5b57\u6b63\u5c04\u5f71\u50cf\u5730\u56fe\u8f85\u52a9\u504f\u5dee\u6821\u6b63\u7b97\u6cd5\u5927\u5927\u6d88\u9664\u4e86\u7cfb\u7edf\u8bef\u5dee\uff1b \uff082\uff09\u70b9\u4e91\u5e73\u6ed1\u7b97\u6cd5\u5927\u5e45\u964d\u4f4e\u566a\u58f0\uff1b (3)\u65ad\u9762\u63d0\u53d6\u7b97\u6cd5\u80fd\u591f\u5b9a\u91cf\u8bc4\u4f30\u6cb3\u9053\u6c89\u79ef\u7269\u53ca\u5176\u53d8\u5316\u3002 2023\u5e742\u6708\u548c11\u6708\u5728\u4e2d\u56fd\u6c76\u5ddd\u53bf\u695a\u5934\u6c9f\u8fdb\u884c\u4e86\u4e24\u6b21\u91ce\u5916\u8bd5\u9a8c\uff0c\u4ee3\u8868\u4e86\u96e8\u5b63\u524d\u540e\u7684\u89c2\u6d4b\u7ed3\u679c\u3002\u6211\u4eec\u5c55\u793a\u4e86 AscDAM \u6781\u5927\u6539\u5584 SLAM \u7ed3\u679c\u7684\u80fd\u529b\uff0c\u4fc3\u8fdb SLAM \u7528\u4e8e\u7ed8\u5236\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u3002\u8be5\u65b9\u6cd5\u5f25\u8865\u4e86\u73b0\u6709\u6280\u672f\u5728\u68c0\u6d4b\u6ce5\u77f3\u6d41\u6cb3\u9053\u5185\u90e8\u7684\u4e0d\u8db3\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u6cb3\u9053\u5f62\u6001\u3001\u4fb5\u8680\u6a21\u5f0f\u3001\u6c89\u79ef\u7269\u533a\u5206\u3001\u4f53\u79ef\u4f30\u8ba1\u548c\u53d8\u5316\u68c0\u6d4b\u3002\u5b83\u6709\u52a9\u4e8e\u52a0\u5f3a\u5bf9\u5168\u9762\u6ce5\u77f3\u6d41\u673a\u5236\u3001\u957f\u671f\u9707\u540e\u6f14\u5316\u548c\u707e\u5bb3\u8bc4\u4f30\u7684\u7814\u7a76\u3002|[2401.13877v1](http://arxiv.org/pdf/2401.13877v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.14038": "|**2024-01-25**|**Deep Clustering with Diffused Sampling and Hardness-aware Self-distillation**|\u5177\u6709\u6269\u6563\u91c7\u6837\u548c\u786c\u5ea6\u611f\u77e5\u81ea\u84b8\u998f\u7684\u6df1\u5ea6\u805a\u7c7b|Hai-Xin Zhang, Dong Huang|Deep clustering has gained significant attention due to its capability in learning clustering-friendly representations without labeled data. However, previous deep clustering methods tend to treat all samples equally, which neglect the variance in the latent distribution and the varying difficulty in classifying or clustering different samples. To address this, this paper proposes a novel end-to-end deep clustering method with diffused sampling and hardness-aware self-distillation (HaDis). Specifically, we first align one view of instances with another view via diffused sampling alignment (DSA), which helps improve the intra-cluster compactness. To alleviate the sampling bias, we present the hardness-aware self-distillation (HSD) mechanism to mine the hardest positive and negative samples and adaptively adjust their weights in a self-distillation fashion, which is able to deal with the potential imbalance in sample contributions during optimization. Further, the prototypical contrastive learning is incorporated to simultaneously enhance the inter-cluster separability and intra-cluster compactness. Experimental results on five challenging image datasets demonstrate the superior clustering performance of our HaDis method over the state-of-the-art. Source code is available at https://github.com/Regan-Zhang/HaDis.|\u6df1\u5ea6\u805a\u7c7b\u7531\u4e8e\u5176\u65e0\u9700\u6807\u8bb0\u6570\u636e\u5373\u53ef\u5b66\u4e60\u805a\u7c7b\u53cb\u597d\u8868\u793a\u7684\u80fd\u529b\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u503e\u5411\u4e8e\u5e73\u7b49\u5730\u5bf9\u5f85\u6240\u6709\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u5206\u5e03\u7684\u65b9\u5dee\u4ee5\u53ca\u5bf9\u4e0d\u540c\u6837\u672c\u8fdb\u884c\u5206\u7c7b\u6216\u805a\u7c7b\u7684\u4e0d\u540c\u96be\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\uff0c\u5177\u6709\u6269\u6563\u91c7\u6837\u548c\u786c\u5ea6\u611f\u77e5\u81ea\u84b8\u998f\uff08HaDis\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u6269\u6563\u91c7\u6837\u5bf9\u9f50\uff08DSA\uff09\u5c06\u5b9e\u4f8b\u7684\u4e00\u4e2a\u89c6\u56fe\u4e0e\u53e6\u4e00\u4e2a\u89c6\u56fe\u5bf9\u9f50\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8\u96c6\u7fa4\u5185\u7684\u7d27\u51d1\u6027\u3002\u4e3a\u4e86\u51cf\u8f7b\u91c7\u6837\u504f\u5dee\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u786c\u5ea6\u611f\u77e5\u81ea\u84b8\u998f\uff08HSD\uff09\u673a\u5236\u6765\u6316\u6398\u6700\u96be\u7684\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\uff0c\u5e76\u4ee5\u81ea\u84b8\u998f\u7684\u65b9\u5f0f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u5b83\u4eec\u7684\u6743\u91cd\uff0c\u8fd9\u80fd\u591f\u5904\u7406\u6837\u672c\u4e2d\u6f5c\u5728\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u4f18\u5316\u671f\u95f4\u7684\u8d21\u732e\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u4e86\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u540c\u65f6\u589e\u5f3a\u7c07\u95f4\u53ef\u5206\u79bb\u6027\u548c\u7c07\u5185\u7d27\u51d1\u6027\u3002\u4e94\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 HaDis \u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u805a\u7c7b\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/Regan-Zhang/HaDis \u83b7\u53d6\u3002|[2401.14038v1](http://arxiv.org/pdf/2401.14038v1)|null|\n", "2401.13942": "|**2024-01-25**|**StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models**|StyleInject\uff1a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u8c03\u6574|Yalong Bai, Mohan Zhou, Qing Yang|The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency.|\u5fae\u8c03\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u9762\u5bf9\u51c6\u786e\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u6587\u672c\u8f93\u5165\u6240\u6d89\u53ca\u7684\u590d\u6742\u6027\u3002\u867d\u7136 LoRA \u5728\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u65b9\u9762\u975e\u5e38\u9ad8\u6548\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u590d\u6742\u8981\u6c42\uff08\u4f8b\u5982\u9002\u5e94\u5e7f\u6cdb\u7684\u98ce\u683c\u548c\u7ec6\u5fae\u5dee\u522b\uff09\uff0c\u5b83\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u5e38\u5e38\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 StyleInject\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91cf\u8eab\u5b9a\u5236\u7684\u5fae\u8c03\u65b9\u6cd5\u3002 StyleInject\u5305\u542b\u591a\u4e2a\u5e76\u884c\u7684\u4f4e\u79e9\u53c2\u6570\u77e9\u9635\uff0c\u4fdd\u6301\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6837\u6027\u3002\u5b83\u901a\u8fc7\u6839\u636e\u8f93\u5165\u4fe1\u53f7\u7684\u7279\u5f81\u8c03\u6574\u89c6\u89c9\u7279\u5f81\u7684\u65b9\u5dee\u6765\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7684\u98ce\u683c\u3002\u8fd9\u79cd\u65b9\u6cd5\u6781\u5927\u5730\u51cf\u5c11\u4e86\u5bf9\u539f\u59cb\u6a21\u578b\u6587\u672c\u56fe\u50cf\u5bf9\u9f50\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u5de7\u5999\u5730\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5404\u79cd\u98ce\u683c\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0cStyleInject \u5728\u5b66\u4e60\u548c\u589e\u5f3a\u4e00\u7cfb\u5217\u5148\u8fdb\u7684\u3001\u7ecf\u8fc7\u793e\u533a\u5fae\u8c03\u7684\u751f\u6210\u6a21\u578b\u65b9\u9762\u7279\u522b\u6709\u6548\u3002\u6211\u4eec\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u5305\u62ec\u5c0f\u6837\u672c\u548c\u5927\u89c4\u6a21\u6570\u636e\u5fae\u8c03\u4ee5\u53ca\u57fa\u7840\u6a21\u578b\u84b8\u998f\uff0c\u8868\u660eStyleInject\u5728\u6587\u672c\u56fe\u50cf\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86\u4f20\u7edfLoRA\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3002|[2401.13942v1](http://arxiv.org/pdf/2401.13942v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.14404": "|**2024-01-25**|**Deconstructing Denoising Diffusion Models for Self-Supervised Learning**|\u89e3\u6784\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b|Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He|In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u68c0\u67e5\u4e86\u6700\u521d\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08DDM\uff09\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002\u6211\u4eec\u7684\u7406\u5ff5\u662f\u89e3\u6784 DDM\uff0c\u9010\u6e10\u5c06\u5176\u8f6c\u53d8\u4e3a\u7ecf\u5178\u7684\u53bb\u566a\u81ea\u52a8\u7f16\u7801\u5668 (DAE)\u3002\u8fd9\u79cd\u89e3\u6784\u8fc7\u7a0b\u4f7f\u6211\u4eec\u80fd\u591f\u63a2\u7d22\u73b0\u4ee3 DDM \u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u5982\u4f55\u5f71\u54cd\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u53ea\u6709\u6781\u5c11\u6570\u73b0\u4ee3\u7ec4\u4ef6\u5bf9\u4e8e\u5b66\u4e60\u826f\u597d\u7684\u8868\u793a\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8bb8\u591a\u5176\u4ed6\u7ec4\u4ef6\u5219\u4e0d\u662f\u5fc5\u9700\u7684\u3002\u6211\u4eec\u7684\u7814\u7a76\u6700\u7ec8\u5f97\u51fa\u4e86\u4e00\u79cd\u9ad8\u5ea6\u7b80\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7c7b\u4f3c\u4e8e\u7ecf\u5178\u7684 DAE\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u7814\u7a76\u80fd\u591f\u91cd\u65b0\u6fc0\u53d1\u4eba\u4eec\u5bf9\u73b0\u4ee3\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u9886\u57df\u4e2d\u4e00\u7cfb\u5217\u7ecf\u5178\u65b9\u6cd5\u7684\u5174\u8da3\u3002|[2401.14404v1](http://arxiv.org/pdf/2401.14404v1)|null|\n", "2401.14257": "|**2024-01-25**|**Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation**|Sketch2NeRF\uff1a\u591a\u89c6\u56fe\u8349\u56fe\u5f15\u5bfc\u7684\u6587\u672c\u5230 3D \u751f\u6210|Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo|Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.|\u6700\u8fd1\uff0c\u6587\u672c\u8f6c 3D \u65b9\u6cd5\u5df2\u7ecf\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f 3D \u5185\u5bb9\u751f\u6210\u3002\u7136\u800c\uff0c\u751f\u6210\u7684\u5bf9\u8c61\u662f\u968f\u673a\u7684\u5e76\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u63a7\u5236\u3002\u8349\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u5ec9\u4ef7\u7684\u65b9\u6cd5\u6765\u5f15\u5165\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u7684\u63a7\u5236\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u8349\u56fe\u7684\u62bd\u8c61\u6027\u548c\u6a21\u7cca\u6027\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u8349\u56fe\u5f15\u5bfc\u7684\u6587\u672c\u5230 3D \u751f\u6210\u6846\u67b6\uff08\u5373 Sketch2NeRF\uff09\uff0c\u4ee5\u5c06\u8349\u56fe\u63a7\u5236\u6dfb\u52a0\u5230 3D \u751f\u6210\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684 2D \u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982\uff0cStable Diffusion \u548c ControlNet\uff09\u6765\u76d1\u7763\u7531\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u8868\u793a\u7684 3D \u573a\u666f\u7684\u4f18\u5316\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u540c\u6b65\u751f\u6210\u548c\u91cd\u5efa\u65b9\u6cd5\u6765\u6709\u6548\u4f18\u5316 NeRF\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e24\u79cd\u591a\u89c6\u56fe\u8349\u56fe\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8349\u56fe\u63a7\u5236\u5408\u6210 3D \u4e00\u81f4\u7684\u5185\u5bb9\uff0c\u540c\u65f6\u5bf9\u6587\u672c\u63d0\u793a\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002\u5927\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8349\u56fe\u76f8\u4f3c\u6027\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.14257v1](http://arxiv.org/pdf/2401.14257v1)|null|\n", "2401.14111": "|**2024-01-25**|**Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models**|\u573a\u666f\u56fe\u5230\u56fe\u50cf\u5408\u6210\uff1a\u5c06 CLIP \u6307\u5bfc\u4e0e\u6269\u6563\u6a21\u578b\u4e2d\u7684\u56fe\u8c03\u8282\u76f8\u96c6\u6210|Rameshwar Mishra, A V Subramanyam|Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.|\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u5728\u9075\u5b88\u7279\u5b9a\u7ed3\u6784\u51c6\u5219\u7684\u540c\u65f6\u751f\u6210\u56fe\u50cf\u7684\u6d53\u539a\u5174\u8da3\u3002\u573a\u666f\u56fe\u5230\u56fe\u50cf\u751f\u6210\u5c31\u662f\u751f\u6210\u4e0e\u7ed9\u5b9a\u573a\u666f\u56fe\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u4efb\u52a1\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u89c6\u89c9\u573a\u666f\u7684\u590d\u6742\u6027\u5bf9\u6839\u636e\u573a\u666f\u56fe\u4e2d\u6307\u5b9a\u7684\u5173\u7cfb\u51c6\u786e\u5bf9\u9f50\u5bf9\u8c61\u63d0\u51fa\u4e86\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u9996\u5148\u9884\u6d4b\u573a\u666f\u5e03\u5c40\u5e76\u4f7f\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u4ece\u8fd9\u4e9b\u5e03\u5c40\u751f\u6210\u56fe\u50cf\u6765\u5b8c\u6210\u6b64\u4efb\u52a1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4ece\u573a\u666f\u56fe\u751f\u6210\u56fe\u50cf\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u9884\u6d4b\u4e2d\u95f4\u5e03\u5c40\u7684\u9700\u8981\u3002\u6211\u4eec\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c CLIP \u6307\u5bfc\u5c06\u56fe\u5f62\u77e5\u8bc6\u8f6c\u5316\u4e3a\u56fe\u50cf\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u4f7f\u7528\u57fa\u4e8e GAN \u7684\u8bad\u7ec3\u5bf9\u56fe\u5f62\u7f16\u7801\u5668\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u5c06\u56fe\u5f62\u7279\u5f81\u4e0e\u76f8\u5e94\u56fe\u50cf\u7684 CLIP \u7279\u5f81\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u56fe\u7279\u5f81\u4e0e\u7ed9\u5b9a\u573a\u666f\u56fe\u4e2d\u5b58\u5728\u7684\u5bf9\u8c61\u6807\u7b7e\u7684 CLIP \u5d4c\u5165\u76f8\u878d\u5408\uff0c\u4ee5\u521b\u5efa\u56fe\u4e00\u81f4\u7684 CLIP \u5f15\u5bfc\u8c03\u8282\u4fe1\u53f7\u3002\u5728\u6761\u4ef6\u8f93\u5165\u4e2d\uff0c\u5bf9\u8c61\u5d4c\u5165\u63d0\u4f9b\u56fe\u50cf\u7684\u7c97\u7565\u7ed3\u6784\uff0c\u56fe\u5f62\u7279\u5f81\u63d0\u4f9b\u57fa\u4e8e\u5bf9\u8c61\u4e4b\u95f4\u5173\u7cfb\u7684\u7ed3\u6784\u5bf9\u9f50\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5177\u6709\u91cd\u5efa\u548c CLIP \u5bf9\u9f50\u635f\u5931\u7684\u56fe\u4e00\u81f4\u6761\u4ef6\u4fe1\u53f7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3002\u8be6\u7ec6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 COCO-stuff \u548c Visual Genome \u6570\u636e\u96c6\u7684\u6807\u51c6\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|[2401.14111v1](http://arxiv.org/pdf/2401.14111v1)|null|\n", "2401.14066": "|**2024-01-25**|**CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion**|CreativeSynth\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u7684\u89c6\u89c9\u827a\u672f\u521b\u610f\u878d\u5408\u4e0e\u5408\u6210|Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu|Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. This allows for the precise manipulation of image style and content while maintaining the integrity of the original model parameters. Rigorous qualitative and quantitative evaluations underscore that CreativeSynth excels in enhancing artistic images' fidelity and preserves their innate aesthetic essence. By bridging the gap between generative models and artistic finesse, CreativeSynth becomes a custom digital palette.|\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8fdb\u6b65\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5408\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c06\u8fd9\u4e9b\u6a21\u578b\u5e94\u7528\u4e8e\u827a\u672f\u56fe\u50cf\u7f16\u8f91\u9762\u4e34\u7740\u4e24\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u7528\u6237\u5f88\u96be\u7cbe\u5fc3\u5236\u4f5c\u6587\u672c\u63d0\u793a\u6765\u8be6\u7ec6\u63cf\u8ff0\u8f93\u5165\u56fe\u50cf\u7684\u89c6\u89c9\u5143\u7d20\u3002\u5176\u6b21\uff0c\u6d41\u884c\u7684\u6a21\u5f0f\u5728\u5bf9\u7279\u5b9a\u533a\u57df\u8fdb\u884c\u4fee\u6539\u65f6\uff0c\u7ecf\u5e38\u4f1a\u7834\u574f\u6574\u4f53\u7684\u827a\u672f\u98ce\u683c\uff0c\u4f7f\u5b9e\u73b0\u6709\u51dd\u805a\u529b\u548c\u5ba1\u7f8e\u7edf\u4e00\u7684\u827a\u672f\u54c1\u53d8\u5f97\u590d\u6742\u5316\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u521b\u65b0\u7684\u7edf\u4e00\u6846\u67b6 CreativeSynth\uff0c\u5b83\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u534f\u8c03\u827a\u672f\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u591a\u6a21\u5f0f\u8f93\u5165\u548c\u591a\u4efb\u52a1\u3002\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u7279\u5f81\u4e0e\u5b9a\u5236\u7684\u6ce8\u610f\u529b\u673a\u5236\u76f8\u7ed3\u5408\uff0cCreativeSynth \u901a\u8fc7\u53cd\u8f6c\u548c\u5b9e\u65f6\u98ce\u683c\u8f6c\u6362\uff0c\u4fc3\u8fdb\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u8bed\u4e49\u5185\u5bb9\u8f93\u5165\u5230\u827a\u672f\u9886\u57df\u3002\u8fd9\u5141\u8bb8\u7cbe\u786e\u64cd\u4f5c\u56fe\u50cf\u6837\u5f0f\u548c\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u53c2\u6570\u7684\u5b8c\u6574\u6027\u3002\u4e25\u683c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5f3a\u8c03\u4e86 CreativeSynth \u5728\u589e\u5f3a\u827a\u672f\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u5e76\u4fdd\u7559\u5176\u56fa\u6709\u7684\u5ba1\u7f8e\u672c\u8d28\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u901a\u8fc7\u5f25\u5408\u751f\u6210\u6a21\u578b\u548c\u827a\u672f\u6280\u5de7\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0cCreativeSynth \u6210\u4e3a\u4e00\u4e2a\u5b9a\u5236\u7684\u6570\u5b57\u8c03\u8272\u677f\u3002|[2401.14066v1](http://arxiv.org/pdf/2401.14066v1)|**[link](https://github.com/haha-lisa/creativesynth)**|\n", "2401.13992": "|**2024-01-25**|**Diffusion-based Data Augmentation for Object Counting Problems**|\u9488\u5bf9\u5bf9\u8c61\u8ba1\u6570\u95ee\u9898\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u589e\u5f3a|Zhen Wang, Yuelei Li, Jia Wan, Nuno Vasconcelos|Crowd counting is an important problem in computer vision due to its wide range of applications in image understanding. Currently, this problem is typically addressed using deep learning approaches, such as Convolutional Neural Networks (CNNs) and Transformers. However, deep networks are data-driven and are prone to overfitting, especially when the available labeled crowd dataset is limited. To overcome this limitation, we have designed a pipeline that utilizes a diffusion model to generate extensive training data. We are the first to generate images conditioned on a location dot map (a binary dot map that specifies the location of human heads) with a diffusion model. We are also the first to use these diverse synthetic data to augment the crowd counting models. Our proposed smoothed density map input for ControlNet significantly improves ControlNet's performance in generating crowds in the correct locations. Also, Our proposed counting loss for the diffusion model effectively minimizes the discrepancies between the location dot map and the crowd images generated. Additionally, our innovative guidance sampling further directs the diffusion process toward regions where the generated crowd images align most accurately with the location dot map. Collectively, we have enhanced ControlNet's ability to generate specified objects from a location dot map, which can be used for data augmentation in various counting problems. Moreover, our framework is versatile and can be easily adapted to all kinds of counting problems. Extensive experiments demonstrate that our framework improves the counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS datasets, showcasing its effectiveness.|\u4eba\u7fa4\u8ba1\u6570\u7531\u4e8e\u5176\u5728\u56fe\u50cf\u7406\u89e3\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u76ee\u524d\uff0c\u8fd9\u4e2a\u95ee\u9898\u901a\u5e38\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\uff0c\u4f8b\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c Transformer\u3002\u7136\u800c\uff0c\u6df1\u5ea6\u7f51\u7edc\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u5f88\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\uff0c\u7279\u522b\u662f\u5f53\u53ef\u7528\u7684\u6807\u8bb0\u4eba\u7fa4\u6570\u636e\u96c6\u6709\u9650\u65f6\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5229\u7528\u6269\u6563\u6a21\u578b\u6765\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7ba1\u9053\u3002\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4ee5\u4f4d\u7f6e\u70b9\u56fe\uff08\u6307\u5b9a\u4eba\u4f53\u5934\u90e8\u4f4d\u7f6e\u7684\u4e8c\u8fdb\u5236\u70b9\u56fe\uff09\u4e3a\u6761\u4ef6\u7684\u56fe\u50cf\u3002\u6211\u4eec\u4e5f\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u7684\u4eba\u3002\u6211\u4eec\u63d0\u51fa\u7684 ControlNet \u5e73\u6ed1\u5bc6\u5ea6\u56fe\u8f93\u5165\u663e\u7740\u63d0\u9ad8\u4e86 ControlNet \u5728\u6b63\u786e\u4f4d\u7f6e\u751f\u6210\u4eba\u7fa4\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u7684\u8ba1\u6570\u635f\u5931\u6709\u6548\u5730\u6700\u5c0f\u5316\u4e86\u4f4d\u7f6e\u70b9\u56fe\u548c\u751f\u6210\u7684\u4eba\u7fa4\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u521b\u65b0\u5f15\u5bfc\u91c7\u6837\u8fdb\u4e00\u6b65\u5c06\u6269\u6563\u8fc7\u7a0b\u5f15\u5bfc\u81f3\u751f\u6210\u7684\u4eba\u7fa4\u56fe\u50cf\u4e0e\u4f4d\u7f6e\u70b9\u56fe\u6700\u51c6\u786e\u5bf9\u9f50\u7684\u533a\u57df\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u589e\u5f3a\u4e86 ControlNet \u4ece\u4f4d\u7f6e\u70b9\u56fe\u751f\u6210\u6307\u5b9a\u5bf9\u8c61\u7684\u80fd\u529b\uff0c\u53ef\u7528\u4e8e\u5404\u79cd\u8ba1\u6570\u95ee\u9898\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u662f\u901a\u7528\u7684\uff0c\u53ef\u4ee5\u8f7b\u677e\u9002\u5e94\u5404\u79cd\u8ba1\u6570\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u63d0\u9ad8\u4e86 ShanghaiTech\u3001NWPU-Crowd\u3001UCF-QNRF \u548c TRANCOS \u6570\u636e\u96c6\u7684\u8ba1\u6570\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002|[2401.13992v1](http://arxiv.org/pdf/2401.13992v1)|null|\n", "2401.13865": "|**2024-01-25**|**Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning**|\u901a\u8fc7\u968f\u673a\u4e3b\u9898\u5bf9\u6297\u6027\u5b66\u4e60\u8fdb\u884c\u5916\u89c2\u53bb\u504f\u6ce8\u89c6\u4f30\u8ba1|Suneung Kim, Woo-Jeoung Nam, Seong-Whan Lee|Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.|\u6700\u8fd1\uff0c\u57fa\u4e8e\u5916\u89c2\u7684\u6ce8\u89c6\u4f30\u8ba1\u5df2\u7ecf\u5f15\u8d77\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u5173\u6ce8\uff0c\u5e76\u4e14\u4f7f\u7528\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u7684\u76ee\u6807\u662f\u76f4\u63a5\u4ece\u56fe\u50cf\u4e2d\u63a8\u65ad\u6ce8\u89c6\u5411\u91cf\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5bf9\u7279\u5b9a\u4e8e\u4eba\u7684\u5916\u89c2\u56e0\u7d20\u7684\u8fc7\u5ea6\u62df\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff1a\u968f\u673a\u4e3b\u9898\u5bf9\u6297\u6027\u51dd\u89c6\u5b66\u4e60\uff08SAZE\uff09\uff0c\u5b83\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u6765\u6982\u62ec\u4e3b\u9898\u7684\u5916\u89c2\u3002\u6211\u4eec\u4f7f\u7528\u9762\u90e8\u5230\u51dd\u89c6\u7f16\u7801\u5668\u548c\u9762\u90e8\u8eab\u4efd\u5206\u7c7b\u5668\u4ee5\u53ca\u63d0\u51fa\u7684\u5bf9\u6297\u6027\u635f\u5931\u6765\u8bbe\u8ba1\u9762\u90e8\u6cdb\u5316\u7f51\u7edc\uff08Fgen-Net\uff09\u3002\u6240\u63d0\u51fa\u7684\u635f\u5931\u6982\u62ec\u4e86\u9762\u90e8\u5916\u89c2\u56e0\u7d20\uff0c\u4ee5\u4fbf\u8eab\u4efd\u5206\u7c7b\u5668\u63a8\u65ad\u51fa\u5747\u5300\u7684\u6982\u7387\u5206\u5e03\u3002\u6b64\u5916\uff0cFgen-Net \u901a\u8fc7\u5b66\u4e60\u673a\u5236\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u91cd\u65b0\u9009\u62e9\u4e3b\u9898\u5b50\u96c6\u6765\u4f18\u5316\u7f51\u7edc\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u62df\u5408\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u56e0\u4e3a\u5b83\u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 MPIIGaze \u548c EyeDiap \u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230 3.89 \u548c 4.42\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u6d89\u53ca\u4e0d\u540c\u98ce\u683c\u7684\u4eba\u8138\u56fe\u50cf\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u79ef\u6781\u7684\u6cdb\u5316\u6548\u679c\u3002|[2401.13865v1](http://arxiv.org/pdf/2401.13865v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.14250": "|**2024-01-25**|**JUMP: A joint multimodal registration pipeline for neuroimaging with minimal preprocessing**|JUMP\uff1a\u7528\u4e8e\u795e\u7ecf\u6210\u50cf\u7684\u8054\u5408\u591a\u6a21\u5f0f\u914d\u51c6\u7ba1\u9053\uff0c\u53ea\u9700\u6700\u5c11\u7684\u9884\u5904\u7406|Adria Casamitjana, Juan Eugenio Iglesias, Raul Tudela, Aida Ninerola-Baizan, Roser Sala-Llonch|We present a pipeline for unbiased and robust multimodal registration of neuroimaging modalities with minimal pre-processing. While typical multimodal studies need to use multiple independent processing pipelines, with diverse options and hyperparameters, we propose a single and structured framework to jointly process different image modalities. The use of state-of-the-art learning-based techniques enables fast inferences, which makes the presented method suitable for large-scale and/or multi-cohort datasets with a diverse number of modalities per session. The pipeline currently works with structural MRI, resting state fMRI and amyloid PET images. We show the predictive power of the derived biomarkers using in a case-control study and study the cross-modal relationship between different image modalities. The code can be found in https: //github.com/acasamitjana/JUMP.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6700\u5c11\u7684\u9884\u5904\u7406\u5bf9\u795e\u7ecf\u5f71\u50cf\u6a21\u5f0f\u8fdb\u884c\u65e0\u504f\u4e14\u7a33\u5065\u7684\u591a\u6a21\u5f0f\u6ce8\u518c\u7684\u7ba1\u9053\u3002\u867d\u7136\u5178\u578b\u7684\u591a\u6a21\u6001\u7814\u7a76\u9700\u8981\u4f7f\u7528\u591a\u4e2a\u72ec\u7acb\u7684\u5904\u7406\u7ba1\u9053\uff0c\u5177\u6709\u591a\u79cd\u9009\u9879\u548c\u8d85\u53c2\u6570\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5355\u4e00\u7684\u7ed3\u6784\u5316\u6846\u67b6\u6765\u8054\u5408\u5904\u7406\u4e0d\u540c\u7684\u56fe\u50cf\u6a21\u6001\u3002\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u8fd9\u4f7f\u5f97\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u6bcf\u4e2a\u4f1a\u8bdd\u5177\u6709\u591a\u79cd\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u548c/\u6216\u591a\u961f\u5217\u6570\u636e\u96c6\u3002\u8be5\u6d41\u7a0b\u76ee\u524d\u9002\u7528\u4e8e\u7ed3\u6784 MRI\u3001\u9759\u606f\u6001 fMRI \u548c\u6dc0\u7c89\u6837\u86cb\u767d PET \u56fe\u50cf\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5728\u75c5\u4f8b\u5bf9\u7167\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u884d\u751f\u751f\u7269\u6807\u5fd7\u7269\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u56fe\u50cf\u6a21\u5f0f\u4e4b\u95f4\u7684\u8de8\u6a21\u5f0f\u5173\u7cfb\u3002\u4ee3\u7801\u53ef\u4ee5\u5728https://github.com/acasamitjana/JUMP\u627e\u5230\u3002|[2401.14250v1](http://arxiv.org/pdf/2401.14250v1)|**[link](https://github.com/acasamitjana/jump)**|\n", "2401.14148": "|**2024-01-25**|**LanDA: Language-Guided Multi-Source Domain Adaptation**|LanDA\uff1a\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u6e90\u57df\u9002\u5e94|Zhenbin Wang, Lei Zhang, Lituan Wang, Minjuan Zhu|Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data distribution when transferring knowledge from multiple labeled source domains to an unlabeled target domain. However, existing MSDA techniques assume target domain images are available, yet overlook image-rich semantic information. Consequently, an open question is whether MSDA can be guided solely by textual cues in the absence of target domain images. By employing a multimodal model with a joint image and language embedding space, we propose a novel language-guided MSDA approach, termed LanDA, based on optimal transfer theory, which facilitates the transfer of multiple source domains to a new target domain, requiring only a textual description of the target domain without needing even a single target domain image, while retaining task-relevant information. We present extensive experiments across different transfer scenarios using a suite of relevant benchmarks, demonstrating that LanDA outperforms standard fine-tuning and ensemble approaches in both target and source domains.|\u591a\u6e90\u57df\u9002\u5e94 (MSDA) \u65e8\u5728\u5c06\u77e5\u8bc6\u4ece\u591a\u4e2a\u6807\u8bb0\u6e90\u57df\u8f6c\u79fb\u5230\u672a\u6807\u8bb0\u76ee\u6807\u57df\u65f6\u51cf\u8f7b\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 MSDA \u6280\u672f\u5047\u8bbe\u76ee\u6807\u57df\u56fe\u50cf\u53ef\u7528\uff0c\u4f46\u5374\u5ffd\u7565\u4e86\u56fe\u50cf\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u662f\uff0c\u5728\u6ca1\u6709\u76ee\u6807\u57df\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0cMSDA \u662f\u5426\u53ef\u4ee5\u4ec5\u901a\u8fc7\u6587\u672c\u7ebf\u7d22\u8fdb\u884c\u6307\u5bfc\u3002\u901a\u8fc7\u91c7\u7528\u5177\u6709\u8054\u5408\u56fe\u50cf\u548c\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u8fc1\u79fb\u7406\u8bba\u7684\u65b0\u578b\u8bed\u8a00\u5f15\u5bfc MSDA \u65b9\u6cd5\uff0c\u79f0\u4e3a LanDA\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5c06\u591a\u4e2a\u6e90\u57df\u8fc1\u79fb\u5230\u65b0\u7684\u76ee\u6807\u57df\uff0c\u4ec5\u9700\u8981\u76ee\u6807\u57df\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u751a\u81f3\u4e0d\u9700\u8981\u5355\u4e2a\u76ee\u6807\u57df\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u5957\u76f8\u5173\u57fa\u51c6\u5728\u4e0d\u540c\u7684\u4f20\u8f93\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e LanDA \u5728\u76ee\u6807\u57df\u548c\u6e90\u57df\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\u548c\u96c6\u6210\u65b9\u6cd5\u3002|[2401.14148v1](http://arxiv.org/pdf/2401.14148v1)|null|\n", "2401.14032": "|**2024-01-25**|**GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting**|GauU-Scene\uff1a\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u7684\u5927\u89c4\u6a21 3D \u91cd\u5efa\u6570\u636e\u96c6\u7684\u573a\u666f\u91cd\u5efa\u57fa\u51c6|Butian Xiong, Zhuo Li, Zhen Li|We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information|\u6211\u4eec\u5728\u5e9e\u5927\u7684 U-Scene \u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u65b0\u5f00\u53d1\u7684 3D \u8868\u793a\u65b9\u6cd5 Gaussian Splatting \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u57fa\u51c6\u3002 U-Scene \u5360\u5730\u8d85\u8fc7\u4e00\u5e73\u65b9\u516c\u91cc\uff0c\u5177\u6709\u5168\u9762\u7684 RGB \u6570\u636e\u96c6\u548c LiDAR \u5730\u9762\u5b9e\u51b5\u3002\u5728\u6570\u636e\u91c7\u96c6\u65b9\u9762\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u914d\u5907\u9ad8\u7cbe\u5ea6Zenmuse L1\u6fc0\u5149\u96f7\u8fbe\u7684Matrix 300\u65e0\u4eba\u673a\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5c4b\u9876\u6570\u636e\u91c7\u96c6\u3002\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u57ce\u5e02\u548c\u5b66\u672f\u73af\u5883\u7684\u72ec\u7279\u878d\u5408\uff0c\u7528\u4e8e\u9ad8\u7ea7\u7a7a\u95f4\u5206\u6790\uff0c\u8f6c\u6362\u8d85\u8fc7 1.5 \u516c\u91cc$^2$\u3002\u6211\u4eec\u5bf9 U \u573a\u666f\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7684\u8bc4\u4f30\u5305\u62ec\u5bf9\u5404\u79cd\u65b0\u9896\u89c2\u70b9\u7684\u8be6\u7ec6\u5206\u6790\u3002\u6211\u4eec\u8fd8\u5c06\u8fd9\u4e9b\u7ed3\u679c\u4e0e\u4ece\u6211\u4eec\u7cbe\u786e\u7684\u70b9\u4e91\u6570\u636e\u96c6\u5f97\u51fa\u7684\u7ed3\u679c\u5e76\u5217\uff0c\u7a81\u51fa\u4e86\u663e\u7740\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u7ec4\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u91cd\u8981\u6027|[2401.14032v1](http://arxiv.org/pdf/2401.14032v1)|null|\n", "2401.13934": "|**2024-01-25**|**MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration**|MambaMorph\uff1a\u57fa\u4e8e Mamba \u7684\u9aa8\u5e72\u7f51\uff0c\u5177\u6709\u7528\u4e8e\u53ef\u53d8\u5f62 MR-CT \u914d\u51c6\u7684\u5bf9\u6bd4\u7279\u5f81\u5b66\u4e60|Tao Guo, Yinuo Wang, Cai Meng|Deformable image registration is an essential approach for medical image analysis.This paper introduces MambaMorph, an innovative multi-modality deformable registration network, specifically designed for Magnetic Resonance (MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with its Mamba-based registration module and a contrastive feature learning approach, addressing the prevalent challenges in multi-modality registration. The network leverages Mamba blocks for efficient long-range modeling and high-dimensional data processing, coupled with a feature extractor that learns fine-grained features for enhanced registration accuracy. Experimental results showcase MambaMorph's superior performance over existing methods in MR-CT registration, underlining its potential in clinical applications. This work underscores the significance of feature learning in multi-modality registration and positions MambaMorph as a trailblazing solution in this field. The code for MambaMorph is available at: https://github.com/Guo-Stone/MambaMorph.|\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u672c\u6587\u4ecb\u7ecd\u4e86MambaMorph\uff0c\u4e00\u79cd\u521b\u65b0\u7684\u591a\u6a21\u6001\u53ef\u53d8\u5f62\u914d\u51c6\u7f51\u7edc\uff0c\u4e13\u4e3a\u78c1\u5171\u632f\uff08MR\uff09\u548c\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u56fe\u50cf\u5bf9\u51c6\u800c\u8bbe\u8ba1\u3002 MambaMorph \u4ee5\u5176\u57fa\u4e8e Mamba \u7684\u6ce8\u518c\u6a21\u5757\u548c\u5bf9\u6bd4\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\u8131\u9896\u800c\u51fa\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6ce8\u518c\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6311\u6218\u3002\u8be5\u7f51\u7edc\u5229\u7528 Mamba \u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u7684\u8fdc\u7a0b\u5efa\u6a21\u548c\u9ad8\u7ef4\u6570\u636e\u5904\u7406\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u63d0\u53d6\u5668\u6765\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7279\u5f81\u4ee5\u63d0\u9ad8\u914d\u51c6\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86 MambaMorph \u5728 MR-CT \u914d\u51c6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u7279\u5f81\u5b66\u4e60\u5728\u591a\u6a21\u6001\u6ce8\u518c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c06 MambaMorph \u5b9a\u4f4d\u4e3a\u8be5\u9886\u57df\u7684\u5f00\u62d3\u6027\u89e3\u51b3\u65b9\u6848\u3002 MambaMorph \u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/Guo-Stone/MambaMorph\u3002|[2401.13934v1](http://arxiv.org/pdf/2401.13934v1)|**[link](https://github.com/guo-stone/mambamorph)**|\n", "2401.13888": "|**2024-01-25**|**Knowledge Graph Supported Benchmark and Video Captioning for Basketball**|\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u7684\u7bee\u7403\u57fa\u51c6\u548c\u89c6\u9891\u5b57\u5e55|Zeyu Xi, Ge Shi, Lifang Wu, Xuefen Li, Junchi Yan, Liang Wang, Zilin Liu|Despite the recent emergence of video captioning models, how to generate the text description with specific entity names and fine-grained actions is far from being solved, which however has great applications such as basketball live text broadcast. In this paper, a new multimodal knowledge supported basketball benchmark for video captioning is proposed. Specifically, we construct a Multimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond videos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset that contains 9 types of fine-grained shooting events and 286 players' knowledge (i.e., images and names) is constructed based on MbgKG. We develop a novel framework in the encoder-decoder form named Entity-Aware Captioner (EAC) for basketball live text broadcast. The temporal information in video is encoded by introducing the bi-directional GRU (Bi-GRU) module. And the multi-head self-attention module is utilized to model the relationships among the players and select the key players. Besides, we propose a new performance evaluation metric named Game Description Score (GDS), which measures not only the linguistic performance but also the accuracy of the names prediction. Extensive experiments on MbgVC dataset demonstrate that EAC effectively leverages external knowledge and outperforms advanced video captioning models. The proposed benchmark and corresponding codes will be publicly available soon.|\u5c3d\u7ba1\u6700\u8fd1\u51fa\u73b0\u4e86\u89c6\u9891\u5b57\u5e55\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u751f\u6210\u5177\u6709\u7279\u5b9a\u5b9e\u4f53\u540d\u79f0\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7684\u6587\u672c\u63cf\u8ff0\u8fd8\u8fdc\u672a\u89e3\u51b3\uff0c\u4f46\u8fd9\u5728\u7bee\u7403\u76f4\u64ad\u6587\u672c\u8f6c\u64ad\u7b49\u9886\u57df\u5177\u6709\u5f88\u5927\u7684\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u652f\u6301\u7684\u89c6\u9891\u5b57\u5e55\u7bee\u7403\u57fa\u51c6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u7bee\u7403\u6bd4\u8d5b\u77e5\u8bc6\u56fe\uff08MbgKG\uff09\u6765\u63d0\u4f9b\u89c6\u9891\u4e4b\u5916\u7684\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u57fa\u4e8eMbgKG\u6784\u5efa\u4e86\u5305\u542b9\u79cd\u7ec6\u7c92\u5ea6\u6295\u7bee\u4e8b\u4ef6\u548c286\u540d\u7403\u5458\u77e5\u8bc6\uff08\u5373\u56fe\u50cf\u548c\u59d3\u540d\uff09\u7684\u591a\u6a21\u6001\u7bee\u7403\u6bd4\u8d5b\u89c6\u9891\u5b57\u5e55\uff08MbgVC\uff09\u6570\u636e\u96c6\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5f62\u5f0f\u7684\u65b0\u9896\u6846\u67b6\uff0c\u540d\u4e3a\u5b9e\u4f53\u611f\u77e5\u5b57\u5e55\u5668\uff08EAC\uff09\uff0c\u7528\u4e8e\u7bee\u7403\u76f4\u64ad\u6587\u672c\u5e7f\u64ad\u3002\u901a\u8fc7\u5f15\u5165\u53cc\u5411 GRU\uff08Bi-GRU\uff09\u6a21\u5757\u5bf9\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002\u5229\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u5bf9\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u5e76\u9009\u62e9\u5173\u952e\u53c2\u4e0e\u8005\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6027\u80fd\u8bc4\u4f30\u6307\u6807\uff0c\u79f0\u4e3a\u6e38\u620f\u63cf\u8ff0\u5f97\u5206\uff08GDS\uff09\uff0c\u5b83\u4e0d\u4ec5\u8861\u91cf\u8bed\u8a00\u6027\u80fd\uff0c\u8fd8\u8861\u91cf\u540d\u79f0\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002 MbgVC \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEAC \u6709\u6548\u5730\u5229\u7528\u4e86\u5916\u90e8\u77e5\u8bc6\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u8fdb\u7684\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u3002\u62df\u8bae\u7684\u57fa\u51c6\u548c\u76f8\u5e94\u7684\u4ee3\u7801\u5c06\u5f88\u5feb\u516c\u5f00\u3002|[2401.13888v1](http://arxiv.org/pdf/2401.13888v1)|null|\n"}, "LLM": {"2401.14007": "|**2024-01-25**|**Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural Image Compression**|\u9ad8\u4fdd\u771f\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7684\u8bed\u4e49\u96c6\u6210\u635f\u5931\u548c\u6f5c\u5728\u7ec6\u5316|Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu|Recent advancements in neural compression have surpassed traditional codecs in PSNR and MS-SSIM measurements. However, at low bit-rates, these methods can introduce visually displeasing artifacts, such as blurring, color shifting, and texture loss, thereby compromising perceptual quality of images. To address these issues, this study presents an enhanced neural compression method designed for optimal visual fidelity. We have trained our model with a sophisticated semantic ensemble loss, integrating Charbonnier loss, perceptual loss, style loss, and a non-binary adversarial loss, to enhance the perceptual quality of image reconstructions. Additionally, we have implemented a latent refinement process to generate content-aware latent codes. These codes adhere to bit-rate constraints, balance the trade-off between distortion and fidelity, and prioritize bit allocation to regions of greater importance. Our empirical findings demonstrate that this approach significantly improves the statistical fidelity of neural image compression. On CLIC2024 validation set, our approach achieves a 62% bitrate saving compared to MS-ILLM under FID metric.|\u795e\u7ecf\u538b\u7f29\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u5728 PSNR \u548c MS-SSIM \u6d4b\u91cf\u65b9\u9762\u5df2\u7ecf\u8d85\u8d8a\u4e86\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u3002\u7136\u800c\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5f15\u5165\u89c6\u89c9\u4e0a\u4ee4\u4eba\u4e0d\u5feb\u7684\u4f2a\u50cf\uff0c\u4f8b\u5982\u6a21\u7cca\u3001\u8272\u5f69\u504f\u79fb\u548c\u7eb9\u7406\u4e22\u5931\uff0c\u4ece\u800c\u635f\u5bb3\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u6700\u4f73\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u4f7f\u7528\u590d\u6742\u7684\u8bed\u4e49\u96c6\u6210\u635f\u5931\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\uff0c\u6574\u5408 Charbonnier \u635f\u5931\u3001\u611f\u77e5\u635f\u5931\u3001\u98ce\u683c\u635f\u5931\u548c\u975e\u4e8c\u5143\u5bf9\u6297\u6027\u635f\u5931\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u7684\u611f\u77e5\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u7ec6\u5316\u8fc7\u7a0b\u6765\u751f\u6210\u5185\u5bb9\u611f\u77e5\u7684\u6f5c\u5728\u4ee3\u7801\u3002\u8fd9\u4e9b\u4ee3\u7801\u9075\u5b88\u6bd4\u7279\u7387\u9650\u5236\uff0c\u5e73\u8861\u5931\u771f\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5c06\u6bd4\u7279\u5206\u914d\u4f18\u5148\u5230\u66f4\u91cd\u8981\u7684\u533a\u57df\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7684\u7edf\u8ba1\u4fdd\u771f\u5ea6\u3002\u5728 CLIC2024 \u9a8c\u8bc1\u96c6\u4e0a\uff0c\u4e0e FID \u6307\u6807\u4e0b\u7684 MS-ILLM \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86 62% \u7684\u6bd4\u7279\u7387\u8282\u7701\u3002|[2401.14007v1](http://arxiv.org/pdf/2401.14007v1)|null|\n"}, "Transformer": {"2401.14051": "|**2024-01-25**|**A real-time rendering method for high albedo anisotropic materials with multiple scattering**|\u4e00\u79cd\u591a\u91cd\u6563\u5c04\u9ad8\u53cd\u7167\u7387\u5404\u5411\u5f02\u6027\u6750\u6599\u7684\u5b9e\u65f6\u6e32\u67d3\u65b9\u6cd5|Shun Fang, Xing Feng, Ming Cui|We propose a neural network-based real-time volume rendering method for realistic and efficient rendering of volumetric media. The traditional volume rendering method uses path tracing to solve the radiation transfer equation, which requires a huge amount of calculation and cannot achieve real-time rendering. Therefore, this paper uses neural networks to simulate the iterative integration process of solving the radiative transfer equation to speed up the volume rendering of volume media. Specifically, the paper first performs data processing on the volume medium to generate a variety of sampling features, including density features, transmittance features and phase features. The hierarchical transmittance fields are fed into a 3D-CNN network to compute more important transmittance features. Secondly, the diffuse reflection sampling template and the highlight sampling template are used to layer the three types of sampling features into the network. This method can pay more attention to light scattering, highlights and shadows, and then select important channel features through the attention module. Finally, the scattering distribution of the center points of all sampling templates is predicted through the backbone neural network. This method can achieve realistic volumetric media rendering effects and greatly increase the rendering speed while maintaining rendering quality, which is of great significance for real-time rendering applications. Experimental results indicate that our method outperforms previous methods.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u4f53\u79ef\u6e32\u67d3\u65b9\u6cd5\uff0c\u7528\u4e8e\u903c\u771f\u4e14\u9ad8\u6548\u5730\u6e32\u67d3\u4f53\u79ef\u5a92\u4f53\u3002\u4f20\u7edf\u7684\u4f53\u7ed8\u5236\u65b9\u6cd5\u91c7\u7528\u8def\u5f84\u8ffd\u8e2a\u6765\u6c42\u89e3\u8f90\u5c04\u4f20\u9012\u65b9\u7a0b\uff0c\u8ba1\u7b97\u91cf\u5de8\u5927\u4e14\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u7ed8\u5236\u3002\u56e0\u6b64\uff0c\u672c\u6587\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u6c42\u89e3\u8f90\u5c04\u4f20\u9012\u65b9\u7a0b\u7684\u8fed\u4ee3\u79ef\u5206\u8fc7\u7a0b\uff0c\u4ee5\u52a0\u901f\u4f53\u5a92\u4f53\u7684\u4f53\u6e32\u67d3\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bba\u6587\u9996\u5148\u5bf9\u4f53\u4ecb\u8d28\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u751f\u6210\u591a\u79cd\u91c7\u6837\u7279\u5f81\uff0c\u5305\u62ec\u5bc6\u5ea6\u7279\u5f81\u3001\u900f\u8fc7\u7387\u7279\u5f81\u548c\u76f8\u4f4d\u7279\u5f81\u3002\u5206\u5c42\u900f\u5c04\u7387\u573a\u88ab\u8f93\u5165 3D-CNN \u7f51\u7edc\u4ee5\u8ba1\u7b97\u66f4\u91cd\u8981\u7684\u900f\u5c04\u7387\u7279\u5f81\u3002\u5176\u6b21\uff0c\u5229\u7528\u6f2b\u53cd\u5c04\u91c7\u6837\u6a21\u677f\u548c\u9ad8\u5149\u91c7\u6837\u6a21\u677f\u5c06\u4e09\u7c7b\u91c7\u6837\u7279\u5f81\u5206\u5c42\u5230\u7f51\u7edc\u4e2d\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u66f4\u591a\u5730\u5173\u6ce8\u5149\u6563\u5c04\u3001\u9ad8\u5149\u548c\u9634\u5f71\uff0c\u7136\u540e\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u9009\u62e9\u91cd\u8981\u7684\u901a\u9053\u7279\u5f81\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4e3b\u5e72\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6240\u6709\u91c7\u6837\u6a21\u677f\u4e2d\u5fc3\u70b9\u7684\u6563\u5c04\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u903c\u771f\u7684\u4f53\u5a92\u4f53\u6e32\u67d3\u6548\u679c\uff0c\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u6e32\u67d3\u901f\u5ea6\uff0c\u5bf9\u4e8e\u5b9e\u65f6\u6e32\u67d3\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002|[2401.14051v1](http://arxiv.org/pdf/2401.14051v1)|null|\n", "2401.14036": "|**2024-01-25**|**Diverse and Lifespan Facial Age Transformation Synthesis with Identity Variation Rationality Metric**|\u5177\u6709\u8eab\u4efd\u53d8\u5f02\u7406\u6027\u5ea6\u91cf\u7684\u591a\u6837\u5316\u548c\u5bff\u547d\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u7efc\u5408|Jiu-Cheng Xie, Jun Yang, Wenqing Wang, Feng Xu, Hao Gao|Face aging has received continuous research attention over the past two decades. Although previous works on this topic have achieved impressive success, two longstanding problems remain unsettled: 1) generating diverse and plausible facial aging patterns at the target age stage; 2) measuring the rationality of identity variation between the original portrait and its syntheses with age progression or regression. In this paper, we introduce DLAT + , the first algorithm that can realize Diverse and Lifespan Age Transformation on human faces, where the diversity jointly manifests in the transformation of facial textures and shapes. Apart from the diversity mechanism embedded in the model, multiple consistency restrictions are leveraged to keep it away from counterfactual aging syntheses. Moreover, we propose a new metric to assess the rationality of Identity Deviation under Age Gaps (IDAG) between the input face and its series of age-transformed generations, which is based on statistical laws summarized from plenty of genuine face-aging data. Extensive experimental results demonstrate the uniqueness and effectiveness of our method in synthesizing diverse and perceptually reasonable faces across the whole lifetime.|\u8fc7\u53bb\u4e8c\u5341\u5e74\u6765\uff0c\u9762\u90e8\u8870\u8001\u4e00\u76f4\u53d7\u5230\u7814\u7a76\u5173\u6ce8\u3002\u5c3d\u7ba1\u4e4b\u524d\u5173\u4e8e\u8be5\u4e3b\u9898\u7684\u5de5\u4f5c\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u529f\uff0c\u4f46\u4e24\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff1a1\uff09\u5728\u76ee\u6807\u5e74\u9f84\u9636\u6bb5\u751f\u6210\u591a\u6837\u5316\u4e14\u5408\u7406\u7684\u9762\u90e8\u8870\u8001\u6a21\u5f0f\uff1b 2\uff09\u8861\u91cf\u539f\u59cb\u8096\u50cf\u53ca\u5176\u5408\u6210\u7269\u4e4b\u95f4\u8eab\u4efd\u968f\u5e74\u9f84\u589e\u957f\u6216\u56de\u5f52\u800c\u53d8\u5316\u7684\u5408\u7406\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86DLAT+\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u53ef\u4ee5\u5b9e\u73b0\u4eba\u8138\u591a\u6837\u6027\u548c\u5bff\u547d\u5e74\u9f84\u53d8\u6362\u7684\u7b97\u6cd5\uff0c\u5176\u4e2d\u591a\u6837\u6027\u5171\u540c\u4f53\u73b0\u5728\u9762\u90e8\u7eb9\u7406\u548c\u5f62\u72b6\u7684\u53d8\u6362\u4e0a\u3002\u9664\u4e86\u6a21\u578b\u4e2d\u5d4c\u5165\u7684\u591a\u6837\u6027\u673a\u5236\u4e4b\u5916\uff0c\u8fd8\u5229\u7528\u591a\u91cd\u4e00\u81f4\u6027\u9650\u5236\u6765\u4f7f\u5176\u8fdc\u79bb\u53cd\u4e8b\u5b9e\u7684\u8001\u5316\u5408\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u8f93\u5165\u4eba\u8138\u53ca\u5176\u4e00\u7cfb\u5217\u5e74\u9f84\u8f6c\u6362\u4ee3\u4e4b\u95f4\u5e74\u9f84\u5dee\u8ddd\u4e0b\u7684\u8eab\u4efd\u504f\u5dee\uff08IDAG\uff09\u7684\u5408\u7406\u6027\uff0c\u8be5\u6307\u6807\u57fa\u4e8e\u4ece\u5927\u91cf\u771f\u5b9e\u7684\u4eba\u8138\u8001\u5316\u6570\u636e\u4e2d\u603b\u7ed3\u51fa\u7684\u7edf\u8ba1\u89c4\u5f8b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6574\u4e2a\u4e00\u751f\u4e2d\u5408\u6210\u591a\u6837\u5316\u4e14\u611f\u77e5\u5408\u7406\u7684\u9762\u5b54\u65b9\u9762\u7684\u72ec\u7279\u6027\u548c\u6709\u6548\u6027\u3002|[2401.14036v1](http://arxiv.org/pdf/2401.14036v1)|null|\n", "2401.13976": "|**2024-01-25**|**Learning to Manipulate Artistic Images**|\u5b66\u4e60\u64cd\u7eb5\u827a\u672f\u56fe\u50cf|Wei Guo, Yuqi Zhang, De Ma, Qian Zheng|Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art methods.Code is available at https://github.com/SnailForce/SIM-Net.|\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u7740\u964d\u4f4e\u4e86\u827a\u672f\u521b\u4f5c\u7684\u969c\u788d\u3002\u57fa\u4e8e\u6837\u672c\u7684\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\u7531\u4e8e\u7075\u6d3b\u6027\u548c\u53ef\u63a7\u6027\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6301\u6709\u5173\u4e8e\u8bed\u4e49\u7684\u5047\u8bbe\u6216\u9700\u8981\u8bed\u4e49\u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\uff0c\u800c\u51c6\u786e\u7684\u8bed\u4e49\u5728\u827a\u672f\u56fe\u50cf\u4e2d\u5e76\u4e0d\u5bb9\u6613\u83b7\u5f97\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u5148\u9a8c\u800c\u53d7\u5230\u8de8\u57df\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u7531\u4e8e\u7a7a\u95f4\u57df\u4e2d\u7684\u7279\u5f81\u538b\u7f29\u800c\u4ea7\u751f\u4e0d\u7cbe\u786e\u7684\u7ed3\u6784\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u610f\u98ce\u683c\u56fe\u50cf\u5904\u7406\u7f51\u7edc\uff08SIM-Net\uff09\uff0c\u5b83\u5229\u7528\u65e0\u8bed\u4e49\u4fe1\u606f\u4f5c\u4e3a\u6307\u5bfc\uff0c\u5e76\u4ee5\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u4f7f\u7528\u533a\u57df\u4f20\u8f93\u7b56\u7565\u6765\u751f\u6210\u56fe\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u9ad8\u5206\u8fa8\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u5229\u4e8e\u96f6\u6837\u672c\u56fe\u50cf\u5904\u7406\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u90fd\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/SnailForce/SIM-Net \u4e0a\u83b7\u53d6\u3002|[2401.13976v1](http://arxiv.org/pdf/2401.13976v1)|**[link](https://github.com/snailforce/sim-net)**|\n"}, "Nerf": {"2401.14354": "|**2024-01-25**|**Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation**|\u5b66\u4e60\u5177\u6709\u53ef\u89c1\u6027\u548c\u7279\u5f81\u589e\u5f3a\u70b9\u8868\u793a\u7684\u9c81\u68d2\u53ef\u6cdb\u5316\u8f90\u5c04\u573a|Jiaxu Wang, Ziyi Zhang, Renjing Xu|This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF). Previous generic NeRF methods combine multiview stereo techniques with image-based neural rendering for generalization, yielding impressive results, while suffering from three issues. First, occlusions often result in inconsistent feature matching. Then, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation. Third, their image-based representations experience severe degradations when source views are not near enough to the target view. To address challenges, we propose the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field (GPF). Our approach explicitly models visibilities by geometric priors and augments them with neural features. We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality. Moreover, we present a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Besides, our representation can be easily manipulated. Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings, preliminarily proving the potential of the new paradigm for generalizable NeRF.|\u672c\u6587\u4ecb\u7ecd\u4e86\u53ef\u6cdb\u5316\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u4e00\u79cd\u65b0\u8303\u5f0f\u3002\u4ee5\u524d\u7684\u901a\u7528 NeRF \u65b9\u6cd5\u5c06\u591a\u89c6\u56fe\u7acb\u4f53\u6280\u672f\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u795e\u7ecf\u6e32\u67d3\u76f8\u7ed3\u5408\u4ee5\u8fdb\u884c\u6cdb\u5316\uff0c\u4ea7\u751f\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u540c\u65f6\u9047\u5230\u4e86\u4e09\u4e2a\u95ee\u9898\u3002\u9996\u5148\uff0c\u906e\u6321\u901a\u5e38\u4f1a\u5bfc\u81f4\u7279\u5f81\u5339\u914d\u4e0d\u4e00\u81f4\u3002\u7136\u540e\uff0c\u7531\u4e8e\u91c7\u6837\u70b9\u548c\u7c97\u7cd9\u7279\u5f81\u805a\u5408\u7684\u5355\u72ec\u5904\u7406\uff0c\u5b83\u4eec\u4f1a\u5728\u51e0\u4f55\u4e0d\u8fde\u7eed\u6027\u548c\u5c40\u90e8\u9510\u5229\u5f62\u72b6\u4e2d\u4ea7\u751f\u626d\u66f2\u548c\u4f2a\u5f71\u3002\u7b2c\u4e09\uff0c\u5f53\u6e90\u89c6\u56fe\u4e0e\u76ee\u6807\u89c6\u56fe\u4e0d\u591f\u63a5\u8fd1\u65f6\uff0c\u5b83\u4eec\u57fa\u4e8e\u56fe\u50cf\u7684\u8868\u793a\u4f1a\u7ecf\u5386\u4e25\u91cd\u7684\u9000\u5316\u3002\u4e3a\u4e86\u5e94\u5bf9\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u70b9\u6e32\u67d3\u800c\u4e0d\u662f\u57fa\u4e8e\u56fe\u50cf\u6e32\u67d3\u6784\u5efa\u53ef\u6cdb\u5316\u795e\u7ecf\u573a\u7684\u8303\u5f0f\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u53ef\u6cdb\u5316\u795e\u7ecf\u70b9\u573a\uff08GPF\uff09\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u663e\u5f0f\u5730\u5bf9\u53ef\u89c1\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7279\u5f81\u5bf9\u5176\u8fdb\u884c\u589e\u5f3a\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5747\u5300\u5bf9\u6570\u91c7\u6837\u7b56\u7565\u6765\u63d0\u9ad8\u6e32\u67d3\u901f\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5185\u6838\uff0c\u5728\u7a7a\u95f4\u4e0a\u589e\u5f3a\u4e86\u7279\u5f81\u805a\u5408\u7684\u7279\u5f81\uff0c\u51cf\u8f7b\u4e86\u51e0\u4f55\u5f62\u72b6\u6025\u5267\u53d8\u5316\u7684\u5730\u65b9\u7684\u626d\u66f2\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u8868\u793a\u5f88\u5bb9\u6613\u88ab\u64cd\u7eb5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u6cdb\u5316\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0a\u90fd\u53ef\u4ee5\u6bd4\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6240\u6709\u540c\u884c\u548c\u57fa\u51c6\u63d0\u4f9b\u66f4\u597d\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u521d\u6b65\u8bc1\u660e\u4e86\u53ef\u6cdb\u5316 NeRF \u65b0\u8303\u5f0f\u7684\u6f5c\u529b\u3002|[2401.14354v1](http://arxiv.org/pdf/2401.14354v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.14401": "|**2024-01-25**|**Range-Agnostic Multi-View Depth Estimation With Keyframe Selection**|\u901a\u8fc7\u5173\u952e\u5e27\u9009\u62e9\u8fdb\u884c\u4e0e\u8303\u56f4\u65e0\u5173\u7684\u591a\u89c6\u56fe\u6df1\u5ea6\u4f30\u8ba1|Andrea Conti, Matteo Poggi, Valerio Cambareri, Stefano Mattoccia|Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing RAMDepth, an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. Additional material can be found on our project page https://andreaconti.github.io/projects/range_agnostic_multi_view_depth.|\u4ece\u59ff\u52bf\u5e27\u8fdb\u884c 3D \u91cd\u5efa\u7684\u65b9\u6cd5\u9700\u8981\u6709\u5173\u573a\u666f\u5ea6\u91cf\u8303\u56f4\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u5e38\u662f\u4e3a\u4e86\u6062\u590d\u6cbf\u6781\u7ebf\u7684\u5339\u914d\u7ebf\u7d22\u5e76\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5148\u9a8c\u53ef\u80fd\u65e0\u6cd5\u76f4\u63a5\u83b7\u5f97\uff0c\u6216\u8005\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u4f30\u8ba1\u4e0d\u51c6\u786e\uff08\u4f8b\u5982\uff0c\u6839\u636e\u89c6\u9891\u5e8f\u5217\u8fdb\u884c\u5ba4\u5916 3D \u91cd\u5efa\uff09\uff0c\u56e0\u6b64\u4e25\u91cd\u5f71\u54cd\u4e86\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa RAMDepth\uff08\u4e00\u79cd\u9ad8\u6548\u4e14\u7eaf 2D \u7684\u6846\u67b6\uff0c\u53ef\u53cd\u8f6c\u6df1\u5ea6\u4f30\u8ba1\u548c\u5339\u914d\u6b65\u9aa4\u987a\u5e8f\uff09\uff0c\u4e13\u6ce8\u4e8e\u591a\u89c6\u56fe\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u65e0\u9700\u4e86\u89e3\u573a\u666f\u7684\u5ea6\u91cf\u8303\u56f4\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u6709\u5173\u7528\u4e8e\u9884\u6d4b\u7684\u89c6\u56fe\u8d28\u91cf\u7684\u4e30\u5bcc\u89c1\u89e3\u7684\u80fd\u529b\u3002\u5176\u4ed6\u6750\u6599\u53ef\u4ee5\u5728\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762 https://andreaconti.github.io/projects/range_agnostic_multi_view_depth \u4e0a\u627e\u5230\u3002|[2401.14401v1](http://arxiv.org/pdf/2401.14401v1)|**[link](https://github.com/andreaconti/ramdepth)**|\n", "2401.14349": "|**2024-01-25**|**Learning to navigate efficiently and precisely in real environments**|\u5b66\u4e60\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u7cbe\u786e\u5730\u5bfc\u822a|Guillaume Bono, Herv\u00e9 Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf|In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.|\u5728\u9646\u5730\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u7684\u80cc\u666f\u4e0b\uff0c\u521b\u5efa\u4ee3\u7406\u52a8\u529b\u5b66\u548c\u4f20\u611f\u7684\u771f\u5b9e\u6a21\u578b\u662f\u673a\u5668\u4eba\u6587\u732e\u548c\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u666e\u904d\u4e60\u60ef\uff0c\u5b83\u4eec\u7528\u4e8e\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u548c/\u6216\u5b9a\u4f4d\u548c\u7ed8\u56fe\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6700\u8fd1\u7684 Embodied AI \u6587\u732e\u4fa7\u91cd\u4e8e\u5728 Habitat \u6216 AI-Thor \u7b49\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u7684\u6a21\u5757\u5316\u6216\u7aef\u5230\u7aef\u4ee3\u7406\uff0c\u5176\u4e2d\u91cd\u70b9\u653e\u5728\u7167\u7247\u771f\u5b9e\u611f\u6e32\u67d3\u548c\u573a\u666f\u591a\u6837\u6027\u4e0a\uff0c\u4f46\u9ad8\u4fdd\u771f\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u88ab\u5206\u914d\u4e86\u4e00\u4e2a\u4e0d\u592a\u7279\u6743\u7684\u89d2\u8272\u3002\u7531\u6b64\u4ea7\u751f\u7684 sim2real \u5dee\u8ddd\u6781\u5927\u5730\u5f71\u54cd\u4e86\u8bad\u7ec3\u6a21\u578b\u5230\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u7684\u8f6c\u79fb\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u4ee3\u7406\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4f20\u611f\u548c\u9a71\u52a8\u65b9\u9762\u7684 sim2real \u5dee\u8ddd\u3002\u6211\u4eec\u7684\u4ee3\u7406\u76f4\u63a5\u9884\u6d4b\uff08\u79bb\u6563\u5316\uff09\u901f\u5ea6\u547d\u4ee4\uff0c\u8fd9\u4e9b\u547d\u4ee4\u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u4e2d\u7684\u95ed\u73af\u63a7\u5236\u6765\u7ef4\u62a4\u3002\u771f\u5b9e\u673a\u5668\u4eba\uff08\u5305\u62ec\u5e95\u5c42\u4f4e\u7ea7\u63a7\u5236\u5668\uff09\u7684\u884c\u4e3a\u5728\u7ecf\u8fc7\u4fee\u6539\u7684\u6816\u606f\u5730\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u8bc6\u522b\u548c\u6a21\u62df\u3002\u7528\u4e8e\u91cc\u7a0b\u8ba1\u548c\u5b9a\u4f4d\u7684\u566a\u58f0\u6a21\u578b\u8fdb\u4e00\u6b65\u6709\u52a9\u4e8e\u964d\u4f4e sim2real \u5dee\u8ddd\u3002\u6211\u4eec\u8bc4\u4f30\u771f\u5b9e\u7684\u5bfc\u822a\u573a\u666f\uff0c\u63a2\u7d22\u4e0d\u540c\u7684\u5b9a\u4f4d\u548c\u70b9\u76ee\u6807\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u62a5\u544a\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002|[2401.14349v1](http://arxiv.org/pdf/2401.14349v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.14403": "|**2024-01-25**|**Adaptive Mobile Manipulation for Articulated Objects In the Open World**|\u5f00\u653e\u4e16\u754c\u4e2d\u94f0\u63a5\u7269\u4f53\u7684\u81ea\u9002\u5e94\u79fb\u52a8\u64cd\u7eb5|Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak|Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation. Video results at https://open-world-mobilemanip.github.io/|\u5728\u5bb6\u5ead\u7b49\u5f00\u653e\u5f0f\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u673a\u5668\u4eba\u4e00\u76f4\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u7814\u7a76\u95ee\u9898\u3002\u7136\u800c\uff0c\u673a\u5668\u4eba\u901a\u5e38\u4ec5\u5728\u5c01\u95ed\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u8fdb\u884c\u7814\u7a76\uff0c\u5e76\u4e14\u5148\u524d\u7684\u79fb\u52a8\u64cd\u7eb5\u5de5\u4f5c\u4ec5\u9650\u4e8e\u62fe\u53d6\u79fb\u52a8\u653e\u7f6e\uff0c\u8fd9\u53ef\u4ee5\u8bf4\u53ea\u662f\u8be5\u9886\u57df\u7684\u51b0\u5c71\u4e00\u89d2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5f00\u653e\u4e16\u754c\u79fb\u52a8\u64cd\u7eb5\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u79cd\u89e3\u51b3\u73b0\u5b9e\u5173\u8282\u5bf9\u8c61\u64cd\u4f5c\u7684\u5168\u6808\u65b9\u6cd5\uff0c\u4f8b\u5982\u5f00\u653e\u5f0f\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u95e8\u3001\u6a71\u67dc\u3001\u62bd\u5c49\u548c\u51b0\u7bb1\u3002\u8be5\u673a\u5668\u4eba\u5229\u7528\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u4ece\u4e00\u5c0f\u7ec4\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u7136\u540e\u4ece\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u65b0\u7269\u4f53\u7684\u5728\u7ebf\u5b9e\u8df5\u4e2d\u5b66\u4e60\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u79fb\u52a8\u64cd\u63a7\u786c\u4ef6\u5e73\u53f0\uff0c\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b89\u5168\u3001\u81ea\u4e3b\u5730\u5728\u7ebf\u9002\u5e94\uff0c\u6210\u672c\u7ea6\u4e3a 20,000 \u7f8e\u5143\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u5728 CMU \u6821\u56ed\u7684 4 \u680b\u5efa\u7b51\u4e2d\u4f7f\u7528\u4e86 20 \u4e2a\u94f0\u63a5\u7269\u4f53\u3002\u6bcf\u4e2a\u5bf9\u8c61\u7684\u5728\u7ebf\u5b66\u4e60\u65f6\u95f4\u4e0d\u5230\u4e00\u4e2a\u5c0f\u65f6\uff0c\u7cfb\u7edf\u80fd\u591f\u5229\u7528\u5728\u7ebf\u9002\u5e94\u5c06 BC \u9884\u8bad\u7ec3\u7684\u6210\u529f\u7387\u4ece 50% \u63d0\u9ad8\u5230 95%\u3002\u89c6\u9891\u7ed3\u679c\u4f4d\u4e8e https://open-world-mobilemanip.github.io/|[2401.14403v1](http://arxiv.org/pdf/2401.14403v1)|null|\n", "2401.14322": "|**2024-01-25**|**Generalized People Diversity: Learning a Human Perception-Aligned Diversity Representation for People Images**|\u5e7f\u4e49\u7684\u4eba\u7269\u591a\u6837\u6027\uff1a\u5b66\u4e60\u4eba\u7269\u56fe\u50cf\u7684\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u7684\u591a\u6837\u6027\u8868\u793a|Hansa Srinivasan, Candice Schumann, Aradhana Sinha, David Madras, Gbolahan Oluwafemi Olanubi, Alex Beutel, Susanna Ricco, Jilin Chen|Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm, is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators.|\u6355\u6349\u56fe\u50cf\u4e2d\u4eba\u7269\u7684\u591a\u6837\u6027\u5177\u6709\u6311\u6218\u6027\uff1a\u6700\u8fd1\u7684\u6587\u732e\u5f80\u5f80\u4fa7\u91cd\u4e8e\u4f7f\u4e00\u4e24\u4e2a\u5c5e\u6027\u591a\u6837\u5316\uff0c\u9700\u8981\u6602\u8d35\u7684\u5c5e\u6027\u6807\u7b7e\u6216\u6784\u5efa\u5206\u7c7b\u5668\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6837\u5316\u7684\u4eba\u7269\u56fe\u50cf\u6392\u540d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u4e00\u79cd\u4e0d\u90a3\u4e48\u89c4\u8303\u3001\u65e0\u6807\u7b7e\u7684\u65b9\u5f0f\u66f4\u7075\u6d3b\u5730\u7b26\u5408\u4eba\u7c7b\u5bf9\u4eba\u7269\u591a\u6837\u6027\u7684\u89c2\u5ff5\u3002\u611f\u77e5\u5bf9\u9f50\u6587\u672c\u884d\u751f\u7684\u4eba\u7c7b\u8868\u793a\u7a7a\u95f4\uff08PATHS\uff09\u65e8\u5728\u6355\u83b7\u4e0e\u4eba\u76f8\u5173\u7684\u591a\u6837\u6027\u7684\u6240\u6709\u6216\u8bb8\u591a\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u4e14\u5f53\u7528\u4f5c\u6807\u51c6\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\uff08MMR\uff09\u6392\u5e8f\u7b97\u6cd5\u4e2d\u7684\u8868\u793a\u7a7a\u95f4\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5c55\u73b0\u4e00\u7cfb\u5217\u4e0e\u4eba\u76f8\u5173\u7684\u591a\u6837\u6027\uff08\u4f8b\u5982\u6b8b\u75be\u3001\u6587\u5316\u670d\u88c5\uff09\u3002 PATHS \u5206\u4e24\u4e2a\u9636\u6bb5\u521b\u5efa\u3002\u9996\u5148\uff0c\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u65b9\u6cd5\u4ece\u9884\u5148\u8bad\u7ec3\u7684\u56fe\u50cf\u6587\u672c\u6a21\u578b\u4e2d\u63d0\u53d6\u4eba\u7269\u591a\u6837\u6027\u8868\u793a\u3002\u7136\u540e\uff0c\u6839\u636e\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684\u611f\u77e5\u5224\u65ad\u5bf9\u8fd9\u79cd\u8868\u793a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6355\u83b7\u4eba\u7c7b\u8ba4\u4e3a\u6700\u663e\u7740\u7684\u4e0e\u4eba\u76f8\u5173\u7684\u76f8\u4f3c\u6027\u65b9\u9762\u3002\u6839\u636e\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684\u5e76\u6392\u8bc4\u5206\uff0c\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e PATHS \u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u597d\u5730\u5b9e\u73b0\u4e86\u591a\u6837\u6027\u3002|[2401.14322v1](http://arxiv.org/pdf/2401.14322v1)|null|\n", "2401.14285": "|**2024-01-25**|**POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation**|POUR-Net\uff1a\u7528\u4e8e\u751f\u6210\u4f4e\u8ba1\u6570 PET \u8870\u51cf\u56fe\u7684\u7fa4\u4f53\u4f18\u5148\u8f85\u52a9\u8fc7\u5ea6\u8868\u793a\u7f51\u7edc|Bo Zhou, Jun Hou, Tianqi Chen, Yinchi Zhou, Xiongchao Chen, Huidong Xie, Qiong Liu, Xueqi Guo, Yu-Jung Tsai, Vladimir Y. Panin, et.al.|Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within a cascade framework enables iterative refinement of $\\mu$-map generation, resulting in the production of high-quality $\\mu$-maps. Experimental results underscore the effectiveness of POUR-Net, showing it as a promising solution for accurate CT-free low-count PET attenuation correction, which also surpasses the performance of previous baseline methods.|\u4f4e\u5242\u91cf PET \u63d0\u4f9b\u4e86\u4e00\u79cd\u6700\u5927\u9650\u5ea6\u51cf\u5c11 PET \u6210\u50cf\u4e2d\u8f90\u5c04\u66b4\u9732\u7684\u5b9d\u8d35\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u91c7\u7528\u989d\u5916\u7684 CT \u626b\u63cf\u6765\u751f\u6210\u8870\u51cf\u56fe (u-map) \u4ee5\u8fdb\u884c PET \u8870\u51cf\u6821\u6b63\u7684\u666e\u904d\u505a\u6cd5\u4f1a\u663e\u7740\u63d0\u9ad8\u8f90\u5c04\u5242\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u8fdb\u4e00\u6b65\u51cf\u8f7b\u4f4e\u5242\u91cf PET \u68c0\u67e5\u4e2d\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 POUR-Net - \u4e00\u79cd\u521b\u65b0\u7684\u4eba\u53e3\u4f18\u5148\u8f85\u52a9\u8fc7\u5ea6\u4ee3\u8868\u6027\u7f51\u7edc\uff0c\u65e8\u5728\u4ece\u4f4e\u5242\u91cf PET \u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8870\u51cf\u56fe\u3002\u9996\u5148\uff0cPOUR-Net \u7ed3\u5408\u4e86\u8fc7\u5ea6\u6b20\u8868\u793a\u7f51\u7edc (OUR-Net)\uff0c\u4ee5\u4fc3\u8fdb\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u6db5\u76d6\u4f4e\u5206\u8fa8\u7387\u62bd\u8c61\u548c\u7cbe\u7ec6\u7ec6\u8282\u7279\u5f81\uff0c\u4ee5\u534f\u52a9\u5168\u5206\u8fa8\u7387\u7ea7\u522b\u7684\u6df1\u5ea6\u751f\u6210\u3002\u5176\u6b21\uff0c\u5bf9 OUR-Net \u8fdb\u884c\u8865\u5145\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u7efc\u5408 CT \u884d\u751f\u7684 u-map \u6570\u636e\u96c6\u7684\u7fa4\u4f53\u5148\u9a8c\u751f\u6210\u673a (PPGM)\uff0c\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u5148\u9a8c\u4fe1\u606f\u6765\u5e2e\u52a9 OUR-Net \u751f\u6210\u3002 OUR-Net \u548c PPGM \u5728\u7ea7\u8054\u6846\u67b6\u5185\u7684\u96c6\u6210\u53ef\u4ee5\u8fed\u4ee3\u7ec6\u5316 $\\mu$-map \u751f\u6210\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u7684 $\\mu$-map\u3002\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86 POUR-Net \u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5b83\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7cbe\u786e\u65e0 CT \u4f4e\u8ba1\u6570 PET \u8870\u51cf\u6821\u6b63\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e5f\u8d85\u8d8a\u4e86\u4ee5\u524d\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002|[2401.14285v1](http://arxiv.org/pdf/2401.14285v1)|null|\n", "2401.14142": "|**2024-01-25**|**Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations**|\u57fa\u4e8e\u80fd\u91cf\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff1a\u7edf\u4e00\u9884\u6d4b\u3001\u6982\u5ff5\u5e72\u9884\u548c\u6761\u4ef6\u89e3\u91ca|Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li|Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not help correct highly correlated concepts (e.g., \"yellow belly\"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label \"Kentucky Warbler\" and a concept \"black bill\", what is the probability that the model correctly predicts another concept \"black crown\"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets.|\u6982\u5ff5\u74f6\u9888\u6a21\u578b (CBM) \u7b49\u73b0\u6709\u65b9\u6cd5\u5df2\u6210\u529f\u4e3a\u9ed1\u76d2\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u3002\u5b83\u4eec\u901a\u5e38\u901a\u8fc7\u7ed9\u5b9a\u8f93\u5165\u9884\u6d4b\u6982\u5ff5\uff0c\u7136\u540e\u6839\u636e\u9884\u6d4b\u6982\u5ff5\u9884\u6d4b\u6700\u7ec8\u7c7b\u522b\u6807\u7b7e\u6765\u5de5\u4f5c\u3002\u7136\u800c\uff0c\uff081\uff09\u5b83\u4eec\u5e38\u5e38\u65e0\u6cd5\u6355\u6349\u6982\u5ff5\u4e4b\u95f4\u7684\u9ad8\u9636\u3001\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\uff0c\u4f8b\u5982\uff0c\u7ea0\u6b63\u9884\u6d4b\u7684\u6982\u5ff5\uff08\u4f8b\u5982\u201c\u9ec4\u80f8\u201d\uff09\u65e0\u52a9\u4e8e\u7ea0\u6b63\u9ad8\u5ea6\u76f8\u5173\u7684\u6982\u5ff5\uff08\u4f8b\u5982\u201c\u9ec4\u809a\u76ae\u201d\uff09\uff0c\u5bfc\u81f4\u6700\u7ec8\u7cbe\u5ea6\u4e0d\u4f73\uff1b \uff082\uff09\u5b83\u4eec\u65e0\u6cd5\u81ea\u7136\u5730\u91cf\u5316\u4e0d\u540c\u6982\u5ff5\u548c\u7c7b\u6807\u7b7e\u4e4b\u95f4\u590d\u6742\u7684\u6761\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff08\u4f8b\u5982\uff0c\u5bf9\u4e8e\u5177\u6709\u7c7b\u6807\u7b7e\u201cKentucky Warbler\u201d\u548c\u6982\u5ff5\u201cblack bill\u201d\u7684\u56fe\u50cf\uff0c\u6a21\u578b\u6b63\u786e\u9884\u6d4b\u53e6\u4e00\u4e2a\u6982\u5ff5\u7684\u6982\u7387\u662f\u591a\u5c11\u201c\u9ed1\u7687\u51a0\u201d\uff09\uff0c\u56e0\u6b64\u672a\u80fd\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u9ed1\u76d2\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u3002\u9488\u5bf9\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u80fd\u6e90\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08ECBM\uff09\u3002\u6211\u4eec\u7684 ECBM \u4f7f\u7528\u4e00\u7ec4\u795e\u7ecf\u7f51\u7edc\u6765\u5b9a\u4e49\u5019\u9009\uff08\u8f93\u5165\u3001\u6982\u5ff5\u3001\u7c7b\uff09\u5143\u7ec4\u7684\u8054\u5408\u80fd\u91cf\u3002\u6709\u4e86\u8fd9\u6837\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u53e3\uff0c\u9884\u6d4b\u3001\u6982\u5ff5\u6821\u6b63\u548c\u6761\u4ef6\u4f9d\u8d56\u91cf\u5316\u5c31\u53ef\u4ee5\u8868\u793a\u4e3a\u6761\u4ef6\u6982\u7387\uff0c\u8fd9\u4e9b\u6982\u7387\u662f\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u7684\u80fd\u91cf\u51fd\u6570\u751f\u6210\u7684\u3002\u6211\u4eec\u7684 ECBM \u89e3\u51b3\u4e86\u73b0\u6709 CBM \u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4e30\u5bcc\u7684\u6982\u5ff5\u89e3\u91ca\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.14142v1](http://arxiv.org/pdf/2401.14142v1)|**[link](https://github.com/xmed-lab/ecbm)**|\n", "2401.14132": "|**2024-01-25**|**Enabling Cross-Camera Collaboration for Video Analytics on Distributed Smart Cameras**|\u5728\u5206\u5e03\u5f0f\u667a\u80fd\u6444\u50cf\u673a\u4e0a\u5b9e\u73b0\u89c6\u9891\u5206\u6790\u7684\u8de8\u6444\u50cf\u673a\u534f\u4f5c|Chulhong Min, Juheon Yi, Utku Gunay Acer, Fahim Kawsar|Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing visual analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to the state-of-the-art), while achieving comparable tracking quality.|\u91cd\u53e0\u6444\u50cf\u673a\u63d0\u4f9b\u4e86\u4ece\u4e0d\u540c\u89d2\u5ea6\u67e5\u770b\u573a\u666f\u7684\u4ee4\u4eba\u5174\u594b\u7684\u673a\u4f1a\uff0c\u4ece\u800c\u53ef\u4ee5\u8fdb\u884c\u66f4\u9ad8\u7ea7\u3001\u66f4\u5168\u9762\u548c\u66f4\u5f3a\u5927\u7684\u5206\u6790\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u591a\u6444\u50cf\u673a\u6d41\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u5927\u591a\u5c40\u9650\u4e8e\uff08i\uff09\u6bcf\u4e2a\u6444\u50cf\u673a\u7684\u5904\u7406\u548c\u805a\u5408\u4ee5\u53ca\uff08ii\uff09\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u65e0\u5173\u7684\u96c6\u4e2d\u5904\u7406\u67b6\u6784\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Argus\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u667a\u80fd\u76f8\u673a\u4e0a\u5b9e\u73b0\u8de8\u76f8\u673a\u534f\u4f5c\u7684\u5206\u5e03\u5f0f\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u3002\u6211\u4eec\u5c06\u591a\u6444\u50cf\u673a\u3001\u591a\u76ee\u6807\u8ddf\u8e2a\u786e\u5b9a\u4e3a\u591a\u6444\u50cf\u673a\u89c6\u9891\u5206\u6790\u7684\u4e3b\u8981\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u5728\u91cd\u53e0\u89c6\u573a\u4e2d\u5229\u7528\u5bf9\u8c61\u65b9\u5f0f\u7684\u65f6\u7a7a\u5173\u8054\u6765\u907f\u514d\u5197\u4f59\u3001\u5904\u7406\u7e41\u91cd\u7684\u8bc6\u522b\u4efb\u52a1\u3002\u591a\u4e2a\u76f8\u673a\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u5957\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u4e91\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u4ee5\u4f4e\u5ef6\u8fdf\u8de8\u5206\u5e03\u5f0f\u6444\u50cf\u673a\u6267\u884c\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u65b9\u6cd5\u662f\uff1a\uff08i\uff09\u52a8\u6001\u6392\u5e8f\u6444\u50cf\u673a\u548c\u5bf9\u8c61\u68c0\u67e5\u5e8f\u5217\uff1b\uff08ii\uff09\u5728\u667a\u80fd\u6444\u50cf\u673a\u4e4b\u95f4\u7075\u6d3b\u5206\u914d\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u540c\u65f6\u8003\u8651\u5230\u7f51\u7edc\u4f20\u8f93\u548c\u5f02\u6784\u8ba1\u7b97\u80fd\u529b\u3002\u4f7f\u7528\u4e24\u4e2a Nvidia Jetson \u8bbe\u5907\u5bf9\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u91cd\u53e0\u76f8\u673a\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u8868\u660e\uff0cArgus \u5c06\u5bf9\u8c61\u8bc6\u522b\u6570\u91cf\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u522b\u51cf\u5c11\u4e86 7.13 \u500d\u548c 2.19 \u500d\uff08\u4e0e\u72b6\u6001\u76f8\u6bd4\u5206\u522b\u4e3a 4.86 \u500d\u548c 1.60 \u500d\uff09 -\u6700\u5148\u8fdb\u7684\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u53ef\u6bd4\u7684\u8ddf\u8e2a\u8d28\u91cf\u3002|[2401.14132v1](http://arxiv.org/pdf/2401.14132v1)|null|\n", "2401.14121": "|**2024-01-25**|**Incorporating Exemplar Optimization into Training with Dual Networks for Human Mesh Recovery**|\u5c06\u793a\u4f8b\u4f18\u5316\u7eb3\u5165\u53cc\u7f51\u7edc\u8bad\u7ec3\u4e2d\u4ee5\u5b9e\u73b0\u4eba\u4f53\u7f51\u683c\u6062\u590d|Yongwei Nie, Mingxian Fan, Chengjiang Long, Qing Zhang, Jian Zhu, Xuemiao Xu|We propose a novel optimization-based human mesh recovery method from a single image. Given a test exemplar, previous approaches optimize the pre-trained regression network to minimize the 2D re-projection loss, which however suffer from over-/under-fitting problems. This is because the ``exemplar optimization'' at testing time has too weak relation to the pre-training process, and the exemplar optimization loss function is different from the training loss function. (1) We incorporate exemplar optimization into the training stage. During training, our method first executes exemplar optimization and subsequently proceeds with training-time optimization. The exemplar optimization may run into a wrong direction, while the subsequent training optimization serves to correct the deviation. Involved in training, the exemplar optimization learns to adapt its behavior to training data, thereby acquires generalibility to test exemplars. (2) We devise a dual-network architecture to convey the novel training paradigm, which is composed of a main regression network and an auxiliary network, in which we can formulate the exemplar optimization loss function in the same form as the training loss function. This further enhances the compatibility between the exemplar and training optimizations. Experiments demonstrate that our exemplar optimization after the novel training scheme significantly outperforms state-of-the-art approaches.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u5355\u56fe\u50cf\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\u3002\u7ed9\u5b9a\u4e00\u4e2a\u6d4b\u8bd5\u6837\u672c\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u4f18\u5316\u4e86\u9884\u8bad\u7ec3\u7684\u56de\u5f52\u7f51\u7edc\u4ee5\u6700\u5c0f\u5316\u4e8c\u7ef4\u91cd\u6295\u5f71\u635f\u5931\uff0c\u7136\u800c\u8fd9\u4f1a\u9047\u5230\u8fc7\u62df\u5408/\u6b20\u62df\u5408\u95ee\u9898\u3002\u8fd9\u662f\u56e0\u4e3a\u6d4b\u8bd5\u65f6\u7684\u201c\u6837\u672c\u4f18\u5316\u201d\u4e0e\u9884\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5173\u7cfb\u592a\u5f31\uff0c\u5e76\u4e14\u6837\u672c\u4f18\u5316\u635f\u5931\u51fd\u6570\u4e0e\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u4e0d\u540c\u3002 (1)\u6211\u4eec\u5c06\u6837\u672c\u4f18\u5316\u7eb3\u5165\u8bad\u7ec3\u9636\u6bb5\u3002\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u6267\u884c\u6837\u672c\u4f18\u5316\uff0c\u7136\u540e\u8fdb\u884c\u8bad\u7ec3\u65f6\u4f18\u5316\u3002\u6837\u672c\u4f18\u5316\u53ef\u80fd\u4f1a\u8d70\u9519\u65b9\u5411\uff0c\u800c\u540e\u7eed\u7684\u8bad\u7ec3\u4f18\u5316\u5219\u7528\u4e8e\u7ea0\u6b63\u504f\u5dee\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6837\u672c\u4f18\u5316\u5b66\u4e60\u4f7f\u5176\u884c\u4e3a\u9002\u5e94\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u83b7\u5f97\u6d4b\u8bd5\u6837\u672c\u7684\u901a\u7528\u6027\u3002 \uff082\uff09\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u7f51\u7edc\u67b6\u6784\u6765\u4f20\u8fbe\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u8be5\u67b6\u6784\u7531\u4e3b\u56de\u5f52\u7f51\u7edc\u548c\u8f85\u52a9\u7f51\u7edc\u7ec4\u6210\uff0c\u5176\u4e2d\u6211\u4eec\u53ef\u4ee5\u4ee5\u4e0e\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u76f8\u540c\u7684\u5f62\u5f0f\u5236\u5b9a\u6837\u672c\u4f18\u5316\u635f\u5931\u51fd\u6570\u3002\u8fd9\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u793a\u4f8b\u548c\u8bad\u7ec3\u4f18\u5316\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u5728\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6848\u4e4b\u540e\u7684\u793a\u4f8b\u4f18\u5316\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.14121v1](http://arxiv.org/pdf/2401.14121v1)|null|\n", "2401.14031": "|**2024-01-25**|**Sparse and Transferable Universal Singular Vectors Attack**|\u7a00\u758f\u4e14\u53ef\u8f6c\u79fb\u7684\u901a\u7528\u5947\u5f02\u5411\u91cf\u653b\u51fb|Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets|The research in the field of adversarial attacks and models' vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly transferable among different models without significantly decreasing the fooling rate. Our findings demonstrate the vulnerability of state-of-the-art models to sparse attacks and highlight the importance of developing robust machine learning systems.|\u5bf9\u6297\u6027\u653b\u51fb\u548c\u6a21\u578b\u8106\u5f31\u6027\u9886\u57df\u7684\u7814\u7a76\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u65b9\u5411\u4e4b\u4e00\u3002\u6700\u8fd1\u7684\u7814\u7a76\u63ed\u793a\u4e86\u8106\u5f31\u6027\u73b0\u8c61\uff0c\u4e86\u89e3\u5176\u80cc\u540e\u7684\u673a\u5236\u5bf9\u4e8e\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u7279\u6027\u548c\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u901a\u7528\u767d\u76d2\u5bf9\u6297\u653b\u51fb\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u622a\u65ad\u5e42\u8fed\u4ee3\uff0c\u4e3a\u96c5\u53ef\u6bd4\u77e9\u9635\u9690\u85cf\u5c42\u7684 $(p,q)$-\u5947\u5f02\u5411\u91cf\u63d0\u4f9b\u7a00\u758f\u6027\u3002\u4f7f\u7528 ImageNet \u57fa\u51c6\u9a8c\u8bc1\u5b50\u96c6\uff0c\u6211\u4eec\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u5206\u6790\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u83b7\u5f97\u4e86\u4e0e\u5bc6\u96c6\u57fa\u7ebf\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u611a\u5f04\u7387\u8d85\u8fc7 50%\uff0c\u540c\u65f6\u4ec5\u635f\u574f 5% \u7684\u50cf\u7d20\uff0c\u5e76\u5229\u7528 256 \u4e2a\u6837\u672c\u8fdb\u884c\u6270\u52a8\u62df\u5408\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u5141\u8bb8\u66f4\u9ad8\u7684\u653b\u51fb\u5f3a\u5ea6\uff0c\u800c\u4e0d\u5f71\u54cd\u4eba\u7c7b\u89e3\u51b3\u4efb\u52a1\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6784\u5efa\u7684\u6270\u52a8\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5177\u6709\u9ad8\u5ea6\u53ef\u8f6c\u79fb\u6027\uff0c\u800c\u4e0d\u4f1a\u663e\u7740\u964d\u4f4e\u611a\u5f04\u7387\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5bf9\u7a00\u758f\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u5f3a\u5927\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002|[2401.14031v1](http://arxiv.org/pdf/2401.14031v1)|null|\n", "2401.13964": "|**2024-01-25**|**An Extensible Framework for Open Heterogeneous Collaborative Perception**|\u5f00\u653e\u5f02\u6784\u534f\u4f5c\u611f\u77e5\u7684\u53ef\u6269\u5c55\u6846\u67b6|Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen, Yanfeng Wang|Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. It also protects new agents' model details from disclosure since the training can be conducted by the agent owner locally. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. Code and data are available at: https://github.com/yifanlu0227/HEAL.|\u534f\u4f5c\u611f\u77e5\u65e8\u5728\u901a\u8fc7\u4fc3\u8fdb\u591a\u4e2a\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u6570\u636e\u4ea4\u6362\u6765\u51cf\u8f7b\u5355\u667a\u80fd\u4f53\u611f\u77e5\u7684\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u906e\u6321\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u8003\u8651\u4e86\u4e00\u79cd\u540c\u8d28\u573a\u666f\uff0c\u5176\u4e2d\u6240\u6709\u4ee3\u7406\u90fd\u4f7f\u7528\u8eab\u4efd\u4f20\u611f\u5668\u548c\u611f\u77e5\u6a21\u578b\u3002\u5b9e\u9645\u4e0a\uff0c\u5f02\u6784\u4ee3\u7406\u7c7b\u578b\u53ef\u80fd\u4f1a\u4e0d\u65ad\u51fa\u73b0\uff0c\u5e76\u4e14\u5728\u4e0e\u73b0\u6709\u4ee3\u7406\u534f\u4f5c\u65f6\u4e0d\u53ef\u907f\u514d\u5730\u9762\u4e34\u9886\u57df\u5dee\u8ddd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5f00\u653e\u5f02\u6784\u95ee\u9898\uff1a\u5982\u4f55\u5c06\u4e0d\u65ad\u51fa\u73b0\u7684\u65b0\u5f02\u6784\u4ee3\u7406\u7c7b\u578b\u5bb9\u7eb3\u5230\u534f\u4f5c\u611f\u77e5\u4e2d\uff0c\u540c\u65f6\u4fdd\u8bc1\u9ad8\u611f\u77e5\u6027\u80fd\u548c\u4f4e\u96c6\u6210\u6210\u672c\uff1f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HEterogeneous ALLiance (HEAL)\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u6269\u5c55\u534f\u4f5c\u611f\u77e5\u6846\u67b6\u3002 HEAL \u9996\u5148\u901a\u8fc7\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u524d\u666f\u611f\u77e5\u91d1\u5b57\u5854\u878d\u5408\u7f51\u7edc\u4e0e\u521d\u59cb\u4ee3\u7406\u5efa\u7acb\u7edf\u4e00\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u5f53\u5f02\u6784\u65b0\u4ee3\u7406\u4ee5\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u65b9\u5f0f\u6216\u6a21\u578b\u51fa\u73b0\u65f6\uff0c\u6211\u4eec\u901a\u8fc7\u521b\u65b0\u7684\u5411\u540e\u5bf9\u9f50\u5c06\u5b83\u4eec\u4e0e\u5df2\u5efa\u7acb\u7684\u7edf\u4e00\u7a7a\u95f4\u5bf9\u9f50\u3002\u6b64\u6b65\u9aa4\u53ea\u9700\u8981\u5bf9\u65b0\u7684Agent\u7c7b\u578b\u8fdb\u884c\u5355\u72ec\u8bad\u7ec3\uff0c\u56e0\u6b64\u8bad\u7ec3\u6210\u672c\u6781\u4f4e\uff0c\u53ef\u6269\u5c55\u6027\u9ad8\u3002\u5b83\u8fd8\u53ef\u4ee5\u4fdd\u62a4\u65b0\u4ee3\u7406\u7684\u6a21\u578b\u8be6\u7ec6\u4fe1\u606f\u4e0d\u88ab\u6cc4\u9732\uff0c\u56e0\u4e3a\u57f9\u8bad\u53ef\u4ee5\u7531\u4ee3\u7406\u6240\u6709\u8005\u5728\u672c\u5730\u8fdb\u884c\u3002\u4e3a\u4e86\u4e30\u5bcc\u667a\u80fd\u4f53\u7684\u6570\u636e\u5f02\u6784\u6027\uff0c\u6211\u4eec\u5e26\u6765\u4e86 OPV2V-H\uff0c\u4e00\u4e2a\u5177\u6709\u66f4\u591a\u6837\u5316\u4f20\u611f\u5668\u7c7b\u578b\u7684\u65b0\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u5728 OPV2V-H \u548c DAIR-V2X \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHEAL \u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86 SOTA \u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u96c6\u6210 3 \u79cd\u65b0\u4ee3\u7406\u7c7b\u578b\u65f6\u5c06\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u4e86 91.5%\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728\uff1ahttps://github.com/yifanlu0227/HEAL \u83b7\u53d6\u3002|[2401.13964v1](http://arxiv.org/pdf/2401.13964v1)|**[link](https://github.com/yifanlu0227/heal)**|\n", "2401.13959": "|**2024-01-25**|**Conditional Neural Video Coding with Spatial-Temporal Super-Resolution**|\u5177\u6709\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u7684\u6761\u4ef6\u795e\u7ecf\u89c6\u9891\u7f16\u7801|Henan Wang, Xiaohan Pan, Runsen Feng, Zongyu Guo, Zhibo Chen|This document is an expanded version of a one-page abstract originally presented at the 2024 Data Compression Conference. It describes our proposed method for the video track of the Challenge on Learned Image Compression (CLIC) 2024. Our scheme follows the typical hybrid coding framework with some novel techniques. Firstly, we adopt Spynet network to produce accurate motion vectors for motion estimation. Secondly, we introduce the context mining scheme with conditional frame coding to fully exploit the spatial-temporal information. As for the low target bitrates given by CLIC, we integrate spatial-temporal super-resolution modules to improve rate-distortion performance. Our team name is IMCLVC.|\u672c\u6587\u6863\u662f\u6700\u521d\u5728 2024 \u5e74\u6570\u636e\u538b\u7f29\u4f1a\u8bae\u4e0a\u63d0\u51fa\u7684\u4e00\u9875\u6458\u8981\u7684\u6269\u5c55\u7248\u672c\u3002\u5b83\u63cf\u8ff0\u4e86\u6211\u4eec\u4e3a 2024 \u5e74\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\u6311\u6218\u8d5b (CLIC) \u7684\u89c6\u9891\u8f68\u9053\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6848\u9075\u5faa\u5178\u578b\u7684\u6df7\u5408\u7f16\u7801\u6846\u67b6\u548c\u4e00\u4e9b\u65b0\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528Spynet\u7f51\u7edc\u6765\u4ea7\u751f\u7cbe\u786e\u7684\u8fd0\u52a8\u77e2\u91cf\u7528\u4e8e\u8fd0\u52a8\u4f30\u8ba1\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5e26\u6709\u6761\u4ef6\u5e27\u7f16\u7801\u7684\u4e0a\u4e0b\u6587\u6316\u6398\u65b9\u6848\uff0c\u4ee5\u5145\u5206\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u3002\u9488\u5bf9CLIC\u7ed9\u51fa\u7684\u4f4e\u76ee\u6807\u7801\u7387\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u6a21\u5757\u6765\u63d0\u9ad8\u7801\u7387\u5931\u771f\u6027\u80fd\u3002\u6211\u4eec\u7684\u56e2\u961f\u540d\u79f0\u662f IMCLVC\u3002|[2401.13959v1](http://arxiv.org/pdf/2401.13959v1)|null|\n"}}