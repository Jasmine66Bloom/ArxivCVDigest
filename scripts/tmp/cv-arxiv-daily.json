{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2402.00868": "|**2024-02-01**|**We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline**|\u6211\u4eec\u6ca1\u6709\u6709\u6548\u5730\u4f7f\u7528\u89c6\u9891\uff1a\u66f4\u65b0\u7684\u57df\u81ea\u9002\u5e94\u89c6\u9891\u5206\u5272\u57fa\u51c6|Simar Kareer, Vivek Vijaykumar, Harsh Maheshwari, Prithvijit Chattopadhyay, Judy Hoffman, Viraj Prabhu|There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA|\u5728\u7528\u4e8e\u8bed\u4e49\u5206\u5272\uff08DAS\uff09\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u9762\u5df2\u7ecf\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5de5\u4f5c\uff0c\u8bd5\u56fe\u5c06\u5728\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4ece\u6807\u8bb0\u7684\u6e90\u57df\u9002\u5e94\u5230\u672a\u6807\u8bb0\u7684\u76ee\u6807\u57df\u3002\u867d\u7136\u7edd\u5927\u591a\u6570\u5148\u524d\u7684\u5de5\u4f5c\u90fd\u5c06\u5176\u4f5c\u4e3a\u5e27\u7ea7\u56fe\u50cf DAS \u95ee\u9898\u8fdb\u884c\u7814\u7a76\uff0c\u4f46\u4e00\u4e9b\u89c6\u9891 DAS \u5de5\u4f5c\u8bd5\u56fe\u989d\u5916\u5229\u7528\u76f8\u90bb\u5e27\u4e2d\u5b58\u5728\u7684\u65f6\u95f4\u4fe1\u53f7\u3002\u7136\u800c\uff0cVideo-DAS \u5de5\u4f5c\u5386\u6765\u7814\u7a76\u4e86\u4e00\u7ec4\u4e0e Image-DAS \u4e0d\u540c\u7684\u57fa\u51c6\uff0c\u5e76\u4e14\u4ea4\u53c9\u57fa\u51c6\u6781\u5c11\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u8fd9\u4e00\u5dee\u8ddd\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0 (1) \u5373\u4f7f\u5728\u4ed4\u7ec6\u63a7\u5236\u6570\u636e\u548c\u6a21\u578b\u67b6\u6784\u4e4b\u540e\uff0c\u6700\u5148\u8fdb\u7684 Image-DAS \u65b9\u6cd5\uff08HRDA \u548c HRDA+MIC\uff09} \u5728\u5df2\u5efa\u7acb\u7684 Video-DAS \u57fa\u51c6\u4e0a\u4ecd\u4f18\u4e8e Video-DAS \u65b9\u6cd5\uff08+ Viper$\\rightarrow$CityscapesSeq \u4e0a\u7684 mIoU \u4e3a 14.5 mIoU\uff0cSynthia$\\rightarrow$CityscapesSeq \u4e0a\u7684 +19.0 mIoU\uff09\uff0c\u4ee5\u53ca (2) Image-DAS \u548c Video-DAS \u6280\u672f\u7684\u7b80\u5355\u7ec4\u5408\u53ea\u4f1a\u5e26\u6765\u8de8\u6570\u636e\u96c6\u7684\u8fb9\u9645\u6539\u8fdb\u3002\u4e3a\u4e86\u907f\u514d Image-DAS \u548c Video-DAS \u4e4b\u95f4\u7684\u5b64\u7acb\u8fdb\u5c55\uff0c\u6211\u4eec\u5f00\u6e90\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u5e93\uff0c\u5e76\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u652f\u6301\u4e00\u5957\u5168\u9762\u7684 Video-DAS \u548c Image-DAS \u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/SimarKareer/UnifiedVideoDA \u83b7\u53d6|[2402.00868v1](http://arxiv.org/pdf/2402.00868v1)|**[link](https://github.com/simarkareer/unifiedvideoda)**|\n", "2402.00865": "|**2024-02-01**|**Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection**|\u9762\u5411\u5206\u5e03\u5916\u68c0\u6d4b\u7684\u6700\u4f73\u7279\u5f81\u6574\u5f62\u65b9\u6cd5|Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould|Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.|\u7279\u5f81\u6574\u5f62\u662f\u6307\u5728\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u6027\u80fd\u7684\u4e00\u7cfb\u5217\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u6765\u81ea\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5012\u6570\u7b2c\u4e8c\u5c42\u6765\u64cd\u7eb5\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u533a\u5206\u5206\u5e03\u5185 (ID) \u548c OOD \u6837\u672c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7279\u5f81\u5851\u9020\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u548c OOD \u6570\u636e\u96c6\u624b\u52a8\u8bbe\u8ba1\u7684\u89c4\u5219\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u9996\u5148\u5236\u5b9a\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u7279\u5f81\u5851\u9020\u65b9\u6cd5\u7684\u62bd\u8c61\u4f18\u5316\u6846\u67b6\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f7f\u7528\u7b80\u5355\u7684\u5206\u6bb5\u5e38\u6570\u6574\u5f62\u51fd\u6570\u5bf9\u6846\u67b6\u8fdb\u884c\u5177\u4f53\u7b80\u5316\uff0c\u5e76\u8868\u660e\u73b0\u6709\u7684\u7279\u5f81\u6574\u5f62\u65b9\u6cd5\u8fd1\u4f3c\u4e8e\u5177\u4f53\u4f18\u5316\u95ee\u9898\u7684\u6700\u4f18\u89e3\u3002\u6b64\u5916\uff0c\u5047\u8bbe OOD \u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5f0f\uff0c\u4ec5\u5229\u7528 ID \u6570\u636e\u5373\u53ef\u751f\u6210\u5206\u6bb5\u5e38\u6570\u6574\u5f62\u51fd\u6570\u7684\u5c01\u95ed\u5f0f\u89e3\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8868\u660e\uff0c\u901a\u8fc7\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u5316\u7684\u7279\u5f81\u6574\u5f62\u51fd\u6570\u63d0\u9ad8\u4e86 OOD \u68c0\u6d4b\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2402.00865v1](http://arxiv.org/pdf/2402.00865v1)|**[link](https://github.com/qinyu-allen-zhao/optfsood)**|\n", "2402.00724": "|**2024-02-01**|**Automatic Segmentation of the Spinal Cord Nerve Rootlets**|\u810a\u9ad3\u795e\u7ecf\u6839\u7684\u81ea\u52a8\u5206\u5272|Jan Valosek, Theo Mathieu, Raphaelle Schlienger, Olivia S. Kowalczyk, Julien Cohen-Adad|Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions. The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.|\u810a\u795e\u7ecf\u6839\u7684\u7cbe\u786e\u8bc6\u522b\u4e0e\u63cf\u7ed8\u810a\u9ad3\u6c34\u5e73\u6709\u5173\uff0c\u4ee5\u7814\u7a76\u810a\u9ad3\u7684\u529f\u80fd\u6d3b\u52a8\u3002\u672c\u7814\u7a76\u7684\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece T2 \u52a0\u6743\u78c1\u5171\u632f\u6210\u50cf (MRI) \u626b\u63cf\u4e2d\u5bf9\u810a\u795e\u7ecf\u6839\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002\u6765\u81ea\u4e24\u4e2a\u5f00\u653e\u83b7\u53d6 MRI \u6570\u636e\u96c6\u7684\u56fe\u50cf\u7528\u4e8e\u8bad\u7ec3 3D \u591a\u7c7b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u6765\u5206\u5272 C2-C8 \u80cc\u795e\u7ecf\u6839\u3002\u6bcf\u4e2a\u8f93\u51fa\u7c7b\u522b\u5bf9\u5e94\u4e8e\u4e00\u4e2a\u810a\u67f1\u6c34\u5e73\u3002\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u7684 3T T2 \u52a0\u6743\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u7ad9\u70b9\u95f4\u3001\u4f1a\u8bdd\u95f4\u548c\u5206\u8fa8\u7387\u95f4\u7684\u53d8\u5f02\u6027\u3002\u6d4b\u8bd5 Dice \u5f97\u5206\u4e3a 0.67 \u00b1 0.16\uff08\u6839\u7cfb\u6c34\u5e73\u7684\u5e73\u5747\u503c \u00b1 \u6807\u51c6\u5dee\uff09\uff0c\u8868\u660e\u6027\u80fd\u826f\u597d\u3002\u8be5\u65b9\u6cd5\u8fd8\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u4f9b\u5e94\u5546\u95f4\u548c\u7ad9\u70b9\u95f4\u53d8\u5f02\u6027\uff08\u53d8\u5f02\u7cfb\u6570 <= 1.41 %\uff09\uff0c\u4ee5\u53ca\u8f83\u4f4e\u7684\u4f1a\u8bdd\u95f4\u53d8\u5f02\u6027\uff08\u53d8\u5f02\u7cfb\u6570 <= 1.30 %\uff09\uff0c\u8868\u660e\u4e0d\u540c MRI \u4f9b\u5e94\u5546\u3001\u7ad9\u70b9\u4e4b\u95f4\u7684\u7a33\u5b9a\u9884\u6d4b\uff0c\u548c\u4f1a\u8bae\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u5728\u810a\u9ad3\u5de5\u5177\u7bb1 (SCT) v6.2 \u53ca\u66f4\u9ad8\u7248\u672c\u4e2d\u968f\u65f6\u4f7f\u7528\u3002|[2402.00724v1](http://arxiv.org/pdf/2402.00724v1)|null|\n", "2402.00703": "|**2024-02-01**|**Vehicle Perception from Satellite**|\u536b\u661f\u8f66\u8f86\u611f\u77e5|Bin Zhao, Pengfei Han, Xuelong Li|Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.|\u536b\u661f\u80fd\u591f\u6355\u6349\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002\u5b83\u4f7f\u5f97\u536b\u661f\u5bf9\u8f66\u8f86\u7684\u611f\u77e5\u6210\u4e3a\u53ef\u80fd\u3002\u4e0e\u8857\u9053\u76d1\u63a7\u3001\u884c\u8f66\u8bb0\u5f55\u4eea\u7b49\u8bbe\u5907\u76f8\u6bd4\uff0c\u536b\u661f\u89c6\u9891\u63d0\u4f9b\u4e86\u66f4\u5e7f\u9614\u7684\u57ce\u5e02\u5c3a\u5ea6\u89c6\u91ce\uff0c\u4ece\u800c\u6355\u6349\u5e76\u663e\u793a\u5168\u7403\u7684\u4ea4\u901a\u52a8\u6001\u573a\u666f\u3002\u536b\u661f\u4ea4\u901a\u76d1\u63a7\u662f\u4e00\u9879\u5177\u6709\u5de8\u5927\u5e94\u7528\u6f5c\u529b\u7684\u65b0\u4efb\u52a1\uff0c\u5305\u62ec\u4ea4\u901a\u62e5\u5835\u9884\u6d4b\u3001\u8def\u5f84\u89c4\u5212\u3001\u8f66\u8f86\u8c03\u5ea6\u7b49\u3002\u5b9e\u9645\u4e0a\uff0c\u53d7\u5206\u8fa8\u7387\u548c\u89c6\u91ce\u7684\u9650\u5236\uff0c\u6355\u83b7\u7684\u8f66\u8f86\u975e\u5e38\u5c0f\uff08\u51e0\u4e2a\u50cf\u7d20\uff09\u5e76\u4e14\u79fb\u52a8\u7f13\u6162\u3002\u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u8fd9\u4e9b\u536b\u661f\u4f4d\u4e8e\u8fd1\u5730\u8f68\u9053\uff08LEO\uff09\u6765\u6355\u6349\u5982\u6b64\u9ad8\u5206\u8fa8\u7387\u7684\u89c6\u9891\uff0c\u56e0\u6b64\u80cc\u666f\u4e5f\u5728\u79fb\u52a8\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u536b\u661f\u89c6\u89d2\u7684\u4ea4\u901a\u76d1\u63a7\u662f\u4e00\u9879\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u4e3a\u4e86\u5438\u5f15\u66f4\u591a\u7814\u7a76\u4eba\u5458\u8fdb\u5165\u8fd9\u4e00\u9886\u57df\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u536b\u661f\u4ea4\u901a\u76d1\u63a7\u57fa\u51c6\u3002\u5b83\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u5305\u62ec\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u3001\u8ba1\u6570\u548c\u5bc6\u5ea6\u4f30\u8ba1\u3002\u8be5\u6570\u636e\u96c6\u57fa\u4e8e GTA-V \u5f55\u5236\u7684 12 \u4e2a\u536b\u661f\u89c6\u9891\u548c 14 \u4e2a\u5408\u6210\u89c6\u9891\u6784\u5efa\u3002\u5b83\u4eec\u88ab\u5206\u4e3a 408 \u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u5176\u4e2d\u5305\u542b 7,336 \u4e2a\u771f\u5b9e\u536b\u661f\u56fe\u50cf\u548c 1,960 \u4e2a\u5408\u6210\u56fe\u50cf\u3002\u603b\u5171\u6807\u6ce8\u4e86 128,801 \u8f86\u8f66\u8f86\uff0c\u6bcf\u5f20\u56fe\u50cf\u4e2d\u7684\u8f66\u8f86\u6570\u91cf\u4ece 0 \u5230 101 \u4e0d\u7b49\u3002\u5728\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u51e0\u79cd\u7ecf\u5178\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4ee5\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5206\u6790\u8fd9\u9879\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u672a\u6765\u7684\u524d\u666f\u3002\u8be5\u6570\u636e\u96c6\u4f4d\u4e8e\uff1ahttps://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos\u3002|[2402.00703v1](http://arxiv.org/pdf/2402.00703v1)|**[link](https://github.com/chenxi1510/vehicle-perception-from-satellite-videos)**|\n", "2402.00695": "|**2024-02-01**|**Approximating Optimal Morphing Attacks using Template Inversion**|\u4f7f\u7528\u6a21\u677f\u53cd\u8f6c\u8fd1\u4f3c\u6700\u4f73\u53d8\u5f62\u653b\u51fb|Laurent Colbois, Hatef Otroshi Shahreza, S\u00e9bastien Marcel|Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run. We hope this might facilitate the development of large scale deep morph datasets for training detection models.|\u6700\u8fd1\u7684\u5de5\u4f5c\u8bc1\u660e\u4e86\u53cd\u8f6c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u5d4c\u5165\u6765\u6062\u590d\u4ee4\u4eba\u4fe1\u670d\u7684\u4eba\u8138\u56fe\u50cf\u3002\u6211\u4eec\u5229\u7528\u8fd9\u79cd\u6a21\u677f\u53cd\u8f6c\u6a21\u578b\u6765\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u7684\u6df1\u5ea6\u53d8\u5f62\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u57fa\u4e8e\u53cd\u8f6c\u7406\u8bba\u4e0a\u7684\u6700\u4f73\u53d8\u5f62\u5d4c\u5165\uff0c\u8be5\u53d8\u5f62\u5d4c\u5165\u662f\u4f5c\u4e3a\u6e90\u56fe\u50cf\u7684\u4eba\u8138\u5d4c\u5165\u7684\u5e73\u5747\u503c\u800c\u83b7\u5f97\u7684\u3002\u6211\u4eec\u5c1d\u8bd5\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u4e24\u79cd\u53d8\u4f53\uff1a\u7b2c\u4e00\u79cd\u5229\u7528\u5b8c\u5168\u72ec\u7acb\u7684\u5d4c\u5165\u5230\u56fe\u50cf\u53cd\u8f6c\u6a21\u578b\uff0c\u800c\u7b2c\u4e8c\u79cd\u5229\u7528\u9884\u8bad\u7ec3 StyleGAN \u7f51\u7edc\u7684\u5408\u6210\u7f51\u7edc\u6765\u63d0\u9ad8\u53d8\u5f62\u771f\u5b9e\u6027\u3002\u6211\u4eec\u4ece\u591a\u4e2a\u6e90\u6570\u636e\u96c6\u751f\u6210\u53d8\u5f62\u653b\u51fb\uff0c\u5e76\u7814\u7a76\u8fd9\u4e9b\u653b\u51fb\u9488\u5bf9\u591a\u4e2a\u4eba\u8138\u8bc6\u522b\u7f51\u7edc\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u653b\u51fb\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u65b9\u9762\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53d8\u5f62\u751f\u6210\u7684\u5148\u524d\u6700\u5148\u8fdb\u6280\u672f\u7ade\u4e89\u5e76\u5b9a\u671f\u51fb\u8d25\uff0c\u5e76\u4e14\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u5f00\u53d1\u7528\u4e8e\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u7684\u5927\u89c4\u6a21\u6df1\u5ea6\u53d8\u5f62\u6570\u636e\u96c6\u3002|[2402.00695v1](http://arxiv.org/pdf/2402.00695v1)|null|\n", "2402.00692": "|**2024-02-01**|**A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation**|\u6784\u5efa\u70b9\u4e91\u6e05\u7406\u3001\u5e73\u9762\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u7684\u6846\u67b6|Ilyass Abouelaziz, Youssef Mourchid|This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u89e3\u51b3\u5efa\u7b51\u70b9\u4e91\u6e05\u7406\u3001\u5e73\u9762\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u6240\u6d89\u53ca\u7684\u6311\u6218\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u589e\u5f3a\u5efa\u7b51\u5efa\u6a21\u3002\u6211\u4eec\u5728\u6e05\u7406\u9636\u6bb5\u7684\u91cd\u70b9\u662f\u901a\u8fc7\u91c7\u7528\u57fa\u4e8e z \u5206\u6570\u6d4b\u91cf\u7684\u81ea\u9002\u5e94\u9608\u503c\u6280\u672f\u4ece\u83b7\u53d6\u7684\u70b9\u4e91\u6570\u636e\u4e2d\u5220\u9664\u5f02\u5e38\u503c\u3002\u5728\u6e05\u6d01\u8fc7\u7a0b\u4e4b\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f3a\u5927\u7684 RANSAC \u8303\u5f0f\u6267\u884c\u5e73\u9762\u68c0\u6d4b\u3002\u76ee\u6807\u662f\u8fdb\u884c\u591a\u4e2a\u5e73\u9762\u5206\u5272\uff0c\u5e76\u5c06\u7247\u6bb5\u5206\u4e3a\u4e0d\u540c\u7684\u7c7b\u522b\uff0c\u4f8b\u5982\u5730\u677f\u3001\u5929\u82b1\u677f\u548c\u5899\u58c1\u3002\u751f\u6210\u7684\u7247\u6bb5\u53ef\u4ee5\u751f\u6210\u4ee3\u8868\u5efa\u7b51\u7269\u5efa\u7b51\u5143\u7d20\u7684\u51c6\u786e\u4e14\u8be6\u7ec6\u7684\u70b9\u4e91\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u89e3\u51b3\u4e86\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5728\u5efa\u7b51\u7269\u5185\u4e0d\u540c\u7ec4\u4ef6\uff08\u4f8b\u5982\u5899\u58c1\u3001\u7a97\u6237\u3001\u95e8\u3001\u5c4b\u9876\u548c\u7269\u4f53\uff09\u7684\u8bc6\u522b\u548c\u5206\u7c7b\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u53d7 PointNet \u67b6\u6784\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5efa\u7b51\u7269\u4e2d\u7684\u9ad8\u6548\u8bed\u4e49\u5206\u5272\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5904\u7406\u5efa\u7b51\u5efa\u6a21\u4efb\u52a1\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u9ad8\u5efa\u7b51\u5efa\u6a21\u9886\u57df\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.00692v1](http://arxiv.org/pdf/2402.00692v1)|null|\n", "2402.00593": "|**2024-02-01**|**Coronary Artery Disease Classification with Different Lesion Degree Ranges based on Deep Learning**|\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e0d\u540c\u75c5\u53d8\u7a0b\u5ea6\u8303\u56f4\u7684\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u5206\u7c7b|Ariadna Jim\u00e9nez-Partinen, Karl Thurnhofer-Hemsi, Esteban J. Palomo, Jorge Rodr\u00edguez-Capit\u00e1n, Ana I. Molina-Ramos|Invasive Coronary Angiography (ICA) images are considered the gold standard for assessing the state of the coronary arteries. Deep learning classification methods are widely used and well-developed in different areas where medical imaging evaluation has an essential impact due to the development of computer-aided diagnosis systems that can support physicians in their clinical procedures. In this paper, a new performance analysis of deep learning methods for binary ICA classification with different lesion degrees is reported. To reach this goal, an annotated dataset of ICA images that contains the ground truth, the location of lesions and seven possible severity degrees ranging between 0% and 100% was employed. The ICA images were divided into 'lesion' or 'non-lesion' patches. We aim to study how binary classification performance is affected by the different lesion degrees considered in the positive class. Therefore, five known convolutional neural network architectures were trained with different input images where different lesion degree ranges were gradually incorporated until considering the seven lesion degrees. Besides, four types of experiments with and without data augmentation were designed, whose F-measure and Area Under Curve (AUC) were computed. Reported results achieved an F-measure and AUC of 92.7% and 98.1%, respectively. However, lesion classification is highly affected by the degree of the lesion intended to classify, with 15% less accuracy when <99% lesion patches are present.|\u4fb5\u5165\u6027\u51a0\u72b6\u52a8\u8109\u9020\u5f71 (ICA) \u56fe\u50cf\u88ab\u8ba4\u4e3a\u662f\u8bc4\u4f30\u51a0\u72b6\u52a8\u8109\u72b6\u6001\u7684\u91d1\u6807\u51c6\u3002\u7531\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u53d1\u5c55\u53ef\u4ee5\u652f\u6301\u533b\u751f\u7684\u4e34\u5e8a\u64cd\u4f5c\uff0c\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u5f71\u54cd\u7684\u4e0d\u540c\u9886\u57df\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002\u672c\u6587\u62a5\u544a\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e0d\u540c\u75c5\u53d8\u7a0b\u5ea6\u7684\u4e8c\u5143 ICA \u5206\u7c7b\u7684\u6027\u80fd\u5206\u6790\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u5e26\u6ce8\u91ca\u7684 ICA \u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u771f\u5b9e\u60c5\u51b5\u3001\u75c5\u53d8\u4f4d\u7f6e\u4ee5\u53ca 0% \u5230 100% \u4e4b\u95f4\u7684\u4e03\u79cd\u53ef\u80fd\u7684\u4e25\u91cd\u7a0b\u5ea6\u3002 ICA \u56fe\u50cf\u88ab\u5206\u4e3a\u201c\u75c5\u53d8\u201d\u6216\u201c\u975e\u75c5\u53d8\u201d\u6591\u5757\u3002\u6211\u4eec\u7684\u76ee\u7684\u662f\u7814\u7a76\u4e8c\u5143\u5206\u7c7b\u6027\u80fd\u5982\u4f55\u53d7\u5230\u6b63\u7c7b\u4e2d\u8003\u8651\u7684\u4e0d\u540c\u75c5\u53d8\u7a0b\u5ea6\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u8f93\u5165\u56fe\u50cf\u6765\u8bad\u7ec3\u4e94\u79cd\u5df2\u77e5\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5176\u4e2d\u9010\u6e10\u5408\u5e76\u4e0d\u540c\u7684\u75c5\u53d8\u7a0b\u5ea6\u8303\u56f4\uff0c\u76f4\u5230\u8003\u8651\u4e03\u4e2a\u75c5\u53d8\u7a0b\u5ea6\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u7c7b\u578b\u7684\u6709\u548c\u6ca1\u6709\u6570\u636e\u589e\u5f3a\u7684\u5b9e\u9a8c\uff0c\u8ba1\u7b97\u4e86\u5176F-measure\u548c\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u3002\u62a5\u544a\u7ed3\u679c\u7684 F \u6d4b\u91cf\u503c\u548c AUC \u5206\u522b\u4e3a 92.7% \u548c 98.1%\u3002\u7136\u800c\uff0c\u75c5\u53d8\u5206\u7c7b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u8981\u5206\u7c7b\u7684\u75c5\u53d8\u7a0b\u5ea6\u7684\u5f71\u54cd\uff0c\u5f53\u5b58\u5728 <99% \u7684\u75c5\u53d8\u6591\u5757\u65f6\uff0c\u51c6\u786e\u5ea6\u4f1a\u964d\u4f4e 15%\u3002|[2402.00593v1](http://arxiv.org/pdf/2402.00593v1)|null|\n", "2402.00570": "|**2024-02-01**|**CADICA: a new dataset for coronary artery disease detection by using invasive coronary angiography**|CADICA\uff1a\u4f7f\u7528\u4fb5\u5165\u6027\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u68c0\u6d4b\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u7684\u65b0\u6570\u636e\u96c6|Ariadna Jim\u00e9nez-Partinen, Miguel A. Molina-Cabello, Karl Thurnhofer-Hemsi, Esteban J. Palomo, Jorge Rodr\u00edguez-Capit\u00e1n, Ana I. Molina-Ramos, Manuel Jim\u00e9nez-Navarro|Coronary artery disease (CAD) remains the leading cause of death globally and invasive coronary angiography (ICA) is considered the gold standard of anatomical imaging evaluation when CAD is suspected. However, risk evaluation based on ICA has several limitations, such as visual assessment of stenosis severity, which has significant interobserver variability. This motivates to development of a lesion classification system that can support specialists in their clinical procedures. Although deep learning classification methods are well-developed in other areas of medical imaging, ICA image classification is still at an early stage. One of the most important reasons is the lack of available and high-quality open-access datasets. In this paper, we reported a new annotated ICA images dataset, CADICA, to provide the research community with a comprehensive and rigorous dataset of coronary angiography consisting of a set of acquired patient videos and associated disease-related metadata. This dataset can be used by clinicians to train their skills in angiographic assessment of CAD severity and by computer scientists to create computer-aided diagnostic systems to help in such assessment. In addition, baseline classification methods are proposed and analyzed, validating the functionality of CADICA and giving the scientific community a starting point to improve CAD detection.|\u51a0\u72b6\u52a8\u8109\u75be\u75c5 (CAD) \u4ecd\u7136\u662f\u5168\u7403\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5f53\u6000\u7591\u60a3\u6709 CAD \u65f6\uff0c\u4fb5\u5165\u6027\u51a0\u72b6\u52a8\u8109\u9020\u5f71 (ICA) \u88ab\u8ba4\u4e3a\u662f\u89e3\u5256\u6210\u50cf\u8bc4\u4f30\u7684\u91d1\u6807\u51c6\u3002\u7136\u800c\uff0c\u57fa\u4e8e ICA \u7684\u98ce\u9669\u8bc4\u4f30\u6709\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u72ed\u7a84\u4e25\u91cd\u7a0b\u5ea6\u7684\u89c6\u89c9\u8bc4\u4f30\uff0c\u89c2\u5bdf\u8005\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u5dee\u5f02\u3002\u8fd9\u63a8\u52a8\u4e86\u75c5\u53d8\u5206\u7c7b\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u652f\u6301\u4e13\u5bb6\u7684\u4e34\u5e8a\u64cd\u4f5c\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\u5728\u533b\u5b66\u6210\u50cf\u7684\u5176\u4ed6\u9886\u57df\u5df2\u7ecf\u5f88\u6210\u719f\uff0c\u4f46 ICA \u56fe\u50cf\u5206\u7c7b\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\u3002\u6700\u91cd\u8981\u7684\u539f\u56e0\u4e4b\u4e00\u662f\u7f3a\u4e4f\u53ef\u7528\u4e14\u9ad8\u8d28\u91cf\u7684\u5f00\u653e\u83b7\u53d6\u6570\u636e\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u62a5\u544a\u4e86\u4e00\u4e2a\u65b0\u7684\u5e26\u6ce8\u91ca\u7684 ICA \u56fe\u50cf\u6570\u636e\u96c6 CADICA\uff0c\u4e3a\u7814\u7a76\u754c\u63d0\u4f9b\u5168\u9762\u4e14\u4e25\u683c\u7684\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u7ec4\u91c7\u96c6\u7684\u60a3\u8005\u89c6\u9891\u548c\u76f8\u5173\u7684\u75be\u75c5\u76f8\u5173\u5143\u6570\u636e\u3002\u4e34\u5e8a\u533b\u751f\u53ef\u4ee5\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u6765\u57f9\u8bad\u4ed6\u4eec\u5bf9 CAD \u4e25\u91cd\u7a0b\u5ea6\u8fdb\u884c\u8840\u7ba1\u9020\u5f71\u8bc4\u4f30\u7684\u6280\u80fd\uff0c\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u53ef\u4ee5\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u6765\u521b\u5efa\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u6765\u5e2e\u52a9\u8fdb\u884c\u6b64\u7c7b\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u57fa\u7ebf\u5206\u7c7b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86 CADICA \u7684\u529f\u80fd\uff0c\u5e76\u4e3a\u79d1\u5b66\u754c\u63d0\u4f9b\u4e86\u6539\u8fdb CAD \u68c0\u6d4b\u7684\u8d77\u70b9\u3002|[2402.00570v1](http://arxiv.org/pdf/2402.00570v1)|null|\n", "2402.00564": "|**2024-02-01**|**A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification**|\u60a8\u53ea\u9700\u8981\u4e00\u4e2a\u56fe\u5377\u79ef\u5373\u53ef\uff1a\u9ad8\u6548\u7684\u7070\u5ea6\u56fe\u50cf\u5206\u7c7b|Jacob Fein-Ashley, Tian Ye, Sachini Wickramasinghe, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna|Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance. Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets.|\u56fe\u50cf\u5206\u7c7b\u5668\u901a\u5e38\u4f9d\u8d56\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u6765\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u672c\u8d28\u4e0a\u6bd4\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u66f4\u91cd\u91cf\u7ea7\uff0c\u8fd9\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u53ef\u80fd\u4f1a\u51fa\u73b0\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u540c\u65f6\u9002\u7528\u4e8e RGB \u548c\u7070\u5ea6\u6570\u636e\u96c6\u3002\u4ec5\u5bf9\u7070\u5ea6\u56fe\u50cf\u8fdb\u884c\u64cd\u4f5c\u7684\u5206\u7c7b\u5668\u5e76\u4e0d\u5e38\u89c1\u3002\u7070\u5ea6\u56fe\u50cf\u5206\u7c7b\u5177\u6709\u591a\u79cd\u5e94\u7528\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\uff08ATR\uff09\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u56fe\u50cf\u77e2\u91cf\u5316\u89c6\u56fe\u7684\u65b0\u9896\u7684\u7070\u5ea6\uff08\u5355\u901a\u9053\uff09\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u56fe\u50cf\u89c6\u4e3a\u5411\u91cf\u5e76\u5c06\u95ee\u9898\u8bbe\u7f6e\u51cf\u5c11\u4e3a\u7070\u5ea6\u56fe\u50cf\u5206\u7c7b\u8bbe\u7f6e\u6765\u5229\u7528 MLP \u7684\u8f7b\u91cf\u6027\u3002\u6211\u4eec\u53d1\u73b0\u6279\u91cf\u4f7f\u7528\u5355\u4e2a\u56fe\u5377\u79ef\u5c42\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6a21\u578b\u6027\u80fd\u7684\u65b9\u5dee\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728 FPGA \u4e0a\u4e3a\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u5236\u52a0\u901f\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u9879\u4f18\u5316\u4ee5\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u6211\u4eec\u5728\u57fa\u51c6\u7070\u5ea6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u5404\u79cd\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5927\u5927\u964d\u4f4e\u7684\u5ef6\u8fdf\uff08\u6700\u591a\u51cf\u5c11 16 \u7f8e\u5143\\\u500d$\uff09\u548c\u5177\u6709\u7ade\u4e89\u529b\u6216\u9886\u5148\u7684\u6027\u80fd\u3002\u7279\u5b9a\u9886\u57df\u7684\u7070\u5ea6\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u3002|[2402.00564v1](http://arxiv.org/pdf/2402.00564v1)|null|\n", "2402.00541": "|**2024-02-01**|**Masked Conditional Diffusion Model for Enhancing Deepfake Detection**|\u7528\u4e8e\u589e\u5f3a Deepfake \u68c0\u6d4b\u7684\u5c4f\u853d\u6761\u4ef6\u6269\u6563\u6a21\u578b|Tiewen Chen, Shanmin Yang, Shu Hu, Zhenghan Fang, Ying Fu, Xi Wu, Xin Wang|Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset. However, their results severely degrade when confronted with forged samples that the model has not yet seen during training. In this paper, deepfake data to help detect deepfakes. this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection. It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts. Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models.|\u5f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4eba\u8138\u6765\u81ea\u540c\u4e00\u6570\u636e\u96c6\u65f6\uff0c\u6700\u8fd1\u5173\u4e8e\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u5f53\u9047\u5230\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c1a\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u6837\u672c\u65f6\uff0c\u4ed6\u4eec\u7684\u7ed3\u679c\u4f1a\u4e25\u91cd\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0cdeepfake \u6570\u636e\u53ef\u4ee5\u5e2e\u52a9\u68c0\u6d4bdeepfakes\u3002\u672c\u6587\u63d0\u51fa\u4e86\u6211\u4eec\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u589e\u5f3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u63a9\u6a21\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08MCDM\uff09\u3002\u5b83\u4ece\u8499\u9762\u7684\u539f\u59cb\u4eba\u8138\u4e2d\u751f\u6210\u5404\u79cd\u4f2a\u9020\u7684\u4eba\u8138\uff0c\u4ece\u800c\u9f13\u52b1\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5b66\u4e60\u901a\u7528\u4e14\u7a33\u5065\u7684\u8868\u793a\uff0c\u800c\u4e0d\u4f1a\u8fc7\u5ea6\u62df\u5408\u7279\u6b8a\u7684\u4f2a\u5f71\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u7684\u4f2a\u9020\u56fe\u50cf\u8d28\u91cf\u5f88\u9ad8\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002|[2402.00541v1](http://arxiv.org/pdf/2402.00541v1)|null|\n", "2402.00534": "|**2024-02-01**|**A Manifold Representation of the Key in Vision Transformers**|\u89c6\u89c9\u53d8\u5f62\u91d1\u521a\u5173\u952e\u7684\u591a\u79cd\u8868\u793a|Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad|Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the budget of such representations and aim for further performance improvements based on our findings.|Vision Transformers \u901a\u8fc7\u5806\u53e0\u591a\u4e2a\u6ce8\u610f\u529b\u5757\u6765\u5b9e\u73b0\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MSA\uff09\u3002\u67e5\u8be2\u3001\u952e\u548c\u503c\u901a\u5e38\u4ea4\u7ec7\u5728\u4e00\u8d77\uff0c\u5e76\u901a\u8fc7\u5355\u4e2a\u5171\u4eab\u7ebf\u6027\u8f6c\u6362\u5728\u8fd9\u4e9b\u5757\u4e2d\u751f\u6210\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u952e\u4e0e\u67e5\u8be2\u548c\u503c\u5206\u5f00\u7684\u6982\u5ff5\uff0c\u5e76\u91c7\u7528\u952e\u7684\u6d41\u5f62\u8868\u793a\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89e3\u8026\u5e76\u8d4b\u4e88\u5bc6\u94a5\u6d41\u5f62\u7ed3\u6784\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0cViT-B \u5728 ImageNet-1K \u6570\u636e\u96c6\u4e0a\u7684 top-1 \u51c6\u786e\u7387\u63d0\u9ad8\u4e86 0.87%\uff0c\u800c Swin-T \u5728\u6d41\u5f62\u952e\u4e2d\u6709 8 \u4e2a\u56fe\u8868\u7684 top-1 \u51c6\u786e\u7387\u63d0\u9ad8\u4e86 0.52%\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u5728 COCO \u6570\u636e\u96c6\u4e0a\u7684\u5bf9\u8c61\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u4ea7\u751f\u4e86\u79ef\u6781\u7684\u7ed3\u679c\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\uff0c\u6211\u4eec\u786e\u5b9a\u8fd9\u4e9b\u6027\u80fd\u63d0\u5347\u4e0d\u4ec5\u4ec5\u662f\u7531\u4e8e\u6dfb\u52a0\u66f4\u591a\u53c2\u6570\u548c\u8ba1\u7b97\u7684\u7b80\u5355\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u80fd\u4f1a\u8c03\u67e5\u524a\u51cf\u6b64\u7c7b\u4ee3\u8868\u9884\u7b97\u7684\u7b56\u7565\uff0c\u5e76\u6839\u636e\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7ee9\u6548\u3002|[2402.00534v1](http://arxiv.org/pdf/2402.00534v1)|null|\n", "2402.00481": "|**2024-02-01**|**Bias Mitigating Few-Shot Class-Incremental Learning**|\u51cf\u5c11\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u504f\u5dee|Li-Jun Zhao, Zhen-Duo Chen, Zi-Chao Zhang, Xin Luo, Xin-Shun Xu|Few-shot class-incremental learning (FSCIL) aims at recognizing novel classes continually with limited novel class samples. A mainstream baseline for FSCIL is first to train the whole model in the base session, then freeze the feature extractor in the incremental sessions. Despite achieving high overall accuracy, most methods exhibit notably low accuracy for incremental classes. Some recent methods somewhat alleviate the accuracy imbalance between base and incremental classes by fine-tuning the feature extractor in the incremental sessions, but they further cause the accuracy imbalance between past and current incremental classes. In this paper, we study the causes of such classification accuracy imbalance for FSCIL, and abstract them into a unified model bias problem. Based on the analyses, we propose a novel method to mitigate model bias of the FSCIL problem during training and inference processes, which includes mapping ability stimulation, separately dual-feature classification, and self-optimizing classifiers. Extensive experiments on three widely-used FSCIL benchmark datasets show that our method significantly mitigates the model bias problem and achieves state-of-the-art performance.|\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u65e8\u5728\u5229\u7528\u6709\u9650\u7684\u65b0\u7c7b\u6837\u672c\u4e0d\u65ad\u8bc6\u522b\u65b0\u7c7b\u3002 FSCIL \u7684\u4e3b\u6d41\u57fa\u7ebf\u662f\u9996\u5148\u5728\u57fa\u672c\u4f1a\u8bdd\u4e2d\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u7136\u540e\u5728\u589e\u91cf\u4f1a\u8bdd\u4e2d\u51bb\u7ed3\u7279\u5f81\u63d0\u53d6\u5668\u3002\u5c3d\u7ba1\u603b\u4f53\u51c6\u786e\u7387\u5f88\u9ad8\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u5bf9\u4e8e\u589e\u91cf\u7c7b\u7684\u51c6\u786e\u7387\u660e\u663e\u8f83\u4f4e\u3002\u6700\u8fd1\u7684\u4e00\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u589e\u91cf\u4f1a\u8bdd\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u5668\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u4e86\u57fa\u7c7b\u548c\u589e\u91cf\u7c7b\u4e4b\u95f4\u7684\u51c6\u786e\u6027\u4e0d\u5e73\u8861\uff0c\u4f46\u5b83\u4eec\u8fdb\u4e00\u6b65\u5bfc\u81f4\u4e86\u8fc7\u53bb\u548c\u5f53\u524d\u589e\u91cf\u7c7b\u4e4b\u95f4\u7684\u51c6\u786e\u6027\u4e0d\u5e73\u8861\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86 FSCIL \u8fd9\u79cd\u5206\u7c7b\u7cbe\u5ea6\u4e0d\u5e73\u8861\u7684\u539f\u56e0\uff0c\u5e76\u5c06\u5176\u62bd\u8c61\u4e3a\u7edf\u4e00\u7684\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002\u57fa\u4e8e\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u51cf\u8f7b\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d FSCIL \u95ee\u9898\u7684\u6a21\u578b\u504f\u5dee\uff0c\u5176\u4e2d\u5305\u62ec\u6620\u5c04\u80fd\u529b\u523a\u6fc0\u3001\u5355\u72ec\u7684\u53cc\u7279\u5f81\u5206\u7c7b\u548c\u81ea\u4f18\u5316\u5206\u7c7b\u5668\u3002\u5bf9\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684 FSCIL \u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\u51cf\u8f7b\u4e86\u6a21\u578b\u504f\u5dee\u95ee\u9898\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2402.00481v1](http://arxiv.org/pdf/2402.00481v1)|null|\n", "2402.00467": "|**2024-02-01**|**Can you see me now? Blind spot estimation for autonomous vehicles using scenario-based simulation with random reference sensors**|\u4f60\u73b0\u5728\u80fd\u770b\u89c1\u6211\u5417\uff1f\u4f7f\u7528\u57fa\u4e8e\u573a\u666f\u7684\u6a21\u62df\u548c\u968f\u673a\u53c2\u8003\u4f20\u611f\u5668\u6765\u4f30\u8ba1\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u76f2\u70b9|Marc Uecker, J. Marius Z\u00f6llner|In this paper, we introduce a method for estimating blind spots for sensor setups of autonomous or automated vehicles and/or robotics applications. In comparison to previous methods that rely on geometric approximations, our presented approach provides more realistic coverage estimates by utilizing accurate and detailed 3D simulation environments. Our method leverages point clouds from LiDAR sensors or camera depth images from high-fidelity simulations of target scenarios to provide accurate and actionable visibility estimates. A Monte Carlo-based reference sensor simulation enables us to accurately estimate blind spot size as a metric of coverage, as well as detection probabilities of objects at arbitrary positions.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f30\u8ba1\u81ea\u4e3b\u6216\u81ea\u52a8\u5316\u8f66\u8f86\u548c/\u6216\u673a\u5668\u4eba\u5e94\u7528\u7684\u4f20\u611f\u5668\u8bbe\u7f6e\u76f2\u70b9\u7684\u65b9\u6cd5\u3002\u4e0e\u4ee5\u524d\u4f9d\u8d56\u51e0\u4f55\u8fd1\u4f3c\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u51c6\u786e\u4e14\u8be6\u7ec6\u7684 3D \u6a21\u62df\u73af\u5883\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u8986\u76d6\u8303\u56f4\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u6765\u81ea LiDAR \u4f20\u611f\u5668\u7684\u70b9\u4e91\u6216\u6765\u81ea\u76ee\u6807\u573a\u666f\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u76f8\u673a\u6df1\u5ea6\u56fe\u50cf\u6765\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u64cd\u4f5c\u7684\u80fd\u89c1\u5ea6\u4f30\u8ba1\u3002\u57fa\u4e8e\u8499\u7279\u5361\u7f57\u7684\u53c2\u8003\u4f20\u611f\u5668\u6a21\u62df\u4f7f\u6211\u4eec\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u76f2\u70b9\u5927\u5c0f\u4f5c\u4e3a\u8986\u76d6\u8303\u56f4\u7684\u5ea6\u91cf\uff0c\u4ee5\u53ca\u4efb\u610f\u4f4d\u7f6e\u7269\u4f53\u7684\u68c0\u6d4b\u6982\u7387\u3002|[2402.00467v1](http://arxiv.org/pdf/2402.00467v1)|**[link](https://github.com/pyrestone/numpy_cukd)**|\n", "2402.00448": "|**2024-02-01**|**Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection**|\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u5b66\u751f\u77e5\u8bc6\u84b8\u998f\u7f51\u7edc|Liyi Yao, Shaobing Gao|Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.|\u7531\u4e8e\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7f3a\u9677\u7684\u591a\u6837\u6027\uff0c\u5b66\u751f-\u6559\u5e08\u7f51\u7edc\uff08S-T\uff09\u5728\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d7\u5230\u9752\u7750\uff0c\u5b83\u63a2\u7d22\u4ece\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5f97\u51fa\u7684\u7279\u5f81\u8868\u793a\u7684\u5dee\u5f02\u6765\u8bc6\u522b\u5f02\u5e38\u3002\u7136\u800c\uff0c\u666e\u901a\u7684 S-T \u7f51\u7edc\u5e76\u4e0d\u7a33\u5b9a\u3002\u91c7\u7528\u76f8\u540c\u7684\u7ed3\u6784\u6784\u5efaS-T\u7f51\u7edc\u53ef\u4ee5\u51cf\u5f31\u5f02\u5e38\u7684\u4ee3\u8868\u6027\u5dee\u5f02\u3002\u4f46\u4f7f\u7528\u4e0d\u540c\u7684\u7ed3\u6784\u4f1a\u589e\u52a0\u6b63\u5e38\u6570\u636e\u4e0a\u51fa\u73b0\u4e0d\u540c\u6027\u80fd\u7684\u53ef\u80fd\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5b66\u751f\u77e5\u8bc6\u84b8\u998f\uff08DSKD\uff09\u67b6\u6784\u3002\u4e0e\u5176\u4ed6 S-T \u7f51\u7edc\u4e0d\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u5b66\u751f\u7f51\u7edc\u548c\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u7f51\u7edc\uff0c\u5176\u4e2d\u5b66\u751f\u5177\u6709\u76f8\u540c\u7684\u89c4\u6a21\u4f46\u7ed3\u6784\u76f8\u53cd\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u589e\u5f3a\u84b8\u998f\u6548\u679c\uff0c\u63d0\u9ad8\u6b63\u5e38\u6570\u636e\u8bc6\u522b\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5f15\u5165\u5f02\u5e38\u8868\u793a\u7684\u591a\u6837\u6027\u3002\u4e3a\u4e86\u63a2\u7d22\u9ad8\u7ef4\u8bed\u4e49\u4fe1\u606f\u4ee5\u6355\u83b7\u5f02\u5e38\u7ebf\u7d22\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e24\u79cd\u7b56\u7565\u3002\u9996\u5148\uff0c\u91c7\u7528\u91d1\u5b57\u5854\u5339\u914d\u6a21\u5f0f\u5bf9\u7f51\u7edc\u4e2d\u95f4\u5c42\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u56fe\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u5d4c\u5165\u6a21\u5757\u4fc3\u8fdb\u4e24\u4e2a\u5b66\u751f\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u8be5\u6a21\u5757\u7684\u7075\u611f\u6765\u81ea\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5c0f\u7ec4\u8ba8\u8bba\u3002\u5728\u5206\u7c7b\u65b9\u9762\uff0c\u6211\u4eec\u901a\u8fc7\u6d4b\u91cf\u6559\u5e08\u548c\u5b66\u751f\u7f51\u7edc\u7684\u8f93\u51fa\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u83b7\u5f97\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u56fe\uff0c\u4ece\u4e2d\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u4ee5\u8fdb\u884c\u6837\u672c\u5224\u5b9a\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30 DSKD\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u63a2\u8ba8\u5185\u90e8\u6a21\u5757\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0cDSKD \u53ef\u4ee5\u5728 ResNet18 \u7b49\u5c0f\u578b\u6a21\u578b\u4e0a\u5b9e\u73b0\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5e76\u6709\u6548\u6539\u8fdb\u666e\u901a S-T \u7f51\u7edc\u3002|[2402.00448v1](http://arxiv.org/pdf/2402.00448v1)|null|\n", "2402.00422": "|**2024-02-01**|**Lightweight Pixel Difference Networks for Efficient Visual Representation Learning**|\u7528\u4e8e\u9ad8\u6548\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u50cf\u7d20\u5dee\u5206\u7f51\u7edc|Zhuo Su, Jiehua Zhang, Longguang Wang, Hua Zhang, Zhen Liu, Matti Pietik\u00e4inen, Li Liu|Recently, there have been tremendous efforts in developing lightweight Deep Neural Networks (DNNs) with satisfactory accuracy, which can enable the ubiquitous deployment of DNNs in edge devices. The core challenge of developing compact and efficient DNNs lies in how to balance the competing goals of achieving high accuracy and high efficiency. In this paper we propose two novel types of convolutions, dubbed \\emph{Pixel Difference Convolution (PDC) and Binary PDC (Bi-PDC)} which enjoy the following benefits: capturing higher-order local differential information, computationally efficient, and able to be integrated with existing DNNs. With PDC and Bi-PDC, we further present two lightweight deep networks named \\emph{Pixel Difference Networks (PiDiNet)} and \\emph{Binary PiDiNet (Bi-PiDiNet)} respectively to learn highly efficient yet more accurate representations for visual tasks including edge detection and object recognition. Extensive experiments on popular datasets (BSDS500, ImageNet, LFW, YTF, \\emph{etc.}) show that PiDiNet and Bi-PiDiNet achieve the best accuracy-efficiency trade-off. For edge detection, PiDiNet is the first network that can be trained without ImageNet, and can achieve the human-level performance on BSDS500 at 100 FPS and with $<$1M parameters. For object recognition, among existing Binary DNNs, Bi-PiDiNet achieves the best accuracy and a nearly $2\\times$ reduction of computational cost on ResNet18. Code available at \\href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet}.|\u6700\u8fd1\uff0c\u4eba\u4eec\u5728\u5f00\u53d1\u5177\u6709\u4ee4\u4eba\u6ee1\u610f\u7684\u7cbe\u5ea6\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u65b9\u9762\u4ed8\u51fa\u4e86\u5de8\u5927\u7684\u52aa\u529b\uff0c\u8fd9\u4f7f\u5f97 DNN \u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e2d\u666e\u904d\u90e8\u7f72\u3002\u5f00\u53d1\u7d27\u51d1\u9ad8\u6548\u7684 DNN \u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5e73\u8861\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u7ade\u4e89\u76ee\u6807\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u5377\u79ef\u7c7b\u578b\uff0c\u79f0\u4e3a\\emph{\u50cf\u7d20\u5dee\u5206\u5377\u79ef\uff08PDC\uff09\u548c\u4e8c\u8fdb\u5236PDC\uff08Bi-PDC\uff09}\uff0c\u5b83\u4eec\u5177\u6709\u4ee5\u4e0b\u4f18\u70b9\uff1a\u6355\u83b7\u9ad8\u9636\u5c40\u90e8\u5dee\u5206\u4fe1\u606f\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5e76\u4e14\u80fd\u591f\u4e0e\u73b0\u6709\u7684 DNN \u96c6\u6210\u3002\u901a\u8fc7 PDC \u548c Bi-PDC\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u7f51\u7edc\uff0c\u5206\u522b\u540d\u4e3a \\emph{Pixel Difference Networks (PiDiNet)} \u548c \\emph{Binary PiDiNet (Bi-PiDiNet)}\uff0c\u4ee5\u5b66\u4e60\u9ad8\u6548\u4e14\u66f4\u51c6\u786e\u7684\u89c6\u89c9\u4efb\u52a1\u8868\u793a\uff0c\u5305\u62ec\u8fb9\u7f18\u68c0\u6d4b\u548c\u7269\u4f53\u8bc6\u522b\u3002\u5bf9\u6d41\u884c\u6570\u636e\u96c6\uff08BSDS500\u3001ImageNet\u3001LFW\u3001YTF\u3001\\emph{etc.}\uff09\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPiDiNet \u548c Bi-PiDiNet \u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002\u5bf9\u4e8e\u8fb9\u7f18\u68c0\u6d4b\uff0cPiDiNet \u662f\u7b2c\u4e00\u4e2a\u65e0\u9700 ImageNet \u5373\u53ef\u8bad\u7ec3\u7684\u7f51\u7edc\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728 BSDS500 \u4e0a\u4ee5 100 FPS \u548c $<1M \u53c2\u6570\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u76ee\u6807\u8bc6\u522b\uff0c\u5728\u73b0\u6709\u7684\u4e8c\u8fdb\u5236 DNN \u4e2d\uff0cBi-PiDiNet \u5b9e\u73b0\u4e86\u6700\u4f73\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728 ResNet18 \u4e0a\u51cf\u5c11\u4e86\u8fd1 2 \u500d\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4ee3\u7801\u53ef\u5728 \\href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet} \u83b7\u53d6\u3002|[2402.00422v1](http://arxiv.org/pdf/2402.00422v1)|**[link](https://github.com/hellozhuo/pidinet)**|\n", "2402.00375": "|**2024-02-01**|**Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser**|\u901a\u8fc7\u57fa\u4e8e Transformer \u7684\u6a21\u6001\u6ce8\u200b\u200b\u5165\u5668\u89e3\u5f00\u591a\u6a21\u6001\u5927\u8111 MR \u56fe\u50cf\u7ffb\u8bd1|Jihoon Cho, Xiaofeng Liu, Fangxu Xing, Jinsong Ouyang, Georges El Fakhri, Jinah Park, Jonghye Woo|Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease diagnosis due to its ability to provide complementary information by analyzing a relationship between multimodal images on the same subject. Acquiring all MR modalities, however, can be expensive, and, during a scanning session, certain MR images may be missed depending on the study protocol. The typical solution would be to synthesize the missing modalities from the acquired images such as using generative adversarial networks (GANs). Yet, GANs constructed with convolutional neural networks (CNNs) are likely to suffer from a lack of global relationships and mechanisms to condition the desired modality. To address this, in this work, we propose a transformer-based modality infuser designed to synthesize multimodal brain MR images. In our method, we extract modality-agnostic features from the encoder and then transform them into modality-specific features using the modality infuser. Furthermore, the modality infuser captures long-range relationships among all brain structures, leading to the generation of more realistic images. We carried out experiments on the BraTS 2018 dataset, translating between four MR modalities, and our experimental results demonstrate the superiority of our proposed method in terms of synthesis quality. In addition, we conducted experiments on a brain tumor segmentation task and different conditioning methods.|\u591a\u6a21\u6001\u78c1\u5171\u632f (MR) \u6210\u50cf\u5728\u75be\u75c5\u8bca\u65ad\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u901a\u8fc7\u5206\u6790\u540c\u4e00\u5bf9\u8c61\u7684\u591a\u6a21\u6001\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\u3002\u7136\u800c\uff0c\u83b7\u53d6\u6240\u6709 MR \u6a21\u5f0f\u7684\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\uff0c\u800c\u4e14\u5728\u626b\u63cf\u8fc7\u7a0b\u4e2d\uff0c\u6839\u636e\u7814\u7a76\u65b9\u6848\uff0c\u53ef\u80fd\u4f1a\u9519\u8fc7\u67d0\u4e9b MR \u56fe\u50cf\u3002\u5178\u578b\u7684\u89e3\u51b3\u65b9\u6848\u662f\u4ece\u83b7\u53d6\u7684\u56fe\u50cf\u4e2d\u5408\u6210\u7f3a\u5931\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u3002\u7136\u800c\uff0c\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u6784\u5efa\u7684 GAN \u53ef\u80fd\u4f1a\u7f3a\u4e4f\u5168\u5c40\u5173\u7cfb\u548c\u673a\u5236\u6765\u8c03\u8282\u6240\u9700\u7684\u6a21\u6001\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6a21\u6001\u6ce8\u5165\u5668\uff0c\u65e8\u5728\u5408\u6210\u591a\u6a21\u6001\u5927\u8111 MR \u56fe\u50cf\u3002\u5728\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u4ece\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u4e0e\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u6a21\u6001\u6ce8\u5165\u5668\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6a21\u6001\u6ce8\u5165\u5668\u6355\u83b7\u6240\u6709\u5927\u8111\u7ed3\u6784\u4e4b\u95f4\u7684\u8fdc\u7a0b\u5173\u7cfb\uff0c\u4ece\u800c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\u3002\u6211\u4eec\u5728 BraTS 2018 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5728\u56db\u79cd MR \u6a21\u6001\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u6210\u8d28\u91cf\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u8111\u80bf\u7624\u5206\u5272\u4efb\u52a1\u548c\u4e0d\u540c\u7684\u8c03\u8282\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002|[2402.00375v1](http://arxiv.org/pdf/2402.00375v1)|null|\n", "2402.00353": "|**2024-02-01**|**High-Quality Medical Image Generation from Free-hand Sketch**|\u4ece\u624b\u7ed8\u8349\u56fe\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u56fe\u50cf|Quan Huu Cap, Atsushi Fukuda|Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications. Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images). However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results. In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it. Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process. Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations. Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics.|\u4ece\u624b\u7ed8\u8349\u56fe\u751f\u6210\u533b\u5b66\u56fe\u50cf\u4e3a\u5404\u79cd\u91cd\u8981\u7684\u533b\u5b66\u6210\u50cf\u5e94\u7528\u5e26\u6765\u4e86\u5e0c\u671b\u3002\u7531\u4e8e\u5728\u533b\u5b66\u9886\u57df\u6536\u96c6\u624b\u7ed8\u8349\u56fe\u6570\u636e\u6781\u5176\u56f0\u96be\uff0c\u5927\u591a\u6570\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u88ab\u63d0\u51fa\u6765\u4ece\u5408\u6210\u8349\u56fe\uff08\u4f8b\u5982\uff0c\u6765\u81ea\u771f\u5b9e\u56fe\u50cf\u7684\u5206\u5272\u63a9\u6a21\u7684\u8fb9\u7f18\u56fe\u6216\u8f6e\u5ed3\uff09\u751f\u6210\u533b\u5b66\u56fe\u50cf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u6982\u62ec\u5f92\u624b\u8349\u56fe\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5f92\u624b\u8349\u56fe\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u79f0\u4e3a Sketch2MedI\uff0c\u5b83\u5b66\u4e60\u5728 StyleGAN \u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u793a\u8349\u56fe\u5e76\u4ece\u4e2d\u751f\u6210\u533b\u5b66\u56fe\u50cf\u3002\u7531\u4e8e\u80fd\u591f\u5c06\u8349\u56fe\u7f16\u7801\u5230\u8fd9\u4e2a\u6709\u610f\u4e49\u7684\u8868\u793a\u7a7a\u95f4\u4e2d\uff0cSketch2Medi \u53ea\u9700\u8981\u5408\u6210\u8349\u56fe\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u6211\u4eec\u7684 Sketch2Medi \u5c55\u793a\u4e86\u5bf9\u5f92\u624b\u8349\u56fe\u7684\u5f3a\u5927\u6cdb\u5316\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u903c\u771f\u7684\u533b\u5b66\u56fe\u50cf\u3002 Sketch2Medi \u4e0e pix2pix\u3001CycleGAN\u3001UNIT \u548c U-GAT-IT \u6a21\u578b\u7684\u6bd4\u8f83\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u751f\u6210\u54bd\u90e8\u56fe\u50cf\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u662f\u5728\u5404\u79cd\u6307\u6807\u7684\u5b9a\u91cf\u8fd8\u662f\u5b9a\u6027\u65b9\u9762\u3002|[2402.00353v1](http://arxiv.org/pdf/2402.00353v1)|null|\n", "2402.00351": "|**2024-02-01**|**Machine Unlearning for Image-to-Image Generative Models**|\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8|Guihong Li, Hsiang Hsu, Chun-Fu, Chen, Radu Marculescu|Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.|\u673a\u5668\u9057\u5fd8\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5b83\u6545\u610f\u5fd8\u8bb0\u7ed9\u5b9a\u6a21\u578b\u4e2d\u7684\u6570\u636e\u6837\u672c\uff0c\u4ee5\u9075\u5b88\u4e25\u683c\u7684\u6cd5\u89c4\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u673a\u5668\u5fd8\u8bb0\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u6a21\u578b\u4e0a\uff0c\u800c\u751f\u6210\u6a21\u578b\u7684\u5fd8\u8bb0\u5b66\u4e60\u9886\u57df\u76f8\u5bf9\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u5145\u5f53\u4e86\u4e00\u5ea7\u6865\u6881\uff0c\u901a\u8fc7\u4e3a\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u673a\u5668\u53d6\u6d88\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u4ee5\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u4e3a\u57fa\u7840\uff0c\u8be5\u7b97\u6cd5\u8bc1\u660e\u4fdd\u7559\u6837\u672c\u7684\u6027\u80fd\u4e0b\u964d\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u540c\u65f6\u6709\u6548\u5730\u4ece\u9057\u5fd8\u6837\u672c\u4e2d\u5220\u9664\u4fe1\u606f\u3002\u5bf9\u4e24\u4e2a\u5927\u578b\u6570\u636e\u96c6ImageNet-1K\u548cPlaces-365\u7684\u5b9e\u8bc1\u7814\u7a76\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u4fdd\u7559\u6837\u672c\u7684\u53ef\u7528\u6027\uff0c\u8fd9\u8fdb\u4e00\u6b65\u7b26\u5408\u6570\u636e\u4fdd\u7559\u7b56\u7565\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u9879\u5de5\u4f5c\u662f\u7b2c\u4e00\u4e2a\u4ee3\u8868\u9488\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e13\u95e8\u5b9a\u5236\u7684\u673a\u5668\u53d6\u6d88\u5b66\u4e60\u7684\u7cfb\u7edf\u6027\u3001\u7406\u8bba\u6027\u548c\u5b9e\u8bc1\u6027\u63a2\u7d22\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jpmorganchase/l2l-generator-unlearning \u83b7\u53d6\u3002|[2402.00351v1](http://arxiv.org/pdf/2402.00351v1)|null|\n", "2402.00300": "|**2024-02-01**|**Self-supervised learning of video representations from a child's perspective**|\u4ece\u513f\u7ae5\u7684\u89d2\u5ea6\u8fdb\u884c\u89c6\u9891\u8868\u793a\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60|A. Emin Orhan, Wentao Wang, Alex N. Wang, Mengye Ren, Brenden M. Lake|Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.|\u5b69\u5b50\u4eec\u4ece\u51e0\u5e74\u7684\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u4f53\u9a8c\u4e2d\u5b66\u4e60\u4e86\u4ed6\u4eec\u5468\u56f4\u4e16\u754c\u7684\u5f3a\u5927\u7684\u5185\u90e8\u6a21\u578b\u3002\u8fd9\u79cd\u5185\u90e8\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u9ad8\u5ea6\u901a\u7528\u7684\u5b66\u4e60\u7b97\u6cd5\u4ece\u513f\u7ae5\u7684\u89c6\u89c9\u4f53\u9a8c\u4e2d\u5b66\u4e60\uff0c\u8fd8\u662f\u9700\u8981\u5f88\u5f3a\u7684\u5f52\u7eb3\u504f\u5dee\uff1f\u6700\u8fd1\u5728\u6536\u96c6\u5927\u89c4\u6a21\u3001\u7eb5\u5411\u3001\u53d1\u5c55\u73b0\u5b9e\u7684\u89c6\u9891\u6570\u636e\u96c6\u548c\u901a\u7528\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7b97\u6cd5\u65b9\u9762\u53d6\u5f97\u7684\u8fdb\u5c55\u4f7f\u6211\u4eec\u80fd\u591f\u5f00\u59cb\u89e3\u51b3\u8fd9\u4e2a\u5148\u5929\u4e0e\u540e\u5929\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5de5\u4f5c\u901a\u5e38\u4fa7\u91cd\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684 SSL \u7b97\u6cd5\u548c\u53ef\u4ee5\u4ece\u9759\u6001\u56fe\u50cf\u4e2d\u5b66\u4e60\u7684\u89c6\u89c9\u529f\u80fd\uff08\u4f8b\u5982\u5bf9\u8c61\u8bc6\u522b\uff09\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u4e16\u754c\u7684\u65f6\u95f4\u65b9\u9762\u3002\u4e3a\u4e86\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u4ece\u513f\u7ae5\u65e9\u671f\u53d1\u80b2\u9636\u6bb5\uff086-31 \u4e2a\u6708\uff09\u4e24\u5e74\u5185\u6536\u96c6\u7684\u7eb5\u5411\u3001\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u5934\u90e8\u6444\u50cf\u5934\u8bb0\u5f55\u6765\u8bad\u7ec3\u81ea\u76d1\u7763\u89c6\u9891\u6a21\u578b\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6a21\u578b\u5728\u4fc3\u8fdb\u4ece\u5c11\u91cf\u6807\u8bb0\u793a\u4f8b\u4e2d\u5b66\u4e60\u52a8\u4f5c\u6982\u5ff5\u65b9\u9762\u975e\u5e38\u6709\u6548\uff1b\u5b83\u4eec\u5177\u6709\u826f\u597d\u7684\u6570\u636e\u5927\u5c0f\u7f29\u653e\u7279\u6027\uff1b\u5b83\u4eec\u8fd8\u663e\u793a\u51fa\u65b0\u5174\u7684\u89c6\u9891\u63d2\u503c\u529f\u80fd\u3002\u4e0e\u4f7f\u7528\u5b8c\u5168\u76f8\u540c\u7684\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u89c6\u9891\u6a21\u578b\u8fd8\u53ef\u4ee5\u5b66\u4e60\u66f4\u5f3a\u5927\u7684\u5bf9\u8c61\u8868\u793a\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5b69\u5b50\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u91cd\u8981\u65f6\u95f4\u65b9\u9762\u53ef\u4ee5\u4f7f\u7528\u9ad8\u5ea6\u901a\u7528\u7684\u5b66\u4e60\u7b97\u6cd5\u4ece\u4ed6\u4eec\u7684\u89c6\u89c9\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e14\u6ca1\u6709\u5f3a\u70c8\u7684\u5f52\u7eb3\u504f\u5dee\u3002|[2402.00300v1](http://arxiv.org/pdf/2402.00300v1)|**[link](https://github.com/eminorhan/video-models)**|\n", "2402.00295": "|**2024-02-01**|**Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images**|\u4f7f\u7528\u65e0\u4eba\u673a\u56fe\u50cf\u8fdb\u884c\u5f03\u571f\u5806\u63cf\u7ed8\u7684\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u5272\u65b9\u6cd5\u7684\u6bd4\u8f83\u8bc4\u4f30|Sureka Thiruchittampalam, Bikram P. Banerjee, Nancy F. Glenn, Simit Raval|The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. Yet, on-site characterisation of individual piles poses a formidable challenge. The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches. This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.|\u77ff\u5c71\u5806\u653e\u573a\u7684\u7a33\u5b9a\u6027\u53d6\u51b3\u4e8e\u5f03\u571f\u5806\u7684\u7cbe\u786e\u5e03\u7f6e\uff0c\u5e76\u8003\u8651\u5230\u5176\u5730\u8d28\u548c\u5ca9\u571f\u7279\u6027\u3002\u7136\u800c\uff0c\u5355\u4e2a\u6869\u7684\u73b0\u573a\u8868\u5f81\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002\u5229\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u6280\u672f\u6765\u8868\u5f81\u5f03\u571f\u5806\uff0c\u5e76\u5229\u7528\u901a\u8fc7\u65e0\u4eba\u673a\u7cfb\u7edf\u8fdc\u7a0b\u83b7\u53d6\u7684\u6570\u636e\uff0c\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u8865\u5145\u89e3\u51b3\u65b9\u6848\u3002\u56fe\u50cf\u5904\u7406\uff0c\u4f8b\u5982\u57fa\u4e8e\u5bf9\u8c61\u7684\u5206\u7c7b\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u4f9d\u8d56\u4e8e\u6709\u6548\u7684\u5206\u5272\u3002\u8fd9\u9879\u7814\u7a76\u5b8c\u5584\u5e76\u5e76\u7f6e\u4e86\u5404\u79cd\u5206\u5272\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u989c\u8272\u548c\u57fa\u4e8e\u5f62\u6001\u7684\u6280\u672f\u3002\u76ee\u6807\u662f\u589e\u5f3a\u548c\u8bc4\u4f30\u91c7\u77ff\u73af\u5883\u4e2d\u57fa\u4e8e\u5bf9\u8c61\u7684\u5f03\u571f\u7279\u5f81\u5206\u6790\u7684\u9014\u5f84\u3002\u6b64\u5916\uff0c\u5bf9\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u5272\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u5728\u8bc4\u4f30\u7684\u5404\u79cd\u5206\u5272\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8e\u5f62\u6001\u5b66\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u65b9\u6cd5\uff0c\u5206\u6bb5\u4efb\u610f\u6a21\u578b\uff08SAM\uff09\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u7ed3\u679c\u5f3a\u8c03\u4e86\u7ed3\u5408\u5148\u8fdb\u7684\u5f62\u6001\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6765\u51c6\u786e\u9ad8\u6548\u5730\u8868\u5f81\u5f03\u571f\u5806\u7684\u6709\u6548\u6027\u3002\u8fd9\u9879\u7814\u7a76\u7684\u7ed3\u679c\u4e3a\u5206\u5272\u7b56\u7565\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u6280\u672f\u5728\u91c7\u77ff\u73af\u5883\u4e2d\u5f03\u571f\u5806\u7279\u5f81\u63cf\u8ff0\u4e2d\u7684\u5e94\u7528\u3002|[2402.00295v1](http://arxiv.org/pdf/2402.00295v1)|null|\n", "2402.00293": "|**2024-02-01**|**FineBio: A Fine-Grained Video Dataset of Biological Experiments with Hierarchical Annotation**|FineBio\uff1a\u5177\u6709\u5206\u5c42\u6ce8\u91ca\u7684\u751f\u7269\u5b9e\u9a8c\u7ec6\u7c92\u5ea6\u89c6\u9891\u6570\u636e\u96c6|Takuma Yagi, Misaki Ohashi, Yifei Huang, Ryosuke Furuta, Shungo Adachi, Toutai Mitsuyama, Yoichi Sato|In the development of science, accurate and reproducible documentation of the experimental process is crucial. Automatic recognition of the actions in experiments from videos would help experimenters by complementing the recording of experiments. Towards this goal, we propose FineBio, a new fine-grained video dataset of people performing biological experiments. The dataset consists of multi-view videos of 32 participants performing mock biological experiments with a total duration of 14.5 hours. One experiment forms a hierarchical structure, where a protocol consists of several steps, each further decomposed into a set of atomic operations. The uniqueness of biological experiments is that while they require strict adherence to steps described in each protocol, there is freedom in the order of atomic operations. We provide hierarchical annotation on protocols, steps, atomic operations, object locations, and their manipulation states, providing new challenges for structured activity understanding and hand-object interaction recognition. To find out challenges on activity understanding in biological experiments, we introduce baseline models and results on four different tasks, including (i) step segmentation, (ii) atomic operation detection (iii) object detection, and (iv) manipulated/affected object detection. Dataset and code are available from https://github.com/aistairc/FineBio.|\u5728\u79d1\u5b66\u7684\u53d1\u5c55\u4e2d\uff0c\u51c6\u786e\u4e14\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u8fc7\u7a0b\u8bb0\u5f55\u81f3\u5173\u91cd\u8981\u3002\u4ece\u89c6\u9891\u4e2d\u81ea\u52a8\u8bc6\u522b\u5b9e\u9a8c\u4e2d\u7684\u52a8\u4f5c\u5c06\u901a\u8fc7\u8865\u5145\u5b9e\u9a8c\u8bb0\u5f55\u6765\u5e2e\u52a9\u5b9e\u9a8c\u8005\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 FineBio\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u4eba\u4eec\u8fdb\u884c\u751f\u7269\u5b9e\u9a8c\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u7531 32 \u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u6a21\u62df\u751f\u7269\u5b9e\u9a8c\u7684\u591a\u89c6\u56fe\u89c6\u9891\u7ec4\u6210\uff0c\u603b\u65f6\u957f\u4e3a 14.5 \u5c0f\u65f6\u3002\u4e00\u4e2a\u5b9e\u9a8c\u5f62\u6210\u4e86\u4e00\u4e2a\u5c42\u6b21\u7ed3\u6784\uff0c\u5176\u4e2d\u534f\u8bae\u7531\u591a\u4e2a\u6b65\u9aa4\u7ec4\u6210\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u8fdb\u4e00\u6b65\u5206\u89e3\u4e3a\u4e00\u7ec4\u539f\u5b50\u64cd\u4f5c\u3002\u751f\u7269\u5b9e\u9a8c\u7684\u72ec\u7279\u6027\u5728\u4e8e\uff0c\u867d\u7136\u5b83\u4eec\u9700\u8981\u4e25\u683c\u9075\u5b88\u6bcf\u4e2a\u534f\u8bae\u4e2d\u63cf\u8ff0\u7684\u6b65\u9aa4\uff0c\u4f46\u539f\u5b50\u64cd\u4f5c\u7684\u987a\u5e8f\u662f\u81ea\u7531\u7684\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u534f\u8bae\u3001\u6b65\u9aa4\u3001\u539f\u5b50\u64cd\u4f5c\u3001\u5bf9\u8c61\u4f4d\u7f6e\u53ca\u5176\u64cd\u4f5c\u72b6\u6001\u7684\u5206\u5c42\u6ce8\u91ca\uff0c\u4e3a\u7ed3\u6784\u5316\u6d3b\u52a8\u7406\u89e3\u548c\u624b\u90e8\u5bf9\u8c61\u4ea4\u4e92\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u627e\u51fa\u751f\u7269\u5b9e\u9a8c\u4e2d\u6d3b\u52a8\u7406\u89e3\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u56db\u79cd\u4e0d\u540c\u4efb\u52a1\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u7ed3\u679c\uff0c\u5305\u62ec\uff08i\uff09\u6b65\u9aa4\u5206\u5272\uff0c\uff08ii\uff09\u539f\u5b50\u64cd\u4f5c\u68c0\u6d4b\uff0c\uff08iii\uff09\u5bf9\u8c61\u68c0\u6d4b\uff0c\u4ee5\u53ca\uff08iv\uff09\u64cd\u7eb5/\u53d7\u5f71\u54cd\u7684\u5bf9\u8c61\u68c0\u6d4b\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u4ece https://github.com/aistairc/FineBio \u83b7\u53d6\u3002|[2402.00293v1](http://arxiv.org/pdf/2402.00293v1)|null|\n", "2402.00281": "|**2024-02-01**|**Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues**|\u901a\u8fc7\u7a7a\u95f4\u52a8\u4f5c\u5355\u5143\u7ebf\u7d22\u5f15\u5bfc\u53ef\u89e3\u91ca\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b|Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger|While state-of-the-art facial expression recognition (FER) classifiers achieve a high level of accuracy, they lack interpretability, an important aspect for end-users. To recognize basic facial expressions, experts resort to a codebook associating a set of spatial action units to a facial expression. In this paper, we follow the same expert footsteps, and propose a learning strategy that allows us to explicitly incorporate spatial action units (aus) cues into the classifier's training to build a deep interpretable model. In particular, using this aus codebook, input image expression label, and facial landmarks, a single action units heatmap is built to indicate the most discriminative regions of interest in the image w.r.t the facial expression. We leverage this valuable spatial cue to train a deep interpretable classifier for FER. This is achieved by constraining the spatial layer features of a classifier to be correlated with \\aus map. Using a composite loss, the classifier is trained to correctly classify an image while yielding interpretable visual layer-wise attention correlated with aus maps, simulating the experts' decision process. This is achieved using only the image class expression as supervision and without any extra manual annotations. Moreover, our method is generic. It can be applied to any CNN- or transformer-based deep classifier without the need for architectural change or adding significant training time. Our extensive evaluation on two public benchmarks RAFDB, and AFFECTNET datasets shows that our proposed strategy can improve layer-wise interpretability without degrading classification performance. In addition, we explore a common type of interpretable classifiers that rely on Class-Activation Mapping methods (CAMs), and we show that our training technique improves the CAM interpretability.|\u867d\u7136\u6700\u5148\u8fdb\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b (FER) \u5206\u7c7b\u5668\u5177\u6709\u5f88\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u8fd9\u5bf9\u6700\u7ec8\u7528\u6237\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u3002\u4e3a\u4e86\u8bc6\u522b\u57fa\u672c\u7684\u9762\u90e8\u8868\u60c5\uff0c\u4e13\u5bb6\u4eec\u6c42\u52a9\u4e8e\u5c06\u4e00\u7ec4\u7a7a\u95f4\u52a8\u4f5c\u5355\u5143\u4e0e\u9762\u90e8\u8868\u60c5\u76f8\u5173\u8054\u7684\u5bc6\u7801\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9075\u5faa\u76f8\u540c\u7684\u4e13\u5bb6\u811a\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u660e\u786e\u5730\u5c06\u7a7a\u95f4\u52a8\u4f5c\u5355\u5143\uff08aus\uff09\u7ebf\u7d22\u7eb3\u5165\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u4e2d\uff0c\u4ee5\u6784\u5efa\u6df1\u5ea6\u53ef\u89e3\u91ca\u6a21\u578b\u3002\u7279\u522b\u662f\uff0c\u4f7f\u7528\u6b64 aus \u4ee3\u7801\u672c\u3001\u8f93\u5165\u56fe\u50cf\u8868\u60c5\u6807\u7b7e\u548c\u9762\u90e8\u6807\u5fd7\uff0c\u6784\u5efa\u5355\u4e2a\u52a8\u4f5c\u5355\u5143\u70ed\u56fe\u6765\u6307\u793a\u56fe\u50cf\u4e2d\u4e0e\u9762\u90e8\u8868\u60c5\u76f8\u5173\u7684\u6700\u5177\u8fa8\u522b\u529b\u7684\u611f\u5174\u8da3\u533a\u57df\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e00\u5b9d\u8d35\u7684\u7a7a\u95f4\u7ebf\u7d22\u6765\u8bad\u7ec3 FER \u7684\u6df1\u5ea6\u53ef\u89e3\u91ca\u5206\u7c7b\u5668\u3002\u8fd9\u662f\u901a\u8fc7\u5c06\u5206\u7c7b\u5668\u7684\u7a7a\u95f4\u5c42\u7279\u5f81\u9650\u5236\u4e3a\u4e0e\\aus\u56fe\u76f8\u5173\u6765\u5b9e\u73b0\u7684\u3002\u4f7f\u7528\u590d\u5408\u635f\u5931\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u6b63\u786e\u5206\u7c7b\u56fe\u50cf\uff0c\u540c\u65f6\u4ea7\u751f\u4e0e aus \u5730\u56fe\u76f8\u5173\u7684\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u5206\u5c42\u6ce8\u610f\u529b\uff0c\u6a21\u62df\u4e13\u5bb6\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u8fd9\u662f\u4ec5\u4f7f\u7528\u56fe\u50cf\u7c7b\u8868\u8fbe\u5f0f\u4f5c\u4e3a\u76d1\u7763\u6765\u5b9e\u73b0\u7684\uff0c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u624b\u52a8\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u901a\u7528\u7684\u3002\u5b83\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e CNN \u6216 Transformer \u7684\u6df1\u5ea6\u5206\u7c7b\u5668\uff0c\u65e0\u9700\u66f4\u6539\u67b6\u6784\u6216\u589e\u52a0\u5927\u91cf\u8bad\u7ec3\u65f6\u95f4\u3002\u6211\u4eec\u5bf9\u4e24\u4e2a\u516c\u5171\u57fa\u51c6 RAFDB \u548c AFFECTNET \u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u964d\u4f4e\u5206\u7c7b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5206\u5c42\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u4f9d\u8d56\u4e8e\u7c7b\u6fc0\u6d3b\u6620\u5c04\u65b9\u6cd5\uff08CAM\uff09\u7684\u5e38\u89c1\u7c7b\u578b\u7684\u53ef\u89e3\u91ca\u5206\u7c7b\u5668\uff0c\u5e76\u4e14\u6211\u4eec\u8868\u660e\u6211\u4eec\u7684\u8bad\u7ec3\u6280\u672f\u63d0\u9ad8\u4e86 CAM \u53ef\u89e3\u91ca\u6027\u3002|[2402.00281v1](http://arxiv.org/pdf/2402.00281v1)|**[link](https://github.com/sbelharbi/interpretable-fer-aus)**|\n", "2402.00250": "|**2024-02-01**|**LRDif: Diffusion Models for Under-Display Camera Emotion Recognition**|LRDif\uff1a\u7528\u4e8e\u5c4f\u4e0b\u6444\u50cf\u5934\u60c5\u7eea\u8bc6\u522b\u7684\u6269\u6563\u6a21\u578b|Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana|This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC's image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field.|\u672c\u7814\u7a76\u5f15\u5165\u4e86 LRDif\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5c4f\u4e0b\u6444\u50cf\u5934 (UDC) \u80cc\u666f\u4e0b\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b (FER) \u800c\u8bbe\u8ba1\u3002\u4e3a\u4e86\u89e3\u51b3 UDC \u56fe\u50cf\u9000\u5316\u5e26\u6765\u7684\u56fa\u6709\u6311\u6218\uff0c\u4f8b\u5982\u6e05\u6670\u5ea6\u964d\u4f4e\u548c\u566a\u58f0\u589e\u52a0\uff0cLRDif \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u96c6\u6210\u538b\u7f29\u521d\u6b65\u63d0\u53d6\u7f51\u7edc\uff08FPEN\uff09\u548c\u654f\u6377\u53d8\u538b\u5668\u7f51\u7edc\uff08UDCformer\uff09\u6765\u6709\u6548\u8bc6\u522b\u60c5\u611f\u6807\u7b7e\u6765\u81ea UDC \u56fe\u50cf\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b (DM) \u5f3a\u5927\u7684\u5206\u5e03\u6620\u5c04\u529f\u80fd\u548c\u53d8\u538b\u5668\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u80fd\u529b\uff0cLRDif \u6709\u6548\u514b\u670d\u4e86 UDC \u73af\u5883\u4e2d\u56fa\u6709\u7684\u566a\u58f0\u548c\u5931\u771f\u969c\u788d\u3002\u5bf9\u6807\u51c6 FER \u6570\u636e\u96c6\uff08\u5305\u62ec RAF-DB\u3001KDEF \u548c FERPlus\u3001LRDif\uff09\u7684\u7efc\u5408\u5b9e\u9a8c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u63a8\u8fdb FER \u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u901a\u8fc7\u89e3\u51b3 FER \u4e2d\u7684 UDC \u6311\u6218\u6765\u5f25\u8865\u6587\u732e\u4e2d\u7684\u91cd\u5927\u7a7a\u767d\uff0c\u800c\u4e14\u8fd8\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002|[2402.00250v1](http://arxiv.org/pdf/2402.00250v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2402.00769": "|**2024-02-01**|**AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning**|AnimateLCM\uff1a\u901a\u8fc7\u89e3\u8026\u4e00\u81f4\u6027\u5b66\u4e60\u52a0\u901f\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u548c\u9002\u914d\u5668\u7684\u52a8\u753b|Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li|Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.|\u89c6\u9891\u6269\u6563\u6a21\u578b\u56e0\u5176\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u9ad8\u4fdd\u771f\u5ea6\u7684\u89c6\u9891\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4f7f\u5176\u8ba1\u7b97\u91cf\u5927\u4e14\u8017\u65f6\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u53d7\u5230\u4e00\u81f4\u6027\u6a21\u578b\uff08CM\uff09\u7684\u542f\u53d1\uff0c\u4e00\u81f4\u6027\u6a21\u578b\uff08CM\uff09\u63d0\u70bc\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6700\u5c11\u7684\u6b65\u9aa4\u52a0\u901f\u91c7\u6837\uff0c\u4ee5\u53ca\u5176\u5728\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e0a\u7684\u6210\u529f\u6269\u5c55\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\uff08LCM\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AnimateLCM\uff0c\u5141\u8bb8\u5728\u6700\u5c11\u7684\u6b65\u9aa4\u5185\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u3002\u6211\u4eec\u6ca1\u6709\u76f4\u63a5\u5728\u539f\u59cb\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u4e00\u81f4\u6027\u5b66\u4e60\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5c06\u56fe\u50cf\u751f\u6210\u5148\u9a8c\u548c\u8fd0\u52a8\u751f\u6210\u5148\u9a8c\u7684\u84b8\u998f\u89e3\u8026\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u589e\u5f3a\u4e86\u751f\u6210\u89c6\u89c9\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u4f7f\u7a33\u5b9a\u6269\u6563\u793e\u533a\u4e2d\u7684\u5373\u63d2\u5373\u7528\u9002\u914d\u5668\u7ec4\u5408\u80fd\u591f\u5b9e\u73b0\u5404\u79cd\u529f\u80fd\uff08\u4f8b\u5982\u7528\u4e8e\u53ef\u63a7\u53d1\u7535\u7684ControlNet\uff09\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u4f7f\u73b0\u6709\u9002\u914d\u5668\u9002\u5e94\u6211\u4eec\u7684\u84b8\u998f\u6587\u672c\u6761\u4ef6\u89c6\u9891\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u6216\u8005\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u800c\u4e0d\u635f\u5bb3\u91c7\u6837\u901f\u5ea6\u3002\u6211\u4eec\u5728\u56fe\u50cf\u6761\u4ef6\u89c6\u9891\u751f\u6210\u548c\u5e03\u5c40\u6761\u4ef6\u89c6\u9891\u751f\u6210\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7b56\u7565\uff0c\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u6743\u91cd\u5c06\u88ab\u516c\u5f00\u3002\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u8bf7\u8bbf\u95ee https://github.com/G-U-N/AnimateLCM\u3002|[2402.00769v1](http://arxiv.org/pdf/2402.00769v1)|**[link](https://github.com/g-u-n/animatelcm)**|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2402.00864": "|**2024-02-01**|**ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields**|ViCA-NeRF\uff1a\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u611f\u77e5 3D \u7f16\u8f91|Jiahua Dong, Yu-Xiong Wang|We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available.|\u6211\u4eec\u63a8\u51fa ViCA-NeRF\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6587\u672c\u6307\u4ee4\u8fdb\u884c 3D \u7f16\u8f91\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u611f\u77e5\u65b9\u6cd5\u3002\u9664\u4e86\u9690\u5f0f\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5efa\u6a21\u4e4b\u5916\uff0c\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\u5229\u7528\u4e24\u4e2a\u6b63\u5219\u5316\u6e90\uff0c\u8fd9\u4e9b\u6b63\u5219\u5316\u6e90\u53ef\u4ee5\u5728\u4e0d\u540c\u89c6\u56fe\u4e4b\u95f4\u663e\u5f0f\u4f20\u64ad\u7f16\u8f91\u4fe1\u606f\uff0c\u4ece\u800c\u786e\u4fdd\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u5bf9\u4e8e\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u6211\u4eec\u5229\u7528 NeRF \u5bfc\u51fa\u7684\u6df1\u5ea6\u4fe1\u606f\u6765\u5efa\u7acb\u4e0d\u540c\u89c6\u56fe\u4e4b\u95f4\u7684\u56fe\u50cf\u5bf9\u5e94\u5173\u7cfb\u3002\u5bf9\u4e8e\u5b66\u4e60\u6b63\u5219\u5316\uff0c\u6211\u4eec\u5728\u7f16\u8f91\u548c\u672a\u7f16\u8f91\u56fe\u50cf\u4e4b\u95f4\u5bf9\u9f50 2D \u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u7f16\u8f91\u5173\u952e\u89c6\u56fe\u5e76\u5c06\u66f4\u65b0\u4f20\u64ad\u5230\u6574\u4e2a\u573a\u666f\u3002\u7ed3\u5408\u8fd9\u4e24\u79cd\u7b56\u7565\uff0c\u6211\u4eec\u7684 ViCA-NeRF \u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\u3002\u5728\u521d\u59cb\u9636\u6bb5\uff0c\u6211\u4eec\u6df7\u5408\u6765\u81ea\u4e0d\u540c\u89c6\u56fe\u7684\u7f16\u8f91\u6765\u521b\u5efa\u521d\u6b65\u7684 3D \u7f16\u8f91\u3002\u63a5\u4e0b\u6765\u662f NeRF \u8bad\u7ec3\u7684\u7b2c\u4e8c\u9636\u6bb5\uff0c\u81f4\u529b\u4e8e\u8fdb\u4e00\u6b65\u5b8c\u5584\u573a\u666f\u7684\u5916\u89c2\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cViCA-NeRF \u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u66f4\u9ad8\u6548\uff08\u5feb 3 \u500d\uff09\u7684\u7f16\u8f91\uff0c\u5177\u6709\u66f4\u9ad8\u6c34\u5e73\u7684\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u3002\u6211\u4eec\u7684\u4ee3\u7801\u662f\u516c\u5f00\u7684\u3002|[2402.00864v1](http://arxiv.org/pdf/2402.00864v1)|**[link](https://github.com/dongjiahua/vica-nerf)**|\n", "2402.00827": "|**2024-02-01**|**Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering**|Emo-Avatar\uff1a\u901a\u8fc7\u7eb9\u7406\u6e32\u67d3\u9ad8\u6548\u7684\u5355\u76ee\u89c6\u9891\u98ce\u683c\u5934\u50cf|Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu|Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.|\u827a\u672f\u89c6\u9891\u8096\u50cf\u751f\u6210\u662f\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u9886\u57df\u4e2d\u4e00\u9879\u91cd\u8981\u4e14\u5907\u53d7\u8ffd\u6367\u7684\u4efb\u52a1\u3002\u867d\u7136\u5df2\u7ecf\u5f00\u53d1\u4e86\u5404\u79cd\u65b9\u6cd5\u5c06 NeRF \u6216 StyleGAN \u4e0e\u6307\u5bfc\u7f16\u8f91\u6a21\u578b\u96c6\u6210\u4ee5\u521b\u5efa\u548c\u7f16\u8f91\u53ef\u9a7e\u9a76\u8096\u50cf\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u9762\u4e34\u7740\u4e00\u4e9b\u6311\u6218\u3002\u5b83\u4eec\u901a\u5e38\u4e25\u91cd\u4f9d\u8d56\u5927\u578b\u6570\u636e\u96c6\uff0c\u9700\u8981\u5927\u91cf\u7684\u5b9a\u5236\u8fc7\u7a0b\uff0c\u5e76\u4e14\u7ecf\u5e38\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u901a\u8fc7\u5ef6\u8fdf\u795e\u7ecf\u6e32\u67d3\u63d0\u51fa\u4e86\u9ad8\u6548\u5355\u8c03\u89c6\u9891\u98ce\u683c\u5934\u50cf\uff08Emo-Avatar\uff09\uff0c\u589e\u5f3a\u4e86 StyleGAN \u751f\u6210\u52a8\u6001\u3001\u53ef\u9a7e\u9a76\u8096\u50cf\u89c6\u9891\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u5ef6\u8fdf\u795e\u7ecf\u6e32\u67d3\u7ba1\u9053\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u5c11\u955c\u5934 PTI \u521d\u59cb\u5316\uff0c\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u91c7\u6837\u7684\u51e0\u4e2a\u6781\u7aef\u59ff\u52bf\u6765\u521d\u59cb\u5316 StyleGAN \u751f\u6210\u5668\uff0c\u4ee5\u6355\u83b7\u76ee\u6807\u8096\u50cf\u4e2d\u5bf9\u9f50\u9762\u90e8\u7684\u4e00\u81f4\u8868\u793a\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\uff0c\u7528\u4e8e\u4ece\u901a\u8fc7\u52a8\u6001\u8868\u8fbe\u6d41\u53d8\u5f62\u7684 UV \u56fe\u8fdb\u884c\u9ad8\u9891\u7eb9\u7406\u91c7\u6837\uff0c\u4ee5\u8fdb\u884c\u8fd0\u52a8\u611f\u77e5\u7eb9\u7406\u5148\u671f\u96c6\u6210\uff0c\u4ee5\u63d0\u4f9b\u8eaf\u5e72\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a StyleGAN \u751f\u6210\u7528\u4e8e\u8096\u50cf\u89c6\u9891\u6e32\u67d3\u7684\u5b8c\u6574\u4e0a\u534a\u8eab\u7684\u80fd\u529b\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cEmo-Avatar \u5c06\u6837\u5f0f\u5b9a\u5236\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u81f3\u4ec5 5 \u5206\u949f\u3002\u6b64\u5916\uff0cEmo-Avatar \u4ec5\u9700\u8981\u5355\u4e2a\u53c2\u8003\u56fe\u50cf\u8fdb\u884c\u7f16\u8f91\uff0c\u5e76\u91c7\u7528\u533a\u57df\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u548c\u8bed\u4e49\u4e0d\u53d8 CLIP \u6307\u5bfc\uff0c\u786e\u4fdd\u4e00\u81f4\u7684\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u548c\u8eab\u4efd\u4fdd\u5b58\u3002\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0cEmo-Avatar \u5728\u8bad\u7ec3\u6548\u7387\u3001\u6e32\u67d3\u8d28\u91cf\u4ee5\u53ca\u81ea\u6211\u548c\u4ea4\u53c9\u91cd\u6f14\u7684\u53ef\u7f16\u8f91\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002|[2402.00827v1](http://arxiv.org/pdf/2402.00827v1)|null|\n", "2402.00627": "|**2024-02-01**|**CapHuman: Capture Your Moments in Parallel Universes**|CapHuman\uff1a\u5728\u5e73\u884c\u5b87\u5b99\u4e2d\u6355\u6349\u4f60\u7684\u77ac\u95f4|Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang|We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.|\u6211\u4eec\u4e13\u6ce8\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u5408\u6210\u4efb\u52a1\uff0c\u5373\u4ec5\u7ed9\u5b9a\u4e00\u5f20\u53c2\u8003\u9762\u90e8\u7167\u7247\uff0c\u671f\u671b\u5728\u4e0d\u540c\u7684\u80cc\u666f\u4e0b\u751f\u6210\u5177\u6709\u4e0d\u540c\u5934\u90e8\u4f4d\u7f6e\u3001\u59ff\u52bf\u548c\u9762\u90e8\u8868\u60c5\u7684\u7279\u5b9a\u4e2a\u4f53\u56fe\u50cf\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u8ba4\u4e3a\u6211\u4eec\u7684\u751f\u6210\u6a21\u578b\u5e94\u8be5\u5177\u6709\u4ee5\u4e0b\u6709\u5229\u7279\u5f81\uff1a\uff081\uff09\u5bf9\u6211\u4eec\u7684\u4e16\u754c\u548c\u4eba\u7c7b\u793e\u4f1a\u6709\u5f88\u5f3a\u7684\u89c6\u89c9\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4ee5\u751f\u6210\u57fa\u672c\u5bf9\u8c61\u548c\u4eba\u7c7b\u56fe\u50cf\u3002 (2)\u53ef\u6982\u62ec\u7684\u8eab\u4efd\u4fdd\u5b58\u80fd\u529b\u3002 (3)\u7075\u6d3b\u7ec6\u7c92\u5ea6\u7684\u5934\u90e8\u63a7\u5236\u3002\u6700\u8fd1\uff0c\u5927\u578b\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u663e\u793a\u51fa\u663e\u7740\u7684\u7ed3\u679c\uff0c\u6210\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u57fa\u7840\u3002\u4f5c\u4e3a\u57fa\u7840\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u91ca\u653e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e0a\u8ff0\u4e24\u9879\u529f\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a CapHuman \u7684\u65b0\u6846\u67b6\u3002\u6211\u4eec\u91c7\u7528\u201c\u7f16\u7801\u7136\u540e\u5b66\u4e60\u5bf9\u9f50\u201d\u8303\u5f0f\uff0c\u8fd9\u4f7f\u5f97\u65b0\u4e2a\u4f53\u80fd\u591f\u8fdb\u884c\u901a\u7528\u7684\u8eab\u4efd\u4fdd\u5b58\uff0c\u800c\u65e0\u9700\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u7e41\u7410\u7684\u8c03\u6574\u3002CapHuman \u5bf9\u8eab\u4efd\u7279\u5f81\u8fdb\u884c\u7f16\u7801\uff0c\u7136\u540e\u5b66\u4e60\u5c06\u5b83\u4eec\u5bf9\u9f50\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86 3D \u9762\u90e8\u5148\u9a8c\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u4ee5\u7075\u6d3b\u4e14 3D \u4e00\u81f4\u7684\u65b9\u5f0f\u63a7\u5236\u4eba\u4f53\u5934\u90e8\u3002\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u8868\u660e\uff0c\u6211\u4eec\u7684 CapHuman \u53ef\u4ee5\u751f\u6210\u8eab\u4efd\u4fdd\u7559\u826f\u597d\u3001\u7167\u7247\u822c\u771f\u5b9e\u4e14\u9ad8\u4fdd\u771f\u7684\u8096\u50cf\uff0c\u5177\u6709\u5185\u5bb9\u4e30\u5bcc\u7684\u8868\u793a\u5f62\u5f0f\u548c\u5404\u79cd\u5934\u90e8\u518d\u73b0\uff0c\u4f18\u4e8e\u65e2\u5b9a\u57fa\u7ebf\u3002\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5c06\u5728 https://github.com/VamosC/CapHuman \u53d1\u5e03\u3002|[2402.00627v1](http://arxiv.org/pdf/2402.00627v1)|**[link](https://github.com/vamosc/caphuman)**|\n", "2402.00606": "|**2024-02-01**|**Dynamic Texture Transfer using PatchMatch and Transformers**|\u4f7f\u7528 PatchMatch \u548c Transformer \u8fdb\u884c\u52a8\u6001\u7eb9\u7406\u4f20\u8f93|Guo Pu, Shiyao Xu, Xixin Cao, Zhouhui Lian|How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art.|\u5982\u4f55\u5c06\u7ed9\u5b9a\u89c6\u9891\u7684\u52a8\u6001\u7eb9\u7406\u81ea\u52a8\u8f6c\u79fb\u5230\u76ee\u6807\u9759\u6001\u56fe\u50cf\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4e14\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6a21\u578b\u6765\u5904\u7406\u6b64\u4efb\u52a1\uff0c\u8be5\u6a21\u578b\u540c\u65f6\u5229\u7528 PatchMatch \u548c Transformer\u3002\u5173\u952e\u601d\u60f3\u662f\u5c06\u52a8\u6001\u7eb9\u7406\u4f20\u8f93\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5176\u4e2d\u5728\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u57fa\u4e8e PatchMatch \u7b97\u6cd5\u7684\u8ddd\u79bb\u56fe\u5f15\u5bfc\u7eb9\u7406\u4f20\u8f93\u6a21\u5757\u5408\u6210\u5177\u6709\u6240\u9700\u52a8\u6001\u7eb9\u7406\u7684\u76ee\u6807\u89c6\u9891\u7684\u8d77\u59cb\u5e27\u3002\u7136\u540e\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5408\u6210\u56fe\u50cf\u88ab\u5206\u89e3\u4e3a\u7ed3\u6784\u4e0d\u53ef\u77e5\u7684\u8865\u4e01\uff0c\u6839\u636e\u8fd9\u4e9b\u8865\u4e01\uff0c\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u914d\u5907VQ-VAE\u7684Transformers\u5904\u7406\u957f\u79bb\u6563\u5e8f\u5217\u7684\u5f3a\u5927\u80fd\u529b\u6765\u9884\u6d4b\u5176\u76f8\u5e94\u7684\u540e\u7eed\u8865\u4e01\u3002\u83b7\u5f97\u6240\u6709\u8fd9\u4e9b\u8865\u4e01\u540e\uff0c\u6211\u4eec\u5e94\u7528\u9ad8\u65af\u52a0\u6743\u5e73\u5747\u5408\u5e76\u7b56\u7565\u5c06\u5b83\u4eec\u5e73\u6ed1\u5730\u7ec4\u88c5\u5230\u76ee\u6807\u98ce\u683c\u5316\u89c6\u9891\u7684\u6bcf\u4e00\u5e27\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u7eb9\u7406\u4f20\u8f93\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002|[2402.00606v1](http://arxiv.org/pdf/2402.00606v1)|null|\n", "2402.00376": "|**2024-02-01**|**Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction**|Image2Points\uff1a\u7528\u4e8e\u9ad8\u8d28\u91cf PET \u56fe\u50cf\u91cd\u5efa\u7684\u57fa\u4e8e 3D \u70b9\u7684\u4e0a\u4e0b\u6587\u805a\u7c7b GAN|Jiaqi Cui, Yan Wang, Lu Wen, Pinxian Zeng, Xi Wu, Jiliu Zhou, Dinggang Shen|To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images. However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction. In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET. Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details. Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images. Experiments on both clinical and phantom datasets demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction methods qualitatively and quantitatively. Code is available at https://github.com/gluucose/PCCGAN.|\u4e3a\u4e86\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf (PET) \u56fe\u50cf\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8f90\u5c04\u66b4\u9732\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\u6765\u4ece\u76f8\u5e94\u7684\u4f4e\u5242\u91cf PET (LPET) \u56fe\u50cf\u91cd\u5efa\u6807\u51c6\u5242\u91cf PET (SPET) \u56fe\u50cf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u57fa\u4e8e\u4f53\u7d20\u7684\u8868\u793a\uff0c\u65e0\u6cd5\u5145\u5206\u8003\u8651\u7cbe\u786e\u7684\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u7684\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u5bfc\u81f4\u91cd\u5efa\u53d7\u635f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e 3D \u70b9\u7684\u4e0a\u4e0b\u6587\u805a\u7c7b GAN\uff0c\u5373 PCC-GAN\uff0c\u7528\u4e8e\u4ece LPET \u91cd\u5efa\u9ad8\u8d28\u91cf\u7684 SPET \u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53d7\u5230\u70b9\u7684\u51e0\u4f55\u8868\u793a\u80fd\u529b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u70b9\u7684\u8868\u793a\u6765\u589e\u5f3a\u56fe\u50cf\u7ed3\u6784\u7684\u663e\u5f0f\u8868\u8fbe\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u7cbe\u7ec6\u7ec6\u8282\u7684\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u5e94\u7528\u4e0a\u4e0b\u6587\u805a\u7c7b\u7b56\u7565\u6765\u63a2\u7d22\u70b9\u4e4b\u95f4\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u8fd9\u51cf\u8f7b\u4e86\u91cd\u5efa\u56fe\u50cf\u4e2d\u5c0f\u7ed3\u6784\u7684\u6a21\u7cca\u6027\u3002\u5bf9\u4e34\u5e8a\u548c\u6a21\u578b\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 PCC-GAN \u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/gluucose/PCCGAN \u83b7\u53d6\u3002|[2402.00376v1](http://arxiv.org/pdf/2402.00376v1)|**[link](https://github.com/gluucose/pccgan)**|\n"}, "\u591a\u6a21\u6001": {"2402.00700": "|**2024-02-01**|**In-Bed Pose Estimation: A Review**|\u5e8a\u4e0a\u59ff\u52bf\u4f30\u8ba1\uff1a\u56de\u987e|Ziya Ata Yaz\u0131c\u0131, Sara Colantonio, Haz\u0131m Kemal Ekenel|Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare. One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed. This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals. Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses. The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map. Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions. Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation. To expedite advancements in this domain, we conduct a review of existing datasets and approaches. Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field.|\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u662f\u4ece\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u8bc6\u522b\u4eba\u8eab\u4f53\u5173\u8282\u4f4d\u7f6e\u7684\u8fc7\u7a0b\uff0c\u4ee3\u8868\u4e86\u5305\u62ec\u533b\u7597\u4fdd\u5065\u5728\u5185\u7684\u5404\u4e2a\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\u3002\u6b64\u7c7b\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e4b\u4e00\u6d89\u53ca\u5e8a\u4e0a\u59ff\u52bf\u4f30\u8ba1\uff0c\u5176\u4e2d\u5206\u6790\u8eba\u5728\u6bef\u5b50\u4e0b\u7684\u4e2a\u4eba\u7684\u8eab\u4f53\u59ff\u52bf\u3002\u4f8b\u5982\uff0c\u8fd9\u9879\u4efb\u52a1\u53ef\u7528\u4e8e\u76d1\u6d4b\u4e00\u4e2a\u4eba\u7684\u7761\u7720\u884c\u4e3a\u5e76\u53ca\u65e9\u53d1\u73b0\u75c7\u72b6\uff0c\u4ee5\u4fbf\u5728\u5bb6\u5ead\u548c\u533b\u9662\u4e2d\u8bca\u65ad\u6f5c\u5728\u7684\u75be\u75c5\u3002\u4e00\u4e9b\u7814\u7a76\u5229\u7528\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u4f30\u8ba1\u4eba\u4f53\u5728\u5e8a\u4e0a\u7684\u59ff\u52bf\u3002\u5355\u6a21\u6001\u7814\u7a76\u901a\u5e38\u91c7\u7528 RGB \u56fe\u50cf\uff0c\u800c\u591a\u6a21\u6001\u7814\u7a76\u5219\u4f7f\u7528 RGB\u3001\u957f\u6ce2\u7ea2\u5916\u3001\u538b\u529b\u56fe\u548c\u6df1\u5ea6\u56fe\u7b49\u6a21\u6001\u3002\u591a\u6a21\u6001\u7814\u7a76\u7684\u4f18\u70b9\u662f\u9664\u4e86 RGB \u4e4b\u5916\u8fd8\u53ef\u4ee5\u4f7f\u7528\u6a21\u6001\uff0c\u53ef\u4ee5\u6355\u83b7\u6709\u52a9\u4e8e\u5e94\u5bf9\u906e\u6321\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u591a\u6a21\u6001\u7814\u7a76\u6392\u9664\u4e86 RGB\uff0c\u8fd9\u6837\u66f4\u9002\u5408\u9690\u79c1\u4fdd\u62a4\u3002\u4e3a\u4e86\u52a0\u5feb\u8be5\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u6211\u4eec\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8fdb\u884c\u4e86\u5ba1\u67e5\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5c55\u793a\u5148\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u3001\u5f53\u524d\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u5e8a\u4e0a\u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u9886\u57df\u7684\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u89c1\u89e3\u3002|[2402.00700v1](http://arxiv.org/pdf/2402.00700v1)|null|\n", "2402.00637": "|**2024-02-01**|**Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird's-Eye-View**|\u9c7c\u773c\u76f8\u673a\u548c\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u878d\u5408\uff0c\u5b9e\u73b0\u9e1f\u77b0\u8fd1\u573a\u969c\u788d\u7269\u611f\u77e5|Arindam Das, Sudarshan Paul, Niko Scholz, Akhilesh Kumar Malviya, Ganesh Sistu, Ujjwal Bhattacharya, Ciar\u00e1n Eising|Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving. Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization. However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare. Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions. Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors. Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality. Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV. The fusion of these two modalities is facilitated via concatenation. At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space. Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception. We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors. We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios. When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach.|\u51c6\u786e\u7684\u969c\u788d\u7269\u8bc6\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u8fd1\u573a\u611f\u77e5\u9886\u57df\u7684\u4e00\u9879\u6839\u672c\u6311\u6218\u3002\u4f20\u7edf\u4e0a\uff0c\u9c7c\u773c\u6444\u50cf\u673a\u7ecf\u5e38\u7528\u4e8e\u5168\u9762\u7684\u73af\u89c6\u611f\u77e5\uff0c\u5305\u62ec\u540e\u89c6\u969c\u788d\u7269\u5b9a\u4f4d\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u76f8\u673a\u7684\u6027\u80fd\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u3001\u591c\u95f4\u6216\u53d7\u5230\u5f3a\u70c8\u9633\u5149\u7167\u5c04\u65f6\u53ef\u80fd\u4f1a\u663e\u7740\u6076\u5316\u3002\u76f8\u53cd\uff0c\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u7b49\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u4f20\u611f\u5668\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u57fa\u672c\u4e0a\u4e0d\u53d7\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7aef\u5230\u7aef\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u9c7c\u773c\u76f8\u673a\u548c\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\uff0c\u5728\u9e1f\u77b0\uff08BEV\uff09\u89c6\u89d2\u4e0b\u5b9e\u73b0\u9ad8\u6548\u969c\u788d\u7269\u611f\u77e5\u3002\u6700\u521d\uff0cResNeXt-50 \u88ab\u7528\u4f5c\u4e00\u7ec4\u5355\u6a21\u6001\u7f16\u7801\u5668\u6765\u63d0\u53d6\u7279\u5b9a\u4e8e\u6bcf\u79cd\u6a21\u6001\u7684\u7279\u5f81\u3002\u968f\u540e\uff0c\u4e0e\u53ef\u89c1\u5149\u8c31\u76f8\u5173\u7684\u7279\u5f81\u7a7a\u95f4\u8f6c\u6362\u4e3a BEV\u3002\u901a\u8fc7\u4e32\u8054\u4fc3\u8fdb\u8fd9\u4e24\u79cd\u6a21\u5f0f\u7684\u878d\u5408\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u8d85\u58f0\u6ce2\u9891\u8c31\u7684\u5355\u5cf0\u7279\u5f81\u56fe\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u6269\u5f20\u5377\u79ef\uff0c\u7528\u4e8e\u51cf\u8f7b\u878d\u5408\u7279\u5f81\u7a7a\u95f4\u4e2d\u4e24\u4e2a\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u4f20\u611f\u5668\u9519\u4f4d\u3002\u6700\u540e\uff0c\u4e24\u7ea7\u8bed\u4e49\u5360\u7528\u89e3\u7801\u5668\u5229\u7528\u878d\u5408\u7684\u7279\u5f81\u6765\u751f\u6210\u7f51\u683c\u9884\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u969c\u788d\u7269\u611f\u77e5\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u7cfb\u7edf\u7814\u7a76\uff0c\u4ee5\u786e\u5b9a\u4e24\u4e2a\u4f20\u611f\u5668\u591a\u6a21\u6001\u878d\u5408\u7684\u6700\u4f73\u7b56\u7565\u3002\u6211\u4eec\u63d0\u4f9b\u6709\u5173\u6570\u636e\u96c6\u521b\u5efa\u7a0b\u5e8f\u3001\u6ce8\u91ca\u6307\u5357\u7684\u89c1\u89e3\uff0c\u5e76\u6267\u884c\u5f7b\u5e95\u7684\u6570\u636e\u5206\u6790\uff0c\u4ee5\u786e\u4fdd\u5145\u5206\u8986\u76d6\u6240\u6709\u573a\u666f\u3002\u5f53\u5e94\u7528\u4e8e\u6211\u4eec\u7684\u6570\u636e\u96c6\u65f6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u6709\u6548\u6027\u3002|[2402.00637v1](http://arxiv.org/pdf/2402.00637v1)|null|\n", "2402.00453": "|**2024-02-01**|**Instruction Makes a Difference**|\u6307\u5bfc\u6709\u6240\u4f5c\u4e3a|Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney|We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improvement.|\u6211\u4eec\u5f15\u5165\u4e86\u6307\u4ee4\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08iDocVQA\uff09\u6570\u636e\u96c6\u548c\u5927\u578b\u8bed\u8a00\u6587\u6863\uff08LLaDoc\uff09\u6a21\u578b\uff0c\u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u7528\u4e8e\u6587\u6863\u5206\u6790\u548c\u6587\u6863\u56fe\u50cf\u9884\u6d4b\u7684\u8bed\u8a00\u89c6\u89c9\uff08LV\uff09\u6a21\u578b\u3002\u901a\u5e38\uff0cDocVQA \u4efb\u52a1\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u662f\u5728\u7f3a\u4e4f\u6307\u4ee4\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4f7f\u7528\u9075\u5faa\u6307\u4ee4\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u6700\u65b0\u7684 (SotA) \u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u52a9\u624b (LLaVA)1.5 \u4f5c\u4e3a\u57fa\u672c\u6a21\u578b\u6765\u6bd4\u8f83\u6587\u6863\u76f8\u5173\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u57fa\u4e8e\u8f6e\u8be2\u7684\u5bf9\u8c61\u63a2\u6d4b\u8bc4\u4f30\uff08POPE\uff09\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6d3e\u751f\u6a21\u578b\u7684\u5bf9\u8c61\u5e7b\u89c9\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6307\u4ee4\u8c03\u4f18\u6027\u80fd\u662f\u96f6\u6837\u672c\u6027\u80fd\u7684 11 \u500d\u5230 32 \u500d\uff0c\u662f\u975e\u6307\u4ee4\uff08\u4f20\u7edf\u4efb\u52a1\uff09\u5fae\u8c03\u7684 0.1% \u5230 4.2%\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u4f46\u4ecd\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0894.36%\uff09\uff0c\u8fd9\u610f\u5473\u7740\u8fd8\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002|[2402.00453v1](http://arxiv.org/pdf/2402.00453v1)|**[link](https://github.com/ltu-machine-learning/idocvqa)**|\n", "2402.00357": "|**2024-02-01**|**Safety of Multimodal Large Language Models on Images and Text**|\u56fe\u50cf\u548c\u6587\u672c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027|Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao|Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text. We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey. Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs. Next, we comprehensively present attack and defense techniques related to MLLMs' safety. Finally, we analyze several unsolved issues and discuss promising research directions.|\u88ab\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u5f3a\u5927\u529f\u80fd\u6240\u5438\u5f15\uff0c\u516c\u4f17\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528\u5b83\u4eec\u6765\u63d0\u9ad8\u65e5\u5e38\u5de5\u4f5c\u7684\u6548\u7387\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u5f53\u8fd9\u4e9b\u6a21\u578b\u90e8\u7f72\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u65f6\uff0cMLLM \u5bf9\u4e0d\u5b89\u5168\u6307\u4ee4\u7684\u8106\u5f31\u6027\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5b89\u5168\u98ce\u9669\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u8c03\u67e5\u4e86\u5f53\u524d\u5728\u56fe\u50cf\u548c\u6587\u672c\u4e0a\u7684 MLLM \u5b89\u5168\u6027\u8bc4\u4f30\u3001\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u9762\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u9996\u5148\u4ecb\u7ecd MLLM \u5728\u56fe\u50cf\u548c\u6587\u672c\u65b9\u9762\u7684\u6982\u8ff0\u4ee5\u53ca\u5bf9\u5b89\u5168\u6027\u7684\u7406\u89e3\uff0c\u8fd9\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u4e86\u89e3\u6211\u4eec\u8c03\u67e5\u7684\u8be6\u7ec6\u8303\u56f4\u3002\u7136\u540e\uff0c\u6211\u4eec\u5ba1\u67e5\u7528\u4e8e\u8861\u91cf MLLM \u5b89\u5168\u6027\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6307\u6807\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5168\u9762\u4ecb\u7ecd\u4e0eMLLM\u5b89\u5168\u76f8\u5173\u7684\u653b\u51fb\u548c\u9632\u5fa1\u6280\u672f\u3002\u6700\u540e\uff0c\u6211\u4eec\u5206\u6790\u4e86\u51e0\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u5e76\u8ba8\u8bba\u4e86\u6709\u524d\u9014\u7684\u7814\u7a76\u65b9\u5411\u3002|[2402.00357v1](http://arxiv.org/pdf/2402.00357v1)|null|\n", "2402.00290": "|**2024-02-01**|**Multimodal Embodied Interactive Agent for Cafe Scene**|\u5496\u5561\u9986\u573a\u666f\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u4ea4\u4e92\u4ee3\u7406|Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin|With the surge in the development of large language models, embodied intelligence has attracted increasing attention. Nevertheless, prior works on embodied intelligence typically encode scene or historical memory in an unimodal manner, either visual or linguistic, which complicates the alignment of the model's action planning with embodied control. To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions. Specifically, we propose a novel Multimodal Environment Memory (MEM) module, facilitating the integration of embodied control with large models through the visual-language memory of scenes. This capability enables MEIA to generate executable action plans based on diverse requirements and the robot's capabilities. We conduct experiments in a dynamic virtual cafe environment, utilizing multiple large models through zero-shot learning, and carefully design scenarios for various situations. The experimental results showcase the promising performance of our MEIA in various embodied interactive tasks.|\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u7684\u6fc0\u589e\uff0c\u5177\u8eab\u667a\u80fd\u8d8a\u6765\u8d8a\u53d7\u5230\u4eba\u4eec\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5148\u524d\u5173\u4e8e\u5177\u8eab\u667a\u80fd\u7684\u7814\u7a76\u901a\u5e38\u4ee5\u89c6\u89c9\u6216\u8bed\u8a00\u7684\u5355\u6a21\u6001\u65b9\u5f0f\u5bf9\u573a\u666f\u6216\u5386\u53f2\u8bb0\u5fc6\u8fdb\u884c\u7f16\u7801\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u7684\u884c\u52a8\u89c4\u5212\u4e0e\u5177\u8eab\u63a7\u5236\u7684\u534f\u8c03\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u6a21\u5f0f\u5d4c\u5165\u5f0f\u4ea4\u4e92\u4ee3\u7406\uff08MEIA\uff09\uff0c\u5b83\u80fd\u591f\u5c06\u4ee5\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u9ad8\u7ea7\u4efb\u52a1\u8f6c\u6362\u4e3a\u4e00\u7cfb\u5217\u53ef\u6267\u884c\u52a8\u4f5c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u73af\u5883\u8bb0\u5fc6\uff08MEM\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u573a\u666f\u7684\u89c6\u89c9\u8bed\u8a00\u8bb0\u5fc6\u4fc3\u8fdb\u4f53\u73b0\u63a7\u5236\u4e0e\u5927\u578b\u6a21\u578b\u7684\u96c6\u6210\u3002\u6b64\u529f\u80fd\u4f7f MEIA \u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u8981\u6c42\u548c\u673a\u5668\u4eba\u7684\u529f\u80fd\u751f\u6210\u53ef\u6267\u884c\u7684\u884c\u52a8\u8ba1\u5212\u3002\u6211\u4eec\u5728\u52a8\u6001\u7684\u865a\u62df\u5496\u5561\u9986\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u5229\u7528\u591a\u4e2a\u5927\u578b\u6a21\u578b\uff0c\u5e76\u9488\u5bf9\u5404\u79cd\u60c5\u51b5\u7cbe\u5fc3\u8bbe\u8ba1\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u6211\u4eec\u7684 MEIA \u5728\u5404\u79cd\u5177\u4f53\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u826f\u597d\u8868\u73b0\u3002|[2402.00290v1](http://arxiv.org/pdf/2402.00290v1)|null|\n"}, "LLM": {"2402.00626": "|**2024-02-01**|**Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks**|\u89c6\u89c9\u6cd5\u5b66\u7855\u58eb\u53ef\u4ee5\u7528\u81ea\u5df1\u751f\u6210\u7684\u5370\u5237\u653b\u51fb\u6765\u6b3a\u9a97\u81ea\u5df1|Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer|Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4.|\u6700\u8fd1\uff0c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff1b\u4e00\u7c7b\u65b0\u7684 VL \u6a21\u578b\uff0c\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5bf9\u5370\u5237\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u7814\u7a76\uff0c\u5370\u5237\u653b\u51fb\u6d89\u53ca\u5c06\u8bef\u5bfc\u6027\u6587\u672c\u53e0\u52a0\u5230\u56fe\u50cf\u4e0a\u3002\u6b64\u5916\uff0c\u5148\u524d\u7684\u5de5\u4f5c\u5370\u5237\u653b\u51fb\u4f9d\u8d56\u4e8e\u4ece\u9884\u5b9a\u4e49\u7684\u7c7b\u96c6\u4e2d\u968f\u673a\u62bd\u53d6\u8bef\u5bfc\u6027\u7c7b\u3002\u7136\u800c\uff0c\u968f\u673a\u9009\u62e9\u7684\u7c7b\u522b\u53ef\u80fd\u4e0d\u662f\u6700\u6709\u6548\u7684\u653b\u51fb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u7528\u4e8e\u6d4b\u8bd5 LVLM \u5bf9\u5370\u5237\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u6709\u6548\u7684\u5370\u5237\u653b\u51fb\uff1a\u81ea\u751f\u6210\u7684\u5370\u5237\u653b\u51fb\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7ed9\u5b9a\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7b80\u5355\u5730\u63d0\u793a\u5b83\u4eec\u63a8\u8350\u5370\u5237\u653b\u51fb\u6765\u5229\u7528 GPT-4V \u7b49\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u8a00\u529f\u80fd\u3002\u4f7f\u7528\u6211\u4eec\u7684\u65b0\u9896\u57fa\u51c6\uff0c\u6211\u4eec\u53d1\u73b0\u5370\u5237\u653b\u51fb\u5bf9 LVLM \u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u653b\u51fb\u76f8\u6bd4\uff0cGPT-4V \u4f7f\u7528\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\u63a8\u8350\u7684\u5370\u5237\u653b\u51fb\u4e0d\u4ec5\u5bf9 GPT-4V \u672c\u8eab\u66f4\u6709\u6548\uff0c\u800c\u4e14\u5bf9\u8bb8\u591a\u529f\u80fd\u8f83\u5f31\u4f46\u6d41\u884c\u7684\u5f00\u6e90\u6a21\u578b\uff08\u5982 LLaVA\u3001InstructBLIP\u3001\u548c MiniGPT4\u3002|[2402.00626v1](http://arxiv.org/pdf/2402.00626v1)|null|\n"}, "Transformer": {"2402.00763": "|**2024-02-01**|**360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming**|360-GS\uff1a\u7528\u4e8e\u5ba4\u5185\u6f2b\u6e38\u7684\u5e03\u5c40\u5f15\u5bfc\u7684\u5168\u666f\u9ad8\u65af\u5206\u5e03|Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo|3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.|3D \u9ad8\u65af\u6e85\u5c04 (3D-GS) \u6700\u8fd1\u56e0\u5176\u5b9e\u65f6\u548c\u903c\u771f\u7684\u6e32\u67d3\u800c\u5f15\u8d77\u4e86\u6781\u5927\u7684\u5173\u6ce8\u3002\u8be5\u6280\u672f\u901a\u5e38\u5c06\u900f\u89c6\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u5c06\u4e00\u7ec4 3D \u692d\u5706\u9ad8\u65af\u5206\u5e03\u5230\u56fe\u50cf\u5e73\u9762\u4e0a\u6765\u4f18\u5316\u5b83\u4eec\uff0c\u4ece\u800c\u4ea7\u751f 2D \u9ad8\u65af\u5206\u5e03\u3002\u7136\u800c\uff0c\u5c06 3D-GS \u5e94\u7528\u4e8e\u5168\u666f\u8f93\u5165\u5728\u4f7f\u7528 2D \u9ad8\u65af\u5bf9 ${360^\\circ}$ \u56fe\u50cf\u7684\u7403\u9762\u6295\u5f71\u8fdb\u884c\u6709\u6548\u5efa\u6a21\u65b9\u9762\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8f93\u5165\u5168\u666f\u56fe\u901a\u5e38\u5f88\u7a00\u758f\uff0c\u5bfc\u81f4 3D \u9ad8\u65af\u521d\u59cb\u5316\u4e0d\u53ef\u9760\uff0c\u8fdb\u800c\u5bfc\u81f4 3D-GS \u8d28\u91cf\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u65e0\u7eb9\u7406\u5e73\u9762\uff08\u4f8b\u5982\u5899\u58c1\u548c\u5730\u677f\uff09\u7684\u51e0\u4f55\u5f62\u72b6\u53d7\u5230\u7ea6\u675f\uff0c3D-GS \u5f88\u96be\u7528\u692d\u5706\u9ad8\u65af\u6a21\u578b\u5bf9\u8fd9\u4e9b\u5e73\u5766\u533a\u57df\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u5bfc\u81f4\u65b0\u89c6\u56fe\u4e2d\u51fa\u73b0\u660e\u663e\u7684\u6f02\u6d6e\u7269\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 360-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u6709\u9650\u7684\u5168\u666f\u8f93\u5165\u96c6\u7684\u65b0\u578b $360^{\\circ}$ \u9ad8\u65af\u6cfc\u6e85\u3002 360-GS \u4e0d\u662f\u5c06 3D \u9ad8\u65af\u76f4\u63a5\u55b7\u5c04\u5230\u7403\u9762\u4e0a\uff0c\u800c\u662f\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u5355\u4f4d\u7403\u4f53\u7684\u5207\u5e73\u9762\u4e0a\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u6620\u5c04\u5230\u7403\u9762\u6295\u5f71\u3002\u8fd9\u79cd\u8c03\u6574\u4f7f\u5f97\u80fd\u591f\u4f7f\u7528\u9ad8\u65af\u6765\u8868\u793a\u6295\u5f71\u3002\u6211\u4eec\u901a\u8fc7\u5229\u7528\u5168\u666f\u56fe\u4e2d\u7684\u5e03\u5c40\u5148\u9a8c\u6765\u6307\u5bfc 360-GS \u7684\u4f18\u5316\uff0c\u8fd9\u4e9b\u5148\u9a8c\u6613\u4e8e\u83b7\u53d6\u5e76\u5305\u542b\u6709\u5173\u5ba4\u5185\u573a\u666f\u7684\u5f3a\u5927\u7ed3\u6784\u4fe1\u606f\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c360-GS \u53ef\u4ee5\u5b9e\u73b0\u5168\u666f\u6e32\u67d3\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u4f2a\u5f71\u66f4\u5c11\uff0c\u4ece\u800c\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u63d0\u4f9b\u6c89\u6d78\u5f0f\u6f2b\u6e38\u3002|[2402.00763v1](http://arxiv.org/pdf/2402.00763v1)|null|\n", "2402.00752": "|**2024-02-01**|**GS++: Error Analyzing and Optimal Gaussian Splatting**|GS++\uff1a\u8bef\u5dee\u5206\u6790\u548c\u6700\u4f73\u9ad8\u65af\u5206\u5e03|Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo|3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements. However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.|3D Gaussian Splatting \u5728\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\u548c\u5e94\u7528\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4eba\u4eec\u5bf9\u8be5\u6280\u672f\u5728\u70b9\u4e91\u5b58\u50a8\u3001\u6027\u80fd\u548c\u7a00\u758f\u89c6\u70b9\u7684\u9c81\u68d2\u6027\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\u63d0\u51fa\u4e86\u62c5\u5fe7\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u5404\u79cd\u6539\u8fdb\u3002\u7136\u800c\uff0c\u4eba\u4eec\u660e\u663e\u7f3a\u4e4f\u5bf9\u6cfc\u6e85\u672c\u8eab\u56fa\u6709\u7684\u5c40\u90e8\u4eff\u5c04\u8fd1\u4f3c\u5f15\u5165\u7684\u6295\u5f71\u8bef\u5dee\u7684\u5173\u6ce8\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u8bef\u5dee\u5bf9\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u672c\u6587\u4ece\u6295\u5f71\u51fd\u6570 $\\phi$ \u7684\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u7684\u6b8b\u5dee\u5f00\u59cb\uff0c\u8ba8\u8bba 3D \u9ad8\u65af\u5206\u5e03\u7684\u6295\u5f71\u8bef\u5dee\u51fd\u6570\u3002\u8be5\u5206\u6790\u5efa\u7acb\u4e86\u8bef\u5dee\u4e0e\u9ad8\u65af\u5e73\u5747\u4f4d\u7f6e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u968f\u540e\uff0c\u672c\u6587\u5229\u7528\u51fd\u6570\u4f18\u5316\u7406\u8bba\uff0c\u5206\u6790\u51fd\u6570\u7684\u6781\u5c0f\u503c\uff0c\u4e3a\u9ad8\u65af\u5206\u5e03\u63d0\u4f9b\u4e00\u79cd\u6700\u4f18\u6295\u5f71\u7b56\u7565\uff0c\u79f0\u4e3a\u6700\u4f18\u9ad8\u65af\u5206\u5e03\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u8fd9\u79cd\u6295\u5f71\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u4f2a\u5f71\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u4ee4\u4eba\u4fe1\u670d\u7684\u771f\u5b9e\u6e32\u67d3\u3002|[2402.00752v1](http://arxiv.org/pdf/2402.00752v1)|null|\n", "2402.00680": "|**2024-02-01**|**LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video Compression**|LVC-LGMC\uff1a\u7528\u4e8e\u5b66\u4e60\u89c6\u9891\u538b\u7f29\u7684\u8054\u5408\u5c40\u90e8\u548c\u5168\u5c40\u8fd0\u52a8\u8865\u507f|Wei Jiang, Junru Li, Kai Zhang, Li Zhang|Existing learned video compression models employ flow net or deformable convolutional networks (DCN) to estimate motion information. However, the limited receptive fields of flow net and DCN inherently direct their attentiveness towards the local contexts. Global contexts, such as large-scale motions and global correlations among frames are ignored, presenting a significant bottleneck for capturing accurate motions. To address this issue, we propose a joint local and global motion compensation module (LGMC) for leaned video coding. More specifically, we adopt flow net for local motion compensation. To capture global context, we employ the cross attention in feature domain for motion compensation. In addition, to avoid the quadratic complexity of vanilla cross attention, we divide the softmax operations in attention into two independent softmax operations, leading to linear complexity. To validate the effectiveness of our proposed LGMC, we integrate it with DCVC-TCM and obtain learned video compression with joint local and global motion compensation (LVC-LGMC). Extensive experiments demonstrate that our LVC-LGMC has significant rate-distortion performance improvements over baseline DCVC-TCM.|\u73b0\u6709\u7684\u5b66\u4e60\u89c6\u9891\u538b\u7f29\u6a21\u578b\u91c7\u7528\u6d41\u7f51\u6216\u53ef\u53d8\u5f62\u5377\u79ef\u7f51\u7edc\uff08DCN\uff09\u6765\u4f30\u8ba1\u8fd0\u52a8\u4fe1\u606f\u3002\u7136\u800c\uff0c\u6d41\u7f51\u548c DCN \u6709\u9650\u7684\u611f\u53d7\u91ce\u672c\u8d28\u4e0a\u5c06\u5b83\u4eec\u7684\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u5c40\u90e8\u73af\u5883\u4e0a\u3002\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4f8b\u5982\u5927\u89c4\u6a21\u8fd0\u52a8\u548c\u5e27\u4e4b\u95f4\u7684\u5168\u5c40\u76f8\u5173\u6027\u88ab\u5ffd\u7565\uff0c\u4e3a\u6355\u83b7\u51c6\u786e\u8fd0\u52a8\u5e26\u6765\u4e86\u91cd\u5927\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u503e\u659c\u89c6\u9891\u7f16\u7801\u7684\u8054\u5408\u5c40\u90e8\u548c\u5168\u5c40\u8fd0\u52a8\u8865\u507f\u6a21\u5757\uff08LGMC\uff09\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u6d41\u7f51\u7edc\u8fdb\u884c\u5c40\u90e8\u8fd0\u52a8\u8865\u507f\u3002\u4e3a\u4e86\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u6211\u4eec\u91c7\u7528\u7279\u5f81\u57df\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u8fdb\u884c\u8fd0\u52a8\u8865\u507f\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u907f\u514d\u666e\u901a\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u6211\u4eec\u5c06\u6ce8\u610f\u529b\u4e2d\u7684softmax\u64cd\u4f5c\u5206\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684softmax\u64cd\u4f5c\uff0c\u4ece\u800c\u5bfc\u81f4\u7ebf\u6027\u590d\u6742\u5ea6\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684 LGMC \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5c06\u5176\u4e0e DCVC-TCM \u96c6\u6210\uff0c\u5e76\u83b7\u5f97\u5177\u6709\u8054\u5408\u5c40\u90e8\u548c\u5168\u5c40\u8fd0\u52a8\u8865\u507f\u7684\u5b66\u4e60\u89c6\u9891\u538b\u7f29\uff08LVC-LGMC\uff09\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 LVC-LGMC \u6bd4\u57fa\u7ebf DCVC-TCM \u5177\u6709\u663e\u7740\u7684\u7387\u5931\u771f\u6027\u80fd\u6539\u8fdb\u3002|[2402.00680v1](http://arxiv.org/pdf/2402.00680v1)|null|\n", "2402.00433": "|**2024-02-01**|**Merging Multi-Task Models via Weight-Ensembling Mixture of Experts**|\u901a\u8fc7\u4e13\u5bb6\u6743\u91cd\u7ec4\u5408\u5408\u5e76\u591a\u4efb\u52a1\u6a21\u578b|Anke Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, Dacheng Tao|Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/|\u5c06\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u5404\u79cd\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u5408\u5e76\u5230\u5355\u4e2a\u7edf\u4e00\u6a21\u578b\u4e2d\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u6240\u6709\u4efb\u52a1\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\uff08\u4ee5\u4efb\u52a1\u7b97\u672f\u4e3a\u4f8b\uff09\u5df2\u88ab\u8bc1\u660e\u65e2\u6709\u6548\u53c8\u53ef\u6269\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5728\u539f\u59cb\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u5185\u5bfb\u627e\u9759\u6001\u6700\u4f18\u89e3\u3002\u4e00\u4e2a\u663e\u7740\u7684\u6311\u6218\u662f\u51cf\u8f7b\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u4e4b\u95f4\u7684\u5e72\u6270\uff0c\u8fd9\u4f1a\u5927\u5927\u964d\u4f4e\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5408\u5e76\u5927\u90e8\u5206\u53c2\u6570\uff0c\u540c\u65f6\u5c06 Transformer \u5c42\u7684 MLP \u5347\u7ea7\u4e3a\u6743\u91cd\u96c6\u6210\u6df7\u5408\u4e13\u5bb6 (MoE) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u52a8\u6001\u96c6\u6210\u5171\u4eab\u77e5\u8bc6\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u9002\u5e94\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u7279\u5b9a\u9700\u6c42\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u79bb\u5171\u4eab\u77e5\u8bc6\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u7136\u540e\u52a8\u6001\u96c6\u6210\u5b83\u4eec\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u53c2\u6570\u5e72\u6270\u95ee\u9898\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4f20\u7edf\u7684\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u5b9e\u9a8c\uff0c\u5e76\u8bc4\u4f30\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6211\u4eec\u65b9\u6cd5\u7684\u5168\u9762\u7406\u89e3\u3002\u4ee3\u7801\u53ef\u5728 https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/ \u83b7\u53d6|[2402.00433v1](http://arxiv.org/pdf/2402.00433v1)|null|\n", "2402.00321": "|**2024-02-01**|**SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and Judger Mechanism**|SmartCooper\uff1a\u5177\u6709\u81ea\u9002\u5e94\u878d\u5408\u548c\u5224\u65ad\u673a\u5236\u7684\u8f66\u8f86\u534f\u540c\u611f\u77e5|Yuang Zhang, Haonan An, Zhengru Fang, Guowen Xu, Yuan Zhou, Xianhao Chen, Yuguang Fang|In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10\\% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\\% compared with state-of-the-art schemes.|\u8fd1\u5e74\u6765\uff0c\u81ea\u52a8\u9a7e\u9a76\u56e0\u5176\u901a\u8fc7\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAV\uff09\u4e4b\u95f4\u7684\u534f\u4f5c\u611f\u77e5\u6765\u6539\u5584\u9053\u8def\u5b89\u5168\u7684\u6f5c\u529b\u800c\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8f66\u8f86\u4f20\u8f93\u73af\u5883\u4e2d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4fe1\u9053\u53d8\u5316\u9700\u8981\u901a\u4fe1\u8d44\u6e90\u7684\u52a8\u6001\u5206\u914d\u3002\u6b64\u5916\uff0c\u5728\u534f\u4f5c\u611f\u77e5\u7684\u80cc\u666f\u4e0b\uff0c\u91cd\u8981\u7684\u662f\u8981\u8ba4\u8bc6\u5230\u5e76\u975e\u6240\u6709 CAV \u90fd\u80fd\u8d21\u732e\u6709\u4ef7\u503c\u7684\u6570\u636e\uff0c\u6709\u4e9b CAV \u6570\u636e\u751a\u81f3\u4f1a\u5bf9\u534f\u4f5c\u611f\u77e5\u4ea7\u751f\u6709\u5bb3\u5f71\u54cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 SmartCooper\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u901a\u4fe1\u4f18\u5316\u548c\u5224\u65ad\u5668\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb CAV \u6570\u636e\u878d\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u662f\u4f18\u5316\u8f66\u8f86\u7684\u8fde\u63a5\u6027\uff0c\u540c\u65f6\u8003\u8651\u901a\u4fe1\u9650\u5236\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u7f16\u7801\u5668\uff0c\u4ee5\u6839\u636e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u52a8\u6001\u8c03\u6574\u538b\u7f29\u6bd4\u3002\u968f\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5224\u65ad\u5668\u673a\u5236\u6765\u8fc7\u6ee4\u81ea\u9002\u5e94\u89e3\u7801\u5668\u91cd\u5efa\u7684\u6709\u5bb3\u56fe\u50cf\u6570\u636e\u3002\u6211\u4eec\u5728 OpenCOOD \u5e73\u53f0\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u975e\u8bc4\u5224\u8005\u65b9\u6848\u76f8\u6bd4\uff0c\u901a\u4fe1\u6210\u672c\u5927\u5e45\u964d\u4f4e\u4e86 23.10%\u3002\u6b64\u5916\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6848\u76f8\u6bd4\uff0c\u6211\u4eec\u5c06 Intersection over Union (AP@IoU) \u7684\u5e73\u5747\u7cbe\u5ea6\u663e\u7740\u63d0\u9ad8\u4e86 7.15%\u3002|[2402.00321v1](http://arxiv.org/pdf/2402.00321v1)|null|\n", "2402.00253": "|**2024-02-01**|**A Survey on Hallucination in Large Vision-Language Models**|\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7b\u89c9\u7684\u8c03\u67e5|Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng|Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.|\u6700\u8fd1\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u53d1\u5c55\u56e0\u5176\u5b9e\u9645\u5b9e\u65bd\u6f5c\u529b\u800c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u201c\u5e7b\u89c9\u201d\uff0c\u6216\u8005\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u5b9e\u9645\u89c6\u89c9\u5185\u5bb9\u4e0e\u76f8\u5e94\u6587\u672c\u751f\u6210\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u5bf9\u4f7f\u7528 LVLM \u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5728\u8fd9\u9879\u7efc\u5408\u8c03\u67e5\u4e2d\uff0c\u6211\u4eec\u5256\u6790\u4e86 LVLM \u76f8\u5173\u7684\u5e7b\u89c9\uff0c\u8bd5\u56fe\u5efa\u7acb\u4e00\u4e2a\u6982\u8ff0\u5e76\u4fc3\u8fdb\u672a\u6765\u7684\u7f13\u89e3\u3002\u6211\u4eec\u7684\u5ba1\u67e5\u4ece\u6f84\u6e05 LVLM \u5e7b\u89c9\u7684\u6982\u5ff5\u5f00\u59cb\uff0c\u5448\u73b0\u5404\u79cd\u5e7b\u89c9\u75c7\u72b6\u5e76\u5f3a\u8c03 LVLM \u5e7b\u89c9\u56fa\u6709\u7684\u72ec\u7279\u6311\u6218\u3002\u968f\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u4e13\u95e8\u4e3a\u8bc4\u4f30 LVLM \u7279\u6709\u7684\u5e7b\u89c9\u800c\u5b9a\u5236\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6df1\u5165\u7814\u7a76\u4e86\u8fd9\u4e9b\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\uff0c\u5305\u62ec\u6765\u81ea\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u7ec4\u4ef6\u7684\u89c1\u89e3\u3002\u6211\u4eec\u8fd8\u4e25\u683c\u5ba1\u67e5\u73b0\u6709\u7684\u51cf\u8f7b\u5e7b\u89c9\u7684\u65b9\u6cd5\u3002\u8ba8\u8bba\u4e86\u4e0e LVLM \u5185\u5e7b\u89c9\u76f8\u5173\u7684\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u7ed3\u675f\u672c\u6b21\u8c03\u67e5\u3002|[2402.00253v1](http://arxiv.org/pdf/2402.00253v1)|null|\n"}, "Nerf": {}, "3DGS": {"2402.00525": "|**2024-02-01**|**StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering**|StopThePop\uff1a\u7528\u4e8e\u89c6\u56fe\u4e00\u81f4\u5b9e\u65f6\u6e32\u67d3\u7684\u6392\u5e8f\u9ad8\u65af\u6cfc\u6e85|Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger|Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.|\u9ad8\u65af\u6cfc\u6e85\u5df2\u6210\u4e3a\u4ece\u4e0d\u540c\u9886\u57df\u7684\u56fe\u50cf\u6784\u5efa 3D \u8868\u793a\u7684\u91cd\u8981\u6a21\u578b\u3002\u7136\u800c\uff0c3D \u9ad8\u65af\u55b7\u5c04\u6e32\u67d3\u7ba1\u9053\u7684\u6548\u7387\u4f9d\u8d56\u4e8e\u591a\u79cd\u7b80\u5316\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528\u5355\u4e2a\u89c6\u56fe\u7a7a\u95f4\u6df1\u5ea6\u5c06\u9ad8\u65af\u56fe\u51cf\u5c11\u4e3a 2D splats \u4f1a\u5728\u89c6\u56fe\u65cb\u8f6c\u671f\u95f4\u5f15\u5165\u5f39\u51fa\u548c\u6df7\u5408\u4f2a\u5f71\u3002\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u9700\u8981\u7cbe\u786e\u7684\u6bcf\u50cf\u7d20\u6df1\u5ea6\u8ba1\u7b97\uff0c\u4f46\u4e0e\u5168\u5c40\u6392\u5e8f\u64cd\u4f5c\u76f8\u6bd4\uff0c\u5b8c\u6574\u7684\u6bcf\u50cf\u7d20\u6392\u5e8f\u88ab\u8bc1\u660e\u6210\u672c\u8fc7\u9ad8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u5149\u6805\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ee5\u6700\u5c0f\u7684\u5904\u7406\u5f00\u9500\u7cfb\u7edf\u5730\u91cd\u65b0\u5229\u7528\u548c\u5254\u9664\u56fe\u5757\u3002\u6b63\u5982\u5b9a\u91cf\u548c\u5b9a\u6027\u6d4b\u91cf\u6240\u8bc1\u660e\u7684\u90a3\u6837\uff0c\u6211\u4eec\u7684\u8f6f\u4ef6\u5149\u6805\u5316\u5668\u6709\u6548\u5730\u6d88\u9664\u4e86\u5f39\u51fa\u4f2a\u5f71\u548c\u89c6\u56fe\u4e0d\u4e00\u81f4\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u901a\u8fc7\u5f39\u51fa\u6765\u6b3a\u9a97\u89c6\u56fe\u76f8\u5173\u6548\u679c\u7684\u53ef\u80fd\u6027\uff0c\u786e\u4fdd\u66f4\u771f\u5b9e\u7684\u8868\u793a\u3002\u5c3d\u7ba1\u6d88\u9664\u4e86\u4f5c\u5f0a\u884c\u4e3a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ecd\u4e3a\u6d4b\u8bd5\u56fe\u50cf\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u5b9a\u91cf\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8fd0\u52a8\u4e2d\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u4e00\u81f4\u6027\u3002\u7531\u4e8e\u5176\u8bbe\u8ba1\uff0c\u6211\u4eec\u7684\u5206\u5c42\u65b9\u6cd5\u5e73\u5747\u4ec5\u6bd4\u539f\u59cb\u9ad8\u65af\u6cfc\u6e85\u6162 4%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f3a\u5236\u4e00\u81f4\u6027\u53ef\u4ee5\u5c06\u9ad8\u65af\u6570\u91cf\u51cf\u5c11\u5927\u7ea6\u4e00\u534a\uff0c\u540c\u65f6\u8d28\u91cf\u548c\u89c6\u56fe\u4e00\u81f4\u6027\u51e0\u4e4e\u76f8\u540c\u3002\u56e0\u6b64\uff0c\u6e32\u67d3\u6027\u80fd\u51e0\u4e4e\u7ffb\u500d\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u539f\u59cb\u9ad8\u65af\u6cfc\u6e85\u6cd5\u5feb 1.6 \u500d\uff0c\u540c\u65f6\u5185\u5b58\u9700\u6c42\u51cf\u5c11 50%\u3002|[2402.00525v1](http://arxiv.org/pdf/2402.00525v1)|null|\n"}, "3D/CG": {"2402.00867": "|**2024-02-01**|**AToM: Amortized Text-to-Mesh using 2D Diffusion**|AToM\uff1a\u4f7f\u7528 2D \u6269\u6563\u7684\u644a\u9500\u6587\u672c\u5230\u7f51\u683c|Guocheng Qian, Junli Cao, Aliaksandr Siarohin, Yash Kant, Chaoyang Wang, Michael Vasilkovsky, Hsin-Ying Lee, Yuwei Fang, Ivan Skorokhodov, Peiye Zhuang, et.al.|We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously. In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts. Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability. Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs. AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions.|\u6211\u4eec\u5f15\u5165\u4e86 Amortized Text-to-Mesh (AToM)\uff0c\u8fd9\u662f\u4e00\u79cd\u540c\u65f6\u8de8\u591a\u4e2a\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f18\u5316\u7684\u524d\u9988\u6587\u672c\u5230\u7f51\u683c\u6846\u67b6\u3002\u73b0\u6709\u7684\u6587\u672c\u8f6c 3D \u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8017\u65f6\u7684\u6bcf\u6b21\u63d0\u793a\u4f18\u5316\u4ee5\u53ca\u901a\u5e38\u8f93\u51fa\u9664\u591a\u8fb9\u5f62\u7f51\u683c\u4e4b\u5916\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u800c AToM \u53ef\u4ee5\u5728\u4e0d\u5230 1 \u79d2\u7684\u65f6\u95f4\u5185\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u7f51\u683c\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea6 10 \u500d\u6210\u672c\uff0c\u5e76\u63a8\u5e7f\u5230\u770b\u4e0d\u89c1\u7684\u63d0\u793a\u3002\u6211\u4eec\u7684\u5173\u952e\u60f3\u6cd5\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4e09\u5e73\u9762\u7684\u6587\u672c\u5230\u7f51\u683c\u67b6\u6784\uff0c\u5177\u6709\u4e24\u9636\u6bb5\u644a\u9500\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u786e\u4fdd\u7a33\u5b9a\u7684\u8bad\u7ec3\u5e76\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u5bf9\u5404\u79cd\u5373\u65f6\u57fa\u51c6\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cAToM \u7684\u51c6\u786e\u5ea6\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u644a\u9500\u65b9\u6cd5\uff08\u5728 DF415 \u6570\u636e\u96c6\u4e2d\uff09\uff0c\u5e76\u4e14\u53ef\u751f\u6210\u66f4\u5177\u53ef\u533a\u5206\u6027\u548c\u66f4\u9ad8\u8d28\u91cf\u7684 3D \u8f93\u51fa\u3002\u4e0e\u6309\u63d0\u793a\u89e3\u51b3\u65b9\u6848\u4e0d\u540c\uff0cAToM \u8868\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u6027\uff0c\u4e3a\u770b\u4e0d\u89c1\u7684\u63d2\u503c\u63d0\u793a\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684 3D \u8d44\u4ea7\uff0c\u65e0\u9700\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u4e00\u6b65\u4f18\u5316\u3002|[2402.00867v1](http://arxiv.org/pdf/2402.00867v1)|null|\n", "2402.00863": "|**2024-02-01**|**Geometry Transfer for Stylizing Radiance Fields**|\u7528\u4e8e\u98ce\u683c\u5316\u8f90\u5c04\u573a\u7684\u51e0\u4f55\u4f20\u9012|Hyunyoung Jung, Seonghyeon Nam, Nikolaos SarafianosSungjoo Yoo, Alexander Sorkine-Hornung, Rakesh Ranjan|Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.|\u5f62\u72b6\u548c\u51e0\u4f55\u56fe\u6848\u5bf9\u4e8e\u5b9a\u4e49\u98ce\u683c\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684 3D \u6837\u5f0f\u4f20\u8f93\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4f20\u8f93\u989c\u8272\u548c\u7eb9\u7406\uff0c\u5e38\u5e38\u5ffd\u7565\u51e0\u4f55\u65b9\u9762\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u51e0\u4f55\u8fc1\u79fb\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u51e0\u4f55\u53d8\u5f62\u8fdb\u884c 3D \u98ce\u683c\u8fc1\u79fb\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8be5\u6280\u672f\u91c7\u7528\u6df1\u5ea6\u56fe\u6765\u63d0\u53d6\u98ce\u683c\u6307\u5357\uff0c\u968f\u540e\u5e94\u7528\u4e8e\u98ce\u683c\u5316\u8f90\u5c04\u573a\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5229\u7528 3D \u573a\u666f\u4e2d\u7684\u51e0\u4f55\u7ebf\u7d22\u7684\u65b0\u6280\u672f\uff0c\u4ece\u800c\u589e\u5f3a\u5ba1\u7f8e\u8868\u73b0\u529b\u5e76\u66f4\u51c6\u786e\u5730\u53cd\u6620\u9884\u671f\u98ce\u683c\u3002\u6211\u4eec\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u51e0\u4f55\u8fc1\u79fb\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u3001\u66f4\u5177\u8868\u73b0\u529b\u7684\u98ce\u683c\u5316\u8303\u56f4\uff0c\u4ece\u800c\u663e\u7740\u6269\u5927 3D \u98ce\u683c\u8fc1\u79fb\u7684\u8303\u56f4\u3002|[2402.00863v1](http://arxiv.org/pdf/2402.00863v1)|null|\n", "2402.00740": "|**2024-02-01**|**DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras**|DRSM\uff1a\u7528\u4e8e\u56fa\u5b9a\u5355\u76ee\u76f8\u673a\u52a8\u6001\u91cd\u5efa\u7684\u9ad8\u6548\u795e\u7ecf 4d \u5206\u89e3|Weixing Xie, Xiao Dong, Yong Yang, Qiqin Lin, Jingze Chen, Junfeng Yao, Xiaohu Guo|With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology. In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras. Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting. Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction. To overcome these difficulties, we propose deep supervised optimization and ray casting strategies. With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation.|\u968f\u7740\u89c6\u9891\u5171\u4eab\u548c\u76f4\u64ad\u5e94\u7528\u751f\u6210\u7684\u5355\u76ee\u89c6\u9891\u7684\u6d41\u884c\uff0c\u5728\u56fa\u5b9a\u5355\u76ee\u6444\u50cf\u673a\u4e2d\u91cd\u5efa\u548c\u7f16\u8f91\u52a8\u6001\u573a\u666f\u5df2\u6210\u4e3a\u4e00\u9879\u7279\u6b8a\u4f46\u4ee4\u4eba\u671f\u5f85\u7684\u6280\u672f\u3002\u4e0e\u5229\u7528\u591a\u89c6\u56fe\u89c2\u5bdf\u7684\u573a\u666f\u91cd\u5efa\u76f8\u6bd4\uff0c\u4ece\u5355\u4e2a\u89c6\u56fe\u5efa\u6a21\u52a8\u6001\u573a\u666f\u7684\u95ee\u9898\u660e\u663e\u66f4\u7f3a\u4e4f\u7ea6\u675f\u548c\u4e0d\u9002\u5b9a\u3002\u53d7\u795e\u7ecf\u6e32\u67d3\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\u6765\u89e3\u51b3\u5355\u76ee\u76f8\u673a\u4e2d\u52a8\u6001\u573a\u666f\u7684 4D \u5206\u89e3\u95ee\u9898\u3002\u6211\u4eec\u7684\u6846\u67b6\u5229\u7528\u5206\u89e3\u7684\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\u5e73\u9762\u6765\u8868\u793a 4D \u573a\u666f\uff0c\u5e76\u5f3a\u8c03\u901a\u8fc7\u5bc6\u96c6\u5149\u7ebf\u6295\u5c04\u6765\u5b66\u4e60\u52a8\u6001\u533a\u57df\u3002\u6765\u81ea\u5355\u4e00\u89c6\u56fe\u548c\u906e\u6321\u7684 3D \u7ebf\u7d22\u4e0d\u8db3\u4e5f\u662f\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u7279\u6b8a\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u56f0\u96be\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6df1\u5ea6\u76d1\u7763\u4f18\u5316\u548c\u5149\u7ebf\u6295\u5c04\u7b56\u7565\u3002\u901a\u8fc7\u5bf9\u5404\u79cd\u89c6\u9891\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684\u5355\u89c6\u56fe\u52a8\u6001\u573a\u666f\u8868\u793a\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u7ed3\u679c\u3002|[2402.00740v1](http://arxiv.org/pdf/2402.00740v1)|null|\n", "2402.00575": "|**2024-02-01**|**Diffusion-based Light Field Synthesis**|\u57fa\u4e8e\u6269\u6563\u7684\u5149\u573a\u5408\u6210|Ruisheng Gao, Yutong Liu, Zeyu Xiao, Zhiwei Xiong|Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing.|\u5149\u573a\uff08LF\uff09\u6709\u5229\u4e8e\u8bb0\u5f55\u8de8\u89d2\u5ea6\u7ef4\u5ea6\u7684\u7efc\u5408\u573a\u666f\u8f90\u5c04\uff0c\u57283D\u91cd\u5efa\u3001\u865a\u62df\u73b0\u5b9e\u548c\u8ba1\u7b97\u6444\u5f71\u7b49\u9886\u57df\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e3b\u6d41\u7684\u91c7\u96c6\u7b56\u7565\uff0c\u5149\u573a\u91c7\u96c6\u4e0d\u53ef\u907f\u514d\u5730\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002\u6d89\u53ca\u624b\u52a8\u6355\u83b7\u6216\u8d39\u529b\u7684\u8f6f\u4ef6\u5408\u6210\u3002\u9274\u4e8e\u8fd9\u6837\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 LFdiff\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3a LF \u5408\u6210\u91cf\u8eab\u5b9a\u5236\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6846\u67b6\uff0c\u5b83\u4ec5\u91c7\u7528\u5355\u4e2a RGB \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u3002LFdiff \u5229\u7528\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4f30\u8ba1\u7684\u89c6\u5dee\u7f51\u7edc\u5e76\u5305\u542b\u4e24\u4e2a\u72ec\u7279\u7684\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u65b0\u9896\u7684\u6761\u4ef6\u65b9\u6848\u548c\u4e00\u4e2a\u9488\u5bf9 LF \u6570\u636e\u5b9a\u5236\u7684\u566a\u58f0\u4f30\u8ba1\u7f51\u7edc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4f4d\u7f6e\u611f\u77e5\u7684\u626d\u66f2\u6761\u4ef6\u65b9\u6848\uff0c\u901a\u8fc7\u9c81\u68d2\u7684\u6761\u4ef6\u4fe1\u53f7\u589e\u5f3a\u89c6\u56fe\u95f4\u51e0\u4f55\u5b66\u4e60\u3002\u7136\u540e\u6211\u4eec\u63d0\u51fa DistgUnet\uff0c\u57fa\u4e8e\u89e3\u7f20\u7ed3\u7684\u566a\u58f0\u4f30\u8ba1\u7f51\u7edc\uff0c\u4ee5\u5229\u7528\u5168\u9762\u7684 LF \u8868\u793a\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLFdiff \u64c5\u957f\u5408\u6210\u89c6\u89c9\u4e0a\u4ee4\u4eba\u6109\u60a6\u4e14\u89c6\u5dee\u53ef\u63a7\u7684\u5149\u573a\uff0c\u5e76\u5177\u6709\u589e\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7efc\u5408\u7ed3\u679c\u8bc1\u5b9e\u4e86\u751f\u6210\u7684 LF \u6570\u636e\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u6db5\u76d6\u4f4e\u9891\u8d85\u5206\u8fa8\u7387\u548c\u91cd\u65b0\u805a\u7126\u7b49\u5e94\u7528\u3002|[2402.00575v1](http://arxiv.org/pdf/2402.00575v1)|null|\n", "2402.00341": "|**2024-02-01**|**Recasting Regional Lighting for Shadow Removal**|\u91cd\u94f8\u533a\u57df\u7167\u660e\u4ee5\u6d88\u9664\u9634\u5f71|Yuhao Liu, Zhanghan Ke, Ke Xu, Fang Liu, Zhenwei Wang, Rynson W. H. Lau|Removing shadows requires an understanding of both lighting conditions and object textures in a scene. Existing methods typically learn pixel-level color mappings between shadow and non-shadow images, in which the joint modeling of lighting and object textures is implicit and inadequate. We observe that in a shadow region, the degradation degree of object textures depends on the local illumination, while simply enhancing the local illumination cannot fully recover the attenuated textures. Based on this observation, we propose to condition the restoration of attenuated textures on the corrected local lighting in the shadow region. Specifically, We first design a shadow-aware decomposition network to estimate the illumination and reflectance layers of shadow regions explicitly. We then propose a novel bilateral correction network to recast the lighting of shadow regions in the illumination layer via a novel local lighting correction module, and to restore the textures conditioned on the corrected illumination layer via a novel illumination-guided texture restoration module. We further annotate pixel-wise shadow masks for the public SRD dataset, which originally contains only image pairs. Experiments on three benchmarks show that our method outperforms existing state-of-the-art shadow removal methods.|\u6d88\u9664\u9634\u5f71\u9700\u8981\u4e86\u89e3\u573a\u666f\u4e2d\u7684\u7167\u660e\u6761\u4ef6\u548c\u5bf9\u8c61\u7eb9\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b66\u4e60\u9634\u5f71\u548c\u975e\u9634\u5f71\u56fe\u50cf\u4e4b\u95f4\u7684\u50cf\u7d20\u7ea7\u989c\u8272\u6620\u5c04\uff0c\u5176\u4e2d\u7167\u660e\u548c\u5bf9\u8c61\u7eb9\u7406\u7684\u8054\u5408\u5efa\u6a21\u662f\u9690\u5f0f\u4e14\u4e0d\u5145\u5206\u7684\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u9634\u5f71\u533a\u57df\uff0c\u7269\u4f53\u7eb9\u7406\u7684\u9000\u5316\u7a0b\u5ea6\u53d6\u51b3\u4e8e\u5c40\u90e8\u7167\u660e\uff0c\u800c\u7b80\u5355\u5730\u589e\u5f3a\u5c40\u90e8\u7167\u660e\u5e76\u4e0d\u80fd\u5b8c\u5168\u6062\u590d\u8870\u51cf\u7684\u7eb9\u7406\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u5efa\u8bae\u4ee5\u9634\u5f71\u533a\u57df\u4e2d\u6821\u6b63\u7684\u5c40\u90e8\u7167\u660e\u4e3a\u6761\u4ef6\u6765\u6062\u590d\u8870\u51cf\u7eb9\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e00\u4e2a\u9634\u5f71\u611f\u77e5\u5206\u89e3\u7f51\u7edc\u6765\u660e\u786e\u4f30\u8ba1\u9634\u5f71\u533a\u57df\u7684\u7167\u660e\u548c\u53cd\u5c04\u5c42\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u8fb9\u6821\u6b63\u7f51\u7edc\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u5c40\u90e8\u7167\u660e\u6821\u6b63\u6a21\u5757\u6765\u91cd\u65b0\u6295\u5c04\u7167\u660e\u5c42\u4e2d\u9634\u5f71\u533a\u57df\u7684\u7167\u660e\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u7167\u660e\u5f15\u5bfc\u7eb9\u7406\u6062\u590d\u6a21\u5757\u6765\u6062\u590d\u4ee5\u6821\u6b63\u540e\u7684\u7167\u660e\u5c42\u4e3a\u6761\u4ef6\u7684\u7eb9\u7406\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u6ce8\u91ca\u516c\u5171 SRD \u6570\u636e\u96c6\u7684\u50cf\u7d20\u7ea7\u9634\u5f71\u63a9\u6a21\uff0c\u8be5\u6570\u636e\u96c6\u6700\u521d\u4ec5\u5305\u542b\u56fe\u50cf\u5bf9\u3002\u4e09\u4e2a\u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9634\u5f71\u53bb\u9664\u65b9\u6cd5\u3002|[2402.00341v1](http://arxiv.org/pdf/2402.00341v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.00672": "|**2024-02-01**|**Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID**|\u63a2\u7d22\u65e0\u76d1\u7763\u53ef\u89c1\u7ea2\u5916\u884c\u4eba\u518d\u8bc6\u522b\u7684\u540c\u8d28\u548c\u5f02\u8d28\u4e00\u81f4\u6807\u7b7e\u5173\u8054|Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao|Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework. Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. The code will be available.|\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\uff08USL-VI-ReID\uff09\u65e8\u5728\u4ece\u4e0d\u540c\u65b9\u5f0f\u68c0\u7d22\u76f8\u540c\u8eab\u4efd\u7684\u884c\u4eba\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u6ce8\u91ca\u3002\u867d\u7136\u5148\u524d\u7684\u5de5\u4f5c\u4fa7\u91cd\u4e8e\u5efa\u7acb\u8de8\u6a21\u6001\u4f2a\u6807\u7b7e\u5173\u8054\u6765\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\uff0c\u4f46\u4ed6\u4eec\u5ffd\u7565\u4e86\u5728\u4f2a\u6807\u7b7e\u7a7a\u95f4\u4e2d\u7ef4\u62a4\u5b9e\u4f8b\u7ea7\u540c\u8d28\u548c\u5f02\u6784\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u7c97\u5173\u8054\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6a21\u6001\u7edf\u4e00\u6807\u7b7e\u4f20\u8f93\uff08MULT\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u540c\u65f6\u8003\u8651\u540c\u8d28\u548c\u5f02\u6784\u7ec6\u7c92\u5ea6\u5b9e\u4f8b\u7ea7\u7ed3\u6784\uff0c\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8de8\u6a21\u6001\u6807\u7b7e\u5173\u8054\u3002\u5b83\u5bf9\u540c\u8d28\u548c\u5f02\u8d28\u4eb2\u548c\u529b\u8fdb\u884c\u5efa\u6a21\uff0c\u5229\u7528\u5b83\u4eec\u6765\u5b9a\u4e49\u4f2a\u6807\u7b7e\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u7136\u540e\u5c06\u5176\u6700\u5c0f\u5316\uff0c\u4ece\u800c\u5bfc\u81f4\u4f2a\u6807\u7b7e\u4fdd\u6301\u8de8\u6a21\u6001\u7684\u5bf9\u9f50\u548c\u6a21\u6001\u5185\u7ed3\u6784\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5373\u63d2\u5373\u7528\u5728\u7ebf\u8de8\u5185\u5b58\u6807\u7b7e\u7ec6\u5316\uff08OCLR\uff09\u6a21\u5757\uff0c\u4ee5\u8fdb\u4e00\u6b65\u51cf\u8f7b\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u5bf9\u9f50\u4e0d\u540c\u7684\u6a21\u6001\uff0c\u5e76\u7ed3\u5408\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\uff08MIRL\uff09\u6846\u67b6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684 USL-VI-ReID \u65b9\u6cd5\uff0c\u51f8\u663e\u4e86\u6211\u4eec\u7684 MULT \u4e0e\u5176\u4ed6\u8de8\u6a21\u6001\u5173\u8054\u65b9\u6cd5\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u3002\u8be5\u4ee3\u7801\u5c06\u53ef\u7528\u3002|[2402.00672v1](http://arxiv.org/pdf/2402.00672v1)|null|\n", "2402.00608": "|**2024-02-01**|**Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters**|\u4f7f\u7528 Soft Silhouette \u5206\u6570\u8fdb\u884c\u6df1\u5ea6\u805a\u7c7b\uff1a\u5b9e\u73b0\u7d27\u51d1\u4e14\u5206\u79bb\u826f\u597d\u7684\u805a\u7c7b|Georgios Vardakas, Ioannis Papakostas, Aristidis Likas|Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.|\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u5927\u6570\u636e\u65f6\u4ee3\u5f97\u5230\u4e86\u91cd\u89c6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u672a\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u7684\u65b9\u6cd5\u3002\u6df1\u5ea6\u805a\u7c7b\u5df2\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u65e0\u76d1\u7763\u7c7b\u522b\uff0c\u65e8\u5728\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u6620\u5c04\u80fd\u529b\u6765\u589e\u5f3a\u805a\u7c7b\u6027\u80fd\u3002\u5927\u591a\u6570\u6df1\u5ea6\u805a\u7c7b\u6587\u732e\u4fa7\u91cd\u4e8e\u6700\u5c0f\u5316\u67d0\u4e9b\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7c07\u5185\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u7684\u8868\u793a\u4e0e\u539f\u59cb\u9ad8\u7ef4\u6570\u636e\u96c6\u4e00\u81f4\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 soft silhoutte\uff0c\u4e00\u79cd\u8f6e\u5ed3\u7cfb\u6570\u7684\u6982\u7387\u516c\u5f0f\u3002\u8f6f\u8f6e\u5ed3\u5956\u52b1\u7d27\u51d1\u4e14\u660e\u663e\u5206\u79bb\u7684\u805a\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u4f20\u7edf\u7684\u8f6e\u5ed3\u7cfb\u6570\u3002\u5f53\u5728\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\u5185\u8fdb\u884c\u4f18\u5316\u65f6\uff0c\u8f6f\u8f6e\u5ed3\u4f1a\u5f15\u5bfc\u5b66\u4e60\u5230\u7684\u8868\u793a\u5f62\u6210\u7d27\u51d1\u4e14\u5206\u79bb\u826f\u597d\u7684\u805a\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u4f18\u5316\u8f6f\u8f6e\u5ed3\u76ee\u6807\u51fd\u6570\u3002\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u5df2\u7ecf\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u51e0\u79cd\u7ecf\u8fc7\u5145\u5206\u7814\u7a76\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ea7\u751f\u4e86\u975e\u5e38\u4ee4\u4eba\u6ee1\u610f\u7684\u805a\u7c7b\u7ed3\u679c\u3002|[2402.00608v1](http://arxiv.org/pdf/2402.00608v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2402.00847": "|**2024-02-01**|**BootsTAP: Bootstrapped Training for Tracking-Any-Point**|BootsTAP\uff1a\u7528\u4e8e\u8ddf\u8e2a\u4efb\u610f\u70b9\u7684\u5f15\u5bfc\u8bad\u7ec3|Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, Jo\u00e3o Carreira, Andrew Zisserman|To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.|\u4e3a\u4e86\u4f7f\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u7269\u7406\u548c\u8fd0\u52a8\uff0c\u8ba9\u5b83\u4eec\u611f\u77e5\u771f\u5b9e\u573a\u666f\u4e2d\u56fa\u4f53\u8868\u9762\u5982\u4f55\u79fb\u52a8\u548c\u53d8\u5f62\u662f\u5f88\u6709\u7528\u7684\u3002\u8fd9\u53ef\u4ee5\u5f62\u5f0f\u5316\u4e3a\u8ddf\u8e2a\u4efb\u610f\u70b9\uff08TAP\uff09\uff0c\u5b83\u8981\u6c42\u7b97\u6cd5\u80fd\u591f\u8ddf\u8e2a\u89c6\u9891\u4e2d\u4e0e\u56fa\u4f53\u8868\u9762\u76f8\u5bf9\u5e94\u7684\u4efb\u4f55\u70b9\uff0c\u53ef\u80fd\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u90fd\u5f88\u5bc6\u96c6\u3002 TAP \u7684\u5927\u89c4\u6a21\u5730\u9762\u5b9e\u51b5\u8bad\u7ec3\u6570\u636e\u4ec5\u5728\u6a21\u62df\u4e2d\u53ef\u7528\uff0c\u76ee\u524d\u6a21\u62df\u7684\u5bf9\u8c61\u548c\u8fd0\u52a8\u79cd\u7c7b\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u81ea\u6211\u76d1\u7763\u7684\u5b66\u751f-\u6559\u5e08\u8bbe\u7f6e\uff0c\u4ee5\u6700\u5c0f\u7684\u67b6\u6784\u66f4\u6539\u6765\u6539\u8fdb\u5927\u89c4\u6a21\u3001\u672a\u6807\u8bb0\u3001\u672a\u6574\u7406\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u3002\u6211\u4eec\u5728 TAP-Vid \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5927\u5e45\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u7ed3\u679c\uff1a\u4f8b\u5982\uff0cTAP-Vid-DAVIS \u6027\u80fd\u4ece 61.3% \u63d0\u9ad8\u5230 66.4%\uff0cTAP-Vid-Kinetics \u4ece 57.2% \u63d0\u9ad8\u81f3 61.5%\u3002|[2402.00847v1](http://arxiv.org/pdf/2402.00847v1)|null|\n", "2402.00712": "|**2024-02-01**|**ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction**|ChaosBench\uff1a\u57fa\u4e8e\u7269\u7406\u7684\u591a\u901a\u9053\u6b21\u5b63\u8282\u5230\u5b63\u8282\u6c14\u5019\u9884\u6d4b\u57fa\u51c6|Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke, Aditya Grover, Pierre Gentine|Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.|\u51c6\u786e\u9884\u6d4b\u6b21\u5b63\u8282\u5230\u5b63\u8282\u5c3a\u5ea6\u7684\u6c14\u5019\u5bf9\u4e8e\u9632\u707e\u3001\u964d\u4f4e\u7ecf\u6d4e\u98ce\u9669\u548c\u6539\u8fdb\u6c14\u5019\u53d8\u5316\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7cfb\u7edf\u7684\u6df7\u6c8c\u6027\u8d28\uff0cS2S \u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u76ee\u524d\uff0c\u73b0\u6709\u7684\u5929\u6c14\u548c\u6c14\u5019\u5e94\u7528\u57fa\u51c6\u5f80\u5f80 (1) \u9884\u6d4b\u8303\u56f4\u8f83\u77ed\uff0c\u6700\u591a 14 \u5929\uff0c(2) \u4e0d\u5305\u62ec\u5e7f\u6cdb\u7684\u4e1a\u52a1\u57fa\u7ebf\u9884\u6d4b\uff0c(3) \u7f3a\u4e4f\u57fa\u4e8e\u7269\u7406\u7684\u7ea6\u675f\u4e3a\u4e86\u4fbf\u4e8e\u89e3\u91ca\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ChaosBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u901a\u9053\u3001\u57fa\u4e8e\u7269\u7406\u7684 S2S \u9884\u6d4b\u57fa\u51c6\u3002 ChaosBench \u62e5\u6709\u8d85\u8fc7 46 \u4e07\u5e27\u7684\u73b0\u5b9e\u4e16\u754c\u89c2\u6d4b\u548c\u6a21\u62df\uff0c\u6bcf\u4e2a\u5e27\u6709 60 \u4e2a\u53ef\u53d8\u901a\u9053\uff0c\u8de8\u5ea6\u957f\u8fbe 45 \u5e74\u3002\u9664\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u6307\u6807\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u51e0\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u6307\u6807\uff0c\u4ee5\u5b9e\u73b0\u7269\u7406\u4e0a\u66f4\u52a0\u4e00\u81f4\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7eb3\u5165\u4e86\u6765\u81ea 4 \u4e2a\u56fd\u5bb6\u6c14\u8c61\u673a\u6784\u7684\u4e00\u7cfb\u5217\u57fa\u4e8e\u7269\u7406\u7684\u9884\u6d4b\uff0c\u4f5c\u4e3a\u6211\u4eec\u7684\u6570\u636e\u9a71\u52a8\u5bf9\u5e94\u9884\u6d4b\u7684\u57fa\u7ebf\u3002\u6211\u4eec\u5efa\u7acb\u4e86\u4e24\u4e2a\u590d\u6742\u7a0b\u5ea6\u4e0d\u540c\u7684\u4efb\u52a1\uff1a\u5b8c\u6574\u52a8\u6001\u9884\u6d4b\u548c\u7a00\u758f\u52a8\u6001\u9884\u6d4b\u3002\u6211\u4eec\u7684\u57fa\u51c6\u662f\u7b2c\u4e00\u4e2a\u5bf9\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ec PanguWeather\u3001FourCastNetV2\u3001GraphCast \u548c ClimaX\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u7684\u57fa\u51c6\u4e4b\u4e00\uff0c\u5e76\u53d1\u73b0\u6700\u521d\u4e3a\u5929\u6c14\u89c4\u6a21\u5e94\u7528\u5f00\u53d1\u7684\u65b9\u6cd5\u5728 S2S \u4efb\u52a1\u4e0a\u5931\u8d25\u3002\u6211\u4eec\u5728 https://leap-stc.github.io/ChaosBench \u53d1\u5e03\u4e86\u57fa\u51c6\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002|[2402.00712v1](http://arxiv.org/pdf/2402.00712v1)|null|\n", "2402.00676": "|**2024-02-01**|**Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching**|\u6df1\u5ea6\u673a\u5668\u4eba\u7d20\u63cf\uff1a\u6df1\u5ea6 Q \u5b66\u4e60\u7f51\u7edc\u5728\u7c7b\u4eba\u7d20\u63cf\u4e2d\u7684\u5e94\u7528|Raul Fernandez-Fernandez, Juan G. Victores, Carlos Balaguer|The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs|\u76ee\u524d\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u6240\u53d6\u5f97\u7684\u6210\u529f\u542f\u53d1\u4e86\u8bb8\u591a\u8ba4\u77e5\u79d1\u5b66\u7684\u6700\u65b0\u7406\u8bba\u65b9\u6cd5\u3002\u827a\u672f\u73af\u5883\u5728\u8ba4\u77e5\u79d1\u5b66\u754c\u88ab\u7814\u7a76\u4e3a\u4e30\u5bcc\u7684\u3001\u81ea\u7136\u7684\u3001\u591a\u611f\u5b98\u7684\u3001\u591a\u6587\u5316\u7684\u73af\u200b\u200b\u5883\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u5584\u827a\u672f\u673a\u5668\u4eba\u5e94\u7528\u7684\u63a7\u5236\u3002\u6df1\u5ea6 Q \u5b66\u4e60\u795e\u7ecf\u7f51\u7edc (DQN) \u662f\u5728\u673a\u5668\u4eba\u9886\u57df\u5b9e\u65bd\u5f3a\u5316\u5b66\u4e60\u6700\u6210\u529f\u7684\u7b97\u6cd5\u4e4b\u4e00\u3002 DQN \u65b9\u6cd5\u751f\u6210\u590d\u6742\u7684\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u5404\u79cd\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u7684\u673a\u5668\u4eba\u5e94\u7528\u7a0b\u5e8f\u3002\u5f53\u524d\u7684\u827a\u672f\u7ed8\u753b\u673a\u5668\u4eba\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u7b80\u5355\u7684\u63a7\u5236\u6cd5\u5219\uff0c\u9650\u5236\u4e86\u6846\u67b6\u5bf9\u4e00\u7ec4\u7b80\u5355\u73af\u5883\u7684\u9002\u5e94\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u5efa\u8bae\u5728\u827a\u672f\u7ed8\u753b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5f15\u5165 DQN\u3002\u76ee\u6807\u662f\u7814\u7a76\u590d\u6742\u63a7\u5236\u7b56\u7565\u7684\u5f15\u5165\u5982\u4f55\u5f71\u54cd\u57fa\u672c\u827a\u672f\u7ed8\u753b\u673a\u5668\u4eba\u5e94\u7528\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u4e3b\u8981\u9884\u671f\u8d21\u732e\u662f\u4f5c\u4e3a\u672a\u6765\u4e3a\u590d\u6742\u827a\u672f\u7ed8\u753b\u673a\u5668\u4eba\u6846\u67b6\u5f15\u5165 DQN \u65b9\u6cd5\u7684\u5de5\u4f5c\u7684\u7b2c\u4e00\u4e2a\u57fa\u7ebf\u3002\u5b9e\u9a8c\u5305\u62ec\u4f7f\u7528 DQN \u751f\u6210\u7684\u7b56\u7565\u548c TEO\uff08\u4eba\u5f62\u673a\u5668\u4eba\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4eba\u7c7b\u7ed8\u5236\u7684\u8349\u56fe\u3002\u6bd4\u8f83\u7ed3\u679c\u7684\u76f8\u4f3c\u6027\u5e76\u83b7\u5f97\u76f8\u5bf9\u4e8e\u53c2\u8003\u8f93\u5165\u7684\u5956\u52b1|[2402.00676v1](http://arxiv.org/pdf/2402.00676v1)|null|\n", "2402.00576": "|**2024-02-01**|**Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks**|\u795e\u7ecf\u7f51\u7edc\u7684\u70ed\u5e26\u51b3\u7b56\u8fb9\u754c\u5bf9\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027|Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, Jefferson Huang|We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.|\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6613\u4e8e\u5b9e\u73b0\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u70ed\u5e26\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5bf9\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u70ed\u5e26\u6295\u5f71\u73af\u4e2d\u7684\u6570\u636e\u5d4c\u5165\u5230\u5355\u4e2a\u9690\u85cf\u5c42\u4e2d\u6765\u5229\u7528\u5206\u6bb5\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u7684\u70ed\u5e26\u6027\u8d28\uff0c\u8be5\u9690\u85cf\u5c42\u53ef\u4ee5\u6dfb\u52a0\u5230\u4efb\u4f55\u6a21\u578b\u4e2d\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5176\u51b3\u7b56\u8fb9\u754c\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u4f7f\u7528\u8ba1\u7b97\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u9488\u5bf9\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002|[2402.00576v1](http://arxiv.org/pdf/2402.00576v1)|null|\n", "2402.00418": "|**2024-02-01**|**Short: Benchmarking transferable adversarial attacks**|\u7b80\u77ed\uff1a\u53ef\u8f6c\u79fb\u5bf9\u6297\u6027\u653b\u51fb\u7684\u57fa\u51c6\u6d4b\u8bd5|Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen|The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \\textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintessential resource for both scholars and practitioners in the field, charting the complex terrain of adversarial transferability and setting a foundation for future explorations in this vital sector. The associated codebase is accessible at: https://github.com/KxPlaug/TAA-Bench|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6297\u5bf9\u6297\u6027\u653b\u51fb\u7684\u7a33\u5065\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u56de\u987e\u3002\u5b83\u7cfb\u7edf\u5730\u5206\u7c7b\u5e76\u4e25\u683c\u8bc4\u4f30\u4e86\u4e3a\u589e\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u800c\u5f00\u53d1\u7684\u5404\u79cd\u65b9\u6cd5\u3002\u8fd9\u9879\u7814\u7a76\u6db5\u76d6\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u751f\u6210\u7ed3\u6784\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u68af\u5ea6\u7f16\u8f91\u3001\u76ee\u6807\u4fee\u6539\u548c\u96c6\u6210\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\\textit{TAA-Bench}\uff0c\u96c6\u6210\u4e86\u5341\u79cd\u9886\u5148\u7684\u5bf9\u6297\u6027\u653b\u51fb\u53ef\u8f6c\u79fb\u6027\u65b9\u6cd5\uff0c\u4ece\u800c\u4e3a\u8de8\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u6bd4\u8f83\u5206\u6790\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u548c\u7cfb\u7edf\u5316\u7684\u5e73\u53f0\u3002\u901a\u8fc7\u5168\u9762\u7684\u5ba1\u67e5\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u529f\u6548\u548c\u5c40\u9650\u6027\uff0c\u9610\u660e\u4e86\u5b83\u4eec\u7684\u57fa\u672c\u64cd\u4f5c\u539f\u7406\u548c\u5b9e\u9645\u7528\u9014\u3002\u8fd9\u7bc7\u7efc\u8ff0\u81f4\u529b\u4e8e\u6210\u4e3a\u8be5\u9886\u57df\u5b66\u8005\u548c\u4ece\u4e1a\u8005\u7684\u5178\u578b\u8d44\u6e90\uff0c\u63cf\u7ed8\u51fa\u5bf9\u6297\u6027\u53ef\u8f6c\u79fb\u6027\u7684\u590d\u6742\u5730\u5f62\uff0c\u5e76\u4e3a\u8fd9\u4e00\u91cd\u8981\u9886\u57df\u7684\u672a\u6765\u63a2\u7d22\u5960\u5b9a\u57fa\u7840\u3002\u76f8\u5173\u7684\u4ee3\u7801\u5e93\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u8bbf\u95ee\uff1ahttps://github.com/KxPlaug/TAA-Bench|[2402.00418v1](http://arxiv.org/pdf/2402.00418v1)|**[link](https://github.com/kxplaug/taa-bench)**|\n", "2402.00411": "|**2024-02-01**|**LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model**|LM-HT SNN\uff1a\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u591a\u5c42\u6b21\u9608\u503c\u6a21\u578b\u589e\u5f3a SNN \u4e0e ANN \u5bf9\u5e94\u7269\u7684\u6027\u80fd|Zecheng Hao, Xinyu Shi, Zhiyu Pan, Yujia Liu, Zhaofei Yu, Tiejun Huang|Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion framework. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our LM-HT model can significantly outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs.|\u4e0e\u4f20\u7edf\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u76f8\u6bd4\uff0c\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u56e0\u5176\u4ee5\u66f4\u52a0\u53d7\u751f\u7269\u542f\u53d1\u548c\u8282\u80fd\u7684\u65b9\u5f0f\u4f20\u8f93\u4fe1\u606f\u7684\u5185\u5728\u80fd\u529b\u800c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u7684\u5b66\u672f\u5174\u8da3\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u4e4b\u524d\u4eba\u4eec\u901a\u8fc7\u5404\u79cd\u65b9\u6cd5\u52aa\u529b\u4f18\u5316 SNN \u7684\u5b66\u4e60\u68af\u5ea6\u548c\u6a21\u578b\u7ed3\u6784\uff0c\u4f46 SNN \u5728\u6027\u80fd\u65b9\u9762\u4ecd\u7136\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u843d\u540e\u4e8e ANN\u3002\u6700\u8fd1\u63d0\u51fa\u7684\u591a\u9608\u503c\u6a21\u578b\u4e3a\u8fdb\u4e00\u6b65\u589e\u5f3aSNN\u7684\u5b66\u4e60\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u591a\u53ef\u80fd\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u6570\u5b66\u89d2\u5ea6\u4e25\u683c\u5206\u6790\u4e86\u591a\u9608\u503c\u6a21\u578b\u3001\u666e\u901a\u5c16\u5cf0\u6a21\u578b\u548c\u91cf\u5316\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LM-HT\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u4ee5\u52a8\u6001\u8c03\u8282\u5168\u5c40\u7684\u7b49\u8ddd\u591a\u5c42\u6b21\u6a21\u578b\u3002\u8f93\u5165\u7535\u6d41\u548c\u819c\u7535\u4f4d\u6cc4\u6f0f\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u57fa\u4e8eLM-HT\u6a21\u578b\u7684\u76f4\u63a5\u8bad\u7ec3\u7b97\u6cd5\u53ef\u4ee5\u4e0e\u4f20\u7edf\u7684ANN-SNN\u8f6c\u6362\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u6df7\u5408\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u6539\u5584\u8f6c\u6362\u540e\u7684 SNN \u5728\u4f4e\u65f6\u5ef6\u4e0b\u76f8\u5bf9\u8f83\u5dee\u7684\u6027\u80fd\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 LM-HT \u6a21\u578b\u5728\u5404\u79cd\u7c7b\u578b\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u53ef\u4ee5\u663e\u7740\u4f18\u4e8e\u4ee5\u524d\u7684\u6700\u5148\u8fdb\u7684\u5de5\u4f5c\uff0c\u8fd9\u4fc3\u4f7f SNN \u8fbe\u5230\u4e0e\u91cf\u5316 ANN \u76f8\u5f53\u7684\u5168\u65b0\u6027\u80fd\u6c34\u5e73\u3002|[2402.00411v1](http://arxiv.org/pdf/2402.00411v1)|null|\n", "2402.00407": "|**2024-02-01**|**InfMAE: A Foundation Model in Infrared Modality**|InfMAE\uff1a\u7ea2\u5916\u6a21\u6001\u7684\u57fa\u7840\u6a21\u578b|Fangcen Liu, Chenqiang Gao, Yaming Zhang, Junjie Guo, Jinhao Wang, Deyu Meng|In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities. However, it remains an open question on how to design an infrared foundation model. In this paper, we propose InfMAE, a foundation model in infrared modality. We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community. Besides, we design an information-aware masking strategy, which is suitable for infrared images. This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation. In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks. Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks. Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks. Our code will be made public at https://github.com/liufangcen/InfMAE.|\u8fd1\u5e74\u6765\uff0c\u57fa\u7840\u6a21\u578b\u5e2d\u5377\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u4fc3\u8fdb\u4e86\u4e0d\u540c\u6a21\u5f0f\u4e0b\u5404\u79cd\u4efb\u52a1\u7684\u5f00\u53d1\u3002\u7136\u800c\uff0c\u5982\u4f55\u8bbe\u8ba1\u7ea2\u5916\u57fa\u7840\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 InfMAE\uff0c\u8fd9\u662f\u7ea2\u5916\u6a21\u6001\u7684\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3a Inf30 \u7684\u7ea2\u5916\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u7ea2\u5916\u89c6\u89c9\u793e\u533a\u7f3a\u4e4f\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u5b66\u4e60\u6570\u636e\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u7ea2\u5916\u56fe\u50cf\u7684\u4fe1\u606f\u611f\u77e5\u63a9\u853d\u7b56\u7565\u3002\u8fd9\u79cd\u63a9\u853d\u7b56\u7565\u5141\u8bb8\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u91cd\u89c6\u7ea2\u5916\u56fe\u50cf\u4e2d\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u533a\u57df\uff0c\u6709\u5229\u4e8e\u5b66\u4e60\u5e7f\u4e49\u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u57fa\u4e8e\u7ea2\u5916\u56fe\u50cf\u6ca1\u6709\u5927\u91cf\u7ec6\u8282\u548c\u7eb9\u7406\u4fe1\u606f\u7684\u4e8b\u5b9e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u7ea2\u5916\u89e3\u7801\u5668\u6a21\u5757\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5 InfMAE \u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u76d1\u7763\u65b9\u6cd5\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728https://github.com/liufangcen/InfMAE\u516c\u5f00\u3002|[2402.00407v1](http://arxiv.org/pdf/2402.00407v1)|null|\n", "2402.00319": "|**2024-02-01**|**SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling**|SCO-VIST\uff1a\u57fa\u4e8e\u793e\u4ea4\u4e92\u52a8\u5e38\u8bc6\u77e5\u8bc6\u7684\u89c6\u89c9\u53d9\u4e8b|Eileen Wang, Soyeon Caren Han, Josiah Poon|Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm. Our proposed framework produces stories superior across multiple metrics in terms of visual grounding, coherence, diversity, and humanness, per both automatic and human evaluations.|\u89c6\u89c9\u53d9\u4e8b\u65e8\u5728\u6839\u636e\u7ed9\u5b9a\u7684\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u751f\u6210\u8fde\u8d2f\u7684\u6545\u4e8b\u3002\u4e0e\u56fe\u50cf\u5b57\u5e55\u7b49\u4efb\u52a1\u4e0d\u540c\uff0c\u89c6\u89c9\u6545\u4e8b\u5e94\u8be5\u5305\u542b\u4e8b\u5b9e\u63cf\u8ff0\u3001\u4e16\u754c\u89c2\u548c\u4eba\u7c7b\u793e\u4f1a\u5e38\u8bc6\uff0c\u5c06\u8131\u8282\u7684\u5143\u7d20\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u5f62\u6210\u4e00\u4e2a\u8fde\u8d2f\u4e14\u5f15\u4eba\u5165\u80dc\u7684\u4eba\u7c7b\u53ef\u5199\u6545\u4e8b\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5c1d\u8bd5\u521b\u5efa\u6545\u4e8b\u65f6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5e94\u7528\u4e8b\u5b9e\u4fe1\u606f\u548c\u4f7f\u7528\u5206\u7c7b/\u8bcd\u6c47\u5916\u90e8\u77e5\u8bc6\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 SCO-VIST\uff0c\u4e00\u4e2a\u5c06\u56fe\u50cf\u5e8f\u5217\u8868\u793a\u4e3a\u5177\u6709\u5bf9\u8c61\u548c\u5173\u7cfb\u7684\u56fe\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u62ec\u4eba\u7c7b\u884c\u4e3a\u52a8\u673a\u53ca\u5176\u793e\u4f1a\u4e92\u52a8\u5e38\u8bc6\u77e5\u8bc6\u3002\u7136\u540e\uff0cSCO-VIST \u91c7\u7528\u8868\u793a\u7ed8\u56fe\u70b9\u7684\u56fe\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u8bed\u4e49\u548c\u57fa\u4e8e\u51fa\u73b0\u7684\u8fb9\u7f18\u6743\u91cd\u5728\u7ed8\u56fe\u70b9\u4e4b\u95f4\u521b\u5efa\u6865\u6881\u3002\u8be5\u52a0\u6743\u6545\u4e8b\u56fe\u4f7f\u7528 Floyd-Warshall \u7b97\u6cd5\u751f\u6210\u4e00\u7cfb\u5217\u4e8b\u4ef6\u7684\u6545\u4e8b\u60c5\u8282\u3002\u6839\u636e\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5728\u89c6\u89c9\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u3001\u591a\u6837\u6027\u548c\u4eba\u6027\u65b9\u9762\u4ea7\u751f\u4e86\u8de8\u591a\u4e2a\u6307\u6807\u7684\u4f18\u79c0\u6545\u4e8b\u3002|[2402.00319v1](http://arxiv.org/pdf/2402.00319v1)|null|\n", "2402.00304": "|**2024-02-01**|**Invariance-powered Trustworthy Defense via Remove Then Restore**|\u901a\u8fc7\u5220\u9664\u7136\u540e\u6062\u590d\u5b9e\u73b0\u4e0d\u53d8\u6027\u7684\u53ef\u4fe1\u9632\u5fa1|Xiaowei Fu, Yuhang Zhou, Lina Ma, Lei Zhang|Adversarial attacks pose a challenge to the deployment of deep neural networks (DNNs), while previous defense models overlook the generalization to various attacks. Inspired by targeted therapies for cancer, we view adversarial samples as local lesions of natural benign samples, because a key finding is that salient attack in an adversarial sample dominates the attacking process, while trivial attack unexpectedly provides trustworthy evidence for obtaining generalizable robustness. Based on this finding, a Pixel Surgery and Semantic Regeneration (PSSR) model following the targeted therapy mechanism is developed, which has three merits: 1) To remove the salient attack, a score-based Pixel Surgery module is proposed, which retains the trivial attack as a kind of invariance information. 2) To restore the discriminative content, a Semantic Regeneration module based on a conditional alignment extrapolator is proposed, which achieves pixel and semantic consistency. 3) To further harmonize robustness and accuracy, an intractable problem, a self-augmentation regularizer with adversarial R-drop is designed. Experiments on numerous benchmarks show the superiority of PSSR.|\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u90e8\u7f72\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u800c\u4ee5\u524d\u7684\u9632\u5fa1\u6a21\u578b\u5ffd\u89c6\u4e86\u5bf9\u5404\u79cd\u653b\u51fb\u7684\u6cdb\u5316\u3002\u53d7\u764c\u75c7\u9776\u5411\u6cbb\u7597\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5c06\u5bf9\u6297\u6027\u6837\u672c\u89c6\u4e3a\u81ea\u7136\u826f\u6027\u6837\u672c\u7684\u5c40\u90e8\u75c5\u53d8\uff0c\u56e0\u4e3a\u4e00\u4e2a\u5173\u952e\u53d1\u73b0\u662f\u5bf9\u6297\u6027\u6837\u672c\u4e2d\u7684\u663e\u7740\u653b\u51fb\u4e3b\u5bfc\u4e86\u653b\u51fb\u8fc7\u7a0b\uff0c\u800c\u7410\u788e\u653b\u51fb\u610f\u5916\u5730\u4e3a\u83b7\u5f97\u666e\u904d\u7a33\u5065\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u5f00\u53d1\u4e86\u9075\u5faa\u9776\u5411\u6cbb\u7597\u673a\u5236\u7684\u50cf\u7d20\u624b\u672f\u548c\u8bed\u4e49\u518d\u751f\uff08PSSR\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u4e09\u4e2a\u4f18\u70b9\uff1a1\uff09\u4e3a\u4e86\u6d88\u9664\u663e\u7740\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bc4\u5206\u7684\u50cf\u7d20\u624b\u672f\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4fdd\u7559\u4e86\u7410\u788e\u7684\u653b\u51fb\u653b\u51fb\u4f5c\u4e3a\u4e00\u79cd\u4e0d\u53d8\u6027\u4fe1\u606f\u3002 2\uff09\u4e3a\u4e86\u6062\u590d\u6709\u5224\u522b\u6027\u7684\u5185\u5bb9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5bf9\u9f50\u5916\u63a8\u5668\u7684\u8bed\u4e49\u518d\u751f\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u50cf\u7d20\u548c\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002 3\uff09\u4e3a\u4e86\u8fdb\u4e00\u6b65\u534f\u8c03\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u8fd9\u4e00\u68d8\u624b\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u5bf9\u6297\u6027 R-drop \u7684\u81ea\u589e\u5f3a\u6b63\u5219\u5316\u5668\u3002\u4f17\u591a\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u663e\u793a\u4e86 PSSR \u7684\u4f18\u8d8a\u6027\u3002|[2402.00304v1](http://arxiv.org/pdf/2402.00304v1)|null|\n", "2402.00261": "|**2024-02-01**|**Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps**|\u4e86\u89e3\u4f7f\u7528\u5411\u91cf\u7a7a\u95f4\u548c\u9006\u6620\u5c04\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u7684\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf|Rebecca Pattichis, Marios S. Pattichis|There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.|\u4eba\u4eec\u5bf9\u5f00\u53d1\u53ef\u7528\u4e8e\u7406\u89e3\u56fe\u50cf\u5206\u6790\u4e2d\u4f7f\u7528\u7684\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b66\u65b9\u6cd5\u6709\u7740\u6d53\u539a\u7684\u5174\u8da3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u7ebf\u6027\u4ee3\u6570\u6280\u672f\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5c42\u5efa\u6a21\u4e3a\u4fe1\u53f7\u7a7a\u95f4\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u9996\u5148\uff0c\u6211\u4eec\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u4fe1\u53f7\u7a7a\u95f4\u6765\u53ef\u89c6\u5316\u6743\u91cd\u7a7a\u95f4\u548c\u5377\u79ef\u5c42\u5185\u6838\u3002\u6211\u4eec\u8fd8\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u7a7a\u95f4\u6765\u8fdb\u4e00\u6b65\u53ef\u89c6\u5316\u6bcf\u5c42\u4e22\u5931\u7684\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u6211\u4eec\u4ecb\u7ecd\u53ef\u9006\u7f51\u7edc\u7684\u6982\u5ff5\u4ee5\u53ca\u7528\u4e8e\u8ba1\u7b97\u4ea7\u751f\u7279\u5b9a\u8f93\u51fa\u7684\u8f93\u5165\u56fe\u50cf\u7684\u7b97\u6cd5\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u53ef\u9006\u7f51\u7edc\u548c ResNet18 \u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002|[2402.00261v1](http://arxiv.org/pdf/2402.00261v1)|null|\n"}}