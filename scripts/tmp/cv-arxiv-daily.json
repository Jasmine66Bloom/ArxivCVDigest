{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.16424": "|**2024-01-29**|**Computer Vision for Primate Behavior Analysis in the Wild**|\u7528\u4e8e\u91ce\u5916\u7075\u957f\u7c7b\u52a8\u7269\u884c\u4e3a\u5206\u6790\u7684\u8ba1\u7b97\u673a\u89c6\u89c9|Richard Vogg, Timo L\u00fcddecke, Jonathan Henrich, Sharmita Dey, Matthias Nuske, Valentin Hassler, Derek Murphy, Julia Fischer, Julia Ostner, Oliver Sch\u00fclke, et.al.|Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.|\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8fdb\u6b65\u4ee5\u53ca\u65e5\u76ca\u5e7f\u6cdb\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u884c\u4e3a\u76d1\u6d4b\u5bf9\u4e8e\u6539\u53d8\u6211\u4eec\u7814\u7a76\u52a8\u7269\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u65b9\u5f0f\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u4ee4\u4eba\u5174\u594b\u7684\u524d\u666f\u4e0e\u4eca\u5929\u5728\u5b9e\u8df5\u4e2d\u5b9e\u9645\u53ef\u4ee5\u5b9e\u73b0\u7684\u76ee\u6807\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u76f8\u5f53\u5927\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u6765\u81ea\u91ce\u5916\u7684\u89c6\u9891\u4e2d\u3002\u901a\u8fc7\u8fd9\u7bc7\u524d\u77bb\u6027\u8bba\u6587\uff0c\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u6307\u5bfc\u884c\u4e3a\u79d1\u5b66\u5bb6\u4e86\u89e3\u5f53\u524d\u65b9\u6cd5\u7684\u9884\u671f\uff0c\u5e76\u5f15\u5bfc\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4eba\u5458\u89e3\u51b3\u4e0e\u52a8\u7269\u884c\u4e3a\u9ad8\u7ea7\u7814\u7a76\u76f8\u5173\u7684\u95ee\u9898\uff0c\u4ece\u800c\u4e3a\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u505a\u51fa\u8d21\u732e\u3002\u6211\u4eec\u9996\u5148\u8c03\u67e5\u4e0e\u57fa\u4e8e\u89c6\u9891\u7684\u52a8\u7269\u884c\u4e3a\u7814\u7a76\u76f4\u63a5\u76f8\u5173\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u591a\u4e2a\u4f53\u8ddf\u8e2a\u3001\uff08\u95f4\uff09\u52a8\u4f5c\u8bc6\u522b\u548c\u4e2a\u4f53\u8bc6\u522b\u3002\u7136\u540e\uff0c\u6211\u4eec\u56de\u987e\u4e86\u9ad8\u6548\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ece\u5b9e\u8df5\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u662f\u6700\u5927\u7684\u6311\u6218\u4e4b\u4e00\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u65b0\u5174\u7684\u52a8\u7269\u884c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u672a\u6765\u8fdb\u884c\u4e86\u5c55\u671b\uff0c\u6211\u4eec\u8ba4\u4e3a\u8be5\u9886\u57df\u5e94\u8be5\u5feb\u901f\u8d85\u8d8a\u5e38\u89c1\u7684\u9010\u5e27\u5904\u7406\uff0c\u5e76\u5c06\u89c6\u9891\u89c6\u4e3a\u4e00\u7b49\u516c\u6c11\u3002|[2401.16424v1](http://arxiv.org/pdf/2401.16424v1)|null|\n", "2401.16423": "|**2024-01-29**|**Synchformer: Efficient Synchronization from Sparse Cues**|Synchformer\uff1a\u7a00\u758f\u7ebf\u7d22\u7684\u9ad8\u6548\u540c\u6b65|Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman|Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse. Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training. This approach achieves state-of-the-art performance in both dense and sparse settings. We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability.|\u6211\u4eec\u7684\u76ee\u6807\u662f\u89c6\u542c\u540c\u6b65\uff0c\u91cd\u70b9\u5173\u6ce8\u201c\u91ce\u5916\u201d\u89c6\u9891\uff0c\u4f8b\u5982 YouTube \u4e0a\u7684\u89c6\u9891\uff0c\u5176\u4e2d\u540c\u6b65\u7ebf\u7d22\u53ef\u80fd\u5f88\u5c11\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u542c\u540c\u6b65\u6a21\u578b\uff0c\u4ee5\u53ca\u901a\u8fc7\u591a\u6a21\u6001\u5206\u6bb5\u7ea7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u5c06\u7279\u5f81\u63d0\u53d6\u4e0e\u540c\u6b65\u5efa\u6a21\u5206\u79bb\u7684\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u8bbe\u7f6e\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c06\u540c\u6b65\u6a21\u578b\u8bad\u7ec3\u6269\u5c55\u5230\u767e\u4e07\u7ea7\u201c\u91ce\u5916\u201d\u6570\u636e\u96c6 AudioSet\uff0c\u7814\u7a76\u53ef\u89e3\u91ca\u6027\u7684\u8bc1\u636e\u5f52\u56e0\u6280\u672f\uff0c\u5e76\u63a2\u7d22\u540c\u6b65\u6a21\u578b\u7684\u65b0\u529f\u80fd\uff1a\u89c6\u542c\u540c\u6b65\u6027\u3002|[2401.16423v1](http://arxiv.org/pdf/2401.16423v1)|null|\n", "2401.16402": "|**2024-01-29**|**A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect**|\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u8c03\u67e5\uff1a\u6311\u6218\u3001\u65b9\u6cd5\u548c\u524d\u666f|Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, Guansong Pang, Weiming Shen|Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection. This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies. Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy. Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey.|\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u81f4\u529b\u4e8e\u67e5\u660e\u89c6\u89c9\u6570\u636e\u4e2d\u4e0e\u6b63\u5e38\u6027\u6982\u5ff5\u7684\u504f\u5dee\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u9886\u57df\uff0c\u4f8b\u5982\u5de5\u4e1a\u7f3a\u9677\u68c0\u67e5\u548c\u533b\u5b66\u75c5\u53d8\u68c0\u6d4b\u3002\u8fd9\u9879\u8c03\u67e5\u901a\u8fc7\u786e\u5b9a\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5168\u9762\u5ba1\u67e5\u4e86 VAD \u7684\u6700\u65b0\u8fdb\u5c55\uff1a1\uff09\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\uff0c2\uff09\u89c6\u89c9\u6a21\u5f0f\u7684\u591a\u6837\u6027\uff0c\u4ee5\u53ca 3\uff09\u5c42\u6b21\u5f02\u5e38\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u4ece\u7b80\u8981\u6982\u8ff0VAD\u80cc\u666f\u53ca\u5176\u901a\u7528\u6982\u5ff5\u5b9a\u4e49\u5f00\u59cb\uff0c\u4ece\u6837\u672c\u6570\u91cf\u3001\u6570\u636e\u6a21\u6001\u548c\u5f02\u5e38\u5c42\u6b21\u7684\u89d2\u5ea6\u9010\u6b65\u5bf9VAD\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u5206\u7c7b\u3001\u5f3a\u8c03\u548c\u8ba8\u8bba\u3002\u901a\u8fc7\u5bf9VAD\u9886\u57df\u7684\u6df1\u5165\u5206\u6790\uff0c\u6211\u4eec\u6700\u7ec8\u603b\u7ed3\u4e86VAD\u7684\u672a\u6765\u53d1\u5c55\uff0c\u5e76\u603b\u7ed3\u4e86\u672c\u6b21\u8c03\u67e5\u7684\u4e3b\u8981\u53d1\u73b0\u548c\u8d21\u732e\u3002|[2401.16402v1](http://arxiv.org/pdf/2401.16402v1)|null|\n", "2401.16363": "|**2024-01-29**|**Evaluation of pseudo-healthy image reconstruction for anomaly detection with deep generative models: Application to brain FDG PET**|\u4f7f\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u4f2a\u5065\u5eb7\u56fe\u50cf\u91cd\u5efa\uff1a\u5728\u5927\u8111 FDG PET \u4e2d\u7684\u5e94\u7528|Ravi Hassanaly, Camille Brianceau, Ma\u00eblys Solal, Olivier Colliot, Ninon Burgos|Over the past years, pseudo-healthy reconstruction for unsupervised anomaly detection has gained in popularity. This approach has the great advantage of not requiring tedious pixel-wise data annotation and offers possibility to generalize to any kind of anomalies, including that corresponding to rare diseases. By training a deep generative model with only images from healthy subjects, the model will learn to reconstruct pseudo-healthy images. This pseudo-healthy reconstruction is then compared to the input to detect and localize anomalies. The evaluation of such methods often relies on a ground truth lesion mask that is available for test data, which may not exist depending on the application.   We propose an evaluation procedure based on the simulation of realistic abnormal images to validate pseudo-healthy reconstruction methods when no ground truth is available. This allows us to extensively test generative models on different kinds of anomalies and measuring their performance using the pair of normal and abnormal images corresponding to the same subject. It can be used as a preliminary automatic step to validate the capacity of a generative model to reconstruct pseudo-healthy images, before a more advanced validation step that would require clinician's expertise. We apply this framework to the reconstruction of 3D brain FDG PET using a convolutional variational autoencoder with the aim to detect as early as possible the neurodegeneration markers that are specific to dementia such as Alzheimer's disease.|\u5728\u8fc7\u53bb\u7684\u51e0\u5e74\u91cc\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u4f2a\u5065\u5eb7\u91cd\u5efa\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u4e0d\u9700\u8981\u7e41\u7410\u7684\u9010\u50cf\u7d20\u6570\u636e\u6ce8\u91ca\u7684\u5de8\u5927\u4f18\u52bf\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u63a8\u5e7f\u5230\u4efb\u4f55\u7c7b\u578b\u5f02\u5e38\u7684\u53ef\u80fd\u6027\uff0c\u5305\u62ec\u4e0e\u7f55\u89c1\u75be\u75c5\u76f8\u5bf9\u5e94\u7684\u5f02\u5e38\u3002\u901a\u8fc7\u4ec5\u4f7f\u7528\u5065\u5eb7\u53d7\u8bd5\u8005\u7684\u56fe\u50cf\u8bad\u7ec3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u5b66\u4e60\u91cd\u5efa\u4f2a\u5065\u5eb7\u56fe\u50cf\u3002\u7136\u540e\u5c06\u8fd9\u79cd\u4f2a\u5065\u5eb7\u91cd\u5efa\u4e0e\u8f93\u5165\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5f02\u5e38\u3002\u6b64\u7c7b\u65b9\u6cd5\u7684\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u4e8e\u53ef\u7528\u4e8e\u6d4b\u8bd5\u6570\u636e\u7684\u5730\u9762\u771f\u5b9e\u75c5\u53d8\u63a9\u6a21\uff0c\u6839\u636e\u5e94\u7528\u60c5\u51b5\uff0c\u8be5\u63a9\u6a21\u53ef\u80fd\u4e0d\u5b58\u5728\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u5f02\u5e38\u56fe\u50cf\u6a21\u62df\u7684\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u4ee5\u5728\u6ca1\u6709\u771f\u5b9e\u6570\u636e\u53ef\u7528\u65f6\u9a8c\u8bc1\u4f2a\u5065\u5eb7\u91cd\u5efa\u65b9\u6cd5\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u5e7f\u6cdb\u6d4b\u8bd5\u4e0d\u540c\u7c7b\u578b\u5f02\u5e38\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e0e\u540c\u4e00\u5bf9\u8c61\u76f8\u5bf9\u5e94\u7684\u4e00\u5bf9\u6b63\u5e38\u548c\u5f02\u5e38\u56fe\u50cf\u6765\u6d4b\u91cf\u5176\u6027\u80fd\u3002\u5b83\u53ef\u4ee5\u7528\u4f5c\u521d\u6b65\u7684\u81ea\u52a8\u6b65\u9aa4\uff0c\u4ee5\u9a8c\u8bc1\u751f\u6210\u6a21\u578b\u91cd\u5efa\u4f2a\u5065\u5eb7\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u7136\u540e\u518d\u8fdb\u884c\u9700\u8981\u4e34\u5e8a\u533b\u751f\u4e13\u4e1a\u77e5\u8bc6\u7684\u66f4\u9ad8\u7ea7\u7684\u9a8c\u8bc1\u6b65\u9aa4\u3002\u6211\u4eec\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u4f7f\u7528\u5377\u79ef\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u91cd\u5efa 3D \u5927\u8111 FDG PET\uff0c\u76ee\u7684\u662f\u5c3d\u65e9\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b49\u75f4\u5446\u75c7\u7279\u6709\u7684\u795e\u7ecf\u9000\u884c\u6027\u6807\u8bb0\u7269\u3002|[2401.16363v1](http://arxiv.org/pdf/2401.16363v1)|null|\n", "2401.16305": "|**2024-01-29**|**MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection**|MixSup\uff1a\u57fa\u4e8e\u6807\u7b7e\u9ad8\u6548 LiDAR \u7684 3D \u7269\u4f53\u68c0\u6d4b\u7684\u6df7\u5408\u7c92\u5ea6\u76d1\u7763|Yuxue Yang, Lue Fan, Zhaoxiang Zhang|Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality. We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors. MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations. Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden. The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup.|\u57fa\u4e8e\u6807\u7b7e\u9ad8\u6548 LiDAR \u7684 3D \u7269\u4f53\u68c0\u6d4b\u76ee\u524d\u4ee5\u5f31/\u534a\u76d1\u7763\u65b9\u6cd5\u4e3a\u4e3b\u3002\u6211\u4eec\u63d0\u51fa\u4e86 MixSup\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u9075\u5faa\u5176\u4e2d\u4e00\u4e2a\uff0c\u8fd9\u662f\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u8303\u4f8b\uff0c\u540c\u65f6\u5229\u7528\u5927\u91cf\u5ec9\u4ef7\u7684\u7c97\u6807\u7b7e\u548c\u6709\u9650\u6570\u91cf\u7684\u7cbe\u786e\u6807\u7b7e\u8fdb\u884c\u6df7\u5408\u7c92\u5ea6\u76d1\u7763\u3002\u6211\u4eec\u9996\u5148\u89c2\u5bdf\u5230\u70b9\u4e91\u901a\u5e38\u662f\u65e0\u7eb9\u7406\u7684\uff0c\u8fd9\u4f7f\u5f97\u5b66\u4e60\u8bed\u4e49\u53d8\u5f97\u56f0\u96be\u3002\u7136\u800c\uff0c\u70b9\u4e91\u5177\u6709\u4e30\u5bcc\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u4e14\u5bf9\u4e8e\u8ddd\u4f20\u611f\u5668\u7684\u8ddd\u79bb\u5177\u6709\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u4f7f\u5f97\u5b66\u4e60\u7269\u4f53\u7684\u51e0\u4f55\u5f62\u72b6\uff08\u4f8b\u5982\u59ff\u52bf\u548c\u5f62\u72b6\uff09\u76f8\u5bf9\u5bb9\u6613\u3002\u56e0\u6b64\uff0cMixSup \u5229\u7528\u5927\u91cf\u7c97\u7565\u7684\u96c6\u7fa4\u7ea7\u6807\u7b7e\u6765\u5b66\u4e60\u8bed\u4e49\uff0c\u5e76\u5229\u7528\u4e00\u4e9b\u6602\u8d35\u7684\u6846\u7ea7\u6807\u7b7e\u6765\u5b66\u4e60\u51c6\u786e\u7684\u59ff\u52bf\u548c\u5f62\u72b6\u3002\u6211\u4eec\u91cd\u65b0\u8bbe\u8ba1\u4e86\u4e3b\u6d41\u68c0\u6d4b\u5668\u4e2d\u7684\u6807\u7b7e\u5206\u914d\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230 MixSup \u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u7528\u6027\u548c\u901a\u7528\u6027\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u68c0\u6d4b\u5668\u5728 nuScenes\u3001Waymo \u5f00\u653e\u6570\u636e\u96c6\u548c KITTI \u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002 MixSup \u4f7f\u7528\u5ec9\u4ef7\u7684\u96c6\u7fa4\u6ce8\u91ca\u548c\u4ec5 10% \u7684\u6846\u6ce8\u91ca\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 97.31% \u7684\u5b8c\u5168\u76d1\u7763\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e Segment Anything Model \u7684 PointSAM \u8fdb\u884c\u81ea\u52a8\u7c97\u6807\u8bb0\uff0c\u8fdb\u4e00\u6b65\u51cf\u8f7b\u6ce8\u91ca\u8d1f\u62c5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/BraveGroup/PointSAM-for-MixSup \u83b7\u53d6\u3002|[2401.16305v1](http://arxiv.org/pdf/2401.16305v1)|null|\n", "2401.16304": "|**2024-01-29**|**Regressing Transformers for Data-efficient Visual Place Recognition**|\u56de\u5f52\u53d8\u538b\u5668\u4ee5\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b|Mar\u00eda Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov|Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required. This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning. By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets.|\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5b9a\u4f4d\u548c\u5bfc\u822a\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bf9\u6bd4\u5b66\u4e60\uff1a\u56fe\u50cf\u63cf\u8ff0\u7b26\u88ab\u8bad\u7ec3\u4e3a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u4e8e\u76f8\u4f3c\u56fe\u50cf\u5177\u6709\u8f83\u5c0f\u7684\u8ddd\u79bb\uff0c\u5bf9\u4e8e\u4e0d\u76f8\u4f3c\u7684\u56fe\u50cf\u5177\u6709\u8f83\u5927\u7684\u8ddd\u79bb\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f88\u96be\u786e\u4fdd\u51c6\u786e\u7684\u57fa\u4e8e\u8ddd\u79bb\u7684\u56fe\u50cf\u76f8\u4f3c\u6027\u8868\u793a\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e8c\u8fdb\u5236\u6210\u5bf9\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u5e76\u4e14\u9700\u8981\u590d\u6742\u7684\u91cd\u65b0\u6392\u5e8f\u7b56\u7565\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5c06\u5730\u70b9\u8bc6\u522b\u89c6\u4e3a\u56de\u5f52\u95ee\u9898\uff0c\u4f7f\u7528\u76f8\u673a\u89c6\u573a\u91cd\u53e0\u4f5c\u4e3a\u5b66\u4e60\u7684\u76f8\u4f3c\u6027\u57fa\u7840\u4e8b\u5b9e\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u63cf\u8ff0\u7b26\u4ee5\u76f4\u63a5\u4e0e\u5206\u7ea7\u76f8\u4f3c\u6027\u6807\u7b7e\u5bf9\u9f50\uff0c\u8fd9\u79cd\u65b9\u6cd5\u589e\u5f3a\u4e86\u6392\u540d\u80fd\u529b\uff0c\u65e0\u9700\u6602\u8d35\u7684\u91cd\u65b0\u6392\u540d\uff0c\u63d0\u4f9b\u6570\u636e\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u8de8\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5f3a\u5927\u6cdb\u5316\u3002|[2401.16304v1](http://arxiv.org/pdf/2401.16304v1)|null|\n", "2401.16298": "|**2024-01-29**|**Breaking the Barrier: Selective Uncertainty-based Active Learning for Medical Image Segmentation**|\u6253\u7834\u969c\u788d\uff1a\u57fa\u4e8e\u9009\u62e9\u6027\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272|Siteng Ma, Haochang Wu, Aonghus Lawlor, Ruihai Dong|Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance. Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics. However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors. Moreover, uncertainty-based selection introduces redundancy. These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling. To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels. Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries. This resolves the aforementioned disregard for target areas and redundancy. Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance. Our code is available at https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL.|\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u65e8\u5728\u51cf\u8f7b\u6ce8\u91ca\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u6027\u80fd\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684 AL \u65b9\u6cd5\uff08\u4f8b\u5982\u71b5\u548c\u8d1d\u53f6\u65af\uff09\u901a\u5e38\u4f9d\u8d56\u4e8e\u6240\u6709\u50cf\u7d20\u7ea7\u6307\u6807\u7684\u805a\u5408\u3002\u7136\u800c\uff0c\u5728\u4e0d\u5e73\u8861\u7684\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u76ee\u6807\u533a\u57df\u7684\u91cd\u8981\u6027\uff0c\u4f8b\u5982\u75c5\u53d8\u548c\u80bf\u7624\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u9009\u62e9\u5f15\u5165\u4e86\u5197\u4f59\u3002\u8fd9\u4e9b\u56e0\u7d20\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u751a\u81f3\u4e0d\u5982\u968f\u673a\u91c7\u6837\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u57fa\u4e8e\u9009\u62e9\u6027\u4e0d\u786e\u5b9a\u6027\u7684 AL \u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5bf9\u6240\u6709\u50cf\u7d20\u7684\u5ea6\u91cf\u6c42\u548c\u7684\u4f20\u7edf\u505a\u6cd5\u3002\u901a\u8fc7\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u6211\u4eec\u7684\u7b56\u7565\u4f18\u5148\u8003\u8651\u76ee\u6807\u533a\u57df\u5185\u548c\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u50cf\u7d20\u3002\u8fd9\u89e3\u51b3\u4e86\u4e0a\u8ff0\u5bf9\u76ee\u6807\u533a\u57df\u548c\u5197\u4f59\u7684\u5ffd\u89c6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u79cd\u4e0d\u540c\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u548c\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u663e\u7740\u7684\u6539\u8fdb\uff0c\u5229\u7528\u66f4\u5c11\u7684\u6807\u8bb0\u6570\u636e\u8fbe\u5230\u76d1\u7763\u57fa\u7ebf\u5e76\u6301\u7eed\u5b9e\u73b0\u6700\u9ad8\u7684\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL\u3002|[2401.16298v1](http://arxiv.org/pdf/2401.16298v1)|null|\n", "2401.16280": "|**2024-01-29**|**Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model**|\u526a\u5207\u548c\u68c0\u6d4b\uff1a\u4f7f\u7528\u5927\u578b\u57fa\u7840\u89c6\u9891\u7406\u89e3\u6a21\u578b\u5bf9\u526a\u5207\u672a\u4fee\u526a\u89c6\u9891\u8fdb\u884c\u4eba\u4f53\u8dcc\u5012\u68c0\u6d4b|Till Grutschus, Ola Karrar, Emir Esenov, Ekta Vats|This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.|\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u5927\u578b\u89c6\u9891\u7406\u89e3\u57fa\u7840\u6a21\u578b\u5728\u672a\u4fee\u526a\u89c6\u9891\u4e0a\u4eba\u4f53\u8dcc\u5012\u68c0\u6d4b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8f6c\u6362\u5668\u8fdb\u884c\u591a\u7c7b\u52a8\u4f5c\u68c0\u6d4b\uff0c\u7c7b\u522b\u5305\u62ec\uff1a\u201c\u8dcc\u5012\u201d\u3001\u201c\u8bf4\u8c0e\u201d\u548c\u201c\u5176\u4ed6/\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff08ADL\uff09\u201d\u3002\u6f14\u793a\u4e86\u4e00\u79cd\u4f9d\u8d56\u4e8e\u672a\u4fee\u526a\u89c6\u9891\u7684\u7b80\u5355\u526a\u5207\u7684\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u9884\u5904\u7406\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5c06\u5e26\u6709\u65f6\u95f4\u6233\u52a8\u4f5c\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u77ed\u52a8\u4f5c\u526a\u8f91\u7684\u6807\u8bb0\u6570\u636e\u96c6\u3002\u4ecb\u7ecd\u4e86\u7b80\u5355\u6709\u6548\u7684\u526a\u8f91\u91c7\u6837\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5df2\u5728\u516c\u5f00\u7684\u9ad8\u8d28\u91cf\u8dcc\u5012\u6a21\u62df\u6570\u636e\u96c6\uff08HQFSD\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u7684\u6027\u80fd\u3002\u7ed3\u679c\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u6765\u8bf4\u662f\u6709\u5e0c\u671b\u7684\uff0c\u5e76\u4e14\u5728\u7ed9\u5b9a\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\uff0c\u5728 HQFSD \u6570\u636e\u96c6\u4e0a\u4ee5\u6700\u5148\u8fdb\u7684 0.96 F1 \u5206\u6570\u5728\u89c6\u9891\u7ea7\u522b\u4e0a\u68c0\u6d4b\u5230\u8dcc\u5012\u3002\u6e90\u4ee3\u7801\u5c06\u5728 GitHub \u4e0a\u63d0\u4f9b\u3002|[2401.16280v1](http://arxiv.org/pdf/2401.16280v1)|null|\n", "2401.16258": "|**2024-01-29**|**MosquIoT: A System Based on IoT and Machine Learning for the Monitoring of Aedes aegypti (Diptera: Culicidae)**|MosquIoT\uff1a\u57fa\u4e8e\u7269\u8054\u7f51\u548c\u673a\u5668\u5b66\u4e60\u7684\u57c3\u53ca\u4f0a\u868a\u76d1\u6d4b\u7cfb\u7edf\uff08\u53cc\u7fc5\u76ee\uff1a\u868a\u79d1\uff09|Javier Aira, Teresa Olivares Montes, Francisco M. Delicado, Dar\u00eco Vezzani|Millions of people around the world are infected with mosquito-borne diseases each year. One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others. Mosquito prevention and eradication campaigns are essential to avoid major public health consequences. In this respect, entomological surveillance is an important tool. At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources. Therefore, new technological tools based on proven techniques need to be designed and developed. However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications. This paper presents the design, development, and testing of an innovative system named MosquIoT. It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs. This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one.|\u5168\u4e16\u754c\u6bcf\u5e74\u6709\u6570\u767e\u4e07\u4eba\u611f\u67d3\u868a\u5a92\u75be\u75c5\u3002\u6700\u5371\u9669\u7684\u7269\u79cd\u4e4b\u4e00\u662f\u57c3\u53ca\u4f0a\u868a\uff0c\u5b83\u662f\u767b\u9769\u70ed\u3001\u9ec4\u70ed\u75c5\u3001\u57fa\u5b54\u80af\u96c5\u70ed\u548c\u5be8\u5361\u75c5\u6bd2\u7b49\u75c5\u6bd2\u7684\u4e3b\u8981\u4f20\u64ad\u5a92\u4ecb\u3002\u9884\u9632\u548c\u6d88\u706d\u868a\u5b50\u8fd0\u52a8\u5bf9\u4e8e\u907f\u514d\u91cd\u5927\u516c\u5171\u536b\u751f\u540e\u679c\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u65b9\u9762\uff0c\u6606\u866b\u5b66\u76d1\u6d4b\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5de5\u5177\u3002\u76ee\u524d\uff0c\u8fd9\u79cd\u4f20\u7edf\u7684\u76d1\u63a7\u5de5\u5177\u662f\u624b\u52a8\u6267\u884c\u7684\uff0c\u9700\u8981\u8fdb\u884c\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u4ee5\u5e2e\u52a9\u5f53\u5c40\u505a\u51fa\u66f4\u597d\u7684\u51b3\u7b56\u3001\u6539\u8fdb\u89c4\u5212\u5de5\u4f5c\u3001\u52a0\u5feb\u6267\u884c\u901f\u5ea6\u5e76\u66f4\u597d\u5730\u7ba1\u7406\u53ef\u7528\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bbe\u8ba1\u548c\u5f00\u53d1\u57fa\u4e8e\u6210\u719f\u6280\u672f\u7684\u65b0\u6280\u672f\u5de5\u5177\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u5de5\u5177\u8fd8\u5e94\u8be5\u5177\u6709\u6210\u672c\u6548\u76ca\u3001\u81ea\u4e3b\u3001\u53ef\u9760\u4e14\u6613\u4e8e\u5b9e\u65bd\uff0c\u5e76\u4e14\u5e94\u901a\u8fc7\u8fde\u63a5\u548c\u591a\u5e73\u53f0\u8f6f\u4ef6\u5e94\u7528\u7a0b\u5e8f\u6765\u5b9e\u73b0\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u540d\u4e3a MosquIoT \u7684\u521b\u65b0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u6d4b\u8bd5\u3002\u5b83\u57fa\u4e8e\u4f20\u7edf\u7684\u8bf1\u4ea7\u5375\u5668\uff0c\u914d\u5907\u5d4c\u5165\u5f0f\u7269\u8054\u7f51 (IoT) \u548c\u5fae\u578b\u673a\u5668\u5b66\u4e60 (TinyML) \u6280\u672f\uff0c\u53ef\u5b9e\u73b0AE\u7684\u68c0\u6d4b\u548c\u91cf\u5316\u3002\u57c3\u53ca\u4f0a\u868a\u86cb\u3002\u8fd9\u79cd\u521b\u65b0\u4e14\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u6709\u52a9\u4e8e\u52a8\u6001\u4e86\u89e3 Ae \u7684\u884c\u4e3a\u3002\u57ce\u5e02\u4e2d\u7684\u57c3\u53ca\u868a\u79cd\u7fa4\uff0c\u4ece\u5f53\u524d\u7684\u53cd\u5e94\u6027\u6606\u866b\u5b66\u76d1\u6d4b\u6a21\u5f0f\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u548c\u9884\u6d4b\u6027\u6570\u5b57\u76d1\u6d4b\u6a21\u5f0f\u3002|[2401.16258v1](http://arxiv.org/pdf/2401.16258v1)|null|\n", "2401.16232": "|**2024-01-29**|**Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis**|\u8de8\u6570\u636e\u5e93\u6d3b\u4f53\u68c0\u6d4b\uff1a\u6bd4\u8f83\u751f\u7269\u8bc6\u522b\u5206\u6790\u7684\u89c1\u89e3|Oleksandr Kuznetsov, Dmytro Zakharov, Emanuele Frontoni, Andrea Maranesi, Serhii Bohucharskyi|In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount. Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge. This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance. Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics. Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses. Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another. Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape. The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain. In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection. In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security.|\u5728\u751f\u7269\u8bc6\u522b\u5b89\u5168\u6210\u4e3a\u73b0\u4ee3\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u57fa\u77f3\u7684\u65f6\u4ee3\uff0c\u786e\u4fdd\u8fd9\u4e9b\u751f\u7269\u8bc6\u522b\u6837\u672c\u7684\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002\u6d3b\u4f53\u68c0\u6d4b\u662f\u533a\u5206\u771f\u5b9e\u548c\u4f2a\u9020\u751f\u7269\u8bc6\u522b\u6837\u672c\u7684\u80fd\u529b\uff0c\u5904\u4e8e\u8fd9\u4e00\u6311\u6218\u7684\u6700\u524d\u6cbf\u3002\u8fd9\u9879\u7814\u7a76\u5bf9\u6d3b\u4f53\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u8de8\u6570\u636e\u5e93\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u8fd9\u662f\u4e00\u79cd\u56e0\u5176\u590d\u6742\u6027\u548c\u73b0\u5b9e\u4e16\u754c\u76f8\u5173\u6027\u800c\u81ed\u540d\u662d\u8457\u7684\u6d4b\u8bd5\u8303\u5f0f\u3002\u6211\u4eec\u7684\u7814\u7a76\u9996\u5148\u4ed4\u7ec6\u8bc4\u4f30\u5404\u4e2a\u6570\u636e\u96c6\u7684\u6a21\u578b\uff0c\u63ed\u793a\u5176\u6027\u80fd\u6307\u6807\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u534a\u603b\u9519\u8bef\u7387\u3001\u9519\u8bef\u63a5\u53d7\u7387\u548c\u9519\u8bef\u62d2\u7edd\u7387\u7b49\u6307\u6807\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u6709\u5173\u6a21\u578b\u4f18\u7f3a\u70b9\u7684\u5b9d\u8d35\u89c1\u89e3\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u5bf9\u8de8\u6570\u636e\u5e93\u6d4b\u8bd5\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u89c6\u89d2\uff0c\u7a81\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3\u4e0e\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u90e8\u7f72\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u4ece\u5377\u79ef\u7f51\u7edc\u5230\u66f4\u590d\u6742\u7684\u7b56\u7565\uff09\u7684\u6bd4\u8f83\u5206\u6790\u4e30\u5bcc\u4e86\u6211\u4eec\u5bf9\u5f53\u524d\u5f62\u52bf\u7684\u7406\u89e3\u3002\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u4e5f\u51f8\u663e\u4e86\u8be5\u9886\u57df\u56fa\u6709\u7684\u6311\u6218\u3002\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u672c\u6587\u65e2\u662f\u7814\u7a76\u7ed3\u679c\u7684\u5b58\u50a8\u5e93\uff0c\u4e5f\u662f\u5bf9\u751f\u7269\u7279\u5f81\u6d3b\u4f53\u68c0\u6d4b\u4e2d\u66f4\u7ec6\u81f4\u3001\u6570\u636e\u591a\u6837\u5316\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u65b9\u6cd5\u7684\u53f7\u89d2\u53f7\u53ec\u3002\u5728\u771f\u5b9e\u6027\u4e0e\u6b3a\u9a97\u6027\u4e4b\u95f4\u7684\u52a8\u6001\u821e\u8e48\u4e2d\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u9a7e\u9a6d\u751f\u7269\u8bc6\u522b\u5b89\u5168\u4e0d\u65ad\u53d1\u5c55\u7684\u8282\u594f\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002|[2401.16232v1](http://arxiv.org/pdf/2401.16232v1)|null|\n", "2401.16173": "|**2024-01-29**|**Reconstructing Close Human Interactions from Multiple Views**|\u4ece\u591a\u4e2a\u89c6\u89d2\u91cd\u5efa\u4eb2\u5bc6\u7684\u4eba\u9645\u4e92\u52a8|Qing Shuai, Zhiyuan Yu, Zhize Zhou, Lixin Fan, Haijun Yang, Can Yang, Xiaowei Zhou|This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.|\u672c\u6587\u89e3\u51b3\u4e86\u91cd\u5efa\u7531\u591a\u4e2a\u6821\u51c6\u76f8\u673a\u6355\u83b7\u7684\u8fdb\u884c\u5bc6\u5207\u4ea4\u4e92\u7684\u591a\u4e2a\u4e2a\u4f53\u7684\u59ff\u52bf\u7684\u6311\u6218\u6027\u4efb\u52a1\u3002\u56f0\u96be\u6765\u81ea\u4e8e\u7531\u4e8e\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u906e\u6321\u800c\u5bfc\u81f4\u7684\u566a\u58f0\u6216\u9519\u8bef\u7684 2D \u5173\u952e\u70b9\u68c0\u6d4b\u3001\u7531\u4e8e\u5bc6\u5207\u4ea4\u4e92\u800c\u5c06\u5173\u952e\u70b9\u4e0e\u4e2a\u4eba\u5173\u8054\u8d77\u6765\u7684\u4e25\u91cd\u6a21\u7cca\u6027\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u56e0\u4e3a\u5728\u62e5\u6324\u7684\u573a\u666f\u4e2d\u6536\u96c6\u548c\u6ce8\u91ca\u8fd0\u52a8\u6570\u636e\u662f\u8d44\u6e90\u3002\u5bc6\u96c6\u7684\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7cfb\u7edf\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u96c6\u6210\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u59ff\u6001\u4f30\u8ba1\u7ec4\u4ef6\u53ca\u5176\u76f8\u5e94\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002\u59ff\u52bf\u4f30\u8ba1\u7ec4\u4ef6\u4ee5\u591a\u89c6\u56fe 2D \u5173\u952e\u70b9\u70ed\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4f7f\u7528 3D \u6761\u4ef6\u4f53\u79ef\u7f51\u7edc\u91cd\u5efa\u6bcf\u4e2a\u4e2a\u4f53\u7684\u59ff\u52bf\u3002\u7531\u4e8e\u7f51\u7edc\u4e0d\u9700\u8981\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u6d4b\u8bd5\u573a\u666f\u4e2d\u5df2\u77e5\u7684\u76f8\u673a\u53c2\u6570\u548c\u5927\u91cf\u73b0\u6709\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6765\u5408\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u6a21\u62df\u6d4b\u8bd5\u573a\u666f\u4e2d\u7684\u771f\u5b9e\u6570\u636e\u5206\u5e03\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u59ff\u52bf\u51c6\u786e\u6027\u65b9\u9762\u663e\u7740\u8d85\u8d8a\u4e86\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u5404\u79cd\u76f8\u673a\u8bbe\u7f6e\u548c\u4eba\u7fa4\u89c4\u6a21\u3002\u8be5\u4ee3\u7801\u53ef\u5728\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4e0a\u627e\u5230\uff1ahttps://github.com/zju3dv/CloseMoCap\u3002|[2401.16173v1](http://arxiv.org/pdf/2401.16173v1)|null|\n", "2401.16131": "|**2024-01-29**|**CIMIL-CRC: a clinically-informed multiple instance learning framework for patient-level colorectal cancer molecular subtypes classification from H\\&E stained images**|CIMIL-CRC\uff1a\u4e00\u79cd\u4e34\u5e8a\u77e5\u60c5\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e H\\&E \u67d3\u8272\u56fe\u50cf\u5bf9\u60a3\u8005\u7ea7\u522b\u7684\u7ed3\u76f4\u80a0\u764c\u5206\u5b50\u4e9a\u578b\u8fdb\u884c\u5206\u7c7b|Hadar Hezi, Matan Gelber, Alexander Balabanov, Yosef E. Maruvka, Moti Freiman|Treatment approaches for colorectal cancer (CRC) are highly dependent on the molecular subtype, as immunotherapy has shown efficacy in cases with microsatellite instability (MSI) but is ineffective for the microsatellite stable (MSS) subtype. There is promising potential in utilizing deep neural networks (DNNs) to automate the differentiation of CRC subtypes by analyzing Hematoxylin and Eosin (H\\&E) stained whole-slide images (WSIs). Due to the extensive size of WSIs, Multiple Instance Learning (MIL) techniques are typically explored. However, existing MIL methods focus on identifying the most representative image patches for classification, which may result in the loss of critical information. Additionally, these methods often overlook clinically relevant information, like the tendency for MSI class tumors to predominantly occur on the proximal (right side) colon. We introduce `CIMIL-CRC', a DNN framework that: 1) solves the MSI/MSS MIL problem by efficiently combining a pre-trained feature extraction model with principal component analysis (PCA) to aggregate information from all patches, and 2) integrates clinical priors, particularly the tumor location within the colon, into the model to enhance patient-level classification accuracy. We assessed our CIMIL-CRC method using the average area under the curve (AUC) from a 5-fold cross-validation experimental setup for model development on the TCGA-CRC-DX cohort, contrasting it with a baseline patch-level classification, MIL-only approach, and Clinically-informed patch-level classification approach. Our CIMIL-CRC outperformed all methods (AUROC: $0.92\\pm0.002$ (95\\% CI 0.91-0.92), vs. $0.79\\pm0.02$ (95\\% CI 0.76-0.82), $0.86\\pm0.01$ (95\\% CI 0.85-0.88), and $0.87\\pm0.01$ (95\\% CI 0.86-0.88), respectively). The improvement was statistically significant.|\u7ed3\u76f4\u80a0\u764c (CRC) \u7684\u6cbb\u7597\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5206\u5b50\u4e9a\u578b\uff0c\u56e0\u4e3a\u514d\u75ab\u7597\u6cd5\u5bf9\u5fae\u536b\u661f\u4e0d\u7a33\u5b9a (MSI) \u4e9a\u578b\u6709\u6548\uff0c\u4f46\u5bf9\u5fae\u536b\u661f\u7a33\u5b9a (MSS) \u4e9a\u578b\u65e0\u6548\u3002\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u901a\u8fc7\u5206\u6790\u82cf\u6728\u7cbe\u548c\u66d9\u7ea2 (H\\&E) \u67d3\u8272\u7684\u5168\u73bb\u7247\u56fe\u50cf (WSI) \u6765\u81ea\u52a8\u533a\u5206 CRC \u4e9a\u578b\uff0c\u5177\u6709\u5e7f\u9614\u7684\u524d\u666f\u3002\u7531\u4e8e WSI \u89c4\u6a21\u5e9e\u5927\uff0c\u901a\u5e38\u4f1a\u63a2\u7d22\u591a\u5b9e\u4f8b\u5b66\u4e60 (MIL) \u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 MIL \u65b9\u6cd5\u4fa7\u91cd\u4e8e\u8bc6\u522b\u6700\u5177\u4ee3\u8868\u6027\u7684\u56fe\u50cf\u5757\u8fdb\u884c\u5206\u7c7b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u7684\u4e22\u5931\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5e38\u5e38\u5ffd\u89c6\u4e34\u5e8a\u76f8\u5173\u4fe1\u606f\uff0c\u4f8b\u5982 MSI \u7c7b\u80bf\u7624\u4e3b\u8981\u53d1\u751f\u5728\u8fd1\u7aef\uff08\u53f3\u4fa7\uff09\u7ed3\u80a0\u7684\u8d8b\u52bf\u3002\u6211\u4eec\u5f15\u5165\u201cCIMIL-CRC\u201d\uff0c\u4e00\u4e2a DNN \u6846\u67b6\uff0c\u5b83\uff1a1\uff09\u901a\u8fc7\u6709\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u4e0e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u76f8\u7ed3\u5408\u6765\u805a\u5408\u6765\u81ea\u6240\u6709\u8865\u4e01\u7684\u4fe1\u606f\u6765\u89e3\u51b3 MSI/MSS MIL \u95ee\u9898\uff0c2\uff09\u96c6\u6210\u5c06\u4e34\u5e8a\u5148\u9a8c\uff0c\u7279\u522b\u662f\u7ed3\u80a0\u5185\u7684\u80bf\u7624\u4f4d\u7f6e\u7eb3\u5165\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u60a3\u8005\u7ea7\u522b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002\u6211\u4eec\u4f7f\u7528\u6765\u81ea TCGA-CRC-DX \u961f\u5217\u6a21\u578b\u5f00\u53d1\u7684 5 \u500d\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5e73\u5747\u66f2\u7ebf\u4e0b\u9762\u79ef (AUC) \u6765\u8bc4\u4f30\u6211\u4eec\u7684 CIMIL-CRC \u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u7ebf\u8865\u4e01\u7ea7\u5206\u7c7b MIL \u8fdb\u884c\u5bf9\u6bd4\u4ec5\u65b9\u6cd5\u548c\u4e34\u5e8a\u77e5\u60c5\u7684\u6591\u5757\u7ea7\u5206\u7c7b\u65b9\u6cd5\u3002\u6211\u4eec\u7684 CIMIL-CRC \u4f18\u4e8e\u6240\u6709\u65b9\u6cd5\uff08AUROC\uff1a$0.92\\pm0.002$ (95\\% CI 0.91-0.92)\uff0c\u5bf9\u6bd4 $0.79\\pm0.02$ (95\\% CI 0.76-0.82)\u3001$0.86\\pm0.01$ (95\\% CI 0.85-0.88) \u548c $0.87\\pm0.01$ (95\\% CI 0.86-0.88)\u3002\u6539\u5584\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u3002|[2401.16131v1](http://arxiv.org/pdf/2401.16131v1)|null|\n", "2401.16110": "|**2024-01-29**|**Towards Scenario Generalization for Vision-based Roadside 3D Object Detection**|\u8fc8\u5411\u57fa\u4e8e\u89c6\u89c9\u7684\u8def\u8fb9 3D \u7269\u4f53\u68c0\u6d4b\u7684\u573a\u666f\u6cdb\u5316|Lei Yang, Xinyu Zhang, Jun Li, Li Wang, Chuang Zhang, Li Ju, Zhiwei Li, Yang Shen|Roadside perception can greatly increase the safety of autonomous vehicles by extending their perception ability beyond the visual range and addressing blind spots. However, current state-of-the-art vision-based roadside detection methods possess high accuracy on labeled scenes but have inferior performance on new scenes. This is because roadside cameras remain stationary after installation and can only collect data from a single scene, resulting in the algorithm overfitting these roadside backgrounds and camera poses. To address this issue, in this paper, we propose an innovative Scenario Generalization Framework for Vision-based Roadside 3D Object Detection, dubbed SGV3D. Specifically, we employ a Background-suppressed Module (BSM) to mitigate background overfitting in vision-centric pipelines by attenuating background features during the 2D to bird's-eye-view projection. Furthermore, by introducing the Semi-supervised Data Generation Pipeline (SSDG) using unlabeled images from new scenes, diverse instance foregrounds with varying camera poses are generated, addressing the risk of overfitting specific camera poses. We evaluate our method on two large-scale roadside benchmarks. Our method surpasses all previous methods by a significant margin in new scenes, including +42.57% for vehicle, +5.87% for pedestrian, and +14.89% for cyclist compared to BEVHeight on the DAIR-V2X-I heterologous benchmark. On the larger-scale Rope3D heterologous benchmark, we achieve notable gains of 14.48% for car and 12.41% for large vehicle. We aspire to contribute insights on the exploration of roadside perception techniques, emphasizing their capability for scenario generalization. The code will be available at {\\url{ https://github.com/yanglei18/SGV3D}}|\u8def\u8fb9\u611f\u77e5\u53ef\u4ee5\u5c06\u611f\u77e5\u80fd\u529b\u6269\u5c55\u5230\u89c6\u89c9\u8303\u56f4\u4e4b\u5916\u5e76\u89e3\u51b3\u76f2\u70b9\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u8def\u8fb9\u68c0\u6d4b\u65b9\u6cd5\u5728\u6807\u8bb0\u573a\u666f\u4e0a\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5728\u65b0\u573a\u666f\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u8fd9\u662f\u56e0\u4e3a\u8def\u8fb9\u6444\u50cf\u5934\u5b89\u88c5\u540e\u4fdd\u6301\u9759\u6b62\uff0c\u53ea\u80fd\u6536\u96c6\u5355\u4e2a\u573a\u666f\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u7b97\u6cd5\u8fc7\u5ea6\u62df\u5408\u8fd9\u4e9b\u8def\u8fb9\u80cc\u666f\u548c\u6444\u50cf\u5934\u59ff\u52bf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u8def\u8fb9 3D \u7269\u4f53\u68c0\u6d4b\u7684\u521b\u65b0\u573a\u666f\u6cdb\u5316\u6846\u67b6\uff0c\u79f0\u4e3a SGV3D\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u80cc\u666f\u6291\u5236\u6a21\u5757\uff08BSM\uff09\uff0c\u901a\u8fc7\u5728 2D \u5230\u9e1f\u77b0\u56fe\u6295\u5f71\u671f\u95f4\u51cf\u5f31\u80cc\u666f\u7279\u5f81\u6765\u51cf\u8f7b\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u7ba1\u9053\u4e2d\u7684\u80cc\u666f\u8fc7\u5ea6\u62df\u5408\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u4f7f\u7528\u6765\u81ea\u65b0\u573a\u666f\u7684\u672a\u6807\u8bb0\u56fe\u50cf\u7684\u534a\u76d1\u7763\u6570\u636e\u751f\u6210\u7ba1\u9053\uff08SSDG\uff09\uff0c\u751f\u6210\u5177\u6709\u4e0d\u540c\u76f8\u673a\u59ff\u52bf\u7684\u591a\u6837\u5316\u5b9e\u4f8b\u524d\u666f\uff0c\u89e3\u51b3\u4e86\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u76f8\u673a\u59ff\u52bf\u7684\u98ce\u9669\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5927\u578b\u8def\u8fb9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u4e0e DAIR-V2X-I \u5f02\u6e90\u57fa\u51c6\u4e0a\u7684 BEVHeight \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65b0\u573a\u666f\u4e2d\u663e\u7740\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6240\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u8f66\u8f86 +42.57%\uff0c\u884c\u4eba +5.87%\uff0c\u9a91\u8f66\u4eba +14.89%\u3002\u5728\u66f4\u5927\u89c4\u6a21\u7684 Rope3D \u5f02\u6e90\u57fa\u51c6\u4e0a\uff0c\u6211\u4eec\u5728\u6c7d\u8f66\u65b9\u9762\u53d6\u5f97\u4e86 14.48% \u7684\u663e\u7740\u6536\u76ca\uff0c\u5728\u5927\u578b\u8f66\u8f86\u65b9\u9762\u53d6\u5f97\u4e86 12.41% \u7684\u663e\u7740\u6536\u76ca\u3002\u6211\u4eec\u6e34\u671b\u4e3a\u8def\u8fb9\u611f\u77e5\u6280\u672f\u7684\u63a2\u7d22\u63d0\u4f9b\u89c1\u89e3\uff0c\u5f3a\u8c03\u5176\u573a\u666f\u6cdb\u5316\u7684\u80fd\u529b\u3002\u4ee3\u7801\u53ef\u5728 {\\url{ https://github.com/yanglei18/SGV3D}} \u83b7\u53d6|[2401.16110v1](http://arxiv.org/pdf/2401.16110v1)|null|\n", "2401.16104": "|**2024-01-29**|**A 2D Sinogram-Based Approach to Defect Localization in Computed Tomography**|\u57fa\u4e8e 2D \u6b63\u5f26\u56fe\u7684\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5|Yuzhong Zhou, Linda-Sophie Schneider, Fuxin Fan, Andreas Maier|The rise of deep learning has introduced a transformative era in the field of image processing, particularly in the context of computed tomography. Deep learning has made a significant contribution to the field of industrial Computed Tomography. However, many defect detection algorithms are applied directly to the reconstructed domain, often disregarding the raw sensor data. This paper shifts the focus to the use of sinograms. Within this framework, we present a comprehensive three-step deep learning algorithm, designed to identify and analyze defects within objects without resorting to image reconstruction. These three steps are defect segmentation, mask isolation, and defect analysis. We use a U-Net-based architecture for defect segmentation. Our method achieves the Intersection over Union of 92.02% on our simulated data, with an average position error of 1.3 pixels for defect detection on a 512-pixel-wide detector.|\u6df1\u5ea6\u5b66\u4e60\u7684\u5174\u8d77\u4e3a\u56fe\u50cf\u5904\u7406\u9886\u57df\u5e26\u6765\u4e86\u4e00\u4e2a\u53d8\u9769\u65f6\u4ee3\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u9886\u57df\u3002\u6df1\u5ea6\u5b66\u4e60\u4e3a\u5de5\u4e1a\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u9886\u57df\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e\u3002\u7136\u800c\uff0c\u8bb8\u591a\u7f3a\u9677\u68c0\u6d4b\u7b97\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u91cd\u5efa\u57df\uff0c\u901a\u5e38\u5ffd\u7565\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u3002\u672c\u6587\u5c06\u91cd\u70b9\u8f6c\u79fb\u5230\u6b63\u5f26\u56fe\u7684\u4f7f\u7528\u4e0a\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u4e09\u6b65\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u65e8\u5728\u8bc6\u522b\u548c\u5206\u6790\u7269\u4f53\u5185\u7684\u7f3a\u9677\uff0c\u800c\u65e0\u9700\u6c42\u52a9\u4e8e\u56fe\u50cf\u91cd\u5efa\u3002\u8fd9\u4e09\u4e2a\u6b65\u9aa4\u662f\u7f3a\u9677\u5206\u5272\u3001\u63a9\u6a21\u9694\u79bb\u548c\u7f3a\u9677\u5206\u6790\u3002\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e U-Net \u7684\u67b6\u6784\u8fdb\u884c\u7f3a\u9677\u5206\u5272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86 92.02% \u7684\u4ea4\u96c6\u6bd4\u5e76\u96c6\uff0c\u5728 512 \u50cf\u7d20\u5bbd\u7684\u68c0\u6d4b\u5668\u4e0a\u8fdb\u884c\u7f3a\u9677\u68c0\u6d4b\u7684\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u4e3a 1.3 \u50cf\u7d20\u3002|[2401.16104v1](http://arxiv.org/pdf/2401.16104v1)|null|\n", "2401.16058": "|**2024-01-29**|**Neuromorphic Valence and Arousal Estimation**|\u795e\u7ecf\u5f62\u6001\u6548\u4ef7\u548c\u5524\u9192\u4f30\u8ba1|Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Alberto Del Bimbo|Recognizing faces and their underlying emotions is an important aspect of biometrics. In fact, estimating emotional states from faces has been tackled from several angles in the literature. In this paper, we follow the novel route of using neuromorphic data to predict valence and arousal values from faces. Due to the difficulty of gathering event-based annotated videos, we leverage an event camera simulator to create the neuromorphic counterpart of an existing RGB dataset. We demonstrate that not only training models on simulated data can still yield state-of-the-art results in valence-arousal estimation, but also that our trained models can be directly applied to real data without further training to address the downstream task of emotion recognition. In the paper we propose several alternative models to solve the task, both frame-based and video-based.|\u8bc6\u522b\u9762\u5b54\u53ca\u5176\u6f5c\u5728\u60c5\u7eea\u662f\u751f\u7269\u8bc6\u522b\u6280\u672f\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6587\u732e\u4e2d\u5df2\u7ecf\u4ece\u591a\u4e2a\u89d2\u5ea6\u89e3\u51b3\u4e86\u4ece\u9762\u90e8\u4f30\u8ba1\u60c5\u7eea\u72b6\u6001\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9075\u5faa\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u6570\u636e\u6765\u9884\u6d4b\u9762\u90e8\u7684\u6548\u4ef7\u548c\u5524\u9192\u503c\u7684\u65b0\u9014\u5f84\u3002\u7531\u4e8e\u6536\u96c6\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5e26\u6ce8\u91ca\u89c6\u9891\u5f88\u56f0\u96be\uff0c\u6211\u4eec\u5229\u7528\u4e8b\u4ef6\u6444\u50cf\u673a\u6a21\u62df\u5668\u6765\u521b\u5efa\u73b0\u6709 RGB \u6570\u636e\u96c6\u7684\u795e\u7ecf\u5f62\u6001\u5bf9\u5e94\u7269\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0d\u4ec5\u6a21\u62df\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u6a21\u578b\u4ecd\u7136\u53ef\u4ee5\u5728\u4ef7\u5524\u9192\u4f30\u8ba1\u4e2d\u4ea7\u751f\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u800c\u4e14\u6211\u4eec\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u771f\u5b9e\u6570\u636e\uff0c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u6765\u89e3\u51b3\u60c5\u611f\u7684\u4e0b\u6e38\u4efb\u52a1\u8ba4\u51fa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u66ff\u4ee3\u6a21\u578b\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\uff0c\u5305\u62ec\u57fa\u4e8e\u5e27\u7684\u6a21\u578b\u548c\u57fa\u4e8e\u89c6\u9891\u7684\u6a21\u578b\u3002|[2401.16058v1](http://arxiv.org/pdf/2401.16058v1)|null|\n", "2401.16051": "|**2024-01-29**|**Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation**|\u52a8\u6001\u539f\u578b\u9002\u5e94\u4e0e\u84b8\u998f\u7528\u4e8e\u5c11\u6837\u672c\u70b9\u4e91\u5206\u5272|Jie Liu, Wenzhe Yin, Haochen Wang, Yunlu CHen, Jan-Jakob Sonke, Efstratios Gavves|Few-shot point cloud segmentation seeks to generate per-point masks for previously unseen categories, using only a minimal set of annotated point clouds as reference. Existing prototype-based methods rely on support prototypes to guide the segmentation of query point clouds, but they encounter challenges when significant object variations exist between the support prototypes and query features. In this work, we present dynamic prototype adaptation (DPA), which explicitly learns task-specific prototypes for each query point cloud to tackle the object variation problem. DPA achieves the adaptation through prototype rectification, aligning vanilla prototypes from support with the query feature distribution, and prototype-to-query attention, extracting task-specific context from query point clouds. Furthermore, we introduce a prototype distillation regularization term, enabling knowledge transfer between early-stage prototypes and their deeper counterparts during adaption. By iteratively applying these adaptations, we generate task-specific prototypes for accurate mask predictions on query point clouds. Extensive experiments on two popular benchmarks show that DPA surpasses state-of-the-art methods by a significant margin, e.g., 7.43\\% and 6.39\\% under the 2-way 1-shot setting on S3DIS and ScanNet, respectively. Code is available at https://github.com/jliu4ai/DPA.|\u5c11\u955c\u5934\u70b9\u4e91\u5206\u5272\u65e8\u5728\u4ec5\u4f7f\u7528\u6700\u5c11\u7684\u6ce8\u91ca\u70b9\u4e91\u96c6\u4f5c\u4e3a\u53c2\u8003\uff0c\u4e3a\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\u751f\u6210\u6bcf\u70b9\u63a9\u6a21\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u539f\u578b\u7684\u65b9\u6cd5\u4f9d\u9760\u652f\u6301\u539f\u578b\u6765\u6307\u5bfc\u67e5\u8be2\u70b9\u4e91\u7684\u5206\u5272\uff0c\u4f46\u662f\u5f53\u652f\u6301\u539f\u578b\u548c\u67e5\u8be2\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u5bf9\u8c61\u53d8\u5316\u65f6\uff0c\u5b83\u4eec\u4f1a\u9047\u5230\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u6001\u539f\u578b\u9002\u5e94\uff08DPA\uff09\uff0c\u5b83\u663e\u5f0f\u5730\u5b66\u4e60\u6bcf\u4e2a\u67e5\u8be2\u70b9\u4e91\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u539f\u578b\u6765\u89e3\u51b3\u5bf9\u8c61\u53d8\u5316\u95ee\u9898\u3002 DPA \u901a\u8fc7\u539f\u578b\u6821\u6b63\u3001\u5c06\u539f\u59cb\u539f\u578b\u4e0e\u67e5\u8be2\u7279\u5f81\u5206\u5e03\u7684\u652f\u6301\u5bf9\u9f50\u3001\u539f\u578b\u5230\u67e5\u8be2\u7684\u6ce8\u610f\u529b\u3001\u4ece\u67e5\u8be2\u70b9\u4e91\u4e2d\u63d0\u53d6\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u6765\u5b9e\u73b0\u9002\u5e94\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u539f\u578b\u84b8\u998f\u6b63\u5219\u5316\u9879\uff0c\u4f7f\u5f97\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u65e9\u671f\u539f\u578b\u4e0e\u5176\u66f4\u6df1\u5c42\u539f\u578b\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u6210\u4e3a\u53ef\u80fd\u3002\u901a\u8fc7\u8fed\u4ee3\u5e94\u7528\u8fd9\u4e9b\u8c03\u6574\uff0c\u6211\u4eec\u751f\u6210\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u539f\u578b\uff0c\u4ee5\u5728\u67e5\u8be2\u70b9\u4e91\u4e0a\u8fdb\u884c\u51c6\u786e\u7684\u63a9\u6a21\u9884\u6d4b\u3002\u5bf9\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDPA \u663e\u7740\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\uff0c\u5728 S3DIS \u548c ScanNet \u4e0a\u7684 2 \u8def 1-shot \u8bbe\u7f6e\u4e0b\u5206\u522b\u4e3a 7.43\\% \u548c 6.39\\%\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/jliu4ai/DPA \u83b7\u53d6\u3002|[2401.16051v1](http://arxiv.org/pdf/2401.16051v1)|null|\n", "2401.16035": "|**2024-01-29**|**Second Order Kinematic Surface Fitting in Anatomical Structures**|\u89e3\u5256\u7ed3\u6784\u4e2d\u7684\u4e8c\u9636\u8fd0\u52a8\u66f2\u9762\u62df\u5408|Wilhelm Wimmer, Herv\u00e9 Delingette|Symmetry detection and morphological classification of anatomical structures play pivotal roles in medical image analysis. The application of kinematic surface fitting, a method for characterizing shapes through parametric stationary velocity fields, has shown promising results in computer vision and computer-aided design. However, existing research has predominantly focused on first order rotational velocity fields, which may not adequately capture the intricate curved and twisted nature of anatomical structures. To address this limitation, we propose an innovative approach utilizing a second order velocity field for kinematic surface fitting. This advancement accommodates higher rotational shape complexity and improves the accuracy of symmetry detection in anatomical structures. We introduce a robust fitting technique and validate its performance through testing on synthetic shapes and real anatomical structures. Our method not only enables the detection of curved rotational symmetries (core lines) but also facilitates morphological classification by deriving intrinsic shape parameters related to curvature and torsion. We illustrate the usefulness of our technique by categorizing the shape of human cochleae in terms of the intrinsic velocity field parameters. The results showcase the potential of our method as a valuable tool for medical image analysis, contributing to the assessment of complex anatomical shapes.|\u89e3\u5256\u7ed3\u6784\u7684\u5bf9\u79f0\u6027\u68c0\u6d4b\u548c\u5f62\u6001\u5206\u7c7b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u8fd0\u52a8\u66f2\u9762\u62df\u5408\u662f\u4e00\u79cd\u901a\u8fc7\u53c2\u6570\u9759\u6b62\u901f\u5ea6\u573a\u8868\u5f81\u5f62\u72b6\u7684\u65b9\u6cd5\uff0c\u5b83\u7684\u5e94\u7528\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\u4e2d\u663e\u793a\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u9636\u65cb\u8f6c\u901f\u5ea6\u573a\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u89e3\u5256\u7ed3\u6784\u590d\u6742\u7684\u5f2f\u66f2\u548c\u626d\u66f2\u6027\u8d28\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e8c\u9636\u901f\u5ea6\u573a\u8fdb\u884c\u8fd0\u52a8\u8868\u9762\u62df\u5408\u7684\u521b\u65b0\u65b9\u6cd5\u3002\u8fd9\u4e00\u8fdb\u6b65\u53ef\u9002\u5e94\u66f4\u9ad8\u7684\u65cb\u8f6c\u5f62\u72b6\u590d\u6742\u6027\uff0c\u5e76\u63d0\u9ad8\u89e3\u5256\u7ed3\u6784\u4e2d\u5bf9\u79f0\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u62df\u5408\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5bf9\u5408\u6210\u5f62\u72b6\u548c\u771f\u5b9e\u89e3\u5256\u7ed3\u6784\u7684\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u68c0\u6d4b\u5f2f\u66f2\u65cb\u8f6c\u5bf9\u79f0\u6027\uff08\u6838\u5fc3\u7ebf\uff09\uff0c\u800c\u4e14\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a8\u5bfc\u4e0e\u66f2\u7387\u548c\u626d\u8f6c\u76f8\u5173\u7684\u5185\u5728\u5f62\u72b6\u53c2\u6570\u6765\u4fc3\u8fdb\u5f62\u6001\u5206\u7c7b\u3002\u6211\u4eec\u901a\u8fc7\u6839\u636e\u56fa\u6709\u901f\u5ea6\u573a\u53c2\u6570\u5bf9\u4eba\u7c7b\u8033\u8717\u7684\u5f62\u72b6\u8fdb\u884c\u5206\u7c7b\u6765\u8bf4\u660e\u6211\u4eec\u7684\u6280\u672f\u7684\u6709\u7528\u6027\u3002\u7ed3\u679c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4f5c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u5b9d\u8d35\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u590d\u6742\u7684\u89e3\u5256\u5f62\u72b6\u3002|[2401.16035v1](http://arxiv.org/pdf/2401.16035v1)|null|\n", "2401.15990": "|**2024-01-29**|**Gland segmentation via dual encoders and boundary-enhanced attention**|\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u548c\u8fb9\u754c\u589e\u5f3a\u6ce8\u610f\u529b\u8fdb\u884c\u817a\u4f53\u5206\u5272|Huadeng Wang, Jiejiang Yu, Bingbing Li, Xipeng Pan, Zhenbing Liu, Rushi Lan, Xiaonan Luo|Accurate and automated gland segmentation on pathological images can assist pathologists in diagnosing the malignancy of colorectal adenocarcinoma. However, due to various gland shapes, severe deformation of malignant glands, and overlapping adhesions between glands. Gland segmentation has always been very challenging. To address these problems, we propose a DEA model. This model consists of two branches: the backbone encoding and decoding network and the local semantic extraction network. The backbone encoding and decoding network extracts advanced Semantic features, uses the proposed feature decoder to restore feature space information, and then enhances the boundary features of the gland through boundary enhancement attention. The local semantic extraction network uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder to realize the extraction of edge features. Experimental results on two public datasets, GlaS and CRAG, confirm that the performance of our method is better than other gland segmentation methods.|\u75c5\u7406\u56fe\u50cf\u4e0a\u51c6\u786e\u3001\u81ea\u52a8\u5316\u7684\u817a\u4f53\u5206\u5272\u53ef\u4ee5\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u8bca\u65ad\u7ed3\u76f4\u80a0\u817a\u764c\u7684\u6076\u6027\u7a0b\u5ea6\u3002\u4f46\u7531\u4e8e\u817a\u4f53\u5f62\u72b6\u591a\u6837\uff0c\u6076\u6027\u817a\u4f53\u53d8\u5f62\u4e25\u91cd\uff0c\u817a\u4f53\u4e4b\u95f4\u91cd\u53e0\u7c98\u8fde\u3002\u817a\u4f53\u5206\u5272\u4e00\u76f4\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DEA \u6a21\u578b\u3002\u8be5\u6a21\u578b\u7531\u4e24\u4e2a\u5206\u652f\u7ec4\u6210\uff1a\u4e3b\u5e72\u7f16\u89e3\u7801\u7f51\u7edc\u548c\u5c40\u90e8\u8bed\u4e49\u63d0\u53d6\u7f51\u7edc\u3002\u4e3b\u5e72\u7f16\u7801\u548c\u89e3\u7801\u7f51\u7edc\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u7279\u5f81\u89e3\u7801\u5668\u6765\u6062\u590d\u7279\u5f81\u7a7a\u95f4\u4fe1\u606f\uff0c\u7136\u540e\u901a\u8fc7\u8fb9\u754c\u589e\u5f3a\u6ce8\u610f\u6765\u589e\u5f3a\u817a\u4f53\u7684\u8fb9\u754c\u7279\u5f81\u3002\u5c40\u90e8\u8bed\u4e49\u63d0\u53d6\u7f51\u7edc\u4f7f\u7528\u9884\u8bad\u7ec3\u7684DeepLabv3+\u4f5c\u4e3a\u5c40\u90e8\u8bed\u4e49\u5f15\u5bfc\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u8fb9\u7f18\u7279\u5f81\u7684\u63d0\u53d6\u3002\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6 GlaS \u548c CRAG \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u817a\u4f53\u5206\u5272\u65b9\u6cd5\u3002|[2401.15990v1](http://arxiv.org/pdf/2401.15990v1)|null|\n", "2401.15942": "|**2024-01-29**|**Generating Multi-Center Classifier via Conditional Gaussian Distribution**|\u901a\u8fc7\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\u751f\u6210\u591a\u4e2d\u5fc3\u5206\u7c7b\u5668|Zhemin Zhang, Xun Gong|The linear classifier is widely used in various image classification tasks. It works by optimizing the distance between a sample and its corresponding class center. However, in real-world data, one class can contain several local clusters, e.g., birds of different poses. To address this complexity, we propose a novel multi-center classifier. Different from the vanilla linear classifier, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. Specifically, we create a conditional Gaussian distribution for each class and then sample multiple sub-centers from that distribution to extend the linear classifier. This approach allows the model to capture intra-class local structures more efficiently. In addition, at test time we set the mean of the conditional Gaussian distribution as the class center of the linear classifier and follow the vanilla linear classifier outputs, thus requiring no additional parameters or computational overhead. Extensive experiments on image classification show that the proposed multi-center classifier is a powerful alternative to widely used linear classifiers. Code available at https://github.com/ZheminZhang1/MultiCenter-Classifier.|\u7ebf\u6027\u5206\u7c7b\u5668\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u662f\u4f18\u5316\u6837\u672c\u4e0e\u5176\u76f8\u5e94\u7684\u7c7b\u4e2d\u5fc3\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u4e2d\uff0c\u4e00\u7c7b\u53ef\u4ee5\u5305\u542b\u591a\u4e2a\u5c40\u90e8\u96c6\u7fa4\uff0c\u4f8b\u5982\u4e0d\u540c\u59ff\u52bf\u7684\u9e1f\u7c7b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4e2d\u5fc3\u5206\u7c7b\u5668\u3002\u4e0e\u666e\u901a\u7ebf\u6027\u5206\u7c7b\u5668\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u5efa\u8bae\u662f\u5efa\u7acb\u5728\u8bad\u7ec3\u96c6\u7684\u6df1\u5c42\u7279\u5f81\u9075\u5faa\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u7684\u5047\u8bbe\u4e4b\u4e0a\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4e3a\u6bcf\u4e2a\u7c7b\u521b\u5efa\u4e00\u4e2a\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\uff0c\u7136\u540e\u4ece\u8be5\u5206\u5e03\u4e2d\u91c7\u6837\u591a\u4e2a\u5b50\u4e2d\u5fc3\u4ee5\u6269\u5c55\u7ebf\u6027\u5206\u7c7b\u5668\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u6a21\u578b\u66f4\u6709\u6548\u5730\u6355\u83b7\u7c7b\u5185\u5c40\u90e8\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5728\u6d4b\u8bd5\u65f6\uff0c\u6211\u4eec\u5c06\u6761\u4ef6\u9ad8\u65af\u5206\u5e03\u7684\u5747\u503c\u8bbe\u7f6e\u4e3a\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u7c7b\u4e2d\u5fc3\uff0c\u5e76\u9075\u5faa\u666e\u901a\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u8f93\u51fa\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u989d\u5916\u7684\u53c2\u6570\u6216\u8ba1\u7b97\u5f00\u9500\u3002\u5927\u91cf\u7684\u56fe\u50cf\u5206\u7c7b\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u4e2d\u5fc3\u5206\u7c7b\u5668\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/ZheminZhang1/MultiCenter-Classifier \u83b7\u53d6\u3002|[2401.15942v1](http://arxiv.org/pdf/2401.15942v1)|null|\n", "2401.15934": "|**2024-01-29**|**HICH Image/Text (HICH-IT): Comprehensive Text and Image Datasets for Hypertensive Intracerebral Hemorrhage Research**|HICH \u56fe\u50cf/\u6587\u672c (HICH-IT)\uff1a\u7528\u4e8e\u9ad8\u8840\u538b\u8111\u51fa\u8840\u7814\u7a76\u7684\u7efc\u5408\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u96c6|Jie Li, Yulong Xia, Tongxin Yang, Fenglin Cai, Miao Wei, Zhiwei Zhang, Li Jiang|In this paper, we introduce a new multimodal dataset in the medical field of hypertensive intracerebral hemorrhage(HICH), called as HICH-IT, which includes both textual information and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of HICH. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the text data, extracting key content from the text information, and categorizes the annotation content of imaging data into four types: brain midline, hematoma, left cerebral ventricle, and right cerebral ventricle. HICH-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition. To further understand the dataset, we have trained deep learning algorithms to observe the performance. The pretrained models have been released at both www.daip.club and github.com/Deep-AI-Application-DAIP. The dataset has been uploaded to https://github.com/CYBUS123456/HICH-IT-Datasets.   Index Terms-HICH, Deep learning, Intraparenchymal hemorrhage, named entity recognition, novel dataset|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u9ad8\u8840\u538b\u8111\u51fa\u8840\uff08HICH\uff09\u533b\u5b66\u9886\u57df\u7684\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u79f0\u4e3aHICH-IT\uff0c\u5176\u4e2d\u5305\u62ec\u6587\u672c\u4fe1\u606f\u548c\u5934\u90e8CT\u56fe\u50cf\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u5728 HICH \u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u7684\u51c6\u786e\u6027\u3002\u8be5\u6570\u636e\u96c6\u5efa\u7acb\u5728\u6807\u51c6\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u6587\u672c\u6570\u636e\u4e2d\u878d\u5165\u7279\u5b9a\u7684\u6ce8\u91ca\uff0c\u4ece\u6587\u672c\u4fe1\u606f\u4e2d\u63d0\u53d6\u5173\u952e\u5185\u5bb9\uff0c\u5e76\u5c06\u5f71\u50cf\u6570\u636e\u7684\u6ce8\u91ca\u5185\u5bb9\u5206\u4e3a\u56db\u79cd\u7c7b\u578b\uff1a\u8111\u4e2d\u7ebf\u3001\u8840\u80bf\u3001\u5de6\u8111\u5ba4\u548c\u53f3\u8111\u5ba4\u3002 HICH-IT \u65e8\u5728\u6210\u4e3a\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7279\u5f81\u5b66\u4e60\u7684\u57fa\u7840\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u4e86\u89e3\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u6765\u89c2\u5bdf\u6027\u80fd\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5728 www.daip.club \u548c github.com/Deep-AI-Application-DAIP \u4e0a\u53d1\u5e03\u3002\u6570\u636e\u96c6\u5df2\u4e0a\u4f20\u81f3https://github.com/CYBUS123456/HICH-IT-Datasets\u3002\u5173\u952e\u8bcd-HICH\uff0c\u6df1\u5ea6\u5b66\u4e60\uff0c\u8111\u5b9e\u8d28\u51fa\u8840\uff0c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u65b0\u9896\u6570\u636e\u96c6|[2401.15934v1](http://arxiv.org/pdf/2401.15934v1)|null|\n", "2401.15914": "|**2024-01-29**|**Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization**|\u514b\u670d OOD \u6cdb\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u9677\u9631|Yuhang Zang, Hanlin Goh, Josh Susskind, Chen Huang|Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution (ID) and out-of-distribution (OOD) samples, but also show some improvements in both ID and OOD accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach OGEN to address this pitfall, with the main focus on improving the OOD GENeralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize OOD features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between ID and OOD data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, i.e., adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in OOD generalization performance in different settings.|\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u9886\u57df\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u6a21\u578b\u4e3b\u8981\u4ee5\u95ed\u96c6\u65b9\u5f0f\u6267\u884c\u96f6\u6837\u672c\u8bc6\u522b\uff0c\u56e0\u6b64\u5f88\u96be\u901a\u8fc7\u8bbe\u8ba1\u6765\u5904\u7406\u5f00\u653e\u57df\u89c6\u89c9\u6982\u5ff5\u3002\u6700\u8fd1\u6709\u4e00\u4e9b\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f8b\u5982\u5373\u65f6\u5b66\u4e60\uff0c\u4e0d\u4ec5\u7814\u7a76\u5206\u5e03\u5185\uff08ID\uff09\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u6837\u672c\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u800c\u4e14\u8fd8\u663e\u793a\u4e86 ID \u548c OOD \u51c6\u786e\u6027\u7684\u4e00\u4e9b\u6539\u8fdb\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u8bc1\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u8fc7\u8db3\u591f\u957f\u7684\u5fae\u8c03\u4f46\u6ca1\u6709\u9002\u5f53\u7684\u6b63\u5219\u5316\u540e\uff0c\u5f80\u5f80\u4f1a\u8fc7\u5ea6\u62df\u5408\u7ed9\u5b9a\u6570\u636e\u96c6\u4e2d\u7684\u5df2\u77e5\u7c7b\uff0c\u4ece\u800c\u5bfc\u81f4\u672a\u77e5\u7c7b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u7136\u540e\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5 OGEN \u6765\u89e3\u51b3\u8fd9\u4e2a\u9677\u9631\uff0c\u4e3b\u8981\u5173\u6ce8\u4e8e\u6539\u8fdb\u5fae\u8c03\u6a21\u578b\u7684 OOD \u6cdb\u5316\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86\u7c7b\u6761\u4ef6\u7279\u5f81\u751f\u6210\u5668\uff0c\u4ec5\u4f7f\u7528\u4efb\u4f55\u672a\u77e5\u7c7b\u7684\u7c7b\u540d\u6765\u5408\u6210 OOD \u7279\u5f81\u3002\u8fd9\u79cd\u7efc\u5408\u7279\u5f81\u5c06\u63d0\u4f9b\u6709\u5173\u672a\u77e5\u6570\u7684\u6709\u7528\u77e5\u8bc6\uff0c\u5e76\u5728\u8054\u5408\u4f18\u5316\u65f6\u5e2e\u52a9\u89c4\u8303 ID \u548c OOD \u6570\u636e\u4e4b\u95f4\u7684\u51b3\u7b56\u8fb9\u754c\u3002\u540c\u6837\u91cd\u8981\u7684\u662f\u6211\u4eec\u7684\u81ea\u9002\u5e94\u81ea\u84b8\u998f\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u8054\u5408\u4f18\u5316\u671f\u95f4\u89c4\u8303\u6211\u4eec\u7684\u7279\u5f81\u751f\u6210\u6a21\u578b\uff0c\u5373\u5728\u6a21\u578b\u72b6\u6001\u4e4b\u95f4\u81ea\u9002\u5e94\u5730\u4f20\u9012\u77e5\u8bc6\u4ee5\u8fdb\u4e00\u6b65\u9632\u6b62\u8fc7\u5ea6\u62df\u5408\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684 OOD \u6cdb\u5316\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u6536\u76ca\u3002|[2401.15914v1](http://arxiv.org/pdf/2401.15914v1)|null|\n", "2401.15900": "|**2024-01-29**|**MV2MAE: Multi-View Video Masked Autoencoders**|MV2MAE\uff1a\u591a\u89c6\u56fe\u89c6\u9891\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668|Ketul Shah, Robert Crandall, Jie Xu, Peng Zhou, Marian George, Mayank Bansal, Rama Chellappa|Videos captured from multiple viewpoints can help in perceiving the 3D structure of the world and benefit computer vision tasks such as action recognition, tracking, etc. In this paper, we present a method for self-supervised learning from synchronized multi-view videos. We use a cross-view reconstruction task to inject geometry information in the model. Our approach is based on the masked autoencoder (MAE) framework. In addition to the same-view decoder, we introduce a separate cross-view decoder which leverages cross-attention mechanism to reconstruct a target viewpoint video using a video from source viewpoint, to help representations robust to viewpoint changes. For videos, static regions can be reconstructed trivially which hinders learning meaningful representations. To tackle this, we introduce a motion-weighted reconstruction loss which improves temporal modeling. We report state-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well as in the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets, demonstrating the robustness of our approach. Code will be made available.|\u4ece\u591a\u4e2a\u89c6\u70b9\u6355\u83b7\u7684\u89c6\u9891\u6709\u52a9\u4e8e\u611f\u77e5\u4e16\u754c\u7684 3D \u7ed3\u6784\uff0c\u5e76\u6709\u5229\u4e8e\u52a8\u4f5c\u8bc6\u522b\u3001\u8ddf\u8e2a\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u540c\u6b65\u591a\u89c6\u70b9\u89c6\u9891\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u4f7f\u7528\u8de8\u89c6\u56fe\u91cd\u5efa\u4efb\u52a1\u5728\u6a21\u578b\u4e2d\u6ce8\u5165\u51e0\u4f55\u4fe1\u606f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u5c4f\u853d\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u6846\u67b6\u3002\u9664\u4e86\u540c\u89c6\u56fe\u89e3\u7801\u5668\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5355\u72ec\u7684\u8de8\u89c6\u56fe\u89e3\u7801\u5668\uff0c\u5b83\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u4f7f\u7528\u6765\u81ea\u6e90\u89c6\u70b9\u7684\u89c6\u9891\u91cd\u5efa\u76ee\u6807\u89c6\u70b9\u89c6\u9891\uff0c\u4ee5\u5e2e\u52a9\u8868\u793a\u5bf9\u89c6\u70b9\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002\u5bf9\u4e8e\u89c6\u9891\uff0c\u9759\u6001\u533a\u57df\u53ef\u4ee5\u88ab\u7b80\u5355\u5730\u91cd\u5efa\uff0c\u8fd9\u963b\u788d\u4e86\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8fd0\u52a8\u52a0\u6743\u91cd\u5efa\u635f\u5931\uff0c\u5b83\u6539\u8fdb\u4e86\u65f6\u95f4\u5efa\u6a21\u3002\u6211\u4eec\u62a5\u544a\u4e86 NTU-60\u3001NTU-120 \u548c ETRI \u6570\u636e\u96c6\u4ee5\u53ca NUCLA\u3001PKU-MMD-II \u548c ROCOG-v2 \u6570\u636e\u96c6\u4e0a\u7684\u8fc1\u79fb\u5b66\u4e60\u8bbe\u7f6e\u7684\u6700\u65b0\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u7a33\u5065\u6027\u65b9\u6cd5\u3002\u4ee3\u7801\u5c06\u53ef\u7528\u3002|[2401.15900v1](http://arxiv.org/pdf/2401.15900v1)|null|\n", "2401.15896": "|**2024-01-29**|**$\\boldsymbol{M^2}$-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining**|$\\boldsymbol{M^2}$-Encoder\uff1a\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u6548\u9884\u8bad\u7ec3\u63a8\u8fdb\u53cc\u8bed\u56fe\u50cf\u6587\u672c\u7406\u89e3|Qingpei Guo, Furong Xu, Hanxiao Zhang, Wang Ren, Ziping Ma, Lin Ju, Jian Wang, Jingdong Chen, Ming Yang|Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced \"M-Square\"), set new benchmarks in both languages for multimodal retrieval and classification tasks. Notably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of 88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification setting, surpassing previously reported SoTA methods by 2.2% and 21.1%, respectively. The $M^2$-Encoder series represents one of the most comprehensive bilingual image-text foundation models to date, so we are making it available to the research community for further exploration and development.|\u50cf CLIP \u8fd9\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u76f8\u5bf9\u7a00\u7f3a\uff0c\u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\u7b49\u591a\u8bed\u8a00\u7684 VLM \u6a21\u578b\u5df2\u7ecf\u6ede\u540e\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7 60 \u4ebf\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u7efc\u5408\u53cc\u8bed\uff08\u4e2d\u82f1\uff09\u6570\u636e\u96c6 BM-6B\uff0c\u65e8\u5728\u589e\u5f3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4e24\u79cd\u8bed\u8a00\u7684\u56fe\u50cf\u3002\u4e3a\u4e86\u5904\u7406\u5982\u6b64\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u8ba1\u7b97\u5206\u7ec4\u805a\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u7740\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u548c GPU \u5185\u5b58\u9700\u6c42\uff0c\u4ece\u800c\u4f7f\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e86 60%\u3002\u6211\u4eec\u5728 BM-6B \u4e0a\u9884\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u589e\u5f3a\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\u7684\u53cc\u8bed\u56fe\u50cf\u6587\u672c\u57fa\u7840\u6a21\u578b\uff0c\u6240\u5f97\u6a21\u578b\u88ab\u79f0\u4e3a $M^2$-Encoders\uff08\u53d1\u97f3\u4e3a\u201cM-Square\u201d\uff09\uff0c\u5728\u4e24\u79cd\u8bed\u8a00\u90fd\u7528\u4e8e\u591a\u6a21\u5f0f\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u6700\u5927\u7684 $M^2$-Encoder-10B \u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u8bbe\u7f6e\u4e0b\u5728 ImageNet \u4e0a\u5b9e\u73b0\u4e86 88.5% \u7684 top-1 \u51c6\u786e\u7387\uff0c\u5728 ImageNet-CN \u4e0a\u5b9e\u73b0\u4e86 80.7% \u7684 top-1 \u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u62a5\u9053\u7684 SoTA \u65b9\u6cd5\u9ad8\u51fa 2.2% \u548c 21.1% \uff05\uff0c \u5206\u522b\u3002 $M^2$-Encoder \u7cfb\u5217\u4ee3\u8868\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5168\u9762\u7684\u53cc\u8bed\u56fe\u50cf\u6587\u672c\u57fa\u7840\u6a21\u578b\u4e4b\u4e00\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u5176\u63d0\u4f9b\u7ed9\u7814\u7a76\u793e\u533a\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\u548c\u5f00\u53d1\u3002|[2401.15896v1](http://arxiv.org/pdf/2401.15896v1)|null|\n", "2401.15886": "|**2024-01-29**|**Grey Level Texture Features for Segmentation of Chromogenic Dye RNAscope From Breast Cancer Tissue**|\u7528\u4e8e\u4e73\u817a\u764c\u7ec4\u7ec7\u663e\u8272\u67d3\u6599 RNAscope \u5206\u5272\u7684\u7070\u5ea6\u7eb9\u7406\u7279\u5f81|Andrew Davidson, Arthur Morley-Bunker, George Wiggins, Logan Walker, Gavin Harris, Ramakrishnan Mukundan, kConFab Investigators|Chromogenic RNAscope dye and haematoxylin staining of cancer tissue facilitates diagnosis of the cancer type and subsequent treatment, and fits well into existing pathology workflows. However, manual quantification of the RNAscope transcripts (dots), which signify gene expression, is prohibitively time consuming. In addition, there is a lack of verified supporting methods for quantification and analysis. This paper investigates the usefulness of gray level texture features for automatically segmenting and classifying the positions of RNAscope transcripts from breast cancer tissue. Feature analysis showed that a small set of gray level features, including Gray Level Dependence Matrix and Neighbouring Gray Tone Difference Matrix features, were well suited for the task. The automated method performed similarly to expert annotators at identifying the positions of RNAscope transcripts, with an F1-score of 0.571 compared to the expert inter-rater F1-score of 0.596. These results demonstrate the potential of gray level texture features for automated quantification of RNAscope in the pathology workflow.|\u764c\u75c7\u7ec4\u7ec7\u7684\u663e\u8272 RNAscope \u67d3\u6599\u548c\u82cf\u6728\u7cbe\u67d3\u8272\u6709\u52a9\u4e8e\u764c\u75c7\u7c7b\u578b\u7684\u8bca\u65ad\u548c\u540e\u7eed\u6cbb\u7597\uff0c\u5e76\u4e14\u975e\u5e38\u9002\u5408\u73b0\u6709\u7684\u75c5\u7406\u5b66\u5de5\u4f5c\u6d41\u7a0b\u3002\u7136\u800c\uff0c\u5bf9\u4ee3\u8868\u57fa\u56e0\u8868\u8fbe\u7684 RNAscope \u8f6c\u5f55\u672c\uff08\u70b9\uff09\u8fdb\u884c\u624b\u52a8\u5b9a\u91cf\u975e\u5e38\u8017\u65f6\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u91cf\u5316\u548c\u5206\u6790\u652f\u6301\u65b9\u6cd5\u3002\u672c\u6587\u7814\u7a76\u4e86\u7070\u5ea6\u7eb9\u7406\u7279\u5f81\u5bf9\u4e8e\u81ea\u52a8\u5206\u5272\u548c\u5206\u7c7b\u4e73\u817a\u764c\u7ec4\u7ec7\u4e2d RNAscope \u8f6c\u5f55\u672c\u4f4d\u7f6e\u7684\u6709\u7528\u6027\u3002\u7279\u5f81\u5206\u6790\u8868\u660e\uff0c\u4e00\u5c0f\u7ec4\u7070\u5ea6\u7279\u5f81\uff08\u5305\u62ec\u7070\u5ea6\u4f9d\u8d56\u77e9\u9635\u548c\u76f8\u90bb\u7070\u5ea6\u8272\u8c03\u5dee\u5f02\u77e9\u9635\u7279\u5f81\uff09\u975e\u5e38\u9002\u5408\u8be5\u4efb\u52a1\u3002\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u8bc6\u522b RNAscope \u8f6c\u5f55\u672c\u4f4d\u7f6e\u65b9\u9762\u4e0e\u4e13\u5bb6\u6ce8\u91ca\u8005\u7c7b\u4f3c\uff0cF1 \u5f97\u5206\u4e3a 0.571\uff0c\u800c\u4e13\u5bb6\u8bc4\u5206\u8005\u95f4 F1 \u5f97\u5206\u4e3a 0.596\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u7070\u5ea6\u7eb9\u7406\u7279\u5f81\u5728\u75c5\u7406\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u81ea\u52a8\u5b9a\u91cf RNAscope \u7684\u6f5c\u529b\u3002|[2401.15886v1](http://arxiv.org/pdf/2401.15886v1)|null|\n", "2401.15885": "|**2024-01-29**|**Rectify the Regression Bias in Long-Tailed Object Detection**|\u7ea0\u6b63\u957f\u5c3e\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u56de\u5f52\u504f\u5dee|Ke Zhu, Minghao Fu, Jie Shao, Tianyu Liu, Jianxin Wu|Long-tailed object detection faces great challenges because of its extremely imbalanced class distribution. Recent methods mainly focus on the classification bias and its loss function design, while ignoring the subtle influence of the regression branch. This paper shows that the regression bias exists and does adversely and seriously impact the detection accuracy. While existing methods fail to handle the regression bias, the class-specific regression head for rare classes is hypothesized to be the main cause of it in this paper. As a result, three kinds of viable solutions to cater for the rare categories are proposed, including adding a class-agnostic branch, clustering heads and merging heads. The proposed methods brings in consistent and significant improvements over existing long-tailed detection methods, especially in rare and common classes. The proposed method achieves state-of-the-art performance in the large vocabulary LVIS dataset with different backbones and architectures. It generalizes well to more difficult evaluation metrics, relatively balanced datasets, and the mask branch. This is the first attempt to reveal and explore rectifying of the regression bias in long-tailed object detection.|\u957f\u5c3e\u76ee\u6807\u68c0\u6d4b\u56e0\u5176\u7c7b\u522b\u5206\u5e03\u6781\u4e0d\u5e73\u8861\u800c\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u504f\u5dee\u53ca\u5176\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u800c\u5ffd\u7565\u4e86\u56de\u5f52\u5206\u652f\u7684\u5fae\u5999\u5f71\u54cd\u3002\u672c\u6587\u8868\u660e\u56de\u5f52\u504f\u5dee\u7684\u5b58\u5728\u5e76\u4e14\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u4ea7\u751f\u4e0d\u5229\u4e14\u4e25\u91cd\u7684\u5f71\u54cd\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u56de\u5f52\u504f\u5dee\uff0c\u4f46\u672c\u6587\u5047\u8bbe\u7a00\u6709\u7c7b\u522b\u7684\u7279\u5b9a\u7c7b\u522b\u56de\u5f52\u5934\u662f\u5176\u4e3b\u8981\u539f\u56e0\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u9488\u5bf9\u7a00\u6709\u7c7b\u522b\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6dfb\u52a0\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u5206\u652f\u3001\u805a\u7c7b\u5934\u548c\u5408\u5e76\u5934\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684\u957f\u5c3e\u68c0\u6d4b\u65b9\u6cd5\u5e26\u6765\u4e86\u4e00\u81f4\u4e14\u663e\u7740\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u548c\u5e38\u89c1\u7c7b\u522b\u4e2d\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u4e0d\u540c\u9aa8\u5e72\u548c\u67b6\u6784\u7684\u5927\u8bcd\u6c47\u91cf LVIS \u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u66f4\u56f0\u96be\u7684\u8bc4\u4f30\u6307\u6807\u3001\u76f8\u5bf9\u5e73\u8861\u7684\u6570\u636e\u96c6\u548c\u63a9\u6a21\u5206\u652f\u3002\u8fd9\u662f\u63ed\u793a\u548c\u63a2\u7d22\u7ea0\u6b63\u957f\u5c3e\u76ee\u6807\u68c0\u6d4b\u4e2d\u56de\u5f52\u504f\u5dee\u7684\u9996\u6b21\u5c1d\u8bd5\u3002|[2401.15885v1](http://arxiv.org/pdf/2401.15885v1)|null|\n", "2401.15875": "|**2024-01-29**|**Combining Satellite and Weather Data for Crop Type Mapping: An Inverse Modelling Approach**|\u7ed3\u5408\u536b\u661f\u548c\u5929\u6c14\u6570\u636e\u8fdb\u884c\u4f5c\u7269\u7c7b\u578b\u7ed8\u56fe\uff1a\u9006\u5411\u5efa\u6a21\u65b9\u6cd5|Praveen Ravirathinam, Rahul Ghosh, Ankush Khandelwal, Xiaowei Jia, David Mulla, Vipin Kumar|Accurate and timely crop mapping is essential for yield estimation, insurance claims, and conservation efforts. Over the years, many successful machine learning models for crop mapping have been developed that use just the multi-spectral imagery from satellites to predict crop type over the area of interest. However, these traditional methods do not account for the physical processes that govern crop growth. At a high level, crop growth can be envisioned as physical parameters, such as weather and soil type, acting upon the plant leading to crop growth which can be observed via satellites. In this paper, we propose Weather-based Spatio-Temporal segmentation network with ATTention (WSTATT), a deep learning model that leverages this understanding of crop growth by formulating it as an inverse model that combines weather (Daymet) and satellite imagery (Sentinel-2) to generate accurate crop maps. We show that our approach provides significant improvements over existing algorithms that solely rely on spectral imagery by comparing segmentation maps and F1 classification scores. Furthermore, effective use of attention in WSTATT architecture enables detection of crop types earlier in the season (up to 5 months in advance), which is very useful for improving food supply projections. We finally discuss the impact of weather by correlating our results with crop phenology to show that WSTATT is able to capture physical properties of crop growth.|\u51c6\u786e\u53ca\u65f6\u7684\u4f5c\u7269\u6d4b\u7ed8\u5bf9\u4e8e\u4ea7\u91cf\u4f30\u7b97\u3001\u4fdd\u9669\u7d22\u8d54\u548c\u4fdd\u62a4\u5de5\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u591a\u5e74\u6765\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u6210\u529f\u7684\u4f5c\u7269\u6d4b\u7ed8\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ec5\u4f7f\u7528\u536b\u661f\u7684\u591a\u5149\u8c31\u56fe\u50cf\u6765\u9884\u6d4b\u611f\u5174\u8da3\u533a\u57df\u7684\u4f5c\u7269\u7c7b\u578b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4f20\u7edf\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u63a7\u5236\u4f5c\u7269\u751f\u957f\u7684\u7269\u7406\u8fc7\u7a0b\u3002\u5728\u8f83\u9ad8\u5c42\u9762\u4e0a\uff0c\u4f5c\u7269\u751f\u957f\u53ef\u4ee5\u88ab\u8bbe\u60f3\u4e3a\u7269\u7406\u53c2\u6570\uff0c\u4f8b\u5982\u5929\u6c14\u548c\u571f\u58e4\u7c7b\u578b\uff0c\u4f5c\u7528\u4e8e\u690d\u7269\u5bfc\u81f4\u4f5c\u7269\u751f\u957f\uff0c\u53ef\u4ee5\u901a\u8fc7\u536b\u661f\u89c2\u5bdf\u5230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5929\u6c14\u7684\u65f6\u7a7a\u5206\u5272\u7f51\u7edc\uff08WSTATT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5176\u5236\u5b9a\u4e3a\u7ed3\u5408\u5929\u6c14\uff08Daymet\uff09\u548c\u536b\u661f\u56fe\u50cf\uff08Sentinel-\uff09\u7684\u9006\u6a21\u578b\u6765\u5229\u7528\u5bf9\u4f5c\u7269\u751f\u957f\u7684\u7406\u89e3\u3002 2\uff09\u751f\u6210\u51c6\u786e\u7684\u4f5c\u7269\u56fe\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u5272\u56fe\u548c F1 \u5206\u7c7b\u5206\u6570\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u4ec5\u4f9d\u8d56\u5149\u8c31\u56fe\u50cf\u7684\u73b0\u6709\u7b97\u6cd5\u63d0\u4f9b\u4e86\u663e\u7740\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u5728 WSTATT \u67b6\u6784\u4e2d\u6709\u6548\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u5728\u5b63\u8282\u65e9\u671f\uff08\u6700\u591a\u63d0\u524d 5 \u4e2a\u6708\uff09\u68c0\u6d4b\u4f5c\u7269\u7c7b\u578b\uff0c\u8fd9\u5bf9\u4e8e\u6539\u5584\u7cae\u98df\u4f9b\u5e94\u9884\u6d4b\u975e\u5e38\u6709\u7528\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u6211\u4eec\u7684\u7ed3\u679c\u4e0e\u4f5c\u7269\u7269\u5019\u5b66\u76f8\u5173\u8054\u6765\u8ba8\u8bba\u5929\u6c14\u7684\u5f71\u54cd\uff0c\u4ee5\u8868\u660e WSTATT \u80fd\u591f\u6355\u83b7\u4f5c\u7269\u751f\u957f\u7684\u7269\u7406\u7279\u6027\u3002|[2401.15875v1](http://arxiv.org/pdf/2401.15875v1)|null|\n", "2401.15865": "|**2024-01-29**|**LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection**|LiDAR-PTQ\uff1a\u70b9\u4e91 3D \u7269\u4f53\u68c0\u6d4b\u7684\u8bad\u7ec3\u540e\u91cf\u5316|Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu Zhao, Xiaobo Lu, Xiangxiang Chu|Due to highly constrained computing power and memory, deploying 3D lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization (PTQ) has been widely adopted in 2D vision tasks. However, applying it directly to 3D lidar-based tasks inevitably leads to performance degradation. As a remedy, we propose an effective PTQ method called LiDAR-PTQ, which is particularly curated for 3D lidar detection (both SPConv-based and SPConv-free). Our LiDAR-PTQ features three main components, \\textbf{(1)} a sparsity-based calibration method to determine the initialization of quantization parameters, \\textbf{(2)} a Task-guided Global Positive Loss (TGPL) to reduce the disparity between the final predictions before and after quantization, \\textbf{(3)} an adaptive rounding-to-nearest operation to minimize the layerwise reconstruction error. Extensive experiments demonstrate that our LiDAR-PTQ can achieve state-of-the-art quantization performance when applied to CenterPoint (both Pillar-based and Voxel-based). To our knowledge, for the very first time in lidar-based 3D detection tasks, the PTQ INT8 model's accuracy is almost the same as the FP32 model while enjoying $3\\times$ inference speedup. Moreover, our LiDAR-PTQ is cost-effective being $30\\times$ faster than the quantization-aware training method. Code will be released at \\url{https://github.com/StiphyJay/LiDAR-PTQ}.|\u7531\u4e8e\u8ba1\u7b97\u80fd\u529b\u548c\u5185\u5b58\u9ad8\u5ea6\u6709\u9650\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u673a\u5668\u4eba\u914d\u5907\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u57fa\u4e8e 3D \u6fc0\u5149\u96f7\u8fbe\u7684\u63a2\u6d4b\u5668\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\u3002\u4f5c\u4e3a\u4e00\u79cd\u65b9\u4fbf\u3001\u76f4\u63a5\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u5df2\u5728 2D \u89c6\u89c9\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u91c7\u7528\u3002\u7136\u800c\uff0c\u5c06\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e 3D \u6fc0\u5149\u96f7\u8fbe\u7684\u4efb\u52a1\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4f5c\u4e3a\u8865\u6551\u63aa\u65bd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LiDAR-PTQ \u7684\u6709\u6548 PTQ \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e13\u95e8\u7528\u4e8e 3D \u6fc0\u5149\u96f7\u8fbe\u68c0\u6d4b\uff08\u57fa\u4e8e SPConv \u548c\u65e0 SPConv\uff09\u3002\u6211\u4eec\u7684 LiDAR-PTQ \u5177\u6709\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff0c\\textbf{(1)} \u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u6027\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u91cf\u5316\u53c2\u6570\u7684\u521d\u59cb\u5316\uff0c\\textbf{(2)} \u662f\u4e00\u79cd\u4efb\u52a1\u5f15\u5bfc\u7684\u5168\u5c40\u6b63\u635f\u5931 (TGPL)\uff0c\u7528\u4e8e\u51cf\u5c11\u91cf\u5316\u524d\u540e\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c \\textbf{(3)} \u662f\u4e00\u79cd\u81ea\u9002\u5e94\u820d\u5165\u5230\u6700\u8fd1\u7684\u64cd\u4f5c\uff0c\u4ee5\u6700\u5c0f\u5316\u5206\u5c42\u91cd\u5efa\u8bef\u5dee\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 LiDAR-PTQ \u5728\u5e94\u7528\u4e8e CenterPoint\uff08\u57fa\u4e8e Pillar \u548c\u57fa\u4e8e\u4f53\u7d20\uff09\u65f6\u53ef\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u91cf\u5316\u6027\u80fd\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5728\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684 3D \u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cPTQ INT8 \u6a21\u578b\u7684\u51c6\u786e\u6027\u51e0\u4e4e\u4e0e FP32 \u6a21\u578b\u76f8\u540c\uff0c\u540c\u65f6\u4eab\u53d7 3 \u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684 LiDAR-PTQ \u5177\u6709\u6210\u672c\u6548\u76ca\uff0c\u6bd4\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u5feb 30 \u7f8e\u5143\u3002\u4ee3\u7801\u5c06\u5728 \\url{https://github.com/StiphyJay/LiDAR-PTQ} \u53d1\u5e03\u3002|[2401.15865v1](http://arxiv.org/pdf/2401.15865v1)|null|\n", "2401.15863": "|**2024-01-29**|**Importance-Aware Adaptive Dataset Distillation**|\u91cd\u8981\u6027\u611f\u77e5\u81ea\u9002\u5e94\u6570\u636e\u96c6\u84b8\u998f|Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama|Herein, we propose a novel dataset distillation method for constructing small informative datasets that preserve the information of the large original datasets. The development of deep learning models is enabled by the availability of large-scale datasets. Despite unprecedented success, large-scale datasets considerably increase the storage and transmission costs, resulting in a cumbersome model training process. Moreover, using raw data for training raises privacy and copyright concerns. To address these issues, a new task named dataset distillation has been introduced, aiming to synthesize a compact dataset that retains the essential information from the large original dataset. State-of-the-art (SOTA) dataset distillation methods have been proposed by matching gradients or network parameters obtained during training on real and synthetic datasets. The contribution of different network parameters to the distillation process varies, and uniformly treating them leads to degraded distillation performance. Based on this observation, we propose an importance-aware adaptive dataset distillation (IADD) method that can improve distillation performance by automatically assigning importance weights to different network parameters during distillation, thereby synthesizing more robust distilled datasets. IADD demonstrates superior performance over other SOTA dataset distillation methods based on parameter matching on multiple benchmark datasets and outperforms them in terms of cross-architecture generalization. In addition, the analysis of self-adaptive weights demonstrates the effectiveness of IADD. Furthermore, the effectiveness of IADD is validated in a real-world medical application such as COVID-19 detection.|\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u4fdd\u7559\u5927\u578b\u539f\u59cb\u6570\u636e\u96c6\u4fe1\u606f\u7684\u5c0f\u578b\u4fe1\u606f\u6570\u636e\u96c6\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u662f\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u6765\u5b9e\u73b0\u7684\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u6210\u529f\uff0c\u4f46\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5927\u5927\u589e\u52a0\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\uff0c\u5bfc\u81f4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u53d8\u5f97\u7e41\u7410\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u539f\u59cb\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4f1a\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u9879\u540d\u4e3a\u6570\u636e\u96c6\u84b8\u998f\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u5408\u6210\u4e00\u4e2a\u7d27\u51d1\u7684\u6570\u636e\u96c6\uff0c\u4fdd\u7559\u5927\u578b\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u57fa\u672c\u4fe1\u606f\u3002\u901a\u8fc7\u5339\u914d\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u671f\u95f4\u83b7\u5f97\u7684\u68af\u5ea6\u6216\u7f51\u7edc\u53c2\u6570\uff0c\u63d0\u51fa\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u3002\u4e0d\u540c\u7684\u7f51\u7edc\u53c2\u6570\u5bf9\u84b8\u998f\u8fc7\u7a0b\u7684\u8d21\u732e\u5404\u4e0d\u76f8\u540c\uff0c\u7edf\u4e00\u5904\u7406\u5b83\u4eec\u4f1a\u5bfc\u81f4\u84b8\u998f\u6027\u80fd\u4e0b\u964d\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u8981\u6027\u611f\u77e5\u81ea\u9002\u5e94\u6570\u636e\u96c6\u84b8\u998f\uff08IADD\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u4e3a\u4e0d\u540c\u7f51\u7edc\u53c2\u6570\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\u6765\u63d0\u9ad8\u84b8\u998f\u6027\u80fd\uff0c\u4ece\u800c\u5408\u6210\u66f4\u7a33\u5065\u7684\u84b8\u998f\u6570\u636e\u96c6\u3002 IADD \u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u53c2\u6570\u5339\u914d\u7684 SOTA \u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8de8\u67b6\u6784\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u81ea\u9002\u5e94\u6743\u91cd\u7684\u5206\u6790\u8bc1\u660e\u4e86IADD\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cIADD \u7684\u6709\u6548\u6027\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u5e94\u7528\uff08\u4f8b\u5982 COVID-19 \u68c0\u6d4b\uff09\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002|[2401.15863v1](http://arxiv.org/pdf/2401.15863v1)|null|\n", "2401.15859": "|**2024-01-29**|**Diffusion Facial Forgery Detection**|\u6269\u6563\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b|Harry Cheng, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli|Detecting diffusion-generated images has recently grown into an emerging research area. Existing diffusion-based datasets predominantly focus on general image generation. However, facial forgeries, which pose a more severe social risk, have remained less explored thus far. To address this gap, this paper introduces DiFF, a comprehensive dataset dedicated to face-focused diffusion-generated images. DiFF comprises over 500,000 images that are synthesized using thirteen distinct generation methods under four conditions. In particular, this dataset leverages 30,000 carefully collected textual and visual prompts, ensuring the synthesis of images with both high fidelity and semantic consistency. We conduct extensive experiments on the DiFF dataset via a human test and several representative forgery detection methods. The results demonstrate that the binary detection accuracy of both human observers and automated detectors often falls below 30%, shedding light on the challenges in detecting diffusion-generated facial forgeries. Furthermore, we propose an edge graph regularization approach to effectively enhance the generalization capability of existing detectors.|\u68c0\u6d4b\u6269\u6563\u751f\u6210\u7684\u56fe\u50cf\u6700\u8fd1\u5df2\u53d1\u5c55\u6210\u4e3a\u4e00\u4e2a\u65b0\u5174\u7684\u7814\u7a76\u9886\u57df\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4e00\u822c\u56fe\u50cf\u751f\u6210\u3002\u7136\u800c\uff0c\u8fc4\u4eca\u4e3a\u6b62\uff0c\u5bf9\u6784\u6210\u66f4\u4e25\u91cd\u793e\u4f1a\u98ce\u9669\u7684\u9762\u90e8\u4f2a\u9020\u7684\u7814\u7a76\u4ecd\u7136\u8f83\u5c11\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u6587\u5f15\u5165\u4e86 DiFF\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u7528\u4e8e\u4ee5\u4eba\u8138\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\u751f\u6210\u56fe\u50cf\u7684\u7efc\u5408\u6570\u636e\u96c6\u3002 DiFF \u5305\u542b\u8d85\u8fc7 500,000 \u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u5728\u56db\u79cd\u6761\u4ef6\u4e0b\u4f7f\u7528 13 \u79cd\u4e0d\u540c\u7684\u751f\u6210\u65b9\u6cd5\u5408\u6210\u7684\u3002\u7279\u522b\u662f\uff0c\u8be5\u6570\u636e\u96c6\u5229\u7528\u4e86 30,000 \u4e2a\u7cbe\u5fc3\u6536\u96c6\u7684\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u786e\u4fdd\u56fe\u50cf\u7684\u5408\u6210\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u6211\u4eec\u901a\u8fc7\u4eba\u4f53\u6d4b\u8bd5\u548c\u51e0\u79cd\u4ee3\u8868\u6027\u7684\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5bf9 DiFF \u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u89c2\u5bdf\u8005\u548c\u81ea\u52a8\u68c0\u6d4b\u5668\u7684\u4e8c\u8fdb\u5236\u68c0\u6d4b\u7cbe\u5ea6\u901a\u5e38\u4f4e\u4e8e 30%\uff0c\u63ed\u793a\u4e86\u68c0\u6d4b\u6269\u6563\u751f\u6210\u7684\u9762\u90e8\u4f2a\u9020\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u56fe\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u6709\u6548\u589e\u5f3a\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2401.15859v1](http://arxiv.org/pdf/2401.15859v1)|null|\n", "2401.15842": "|**2024-01-29**|**LCVO: An Efficient Pretraining-Free Framework for Visual Question Answering Grounding**|LCVO\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u514d\u9884\u8bad\u7ec3\u89c6\u89c9\u95ee\u7b54\u57fa\u7840\u6846\u67b6|Yuhan Chen, Lumei Su, Lihua Chen, Zhiwei Lin|In this paper, the LCVO modular method is proposed for the Visual Question Answering (VQA) Grounding task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf Open-Vocabulary Object Detection (OVD) model, where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCVO establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models, exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources, evaluating the proposed method's performance on benchmark datasets including GQA, CLEVR, and VizWiz-VQA-Grounding. Comparative analyses with baseline methods demonstrate the robust competitiveness of LCVO.|\u672c\u6587\u63d0\u51fa\u4e86 LCVO \u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u591a\u6a21\u6001\u9886\u57df\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u57fa\u7840\u4efb\u52a1\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u73b0\u6210\u7684 VQA \u6a21\u578b\u548c\u73b0\u6210\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b (OVD) \u6a21\u578b\u4e4b\u95f4\u7684\u4e2d\u95f4\u4e2d\u4ecb\uff0c\u5176\u4e2d LLM \u5728 VQA \u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b (OVD) \u6a21\u578b\u4e4b\u95f4\u8f6c\u6362\u548c\u4f20\u9012\u6587\u672c\u4fe1\u606f\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u57fa\u4e8e\u8bbe\u8ba1\u7684\u63d0\u793a\u3002 LCVO\u5efa\u7acb\u4e86\u4e00\u4e2a\u96c6\u6210\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u65e0\u9700\u4efb\u4f55\u9884\u8bad\u7ec3\u8fc7\u7a0b\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u4f4e\u8ba1\u7b97\u8d44\u6e90\u4e0b\u90e8\u7f72\u7528\u4e8e VQA Grounding \u4efb\u52a1\u3002\u6846\u67b6\u5185\u7684\u6a21\u5757\u5316\u6a21\u578b\u5141\u8bb8\u5e94\u7528\u5404\u79cd\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u4e0e\u65f6\u4ff1\u8fdb\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5b9e\u9a8c\u5b9e\u73b0\u662f\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7684\uff0c\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 GQA\u3001CLEVR \u548c VizWiz-VQA-Grounding \u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6bd4\u8f83\u5206\u6790\u8bc1\u660e\u4e86 LCVO \u5f3a\u5927\u7684\u7ade\u4e89\u529b\u3002|[2401.15842v1](http://arxiv.org/pdf/2401.15842v1)|null|\n", "2401.15834": "|**2024-01-29**|**Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes**|\u8d8a\u6765\u8d8a\u5c11\uff1a\u4f7f\u7528\u66f4\u5c11\u7684\u57fa\u7c7b\u4ece\u5f88\u5c11\u7684\u793a\u4f8b\u4e2d\u66f4\u597d\u5730\u5b66\u4e60|Raphael Lafargue, Yassir Bendou, Bastien Pasdeloup, Jean-Philippe Diguet, Ian Reid, Vincent Gripon, Jack Valmadre|When training data is scarce, it is common to make use of a feature extractor that has been pre-trained on a large base dataset, either by fine-tuning its parameters on the ``target'' dataset or by directly adopting its representation as features for a simple classifier. Fine-tuning is ineffective for few-shot learning, since the target dataset contains only a handful of examples. However, directly adopting the features without fine-tuning relies on the base and target distributions being similar enough that these features achieve separability and generalization. This paper investigates whether better features for the target dataset can be obtained by training on fewer base classes, seeking to identify a more useful base dataset for a given task.We consider cross-domain few-shot image classification in eight different domains from Meta-Dataset and entertain multiple real-world settings (domain-informed, task-informed and uninformed) where progressively less detail is known about the target task. To our knowledge, this is the first demonstration that fine-tuning on a subset of carefully selected base classes can significantly improve few-shot learning. Our contributions are simple and intuitive methods that can be implemented in any few-shot solution. We also give insights into the conditions in which these solutions are likely to provide a boost in accuracy. We release the code to reproduce all experiments from this paper on GitHub. https://github.com/RafLaf/Few-and-Fewer.git|\u5f53\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u901a\u5e38\u4f7f\u7528\u5728\u5927\u578b\u57fa\u7840\u6570\u636e\u96c6\u4e0a\u9884\u5148\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u6216\u8005\u901a\u8fc7\u5728\u201c\u76ee\u6807\u201d\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5176\u53c2\u6570\uff0c\u6216\u8005\u76f4\u63a5\u91c7\u7528\u5176\u8868\u793a\u5f62\u5f0f\u7b80\u5355\u5206\u7c7b\u5668\u7684\u7279\u5f81\u3002\u5fae\u8c03\u5bf9\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u662f\u65e0\u6548\u7684\uff0c\u56e0\u4e3a\u76ee\u6807\u6570\u636e\u96c6\u4ec5\u5305\u542b\u5c11\u6570\u793a\u4f8b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u91c7\u7528\u7279\u5f81\u800c\u4e0d\u8fdb\u884c\u5fae\u8c03\u4f9d\u8d56\u4e8e\u57fa\u7840\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u8db3\u591f\u76f8\u4f3c\u4ee5\u4f7f\u8fd9\u4e9b\u7279\u5f81\u5b9e\u73b0\u53ef\u5206\u79bb\u6027\u548c\u6cdb\u5316\u6027\u3002\u672c\u6587\u7814\u7a76\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5728\u66f4\u5c11\u7684\u57fa\u7c7b\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u83b7\u5f97\u66f4\u597d\u7684\u76ee\u6807\u6570\u636e\u96c6\u7279\u5f81\uff0c\u5bfb\u6c42\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u8bc6\u522b\u66f4\u6709\u7528\u7684\u57fa\u6570\u636e\u96c6\u3002\u6211\u4eec\u8003\u8651\u6765\u81ea\u5143\u7684\u516b\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u8de8\u57df\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u3002\u6570\u636e\u96c6\u548c\u5a31\u4e50\u591a\u4e2a\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\uff08\u9886\u57df\u901a\u77e5\u3001\u4efb\u52a1\u901a\u77e5\u548c\u975e\u901a\u77e5\uff09\uff0c\u5176\u4e2d\u6709\u5173\u76ee\u6807\u4efb\u52a1\u7684\u8be6\u7ec6\u4fe1\u606f\u9010\u6e10\u51cf\u5c11\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u6b21\u8bc1\u660e\u5bf9\u7cbe\u5fc3\u9009\u62e9\u7684\u57fa\u7c7b\u5b50\u96c6\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u663e\u7740\u6539\u5584\u5c0f\u6837\u672c\u5b66\u4e60\u3002\u6211\u4eec\u7684\u8d21\u732e\u662f\u7b80\u5355\u76f4\u89c2\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u5c11\u6570\u89e3\u51b3\u65b9\u6848\u4e2d\u5b9e\u73b0\u3002\u6211\u4eec\u8fd8\u6df1\u5165\u4e86\u89e3\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u6761\u4ef6\u3002\u6211\u4eec\u5728 GitHub \u4e0a\u53d1\u5e03\u4e86\u91cd\u73b0\u672c\u6587\u6240\u6709\u5b9e\u9a8c\u7684\u4ee3\u7801\u3002 https://github.com/RafLaf/Few-and-Fewer.git|[2401.15834v1](http://arxiv.org/pdf/2401.15834v1)|null|\n", "2401.15820": "|**2024-01-29**|**Knowledge-Aware Neuron Interpretation for Scene Classification**|\u7528\u4e8e\u573a\u666f\u5206\u7c7b\u7684\u77e5\u8bc6\u611f\u77e5\u795e\u7ecf\u5143\u89e3\u91ca|Yong Guan, Freddy Lecue, Jiaoyan Chen, Ru Li, Jeff Z. Pan|Although neural models have achieved remarkable performance, they still encounter doubts due to the intransparency. To this end, model prediction explanation is attracting more and more attentions. However, current methods rarely incorporate external knowledge and still suffer from three limitations: (1) Neglecting concept completeness. Merely selecting concepts may not sufficient for prediction. (2) Lacking concept fusion. Failure to merge semantically-equivalent concepts. (3) Difficult in manipulating model behavior. Lack of verification for explanation on original model. To address these issues, we propose a novel knowledge-aware neuron interpretation framework to explain model predictions for image scene classification. Specifically, for concept completeness, we present core concepts of a scene based on knowledge graph, ConceptNet, to gauge the completeness of concepts. Our method, incorporating complete concepts, effectively provides better prediction explanations compared to baselines. Furthermore, for concept fusion, we introduce a knowledge graph-based method known as Concept Filtering, which produces over 23% point gain on neuron behaviors for neuron interpretation. At last, we propose Model Manipulation, which aims to study whether the core concepts based on ConceptNet could be employed to manipulate model behavior. The results show that core concepts can effectively improve the performance of original model by over 26%.|\u5c3d\u7ba1\u795e\u7ecf\u6a21\u578b\u5df2\u7ecf\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u4e0d\u900f\u660e\uff0c\u5b83\u4eec\u4ecd\u7136\u53d7\u5230\u8d28\u7591\u3002\u4e3a\u6b64\uff0c\u6a21\u578b\u9884\u6d4b\u89e3\u91ca\u8d8a\u6765\u8d8a\u53d7\u5230\u4eba\u4eec\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5f88\u5c11\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u5e76\u4e14\u4ecd\u7136\u5b58\u5728\u4e09\u4e2a\u5c40\u9650\u6027\uff1a\uff081\uff09\u5ffd\u89c6\u6982\u5ff5\u5b8c\u6574\u6027\u3002\u4ec5\u4ec5\u9009\u62e9\u6982\u5ff5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8fdb\u884c\u9884\u6d4b\u3002 (2)\u7f3a\u4e4f\u6982\u5ff5\u878d\u5408\u3002\u672a\u80fd\u5408\u5e76\u8bed\u4e49\u7b49\u6548\u7684\u6982\u5ff5\u3002 (3)\u96be\u4ee5\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\u3002\u7f3a\u4e4f\u5bf9\u539f\u59cb\u6a21\u578b\u89e3\u91ca\u7684\u9a8c\u8bc1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u611f\u77e5\u795e\u7ecf\u5143\u89e3\u91ca\u6846\u67b6\u6765\u89e3\u91ca\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u7684\u6a21\u578b\u9884\u6d4b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u6982\u5ff5\u5b8c\u6574\u6027\uff0c\u6211\u4eec\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u3001ConceptNet \u63d0\u51fa\u573a\u666f\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u4ee5\u8861\u91cf\u6982\u5ff5\u7684\u5b8c\u6574\u6027\u3002\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u5b8c\u6574\u7684\u6982\u5ff5\uff0c\u6709\u6548\u5730\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6982\u5ff5\u878d\u5408\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6982\u5ff5\u8fc7\u6ee4\uff0c\u5b83\u53ef\u4ee5\u4f7f\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u795e\u7ecf\u5143\u884c\u4e3a\u83b7\u5f97\u8d85\u8fc7 23% \u7684\u589e\u76ca\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6a21\u578b\u64cd\u7eb5\uff0c\u65e8\u5728\u7814\u7a76\u57fa\u4e8eConceptNet\u7684\u6838\u5fc3\u6982\u5ff5\u662f\u5426\u53ef\u4ee5\u7528\u4e8e\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6838\u5fc3\u6982\u5ff5\u53ef\u6709\u6548\u5c06\u539f\u6a21\u578b\u6027\u80fd\u63d0\u534726%\u4ee5\u4e0a\u3002|[2401.15820v1](http://arxiv.org/pdf/2401.15820v1)|null|\n", "2401.15817": "|**2024-01-29**|**Transparency Attacks: How Imperceptible Image Layers Can Fool AI Perception**|\u900f\u660e\u653b\u51fb\uff1a\u96be\u4ee5\u5bdf\u89c9\u7684\u56fe\u50cf\u5c42\u5982\u4f55\u6b3a\u9a97\u4eba\u5de5\u667a\u80fd\u611f\u77e5|Forrest McKee, David Noever|This paper investigates a novel algorithmic vulnerability when imperceptible image layers confound multiple vision models into arbitrary label assignments and captions. We explore image preprocessing methods to introduce stealth transparency, which triggers AI misinterpretation of what the human eye perceives. The research compiles a broad attack surface to investigate the consequences ranging from traditional watermarking, steganography, and background-foreground miscues. We demonstrate dataset poisoning using the attack to mislabel a collection of grayscale landscapes and logos using either a single attack layer or randomly selected poisoning classes. For example, a military tank to the human eye is a mislabeled bridge to object classifiers based on convolutional networks (YOLO, etc.) and vision transformers (ViT, GPT-Vision, etc.). A notable attack limitation stems from its dependency on the background (hidden) layer in grayscale as a rough match to the transparent foreground image that the human eye perceives. This dependency limits the practical success rate without manual tuning and exposes the hidden layers when placed on the opposite display theme (e.g., light background, light transparent foreground visible, works best against a light theme image viewer or browser). The stealth transparency confounds established vision systems, including evading facial recognition and surveillance, digital watermarking, content filtering, dataset curating, automotive and drone autonomy, forensic evidence tampering, and retail product misclassifying. This method stands in contrast to traditional adversarial attacks that typically focus on modifying pixel values in ways that are either slightly perceptible or entirely imperceptible for both humans and machines.|\u672c\u6587\u7814\u7a76\u4e86\u5f53\u96be\u4ee5\u5bdf\u89c9\u7684\u56fe\u50cf\u5c42\u5c06\u591a\u4e2a\u89c6\u89c9\u6a21\u578b\u6df7\u6dc6\u4e3a\u4efb\u610f\u6807\u7b7e\u5206\u914d\u548c\u6807\u9898\u65f6\u7684\u4e00\u79cd\u65b0\u7b97\u6cd5\u6f0f\u6d1e\u3002\u6211\u4eec\u63a2\u7d22\u56fe\u50cf\u9884\u5904\u7406\u65b9\u6cd5\u6765\u5f15\u5165\u9690\u5f62\u900f\u660e\u5ea6\uff0c\u8fd9\u4f1a\u5f15\u53d1\u4eba\u5de5\u667a\u80fd\u5bf9\u4eba\u773c\u611f\u77e5\u7684\u8bef\u89e3\u3002\u8be5\u7814\u7a76\u7f16\u5236\u4e86\u5e7f\u6cdb\u7684\u653b\u51fb\u9762\uff0c\u4ee5\u8c03\u67e5\u4f20\u7edf\u6c34\u5370\u3001\u9690\u5199\u672f\u548c\u80cc\u666f-\u524d\u666f\u9519\u8bef\u7b49\u540e\u679c\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u5355\u4e2a\u653b\u51fb\u5c42\u6216\u968f\u673a\u9009\u62e9\u7684\u4e2d\u6bd2\u7c7b\u522b\u6765\u9519\u8bef\u6807\u8bb0\u7070\u5ea6\u666f\u89c2\u548c\u5fbd\u6807\u96c6\u5408\u7684\u653b\u51fb\u6765\u6f14\u793a\u6570\u636e\u96c6\u4e2d\u6bd2\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4eba\u773c\u6765\u8bf4\uff0c\u519b\u7528\u5766\u514b\u662f\u901a\u5f80\u57fa\u4e8e\u5377\u79ef\u7f51\u7edc\uff08YOLO \u7b49\uff09\u548c\u89c6\u89c9\u8f6c\u6362\u5668\uff08ViT\u3001GPT-Vision \u7b49\uff09\u7684\u5bf9\u8c61\u5206\u7c7b\u5668\u7684\u9519\u8bef\u6807\u7b7e\u6865\u6881\u3002\u4e00\u4e2a\u663e\u7740\u7684\u653b\u51fb\u9650\u5236\u6e90\u4e8e\u5b83\u5bf9\u7070\u5ea6\u80cc\u666f\uff08\u9690\u85cf\uff09\u5c42\u7684\u4f9d\u8d56\uff0c\u4f5c\u4e3a\u4e0e\u4eba\u773c\u611f\u77e5\u7684\u900f\u660e\u524d\u666f\u56fe\u50cf\u7684\u7c97\u7565\u5339\u914d\u3002\u8fd9\u79cd\u4f9d\u8d56\u6027\u9650\u5236\u4e86\u65e0\u9700\u624b\u52a8\u8c03\u6574\u7684\u5b9e\u9645\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5f53\u653e\u7f6e\u5728\u76f8\u53cd\u7684\u663e\u793a\u4e3b\u9898\u4e0a\u65f6\u4f1a\u66b4\u9732\u9690\u85cf\u5c42\uff08\u4f8b\u5982\uff0c\u6d45\u8272\u80cc\u666f\u3001\u6d45\u8272\u900f\u660e\u524d\u666f\u53ef\u89c1\uff0c\u6700\u9002\u5408\u6d45\u8272\u4e3b\u9898\u56fe\u50cf\u67e5\u770b\u5668\u6216\u6d4f\u89c8\u5668\uff09\u3002\u9690\u5f62\u900f\u660e\u5ea6\u6df7\u6dc6\u4e86\u73b0\u6709\u7684\u89c6\u89c9\u7cfb\u7edf\uff0c\u5305\u62ec\u9003\u907f\u9762\u90e8\u8bc6\u522b\u548c\u76d1\u89c6\u3001\u6570\u5b57\u6c34\u5370\u3001\u5185\u5bb9\u8fc7\u6ee4\u3001\u6570\u636e\u96c6\u7ba1\u7406\u3001\u6c7d\u8f66\u548c\u65e0\u4eba\u673a\u81ea\u4e3b\u3001\u6cd5\u533b\u8bc1\u636e\u7be1\u6539\u4ee5\u53ca\u96f6\u552e\u4ea7\u54c1\u9519\u8bef\u5206\u7c7b\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u653b\u51fb\u901a\u5e38\u4fa7\u91cd\u4e8e\u4ee5\u4eba\u7c7b\u548c\u673a\u5668\u7a0d\u5fae\u53ef\u611f\u77e5\u6216\u5b8c\u5168\u4e0d\u53ef\u611f\u77e5\u7684\u65b9\u5f0f\u4fee\u6539\u50cf\u7d20\u503c\u3002|[2401.15817v1](http://arxiv.org/pdf/2401.15817v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.16144": "|**2024-01-29**|**Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields**|\u5206\u800c\u6cbb\u4e4b\uff1a\u91cd\u65b0\u601d\u8003\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u8bad\u7ec3\u8303\u5f0f|Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado|Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum.|\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5728\u5408\u6210 3D \u573a\u666f\u7684\u9ad8\u4fdd\u771f\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u6f5c\u529b\uff0c\u4f46 NeRF \u7684\u6807\u51c6\u8bad\u7ec3\u8303\u4f8b\u9884\u8bbe\u4e86\u8bad\u7ec3\u96c6\u4e2d\u7684\u6bcf\u4e2a\u56fe\u50cf\u5177\u6709\u540c\u7b49\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u5047\u8bbe\u5bf9\u6e32\u67d3\u5448\u73b0\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7684\u7279\u5b9a\u89c6\u56fe\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u4ece\u800c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ed4\u7ec6\u7814\u7a76\u4e86\u5f53\u524d\u8bad\u7ec3\u8303\u4f8b\u7684\u542b\u4e49\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u5b83\uff0c\u4ee5\u901a\u8fc7 NeRF \u83b7\u5f97\u66f4\u51fa\u8272\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u6839\u636e\u8f93\u5165\u89c6\u56fe\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\u5c06\u8f93\u5165\u89c6\u56fe\u5206\u4e3a\u591a\u4e2a\u7ec4\uff0c\u5e76\u5728\u6bcf\u4e2a\u7ec4\u4e0a\u8bad\u7ec3\u5355\u72ec\u7684\u6a21\u578b\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u578b\u80fd\u591f\u4e13\u95e8\u7814\u7a76\u7279\u5b9a\u533a\u57df\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u901f\u5ea6\u6216\u6548\u7387\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u4e13\u95e8\u6a21\u578b\u7684\u77e5\u8bc6\u901a\u8fc7\u5e08\u751f\u84b8\u998f\u8303\u4f8b\u805a\u5408\u6210\u5355\u4e2a\u5b9e\u4f53\uff0c\u4ece\u800c\u5b9e\u73b0\u5728\u7ebf\u6e32\u67d3\u7684\u7a7a\u95f4\u6548\u7387\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff08\u5373 NeRF \u5408\u6210\u6570\u636e\u96c6\u548c Tanks&Temples\uff09\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b0\u9896\u8bad\u7ec3\u6846\u67b6\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684 DaC \u8bad\u7ec3\u7ba1\u9053\u63d0\u9ad8\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6700\u4f4e\u6536\u655b\u6027\u3002|[2401.16144v1](http://arxiv.org/pdf/2401.16144v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.16224": "|**2024-01-29**|**Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models**|Diffutoon\uff1a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u53ef\u7f16\u8f91\u5361\u901a\u7740\u8272|Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang|Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \\textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/).|\u5361\u901a\u7740\u8272\u662f\u4e00\u79cd\u975e\u771f\u5b9e\u611f\u52a8\u753b\u6e32\u67d3\u4efb\u52a1\u3002\u5176\u4e3b\u8981\u76ee\u7684\u662f\u6e32\u67d3\u5177\u6709\u5e73\u5766\u4e14\u98ce\u683c\u5316\u5916\u89c2\u7684\u5bf9\u8c61\u3002\u968f\u7740\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u4e0a\u5347\u5230\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u7684\u6700\u524d\u6cbf\uff0c\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5361\u901a\u7740\u8272\u7684\u521b\u65b0\u5f62\u5f0f\uff0c\u65e8\u5728\u5c06\u903c\u771f\u7684\u89c6\u9891\u76f4\u63a5\u6e32\u67d3\u4e3a\u52a8\u6f2b\u98ce\u683c\u3002\u5728\u89c6\u9891\u98ce\u683c\u5316\u4e2d\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u9047\u5230\u4e86\u6301\u7eed\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u548c\u5b9e\u73b0\u9ad8\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u5361\u901a\u7740\u8272\u95ee\u9898\u5efa\u6a21\u4e3a\u56db\u4e2a\u5b50\u95ee\u9898\uff1a\u98ce\u683c\u5316\u3001\u4e00\u81f4\u6027\u589e\u5f3a\u3001\u7ed3\u6784\u6307\u5bfc\u548c\u7740\u8272\u3002\u4e3a\u4e86\u89e3\u51b3\u89c6\u9891\u98ce\u683c\u5316\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5361\u901a\u7740\u8272\u65b9\u6cd5\uff0c\u79f0\u4e3a \\textit{Diffutoon}\u3002 Diffutoon \u80fd\u591f\u6e32\u67d3\u975e\u5e38\u8be6\u7ec6\u3001\u9ad8\u5206\u8fa8\u7387\u4e14\u6301\u7eed\u65f6\u95f4\u8f83\u957f\u7684\u52a8\u6f2b\u98ce\u683c\u89c6\u9891\u3002\u5b83\u8fd8\u53ef\u4ee5\u901a\u8fc7\u9644\u52a0\u5206\u652f\u6839\u636e\u63d0\u793a\u7f16\u8f91\u5185\u5bb9\u3002 Diffutoon \u7684\u529f\u6548\u662f\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u6765\u8bc4\u4f30\u7684\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0cDiffutoon \u8d85\u8d8a\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u57fa\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4f34\u968f\u7740 Github \u4e0a\u6e90\u4ee3\u7801\u548c\u793a\u4f8b\u89c6\u9891\u7684\u53d1\u5e03\uff08\u9879\u76ee\u9875\u9762\uff1ahttps://ecnu-cilab.github.io/DiffutoonProjectPage/\uff09\u3002|[2401.16224v1](http://arxiv.org/pdf/2401.16224v1)|null|\n", "2401.16157": "|**2024-01-29**|**Spatial-Aware Latent Initialization for Controllable Image Generation**|\u7528\u4e8e\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u7684\u7a7a\u95f4\u611f\u77e5\u6f5c\u5728\u521d\u59cb\u5316|Wenqiang Sun, Teng Li, Zehong Lin, Jun Zhang|Recently, text-to-image diffusion models have demonstrated impressive ability to generate high-quality images conditioned on the textual input. However, these models struggle to accurately adhere to textual instructions regarding spatial layout information. While previous research has primarily focused on aligning cross-attention maps with layout conditions, they overlook the impact of the initialization noise on the layout guidance. To achieve better layout control, we propose leveraging a spatial-aware initialization noise during the denoising process. Specifically, we find that the inverted reference image with finite inversion steps contains valuable spatial awareness regarding the object's position, resulting in similar layouts in the generated images. Based on this observation, we develop an open-vocabulary framework to customize a spatial-aware initialization noise for each layout condition. Without modifying other modules except the initialization noise, our approach can be seamlessly integrated as a plug-and-play module within other training-free layout guidance frameworks. We evaluate our approach quantitatively and qualitatively on the available Stable Diffusion model and COCO dataset. Equipped with the spatial-aware latent initialization, our method significantly improves the effectiveness of layout guidance while preserving high-quality content.|\u6700\u8fd1\uff0c\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u5c55\u793a\u4e86\u6839\u636e\u6587\u672c\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f88\u96be\u51c6\u786e\u5730\u9075\u5b88\u6709\u5173\u7a7a\u95f4\u5e03\u5c40\u4fe1\u606f\u7684\u6587\u672c\u6307\u4ee4\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5c06\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u4e0e\u5e03\u5c40\u6761\u4ef6\u5bf9\u9f50\uff0c\u4f46\u4ed6\u4eec\u5ffd\u7565\u4e86\u521d\u59cb\u5316\u566a\u58f0\u5bf9\u5e03\u5c40\u6307\u5bfc\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u5b9e\u73b0\u66f4\u597d\u7684\u5e03\u5c40\u63a7\u5236\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5229\u7528\u7a7a\u95f4\u611f\u77e5\u521d\u59cb\u5316\u566a\u58f0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u53d1\u73b0\u5177\u6709\u6709\u9650\u53cd\u8f6c\u6b65\u9aa4\u7684\u53cd\u8f6c\u53c2\u8003\u56fe\u50cf\u5305\u542b\u6709\u5173\u5bf9\u8c61\u4f4d\u7f6e\u7684\u6709\u4ef7\u503c\u7684\u7a7a\u95f4\u610f\u8bc6\uff0c\u4ece\u800c\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u4ea7\u751f\u76f8\u4f3c\u7684\u5e03\u5c40\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u653e\u8bcd\u6c47\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u5e03\u5c40\u6761\u4ef6\u5b9a\u5236\u7a7a\u95f4\u611f\u77e5\u7684\u521d\u59cb\u5316\u566a\u58f0\u3002\u9664\u4e86\u521d\u59cb\u5316\u566a\u58f0\u4e4b\u5916\uff0c\u65e0\u9700\u4fee\u6539\u5176\u4ed6\u6a21\u5757\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6\u514d\u57f9\u8bad\u5e03\u5c40\u6307\u5bfc\u6846\u67b6\u4e2d\u3002\u6211\u4eec\u5728\u53ef\u7528\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u548c COCO \u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u5730\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u914d\u5907\u7a7a\u95f4\u611f\u77e5\u6f5c\u5728\u521d\u59cb\u5316\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u5e03\u5c40\u6307\u5bfc\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u9ad8\u8d28\u91cf\u7684\u5185\u5bb9\u3002|[2401.16157v1](http://arxiv.org/pdf/2401.16157v1)|null|\n", "2401.15977": "|**2024-01-29**|**Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling**|Motion-I2V\uff1a\u901a\u8fc7\u663e\u5f0f\u8fd0\u52a8\u5efa\u6a21\u5b9e\u73b0\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210|Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et.al.|We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image's feature to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even at the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region annotations. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation.|\u6211\u4eec\u63a8\u51fa Motion-I2V\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210 (I2V) \u7684\u65b0\u9896\u6846\u67b6\u3002\u4e0e\u4e4b\u524d\u76f4\u63a5\u5b66\u4e60\u590d\u6742\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6620\u5c04\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cMotion-I2V \u901a\u8fc7\u663e\u5f0f\u8fd0\u52a8\u5efa\u6a21\u5c06 I2V \u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\u3002\u5bf9\u4e8e\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8fd0\u52a8\u573a\u9884\u6d4b\u5668\uff0c\u5176\u91cd\u70b9\u662f\u63a8\u5bfc\u53c2\u8003\u56fe\u50cf\u50cf\u7d20\u7684\u8f68\u8ff9\u3002\u5bf9\u4e8e\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u8fd0\u52a8\u589e\u5f3a\u65f6\u95f4\u6ce8\u610f\u529b\u6765\u589e\u5f3a\u89c6\u9891\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u6709\u9650\u7684\u4e00\u7ef4\u65f6\u95f4\u6ce8\u610f\u529b\u3002\u8be5\u6a21\u5757\u53ef\u4ee5\u5728\u7b2c\u4e00\u9636\u6bb5\u9884\u6d4b\u8f68\u8ff9\u7684\u6307\u5bfc\u4e0b\uff0c\u6709\u6548\u5730\u5c06\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u4f20\u64ad\u5230\u5408\u6210\u5e27\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5373\u4f7f\u5b58\u5728\u8f83\u5927\u8fd0\u52a8\u548c\u89c6\u70b9\u53d8\u5316\uff0cMotion-I2V \u4e5f\u53ef\u4ee5\u751f\u6210\u66f4\u4e00\u81f4\u7684\u89c6\u9891\u3002\u901a\u8fc7\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u7a00\u758f\u8f68\u8ff9ControlNet\uff0cMotion-I2V\u53ef\u4ee5\u652f\u6301\u7528\u6237\u901a\u8fc7\u7a00\u758f\u8f68\u8ff9\u548c\u533a\u57df\u6ce8\u91ca\u7cbe\u786e\u63a7\u5236\u8fd0\u52a8\u8f68\u8ff9\u548c\u8fd0\u52a8\u533a\u57df\u3002\u4e0e\u4ec5\u4f9d\u8d56\u6587\u672c\u6307\u4ee4\u76f8\u6bd4\uff0c\u8fd9\u4e3a I2V \u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u53ef\u63a7\u6027\u3002\u6b64\u5916\uff0cMotion-I2V \u7684\u7b2c\u4e8c\u9636\u6bb5\u81ea\u7136\u652f\u6301\u96f6\u955c\u5934\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u90fd\u8bc1\u660e\u4e86 Motion-I2V \u76f8\u5bf9\u4e8e\u5148\u524d\u65b9\u6cd5\u5728\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u9762\u7684\u4f18\u52bf\u3002|[2401.15977v1](http://arxiv.org/pdf/2401.15977v1)|null|\n", "2401.15902": "|**2024-01-29**|**A Concise but Effective Network for Image Guided Depth Completion in Autonomous Driving**|\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56fe\u50cf\u5f15\u5bfc\u6df1\u5ea6\u8865\u5168\u7684\u7b80\u6d01\u800c\u6709\u6548\u7684\u7f51\u7edc|Moyun Liu, Youping Chen, Jingming Xie, Lei Yao, Yang Zhang, Joey Tianyi Zhou|Depth completion is a crucial task in autonomous driving, aiming to convert a sparse depth map into a dense depth prediction. Due to its potentially rich semantic information, RGB image is commonly fused to enhance the completion effect. Image-guided depth completion involves three key challenges: 1) how to effectively fuse the two modalities; 2) how to better recover depth information; and 3) how to achieve real-time prediction for practical autonomous driving. To solve the above problems, we propose a concise but effective network, named CENet, to achieve high-performance depth completion with a simple and elegant structure. Firstly, we use a fast guidance module to fuse the two sensor features, utilizing abundant auxiliary features extracted from the color space. Unlike other commonly used complicated guidance modules, our approach is intuitive and low-cost. In addition, we find and analyze the optimization inconsistency problem for observed and unobserved positions, and a decoupled depth prediction head is proposed to alleviate the issue. The proposed decoupled head can better output the depth of valid and invalid positions with very few extra inference time. Based on the simple structure of dual-encoder and single-decoder, our CENet can achieve superior balance between accuracy and efficiency. In the KITTI depth completion benchmark, our CENet attains competitive performance and inference speed compared with the state-of-the-art methods. To validate the generalization of our method, we also evaluate on indoor NYUv2 dataset, and our CENet still achieve impressive results. The code of this work will be available at https://github.com/lmomoy/CENet.|\u6df1\u5ea6\u8865\u5168\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u65e8\u5728\u5c06\u7a00\u758f\u6df1\u5ea6\u56fe\u8f6c\u6362\u4e3a\u5bc6\u96c6\u6df1\u5ea6\u9884\u6d4b\u3002\u7531\u4e8e\u5176\u6f5c\u5728\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0cRGB\u56fe\u50cf\u901a\u5e38\u88ab\u878d\u5408\u4ee5\u589e\u5f3a\u8865\u5168\u6548\u679c\u3002\u56fe\u50cf\u5f15\u5bfc\u6df1\u5ea6\u8865\u5168\u6d89\u53ca\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u5982\u4f55\u6709\u6548\u878d\u5408\u4e24\u79cd\u6a21\u5f0f\uff1b 2\uff09\u5982\u4f55\u66f4\u597d\u7684\u6062\u590d\u6df1\u5ea6\u4fe1\u606f\uff1b 3\uff09\u5982\u4f55\u5b9e\u73b0\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u65f6\u9884\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u6d01\u4f46\u6709\u6548\u7684\u7f51\u7edc\uff0c\u547d\u540d\u4e3aCENet\uff0c\u4ee5\u7b80\u5355\u800c\u4f18\u96c5\u7684\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6027\u80fd\u6df1\u5ea6\u5b8c\u6210\u3002\u9996\u5148\uff0c\u6211\u4eec\u4f7f\u7528\u5feb\u901f\u5f15\u5bfc\u6a21\u5757\u6765\u878d\u5408\u4e24\u4e2a\u4f20\u611f\u5668\u7279\u5f81\uff0c\u5229\u7528\u4ece\u989c\u8272\u7a7a\u95f4\u63d0\u53d6\u7684\u4e30\u5bcc\u8f85\u52a9\u7279\u5f81\u3002\u4e0e\u5176\u4ed6\u5e38\u7528\u7684\u590d\u6742\u5f15\u5bfc\u6a21\u5757\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f4\u89c2\u4e14\u6210\u672c\u4f4e\u5ec9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u5e76\u5206\u6790\u4e86\u89c2\u6d4b\u4f4d\u7f6e\u548c\u672a\u89c2\u6d4b\u4f4d\u7f6e\u7684\u4f18\u5316\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u8026\u6df1\u5ea6\u9884\u6d4b\u5934\u6765\u7f13\u89e3\u8be5\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u89e3\u8026\u5934\u53ef\u4ee5\u66f4\u597d\u5730\u8f93\u51fa\u6709\u6548\u548c\u65e0\u6548\u4f4d\u7f6e\u7684\u6df1\u5ea6\uff0c\u5e76\u4e14\u53ea\u9700\u5f88\u5c11\u7684\u989d\u5916\u63a8\u7406\u65f6\u95f4\u3002\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u548c\u5355\u89e3\u7801\u5668\u7684\u7b80\u5355\u7ed3\u6784\uff0c\u6211\u4eec\u7684 CENet \u53ef\u4ee5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u5353\u8d8a\u7684\u5e73\u8861\u3002\u5728 KITTI \u6df1\u5ea6\u5b8c\u6210\u57fa\u51c6\u4e2d\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 CENet \u83b7\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\uff0c\u6211\u4eec\u8fd8\u5bf9\u5ba4\u5185 NYUv2 \u6570\u636e\u96c6\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e14\u6211\u4eec\u7684 CENet \u4ecd\u7136\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/lmomoy/CENet \u4e0a\u63d0\u4f9b\u3002|[2401.15902v1](http://arxiv.org/pdf/2401.15902v1)|null|\n", "2401.15889": "|**2024-01-29**|**Sliced Wasserstein with Random-Path Projecting Directions**|\u5177\u6709\u968f\u673a\u8def\u5f84\u6295\u5f71\u65b9\u5411\u7684\u5207\u7247 Wasserstein|Khai Nguyen, Shujian Zhang, Tam Le, Nhat Ho|Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational properties of RPSW and IWRPSW. Finally, we showcase the favorable performance of RPSW and IWRPSW in gradient flow and the training of denoising diffusion generative models on images.|\u5207\u7247\u5206\u5e03\u9009\u62e9\u5df2\u88ab\u7528\u4f5c\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u57fa\u4e8e\u6700\u5c0f\u5316\u5e94\u7528\u4e2d\u7684\u5207\u7247 Wasserstein \u8ddd\u79bb\u6765\u63d0\u9ad8\u53c2\u6570\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u8981\u4e48\u5229\u7528\u6602\u8d35\u7684\u4f18\u5316\u6765\u9009\u62e9\u5207\u7247\u5206\u5e03\uff0c\u8981\u4e48\u4f7f\u7528\u9700\u8981\u6602\u8d35\u7684\u91c7\u6837\u65b9\u6cd5\u7684\u5207\u7247\u5206\u5e03\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u4f18\u5316\u7684\u5207\u7247\u5206\u5e03\uff0c\u4e3a\u8499\u7279\u5361\u7f57\u671f\u671b\u4f30\u8ba1\u63d0\u4f9b\u5feb\u901f\u91c7\u6837\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u968f\u673a\u8def\u5f84\u6295\u5f71\u65b9\u5411\uff08RPD\uff09\uff0c\u5b83\u662f\u901a\u8fc7\u5229\u7528\u4e24\u4e2a\u8f93\u5165\u6d4b\u91cf\u540e\u7684\u4e24\u4e2a\u968f\u673a\u5411\u91cf\u4e4b\u95f4\u7684\u5f52\u4e00\u5316\u5dee\u6765\u6784\u9020\u7684\u3002\u4ece RPD \u4e2d\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u968f\u673a\u8def\u5f84\u5207\u7247\u5206\u5e03 (RPSD) \u548c\u5207\u7247 Wasserstein \u7684\u4e24\u4e2a\u53d8\u4f53\uff0c\u5373\u968f\u673a\u8def\u5f84\u6295\u5f71\u5207\u7247 Wasserstein (RPSW) \u548c\u91cd\u8981\u6027\u52a0\u6743\u968f\u673a\u8def\u5f84\u6295\u5f71\u5207\u7247 Wasserstein (IWRPSW)\u3002\u7136\u540e\u6211\u4eec\u8ba8\u8bba RPSW \u548c IWRPSW \u7684\u62d3\u6251\u3001\u7edf\u8ba1\u548c\u8ba1\u7b97\u7279\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86 RPSW \u548c IWRPSW \u5728\u68af\u5ea6\u6d41\u4e2d\u7684\u826f\u597d\u6027\u80fd\u4ee5\u53ca\u56fe\u50cf\u4e0a\u7684\u53bb\u566a\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u3002|[2401.15889v1](http://arxiv.org/pdf/2401.15889v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.16420": "|**2024-01-29**|**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**|InternLM-XComposer2\uff1a\u638c\u63e1\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u7684\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u56fe\u50cf\u5408\u6210\u548c\u7406\u89e3|Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et.al.|We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.|\u6211\u4eec\u63a8\u51fa InternLM-XComposer2\uff0c\u8fd9\u662f\u4e00\u79cd\u5c16\u7aef\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u81ea\u7531\u5f62\u5f0f\u7684\u6587\u672c\u56fe\u50cf\u5408\u6210\u548c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u8be5\u6a21\u578b\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\uff0c\u53ef\u4ee5\u6839\u636e\u8f6e\u5ed3\u3001\u8be6\u7ec6\u6587\u672c\u89c4\u8303\u548c\u53c2\u8003\u56fe\u50cf\u7b49\u4e0d\u540c\u8f93\u5165\u5de7\u5999\u5730\u5236\u4f5c\u4ea4\u9519\u7684\u6587\u672c\u56fe\u50cf\u5185\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u5185\u5bb9\u521b\u5efa\u3002 InternLM-XComposer2\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u5206LoRA\uff08PLoRA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u989d\u5916\u7684LoRA\u53c2\u6570\u4e13\u95e8\u5e94\u7528\u4e8e\u56fe\u50cf\u6807\u8bb0\uff0c\u4ee5\u4fdd\u6301\u9884\u5148\u8bad\u7ec3\u7684\u8bed\u8a00\u77e5\u8bc6\u7684\u5b8c\u6574\u6027\uff0c\u5728\u7cbe\u786e\u7684\u89c6\u89c9\u7406\u89e3\u548c\u5177\u6709\u6587\u5b66\u5929\u8d4b\u7684\u6587\u672c\u5199\u4f5c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e InternLM2-7B \u7684 InternLM-XComposer2 \u5728\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u591a\u6a21\u6001\u5185\u5bb9\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u51fa\u8272\u7684\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u6027\u80fd\uff0c\u4e0d\u4ec5\u663e\u7740\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u800c\u4e14\u8fd8\u5339\u914d\u751a\u81f3\u5728\u67d0\u4e9b\u8bc4\u6d4b\u4e2d\u8d85\u8fc7\u4e86GPT-4V\u548cGemini Pro\u3002\u8fd9\u51f8\u663e\u4e86\u5176\u5728\u591a\u6a21\u6001\u7406\u89e3\u9886\u57df\u7684\u5353\u8d8a\u719f\u7ec3\u7a0b\u5ea6\u3002\u5177\u6709 7B \u53c2\u6570\u7684 InternLM-XComposer2 \u6a21\u578b\u7cfb\u5217\u53ef\u5728 https://github.com/InternLM/InternLM-XComposer \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.16420v1](http://arxiv.org/pdf/2401.16420v1)|null|\n", "2401.16355": "|**2024-01-29**|**PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology**|PathMMU\uff1a\u7528\u4e8e\u75c5\u7406\u5b66\u7406\u89e3\u548c\u63a8\u7406\u7684\u5927\u89c4\u6a21\u591a\u6a21\u5f0f\u4e13\u5bb6\u7ea7\u57fa\u51c6|Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Xiaoxiao Lan, Mengyue Zheng, Jingxiong Li, et.al.|The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology. However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation. To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question. The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\\&As. Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU. We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption. We also fine-tune representative LMMs to assess their adaptability to PathMMU. The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\% zero-shot performance, significantly lower than the 71.4\\% demonstrated by human pathologists. After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\\%, but still fall short of the expertise shown by pathologists. We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology.|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u51fa\u73b0\u91ca\u653e\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u75c5\u7406\u5b66\u9886\u57df\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\uff0c\u963b\u788d\u4e86\u5176\u53d1\u5c55\u548c\u7cbe\u786e\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 PathMMU\uff0c\u8fd9\u662f\u6700\u5927\u3001\u8d28\u91cf\u6700\u9ad8\u3001\u7ecf\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u7684 LMM \u75c5\u7406\u5b66\u57fa\u51c6\u3002\u5b83\u5305\u542b 33,573 \u4e2a\u591a\u6a21\u6001\u591a\u9879\u9009\u62e9\u95ee\u9898\u548c\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684 21,599 \u5f20\u56fe\u50cf\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u9644\u6709\u6b63\u786e\u7b54\u6848\u7684\u89e3\u91ca\u3002 PathMMU \u7684\u6784\u5efa\u5229\u7528\u4e86 GPT-4V \u7684\u5f3a\u5927\u529f\u80fd\uff0c\u5229\u7528\u5927\u7ea6 30,000 \u4e2a\u6536\u96c6\u7684\u56fe\u50cf\u6807\u9898\u5bf9\u6765\u751f\u6210\u95ee\u7b54\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e3a\u4e86\u6700\u5927\u9650\u5ea6\u5730\u53d1\u6325PathMMU\u7684\u6743\u5a01\uff0c\u6211\u4eec\u9080\u8bf7\u4e86\u516d\u4f4d\u75c5\u7406\u5b66\u5bb6\u5728PathMMU\u7684\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u4e2d\u4e25\u683c\u6807\u51c6\u4e0b\u4ed4\u7ec6\u5ba1\u67e5\u6bcf\u4e2a\u95ee\u9898\uff0c\u540c\u65f6\u4e3aPathMMU\u8bbe\u5b9a\u4e86\u4e13\u5bb6\u7ea7\u7684\u6027\u80fd\u57fa\u51c6\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u5bf9 14 \u4e2a\u5f00\u6e90\u548c\u4e09\u4e2a\u95ed\u6e90 LMM \u53ca\u5176\u5bf9\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u5bf9\u4ee3\u8868\u6027 LMM \u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5bf9 PathMMU \u7684\u9002\u5e94\u6027\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5148\u8fdb\u7684 LMM \u5f88\u96be\u5e94\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684 PathMMU \u57fa\u51c6\uff0c\u5176\u4e2d\u8868\u73b0\u6700\u597d\u7684 LMM GPT-4V \u4ec5\u5b9e\u73b0\u4e86 51.7% \u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u663e\u7740\u4f4e\u4e8e\u4eba\u7c7b\u75c5\u7406\u5b66\u5bb6\u6240\u8bc1\u660e\u7684 71.4%\u3002\u7ecf\u8fc7\u5fae\u8c03\uff0c\u5373\u4f7f\u662f\u5f00\u6e90\u7684 LMM \u4e5f\u80fd\u4ee5\u8d85\u8fc7 60% \u7684\u6027\u80fd\u8d85\u8d8a GPT-4V\uff0c\u4f46\u4ecd\u8fbe\u4e0d\u5230\u75c5\u7406\u5b66\u5bb6\u6240\u8868\u73b0\u51fa\u7684\u4e13\u4e1a\u6c34\u5e73\u3002\u6211\u4eec\u5e0c\u671b PathMMU \u80fd\u591f\u63d0\u4f9b\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5e76\u4fc3\u8fdb\u66f4\u4e13\u4e1a\u7684\u4e0b\u4e00\u4ee3\u75c5\u7406\u5b66\u6cd5\u5b66\u7855\u58eb\u7684\u53d1\u5c55\u3002|[2401.16355v1](http://arxiv.org/pdf/2401.16355v1)|null|\n", "2401.16160": "|**2024-01-29**|**LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs**|LLaVA-MoLE\uff1aLoRA \u4e13\u5bb6\u7684\u7a00\u758f\u7ec4\u5408\uff0c\u7528\u4e8e\u7f13\u89e3\u6307\u4ee4\u5fae\u8c03 MLLM \u4e2d\u7684\u6570\u636e\u51b2\u7a81|Shaoxiang Chen, Zequn Jie, Lin Ma|Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply a sparse mixture of LoRA experts for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA finetuing of LLaVA-1.5, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.|\u5bf9\u5404\u79cd\u56fe\u50cf\u6587\u672c\u6307\u4ee4\u6570\u636e\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u662f\u83b7\u5f97\u901a\u7528\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5173\u952e\uff0c\u6307\u4ee4\u6570\u636e\u7684\u4e0d\u540c\u914d\u7f6e\u53ef\u4ee5\u5bfc\u81f4\u5177\u6709\u4e0d\u540c\u80fd\u529b\u7684\u5fae\u8c03\u6a21\u578b\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5f53\u6df7\u5408\u6765\u81ea\u4e0d\u540c\u57df\u7684\u6307\u4ee4\u6570\u636e\u65f6\uff0c\u6570\u636e\u51b2\u7a81\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7279\u5b9a\u57df\u7684\u4efb\u52a1\u7684\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5e94\u7528 LoRA \u4e13\u5bb6\u7684\u7a00\u758f\u6df7\u5408\u6765\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03 MLLM\u3002\u5728 Transformer \u5c42\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4e13\u95e8\u4e3a MLP \u5c42\u521b\u5efa\u4e00\u7ec4 LoRA \u4e13\u5bb6\u6765\u6269\u5c55\u6d41\u884c\u7684\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA) \u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u8def\u7531\u51fd\u6570\u5c06\u6bcf\u4e2a\u4ee4\u724c\u8def\u7531\u5230 top-1 \u4e13\u5bb6\uff0c\u4ece\u800c\u5141\u8bb8\u81ea\u9002\u5e94\u9009\u62e9\u6765\u81ea\u4e0d\u540c\u57df\u7684\u4ee4\u724c\u3002\u7531\u4e8e LoRA \u4e13\u5bb6\u662f\u7a00\u758f\u6fc0\u6d3b\u7684\uff0c\u56e0\u6b64\u4e0e\u539f\u59cb LoRA \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u5927\u81f4\u4fdd\u6301\u4e0d\u53d8\u3002\u901a\u8fc7\u66ff\u6362 LLaVA-1.5 \u7684\u666e\u901a LoRA \u5fae\u8c03\uff0c\u6211\u4eec\u7684\u6700\u7ec8\u6a21\u578b\u88ab\u547d\u540d\u4e3a LLaVA-MoLE\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cLLaVA-MoLE \u6709\u6548\u7f13\u89e3\u4e86\u5c06\u591a\u4e2a\u4e0d\u540c\u7684\u6307\u4ee4\u6570\u636e\u96c6\u4e0e\u5404\u79cd\u914d\u7f6e\u6df7\u5408\u65f6\u7684\u6570\u636e\u51b2\u7a81\u95ee\u9898\uff0c\u5e76\u5728\u5f3a\u5927\u7684 plain-LoRA \u57fa\u7ebf\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u589e\u76ca\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\uff0cLLaVA-MoLE \u751a\u81f3\u53ef\u4ee5\u4f18\u4e8e\u4f7f\u7528\u4e24\u500d\u6837\u672c\u8bad\u7ec3\u7684\u666e\u901a LoRA \u57fa\u7ebf\u3002|[2401.16160v1](http://arxiv.org/pdf/2401.16160v1)|null|\n", "2401.16158": "|**2024-01-29**|**Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception**|Mobile-Agent\uff1a\u5177\u6709\u89c6\u89c9\u611f\u77e5\u7684\u81ea\u4e3b\u591a\u6a21\u5f0f\u79fb\u52a8\u8bbe\u5907\u4ee3\u7406|Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang|Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.|\u57fa\u4e8e\u591a\u6a21\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u79fb\u52a8\u8bbe\u5907\u4ee3\u7406\u6b63\u5728\u6210\u4e3a\u6d41\u884c\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Mobile-Agent\uff0c\u4e00\u79cd\u81ea\u4e3b\u7684\u591a\u6a21\u5f0f\u79fb\u52a8\u8bbe\u5907\u4ee3\u7406\u3002 Mobile-Agent \u9996\u5148\u5229\u7528\u89c6\u89c9\u611f\u77e5\u5de5\u5177\u6765\u51c6\u786e\u8bc6\u522b\u548c\u5b9a\u4f4d\u5e94\u7528\u7a0b\u5e8f\u524d\u7aef\u754c\u9762\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u3002\u57fa\u4e8e\u611f\u77e5\u5230\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u5b83\u4f1a\u81ea\u4e3b\u89c4\u5212\u548c\u5206\u89e3\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u9010\u6b65\u5bfc\u822a\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u64cd\u4f5c\u3002\u4e0e\u4ee5\u524d\u4f9d\u8d56\u5e94\u7528\u7a0b\u5e8f\u7684 XML \u6587\u4ef6\u6216\u79fb\u52a8\u7cfb\u7edf\u5143\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u4e0d\u540c\uff0cMobile-Agent \u5141\u8bb8\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u65b9\u5f0f\u5728\u4e0d\u540c\u7684\u79fb\u52a8\u64cd\u4f5c\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u5927\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u7279\u5b9a\u4e8e\u7cfb\u7edf\u7684\u5b9a\u5236\u7684\u5fc5\u8981\u6027\u3002\u4e3a\u4e86\u8bc4\u4f30 Mobile-Agent \u7684\u6027\u80fd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Mobile-Eval\uff0c\u8fd9\u662f\u8bc4\u4f30\u79fb\u52a8\u8bbe\u5907\u64cd\u4f5c\u7684\u57fa\u51c6\u3002\u57fa\u4e8eMobile-Eval\uff0c\u6211\u4eec\u5bf9Mobile-Agent\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMobile-Agent\u53d6\u5f97\u4e86\u663e\u7740\u7684\u51c6\u786e\u7387\u548c\u5b8c\u6210\u7387\u3002\u5373\u4f7f\u6709\u6311\u6218\u6027\u7684\u6307\u4ee4\uff0c\u4f8b\u5982\u591a\u5e94\u7528\u7a0b\u5e8f\u64cd\u4f5c\uff0cMobile-Agent \u4ecd\u7136\u53ef\u4ee5\u5b8c\u6210\u8981\u6c42\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://github.com/X-PLUG/MobileAgent \u5f00\u6e90\u3002|[2401.16158v1](http://arxiv.org/pdf/2401.16158v1)|null|\n", "2401.16123": "|**2024-01-29**|**Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers**|\u6b63\u5728\u5bfb\u627e\u66f4\u5408\u9002\u7684\u9009\u62e9\uff1f\u9002\u5e94\u4e2a\u4f53\u9a7e\u9a76\u5458\u7684\u589e\u91cf\u5b66\u4e60\u591a\u6a21\u6001\u5bf9\u8c61\u5f15\u7528\u6846\u67b6|Amr Gomaa, Guillermo Reyes, Michael Feld, Antonio Kr\u00fcger|The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we propose \\textit{IcRegress}, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at \\url{https://github.com/amrgomaaelhady/IcRegress}.|\u6c7d\u8f66\u884c\u4e1a\u5411\u81ea\u52a8\u548c\u534a\u81ea\u52a8\u8f66\u8f86\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4f20\u7edf\u7684\u8f66\u8f86\u4ea4\u4e92\u65b9\u6cd5\uff08\u4f8b\u5982\u57fa\u4e8e\u89e6\u6478\u548c\u8bed\u97f3\u547d\u4ee4\u7cfb\u7edf\uff09\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u8d8a\u6765\u8d8a\u591a\u7684\u975e\u9a7e\u9a76\u76f8\u5173\u4efb\u52a1\uff0c\u4f8b\u5982\u5f15\u7528\u5916\u90e8\u7269\u4f53\u3002\u673a\u52a8\u8f66\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8f6c\u5411\u624b\u52bf\u8f93\u5165\uff08\u4f8b\u5982\u624b\u3001\u51dd\u89c6\u548c\u5934\u90e8\u59ff\u52bf\u624b\u52bf\uff09\u4f5c\u4e3a\u9a7e\u9a76\u8fc7\u7a0b\u4e2d\u66f4\u5408\u9002\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9a7e\u9a76\u7684\u52a8\u6001\u6027\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u9a7e\u9a76\u5458\u7684\u624b\u52bf\u8f93\u5165\u8868\u73b0\u5b58\u5728\u663e\u7740\u5dee\u5f02\u3002\u867d\u7136\u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u8fd9\u79cd\u56fa\u6709\u7684\u53ef\u53d8\u6027\u53ef\u4ee5\u901a\u8fc7\u5927\u91cf\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u8c03\u8282\uff0c\u4f46\u666e\u904d\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u7528\u4e8e\u5bf9\u8c61\u5f15\u7528\u7684\u53d7\u7ea6\u675f\u7684\u3001\u5355\u5b9e\u4f8b\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u6301\u7eed\u9002\u5e94\u4e2a\u4f53\u9a7e\u9a76\u5458\u7684\u4e0d\u540c\u884c\u4e3a\u548c\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u7684\u80fd\u529b\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 \\textit{IcRegress}\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56de\u5f52\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u884c\u4e3a\u4ee5\u53ca\u4ece\u4e8b\u9a7e\u9a76\u548c\u53c2\u8003\u7269\u4f53\u53cc\u91cd\u4efb\u52a1\u7684\u9a7e\u9a76\u5458\u7684\u72ec\u7279\u7279\u5f81\u3002\u6211\u4eec\u5efa\u8bae\u4e3a\u591a\u6a21\u5f0f\u624b\u52bf\u754c\u9762\u63d0\u4f9b\u66f4\u52a0\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u6301\u7eed\u7684\u7ec8\u8eab\u5b66\u4e60\u6765\u589e\u5f3a\u9a7e\u9a76\u5458\u4f53\u9a8c\u3001\u5b89\u5168\u6027\u548c\u4fbf\u5229\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8f66\u5916\u5bf9\u8c61\u5f15\u7528\u7528\u4f8b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7a81\u663e\u4e86\u589e\u91cf\u5b66\u4e60\u6a21\u578b\u76f8\u5bf9\u4e8e\u5355\u4e2a\u8bad\u7ec3\u6a21\u578b\u5728\u5404\u79cd\u9a7e\u9a76\u5458\u7279\u5f81\uff08\u4f8b\u5982\u60ef\u7528\u624b\u3001\u9a7e\u9a76\u7ecf\u9a8c\u548c\u591a\u79cd\u9a7e\u9a76\u6761\u4ef6\uff09\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3001\u7b80\u5316\u90e8\u7f72\u5e76\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5728 \\url{https://github.com/amrgomaaelhady/IcRegress} \u63d0\u4f9b\u6211\u4eec\u7684\u65b9\u6cd5\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\u3002|[2401.16123v1](http://arxiv.org/pdf/2401.16123v1)|null|\n", "2401.16076": "|**2024-01-29**|**Find the Cliffhanger: Multi-Modal Trailerness in Soap Operas**|\u5bfb\u627e\u60ac\u5ff5\uff1a\u80a5\u7682\u5267\u4e2d\u7684\u591a\u6a21\u5f0f\u9884\u544a\u7247|Carlo Bretti, Pascal Mettes, Hendrik Vincent Koops, Daan Odijk, Nanne van Noord|Creating a trailer requires carefully picking out and piecing together brief enticing moments out of a longer video, making it a chal- lenging and time-consuming task. This requires selecting moments based on both visual and dialogue information. We introduce a multi-modal method for predicting the trailerness to assist editors in selecting trailer- worthy moments from long-form videos. We present results on a newly introduced soap opera dataset, demonstrating that predicting trailerness is a challenging task that benefits from multi-modal information. Code is available at https://github.com/carlobretti/cliffhanger|\u5236\u4f5c\u9884\u544a\u7247\u9700\u8981\u4ece\u8f83\u957f\u7684\u89c6\u9891\u4e2d\u4ed4\u7ec6\u6311\u9009\u5e76\u62fc\u51d1\u51fa\u7b80\u77ed\u800c\u8bf1\u4eba\u7684\u65f6\u523b\uff0c\u8fd9\u4f7f\u5176\u6210\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u4e14\u8017\u65f6\u7684\u4efb\u52a1\u3002\u8fd9\u9700\u8981\u6839\u636e\u89c6\u89c9\u548c\u5bf9\u8bdd\u4fe1\u606f\u6765\u9009\u62e9\u65f6\u523b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u9884\u6d4b\u9884\u544a\u7247\uff0c\u4ee5\u5e2e\u52a9\u7f16\u8f91\u4ece\u957f\u89c6\u9891\u4e2d\u9009\u62e9\u503c\u5f97\u9884\u544a\u7247\u7684\u65f6\u523b\u3002\u6211\u4eec\u5728\u65b0\u5f15\u5165\u7684\u80a5\u7682\u5267\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7ed3\u679c\uff0c\u8868\u660e\u9884\u6d4b\u9884\u544a\u7247\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u53d7\u76ca\u4e8e\u591a\u6a21\u6001\u4fe1\u606f\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/carlobretti/cliffhanger \u83b7\u53d6|[2401.16076v1](http://arxiv.org/pdf/2401.16076v1)|null|\n", "2401.15947": "|**2024-01-29**|**MoE-LLaVA: Mixture of Experts for Large Vision-Language Models**|MoE-LLaVA\uff1a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e13\u5bb6\u7ec4\u5408|Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, Li Yuan|For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation. In this work, we propose a novel training strategy MoE-tuning for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity. Furthermore, we present the MoE-LLaVA framework, a MoE-based sparse LVLM architecture. This framework uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Our extensive experiments highlight the excellent capabilities of MoE-LLaVA in visual understanding and its potential to reduce hallucinations in model outputs. Remarkably, with just 3 billion sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmarks. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at \\url{https://github.com/PKU-YuanGroup/MoE-LLaVA}.|\u5bf9\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\uff0c\u7f29\u653e\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u6027\u80fd\u3002\u7136\u800c\uff0c\u6269\u5c55\u6a21\u578b\u53c2\u6570\u4f1a\u663e\u7740\u589e\u52a0\u8bad\u7ec3\u548c\u63a8\u65ad\u6210\u672c\uff0c\u56e0\u4e3a\u8ba1\u7b97\u4e2d\u7684\u6bcf\u4e2a\u6807\u8bb0\u90fd\u4f1a\u6fc0\u6d3b\u6240\u6709\u6a21\u578b\u53c2\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9 LVLM \u7684\u65b0\u578b\u8bad\u7ec3\u7b56\u7565 MoE-tuning\uff0c\u5b83\u53ef\u4ee5\u6784\u5efa\u53c2\u6570\u6570\u91cf\u60ca\u4eba\u4f46\u8ba1\u7b97\u6210\u672c\u6052\u5b9a\u7684\u7a00\u758f\u6a21\u578b\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u901a\u5e38\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u548c\u6a21\u578b\u7a00\u758f\u6027\u76f8\u5173\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 MoE-LLaVA \u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e MoE \u7684\u7a00\u758f LVLM \u67b6\u6784\u3002\u8be5\u6846\u67b6\u72ec\u7279\u5730\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u8def\u7531\u5668\u4ec5\u6fc0\u6d3b\u524d k \u4e2a\u4e13\u5bb6\uff0c\u4f7f\u5176\u4f59\u4e13\u5bb6\u4fdd\u6301\u4e0d\u6d3b\u52a8\u72b6\u6001\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86 MoE-LLaVA \u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u51fa\u8272\u80fd\u529b\u53ca\u5176\u51cf\u5c11\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u7684\u6f5c\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMoE-LLaVA \u4ec5\u5177\u6709 30 \u4ebf\u4e2a\u7a00\u758f\u6fc0\u6d3b\u53c2\u6570\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0e LLaVA-1.5-7B \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u7269\u4f53\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7\u4e86 LLaVA-1.5-13B\u3002\u901a\u8fc7 MoE-LLaVA\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4e3a\u7a00\u758f LVLM \u5efa\u7acb\u57fa\u7ebf\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u4ee3\u7801\u53d1\u5e03\u4e8e\\url{https://github.com/PKU-YuanGroup/MoE-LLaVA}\u3002|[2401.15947v1](http://arxiv.org/pdf/2401.15947v1)|null|\n", "2401.15847": "|**2024-01-29**|**Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA**|\u677e\u997c\u8fd8\u662f\u5409\u5a03\u5a03\uff1f\u4f7f\u7528\u591a\u9762\u677f VQA \u6311\u6218\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b|Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang|Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, our paper introduces Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark that specifically challenges models in comprehending multipanel images. The benchmark comprises 6,600 questions and answers related to multipanel images. While these questions are straightforward for average humans, achieving nearly perfect correctness, they pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) we tested. In our study, we utilized synthetically curated multipanel images specifically designed to isolate and evaluate the impact of diverse factors on model performance, revealing the sensitivity of LVLMs to various interferences in multipanel images, such as adjacent subfigures and layout complexity. As a result, MultipanelVQA highlights the need and direction for improving LVLMs' ability to understand complex visual-language contexts. Code and data are released at https://sites.google.com/view/multipanelvqa/home.|\u591a\u9762\u677f\u56fe\u50cf\uff08\u901a\u5e38\u4e3a\u7f51\u9875\u622a\u56fe\u3001\u6d77\u62a5\u7b49\uff09\u904d\u5e03\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u3002\u8fd9\u4e9b\u56fe\u50cf\u7684\u7279\u70b9\u662f\u7531\u4e0d\u540c\u5e03\u5c40\u7684\u591a\u4e2a\u5b50\u56fe\u7ec4\u6210\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5411\u4eba\u4eec\u4f20\u8fbe\u4fe1\u606f\u3002\u4e3a\u4e86\u6784\u5efa\u5148\u8fdb\u7684\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f8b\u5982\u7406\u89e3\u590d\u6742\u573a\u666f\u548c\u6d4f\u89c8\u7f51\u9875\u7684\u4ee3\u7406\uff0c\u591a\u9762\u677f\u89c6\u89c9\u63a8\u7406\u6280\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u5bf9\u8fd9\u65b9\u9762\u7684\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u4e5f\u5f88\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u8bba\u6587\u4ecb\u7ecd\u4e86\u591a\u9762\u677f\u89c6\u89c9\u95ee\u7b54\uff08MultipanelVQA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u51c6\uff0c\u4e13\u95e8\u6311\u6218\u6a21\u578b\u7406\u89e3\u591a\u9762\u677f\u56fe\u50cf\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b 6,600 \u4e2a\u4e0e\u591a\u9762\u677f\u56fe\u50cf\u76f8\u5173\u7684\u95ee\u9898\u548c\u7b54\u6848\u3002\u867d\u7136\u8fd9\u4e9b\u95ee\u9898\u5bf9\u4e8e\u666e\u901a\u4eba\u6765\u8bf4\u5f88\u7b80\u5355\uff0c\u51e0\u4e4e\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u7f8e\u7684\u6b63\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u5bf9\u6211\u4eec\u6d4b\u8bd5\u7684\u6700\u5148\u8fdb\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u7efc\u5408\u591a\u9762\u677f\u56fe\u50cf\u6765\u9694\u79bb\u548c\u8bc4\u4f30\u4e0d\u540c\u56e0\u7d20\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a LVLM \u5bf9\u591a\u9762\u677f\u56fe\u50cf\u4e2d\u5404\u79cd\u5e72\u6270\u7684\u654f\u611f\u6027\uff0c\u4f8b\u5982\u76f8\u90bb\u5b50\u56fe\u548c\u5e03\u5c40\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0cMultipanelVQA \u5f3a\u8c03\u4e86\u63d0\u9ad8 LVLM \u7406\u89e3\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u7684\u9700\u6c42\u548c\u65b9\u5411\u3002\u4ee3\u7801\u548c\u6570\u636e\u53d1\u5e03\u4e8e https://sites.google.com/view/multipanelvqa/home\u3002|[2401.15847v1](http://arxiv.org/pdf/2401.15847v1)|null|\n"}, "LLM": {}, "Transformer": {}, "Nerf": {"2401.16416": "|**2024-01-29**|**Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting**|Endo-4DGS\uff1a\u4f7f\u7528 4D \u9ad8\u65af\u6e85\u5c04\u8fdb\u884c\u5185\u7aa5\u955c\u5355\u773c\u573a\u666f\u91cd\u5efa\u7684\u84b8\u998f\u6df1\u5ea6\u6392\u5e8f|Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Hongliang Ren|In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes. Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands. Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras. Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate. To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data. This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations. This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions. We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process. Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy. These results underline the vast potential of Endo-4DGS to improve surgical assistance.|\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u9886\u57df\uff0c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u53ef\u4ee5\u663e\u7740\u589e\u5f3a\u4e0b\u6e38\u4efb\u52a1\u5e76\u6539\u5584\u624b\u672f\u7ed3\u679c\u3002\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u65b9\u6cd5\u6700\u8fd1\u56e0\u5176\u91cd\u5efa\u573a\u666f\u7684\u5353\u8d8a\u80fd\u529b\u800c\u53d7\u5230\u5173\u6ce8\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53d7\u5230\u7f13\u6162\u7684\u63a8\u7406\u3001\u957f\u65f6\u95f4\u7684\u8bad\u7ec3\u548c\u5927\u91cf\u7684\u8ba1\u7b97\u9700\u6c42\u7684\u963b\u788d\u3002\u6b64\u5916\uff0c\u6709\u4e9b\u4f9d\u8d56\u4e8e\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u7531\u4e8e\u4e0e\u7acb\u4f53\u76f8\u673a\u76f8\u5173\u7684\u9ad8\u6210\u672c\u548c\u540e\u52e4\u6311\u6218\uff0c\u8fd9\u901a\u5e38\u662f\u4e0d\u53ef\u884c\u7684\u3002\u6b64\u5916\uff0c\u76ee\u524d\u53ef\u53d8\u5f62\u573a\u666f\u7684\u5355\u76ee\u91cd\u5efa\u8d28\u91cf\u8fd8\u4e0d\u591f\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 Endo-4DGS\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5b9e\u65f6\u5185\u7aa5\u955c\u52a8\u6001\u91cd\u5efa\u65b9\u6cd5\uff0c\u5b83\u5229\u7528 4D \u9ad8\u65af\u5206\u5e03 (GS)\uff0c\u4e0d\u9700\u8981\u5730\u9762\u771f\u5b9e\u6df1\u5ea6\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5408\u5e76\u65f6\u95f4\u7ec4\u4ef6\u6765\u6269\u5c55 3D GS\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7 MLP \u6765\u6355\u83b7\u65f6\u95f4\u9ad8\u65af\u53d8\u5f62\u3002\u8fd9\u6709\u6548\u5730\u4fc3\u8fdb\u4e86\u591a\u6761\u4ef6\u4e0b\u52a8\u6001\u624b\u672f\u573a\u666f\u7684\u91cd\u5efa\u3002\u6211\u4eec\u8fd8\u96c6\u6210 Depth-Anything \u4ece\u5355\u76ee\u89c6\u56fe\u751f\u6210\u4f2a\u6df1\u5ea6\u56fe\uff0c\u589e\u5f3a\u6df1\u5ea6\u5f15\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u7ecf\u5728\u4e24\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u4e8b\u5b9e\u8bc1\u660e\u5b83\u53ef\u4ee5\u5b9e\u65f6\u6e32\u67d3\u3001\u9ad8\u6548\u8ba1\u7b97\u5e76\u4ee5\u6781\u9ad8\u7684\u51c6\u786e\u6027\u8fdb\u884c\u91cd\u5efa\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86 Endo-4DGS \u5728\u6539\u5584\u624b\u672f\u8f85\u52a9\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002|[2401.16416v1](http://arxiv.org/pdf/2401.16416v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.16375": "|**2024-01-29**|**Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator**|\u53d1\u73b0\u9519\u8bef\uff1a\u4f7f\u7528\u7ebf\u6846\u5b9a\u4f4d\u5668\u751f\u6210\u975e\u81ea\u56de\u5f52\u56fe\u5f62\u5e03\u5c40|Jieru Lin, Danqing Huang, Tiejun Zhao, Dechen Zhan, Chin-Yew Lin|Layout generation is a critical step in graphic design to achieve meaningful compositions of elements. Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position). So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens. Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding. However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate. This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework. Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input. We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance. Experiments on two public datasets show that our approach outperforms both AR and NAR baselines. Extensive studies further prove the effectiveness of different modules with interesting findings. Our code will be available at https://github.com/ffffatgoose/SpotError.|\u5e03\u5c40\u751f\u6210\u662f\u56fe\u5f62\u8bbe\u8ba1\u4e2d\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u5143\u7d20\u7ec4\u5408\u7684\u5173\u952e\u6b65\u9aa4\u3002\u5927\u591a\u6570\u4ee5\u524d\u7684\u4f5c\u54c1\u5c06\u5176\u89c6\u4e3a\u901a\u8fc7\u8fde\u63a5\u5143\u7d20\u5c5e\u6027\u6807\u8bb0\uff08\u5373\u7c7b\u522b\u3001\u5927\u5c0f\u3001\u4f4d\u7f6e\uff09\u7684\u5e8f\u5217\u751f\u6210\u95ee\u9898\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u81ea\u56de\u5f52\u65b9\u6cd5\uff08AR\uff09\u5df2\u7ecf\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u4ecd\u7136\u53d7\u5230\u9650\u5236\uff0c\u5e76\u4e14\u7531\u4e8e\u5b83\u53ea\u80fd\u5904\u7406\u5148\u524d\u751f\u6210\u7684\u6807\u8bb0\u800c\u53d7\u5230\u9519\u8bef\u4f20\u64ad\u7684\u5f71\u54cd\u3002\u6700\u8fd1\u7684\u975e\u81ea\u56de\u5f52\u5c1d\u8bd5\uff08NAR\uff09\u5df2\u7ecf\u663e\u793a\u51fa\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5b83\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u8303\u56f4\u548c\u901a\u8fc7\u8fed\u4ee3\u89e3\u7801\u8fdb\u884c\u7ec6\u5316\u7684\u7075\u6d3b\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u5de5\u4f5c\u4ec5\u4f7f\u7528\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u8bc6\u522b\u9519\u8bef\u7684\u6807\u8bb0\u4ee5\u8fdb\u884c\u7ec6\u5316\uff0c\u8fd9\u662f\u4e0d\u51c6\u786e\u7684\u3002\u672c\u6587\u9996\u5148\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3AR\u548cNAR\u6846\u67b6\u4e4b\u95f4\u7684\u533a\u522b\u3002\u6b64\u5916\uff0c\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u50cf\u7d20\u7a7a\u95f4\u5728\u6355\u83b7\u56fe\u5f62\u5e03\u5c40\u7684\u7a7a\u95f4\u6a21\u5f0f\uff08\u4f8b\u5982\u91cd\u53e0\u3001\u5bf9\u9f50\uff09\u65f6\u66f4\u52a0\u654f\u611f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5b9a\u4f4d\u5668\u6765\u68c0\u6d4b\u9519\u8bef\u6807\u8bb0\uff0c\u8be5\u5b9a\u4f4d\u5668\u5c06\u4ece\u751f\u6210\u7684\u5e03\u5c40\u5e8f\u5217\u6e32\u67d3\u7684\u7ebf\u6846\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u3002\u6211\u4eec\u8bc1\u660e\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u5bf9\u8c61\u7a7a\u95f4\u4e2d\u5143\u7d20\u5e8f\u5217\u7684\u8865\u5145\u6a21\u5f0f\uff0c\u5e76\u5bf9\u6574\u4f53\u6027\u80fd\u505a\u51fa\u5de8\u5927\u8d21\u732e\u3002\u5bf9\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e AR \u548c NAR \u57fa\u7ebf\u3002\u5e7f\u6cdb\u7684\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u4e0d\u540c\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f97\u51fa\u4e86\u6709\u8da3\u7684\u53d1\u73b0\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/ffffatgoose/SpotError \u4e0a\u63d0\u4f9b\u3002|[2401.16375v1](http://arxiv.org/pdf/2401.16375v1)|null|\n", "2401.16329": "|**2024-01-29**|**Synthesis of 3D on-air signatures with the Sigma-Lognormal model**|\u4f7f\u7528 Sigma-Lognormal \u6a21\u578b\u5408\u6210 3D \u76f4\u64ad\u7b7e\u540d|Miguel A. Ferrer, Moises Diaz, Cristina Carmona-Duarte, Jose J. Quintana Hernandez, Rejean Plamondon|Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification. A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas. This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves. Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures. Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases. We also observed that training 3D automatic signature verifiers with duplicates can reduce errors. We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures. Finally, a perception test confirmed the human likeness of the generated specimens. The databases generated are publicly available, only for research purposes, at .|\u7b7e\u540d\u5408\u6210\u662f\u4e00\u79cd\u751f\u6210\u4eba\u5de5\u6837\u672c\u7684\u8ba1\u7b97\u6280\u672f\uff0c\u53ef\u4ee5\u652f\u6301\u81ea\u52a8\u7b7e\u540d\u9a8c\u8bc1\u4e2d\u7684\u51b3\u7b56\u3002\u8bb8\u591a\u5de5\u4f5c\u81f4\u529b\u4e8e\u8fd9\u4e2a\u4e3b\u9898\uff0c\u5176\u91cd\u70b9\u662f\u5728\u753b\u5e03\u4e0a\u5408\u6210\u52a8\u6001\u548c\u9759\u6001\u4e8c\u7ef4\u624b\u5199\u4f53\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5bf9\u6570\u6b63\u6001\u6027\u539f\u7406\u751f\u6210\u5408\u6210 3D \u76f4\u64ad\u7b7e\u540d\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u4eff\u6307\u5c16\u79fb\u52a8\u65f6\u8d77\u4f5c\u7528\u7684\u590d\u6742\u795e\u7ecf\u8fd0\u52a8\u63a7\u5236\u8fc7\u7a0b\u3002\u9488\u5bf9\u6d89\u53ca\u4eba\u5de5\u4e2a\u4f53\u548c\u91cd\u590d\u6837\u672c\u5f00\u53d1\u7684\u5e38\u89c1\u60c5\u51b5\uff0c\u672c\u6587\u6709\u52a9\u4e8e\u7efc\u5408\uff1a\uff081\uff09\u5b8c\u5168 3D \u65b0\u7b7e\u540d\u7684\u8f68\u8ff9\u548c\u901f\u5ea6\uff1b (2) \u4ec5\u77e5\u9053\u7b7e\u540d\u7684 3D \u8f68\u8ff9\u65f6\u7684\u8fd0\u52a8\u4fe1\u606f\uff0c\u4ee5\u53ca (3) 3D \u771f\u5b9e\u7b7e\u540d\u7684\u91cd\u590d\u6837\u672c\u3002\u9a8c\u8bc1\u662f\u901a\u8fc7\u751f\u6210\u6a21\u4eff\u771f\u5b9e\u7b7e\u540d\u7684\u5408\u6210 3D \u7b7e\u540d\u6570\u636e\u5e93\u6765\u8fdb\u884c\u7684\uff0c\u5e76\u663e\u793a\u5bf9\u771f\u5b9e\u548c\u719f\u7ec3\u4f2a\u9020\u54c1\u7684\u81ea\u52a8\u7b7e\u540d\u9a8c\u8bc1\u62a5\u200b\u200b\u544a\u7684\u6027\u80fd\u4e0e\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u5e93\u7684\u6027\u80fd\u76f8\u4f3c\u3002\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\uff0c\u4f7f\u7528\u91cd\u590d\u9879\u8bad\u7ec3 3D \u81ea\u52a8\u7b7e\u540d\u9a8c\u8bc1\u5668\u53ef\u4ee5\u51cf\u5c11\u9519\u8bef\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u5efa\u8bae\u5bf9\u4e8e\u5408\u6210 3D \u7a7a\u4e2d\u4e66\u5199\u548c\u624b\u52bf\u4e5f\u6709\u6548\u3002\u6700\u540e\uff0c\u611f\u77e5\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u751f\u6210\u7684\u6837\u672c\u4e0e\u4eba\u7c7b\u7684\u76f8\u4f3c\u6027\u3002\u751f\u6210\u7684\u6570\u636e\u5e93\u53ef\u516c\u5f00\u83b7\u53d6\uff0c\u4ec5\u7528\u4e8e\u7814\u7a76\u76ee\u7684\uff0c\u7f51\u5740\u4e3a \u3002|[2401.16329v1](http://arxiv.org/pdf/2401.16329v1)|null|\n", "2401.16284": "|**2024-01-29**|**Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation**|\u5229\u7528\u4f4d\u7f6e\u7f16\u7801\u8fdb\u884c\u9c81\u68d2\u7684\u57fa\u4e8e\u591a\u53c2\u8003\u7684\u5bf9\u8c61 6D \u59ff\u6001\u4f30\u8ba1|Jaewoo Park, Jaeguk Kim, Nam Ik Cho|Accurately estimating the pose of an object is a crucial task in computer vision and robotics. There are two main deep learning approaches for this: geometric representation regression and iterative refinement. However, these methods have some limitations that reduce their effectiveness. In this paper, we analyze these limitations and propose new strategies to overcome them. To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates. To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints. Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object. Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods. We will soon release the code.|\u51c6\u786e\u4f30\u8ba1\u7269\u4f53\u7684\u59ff\u6001\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\u3002\u4e3a\u6b64\u6709\u4e24\u79cd\u4e3b\u8981\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff1a\u51e0\u4f55\u8868\u793a\u56de\u5f52\u548c\u8fed\u4ee3\u7ec6\u5316\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6709\u4e00\u4e9b\u9650\u5236\uff0c\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u8fd9\u4e9b\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u514b\u670d\u5b83\u4eec\u7684\u65b0\u7b56\u7565\u3002\u4e3a\u4e86\u89e3\u51b3\u51e0\u4f55\u8868\u793a\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u5bf9\u8c61\u7684 3D \u5750\u6807\u4f7f\u7528\u5177\u6709\u9ad8\u9891\u5206\u91cf\u7684\u4f4d\u7f6e\u7f16\u7801\u3002\u4e3a\u4e86\u89e3\u51b3\u7ec6\u5316\u65b9\u6cd5\u4e2d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u5185\u5728\u77e9\u9635\u7ea6\u675f\u7684\u57fa\u4e8e\u5f52\u4e00\u5316\u56fe\u50cf\u5e73\u9762\u7684\u591a\u53c2\u8003\u7ec6\u5316\u7b56\u7565\u3002\u6700\u540e\uff0c\u6211\u4eec\u5229\u7528\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u548c\u7b80\u5355\u7684\u906e\u6321\u589e\u5f3a\u65b9\u6cd5\u6765\u5e2e\u52a9\u6211\u4eec\u7684\u6a21\u578b\u4e13\u6ce8\u4e8e\u76ee\u6807\u5bf9\u8c61\u3002\u6211\u4eec\u5728 Linemod\u3001Linemod-Occlusion \u548c YCB-Video \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6211\u4eec\u5c06\u5f88\u5feb\u53d1\u5e03\u4ee3\u7801\u3002|[2401.16284v1](http://arxiv.org/pdf/2401.16284v1)|null|\n", "2401.16189": "|**2024-01-29**|**FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction**|FIMP\uff1a\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u672a\u6765\u4ea4\u4e92\u5efa\u6a21|Sungmin Woo, Minjung Kim, Donghyeong Kim, Sungjun Jang, Sangyoun Lee|Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.|\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u52a8\u6001\u667a\u80fd\u4f53\u7684\u6a21\u7cca\u610f\u56fe\u53ca\u5176\u590d\u6742\u7684\u4ea4\u4e92\uff0c\u5b83\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u4f7f\u7528\u5386\u53f2\u65f6\u95f4\u6b65\u957f\u4e2d\u7684\u786e\u5b9a\u6570\u636e\u6765\u6355\u83b7\u9053\u8def\u5b9e\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u56e0\u4e3a\u672a\u6765\u4fe1\u606f\u4e0d\u53ef\u7528\u5e76\u4e14\u6d89\u53ca\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u3002\u7136\u800c\uff0c\u5982\u679c\u6ca1\u6709\u8db3\u591f\u7684\u6307\u5bfc\u6765\u6355\u83b7\u4ea4\u4e92\u4ee3\u7406\u7684\u672a\u6765\u72b6\u6001\uff0c\u5b83\u4eec\u7ecf\u5e38\u4f1a\u4ea7\u751f\u4e0d\u5207\u5b9e\u9645\u7684\u8f68\u8ff9\u91cd\u53e0\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd0\u52a8\u9884\u6d4b\u7684\u672a\u6765\u4ea4\u4e92\u5efa\u6a21\uff08FIMP\uff09\uff0c\u5b83\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u6355\u83b7\u6f5c\u5728\u7684\u672a\u6765\u4ea4\u4e92\u3002 FIMP\u91c7\u7528\u672a\u6765\u89e3\u7801\u5668\uff0c\u9690\u5f0f\u63d0\u53d6\u4e2d\u95f4\u7279\u5f81\u7ea7\u522b\u4e2d\u6f5c\u5728\u7684\u672a\u6765\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u672a\u6765\u4eb2\u548c\u529b\u5b66\u4e60\u548ctop-k\u8fc7\u6ee4\u7b56\u7565\u8bc6\u522b\u4ea4\u4e92\u5b9e\u4f53\u5bf9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u672a\u6765\u7684\u4ea4\u4e92\u5efa\u6a21\u663e\u7740\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4ece\u800c\u5728 Argoverse \u8fd0\u52a8\u9884\u6d4b\u57fa\u51c6\u4e0a\u83b7\u5f97\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2401.16189v1](http://arxiv.org/pdf/2401.16189v1)|null|\n", "2401.16122": "|**2024-01-29**|**DeFlow: Decoder of Scene Flow Network in Autonomous Driving**|DeFlow\uff1a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u573a\u666f\u6d41\u7f51\u7edc\u7684\u89e3\u7801\u5668|Qingwen Zhang, Yi Yang, Heng Fang, Ruoyu Geng, Patric Jensfelt|Scene flow estimation determines a scene's 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is open-sourced at https://github.com/KTH-RPL/deflow.|\u573a\u666f\u6d41\u4f30\u8ba1\u901a\u8fc7\u9884\u6d4b\u573a\u666f\u4e2d\u70b9\u7684\u8fd0\u52a8\u6765\u786e\u5b9a\u573a\u666f\u7684 3D \u8fd0\u52a8\u573a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u3002\u8bb8\u591a\u4ee5\u5927\u89c4\u6a21\u70b9\u4e91\u4f5c\u4e3a\u8f93\u5165\u7684\u7f51\u7edc\u4f7f\u7528\u4f53\u7d20\u5316\u6765\u521b\u5efa\u7528\u4e8e\u5b9e\u65f6\u8fd0\u884c\u7684\u4f2a\u56fe\u50cf\u3002\u7136\u800c\uff0c\u4f53\u7d20\u5316\u8fc7\u7a0b\u901a\u5e38\u4f1a\u5bfc\u81f4\u7279\u5b9a\u70b9\u7279\u5f81\u7684\u4e22\u5931\u3002\u8fd9\u7ed9\u573a\u666f\u6d41\u4efb\u52a1\u6062\u590d\u8fd9\u4e9b\u7279\u5f81\u5e26\u6765\u4e86\u6311\u6218\u3002\u6211\u4eec\u7684\u8bba\u6587\u4ecb\u7ecd\u4e86 DeFlow\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u5355\u5143 (GRU) \u7ec6\u5316\u4ece\u57fa\u4e8e\u4f53\u7d20\u7684\u7279\u5f81\u8fc7\u6e21\u5230\u70b9\u7279\u5f81\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u573a\u666f\u6d41\u4f30\u8ba1\u6027\u80fd\uff0c\u6211\u4eec\u5236\u5b9a\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u8003\u8651\u4e86\u9759\u6001\u70b9\u548c\u52a8\u6001\u70b9\u4e4b\u95f4\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3002\u5bf9 Argoverse 2 \u573a\u666f\u6d41\u4efb\u52a1\u7684\u8bc4\u4f30\u8868\u660e\uff0cDeFlow \u5728\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8868\u660e\u6211\u4eec\u7684\u7f51\u7edc\u4e0e\u5176\u4ed6\u7f51\u7edc\u76f8\u6bd4\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002\u8be5\u4ee3\u7801\u5728 https://github.com/KTH-RPL/deflow \u4e0a\u5f00\u6e90\u3002|[2401.16122v1](http://arxiv.org/pdf/2401.16122v1)|null|\n", "2401.16027": "|**2024-01-29**|**Domain adaptation strategies for 3D reconstruction of the lumbar spine using real fluoroscopy data**|\u4f7f\u7528\u771f\u5b9e\u900f\u89c6\u6570\u636e\u8fdb\u884c\u8170\u690e 3D \u91cd\u5efa\u7684\u57df\u9002\u5e94\u7b56\u7565|Sascha Jecklin, Youyang Shen, Amandine Gout, Daniel Suter, Lilian Calvet, Lukas Zingg, Jennifer Straub, Nicola Alessandro Cavalcanti, Mazda Farshad, Philipp F\u00fcrnstahl, et.al.|This study tackles key obstacles in adopting surgical navigation in orthopedic surgeries, including time, cost, radiation, and workflow integration challenges. Recently, our work X23D showed an approach for generating 3D anatomical models of the spine from only a few intraoperative fluoroscopic images. This negates the need for conventional registration-based surgical navigation by creating a direct intraoperative 3D reconstruction of the anatomy. Despite these strides, the practical application of X23D has been limited by a domain gap between synthetic training data and real intraoperative images.   In response, we devised a novel data collection protocol for a paired dataset consisting of synthetic and real fluoroscopic images from the same perspectives. Utilizing this dataset, we refined our deep learning model via transfer learning, effectively bridging the domain gap between synthetic and real X-ray data. A novel style transfer mechanism also allows us to convert real X-rays to mirror the synthetic domain, enabling our in-silico-trained X23D model to achieve high accuracy in real-world settings.   Our results demonstrated that the refined model can rapidly generate accurate 3D reconstructions of the entire lumbar spine from as few as three intraoperative fluoroscopic shots. It achieved an 84% F1 score, matching the accuracy of our previous synthetic data-based research. Additionally, with a computational time of only 81.1 ms, our approach provides real-time capabilities essential for surgery integration.   Through examining ideal imaging setups and view angle dependencies, we've further confirmed our system's practicality and dependability in clinical settings. Our research marks a significant step forward in intraoperative 3D reconstruction, offering enhancements to surgical planning, navigation, and robotics.|\u8fd9\u9879\u7814\u7a76\u89e3\u51b3\u4e86\u5728\u9aa8\u79d1\u624b\u672f\u4e2d\u91c7\u7528\u624b\u672f\u5bfc\u822a\u7684\u5173\u952e\u969c\u788d\uff0c\u5305\u62ec\u65f6\u95f4\u3001\u6210\u672c\u3001\u8f90\u5c04\u548c\u5de5\u4f5c\u6d41\u7a0b\u96c6\u6210\u6311\u6218\u3002\u6700\u8fd1\uff0c\u6211\u4eec\u7684 X23D \u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u4ec5\u901a\u8fc7\u5c11\u91cf\u672f\u4e2d\u900f\u89c6\u56fe\u50cf\u5373\u53ef\u751f\u6210\u810a\u67f1 3D \u89e3\u5256\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u521b\u5efa\u76f4\u63a5\u7684\u672f\u4e2d\u89e3\u5256\u7ed3\u6784 3D \u91cd\u5efa\uff0c\u65e0\u9700\u4f20\u7edf\u7684\u57fa\u4e8e\u914d\u51c6\u7684\u624b\u672f\u5bfc\u822a\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u6b65\uff0cX23D \u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u53d7\u5230\u5408\u6210\u8bad\u7ec3\u6570\u636e\u548c\u771f\u5b9e\u672f\u4e2d\u56fe\u50cf\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u7684\u9650\u5236\u3002\u4f5c\u4e3a\u56de\u5e94\uff0c\u6211\u4eec\u4e3a\u914d\u5bf9\u6570\u636e\u96c6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u6536\u96c6\u534f\u8bae\uff0c\u8be5\u6570\u636e\u96c6\u7531\u6765\u81ea\u76f8\u540c\u89d2\u5ea6\u7684\u5408\u6210\u548c\u771f\u5b9e\u8367\u5149\u900f\u89c6\u56fe\u50cf\u7ec4\u6210\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5b8c\u5584\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6709\u6548\u5730\u5f25\u5408\u4e86\u5408\u6210 X \u5c04\u7ebf\u6570\u636e\u4e0e\u771f\u5b9e X \u5c04\u7ebf\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002\u65b0\u9896\u7684\u4f20\u8f93\u673a\u5236\u8fd8\u5141\u8bb8\u6211\u4eec\u5c06\u771f\u5b9e\u7684 X \u5c04\u7ebf\u8f6c\u6362\u4e3a\u955c\u50cf\u5408\u6210\u57df\uff0c\u4f7f\u6211\u4eec\u7684\u8ba1\u7b97\u673a\u6a21\u62df\u8bad\u7ec3\u7684 X23D \u6a21\u578b\u80fd\u591f\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u6539\u8fdb\u7684\u6a21\u578b\u53ea\u9700\u8fdb\u884c 3 \u6b21\u672f\u4e2d\u900f\u89c6\u62cd\u6444\u5373\u53ef\u5feb\u901f\u751f\u6210\u6574\u4e2a\u8170\u690e\u7684\u51c6\u786e 3D \u91cd\u5efa\u3002\u5b83\u83b7\u5f97\u4e86 84% \u7684 F1 \u5206\u6570\uff0c\u4e0e\u6211\u4eec\u4e4b\u524d\u57fa\u4e8e\u7efc\u5408\u6570\u636e\u7684\u7814\u7a76\u7684\u51c6\u786e\u6027\u76f8\u5339\u914d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a 81.1 \u6beb\u79d2\uff0c\u63d0\u4f9b\u4e86\u624b\u672f\u96c6\u6210\u6240\u5fc5\u9700\u7684\u5b9e\u65f6\u529f\u80fd\u3002\u901a\u8fc7\u68c0\u67e5\u7406\u60f3\u7684\u6210\u50cf\u8bbe\u7f6e\u548c\u89c6\u89d2\u4f9d\u8d56\u6027\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u6807\u5fd7\u7740\u672f\u4e2d 3D \u91cd\u5efa\u9886\u57df\u5411\u524d\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u624b\u672f\u89c4\u5212\u3001\u5bfc\u822a\u548c\u673a\u5668\u4eba\u6280\u672f\u63d0\u4f9b\u4e86\u589e\u5f3a\u3002|[2401.16027v1](http://arxiv.org/pdf/2401.16027v1)|null|\n", "2401.15996": "|**2024-01-29**|**AccessLens: Auto-detecting Inaccessibility of Everyday Objects**|AccessLens\uff1a\u81ea\u52a8\u68c0\u6d4b\u65e5\u5e38\u5bf9\u8c61\u7684\u4e0d\u53ef\u8bbf\u95ee\u6027|Nahyun Kwon, Qian Lu, Muhammad Hasham Qazi, Joanne Liu, Changhoon Oh, Shu Kong, Jeeeun Kim|In our increasingly diverse society, everyday physical interfaces often present barriers, impacting individuals across various contexts. This oversight, from small cabinet knobs to identical wall switches that can pose different contextual challenges, highlights an imperative need for solutions. Leveraging low-cost 3D-printed augmentations such as knob magnifiers and tactile labels seems promising, yet the process of discovering unrecognized barriers remains challenging because disability is context-dependent. We introduce AccessLens, an end-to-end system designed to identify inaccessible interfaces in daily objects, and recommend 3D-printable augmentations for accessibility enhancement. Our approach involves training a detector using the novel AccessDB dataset designed to automatically recognize 21 distinct Inaccessibility Classes (e.g., bar-small and round-rotate) within 6 common object categories (e.g., handle and knob). AccessMeta serves as a robust way to build a comprehensive dictionary linking these accessibility classes to open-source 3D augmentation designs. Experiments demonstrate our detector's performance in detecting inaccessible objects.|\u5728\u6211\u4eec\u65e5\u76ca\u591a\u5143\u5316\u7684\u793e\u4f1a\u4e2d\uff0c\u65e5\u5e38\u7269\u7406\u754c\u9762\u5e38\u5e38\u4f1a\u5e26\u6765\u969c\u788d\uff0c\u5f71\u54cd\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u4e2a\u4eba\u3002\u8fd9\u79cd\u758f\u5ffd\uff0c\u4ece\u5c0f\u578b\u6a71\u67dc\u65cb\u94ae\u5230\u76f8\u540c\u7684\u5899\u58c1\u5f00\u5173\uff0c\u53ef\u80fd\u4f1a\u5e26\u6765\u4e0d\u540c\u7684\u73af\u5883\u6311\u6218\uff0c\u51f8\u663e\u4e86\u5bf9\u89e3\u51b3\u65b9\u6848\u7684\u8feb\u5207\u9700\u6c42\u3002\u5229\u7528\u65cb\u94ae\u653e\u5927\u955c\u548c\u89e6\u89c9\u6807\u7b7e\u7b49\u4f4e\u6210\u672c 3D \u6253\u5370\u589e\u5f3a\u529f\u80fd\u4f3c\u4e4e\u5f88\u6709\u5e0c\u671b\uff0c\u4f46\u53d1\u73b0\u672a\u88ab\u8bc6\u522b\u7684\u969c\u788d\u7684\u8fc7\u7a0b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6b8b\u75be\u662f\u4e0e\u73af\u5883\u76f8\u5173\u7684\u3002\u6211\u4eec\u63a8\u51fa\u4e86 AccessLens\uff0c\u8fd9\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u65e8\u5728\u8bc6\u522b\u65e5\u5e38\u7269\u54c1\u4e2d\u4e0d\u53ef\u8bbf\u95ee\u7684\u754c\u9762\uff0c\u5e76\u63a8\u8350\u53ef 3D \u6253\u5370\u7684\u589e\u5f3a\u529f\u80fd\u6765\u589e\u5f3a\u53ef\u8bbf\u95ee\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u65b0\u9896\u7684 AccessDB \u6570\u636e\u96c6\u6765\u8bad\u7ec3\u68c0\u6d4b\u5668\uff0c\u8be5\u6570\u636e\u96c6\u65e8\u5728\u81ea\u52a8\u8bc6\u522b 6 \u4e2a\u5e38\u89c1\u5bf9\u8c61\u7c7b\u522b\uff08\u4f8b\u5982\u624b\u67c4\u548c\u65cb\u94ae\uff09\u5185\u7684 21 \u4e2a\u4e0d\u540c\u7684\u4e0d\u53ef\u8bbf\u95ee\u6027\u7c7b\u522b\uff08\u4f8b\u5982\uff0c\u6761\u5f62\u5c0f\u548c\u5706\u5f62\u65cb\u8f6c\uff09\u3002 AccessMeta \u662f\u6784\u5efa\u7efc\u5408\u5b57\u5178\u7684\u5f3a\u5927\u65b9\u6cd5\uff0c\u5c06\u8fd9\u4e9b\u8f85\u52a9\u529f\u80fd\u7c7b\u4e0e\u5f00\u6e90 3D \u589e\u5f3a\u8bbe\u8ba1\u8054\u7cfb\u8d77\u6765\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u63a2\u6d4b\u5668\u5728\u63a2\u6d4b\u96be\u4ee5\u63a5\u8fd1\u7684\u7269\u4f53\u65b9\u9762\u7684\u6027\u80fd\u3002|[2401.15996v1](http://arxiv.org/pdf/2401.15996v1)|null|\n", "2401.15987": "|**2024-01-29**|**Hand-Centric Motion Refinement for 3D Hand-Object Interaction via Hierarchical Spatial-Temporal Modeling**|\u901a\u8fc7\u5206\u5c42\u65f6\u7a7a\u5efa\u6a21\u5b9e\u73b0\u4ee5\u624b\u4e3a\u4e2d\u5fc3\u7684 3D \u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u8fd0\u52a8\u7ec6\u5316|Yuze Hao, Jianrong Zhang, Tao Zhuo, Fuan Wen, Hehe Fan|Hands are the main medium when people interact with the world. Generating proper 3D motion for hand-object interaction is vital for applications such as virtual reality and robotics. Although grasp tracking or object manipulation synthesis can produce coarse hand motion, this kind of motion is inevitably noisy and full of jitter. To address this problem, we propose a data-driven method for coarse motion refinement. First, we design a hand-centric representation to describe the dynamic spatial-temporal relation between hands and objects. Compared to the object-centric representation, our hand-centric representation is straightforward and does not require an ambiguous projection process that converts object-based prediction into hand motion. Second, to capture the dynamic clues of hand-object interaction, we propose a new architecture that models the spatial and temporal structure in a hierarchical manner. Extensive experiments demonstrate that our method outperforms previous methods by a noticeable margin.|\u624b\u662f\u4eba\u4eec\u4e0e\u4e16\u754c\u4e92\u52a8\u7684\u4e3b\u8981\u5a92\u4ecb\u3002\u4e3a\u624b\u90e8\u7269\u4f53\u4ea4\u4e92\u751f\u6210\u9002\u5f53\u7684 3D \u8fd0\u52a8\u5bf9\u4e8e\u865a\u62df\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6293\u53d6\u8ddf\u8e2a\u6216\u5bf9\u8c61\u64cd\u7eb5\u5408\u6210\u53ef\u4ee5\u4ea7\u751f\u7c97\u7cd9\u7684\u624b\u90e8\u8fd0\u52a8\uff0c\u4f46\u8fd9\u79cd\u8fd0\u52a8\u4e0d\u53ef\u907f\u514d\u5730\u5145\u6ee1\u566a\u58f0\u4e14\u5145\u6ee1\u6296\u52a8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u7c97\u8fd0\u52a8\u7ec6\u5316\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4ee5\u624b\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u6765\u63cf\u8ff0\u624b\u548c\u7269\u4f53\u4e4b\u95f4\u7684\u52a8\u6001\u65f6\u7a7a\u5173\u7cfb\u3002\u4e0e\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u76f8\u6bd4\uff0c\u6211\u4eec\u4ee5\u624b\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u5f88\u7b80\u5355\uff0c\u4e0d\u9700\u8981\u5c06\u57fa\u4e8e\u5bf9\u8c61\u7684\u9884\u6d4b\u8f6c\u6362\u4e3a\u624b\u90e8\u8fd0\u52a8\u7684\u6a21\u7cca\u6295\u5f71\u8fc7\u7a0b\u3002\u5176\u6b21\uff0c\u4e3a\u4e86\u6355\u6349\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u52a8\u6001\u7ebf\u7d22\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u4ee5\u5206\u5c42\u65b9\u5f0f\u5bf9\u7a7a\u95f4\u548c\u65f6\u95f4\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002|[2401.15987v1](http://arxiv.org/pdf/2401.15987v1)|null|\n", "2401.15975": "|**2024-01-29**|**StableIdentity: Inserting Anybody into Anywhere at First Sight**|StableIdentity\uff1a\u7b2c\u4e00\u773c\u5c31\u628a\u4efb\u4f55\u4eba\u63d2\u5165\u5230\u4efb\u4f55\u5730\u65b9|Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, Huchuan Lu|Recent advances in large pretrained text-to-image models have shown unprecedented capabilities for high-quality human-centric generation, however, customizing face identity is still an intractable problem. Existing methods cannot ensure stable identity preservation and flexible editability, even with several images for each subject during training. In this work, we propose StableIdentity, which allows identity-consistent recontextualization with just one face image. More specifically, we employ a face encoder with an identity prior to encode the input face, and then land the face representation into a space with an editable prior, which is constructed from celeb names. By incorporating identity prior and editability prior, the learned identity can be injected anywhere with various contexts. In addition, we design a masked two-phase diffusion loss to boost the pixel-level perception of the input face and maintain the diversity of generation. Extensive experiments demonstrate our method outperforms previous customization methods. In addition, the learned identity can be flexibly combined with the off-the-shelf modules such as ControlNet. Notably, to the best knowledge, we are the first to directly inject the identity learned from a single image into video/3D generation without finetuning. We believe that the proposed StableIdentity is an important step to unify image, video, and 3D customized generation models.|\u5927\u578b\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u793a\u51fa\u524d\u6240\u672a\u6709\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u80fd\u529b\uff0c\u7136\u800c\uff0c\u5b9a\u5236\u9762\u90e8\u8eab\u4efd\u4ecd\u7136\u662f\u4e00\u4e2a\u68d8\u624b\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u786e\u4fdd\u7a33\u5b9a\u7684\u8eab\u4efd\u4fdd\u5b58\u548c\u7075\u6d3b\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u671f\u95f4\u6bcf\u4e2a\u53d7\u8bd5\u8005\u6709\u591a\u4e2a\u56fe\u50cf\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 StableIdentity\uff0c\u5b83\u5141\u8bb8\u4ec5\u4f7f\u7528\u4e00\u5f20\u9762\u90e8\u56fe\u50cf\u8fdb\u884c\u8eab\u4efd\u4e00\u81f4\u7684\u91cd\u65b0\u4e0a\u4e0b\u6587\u5316\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u5177\u6709\u8eab\u4efd\u5148\u9a8c\u7684\u9762\u90e8\u7f16\u7801\u5668\u5bf9\u8f93\u5165\u9762\u90e8\u8fdb\u884c\u7f16\u7801\uff0c\u7136\u540e\u5c06\u9762\u90e8\u8868\u793a\u653e\u5165\u5177\u6709\u53ef\u7f16\u8f91\u5148\u9a8c\u7684\u7a7a\u95f4\u4e2d\uff0c\u8be5\u5148\u9a8c\u662f\u7531\u540d\u4eba\u59d3\u540d\u6784\u9020\u7684\u3002\u901a\u8fc7\u5408\u5e76\u8eab\u4efd\u5148\u9a8c\u548c\u53ef\u7f16\u8f91\u6027\u5148\u9a8c\uff0c\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u53ef\u4ee5\u6ce8\u5165\u5230\u5177\u6709\u5404\u79cd\u4e0a\u4e0b\u6587\u7684\u4efb\u4f55\u5730\u65b9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a9\u7801\u4e24\u76f8\u6269\u6563\u635f\u5931\u6765\u589e\u5f3a\u8f93\u5165\u4eba\u8138\u7684\u50cf\u7d20\u7ea7\u611f\u77e5\u5e76\u4fdd\u6301\u751f\u6210\u7684\u591a\u6837\u6027\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u524d\u7684\u5b9a\u5236\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u53ef\u4ee5\u4e0eControlNet\u7b49\u73b0\u6210\u6a21\u5757\u7075\u6d3b\u7ed3\u5408\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u636e\u4e86\u89e3\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5c06\u4ece\u5355\u4e2a\u56fe\u50cf\u4e2d\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u76f4\u63a5\u6ce8\u5165\u5230\u89c6\u9891/3D \u751f\u6210\u4e2d\u800c\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u7684\u4eba\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u6240\u63d0\u51fa\u7684 StableIdentity \u662f\u7edf\u4e00\u56fe\u50cf\u3001\u89c6\u9891\u548c 3D \u5b9a\u5236\u751f\u6210\u6a21\u578b\u7684\u91cd\u8981\u4e00\u6b65\u3002|[2401.15975v1](http://arxiv.org/pdf/2401.15975v1)|null|\n", "2401.15969": "|**2024-01-29**|**Routers in Vision Mixture of Experts: An Empirical Study**|\u4e13\u5bb6\u89c6\u89c9\u6df7\u5408\u4e2d\u7684\u8def\u7531\u5668\uff1a\u5b9e\u8bc1\u7814\u7a76|Tianlin Liu, Mathieu Blondel, Carlos Riquelme, Joan Puigcerver|Mixture-of-Experts (MoE) models are a promising way to scale up model capacity without significantly increasing computational cost. A key component of MoEs is the router, which decides which subset of parameters (experts) process which feature embeddings (tokens). In this paper, we present a comprehensive study of routers in MoEs for computer vision tasks. We introduce a unified MoE formulation that subsumes different MoEs with two parametric routing tensors. This formulation covers both sparse MoE, which uses a binary or hard assignment between experts and tokens, and soft MoE, which uses a soft assignment between experts and weighted combinations of tokens. Routers for sparse MoEs can be further grouped into two variants: Token Choice, which matches experts to each token, and Expert Choice, which matches tokens to each expert. We conduct head-to-head experiments with 6 different routers, including existing routers from prior work and new ones we introduce. We show that (i) many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers generally outperform Token Choice routers, and (iii) soft MoEs generally outperform sparse MoEs with a fixed compute budget. These results provide new insights regarding the crucial role of routers in vision MoE models.|\u4e13\u5bb6\u6df7\u5408 (MoE) \u6a21\u578b\u662f\u4e00\u79cd\u5728\u4e0d\u663e\u7740\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u6269\u5927\u6a21\u578b\u5bb9\u91cf\u7684\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002 MoE \u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u4ef6\u662f\u8def\u7531\u5668\uff0c\u5b83\u51b3\u5b9a\u54ea\u4e2a\u53c2\u6570\u5b50\u96c6\uff08\u4e13\u5bb6\uff09\u5904\u7406\u54ea\u4e9b\u7279\u5f81\u5d4c\u5165\uff08\u4ee4\u724c\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9 MoE \u4e2d\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u8def\u7531\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7814\u7a76\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684 MoE \u516c\u5f0f\uff0c\u5176\u4e2d\u5305\u542b\u5177\u6709\u4e24\u4e2a\u53c2\u6570\u8def\u7531\u5f20\u91cf\u7684\u4e0d\u540c MoE\u3002\u8be5\u516c\u5f0f\u6db5\u76d6\u4e86\u7a00\u758f MoE\uff08\u5728\u4e13\u5bb6\u548c\u4ee4\u724c\u4e4b\u95f4\u4f7f\u7528\u4e8c\u8fdb\u5236\u6216\u786c\u5206\u914d\uff09\u548c\u8f6f MoE\uff08\u5728\u4e13\u5bb6\u548c\u4ee4\u724c\u7684\u52a0\u6743\u7ec4\u5408\u4e4b\u95f4\u4f7f\u7528\u8f6f\u5206\u914d\uff09\u3002\u7a00\u758f MoE \u7684\u8def\u7531\u5668\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5206\u4e3a\u4e24\u79cd\u53d8\u4f53\uff1a\u4ee4\u724c\u9009\u62e9\uff08\u5c06\u4e13\u5bb6\u4e0e\u6bcf\u4e2a\u4ee4\u724c\u5339\u914d\uff09\u548c\u4e13\u5bb6\u9009\u62e9\uff08\u5c06\u4ee4\u724c\u4e0e\u6bcf\u4e2a\u4e13\u5bb6\u5339\u914d\uff09\u3002\u6211\u4eec\u4f7f\u7528 6 \u4e2a\u4e0d\u540c\u7684\u8def\u7531\u5668\u8fdb\u884c\u4e86\u5934\u5bf9\u5934\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec\u4e4b\u524d\u5de5\u4f5c\u4e2d\u7684\u73b0\u6709\u8def\u7531\u5668\u548c\u6211\u4eec\u5f15\u5165\u7684\u65b0\u8def\u7531\u5668\u3002\u6211\u4eec\u8868\u660e\uff0c\uff08i\uff09\u8bb8\u591a\u6700\u521d\u4e3a\u8bed\u8a00\u5efa\u6a21\u800c\u5f00\u53d1\u7684\u8def\u7531\u5668\u53ef\u4ee5\u9002\u5e94\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\uff08ii\uff09\u5728\u7a00\u758f MoE \u4e2d\uff0c\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u5668\u901a\u5e38\u4f18\u4e8e\u4ee4\u724c\u9009\u62e9\u8def\u7531\u5668\uff0c\u4ee5\u53ca\uff08iii\uff09\u8f6f MoE \u901a\u5e38\u4f18\u4e8e\u7a00\u758f MoE\u56fa\u5b9a\u7684\u8ba1\u7b97\u9884\u7b97\u3002\u8fd9\u4e9b\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u8def\u7531\u5668\u5728\u89c6\u89c9 MoE \u6a21\u578b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u7684\u65b0\u89c1\u89e3\u3002|[2401.15969v1](http://arxiv.org/pdf/2401.15969v1)|null|\n", "2401.15938": "|**2024-01-29**|**Motion-induced error reduction for high-speed dynamic digital fringe projection system**|\u9ad8\u901f\u52a8\u6001\u6570\u5b57\u6761\u7eb9\u6295\u5f71\u7cfb\u7edf\u7684\u8fd0\u52a8\u5f15\u8d77\u7684\u8bef\u5dee\u51cf\u5c11|Sanghoon Jeon, Hyo-Geon Lee, Jae-Sung Lee, Bo-Min Kang, Byung-Wook Jeon, Jun Young Yoon, Jae-Sang Hyun|In phase-shifting profilometry (PSP), any motion during the acquisition of fringe patterns can introduce errors because it assumes both the object and measurement system are stationary. Therefore, we propose a method to pixel-wise reduce the errors when the measurement system is in motion due to a motorized linear stage. The proposed method introduces motion-induced error reduction algorithm, which leverages the motor's encoder and pinhole model of the camera and projector. 3D shape measurement is possible with only three fringe patterns by applying geometric constraints of the digital fringe projection system. We address the mismatch problem due to the motion-induced camera pixel disparities and reduce phase-shift errors. These processes are easy to implement and require low computational cost. Experimental results demonstrate that the presented method effectively reduces the errors even in non-uniform motion.|\u5728\u76f8\u79fb\u8f6e\u5ed3\u6d4b\u91cf (PSP) \u4e2d\uff0c\u91c7\u96c6\u6761\u7eb9\u56fe\u6848\u671f\u95f4\u7684\u4efb\u4f55\u8fd0\u52a8\u90fd\u53ef\u80fd\u5f15\u5165\u8bef\u5dee\uff0c\u56e0\u4e3a\u5b83\u5047\u8bbe\u7269\u4f53\u548c\u6d4b\u91cf\u7cfb\u7edf\u90fd\u662f\u9759\u6b62\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6d4b\u91cf\u7cfb\u7edf\u7531\u4e8e\u7535\u52a8\u7ebf\u6027\u5e73\u53f0\u800c\u8fd0\u52a8\u65f6\u9010\u50cf\u7d20\u51cf\u5c11\u8bef\u5dee\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u8fd0\u52a8\u5f15\u8d77\u7684\u8bef\u5dee\u51cf\u5c11\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u7535\u673a\u7684\u7f16\u7801\u5668\u4ee5\u53ca\u76f8\u673a\u548c\u6295\u5f71\u4eea\u7684\u9488\u5b54\u6a21\u578b\u3002\u901a\u8fc7\u5e94\u7528\u6570\u5b57\u6761\u7eb9\u6295\u5f71\u7cfb\u7edf\u7684\u51e0\u4f55\u7ea6\u675f\uff0c\u4ec5\u4f7f\u7528\u4e09\u4e2a\u6761\u7eb9\u56fe\u6848\u5373\u53ef\u8fdb\u884c 3D \u5f62\u72b6\u6d4b\u91cf\u3002\u6211\u4eec\u89e3\u51b3\u4e86\u7531\u4e8e\u8fd0\u52a8\u5f15\u8d77\u7684\u76f8\u673a\u50cf\u7d20\u5dee\u5f02\u800c\u5bfc\u81f4\u7684\u5931\u914d\u95ee\u9898\uff0c\u5e76\u51cf\u5c11\u4e86\u76f8\u79fb\u8bef\u5dee\u3002\u8fd9\u4e9b\u8fc7\u7a0b\u6613\u4e8e\u5b9e\u73b0\u5e76\u4e14\u9700\u8981\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u975e\u5300\u901f\u8fd0\u52a8\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u6709\u6548\u5730\u51cf\u5c11\u8bef\u5dee\u3002|[2401.15938v1](http://arxiv.org/pdf/2401.15938v1)|null|\n", "2401.15913": "|**2024-01-29**|**Vision-Informed Flow Image Super-Resolution with Quaternion Spatial Modeling and Dynamic Flow Convolution**|\u57fa\u4e8e\u56db\u5143\u6570\u7a7a\u95f4\u5efa\u6a21\u548c\u52a8\u6001\u6d41\u5377\u79ef\u7684\u89c6\u89c9\u4fe1\u606f\u6d41\u56fe\u50cf\u8d85\u5206\u8fa8\u7387|Qinglong Cao, Zhengqin Xu, Chao Ma, Xiaokang Yang, Yuntian Chen|Flow image super-resolution (FISR) aims at recovering high-resolution turbulent velocity fields from low-resolution flow images. Existing FISR methods mainly process the flow images in natural image patterns, while the critical and distinct flow visual properties are rarely considered. This negligence would cause the significant domain gap between flow and natural images to severely hamper the accurate perception of flow turbulence, thereby undermining super-resolution performance. To tackle this dilemma, we comprehensively consider the flow visual properties, including the unique flow imaging principle and morphological information, and propose the first flow visual property-informed FISR algorithm. Particularly, different from natural images that are constructed by independent RGB channels in the light field, flow images build on the orthogonal UVW velocities in the flow field. To empower the FISR network with an awareness of the flow imaging principle, we propose quaternion spatial modeling to model this orthogonal spatial relationship for improved FISR. Moreover, due to viscosity and surface tension characteristics, fluids often exhibit a droplet-like morphology in flow images. Inspired by this morphological property, we design the dynamic flow convolution to effectively mine the morphological information to enhance FISR. Extensive experiments on the newly acquired flow image datasets demonstrate the state-of-the-art performance of our method. Code and data will be made available.|\u6d41\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08FISR\uff09\u65e8\u5728\u4ece\u4f4e\u5206\u8fa8\u7387\u6d41\u56fe\u50cf\u4e2d\u6062\u590d\u9ad8\u5206\u8fa8\u7387\u6e4d\u6d41\u901f\u5ea6\u573a\u3002\u73b0\u6709\u7684 FISR \u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u81ea\u7136\u56fe\u50cf\u6a21\u5f0f\u4e2d\u7684\u6d41\u56fe\u50cf\uff0c\u800c\u5f88\u5c11\u8003\u8651\u5173\u952e\u548c\u72ec\u7279\u7684\u6d41\u89c6\u89c9\u5c5e\u6027\u3002\u8fd9\u79cd\u758f\u5ffd\u4f1a\u5bfc\u81f4\u6d41\u52a8\u548c\u81ea\u7136\u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u57df\u5dee\u8ddd\uff0c\u4e25\u91cd\u59a8\u788d\u6d41\u52a8\u6e4d\u6d41\u7684\u51c6\u786e\u611f\u77e5\uff0c\u4ece\u800c\u7834\u574f\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u56f0\u5883\uff0c\u6211\u4eec\u7efc\u5408\u8003\u8651\u6d41\u89c6\u89c9\u7279\u6027\uff0c\u5305\u62ec\u72ec\u7279\u7684\u6d41\u6210\u50cf\u539f\u7406\u548c\u5f62\u6001\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u6d41\u89c6\u89c9\u7279\u6027\u7684FISR\u7b97\u6cd5\u3002\u7279\u522b\u662f\uff0c\u4e0e\u5149\u573a\u4e2d\u7531\u72ec\u7acb\u7684 RGB \u901a\u9053\u6784\u5efa\u7684\u81ea\u7136\u56fe\u50cf\u4e0d\u540c\uff0c\u6d41\u56fe\u50cf\u5efa\u7acb\u5728\u6d41\u573a\u4e2d\u7684\u6b63\u4ea4 UVW \u901f\u5ea6\u4e4b\u4e0a\u3002\u4e3a\u4e86\u4f7f FISR \u7f51\u7edc\u80fd\u591f\u4e86\u89e3\u6d41\u6210\u50cf\u539f\u7406\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u56db\u5143\u6570\u7a7a\u95f4\u5efa\u6a21\u6765\u5bf9\u8fd9\u79cd\u6b63\u4ea4\u7a7a\u95f4\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u6539\u8fdb FISR\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u7c98\u5ea6\u548c\u8868\u9762\u5f20\u529b\u7279\u6027\uff0c\u6d41\u4f53\u901a\u5e38\u5728\u6d41\u56fe\u50cf\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u6db2\u6ef4\u7684\u5f62\u6001\u3002\u53d7\u8fd9\u79cd\u5f62\u6001\u5b66\u7279\u6027\u7684\u542f\u53d1\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u52a8\u6001\u6d41\u5377\u79ef\u6765\u6709\u6548\u6316\u6398\u5f62\u6001\u5b66\u4fe1\u606f\u4ee5\u589e\u5f3a FISR\u3002\u5bf9\u65b0\u83b7\u53d6\u7684\u6d41\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5c06\u63d0\u4f9b\u4ee3\u7801\u548c\u6570\u636e\u3002|[2401.15913v1](http://arxiv.org/pdf/2401.15913v1)|null|\n", "2401.15877": "|**2024-01-29**|**3DPFIX: Improving Remote Novices' 3D Printing Troubleshooting through Human-AI Collaboration**|3DPFIX\uff1a\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6539\u5584\u8fdc\u7a0b\u65b0\u624b\u7684 3D \u6253\u5370\u6545\u969c\u6392\u9664|Nahyun Kwon, Tong Sun, Yuyang Gao, Liang Zhao, Xu Wang, Jeeeun Kim, Sungsoo Ray Hong|The widespread consumer-grade 3D printers and learning resources online enable novices to self-train in remote settings. While troubleshooting plays an essential part of 3D printing, the process remains challenging for many remote novices even with the help of well-developed online sources, such as online troubleshooting archives and online community help. We conducted a formative study with 76 active 3D printing users to learn how remote novices leverage online resources in troubleshooting and their challenges. We found that remote novices cannot fully utilize online resources. For example, the online archives statically provide general information, making it hard to search and relate their unique cases with existing descriptions. Online communities can potentially ease their struggles by providing more targeted suggestions, but a helper who can provide custom help is rather scarce, making it hard to obtain timely assistance. We propose 3DPFIX, an interactive 3D troubleshooting system powered by the pipeline to facilitate Human-AI Collaboration, designed to improve novices' 3D printing experiences and thus help them easily accumulate their domain knowledge. We built 3DPFIX that supports automated diagnosis and solution-seeking. 3DPFIX was built upon shared dialogues about failure cases from Q\\&A discourses accumulated in online communities. We leverage social annotations (i.e., comments) to build an annotated failure image dataset for AI classifiers and extract a solution pool. Our summative study revealed that using 3DPFIX helped participants spend significantly less effort in diagnosing failures and finding a more accurate solution than relying on their common practice. We also found that 3DPFIX users learn about 3D printing domain-specific knowledge. We discuss the implications of leveraging community-driven data in developing future Human-AI Collaboration designs.|\u5e7f\u6cdb\u4f7f\u7528\u7684\u6d88\u8d39\u7ea7 3D \u6253\u5370\u673a\u548c\u5728\u7ebf\u5b66\u4e60\u8d44\u6e90\u4f7f\u65b0\u624b\u80fd\u591f\u5728\u8fdc\u7a0b\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u6211\u8bad\u7ec3\u3002\u867d\u7136\u6545\u969c\u6392\u9664\u5728 3D \u6253\u5370\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5373\u4f7f\u6709\u5b8c\u5584\u7684\u5728\u7ebf\u8d44\u6e90\uff08\u4f8b\u5982\u5728\u7ebf\u6545\u969c\u6392\u9664\u6863\u6848\u548c\u5728\u7ebf\u793e\u533a\u5e2e\u52a9\uff09\u7684\u5e2e\u52a9\uff0c\u8be5\u8fc7\u7a0b\u5bf9\u4e8e\u8bb8\u591a\u8fdc\u7a0b\u65b0\u624b\u6765\u8bf4\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u5bf9 76 \u540d\u6d3b\u8dc3 3D \u6253\u5370\u7528\u6237\u8fdb\u884c\u4e86\u4e00\u9879\u5f62\u6210\u6027\u7814\u7a76\uff0c\u4ee5\u4e86\u89e3\u8fdc\u7a0b\u65b0\u624b\u5982\u4f55\u5229\u7528\u5728\u7ebf\u8d44\u6e90\u8fdb\u884c\u6545\u969c\u6392\u9664\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\u3002\u6211\u4eec\u53d1\u73b0\u8fdc\u7a0b\u65b0\u624b\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5728\u7ebf\u8d44\u6e90\u3002\u4f8b\u5982\uff0c\u5728\u7ebf\u6863\u6848\u9759\u6001\u5730\u63d0\u4f9b\u4e00\u822c\u4fe1\u606f\uff0c\u4f7f\u5f97\u5f88\u96be\u641c\u7d22\u5176\u72ec\u7279\u6848\u4f8b\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u63cf\u8ff0\u8054\u7cfb\u8d77\u6765\u3002\u5728\u7ebf\u793e\u533a\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u66f4\u6709\u9488\u5bf9\u6027\u7684\u5efa\u8bae\u6765\u7f13\u89e3\u4ed6\u4eec\u7684\u56f0\u5883\uff0c\u4f46\u80fd\u591f\u63d0\u4f9b\u5b9a\u5236\u5e2e\u52a9\u7684\u5e2e\u52a9\u8005\u76f8\u5f53\u7a00\u7f3a\uff0c\u56e0\u6b64\u5f88\u96be\u83b7\u5f97\u53ca\u65f6\u7684\u5e2e\u52a9\u3002\u6211\u4eec\u63d0\u51fa\u4e86 3DPFIX\uff0c\u8fd9\u662f\u4e00\u79cd\u7531\u7ba1\u9053\u63d0\u4f9b\u652f\u6301\u7684\u4ea4\u4e92\u5f0f 3D \u6545\u969c\u6392\u9664\u7cfb\u7edf\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\uff0c\u65e8\u5728\u6539\u5584\u65b0\u624b\u7684 3D \u6253\u5370\u4f53\u9a8c\uff0c\u4ece\u800c\u5e2e\u52a9\u4ed6\u4eec\u8f7b\u677e\u79ef\u7d2f\u9886\u57df\u77e5\u8bc6\u3002\u6211\u4eec\u6784\u5efa\u4e86\u652f\u6301\u81ea\u52a8\u8bca\u65ad\u548c\u89e3\u51b3\u65b9\u6848\u5bfb\u6c42\u7684 3DPFIX\u3002 3DPFIX \u662f\u57fa\u4e8e\u5728\u7ebf\u793e\u533a\u4e2d\u79ef\u7d2f\u7684\u95ee\u7b54\u8ba8\u8bba\u4e2d\u6709\u5173\u5931\u8d25\u6848\u4f8b\u7684\u5171\u4eab\u5bf9\u8bdd\u800c\u6784\u5efa\u7684\u3002\u6211\u4eec\u5229\u7528\u793e\u4ea4\u6ce8\u91ca\uff08\u5373\u8bc4\u8bba\uff09\u4e3a\u4eba\u5de5\u667a\u80fd\u5206\u7c7b\u5668\u6784\u5efa\u5e26\u6ce8\u91ca\u7684\u6545\u969c\u56fe\u50cf\u6570\u636e\u96c6\u5e76\u63d0\u53d6\u89e3\u51b3\u65b9\u6848\u6c60\u3002\u6211\u4eec\u7684\u603b\u7ed3\u6027\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f9d\u8d56\u5e38\u89c4\u5b9e\u8df5\u76f8\u6bd4\uff0c\u4f7f\u7528 3DPFIX \u5e2e\u52a9\u53c2\u4e0e\u8005\u5728\u8bca\u65ad\u6545\u969c\u548c\u627e\u5230\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u82b1\u8d39\u7684\u7cbe\u529b\u663e\u7740\u51cf\u5c11\u3002\u6211\u4eec\u8fd8\u53d1\u73b0 3DPFIX \u7528\u6237\u5b66\u4e60\u4e86 3D \u6253\u5370\u9886\u57df\u7684\u7279\u5b9a\u77e5\u8bc6\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u5229\u7528\u793e\u533a\u9a71\u52a8\u7684\u6570\u636e\u6765\u5f00\u53d1\u672a\u6765\u7684\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002|[2401.15877v1](http://arxiv.org/pdf/2401.15877v1)|null|\n", "2401.15841": "|**2024-01-29**|**2L3: Lifting Imperfect Generated 2D Images into Accurate 3D**|2L3\uff1a\u5c06\u4e0d\u5b8c\u7f8e\u7684\u751f\u6210 2D \u56fe\u50cf\u63d0\u5347\u4e3a\u7cbe\u786e\u7684 3D|Yizheng Chen, Rengan Xie, Qi Ye, Sen Yang, Zixuan Xie, Tianxiao Chen, Rong Li, Yuchi Huo|Reconstructing 3D objects from a single image is an intriguing but challenging problem. One promising solution is to utilize multi-view (MV) 3D reconstruction to fuse generated MV images into consistent 3D objects. However, the generated images usually suffer from inconsistent lighting, misaligned geometry, and sparse views, leading to poor reconstruction quality. To cope with these problems, we present a novel 3D reconstruction framework that leverages intrinsic decomposition guidance, transient-mono prior guidance, and view augmentation to cope with the three issues, respectively. Specifically, we first leverage to decouple the shading information from the generated images to reduce the impact of inconsistent lighting; then, we introduce mono prior with view-dependent transient encoding to enhance the reconstructed normal; and finally, we design a view augmentation fusion strategy that minimizes pixel-level loss in generated sparse views and semantic loss in augmented random views, resulting in view-consistent geometry and detailed textures. Our approach, therefore, enables the integration of a pre-trained MV image generator and a neural network-based volumetric signed distance function (SDF) representation for a single image to 3D object reconstruction. We evaluate our framework on various datasets and demonstrate its superior performance in both quantitative and qualitative assessments, signifying a significant advancement in 3D object reconstruction. Compared with the latest state-of-the-art method Syncdreamer~\\cite{liu2023syncdreamer}, we reduce the Chamfer Distance error by about 36\\% and improve PSNR by about 30\\% .|\u4ece\u5355\u4e2a\u56fe\u50cf\u91cd\u5efa 3D \u5bf9\u8c61\u662f\u4e00\u4e2a\u6709\u8da3\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5229\u7528\u591a\u89c6\u56fe (MV) 3D \u91cd\u5efa\u5c06\u751f\u6210\u7684 MV \u56fe\u50cf\u878d\u5408\u4e3a\u4e00\u81f4\u7684 3D \u5bf9\u8c61\u3002\u7136\u800c\uff0c\u751f\u6210\u7684\u56fe\u50cf\u901a\u5e38\u4f1a\u53d7\u5230\u5149\u7167\u4e0d\u4e00\u81f4\u3001\u51e0\u4f55\u5f62\u72b6\u4e0d\u5bf9\u9f50\u548c\u89c6\u56fe\u7a00\u758f\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 3D \u91cd\u5efa\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5185\u5728\u5206\u89e3\u6307\u5bfc\u3001\u77ac\u6001\u5355\u5148\u9a8c\u6307\u5bfc\u548c\u89c6\u56fe\u589e\u5f3a\u6765\u5206\u522b\u89e3\u51b3\u8fd9\u4e09\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u4ece\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u89e3\u8026\u9634\u5f71\u4fe1\u606f\u6765\u51cf\u5c11\u4e0d\u4e00\u81f4\u7167\u660e\u7684\u5f71\u54cd\uff1b\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u5355\u5148\u9a8c\u548c\u4e0e\u89c6\u56fe\u76f8\u5173\u7684\u77ac\u6001\u7f16\u7801\uff0c\u4ee5\u589e\u5f3a\u91cd\u5efa\u7684\u6cd5\u7ebf\uff1b\u6700\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89c6\u56fe\u589e\u5f3a\u878d\u5408\u7b56\u7565\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u751f\u6210\u7684\u7a00\u758f\u89c6\u56fe\u4e2d\u7684\u50cf\u7d20\u7ea7\u635f\u5931\u548c\u589e\u5f3a\u968f\u673a\u89c6\u56fe\u4e2d\u7684\u8bed\u4e49\u635f\u5931\uff0c\u4ece\u800c\u4ea7\u751f\u89c6\u56fe\u4e00\u81f4\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u8be6\u7ec6\u7eb9\u7406\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u96c6\u6210\u9884\u8bad\u7ec3\u7684 MV \u56fe\u50cf\u751f\u6210\u5668\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4f53\u79ef\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570 (SDF) \u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u5355\u4e2a\u56fe\u50cf\u5230 3D \u5bf9\u8c61\u7684\u91cd\u5efa\u3002\u6211\u4eec\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u5176\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8fd9\u6807\u5fd7\u7740 3D \u5bf9\u8c61\u91cd\u5efa\u65b9\u9762\u7684\u91cd\u5927\u8fdb\u6b65\u3002\u4e0e\u6700\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5Syncdreamer~\\cite{liu2023syncdreamer}\u76f8\u6bd4\uff0c\u6211\u4eec\u5c06Chamfer Distance\u8bef\u5dee\u51cf\u5c11\u4e86\u7ea636%\uff0cPSNR\u63d0\u9ad8\u4e86\u7ea630%\u3002|[2401.15841v1](http://arxiv.org/pdf/2401.15841v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.16386": "|**2024-01-29**|**Continual Learning with Pre-Trained Models: A Survey**|\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\uff1a\u4e00\u9879\u8c03\u67e5|Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan|Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT|\u5982\u4eca\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u7a0b\u5e8f\u7ecf\u5e38\u9762\u5bf9\u6d41\u6570\u636e\uff0c\u8fd9\u9700\u8981\u5b66\u4e60\u7cfb\u7edf\u968f\u7740\u6570\u636e\u7684\u6f14\u53d8\u5438\u6536\u65b0\u7684\u77e5\u8bc6\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65e8\u5728\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u540c\u65f6\u514b\u670d\u5b66\u4e60\u65b0\u77e5\u8bc6\u65f6\u5bf9\u65e7\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002\u5178\u578b\u7684 CL \u65b9\u6cd5\u4ece\u5934\u5f00\u59cb\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u968f\u7740\u4f20\u5165\u6570\u636e\u800c\u589e\u957f\u3002\u7136\u800c\uff0c\u9884\u8bad\u7ec3\u6a21\u578b (PTM) \u65f6\u4ee3\u7684\u5230\u6765\u5f15\u53d1\u4e86\u5de8\u5927\u7684\u7814\u7a76\u5174\u8da3\uff0c\u7279\u522b\u662f\u5728\u5229\u7528 PTM \u5f3a\u5927\u7684\u8868\u5f81\u80fd\u529b\u65b9\u9762\u3002\u672c\u6587\u5bf9\u57fa\u4e8e PTM \u7684 CL \u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8c03\u67e5\u3002\u6211\u4eec\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e09\u4e2a\u4e0d\u540c\u7684\u7ec4\uff0c\u5bf9\u5b83\u4eec\u7684\u76f8\u4f3c\u70b9\u3001\u5dee\u5f02\u4ee5\u53ca\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u9879\u5bf9\u6bd4\u5404\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ee5\u5f3a\u8c03\u5bf9\u6bd4\u8f83\u516c\u5e73\u6027\u7684\u62c5\u5fe7\u3002\u91cd\u73b0\u8fd9\u4e9b\u8bc4\u4f30\u7684\u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/sun-hailong/LAMDA-PILOT|[2401.16386v1](http://arxiv.org/pdf/2401.16386v1)|null|\n", "2401.15952": "|**2024-01-29**|**A Class-aware Optimal Transport Approach with Higher-Order Moment Matching for Unsupervised Domain Adaptation**|\u4e00\u79cd\u5177\u6709\u9ad8\u9636\u77e9\u5339\u914d\u7684\u7c7b\u611f\u77e5\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u57df\u9002\u5e94|Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung Tran, Dinh Phung|Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we introduce a novel approach called class-aware optimal transport (OT), which measures the OT distance between a distribution over the source class-conditional distributions and a mixture of source and target data distribution. Our class-aware OT leverages a cost function that determines the matching extent between a given data example and a source class-conditional distribution. By optimizing this cost function, we find the optimal matching between target examples and source class-conditional distributions, effectively addressing the data and label shifts that occur between the two domains. To handle the class-aware OT efficiently, we propose an amortization solution that employs deep neural networks to formulate the transportation probabilities and the cost function. Additionally, we propose minimizing class-aware Higher-order Moment Matching (HMM) to align the corresponding class regions on the source and target domains. The class-aware HMM component offers an economical computational approach for accurately evaluating the HMM distance between the two distributions. Extensive experiments on benchmark datasets demonstrate that our proposed method significantly outperforms existing state-of-the-art baselines.|\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65e8\u5728\u5c06\u77e5\u8bc6\u4ece\u6807\u8bb0\u7684\u6e90\u57df\u8f6c\u79fb\u5230\u672a\u6807\u8bb0\u7684\u76ee\u6807\u57df\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u7c7b\u611f\u77e5\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6d4b\u91cf\u6e90\u7c7b\u6761\u4ef6\u5206\u5e03\u4e0a\u7684\u5206\u5e03\u4e0e\u6e90\u548c\u76ee\u6807\u6570\u636e\u5206\u5e03\u7684\u6df7\u5408\u4e4b\u95f4\u7684 OT \u8ddd\u79bb\u3002\u6211\u4eec\u7684\u7c7b\u611f\u77e5 OT \u5229\u7528\u6210\u672c\u51fd\u6570\u6765\u786e\u5b9a\u7ed9\u5b9a\u6570\u636e\u793a\u4f8b\u548c\u6e90\u7c7b\u6761\u4ef6\u5206\u5e03\u4e4b\u95f4\u7684\u5339\u914d\u7a0b\u5ea6\u3002\u901a\u8fc7\u4f18\u5316\u8fd9\u4e2a\u6210\u672c\u51fd\u6570\uff0c\u6211\u4eec\u627e\u5230\u4e86\u76ee\u6807\u793a\u4f8b\u548c\u6e90\u7c7b\u6761\u4ef6\u5206\u5e03\u4e4b\u95f4\u7684\u6700\u4f73\u5339\u914d\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4e24\u4e2a\u57df\u4e4b\u95f4\u53d1\u751f\u7684\u6570\u636e\u548c\u6807\u7b7e\u79fb\u4f4d\u95ee\u9898\u3002\u4e3a\u4e86\u6709\u6548\u5730\u5904\u7406\u7c7b\u611f\u77e5 OT\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u644a\u9500\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u5236\u5b9a\u8fd0\u8f93\u6982\u7387\u548c\u6210\u672c\u51fd\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5efa\u8bae\u6700\u5c0f\u5316\u7c7b\u611f\u77e5\u9ad8\u9636\u77e9\u5339\u914d\uff08HMM\uff09\uff0c\u4ee5\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e0a\u7684\u76f8\u5e94\u7c7b\u533a\u57df\u3002\u7c7b\u611f\u77e5 HMM \u7ec4\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u51c6\u786e\u8bc4\u4f30\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684 HMM \u8ddd\u79bb\u3002\u5bf9\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u7740\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002|[2401.15952v1](http://arxiv.org/pdf/2401.15952v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.16393": "|**2024-01-29**|**Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River Contraction**|\u4e9a\u9a6c\u900a 2023 \u5e74\u5e72\u65f1\uff1aSentinel-1 \u63ed\u793a\u91cc\u5965\u5185\u683c\u7f57\u6cb3\u6781\u7aef\u6536\u7f29|Fabien H Wagner, Samuel Favrichon, Ricardo Dalagnol, Mayumi CM Hirye, Adugna Mullissa, Sassan Saatchi|The Amazon, the world's largest rainforest, faces a severe historic drought. The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023. Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. The accuracy of the water surface model was high with an F1-score of 0.93. The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction. The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686). The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds. Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$). Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions.|\u4e16\u754c\u4e0a\u6700\u5927\u7684\u96e8\u6797\u4e9a\u9a6c\u900a\u96e8\u6797\u6b63\u9762\u4e34\u5386\u53f2\u6027\u7684\u4e25\u91cd\u5e72\u65f1\u3002\u4e9a\u9a6c\u900a\u6cb3\u4e3b\u8981\u652f\u6d41\u4e4b\u4e00\u7684\u5185\u683c\u7f57\u6cb3\u5728 2023 \u5e74 10 \u6708\u8fbe\u5230\u4e00\u4e2a\u4e16\u7eaa\u4ee5\u6765\u7684\u6700\u4f4e\u6c34\u4f4d\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528 U-net \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed8\u5236\u4e86 2022 \u5e74\u6bcf 12 \u5929\u7684\u5185\u683c\u7f57\u6cb3\u6d41\u57df\u6c34\u9762\u56fe2023 \u5e74\u4f7f\u7528 10 m \u7a7a\u95f4\u5206\u8fa8\u7387\u7684 Sentinel-1 \u536b\u661f\u96f7\u8fbe\u56fe\u50cf\u3002\u6c34\u9762\u6a21\u578b\u7cbe\u5ea6\u8f83\u9ad8\uff0cF1\u5206\u6570\u4e3a0.93\u3002 12 \u5929\u7684\u6c34\u9762\u9a6c\u8d5b\u514b\u65f6\u95f4\u5e8f\u5217\u662f\u6839\u636e Sentinel-1 \u9884\u6d4b\u751f\u6210\u7684\u3002\u6c34\u9762\u63a9\u6a21\u4e0e\u8054\u5408\u7814\u7a76\u4e2d\u5fc3\u7684\u5168\u7403\u5730\u8868\u6c34 (GSW) \u4ea7\u54c1\uff08F1 \u5206\u6570\uff1a0.708\uff09\u548c\u5df4\u897f Mapbiomas \u6c34\u8ba1\u5212\uff08F1 \u5206\u6570\uff1a0.686\uff09\u8868\u73b0\u51fa\u76f8\u5bf9\u4e00\u81f4\u7684\u4e00\u81f4\u6027\u3002\u5730\u56fe\u7684\u4e3b\u8981\u9519\u8bef\u662f\u88ab\u6df9\u6ca1\u7684\u6797\u5730\u3001\u88ab\u6df9\u6ca1\u7684\u704c\u6728\u4e1b\u548c\u4e91\u5c42\u7684\u9057\u6f0f\u9519\u8bef\u3002\u91cc\u5965\u5185\u683c\u7f57\u6c34\u9762\u4e8e 2023 \u5e74 11 \u6708 25 \u65e5\u5de6\u53f3\u8fbe\u5230\u6700\u4f4e\u6c34\u5e73\uff0c\u5e76\u51cf\u5c11\u81f3 2022-2023 \u5e74\u671f\u95f4\u89c2\u6d4b\u5230\u7684\u6700\u5927\u6c34\u9762\uff0814,036.3 km$^2$\uff09\u7684 68.1%\uff089,559.9 km$^2$\uff09\u3002\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u7740\u6539\u5584\u70ed\u5e26\u5730\u533a\u6c34\u9762\u7684\u8fd1\u5b9e\u65f6\u6d4b\u7ed8\u3002|[2401.16393v1](http://arxiv.org/pdf/2401.16393v1)|null|\n", "2401.16352": "|**2024-01-29**|**Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization**|\u7eaf\u5316\u5bf9\u6297\u8bad\u7ec3\uff08AToP\uff09\uff1a\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027|Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao|The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks.|\u4f17\u6240\u5468\u77e5\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5f88\u5bb9\u6613\u53d7\u5230\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u653b\u51fb\u3002\u57fa\u4e8e\u5bf9\u6297\u6027\u8bad\u7ec3\uff08AT\uff09\u7684\u6700\u6210\u529f\u7684\u9632\u5fa1\u6280\u672f\u53ef\u4ee5\u9488\u5bf9\u7279\u5b9a\u653b\u51fb\u5b9e\u73b0\u6700\u4f73\u9c81\u68d2\u6027\uff0c\u4f46\u4e0d\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u653b\u51fb\u3002\u53e6\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6027\u51c0\u5316\uff08AP\uff09\u7684\u6709\u6548\u9632\u5fa1\u6280\u672f\u53ef\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u5b9e\u73b0\u6700\u4f73\u7684\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u7684\u9650\u5236\uff0c\u5373\u6807\u51c6\u7cbe\u5ea6\u4e0b\u964d\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u51c0\u5316\u5bf9\u6297\u8bad\u7ec3\uff08AToP\uff09\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a\u901a\u8fc7\u968f\u673a\u53d8\u6362\uff08RT\uff09\u8fdb\u884c\u7684\u6270\u52a8\u7834\u574f\u548c\u901a\u8fc7\u5bf9\u6297\u6027\u635f\u5931\u8fdb\u884c\u7684\u51c0\u5316\u5668\u6a21\u578b\u5fae\u8c03\uff08FT\uff09\u3002 RT \u5bf9\u4e8e\u907f\u514d\u5bf9\u5df2\u77e5\u653b\u51fb\u7684\u8fc7\u5ea6\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u5bfc\u81f4\u5bf9\u672a\u89c1\u8fc7\u7684\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6cdb\u5316\uff0c\u800c FT \u5bf9\u4e8e\u63d0\u9ad8\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u4ee5\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNette \u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u8868\u73b0\u51fa\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2401.16352v1](http://arxiv.org/pdf/2401.16352v1)|null|\n", "2401.16347": "|**2024-01-29**|**Cross-Modal Coordination Across a Diverse Set of Input Modalities**|\u8de8\u591a\u79cd\u8f93\u5165\u6a21\u5f0f\u7684\u8de8\u6a21\u5f0f\u534f\u8c03|Jorge S\u00e1nchez, Rodrigo Laguna|Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.|\u8de8\u6a21\u6001\u68c0\u7d22\u662f\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u6a21\u6001\u7684\u67e5\u8be2\u6765\u68c0\u7d22\u7ed9\u5b9a\u6a21\u6001\u7684\u6837\u672c\u7684\u4efb\u52a1\u3002\u7531\u4e8e\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u5e7f\u6cdb\uff0c\u95ee\u9898\u4e3b\u8981\u96c6\u4e2d\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6848\u4f8b\u4e0a\uff0c\u4f8b\u5982\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff0c\u50cf CLIP \u8fd9\u6837\u7684\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\u3002\u5b66\u4e60\u8fd9\u79cd\u534f\u8c03\u8868\u793a\u7684\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u4e00\u4e2a\u516c\u5171\u7a7a\u95f4\u4e0a\uff0c\u5728\u8be5\u7a7a\u95f4\u4e2d\uff0c\u5339\u914d\u7684\u89c6\u56fe\u4fdd\u6301\u9760\u8fd1\uff0c\u800c\u6765\u81ea\u975e\u5339\u914d\u5bf9\u7684\u89c6\u56fe\u5f7c\u6b64\u8fdc\u79bb\u3002\u5c3d\u7ba1\u8fd9\u79cd\u8de8\u6a21\u5f0f\u534f\u8c03\u4e5f\u5df2\u5e94\u7528\u4e8e\u5176\u4ed6\u6210\u5bf9\u7ec4\u5408\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u4efb\u610f\u6570\u91cf\u7684\u4e0d\u540c\u6a21\u5f0f\u662f\u6587\u732e\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5c06 CLIP \u5bf9\u6bd4\u76ee\u6807\u6269\u5c55\u5230\u4efb\u610f\u6570\u91cf\u7684\u8f93\u5165\u6a21\u6001\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5219\u504f\u79bb\u5bf9\u6bd4\u516c\u5f0f\uff0c\u901a\u8fc7\u5c06\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u56de\u5f52\u5230\u53cd\u6620\u4e24\u4e2a\u7b80\u5355\u76f4\u89c2\u7ea6\u675f\u7684\u76ee\u6807\u6765\u89e3\u51b3\u534f\u8c03\u95ee\u9898\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u3001\u4e0d\u540c\u7684\u8f93\u5165\u6a21\u5f0f\u7ec4\u5408\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5355\u6709\u6548\uff0c\u800c\u4e14\u8fd8\u5141\u8bb8\u4ee5\u65b0\u9896\u7684\u65b9\u5f0f\u89e3\u51b3\u68c0\u7d22\u95ee\u9898\u3002\u9664\u4e86\u6355\u83b7\u66f4\u591a\u6837\u5316\u7684\u6210\u5bf9\u4ea4\u4e92\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u7ec4\u5408\u4e24\u79cd\u6216\u591a\u79cd\u6b64\u7c7b\u6a21\u5f0f\u7684\u5d4c\u5165\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u6765\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\u3002|[2401.16347v1](http://arxiv.org/pdf/2401.16347v1)|null|\n", "2401.16318": "|**2024-01-29**|**Defining and Extracting generalizable interaction primitives from DNNs**|\u4ece DNN \u4e2d\u5b9a\u4e49\u548c\u63d0\u53d6\u53ef\u63a8\u5e7f\u7684\u4ea4\u4e92\u539f\u8bed|Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang|Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.|\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u7f16\u7801\u7684\u77e5\u8bc6\u5fe0\u5b9e\u5730\u603b\u7ed3\u4e3a\u4e00\u4e9b\u7b26\u53f7\u539f\u59cb\u6a21\u5f0f\u800c\u4e0d\u4e22\u5931\u592a\u591a\u4fe1\u606f\uff0c\u8fd9\u662f\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u4efb\u7b49\u4eba\u3002 (2023c) \u5bfc\u51fa\u4e86\u4e00\u7cfb\u5217\u5b9a\u7406\u6765\u8bc1\u660e DNN \u7684\u63a8\u7406\u5206\u6570\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8f93\u5165\u53d8\u91cf\u4e4b\u95f4\u7684\u4e00\u5c0f\u7ec4\u76f8\u4e92\u4f5c\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u4ecd\u7136\u5f88\u96be\u5c06\u6b64\u7c7b\u4ea4\u4e92\u89c6\u4e3a\u7531 DNN \u7f16\u7801\u7684\u5fe0\u5b9e\u539f\u59cb\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u8003\u8651\u5230\u9488\u5bf9\u540c\u4e00\u4efb\u52a1\u8bad\u7ec3\u7684\u4e0d\u540c DNN\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u53d6\u8fd9\u4e9b DNN \u5171\u4eab\u7684\u4ea4\u4e92\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u53d6\u7684\u4ea4\u4e92\u53ef\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4e0d\u540c DNN \u5171\u4eab\u7684\u5171\u540c\u77e5\u8bc6\u3002|[2401.16318v1](http://arxiv.org/pdf/2401.16318v1)|null|\n", "2401.16087": "|**2024-01-29**|**High Resolution Image Quality Database**|\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93|Huang Huang, Qiang Wan, Jari Korhonen|With technology for digital photography and high resolution displays rapidly evolving and gaining popularity, there is a growing demand for blind image quality assessment (BIQA) models for high resolution images. Unfortunately, the publicly available large scale image quality databases used for training BIQA models contain mostly low or general resolution images. Since image resizing affects image quality, we assume that the accuracy of BIQA models trained on low resolution images would not be optimal for high resolution images. Therefore, we created a new high resolution image quality database (HRIQ), consisting of 1120 images with resolution of 2880x2160 pixels. We conducted a subjective study to collect the subjective quality ratings for HRIQ in a controlled laboratory setting, resulting in accurate MOS at high resolution. To demonstrate the importance of a high resolution image quality database for training BIQA models to predict mean opinion scores (MOS) of high resolution images accurately, we trained and tested several traditional and deep learning based BIQA methods on different resolution versions of our database. The database is publicly available in https://github.com/jarikorhonen/hriq.|\u968f\u7740\u6570\u5b57\u6444\u5f71\u548c\u9ad8\u5206\u8fa8\u7387\u663e\u793a\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u548c\u666e\u53ca\uff0c\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (BIQA) \u6a21\u578b\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u7528\u4e8e\u8bad\u7ec3 BIQA \u6a21\u578b\u7684\u516c\u5f00\u53ef\u7528\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93\u5927\u591a\u5305\u542b\u4f4e\u5206\u8fa8\u7387\u6216\u4e00\u822c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u3002\u7531\u4e8e\u56fe\u50cf\u5927\u5c0f\u8c03\u6574\u4f1a\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u56e0\u6b64\u6211\u4eec\u5047\u8bbe\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684 BIQA \u6a21\u578b\u7684\u51c6\u786e\u6027\u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6765\u8bf4\u5e76\u4e0d\u662f\u6700\u4f73\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93\uff08HRIQ\uff09\uff0c\u75311120\u5f20\u5206\u8fa8\u7387\u4e3a2880x2160\u50cf\u7d20\u7684\u56fe\u50cf\u7ec4\u6210\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u4e3b\u89c2\u7814\u7a76\uff0c\u5728\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u6536\u96c6 HRIQ \u7684\u4e3b\u89c2\u8d28\u91cf\u8bc4\u7ea7\uff0c\u4ece\u800c\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u7684\u51c6\u786e MOS\u3002\u4e3a\u4e86\u8bc1\u660e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u5e93\u5bf9\u4e8e\u8bad\u7ec3 BIQA \u6a21\u578b\u4ee5\u51c6\u786e\u9884\u6d4b\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5e73\u5747\u610f\u89c1\u5f97\u5206 (MOS) \u7684\u91cd\u8981\u6027\uff0c\u6211\u4eec\u5728\u6570\u636e\u5e93\u7684\u4e0d\u540c\u5206\u8fa8\u7387\u7248\u672c\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e86\u51e0\u79cd\u4f20\u7edf\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684 BIQA \u65b9\u6cd5\u3002\u8be5\u6570\u636e\u5e93\u53ef\u5728 https://github.com/jarikorhonen/hriq \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.16087v1](http://arxiv.org/pdf/2401.16087v1)|null|\n", "2401.16039": "|**2024-01-29**|**Data-Driven Filter Design in FBP: Transforming CT Reconstruction with Trainable Fourier Series**|FBP \u4e2d\u7684\u6570\u636e\u9a71\u52a8\u6ee4\u6ce2\u5668\u8bbe\u8ba1\uff1a\u5229\u7528\u53ef\u8bad\u7ec3\u7684\u5085\u91cc\u53f6\u7ea7\u6570\u6539\u53d8 CT \u91cd\u5efa|Yipeng Sun, Linda-Sophie Schneider, Fuxin Fan, Mareike Thies, Mingxuan Gu, Siyuan Mei, Yuzhong Zhou, Siming Bayer, Andreas Maier|In this study, we introduce a Fourier series-based trainable filter for computed tomography (CT) reconstruction within the filtered backprojection (FBP) framework. This method overcomes the limitation in noise reduction, inherent in conventional FBP methods, by optimizing Fourier series coefficients to construct the filter. This method enables robust performance across different resolution scales and maintains computational efficiency with minimal increment for the trainable parameters compared to other deep learning frameworks. Additionally, we propose Gaussian edge-enhanced (GEE) loss function that prioritizes the $L_1$ norm of high-frequency magnitudes, effectively countering the blurring problems prevalent in mean squared error (MSE) approaches. The model's foundation in the FBP algorithm ensures excellent interpretability, as it relies on a data-driven filter with all other parameters derived through rigorous mathematical procedures. Designed as a plug-and-play solution, our Fourier series-based filter can be easily integrated into existing CT reconstruction models, making it a versatile tool for a wide range of practical applications. Our research presents a robust and scalable method that expands the utility of FBP in both medical and scientific imaging.|\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u7acb\u53f6\u7ea7\u6570\u7684\u53ef\u8bad\u7ec3\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u6ee4\u6ce2\u53cd\u6295\u5f71\uff08FBP\uff09\u6846\u67b6\u5185\u7684\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u5085\u91cc\u53f6\u7ea7\u6570\u7cfb\u6570\u200b\u200b\u6765\u6784\u9020\u6ee4\u6ce2\u5668\uff0c\u514b\u670d\u4e86\u4f20\u7edfFBP\u65b9\u6cd5\u56fa\u6709\u7684\u964d\u566a\u9650\u5236\u3002\u4e0e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u7a33\u5065\u7684\u6027\u80fd\uff0c\u5e76\u4ee5\u6700\u5c0f\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u589e\u91cf\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9ad8\u65af\u8fb9\u7f18\u589e\u5f3a\uff08GEE\uff09\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4f18\u5148\u8003\u8651\u9ad8\u9891\u5e45\u5ea6\u7684 $L_1$ \u8303\u6570\uff0c\u6709\u6548\u89e3\u51b3\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6a21\u7cca\u95ee\u9898\u3002\u8be5\u6a21\u578b\u4ee5 FBP \u7b97\u6cd5\u4e3a\u57fa\u7840\uff0c\u786e\u4fdd\u4e86\u51fa\u8272\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u6ee4\u5668\uff0c\u6240\u6709\u5176\u4ed6\u53c2\u6570\u5747\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u7a0b\u5e8f\u5f97\u51fa\u3002\u6211\u4eec\u7684\u57fa\u4e8e\u5085\u7acb\u53f6\u7ea7\u6570\u7684\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e3a\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684 CT \u91cd\u5efa\u6a21\u578b\u4e2d\uff0c\u4f7f\u5176\u6210\u4e3a\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u7684\u591a\u529f\u80fd\u5de5\u5177\u3002\u6211\u4eec\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u53ef\u6269\u5c55 FBP \u5728\u533b\u5b66\u548c\u79d1\u5b66\u6210\u50cf\u4e2d\u7684\u5b9e\u7528\u6027\u3002|[2401.16039v1](http://arxiv.org/pdf/2401.16039v1)|null|\n", "2401.15944": "|**2024-01-29**|**Bridging the Domain Gap: A Simple Domain Matching Method for Reference-based Image Super-Resolution in Remote Sensing**|\u5f25\u5408\u57df\u5dee\u8ddd\uff1a\u9065\u611f\u4e2d\u57fa\u4e8e\u53c2\u8003\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7b80\u5355\u57df\u5339\u914d\u65b9\u6cd5|Jeongho Min, Yejun Lee, Dongyoung Kim, Jaejun Yoo|Recently, reference-based image super-resolution (RefSR) has shown excellent performance in image super-resolution (SR) tasks. The main idea of RefSR is to utilize additional information from the reference (Ref) image to recover the high-frequency components in low-resolution (LR) images. By transferring relevant textures through feature matching, RefSR models outperform existing single image super-resolution (SISR) models. However, their performance significantly declines when a domain gap between Ref and LR images exists, which often occurs in real-world scenarios, such as satellite imaging. In this letter, we introduce a Domain Matching (DM) module that can be seamlessly integrated with existing RefSR models to enhance their performance in a plug-and-play manner. To the best of our knowledge, we are the first to explore Domain Matching-based RefSR in remote sensing image processing. Our analysis reveals that their domain gaps often occur in different satellites, and our model effectively addresses these challenges, whereas existing models struggle. Our experiments demonstrate that the proposed DM module improves SR performance both qualitatively and quantitatively for remote sensing super-resolution tasks.|\u6700\u8fd1\uff0c\u57fa\u4e8e\u53c2\u8003\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08RefSR\uff09\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002 RefSR \u7684\u4e3b\u8981\u601d\u60f3\u662f\u5229\u7528\u53c2\u8003 (Ref) \u56fe\u50cf\u4e2d\u7684\u9644\u52a0\u4fe1\u606f\u6765\u6062\u590d\u4f4e\u5206\u8fa8\u7387 (LR) \u56fe\u50cf\u4e2d\u7684\u9ad8\u9891\u5206\u91cf\u3002\u901a\u8fc7\u7279\u5f81\u5339\u914d\u4f20\u8f93\u76f8\u5173\u200b\u200b\u7eb9\u7406\uff0cRefSR \u6a21\u578b\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387 (SISR) \u6a21\u578b\u3002\u7136\u800c\uff0c\u5f53\u53c2\u8003\u56fe\u50cf\u548c LR \u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u57df\u5dee\u8ddd\u65f6\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u4f1a\u663e\u7740\u4e0b\u964d\uff0c\u8fd9\u79cd\u60c5\u51b5\u7ecf\u5e38\u53d1\u751f\u5728\u536b\u661f\u6210\u50cf\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u3002\u5728\u8fd9\u5c01\u4fe1\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57df\u5339\u914d\uff08DM\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684 RefSR \u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u589e\u5f3a\u5176\u6027\u80fd\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5728\u9065\u611f\u56fe\u50cf\u5904\u7406\u4e2d\u63a2\u7d22\u57fa\u4e8e\u57df\u5339\u914d\u7684 RefSR \u7684\u4eba\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5b83\u4eec\u7684\u57df\u5dee\u8ddd\u7ecf\u5e38\u51fa\u73b0\u5728\u4e0d\u540c\u7684\u536b\u661f\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5219\u9677\u5165\u56f0\u5883\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 DM \u6a21\u5757\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u63d0\u9ad8\u4e86\u9065\u611f\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684 SR \u6027\u80fd\u3002|[2401.15944v1](http://arxiv.org/pdf/2401.15944v1)|null|\n", "2401.15893": "|**2024-01-29**|**Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation**|\u4f7f\u7528\u9690\u5f0f\u8fde\u7eed\u8868\u793a\u4efb\u610f\u5c3a\u5ea6\u7f29\u5c0f\u6f6e\u6c50\u6d41\u6570\u636e|Dongheon Lee, Seungmyong Jeong, Youngmin Ro|Numerical models have long been used to understand geoscientific phenomena, including tidal currents, crucial for renewable energy production and coastal engineering. However, their computational cost hinders generating data of varying resolutions. As an alternative, deep learning-based downscaling methods have gained traction due to their faster inference speeds. But most of them are limited to only inference fixed scale and overlook important characteristics of target geoscientific data. In this paper, we propose a novel downscaling framework for tidal current data, addressing its unique characteristics, which are dissimilar to images: heterogeneity and local dependency. Moreover, our framework can generate any arbitrary-scale output utilizing a continuous representation model. Our proposed framework demonstrates significantly improved flow velocity predictions by 93.21% (MSE) and 63.85% (MAE) compared to the Baseline model while achieving a remarkable 33.2% reduction in FLOPs.|\u6570\u503c\u6a21\u578b\u957f\u671f\u4ee5\u6765\u88ab\u7528\u6765\u7406\u89e3\u5730\u7403\u79d1\u5b66\u73b0\u8c61\uff0c\u5305\u62ec\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u548c\u6cbf\u6d77\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\u7684\u6f6e\u6c50\u6d41\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u751f\u6210\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u6570\u636e\u3002\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7f29\u51cf\u65b9\u6cd5\u56e0\u5176\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u800c\u53d7\u5230\u5173\u6ce8\u3002\u4f46\u5927\u591a\u6570\u4ec5\u9650\u4e8e\u63a8\u65ad\u56fa\u5b9a\u5c3a\u5ea6\uff0c\u5ffd\u89c6\u4e86\u76ee\u6807\u5730\u7403\u79d1\u5b66\u6570\u636e\u7684\u91cd\u8981\u7279\u5f81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6f6e\u6d41\u6570\u636e\u964d\u5c3a\u5ea6\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5176\u4e0e\u56fe\u50cf\u4e0d\u540c\u7684\u72ec\u7279\u7279\u5f81\uff1a\u5f02\u8d28\u6027\u548c\u5c40\u90e8\u4f9d\u8d56\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u5229\u7528\u8fde\u7eed\u8868\u793a\u6a21\u578b\u751f\u6210\u4efb\u4f55\u4efb\u610f\u89c4\u6a21\u7684\u8f93\u51fa\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u6d41\u901f\u9884\u6d4b\u663e\u7740\u63d0\u9ad8\u4e86 93.21% (MSE) \u548c 63.85% (MAE)\uff0c\u540c\u65f6 FLOP \u663e\u7740\u964d\u4f4e\u4e86 33.2%\u3002|[2401.15893v1](http://arxiv.org/pdf/2401.15893v1)|null|\n", "2401.15883": "|**2024-01-29**|**TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability**|TransTroj\uff1a\u901a\u8fc7\u5d4c\u5165\u4e0d\u53ef\u533a\u5206\u6027\u5c06\u540e\u95e8\u653b\u51fb\u8f6c\u79fb\u5230\u9884\u8bad\u7ec3\u6a21\u578b|Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang|Pre-trained models (PTMs) are extensively utilized in various downstream tasks. Adopting untrusted PTMs may suffer from backdoor attacks, where the adversary can compromise the downstream models by injecting backdoors into the PTM. However, existing backdoor attacks to PTMs can only achieve partially task-agnostic and the embedded backdoors are easily erased during the fine-tuning process. In this paper, we propose a novel transferable backdoor attack, TransTroj, to simultaneously meet functionality-preserving, durable, and task-agnostic. In particular, we first formalize transferable backdoor attacks as the indistinguishability problem between poisoned and clean samples in the embedding space. We decompose the embedding indistinguishability into pre- and post-indistinguishability, representing the similarity of the poisoned and reference embeddings before and after the attack. Then, we propose a two-stage optimization that separately optimizes triggers and victim PTMs to achieve embedding indistinguishability. We evaluate TransTroj on four PTMs and six downstream tasks. Experimental results show that TransTroj significantly outperforms SOTA task-agnostic backdoor attacks (18%$\\sim$99%, 68% on average) and exhibits superior performance under various system settings. The code is available at https://github.com/haowang-cqu/TransTroj .|\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTM\uff09\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u91c7\u7528\u4e0d\u53d7\u4fe1\u4efb\u7684 PTM \u53ef\u80fd\u4f1a\u906d\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u5411 PTM \u6ce8\u5165\u540e\u95e8\u6765\u7834\u574f\u4e0b\u6e38\u6a21\u578b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u9488\u5bf9 PTM \u7684\u540e\u95e8\u653b\u51fb\u53ea\u80fd\u5b9e\u73b0\u90e8\u5206\u4efb\u52a1\u65e0\u5173\uff0c\u5e76\u4e14\u5d4c\u5165\u7684\u540e\u95e8\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f88\u5bb9\u6613\u88ab\u5220\u9664\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u8f6c\u79fb\u540e\u95e8\u653b\u51fb TransTroj\uff0c\u4ee5\u540c\u65f6\u6ee1\u8db3\u529f\u80fd\u4fdd\u7559\u3001\u6301\u4e45\u548c\u4efb\u52a1\u65e0\u5173\u7684\u8981\u6c42\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u9996\u5148\u5c06\u53ef\u8f6c\u79fb\u540e\u95e8\u653b\u51fb\u5f62\u5f0f\u5316\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6709\u6bd2\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u4e4b\u95f4\u7684\u4e0d\u53ef\u533a\u5206\u95ee\u9898\u3002\u6211\u4eec\u5c06\u5d4c\u5165\u4e0d\u53ef\u533a\u5206\u6027\u5206\u89e3\u4e3a\u524d\u540e\u4e0d\u53ef\u533a\u5206\u6027\uff0c\u8868\u793a\u653b\u51fb\u524d\u540e\u4e2d\u6bd2\u5d4c\u5165\u548c\u53c2\u8003\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\uff0c\u5206\u522b\u4f18\u5316\u89e6\u53d1\u5668\u548c\u53d7\u5bb3\u8005 PTM\uff0c\u4ee5\u5b9e\u73b0\u5d4c\u5165\u7684\u4e0d\u53ef\u533a\u5206\u6027\u3002\u6211\u4eec\u5728\u56db\u4e2a PTM \u548c\u516d\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8bc4\u4f30 TransTroj\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTransTroj \u663e\u7740\u4f18\u4e8e SOTA \u4efb\u52a1\u65e0\u5173\u540e\u95e8\u653b\u51fb\uff0818%$\\sim$99%\uff0c\u5e73\u5747 68%\uff09\uff0c\u5e76\u5728\u5404\u79cd\u7cfb\u7edf\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/haowang-cqu/TransTroj \u83b7\u53d6\u3002|[2401.15883v1](http://arxiv.org/pdf/2401.15883v1)|null|\n", "2401.15864": "|**2024-01-29**|**Spatial Decomposition and Temporal Fusion based Inter Prediction for Learned Video Compression**|\u57fa\u4e8e\u7a7a\u95f4\u5206\u89e3\u548c\u65f6\u95f4\u878d\u5408\u7684\u5b66\u4e60\u89c6\u9891\u538b\u7f29\u5e27\u95f4\u9884\u6d4b|Xihua Sheng, Li Li, Dong Liu, Houqiang Li|Video compression performance is closely related to the accuracy of inter prediction. It tends to be difficult to obtain accurate inter prediction for the local video regions with inconsistent motion and occlusion. Traditional video coding standards propose various technologies to handle motion inconsistency and occlusion, such as recursive partitions, geometric partitions, and long-term references. However, existing learned video compression schemes focus on obtaining an overall minimized prediction error averaged over all regions while ignoring the motion inconsistency and occlusion in local regions. In this paper, we propose a spatial decomposition and temporal fusion based inter prediction for learned video compression. To handle motion inconsistency, we propose to decompose the video into structure and detail (SDD) components first. Then we perform SDD-based motion estimation and SDD-based temporal context mining for the structure and detail components to generate short-term temporal contexts. To handle occlusion, we propose to propagate long-term temporal contexts by recurrently accumulating the temporal information of each historical reference feature and fuse them with short-term temporal contexts. With the SDD-based motion model and long short-term temporal contexts fusion, our proposed learned video codec can obtain more accurate inter prediction. Comprehensive experimental results demonstrate that our codec outperforms the reference software of H.266/VVC on all common test datasets for both PSNR and MS-SSIM.|\u89c6\u9891\u538b\u7f29\u6027\u80fd\u4e0e\u5e27\u95f4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u5bc6\u5207\u76f8\u5173\u3002\u5bf9\u4e8e\u8fd0\u52a8\u548c\u906e\u6321\u4e0d\u4e00\u81f4\u7684\u5c40\u90e8\u89c6\u9891\u533a\u57df\u5f80\u5f80\u5f88\u96be\u83b7\u5f97\u51c6\u786e\u7684\u5e27\u95f4\u9884\u6d4b\u3002\u4f20\u7edf\u89c6\u9891\u7f16\u7801\u6807\u51c6\u63d0\u51fa\u4e86\u5404\u79cd\u6280\u672f\u6765\u5904\u7406\u8fd0\u52a8\u4e0d\u4e00\u81f4\u548c\u906e\u6321\uff0c\u4f8b\u5982\u9012\u5f52\u5206\u533a\u3001\u51e0\u4f55\u5206\u533a\u548c\u957f\u671f\u53c2\u8003\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5b66\u4e60\u89c6\u9891\u538b\u7f29\u65b9\u6848\u4fa7\u91cd\u4e8e\u83b7\u5f97\u6240\u6709\u533a\u57df\u5e73\u5747\u7684\u603b\u4f53\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\uff0c\u800c\u5ffd\u7565\u4e86\u5c40\u90e8\u533a\u57df\u7684\u8fd0\u52a8\u4e0d\u4e00\u81f4\u548c\u906e\u6321\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u5206\u89e3\u548c\u65f6\u95f4\u878d\u5408\u7684\u5b66\u4e60\u89c6\u9891\u538b\u7f29\u5e27\u95f4\u9884\u6d4b\u3002\u4e3a\u4e86\u5904\u7406\u8fd0\u52a8\u4e0d\u4e00\u81f4\uff0c\u6211\u4eec\u5efa\u8bae\u9996\u5148\u5c06\u89c6\u9891\u5206\u89e3\u4e3a\u7ed3\u6784\u548c\u7ec6\u8282\uff08SDD\uff09\u7ec4\u4ef6\u3002\u7136\u540e\uff0c\u6211\u4eec\u5bf9\u7ed3\u6784\u548c\u7ec6\u8282\u7ec4\u4ef6\u6267\u884c\u57fa\u4e8e SDD \u7684\u8fd0\u52a8\u4f30\u8ba1\u548c\u57fa\u4e8e SDD \u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u6316\u6398\uff0c\u4ee5\u751f\u6210\u77ed\u671f\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u4e3a\u4e86\u5904\u7406\u906e\u6321\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u53cd\u590d\u7d2f\u79ef\u6bcf\u4e2a\u5386\u53f2\u53c2\u8003\u7279\u5f81\u7684\u65f6\u95f4\u4fe1\u606f\u5e76\u5c06\u5176\u4e0e\u77ed\u671f\u65f6\u95f4\u4e0a\u4e0b\u6587\u878d\u5408\u6765\u4f20\u64ad\u957f\u671f\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7\u57fa\u4e8e SDD \u7684\u8fd0\u52a8\u6a21\u578b\u548c\u957f\u77ed\u671f\u65f6\u95f4\u4e0a\u4e0b\u6587\u878d\u5408\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u5b66\u4e60\u89c6\u9891\u7f16\u89e3\u7801\u5668\u53ef\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u5e27\u95f4\u9884\u6d4b\u3002\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7f16\u89e3\u7801\u5668\u5728 PSNR \u548c MS-SSIM \u7684\u6240\u6709\u5e38\u89c1\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e H.266/VVC \u53c2\u8003\u8f6f\u4ef6\u3002|[2401.15864v1](http://arxiv.org/pdf/2401.15864v1)|null|\n", "2401.15855": "|**2024-01-29**|**Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing**|\u8de8\u5c3a\u5ea6 MAE\uff1a\u9065\u611f\u591a\u5c3a\u5ea6\u5f00\u53d1\u7684\u6545\u4e8b|Maofeng Tang, Andrei Cozma, Konstantinos Georgiou, Hairong Qi|Remote sensing images present unique challenges to image analysis due to the extensive geographic coverage, hardware limitations, and misaligned multi-scale images. This paper revisits the classical multi-scale representation learning problem but under the general framework of self-supervised learning for remote sensing image understanding. We present Cross-Scale MAE, a self-supervised model built upon the Masked Auto-Encoder (MAE).During pre-training, Cross-Scale MAE employs scale augmentation techniques and enforces cross-scale consistency constraints through both contrastive and generative losses to ensure consistent and meaningful representations well-suited for a wide range of downstream tasks. Further, our implementation leverages the xFormers library to accelerate network pre-training on a single GPU while maintaining the quality of learned representations. Experimental evaluations demonstrate that Cross-Scale MAE exhibits superior performance compared to standard MAE and other state-of-the-art remote sensing MAE methods.|\u7531\u4e8e\u5e7f\u6cdb\u7684\u5730\u7406\u8986\u76d6\u8303\u56f4\u3001\u786c\u4ef6\u9650\u5236\u548c\u672a\u5bf9\u9f50\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\uff0c\u9065\u611f\u56fe\u50cf\u7ed9\u56fe\u50cf\u5206\u6790\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u672c\u6587\u5728\u9065\u611f\u56fe\u50cf\u7406\u89e3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u603b\u4f53\u6846\u67b6\u4e0b\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u7684\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Cross-Scale MAE\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e Masked Auto-Encoder (MAE) \u6784\u5efa\u7684\u81ea\u76d1\u7763\u6a21\u578b\u3002\u5728\u9884\u8bad\u7ec3\u671f\u95f4\uff0cCross-Scale MAE \u91c7\u7528\u5c3a\u5ea6\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u751f\u6210\u635f\u5931\u6765\u5f3a\u5236\u8de8\u5c3a\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u786e\u4fdd\u4e00\u81f4\u4e14\u6709\u610f\u4e49\u7684\u8868\u793a\u975e\u5e38\u9002\u5408\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5b9e\u73b0\u5229\u7528 xFormers \u5e93\u6765\u52a0\u901f\u5355\u4e2a GPU \u4e0a\u7684\u7f51\u7edc\u9884\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u8868\u793a\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6807\u51c6 MAE \u548c\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u9065\u611f MAE \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8de8\u5c3a\u5ea6 MAE \u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002|[2401.15855v1](http://arxiv.org/pdf/2401.15855v1)|null|\n"}}