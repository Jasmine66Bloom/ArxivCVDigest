{"\u751f\u6210\u6a21\u578b": {"2404.07990": "|**2024-04-11**|**OpenBias: Open-set Bias Detection in Text-to-Image Generative Models**|OpenBias\uff1a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5f00\u653e\u96c6\u504f\u5dee\u68c0\u6d4b|Moreno D'Inc\u00e0, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe|Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.||[2404.07990v1](http://arxiv.org/pdf/2404.07990v1)|null|\n", "2404.07987": "|**2024-04-11**|**ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback**|ControlNet++\uff1a\u901a\u8fc7\u9ad8\u6548\u7684\u4e00\u81f4\u6027\u53cd\u9988\u6539\u8fdb\u6761\u4ef6\u63a7\u5236|Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen|To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions.||[2404.07987v1](http://arxiv.org/pdf/2404.07987v1)|null|\n", "2404.07949": "|**2024-04-11**|**Taming Stable Diffusion for Text to 360\u00b0 Panorama Image Generation**|\u9a6f\u670d\u6587\u672c\u5230 360\u00b0 \u5168\u666f\u56fe\u50cf\u751f\u6210\u7684\u7a33\u5b9a\u6269\u6563|Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai|Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at https://chengzhag.github.io/publication/panfusion.||[2404.07949v1](http://arxiv.org/pdf/2404.07949v1)|**[link](https://github.com/chengzhag/panfusion)**|\n", "2404.07773": "|**2024-04-11**|**ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model**|ConsistencyDet\uff1a\u5177\u6709\u4e00\u81f4\u6027\u6a21\u578b\u53bb\u566a\u8303\u5f0f\u7684\u9c81\u68d2\u76ee\u6807\u68c0\u6d4b\u5668|Lifan Jiang, Zhihui Wang, Changmiao Wang, Ming Li, Jiaxu Leng, Xindong Wu|Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on perturbed bounding boxes of annotated entities. This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising'' mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model. Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections. Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics.||[2404.07773v1](http://arxiv.org/pdf/2404.07773v1)|null|\n", "2404.07770": "|**2024-04-11**|**Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations**|\u6df7\u5408\u9000\u5316\u56fe\u50cf\u6062\u590d\u7684\u8054\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b|Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang|Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods.||[2404.07770v1](http://arxiv.org/pdf/2404.07770v1)|null|\n", "2404.07724": "|**2024-04-11**|**Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models**|\u5728\u6709\u9650\u7684\u65f6\u95f4\u95f4\u9694\u5185\u5e94\u7528\u6307\u5bfc\u53ef\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6837\u672c\u548c\u5206\u5e03\u8d28\u91cf|Tuomas Kynk\u00e4\u00e4nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen|Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.||[2404.07724v1](http://arxiv.org/pdf/2404.07724v1)|null|\n", "2404.07668": "|**2024-04-11**|**Shape Completion in the Dark: Completing Vertebrae Morphology from 3D Ultrasound**|\u5728\u9ed1\u6697\u4e2d\u5b8c\u6210\u5f62\u72b6\uff1a\u901a\u8fc7 3D \u8d85\u58f0\u5b8c\u6210\u690e\u9aa8\u5f62\u6001|Miruna-Alexandra Gafencu, Yordanka Velikova, Mahdi Saleh, Tamas Ungi, Nassir Navab, Thomas Wendler, Mohammad Farid Azampour|Purpose: Ultrasound (US) imaging, while advantageous for its radiation-free nature, is challenging to interpret due to only partially visible organs and a lack of complete 3D information. While performing US-based diagnosis or investigation, medical professionals therefore create a mental map of the 3D anatomy. In this work, we aim to replicate this process and enhance the visual representation of anatomical structures.   Methods: We introduce a point-cloud-based probabilistic DL method to complete occluded anatomical structures through 3D shape completion and choose US-based spine examinations as our application. To enable training, we generate synthetic 3D representations of partially occluded spinal views by mimicking US physics and accounting for inherent artifacts.   Results: The proposed model performs consistently on synthetic and patient data, with mean and median differences of 2.02 and 0.03 in CD, respectively. Our ablation study demonstrates the importance of US physics-based data generation, reflected in the large mean and median difference of 11.8 CD and 9.55 CD, respectively. Additionally, we demonstrate that anatomic landmarks, such as the spinous process (with reconstruction CD of 4.73) and the facet joints (mean distance to GT of 4.96mm) are preserved in the 3D completion.   Conclusion: Our work establishes the feasibility of 3D shape completion for lumbar vertebrae, ensuring the preservation of level-wise characteristics and successful generalization from synthetic to real data. The incorporation of US physics contributes to more accurate patient data completions. Notably, our method preserves essential anatomic landmarks and reconstructs crucial injections sites at their correct locations. The generated data and source code will be made publicly available (https://github.com/miruna20/Shape-Completion-in-the-Dark).||[2404.07668v1](http://arxiv.org/pdf/2404.07668v1)|**[link](https://github.com/miruna20/shape-completion-in-the-dark)**|\n", "2404.07649": "|**2024-04-11**|**Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method**|\u5206\u79bb\u6ce8\u610f\u529b\uff1a\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u5faa\u73afGAN\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5|Tashmoy Ghosh|In this paper we have present an improved Cycle GAN based model for under water image enhancement. We have utilized the cycle consistent learning technique of the state-of-the-art Cycle GAN model with modification in the loss function in terms of depth-oriented attention which enhance the contrast of the overall image, keeping global content, color, local texture, and style information intact. We trained the Cycle GAN model with the modified loss functions on the benchmarked Enhancing Underwater Visual Perception (EUPV) dataset a large dataset including paired and unpaired sets of underwater images (poor and good quality) taken with seven distinct cameras in a range of visibility situation during research on ocean exploration and human-robot cooperation. In addition, we perform qualitative and quantitative evaluation which supports the given technique applied and provided a better contrast enhancement model of underwater imagery. More significantly, the upgraded images provide better results from conventional models and further for under water navigation, pose estimation, saliency prediction, object detection and tracking. The results validate the appropriateness of the model for autonomous underwater vehicles (AUV) in visual navigation.||[2404.07649v1](http://arxiv.org/pdf/2404.07649v1)|null|\n", "2404.07620": "|**2024-04-11**|**Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation**|\u7528\u4e8e\u51cf\u5c11\u80f0\u817a\u5206\u5272\u4e2d\u8fb9\u7f18\u4e0d\u786e\u5b9a\u6027\u7684\u6269\u6563\u6982\u7387\u591a\u7ebf\u7d22\u6c34\u5e73\u96c6|Yue Gou, Yuming Xing, Shengzhu Shi, Zhichang Guo|Accurately segmenting the pancreas remains a huge challenge. Traditional methods encounter difficulties in semantic localization due to the small volume and distorted structure of the pancreas, while deep learning methods encounter challenges in obtaining accurate edges because of low contrast and organ overlapping. To overcome these issues, we propose a multi-cue level set method based on the diffusion probabilistic model, namely Diff-mcs. Our method adopts a coarse-to-fine segmentation strategy. We use the diffusion probabilistic model in the coarse segmentation stage, with the obtained probability distribution serving as both the initial localization and prior cues for the level set method. In the fine segmentation stage, we combine the prior cues with grayscale cues and texture cues to refine the edge by maximizing the difference between probability distributions of the cues inside and outside the level set curve. The method is validated on three public datasets and achieves state-of-the-art performance, which can obtain more accurate segmentation results with lower uncertainty segmentation edges. In addition, we conduct ablation studies and uncertainty analysis to verify that the diffusion probability model provides a more appropriate initialization for the level set method. Furthermore, when combined with multiple cues, the level set method can better obtain edges and improve the overall accuracy. Our code is available at https://github.com/GOUYUEE/Diff-mcs.||[2404.07620v1](http://arxiv.org/pdf/2404.07620v1)|null|\n", "2404.07600": "|**2024-04-11**|**Implicit and Explicit Language Guidance for Diffusion-based Visual Perception**|\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u611f\u77e5\u7684\u9690\u5f0f\u548c\u663e\u5f0f\u8bed\u8a00\u6307\u5bfc|Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang|Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%.||[2404.07600v1](http://arxiv.org/pdf/2404.07600v1)|null|\n", "2404.07564": "|**2024-04-11**|**ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation**|ObjBlur\uff1a\u4e00\u79cd\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u6e10\u8fdb\u5f0f\u5bf9\u8c61\u7ea7\u6a21\u7cca\u529f\u80fd\uff0c\u53ef\u6539\u8fdb\u5e03\u5c40\u5230\u56fe\u50cf\u7684\u751f\u6210|Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel|We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets.||[2404.07564v1](http://arxiv.org/pdf/2404.07564v1)|null|\n", "2404.07554": "|**2024-04-11**|**CAT: Contrastive Adapter Training for Personalized Image Generation**|CAT\uff1a\u7528\u4e8e\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u7684\u5bf9\u6bd4\u9002\u914d\u5668\u8bad\u7ec3|Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, Min Song|The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed diffusion models to personalize image generation at a low cost. However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model's prior knowledge. One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations. This poses challenges in generation capabilities. To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss. Our approach facilitates the preservation of the base model's original knowledge when the model initiates adapters. Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to keep the former information. We qualitatively and quantitatively compare CAT's improvement. Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization.||[2404.07554v1](http://arxiv.org/pdf/2404.07554v1)|**[link](https://github.com/OnomaAi101/CAT)**|\n", "2404.07518": "|**2024-04-11**|**Remembering Transformer for Continual Learning**|\u8bb0\u4f4f Transformer \u4ee5\u8fdb\u884c\u6301\u7eed\u5b66\u4e60|Yuwei Sun, Jun Sakuma, Ryota Kanai|Neural networks encounter the challenge of Catastrophic Forgetting (CF) in continual learning, where new task knowledge interferes with previously learned knowledge. We propose Remembering Transformer, inspired by the brain's Complementary Learning Systems (CLS), to tackle this issue. Remembering Transformer employs a mixture-of-adapters and a generative model-based routing mechanism to alleviate CF by dynamically routing task data to relevant adapters. Our approach demonstrated a new SOTA performance in various vision continual learning tasks and great parameter efficiency.||[2404.07518v1](http://arxiv.org/pdf/2404.07518v1)|null|\n", "2404.07474": "|**2024-04-11**|**G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images**|G-NeRF\uff1a\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u4e2d\u8fdb\u884c\u51e0\u4f55\u589e\u5f3a\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210|Zixiong Huang, Qi Chen, Libo Sun, Yifan Yang, Naizhou Wang, Mingkui Tan, Qi Wu|Novel view synthesis aims to generate new view images of a given view image collection. Recent attempts address this problem relying on 3D geometry priors (e.g., shapes, sizes, and positions) learned from multi-view images. However, such methods encounter the following limitations: 1) they require a set of multi-view images as training data for a specific scene (e.g., face, car or chair), which is often unavailable in many real-world scenarios; 2) they fail to extract the geometry priors from single-view images due to the lack of multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF (G-NeRF), which seeks to enhance the geometry priors by a geometry-guided multi-view synthesis approach, followed by a depth-aware training. In the synthesis process, inspired that existing 3D GAN models can unconditionally synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D GAN models, such as EG3D, as a free source to provide geometry priors through synthesizing multi-view data. Simultaneously, to further improve the geometry quality of the synthetic data, we introduce a truncation method to effectively sample latent codes within 3D GAN models. To tackle the absence of multi-view supervision for single-view images, we design the depth-aware training approach, incorporating a depth-aware discriminator to guide geometry priors through depth maps. Experiments demonstrate the effectiveness of our method in terms of both qualitative and quantitative results.||[2404.07474v1](http://arxiv.org/pdf/2404.07474v1)|null|\n", "2404.07473": "|**2024-04-11**|**LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation**|LUCF-Net\uff1a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8f7b\u91cf\u7ea7 U \u5f62\u7ea7\u8054\u878d\u5408\u7f51\u7edc|Songkai Sun, Qingshan She, Yuliang Ma, Rihui Li, Yingchun Zhang|In this study, the performance of existing U-shaped neural network architectures was enhanced for medical image segmentation by adding Transformer. Although Transformer architectures are powerful at extracting global information, its ability to capture local information is limited due to its high complexity. To address this challenge, we proposed a new lightweight U-shaped cascade fusion network (LUCF-Net) for medical image segmentation. It utilized an asymmetrical structural design and incorporated both local and global modules to enhance its capacity for local and global modeling. Additionally, a multi-layer cascade fusion decoding network was designed to further bolster the network's information fusion capabilities. Validation results achieved on multi-organ datasets in CT format, cardiac segmentation datasets in MRI format, and dermatology datasets in image format demonstrated that the proposed model outperformed other state-of-the-art methods in handling local-global information, achieving an improvement of 1.54% in Dice coefficient and 2.6 mm in Hausdorff distance on multi-organ segmentation. Furthermore, as a network that combines Convolutional Neural Network and Transformer architectures, it achieves competitive segmentation performance with only 6.93 million parameters and 6.6 gigabytes of floating point operations, without the need of pre-training. In summary, the proposed method demonstrated enhanced performance while retaining a simpler model design compared to other Transformer-based segmentation networks.||[2404.07473v1](http://arxiv.org/pdf/2404.07473v1)|null|\n"}, "\u591a\u6a21\u6001": {"2404.07993": "|**2024-04-11**|**Connecting NeRFs, Images, and Text**|\u8fde\u63a5 NeRF\u3001\u56fe\u50cf\u548c\u6587\u672c|Francesco Ballerini, Pierluigi Zama Ramirez, Roberto Mirabella, Samuele Salti, Luigi Di Stefano|Neural Radiance Fields (NeRFs) have emerged as a standard framework for representing 3D scenes and objects, introducing a novel data type for information exchange and storage. Concurrently, significant progress has been made in multimodal representation learning for text and image data. This paper explores a novel research direction that aims to connect the NeRF modality with other modalities, similar to established methodologies for images and text. To this end, we propose a simple framework that exploits pre-trained models for NeRF representations alongside multimodal models for text and image processing. Our framework learns a bidirectional mapping between NeRF embeddings and those obtained from corresponding images and text. This mapping unlocks several novel and useful applications, including NeRF zero-shot classification and NeRF retrieval from images or text.||[2404.07993v1](http://arxiv.org/pdf/2404.07993v1)|null|\n", "2404.07922": "|**2024-04-11**|**LaVy: Vietnamese Multimodal Large Language Model**|LaVy\uff1a\u8d8a\u5357\u8bed\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b|Chi Tran, Huong Le Thanh|Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. All code and model weights are public at https://github.com/baochi0212/LaVy||[2404.07922v1](http://arxiv.org/pdf/2404.07922v1)|null|\n", "2404.07824": "|**2024-04-11**|**Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese**|Heron-Bench\uff1a\u8bc4\u4f30\u65e5\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6|Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, Yu Yamaguchi|Vision Language Models (VLMs) have undergone a rapid evolution, giving rise to significant advancements in the realm of multimodal understanding tasks. However, the majority of these models are trained and evaluated on English-centric datasets, leaving a gap in the development and evaluation of VLMs for other languages, such as Japanese. This gap can be attributed to the lack of methodologies for constructing VLMs and the absence of benchmarks to accurately measure their performance. To address this issue, we introduce a novel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of VLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer pairs tailored to the Japanese context. Additionally, we present a baseline Japanese VLM that has been trained with Japanese visual instruction tuning datasets. Our Heron-Bench reveals the strengths and limitations of the proposed VLM across various ability dimensions. Furthermore, we clarify the capability gap between strong closed models like GPT-4V and the baseline model, providing valuable insights for future research in this domain. We release the benchmark dataset and training code to facilitate further developments in Japanese VLM research.||[2404.07824v1](http://arxiv.org/pdf/2404.07824v1)|**[link](https://github.com/turingmotors/heron)**|\n", "2404.07790": "|**2024-04-11**|**VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing**|VIFNet\uff1a\u7528\u4e8e\u56fe\u50cf\u53bb\u96fe\u7684\u7aef\u5230\u7aef\u53ef\u89c1\u5149-\u7ea2\u5916\u878d\u5408\u7f51\u7edc|Meng Yu, Te Cui, Haoyang Lu, Yufeng Yue|Image dehazing poses significant challenges in environmental perception. Recent research mainly focus on deep learning-based methods with single modality, while they may result in severe information loss especially in dense-haze scenarios. The infrared image exhibits robustness to the haze, however, existing methods have primarily treated the infrared modality as auxiliary information, failing to fully explore its rich information in dehazing. To address this challenge, the key insight of this study is to design a visible-infrared fusion network for image dehazing. In particular, we propose a multi-scale Deep Structure Feature Extraction (DSFE) module, which incorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial and marginal information within the deep structural features. Additionally, we introduce an inconsistency weighted fusion strategy to merge the two modalities by leveraging the more reliable information. To validate this, we construct a visible-infrared multimodal dataset called AirSim-VID based on the AirSim simulation platform. Extensive experiments performed on challenging real and simulated image datasets demonstrate that VIFNet can outperform many state-of-the-art competing methods. The code and dataset are available at https://github.com/mengyu212/VIFNet_dehazing.||[2404.07790v1](http://arxiv.org/pdf/2404.07790v1)|null|\n", "2404.07785": "|**2024-04-11**|**PRAM: Place Recognition Anywhere Model for Efficient Visual Localization**|PRAM\uff1a\u7528\u4e8e\u9ad8\u6548\u89c6\u89c9\u5b9a\u4f4d\u7684\u4efb\u610f\u5730\u70b9\u8bc6\u522b\u6a21\u578b|Fei Xue, Ignas Budvytis, Roberto Cipolla|Humans localize themselves efficiently in known environments by first recognizing landmarks defined on certain objects and their spatial relationships, and then verifying the location by aligning detailed structures of recognized objects with those in the memory. Inspired by this, we propose the place recognition anywhere model (PRAM) to perform visual localization as efficiently as humans do. PRAM consists of two main components - recognition and registration. In detail, first of all, a self-supervised map-centric landmark definition strategy is adopted, making places in either indoor or outdoor scenes act as unique landmarks. Then, sparse keypoints extracted from images, are utilized as the input to a transformer-based deep neural network for landmark recognition; these keypoints enable PRAM to recognize hundreds of landmarks with high time and memory efficiency. Keypoints along with recognized landmark labels are further used for registration between query images and the 3D landmark map. Different from previous hierarchical methods, PRAM discards global and local descriptors, and reduces over 90% storage. Since PRAM utilizes recognition and landmark-wise verification to replace global reference search and exhaustive matching respectively, it runs 2.4 times faster than prior state-of-the-art approaches. Moreover, PRAM opens new directions for visual localization including multi-modality localization, map-centric feature learning, and hierarchical scene coordinate regression.||[2404.07785v1](http://arxiv.org/pdf/2404.07785v1)|null|\n", "2404.07449": "|**2024-04-11**|**Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs**|\u5b66\u4e60\u5b9a\u4f4d\u5bf9\u8c61\u53ef\u63d0\u9ad8\u89c6\u89c9\u6cd5\u5b66\u7855\u58eb\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b|Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin|Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.||[2404.07449v1](http://arxiv.org/pdf/2404.07449v1)|null|\n", "2404.07399": "|**2024-04-11**|**Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach**|\u4f7f\u7528\u8857\u666f\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u98d3\u98ce\u540e\u5efa\u7b51\u635f\u574f\u8bc4\u4f30\uff1a\u591a\u6a21\u5f0f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5|Zhuoqun Xue, Xiaojian Zhang, David O. Prevatt, Jennifer Bridge, Susu Xu, Xilei Zhao|Accurately assessing building damage is critical for disaster response and recovery. However, many existing models for detecting building damage have poor prediction accuracy due to their limited capabilities of identifying detailed, comprehensive structural and/or non-structural damage from the street-view image. Additionally, these models mainly rely on the imagery data for damage classification, failing to account for other critical information, such as wind speed, building characteristics, evacuation zones, and distance of the building to the hurricane track. To address these limitations, in this study, we propose a novel multi-modal (i.e., imagery and structured data) approach for post-hurricane building damage classification, named the Multi-Modal Swin Transformer (MMST). We empirically train and evaluate the proposed MMST using data collected from the 2022 Hurricane Ian in Florida, USA. Results show that MMST outperforms all selected state-of-the-art benchmark models and can achieve an accuracy of 92.67%, which are 7.71% improvement in accuracy compared to Visual Geometry Group 16 (VGG-16). In addition to the street-view imagery data, building value, building age, and wind speed are the most important predictors for damage level classification. The proposed MMST can be deployed to assist in rapid damage assessment and guide reconnaissance efforts in future hurricanes.||[2404.07399v1](http://arxiv.org/pdf/2404.07399v1)|null|\n"}, "Nerf": {"2404.07933": "|**2024-04-11**|**Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation**|\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4fc3\u8fdb\u5355\u89c6\u56fe\u573a\u666f\u5b8c\u6210\u7684\u81ea\u6211\u76d1\u7763|Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers|Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions.||[2404.07933v1](http://arxiv.org/pdf/2404.07933v1)|null|\n", "2404.07762": "|**2024-04-11**|**NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous Driving**|NeuroNCAP\uff1a\u81ea\u52a8\u9a7e\u9a76\u7684\u771f\u5b9e\u611f\u95ed\u73af\u5b89\u5168\u6d4b\u8bd5|William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle \u00c5str\u00f6m, Michael Felsberg, Christoffer Petersson|We present a versatile NeRF-based simulator for testing autonomous driving (AD) software systems, designed with a focus on sensor-realistic closed-loop evaluation and the creation of safety-critical scenarios. The simulator learns from sequences of real-world driving sensor data and enables reconfigurations and renderings of new, unseen scenarios. In this work, we use our simulator to test the responses of AD models to safety-critical scenarios inspired by the European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that, while state-of-the-art end-to-end planners excel in nominal driving scenarios in an open-loop setting, they exhibit critical flaws when navigating our safety-critical scenarios in a closed-loop setting. This highlights the need for advancements in the safety and real-world usability of end-to-end planners. By publicly releasing our simulator and scenarios as an easy-to-run evaluation suite, we invite the research community to explore, refine, and validate their AD models in controlled, yet highly configurable and challenging sensor-realistic environments. Code and instructions can be found at https://github.com/wljungbergh/NeuroNCAP||[2404.07762v1](http://arxiv.org/pdf/2404.07762v1)|**[link](https://github.com/wljungbergh/neuroncap)**|\n"}, "3DGS": {"2404.07991": "|**2024-04-11**|**GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh**|GoMAvatar\uff1a\u4f7f\u7528\u7f51\u683c\u4e0a\u7684\u9ad8\u65af\u4ece\u5355\u76ee\u89c6\u9891\u8fdb\u884c\u9ad8\u6548\u7684\u53ef\u52a8\u753b\u4eba\u4f53\u5efa\u6a21|Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang|We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).||[2404.07991v1](http://arxiv.org/pdf/2404.07991v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2404.07976": "|**2024-04-11**|**Self-supervised Dataset Distillation: A Good Compression Is All You Need**|\u81ea\u76d1\u7763\u6570\u636e\u96c6\u84b8\u998f\uff1a\u826f\u597d\u7684\u538b\u7f29\u5c31\u662f\u60a8\u6240\u9700\u8981\u7684|Muxin Zhou, Zeyuan Yin, Shitong Shao, Zhiqiang Shen|Dataset distillation aims to compress information from a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of the original data informational essence. Previous studies have predominantly concentrated on aligning the intermediate statistics between the original and distilled data, such as weight trajectory, features, gradient, BatchNorm, etc. In this work, we consider addressing this task through the new lens of model informativeness in the compression stage on the original dataset pretraining. We observe that with the prior state-of-the-art SRe$^2$L, as model sizes increase, it becomes increasingly challenging for supervised pretrained models to recover learned information during data synthesis, as the channel-wise mean and variance inside the model are flatting and less informative. We further notice that larger variances in BN statistics from self-supervised models enable larger loss signals to update the recovered data by gradients, enjoying more informativeness during synthesis. Building on this observation, we introduce SC-DD, a simple yet effective Self-supervised Compression framework for Dataset Distillation that facilitates diverse information compression and recovery compared to traditional supervised learning schemes, further reaps the potential of large pretrained models with enhanced capabilities. Extensive experiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasets to demonstrate the superiority of our proposed approach. The proposed SC-DD outperforms all previous state-of-the-art supervised dataset distillation methods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE, etc., by large margins under the same recovery and post-training budgets. Code is available at https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/.||[2404.07976v1](http://arxiv.org/pdf/2404.07976v1)|**[link](https://github.com/VILA-Lab/SRe2L)**|\n", "2404.07846": "|**2024-04-11**|**TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising**|TBSN\uff1a\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u81ea\u76d1\u7763\u56fe\u50cf\u53bb\u566a\u76f2\u70b9\u7f51\u7edc|Junyi Li, Zhilu Zhang, Wangmeng Zuo|Blind-spot networks (BSN) have been prevalent network architectures in self-supervised image denoising (SSID). Existing BSNs are mostly conducted with convolution layers. Although transformers offer potential solutions to the limitations of convolutions and have demonstrated success in various image restoration tasks, their attention mechanisms may violate the blind-spot requirement, thus restricting their applicability in SSID. In this paper, we present a transformer-based blind-spot network (TBSN) by analyzing and redesigning the transformer operators that meet the blind-spot requirement. Specifically, TBSN follows the architectural principles of dilated BSNs, and incorporates spatial as well as channel self-attention layers to enhance the network capability. For spatial self-attention, an elaborate mask is applied to the attention matrix to restrict its receptive field, thus mimicking the dilated convolution. For channel self-attention, we observe that it may leak the blind-spot information when the channel number is greater than spatial size in the deep layers of multi-scale architectures. To eliminate this effect, we divide the channel into several groups and perform channel attention separately. Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods. The code and pre-trained models will be publicly available at https://github.com/nagejacob/TBSN.||[2404.07846v1](http://arxiv.org/pdf/2404.07846v1)|**[link](https://github.com/nagejacob/tbsn)**|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2404.07983": "|**2024-04-11**|**Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning**|\u4e24\u79cd\u5f71\u54cd\uff0c\u4e00\u79cd\u89e6\u53d1\uff1a\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u3001\u5bf9\u8c61\u504f\u5dee\u548c\u4fe1\u606f\u4e0d\u5e73\u8861|Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, Thomas Brox|Contrastive vision-language models like CLIP have gained popularity for their versatile applicable learned representations in various downstream tasks. Despite their successes in some tasks, like zero-shot image recognition, they also perform surprisingly poor on other tasks, like attribute detection. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and a bias towards objects over other factors, such as attributes. In this work we investigate both phenomena. We find that only a few embedding dimensions drive the modality gap. Further, we propose a measure for object bias and find that object bias does not lead to worse performance on other concepts, such as attributes. But what leads to the emergence of the modality gap and object bias? To answer this question we carefully designed an experimental setting which allows us to control the amount of shared information between the modalities. This revealed that the driving factor behind both, the modality gap and the object bias, is the information imbalance between images and captions.||[2404.07983v1](http://arxiv.org/pdf/2404.07983v1)|null|\n", "2404.07977": "|**2024-04-11**|**Gaga: Group Any Gaussians via 3D-aware Memory Bank**|Gaga\uff1a\u901a\u8fc7 3D \u611f\u77e5\u5185\u5b58\u5e93\u5bf9\u4efb\u4f55\u9ad8\u65af\u8fdb\u884c\u5206\u7ec4|Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang|We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation models. Contrasted to prior 3D scene segmentation approaches that heavily rely on video object tracking, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot segmentation models, enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as scene understanding and manipulation.||[2404.07977v1](http://arxiv.org/pdf/2404.07977v1)|null|\n", "2404.07887": "|**2024-04-11**|**Context-aware Video Anomaly Detection in Long-Term Datasets**|\u957f\u671f\u6570\u636e\u96c6\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b|Zhengye Yang, Richard Radke|Video anomaly detection research is generally evaluated on short, isolated benchmark videos only a few minutes long. However, in real-world environments, security cameras observe the same scene for months or years at a time, and the notion of anomalous behavior critically depends on context, such as the time of day, day of week, or schedule of events. Here, we propose a context-aware video anomaly detection algorithm, Trinity, specifically targeted to these scenarios. Trinity is especially well-suited to crowded scenes in which individuals cannot be easily tracked, and anomalies are due to speed, direction, or absence of group motion. Trinity is a contrastive learning framework that aims to learn alignments between context, appearance, and motion, and uses alignment quality to classify videos as normal or anomalous. We evaluate our algorithm on both conventional benchmarks and a public webcam-based dataset we collected that spans more than three months of activity.||[2404.07887v1](http://arxiv.org/pdf/2404.07887v1)|null|\n", "2404.07867": "|**2024-04-11**|**The Power of Properties: Uncovering the Influential Factors in Emotion Classification**|\u5c5e\u6027\u7684\u529b\u91cf\uff1a\u63ed\u793a\u60c5\u7eea\u5206\u7c7b\u7684\u5f71\u54cd\u56e0\u7d20|Tim B\u00fcchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler|Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.||[2404.07867v1](http://arxiv.org/pdf/2404.07867v1)|null|\n", "2404.07833": "|**2024-04-11**|**Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution**|\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u7b80\u5316\u5149\u58f0\u56fe\u50cf\u5904\u7406\uff1a\u514d\u57f9\u8bad\u89e3\u51b3\u65b9\u6848|Handi Deng, Yucheng Zhou, Jiaxuan Xiang, Liujie Gu, Yan Luo, Hai Feng, Mingyuan Liu, Cheng Ma|Foundation models have rapidly evolved and have achieved significant accomplishments in computer vision tasks. Specifically, the prompt mechanism conveniently allows users to integrate image prior information into the model, making it possible to apply models without any training. Therefore, we propose a method based on foundation models and zero training to solve the tasks of photoacoustic (PA) image segmentation. We employed the segment anything model (SAM) by setting simple prompts and integrating the model's outputs with prior knowledge of the imaged objects to accomplish various tasks, including: (1) removing the skin signal in three-dimensional PA image rendering; (2) dual speed-of-sound reconstruction, and (3) segmentation of finger blood vessels. Through these demonstrations, we have concluded that deep learning can be directly applied in PA imaging without the requirement for network design and training. This potentially allows for a hands-on, convenient approach to achieving efficient and accurate segmentation of PA images. This letter serves as a comprehensive tutorial, facilitating the mastery of the technique through the provision of code and sample datasets.||[2404.07833v1](http://arxiv.org/pdf/2404.07833v1)|null|\n", "2404.07821": "|**2024-04-11**|**Sparse Laneformer**|\u7a00\u758f\u8f66\u9053\u5f62\u6210\u5668|Ji Liu, Zifeng Zhang, Mingjie Lu, Hongyang Wei, Dong Li, Yile Xie, Jinzhang Peng, Lu Tian, Ashish Sirasao, Emad Barsoum|Lane detection is a fundamental task in autonomous driving, and has achieved great progress as deep learning emerges. Previous anchor-based methods often design dense anchors, which highly depend on the training dataset and remain fixed during inference. We analyze that dense anchors are not necessary for lane detection, and propose a transformer-based lane detection framework based on a sparse anchor mechanism. To this end, we generate sparse anchors with position-aware lane queries and angle queries instead of traditional explicit anchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane features along the horizontal direction, and adopt Lane-Angle Cross Attention (LACA) to perform interactions between lane queries and angle queries. We also propose Lane Perceptual Attention (LPA) based on deformable cross attention to further refine the lane predictions. Our method, named Sparse Laneformer, is easy-to-implement and end-to-end trainable. Extensive experiments demonstrate that Sparse Laneformer performs favorably against the state-of-the-art methods, e.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score with fewer MACs on CULane with the same ResNet-34 backbone.||[2404.07821v1](http://arxiv.org/pdf/2404.07821v1)|null|\n", "2404.07807": "|**2024-04-11**|**Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network**|\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u97f3\u8f85\u52a9\u5b9e\u65f6\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf|Mayura Manawadu, Udaya Wijenayake|Traffic signs are important in communicating information to drivers. Thus, comprehension of traffic signs is essential for road safety and ignorance may result in road accidents. Traffic sign detection has been a research spotlight over the past few decades. Real-time and accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved. This study presents a voice-assisted real-time traffic sign recognition system which is capable of assisting drivers. This system functions under two subsystems. Initially, the detection and recognition of the traffic signs are carried out using a trained Convolutional Neural Network (CNN). After recognizing the specific traffic sign, it is narrated to the driver as a voice message using a text-to-speech engine. An efficient CNN model for a benchmark dataset is developed for real-time detection and recognition using Deep Learning techniques. The advantage of this system is that even if the driver misses a traffic sign, or does not look at the traffic sign, or is unable to comprehend the sign, the system detects it and narrates it to the driver. A system of this type is also important in the development of autonomous vehicles.||[2404.07807v1](http://arxiv.org/pdf/2404.07807v1)|null|\n", "2404.07788": "|**2024-04-11**|**AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation**|AUG\uff1a\u7528\u4e8e\u822a\u7a7a\u56fe\u50cf\u57ce\u5e02\u573a\u666f\u56fe\u751f\u6210\u7684\u65b0\u6570\u636e\u96c6\u548c\u9ad8\u6548\u6a21\u578b|Yansheng Li, Kun Li, Yongjun Zhang, Linlin Wang, Dingwen Zhang|Scene graph generation (SGG) aims to understand the visual objects and their semantic relationships from one given image. Until now, lots of SGG datasets with the eyelevel view are released but the SGG dataset with the overhead view is scarcely studied. By contrast to the object occlusion problem in the eyelevel view, which impedes the SGG, the overhead view provides a new perspective that helps to promote the SGG by providing a clear perception of the spatial relationships of objects in the ground scene. To fill in the gap of the overhead view dataset, this paper constructs and releases an aerial image urban scene graph generation (AUG) dataset. Images from the AUG dataset are captured with the low-attitude overhead view. In the AUG dataset, 25,594 objects, 16,970 relationships, and 27,175 attributes are manually annotated. To avoid the local context being overwhelmed in the complex aerial urban scene, this paper proposes one new locality-preserving graph convolutional network (LPG). Different from the traditional graph convolutional network, which has the natural advantage of capturing the global context for SGG, the convolutional layer in the LPG integrates the non-destructive initial features of the objects with dynamically updated neighborhood information to preserve the local context under the premise of mining the global context. To address the problem that there exists an extra-large number of potential object relationship pairs but only a small part of them is meaningful in AUG, we propose the adaptive bounding box scaling factor for potential relationship detection (ABS-PRD) to intelligently prune the meaningless relationship pairs. Extensive experiments on the AUG dataset show that our LPG can significantly outperform the state-of-the-art methods and the effectiveness of the proposed locality-preserving strategy.||[2404.07788v1](http://arxiv.org/pdf/2404.07788v1)|null|\n", "2404.07748": "|**2024-04-11**|**3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces**|3D-CSAD\uff1a\u9488\u5bf9\u590d\u6742\u5236\u9020\u8868\u9762\u7684\u672a\u7ecf\u8bad\u7ec3\u7684 3D \u5f02\u5e38\u68c0\u6d4b|Xuanming Cao, Chengyu Tao, Juan Du|The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.||[2404.07748v1](http://arxiv.org/pdf/2404.07748v1)|null|\n", "2404.07739": "|**2024-04-11**|**Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification**|\u5229\u7528\u57fa\u4e8e\u5bf9\u8c61\u548c\u57fa\u4e8e\u5206\u5272\u7684\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5ba4\u5185\u573a\u666f\u5206\u7c7b|Ricardo Pereira, Lu\u00eds Garrote, Tiago Barros, Ana Lopes, Urbano J. Nunes|Indoor scenes are usually characterized by scattered objects and their relationships, which turns the indoor scene classification task into a challenging computer vision task. Despite the significant performance boost in classification tasks achieved in recent years, provided by the use of deep-learning-based methods, limitations such as inter-category ambiguity and intra-category variation have been holding back their performance. To overcome such issues, gathering semantic information has been shown to be a promising source of information towards a more complete and discriminative feature representation of indoor scenes. Therefore, the work described in this paper uses both semantic information, obtained from object detection, and semantic segmentation techniques. While object detection techniques provide the 2D location of objects allowing to obtain spatial distributions between objects, semantic segmentation techniques provide pixel-level information that allows to obtain, at a pixel-level, a spatial distribution and shape-related features of the segmentation categories. Hence, a novel approach that uses a semantic segmentation mask to provide Hu-moments-based segmentation categories' shape characterization, designated by Segmentation-based Hu-Moments Features (SHMFs), is proposed. Moreover, a three-main-branch network, designated by GOS$^2$F$^2$App, that exploits deep-learning-based global features, object-based features, and semantic segmentation-based features is also proposed. GOS$^2$F$^2$App was evaluated in two indoor scene benchmark datasets: SUN RGB-D and NYU Depth V2, where, to the best of our knowledge, state-of-the-art results were achieved on both datasets, which present evidences of the effectiveness of the proposed approach.||[2404.07739v1](http://arxiv.org/pdf/2404.07739v1)|null|\n", "2404.07711": "|**2024-04-11**|**OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities**|OpenTrench3D\uff1a\u7528\u4e8e\u5730\u4e0b\u516c\u7528\u8bbe\u65bd\u8bed\u4e49\u5206\u5272\u7684\u6444\u5f71\u6d4b\u91cf 3D \u70b9\u4e91\u6570\u636e\u96c6|Lasse H. Hansen, Simon B. Jensen, Mark P. Philipsen, Andreas M\u00f8gelmose, Lars Bodum, Thomas B. Moeslund|Identifying and classifying underground utilities is an important task for efficient and effective urban planning and infrastructure maintenance. We present OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point cloud dataset, designed to advance research and development in underground utility surveying and mapping. OpenTrench3D covers a completely novel domain for public 3D point cloud datasets and is unique in its focus, scope, and cost-effective capturing method. The dataset consists of 310 point clouds collected across 7 distinct areas. These include 5 water utility areas and 2 district heating utility areas. The inclusion of different geographical areas and main utilities (water and district heating utilities) makes OpenTrench3D particularly valuable for inter-domain transfer learning experiments. We provide benchmark results for the dataset using three state-of-the-art semantic segmentation models, PointNeXt, PointVector and PointMetaBase. Benchmarks are conducted by training on data from water areas, fine-tuning on district heating area 1 and evaluating on district heating area 2. The dataset is publicly available. With OpenTrench3D, we seek to foster innovation and progress in the field of 3D semantic segmentation in applications related to detection and documentation of underground utilities as well as in transfer learning methods in general.||[2404.07711v1](http://arxiv.org/pdf/2404.07711v1)|null|\n", "2404.07705": "|**2024-04-11**|**ViM-UNet: Vision Mamba for Biomedical Segmentation**|ViM-UNet\uff1a\u7528\u4e8e\u751f\u7269\u533b\u5b66\u7ec6\u5206\u7684 Vision Mamba|Anwai Archit, Constantin Pape|CNNs, most notably the UNet, are the default architecture for biomedical segmentation. Transformer-based approaches, such as UNETR, have been proposed to replace them, benefiting from a global field of view, but suffering from larger runtimes and higher parameter counts. The recent Vision Mamba architecture offers a compelling alternative to transformers, also providing a global field of view, but at higher efficiency. Here, we introduce ViM-UNet, a novel segmentation architecture based on it and compare it to UNet and UNETR for two challenging microscopy instance segmentation tasks. We find that it performs similarly or better than UNet, depending on the task, and outperforms UNETR while being more efficient. Our code is open source and documented at https://github.com/constantinpape/torch-em/blob/main/vimunet.md.||[2404.07705v1](http://arxiv.org/pdf/2404.07705v1)|**[link](https://github.com/constantinpape/torch-em)**|\n", "2404.07696": "|**2024-04-11**|**Flatness Improves Backbone Generalisation in Few-shot Classification**|\u5e73\u5766\u5ea6\u63d0\u9ad8\u4e86\u5c11\u6837\u672c\u5206\u7c7b\u4e2d\u7684\u4e3b\u5e72\u6cdb\u5316\u80fd\u529b|Rui Li, Martin Trapp, Marcus Klasson, Arno Solin|Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples. Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. Surprisingly, most efforts have only focused on developing architectures for easing the adaptation to the target domain without considering the importance of backbone training for good generalisation. We show that flatness-aware backbone training with vanilla fine-tuning results in a simpler yet competitive baseline compared to the state-of-the-art. Our results indicate that for in- and cross-domain FSC, backbone training is crucial to achieving good generalisation across different adaptation methods. We advocate more care should be taken when training these models.||[2404.07696v1](http://arxiv.org/pdf/2404.07696v1)|null|\n", "2404.07687": "|**2024-04-11**|**Chaos in Motion: Unveiling Robustness in Remote Heart Rate Measurement through Brain-Inspired Skin Tracking**|\u8fd0\u52a8\u4e2d\u7684\u6df7\u6c8c\uff1a\u901a\u8fc7\u53d7\u5927\u8111\u542f\u53d1\u7684\u76ae\u80a4\u8ddf\u8e2a\u63ed\u793a\u8fdc\u7a0b\u5fc3\u7387\u6d4b\u91cf\u7684\u9c81\u68d2\u6027|Jie Wang, Jing Lian, Minjie Ma, Junqiang Lei, Chunbiao Li, Bin Li, Jizhao Liu|Heart rate is an important physiological indicator of human health status. Existing remote heart rate measurement methods typically involve facial detection followed by signal extraction from the region of interest (ROI). These SOTA methods have three serious problems: (a) inaccuracies even failures in detection caused by environmental influences or subject movement; (b) failures for special patients such as infants and burn victims; (c) privacy leakage issues resulting from collecting face video. To address these issues, we regard the remote heart rate measurement as the process of analyzing the spatiotemporal characteristics of the optical flow signal in the video. We apply chaos theory to computer vision tasks for the first time, thus designing a brain-inspired framework. Firstly, using an artificial primary visual cortex model to extract the skin in the videos, and then calculate heart rate by time-frequency analysis on all pixels. Our method achieves Robust Skin Tracking for Heart Rate measurement, called HR-RST. The experimental results show that HR-RST overcomes the difficulty of environmental influences and effectively tracks the subject movement. Moreover, the method could extend to other body parts. Consequently, the method can be applied to special patients and effectively protect individual privacy, offering an innovative solution.||[2404.07687v1](http://arxiv.org/pdf/2404.07687v1)|null|\n", "2404.07686": "|**2024-04-11**|**Depth Estimation using Weighted-loss and Transfer Learning**|\u4f7f\u7528\u52a0\u6743\u635f\u5931\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6df1\u5ea6\u4f30\u8ba1|Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah|Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.||[2404.07686v1](http://arxiv.org/pdf/2404.07686v1)|null|\n", "2404.07685": "|**2024-04-11**|**Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns**|\u4f7f\u7528\u65e9\u671f\u5c42\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684 3D \u7269\u4f53\u68c0\u6d4b\u8fdb\u884c\u8fd0\u884c\u65f6\u76d1\u63a7|Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman|Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.||[2404.07685v1](http://arxiv.org/pdf/2404.07685v1)|null|\n", "2404.07671": "|**2024-04-11**|**Deep learning-driven pulmonary arteries and veins segmentation reveals demography-associated pulmonary vasculature anatomy**|\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u80ba\u52a8\u8109\u548c\u9759\u8109\u5206\u5272\u63ed\u793a\u4e86\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u76f8\u5173\u7684\u80ba\u8109\u7ba1\u7cfb\u7edf\u89e3\u5256\u7ed3\u6784|Yuetan Chu, Gongning Luo, Longxi Zhou, Shaodong Cao, Guolin Ma, Xianglin Meng, Juexiao Zhou, Changchun Yang, Dexuan Xie, Ricardo Henao, et.al.|Pulmonary artery-vein segmentation is crucial for diagnosing pulmonary diseases and surgical planning, and is traditionally achieved by Computed Tomography Pulmonary Angiography (CTPA). However, concerns regarding adverse health effects from contrast agents used in CTPA have constrained its clinical utility. In contrast, identifying arteries and veins using non-contrast CT, a conventional and low-cost clinical examination routine, has long been considered impossible. Here we propose a High-abundant Pulmonary Artery-vein Segmentation (HiPaS) framework achieving accurate artery-vein segmentation on both non-contrast CT and CTPA across various spatial resolutions. HiPaS first performs spatial normalization on raw CT scans via a super-resolution module, and then iteratively achieves segmentation results at different branch levels by utilizing the low-level vessel segmentation as a prior for high-level vessel segmentation. We trained and validated HiPaS on our established multi-centric dataset comprising 1,073 CT volumes with meticulous manual annotation. Both quantitative experiments and clinical evaluation demonstrated the superior performance of HiPaS, achieving a dice score of 91.8% and a sensitivity of 98.0%. Further experiments demonstrated the non-inferiority of HiPaS segmentation on non-contrast CT compared to segmentation on CTPA. Employing HiPaS, we have conducted an anatomical study of pulmonary vasculature on 10,613 participants in China (five sites), discovering a new association between pulmonary vessel abundance and sex and age: vessel abundance is significantly higher in females than in males, and slightly decreases with age, under the controlling of lung volumes (p < 0.0001). HiPaS realizing accurate artery-vein segmentation delineates a promising avenue for clinical diagnosis and understanding pulmonary physiology in a non-invasive manner.||[2404.07671v1](http://arxiv.org/pdf/2404.07671v1)|**[link](https://github.com/arturia-pendragon-iris/hipas_av_segmentation)**|\n", "2404.07667": "|**2024-04-11**|**Dealing with Subject Similarity in Differential Morphing Attack Detection**|\u5904\u7406\u5dee\u5206\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u4e3b\u9898\u76f8\u4f3c\u6027|Nicol\u00f2 Di Domenico, Guido Borghi, Annalisa Franco, Davide Maltoni|The advent of morphing attacks has posed significant security concerns for automated Face Recognition systems, raising the pressing need for robust and effective Morphing Attack Detection (MAD) methods able to effectively address this issue. In this paper, we focus on Differential MAD (D-MAD), where a trusted live capture, usually representing the criminal, is compared with the document image to classify it as morphed or bona fide. We show these approaches based on identity features are effective when the morphed image and the live one are sufficiently diverse; unfortunately, the effectiveness is significantly reduced when the same approaches are applied to look-alike subjects or in all those cases when the similarity between the two compared images is high (e.g. comparison between the morphed image and the accomplice). Therefore, in this paper, we propose ACIdA, a modular D-MAD system, consisting of a module for the attempt type classification, and two modules for the identity and artifacts analysis on input images. Successfully addressing this task would allow broadening the D-MAD applications including, for instance, the document enrollment stage, which currently relies entirely on human evaluation, thus limiting the possibility of releasing ID documents with manipulated images, as well as the automated gates to detect both accomplices and criminals. An extensive cross-dataset experimental evaluation conducted on the introduced scenario shows that ACIdA achieves state-of-the-art results, outperforming literature competitors, while maintaining good performance in traditional D-MAD benchmarks.||[2404.07667v1](http://arxiv.org/pdf/2404.07667v1)|**[link](https://github.com/ndido98/acida)**|\n", "2404.07664": "|**2024-04-11**|**Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes**|\u5bfb\u627e\u6050\u9f99\uff1a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u7528\u539f\u578b\u5bf9\u5206\u5e03\u5916\u5bf9\u8c61\u8fdb\u884c\u65e0\u76d1\u7763\u68c0\u6d4b|Poulami Sinhamahapatra, Franziska Schwaiger, Shirsha Bose, Huiyu Wang, Karsten Roscher, Stephan Guennemann|Detecting and localising unknown or Out-of-distribution (OOD) objects in any scene can be a challenging task in vision. Particularly, in safety-critical cases involving autonomous systems like automated vehicles or trains. Supervised anomaly segmentation or open-world object detection models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing between background and OOD objects. In this work, we present a plug-and-play generalised framework - PRototype-based zero-shot OOD detection Without Labels (PROWL). It is an inference-based method that does not require training on the domain dataset and relies on extracting relevant features from self-supervised pre-trained models. PROWL can be easily adapted to detect OOD objects in any operational design domain by specifying a list of known classes from this domain. PROWL, as an unsupervised method, outperforms other supervised methods trained without auxiliary OOD data on the RoadAnomaly and RoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) benchmark. We also demonstrate its suitability for other domains such as rail and maritime scenes.||[2404.07664v1](http://arxiv.org/pdf/2404.07664v1)|null|\n", "2404.07645": "|**2024-04-11**|**Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos**|Simba\uff1aMamba \u589e\u5f3a\u4e86 U-ShiftGCN\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u7684\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b|Soumyabrata Chaudhuri, Saumik Bhattacharya|Skeleton Action Recognition (SAR) involves identifying human actions using skeletal joint coordinates and their interconnections. While plain Transformers have been attempted for this task, they still fall short compared to the current leading methods, which are rooted in Graph Convolutional Networks (GCNs) due to the absence of structural priors. Recently, a novel selective state space model, Mamba, has surfaced as a compelling alternative to the attention mechanism in Transformers, offering efficient modeling of long sequences. In this work, to the utmost extent of our awareness, we present the first SAR framework incorporating Mamba. Each fundamental block of our model adopts a novel U-ShiftGCN architecture with Mamba as its core component. The encoder segment of the U-ShiftGCN is devised to extract spatial features from the skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial features then undergo intermediate temporal modeling facilitated by the Mamba block before progressing to the encoder section, which comprises vanilla upsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal modeling unit is employed before the exit of each fundamental block to refine temporal representations. This particular integration of downsampling spatial, intermediate temporal, upsampling spatial, and ultimate temporal subunits yields promising results for skeleton action recognition. We dub the resulting model \\textbf{Simba}, which attains state-of-the-art performance across three well-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without Intermediate Mamba Block) by itself is capable of performing reasonably well and surpasses our baseline.||[2404.07645v1](http://arxiv.org/pdf/2404.07645v1)|null|\n", "2404.07626": "|**2024-04-11**|**Homography Guided Temporal Fusion for Road Line and Marking Segmentation**|\u7528\u4e8e\u9053\u8def\u7ebf\u548c\u6807\u8bb0\u5206\u5272\u7684\u5355\u5e94\u6027\u5f15\u5bfc\u65f6\u95f4\u878d\u5408|Shan Wang, Chuong Nguyen, Jiawei Liu, Kaihao Zhang, Wenhan Luo, Yanhao Zhang, Sundaram Muthu, Fahira Afzal Maken, Hongdong Li|Reliable segmentation of road lines and markings is critical to autonomous driving. Our work is motivated by the observations that road lines and markings are (1) frequently occluded in the presence of moving vehicles, shadow, and glare and (2) highly structured with low intra-class shape variance and overall high appearance consistency. To solve these issues, we propose a Homography Guided Fusion (HomoFusion) module to exploit temporally-adjacent video frames for complementary cues facilitating the correct classification of the partially occluded road lines or markings. To reduce computational complexity, a novel surface normal estimator is proposed to establish spatial correspondences between the sampled frames, allowing the HomoFusion module to perform a pixel-to-pixel attention mechanism in updating the representation of the occluded road lines or markings. Experiments on ApolloScape, a large-scale lane mark segmentation dataset, and ApolloScape Night with artificial simulated night-time road conditions, demonstrate that our method outperforms other existing SOTA lane mark segmentation models with less than 9\\% of their parameters and computational complexity. We show that exploiting available camera intrinsic data and ground plane assumption for cross-frame correspondence can lead to a light-weight network with significantly improved performances in speed and accuracy. We also prove the versatility of our HomoFusion approach by applying it to the problem of water puddle segmentation and achieving SOTA performance.||[2404.07626v1](http://arxiv.org/pdf/2404.07626v1)|**[link](https://github.com/shanwang-shan/homofusion)**|\n", "2404.07622": "|**2024-04-11**|**Multi-Image Visual Question Answering for Unsupervised Anomaly Detection**|\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u591a\u56fe\u50cf\u89c6\u89c9\u95ee\u7b54|Jun Li, Cosmin I. Bercea, Philip M\u00fcller, Lina Felsner, Suhwan Kim, Daniel Rueckert, Benedikt Wiestler, Julia A. Schnabel|Unsupervised anomaly detection enables the identification of potential pathological areas by juxtaposing original images with their pseudo-healthy reconstructions generated by models trained exclusively on normal images. However, the clinical interpretation of resultant anomaly maps presents a challenge due to a lack of detailed, understandable explanations. Recent advancements in language models have shown the capability of mimicking human-like understanding and providing detailed descriptions. This raises an interesting question: \\textit{How can language models be employed to make the anomaly maps more explainable?} To the best of our knowledge, we are the first to leverage a language model for unsupervised anomaly detection, for which we construct a dataset with different questions and answers. Additionally, we present a novel multi-image visual question answering framework tailored for anomaly detection, incorporating diverse feature fusion strategies to enhance visual knowledge extraction. Our experiments reveal that the framework, augmented by our new Knowledge Q-Former module, adeptly answers questions on the anomaly detection dataset. Besides, integrating anomaly maps as inputs distinctly aids in improving the detection of unseen pathologies.||[2404.07622v1](http://arxiv.org/pdf/2404.07622v1)|null|\n", "2404.07607": "|**2024-04-11**|**Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery**|\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u536b\u661f\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u9ed1\u6697\u7684\u8239\u5bf9\u8239\u4f20\u8f93|Ollie Ballinger|Despite extensive research into ship detection via remote sensing, no studies identify ship-to-ship transfers in satellite imagery. Given the importance of transshipment in illicit shipping practices, this is a significant gap. In what follows, I train a convolutional neural network to accurately detect 4 different types of cargo vessel and two different types of Ship-to-Ship transfer in PlanetScope satellite imagery. I then elaborate a pipeline for the automatic detection of suspected illicit ship-to-ship transfers by cross-referencing satellite detections with vessel borne GPS data. Finally, I apply this method to the Kerch Strait between Ukraine and Russia to identify over 400 dark transshipment events since 2022.||[2404.07607v1](http://arxiv.org/pdf/2404.07607v1)|null|\n", "2404.07605": "|**2024-04-11**|**Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification**|\u7528\u4e8e\u6807\u7b7e\u6297\u566a\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u57fa\u4e8e\u5bf9\u6bd4\u7684\u6df1\u5ea6\u5d4c\u5165|Lucas Dedieu, Nicolas Nerrienet, Adrien Nivaggioli, Clara Simmat, Marceau Clavel, Arnaud Gauthier, St\u00e9phane Sockeel, R\u00e9my Peyret|Recent advancements in deep learning have proven highly effective in medical image classification, notably within histopathology. However, noisy labels represent a critical challenge in histopathology image classification, where accurate annotations are vital for training robust deep learning models. Indeed, deep neural networks can easily overfit label noise, leading to severe degradations in model performance. While numerous public pathology foundation models have emerged recently, none have evaluated their resilience to label noise. Through thorough empirical analyses across multiple datasets, we exhibit the label noise resilience property of embeddings extracted from foundation models trained in a self-supervised contrastive manner. We demonstrate that training with such embeddings substantially enhances label noise robustness when compared to non-contrastive-based ones as well as commonly used noise-resilient methods. Our results unequivocally underline the superiority of contrastive learning in effectively mitigating the label noise challenge. Code is publicly available at https://github.com/LucasDedieu/NoiseResilientHistopathology.||[2404.07605v1](http://arxiv.org/pdf/2404.07605v1)|**[link](https://github.com/lucasdedieu/noiseresilienthistopathology)**|\n", "2404.07603": "|**2024-04-11**|**GLID: Pre-training a Generalist Encoder-Decoder Vision Model**|GLID\uff1a\u9884\u8bad\u7ec3\u901a\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u89c6\u89c9\u6a21\u578b|Jihao Liu, Jinliang Zheng, Yu Liu, Hongsheng Li|This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks. While self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown success in transfer learning, task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training. GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be fine-tuned on various vision tasks with minimal task-specific architecture modifications. In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as \"query-to-answer\" problems, including the pre-training pretext task and other downstream tasks. We pre-train a task-agnostic encoder-decoder with query-mask pairs. During fine-tuning, GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads. This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks. GLID achieves competitive performance on various vision tasks, including object detection, image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer.||[2404.07603v1](http://arxiv.org/pdf/2404.07603v1)|null|\n", "2404.07602": "|**2024-04-11**|**Attention based End to end network for Offline Writer Identification on Word level data**|\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u7528\u4e8e\u5b57\u7ea7\u6570\u636e\u7684\u79bb\u7ebf\u4f5c\u8005\u8bc6\u522b|Vineet Kumar, Suresh Sundaram|Writer identification due to its widespread application in various fields has gained popularity over the years. In scenarios where optimum handwriting samples are available, whether they be in the form of a single line, a sentence, or an entire page, writer identification algorithms have demonstrated noteworthy levels of accuracy. However, in scenarios where only a limited number of handwritten samples are available, particularly in the form of word images, there is a significant scope for improvement.   In this paper, we propose a writer identification system based on an attention-driven Convolutional Neural Network (CNN). The system is trained utilizing image segments, known as fragments, extracted from word images, employing a pyramid-based strategy. This methodology enables the system to capture a comprehensive representation of the data, encompassing both fine-grained details and coarse features across various levels of abstraction. These extracted fragments serve as the training data for the convolutional network, enabling it to learn a more robust representation compared to traditional convolution-based networks trained on word images. Additionally, the paper explores the integration of an attention mechanism to enhance the representational power of the learned features. The efficacy of the proposed algorithm is evaluated on three benchmark databases, demonstrating its proficiency in writer identification tasks, particularly in scenarios with limited access to handwriting data.||[2404.07602v1](http://arxiv.org/pdf/2404.07602v1)|null|\n", "2404.07594": "|**2024-04-11**|**Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization**|\u901a\u8fc7\u591a\u8fb9\u89e3\u7801\u5668\u5206\u652f\u8fdb\u884c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5fc3\u8840\u7ba1\u5bfc\u7ba1\u63d2\u5165\u672f\u4e2d\u7684\u5bfc\u4e1d\u5206\u5272|Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang|Although robot-assisted cardiovascular catheterization is commonly performed for intervention of cardiovascular diseases, more studies are needed to support the procedure with automated tool segmentation. This can aid surgeons on tool tracking and visualization during intervention. Learning-based segmentation has recently offered state-of-the-art segmentation performances however, generating ground-truth signals for fully-supervised methods is labor-intensive and time consuming for the interventionists. In this study, a weakly-supervised learning method with multi-lateral pseudo labeling is proposed for tool segmentation in cardiac angiograms. The method includes a modified U-Net model with one encoder and multiple lateral-branched decoders that produce pseudo labels as supervision signals under different perturbation. The pseudo labels are self-generated through a mixed loss function and shared consistency in the decoders. We trained the model end-to-end with weakly-annotated data obtained during robotic cardiac catheterization. Experiments with the proposed model shows weakly annotated data has closer performance to when fully annotated data is used. Compared to three existing weakly-supervised methods, our approach yielded higher segmentation performance across three different cardiac angiogram data. With ablation study, we showed consistent performance under different parameters. Thus, we offer a less expensive method for real-time tool segmentation and tracking during robot-assisted cardiac catheterization.||[2404.07594v1](http://arxiv.org/pdf/2404.07594v1)|null|\n", "2404.07580": "|**2024-04-11**|**Multi-rater Prompting for Ambiguous Medical Image Segmentation**|\u591a\u8bc4\u4f30\u8005\u63d0\u793a\u6a21\u7cca\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272|Jinhong Wang, Yi Cheng, Jintai Chen, Hongxia Xu, Danny Chen, Jian Wu|Multi-rater annotations commonly occur when medical images are independently annotated by multiple experts (raters). In this paper, we tackle two challenges arisen in multi-rater annotations for medical image segmentation (called ambiguous medical image segmentation): (1) How to train a deep learning model when a group of raters produces a set of diverse but plausible annotations, and (2) how to fine-tune the model efficiently when computation resources are not available for re-training the entire model on a different dataset domain. We propose a multi-rater prompt-based approach to address these two challenges altogether. Specifically, we introduce a series of rater-aware prompts that can be plugged into the U-Net model for uncertainty estimation to handle multi-annotation cases. During the prompt-based fine-tuning process, only 0.3% of learnable parameters are required to be updated comparing to training the entire model. Further, in order to integrate expert consensus and disagreement, we explore different multi-rater incorporation strategies and design a mix-training strategy for comprehensive insight learning. Extensive experiments verify the effectiveness of our new approach for ambiguous medical image segmentation on two public datasets while alleviating the heavy burden of model re-training.||[2404.07580v1](http://arxiv.org/pdf/2404.07580v1)|null|\n", "2404.07553": "|**2024-04-11**|**SFSORT: Scene Features-based Simple Online Real-Time Tracker**|SFSORT\uff1a\u57fa\u4e8e\u573a\u666f\u7279\u5f81\u7684\u7b80\u5355\u5728\u7ebf\u5b9e\u65f6\u8ddf\u8e2a\u5668|M. M. Morsali, Z. Sharifi, F. Fallah, S. Hashembeiki, H. Mohammadzade, S. Bagheri Shouraki|This paper introduces SFSORT, the world's fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets. To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature. By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements. Additionally, this paper demonstrates the impact of scene features on enhancing object-track association and improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7\\% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9\\% with a processing speed of 304 Hz on the MOT20 dataset. The tracker's source code, fine-tuned object detection model, and tutorials are available at \\url{https://github.com/gitmehrdad/SFSORT}.||[2404.07553v1](http://arxiv.org/pdf/2404.07553v1)|**[link](https://github.com/gitmehrdad/sfsort)**|\n", "2404.07514": "|**2024-04-11**|**Generalization Gap in Data Augmentation: Insights from Illumination**|\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u6cdb\u5316\u5dee\u8ddd\uff1a\u6765\u81ea\u542f\u53d1\u7684\u89c1\u89e3|Jianqiang Xiao, Weiwen Guo, Junfeng Liu, Mengze Li|In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed. This study focuses on the visual representation variable 'illumination', by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions. Results indicate that after undergoing various data augmentation methods, model performance has been significantly improved. Yet, a noticeable generalization gap still exists after utilizing various data augmentation methods, emphasizing the critical role of feature diversity in the training set for enhancing model generalization.||[2404.07514v1](http://arxiv.org/pdf/2404.07514v1)|null|\n", "2404.07507": "|**2024-04-11**|**Learning to Classify New Foods Incrementally Via Compressed Exemplars**|\u5b66\u4e60\u901a\u8fc7\u538b\u7f29\u793a\u4f8b\u5bf9\u65b0\u98df\u7269\u8fdb\u884c\u589e\u91cf\u5206\u7c7b|Justin Yang, Zhihao Duan, Jiangpeng He, Fengqing Zhu|Food image classification systems play a crucial role in health monitoring and diet tracking through image-based dietary assessment techniques. However, existing food recognition systems rely on static datasets characterized by a pre-defined fixed number of food classes. This contrasts drastically with the reality of food consumption, which features constantly changing data. Therefore, food image classification systems should adapt to and manage data that continuously evolves. This is where continual learning plays an important role. A challenge in continual learning is catastrophic forgetting, where ML models tend to discard old knowledge upon learning new information. While memory-replay algorithms have shown promise in mitigating this problem by storing old data as exemplars, they are hampered by the limited capacity of memory buffers, leading to an imbalance between new and previously learned data. To address this, our work explores the use of neural image compression to extend buffer size and enhance data diversity. We introduced the concept of continuously learning a neural compression model to adaptively improve the quality of compressed data and optimize the bitrates per pixel (bpp) to store more exemplars. Our extensive experiments, including evaluations on food-specific datasets including Food-101 and VFN-74, as well as the general dataset ImageNet-100, demonstrate improvements in classification accuracy. This progress is pivotal in advancing more realistic food recognition systems that are capable of adapting to continually evolving data. Moreover, the principles and methodologies we've developed hold promise for broader applications, extending their benefits to other domains of continual machine learning systems.||[2404.07507v1](http://arxiv.org/pdf/2404.07507v1)|null|\n", "2404.07495": "|**2024-04-11**|**PillarTrack: Redesigning Pillar-based Transformer Network for Single Object Tracking on Point Clouds**|PillarTrack\uff1a\u91cd\u65b0\u8bbe\u8ba1\u57fa\u4e8e Pillar \u7684\u53d8\u538b\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u70b9\u4e91\u4e0a\u7684\u5355\u4e2a\u5bf9\u8c61\u8ddf\u8e2a|Weisheng Xu, Sifan Zhou, Zhihang Yuan|LiDAR-based 3D single object tracking (3D SOT) is a critical issue in robotics and autonomous driving. It aims to obtain accurate 3D BBox from the search area based on similarity or motion. However, existing 3D SOT methods usually follow the point-based pipeline, where the sampling operation inevitably leads to redundant or lost information, resulting in unexpected performance. To address these issues, we propose PillarTrack, a pillar-based 3D single object tracking framework. Firstly, we transform sparse point clouds into dense pillars to preserve the local and global geometrics. Secondly, we introduce a Pyramid-type Encoding Pillar Feature Encoder (PE-PFE) design to help the feature representation of each pillar. Thirdly, we present an efficient Transformer-based backbone from the perspective of modality differences. Finally, we construct our PillarTrack tracker based above designs. Extensive experiments on the KITTI and nuScenes dataset demonstrate the superiority of our proposed method. Notably, our method achieves state-of-the-art performance on the KITTI and nuScenes dataset and enables real-time tracking speed. We hope our work could encourage the community to rethink existing 3D SOT tracker designs.We will open source our code to the research community in https://github.com/StiphyJay/PillarTrack.||[2404.07495v1](http://arxiv.org/pdf/2404.07495v1)|**[link](https://github.com/stiphyjay/pillartrack)**|\n", "2404.07487": "|**2024-04-11**|**Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition**|\u7ec6\u7c92\u5ea6\u8f85\u52a9\u4fe1\u606f\u5f15\u5bfc\u53cc\u63d0\u793a\u5b9e\u73b0\u96f6\u6837\u672c\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b|Yang Chen, Jingcai Guo, Tian He, Ling Wang|Skeleton-based zero-shot action recognition aims to recognize unknown human actions based on the learned priors of the known skeleton-based actions and a semantic descriptor space shared by both known and unknown categories. However, previous works focus on establishing the bridges between the known skeleton representation space and semantic descriptions space at the coarse-grained level for recognizing unknown action categories, ignoring the fine-grained alignment of these two spaces, resulting in suboptimal performance in distinguishing high-similarity action categories. To address these challenges, we propose a novel method via Side information and dual-prompts learning for skeleton-based zero-shot action recognition (STAR) at the fine-grained level. Specifically, 1) we decompose the skeleton into several parts based on its topology structure and introduce the side information concerning multi-part descriptions of human body movements for alignment between the skeleton and the semantic space at the fine-grained level; 2) we design the visual-attribute and semantic-part prompts to improve the intra-class compactness within the skeleton space and inter-class separability within the semantic space, respectively, to distinguish the high-similarity actions. Extensive experiments show that our method achieves state-of-the-art performance in ZSL and GZSL settings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.||[2404.07487v1](http://arxiv.org/pdf/2404.07487v1)|null|\n", "2404.07467": "|**2024-04-11**|**Trashbusters: Deep Learning Approach for Litter Detection and Tracking**|Trashbusters\uff1a\u7528\u4e8e\u5783\u573e\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5|Kashish Jain, Manthan Juthani, Jash Jain, Anant V. Nimkar|The illegal disposal of trash is a major public health and environmental concern. Disposing of trash in unplanned places poses serious health and environmental risks. We should try to restrict public trash cans as much as possible. This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places. Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues. To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and face recognition. The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through face recognition, and enables efficient enforcement of anti-littering policies. By reducing reliance on manual intervention, minimizing human error, and providing prompt identification, the proposed system offers significant advantages in addressing littering incidents. The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs.||[2404.07467v1](http://arxiv.org/pdf/2404.07467v1)|null|\n", "2404.07448": "|**2024-04-11**|**Transferable and Principled Efficiency for Open-Vocabulary Segmentation**|\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u53ef\u8f6c\u79fb\u548c\u539f\u5219\u6548\u7387|Jingxuan Xu, Wuyang Chen, Yao Zhao, Yunchao Wei|Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning. These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as model compression and efficient fine-tuning can address these challenges, they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different models, which comes at a cost. In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models, by utilizing smaller models that incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on https://github.com/Xujxyang/OpenTrans||[2404.07448v1](http://arxiv.org/pdf/2404.07448v1)|**[link](https://github.com/xujxyang/opentrans)**|\n", "2404.07445": "|**2024-04-11**|**Multi-view Aggregation Network for Dichotomous Image Segmentation**|\u7528\u4e8e\u4e8c\u5206\u56fe\u50cf\u5206\u5272\u7684\u591a\u89c6\u56fe\u805a\u5408\u7f51\u7edc|Qian Yu, Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu|Dichotomous Image Segmentation (DIS) has recently emerged towards high-precision object segmentation from high-resolution natural images.   When designing an effective DIS model, the main challenge is how to balance the semantic dispersion of high-resolution targets in the small receptive field and the loss of high-precision details in the large receptive field. Existing methods rely on tedious multiple encoder-decoder streams and stages to gradually complete the global localization and local refinement.   Human visual system captures regions of interest by observing them from multiple views. Inspired by it, we model DIS as a multi-view object perception problem and provide a parsimonious multi-view aggregation network (MVANet), which unifies the feature fusion of the distant view and close-up view into a single stream with one encoder-decoder structure. With the help of the proposed multi-view complementary localization and refinement modules, our approach established long-range, profound visual interactions across multiple views, allowing the features of the detailed close-up view to focus on highly slender structures.Experiments on the popular DIS-5K dataset show that our MVANet significantly outperforms state-of-the-art methods in both accuracy and speed. The source code and datasets will be publicly available at \\href{https://github.com/qianyu-dlut/MVANet}{MVANet}.||[2404.07445v1](http://arxiv.org/pdf/2404.07445v1)|**[link](https://github.com/qianyu-dlut/mvanet)**|\n", "2404.07424": "|**2024-04-11**|**CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models**|CopilotCAD\uff1a\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u63d0\u4f9b\u62a5\u544a\u5b8c\u6210\u6a21\u578b\u548c\u6765\u81ea\u533b\u5b66\u56fe\u50cf\u57fa\u7840\u6a21\u578b\u7684\u5b9a\u91cf\u8bc1\u636e|Sheng Wang, Tianming Du, Katherine Fischer, Gregory E Tasian, Justin Ziemba, Joanie M Garratt, Hersh Sagreiya, Yong Fan|Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency. However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists' expertise. This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging Large Language Models (LLMs) and medical image analysis tools. Specifically, we develop a collaborative framework to integrate LLMs and quantitative medical image analysis results generated by foundation models with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals. This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians. Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology.||[2404.07424v1](http://arxiv.org/pdf/2404.07424v1)|null|\n", "2404.07410": "|**2024-04-11**|**Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling**|\u901a\u8fc7\u5e73\u79fb\u4e0d\u53d8\u591a\u76f8\u91c7\u6837\u63d0\u9ad8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5e73\u79fb\u4e0d\u53d8\u6027|Sourajit Saha, Tejas Gokhale|Downsampling operators break the shift invariance of convolutional neural networks (CNNs) and this affects the robustness of features learned by CNNs when dealing with even small pixel-level shift. Through a large-scale correlation analysis framework, we study shift invariance of CNNs by inspecting existing downsampling operators in terms of their maximum-sampling bias (MSB), and find that MSB is negatively correlated with shift invariance. Based on this crucial insight, we propose a learnable pooling operator called Translation Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps of TIPS to reduce MSB and learn translation-invariant representations. TIPS can be integrated into any CNN and can be trained end-to-end with marginal computational overhead. Our experiments demonstrate that TIPS results in consistent performance gains in terms of accuracy, shift consistency, and shift fidelity on multiple benchmarks for image classification and semantic segmentation compared to previous methods and also leads to improvements in adversarial and distributional robustness. TIPS results in the lowest MSB compared to all previous methods, thus explaining our strong empirical results.||[2404.07410v1](http://arxiv.org/pdf/2404.07410v1)|null|\n", "2404.07405": "|**2024-04-11**|**Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing**|\u7b80\u5316\u9065\u611f\u4e2d\u8bbe\u5907\u4e0a\u63a8\u7406\u7684\u4e24\u7ea7\u63a2\u6d4b\u5668|Jaemin Kang, Hoeseok Yang, Hyungshin Kim|Deep learning has been successfully applied to object detection from remotely sensed images. Images are typically processed on the ground rather than on-board due to the computation power of the ground system. Such offloaded processing causes delays in acquiring target mission information, which hinders its application to real-time use cases. For on-device object detection, researches have been conducted on designing efficient detectors or model compression to reduce inference latency. However, highly accurate two-stage detectors still need further exploitation for acceleration. In this paper, we propose a model simplification method for two-stage object detectors. Instead of constructing a general feature pyramid, we utilize only one feature extraction in the two-stage detector. To compensate for the accuracy drop, we apply a high pass filter to the RPN's score map. Our approach is applicable to any two-stage detector using a feature pyramid network. In the experiments with state-of-the-art two-stage detectors such as ReDet, Oriented-RCNN, and LSKNet, our method reduced computation costs upto 61.2% with the accuracy loss within 2.1% on the DOTAv1.5 dataset. Source code will be released.||[2404.07405v1](http://arxiv.org/pdf/2404.07405v1)|null|\n", "2404.07395": "|**2024-04-11**|**Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation**|\u5168\u5c40\u4e0e\u5c40\u90e8\uff1a\u8bc4\u4f30\u7528\u4e8e\u70ed\u5e26\u6c14\u65cb\u5f3a\u5ea6\u4f30\u8ba1\u7684 AlexNet \u67b6\u6784|Vikas Dwivedi|Given the destructive impacts of tropical cyclones, it is critical to have a reliable system for cyclone intensity detection. Various techniques are available for this purpose, each with differing levels of accuracy. In this paper, we introduce two ensemble-based models based on AlexNet architecture to estimate tropical cyclone intensity using visible satellite images. The first model, trained on the entire dataset, is called the global AlexNet model. The second model is a distributed version of AlexNet in which multiple AlexNets are trained separately on subsets of the training data categorized according to the Saffir-Simpson wind speed scale prescribed by the meterologists. We evaluated the performance of both models against a deep learning benchmark model called \\textit{Deepti} using a publicly available cyclone image dataset. Results indicate that both the global model (with a root mean square error (RMSE) of 9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the benchmark model (with a RMSE of 13.62 knots). We provide a thorough discussion of our solution approach, including an explanantion of the AlexNet's performance using gradient class activation maps (grad-CAM). Our proposed solution strategy allows future experimentation with various deep learning models in both single and multi-channel settings.||[2404.07395v1](http://arxiv.org/pdf/2404.07395v1)|**[link](https://github.com/vikas-dwivedi-2022/cyclone_intensity_estimation_with_alexnets)**|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2404.07766": "|**2024-04-11**|**RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network**|RMAFF-PSN\uff1a\u6b8b\u4f59\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u7279\u5f81\u878d\u5408\u5149\u5ea6\u7acb\u4f53\u7f51\u7edc|Kai Luo, Yakun Ju, Lin Qi, Kaixuan Wang, Junyu Dong|Predicting accurate normal maps of objects from two-dimensional images in regions of complex structure and spatial material variations is challenging using photometric stereo methods due to the influence of surface reflection properties caused by variations in object geometry and surface materials. To address this issue, we propose a photometric stereo network called a RMAFF-PSN that uses residual multiscale attentional feature fusion to handle the ``difficult'' regions of the object. Unlike previous approaches that only use stacked convolutional layers to extract deep features from the input image, our method integrates feature information from different resolution stages and scales of the image. This approach preserves more physical information, such as texture and geometry of the object in complex regions, through shallow-deep stage feature extraction, double branching enhancement, and attention optimization. To test the network structure under real-world conditions, we propose a new real dataset called Simple PS data, which contains multiple objects with varying structures and materials. Experimental results on a publicly available benchmark dataset demonstrate that our method outperforms most existing calibrated photometric stereo methods for the same number of input images, especially in the case of highly non-convex object structures. Our method also obtains good results under sparse lighting conditions.||[2404.07766v1](http://arxiv.org/pdf/2404.07766v1)|null|\n", "2404.07729": "|**2024-04-11**|**Realistic Continual Learning Approach using Pre-trained Models**|\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u73b0\u5b9e\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5|Nadia Nasri, Carlos Guti\u00e9rrez-\u00c1lvarez, Sergio Lafuente-Arroyo, Saturnino Maldonado-Basc\u00f3n, Roberto J. L\u00f3pez-Sastre|Continual learning (CL) is crucial for evaluating adaptability in learning solutions to retain knowledge. Our research addresses the challenge of catastrophic forgetting, where models lose proficiency in previously learned tasks as they acquire new ones. While numerous solutions have been proposed, existing experimental setups often rely on idealized class-incremental learning scenarios. We introduce Realistic Continual Learning (RealCL), a novel CL paradigm where class distributions across tasks are random, departing from structured setups.   We also present CLARE (Continual Learning Approach with pRE-trained models for RealCL scenarios), a pre-trained model-based solution designed to integrate new knowledge while preserving past learning. Our contributions include pioneering RealCL as a generalization of traditional CL setups, proposing CLARE as an adaptable approach for RealCL tasks, and conducting extensive experiments demonstrating its effectiveness across various RealCL scenarios. Notably, CLARE outperforms existing models on RealCL benchmarks, highlighting its versatility and robustness in unpredictable learning environments.||[2404.07729v1](http://arxiv.org/pdf/2404.07729v1)|null|\n", "2404.07545": "|**2024-04-11**|**Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion**|\u5229\u7528\u53ef\u53d8\u5f62\u4f20\u64ad\u548c\u5b66\u4e60\u89c6\u5dee-\u6df1\u5ea6\u8f6c\u6362\u8fdb\u884c\u7acb\u4f53 LiDAR \u6df1\u5ea6\u4f30\u8ba1|Ang Li, Anning Hu, Wei Xi, Wenxian Yu, Danping Zou|Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance. Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth.||[2404.07545v1](http://arxiv.org/pdf/2404.07545v1)|null|\n"}, "LLM": {"2404.07973": "|**2024-04-11**|**Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models**|Ferret-v2\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u8003\u548c\u57fa\u7840\u7684\u6539\u8fdb\u57fa\u7ebf|Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et.al.|While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning. Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing.||[2404.07973v1](http://arxiv.org/pdf/2404.07973v1)|null|\n"}, "Transformer": {"2404.07989": "|**2024-04-11**|**Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding**|Any2Point\uff1a\u652f\u6301\u4efb\u4f55\u6a21\u6001\u5927\u578b\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684 3D \u7406\u89e3|Yiwen Tang, Jiaming Liu, Dong Wang, Zhigang Wang, Shanghang Zhang, Bin Zhao, Xuelong Li|Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors. Then, within each transformer block, we insert an any-to-3D guided adapter module for parameter-efficient fine-tuning. The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality transformers. We conduct extensive experiments to showcase the effectiveness and efficiency of our method. Code and models are released at https://github.com/Ivan-Tang-3D/Any2Point.||[2404.07989v1](http://arxiv.org/pdf/2404.07989v1)|**[link](https://github.com/ivan-tang-3d/any2point)**|\n", "2404.07932": "|**2024-04-11**|**FusionMamba: Efficient Image Fusion with State Space Model**|FusionMamba\uff1a\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9ad8\u6548\u56fe\u50cf\u878d\u5408|Siran Peng, Xiangyu Zhu, Haoyu Deng, Zhen Lei, Liang-Jian Deng|Image fusion aims to generate a high-resolution multi/hyper-spectral image by combining a high-resolution image with limited spectral information and a low-resolution image with abundant spectral data. Current deep learning (DL)-based methods for image fusion primarily rely on CNNs or Transformers to extract features and merge different types of data. While CNNs are efficient, their receptive fields are limited, restricting their capacity to capture global context. Conversely, Transformers excel at learning global information but are hindered by their quadratic complexity. Fortunately, recent advancements in the State Space Model (SSM), particularly Mamba, offer a promising solution to this issue by enabling global awareness with linear complexity. However, there have been few attempts to explore the potential of SSM in information fusion, which is a crucial ability in domains like image fusion. Therefore, we propose FusionMamba, an innovative method for efficient image fusion. Our contributions mainly focus on two aspects. Firstly, recognizing that images from different sources possess distinct properties, we incorporate Mamba blocks into two U-shaped networks, presenting a novel architecture that extracts spatial and spectral features in an efficient, independent, and hierarchical manner. Secondly, to effectively combine spatial and spectral information, we extend the Mamba block to accommodate dual inputs. This expansion leads to the creation of a new module called the FusionMamba block, which outperforms existing fusion techniques such as concatenation and cross-attention. To validate FusionMamba's effectiveness, we conduct a series of experiments on five datasets related to three image fusion tasks. The quantitative and qualitative evaluation results demonstrate that our method achieves state-of-the-art (SOTA) performance, underscoring the superiority of FusionMamba.||[2404.07932v1](http://arxiv.org/pdf/2404.07932v1)|null|\n", "2404.07794": "|**2024-04-11**|**DGMamba: Domain Generalization via Generalized State Space Model**|DGMamba\uff1a\u901a\u8fc7\u5e7f\u4e49\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u57df\u6cdb\u5316|Shaocong Long, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan|Domain generalization~(DG) aims at solving distribution shift problems in various scenes. Existing approaches are based on Convolution Neural Networks (CNNs) or Vision Transformers (ViTs), which suffer from limited receptive fields or quadratic complexities issues. Mamba, as an emerging state space model (SSM), possesses superior linear complexity and global receptive fields. Despite this, it can hardly be applied to DG to address distribution shifts, due to the hidden state issues and inappropriate scan mechanisms. In this paper, we propose a novel framework for DG, named DGMamba, that excels in strong generalizability toward unseen domains and meanwhile has the advantages of global receptive fields, and efficient linear complexity. Our DGMamba compromises two core components: Hidden State Suppressing~(HSS) and Semantic-aware Patch refining~(SPR). In particular, HSS is introduced to mitigate the influence of hidden states associated with domain-specific features during output prediction. SPR strives to encourage the model to concentrate more on objects rather than context, consisting of two designs: Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely, PFS aims to shuffle the non-semantic patches within images, creating more flexible and effective sequences from images, and DCI is designed to regularize Mamba with the combination of mismatched non-semantic and semantic information by fusing patches among domains. Extensive experiments on four commonly used DG benchmarks demonstrate that the proposed DGMamba achieves remarkably superior results to state-of-the-art models. The code will be made publicly available.||[2404.07794v1](http://arxiv.org/pdf/2404.07794v1)|**[link](https://github.com/longshaocong/dgmamba)**|\n", "2404.07713": "|**2024-04-11**|**Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning**|\u7528\u4e8e\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6e10\u8fdb\u5f0f\u8bed\u4e49\u5f15\u5bfc\u89c6\u89c9\u8f6c\u6362\u5668|Shiming Chen, Wenjin Hou, Salman Khan, Fahad Shahbaz Khan|Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer semantic knowledge from seen classes to unseen ones, supported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for representing semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we propose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly considers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specifically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual information for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.||[2404.07713v1](http://arxiv.org/pdf/2404.07713v1)|null|\n", "2404.07610": "|**2024-04-11**|**Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval**|\u4f60\u662f\u5426\u8bb0\u5f97\uff1f\u5177\u6709\u8de8\u6a21\u5f0f\u5185\u5b58\u68c0\u7d22\u7684\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55|Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim|There has been significant attention to the research on dense video captioning, which aims to automatically localize and caption all events within untrimmed video. Several studies introduce methods by designing dense video captioning as a multitasking problem of event localization and event captioning to consider inter-task relations. However, addressing both tasks using only visual input is challenging due to the lack of semantic content. In this study, we address this by proposing a novel framework inspired by the cognitive information processing of humans. Our model utilizes external memory to incorporate prior knowledge. The memory retrieval method is proposed with cross-modal video-to-text matching. To effectively incorporate retrieved text features, the versatile encoder and the decoder with visual and textual cross-attention modules are designed. Comparative experiments have been conducted to show the effectiveness of the proposed method on ActivityNet Captions and YouCook2 datasets. Experimental results show promising performance of our model without extensive pretraining from a large video dataset.||[2404.07610v1](http://arxiv.org/pdf/2404.07610v1)|**[link](https://github.com/ailab-kyunghee/cm2_dvc)**|\n", "2404.07556": "|**2024-04-11**|**Attention-Aware Laparoscopic Image Desmoking Network with Lightness Embedding and Hybrid Guided Embedding**|\u5177\u6709\u4eae\u5ea6\u5d4c\u5165\u548c\u6df7\u5408\u5f15\u5bfc\u5d4c\u5165\u7684\u6ce8\u610f\u529b\u611f\u77e5\u8179\u8154\u955c\u56fe\u50cf\u9664\u70df\u7f51\u7edc|Ziteng Liu, Jiahua Zhu, Bainan Liu, Hao Liu, Wenpeng Gao, Yili Fu|This paper presents a novel method of smoke removal from the laparoscopic images. Due to the heterogeneous nature of surgical smoke, a two-stage network is proposed to estimate the smoke distribution and reconstruct a clear, smoke-free surgical scene. The utilization of the lightness channel plays a pivotal role in providing vital information pertaining to smoke density. The reconstruction of smoke-free image is guided by a hybrid embedding, which combines the estimated smoke mask with the initial image. Experimental results demonstrate that the proposed method boasts a Peak Signal to Noise Ratio that is $2.79\\%$ higher than the state-of-the-art methods, while also exhibits a remarkable $38.2\\%$ reduction in run-time. Overall, the proposed method offers comparable or even superior performance in terms of both smoke removal quality and computational efficiency when compared to existing state-of-the-art methods. This work will be publicly available on http://homepage.hit.edu.cn/wpgao||[2404.07556v1](http://arxiv.org/pdf/2404.07556v1)|null|\n", "2404.07551": "|**2024-04-11**|**Event-Enhanced Snapshot Compressive Videography at 10K FPS**|10K FPS \u4e8b\u4ef6\u589e\u5f3a\u5feb\u7167\u538b\u7f29\u6444\u50cf|Bo Zhang, Jinli Suo, Qionghai Dai|Video snapshot compressive imaging (SCI) encodes the target dynamic scene compactly into a snapshot and reconstructs its high-speed frame sequence afterward, greatly reducing the required data footprint and transmission bandwidth as well as enabling high-speed imaging with a low frame rate intensity camera. In implementation, high-speed dynamics are encoded via temporally varying patterns, and only frames at corresponding temporal intervals can be reconstructed, while the dynamics occurring between consecutive frames are lost. To unlock the potential of conventional snapshot compressive videography, we propose a novel hybrid \"intensity+event\" imaging scheme by incorporating an event camera into a video SCI setup. Our proposed system consists of a dual-path optical setup to record the coded intensity measurement and intermediate event signals simultaneously, which is compact and photon-efficient by collecting the half photons discarded in conventional video SCI. Correspondingly, we developed a dual-branch Transformer utilizing the reciprocal relationship between two data modes to decode dense video frames. Extensive experiments on both simulated and real-captured data demonstrate our superiority to state-of-the-art video SCI and video frame interpolation (VFI) methods. Benefiting from the new hybrid design leveraging both intrinsic redundancy in videos and the unique feature of event cameras, we achieve high-quality videography at 0.1ms time intervals with a low-cost CMOS image sensor working at 24 FPS.||[2404.07551v1](http://arxiv.org/pdf/2404.07551v1)|null|\n", "2404.07537": "|**2024-04-11**|**How is Visual Attention Influenced by Text Guidance? Database and Model**|\u6587\u672c\u5f15\u5bfc\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u6ce8\u610f\u529b\uff1f\u6570\u636e\u5e93\u548c\u6a21\u578b|Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai|The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: https://github.com/IntMeGroup/TGSal.||[2404.07537v1](http://arxiv.org/pdf/2404.07537v1)|null|\n"}, "3D/CG": {"2404.07992": "|**2024-04-11**|**GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo**|GoMVS\uff1a\u591a\u89c6\u56fe\u7acb\u4f53\u7684\u51e0\u4f55\u4e00\u81f4\u6210\u672c\u805a\u5408|Jiang Wu, Rui Li, Haofei Xu, Wenxun Zhao, Yu Zhu, Jinqiu Sun, Yanning Zhang|Matching cost aggregation plays a fundamental role in learning-based multi-view stereo networks. However, directly aggregating adjacent costs can lead to suboptimal results due to local geometric inconsistency. Related methods either seek selective aggregation or improve aggregated depth in the 2D space, both are unable to handle geometric inconsistency in the cost volume effectively. In this paper, we propose GoMVS to aggregate geometrically consistent costs, yielding better utilization of adjacent geometries. More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals. We achieve this by the geometric consistent propagation (GCP) module. It computes the correspondence from the adjacent depth hypothesis space to the reference depth space using surface normals, then uses the correspondence to propagate adjacent costs to the reference geometry, followed by a convolution for aggregation. Our method achieves new state-of-the-art performance on DTU, Tanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks & Temple Advanced benchmark.||[2404.07992v1](http://arxiv.org/pdf/2404.07992v1)|**[link](https://github.com/wuuu3511/gomvs)**|\n", "2404.07984": "|**2024-04-11**|**View Selection for 3D Captioning via Diffusion Ranking**|\u901a\u8fc7\u6269\u6563\u6392\u540d\u67e5\u770b 3D \u5b57\u5e55\u7684\u9009\u62e9|Tiange Luo, Justin Johnson, Honglak Lee|Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object's characteristics. By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model.||[2404.07984v1](http://arxiv.org/pdf/2404.07984v1)|null|\n", "2404.07435": "|**2024-04-11**|**Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling**|\u7f16\u7801\u57ce\u5e02\u751f\u6001\uff1a\u901a\u8fc7\u80fd\u6e90\u5efa\u6a21\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u5efa\u7b51\u539f\u578b|Xinwei Zhuang, Zixun Huang, Wentao Zeng, Luisa Caldas|As the global population and urbanization expand, the building sector has emerged as the predominant energy consumer and carbon emission contributor. The need for innovative Urban Building Energy Modeling grows, yet existing building archetypes often fail to capture the unique attributes of local buildings and the nuanced distinctions between different cities, jeopardizing the precision of energy modeling. This paper presents an alternative tool employing self-supervised learning to distill complex geometric data into representative, locale-specific archetypes. This study attempts to foster a new paradigm of interaction with built environments, incorporating local parameters to conduct bespoke energy simulations at the community level. The catered archetypes can augment the precision and applicability of energy consumption modeling at different scales across diverse building inventories. This tool provides a potential solution that encourages the exploration of emerging local ecologies. By integrating building envelope characteristics and cultural granularity into the building archetype generation process, we seek a future where architecture and urban design are intricately interwoven with the energy sector in shaping our built environments.||[2404.07435v1](http://arxiv.org/pdf/2404.07435v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2404.07520": "|**2024-04-11**|**PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination**|PromptSync\uff1a\u901a\u8fc7\u7c7b\u611f\u77e5\u539f\u578b\u5bf9\u9f50\u548c\u533a\u5206\u6765\u5f25\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9886\u57df\u5dee\u8ddd|Anant Khandelwal|The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning. The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 2.33\\% in overall performance, by 1\\% in base-to-novel generalization, and by 2.84\\% in cross-dataset transfer tasks.||[2404.07520v1](http://arxiv.org/pdf/2404.07520v1)|null|\n"}, "\u5176\u4ed6": {"2404.07988": "|**2024-04-11**|**QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer**|QuasiSim\uff1a\u7528\u4e8e\u7075\u5de7\u64cd\u4f5c\u4f20\u8f93\u7684\u53c2\u6570\u5316\u51c6\u7269\u7406\u6a21\u62df\u5668|Xueyi Liu, Kangbo Lyu, Jieqiong Zhang, Tao Du, Li Yi|We explore the dexterous manipulation transfer problem by designing simulators. The task wishes to transfer human manipulations to dexterous robot hand simulations and is inherently difficult due to its intricate, highly-constrained, and discontinuous dynamics and the need to control a dexterous hand with a DoF to accurately replicate human manipulations. Previous approaches that optimize in high-fidelity black-box simulators or a modified one with relaxed constraints only demonstrate limited capabilities or are restricted by insufficient simulation fidelity. We introduce parameterized quasi-physical simulators and a physics curriculum to overcome these limitations. The key ideas are 1) balancing between fidelity and optimizability of the simulation via a curriculum of parameterized simulators, and 2) solving the problem in each of the simulators from the curriculum, with properties ranging from high task optimizability to high fidelity. We successfully enable a dexterous hand to track complex and diverse manipulations in high-fidelity simulated environments, boosting the success rate by 11\\%+ from the best-performed baseline. The project website is available at https://meowuu7.github.io/QuasiSim/.||[2404.07988v1](http://arxiv.org/pdf/2404.07988v1)|**[link](https://github.com/meowuu7/quasisim)**|\n", "2404.07985": "|**2024-04-11**|**WaveMo: Learning Wavefront Modulations to See Through Scattering**|WaveMo\uff1a\u5b66\u4e60\u6ce2\u524d\u8c03\u5236\u4ee5\u900f\u89c6\u6563\u5c04|Mingyang Xie, Haiyun Guo, Brandon Y. Feng, Lingbo Jin, Ashok Veeraraghavan, Christopher A. Metzler|Imaging through scattering media is a fundamental and pervasive challenge in fields ranging from medical diagnostics to astronomy. A promising strategy to overcome this challenge is wavefront modulation, which induces measurement diversity during image acquisition. Despite its importance, designing optimal wavefront modulations to image through scattering remains under-explored. This paper introduces a novel learning-based framework to address the gap. Our approach jointly optimizes wavefront modulations and a computationally lightweight feedforward \"proxy\" reconstruction network. This network is trained to recover scenes obscured by scattering, using measurements that are modified by these modulations. The learned modulations produced by our framework generalize effectively to unseen scattering scenarios and exhibit remarkable versatility. During deployment, the learned modulations can be decoupled from the proxy network to augment other more computationally expensive restoration algorithms. Through extensive experiments, we demonstrate our approach significantly advances the state of the art in imaging through scattering media. Our project webpage is at https://wavemo-2024.github.io/.||[2404.07985v1](http://arxiv.org/pdf/2404.07985v1)|null|\n", "2404.07930": "|**2024-04-11**|**Parameter Hierarchical Optimization for Visible-Infrared Person Re-Identification**|\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u53c2\u6570\u5206\u5c42\u4f18\u5316|Zeng YU, Yunxiao Shi|Visible-infrared person re-identification (VI-reID) aims at matching cross-modality pedestrian images captured by disjoint visible or infrared cameras. Existing methods alleviate the cross-modality discrepancies via designing different kinds of network architectures. Different from available methods, in this paper, we propose a novel parameter optimizing paradigm, parameter hierarchical optimization (PHO) method, for the task of VI-ReID. It allows part of parameters to be directly optimized without any training, which narrows the search space of parameters and makes the whole network more easier to be trained. Specifically, we first divide the parameters into different types, and then introduce a self-adaptive alignment strategy (SAS) to automatically align the visible and infrared images through transformation. Considering that features in different dimension have varying importance, we develop an auto-weighted alignment learning (AAL) module that can automatically weight features according to their importance. Importantly, in the alignment process of SAS and AAL, all the parameters are immediately optimized with optimization principles rather than training the whole network, which yields a better parameter training manner. Furthermore, we establish the cross-modality consistent learning (CCL) loss to extract discriminative person representations with translation consistency. We provide both theoretical justification and empirical evidence that our proposed PHO method outperform existing VI-reID approaches.||[2404.07930v1](http://arxiv.org/pdf/2404.07930v1)|null|\n", "2404.07855": "|**2024-04-11**|**Resolve Domain Conflicts for Generalizable Remote Physiological Measurement**|\u89e3\u51b3\u901a\u7528\u8fdc\u7a0b\u751f\u7406\u6d4b\u91cf\u7684\u9886\u57df\u51b2\u7a81|Weiyu Sun, Xinyu Zhang, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen|Remote photoplethysmography (rPPG) technology has become increasingly popular due to its non-invasive monitoring of various physiological indicators, making it widely applicable in multimedia interaction, healthcare, and emotion analysis. Existing rPPG methods utilize multiple datasets for training to enhance the generalizability of models. However, they often overlook the underlying conflict issues across different datasets, such as (1) label conflict resulting from different phase delays between physiological signal labels and face videos at the instance level, and (2) attribute conflict stemming from distribution shifts caused by head movements, illumination changes, skin types, etc. To address this, we introduce the DOmain-HArmonious framework (DOHA). Specifically, we first propose a harmonious phase strategy to eliminate uncertain phase delays and preserve the temporal variation of physiological signals. Next, we design a harmonious hyperplane optimization that reduces irrelevant attribute shifts and encourages the model's optimization towards a global solution that fits more valid scenarios. Our experiments demonstrate that DOHA significantly improves the performance of existing methods under multiple protocols. Our code is available at https://github.com/SWY666/rPPG-DOHA.||[2404.07855v1](http://arxiv.org/pdf/2404.07855v1)|**[link](https://github.com/swy666/rppg-doha)**|\n", "2404.07850": "|**2024-04-11**|**MindBridge: A Cross-Subject Brain Decoding Framework**|MindBridge\uff1a\u8de8\u5b66\u79d1\u5927\u8111\u89e3\u7801\u6846\u67b6|Shizun Wang, Songhua Liu, Zhenxiong Tan, Xinchao Wang|Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning. Notably, by cycle reconstruction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation. Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Furthermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios. Project page: https://littlepure2333.github.io/MindBridge||[2404.07850v1](http://arxiv.org/pdf/2404.07850v1)|**[link](https://github.com/littlepure2333/mindbridge)**|\n", "2404.07847": "|**2024-04-11**|**Fuss-Free Network: A Simplified and Efficient Neural Network for Crowd Counting**|\u65e0\u5fe7\u65e0\u8651\u7684\u7f51\u7edc\uff1a\u7528\u4e8e\u4eba\u7fa4\u8ba1\u6570\u7684\u7b80\u5316\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc|Lei Chen, Xingen Gao|In the field of crowd-counting research, many recent deep learning based methods have demonstrated robust capabilities for accurately estimating crowd sizes. However, the enhancement in their performance often arises from an increase in the complexity of the model structure. This paper introduces the Fuss-Free Network (FFNet), a crowd counting deep learning model that is characterized by its simplicity and efficiency in terms of its structure. The model comprises only a backbone of a neural network and a multi-scale feature fusion structure.The multi-scale feature fusion structure is a simple architecture consisting of three branches, each only equipped with a focus transition module, and combines the features from these branches through the concatenation operation.Our proposed crowd counting model is trained and evaluated on four widely used public datasets, and it achieves accuracy that is comparable to that of existing complex models.The experimental results further indicate that excellent performance in crowd counting tasks can also be achieved by utilizing a simple, low-parameter, and computationally efficient neural network structure.||[2404.07847v1](http://arxiv.org/pdf/2404.07847v1)|null|\n", "2404.07754": "|**2024-04-11**|**Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification**|\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u5408\u6210\u536b\u661f\u56fe\u50cf\u2014\u2014\u76d1\u6d4b\u548c\u9a8c\u8bc1\u7684\u6280\u672f\u6311\u6218\u548c\u5f71\u54cd|Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann|Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.||[2404.07754v1](http://arxiv.org/pdf/2404.07754v1)|null|\n", "2404.07698": "|**2024-04-11**|**Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator**|\u4f7f\u7528\u8d28\u91cf\u6761\u4ef6\u6f5c\u5728\u6982\u7387\u4f30\u8ba1\u5668\u8fdb\u884c\u70b9\u4e91\u51e0\u4f55\u53ef\u6269\u5c55\u7f16\u7801|Daniele Mari, Andr\u00e9 F. R. Guarda, Nuno M. M. Rodrigues, Simone Milani, Fernando Pereira|The widespread usage of point clouds (PC) for immersive visual applications has resulted in the use of very heterogeneous receiving conditions and devices, notably in terms of network, hardware, and display capabilities. In this scenario, quality scalability, i.e., the ability to reconstruct a signal at different qualities by progressively decoding a single bitstream, is a major requirement that has yet to be conveniently addressed, notably in most learning-based PC coding solutions. This paper proposes a quality scalability scheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based static point cloud geometry codecs, which uses a Quality-conditioned Latents Probability Estimator (QuLPE) to decode a high-quality version of a PC learning-based representation, based on an available lower quality base layer. SQH is integrated in the future JPEG PC coding standard, allowing to create a layered bitstream that can be used to progressively decode the PC geometry with increasing quality and fidelity. Experimental results show that SQH offers the quality scalability feature with very limited or no compression performance penalty at all when compared with the corresponding non-scalable solution, thus preserving the significant compression gains over other state-of-the-art PC codecs.||[2404.07698v1](http://arxiv.org/pdf/2404.07698v1)|null|\n", "2404.07676": "|**2024-04-11**|**Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis**|\u57fa\u4e8e\u6a21\u578b\u7684 QUILT-1M \u75c5\u7406\u6570\u636e\u96c6\u6e05\u7406\uff0c\u7528\u4e8e\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u5408\u6210|Marc Aubreville, Jonathan Ganz, Jonas Ammeling, Christopher C. Kaltenecker, Christof A. Bertram|The QUILT-1M dataset is the first openly available dataset containing images harvested from various online sources. While it provides a huge data variety, the image quality and composition is highly heterogeneous, impacting its utility for text-conditional image synthesis. We propose an automatic pipeline that provides predictions of the most common impurities within the images, e.g., visibility of narrators, desktop environment and pathology software, or text within the image. Additionally, we propose to use semantic alignment filtering of the image-text pairs. Our findings demonstrate that by rigorously filtering the dataset, there is a substantial enhancement of image fidelity in text-to-image tasks.||[2404.07676v1](http://arxiv.org/pdf/2404.07676v1)|null|\n", "2404.07543": "|**2024-04-11**|**Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening**|\u7528\u4e8e\u9065\u611f\u5168\u8272\u9510\u5316\u7684\u5185\u5bb9\u81ea\u9002\u5e94\u975e\u5c40\u90e8\u5377\u79ef|Yule Duan, Xiao Wu, Haoyu Deng, Liang-Jian Deng|Currently, machine learning-based methods for remote sensing pansharpening have progressed rapidly. However, existing pansharpening methods often do not fully exploit differentiating regional information in non-local spaces, thereby limiting the effectiveness of the methods and resulting in redundant learning parameters. In this paper, we introduce a so-called content-adaptive non-local convolution (CANConv), a novel method tailored for remote sensing image pansharpening. Specifically, CANConv employs adaptive convolution, ensuring spatial adaptability, and incorporates non-local self-similarity through the similarity relationship partition (SRP) and the partition-wise adaptive convolution (PWAC) sub-modules. Furthermore, we also propose a corresponding network architecture, called CANNet, which mainly utilizes the multi-scale self-similarity. Extensive experiments demonstrate the superior performance of CANConv, compared with recent promising fusion methods. Besides, we substantiate the method's effectiveness through visualization, ablation experiments, and comparison with existing methods on multiple test sets. The source code is publicly available at https://github.com/duanyll/CANConv.||[2404.07543v1](http://arxiv.org/pdf/2404.07543v1)|**[link](https://github.com/duanyll/canconv)**|\n", "2404.07504": "|**2024-04-11**|**Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange**|\u51cf\u8f7b\u5bf9\u8c61\u4f9d\u8d56\u6027\uff1a\u901a\u8fc7\u5bf9\u8c61\u4ea4\u6362\u6539\u8fdb\u70b9\u4e91\u81ea\u76d1\u7763\u5b66\u4e60|Yanhao Wu, Tong Zhang, Wei Ke, Congpei Qiu, Sabine Susstrunk, Mathieu Salzmann|In the realm of point cloud scene understanding, particularly in indoor scenes, objects are arranged following human habits, resulting in objects of certain semantics being closely positioned and displaying notable inter-object correlations. This can create a tendency for neural networks to exploit these strong dependencies, bypassing the individual object patterns. To address this challenge, we introduce a novel self-supervised learning (SSL) strategy. Our approach leverages both object patterns and contextual cues to produce robust features. It begins with the formulation of an object-exchanging strategy, where pairs of objects with comparable sizes are exchanged across different scenes, effectively disentangling the strong contextual dependencies. Subsequently, we introduce a context-aware feature learning strategy, which encodes object patterns without relying on their specific context by aggregating object features across various scenes. Our extensive experiments demonstrate the superiority of our method over existing SSL techniques, further showing its better robustness to environmental changes. Moreover, we showcase the applicability of our approach by transferring pre-trained models to diverse point cloud datasets.||[2404.07504v1](http://arxiv.org/pdf/2404.07504v1)|null|\n"}}