{"\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.04647": "|**2024-01-09**|**Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks**|\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u63a8\u8fdb\u4e8b\u524d\u53ef\u89e3\u91ca\u6a21\u578b|Tanmay Garg, Deepika Vemuri, Vineeth N Balasubramanian|This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this work presents a significant step towards building inherently interpretable deep vision models with task-aligned concept representations - a key enabler for developing trustworthy AI for real-world perception tasks.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u5ff5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u65e0\u76d1\u7763\u89e3\u91ca\u751f\u6210\u5668\u9644\u52a0\u5230\u4e3b\u5206\u7c7b\u5668\u7f51\u7edc\uff0c\u5e76\u5229\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u89e3\u91ca\u6a21\u5757\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u4ee5\u4ece\u5206\u7c7b\u5668\u7684\u6f5c\u5728\u8868\u793a\u4e2d\u63d0\u53d6\u89c6\u89c9\u6982\u5ff5\uff0c\u800c\u57fa\u4e8e GAN \u7684\u6a21\u5757\u65e8\u5728\u533a\u5206\u4ece\u6982\u5ff5\u751f\u6210\u7684\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u3002\u8fd9\u79cd\u8054\u5408\u8bad\u7ec3\u65b9\u6848\u4f7f\u6a21\u578b\u80fd\u591f\u9690\u5f0f\u5730\u5c06\u5176\u5185\u90e8\u5b66\u4e60\u7684\u6982\u5ff5\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u5c5e\u6027\u5bf9\u9f50\u3002\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u540c\u65f6\u4ea7\u751f\u4e86\u8fde\u8d2f\u7684\u6982\u5ff5\u6fc0\u6d3b\u3002\u6211\u4eec\u5206\u6790\u5b66\u5230\u7684\u6982\u5ff5\uff0c\u663e\u793a\u5b83\u4eec\u4e0e\u7269\u4f53\u90e8\u5206\u548c\u89c6\u89c9\u5c5e\u6027\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u5bf9\u6297\u6027\u8bad\u7ec3\u534f\u8bae\u4e2d\u7684\u6270\u52a8\u5982\u4f55\u5f71\u54cd\u5206\u7c7b\u548c\u6982\u5ff5\u83b7\u53d6\u3002\u603b\u4e4b\uff0c\u8fd9\u9879\u5de5\u4f5c\u5728\u6784\u5efa\u5177\u6709\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6982\u5ff5\u8868\u793a\u7684\u672c\u8d28\u4e0a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u89c6\u89c9\u6a21\u578b\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u8fd9\u662f\u4e3a\u73b0\u5b9e\u4e16\u754c\u611f\u77e5\u4efb\u52a1\u5f00\u53d1\u503c\u5f97\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\u3002|[2401.04647v1](http://arxiv.org/pdf/2401.04647v1)|null|\n", "2401.04578": "|**2024-01-09**|**Effective pruning of web-scale datasets based on complexity of concept clusters**|\u57fa\u4e8e\u6982\u5ff5\u7c07\u590d\u6742\u5ea6\u7684\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u96c6\u7684\u6709\u6548\u526a\u679d|Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, Ari S. Morcos|Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.|\u5229\u7528\u5927\u89c4\u6a21\u7f51\u7edc\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u83b7\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u4e5f\u5bf9\u5176\u8bad\u7ec3\u63d0\u51fa\u4e86\u5f02\u5e38\u7684\u8ba1\u7b97\u8981\u6c42\u3002\u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u548c\u6570\u636e\u6548\u7387\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u7a81\u7834\u4e86\u4fee\u526a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3 CLIP \u5f0f\u6a21\u578b\u7684\u6781\u9650\u3002\u5f53\u4eca ImageNet \u4e0a\u6700\u6709\u6548\u7684\u4fee\u526a\u65b9\u6cd5\u6839\u636e\u6570\u636e\u6837\u672c\u7684\u5d4c\u5165\u5c06\u6570\u636e\u6837\u672c\u805a\u7c7b\u6210\u5355\u72ec\u7684\u6982\u5ff5\uff0c\u5e76\u4fee\u526a\u6389\u6700\u5178\u578b\u7684\u6837\u672c\u3002\u6211\u4eec\u5c06\u6b64\u65b9\u6cd5\u6269\u5c55\u5230 LAION \u5e76\u901a\u8fc7\u6ce8\u610f\u5230\u526a\u679d\u7387\u5e94\u8be5\u9488\u5bf9\u7279\u5b9a\u6982\u5ff5\u5e76\u9002\u5e94\u6982\u5ff5\u7684\u590d\u6742\u6027\u6765\u6539\u8fdb\u5b83\u3002\u4f7f\u7528\u7b80\u5355\u76f4\u89c2\u7684\u590d\u6742\u6027\u8861\u91cf\u6807\u51c6\uff0c\u6211\u4eec\u80fd\u591f\u5c06\u57f9\u8bad\u6210\u672c\u964d\u4f4e\u5230\u5e38\u89c4\u57f9\u8bad\u7684\u56db\u5206\u4e4b\u4e00\u3002\u901a\u8fc7\u4ece LAION \u6570\u636e\u96c6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u8f83\u5c0f\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u5e26\u6765\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u7740\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u80fd\u591f\u5728 ImageNet \u96f6\u6837\u672c\u7cbe\u5ea6\u4e0a\u8d85\u8d8a LAION \u8bad\u7ec3\u7684 OpenCLIP-ViT-B32 \u6a21\u578b 1.1p.p\u3002\u800c\u53ea\u4f7f\u7528\u4e86 27.7% \u7684\u6570\u636e\u548c\u8bad\u7ec3\u8ba1\u7b97\u3002\u5c3d\u7ba1\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u4f46\u6211\u4eec\u4e5f\u770b\u5230\u4e86 ImageNet dist \u7684\u6539\u8fdb\u3002\u8f6e\u73ed\u3001\u68c0\u7d22\u4efb\u52a1\u548c VTAB\u3002\u5728 DataComp Medium \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u5728 38 \u9879\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 ImageNet \u96f6\u6837\u672c\u7cbe\u5ea6\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u5e73\u5747\u96f6\u6837\u672c\u7cbe\u5ea6\u3002|[2401.04578v1](http://arxiv.org/pdf/2401.04578v1)|null|\n", "2401.04357": "|**2024-01-09**|**Iterative Feedback Network for Unsupervised Point Cloud Registration**|\u7528\u4e8e\u65e0\u76d1\u7763\u70b9\u4e91\u914d\u51c6\u7684\u8fed\u4ee3\u53cd\u9988\u7f51\u7edc|Yifan Xie, Boyu Wang, Shiqi Li, Jihua Zhu|As a fundamental problem in computer vision, point cloud registration aims to seek the optimal transformation for aligning a pair of point clouds. In most existing methods, the information flows are usually forward transferring, thus lacking the guidance from high-level information to low-level information. Besides, excessive high-level information may be overly redundant, and directly using it may conflict with the original low-level information. In this paper, we propose a novel Iterative Feedback Network (IFNet) for unsupervised point cloud registration, in which the representation of low-level features is efficiently enriched by rerouting subsequent high-level features. Specifically, our IFNet is built upon a series of Feedback Registration Block (FRB) modules, with each module responsible for generating the feedforward rigid transformation and feedback high-level features. These FRB modules are cascaded and recurrently unfolded over time. Further, the Feedback Transformer is designed to efficiently select relevant information from feedback high-level features, which is utilized to refine the low-level features. What's more, we incorporate a geometry-awareness descriptor to empower the network for making full use of most geometric information, which leads to more precise registration results. Extensive experiments on various benchmark datasets demonstrate the superior registration performance of our IFNet.|\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u70b9\u4e91\u914d\u51c6\u65e8\u5728\u5bfb\u6c42\u5bf9\u9f50\u4e00\u5bf9\u70b9\u4e91\u7684\u6700\u4f73\u53d8\u6362\u3002\u73b0\u6709\u7684\u5927\u591a\u6570\u65b9\u6cd5\u4e2d\uff0c\u4fe1\u606f\u6d41\u901a\u5e38\u662f\u524d\u5411\u4f20\u9012\u7684\uff0c\u7f3a\u4e4f\u4ece\u9ad8\u5c42\u4fe1\u606f\u5230\u4f4e\u5c42\u4fe1\u606f\u7684\u5f15\u5bfc\u3002\u6b64\u5916\uff0c\u8fc7\u591a\u7684\u9ad8\u5c42\u4fe1\u606f\u53ef\u80fd\u4f1a\u8fc7\u4e8e\u5197\u4f59\uff0c\u76f4\u63a5\u4f7f\u7528\u53ef\u80fd\u4f1a\u4e0e\u539f\u6709\u7684\u4f4e\u5c42\u4fe1\u606f\u53d1\u751f\u51b2\u7a81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u70b9\u4e91\u914d\u51c6\u7684\u65b0\u578b\u8fed\u4ee3\u53cd\u9988\u7f51\u7edc\uff08IFNet\uff09\uff0c\u5176\u4e2d\u901a\u8fc7\u91cd\u65b0\u8def\u7531\u540e\u7eed\u9ad8\u7ea7\u7279\u5f81\u6765\u6709\u6548\u5730\u4e30\u5bcc\u4f4e\u7ea7\u7279\u5f81\u7684\u8868\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684 IFNet \u5efa\u7acb\u5728\u4e00\u7cfb\u5217\u53cd\u9988\u6ce8\u518c\u5757\uff08FRB\uff09\u6a21\u5757\u7684\u57fa\u7840\u4e0a\uff0c\u6bcf\u4e2a\u6a21\u5757\u8d1f\u8d23\u751f\u6210\u524d\u9988\u521a\u6027\u53d8\u6362\u548c\u53cd\u9988\u9ad8\u7ea7\u7279\u5f81\u3002\u8fd9\u4e9b FRB \u6a21\u5757\u662f\u7ea7\u8054\u7684\uff0c\u5e76\u4e14\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u4e0d\u65ad\u5c55\u5f00\u3002\u6b64\u5916\uff0c\u53cd\u9988\u53d8\u538b\u5668\u88ab\u8bbe\u8ba1\u4e3a\u4ece\u53cd\u9988\u7684\u9ad8\u7ea7\u7279\u5f81\u4e2d\u6709\u6548\u5730\u9009\u62e9\u76f8\u5173\u4fe1\u606f\uff0c\u7528\u4e8e\u7ec6\u5316\u4f4e\u7ea7\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u51e0\u4f55\u611f\u77e5\u63cf\u8ff0\u7b26\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u5145\u5206\u5229\u7528\u5927\u591a\u6570\u51e0\u4f55\u4fe1\u606f\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u914d\u51c6\u7ed3\u679c\u3002\u5bf9\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 IFNet \u7684\u5353\u8d8a\u914d\u51c6\u6027\u80fd\u3002|[2401.04357v1](http://arxiv.org/pdf/2401.04357v1)|null|\n", "2401.04350": "|**2024-01-09**|**Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness**|\u9884\u8bad\u7ec3\u6a21\u578b\u5f15\u5bfc\u7684\u96f6\u6837\u672c\u5bf9\u6297\u9c81\u68d2\u6027\u5fae\u8c03|Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan|Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks, and exhibit remarkable zero-shot generalization capability, while they are also vulnerable to imperceptible adversarial examples. Existing works typically employ adversarial training (fine-tuning) as a defense method against adversarial examples. However, direct application to the CLIP model may result in overfitting, compromising the model's capacity for generalization. In this paper, we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch, to enhance the model's zero-shot adversarial robustness. Specifically, PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model, aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outperforms the state-of-the-art method, improving the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach consistently improves clean accuracy by an average of 8.72%.|\u50cf CLIP \u8fd9\u6837\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e5f\u5bb9\u6613\u53d7\u5230\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6027\u4f8b\u5b50\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7684\u4f5c\u54c1\u901a\u5e38\u91c7\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\uff08\u5fae\u8c03\uff09\u4f5c\u4e3a\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e CLIP \u6a21\u578b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\uff0c\u4ece\u800c\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5f15\u5bfc\u5bf9\u6297\u6027\u5fae\u8c03\uff08PMG-AFT\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ed4\u7ec6\u8bbe\u8ba1\u8f85\u52a9\u5206\u652f\u6765\u5229\u7528\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u76d1\u7763\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u96f6\u6837\u672c\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cPMG-AFT \u6700\u5c0f\u5316\u4e86\u76ee\u6807\u6a21\u578b\u4e2d\u7684\u5bf9\u6297\u6837\u672c\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u5bf9\u6297\u6837\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u65e8\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u7ecf\u6355\u83b7\u7684\u6cdb\u5316\u7279\u5f81\u3002\u5bf9 15 \u4e2a\u96f6\u6837\u672c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPMG-AFT \u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c06 top-1 \u7a33\u5065\u7cbe\u5ea6\u5e73\u5747\u63d0\u9ad8\u4e86 4.99%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6301\u7eed\u5c06\u6e05\u6d01\u51c6\u786e\u5ea6\u5e73\u5747\u63d0\u9ad8\u4e86 8.72%\u3002|[2401.04350v1](http://arxiv.org/pdf/2401.04350v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.04722": "|**2024-01-09**|**U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation**|U-Mamba\uff1a\u589e\u5f3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8fdc\u7a0b\u4f9d\u8d56\u6027|Jun Ma, Feifei Li, Bo Wang|Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at https://wanglab.ai/u-mamba.html.|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c Transformer \u4e00\u76f4\u662f\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6700\u6d41\u884c\u7684\u67b6\u6784\uff0c\u4f46\u7531\u4e8e\u56fa\u6709\u7684\u5c40\u90e8\u6027\u6216\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5b83\u4eec\u5904\u7406\u8fdc\u7a0b\u4f9d\u8d56\u6027\u7684\u80fd\u529b\u6709\u9650\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 U-Mamba\uff0c\u4e00\u79cd\u7528\u4e8e\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u901a\u7528\u7f51\u7edc\u3002\u53d7\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\u6a21\u578b\uff08SSM\uff09\u8fd9\u4e00\u65b0\u7684\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\u5bb6\u65cf\u7684\u542f\u53d1\uff0c\u8be5\u6a21\u578b\u4ee5\u5176\u5904\u7406\u957f\u5e8f\u5217\u7684\u5f3a\u5927\u80fd\u529b\u800c\u95fb\u540d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6df7\u5408 CNN-SSM \u6a21\u5757\uff0c\u5b83\u5c06\u5377\u79ef\u5c42\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0e\u4ee5\u4e0b\u80fd\u529b\u96c6\u6210\u5728\u4e00\u8d77\uff1a\u7528\u4e8e\u6355\u83b7\u8fdc\u7a0b\u4f9d\u8d56\u6027\u7684 SSM\u3002\u6b64\u5916\uff0cU-Mamba \u5177\u6709\u81ea\u6211\u914d\u7f6e\u673a\u5236\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u81ea\u52a8\u9002\u5e94\u5404\u79cd\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9\u56db\u79cd\u4e0d\u540c\u7684\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec CT \u548c MR \u56fe\u50cf\u4e2d\u7684 3D \u8179\u90e8\u5668\u5b98\u5206\u5272\u3001\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u7684\u5668\u68b0\u5206\u5272\u4ee5\u53ca\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u5206\u5272\u3002\u7ed3\u679c\u8868\u660e\uff0cU-Mamba \u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e CNN \u548c Transformer \u7684\u5206\u5272\u7f51\u7edc\u3002\u8fd9\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u9ad8\u6548\u7684\u8fdc\u7a0b\u4f9d\u8d56\u6027\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u53ef\u5728 https://wanglab.ai/u-mamba.html \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.04722v1](http://arxiv.org/pdf/2401.04722v1)|null|\n", "2401.04720": "|**2024-01-09**|**Low-resource finetuning of foundation models beats state-of-the-art in histopathology**|\u57fa\u7840\u6a21\u578b\u7684\u4f4e\u8d44\u6e90\u5fae\u8c03\u51fb\u8d25\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u9886\u57df\u7684\u6700\u5148\u8fdb\u6280\u672f|Benedikt Roth, Valentin Koch, Sophia J. Wagner, Julia A. Schnabel, Carsten Marr, Tingying Peng|To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.|\u4e3a\u4e86\u5904\u7406\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5927\u89c4\u6a21\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u9996\u5148\u5c06\u56fe\u50cf\u7ec6\u5206\u4e3a\u8f83\u5c0f\u7684\u5757\uff0c\u4ece\u8fd9\u4e9b\u5757\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u6700\u540e\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u805a\u5408\u7279\u5f81\u5411\u91cf\u3002\u8be5\u5de5\u4f5c\u6d41\u7a0b\u7684\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u63d0\u53d6\u7279\u5f81\u7684\u8d28\u91cf\u3002\u6700\u8fd1\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u7840\u6a21\u578b\u8868\u660e\uff0c\u901a\u8fc7\u76d1\u7763\u6216\u81ea\u76d1\u7763\u5b66\u4e60\u5229\u7528\u5927\u91cf\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u5404\u79cd\u4efb\u52a1\u7684\u7279\u5f81\u8d28\u91cf\u548c\u901a\u7528\u6027\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5c06\u6700\u6d41\u884c\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u7684\u7279\u5f81\u63d0\u53d6\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6a21\u578b\uff1a\u5e7b\u706f\u7247\u7ea7\u5206\u7c7b\u548c\u8865\u4e01\u7ea7\u5206\u7c7b\u3002\u6211\u4eec\u8bc1\u660e\u57fa\u7840\u6a21\u578b\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6839\u636e\u6570\u636e\u96c6\u5728\u5355\u4e2a GPU \u4e0a\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u4ec5\u4e24\u5c0f\u65f6\u6216\u4e09\u5929\uff0c\u6211\u4eec\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u8ba1\u7b97\u75c5\u7406\u5b66\u7279\u5f81\u63d0\u53d6\u5668\u3002\u8fd9\u4e9b\u53d1\u73b0\u610f\u5473\u7740\uff0c\u5373\u4f7f\u8d44\u6e90\u5f88\u5c11\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5fae\u8c03\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u548c\u6570\u636e\u96c6\u5b9a\u5236\u7684\u7279\u5f81\u63d0\u53d6\u5668\u3002\u4e0e\u76ee\u524d\u7684\u72b6\u6001\u76f8\u6bd4\uff0c\u8fd9\u662f\u4e00\u4e2a\u76f8\u5f53\u5927\u7684\u8f6c\u53d8\uff0c\u76ee\u524d\u53ea\u6709\u5c11\u6570\u62e5\u6709\u5927\u91cf\u8d44\u6e90\u548c\u6570\u636e\u96c6\u7684\u673a\u6784\u80fd\u591f\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u6240\u6709\u4ee3\u7801\u4ee5\u53ca\u5fae\u8c03\u540e\u7684\u6a21\u578b\u3002|[2401.04720v1](http://arxiv.org/pdf/2401.04720v1)|null|\n", "2401.04666": "|**2024-01-09**|**Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset**|ASSIRA\u732b\u72d7\u6570\u636e\u96c6\u4e0a\u5404\u79cd\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u51c6\u5206\u6790|Galib Muhammad Shahriar Himel, Md. Masudul Islam|As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.|\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6700\u57fa\u672c\u7684\u5e94\u7528\u548c\u5b9e\u73b0\uff0c\u56fe\u50cf\u5206\u7c7b\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u8457\u540d\u6570\u636e\u79d1\u5b66\u793e\u533a\u63d0\u4f9b\u5404\u79cd\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5bf9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002 ASSIRA Cats & Dogs \u6570\u636e\u96c6\u5c31\u662f\u5176\u4e2d\u4e4b\u4e00\uff0c\u5728\u672c\u7814\u7a76\u4e2d\u7528\u4e8e\u5176\u6574\u4f53\u63a5\u53d7\u5ea6\u548c\u57fa\u51c6\u6807\u51c6\u3002\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\u6765\u6f14\u793a\u5404\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6bd4\u8f83\u3002\u66f4\u6539\u8d85\u53c2\u6570\u4ee5\u83b7\u5f97\u6a21\u578b\u7684\u6700\u4f73\u7ed3\u679c\u3002\u901a\u8fc7\u5e94\u7528\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u53d1\u751f\u91cd\u5927\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002\u4e3a\u4e86\u8fd0\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u8ba1\u7b97\u673a\u67b6\u6784\uff1a\u4e00\u53f0\u914d\u5907 NVIDIA GeForce GTX 1070 \u7684\u7b14\u8bb0\u672c\u7535\u8111\u3001\u4e00\u53f0\u914d\u5907 NVIDIA GeForce RTX 3080Ti \u7684\u7b14\u8bb0\u672c\u7535\u8111\u548c\u4e00\u53f0\u914d\u5907 NVIDIA GeForce RTX 3090 \u7684\u53f0\u5f0f\u673a\u3002\u6240\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u8ba1\u7b97\u673a\u3002\u4e4b\u524d\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5b8c\u6210\u7684\u5b9e\u9a8c\u3002\u4ece\u8fd9\u4e2a\u5b9e\u9a8c\u6765\u770b\uff0c\u4f7f\u7528 NASNet Large \u83b7\u5f97\u4e86 99.65% \u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002|[2401.04666v1](http://arxiv.org/pdf/2401.04666v1)|null|\n", "2401.04651": "|**2024-01-09**|**Learning to Prompt Segment Anything Models**|\u5b66\u4e60\u63d0\u793a\u5206\u5272\u4efb\u4f55\u6a21\u578b|Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric Xing|Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.|SEEM \u548c SAM \u7b49\u5206\u5272\u4efb\u4f55\u6a21\u578b (SAM) \u5728\u5b66\u4e60\u5206\u5272\u4efb\u4f55\u4e1c\u897f\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002 SAM \u7684\u6838\u5fc3\u8bbe\u8ba1\u5728\u4e8e Promptable Segmentation\uff0c\u5b83\u5c06\u624b\u5de5\u5236\u4f5c\u7684\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165\u5e76\u8fd4\u56de\u9884\u671f\u7684\u5206\u5272\u63a9\u7801\u3002 SAM \u4f7f\u7528\u4e24\u79cd\u7c7b\u578b\u7684\u63d0\u793a\uff0c\u5305\u62ec\u7a7a\u95f4\u63d0\u793a\uff08\u4f8b\u5982\u70b9\uff09\u548c\u8bed\u4e49\u63d0\u793a\uff08\u4f8b\u5982\u6587\u672c\uff09\uff0c\u5b83\u4eec\u5171\u540c\u4f5c\u7528\u4ee5\u63d0\u793a SAM \u5bf9\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u4efb\u4f55\u5185\u5bb9\u8fdb\u884c\u5206\u6bb5\u3002\u5c3d\u7ba1\u63d0\u793a\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5982\u4f55\u4e3a SAM \u83b7\u53d6\u5408\u9002\u7684\u63d0\u793a\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86 SAM \u7684\u67b6\u6784\uff0c\u5e76\u786e\u5b9a\u4e86\u5b66\u4e60 SAM \u6709\u6548\u63d0\u793a\u7684\u4e24\u4e2a\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7a7a\u95f4\u8bed\u4e49\u63d0\u793a\u5b66\u4e60\uff08SSPrompt\uff09\uff0c\u5b83\u53ef\u4ee5\u5b66\u4e60\u6709\u6548\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u63d0\u793a\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684 SAM\u3002\u5177\u4f53\u6765\u8bf4\uff0cSSPrompt \u5f15\u5165\u4e86\u7a7a\u95f4\u63d0\u793a\u5b66\u4e60\u548c\u8bed\u4e49\u63d0\u793a\u5b66\u4e60\uff0c\u76f4\u63a5\u5728\u5d4c\u5165\u7a7a\u95f4\u4e0a\u4f18\u5316\u7a7a\u95f4\u63d0\u793a\u548c\u8bed\u4e49\u63d0\u793a\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u63d0\u793a\u7f16\u7801\u5668\u4e2d\u7f16\u7801\u7684\u77e5\u8bc6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSSPrompt \u5728\u591a\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u5730\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002|[2401.04651v1](http://arxiv.org/pdf/2401.04651v1)|null|\n", "2401.04614": "|**2024-01-09**|**Generic Knowledge Boosted Pre-training For Remote Sensing Images**|\u901a\u7528\u77e5\u8bc6\u4fc3\u8fdb\u9065\u611f\u56fe\u50cf\u7684\u9884\u8bad\u7ec3|Ziyue Huang, Mingming Zhang, Yuan Gong, Qingjie Liu, Yunhong Wang|Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing image understanding tasks. Most backbones of existing remote sensing deep learning models are typically initialized by pre-trained weights obtained from ImageNet pre-training (IMP). However, domain gaps exist between remote sensing images and natural images (e.g., ImageNet), making deep learning models initialized by pre-trained weights of IMP perform poorly for remote sensing image understanding. Although some pre-training methods are studied in the remote sensing community, current remote sensing pre-training methods face the problem of vague generalization by only using remote sensing images. In this paper, we propose a novel remote sensing pre-training framework, Generic Knowledge Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations from remote sensing and natural images for remote sensing understanding tasks. GeRSP contains two pre-training branches: (1) A self-supervised pre-training branch is adopted to learn domain-related representations from unlabeled remote sensing images. (2) A supervised pre-training branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pre-training branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pre-trained model for deep learning model initialization. Finally, we evaluate GeRSP and other remote sensing pre-training methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of remote sensing downstream tasks.|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u4e8e\u573a\u666f\u5206\u7c7b\u3001\u53d8\u5316\u68c0\u6d4b\u3001\u571f\u5730\u8986\u76d6\u5206\u5272\u548c\u5176\u4ed6\u9065\u611f\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5927\u591a\u6570\u4e3b\u5e72\u901a\u5e38\u662f\u901a\u8fc7\u4ece ImageNet \u9884\u8bad\u7ec3 (IMP) \u83b7\u5f97\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u6765\u521d\u59cb\u5316\u7684\u3002\u7136\u800c\uff0c\u9065\u611f\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\uff08\u4f8b\u5982 ImageNet\uff09\u4e4b\u95f4\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u4f7f\u5f97\u7531 IMP \u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u5c3d\u7ba1\u9065\u611f\u754c\u7814\u7a76\u4e86\u4e00\u4e9b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f46\u5f53\u524d\u7684\u9065\u611f\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u4f7f\u7528\u9065\u611f\u56fe\u50cf\uff0c\u9762\u4e34\u7740\u6cdb\u5316\u6a21\u7cca\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9065\u611f\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5373\u901a\u7528\u77e5\u8bc6\u589e\u5f3a\u9065\u611f\u9884\u8bad\u7ec3\uff08GeRSP\uff09\uff0c\u7528\u4e8e\u4ece\u9065\u611f\u548c\u81ea\u7136\u56fe\u50cf\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u9065\u611f\u7406\u89e3\u4efb\u52a1\u3002 GeRSP\u5305\u542b\u4e24\u4e2a\u9884\u8bad\u7ec3\u5206\u652f\uff1a\uff081\uff09\u91c7\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5206\u652f\u4ece\u672a\u6807\u8bb0\u7684\u9065\u611f\u56fe\u50cf\u4e2d\u5b66\u4e60\u9886\u57df\u76f8\u5173\u7684\u8868\u793a\u3002 (2) \u5c06\u6709\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u5206\u652f\u96c6\u6210\u5230 GeRSP \u4e2d\uff0c\u4ee5\u4fbf\u4ece\u6807\u8bb0\u7684\u81ea\u7136\u56fe\u50cf\u4e2d\u5b66\u4e60\u4e00\u822c\u77e5\u8bc6\u3002\u6b64\u5916\uff0cGeRSP \u4f7f\u7528\u5e08\u751f\u67b6\u6784\u7ed3\u5408\u4e24\u4e2a\u9884\u8bad\u7ec3\u5206\u652f\uff0c\u540c\u65f6\u5b66\u4e60\u5177\u6709\u4e00\u822c\u77e5\u8bc6\u548c\u7279\u6b8a\u77e5\u8bc6\u7684\u8868\u793a\uff0c\u4ece\u800c\u751f\u6210\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u521d\u59cb\u5316\u7684\u5f3a\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5373\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u573a\u666f\u5206\u7c7b\uff09\u4e0a\u8bc4\u4f30 GeRSP \u548c\u5176\u4ed6\u9065\u611f\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u8868\u660e\uff0cGeRSP \u53ef\u4ee5\u6709\u6548\u5730\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u9065\u611f\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002|[2401.04614v1](http://arxiv.org/pdf/2401.04614v1)|**[link](https://github.com/floatingstarZ/GeRSP)**|\n", "2401.04575": "|**2024-01-09**|**Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding**|Let's Go Shopping (LGS)\u2014\u2014\u7528\u4e8e\u89c6\u89c9\u6982\u5ff5\u7406\u89e3\u7684\u7f51\u7edc\u89c4\u6a21\u56fe\u50cf\u6587\u672c\u6570\u636e\u96c6|Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli, Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot Branson, et.al.|Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.|\u795e\u7ecf\u7f51\u7edc\u7684\u89c6\u89c9\u548c\u89c6\u89c9\u8bed\u8a00\u5e94\u7528\uff08\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u5b57\u5e55\uff09\u4f9d\u8d56\u4e8e\u9700\u8981\u975e\u5e73\u51e1\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u7684\u5927\u89c4\u6a21\u6ce8\u91ca\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u8017\u65f6\u7684\u5de5\u4f5c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff0c\u5c06\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u7684\u9009\u62e9\u9650\u5236\u5728\u5c11\u6570\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5bfb\u6c42\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6536\u96c6\u548c\u6ce8\u91ca\u56fe\u50cf\u3002\u4ee5\u524d\u7684\u4e3e\u63aa\u662f\u4ece HTML \u66ff\u4ee3\u6587\u672c\u4e2d\u6536\u96c6\u6807\u9898\u5e76\u6293\u53d6\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u6e90\u5b58\u5728\u566a\u97f3\u3001\u7a00\u758f\u6027\u6216\u4e3b\u89c2\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9009\u62e9\u6570\u636e\u7b26\u5408\u4e09\u4e2a\u6807\u51c6\u7684\u5546\u4e1a\u8d2d\u7269\u7f51\u7ad9\uff1a\u5e72\u51c0\u3001\u4fe1\u606f\u4e30\u5bcc\u3001\u6d41\u7545\u3002\u6211\u4eec\u4ecb\u7ecd Let's Go Shopping (LGS) \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u516c\u5f00\u7535\u5b50\u5546\u52a1\u7f51\u7ad9\u7684 1500 \u4e07\u4e2a\u56fe\u50cf\u6807\u9898\u5bf9\u3002\u4e0e\u73b0\u6709\u7684\u901a\u7528\u57df\u6570\u636e\u96c6\u76f8\u6bd4\uff0cLGS \u56fe\u50cf\u4e13\u6ce8\u4e8e\u524d\u666f\u7269\u4f53\uff0c\u80cc\u666f\u4e0d\u592a\u590d\u6742\u3002\u6211\u4eec\u5728 LGS \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u4e0d\u5bb9\u6613\u6cdb\u5316\u5230\u7535\u5b50\u5546\u52a1\u6570\u636e\uff0c\u800c\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\u53ef\u4ee5\u66f4\u597d\u5730\u6cdb\u5316\u3002\u6b64\u5916\uff0cLGS \u7684\u9ad8\u8d28\u91cf\u7535\u5b50\u5546\u52a1\u56fe\u50cf\u548c\u53cc\u6a21\u6001\u7279\u6027\u4f7f\u5176\u5728\u89c6\u89c9\u8bed\u8a00\u53cc\u6a21\u6001\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff1aLGS \u4f7f\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u5b57\u5e55\uff0c\u5e76\u5e2e\u52a9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u7535\u5b50\u5546\u52a1\u98ce\u683c\u8f6c\u79fb\u3002|[2401.04575v1](http://arxiv.org/pdf/2401.04575v1)|null|\n", "2401.04570": "|**2024-01-09**|**An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and Hemorrhagic Volume Estimation**|\u7528\u4e8e\u51fa\u8840\u6027\u5352\u4e2d\u5206\u5272\u548c\u51fa\u8840\u91cf\u4f30\u8ba1\u7684\u81ea\u52a8\u7ea7\u8054\u6a21\u578b|Weijin Xu, Zhuang Sha, Huihua Yang, Rongcai Jiang, Zhanying Li, Wentao Liu, Ruisheng Su|Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that poses a great health threat. Promptly and accurately delineating the bleeding region and estimating the volume of bleeding in Computer Tomography (CT) images can assist clinicians in treatment planning, leading to improved treatment outcomes for patients. In this paper, a cascaded 3D model is constructed based on UNet to perform a two-stage segmentation of the hemorrhage area in CT images from rough to fine, and the hemorrhage volume is automatically calculated from the segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans, the proposed model provides high-quality segmentation outcome with higher accuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample) when compared to the traditional Tada formula with respect to hemorrhage volume estimation.|\u51fa\u8840\u6027\u4e2d\u98ce\uff08HS\uff09\u53d1\u75c5\u8fc5\u901f\uff0c\u662f\u4e00\u79cd\u5bf9\u5065\u5eb7\u6784\u6210\u5de8\u5927\u5a01\u80c1\u7684\u4e25\u91cd\u75be\u75c5\u3002\u5728\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u56fe\u50cf\u4e2d\u53ca\u65f6\u51c6\u786e\u5730\u63cf\u7ed8\u51fa\u8840\u533a\u57df\u5e76\u4f30\u8ba1\u51fa\u8840\u91cf\u53ef\u4ee5\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u5236\u5b9a\u6cbb\u7597\u8ba1\u5212\uff0c\u4ece\u800c\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u7ed3\u679c\u3002\u672c\u6587\u57fa\u4e8eUNet\u6784\u5efa\u7ea7\u80543D\u6a21\u578b\uff0c\u5bf9CT\u56fe\u50cf\u4e2d\u7684\u51fa\u8840\u533a\u57df\u8fdb\u884c\u7531\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u5206\u5272\uff0c\u5e76\u6839\u636e\u5206\u5272\u533a\u57df\u81ea\u52a8\u8ba1\u7b97\u51fa\u8840\u91cf\u3002\u5728\u5305\u542b 341 \u4f8b\u51fa\u8840\u6027\u4e2d\u98ce CT \u626b\u63cf\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u4f20\u7edf\u7684\u51fa\u8840\u6027 Tada \u516c\u5f0f\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u7ed3\u679c\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u5ea6\uff08DSC 85.66%\uff09\u548c\u66f4\u597d\u7684\u8ba1\u7b97\u6548\u7387\uff08\u6bcf\u4e2a\u6837\u672c 6.2 \u79d2\uff09\u4f53\u79ef\u4f30\u8ba1\u3002|[2401.04570v1](http://arxiv.org/pdf/2401.04570v1)|null|\n", "2401.04464": "|**2024-01-09**|**PhilEO Bench: Evaluating Geo-Spatial Foundation Models**|PhilEO Bench\uff1a\u8bc4\u4f30\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b|Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis, Bertrand Le Saux|Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.|\u5730\u7403\u89c2\u6d4b (EO) \u536b\u661f\u6355\u83b7\u4e86\u5927\u91cf\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u5176\u4e2d Sentinel-2 \u661f\u5ea7\u6bcf\u5929\u751f\u6210 1.6 TB \u7684\u6570\u636e\u3002\u8fd9\u4f7f\u5f97\u9065\u611f\u6210\u4e3a\u4e00\u4e2a\u6570\u636e\u4e30\u5bcc\u7684\u9886\u57df\uff0c\u975e\u5e38\u9002\u5408\u673a\u5668\u5b66\u4e60 (ML) \u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e94\u7528\u4e8e EO \u7684\u4e00\u4e2a\u74f6\u9888\u662f\u7f3a\u4e4f\u6ce8\u91ca\u6570\u636e\uff0c\u56e0\u4e3a\u6ce8\u91ca\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6210\u672c\u9ad8\u6602\u7684\u8fc7\u7a0b\u3002\u56e0\u6b64\uff0c\u8be5\u9886\u57df\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u4e0a\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165 PhilEO Bench\uff08\u4e00\u79cd\u9488\u5bf9 EO \u57fa\u7840\u6a21\u578b\u7684\u65b0\u9896\u8bc4\u4f30\u6846\u67b6\uff09\u89e3\u51b3\u4e86\u5728\u516c\u5e73\u3001\u7edf\u4e00\u7684\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684\u9700\u6c42\u3002\u8be5\u6846\u67b6\u7531\u4e00\u4e2a\u6d4b\u8bd5\u53f0\u548c\u4e00\u4e2a\u65b0\u9896\u7684 400 GB Sentinel-2 \u6570\u636e\u96c6\u7ec4\u6210\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6807\u7b7e\uff1a\u5efa\u7b51\u5bc6\u5ea6\u4f30\u8ba1\u3001\u9053\u8def\u5206\u5272\u548c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3002\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u4e2a n \u6b21\u955c\u5934\u548c\u6536\u655b\u901f\u7387\u4e0b\u8bc4\u4f30\u4e0d\u540c\u7684\u57fa\u7840\u6a21\u578b\uff08\u5305\u62ec Prithvi \u548c SatMAE\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002|[2401.04464v1](http://arxiv.org/pdf/2401.04464v1)|null|\n", "2401.04463": "|**2024-01-09**|**D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection**|D3AD\uff1a\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u52a8\u6001\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b|Justin Tebbe, Jawad Tayyub|Diffusion models have found valuable applications in anomaly detection by capturing the nominal data distribution and identifying anomalies via reconstruction. Despite their merits, they struggle to localize anomalies of varying scales, especially larger anomalies like entire missing components. Addressing this, we present a novel framework that enhances the capability of diffusion models, by extending the previous introduced implicit conditioning approach Meng et al. (2022) in three significant ways. First, we incorporate a dynamic step size computation that allows for variable noising steps in the forward process guided by an initial anomaly prediction. Second, we demonstrate that denoising an only scaled input, without any added noise, outperforms conventional denoising process. Third, we project images in a latent space to abstract away from fine details that interfere with reconstruction of large missing components. Additionally, we propose a fine-tuning mechanism that facilitates the model to effectively grasp the nuances of the target domain. Our method undergoes rigorous evaluation on two prominent anomaly detection datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our framework effectively localizes anomalies regardless of their scale, marking a pivotal advancement in diffusion-based anomaly detection.|\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6355\u83b7\u6807\u79f0\u6570\u636e\u5206\u5e03\u5e76\u901a\u8fc7\u91cd\u5efa\u8bc6\u522b\u5f02\u5e38\uff0c\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u627e\u5230\u4e86\u6709\u4ef7\u503c\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u6709\u5176\u4f18\u70b9\uff0c\u4f46\u5b83\u4eec\u5f88\u96be\u5b9a\u4f4d\u4e0d\u540c\u89c4\u6a21\u7684\u5f02\u5e38\uff0c\u5c24\u5176\u662f\u8f83\u5927\u7684\u5f02\u5e38\uff0c\u4f8b\u5982\u6574\u4e2a\u7f3a\u5931\u7684\u7ec4\u4ef6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u5148\u524d\u5f15\u5165\u7684\u9690\u5f0f\u8c03\u8282\u65b9\u6cd5\uff0c\u589e\u5f3a\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u3002 \uff082022\uff09\u4ee5\u4e09\u4e2a\u91cd\u8981\u7684\u65b9\u5f0f\u3002\u9996\u5148\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u52a8\u6001\u6b65\u957f\u8ba1\u7b97\uff0c\u5141\u8bb8\u5728\u521d\u59cb\u5f02\u5e38\u9884\u6d4b\u6307\u5bfc\u4e0b\u7684\u524d\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u53ef\u53d8\u566a\u58f0\u6b65\u9aa4\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bc1\u660e\u5bf9\u4ec5\u7f29\u653e\u7684\u8f93\u5165\u8fdb\u884c\u53bb\u566a\uff0c\u800c\u4e0d\u6dfb\u52a0\u4efb\u4f55\u566a\u58f0\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5c06\u56fe\u50cf\u6295\u5f71\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u62bd\u8c61\u51fa\u5e72\u6270\u5927\u578b\u7f3a\u5931\u7ec4\u4ef6\u91cd\u5efa\u7684\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u8c03\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u6709\u6548\u5730\u638c\u63e1\u76ee\u6807\u57df\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e24\u4e2a\u8457\u540d\u7684\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6 VISA \u548c BTAD \u8fdb\u884c\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u5b9a\u4f4d\u5f02\u5e38\uff0c\u65e0\u8bba\u5176\u89c4\u6a21\u5982\u4f55\uff0c\u8fd9\u6807\u5fd7\u7740\u57fa\u4e8e\u6269\u6563\u7684\u5f02\u5e38\u68c0\u6d4b\u7684\u5173\u952e\u8fdb\u6b65\u3002|[2401.04463v1](http://arxiv.org/pdf/2401.04463v1)|null|\n", "2401.04448": "|**2024-01-09**|**A Novel Dataset for Non-Destructive Inspection of Handwritten Documents**|\u7528\u4e8e\u624b\u5199\u6587\u6863\u65e0\u635f\u68c0\u6d4b\u7684\u65b0\u578b\u6570\u636e\u96c6|Eleonora Breci, Luca Guarnera, Sebastiano Battiato|Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper\" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.|\u6cd5\u533b\u7b14\u8ff9\u68c0\u67e5\u662f\u6cd5\u533b\u5b66\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u65e8\u5728\u68c0\u67e5\u624b\u5199\u6587\u4ef6\uff0c\u4ee5\u4fbf\u6b63\u786e\u5b9a\u4e49\u6216\u5047\u8bbe\u624b\u7a3f\u7684\u4f5c\u8005\u3002\u8fd9\u4e9b\u5206\u6790\u6d89\u53ca\u901a\u8fc7\u5168\u9762\u6bd4\u8f83\u5185\u5728\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u6765\u6bd4\u8f83\u4e24\u4e2a\u6216\u591a\u4e2a\uff08\u6570\u5b57\u5316\uff09\u6587\u6863\u3002\u5982\u679c\u5b58\u5728\u76f8\u5173\u6027\u5e76\u4e14\u6ee1\u8db3\u7279\u5b9a\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5219\u53ef\u4ee5\u786e\u8ba4\u6240\u5206\u6790\u7684\u6587\u6863\u662f\u7531\u540c\u4e00\u4e2a\u4eba\u7f16\u5199\u7684\u3002\u5bf9\u521b\u5efa\u80fd\u591f\u63d0\u53d6\u548c\u6bd4\u8f83\u91cd\u8981\u7279\u5f81\u7684\u590d\u6742\u5de5\u5177\u7684\u9700\u6c42\u5bfc\u81f4\u4e86\u5177\u6709\u51e0\u4e4e\u5b8c\u5168\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u5c16\u7aef\u8f6f\u4ef6\u7684\u5f00\u53d1\uff0c\u6539\u8fdb\u4e86\u7b14\u8ff9\u7684\u53d6\u8bc1\u68c0\u67e5\u5e76\u5b9e\u73b0\u4e86\u8d8a\u6765\u8d8a\u5ba2\u89c2\u7684\u8bc4\u4f30\u3002\u8fd9\u662f\u901a\u8fc7\u57fa\u4e8e\u7eaf\u6570\u5b66\u6982\u5ff5\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u7684\u3002\u4f7f\u7528\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u6210\u4e3a\u6700\u597d\u5730\u89e3\u51b3\u624b\u5934\u4efb\u52a1\u7684\u5173\u952e\u8981\u7d20\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff0c\u7531\u4e24\u4e2a\u5b50\u96c6\u7ec4\u6210\uff1a\u7b2c\u4e00\u4e2a\u7531 21 \u4e2a\u6587\u6863\u7ec4\u6210\uff0c\u8fd9\u4e9b\u6587\u6863\u662f\u901a\u8fc7\u7ecf\u5178\u7684\u201c\u7b14\u548c\u7eb8\u201d\u65b9\u6cd5\uff08\u540e\u6765\u6570\u5b57\u5316\uff09\u7f16\u5199\u7684\uff0c\u5e76\u76f4\u63a5\u5728\u5e73\u677f\u7535\u8111\u7b49\u5e38\u89c1\u8bbe\u5907\u4e0a\u83b7\u53d6\uff1b\u7b2c\u4e8c\u4e2a\u7531 124 \u4e2a\u4e0d\u540c\u4eba\u7684 362 \u4efd\u624b\u5199\u624b\u7a3f\u7ec4\u6210\uff0c\u8fd9\u4e9b\u624b\u7a3f\u662f\u6309\u7167\u7279\u5b9a\u6d41\u7a0b\u83b7\u5f97\u7684\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f00\u521b\u4e86\u4f20\u7edf\u624b\u5199\u6587\u6863\u4e0e\u6570\u5b57\u5de5\u5177\uff08\u4f8b\u5982\u5e73\u677f\u7535\u8111\uff09\u751f\u6210\u7684\u6587\u6863\u4e4b\u95f4\u7684\u6bd4\u8f83\u3002\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5206\u7c7b\u51c6\u786e\u5ea6\u4e3a 90%\u53ef\u4ee5\u5728\u7b2c\u4e00\u4e2a\u5b50\u96c6\uff08\u5199\u5728\u7eb8\u548c\u7b14\u4e0a\u7684\u6587\u6863\uff0c\u7136\u540e\u6570\u5b57\u5316\u5e76\u5728\u5e73\u677f\u7535\u8111\u4e0a\uff09\u4e0a\u5b9e\u73b0\uff0c\u800c\u5728\u6570\u636e\u7684\u7b2c\u4e8c\u90e8\u5206\u4e0a\u53ef\u4ee5\u5b9e\u73b0 96%\u3002\u6570\u636e\u96c6\u53ef\u5728 https://iplab.dmi.unict.it/ \u4e0a\u83b7\u5f97\u3002 mfs/forensic-handwriting-analysis/novel-dataset-2023/\u3002|[2401.04448v1](http://arxiv.org/pdf/2401.04448v1)|null|\n", "2401.04441": "|**2024-01-09**|**Image classification network enhancement methods based on knowledge injection**|\u57fa\u4e8e\u77e5\u8bc6\u6ce8\u5165\u7684\u56fe\u50cf\u5206\u7c7b\u7f51\u7edc\u589e\u5f3a\u65b9\u6cd5|Yishuang Tian, Ning Wang, Liang Zhang|The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network. The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task. We constructed a knowledge injection dataset with matching knowledge data and image classification data. The knowledge injection dataset is the benchmark dataset for the experiments in the paper. Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales.|\u76ee\u524d\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u8fd8\u505c\u7559\u5728\u50cfImage-Label\u5bf9\u8fd9\u6837\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u76d1\u7763\u65b9\u5f0f\uff0c\u8fd9\u4f7f\u5f97\u4f20\u7edf\u7b97\u6cd5\u96be\u4ee5\u89e3\u91ca\u7ed3\u679c\u7684\u539f\u56e0\uff0c\u9884\u6d4b\u903b\u8f91\u4e5f\u96be\u4ee5\u7406\u89e3\u548c\u5206\u6790\u3002\u76ee\u524d\u7684\u7b97\u6cd5\u6ca1\u6709\u5229\u7528\u4eba\u7c7b\u73b0\u6709\u7684\u77e5\u8bc6\u4fe1\u606f\uff0c\u4f7f\u5f97\u6a21\u578b\u4e0d\u7b26\u5408\u4eba\u7c7b\u7684\u8ba4\u77e5\u6a21\u578b\uff0c\u4f7f\u5f97\u6a21\u578b\u4e0d\u9002\u5408\u4eba\u7c7b\u4f7f\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u672c\u53d1\u660e\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u77e5\u8bc6\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\uff0c\u5229\u7528\u4eba\u7c7b\u73b0\u6709\u77e5\u8bc6\u4fe1\u606f\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\u3002\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u5206\u5c42\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7531\u591a\u7ea7\u5206\u5c42\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u591a\u7ea7\u5206\u5c42\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7ec4\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7684\u9690\u85cf\u4fe1\u606f\u3002\u6211\u4eec\u7814\u7a76\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5206\u6790\u77e5\u8bc6\u6ce8\u5165\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u6765\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u5339\u914d\u77e5\u8bc6\u6570\u636e\u548c\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u7684\u77e5\u8bc6\u6ce8\u5165\u6570\u636e\u96c6\u3002\u77e5\u8bc6\u6ce8\u5165\u6570\u636e\u96c6\u662f\u672c\u6587\u5b9e\u9a8c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6a21\u578b\u8868\u8fbe\u4e86\u4e0d\u540c\u5c3a\u5ea6\u9690\u85cf\u5c42\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u7684\u63d0\u9ad8\u3002|[2401.04441v1](http://arxiv.org/pdf/2401.04441v1)|null|\n", "2401.04437": "|**2024-01-09**|**Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods**|\u4f7f\u7528\u964d\u7ef4\u65b9\u6cd5\u8fdb\u884c\u9ad8\u5149\u8c31\u6210\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u8bc1\u5206\u6790|Dongeon Kim, YeongHyeon Park|Recent studies try to use hyperspectral imaging (HSI) to detect foreign matters in products because it enables to visualize the invisible wavelengths including ultraviolet and infrared. Considering the enormous image channels of the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be considered to reduce but those cannot ease the fundamental limitations, as follows: (1) latency of HSI capturing. (2) less explanation ability of the important channels. In this paper, to circumvent the aforementioned methods, one of the ways to channel reduction, on anomaly detection proposed HSI. Different from feature extraction methods (i.e., PCA or UMAP), feature selection can sort the feature by impact and show better explainability so we might redesign the task-optimized and cost-effective spectroscopic camera. Via the extensive experiment results with synthesized MVTec AD dataset, we confirm that the feature selection method shows 6.90x faster at the inference phase compared with feature extraction-based approaches while preserving anomaly detection performance. Ultimately, we conclude the advantage of feature selection which is effective yet fast.|\u6700\u8fd1\u7684\u7814\u7a76\u5c1d\u8bd5\u4f7f\u7528\u9ad8\u5149\u8c31\u6210\u50cf (HSI) \u6765\u68c0\u6d4b\u4ea7\u54c1\u4e2d\u7684\u5f02\u7269\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u53ef\u89c6\u5316\u5305\u62ec\u7d2b\u5916\u7ebf\u548c\u7ea2\u5916\u7ebf\u5728\u5185\u7684\u4e0d\u53ef\u89c1\u6ce2\u957f\u3002\u8003\u8651\u5230HSI\u5de8\u5927\u7684\u56fe\u50cf\u901a\u9053\uff0c\u53ef\u4ee5\u8003\u8651\u91c7\u7528PCA\u6216UMAP\u7b49\u591a\u79cd\u964d\u7ef4\u65b9\u6cd5\u6765\u964d\u4f4e\u7ef4\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u4e0d\u80fd\u7f13\u89e3\u6839\u672c\u6027\u7684\u9650\u5236\uff0c\u5982\u4e0b\uff1a\uff081\uff09HSI\u6355\u83b7\u7684\u5ef6\u8fdf\u3002 (2)\u91cd\u8981\u6e20\u9053\u7684\u89e3\u91ca\u80fd\u529b\u8f83\u5dee\u3002\u672c\u6587\u4e3a\u4e86\u89c4\u907f\u524d\u8ff0\u65b9\u6cd5\uff0c\u901a\u9053\u7f29\u51cf\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5728\u5f02\u5e38\u68c0\u6d4b\u4e0a\u63d0\u51fa\u4e86HSI\u3002\u4e0e\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff08\u5373 PCA \u6216 UMAP\uff09\u4e0d\u540c\uff0c\u7279\u5f81\u9009\u62e9\u53ef\u4ee5\u6309\u5f71\u54cd\u5bf9\u7279\u5f81\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u663e\u793a\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u91cd\u65b0\u8bbe\u8ba1\u4efb\u52a1\u4f18\u5316\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u5149\u8c31\u76f8\u673a\u3002\u901a\u8fc7\u5408\u6210 MVTec AD \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6211\u4eec\u786e\u8ba4\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u63a8\u7406\u9636\u6bb5\u7684\u901f\u5ea6\u6bd4\u57fa\u4e8e\u7279\u5f81\u63d0\u53d6\u7684\u65b9\u6cd5\u5feb 6.90 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u603b\u7ed3\u51fa\u7279\u5f81\u9009\u62e9\u7684\u4f18\u52bf\u662f\u6709\u6548\u4e14\u5feb\u901f\u3002|[2401.04437v1](http://arxiv.org/pdf/2401.04437v1)|null|\n", "2401.04425": "|**2024-01-09**|**Meta-forests: Domain generalization on random forests with meta-learning**|\u5143\u68ee\u6797\uff1a\u901a\u8fc7\u5143\u5b66\u4e60\u5bf9\u968f\u673a\u68ee\u6797\u8fdb\u884c\u9886\u57df\u6cdb\u5316|Yuyang Sun, Panagiotis Kosmas|Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called \"meta-forests\", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.|\u9886\u57df\u6cdb\u5316\u662f\u4e00\u79cd\u6d41\u884c\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u4ece\u591a\u4e2a\u6e90\u9886\u57df\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u770b\u4e0d\u89c1\u7684\u76ee\u6807\u9886\u57df\u4e0a\u8868\u73b0\u826f\u597d\u3002\u9886\u57df\u6cdb\u5316\u5728\u6570\u636e\u6709\u9650\u3001\u6536\u96c6\u56f0\u96be\u6216\u6536\u96c6\u6210\u672c\u6602\u8d35\u7684\u60c5\u51b5\u4e0b\u975e\u5e38\u6709\u7528\uff0c\u4f8b\u5982\u5728\u5bf9\u8c61\u8bc6\u522b\u548c\u751f\u7269\u533b\u5b66\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5143\u68ee\u6797\u201d\u7684\u65b0\u578b\u9886\u57df\u6cdb\u5316\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u57fa\u672c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u7b56\u7565\u548c\u6700\u5927\u5e73\u5747\u5dee\u5f02\u5ea6\u91cf\u3002\u5143\u68ee\u6797\u7684\u76ee\u7684\u662f\u901a\u8fc7\u51cf\u5c11\u6811\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5e76\u589e\u52a0\u6811\u7684\u5f3a\u5ea6\u6765\u589e\u5f3a\u5206\u7c7b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u5143\u68ee\u6797\u5728\u6bcf\u4e2a\u5143\u4efb\u52a1\u671f\u95f4\u8fdb\u884c\u5143\u5b66\u4e60\u4f18\u5316\uff0c\u540c\u65f6\u8fd8\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u6765\u60e9\u7f5a\u5143\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u8f83\u5dee\u7684\u6cdb\u5316\u6027\u80fd\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u4e4b\u524d\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u4e24\u4e2a\u516c\u5f00\u5bf9\u8c61\u8bc6\u522b\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u96c6\u4e0a\u5bf9\u5176\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5143\u68ee\u6797\u5728\u5bf9\u8c61\u8bc6\u522b\u548c\u8461\u8404\u7cd6\u76d1\u6d4b\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.04425v1](http://arxiv.org/pdf/2401.04425v1)|null|\n", "2401.04406": "|**2024-01-09**|**MapAI: Precision in Building Segmentation**|MapAI\uff1a\u7cbe\u786e\u7684\u5efa\u7b51\u5206\u5272|Sander Riis\u00f8en Jyhne, Morten Goodwin, Per Arne Andersen, Ivar Oveland, Alexander Salveson Nossum, Karianne Ormseth, Mathilde \u00d8rstavik, Andrew C. Flatman|MapAI: Precision in Building Segmentation is a competition arranged with the Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration with Centre for Artificial Intelligence Research at the University of Agder (CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency for Data Supply and Infrastructure. The competition will be held in the fall of 2022. It will be concluded at the Northern Lights Deep Learning conference focusing on the segmentation of buildings using aerial images and laser data. We propose two different tasks to segment buildings, where the first task can only utilize aerial images, while the second must use laser data (LiDAR) with or without aerial images. Furthermore, we use IoU and Boundary IoU to properly evaluate the precision of the models, with the latter being an IoU measure that evaluates the results' boundaries. We provide the participants with a training dataset and keep a test dataset for evaluation.|MapAI\uff1a\u7cbe\u786e\u5efa\u7b51\u5206\u5272\u662f\u7531\u632a\u5a01\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u8054\u76df (NORA) \u4e0e\u963f\u683c\u5fb7\u5c14\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u5fc3 (CAIR)\u3001\u632a\u5a01\u6d4b\u7ed8\u5c40\u3001AI:Hub\u3001Norkart \u548c\u4e39\u9ea6\u6570\u636e\u4f9b\u5e94\u548c\u57fa\u7840\u8bbe\u65bd\u673a\u6784\u3002\u8be5\u7ade\u8d5b\u5c06\u4e8e 2022 \u5e74\u79cb\u5b63\u4e3e\u884c\u3002\u6bd4\u8d5b\u5c06\u5728\u5317\u6781\u5149\u6df1\u5ea6\u5b66\u4e60\u4f1a\u8bae\u4e0a\u7ed3\u675f\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u822a\u7a7a\u56fe\u50cf\u548c\u6fc0\u5149\u6570\u636e\u8fdb\u884c\u5efa\u7b51\u7269\u5206\u5272\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u4efb\u52a1\u6765\u5206\u5272\u5efa\u7b51\u7269\uff0c\u5176\u4e2d\u7b2c\u4e00\u4e2a\u4efb\u52a1\u53ea\u80fd\u5229\u7528\u822a\u7a7a\u56fe\u50cf\uff0c\u800c\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u5fc5\u987b\u4f7f\u7528\u5e26\u6709\u6216\u4e0d\u5e26\u6709\u822a\u7a7a\u56fe\u50cf\u7684\u6fc0\u5149\u6570\u636e\uff08LiDAR\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528 IoU \u548c\u8fb9\u754c IoU \u6765\u6b63\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u7cbe\u5ea6\uff0c\u540e\u8005\u662f\u8bc4\u4f30\u7ed3\u679c\u8fb9\u754c\u7684 IoU \u5ea6\u91cf\u3002\u6211\u4eec\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u4fdd\u7559\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u8bc4\u4f30\u3002|[2401.04406v1](http://arxiv.org/pdf/2401.04406v1)|null|\n", "2401.04405": "|**2024-01-09**|**Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation**|\u7528\u4e8e\u9ad8\u6548\u6bcf\u6807\u9898\u6bd4\u7279\u7387\u9636\u68af\u4f30\u8ba1\u7684\u6700\u4f73\u8f6c\u7801\u5206\u8fa8\u7387\u9884\u6d4b|Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang|Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.|\u81ea\u9002\u5e94\u89c6\u9891\u6d41\u9700\u8981\u9ad8\u6548\u7684\u6bd4\u7279\u7387\u9636\u68af\u6784\u5efa\uff0c\u4ee5\u6ee1\u8db3\u5f02\u6784\u7f51\u7edc\u6761\u4ef6\u548c\u6700\u7ec8\u7528\u6237\u9700\u6c42\u3002\u6309\u6807\u9898\u4f18\u5316\u7684\u7f16\u7801\u901a\u5e38\u4f1a\u904d\u5386\u5927\u91cf\u7f16\u7801\u53c2\u6570\u6765\u641c\u7d22\u6bcf\u4e2a\u89c6\u9891\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u64cd\u4f5c\u200b\u200b\u70b9\u3002\u6700\u8fd1\uff0c\u7814\u7a76\u4eba\u5458\u8bd5\u56fe\u9884\u6d4b\u5185\u5bb9\u4f18\u5316\u7684\u6bd4\u7279\u7387\u9636\u68af\u4ee5\u51cf\u5c11\u9884\u7f16\u7801\u5f00\u9500\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f30\u8ba1Pareto\u524d\u6cbf\u4e0a\u7684\u7f16\u7801\u53c2\u6570\uff0c\u5e76\u4e14\u4ecd\u7136\u9700\u8981\u540e\u7eed\u7684\u9884\u7f16\u7801\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u76f4\u63a5\u9884\u6d4b\u6bcf\u4e2a\u9884\u8bbe\u6bd4\u7279\u7387\u7684\u6700\u4f73\u8f6c\u7801\u5206\u8fa8\u7387\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6bd4\u7279\u7387\u9636\u68af\u6784\u5efa\u3002\u6211\u4eec\u91c7\u7528\u65f6\u95f4\u6ce8\u610f\u529b\u95e8\u63a7\u5faa\u73af\u7f51\u7edc\u6765\u6355\u83b7\u65f6\u7a7a\u7279\u5f81\u5e76\u9884\u6d4b\u8f6c\u7801\u5206\u8fa8\u7387\u4f5c\u4e3a\u591a\u4efb\u52a1\u5206\u7c7b\u95ee\u9898\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u65e0\u9700\u4efb\u4f55\u9884\u7f16\u7801\u5373\u53ef\u6709\u6548\u786e\u5b9a\u5185\u5bb9\u4f18\u5316\u7684\u6bd4\u7279\u7387\u9636\u68af\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u8fd1\u4f3c\u4e86\u771f\u5b9e\u7684\u6bd4\u7279\u7387\u5206\u8fa8\u7387\u5bf9\uff0cBj{\\o}ntegaard Delta \u901f\u7387\u635f\u5931\u4e3a 1.21%\uff0c\u5e76\u4e14\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u56fa\u5b9a\u9636\u68af\u3002|[2401.04405v1](http://arxiv.org/pdf/2401.04405v1)|null|\n", "2401.04403": "|**2024-01-09**|**MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation**|MST\uff1a\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u4ee4\u724c\u5f15\u5bfc\u4ea4\u4e92\u5f0f\u5206\u5272|Long Xu, Shanghong Li, Yongquan Chen, Jun Luo|In the field of Industrial Informatics, interactive segmentation has gained significant attention for its application in human-computer interaction and data annotation. Existing algorithms, however, face challenges in balancing the segmentation accuracy between large and small targets, often leading to an increased number of user interactions. To tackle this, a novel multi-scale token adaptation algorithm, leveraging token similarity, has been devised to enhance segmentation across varying target sizes. This algorithm utilizes a differentiable top-k tokens selection mechanism, allowing for fewer tokens to be used while maintaining efficient multi-scale token interaction. Furthermore, a contrastive loss is introduced to better discriminate between target and background tokens, improving the correctness and robustness of the tokens similar to the target. Extensive benchmarking shows that the algorithm achieves state-of-the-art (SOTA) performance compared to current methods. An interactive demo and all reproducible codes will be released at https://github.com/hahamyt/mst.|\u5728\u5de5\u4e1a\u4fe1\u606f\u5b66\u9886\u57df\uff0c\u4ea4\u4e92\u5f0f\u5206\u5272\u56e0\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u6570\u636e\u6ce8\u91ca\u4e2d\u7684\u5e94\u7528\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u5e73\u8861\u5927\u76ee\u6807\u548c\u5c0f\u76ee\u6807\u4e4b\u95f4\u7684\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u7528\u6237\u4ea4\u4e92\u6570\u91cf\u589e\u52a0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u4ee4\u724c\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u5229\u7528\u4ee4\u724c\u76f8\u4f3c\u6027\u6765\u589e\u5f3a\u8de8\u4e0d\u540c\u76ee\u6807\u5927\u5c0f\u7684\u5206\u5272\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u53ef\u5fae\u7684 top-k \u4ee4\u724c\u9009\u62e9\u673a\u5236\uff0c\u5141\u8bb8\u4f7f\u7528\u66f4\u5c11\u7684\u4ee4\u724c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u4ee4\u724c\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u5f15\u5165\u5bf9\u6bd4\u635f\u5931\u4ee5\u66f4\u597d\u5730\u533a\u5206\u76ee\u6807\u548c\u80cc\u666f\u6807\u8bb0\uff0c\u63d0\u9ad8\u4e0e\u76ee\u6807\u76f8\u4f3c\u7684\u6807\u8bb0\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u4e0e\u5f53\u524d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\u3002\u4ea4\u4e92\u5f0f\u6f14\u793a\u548c\u6240\u6709\u53ef\u91cd\u73b0\u7684\u4ee3\u7801\u5c06\u5728 https://github.com/hahamyt/mst \u53d1\u5e03\u3002|[2401.04403v1](http://arxiv.org/pdf/2401.04403v1)|null|\n", "2401.04364": "|**2024-01-09**|**SoK: Facial Deepfake Detectors**|SoK\uff1a\u9762\u90e8 Deepfake \u63a2\u6d4b\u5668|Binh M. Le, Jiwon Kim, Shahroz Tariq, Kristen Moore, Alsharif Abuadbba, Simon S. Woo|Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-box, white-box, and gray-box settings. Our systematized analysis and experimentation lay the groundwork for a deeper understanding of deepfake detectors and their generalizability, paving the way for future research focused on creating detectors adept at countering various attack scenarios. Additionally, this work offers insights for developing more proactive defenses against deepfakes.|\u6df1\u5ea6\u9020\u5047\u5df2\u8fc5\u901f\u6210\u4e3a\u5bf9\u793e\u4f1a\u7684\u6df1\u523b\u800c\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u5b83\u4eec\u6613\u4e8e\u521b\u5efa\u548c\u4f20\u64ad\u3002\u8fd9\u79cd\u60c5\u51b5\u5f15\u53d1\u4e86 Deepfake \u68c0\u6d4b\u6280\u672f\u7684\u52a0\u901f\u53d1\u5c55\u3002\u7136\u800c\uff0c\u8bb8\u591a\u73b0\u6709\u7684\u68c0\u6d4b\u5668\u4e25\u91cd\u4f9d\u8d56\u5b9e\u9a8c\u5ba4\u751f\u6210\u7684\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5730\u4e3a\u65b0\u9896\u7684\u3001\u65b0\u5174\u7684\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u505a\u597d\u51c6\u5907\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u6700\u65b0\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u9020\u5047\u63a2\u6d4b\u5668\u8fdb\u884c\u4e86\u5e7f\u6cdb\u800c\u5168\u9762\u7684\u5ba1\u67e5\u548c\u5206\u6790\uff0c\u5e76\u6839\u636e\u51e0\u4e2a\u5173\u952e\u6807\u51c6\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8fd9\u4e9b\u6807\u51c6\u6709\u52a9\u4e8e\u5c06\u8fd9\u4e9b\u63a2\u6d4b\u5668\u5206\u4e3a 4 \u4e2a\u9ad8\u7ea7\u7ec4\u548c 13 \u4e2a\u7ec6\u7c92\u5ea6\u5b50\u7ec4\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u7b26\u5408\u7edf\u4e00\u7684\u6807\u51c6\u6982\u5ff5\u6846\u67b6\u3002\u8fd9\u79cd\u5206\u7c7b\u548c\u6846\u67b6\u4e3a\u5f71\u54cd\u63a2\u6d4b\u5668\u529f\u6548\u7684\u56e0\u7d20\u63d0\u4f9b\u4e86\u6df1\u5165\u800c\u5b9e\u7528\u7684\u89c1\u89e3\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 16 \u4e2a\u9886\u5148\u68c0\u6d4b\u5668\u5728\u5404\u79cd\u6807\u51c6\u653b\u51fb\u573a\u666f\uff08\u5305\u62ec\u9ed1\u76d2\u3001\u767d\u76d2\u548c\u7070\u76d2\u8bbe\u7f6e\uff09\u4e2d\u7684\u901a\u7528\u6027\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u5316\u5206\u6790\u548c\u5b9e\u9a8c\u4e3a\u66f4\u6df1\u5165\u5730\u4e86\u89e3 Deepfake \u63a2\u6d4b\u5668\u53ca\u5176\u666e\u904d\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u4e13\u6ce8\u4e8e\u521b\u5efa\u64c5\u957f\u5e94\u5bf9\u5404\u79cd\u653b\u51fb\u573a\u666f\u7684\u63a2\u6d4b\u5668\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u4e3a\u5f00\u53d1\u66f4\u4e3b\u52a8\u7684\u6df1\u5ea6\u4f2a\u9020\u9632\u5fa1\u63aa\u65bd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002|[2401.04364v1](http://arxiv.org/pdf/2401.04364v1)|null|\n", "2401.04354": "|**2024-01-09**|**Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition**|\u7528\u4e8e\u573a\u666f\u8bc6\u522b\u7684\u77e5\u8bc6\u589e\u5f3a\u591a\u89c6\u89d2\u89c6\u9891\u8868\u793a\u5b66\u4e60|Xuzheng Yu, Chen Jiang, Wei Zhang, Tian Gan, Linlin Chao, Jianan Zhao, Yuan Cheng, Qingpei Guo, Wei Chu|With the explosive growth of video data in real-world applications, a comprehensive representation of videos becomes increasingly important. In this paper, we address the problem of video scene recognition, whose goal is to learn a high-level video representation to classify scenes in videos. Due to the diversity and complexity of video contents in realistic scenarios, this task remains a challenge. Most existing works identify scenes for videos only from visual or textual information in a temporal perspective, ignoring the valuable information hidden in single frames, while several earlier studies only recognize scenes for separate images in a non-temporal perspective. We argue that these two perspectives are both meaningful for this task and complementary to each other, meanwhile, externally introduced knowledge can also promote the comprehension of videos. We propose a novel two-stream framework to model video representations from multiple perspectives, i.e. temporal and non-temporal perspectives, and integrate the two perspectives in an end-to-end manner by self-distillation. Besides, we design a knowledge-enhanced feature fusion and label prediction method that contributes to naturally introducing knowledge into the task of video scene recognition. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method.|\u968f\u7740\u5b9e\u9645\u5e94\u7528\u4e2d\u89c6\u9891\u6570\u636e\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u89c6\u9891\u7684\u5168\u9762\u8868\u793a\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u89c6\u9891\u573a\u666f\u8bc6\u522b\u95ee\u9898\uff0c\u5176\u76ee\u6807\u662f\u5b66\u4e60\u9ad8\u7ea7\u89c6\u9891\u8868\u793a\u6765\u5bf9\u89c6\u9891\u4e2d\u7684\u573a\u666f\u8fdb\u884c\u5206\u7c7b\u3002\u7531\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u89c6\u9891\u5185\u5bb9\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5927\u591a\u6570\u73b0\u6709\u4f5c\u54c1\u4ec5\u4ece\u65f6\u95f4\u89d2\u5ea6\u7684\u89c6\u89c9\u6216\u6587\u672c\u4fe1\u606f\u8bc6\u522b\u89c6\u9891\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u9690\u85cf\u5728\u5355\u5e27\u4e2d\u7684\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u800c\u4e00\u4e9b\u65e9\u671f\u7684\u7814\u7a76\u4ec5\u4ece\u975e\u65f6\u95f4\u89d2\u5ea6\u8bc6\u522b\u5355\u72ec\u56fe\u50cf\u7684\u573a\u666f\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e24\u79cd\u89c6\u89d2\u5bf9\u4e8e\u8fd9\u9879\u4efb\u52a1\u90fd\u662f\u6709\u610f\u4e49\u7684\u5e76\u4e14\u662f\u76f8\u4e92\u8865\u5145\u7684\uff0c\u540c\u65f6\uff0c\u5916\u90e8\u5f15\u5165\u7684\u77e5\u8bc6\u4e5f\u53ef\u4ee5\u4fc3\u8fdb\u5bf9\u89c6\u9891\u7684\u7406\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6d41\u6846\u67b6\uff0c\u4ece\u591a\u4e2a\u89c6\u89d2\uff08\u5373\u65f6\u95f4\u548c\u975e\u65f6\u95f4\u89c6\u89d2\uff09\u5bf9\u89c6\u9891\u8868\u793a\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u81ea\u84b8\u998f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u96c6\u6210\u8fd9\u4e24\u4e2a\u89c6\u89d2\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u7279\u5f81\u878d\u5408\u548c\u6807\u7b7e\u9884\u6d4b\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5c06\u77e5\u8bc6\u81ea\u7136\u5730\u5f15\u5165\u89c6\u9891\u573a\u666f\u8bc6\u522b\u4efb\u52a1\u4e2d\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|[2401.04354v1](http://arxiv.org/pdf/2401.04354v1)|null|\n", "2401.04330": "|**2024-01-09**|**BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation**|BD-MSA\uff1a\u591a\u5c3a\u5ea6\u7279\u5f81\u4fe1\u606f\u805a\u5408\u5f15\u5bfc\u7684\u4f53\u89e3\u8026VHR\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5|Yonghui Tan, Xiaolong Li, Yishu Chen, Jinquan Ai|The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment metrics and evaluation effects of the model described in this paper on the publicly available datasets DSIFN-CD and S2Looking are the best when compared to other models.|\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\uff08RSCD\uff09\u7684\u76ee\u7684\u662f\u68c0\u6d4b\u5728\u540c\u4e00\u5730\u70b9\u62cd\u6444\u7684\u53cc\u65f6\u6001\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6df1\u5ea6\u5b66\u4e60\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e RSCD \u4efb\u52a1\uff0c\u5728\u7ed3\u679c\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u536b\u661f\u7684\u62cd\u6444\u89d2\u5ea6\u3001\u8584\u4e91\u5c42\u7684\u5f71\u54cd\u4ee5\u53ca\u4e00\u5b9a\u7684\u5149\u7167\u6761\u4ef6\uff0c\u76ee\u524d\u7684RSCD\u7b97\u6cd5\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406\u4e00\u4e9b\u9065\u611f\u7167\u7247\u53d8\u5316\u533a\u57df\u8fb9\u7f18\u6a21\u7cca\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6050\u60e7\u805a\u5408\u53d8\u5316\u68c0\u6d4b\u8fdb\u884c\u8eab\u4f53\u89e3\u8026\u591a\u5c3a\u5ea6\uff08BD-MSA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u548c\u8bad\u7ec3\u671f\u95f4\u6536\u96c6\u7279\u5f81\u56fe\u7684\u901a\u9053\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u56fe\u4fe1\u606f\u3002\u9884\u6d4b\u9636\u6bb5\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u6210\u529f\u63d0\u53d6\u53d8\u5316\u533a\u57df\u7684\u8fb9\u754c\u4fe1\u606f\uff0c\u540c\u65f6\u5c06\u53d8\u5316\u533a\u57df\u7684\u4e3b\u4f53\u4e0e\u5176\u8fb9\u754c\u5206\u5f00\u3002\u5927\u91cf\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\uff0c\u672c\u6587\u63cf\u8ff0\u7684\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6 DSIFN-CD \u548c S2Looking \u4e0a\u7684\u8bc4\u4f30\u6307\u6807\u548c\u8bc4\u4f30\u6548\u679c\u662f\u6700\u597d\u7684\u3002|[2401.04330v1](http://arxiv.org/pdf/2401.04330v1)|null|\n"}, "OCR": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2401.04585": "|**2024-01-09**|**Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models**|\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u540e\u91cf\u5316\u7684\u589e\u5f3a\u5206\u5e03\u5bf9\u9f50|Xuewen Liu, Zhikai Li, Junrui Xiao, Qingyi Gu|Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the density and diversity in the latent space, thus facilitating the alignment of their distribution with the overall samples; and at the reconstruction output level, we propose Fine-grained Block Reconstruction, which can align the outputs of the quantized model and the full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM outperforms the existing post-training quantization frameworks in both unconditional and conditional generation scenarios. At low-bit precision, the quantized models with our method even outperform the full-precision models on most datasets.|\u6269\u6563\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u566a\u58f0\u4f30\u8ba1\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u7e41\u91cd\u7684\u53bb\u566a\u8fc7\u7a0b\u548c\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u963b\u788d\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u5e94\u7528\u3002\u91cf\u5316\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5fae\u8c03\u7684\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u5728\u52a0\u901f\u53bb\u566a\u8fc7\u7a0b\u65b9\u9762\u975e\u5e38\u6709\u524d\u666f\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u7531\u4e8e\u4e0d\u540c\u53bb\u566a\u6b65\u9aa4\u4e2d\u6fc0\u6d3b\u7684\u9ad8\u5ea6\u52a8\u6001\u5206\u5e03\uff0c\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b PTQ \u65b9\u6cd5\u5728\u6821\u51c6\u6837\u672c\u7ea7\u522b\u548c\u91cd\u5efa\u8f93\u51fa\u7ea7\u522b\u4e0a\u90fd\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u6027\u80fd\u8fdc\u4e0d\u80fd\u4ee4\u4eba\u6ee1\u610f\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u566a\u58f0\u60c5\u51b5\u4e0b\u3002\u4f4d\u6848\u4f8b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u540e\u91cf\u5316\u7684\u589e\u5f3a\u5206\u5e03\u5bf9\u9f50\uff08EDA-DM\uff09\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u6821\u51c6\u6837\u672c\u5c42\u9762\uff0c\u6211\u4eec\u6839\u636e\u6f5c\u5728\u7a7a\u95f4\u7684\u5bc6\u5ea6\u548c\u591a\u6837\u6027\u6765\u9009\u62e9\u6821\u51c6\u6837\u672c\uff0c\u4ece\u800c\u6709\u5229\u4e8e\u5b83\u4eec\u7684\u5206\u5e03\u4e0e\u6574\u4f53\u6837\u672c\u7684\u5bf9\u9f50\uff1b\u5728\u91cd\u5efa\u8f93\u51fa\u5c42\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u5757\u91cd\u5efa\uff08Fine-grained Block Reconstruction\uff09\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0d\u540c\u7f51\u7edc\u7c92\u5ea6\u4e0b\u5bf9\u9f50\u91cf\u5316\u6a21\u578b\u548c\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u8f93\u51fa\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEDA-DM \u5728\u65e0\u6761\u4ef6\u548c\u6761\u4ef6\u751f\u6210\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u6846\u67b6\u3002\u5728\u4f4e\u4f4d\u7cbe\u5ea6\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u91cf\u5316\u6a21\u578b\u751a\u81f3\u4f18\u4e8e\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u7684\u5168\u7cbe\u5ea6\u6a21\u578b\u3002|[2401.04585v1](http://arxiv.org/pdf/2401.04585v1)|null|\n", "2401.04339": "|**2024-01-09**|**Memory-Efficient Personalization using Quantized Diffusion Model**|\u4f7f\u7528\u91cf\u5316\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5185\u5b58\u9ad8\u6548\u7684\u4e2a\u6027\u5316|Hyogon Ryu, Seohyun Lim, Hyunjung Shim|The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. Our approach not only enhances personalization but also upholds prompt fidelity and image quality, significantly outperforming the baseline qualitatively and quantitatively. The code will be made publicly available.|Stable Diffusion XL\u3001Imagen \u548c Dall-E3 \u7b49\u5341\u4ebf\u53c2\u6570\u6269\u6563\u6a21\u578b\u7684\u5174\u8d77\u663e\u7740\u63a8\u8fdb\u4e86\u751f\u6210\u5f0f AI \u9886\u57df\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8d44\u6e90\u9700\u6c42\u9ad8\u548c\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u5b83\u4eec\u7684\u5927\u89c4\u6a21\u6027\u8d28\u7ed9\u5fae\u8c03\u548c\u90e8\u7f72\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u6d89\u8db3\u4e86\u76f8\u5bf9\u672a\u7ecf\u63a2\u7d22\u4f46\u524d\u666f\u5e7f\u9614\u7684\u5fae\u8c03\u91cf\u5316\u6269\u6563\u6a21\u578b\u9886\u57df\u3002\u6211\u4eec\u901a\u8fc7\u5b9a\u5236\u4e09\u4e2a\u6a21\u578b\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\uff1a\u7528\u4e8e\u5fae\u8c03\u91cf\u5316\u53c2\u6570\u7684 PEQA\u3001\u7528\u4e8e\u8bad\u7ec3\u540e\u91cf\u5316\u7684 Q-Diffusion \u4ee5\u53ca\u7528\u4e8e\u4e2a\u6027\u5316\u7684 DreamBooth\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u57fa\u7ebf\u6a21\u578b\u4e2d\u4e3b\u9898\u548c\u63d0\u793a\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u663e\u7740\u6743\u8861\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u53d7\u6269\u6563\u6a21\u578b\u4e2d\u4e0d\u540c\u65f6\u95f4\u6b65\u957f\u4e0d\u540c\u4f5c\u7528\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u7b56\u7565\uff1aS1 \u4ec5\u5728\u9009\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u5185\u4f18\u5316\u4e00\u7ec4\u5fae\u8c03\u53c2\u6570\uff0cS2 \u521b\u5efa\u591a\u4e2a\u5fae\u8c03\u53c2\u6570\u96c6\uff0c\u6bcf\u4e2a\u53c2\u6570\u96c6\u4e13\u95e8\u7528\u4e8e\u4e0d\u540c\u7684\u65f6\u95f4\u6b65\u95f4\u9694\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u4e2a\u6027\u5316\uff0c\u800c\u4e14\u8fd8\u4fdd\u6301\u4e86\u5373\u65f6\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u90fd\u663e\u7740\u4f18\u4e8e\u57fa\u7ebf\u3002\u8be5\u4ee3\u7801\u5c06\u516c\u5f00\u3002|[2401.04339v1](http://arxiv.org/pdf/2401.04339v1)|null|\n"}, "Nerf": {}, "\u751f\u6210\u6a21\u578b": {"2401.04728": "|**2024-01-09**|**Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**|\u53ef\u53d8\u5f62\u6269\u6563\uff1a\u7528\u4e8e\u5355\u56fe\u50cf\u5934\u50cf\u521b\u5efa\u7684 3D \u4e00\u81f4\u6269\u6563|Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang|Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.|\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5b9e\u73b0\u4e86\u4ece\u5355\u4e2a\u8f93\u5165\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210 3D \u8d44\u4ea7\u7684\u5148\u524d\u4e0d\u53ef\u884c\u7684\u529f\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u8fd9\u4e9b\u6a21\u578b\u7684\u8d28\u91cf\u548c\u529f\u80fd\uff0c\u4ee5\u5b8c\u6210\u521b\u5efa\u53ef\u63a7\u3001\u903c\u771f\u7684\u4eba\u7c7b\u5316\u8eab\u7684\u4efb\u52a1\u3002\u6211\u4eec\u901a\u8fc7\u5c06 3D \u53ef\u53d8\u5f62\u6a21\u578b\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6269\u6563\u65b9\u6cd5\u4e2d\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5bf9\u94f0\u63a5\u5f0f 3D \u6a21\u578b\u4e0a\u7684\u751f\u6210\u7ba1\u9053\u8fdb\u884c\u7cbe\u786e\u8c03\u8282\u53ef\u4ee5\u589e\u5f3a\u5728\u4ece\u5355\u4e2a\u56fe\u50cf\u5408\u6210\u65b0\u9896\u89c6\u56fe\u7684\u4efb\u52a1\u4e2d\u7684\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u79cd\u96c6\u6210\u6709\u52a9\u4e8e\u5c06\u9762\u90e8\u8868\u60c5\u548c\u8eab\u4f53\u59ff\u52bf\u63a7\u5236\u65e0\u7f1d\u4e14\u51c6\u786e\u5730\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u662f\u7b2c\u4e00\u4e2a\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u770b\u4e0d\u89c1\u7684\u4e3b\u4f53\u7684\u5355\u4e2a\u56fe\u50cf\u521b\u5efa\u5b8c\u5168 3D \u4e00\u81f4\u3001\u53ef\u52a8\u753b\u4e14\u903c\u771f\u7684\u4eba\u7c7b\u5316\u8eab\uff1b\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65b0\u9896\u7684\u89c6\u56fe\u548c\u65b0\u9896\u7684\u8868\u8fbe\u5408\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5316\u8eab\u521b\u5efa\u6a21\u578b\u3002|[2401.04728v1](http://arxiv.org/pdf/2401.04728v1)|null|\n", "2401.04716": "|**2024-01-09**|**Low-Resource Vision Challenges for Foundation Models**|\u57fa\u7840\u6a21\u578b\u7684\u4f4e\u8d44\u6e90\u89c6\u89c9\u6311\u6218|Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek|Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project website: https://xiaobai1217.github.io/Low-Resource-Vision/.|\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5df2\u7ecf\u5b58\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u60c5\u51b5\uff0c\u8bb8\u591a\u8bed\u8a00\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u6765\u8fdb\u884c\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u3002\u7136\u800c\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4f4e\u8d44\u6e90\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u52aa\u529b\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63a2\u7d22\u4f4e\u8d44\u6e90\u56fe\u50cf\u4efb\u52a1\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9996\u5148\u6536\u96c6\u771f\u6b63\u7684\u4f4e\u8d44\u6e90\u56fe\u50cf\u6570\u636e\u57fa\u51c6\uff0c\u6db5\u76d6\u5386\u53f2\u5730\u56fe\u3001\u7535\u8def\u56fe\u548c\u673a\u68b0\u56fe\u7eb8\u3002\u8fd9\u4e9b\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u90fd\u9762\u4e34\u7740\u6570\u636e\u7a00\u7f3a\u3001\u7ec6\u7c92\u5ea6\u5dee\u5f02\u4ee5\u53ca\u4ece\u81ea\u7136\u56fe\u50cf\u5230\u611f\u5174\u8da3\u7684\u4e13\u4e1a\u9886\u57df\u7684\u5206\u5e03\u8f6c\u53d8\u8fd9\u4e09\u4e2a\u6311\u6218\u3002\u867d\u7136\u73b0\u6709\u7684\u57fa\u7840\u6a21\u578b\u663e\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u901a\u7528\u6027\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u5b83\u4eec\u4e0d\u80fd\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u6211\u4eec\u7684\u4f4e\u8d44\u6e90\u4efb\u52a1\u3002\u4e3a\u4e86\u5f00\u59cb\u5e94\u5bf9\u4f4e\u8d44\u6e90\u89c6\u89c9\u7684\u6311\u6218\uff0c\u6211\u4eec\u4e3a\u6bcf\u4e2a\u6311\u6218\u5f15\u5165\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u7ebf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5efa\u8baei\uff09\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6269\u5927\u6570\u636e\u7a7a\u95f4\uff0cii\uff09\u91c7\u7528\u6700\u4f73\u5b50\u5185\u6838\u5bf9\u5c40\u90e8\u533a\u57df\u8fdb\u884c\u7f16\u7801\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5dee\u5f02\u53d1\u73b0\uff0c\u4ee5\u53caiii\uff09\u5b66\u4e60\u5bf9\u4e13\u4e1a\u9886\u57df\u7684\u5173\u6ce8\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e09\u4e2a\u4f4e\u8d44\u6e90\u6570\u636e\u6e90\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5efa\u8bae\u5df2\u7ecf\u63d0\u4f9b\u4e86\u6bd4\u5e38\u89c1\u7684\u8fc1\u79fb\u5b66\u4e60\u3001\u6570\u636e\u589e\u5f3a\u548c\u7ec6\u7c92\u5ea6\u65b9\u6cd5\u66f4\u597d\u7684\u57fa\u7ebf\u3002\u8fd9\u51f8\u663e\u4e86\u57fa\u7840\u6a21\u578b\u7684\u4f4e\u8d44\u6e90\u613f\u666f\u7684\u72ec\u7279\u7279\u5f81\u548c\u6311\u6218\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://xiaobai1217.github.io/Low-Resource-Vision/\u3002|[2401.04716v1](http://arxiv.org/pdf/2401.04716v1)|null|\n", "2401.04608": "|**2024-01-09**|**EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models**|EmoGen\uff1a\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u60c5\u611f\u56fe\u50cf\u5185\u5bb9|Jingyuan Yang, Jiawei Feng, Hui Huang|Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.|\u8fd1\u5e74\u6765\uff0c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u53d6\u5f97\u4e86\u663e\u7740\u8fdb\u5c55\uff0c\u7528\u6237\u53ef\u4ee5\u521b\u5efa\u89c6\u89c9\u4e0a\u4ee4\u4eba\u60ca\u53f9\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u64c5\u957f\u751f\u6210\u5177\u4f53\u6982\u5ff5\uff08\u72d7\uff09\uff0c\u4f46\u5728\u751f\u6210\u66f4\u62bd\u8c61\u7684\u6982\u5ff5\uff08\u60c5\u611f\uff09\u65b9\u9762\u9047\u5230\u4e86\u6311\u6218\u3002\u4eba\u4eec\u5df2\u7ecf\u505a\u51fa\u4e86\u4e00\u4e9b\u52aa\u529b\u6765\u901a\u8fc7\u989c\u8272\u548c\u98ce\u683c\u8c03\u6574\u6765\u4fee\u6539\u56fe\u50cf\u60c5\u611f\uff0c\u4f46\u5728\u7528\u56fa\u5b9a\u7684\u56fe\u50cf\u5185\u5bb9\u6709\u6548\u4f20\u8fbe\u60c5\u611f\u65b9\u9762\u9762\u4e34\u7740\u5c40\u9650\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u60c5\u611f\u56fe\u50cf\u5185\u5bb9\u751f\u6210\uff08EICG\uff09\uff0c\u8fd9\u662f\u4e00\u9879\u65b0\u4efb\u52a1\uff0c\u7528\u4e8e\u5728\u7ed9\u5b9a\u60c5\u611f\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u8bed\u4e49\u6e05\u6670\u4e14\u60c5\u611f\u5fe0\u5b9e\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u60c5\u611f\u7a7a\u95f4\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6620\u5c04\u7f51\u7edc\uff0c\u5c06\u5176\u4e0e\u5f3a\u5927\u7684\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u7a7a\u95f4\u5bf9\u9f50\uff0c\u63d0\u4f9b\u4e86\u62bd\u8c61\u60c5\u611f\u7684\u5177\u4f53\u89e3\u91ca\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u5c5e\u6027\u635f\u5931\u548c\u60c5\u611f\u7f6e\u4fe1\u5ea6\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u591a\u6837\u6027\u548c\u60c5\u611f\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u65b9\u6cd5\uff0c\u6211\u4eec\u5f97\u51fa\u4e86\u4e09\u4e2a\u81ea\u5b9a\u4e49\u6307\u6807\uff0c\u5373\u60c5\u611f\u51c6\u786e\u6027\u3001\u8bed\u4e49\u6e05\u6670\u5ea6\u548c\u8bed\u4e49\u591a\u6837\u6027\u3002\u9664\u4e86\u751f\u6210\u4e4b\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u5e2e\u52a9\u60c5\u611f\u7406\u89e3\u5e76\u6fc0\u53d1\u60c5\u611f\u827a\u672f\u8bbe\u8ba1\u3002|[2401.04608v1](http://arxiv.org/pdf/2401.04608v1)|null|\n", "2401.04468": "|**2024-01-09**|**MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation**|MagicVideo-V2\uff1a\u591a\u9636\u6bb5\u9ad8\u7f8e\u89c6\u9891\u751f\u6210|Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, et.al.|The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.|\u5bf9\u4ece\u6587\u672c\u63cf\u8ff0\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 MagicVideo-V2\uff0c\u5b83\u5c06\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3001\u89c6\u9891\u8fd0\u52a8\u751f\u6210\u5668\u3001\u53c2\u8003\u56fe\u50cf\u5d4c\u5165\u6a21\u5757\u548c\u5e27\u63d2\u503c\u6a21\u5757\u96c6\u6210\u5230\u7aef\u5230\u7aef\u89c6\u9891\u751f\u6210\u7ba1\u9053\u4e2d\u3002\u53d7\u76ca\u4e8e\u8fd9\u4e9b\u67b6\u6784\u8bbe\u8ba1\uff0cMagicVideo-V2 \u53ef\u4ee5\u751f\u6210\u5177\u6709\u51fa\u8272\u4fdd\u771f\u5ea6\u548c\u5e73\u6ed1\u5ea6\u7684\u7f8e\u89c2\u3001\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u7528\u6237\u8bc4\u4f30\uff0c\u5b83\u8868\u73b0\u51fa\u4e86\u4f18\u4e8e Runway\u3001Pika 1.0\u3001Morph\u3001Moon Valley \u548c Stable Video Diffusion \u6a21\u578b\u7b49\u9886\u5148\u6587\u672c\u8f6c\u89c6\u9891\u7cfb\u7edf\u7684\u6027\u80fd\u3002|[2401.04468v1](http://arxiv.org/pdf/2401.04468v1)|null|\n", "2401.04362": "|**2024-01-09**|**Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example**|\u8349\u56fe\u63d0\u53d6\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u4ee3\u8868\u6027\u7279\u5f81\u63d0\u53d6\uff08\u4ee5\u4e00\u4f8b\u4e3a\u4f8b\uff09|Kwan Yun, Youngseo Kim, Kwanggyoon Seo, Chang Wook Seo, Junyong Noh|We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.|\u6211\u4eec\u4ecb\u7ecd DiffSketch\uff0c\u4e00\u79cd\u4ece\u56fe\u50cf\u751f\u6210\u5404\u79cd\u98ce\u683c\u5316\u8349\u56fe\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u6df1\u5c42\u7279\u5f81\u7684\u4e30\u5bcc\u8bed\u4e49\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u7279\u5f81\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u8349\u56fe\u751f\u6210\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u4e00\u5f20\u624b\u52a8\u7ed8\u56fe\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5c06\u7ecf\u8fc7\u8bad\u7ec3\u7684\u751f\u6210\u5668\u63d0\u70bc\u4e3a\u7b80\u5316\u7684\u63d0\u53d6\u5668\uff0c\u53ef\u4ee5\u786e\u4fdd\u9ad8\u6548\u7684\u8349\u56fe\u63d0\u53d6\u3002\u6211\u4eec\u901a\u8fc7\u5206\u6790\u9009\u62e9\u53bb\u566a\u6269\u6563\u7279\u5f81\uff0c\u5e76\u5c06\u8fd9\u4e9b\u9009\u5b9a\u7684\u7279\u5f81\u4e0e VAE \u7279\u5f81\u96c6\u6210\u4ee5\u751f\u6210\u8349\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u7684\u91c7\u6837\u65b9\u6848\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u6bd4\u8f83\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u84b8\u998f\u540e\u7684 DiffSketch \u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8349\u56fe\u63d0\u53d6\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u63d0\u53d6\u8349\u56fe\u7684\u4efb\u52a1\u4e2d\u4e5f\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u98ce\u683c\u5316\u65b9\u6cd5\u3002|[2401.04362v1](http://arxiv.org/pdf/2401.04362v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.04317": "|**2024-01-09**|**Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging**|\u89c6\u89c9\u91cd\u65b0\u6784\u60f3\uff1a\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684 WiFi \u5ba4\u5185\u6210\u50cf\u7a81\u7834|Jianyang Shi, Bowen Zhang, Amartansh Dubey, Ross Murch, Liwen Jing|Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance.|\u5ba4\u5185\u6210\u50cf\u662f\u673a\u5668\u4eba\u548c\u7269\u8054\u7f51\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\u3002 WiFi \u4f5c\u4e3a\u4e00\u79cd\u65e0\u6240\u4e0d\u5728\u7684\u4fe1\u53f7\uff0c\u662f\u6267\u884c\u88ab\u52a8\u6210\u50cf\u5e76\u5c06\u6700\u65b0\u4fe1\u606f\u540c\u6b65\u5230\u6240\u6709\u8fde\u63a5\u8bbe\u5907\u7684\u6709\u524d\u9014\u7684\u5019\u9009\u8005\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5c06 WiFi \u5ba4\u5185\u6210\u50cf\u89c6\u4e3a\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u5c06\u6d4b\u91cf\u7684 WiFi \u529f\u7387\u8f6c\u6362\u4e3a\u9ad8\u5206\u8fa8\u7387\u7684\u5ba4\u5185\u56fe\u50cf\u3002\u6211\u4eec\u63d0\u51fa\u7684 WiFi-GEN \u7f51\u7edc\u5b9e\u73b0\u7684\u5f62\u72b6\u91cd\u5efa\u7cbe\u5ea6\u662f\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u53cd\u6f14\u65b9\u6cd5\u7684 275%\u3002\u6b64\u5916\uff0cFrechet Inception Distance \u5206\u6570\u663e\u7740\u964d\u4f4e\u4e86 82%\u3002\u4e3a\u4e86\u68c0\u9a8c\u8be5\u4efb\u52a1\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u53d1\u5e03\u4e86\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 80,000 \u5bf9 WiFi \u4fe1\u53f7\u548c\u6210\u50cf\u76ee\u6807\u3002\u6211\u4eec\u7684\u6a21\u578b\u5438\u6536\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5305\u62ec\u5c06\u975e\u7ebf\u6027\u3001\u4e0d\u9002\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u6211\u4eec\u7684\u751f\u6210\u4eba\u5de5\u667a\u80fd\u7f51\u7edc\u7684\u5927\u91cf\u53c2\u6570\u4e2d\u3002\u8be5\u7f51\u7edc\u8fd8\u65e8\u5728\u6700\u9002\u5408\u6d4b\u91cf\u7684 WiFi \u4fe1\u53f7\u548c\u6240\u9700\u7684\u6210\u50cf\u8f93\u51fa\u3002\u4e3a\u4e86\u53ef\u91cd\u590d\u6027\uff0c\u6211\u4eec\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u6570\u636e\u548c\u4ee3\u7801\u3002|[2401.04317v1](http://arxiv.org/pdf/2401.04317v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.04718": "|**2024-01-09**|**Jump Cut Smoothing for Talking Heads**|\u8bf4\u8bdd\u5934\u50cf\u7684\u8df3\u5207\u5e73\u6ed1|Xiaojuan Wang, Taesung Park, Yang Zhou, Eli Shechtman, Richard Zhang|A jump cut offers an abrupt, sometimes unwanted change in the viewing experience. We present a novel framework for smoothing these jump cuts, in the context of talking head videos. We leverage the appearance of the subject from the other source frames in the video, fusing it with a mid-level representation driven by DensePose keypoints and face landmarks. To achieve motion, we interpolate the keypoints and landmarks between the end frames around the cut. We then use an image translation network from the keypoints and source frames, to synthesize pixels. Because keypoints can contain errors, we propose a cross-modal attention scheme to select and pick the most appropriate source amongst multiple options for each key point. By leveraging this mid-level representation, our method can achieve stronger results than a strong video interpolation baseline. We demonstrate our method on various jump cuts in the talking head videos, such as cutting filler words, pauses, and even random cuts. Our experiments show that we can achieve seamless transitions, even in the challenging cases where the talking head rotates or moves drastically in the jump cut.|\u8df3\u5207\u4f1a\u7ed9\u89c2\u770b\u4f53\u9a8c\u5e26\u6765\u7a81\u7136\u7684\u3001\u6709\u65f6\u662f\u4e0d\u5fc5\u8981\u7684\u6539\u53d8\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5934\u90e8\u8bf4\u8bdd\u89c6\u9891\u7684\u80cc\u666f\u4e0b\u5e73\u6ed1\u8fd9\u4e9b\u8df3\u5207\u3002\u6211\u4eec\u5229\u7528\u89c6\u9891\u4e2d\u5176\u4ed6\u6e90\u5e27\u4e2d\u4e3b\u9898\u7684\u5916\u89c2\uff0c\u5c06\u5176\u4e0e\u7531 DensePose \u5173\u952e\u70b9\u548c\u9762\u90e8\u6807\u5fd7\u9a71\u52a8\u7684\u4e2d\u7ea7\u8868\u793a\u878d\u5408\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd0\u52a8\uff0c\u6211\u4eec\u5728\u526a\u5207\u5468\u56f4\u7684\u7ed3\u675f\u5e27\u4e4b\u95f4\u63d2\u5165\u5173\u952e\u70b9\u548c\u5730\u6807\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5173\u952e\u70b9\u548c\u6e90\u5e27\u7684\u56fe\u50cf\u8f6c\u6362\u7f51\u7edc\u6765\u5408\u6210\u50cf\u7d20\u3002\u7531\u4e8e\u5173\u952e\u70b9\u53ef\u80fd\u5305\u542b\u9519\u8bef\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u5f0f\u6ce8\u610f\u65b9\u6848\uff0c\u4ee5\u5728\u6bcf\u4e2a\u5173\u952e\u70b9\u7684\u591a\u4e2a\u9009\u9879\u4e2d\u9009\u62e9\u6700\u5408\u9002\u7684\u6765\u6e90\u3002\u901a\u8fc7\u5229\u7528\u8fd9\u79cd\u4e2d\u7ea7\u8868\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u83b7\u5f97\u6bd4\u5f3a\u5927\u7684\u89c6\u9891\u63d2\u503c\u57fa\u7ebf\u66f4\u5f3a\u7684\u7ed3\u679c\u3002\u6211\u4eec\u5728\u5934\u90e8\u8bf4\u8bdd\u89c6\u9891\u4e2d\u7684\u5404\u79cd\u8df3\u5207\u4e0a\u6f14\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u526a\u5207\u586b\u5145\u8bcd\u3001\u505c\u987f\uff0c\u751a\u81f3\u968f\u673a\u526a\u5207\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u8df3\u8dc3\u526a\u8f91\u4e2d\u8bf4\u8bdd\u5934\u65cb\u8f6c\u6216\u5267\u70c8\u79fb\u52a8\u7684\u6311\u6218\u6027\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u8fc7\u6e21\u3002|[2401.04718v1](http://arxiv.org/pdf/2401.04718v1)|null|\n", "2401.04550": "|**2024-01-09**|**WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal**|WaveletFormerNet\uff1a\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u5c0f\u6ce2\u7f51\u7edc\uff0c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u975e\u5747\u5300\u548c\u5bc6\u96c6\u9664\u96fe|Shengli Zhang, Zhiyong Tao, Sen Lin|Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Extensive experiments demonstrate that our WaveletFormerNet performs better than state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications.|\u5c3d\u7ba1\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u53bb\u9664\u5408\u6210\u96fe\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6210\u529f\uff0c\u4f46\u5fc5\u987b\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u96fe\u6761\u4ef6\uff08\u4f8b\u5982\u6d53\u96fe\u6216\u975e\u5747\u5300\u96fe\uff09\u4e0b\u62cd\u6444\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u96fe\u5ea6\u5206\u5e03\u5f88\u590d\u6742\uff0c\u968f\u7740\u7279\u5f81\u56fe\u5206\u8fa8\u7387\u6216\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u964d\u4f4e\uff0c\u4e0b\u91c7\u6837\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8f93\u51fa\u7ed3\u679c\u4e2d\u7684\u989c\u8272\u5931\u771f\u6216\u7ec6\u8282\u4e22\u5931\u3002\u9664\u4e86\u83b7\u53d6\u8db3\u591f\u8bad\u7ec3\u6570\u636e\u7684\u6311\u6218\u4e4b\u5916\uff0c\u96fe\u56fe\u50cf\u5904\u7406\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e5f\u4f1a\u51fa\u73b0\u8fc7\u5ea6\u62df\u5408\uff0c\u8fd9\u4f1a\u9650\u5236\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u6311\u6218\u3002\u8003\u8651\u5230\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u5c0f\u6ce2\u7f51\u7edc\uff08WaveletFormerNet\uff09\uff0c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u96fe\u56fe\u50cf\u6062\u590d\u3002\u6211\u4eec\u901a\u8fc7\u63d0\u51fa WaveletFormer \u548c IWaveletFormer \u5757\u5c06\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5d4c\u5165\u5230 Vision Transformer \u4e2d\uff0c\u65e8\u5728\u51cf\u8f7b\u7531\u4e8e\u4e0b\u91c7\u6837\u800c\u5bfc\u81f4\u7684\u56fe\u50cf\u4e2d\u7684\u7eb9\u7406\u7ec6\u8282\u635f\u5931\u548c\u989c\u8272\u5931\u771f\u3002\u6211\u4eec\u5728 Transformer \u5757\u4e2d\u5f15\u5165\u5e76\u884c\u5377\u79ef\uff0c\u5b83\u5141\u8bb8\u4ee5\u8f7b\u91cf\u7ea7\u673a\u5236\u6355\u83b7\u591a\u9891\u7387\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u7279\u5f81\u805a\u5408\u6a21\u5757\uff08FAM\uff09\u6765\u4fdd\u6301\u56fe\u50cf\u5206\u8fa8\u7387\u5e76\u589e\u5f3a\u6a21\u578b\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u4f7f\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u96fe\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 WaveletFormerNet \u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5982\u5bf9\u6b21\u8981\u6a21\u578b\u590d\u6742\u6027\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u6240\u793a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u9664\u5c18\u548c\u5e94\u7528\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7684\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u5c55\u793a\u4e86 WaveletFormerNet \u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u5e94\u7528\u4e2d\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6539\u8fdb\u7684\u6027\u80fd\u3002|[2401.04550v1](http://arxiv.org/pdf/2401.04550v1)|null|\n", "2401.04486": "|**2024-01-09**|**Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks**|\u8d70\u6377\u5f84\u56de\u6765\uff1a\u51cf\u8f7b\u8bad\u7ec3\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u6d88\u5931|Yufei Guo, Yuanpei Chen|The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in our paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.|\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u662f\u4e00\u79cd\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\uff0c\u6700\u8fd1\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u5b83\u5229\u7528\u4e8c\u8fdb\u5236\u5c16\u5cf0\u6fc0\u6d3b\u6765\u4f20\u8f93\u4fe1\u606f\uff0c\u4ece\u800c\u7528\u52a0\u6cd5\u4ee3\u66ff\u4e58\u6cd5\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u80fd\u6548\u3002\u7136\u800c\uff0c\u7531\u4e8e\u653e\u7535\u5c16\u5cf0\u8fc7\u7a0b\u7684\u68af\u5ea6\u672a\u5b9a\u4e49\uff0c\u76f4\u63a5\u8bad\u7ec3 SNN \u4f1a\u5e26\u6765\u6311\u6218\u3002\u5c3d\u7ba1\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u91c7\u7528\u4e86\u5404\u79cd\u66ff\u4ee3\u68af\u5ea6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f7f\u7528\u66ff\u4ee3\u51fd\u6570\u6765\u4ee3\u66ff\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u53d1\u8fc7\u7a0b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e00\u4e2a\u5185\u5728\u95ee\u9898\uff1a\u68af\u5ea6\u6d88\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u8bba\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u6377\u7684\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e3b\u5f20\u5c06\u68af\u5ea6\u76f4\u63a5\u4ece\u635f\u5931\u4f20\u8f93\u5230\u6d45\u5c42\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u76f4\u63a5\u5c06\u68af\u5ea6\u5448\u73b0\u7ed9\u6d45\u5c42\uff0c\u4ece\u800c\u663e\u7740\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f1a\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u4efb\u4f55\u8d1f\u62c5\u3002\u4e3a\u4e86\u5728\u6700\u7ec8\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7684\u7b80\u6613\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8fdb\u5316\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u968f\u8bad\u7ec3\u5468\u671f\u52a8\u6001\u53d8\u5316\u7684\u5e73\u8861\u7cfb\u6570\u6765\u5b9e\u73b0\u5b83\uff0c\u8fd9\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7f51\u7edc\u7684\u6027\u80fd\u3002\u4f7f\u7528\u51e0\u79cd\u6d41\u884c\u7684\u7f51\u7edc\u7ed3\u6784\u5bf9\u9759\u6001\u548c\u52a8\u6001\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.04486v1](http://arxiv.org/pdf/2401.04486v1)|null|\n", "2401.04390": "|**2024-01-09**|**Learning with Noisy Labels: Interconnection of Two Expectation-Maximizations**|\u4f7f\u7528\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\uff1a\u4e24\u4e2a\u671f\u671b\u6700\u5927\u5316\u7684\u4e92\u8fde|Heewon Kim, Hyun Sung Chang, Kiho Cho, Jaeyun Lee, Bohyung Han|Labor-intensive labeling becomes a bottleneck in developing computer vision algorithms based on deep learning. For this reason, dealing with imperfect labels has increasingly gained attention and has become an active field of study. We address learning with noisy labels (LNL) problem, which is formalized as a task of finding a structured manifold in the midst of noisy data. In this framework, we provide a proper objective function and an optimization algorithm based on two expectation-maximization (EM) cycles. The separate networks associated with the two EM cycles collaborate to optimize the objective function, where one model is for distinguishing clean labels from corrupted ones while the other is for refurbishing the corrupted labels. This approach results in a non-collapsing LNL-flywheel model in the end. Experiments show that our algorithm achieves state-of-the-art performance in multiple standard benchmarks with substantial margins under various types of label noise.|\u52b3\u52a8\u5bc6\u96c6\u578b\u6807\u6ce8\u6210\u4e3a\u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u7684\u74f6\u9888\u3002\u56e0\u6b64\uff0c\u5904\u7406\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u8d8a\u6765\u8d8a\u53d7\u5230\u5173\u6ce8\uff0c\u5e76\u6210\u4e3a\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002\u6211\u4eec\u89e3\u51b3\u4e86\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\uff08LNL\uff09\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u88ab\u5f62\u5f0f\u5316\u4e3a\u5728\u566a\u58f0\u6570\u636e\u4e2d\u627e\u5230\u7ed3\u6784\u5316\u6d41\u5f62\u7684\u4efb\u52a1\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u9002\u5f53\u7684\u76ee\u6807\u51fd\u6570\u548c\u57fa\u4e8e\u4e24\u4e2a\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u5faa\u73af\u7684\u4f18\u5316\u7b97\u6cd5\u3002\u4e0e\u4e24\u4e2a EM \u5468\u671f\u76f8\u5173\u7684\u72ec\u7acb\u7f51\u7edc\u534f\u4f5c\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5176\u4e2d\u4e00\u4e2a\u6a21\u578b\u7528\u4e8e\u533a\u5206\u5e72\u51c0\u6807\u7b7e\u548c\u635f\u574f\u6807\u7b7e\uff0c\u800c\u53e6\u4e00\u4e2a\u6a21\u578b\u7528\u4e8e\u7ffb\u65b0\u635f\u574f\u6807\u7b7e\u3002\u8fd9\u79cd\u65b9\u6cd5\u6700\u7ec8\u4ea7\u751f\u4e86\u4e00\u4e2a\u4e0d\u584c\u9677\u7684 LNL \u98de\u8f6e\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u7c7b\u578b\u7684\u6807\u7b7e\u566a\u58f0\u4e0b\u5177\u6709\u5f88\u5927\u7684\u88d5\u5ea6\u3002|[2401.04390v1](http://arxiv.org/pdf/2401.04390v1)|null|\n"}, "3D/CG": {"2401.04730": "|**2024-01-09**|**A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars**|\u4f7f\u7528 3D \u5934\u50cf\u8fdb\u884c\u53e3\u8bed\u5230\u624b\u8bed\u7ffb\u8bd1\u7684\u7b80\u5355\u57fa\u7ebf|Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong|The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models will be available at https://github.com/FangyunWei/SLRT|\u672c\u6587\u7684\u76ee\u7684\u662f\u5f00\u53d1\u4e00\u4e2a\u5c06\u53e3\u8bed\u7ffb\u8bd1\u6210\u624b\u8bed\u7684\u529f\u80fd\u7cfb\u7edf\uff0c\u79f0\u4e3a Spoken2Sign \u7ffb\u8bd1\u3002 Spoken2Sign \u4efb\u52a1\u4e0e\u4f20\u7edf\u624b\u8bed\u5230\u53e3\u8bed (Sign2Spoken) \u7ffb\u8bd1\u662f\u6b63\u4ea4\u548c\u4e92\u8865\u7684\u3002\u4e3a\u4e86\u5b9e\u73b0 Spoken2Sign \u7ffb\u8bd1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u7ebf\uff0c\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u4f7f\u7528\u73b0\u6709\u7684 Sign2Spoken \u57fa\u51c6\u521b\u5efa\u6ce8\u91ca\u89c6\u9891\u8bcd\u5178\uff1b 2) \u4f30\u8ba1\u5b57\u5178\u4e2d\u6bcf\u4e2a\u624b\u52bf\u89c6\u9891\u76843D\u624b\u52bf\uff1b 3\uff09\u501f\u52a9\u751f\u6210\u7684gloss-3D\u7b26\u53f7\u5b57\u5178\u8bad\u7ec3Spoken2Sign\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7531Text2Gloss\u7ffb\u8bd1\u5668\u3001\u7b26\u53f7\u8fde\u63a5\u5668\u548c\u6e32\u67d3\u6a21\u5757\u7ec4\u6210\u3002\u7136\u540e\u7ffb\u8bd1\u7ed3\u679c\u901a\u8fc7\u7b26\u53f7\u5934\u50cf\u663e\u793a\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u4ee5 3D \u7b26\u53f7\u8f93\u51fa\u683c\u5f0f\u5448\u73b0 Spoken2Sign \u4efb\u52a1\u7684\u4eba\u3002\u9664\u4e86 Spoken2Sign \u7ffb\u8bd1\u529f\u80fd\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4e24\u4e2a\u526f\u4ea7\u54c1\u2014\u20143D \u5173\u952e\u70b9\u589e\u5f3a\u548c\u591a\u89c6\u56fe\u7406\u89e3\u2014\u2014\u53ef\u4ee5\u5e2e\u52a9\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u624b\u8bed\u7406\u89e3\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/FangyunWei/SLRT \u83b7\u53d6|[2401.04730v1](http://arxiv.org/pdf/2401.04730v1)|null|\n", "2401.04435": "|**2024-01-09**|**Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning**|\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u4e0d\u786e\u5b9a\u6027\u91c7\u6837|Kuo Yang, Duo Li, Menghan Hu, Guangtao Zhai, Xiaokang Yang, Xiao-Ping Zhang|For semi-supervised learning with imbalance classes, the long-tailed distribution of data will increase the model prediction bias toward dominant classes, undermining performance on less frequent classes. Existing methods also face challenges in ensuring the selection of sufficiently reliable pseudo-labels for model training and there is a lack of mechanisms to adjust the selection of more reliable pseudo-labels based on different training stages. To mitigate this issue, we introduce uncertainty into the modeling process for pseudo-label sampling, taking into account that the model performance on the tailed classes varies over different training stages. For example, at the early stage of model training, the limited predictive accuracy of model results in a higher rate of uncertain pseudo-labels. To counter this, we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach. This approach allows the model to perceive the uncertainty of pseudo-labels at different training stages, thereby adaptively adjusting the selection thresholds for different classes. Compared to other methods such as the baseline method FixMatch, UDTS achieves an increase in accuracy of at least approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset TissueMNIST, respectively. The source code of UDTS is publicly available at: https://github.com/yangk/UDTS.|\u5bf9\u4e8e\u4e0d\u5e73\u8861\u7c7b\u7684\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u7684\u957f\u5c3e\u5206\u5e03\u5c06\u589e\u52a0\u6a21\u578b\u5bf9\u4e3b\u5bfc\u7c7b\u7684\u9884\u6d4b\u504f\u5dee\uff0c\u4ece\u800c\u635f\u5bb3\u4e0d\u592a\u9891\u7e41\u7684\u7c7b\u7684\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u786e\u4fdd\u4e3a\u6a21\u578b\u8bad\u7ec3\u9009\u62e9\u8db3\u591f\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u65b9\u9762\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14\u7f3a\u4e4f\u6839\u636e\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u8c03\u6574\u66f4\u53ef\u9760\u4f2a\u6807\u7b7e\u7684\u9009\u62e9\u7684\u673a\u5236\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u4f2a\u6807\u7b7e\u91c7\u6837\u7684\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\uff0c\u8003\u8651\u5230\u5c3e\u7c7b\u4e0a\u7684\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u4f1a\u6709\u6240\u4e0d\u540c\u3002\u4f8b\u5982\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u65e9\u671f\u9636\u6bb5\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u6709\u9650\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u4f2a\u6807\u7b7e\u7684\u6bd4\u4f8b\u8f83\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u6001\u9608\u503c\u9009\u62e9\uff08UDTS\uff09\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u611f\u77e5\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u4f2a\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u8c03\u6574\u4e0d\u540c\u7c7b\u522b\u7684\u9009\u62e9\u9608\u503c\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5 FixMatch \u7b49\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0cUDTS \u5728\u81ea\u7136\u573a\u666f\u56fe\u50cf\u6570\u636e\u96c6 CIFAR10-LT\u3001CIFAR100-LT\u3001STL-10 \u4e0a\u5b9e\u73b0\u4e86\u81f3\u5c11\u7ea6 5.26%\u30011.75%\u30019.96% \u548c 1.28% \u7684\u7cbe\u5ea6\u63d0\u5347-LT \u548c\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6 TissueMNIST\u3002 UDTS\u7684\u6e90\u4ee3\u7801\u516c\u5f00\u5728\uff1ahttps://github.com/yangk/UDTS\u3002|[2401.04435v1](http://arxiv.org/pdf/2401.04435v1)|null|\n", "2401.04345": "|**2024-01-09**|**RomniStereo: Recurrent Omnidirectional Stereo Matching**|RomniStereo\uff1a\u5faa\u73af\u5168\u5411\u7acb\u4f53\u5339\u914d|Hualie Jiang, Rui Xu, Minglang Tan, Wenjie Jiang|Omnidirectional stereo matching (OSM) is an essential and reliable means for $360^{\\circ}$ depth sensing. However, following earlier works on conventional stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D encoder-decoder block to regularize the cost volume, causing the whole system complicated and sub-optimal results. Recently, the Recurrent All-pairs Field Transforms (RAFT) based approach employs the recurrent update in 2D and has efficiently improved image-matching tasks, \\ie, optical flow, and stereo matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite adaptive weighting scheme to seamlessly transform the outputs of spherical sweeping of OSM into the required inputs for the recurrent update, thus creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm. Furthermore, we introduce two techniques, \\ie, grid embedding and adaptive context feature generation, which also contribute to RomniStereo's performance. Our best model improves the average MAE metric by 40.7\\% over the previous SOTA baseline across five datasets. When visualizing the results, our models demonstrate clear advantages on both synthetic and realistic examples. The code is available at \\url{https://github.com/HalleyJiang/RomniStereo}.|\u5168\u5411\u7acb\u4f53\u5339\u914d (OSM) \u662f 360^{\\circ}$ \u6df1\u5ea6\u4f20\u611f\u7684\u91cd\u8981\u4e14\u53ef\u9760\u7684\u624b\u6bb5\u3002\u7136\u800c\uff0c\u7ee7\u65e9\u671f\u7684\u4f20\u7edf\u7acb\u4f53\u5339\u914d\u5de5\u4f5c\u4e4b\u540e\uff0c\u73b0\u6709\u7684\u6700\u5148\u8fdb (SOTA) \u65b9\u6cd5\u4f9d\u8d56 3D \u7f16\u7801\u5668-\u89e3\u7801\u5668\u5757\u6765\u89c4\u8303\u6210\u672c\u91cf\uff0c\u5bfc\u81f4\u6574\u4e2a\u7cfb\u7edf\u590d\u6742\u4e14\u7ed3\u679c\u6b21\u4f18\u3002\u6700\u8fd1\uff0c\u57fa\u4e8e\u5faa\u73af\u5168\u5bf9\u573a\u53d8\u6362\uff08RAFT\uff09\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e8c\u7ef4\u5faa\u73af\u66f4\u65b0\uff0c\u5e76\u6709\u6548\u5730\u6539\u8fdb\u4e86\u56fe\u50cf\u5339\u914d\u4efb\u52a1\uff0c\u5373\u5149\u6d41\u548c\u7acb\u4f53\u5339\u914d\u3002\u4e3a\u4e86\u5f25\u8865 OSM \u548c RAFT \u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u4e3b\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u53cd\u7684\u81ea\u9002\u5e94\u52a0\u6743\u65b9\u6848\uff0c\u5c06 OSM \u7403\u9762\u626b\u63cf\u7684\u8f93\u51fa\u65e0\u7f1d\u8f6c\u6362\u4e3a\u5faa\u73af\u66f4\u65b0\u6240\u9700\u7684\u8f93\u5165\uff0c\u4ece\u800c\u521b\u5efa\u5faa\u73af\u5168\u5411\u7acb\u4f53\u5339\u914d\uff08RomniStereo\uff09\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u6280\u672f\uff0c\u5373\u7f51\u683c\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u7279\u5f81\u751f\u6210\uff0c\u8fd9\u4e5f\u6709\u52a9\u4e8e RomniStereo \u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u6700\u4f73\u6a21\u578b\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747 MAE \u6307\u6807\u6bd4\u4e4b\u524d\u7684 SOTA \u57fa\u7ebf\u63d0\u9ad8\u4e86 40.7%\u3002\u5728\u53ef\u89c6\u5316\u7ed3\u679c\u65f6\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5408\u6210\u548c\u73b0\u5b9e\u793a\u4f8b\u4e0a\u90fd\u8868\u73b0\u51fa\u4e86\u660e\u663e\u7684\u4f18\u52bf\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/HalleyJiang/RomniStereo} \u83b7\u53d6\u3002|[2401.04345v1](http://arxiv.org/pdf/2401.04345v1)|null|\n"}, "\u56fe\u50cf\u7406\u89e3": {"2401.04325": "|**2024-01-09**|**RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale**|RadarCam-Depth\uff1a\u96f7\u8fbe\u76f8\u673a\u878d\u5408\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u516c\u5236\u5c3a\u5ea6\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1|Han Li, Yukai Ma, Yaqing Gu, Kewei Hu, Yong Liu, Xingxing Zuo|We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u89c6\u56fe\u56fe\u50cf\u548c\u7a00\u758f\u3001\u566a\u58f0\u96f7\u8fbe\u70b9\u4e91\u878d\u5408\u7684\u5ea6\u91cf\u5bc6\u96c6\u6df1\u5ea6\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\u3002\u5f02\u6784\u96f7\u8fbe\u548c\u56fe\u50cf\u6570\u636e\u6216\u5176\u7f16\u7801\u7684\u76f4\u63a5\u878d\u5408\u5f80\u5f80\u4f1a\u4ea7\u751f\u5177\u6709\u660e\u663e\u4f2a\u5f71\u3001\u6a21\u7cca\u8fb9\u754c\u548c\u6b21\u4f18\u7cbe\u5ea6\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5b66\u4e60\u5229\u7528\u7a00\u758f\u548c\u5608\u6742\u7684\u96f7\u8fbe\u6570\u636e\u4ea7\u751f\u7684\u5bc6\u96c6\u5ea6\u91cf\u5c3a\u5ea6\u6765\u589e\u5f3a\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f7\u8fbe\u76f8\u673a\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u548c\u7cbe\u7ec6\u7684\u5bc6\u96c6\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u3001\u5355\u76ee\u6df1\u5ea6\u4e0e\u7a00\u758f\u96f7\u8fbe\u70b9\u7684\u5168\u5c40\u5c3a\u5ea6\u5bf9\u9f50\u3001\u901a\u8fc7\u5b66\u4e60\u96f7\u8fbe\u70b9\u548c\u96f7\u8fbe\u70b9\u4e4b\u95f4\u7684\u5173\u8054\u6765\u8fdb\u884c\u51c6\u5bc6\u96c6\u5c3a\u5ea6\u4f30\u8ba1\u3002\u56fe\u50cf\u8865\u4e01\uff0c\u4ee5\u53ca\u4f7f\u7528\u6bd4\u4f8b\u56fe\u5b66\u4e60\u5668\u5bf9\u5bc6\u96c6\u6df1\u5ea6\u8fdb\u884c\u5c40\u90e8\u6bd4\u4f8b\u7ec6\u5316\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684 nuScenes \u6570\u636e\u96c6\u548c\u6211\u4eec\u81ea\u884c\u6536\u96c6\u7684 ZJU-4DRadarCam \u4e0a\u5c06\u6df1\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u964d\u4f4e\u4e86 25.6% \u548c 40.2%\uff0c\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f7\u8fbe\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u6570\u636e\u96c6\uff0c\u5206\u522b\u3002|[2401.04325v1](http://arxiv.org/pdf/2401.04325v1)|null|\n"}, "GNN": {}, "\u5176\u4ed6": {"2401.04727": "|**2024-01-09**|**Revisiting Adversarial Training at Scale**|\u91cd\u65b0\u5ba1\u89c6\u5927\u89c4\u6a21\u5bf9\u6297\u6027\u8bad\u7ec3|Zeyu Wang, Xianhang Li, Hongru Zhu, Cihang Xie|The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL.   Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at https://github.com/UCSC-VLAA/AdvXL.|\u673a\u5668\u5b66\u4e60\u793e\u533a\u89c1\u8bc1\u4e86\u8bad\u7ec3\u6d41\u7a0b\u7684\u5de8\u5927\u53d8\u5316\uff0c\u8fd9\u4e9b\u53d8\u5316\u4ee5\u89c4\u6a21\u7a7a\u524d\u7684\u201c\u57fa\u7840\u6a21\u578b\u201d\u4e3a\u4e2d\u5fc3\u3002\u7136\u800c\uff0c\u5bf9\u6297\u8bad\u7ec3\u9886\u57df\u5374\u76f8\u5bf9\u6ede\u540e\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u50cf ResNet-50 \u8fd9\u6837\u7684\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u4ee5\u53ca\u50cf CIFAR-10 \u8fd9\u6837\u7684\u5c0f\u578b\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u8f6c\u578b\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e26\u6709\u5bf9\u6297\u6027\u8bad\u7ec3\u7684\u73b0\u4ee3\u91cd\u65b0\u68c0\u9a8c\uff0c\u8c03\u67e5\u5176\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u7684\u6f5c\u5728\u597d\u5904\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u53ef\u627f\u53d7\u7684\u8ba1\u7b97\u6210\u672c\u4f7f\u7528\u5de8\u578b\u6a21\u578b\u548c\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u8fdb\u884c\u5bf9\u6297\u6027\u8bad\u7ec3\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u65b0\u5f15\u5165\u7684\u6846\u67b6\u8868\u793a\u4e3a AdvXL\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cAdvXL \u5728 ImageNet-1K \u4e0a\u7684 AutoAttack \u4e0b\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u7cbe\u5ea6\u8bb0\u5f55\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u5728 DataComp-1B \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u7684 AdvXL \u4f7f\u666e\u901a ViT-g \u6a21\u578b\u80fd\u591f\u5927\u5e45\u8d85\u8d8a $l_{\\infty}$-\u3001$l_{2}$- \u548c $l_{1} \u4e4b\u524d\u7684\u8bb0\u5f55\u7a33\u5065\u51c6\u786e\u5ea6\u5206\u522b\u4e3a 11.4%\u300114.2% \u548c 12.9%\u3002\u8fd9\u4e00\u6210\u5c31\u5c06 AdvXL \u89c6\u4e3a\u4e00\u79cd\u5f00\u521b\u6027\u65b9\u6cd5\uff0c\u4e3a\u5728\u66f4\u5927\u5c3a\u5ea6\u4e0a\u6709\u6548\u8bad\u7ec3\u7a33\u5065\u7684\u89c6\u89c9\u8868\u793a\u7ed8\u5236\u4e86\u65b0\u7684\u8f68\u8ff9\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8e https://github.com/UCSC-VLAA/AdvXL\u3002|[2401.04727v1](http://arxiv.org/pdf/2401.04727v1)|null|\n", "2401.04680": "|**2024-01-09**|**CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks**|CoordGate\uff1a\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u9ad8\u6548\u8ba1\u7b97\u7a7a\u95f4\u53d8\u5316\u7684\u5377\u79ef|Sunny Howard, Peter Norreys, Andreas D\u00f6pp|Optical imaging systems are inherently limited in their resolution due to the point spread function (PSF), which applies a static, yet spatially-varying, convolution to the image. This degradation can be addressed via Convolutional Neural Networks (CNNs), particularly through deblurring techniques. However, current solutions face certain limitations in efficiently computing spatially-varying convolutions. In this paper we propose CoordGate, a novel lightweight module that uses a multiplicative gate and a coordinate encoding network to enable efficient computation of spatially-varying convolutions in CNNs. CoordGate allows for selective amplification or attenuation of filters based on their spatial position, effectively acting like a locally connected neural network. The effectiveness of the CoordGate solution is demonstrated within the context of U-Nets and applied to the challenging problem of image deblurring. The experimental results show that CoordGate outperforms conventional approaches, offering a more robust and spatially aware solution for CNNs in various computer vision applications.|\u7531\u4e8e\u70b9\u6269\u6563\u51fd\u6570 (PSF) \u5bf9\u56fe\u50cf\u5e94\u7528\u9759\u6001\u4f46\u7a7a\u95f4\u53d8\u5316\u7684\u5377\u79ef\uff0c\u5149\u5b66\u6210\u50cf\u7cfb\u7edf\u7684\u5206\u8fa8\u7387\u672c\u8d28\u4e0a\u53d7\u5230\u9650\u5236\u3002\u8fd9\u79cd\u9000\u5316\u53ef\u4ee5\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6765\u89e3\u51b3\uff0c\u7279\u522b\u662f\u901a\u8fc7\u53bb\u6a21\u7cca\u6280\u672f\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5728\u6709\u6548\u8ba1\u7b97\u7a7a\u95f4\u53d8\u5316\u7684\u5377\u79ef\u65b9\u9762\u9762\u4e34\u67d0\u4e9b\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CoordGate\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5b83\u4f7f\u7528\u4e58\u6cd5\u95e8\u548c\u5750\u6807\u7f16\u7801\u7f51\u7edc\u6765\u5b9e\u73b0 CNN \u4e2d\u7a7a\u95f4\u53d8\u5316\u5377\u79ef\u7684\u9ad8\u6548\u8ba1\u7b97\u3002 CoordGate \u5141\u8bb8\u6839\u636e\u6ee4\u6ce2\u5668\u7684\u7a7a\u95f4\u4f4d\u7f6e\u9009\u62e9\u6027\u653e\u5927\u6216\u8870\u51cf\u6ee4\u6ce2\u5668\uff0c\u6709\u6548\u5730\u5145\u5f53\u672c\u5730\u8fde\u63a5\u7684\u795e\u7ecf\u7f51\u7edc\u3002 CoordGate \u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u5728 U-Net \u7684\u80cc\u666f\u4e0b\u5f97\u5230\u4e86\u8bc1\u660e\uff0c\u5e76\u5e94\u7528\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoordGate \u7684\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684 CNN \u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u4e14\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002|[2401.04680v1](http://arxiv.org/pdf/2401.04680v1)|null|\n", "2401.04560": "|**2024-01-09**|**Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video**|\u76f8\u79fb\u8fdc\u7a0b\u5149\u7535\u4f53\u79ef\u63cf\u8bb0\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u9762\u90e8\u89c6\u9891\u4f30\u7b97\u5fc3\u7387\u548c\u8840\u538b|Gyutae Hwang, Sang Jun Lee|Human health can be critically affected by cardiovascular diseases, such as hypertension, arrhythmias, and stroke. Heart rate and blood pressure are important biometric information for the monitoring of cardiovascular system and early diagnosis of cardiovascular diseases. Existing methods for estimating the heart rate are based on electrocardiography and photoplethyomography, which require contacting the sensor to the skin surface. Moreover, catheter and cuff-based methods for measuring blood pressure cause inconvenience and have limited applicability. Therefore, in this thesis, we propose a vision-based method for estimating the heart rate and blood pressure. This thesis proposes a 2-stage deep learning framework consisting of a dual remote photoplethysmography network (DRP-Net) and bounded blood pressure network (BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography (rPPG) signals for the acral and facial regions, and these phase-shifted rPPG signals are utilized to estimate the heart rate. In the second stage, BBP-Net integrates temporal features and analyzes phase discrepancy between the acral and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy of estimating the heart rate, we employed a data augmentation method based on a frame interpolation model. Moreover, we designed BBP-Net to infer blood pressure within a predefined range by incorporating a scaled sigmoid function. Our method resulted in estimating the heart rate with the mean absolute error (MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method, on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure (SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64 mmHg, and 9.4 mmHg, respectively.|\u4eba\u7c7b\u5065\u5eb7\u53ef\u80fd\u53d7\u5230\u9ad8\u8840\u538b\u3001\u5fc3\u5f8b\u5931\u5e38\u548c\u4e2d\u98ce\u7b49\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u5fc3\u7387\u548c\u8840\u538b\u662f\u5fc3\u8840\u7ba1\u7cfb\u7edf\u76d1\u6d4b\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u65e9\u671f\u8bca\u65ad\u7684\u91cd\u8981\u751f\u7269\u8bc6\u522b\u4fe1\u606f\u3002\u73b0\u6709\u7684\u4f30\u8ba1\u5fc3\u7387\u7684\u65b9\u6cd5\u57fa\u4e8e\u5fc3\u7535\u56fe\u548c\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\u672f\uff0c\u8fd9\u9700\u8981\u5c06\u4f20\u611f\u5668\u63a5\u89e6\u76ae\u80a4\u8868\u9762\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u5bfc\u7ba1\u548c\u8896\u5e26\u7684\u8840\u538b\u6d4b\u91cf\u65b9\u6cd5\u9020\u6210\u4e0d\u4fbf\u5e76\u4e14\u9002\u7528\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u5728\u672c\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5fc3\u7387\u548c\u8840\u538b\u4f30\u8ba1\u65b9\u6cd5\u3002\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u53cc\u8fdc\u7a0b\u5149\u7535\u4f53\u79ef\u63cf\u8bb0\u7f51\u7edc\uff08DRP-Net\uff09\u548c\u6709\u754c\u8840\u538b\u7f51\u7edc\uff08BBP-Net\uff09\u7ec4\u6210\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0cDRP-Net \u63a8\u65ad\u80a2\u7aef\u548c\u9762\u90e8\u533a\u57df\u7684\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\u6cd5 (rPPG) \u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u76f8\u79fb rPPG \u4fe1\u53f7\u6765\u4f30\u8ba1\u5fc3\u7387\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0cBBP-Net \u96c6\u6210\u65f6\u95f4\u7279\u5f81\u5e76\u5206\u6790\u80a2\u7aef\u548c\u9762\u90e8 rPPG \u4fe1\u53f7\u4e4b\u95f4\u7684\u76f8\u4f4d\u5dee\u5f02\uff0c\u4ee5\u4f30\u8ba1 SBP \u548c DBP \u503c\u3002\u4e3a\u4e86\u63d0\u9ad8\u4f30\u8ba1\u5fc3\u7387\u7684\u51c6\u786e\u6027\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u57fa\u4e8e\u5e27\u63d2\u503c\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86 BBP-Net\uff0c\u901a\u8fc7\u7ed3\u5408\u7f29\u653e\u7684 sigmoid \u51fd\u6570\u6765\u63a8\u65ad\u9884\u5b9a\u4e49\u8303\u56f4\u5185\u7684\u8840\u538b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 MMSE-HR \u6570\u636e\u96c6\u4e0a\u4f30\u8ba1\u5fc3\u7387\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u4e3a 1.78 BPM\uff0c\u4e0e\u6700\u8fd1\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cMAE \u964d\u4f4e\u4e86 34.31%\u3002\u4f30\u8ba1\u6536\u7f29\u538b\uff08SBP\uff09\u548c\u8212\u5f20\u538b\uff08DBP\uff09\u7684 MAE \u5206\u522b\u4e3a 10.19 mmHg \u548c 7.09 mmHg\u3002\u5728 V4V \u6570\u636e\u96c6\u4e0a\uff0c\u5fc3\u7387\u3001SBP \u548c DBP \u7684 MAE \u5206\u522b\u4e3a 3.83 BPM\u300113.64 mmHg \u548c 9.4 mmHg\u3002|[2401.04560v1](http://arxiv.org/pdf/2401.04560v1)|null|\n", "2401.04377": "|**2024-01-09**|**Towards Real-World Aerial Vision Guidance with Categorical 6D Pose Tracker**|\u901a\u8fc7\u5206\u7c7b 6D \u59ff\u52bf\u8ddf\u8e2a\u5668\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u7a7a\u4e2d\u89c6\u89c9\u5f15\u5bfc|Jingtao Sun, Yaonan Wang, Danwei Wang|Tracking the object 6-DoF pose is crucial for various downstream robot tasks and real-world applications. In this paper, we investigate the real-world robot task of aerial vision guidance for aerial robotics manipulation, utilizing category-level 6-DoF pose tracking. Aerial conditions inevitably introduce special challenges, such as rapid viewpoint changes in pitch and roll. To support this task and challenge, we firstly introduce a robust category-level 6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal prior knowledge to explore optimal inter-frame keypoint pairs, generated under a priori structural adaptive supervision in a coarse-to-fine manner. Notably, our Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the problems of the inter-frame differences and intra-class shape variations through both temporal dynamic filtering and shape-similarity filtering. We further present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a decoupling approach to implement the final aerial vision guidance task. It contains two servo action policies to better accommodate the structural properties of aerial robotics manipulation. Exhaustive experiments on four well-known public benchmarks demonstrate the superiority of our Robust6DoF. Real-world tests directly verify that our Robust6DoF along with PAD-Servo can be readily used in real-world aerial robotic applications.|\u8ddf\u8e2a\u7269\u4f53 6-DoF \u4f4d\u59ff\u5bf9\u4e8e\u5404\u79cd\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u7c7b\u522b\u7ea7 6-DoF \u4f4d\u59ff\u8ddf\u8e2a\uff0c\u7814\u7a76\u4e86\u7528\u4e8e\u7a7a\u4e2d\u673a\u5668\u4eba\u64cd\u7eb5\u7684\u7a7a\u4e2d\u89c6\u89c9\u5f15\u5bfc\u7684\u73b0\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u3002\u7a7a\u4e2d\u6761\u4ef6\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5e26\u6765\u7279\u6b8a\u7684\u6311\u6218\uff0c\u4f8b\u5982\u4fef\u4ef0\u548c\u6a2a\u6eda\u7684\u5feb\u901f\u89c6\u70b9\u53d8\u5316\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u9879\u4efb\u52a1\u548c\u6311\u6218\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e00\u4e2a\u5f3a\u5927\u7684\u7c7b\u522b\u7ea7 6-DoF \u59ff\u52bf\u8ddf\u8e2a\u5668\uff08Robust6DoF\uff09\u3002\u8be5\u8ddf\u8e2a\u5668\u5229\u7528\u5f62\u72b6\u548c\u65f6\u95f4\u5148\u9a8c\u77e5\u8bc6\u6765\u63a2\u7d22\u6700\u4f73\u7684\u5e27\u95f4\u5173\u952e\u70b9\u5bf9\uff0c\u8fd9\u4e9b\u5173\u952e\u70b9\u5bf9\u662f\u5728\u5148\u9a8c\u7ed3\u6784\u81ea\u9002\u5e94\u76d1\u7763\u4e0b\u4ee5\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u751f\u6210\u7684\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684 Robust6DoF \u91c7\u7528\u65f6\u7a7a\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u52a8\u6001\u8fc7\u6ee4\u548c\u5f62\u72b6\u76f8\u4f3c\u6027\u8fc7\u6ee4\u6765\u5904\u7406\u5e27\u95f4\u5dee\u5f02\u548c\u7c7b\u5185\u5f62\u72b6\u53d8\u5316\u7684\u95ee\u9898\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u59ff\u6001\u611f\u77e5\u79bb\u6563\u4f3a\u670d\u7b56\u7565\uff08PAD-Servo\uff09\uff0c\u4f5c\u4e3a\u5b9e\u73b0\u6700\u7ec8\u822a\u7a7a\u89c6\u89c9\u5f15\u5bfc\u4efb\u52a1\u7684\u89e3\u8026\u65b9\u6cd5\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u4f3a\u670d\u52a8\u4f5c\u7b56\u7565\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u7a7a\u4e2d\u673a\u5668\u4eba\u64cd\u7eb5\u7684\u7ed3\u6784\u7279\u6027\u3002\u5bf9\u56db\u4e2a\u8457\u540d\u516c\u5171\u57fa\u51c6\u7684\u8be6\u5c3d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec Robust6DoF \u7684\u4f18\u8d8a\u6027\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u6d4b\u8bd5\u76f4\u63a5\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684 Robust6DoF \u4e0e \u200b\u200bPAD-Servo \u53ef\u4ee5\u8f7b\u677e\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u7a7a\u4e2d\u673a\u5668\u4eba\u5e94\u7528\u3002|[2401.04377v1](http://arxiv.org/pdf/2401.04377v1)|null|\n", "2401.04332": "|**2024-01-09**|**Mix-GENEO: A flexible filtration for multiparameter persistent homology detects digital images**|Mix-GENEO\uff1a\u7528\u4e8e\u591a\u53c2\u6570\u6301\u4e45\u540c\u6e90\u6027\u68c0\u6d4b\u6570\u5b57\u56fe\u50cf\u7684\u7075\u6d3b\u8fc7\u6ee4|Jiaxing He, Bingzhe Hou, Tieru Wu, Yue Xin|Two important problems in the field of Topological Data Analysis are defining practical multifiltrations on objects and showing ability of TDA to detect the geometry. Motivated by the problems, we constuct three multifiltrations named multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the interleaving distance and multiparameter persistence landscape of multi-GENEO with respect to the pseudometric of the subspace of bounded functions. We also give the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we provide experiment results on MNIST dataset to demonstrate our bifiltrations have ability to detect geometric and topological differences of digital images.|\u62d3\u6251\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u4e24\u4e2a\u91cd\u8981\u95ee\u9898\u662f\u5b9a\u4e49\u5bf9\u8c61\u7684\u5b9e\u9645\u591a\u91cd\u8fc7\u6ee4\u548c\u663e\u793a TDA \u68c0\u6d4b\u51e0\u4f55\u5f62\u72b6\u7684\u80fd\u529b\u3002\u53d7\u8fd9\u4e9b\u95ee\u9898\u7684\u542f\u53d1\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e09\u79cd\u591a\u91cd\u8fc7\u6ee4\uff1amulti-GENEO\u3001multi-DGENEO \u548c mix-GENEO\uff0c\u5e76\u8bc1\u660e\u4e86 multi-GENEO \u7684\u4ea4\u9519\u8ddd\u79bb\u548c\u591a\u53c2\u6570\u6301\u4e45\u6027\u666f\u89c2\u76f8\u5bf9\u4e8e\u6709\u754c\u51fd\u6570\u5b50\u7a7a\u95f4\u7684\u4f2a\u5ea6\u91cf\u7684\u7a33\u5b9a\u6027\u3002\u6211\u4eec\u8fd8\u7ed9\u51fa\u4e86 multi-DGENEO \u548c mix-GENEO \u7684\u4e0a\u9650\u4f30\u8ba1\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86 MNIST \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4ee5\u8bc1\u660e\u6211\u4eec\u7684\u53cc\u8fc7\u6ee4\u80fd\u591f\u68c0\u6d4b\u6570\u5b57\u56fe\u50cf\u7684\u51e0\u4f55\u548c\u62d3\u6251\u5dee\u5f02\u3002|[2401.04332v1](http://arxiv.org/pdf/2401.04332v1)|null|\n", "2401.04290": "|**2024-01-09**|**StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments**|StarCraftImage\uff1a\u7528\u4e8e\u591a\u4ee3\u7406\u73af\u5883\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u539f\u578b\u8bbe\u8ba1\u7684\u6570\u636e\u96c6|Sean Kulinski, Nicholas R. Waytowich, James Z. Hare, David I. Inouye|Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com|\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff08\u4f8b\u5982\u4e8b\u4ef6\u9884\u6d4b\u3001\u667a\u80fd\u4f53\u7c7b\u578b\u8bc6\u522b\u6216\u7f3a\u5931\u6570\u636e\u63d2\u8865\uff09\u5bf9\u4e8e\u591a\u79cd\u5e94\u7528\u975e\u5e38\u91cd\u8981\uff08\u4f8b\u5982\uff0c\u4f20\u611f\u5668\u7f51\u7edc\u7684\u81ea\u4e3b\u76d1\u89c6\u548c\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u5b50\u4efb\u52a1\uff09\u3002 \u300a\u661f\u9645\u4e89\u9738 II\u300b\u6e38\u620f\u91cd\u64ad\u7f16\u7801\u667a\u80fd\uff08\u548c\u5bf9\u6297\u6027\uff09\u591a\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u5e76\u53ef\u4ee5\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u63d0\u4f9b\u6d4b\u8bd5\u5e73\u53f0\uff1b\u7136\u800c\uff0c\u63d0\u53d6\u7b80\u5355\u4e14\u6807\u51c6\u5316\u7684\u8868\u793a\u6765\u5bf9\u8fd9\u4e9b\u4efb\u52a1\u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u975e\u5e38\u8d39\u529b\u5e76\u4e14\u963b\u788d\u4e86\u53ef\u91cd\u590d\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cMNIST \u548c CIFAR10 \u5c3d\u7ba1\u6781\u5176\u7b80\u5355\uff0c\u5374\u80fd\u591f\u5b9e\u73b0 ML \u65b9\u6cd5\u7684\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u53ef\u91cd\u590d\u6027\u3002\u9075\u5faa\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u7b80\u5355\u6027\uff0c\u6211\u4eec\u57fa\u4e8e\u300a\u661f\u9645\u4e89\u9738 II\u300b\u56de\u653e\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u8868\u73b0\u51fa\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u540c\u65f6\u4ecd\u7136\u50cf MNIST \u548c CIFAR10 \u4e00\u6837\u6613\u4e8e\u4f7f\u7528\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ed4\u7ec6\u603b\u7ed3\u4e86 255 \u4e2a\u8fde\u7eed\u6e38\u620f\u72b6\u6001\u7684\u7a97\u53e3\uff0c\u4ece 60,000 \u6b21\u91cd\u64ad\u4e2d\u521b\u5efa 360 \u4e07\u5f20\u6458\u8981\u56fe\u50cf\uff0c\u5305\u62ec\u6240\u6709\u76f8\u5173\u5143\u6570\u636e\uff0c\u4f8b\u5982\u6e38\u620f\u7ed3\u679c\u548c\u73a9\u5bb6\u7ade\u8d5b\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e09\u79cd\u964d\u4f4e\u590d\u6742\u6027\u7684\u683c\u5f0f\uff1a\u6bcf\u79cd\u5355\u4f4d\u7c7b\u578b\u90fd\u6709\u4e00\u4e2a\u901a\u9053\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\uff08\u7c7b\u4f3c\u4e8e\u591a\u5149\u8c31\u5730\u7406\u7a7a\u95f4\u56fe\u50cf\uff09\u3001\u6a21\u4eff CIFAR10 \u7684 RGB \u56fe\u50cf\u4ee5\u53ca\u6a21\u4eff MNIST \u7684\u7070\u5ea6\u56fe\u50cf\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u6765\u6784\u5efa\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u7684\u539f\u578b\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u63d0\u53d6\u4ee3\u7801\u548c\u6570\u636e\u96c6\u52a0\u8f7d\u4ee3\u7801\u90fd\u53ef\u4ee5\u5728 https://starcraftdata.davidinouye.com \u627e\u5230|[2401.04290v1](http://arxiv.org/pdf/2401.04290v1)|null|\n"}}