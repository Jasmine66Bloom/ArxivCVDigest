{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2408.12590": "|**2024-08-22**|**xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations**|xGen-VideoSyn-1\uff1a\u91c7\u7528\u538b\u7f29\u8868\u793a\u7684\u9ad8\u4fdd\u771f\u6587\u672c\u5230\u89c6\u9891\u5408\u6210|Can Qin, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu, Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, Jun Wang, et.al.|We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of producing realistic scenes from textual descriptions. Building on recent advancements, such as OpenAI's Sora, we explore the latent diffusion model (LDM) architecture and introduce a video variational autoencoder (VidVAE). VidVAE compresses video data both spatially and temporally, significantly reducing the length of visual tokens and the computational demands associated with generating long-sequence videos. To further address the computational costs, we propose a divide-and-merge strategy that maintains temporal consistency across video segments. Our Diffusion Transformer (DiT) model incorporates spatial and temporal self-attention layers, enabling robust generalization across different timeframes and aspect ratios. We have devised a data processing pipeline from the very beginning and collected over 13M high-quality video-text pairs. The pipeline includes multiple steps such as clipping, text detection, motion estimation, aesthetics scoring, and dense captioning based on our in-house video-LLM model. Training the VidVAE and DiT models required approximately 40 and 642 H100 days, respectively. Our model supports over 14-second 720p video generation in an end-to-end way and demonstrates competitive performance against state-of-the-art T2V models.|\u6211\u4eec\u63d0\u51fa\u4e86 xGen-VideoSyn-1\uff0c\u8fd9\u662f\u4e00\u79cd\u6587\u672c\u5230\u89c6\u9891 (T2V) \u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u903c\u771f\u7684\u573a\u666f\u3002\u57fa\u4e8e OpenAI \u7684 Sora \u7b49\u6700\u65b0\u8fdb\u5c55\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u67b6\u6784\u5e76\u5f15\u5165\u4e86\u89c6\u9891\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (VidVAE)\u3002VidVAE \u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u538b\u7f29\u89c6\u9891\u6570\u636e\uff0c\u663e\u7740\u51cf\u5c11\u4e86\u89c6\u89c9\u6807\u8bb0\u7684\u957f\u5ea6\u548c\u4e0e\u751f\u6210\u957f\u5e8f\u5217\u89c6\u9891\u76f8\u5173\u7684\u8ba1\u7b97\u9700\u6c42\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u89e3\u51b3\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u800c\u5408\u5e76\u7b56\u7565\uff0c\u4ee5\u4fdd\u6301\u89c6\u9891\u7247\u6bb5\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u6269\u6563\u53d8\u6362\u5668 (DiT) \u6a21\u578b\u7ed3\u5408\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u548c\u7eb5\u6a2a\u6bd4\u7684\u7a33\u5065\u6cdb\u5316\u3002\u6211\u4eec\u4ece\u4e00\u5f00\u59cb\u5c31\u8bbe\u8ba1\u4e86\u4e00\u6761\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u5e76\u6536\u96c6\u4e86\u8d85\u8fc7 1300 \u4e07\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u6587\u672c\u5bf9\u3002\u8be5\u7ba1\u9053\u5305\u62ec\u591a\u4e2a\u6b65\u9aa4\uff0c\u4f8b\u5982\u57fa\u4e8e\u6211\u4eec\u5185\u90e8\u89c6\u9891 LLM \u6a21\u578b\u7684\u526a\u8f91\u3001\u6587\u672c\u68c0\u6d4b\u3001\u8fd0\u52a8\u4f30\u8ba1\u3001\u7f8e\u5b66\u8bc4\u5206\u548c\u5bc6\u96c6\u5b57\u5e55\u3002\u8bad\u7ec3 VidVAE \u548c DiT \u6a21\u578b\u5206\u522b\u9700\u8981\u5927\u7ea6 40 \u548c 642 \u4e2a H100 \u5929\u3002\u6211\u4eec\u7684\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u652f\u6301\u8d85\u8fc7 14 \u79d2\u7684 720p \u89c6\u9891\u751f\u6210\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684 T2V \u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002|[2408.12590v1](http://arxiv.org/pdf/2408.12590v1)|null|\n", "2408.12575": "|**2024-08-22**|**Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers**|\u901a\u8fc7\u591a\u4efb\u52a1\u9c7c\u773c\u4ea4\u53c9\u89c6\u56fe Transformers \u589e\u5f3a\u505c\u8f66\u611f\u77e5|Antonyo Musabini, Ivan Novikov, Sana Soula, Christel Leonet, Lihao Wang, Rachid Benmokhtar, Fabian Burger, Thomas Boulay, Xavier Perrotton|Current parking area perception algorithms primarily focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a complete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multihead attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehicles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: https://streamable.com/jjw54x.|\u5f53\u524d\u7684\u505c\u8f66\u573a\u611f\u77e5\u7b97\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5728\u6709\u9650\u8303\u56f4\u5185\u68c0\u6d4b\u7a7a\u4f4d\uff0c\u4f9d\u9760\u5bb9\u6613\u51fa\u9519\u7684\u5355\u5e94\u6295\u5f71\u8fdb\u884c\u6807\u8bb0\u548c\u63a8\u7406\u3002\u7136\u800c\uff0c\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf (ADAS) \u7684\u6700\u65b0\u8fdb\u5c55\u9700\u8981\u901a\u8fc7\u5168\u9762\u800c\u667a\u80fd\u7684\u4eba\u673a\u754c\u9762 (HMI) \u4e0e\u6700\u7ec8\u7528\u6237\u8fdb\u884c\u4ea4\u4e92\u3002\u8fd9\u4e9b\u754c\u9762\u5e94\u8be5\u63d0\u4f9b\u5bf9\u505c\u8f66\u573a\u7684\u5b8c\u6574\u611f\u77e5\uff0c\u4ece\u533a\u5206\u7a7a\u4f4d\u7684\u5165\u53e3\u7ebf\u5230\u5176\u4ed6\u505c\u653e\u8f66\u8f86\u7684\u65b9\u5411\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u4efb\u52a1\u9c7c\u773c\u4ea4\u53c9\u89c6\u56fe\u53d8\u6362\u5668 (MT F-CVT)\uff0c\u5b83\u5229\u7528\u5177\u6709\u591a\u5934\u6ce8\u610f\u7684\u56db\u6444\u50cf\u5934\u9c7c\u773c\u73af\u89c6\u6444\u50cf\u5934\u7cfb\u7edf (SVCS) \u7684\u529f\u80fd\u6765\u521b\u5efa\u8be6\u7ec6\u7684\u9e1f\u77b0\u89c6\u56fe (BEV) \u7f51\u683c\u7279\u5f81\u56fe\u3002\u7279\u5f81\u7531\u5206\u5272\u89e3\u7801\u5668\u548c\u57fa\u4e8e Polygon-Yolo \u7684\u505c\u8f66\u4f4d\u548c\u8f66\u8f86\u5bf9\u8c61\u68c0\u6d4b\u89e3\u7801\u5668\u5904\u7406\u3002\u4f7f\u7528 LiDAR \u6807\u8bb0\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u540e\uff0cMT F-CVT \u53ef\u4ee5\u5728 25m x 25m \u7684\u771f\u5b9e\u5f00\u653e\u9053\u8def\u573a\u666f\u4e2d\u5b9a\u4f4d\u7269\u4f53\uff0c\u5e73\u5747\u8bef\u5dee\u4ec5\u4e3a 20 \u5398\u7c73\u3002\u6211\u4eec\u7684\u5927\u578b\u6a21\u578b\u7684 F-1 \u5f97\u5206\u4e3a 0.89\u3002\u6b64\u5916\uff0c\u5c0f\u578b\u6a21\u578b\u5728 Nvidia Jetson Orin \u5d4c\u5165\u5f0f\u677f\u4e0a\u4ee5 16 fps \u8fd0\u884c\uff0c\u68c0\u6d4b\u7ed3\u679c\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u4f3c\u3002MT F-CVT \u5c55\u793a\u4e86\u8de8\u4e0d\u540c\u8f66\u8f86\u548c\u6444\u50cf\u673a\u914d\u7f6e\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002\u6765\u81ea\u672a\u89c1\u8fc7\u7684\u8f66\u8f86\u548c\u6444\u50cf\u673a\u7684\u6f14\u793a\u89c6\u9891\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://streamable.com/jjw54x\u3002|[2408.12575v1](http://arxiv.org/pdf/2408.12575v1)|null|\n", "2408.12569": "|**2024-08-22**|**Sapiens: Foundation for Human Vision Models**|\u667a\u4eba\uff1a\u4eba\u7c7b\u89c6\u89c9\u6a21\u578b\u7684\u57fa\u7840|Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito|We present Sapiens, a family of models for four fundamental human-centric vision tasks - 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability - model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error.|\u6211\u4eec\u63a8\u51fa\u4e86 Sapiens\uff0c\u8fd9\u662f\u4e00\u7ec4\u7528\u4e8e\u56db\u9879\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u7684\u6a21\u578b - 2D \u59ff\u52bf\u4f30\u8ba1\u3001\u8eab\u4f53\u90e8\u4f4d\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\u3002\u6211\u4eec\u7684\u6a21\u578b\u539f\u751f\u652f\u6301 1K \u9ad8\u5206\u8fa8\u7387\u63a8\u7406\uff0c\u5e76\u4e14\u975e\u5e38\u5bb9\u6613\u901a\u8fc7\u7b80\u5355\u5730\u5fae\u8c03\u5728\u8d85\u8fc7 3 \u4ebf\u5f20\u91ce\u751f\u4eba\u7c7b\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u9002\u5e94\u5355\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u7ed9\u5b9a\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7cbe\u9009\u7684\u4eba\u7c7b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u9ad8\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5404\u79cd\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5373\u4f7f\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u6216\u5b8c\u5168\u5408\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u5bf9\u91ce\u751f\u6570\u636e\u7684\u663e\u8457\u6cdb\u5316\u3002\u6211\u4eec\u7b80\u5355\u7684\u6a21\u578b\u8bbe\u8ba1\u8fd8\u5e26\u6765\u4e86\u53ef\u6269\u5c55\u6027 - \u968f\u7740\u6211\u4eec\u5c06\u53c2\u6570\u6570\u91cf\u4ece 0.3 \u6269\u5c55\u5230 20 \u4ebf\uff0c\u8de8\u4efb\u52a1\u7684\u6a21\u578b\u6027\u80fd\u5f97\u5230\u63d0\u9ad8\u3002Sapiens \u5728\u5404\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u4e2d\u59cb\u7ec8\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002\u4e0e\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c\u76f8\u6bd4\uff0c\u6211\u4eec\u5728 Humans-5K\uff08\u59ff\u52bf\uff09\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u63d0\u9ad8\u4e86 7.6 mAP\uff0c\u5728 Humans-2K\uff08\u90e8\u5206\u5206\u6bb5\uff09\u4e0a\u53d6\u5f97\u4e86 17.1 mIoU\uff0c\u5728 Hi4D\uff08\u6df1\u5ea6\uff09\u4e0a\u53d6\u5f97\u4e86 22.4% \u7684\u76f8\u5bf9 RMSE\uff0c\u5728 THuman2\uff08\u6b63\u5e38\uff09\u4e0a\u53d6\u5f97\u4e86 53.5% \u7684\u76f8\u5bf9\u89d2\u5ea6\u8bef\u5dee\u3002|[2408.12569v1](http://arxiv.org/pdf/2408.12569v1)|null|\n", "2408.12568": "|**2024-08-22**|**Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers**|\u91cd\u65b0\u5ba1\u89c6\u901a\u8fc7\u89e3\u91ca\u8fdb\u884c\u4fee\u526a\uff1a\u4f18\u5316\u5f52\u56e0\u65b9\u6cd5\u4ee5\u4fee\u526a CNN \u548c Transformer|Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin|To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of eXplainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer- and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at $\\href{https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch}{\\text{this https link}}$.|\u4e3a\u4e86\u89e3\u51b3\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u95ee\u9898\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u88ab\u6269\u5c55\u5230\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u51cf\u5c11\u8ba1\u7b97\u8981\u6c42\u548c\u63d0\u9ad8\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u662f\u4fee\u526a\u8fd9\u4e9b\u901a\u5e38\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u7f51\u7edc\u4e2d\u4e0d\u5fc5\u8981\u7684\u7ec4\u4ef6\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u6765\u81ea\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u5f52\u56e0\u65b9\u6cd5\u662f\u4ee5\u5c11\u91cf\u65b9\u5f0f\u63d0\u53d6\u548c\u4fee\u526a\u6700\u4e0d\u76f8\u5173\u7684\u7f51\u7edc\u7ec4\u4ef6\u7684\u6709\u6548\u624b\u6bb5\u3002\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u660e\u786e\u4f18\u5316\u5f52\u56e0\u65b9\u6cd5\u7684\u8d85\u53c2\u6570\u4ee5\u5b8c\u6210\u4fee\u526a\u4efb\u52a1\u6765\u6269\u5c55\u5f53\u524d\u72b6\u6001\uff0c\u5e76\u5728\u6211\u4eec\u7684\u5206\u6790\u4e2d\u8fdb\u4e00\u6b65\u5305\u62ec\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f51\u7edc\u3002\u4e0e\u4e4b\u524d\u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u53d8\u538b\u5668\u548c\u5377\u79ef\u67b6\u6784\uff08VGG\u3001ResNet\u3001ViT\uff09\u7684\u6a21\u578b\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4ecd\u5728 ImageNet \u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u9ad8\u6027\u80fd\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u6bd4\uff0c\u53d8\u538b\u5668\u7684\u8fc7\u5ea6\u53c2\u6570\u5316\u7a0b\u5ea6\u66f4\u9ad8\u3002\u4ee3\u7801\u53ef\u5728$\\href{https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch}{\\text{\u6b64 https \u94fe\u63a5}}$\u83b7\u53d6\u3002|[2408.12568v1](http://arxiv.org/pdf/2408.12568v1)|null|\n", "2408.12550": "|**2024-08-22**|**Comparing YOLOv5 Variants for Vehicle Detection: A Performance Analysis**|\u6bd4\u8f83 YOLOv5 \u8f66\u8f86\u68c0\u6d4b\u53d8\u4f53\uff1a\u6027\u80fd\u5206\u6790|Athulya Sundaresan Geetha|Vehicle detection is an important task in the management of traffic and automatic vehicles. This study provides a comparative analysis of five YOLOv5 variants, YOLOv5n6s, YOLOv5s6s, YOLOv5m6s, YOLOv5l6s, and YOLOv5x6s, for vehicle detection in various environments. The research focuses on evaluating the effectiveness of these models in detecting different types of vehicles, such as Car, Bus, Truck, Bicycle, and Motorcycle, under varying conditions including lighting, occlusion, and weather. Performance metrics such as precision, recall, F1-score, and mean Average Precision are utilized to assess the accuracy and reliability of each model. YOLOv5n6s demonstrated a strong balance between precision and recall, particularly in detecting Cars. YOLOv5s6s and YOLOv5m6s showed improvements in recall, enhancing their ability to detect all relevant objects. YOLOv5l6s, with its larger capacity, provided robust performance, especially in detecting Cars, but not good with identifying Motorcycles and Bicycles. YOLOv5x6s was effective in recognizing Buses and Cars but faced challenges with Motorcycle class.|\u8f66\u8f86\u68c0\u6d4b\u662f\u4ea4\u901a\u548c\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7ba1\u7406\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\u3002\u672c\u7814\u7a76\u5bf9 YOLOv5 \u7684\u4e94\u4e2a\u53d8\u4f53 YOLOv5n6s\u3001YOLOv5s6s\u3001YOLOv5m6s\u3001YOLOv5l6s \u548c YOLOv5x6s \u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u68c0\u6d4b\u60c5\u51b5\u3002\u672c\u7814\u7a76\u91cd\u70b9\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u5149\u7167\u3001\u906e\u6321\u548c\u5929\u6c14\u7b49\u4e0d\u540c\u6761\u4ef6\u4e0b\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u8f66\u8f86\uff08\u5982\u6c7d\u8f66\u3001\u516c\u5171\u6c7d\u8f66\u3001\u5361\u8f66\u3001\u81ea\u884c\u8f66\u548c\u6469\u6258\u8f66\uff09\u7684\u6709\u6548\u6027\u3002\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1 \u5206\u6570\u548c\u5e73\u5747\u7cbe\u5ea6\u7b49\u6027\u80fd\u6307\u6807\u7528\u4e8e\u8bc4\u4f30\u6bcf\u4e2a\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002YOLOv5n6s \u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u6c7d\u8f66\u65b9\u9762\u3002YOLOv5s6s \u548c YOLOv5m6s \u5728\u53ec\u56de\u7387\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u589e\u5f3a\u4e86\u5b83\u4eec\u68c0\u6d4b\u6240\u6709\u76f8\u5173\u7269\u4f53\u7684\u80fd\u529b\u3002 YOLOv5l6s \u5bb9\u91cf\u66f4\u5927\uff0c\u6027\u80fd\u5f3a\u52b2\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u6c7d\u8f66\u65b9\u9762\uff0c\u4f46\u5728\u8bc6\u522b\u6469\u6258\u8f66\u548c\u81ea\u884c\u8f66\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002YOLOv5x6s \u5728\u8bc6\u522b\u516c\u5171\u6c7d\u8f66\u548c\u6c7d\u8f66\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u5728\u8bc6\u522b\u6469\u6258\u8f66\u7c7b\u522b\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002|[2408.12550v1](http://arxiv.org/pdf/2408.12550v1)|null|\n", "2408.12534": "|**2024-08-22**|**Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge**|\u8179\u90e8 CT \u4e2d\u7684\u81ea\u52a8\u5668\u5b98\u548c\u5168\u764c\u75c7\u5206\u5272\uff1aFLARE 2023 \u6311\u6218\u8d5b|Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Ershuai Wang, Qin Zhou, Ziyan Huang, Pengju Lyu, Jian He, Bo Wang|Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is the prerequisite for precise cancer diagnosis and treatment. Most existing benchmarks and algorithms are tailored to specific cancer types, limiting their ability to provide comprehensive cancer analysis. This work presents the first international competition on abdominal organ and pan-cancer segmentation by providing a large-scale and diverse dataset, including 4650 CT scans with various cancer types from over 40 medical centers. The winning team established a new state-of-the-art with a deep learning-based cascaded framework, achieving average Dice Similarity Coefficient scores of 92.3% for organs and 64.9% for lesions on the hidden multi-national testing set. The dataset and code of top teams are publicly available, offering a benchmark platform to drive further innovations https://codalab.lisn.upsaclay.fr/competitions/12239.|\u8179\u90e8\u5668\u5b98\u548c\u764c\u75c7\u5206\u5272 \u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf (CT) \u626b\u63cf\u662f\u7cbe\u51c6\u764c\u75c7\u8bca\u65ad\u548c\u6cbb\u7597\u7684\u5148\u51b3\u6761\u4ef6\u3002\u5927\u591a\u6570\u73b0\u6709\u57fa\u51c6\u548c\u7b97\u6cd5\u90fd\u662f\u9488\u5bf9\u7279\u5b9a\u764c\u75c7\u7c7b\u578b\u91cf\u8eab\u5b9a\u5236\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u63d0\u4f9b\u5168\u9762\u764c\u75c7\u5206\u6790\u7684\u80fd\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5305\u62ec\u6765\u81ea 40 \u591a\u4e2a\u533b\u7597\u4e2d\u5fc3\u7684 4650 \u5f20\u5404\u79cd\u764c\u75c7\u7c7b\u578b\u7684 CT \u626b\u63cf\uff0c\u5c55\u793a\u4e86\u9996\u6b21\u5173\u4e8e\u8179\u90e8\u5668\u5b98\u548c\u5168\u764c\u75c7\u5206\u5272\u7684\u56fd\u9645\u7ade\u8d5b\u3002\u83b7\u80dc\u56e2\u961f\u4ee5\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ea7\u8054\u6846\u67b6\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u9690\u85cf\u7684\u591a\u56fd\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5668\u5b98\u5e73\u5747 Dice \u76f8\u4f3c\u7cfb\u6570\u5f97\u5206\u4e3a 92.3%\uff0c\u75c5\u53d8\u5e73\u5747 Dice \u76f8\u4f3c\u7cfb\u6570\u5f97\u5206\u4e3a 64.9%\u3002\u9876\u7ea7\u56e2\u961f\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5747\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u63a8\u52a8\u8fdb\u4e00\u6b65\u521b\u65b0\u63d0\u4f9b\u4e86\u57fa\u51c6\u5e73\u53f0 https://codalab.lisn.upsaclay.fr/competitions/12239\u3002|[2408.12534v1](http://arxiv.org/pdf/2408.12534v1)|null|\n", "2408.12527": "|**2024-08-22**|**UMAD: University of Macau Anomaly Detection Benchmark Dataset**|UMAD\uff1a\u6fb3\u95e8\u5927\u5b66\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6|Dong Li, Lineng Chen, Cheng-Zhong Xu, Hui Kong|Anomaly detection is critical in surveillance systems and patrol robots by identifying anomalous regions in images for early warning. Depending on whether reference data are utilized, anomaly detection can be categorized into anomaly detection with reference and anomaly detection without reference. Currently, anomaly detection without reference, which is closely related to out-of-distribution (OoD) object detection, struggles with learning anomalous patterns due to the difficulty of collecting sufficiently large and diverse anomaly datasets with the inherent rarity and novelty of anomalies. Alternatively, anomaly detection with reference employs the scheme of change detection to identify anomalies by comparing semantic changes between a reference image and a query one. However, there are very few ADr works due to the scarcity of public datasets in this domain. In this paper, we aim to address this gap by introducing the UMAD Benchmark Dataset. To our best knowledge, this is the first benchmark dataset designed specifically for anomaly detection with reference in robotic patrolling scenarios, e.g., where an autonomous robot is employed to detect anomalous objects by comparing a reference and a query video sequences. The reference sequences can be taken by the robot along a specified route when there are no anomalous objects in the scene. The query sequences are captured online by the robot when it is patrolling in the same scene following the same route. Our benchmark dataset is elaborated such that each query image can find a corresponding reference based on accurate robot localization along the same route in the prebuilt 3D map, with which the reference and query images can be geometrically aligned using adaptive warping. Besides the proposed benchmark dataset, we evaluate the baseline models of ADr on this dataset.|\u5f02\u5e38\u68c0\u6d4b\u5728\u76d1\u63a7\u7cfb\u7edf\u548c\u5de1\u903b\u673a\u5668\u4eba\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u901a\u8fc7\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u533a\u57df\u8fdb\u884c\u9884\u8b66\u3002\u6839\u636e\u662f\u5426\u4f7f\u7528\u53c2\u8003\u6570\u636e\uff0c\u5f02\u5e38\u68c0\u6d4b\u53ef\u4ee5\u5206\u4e3a\u6709\u53c2\u8003\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u65e0\u53c2\u8003\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u76ee\u524d\uff0c\u65e0\u53c2\u8003\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5206\u5e03\u5916 (OoD) \u5bf9\u8c61\u68c0\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u7531\u4e8e\u5f02\u5e38\u672c\u8eab\u7684\u7a00\u6709\u6027\u548c\u65b0\u9896\u6027\uff0c\u96be\u4ee5\u6536\u96c6\u8db3\u591f\u5927\u4e14\u591a\u6837\u5316\u7684\u5f02\u5e38\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u96be\u4ee5\u5b66\u4e60\u5f02\u5e38\u6a21\u5f0f\u3002\u6216\u8005\uff0c\u6709\u53c2\u8003\u7684\u5f02\u5e38\u68c0\u6d4b\u91c7\u7528\u53d8\u5316\u68c0\u6d4b\u65b9\u6848\uff0c\u901a\u8fc7\u6bd4\u8f83\u53c2\u8003\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u53d8\u5316\u6765\u8bc6\u522b\u5f02\u5e38\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8be5\u9886\u57df\u7684\u516c\u5171\u6570\u636e\u96c6\u7a00\u7f3a\uff0cADr \u7684\u5de5\u4f5c\u975e\u5e38\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u65e8\u5728\u901a\u8fc7\u5f15\u5165 UMAD \u57fa\u51c6\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u673a\u5668\u4eba\u5de1\u903b\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f8b\u5982\uff0c\u4f7f\u7528\u81ea\u4e3b\u673a\u5668\u4eba\u901a\u8fc7\u6bd4\u8f83\u53c2\u8003\u548c\u67e5\u8be2\u89c6\u9891\u5e8f\u5217\u6765\u68c0\u6d4b\u5f02\u5e38\u7269\u4f53\u3002\u5f53\u573a\u666f\u4e2d\u6ca1\u6709\u5f02\u5e38\u7269\u4f53\u65f6\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u6cbf\u7740\u6307\u5b9a\u8def\u7ebf\u83b7\u53d6\u53c2\u8003\u5e8f\u5217\u3002\u5f53\u673a\u5668\u4eba\u6cbf\u7740\u76f8\u540c\u8def\u7ebf\u5728\u540c\u4e00\u573a\u666f\u4e2d\u5de1\u903b\u65f6\uff0c\u5b83\u4f1a\u5728\u7ebf\u6355\u83b7\u67e5\u8be2\u5e8f\u5217\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6570\u636e\u96c6\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u6bcf\u4e2a\u67e5\u8be2\u56fe\u50cf\u90fd\u53ef\u4ee5\u57fa\u4e8e\u9884\u5efa 3D \u5730\u56fe\u4e2d\u6cbf\u76f8\u540c\u8def\u7ebf\u7684\u7cbe\u786e\u673a\u5668\u4eba\u5b9a\u4f4d\u627e\u5230\u76f8\u5e94\u7684\u53c2\u8003\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u81ea\u9002\u5e94\u626d\u66f2\u5c06\u53c2\u8003\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u8fdb\u884c\u51e0\u4f55\u5bf9\u9f50\u3002\u9664\u4e86\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86 ADr \u7684\u57fa\u7ebf\u6a21\u578b\u3002|[2408.12527v1](http://arxiv.org/pdf/2408.12527v1)|null|\n", "2408.12489": "|**2024-08-22**|**Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets**|\u6240\u6709\u4eba\u7684\u6d82\u9e26\uff1a\u8de8\u6570\u636e\u96c6\u5bf9\u6d82\u9e26\u76d1\u7763\u5206\u5272\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5|Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Jan Eric Lenssen, Bernt Schiele|In this work, we introduce Scribbles for All, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, Scribbles for All provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Scribbles for All\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u6d82\u9e26\u6807\u7b7e\u4e0a\u8bad\u7ec3\u7684\u8bed\u4e49\u5206\u5272\u6807\u7b7e\u548c\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7b97\u6cd5\u3002\u8bad\u7ec3\u6216\u5fae\u8c03\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6a21\u578b\u6700\u8fd1\u5df2\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\uff0c\u5e76\u4e14\u6a21\u578b\u8d28\u91cf\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6d82\u9e26\u662f\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u6807\u7b7e\u7c7b\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u7ed3\u679c\uff0c\u540c\u65f6\u6240\u9700\u7684\u6ce8\u91ca\u5de5\u4f5c\u91cf\u6bd4\u901a\u5e38\u7684\u50cf\u7d20\u7ea7\u5bc6\u96c6\u8bed\u4e49\u5206\u5272\u6ce8\u91ca\u8981\u5c11\u5f97\u591a\u3002\u6d82\u9e26\u4f5c\u4e3a\u5f31\u76d1\u7763\u6765\u6e90\u7684\u4e3b\u8981\u9650\u5236\u662f\u7f3a\u4e4f\u5177\u6709\u6311\u6218\u6027\u7684\u6d82\u9e26\u5206\u5272\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u65b0\u65b9\u6cd5\u548c\u7ed3\u8bba\u6027\u8bc4\u4f30\u7684\u5f00\u53d1\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0cScribbles for All \u4e3a\u51e0\u4e2a\u6d41\u884c\u7684\u5206\u5272\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6d82\u9e26\u6807\u7b7e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b97\u6cd5\u6765\u81ea\u52a8\u4e3a\u4efb\u4f55\u5177\u6709\u5bc6\u96c6\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u751f\u6210\u6d82\u9e26\u6807\u7b7e\uff0c\u4e3a\u5f31\u76d1\u7763\u5206\u5272\u9886\u57df\u7684\u65b0\u89c1\u89e3\u548c\u6a21\u578b\u8fdb\u6b65\u94fa\u5e73\u4e86\u9053\u8def\u3002\u9664\u4e86\u63d0\u4f9b\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u6a21\u578b\uff0c\u5e76\u8868\u660e\u4f7f\u7528\u6211\u4eec\u7684\u5408\u6210\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u4e0e\u4f7f\u7528\u624b\u52a8\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u652f\u6301\u5bf9\u6d82\u9e26\u6807\u8bb0\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u8fdb\u884c\u6700\u5148\u8fdb\u7684\u7814\u7a76\u3002\u6570\u636e\u96c6\u3001\u6d82\u9e26\u751f\u6210\u7b97\u6cd5\u548c\u57fa\u7ebf\u53ef\u5728 https://github.com/wbkit/Scribbles4All \u4e0a\u516c\u5f00\u83b7\u53d6|[2408.12489v1](http://arxiv.org/pdf/2408.12489v1)|null|\n", "2408.12475": "|**2024-08-22**|**Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition**|\u5e27\u987a\u5e8f\u5f88\u91cd\u8981\uff1a\u7528\u4e8e\u5c11\u955c\u5934\u52a8\u4f5c\u8bc6\u522b\u7684\u65f6\u95f4\u5e8f\u5217\u611f\u77e5\u6a21\u578b|Bozheng Li, Mushui Liu, Gaoang Wang, Yunlong Yu|In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for few-shot action recognition (FSAR), which incorporates a sequential perceiver adapter into the pre-training framework, to integrate both the spatial information and the sequential temporal dynamics into the feature embeddings. Different from the existing fine-tuning approaches that capture temporal information by exploring the relationships among all the frames, our perceiver-based adapter recurrently captures the sequential dynamics alongside the timeline, which could perceive the order change. To obtain the discriminative representations for each class, we extend a textual corpus for each class derived from the large language models (LLMs) and enrich the visual prototypes by integrating the contextual semantic information. Besides, We introduce an unbalanced optimal transport strategy for feature matching that mitigates the impact of class-unrelated features, thereby facilitating more effective decision-making. Experimental results on five FSAR datasets demonstrate that our method set a new benchmark, beating the second-best competitors with large margins.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c0f\u6837\u672c\u52a8\u4f5c\u8bc6\u522b (FSAR) \u7684\u65b0\u578b\u65f6\u95f4\u5e8f\u5217\u611f\u77e5\u6a21\u578b (TSAM)\uff0c\u8be5\u6a21\u578b\u5c06\u4e00\u4e2a\u5e8f\u5217\u611f\u77e5\u5668\u9002\u914d\u5668\u6574\u5408\u5230\u9884\u8bad\u7ec3\u6846\u67b6\u4e2d\uff0c\u5c06\u7a7a\u95f4\u4fe1\u606f\u548c\u5e8f\u5217\u65f6\u95f4\u52a8\u6001\u96c6\u6210\u5230\u7279\u5f81\u5d4c\u5165\u4e2d\u3002\u4e0e\u73b0\u6709\u7684\u901a\u8fc7\u63a2\u7d22\u6240\u6709\u5e27\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u6355\u83b7\u65f6\u95f4\u4fe1\u606f\u7684\u5fae\u8c03\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u57fa\u4e8e\u611f\u77e5\u5668\u7684\u9002\u914d\u5668\u4f1a\u6cbf\u7740\u65f6\u95f4\u7ebf\u53cd\u590d\u6355\u83b7\u5e8f\u5217\u52a8\u6001\uff0c\u4ece\u800c\u53ef\u4ee5\u611f\u77e5\u987a\u5e8f\u53d8\u5316\u3002\u4e3a\u4e86\u83b7\u5f97\u6bcf\u4e2a\u7c7b\u7684\u5224\u522b\u6027\u8868\u793a\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6d3e\u751f\u7684\u6bcf\u4e2a\u7c7b\u7684\u6587\u672c\u8bed\u6599\u5e93\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u4e0a\u4e0b\u6587\u8bed\u4e49\u4fe1\u606f\u4e30\u5bcc\u4e86\u89c6\u89c9\u539f\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e0d\u5e73\u8861\u7684\u6700\u4f73\u4f20\u8f93\u7b56\u7565\u6765\u8fdb\u884c\u7279\u5f81\u5339\u914d\uff0c\u4ee5\u51cf\u8f7b\u4e0e\u7c7b\u65e0\u5173\u7684\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u51b3\u7b56\u3002\u5728\u4e94\u4e2a FSAR \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\uff0c\u4ee5\u8f83\u5927\u7684\u4f18\u52bf\u51fb\u8d25\u4e86\u7b2c\u4e8c\u597d\u7684\u7ade\u4e89\u5bf9\u624b\u3002|[2408.12475v1](http://arxiv.org/pdf/2408.12475v1)|null|\n", "2408.12469": "|**2024-08-22**|**Envisioning Class Entity Reasoning by Large Language Models for Few-shot Learning**|\u8bbe\u60f3\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u7c7b\u5b9e\u4f53\u63a8\u7406|Mushui Liu, Fangtai Wu, Bozheng Li, Ziqian Lu, Yunlong Yu, Xi Li|Few-shot learning (FSL) aims to recognize new concepts using a limited number of visual samples. Existing approaches attempt to incorporate semantic information into the limited visual data for category understanding. However, these methods often enrich class-level feature representations with abstract category names, failing to capture the nuanced features essential for effective generalization. To address this issue, we propose a novel framework for FSL, which incorporates both the abstract class semantics and the concrete class entities extracted from Large Language Models (LLMs), to enhance the representation of the class prototypes. Specifically, our framework composes a Semantic-guided Visual Pattern Extraction (SVPE) module and a Prototype-Calibration (PC) module, where the SVPE meticulously extracts semantic-aware visual patterns across diverse scales, while the PC module seamlessly integrates these patterns to refine the visual prototype, enhancing its representativeness. Extensive experiments on four few-shot classification benchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable advancements over the current state-of-the-art methods. Notably, for the challenging one-shot setting, our approach, utilizing the ResNet-12 backbone, achieves an impressive average improvement of 1.95% over the second-best competitor.|\u5c0f\u6837\u672c\u5b66\u4e60 (FSL) \u65e8\u5728\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u89c6\u89c9\u6837\u672c\u8bc6\u522b\u65b0\u6982\u5ff5\u3002\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u5c06\u8bed\u4e49\u4fe1\u606f\u7eb3\u5165\u6709\u9650\u7684\u89c6\u89c9\u6570\u636e\u4e2d\uff0c\u4ee5\u7406\u89e3\u7c7b\u522b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u62bd\u8c61\u7684\u7c7b\u522b\u540d\u79f0\u6765\u4e30\u5bcc\u7c7b\u7ea7\u7279\u5f81\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u6709\u6548\u6cdb\u5316\u6240\u5fc5\u9700\u7684\u7ec6\u5fae\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684 FSL \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u63d0\u53d6\u7684\u62bd\u8c61\u7c7b\u8bed\u4e49\u548c\u5177\u4f53\u7c7b\u5b9e\u4f53\uff0c\u4ee5\u589e\u5f3a\u7c7b\u539f\u578b\u7684\u8868\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u7531\u8bed\u4e49\u5f15\u5bfc\u7684\u89c6\u89c9\u6a21\u5f0f\u63d0\u53d6 (SVPE) \u6a21\u5757\u548c\u539f\u578b\u6821\u51c6 (PC) \u6a21\u5757\u7ec4\u6210\uff0c\u5176\u4e2d SVPE \u7ec6\u81f4\u5730\u63d0\u53d6\u4e0d\u540c\u5c3a\u5ea6\u7684\u8bed\u4e49\u611f\u77e5\u89c6\u89c9\u6a21\u5f0f\uff0c\u800c PC \u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u8fd9\u4e9b\u6a21\u5f0f\u4ee5\u7ec6\u5316\u89c6\u89c9\u539f\u578b\uff0c\u589e\u5f3a\u5176\u4ee3\u8868\u6027\u3002\u5728\u56db\u4e2a\u5c0f\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u548c BSCD-FSL \u8de8\u57df\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u7684\u663e\u7740\u8fdb\u6b65\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5bf9\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u4e00\u6b21\u6027\u8bbe\u7f6e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528 ResNet-12 \u4e3b\u5e72\uff0c\u6bd4\u7b2c\u4e8c\u597d\u7684\u7ade\u4e89\u5bf9\u624b\u5b9e\u73b0\u4e86 1.95% \u7684\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5e73\u5747\u6539\u8fdb\u3002|[2408.12469v1](http://arxiv.org/pdf/2408.12469v1)|null|\n", "2408.12466": "|**2024-08-22**|**WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation**|WCEbleedGen\uff1a\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c\u6570\u636e\u96c6\u53ca\u5176\u81ea\u52a8\u51fa\u8840\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u7684\u57fa\u51c6\u6d4b\u8bd5|Palak Handa, Manas Dhir, Amirreza Mahbod, Florian Schwarzhans, Ramona Woitek, Nidhi Goel, Deepak Gunjan|Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial. However, a medically annotated WCE dataset for training and evaluation of automatic classification, detection, and segmentation of bleeding and non-bleeding frames is currently lacking. The present work focused on development of a medically annotated WCE dataset called WCEbleedGen for automatic classification, detection, and segmentation of bleeding and non-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding frames which were collected from various internet resources and existing WCE datasets. A comprehensive benchmarking and evaluation of the developed dataset was done using nine classification-based, three detection-based, and three segmentation-based deep learning models. The dataset is of high-quality, is class-balanced and contains single and multiple bleeding sites. Overall, our standard benchmark results show that Visual Geometric Group (VGG) 19, You Only Look Once version 8 nano (YOLOv8n), and Link network (Linknet) performed best in automatic classification, detection, and segmentation-based evaluations, respectively. Automatic bleeding diagnosis is crucial for WCE video interpretations. This diverse dataset will aid in developing of real-time, multi-task learning-based innovative solutions for automatic bleeding diagnosis in WCE. The dataset and code are publicly available at https://zenodo.org/records/10156571 and https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.|\u57fa\u4e8e\u8ba1\u7b97\u673a\u7684\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c (WCE) \u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fa\u8840\u548c\u975e\u51fa\u8840\u5e27\u7684\u81ea\u52a8\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u7684\u533b\u5b66\u6ce8\u91ca WCE \u6570\u636e\u96c6\u3002\u672c\u7814\u7a76\u91cd\u70b9\u5f00\u53d1\u4e00\u79cd\u540d\u4e3a WCEbleedGen \u7684\u533b\u5b66\u6ce8\u91ca WCE \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u51fa\u8840\u548c\u975e\u51fa\u8840\u5e27\u7684\u81ea\u52a8\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u3002\u5b83\u5305\u542b 2,618 \u4e2a WCE \u51fa\u8840\u548c\u975e\u51fa\u8840\u5e27\uff0c\u8fd9\u4e9b\u5e27\u662f\u4ece\u5404\u79cd\u4e92\u8054\u7f51\u8d44\u6e90\u548c\u73b0\u6709 WCE \u6570\u636e\u96c6\u6536\u96c6\u7684\u3002\u4f7f\u7528\u4e5d\u79cd\u57fa\u4e8e\u5206\u7c7b\u3001\u4e09\u79cd\u57fa\u4e8e\u68c0\u6d4b\u548c\u4e09\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5f00\u53d1\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u3002\u8be5\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\u3001\u7c7b\u522b\u5e73\u8861\uff0c\u5305\u542b\u5355\u4e2a\u548c\u591a\u4e2a\u51fa\u8840\u90e8\u4f4d\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0cVisual Geometric Group (VGG) 19\u3001You Only Look Once version 8 nano (YOLOv8n) \u548c Link \u7f51\u7edc (Linknet) \u5206\u522b\u5728\u81ea\u52a8\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u57fa\u4e8e\u5206\u5272\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u81ea\u52a8\u51fa\u8840\u8bca\u65ad\u5bf9\u4e8e WCE \u89c6\u9891\u89e3\u91ca\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e2a\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u5c06\u6709\u52a9\u4e8e\u5f00\u53d1\u5b9e\u65f6\u3001\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684 WCE \u81ea\u52a8\u51fa\u8840\u8bca\u65ad\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u5728 https://zenodo.org/records/10156571 \u548c https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2408.12466v1](http://arxiv.org/pdf/2408.12466v1)|null|\n", "2408.12460": "|**2024-08-22**|**Finding Closure: A Closer Look at the Gestalt Law of Closure in Convolutional Neural Networks**|\u5bfb\u627e\u95ed\u5305\uff1a\u8fdb\u4e00\u6b65\u7814\u7a76\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u95ed\u5305\u683c\u5f0f\u5854\u5b9a\u5f8b|Yuyan Zhang, Derya Soydaner, Lisa Ko\u00dfmann, Fatemeh Behrad, Johan Wagemans|The human brain has an inherent ability to fill in gaps to perceive figures as complete wholes, even when parts are missing or fragmented. This phenomenon is known as Closure in psychology, one of the Gestalt laws of perceptual organization, explaining how the human brain interprets visual stimuli. Given the importance of Closure for human object recognition, we investigate whether neural networks rely on a similar mechanism. Exploring this crucial human visual skill in neural networks has the potential to highlight their comparability to humans. Recent studies have examined the Closure effect in neural networks. However, they typically focus on a limited selection of Convolutional Neural Networks (CNNs) and have not reached a consensus on their capability to perform Closure. To address these gaps, we present a systematic framework for investigating the Closure principle in neural networks. We introduce well-curated datasets designed to test for Closure effects, including both modal and amodal completion. We then conduct experiments on various CNNs employing different measurements. Our comprehensive analysis reveals that VGG16 and DenseNet-121 exhibit the Closure effect, while other CNNs show variable results. We interpret these findings by blending insights from psychology and neural network research, offering a unique perspective that enhances transparency in understanding neural networks. Our code and dataset will be made available on GitHub.|\u4eba\u7c7b\u5927\u8111\u5177\u6709\u586b\u8865\u7a7a\u767d\u7684\u56fa\u6709\u80fd\u529b\uff0c\u5373\u4f7f\u90e8\u5206\u7f3a\u5931\u6216\u652f\u79bb\u7834\u788e\uff0c\u4e5f\u80fd\u5c06\u56fe\u5f62\u89c6\u4e3a\u5b8c\u6574\u7684\u6574\u4f53\u3002\u8fd9\u79cd\u73b0\u8c61\u5728\u5fc3\u7406\u5b66\u4e2d\u88ab\u79f0\u4e3a\u95ed\u5408\uff0c\u662f\u611f\u77e5\u7ec4\u7ec7\u7684\u683c\u5f0f\u5854\u5b9a\u5f8b\u4e4b\u4e00\uff0c\u89e3\u91ca\u4e86\u4eba\u7c7b\u5927\u8111\u5982\u4f55\u89e3\u91ca\u89c6\u89c9\u523a\u6fc0\u3002\u9274\u4e8e\u95ed\u5408\u5bf9\u4e8e\u4eba\u7c7b\u7269\u4f53\u8bc6\u522b\u7684\u91cd\u8981\u6027\uff0c\u6211\u4eec\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u4f9d\u8d56\u4e8e\u7c7b\u4f3c\u7684\u673a\u5236\u3002\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u4e2d\u8fd9\u4e00\u5173\u952e\u7684\u4eba\u7c7b\u89c6\u89c9\u6280\u80fd\u6709\u53ef\u80fd\u51f8\u663e\u5b83\u4eec\u4e0e\u4eba\u7c7b\u7684\u53ef\u6bd4\u6027\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u7ecf\u68c0\u67e5\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u95ed\u5408\u6548\u5e94\u3002\u7136\u800c\uff0c\u5b83\u4eec\u901a\u5e38\u4e13\u6ce8\u4e8e\u6709\u9650\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u9009\u62e9\uff0c\u5e76\u4e14\u5c1a\u672a\u5c31\u5b83\u4eec\u6267\u884c\u95ed\u5408\u7684\u80fd\u529b\u8fbe\u6210\u5171\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\u6765\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u95ed\u5408\u539f\u7406\u3002\u6211\u4eec\u5f15\u5165\u4e86\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6d4b\u8bd5\u95ed\u5408\u6548\u5e94\uff0c\u5305\u62ec\u6a21\u6001\u548c\u975e\u6a21\u6001\u5b8c\u6210\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u6d4b\u91cf\u65b9\u6cd5\u5bf9\u5404\u79cd CNN \u8fdb\u884c\u5b9e\u9a8c\u3002\u6211\u4eec\u7684\u7efc\u5408\u5206\u6790\u8868\u660e\uff0cVGG16 \u548c DenseNet-121 \u8868\u73b0\u51fa\u95ed\u5408\u6548\u5e94\uff0c\u800c\u5176\u4ed6 CNN \u5219\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u6211\u4eec\u901a\u8fc7\u878d\u5408\u5fc3\u7406\u5b66\u548c\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u7684\u89c1\u89e3\u6765\u89e3\u91ca\u8fd9\u4e9b\u53d1\u73b0\uff0c\u4ece\u800c\u63d0\u4f9b\u72ec\u7279\u7684\u89c6\u89d2\uff0c\u589e\u5f3a\u5bf9\u795e\u7ecf\u7f51\u7edc\u7684\u7406\u89e3\u900f\u660e\u5ea6\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 GitHub \u4e0a\u63d0\u4f9b\u3002|[2408.12460v1](http://arxiv.org/pdf/2408.12460v1)|null|\n", "2408.12454": "|**2024-08-22**|**Relaxed Rotational Equivariance via $G$-Biases in Vision**|\u901a\u8fc7\u89c6\u89c9\u4e2d\u7684 $G$-Biases \u5b9e\u73b0\u653e\u677e\u65cb\u8f6c\u7b49\u53d8\u6027|Zhiqiang Wu, Licheng Sun, Yingjie Liu, Jian Yang, Hanlin Dong, Shing-Ho J. Lin, Xuan Tang, Jinpeng Mi, Bo Jin, Xian Wei|Group Equivariant Convolution (GConv) can effectively handle rotational symmetry data. They assume uniform and strict rotational symmetry across all features, as the transformations under the specific group. However, real-world data rarely conforms to strict rotational symmetry commonly referred to as Rotational Symmetry-Breaking in the system or dataset, making GConv unable to adapt effectively to this phenomenon. Motivated by this, we propose a simple but highly effective method to address this problem, which utilizes a set of learnable biases called the $G$-Biases under the group order to break strict group constraints and achieve \\textbf{R}elaxed \\textbf{R}otational \\textbf{E}quivarant \\textbf{Conv}olution (RREConv). We conduct extensive experiments to validate Relaxed Rotational Equivariance on rotational symmetry groups $\\mathcal{C}_n$ (e.g. $\\mathcal{C}_2$, $\\mathcal{C}_4$, and $\\mathcal{C}_6$ groups). Further experiments demonstrate that our proposed RREConv-based methods achieve excellent performance, compared to existing GConv-based methods in classification and detection tasks on natural image datasets.|\u7ec4\u7b49\u53d8\u5377\u79ef (GConv) \u53ef\u4ee5\u6709\u6548\u5904\u7406\u65cb\u8f6c\u5bf9\u79f0\u6570\u636e\u3002\u5b83\u4eec\u5047\u8bbe\u6240\u6709\u7279\u5f81\u90fd\u5177\u6709\u5747\u5300\u4e14\u4e25\u683c\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u5c31\u50cf\u7279\u5b9a\u7ec4\u4e0b\u7684\u53d8\u6362\u4e00\u6837\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5f88\u5c11\u7b26\u5408\u7cfb\u7edf\u6216\u6570\u636e\u96c6\u4e2d\u901a\u5e38\u79f0\u4e3a\u65cb\u8f6c\u5bf9\u79f0\u7834\u574f\u7684\u4e25\u683c\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u8fd9\u4f7f\u5f97 GConv \u65e0\u6cd5\u6709\u6548\u9002\u5e94\u8fd9\u79cd\u73b0\u8c61\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u504f\u5dee\uff08\u79f0\u4e3a\u7ec4\u5e8f\u4e0b\u7684 $G$-Biases\uff09\u6765\u6253\u7834\u4e25\u683c\u7684\u7ec4\u7ea6\u675f\u5e76\u5b9e\u73b0 \\textbf{R} \u5bbd\u677e \\textbf{R} \u65cb\u8f6c \\textbf{E} \u7b49\u53d8 \\textbf{Conv} \u89e3 (RREConv)\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u65cb\u8f6c\u5bf9\u79f0\u7fa4 $\\mathcal{C}_n$\uff08\u4f8b\u5982 $\\mathcal{C}_2$\u3001$\\mathcal{C}_4$ \u548c $\\mathcal{C}_6$ \u7ec4\uff09\u4e0a\u7684\u677e\u5f1b\u65cb\u8f6c\u7b49\u53d8\u6027\u3002\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e GConv \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e RREConv \u7684\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5206\u7c7b\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002|[2408.12454v1](http://arxiv.org/pdf/2408.12454v1)|null|\n", "2408.12447": "|**2024-08-22**|**The 2nd Solution for LSVOS Challenge RVOS Track: Spatial-temporal Refinement for Consistent Semantic Segmentation**|LSVOS \u6311\u6218\u8d5b RVOS \u8d5b\u9053\u7b2c\u4e8c\u5957\u89e3\u51b3\u65b9\u6848\uff1a\u65f6\u7a7a\u7ec6\u5316\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u8bed\u4e49\u5206\u5272|Tuyen Tran|Referring Video Object Segmentation (RVOS) is a challenging task due to its requirement for temporal understanding. Due to the obstacle of computational complexity, many state-of-the-art models are trained on short time intervals. During testing, while these models can effectively process information over short time steps, they struggle to maintain consistent perception over prolonged time sequences, leading to inconsistencies in the resulting semantic segmentation masks. To address this challenge, we take a step further in this work by leveraging the tracking capabilities of the newly introduced Segment Anything Model version 2 (SAM-v2) to enhance the temporal consistency of the referring object segmentation model. Our method achieved a score of 60.40 \\mathcal{J\\text{\\&}F} on the test set of the MeViS dataset, placing 2nd place in the final ranking of the RVOS Track at the ECCV 2024 LSVOS Challenge.|\u5f15\u7528\u89c6\u9891\u5bf9\u8c61\u5206\u5272 (RVOS) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u65f6\u95f4\u7406\u89e3\u3002\u7531\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u7684\u969c\u788d\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u90fd\u662f\u5728\u77ed\u65f6\u95f4\u95f4\u9694\u5185\u8fdb\u884c\u8bad\u7ec3\u7684\u3002\u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u5728\u77ed\u65f6\u95f4\u6b65\u9aa4\u5185\u6709\u6548\u5730\u5904\u7406\u4fe1\u606f\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u5728\u957f\u65f6\u95f4\u5e8f\u5217\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u611f\u77e5\uff0c\u4ece\u800c\u5bfc\u81f4\u751f\u6210\u7684\u8bed\u4e49\u5206\u5272\u63a9\u7801\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u66f4\u8fdb\u4e00\u6b65\uff0c\u5229\u7528\u65b0\u63a8\u51fa\u7684 Segment Anything Model \u7248\u672c 2 (SAM-v2) \u7684\u8ddf\u8e2a\u529f\u80fd\u6765\u589e\u5f3a\u5f15\u7528\u5bf9\u8c61\u5206\u5272\u6a21\u578b\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 MeViS \u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e86 60.40 \\mathcal{J\\text{\\&}F} \u7684\u5206\u6570\uff0c\u5728 ECCV 2024 LSVOS \u6311\u6218\u8d5b\u7684 RVOS \u8d5b\u9053\u6700\u7ec8\u6392\u540d\u4e2d\u4f4d\u5c45\u7b2c\u4e8c\u3002|[2408.12447v1](http://arxiv.org/pdf/2408.12447v1)|null|\n", "2408.12426": "|**2024-08-22**|**Enhanced Infield Agriculture with Interpretable Machine Learning Approaches for Crop Classification**|\u5229\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4f5c\u7269\u8fdb\u884c\u5206\u7c7b\uff0c\u589e\u5f3a\u7530\u95f4\u519c\u4e1a|Sudi Murindanyi, Joyce Nakatumba-Nabende, Rahman Sanya, Rose Nakibuule, Andrew Katumba|The increasing popularity of Artificial Intelligence in recent years has led to a surge in interest in image classification, especially in the agricultural sector. With the help of Computer Vision, Machine Learning, and Deep Learning, the sector has undergone a significant transformation, leading to the development of new techniques for crop classification in the field. Despite the extensive research on various image classification techniques, most have limitations such as low accuracy, limited use of data, and a lack of reporting model size and prediction. The most significant limitation of all is the need for model explainability. This research evaluates four different approaches for crop classification, namely traditional ML with handcrafted feature extraction methods like SIFT, ORB, and Color Histogram; Custom Designed CNN and established DL architecture like AlexNet; transfer learning on five models pre-trained using ImageNet such as EfficientNetV2, ResNet152V2, Xception, Inception-ResNetV2, MobileNetV3; and cutting-edge foundation models like YOLOv8 and DINOv2, a self-supervised Vision Transformer Model. All models performed well, but Xception outperformed all of them in terms of generalization, achieving 98% accuracy on the test data, with a model size of 80.03 MB and a prediction time of 0.0633 seconds. A key aspect of this research was the application of Explainable AI to provide the explainability of all the models. This journal presents the explainability of Xception model with LIME, SHAP, and GradCAM, ensuring transparency and trustworthiness in the models' predictions. This study highlights the importance of selecting the right model according to task-specific needs. It also underscores the important role of explainability in deploying AI in agriculture, providing insightful information to help enhance AI-driven crop management strategies.|\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u65e5\u76ca\u666e\u53ca\u5bfc\u81f4\u4eba\u4eec\u5bf9\u56fe\u50cf\u5206\u7c7b\u7684\u5174\u8da3\u6fc0\u589e\uff0c\u5c24\u5176\u662f\u5728\u519c\u4e1a\u9886\u57df\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u5e2e\u52a9\u4e0b\uff0c\u8be5\u9886\u57df\u53d1\u751f\u4e86\u91cd\u5927\u8f6c\u53d8\uff0c\u4ece\u800c\u5f00\u53d1\u4e86\u7528\u4e8e\u7530\u95f4\u4f5c\u7269\u5206\u7c7b\u7684\u65b0\u6280\u672f\u3002\u5c3d\u7ba1\u5bf9\u5404\u79cd\u56fe\u50cf\u5206\u7c7b\u6280\u672f\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u5927\u591a\u6570\u6280\u672f\u90fd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u51c6\u786e\u5ea6\u4f4e\u3001\u6570\u636e\u4f7f\u7528\u6709\u9650\u4ee5\u53ca\u7f3a\u4e4f\u62a5\u544a\u6a21\u578b\u5927\u5c0f\u548c\u9884\u6d4b\u3002\u6240\u6709\u6280\u672f\u4e2d\u6700\u91cd\u8981\u7684\u9650\u5236\u662f\u9700\u8981\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u9879\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u4f5c\u7269\u5206\u7c7b\u65b9\u6cd5\uff0c\u5373\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7684\u4f20\u7edf ML\uff0c\u5982 SIFT\u3001ORB \u548c\u989c\u8272\u76f4\u65b9\u56fe\uff1b\u5b9a\u5236\u8bbe\u8ba1\u7684 CNN \u548c\u6210\u719f\u7684 DL \u67b6\u6784\uff0c\u5982 AlexNet\uff1b\u4f7f\u7528 ImageNet \u9884\u8bad\u7ec3\u7684\u4e94\u79cd\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5982 EfficientNetV2\u3001ResNet152V2\u3001Xception\u3001Inception-ResNetV2\u3001MobileNetV3\uff1b\u4ee5\u53ca\u5c16\u7aef\u57fa\u7840\u6a21\u578b\uff0c\u5982 YOLOv8 \u548c DINOv2\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u89c6\u89c9\u53d8\u6362\u6a21\u578b\u3002\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u826f\u597d\uff0c\u4f46 Xception \u5728\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86 98% \u7684\u51c6\u786e\u7387\uff0c\u6a21\u578b\u5927\u5c0f\u4e3a 80.03 MB\uff0c\u9884\u6d4b\u65f6\u95f4\u4e3a 0.0633 \u79d2\u3002\u8fd9\u9879\u7814\u7a76\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u5e94\u7528\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u6765\u63d0\u4f9b\u6240\u6709\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u671f\u520a\u4ecb\u7ecd\u4e86 Xception \u6a21\u578b\u4e0e LIME\u3001SHAP \u548c GradCAM \u7684\u53ef\u89e3\u91ca\u6027\uff0c\u786e\u4fdd\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u6839\u636e\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u6b63\u786e\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002\u5b83\u8fd8\u5f3a\u8c03\u4e86\u53ef\u89e3\u91ca\u6027\u5728\u519c\u4e1a\u4e2d\u90e8\u7f72\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u63d0\u4f9b\u4e86\u6709\u89c1\u5730\u7684\u4fe1\u606f\u6765\u5e2e\u52a9\u52a0\u5f3a\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u4f5c\u7269\u7ba1\u7406\u7b56\u7565\u3002|[2408.12426v1](http://arxiv.org/pdf/2408.12426v1)|null|\n", "2408.12406": "|**2024-08-22**|**Generalized SAM: Efficient Fine-Tuning of SAM for Variable Input Image Sizes**|\u5e7f\u4e49 SAM\uff1a\u9488\u5bf9\u53ef\u53d8\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u7684 SAM \u9ad8\u6548\u5fae\u8c03|Sota Kato, Hinako Mitsuoka, Kazuhiro Hotta|There has been a lot of recent research on improving the efficiency of fine-tuning foundation models. In this paper, we propose a novel efficient fine-tuning method that allows the input image size of Segment Anything Model (SAM) to be variable. SAM is a powerful foundational model for image segmentation trained on huge datasets, but it requires fine-tuning to recognize arbitrary classes. The input image size of SAM is fixed at 1024 x 1024, resulting in substantial computational demands during training. Furthermore, the fixed input image size may result in the loss of image information, e.g. due to fixed aspect ratios. To address this problem, we propose Generalized SAM (GSAM). Different from the previous methods, GSAM is the first to apply random cropping during training with SAM, thereby significantly reducing the computational cost of training. Experiments on datasets of various types and various pixel counts have shown that GSAM can train more efficiently than SAM and other fine-tuning methods for SAM, achieving comparable or higher accuracy.|\u6700\u8fd1\u6709\u5f88\u591a\u5173\u4e8e\u63d0\u9ad8\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u6548\u7387\u7684\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5141\u8bb8\u5206\u5272\u4efb\u610f\u6a21\u578b (SAM) \u7684\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u53ef\u53d8\u3002SAM \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\uff0c\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u8fdb\u884c\u5fae\u8c03\u624d\u80fd\u8bc6\u522b\u4efb\u610f\u7c7b\u522b\u3002SAM \u7684\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u56fa\u5b9a\u4e3a 1024 x 1024\uff0c\u5bfc\u81f4\u8bad\u7ec3\u671f\u95f4\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\u3002\u6b64\u5916\uff0c\u56fa\u5b9a\u7684\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u53ef\u80fd\u4f1a\u5bfc\u81f4\u56fe\u50cf\u4fe1\u606f\u4e22\u5931\uff0c\u4f8b\u5982\u7531\u4e8e\u56fa\u5b9a\u7684\u7eb5\u6a2a\u6bd4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5e7f\u4e49 SAM (GSAM)\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cGSAM \u662f\u7b2c\u4e00\u4e2a\u5728\u4f7f\u7528 SAM \u8fdb\u884c\u8bad\u7ec3\u65f6\u5e94\u7528\u968f\u673a\u88c1\u526a\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u3002\u5728\u5404\u79cd\u7c7b\u578b\u548c\u5404\u79cd\u50cf\u7d20\u6570\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGSAM \u53ef\u4ee5\u6bd4 SAM \u548c\u5176\u4ed6 SAM \u5fae\u8c03\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002|[2408.12406v1](http://arxiv.org/pdf/2408.12406v1)|null|\n", "2408.12400": "|**2024-08-22**|**Multi-Style Facial Sketch Synthesis through Masked Generative Modeling**|\u901a\u8fc7\u8499\u7248\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u591a\u98ce\u683c\u9762\u90e8\u7d20\u63cf\u5408\u6210|Bowen Sun, Guo Lu, Shibao Zheng|The facial sketch synthesis (FSS) model, capable of generating sketch portraits from given facial photographs, holds profound implications across multiple domains, encompassing cross-modal face recognition, entertainment, art, media, among others. However, the production of high-quality sketches remains a formidable task, primarily due to the challenges and flaws associated with three key factors: (1) the scarcity of artist-drawn data, (2) the constraints imposed by limited style types, and (3) the deficiencies of processing input information in existing models. To address these difficulties, we propose a lightweight end-to-end synthesis model that efficiently converts images to corresponding multi-stylized sketches, obviating the necessity for any supplementary inputs (\\eg, 3D geometry). In this study, we overcome the issue of data insufficiency by incorporating semi-supervised learning into the training process. Additionally, we employ a feature extraction module and style embeddings to proficiently steer the generative transformer during the iterative prediction of masked image tokens, thus achieving a continuous stylized output that retains facial features accurately in sketches. The extensive experiments demonstrate that our method consistently outperforms previous algorithms across multiple benchmarks, exhibiting a discernible disparity.|\u9762\u90e8\u7d20\u63cf\u5408\u6210 (FSS) \u6a21\u578b\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u7684\u9762\u90e8\u7167\u7247\u751f\u6210\u7d20\u63cf\u8096\u50cf\uff0c\u5bf9\u8de8\u6a21\u6001\u4eba\u8138\u8bc6\u522b\u3001\u5a31\u4e50\u3001\u827a\u672f\u3001\u5a92\u4f53\u7b49\u591a\u4e2a\u9886\u57df\u90fd\u6709\u7740\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u5236\u4f5c\u9ad8\u8d28\u91cf\u7684\u7d20\u63cf\u4ecd\u7136\u662f\u4e00\u9879\u8270\u5de8\u7684\u4efb\u52a1\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4e0e\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\u76f8\u5173\u7684\u6311\u6218\u548c\u7f3a\u9677\uff1a(1) \u827a\u672f\u5bb6\u7ed8\u5236\u6570\u636e\u7684\u7a00\u7f3a\uff0c(2) \u98ce\u683c\u7c7b\u578b\u6709\u9650\u7684\u9650\u5236\uff0c\u4ee5\u53ca (3) \u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u8f93\u5165\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u56f0\u96be\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7aef\u5230\u7aef\u5408\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u76f8\u5e94\u7684\u591a\u98ce\u683c\u5316\u7d20\u63cf\uff0c\u4ece\u800c\u65e0\u9700\u4efb\u4f55\u8865\u5145\u8f93\u5165\uff08\u4f8b\u5982 3D \u51e0\u4f55\uff09\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u534a\u76d1\u7763\u5b66\u4e60\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\u6765\u514b\u670d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u98ce\u683c\u5d4c\u5165\u6765\u719f\u7ec3\u5730\u5728\u8499\u7248\u56fe\u50cf\u6807\u8bb0\u7684\u8fed\u4ee3\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u63a7\u5236\u751f\u6210\u8f6c\u6362\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u8fde\u7eed\u7684\u98ce\u683c\u5316\u8f93\u51fa\uff0c\u5728\u8349\u56fe\u4e2d\u51c6\u786e\u4fdd\u7559\u9762\u90e8\u7279\u5f81\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u7b97\u6cd5\uff0c\u8868\u73b0\u51fa\u660e\u663e\u7684\u5dee\u5f02\u3002|[2408.12400v1](http://arxiv.org/pdf/2408.12400v1)|null|\n", "2408.12381": "|**2024-08-22**|**Sampling Strategies based on Wisdom of Crowds for Amazon Deforestation Detection**|\u57fa\u4e8e\u7fa4\u4f53\u667a\u6167\u7684\u4e9a\u9a6c\u900a\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u91c7\u6837\u7b56\u7565|Hugo Resende, Eduardo B. Neto, Fabio A. M. Cappabianco, Alvaro L. Fazenda, Fabio A. Faria|Conserving tropical forests is highly relevant socially and ecologically because of their critical role in the global ecosystem. However, the ongoing deforestation and degradation affect millions of hectares each year, necessitating government or private initiatives to ensure effective forest monitoring. In April 2019, a project based on Citizen Science and Machine Learning models called ForestEyes (FE) was launched with the aim of providing supplementary data to assist experts from government and non-profit organizations in their deforestation monitoring efforts. Recent research has shown that labeling FE project volunteers/citizen scientists helps tailor machine learning models. In this sense, we adopt the FE project to create different sampling strategies based on the wisdom of crowds to select the most suitable samples from the training set to learn an SVM technique and obtain better classification results in deforestation detection tasks. In our experiments, we can show that our strategy based on user entropy-increasing achieved the best classification results in the deforestation detection task when compared with the random sampling strategies, as well as, reducing the convergence time of the SVM technique.|\u4fdd\u62a4\u70ed\u5e26\u68ee\u6797\u5177\u6709\u9ad8\u5ea6\u7684\u793e\u4f1a\u548c\u751f\u6001\u610f\u4e49\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u5168\u7403\u751f\u6001\u7cfb\u7edf\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6301\u7eed\u4e0d\u65ad\u7684\u68ee\u6797\u780d\u4f10\u548c\u9000\u5316\u6bcf\u5e74\u5f71\u54cd\u6570\u767e\u4e07\u516c\u9877\u7684\u68ee\u6797\uff0c\u56e0\u6b64\u9700\u8981\u653f\u5e9c\u6216\u79c1\u4eba\u4e3e\u63aa\u6765\u786e\u4fdd\u6709\u6548\u7684\u68ee\u6797\u76d1\u6d4b\u30022019 \u5e74 4 \u6708\uff0c\u4e00\u9879\u57fa\u4e8e\u516c\u6c11\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9879\u76ee ForestEyes (FE) \u542f\u52a8\uff0c\u65e8\u5728\u63d0\u4f9b\u8865\u5145\u6570\u636e\uff0c\u534f\u52a9\u653f\u5e9c\u548c\u975e\u8425\u5229\u7ec4\u7ec7\u7684\u4e13\u5bb6\u5f00\u5c55\u68ee\u6797\u780d\u4f10\u76d1\u6d4b\u5de5\u4f5c\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u7ed9 FE \u9879\u76ee\u5fd7\u613f\u8005/\u516c\u6c11\u79d1\u5b66\u5bb6\u8d34\u4e0a\u6807\u7b7e\u6709\u52a9\u4e8e\u5b9a\u5236\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u4ece\u8fd9\u4e2a\u610f\u4e49\u4e0a\u8bf4\uff0c\u6211\u4eec\u91c7\u7528 FE \u9879\u76ee\uff0c\u6839\u636e\u7fa4\u4f53\u667a\u6167\u521b\u5efa\u4e0d\u540c\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4ece\u8bad\u7ec3\u96c6\u4e2d\u9009\u62e9\u6700\u5408\u9002\u7684\u6837\u672c\u6765\u5b66\u4e60 SVM \u6280\u672f\uff0c\u5e76\u5728\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u5206\u7c7b\u7ed3\u679c\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u8bc1\u660e\uff0c\u4e0e\u968f\u673a\u91c7\u6837\u7b56\u7565\u76f8\u6bd4\uff0c\u57fa\u4e8e\u7528\u6237\u71b5\u589e\u52a0\u7684\u7b56\u7565\u5728\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u5206\u7c7b\u7ed3\u679c\uff0c\u540c\u65f6\u7f29\u77ed\u4e86 SVM \u6280\u672f\u7684\u6536\u655b\u65f6\u95f4\u3002|[2408.12381v1](http://arxiv.org/pdf/2408.12381v1)|null|\n", "2408.12364": "|**2024-08-22**|**SAM-SP: Self-Prompting Makes SAM Great Again**|SAM-SP\uff1a\u81ea\u6211\u6fc0\u52b1\u8ba9 SAM \u518d\u6b21\u4f1f\u5927|Chunpeng Zhou, Kangjie Ning, Qianqian Shen, Sheng Zhou, Zhi Yu, Haishuai Wang|The recently introduced Segment Anything Model (SAM), a Visual Foundation Model (VFM), has demonstrated impressive capabilities in zero-shot segmentation tasks across diverse natural image datasets. Despite its success, SAM encounters noticeably performance degradation when applied to specific domains, such as medical images. Current efforts to address this issue have involved fine-tuning strategies, intended to bolster the generalizability of the vanilla SAM. However, these approaches still predominantly necessitate the utilization of domain specific expert-level prompts during the evaluation phase, which severely constrains the model's practicality.   To overcome this limitation, we introduce a novel self-prompting based fine-tuning approach, called SAM-SP, tailored for extending the vanilla SAM model. Specifically, SAM-SP leverages the output from the previous iteration of the model itself as prompts to guide subsequent iteration of the model. This self-prompting module endeavors to learn how to generate useful prompts autonomously and alleviates the dependence on expert prompts during the evaluation phase, significantly broadening SAM's applicability. Additionally, we integrate a self-distillation module to enhance the self-prompting process further. Extensive experiments across various domain specific datasets validate the effectiveness of the proposed SAM-SP. Our SAM-SP not only alleviates the reliance on expert prompts but also exhibits superior segmentation performance comparing to the state-of-the-art task-specific segmentation approaches, the vanilla SAM, and SAM-based approaches.|\u6700\u8fd1\u63a8\u51fa\u7684 Segment Anything Model (SAM) \u662f\u4e00\u79cd\u53ef\u89c6\u5316\u57fa\u7840\u6a21\u578b (VFM)\uff0c\u5b83\u5728\u5404\u79cd\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u7684\u96f6\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u5c3d\u7ba1 SAM \u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u4f8b\u5982\u533b\u5b66\u56fe\u50cf\uff09\u65f6\uff0c\u5176\u6027\u80fd\u4f1a\u660e\u663e\u4e0b\u964d\u3002\u76ee\u524d\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u6240\u505a\u7684\u52aa\u529b\u6d89\u53ca\u5fae\u8c03\u7b56\u7565\uff0c\u65e8\u5728\u589e\u5f3a vanilla SAM \u7684\u901a\u7528\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u4e3b\u8981\u9700\u8981\u5728\u8bc4\u4f30\u9636\u6bb5\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684\u4e13\u5bb6\u7ea7\u63d0\u793a\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u81ea\u63d0\u793a\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u79f0\u4e3a SAM-SP\uff0c\u4e13\u4e3a\u6269\u5c55 vanilla SAM \u6a21\u578b\u800c\u91cf\u8eab\u5b9a\u5236\u3002\u5177\u4f53\u800c\u8a00\uff0cSAM-SP \u5229\u7528\u6a21\u578b\u672c\u8eab\u4e0a\u4e00\u6b21\u8fed\u4ee3\u7684\u8f93\u51fa\u4f5c\u4e3a\u63d0\u793a\u6765\u6307\u5bfc\u6a21\u578b\u7684\u540e\u7eed\u8fed\u4ee3\u3002\u8fd9\u4e2a\u81ea\u63d0\u793a\u6a21\u5757\u81f4\u529b\u4e8e\u5b66\u4e60\u5982\u4f55\u81ea\u4e3b\u751f\u6210\u6709\u7528\u7684\u63d0\u793a\uff0c\u5e76\u5728\u8bc4\u4f30\u9636\u6bb5\u51cf\u8f7b\u5bf9\u4e13\u5bb6\u63d0\u793a\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u5927\u5927\u62d3\u5bbd SAM \u7684\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4e00\u4e2a\u81ea\u63d0\u70bc\u6a21\u5757\uff0c\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u81ea\u63d0\u793a\u8fc7\u7a0b\u3002\u5728\u5404\u79cd\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 SAM-SP \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684 SAM-SP \u4e0d\u4ec5\u51cf\u8f7b\u4e86\u5bf9\u4e13\u5bb6\u63d0\u793a\u7684\u4f9d\u8d56\uff0c\u800c\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u5206\u5272\u65b9\u6cd5\u3001\u666e\u901a SAM \u548c\u57fa\u4e8e SAM \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd8\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u5206\u5272\u6027\u80fd\u3002|[2408.12364v1](http://arxiv.org/pdf/2408.12364v1)|null|\n", "2408.12355": "|**2024-08-22**|**Class-balanced Open-set Semi-supervised Object Detection for Medical Images**|\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u7c7b\u522b\u5e73\u8861\u5f00\u653e\u96c6\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b|Zhanyun Lu, Renshu Gu, Huimin Cheng, Siyu Pang, Mingyu Xu, Peifang Xu, Yaqi Wang, Yuichiro Kinoshita, Juan Ye, Gangyong Jia, et.al.|Medical image datasets in the real world are often unlabeled and imbalanced, and Semi-Supervised Object Detection (SSOD) can utilize unlabeled data to improve an object detector. However, existing approaches predominantly assumed that the unlabeled data and test data do not contain out-of-distribution (OOD) classes. The few open-set semi-supervised object detection methods have two weaknesses: first, the class imbalance is not considered; second, the OOD instances are distinguished and simply discarded during pseudo-labeling. In this paper, we consider the open-set semi-supervised object detection problem which leverages unlabeled data that contain OOD classes to improve object detection for medical images. Our study incorporates two key innovations: Category Control Embed (CCE) and out-of-distribution Detection Fusion Classifier (OODFC). CCE is designed to tackle dataset imbalance by constructing a Foreground information Library, while OODFC tackles open-set challenges by integrating the ``unknown'' information into basic pseudo-labels. Our method outperforms the state-of-the-art SSOD performance, achieving a 4.25 mAP improvement on the public Parasite dataset.|\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u901a\u5e38\u672a\u6807\u8bb0\u4e14\u4e0d\u5e73\u8861\uff0c\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b (SSOD) \u53ef\u4ee5\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u6765\u6539\u8fdb\u76ee\u6807\u68c0\u6d4b\u5668\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5047\u8bbe\u672a\u6807\u8bb0\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u4e0d\u5305\u542b\u5206\u5e03\u5916 (OOD) \u7c7b\u522b\u3002\u5c11\u6570\u5f00\u653e\u96c6\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u6709\u4e24\u4e2a\u7f3a\u70b9\uff1a\u9996\u5148\uff0c\u6ca1\u6709\u8003\u8651\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b\u5176\u6b21\uff0c\u5728\u4f2a\u6807\u8bb0\u8fc7\u7a0b\u4e2d\u533a\u5206\u5e76\u7b80\u5355\u4e22\u5f03 OOD \u5b9e\u4f8b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u5f00\u653e\u96c6\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5229\u7528\u5305\u542b OOD \u7c7b\u522b\u7684\u672a\u6807\u8bb0\u6570\u636e\u6765\u6539\u8fdb\u533b\u5b66\u56fe\u50cf\u7684\u76ee\u6807\u68c0\u6d4b\u3002\u6211\u4eec\u7684\u7814\u7a76\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u7c7b\u522b\u63a7\u5236\u5d4c\u5165 (CCE) \u548c\u5206\u5e03\u5916\u68c0\u6d4b\u878d\u5408\u5206\u7c7b\u5668 (OODFC)\u3002CCE \u65e8\u5728\u901a\u8fc7\u6784\u5efa\u524d\u666f\u4fe1\u606f\u5e93\u6765\u89e3\u51b3\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u800c OODFC \u901a\u8fc7\u5c06\u201c\u672a\u77e5\u201d\u4fe1\u606f\u96c6\u6210\u5230\u57fa\u672c\u4f2a\u6807\u7b7e\u4e2d\u6765\u89e3\u51b3\u5f00\u653e\u96c6\u6311\u6218\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 SSOD \u6027\u80fd\uff0c\u5728\u516c\u5171\u5bc4\u751f\u866b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 4.25 mAP \u7684\u63d0\u5347\u3002|[2408.12355v1](http://arxiv.org/pdf/2408.12355v1)|null|\n", "2408.12323": "|**2024-08-22**|**EUIS-Net: A Convolutional Neural Network for Efficient Ultrasound Image Segmentation**|EUIS-Net\uff1a\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc|Shahzaib Iqbal, Hasnat Ahmed, Muhammad Sharif, Madiha Hena, Tariq M. Khan, Imran Razzak|Segmenting ultrasound images is critical for various medical applications, but it offers significant challenges due to ultrasound images' inherent noise and unpredictability. To address these challenges, we proposed EUIS-Net, a CNN network designed to segment ultrasound images efficiently and precisely. The proposed EUIS-Net utilises four encoder-decoder blocks, resulting in a notable decrease in computational complexity while achieving excellent performance. The proposed EUIS-Net integrates both channel and spatial attention mechanisms into the bottleneck to improve feature representation and collect significant contextual information. In addition, EUIS-Net incorporates a region-aware attention module in skip connections, which enhances the ability to concentrate on the region of the injury. To enable thorough information exchange across various network blocks, skip connection aggregation is employed from the network's lowermost to the uppermost block. Comprehensive evaluations are conducted on two publicly available ultrasound image segmentation datasets. The proposed EUIS-Net achieved mean IoU and dice scores of 78. 12\\%, 85. 42\\% and 84. 73\\%, 89. 01\\% in the BUSI and DDTI datasets, respectively. The findings of our study showcase the substantial capabilities of EUIS-Net for immediate use in clinical settings and its versatility in various ultrasound imaging tasks.|\u5206\u5272\u8d85\u58f0\u56fe\u50cf\u5bf9\u4e8e\u5404\u79cd\u533b\u7597\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8d85\u58f0\u56fe\u50cf\u56fa\u6709\u7684\u566a\u58f0\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5206\u5272\u8d85\u58f0\u56fe\u50cf\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EUIS-Net\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u9ad8\u6548\u3001\u7cbe\u786e\u5730\u5206\u5272\u8d85\u58f0\u56fe\u50cf\u7684 CNN \u7f51\u7edc\u3002\u6240\u63d0\u51fa\u7684 EUIS-Net \u4f7f\u7528\u56db\u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5757\uff0c\u5728\u5b9e\u73b0\u51fa\u8272\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u6240\u63d0\u51fa\u7684 EUIS-Net \u5c06\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u673a\u5236\u96c6\u6210\u5230\u74f6\u9888\u4e2d\uff0c\u4ee5\u6539\u8fdb\u7279\u5f81\u8868\u793a\u5e76\u6536\u96c6\u91cd\u8981\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u6b64\u5916\uff0cEUIS-Net \u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u6574\u5408\u4e86\u4e00\u4e2a\u533a\u57df\u611f\u77e5\u6ce8\u610f\u6a21\u5757\uff0c\u8fd9\u589e\u5f3a\u4e86\u96c6\u4e2d\u6ce8\u610f\u529b\u4e8e\u53d7\u4f24\u533a\u57df\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u5b9e\u73b0\u5404\u4e2a\u7f51\u7edc\u5757\u4e4b\u95f4\u7684\u5f7b\u5e95\u4fe1\u606f\u4ea4\u6362\uff0c\u4ece\u7f51\u7edc\u7684\u6700\u4e0b\u5c42\u5230\u6700\u4e0a\u5c42\u5757\u91c7\u7528\u4e86\u8df3\u8dc3\u8fde\u63a5\u805a\u5408\u3002\u5bf9\u4e24\u4e2a\u516c\u5f00\u7684\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\u3002\u6240\u63d0\u51fa\u7684 EUIS-Net \u5728 BUSI \u548c DDTI \u6570\u636e\u96c6\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86 78. 12%\u300185. 42% \u548c 84. 73%\u300189. 01% \u7684\u5e73\u5747 IoU \u548c dice \u5f97\u5206\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86 EUIS-Net \u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7acb\u5373\u4f7f\u7528\u7684\u5f3a\u5927\u529f\u80fd\u53ca\u5176\u5728\u5404\u79cd\u8d85\u58f0\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002|[2408.12323v1](http://arxiv.org/pdf/2408.12323v1)|null|\n", "2408.12322": "|**2024-08-22**|**Multimodal Foundational Models for Unsupervised 3D General Obstacle Detection**|\u65e0\u76d1\u7763 3D \u4e00\u822c\u969c\u788d\u7269\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b|Tam\u00e1s Matuszka, P\u00e9ter Hajas, D\u00e1vid Szeghy|Current autonomous driving perception models primarily rely on supervised learning with predefined categories. However, these models struggle to detect general obstacles not included in the fixed category set due to their variability and numerous edge cases. To address this issue, we propose a combination of multimodal foundational model-based obstacle segmentation with traditional unsupervised computational geometry-based outlier detection. Our approach operates offline, allowing us to leverage non-causality, and utilizes training-free methods. This enables the detection of general obstacles in 3D without the need for expensive retraining. To overcome the limitations of publicly available obstacle detection datasets, we collected and annotated our dataset, which includes various obstacles even in distant regions.|\u5f53\u524d\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5177\u6709\u9884\u5b9a\u4e49\u7c7b\u522b\u7684\u76d1\u7763\u5b66\u4e60\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684\u591a\u53d8\u6027\u548c\u5927\u91cf\u8fb9\u7f18\u60c5\u51b5\uff0c\u5b83\u4eec\u5f88\u96be\u68c0\u6d4b\u5230\u56fa\u5b9a\u7c7b\u522b\u96c6\u5408\u4e2d\u672a\u5305\u542b\u7684\u4e00\u822c\u969c\u788d\u7269\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u969c\u788d\u7269\u5206\u5272\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u65e0\u76d1\u7763\u8ba1\u7b97\u51e0\u4f55\u7684\u5f02\u5e38\u503c\u68c0\u6d4b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u79bb\u7ebf\u8fd0\u884c\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5229\u7528\u975e\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u65e0\u9700\u6602\u8d35\u7684\u518d\u8bad\u7ec3\u5c31\u53ef\u4ee5\u68c0\u6d4b 3D \u4e2d\u7684\u4e00\u822c\u969c\u788d\u7269\u3002\u4e3a\u4e86\u514b\u670d\u516c\u5f00\u53ef\u7528\u7684\u969c\u788d\u7269\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u6536\u96c6\u5e76\u6ce8\u91ca\u4e86\u6211\u4eec\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u751a\u81f3\u5728\u9065\u8fdc\u5730\u533a\u7684\u5404\u79cd\u969c\u788d\u7269\u3002|[2408.12322v1](http://arxiv.org/pdf/2408.12322v1)|null|\n", "2408.12312": "|**2024-08-22**|**MakeupAttack: Feature Space Black-box Backdoor Attack on Face Recognition via Makeup Transfer**|MakeupAttack\uff1a\u901a\u8fc7\u5316\u5986\u8f6c\u79fb\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\u7684\u7279\u5f81\u7a7a\u95f4\u9ed1\u76d2\u540e\u95e8\u653b\u51fb|Ming Sun, Lihua Jing, Zixuan Zhu, Rui Wang|Backdoor attacks pose a significant threat to the training process of deep neural networks (DNNs). As a widely-used DNN-based application in real-world scenarios, face recognition systems once implanted into the backdoor, may cause serious consequences. Backdoor research on face recognition is still in its early stages, and the existing backdoor triggers are relatively simple and visible. Furthermore, due to the perceptibility, diversity, and similarity of facial datasets, many state-of-the-art backdoor attacks lose effectiveness on face recognition tasks. In this work, we propose a novel feature space backdoor attack against face recognition via makeup transfer, dubbed MakeupAttack. In contrast to many feature space attacks that demand full access to target models, our method only requires model queries, adhering to black-box attack principles. In our attack, we design an iterative training paradigm to learn the subtle features of the proposed makeup-style trigger. Additionally, MakeupAttack promotes trigger diversity using the adaptive selection method, dispersing the feature distribution of malicious samples to bypass existing defense methods. Extensive experiments were conducted on two widely-used facial datasets targeting multiple models. The results demonstrate that our proposed attack method can bypass existing state-of-the-art defenses while maintaining effectiveness, robustness, naturalness, and stealthiness, without compromising model performance.|\u540e\u95e8\u653b\u51fb\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6784\u6210\u4e86\u91cd\u5927\u5a01\u80c1\u3002\u4f5c\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8eDNN\u7684\u5e94\u7528\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e00\u65e6\u88ab\u690d\u5165\u540e\u95e8\uff0c\u5c06\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u540e\u679c\u3002\u4eba\u8138\u8bc6\u522b\u7684\u540e\u95e8\u7814\u7a76\u5c1a\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff0c\u73b0\u6709\u7684\u540e\u95e8\u89e6\u53d1\u5668\u76f8\u5bf9\u7b80\u5355\u4e14\u53ef\u89c1\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u4eba\u8138\u6570\u636e\u96c6\u7684\u53ef\u611f\u77e5\u6027\u3001\u591a\u6837\u6027\u548c\u76f8\u4f3c\u6027\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u540e\u95e8\u653b\u51fb\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e0a\u5931\u6548\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5316\u5986\u8fc1\u79fb\u4eba\u8138\u8bc6\u522b\u7684\u65b0\u578b\u7279\u5f81\u7a7a\u95f4\u540e\u95e8\u653b\u51fb\uff0c\u79f0\u4e3aMakeupAttack\u3002\u4e0e\u8bb8\u591a\u9700\u8981\u5b8c\u5168\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u7684\u7279\u5f81\u7a7a\u95f4\u653b\u51fb\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ea\u9700\u8981\u6a21\u578b\u67e5\u8be2\uff0c\u9075\u5faa\u9ed1\u7bb1\u653b\u51fb\u539f\u5219\u3002\u5728\u6211\u4eec\u7684\u653b\u51fb\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u8bad\u7ec3\u8303\u5f0f\u6765\u5b66\u4e60\u6240\u63d0\u51fa\u7684\u5316\u5986\u98ce\u683c\u89e6\u53d1\u5668\u7684\u7ec6\u5fae\u7279\u5f81\u3002\u6b64\u5916\uff0cMakeupAttack\u4f7f\u7528\u81ea\u9002\u5e94\u9009\u62e9\u65b9\u6cd5\u4fc3\u8fdb\u89e6\u53d1\u5668\u591a\u6837\u6027\uff0c\u5206\u6563\u6076\u610f\u6837\u672c\u7684\u7279\u5f81\u5206\u5e03\u4ee5\u7ed5\u8fc7\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9762\u90e8\u6570\u636e\u96c6\u4e0a\u9488\u5bf9\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u53ef\u4ee5\u7ed5\u8fc7\u73b0\u6709\u7684\u6700\u5148\u8fdb\u9632\u5fa1\u63aa\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u6027\u3001\u7a33\u5065\u6027\u3001\u81ea\u7136\u6027\u548c\u9690\u853d\u6027\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002|[2408.12312v1](http://arxiv.org/pdf/2408.12312v1)|null|\n", "2408.12293": "|**2024-08-22**|**AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network**|AT-SNN\uff1a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u89c6\u89c9\u53d8\u6362\u5668\u81ea\u9002\u5e94\u6807\u8bb0|Donghwa Kang, Youngmoon Lee, Eun-Kyu Lee, Brent Kang, Jinkyu Lee, Hyeongboo Baek|In the training and inference of spiking neural networks (SNNs), direct training and lightweight computation methods have been orthogonally developed, aimed at reducing power consumption. However, only a limited number of approaches have applied these two mechanisms simultaneously and failed to fully leverage the advantages of SNN-based vision transformers (ViTs) since they were originally designed for convolutional neural networks (CNNs). In this paper, we propose AT-SNN designed to dynamically adjust the number of tokens processed during inference in SNN-based ViTs with direct training, wherein power consumption is proportional to the number of tokens. We first demonstrate the applicability of adaptive computation time (ACT), previously limited to RNNs and ViTs, to SNN-based ViTs, enhancing it to discard less informative spatial tokens selectively. Also, we propose a new token-merge mechanism that relies on the similarity of tokens, which further reduces the number of tokens while enhancing accuracy. We implement AT-SNN to Spikformer and show the effectiveness of AT-SNN in achieving high energy efficiency and accuracy compared to state-of-the-art approaches on the image classification tasks, CIFAR10, CIFAR-100, and TinyImageNet. For example, our approach uses up to 42.4% fewer tokens than the existing best-performing method on CIFAR-100, while conserving higher accuracy.|\u5728\u8109\u51b2\u795e\u7ecf\u7f51\u7edc (SNN) \u7684\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\uff0c\u76f4\u63a5\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u65b9\u6cd5\u5df2\u7ecf\u5f97\u5230\u6b63\u4ea4\u5f00\u53d1\uff0c\u65e8\u5728\u964d\u4f4e\u529f\u8017\u3002\u7136\u800c\uff0c\u53ea\u6709\u6709\u9650\u6570\u91cf\u7684\u65b9\u6cd5\u540c\u65f6\u5e94\u7528\u4e86\u8fd9\u4e24\u79cd\u673a\u5236\uff0c\u5e76\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u57fa\u4e8e SNN \u7684\u89c6\u89c9\u53d8\u6362\u5668 (ViT) \u7684\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u4eec\u6700\u521d\u662f\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u8bbe\u8ba1\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AT-SNN\uff0c\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u8bad\u7ec3\u52a8\u6001\u8c03\u6574\u57fa\u4e8e SNN \u7684 ViT \u5728\u63a8\u7406\u671f\u95f4\u5904\u7406\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u5176\u4e2d\u529f\u8017\u4e0e\u6807\u8bb0\u6570\u91cf\u6210\u6b63\u6bd4\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u8ba1\u7b97\u65f6\u95f4 (ACT)\uff08\u4ee5\u524d\u4ec5\u9650\u4e8e RNN \u548c ViT\uff09\u5bf9\u57fa\u4e8e SNN \u7684 ViT \u7684\u9002\u7528\u6027\uff0c\u589e\u5f3a\u4e86\u5b83\u4ee5\u6709\u9009\u62e9\u5730\u4e22\u5f03\u4fe1\u606f\u91cf\u8f83\u5c11\u7684\u7a7a\u95f4\u6807\u8bb0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f9d\u8d56\u4e8e\u6807\u8bb0\u76f8\u4f3c\u6027\u7684\u65b0\u6807\u8bb0\u5408\u5e76\u673a\u5236\uff0c\u8fd9\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u6211\u4eec\u5c06 AT-SNN \u5b9e\u73b0\u5230 Spikformer\uff0c\u5e76\u5c55\u793a\u4e86 AT-SNN \u5728\u5b9e\u73b0\u9ad8\u80fd\u6548\u548c\u51c6\u786e\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e0e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1 CIFAR10\u3001CIFAR-100 \u548c TinyImageNet \u4e0a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u7684\u6807\u8bb0\u6bd4 CIFAR-100 \u4e0a\u73b0\u6709\u7684\u6700\u4f73\u65b9\u6cd5\u5c11 42.4%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002|[2408.12293v1](http://arxiv.org/pdf/2408.12293v1)|null|\n", "2408.12275": "|**2024-08-22**|**Whole Slide Image Classification of Salivary Gland Tumours**|\u6d8e\u817a\u80bf\u7624\u7684\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u5206\u7c7b|John Charlton, Ibrahim Alsanie, Syed Ali Khurram|This work shows promising results using multiple instance learning on salivary gland tumours in classifying cancers on whole slide images. Utilising CTransPath as a patch-level feature extractor and CLAM as a feature aggregator, an F1 score of over 0.88 and AUROC of 0.92 are obtained for detecting cancer in whole slide images.|\u8fd9\u9879\u7814\u7a76\u4f7f\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\u5bf9\u553e\u6db2\u817a\u80bf\u7624\u8fdb\u884c\u5168\u5207\u7247\u56fe\u50cf\u4e0a\u7684\u764c\u75c7\u5206\u7c7b\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002\u5229\u7528 CTransPath \u4f5c\u4e3a\u5757\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\uff0cCLAM \u4f5c\u4e3a\u7279\u5f81\u805a\u5408\u5668\uff0c\u5728\u5168\u5207\u7247\u56fe\u50cf\u4e2d\u68c0\u6d4b\u764c\u75c7\u7684 F1 \u5f97\u5206\u8d85\u8fc7 0.88\uff0cAUROC \u4e3a 0.92\u3002|[2408.12275v1](http://arxiv.org/pdf/2408.12275v1)|null|\n", "2408.12248": "|**2024-08-22**|**PRG: Prompt-Based Distillation Without Annotation via Proxy Relational Graph**|PRG\uff1a\u57fa\u4e8e\u4ee3\u7406\u5173\u7cfb\u56fe\u7684\u65e0\u6ce8\u91ca\u63d0\u793a\u5f0f\u84b8\u998f|Yijin Xu, Jialun Liu, Hualiang Wei, Wenhui Li|In this paper, we propose a new distillation method for extracting knowledge from Large Foundation Models (LFM) into lightweight models, introducing a novel supervision mode that does not require manually annotated data. While LFMs exhibit exceptional zero-shot classification abilities across datasets, relying solely on LFM-generated embeddings for distillation poses two main challenges: LFM's task-irrelevant knowledge and the high density of features. The transfer of task-irrelevant knowledge could compromise the student model's discriminative capabilities, and the high density of features within target domains obstructs the extraction of discriminative knowledge essential for the task. To address this issue, we introduce the Proxy Relational Graph (PRG) method. We initially extract task-relevant knowledge from LFMs by calculating a weighted average of logits obtained through text prompt embeddings. Then we construct sample-class proxy graphs for LFM and student models, respectively, to model the correlation between samples and class proxies. Then, we achieve the distillation of selective knowledge by aligning the relational graphs produced by both the LFM and the student model. Specifically, the distillation from LFM to the student model is achieved through two types of alignment: 1) aligning the sample nodes produced by the student model with those produced by the LFM, and 2) aligning the edge relationships in the student model's graph with those in the LFM's graph. Our experimental results validate the effectiveness of PRG, demonstrating its ability to leverage the extensive knowledge base of LFMs while skillfully circumventing their inherent limitations in focused learning scenarios. Notably, in our annotation-free framework, PRG achieves an accuracy of 76.23\\% (T: 77.9\\%) on CIFAR-100 and 72.44\\% (T: 75.3\\%) on the ImageNet-1K.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5927\u578b\u57fa\u7840\u6a21\u578b (LFM) \u4e2d\u7684\u77e5\u8bc6\u63d0\u53d6\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2d\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u4e0d\u9700\u8981\u624b\u52a8\u6ce8\u91ca\u6570\u636e\u7684\u65b0\u578b\u76d1\u7763\u6a21\u5f0f\u3002\u867d\u7136 LFM \u5728\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u4f9d\u9760 LFM \u751f\u6210\u7684\u5d4c\u5165\u8fdb\u884c\u84b8\u998f\u4f1a\u5e26\u6765\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1aLFM \u7684\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\u548c\u9ad8\u5bc6\u5ea6\u7279\u5f81\u3002\u4efb\u52a1\u65e0\u5173\u77e5\u8bc6\u7684\u8f6c\u79fb\u53ef\u80fd\u4f1a\u635f\u5bb3\u5b66\u751f\u6a21\u578b\u7684\u5224\u522b\u80fd\u529b\uff0c\u800c\u76ee\u6807\u57df\u5185\u7684\u9ad8\u5bc6\u5ea6\u7279\u5f81\u4f1a\u963b\u788d\u63d0\u53d6\u5bf9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u7684\u5224\u522b\u77e5\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4ee3\u7406\u5173\u7cfb\u56fe (PRG) \u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7\u8ba1\u7b97\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5d4c\u5165\u83b7\u5f97\u7684 logit \u7684\u52a0\u6743\u5e73\u5747\u503c\u4ece LFM \u4e2d\u63d0\u53d6\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u6211\u4eec\u5206\u522b\u4e3a LFM \u548c\u5b66\u751f\u6a21\u578b\u6784\u5efa\u6837\u672c-\u7c7b\u4ee3\u7406\u56fe\uff0c\u4ee5\u6a21\u62df\u6837\u672c\u548c\u7c7b\u4ee3\u7406\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u9f50 LFM \u548c\u5b66\u751f\u6a21\u578b\u751f\u6210\u7684\u5173\u7cfb\u56fe\u6765\u5b9e\u73b0\u9009\u62e9\u6027\u77e5\u8bc6\u7684\u63d0\u70bc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ece LFM \u5230\u5b66\u751f\u6a21\u578b\u7684\u63d0\u70bc\u662f\u901a\u8fc7\u4e24\u79cd\u7c7b\u578b\u7684\u5bf9\u9f50\u5b9e\u73b0\u7684\uff1a1\uff09\u5c06\u5b66\u751f\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u8282\u70b9\u4e0e LFM \u751f\u6210\u7684\u6837\u672c\u8282\u70b9\u5bf9\u9f50\uff0c2\uff09\u5c06\u5b66\u751f\u6a21\u578b\u56fe\u4e2d\u7684\u8fb9\u7f18\u5173\u7cfb\u4e0e LFM \u56fe\u4e2d\u7684\u8fb9\u7f18\u5173\u7cfb\u5bf9\u9f50\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86 PRG \u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u80fd\u591f\u5229\u7528 LFM \u7684\u5e7f\u6cdb\u77e5\u8bc6\u5e93\uff0c\u540c\u65f6\u5de7\u5999\u5730\u89c4\u907f\u5176\u5728\u4e13\u6ce8\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u6211\u4eec\u7684\u65e0\u6ce8\u91ca\u6846\u67b6\u4e2d\uff0cPRG \u5728 CIFAR-100 \u4e0a\u7684\u51c6\u786e\u7387\u4e3a 76.23\\%\uff08T\uff1a77.9\\%\uff09\uff0c\u5728 ImageNet-1K \u4e0a\u7684\u51c6\u786e\u7387\u4e3a 72.44\\%\uff08T\uff1a75.3\\%\uff09\u3002|[2408.12248v1](http://arxiv.org/pdf/2408.12248v1)|null|\n", "2408.12246": "|**2024-08-22**|**OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text Alignment and Fusion**|OVA-DETR\uff1a\u4f7f\u7528\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\u548c\u878d\u5408\u7684\u5f00\u653e\u8bcd\u6c47\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b|Guoting Wei, Xia Yuan, Yu Liu, Zhenhao Shang, Kelu Yao, Chao Li, Qingsen Yan, Chunxia Zhao, Haokui Zhang, Rong Xiao|Aerial object detection has been a hot topic for many years due to its wide application requirements. However, most existing approaches can only handle predefined categories, which limits their applicability for the open scenarios in real-world. In this paper, we extend aerial object detection to open scenarios by exploiting the relationship between image and text, and propose OVA-DETR, a high-efficiency open-vocabulary detector for aerial images. Specifically, based on the idea of image-text alignment, we propose region-text contrastive loss to replace the category regression loss in the traditional detection framework, which breaks the category limitation. Then, we propose Bidirectional Vision-Language Fusion (Bi-VLF), which includes a dual-attention fusion encoder and a multi-level text-guided Fusion Decoder. The dual-attention fusion encoder enhances the feature extraction process in the encoder part. The multi-level text-guided Fusion Decoder is designed to improve the detection ability for small objects, which frequently appear in aerial object detection scenarios. Experimental results on three widely used benchmark datasets show that our proposed method significantly improves the mAP and recall, while enjoying faster inference speed. For instance, in zero shot detection experiments on DIOR, the proposed OVA-DETR outperforms DescReg and YOLO-World by 37.4% and 33.1%, respectively, while achieving 87 FPS inference speed, which is 7.9x faster than DescReg and 3x faster than YOLO-world. The code is available at https://github.com/GT-Wei/OVA-DETR.|\u7531\u4e8e\u5e94\u7528\u8303\u56f4\u5e7f\u6cdb\uff0c\u7a7a\u4e2d\u76ee\u6807\u68c0\u6d4b\u591a\u5e74\u6765\u4e00\u76f4\u662f\u7814\u7a76\u7684\u70ed\u70b9\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5927\u591a\u6570\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u9884\u5b9a\u4e49\u7684\u7c7b\u522b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u5f00\u653e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u672c\u6587\uff0c\u6211\u4eec\u5229\u7528\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\u5c06\u7a7a\u4e2d\u76ee\u6807\u68c0\u6d4b\u6269\u5c55\u5230\u5f00\u653e\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7a7a\u4e2d\u56fe\u50cf\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668OVA-DETR\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u57fa\u4e8e\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u533a\u57df-\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u6765\u53d6\u4ee3\u4f20\u7edf\u68c0\u6d4b\u6846\u67b6\u4e2d\u7684\u7c7b\u522b\u56de\u5f52\u635f\u5931\uff0c\u4ece\u800c\u6253\u7834\u4e86\u7c7b\u522b\u9650\u5236\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53cc\u5411\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\uff08Bi-VLF\uff09\uff0c\u5176\u4e2d\u5305\u62ec\u53cc\u6ce8\u610f\u878d\u5408\u7f16\u7801\u5668\u548c\u591a\u7ea7\u6587\u672c\u5f15\u5bfc\u7684\u878d\u5408\u89e3\u7801\u5668\u3002\u53cc\u6ce8\u610f\u878d\u5408\u7f16\u7801\u5668\u589e\u5f3a\u4e86\u7f16\u7801\u5668\u90e8\u5206\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u3002\u591a\u7ea7\u6587\u672c\u5f15\u5bfc\u7684\u878d\u5408\u89e3\u7801\u5668\u65e8\u5728\u63d0\u9ad8\u5bf9\u7a7a\u4e2d\u76ee\u6807\u68c0\u6d4b\u573a\u666f\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u5c0f\u7269\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86 mAP \u548c\u53ec\u56de\u7387\uff0c\u540c\u65f6\u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002\u4f8b\u5982\uff0c\u5728 DIOR \u4e0a\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684 OVA-DETR \u5206\u522b\u6bd4 DescReg \u548c YOLO-World \u5206\u522b\u9ad8\u51fa 37.4% \u548c 33.1%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86 87 FPS \u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6bd4 DescReg \u5feb 7.9 \u500d\uff0c\u6bd4 YOLO-world \u5feb 3 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/GT-Wei/OVA-DETR \u4e0a\u627e\u5230\u3002|[2408.12246v1](http://arxiv.org/pdf/2408.12246v1)|null|\n", "2408.12232": "|**2024-08-22**|**BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking**|BihoT\uff1a\u9ad8\u5149\u8c31\u4f2a\u88c5\u7269\u4f53\u8ffd\u8e2a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6|Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du|Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.|\u9ad8\u5149\u8c31\u7269\u4f53\u8ddf\u8e2a (HOT) \u5728\u5404\u79cd\u5e94\u7528\u4e2d\u90fd\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u88ab\u4f2a\u88c5\u7684\u573a\u666f\u4e2d\u3002\u73b0\u6709\u7684\u8ddf\u8e2a\u5668\u53ef\u4ee5\u901a\u8fc7\u6ce2\u6bb5\u91cd\u7ec4\u6709\u6548\u5730\u68c0\u7d22\u7269\u4f53\uff0c\u8fd9\u662f\u56e0\u4e3a\u73b0\u6709 HOT \u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\uff0c\u5176\u4e2d\u5927\u591a\u6570\u7269\u4f53\u5f80\u5f80\u5177\u6709\u53ef\u533a\u5206\u7684\u89c6\u89c9\u5916\u89c2\u800c\u4e0d\u662f\u5149\u8c31\u7279\u5f81\u3002\u8fd9\u79cd\u504f\u5dee\u5141\u8bb8\u8ddf\u8e2a\u5668\u76f4\u63a5\u4f7f\u7528\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u751f\u6210\u7684\u5047\u5f69\u8272\u56fe\u50cf\u4e2d\u83b7\u5f97\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u800c\u65e0\u9700\u63d0\u53d6\u5149\u8c31\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u504f\u5dee\uff0c\u6211\u4eec\u53d1\u73b0\u5f53\u7269\u4f53\u5916\u89c2\u4e0d\u53ef\u9760\u65f6\uff0c\u8ddf\u8e2a\u5668\u5e94\u8be5\u5173\u6ce8\u5149\u8c31\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u9879\u540d\u4e3a\u9ad8\u5149\u8c31\u4f2a\u88c5\u7269\u4f53\u8ddf\u8e2a (HCOT) \u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u7cbe\u5fc3\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21 HCOT \u6570\u636e\u96c6\uff0c\u79f0\u4e3a BihoT\uff0c\u5b83\u7531 41,912 \u5f20\u9ad8\u5149\u8c31\u56fe\u50cf\u7ec4\u6210\uff0c\u6db5\u76d6 49 \u4e2a\u89c6\u9891\u5e8f\u5217\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u5404\u79cd\u4eba\u5de5\u4f2a\u88c5\u573a\u666f\uff0c\u5176\u4e2d\u7269\u4f53\u5177\u6709\u76f8\u4f3c\u7684\u5916\u89c2\u3001\u591a\u6837\u7684\u5149\u8c31\u548c\u9891\u7e41\u7684\u906e\u6321\uff0c\u4f7f\u5176\u6210\u4e3a HCOT \u7684\u4e00\u4e2a\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5373\u57fa\u4e8e\u5149\u8c31\u63d0\u793a\u7684\u5e72\u6270\u611f\u77e5\u7f51\u7edc (SPDAN)\uff0c\u8be5\u6a21\u578b\u7531\u5149\u8c31\u5d4c\u5165\u7f51\u7edc (SEN)\u3001\u57fa\u4e8e\u5149\u8c31\u63d0\u793a\u7684\u9aa8\u5e72\u7f51\u7edc (SPBN) \u548c\u5e72\u6270\u611f\u77e5\u6a21\u5757 (DAM) \u7ec4\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0cSEN \u901a\u8fc7 3-D \u548c 2-D \u5377\u79ef\u63d0\u53d6\u5149\u8c31\u7a7a\u95f4\u7279\u5f81\u3002\u7136\u540e\uff0cSPBN \u4f7f\u7528\u5149\u8c31\u63d0\u793a\u5fae\u8c03\u5f3a\u5927\u7684 RGB \u8ddf\u8e2a\u5668\u5e76\u7f13\u89e3\u8bad\u7ec3\u6837\u672c\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0cDAM \u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u8ba1\u6570\u636e\u6765\u6355\u83b7\u7531\u7269\u4f53\u548c\u80cc\u666f\u906e\u6321\u5f15\u8d77\u7684\u5e72\u6270\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 SPDAN \u5728\u63d0\u51fa\u7684 BihoT \u548c\u5176\u4ed6 HOT \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2408.12232v1](http://arxiv.org/pdf/2408.12232v1)|null|\n", "2408.12211": "|**2024-08-22**|**Computer-Aided Fall Recognition Using a Three-Stream Spatial-Temporal GCN Model with Adaptive Feature Aggregation**|\u4f7f\u7528\u5177\u6709\u81ea\u9002\u5e94\u7279\u5f81\u805a\u5408\u7684\u4e09\u6d41\u65f6\u7a7a GCN \u6a21\u578b\u8fdb\u884c\u8ba1\u7b97\u673a\u8f85\u52a9\u8dcc\u5012\u8bc6\u522b|Jungpil Shin, Abu Saleh Musa Miah, Rei Egawa1, Koki Hirooka, Md. Al Mehedi Hasan, Yoichi Tomioka, Yong Seok Hwang|The prevention of falls is paramount in modern healthcare, particularly for the elderly, as falls can lead to severe injuries or even fatalities. Additionally, the growing incidence of falls among the elderly, coupled with the urgent need to prevent suicide attempts resulting from medication overdose, underscores the critical importance of accurate and efficient fall detection methods. In this scenario, a computer-aided fall detection system is inevitable to save elderly people's lives worldwide. Many researchers have been working to develop fall detection systems. However, the existing fall detection systems often struggle with issues such as unsatisfactory performance accuracy, limited robustness, high computational complexity, and sensitivity to environmental factors due to a lack of effective features. In response to these challenges, this paper proposes a novel three-stream spatial-temporal feature-based fall detection system. Our system incorporates joint skeleton-based spatial and temporal Graph Convolutional Network (GCN) features, joint motion-based spatial and temporal GCN features, and residual connections-based features. Each stream employs adaptive graph-based feature aggregation and consecutive separable convolutional neural networks (Sep-TCN), significantly reducing computational complexity and model parameters compared to prior systems. Experimental results across multiple datasets demonstrate the superior effectiveness and efficiency of our proposed system, with accuracies of 99.51\\%, 99.15\\%, 99.79\\% and 99.85 \\% achieved on the ImViA, UR-Fall, Fall-UP and FU-Kinect datasets, respectively. The remarkable performance of our system highlights its superiority, efficiency, and generalizability in real-world fall detection scenarios, offering significant advancements in healthcare and societal well-being.|\u9884\u9632\u8dcc\u5012\u662f\u73b0\u4ee3\u533b\u7597\u4fdd\u5065\u7684\u91cd\u4e2d\u4e4b\u91cd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8001\u5e74\u4eba\u800c\u8a00\uff0c\u56e0\u4e3a\u8dcc\u5012\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u4f24\u5bb3\u751a\u81f3\u6b7b\u4ea1\u3002\u6b64\u5916\uff0c\u8001\u5e74\u4eba\u8dcc\u5012\u7684\u53d1\u751f\u7387\u4e0d\u65ad\u4e0a\u5347\uff0c\u518d\u52a0\u4e0a\u8feb\u5207\u9700\u8981\u9632\u6b62\u56e0\u836f\u7269\u8fc7\u91cf\u5bfc\u81f4\u7684\u81ea\u6740\uff0c\u8fd9\u4e9b\u90fd\u51f8\u663e\u4e86\u51c6\u786e\u6709\u6548\u7684\u8dcc\u5012\u68c0\u6d4b\u65b9\u6cd5\u7684\u5173\u952e\u91cd\u8981\u6027\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u673a\u8f85\u52a9\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u4e8e\u633d\u6551\u5168\u7403\u8001\u5e74\u4eba\u7684\u751f\u547d\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002\u8bb8\u591a\u7814\u7a76\u4eba\u5458\u4e00\u76f4\u81f4\u529b\u4e8e\u5f00\u53d1\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7531\u4e8e\u7f3a\u4e4f\u6709\u6548\u7684\u7279\u5f81\uff0c\u5e38\u5e38\u9762\u4e34\u6027\u80fd\u7cbe\u5ea6\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3001\u9c81\u68d2\u6027\u6709\u9650\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4ee5\u53ca\u5bf9\u73af\u5883\u56e0\u7d20\u654f\u611f\u7b49\u95ee\u9898\u3002\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u4e09\u6d41\u65f6\u7a7a\u7279\u5f81\u7684\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u7ed3\u5408\u4e86\u57fa\u4e8e\u5173\u8282\u9aa8\u67b6\u7684\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc (GCN) \u7279\u5f81\u3001\u57fa\u4e8e\u5173\u8282\u8fd0\u52a8\u7684\u65f6\u7a7a GCN \u7279\u5f81\u4ee5\u53ca\u57fa\u4e8e\u6b8b\u5dee\u8fde\u63a5\u7684\u7279\u5f81\u3002\u6bcf\u4e2a\u6d41\u90fd\u91c7\u7528\u81ea\u9002\u5e94\u7684\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u805a\u5408\u548c\u8fde\u7eed\u53ef\u5206\u79bb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (Sep-TCN)\uff0c\u4e0e\u4e4b\u524d\u7684\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6a21\u578b\u53c2\u6570\u663e\u8457\u964d\u4f4e\u3002\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u7cfb\u7edf\u7684\u5353\u8d8a\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5728 ImViA\u3001UR-Fall\u3001Fall-UP \u548c FU-Kinect \u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230 99.51%\u300199.15%\u300199.79% \u548c 99.85%\u3002\u6211\u4eec\u7cfb\u7edf\u7684\u51fa\u8272\u6027\u80fd\u51f8\u663e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u8dcc\u5012\u68c0\u6d4b\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3001\u6548\u7387\u548c\u901a\u7528\u6027\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u548c\u793e\u4f1a\u798f\u7949\u63d0\u4f9b\u4e86\u91cd\u5927\u8fdb\u6b65\u3002|[2408.12211v1](http://arxiv.org/pdf/2408.12211v1)|null|\n", "2408.12161": "|**2024-08-22**|**Rebalancing Multi-Label Class-Incremental Learning**|\u91cd\u65b0\u5e73\u8861\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60|Kaile Du, Yifan Zhou, Fan Lyu, Yuyang Li, Junzhou Xie, Yixi Shen, Fuyuan Hu, Guangcan Liu|Multi-label class-incremental learning (MLCIL) is essential for real-world multi-label applications, allowing models to learn new labels while retaining previously learned knowledge continuously. However, recent MLCIL approaches can only achieve suboptimal performance due to the oversight of the positive-negative imbalance problem, which manifests at both the label and loss levels because of the task-level partial label issue. The imbalance at the label level arises from the substantial absence of negative labels, while the imbalance at the loss level stems from the asymmetric contributions of the positive and negative loss parts to the optimization. To address the issue above, we propose a Rebalance framework for both the Loss and Label levels (RebLL), which integrates two key modules: asymmetric knowledge distillation (AKD) and online relabeling (OR). AKD is proposed to rebalance at the loss level by emphasizing the negative label learning in classification loss and down-weighting the contribution of overconfident predictions in distillation loss. OR is designed for label rebalance, which restores the original class distribution in memory by online relabeling the missing classes. Our comprehensive experiments on the PASCAL VOC and MS-COCO datasets demonstrate that this rebalancing strategy significantly improves performance, achieving new state-of-the-art results even with a vanilla CNN backbone.|\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60 (MLCIL) \u5bf9\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u6807\u7b7e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u5141\u8bb8\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u6807\u7b7e\u7684\u540c\u65f6\u4e0d\u65ad\u4fdd\u7559\u4ee5\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684 MLCIL \u65b9\u6cd5\u53ea\u80fd\u5b9e\u73b0\u6b21\u4f18\u6027\u80fd\uff0c\u56e0\u4e3a\u5ffd\u7565\u4e86\u6b63\u8d1f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7531\u4e8e\u4efb\u52a1\u7ea7\u522b\u7684\u90e8\u5206\u6807\u7b7e\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5728\u6807\u7b7e\u548c\u635f\u5931\u7ea7\u522b\u90fd\u8868\u73b0\u51fa\u6765\u3002\u6807\u7b7e\u7ea7\u522b\u7684\u4e0d\u5e73\u8861\u6e90\u4e8e\u8d1f\u6807\u7b7e\u7684\u5927\u91cf\u7f3a\u5931\uff0c\u800c\u635f\u5931\u7ea7\u522b\u7684\u4e0d\u5e73\u8861\u6e90\u4e8e\u6b63\u8d1f\u635f\u5931\u90e8\u5206\u5bf9\u4f18\u5316\u7684\u8d21\u732e\u4e0d\u5bf9\u79f0\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u635f\u5931\u548c\u6807\u7b7e\u7ea7\u522b\u7684\u91cd\u65b0\u5e73\u8861\u6846\u67b6 (RebLL)\uff0c\u5b83\u96c6\u6210\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u4e0d\u5bf9\u79f0\u77e5\u8bc6\u84b8\u998f (AKD) \u548c\u5728\u7ebf\u91cd\u65b0\u6807\u8bb0 (OR)\u3002AKD \u88ab\u63d0\u51fa\u901a\u8fc7\u5f3a\u8c03\u5206\u7c7b\u635f\u5931\u4e2d\u7684\u8d1f\u6807\u7b7e\u5b66\u4e60\u5e76\u964d\u4f4e\u84b8\u998f\u635f\u5931\u4e2d\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\u7684\u8d21\u732e\u6765\u5728\u635f\u5931\u7ea7\u522b\u8fdb\u884c\u91cd\u65b0\u5e73\u8861\u3002 OR \u4e13\u4e3a\u6807\u7b7e\u91cd\u65b0\u5e73\u8861\u800c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5728\u7ebf\u91cd\u65b0\u6807\u8bb0\u7f3a\u5931\u7684\u7c7b\u522b\u6765\u6062\u590d\u5185\u5b58\u4e2d\u7684\u539f\u59cb\u7c7b\u522b\u5206\u5e03\u3002\u6211\u4eec\u5728 PASCAL VOC \u548c MS-COCO \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u91cd\u65b0\u5e73\u8861\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5373\u4f7f\u4f7f\u7528 vanilla CNN \u4e3b\u5e72\u4e5f\u80fd\u5b9e\u73b0\u65b0\u7684\u6700\u4f73\u7ed3\u679c\u3002|[2408.12161v1](http://arxiv.org/pdf/2408.12161v1)|null|\n", "2408.12111": "|**2024-08-22**|**ZipGait: Bridging Skeleton and Silhouette with Diffusion Model for Advancing Gait Recognition**|ZipGait\uff1a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fde\u63a5\u9aa8\u67b6\u548c\u8f6e\u5ed3\uff0c\u63d0\u9ad8\u6b65\u6001\u8bc6\u522b\u80fd\u529b|Fanxu Min, Qing Cai, Shaoxiang Guo, Yang Yu, Hao Fan, Junyu Dong|Current gait recognition research predominantly focuses on extracting appearance features effectively, but the performance is severely compromised by the vulnerability of silhouettes under unconstrained scenes. Consequently, numerous studies have explored how to harness information from various models, particularly by sufficiently utilizing the intrinsic information of skeleton sequences. While these model-based methods have achieved significant performance, there is still a huge gap compared to appearance-based methods, which implies the potential value of bridging silhouettes and skeletons. In this work, we make the first attempt to reconstruct dense body shapes from discrete skeleton distributions via the diffusion model, demonstrating a new approach that connects cross-modal features rather than focusing solely on intrinsic features to improve model-based methods. To realize this idea, we propose a novel gait diffusion model named DiffGait, which has been designed with four specific adaptations suitable for gait recognition. Furthermore, to effectively utilize the reconstructed silhouettes and skeletons, we introduce Perception Gait Integration (PGI) to integrate different gait features through a two-stage process. Incorporating those modifications leads to an efficient model-based gait recognition framework called ZipGait. Through extensive experiments on four public benchmarks, ZipGait demonstrates superior performance, outperforming the state-of-the-art methods by a large margin under both cross-domain and intra-domain settings, while achieving significant plug-and-play performance improvements.|\u5f53\u524d\u7684\u6b65\u6001\u8bc6\u522b\u7814\u7a76\u4e3b\u8981\u4fa7\u91cd\u4e8e\u6709\u6548\u63d0\u53d6\u5916\u89c2\u7279\u5f81\uff0c\u4f46\u5728\u65e0\u7ea6\u675f\u573a\u666f\u4e0b\u8f6e\u5ed3\u7684\u8106\u5f31\u6027\u4e25\u91cd\u635f\u5bb3\u4e86\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8bb8\u591a\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u5229\u7528\u6765\u81ea\u5404\u79cd\u6a21\u578b\u7684\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5145\u5206\u5229\u7528\u9aa8\u67b6\u5e8f\u5217\u7684\u5185\u5728\u4fe1\u606f\u3002\u867d\u7136\u8fd9\u4e9b\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u4f46\u4e0e\u57fa\u4e8e\u5916\u89c2\u7684\u65b9\u6cd5\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u5de8\u5927\u7684\u5dee\u8ddd\uff0c\u8fd9\u610f\u5473\u7740\u8fde\u63a5\u8f6e\u5ed3\u548c\u9aa8\u67b6\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4ece\u79bb\u6563\u9aa8\u67b6\u5206\u5e03\u4e2d\u91cd\u5efa\u5bc6\u96c6\u7684\u8eab\u4f53\u5f62\u72b6\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u8fde\u63a5\u8de8\u6a21\u6001\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u5173\u6ce8\u5185\u5728\u7279\u5f81\u6765\u6539\u8fdb\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiffGait \u7684\u65b0\u578b\u6b65\u6001\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8bbe\u8ba1\u4e86\u56db\u79cd\u9002\u5408\u6b65\u6001\u8bc6\u522b\u7684\u7279\u5b9a\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6709\u6548\u5229\u7528\u91cd\u5efa\u7684\u8f6e\u5ed3\u548c\u9aa8\u67b6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u611f\u77e5\u6b65\u6001\u6574\u5408 (PGI)\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u6574\u5408\u4e0d\u540c\u7684\u6b65\u6001\u7279\u5f81\u3002\u7ed3\u5408\u8fd9\u4e9b\u4fee\u6539\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u6b65\u6001\u8bc6\u522b\u6846\u67b6 ZipGait\u3002\u901a\u8fc7\u5bf9\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cZipGait \u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u8de8\u57df\u548c\u57df\u5185\u8bbe\u7f6e\u4e0b\u90fd\u8fdc\u8fdc\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5373\u63d2\u5373\u7528\u6027\u80fd\u6539\u8fdb\u3002|[2408.12111v1](http://arxiv.org/pdf/2408.12111v1)|null|\n", "2408.12102": "|**2024-08-22**|**Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization**|\u6574\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\u4ee5\u589e\u5f3a\u591a\u6a21\u5f0f\u8bf4\u8bdd\u4eba\u5206\u7c7b|Luyao Cheng, Hui Wang, Siqi Zheng, Yafeng Chen, Rongjie Huang, Qinglin Zhang, Qian Chen, Xihao Li|Speaker diarization, the process of segmenting an audio stream or transcribed speech content into homogenous partitions based on speaker identity, plays a crucial role in the interpretation and analysis of human speech. Most existing speaker diarization systems rely exclusively on unimodal acoustic information, making the task particularly challenging due to the innate ambiguities of audio signals. Recent studies have made tremendous efforts towards audio-visual or audio-semantic modeling to enhance performance. However, even the incorporation of up to two modalities often falls short in addressing the complexities of spontaneous and unstructured conversations. To exploit more meaningful dialogue patterns, we propose a novel multimodal approach that jointly utilizes audio, visual, and semantic cues to enhance speaker diarization. Our method elegantly formulates the multimodal modeling as a constrained optimization problem. First, we build insights into the visual connections among active speakers and the semantic interactions within spoken content, thereby establishing abundant pairwise constraints. Then we introduce a joint pairwise constraint propagation algorithm to cluster speakers based on these visual and semantic constraints. This integration effectively leverages the complementary strengths of different modalities, refining the affinity estimation between individual speaker embeddings. Extensive experiments conducted on multiple multimodal datasets demonstrate that our approach consistently outperforms state-of-the-art speaker diarization methods.|\u8bf4\u8bdd\u4eba\u5206\u7c7b\u662f\u6839\u636e\u8bf4\u8bdd\u4eba\u8eab\u4efd\u5c06\u97f3\u9891\u6d41\u6216\u8f6c\u5f55\u8bed\u97f3\u5185\u5bb9\u5206\u5272\u6210\u540c\u8d28\u5206\u533a\u7684\u8fc7\u7a0b\uff0c\u5728\u4eba\u7c7b\u8bed\u97f3\u7684\u89e3\u91ca\u548c\u5206\u6790\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u5206\u7c7b\u7cfb\u7edf\u5b8c\u5168\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u58f0\u5b66\u4fe1\u606f\uff0c\u7531\u4e8e\u97f3\u9891\u4fe1\u53f7\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u8fd9\u9879\u4efb\u52a1\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5728\u89c6\u542c\u6216\u97f3\u9891\u8bed\u4e49\u5efa\u6a21\u65b9\u9762\u505a\u51fa\u4e86\u5de8\u5927\u52aa\u529b\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u7136\u800c\uff0c\u5373\u4f7f\u7ed3\u5408\u591a\u8fbe\u4e24\u79cd\u6a21\u6001\uff0c\u4e5f\u5e38\u5e38\u65e0\u6cd5\u89e3\u51b3\u81ea\u53d1\u548c\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u5f00\u53d1\u66f4\u6709\u610f\u4e49\u7684\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u5229\u7528\u97f3\u9891\u3001\u89c6\u89c9\u548c\u8bed\u4e49\u7ebf\u7d22\u6765\u589e\u5f3a\u8bf4\u8bdd\u4eba\u5206\u7c7b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u96c5\u5730\u5c06\u591a\u6a21\u6001\u5efa\u6a21\u8868\u8ff0\u4e3a\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u6df1\u5165\u4e86\u89e3\u6d3b\u8dc3\u8bf4\u8bdd\u4eba\u4e4b\u95f4\u7684\u89c6\u89c9\u8054\u7cfb\u4ee5\u53ca\u53e3\u8bed\u5185\u5bb9\u4e2d\u7684\u8bed\u4e49\u4ea4\u4e92\uff0c\u4ece\u800c\u5efa\u7acb\u4e30\u5bcc\u7684\u6210\u5bf9\u7ea6\u675f\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8054\u5408\u6210\u5bf9\u7ea6\u675f\u4f20\u64ad\u7b97\u6cd5\uff0c\u6839\u636e\u8fd9\u4e9b\u89c6\u89c9\u548c\u8bed\u4e49\u7ea6\u675f\u5bf9\u8bf4\u8bdd\u4eba\u8fdb\u884c\u805a\u7c7b\u3002\u8fd9\u79cd\u6574\u5408\u6709\u6548\u5730\u5229\u7528\u4e86\u4e0d\u540c\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6539\u8fdb\u4e86\u5355\u4e2a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e4b\u95f4\u7684\u4eb2\u548c\u529b\u4f30\u8ba1\u3002\u5728\u591a\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8bf4\u8bdd\u4eba\u5206\u7c7b\u65b9\u6cd5\u3002|[2408.12102v1](http://arxiv.org/pdf/2408.12102v1)|null|\n", "2408.12099": "|**2024-08-22**|**Query-Efficient Video Adversarial Attack with Stylized Logo**|\u5177\u6709\u98ce\u683c\u5316\u5fbd\u6807\u7684\u67e5\u8be2\u9ad8\u6548\u89c6\u9891\u5bf9\u6297\u653b\u51fb|Duoxun Tang, Yuxin Cao, Xi Xiao, Derui Wang, Sheng Wen, Tianqing Zhu|Video classification systems based on Deep Neural Networks (DNNs) have demonstrated excellent performance in accurately verifying video content. However, recent studies have shown that DNNs are highly vulnerable to adversarial examples. Therefore, a deep understanding of adversarial attacks can better respond to emergency situations. In order to improve attack performance, many style-transfer-based attacks and patch-based attacks have been proposed. However, the global perturbation of the former will bring unnatural global color, while the latter is difficult to achieve success in targeted attacks due to the limited perturbation space. Moreover, compared to a plethora of methods targeting image classifiers, video adversarial attacks are still not that popular. Therefore, to generate adversarial examples with a low budget and to provide them with a higher verisimilitude, we propose a novel black-box video attack framework, called Stylized Logo Attack (SLA). SLA is conducted through three steps. The first step involves building a style references set for logos, which can not only make the generated examples more natural, but also carry more target class features in the targeted attacks. Then, reinforcement learning (RL) is employed to determine the style reference and position parameters of the logo within the video, which ensures that the stylized logo is placed in the video with optimal attributes. Finally, perturbation optimization is designed to optimize perturbations to improve the fooling rate in a step-by-step manner. Sufficient experimental results indicate that, SLA can achieve better performance than state-of-the-art methods and still maintain good deception effects when facing various defense methods.|\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u7684\u89c6\u9891\u5206\u7c7b\u7cfb\u7edf\u5728\u51c6\u786e\u9a8c\u8bc1\u89c6\u9891\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cDNN \u6781\u6613\u53d7\u5230\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u6df1\u5165\u4e86\u89e3\u5bf9\u6297\u6027\u653b\u51fb\u53ef\u4ee5\u66f4\u597d\u5730\u5e94\u5bf9\u7d27\u6025\u60c5\u51b5\u3002\u4e3a\u4e86\u63d0\u9ad8\u653b\u51fb\u6027\u80fd\uff0c\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u57fa\u4e8e\u98ce\u683c\u8fc1\u79fb\u7684\u653b\u51fb\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u653b\u51fb\u3002\u7136\u800c\uff0c\u524d\u8005\u7684\u5168\u5c40\u6270\u52a8\u4f1a\u5e26\u6765\u4e0d\u81ea\u7136\u7684\u5168\u5c40\u8272\u5f69\uff0c\u800c\u540e\u8005\u7531\u4e8e\u6270\u52a8\u7a7a\u95f4\u6709\u9650\uff0c\u96be\u4ee5\u5728\u9488\u5bf9\u6027\u653b\u51fb\u4e2d\u53d6\u5f97\u6210\u529f\u3002\u6b64\u5916\uff0c\u4e0e\u5927\u91cf\u9488\u5bf9\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u89c6\u9891\u5bf9\u6297\u6027\u653b\u51fb\u4ecd\u7136\u4e0d\u90a3\u4e48\u53d7\u6b22\u8fce\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u4ee5\u8f83\u4f4e\u7684\u9884\u7b97\u751f\u6210\u5bf9\u6297\u6027\u793a\u4f8b\u5e76\u4e3a\u5176\u63d0\u4f9b\u66f4\u9ad8\u7684\u903c\u771f\u5ea6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2\u89c6\u9891\u653b\u51fb\u6846\u67b6\uff0c\u79f0\u4e3a\u98ce\u683c\u5316\u5fbd\u6807\u653b\u51fb (SLA)\u3002SLA \u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u8fdb\u884c\u3002\u7b2c\u4e00\u6b65\u662f\u4e3a\u5fbd\u6807\u6784\u5efa\u98ce\u683c\u53c2\u8003\u96c6\uff0c\u8fd9\u4e0d\u4ec5\u53ef\u4ee5\u4f7f\u751f\u6210\u7684\u6837\u672c\u66f4\u52a0\u81ea\u7136\uff0c\u800c\u4e14\u53ef\u4ee5\u5728\u9488\u5bf9\u6027\u653b\u51fb\u4e2d\u643a\u5e26\u66f4\u591a\u7684\u76ee\u6807\u7c7b\u522b\u7279\u5f81\u3002\u7136\u540e\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u786e\u5b9a\u89c6\u9891\u4e2d\u5fbd\u6807\u7684\u98ce\u683c\u53c2\u8003\u548c\u4f4d\u7f6e\u53c2\u6570\uff0c\u786e\u4fdd\u98ce\u683c\u5316\u7684\u5fbd\u6807\u4ee5\u6700\u4f18\u5c5e\u6027\u653e\u7f6e\u5728\u89c6\u9891\u4e2d\u3002\u6700\u540e\uff0c\u8bbe\u8ba1\u6270\u52a8\u4f18\u5316\uff0c\u9010\u6b65\u4f18\u5316\u6270\u52a8\u4ee5\u63d0\u9ad8\u6b3a\u9a97\u7387\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSLA \u53ef\u4ee5\u53d6\u5f97\u6bd4\u6700\u65b0\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u5404\u79cd\u9632\u5fa1\u65b9\u6cd5\u65f6\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u6b3a\u9a97\u6548\u679c\u3002|[2408.12099v1](http://arxiv.org/pdf/2408.12099v1)|null|\n", "2408.12093": "|**2024-08-22**|**LLM-enhanced Scene Graph Learning for Household Rearrangement**|\u6cd5\u5b66\u7855\u58eb (LLM) \u589e\u5f3a\u578b\u573a\u666f\u56fe\u5b66\u4e60\uff0c\u7528\u4e8e\u5bb6\u5ead\u91cd\u65b0\u5e03\u7f6e|Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan, Chenyang Zhu, Ruizhen Hu, Kai Xu|The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.|\u5bb6\u5ead\u91cd\u65b0\u5e03\u7f6e\u4efb\u52a1\u6d89\u53ca\u5728\u573a\u666f\u4e2d\u53d1\u73b0\u653e\u9519\u4f4d\u7f6e\u7684\u7269\u4f53\u5e76\u5c06\u5b83\u4eec\u5b89\u7f6e\u5728\u9002\u5f53\u7684\u4f4d\u7f6e\u3002\u5b83\u65e2\u53d6\u51b3\u4e8e\u5ba2\u89c2\u65b9\u9762\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u4e5f\u53d6\u51b3\u4e8e\u4e3b\u89c2\u65b9\u9762\u7684\u4eba\u7c7b\u7528\u6237\u504f\u597d\u3002\u5728\u5b8c\u6210\u6b64\u7c7b\u4efb\u52a1\u65f6\uff0c\u6211\u4eec\u5efa\u8bae\u76f4\u63a5\u4ece\u573a\u666f\u672c\u8eab\u6316\u6398\u5177\u6709\u7528\u6237\u504f\u597d\u5bf9\u9f50\u7684\u5bf9\u8c61\u529f\u80fd\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u4eba\u5de5\u5e72\u9884\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4f7f\u7528\u573a\u666f\u56fe\u8868\u793a\u5e76\u63d0\u51fa LLM \u589e\u5f3a\u7684\u573a\u666f\u56fe\u5b66\u4e60\uff0c\u5c06\u8f93\u5165\u573a\u666f\u56fe\u8f6c\u6362\u4e3a\u5177\u6709\u4fe1\u606f\u589e\u5f3a\u8282\u70b9\u548c\u65b0\u53d1\u73b0\u8fb9\u7f18\uff08\u5173\u7cfb\uff09\u7684\u53ef\u4f9b\u6027\u589e\u5f3a\u56fe (AEG)\u3002\u5728 AEG \u4e2d\uff0c\u4e0e\u5bb9\u5668\u5bf9\u8c61\u76f8\u5bf9\u5e94\u7684\u8282\u70b9\u901a\u8fc7\u4e0a\u4e0b\u6587\u8bf1\u5bfc\u7684\u53ef\u4f9b\u6027\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u8be5\u53ef\u4f9b\u6027\u7f16\u7801\u4e86\u53ef\u4ee5\u653e\u7f6e\u5728\u5176\u4e0a\u7684\u53ef\u643a\u5e26\u5bf9\u8c61\u7c7b\u578b\u3002\u901a\u8fc7\u65b0\u53d1\u73b0\u7684\u975e\u5c40\u90e8\u5173\u7cfb\u53d1\u73b0\u4e86\u65b0\u7684\u8fb9\u7f18\u3002\u4f7f\u7528 AEG\uff0c\u6211\u4eec\u901a\u8fc7\u68c0\u6d4b\u653e\u9519\u4f4d\u7f6e\u7684\u53ef\u643a\u5e26\u7269\u5e76\u786e\u5b9a\u6bcf\u4e2a\u53ef\u643a\u5e26\u7269\u7684\u9002\u5f53\u4f4d\u7f6e\u6765\u6267\u884c\u573a\u666f\u91cd\u65b0\u5e03\u7f6e\u7684\u4efb\u52a1\u89c4\u5212\u3002\u6211\u4eec\u901a\u8fc7\u5728\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u6574\u7406\u673a\u5668\u4eba\u6765\u6d4b\u8bd5\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6211\u4eec\u6784\u5efa\u7684\u65b0\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u9519\u4f4d\u68c0\u6d4b\u548c\u968f\u540e\u7684\u91cd\u6392\u89c4\u5212\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2408.12093v1](http://arxiv.org/pdf/2408.12093v1)|null|\n", "2408.12086": "|**2024-08-22**|**Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and VisualAnalysis Strategy**|\u89e3\u9501\u5c5e\u6027\u5bf9\u6210\u529f\u4f2a\u88c5\u7684\u8d21\u732e\uff1a\u6587\u672c\u4e0e\u89c6\u89c9\u5206\u6790\u76f8\u7ed3\u5408\u7684\u7b56\u7565|Hong Zhang, Yixuan Lyu, Qian Yu, Hanyang Liu, Huimin Ma, Ding Yuan, Yifan Yang|In the domain of Camouflaged Object Segmentation (COS), despite continuous improvements in segmentation performance, the underlying mechanisms of effective camouflage remain poorly understood, akin to a black box. To address this gap, we present the first comprehensive study to examine the impact of camouflage attributes on the effectiveness of camouflage patterns, offering a quantitative framework for the evaluation of camouflage designs. To support this analysis, we have compiled the first dataset comprising descriptions of camouflaged objects and their attribute contributions, termed COD-Text And X-attributions (COD-TAX). Moreover, drawing inspiration from the hierarchical process by which humans process information: from high-level textual descriptions of overarching scenarios, through mid-level summaries of local areas, to low-level pixel data for detailed analysis. We have developed a robust framework that combines textual and visual information for the task of COS, named Attribution CUe Modeling with Eye-fixation Network (ACUMEN). ACUMEN demonstrates superior performance, outperforming nine leading methods across three widely-used datasets. We conclude by highlighting key insights derived from the attributes identified in our study. Code: https://github.com/lyu-yx/ACUMEN.|\u5728\u4f2a\u88c5\u5bf9\u8c61\u5206\u5272 (COS) \u9886\u57df\uff0c\u5c3d\u7ba1\u5206\u5272\u6027\u80fd\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u6709\u6548\u4f2a\u88c5\u7684\u5e95\u5c42\u673a\u5236\u4ecd\u7136\u4e0d\u592a\u4e3a\u4eba\u6240\u77e5\uff0c\u5c31\u50cf\u4e00\u4e2a\u9ed1\u5323\u5b50\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u9879\u5168\u9762\u7684\u7814\u7a76\uff0c\u4ee5\u7814\u7a76\u4f2a\u88c5\u5c5e\u6027\u5bf9\u4f2a\u88c5\u56fe\u6848\u6709\u6548\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u4f2a\u88c5\u8bbe\u8ba1\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u5206\u6790\uff0c\u6211\u4eec\u7f16\u5236\u4e86\u7b2c\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f2a\u88c5\u5bf9\u8c61\u53ca\u5176\u5c5e\u6027\u8d21\u732e\u7684\u63cf\u8ff0\uff0c\u79f0\u4e3a COD-Text \u548c X-attributions (COD-TAX)\u3002\u6b64\u5916\uff0c\u4ece\u4eba\u7c7b\u5904\u7406\u4fe1\u606f\u7684\u5c42\u6b21\u5316\u8fc7\u7a0b\u4e2d\u6c72\u53d6\u7075\u611f\uff1a\u4ece\u603b\u4f53\u573a\u666f\u7684\u9ad8\u7ea7\u6587\u672c\u63cf\u8ff0\uff0c\u5230\u5c40\u90e8\u533a\u57df\u7684\u4e2d\u7ea7\u6458\u8981\uff0c\u518d\u5230\u7528\u4e8e\u8be6\u7ec6\u5206\u6790\u7684\u4f4e\u7ea7\u50cf\u7d20\u6570\u636e\u3002\u6211\u4eec\u4e3a COS \u4efb\u52a1\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5f3a\u5927\u6846\u67b6\uff0c\u79f0\u4e3a\u5f52\u56e0 CUe \u5efa\u6a21\u4e0e\u773c\u52a8\u7f51\u7edc (ACUMEN)\u3002 ACUMEN \u8868\u73b0\u51fa\u8272\uff0c\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4e5d\u79cd\u9886\u5148\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4ece\u7814\u7a76\u4e2d\u786e\u5b9a\u7684\u5c5e\u6027\u4e2d\u5f97\u51fa\u7684\u5173\u952e\u89c1\u89e3\u3002\u4ee3\u7801\uff1ahttps://github.com/lyu-yx/ACUMEN\u3002|[2408.12086v1](http://arxiv.org/pdf/2408.12086v1)|null|\n", "2408.12084": "|**2024-08-22**|**Vision-Based Detection of Uncooperative Targets and Components on Small Satellites**|\u57fa\u4e8e\u89c6\u89c9\u7684\u5c0f\u578b\u536b\u661f\u4e0d\u5408\u4f5c\u76ee\u6807\u53ca\u90e8\u4ef6\u68c0\u6d4b|Hannah Grauer, Elena-Sorina Lupu, Connor Lee, Soon-Jo Chung, Darren Rowen, Benjamen Bycroft, Phaedrus Leeds, John Brader|Space debris and inactive satellites pose a threat to the safety and integrity of operational spacecraft and motivate the need for space situational awareness techniques. These uncooperative targets create a challenging tracking and detection problem due to a lack of prior knowledge of their features, trajectories, or even existence. Recent advancements in computer vision models can be used to improve upon existing methods for tracking such uncooperative targets to make them more robust and reliable to the wide-ranging nature of the target. This paper introduces an autonomous detection model designed to identify and monitor these objects using learning and computer vision. The autonomous detection method aims to identify and accurately track the uncooperative targets in varied circumstances, including different camera spectral sensitivities, lighting, and backgrounds. Our method adapts to the relative distance between the observing spacecraft and the target, and different detection strategies are adjusted based on distance. At larger distances, we utilize You Only Look Once (YOLOv8), a multitask Convolutional Neural Network (CNN), for zero-shot and domain-specific single-shot real time detection of the target. At shorter distances, we use knowledge distillation to combine visual foundation models with a lightweight fast segmentation CNN (Fast-SCNN) to segment the spacecraft components with low storage requirements and fast inference times, and to enable weight updates from earth and possible onboard training. Lastly, we test our method on a custom dataset simulating the unique conditions encountered in space, as well as a publicly-available dataset.|\u7a7a\u95f4\u788e\u7247\u548c\u505c\u7528\u536b\u661f\u5bf9\u8fd0\u884c\u822a\u5929\u5668\u7684\u5b89\u5168\u6027\u548c\u5b8c\u6574\u6027\u6784\u6210\u5a01\u80c1\uff0c\u5e76\u63a8\u52a8\u4e86\u5bf9\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\u6280\u672f\u7684\u9700\u6c42\u3002\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u5176\u7279\u5f81\u3001\u8f68\u8ff9\u751a\u81f3\u5b58\u5728\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u4e0d\u5408\u4f5c\u7684\u76ee\u6807\u4ea7\u751f\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u8ddf\u8e2a\u548c\u68c0\u6d4b\u95ee\u9898\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u53ef\u7528\u4e8e\u6539\u8fdb\u73b0\u6709\u7684\u8ddf\u8e2a\u6b64\u7c7b\u4e0d\u5408\u4f5c\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u5bf9\u76ee\u6807\u7684\u5e7f\u6cdb\u6027\u8d28\u66f4\u52a0\u7a33\u5065\u548c\u53ef\u9760\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u4e3b\u68c0\u6d4b\u6a21\u578b\uff0c\u65e8\u5728\u4f7f\u7528\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6765\u8bc6\u522b\u548c\u76d1\u63a7\u8fd9\u4e9b\u7269\u4f53\u3002\u81ea\u4e3b\u68c0\u6d4b\u65b9\u6cd5\u65e8\u5728\u8bc6\u522b\u548c\u51c6\u786e\u8ddf\u8e2a\u5404\u79cd\u60c5\u51b5\u4e0b\u7684\u4e0d\u5408\u4f5c\u76ee\u6807\uff0c\u5305\u62ec\u4e0d\u540c\u7684\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u3001\u7167\u660e\u548c\u80cc\u666f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9002\u5e94\u89c2\u5bdf\u822a\u5929\u5668\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\uff0c\u5e76\u6839\u636e\u8ddd\u79bb\u8c03\u6574\u4e0d\u540c\u7684\u68c0\u6d4b\u7b56\u7565\u3002\u5728\u66f4\u8fdc\u7684\u8ddd\u79bb\uff0c\u6211\u4eec\u5229\u7528\u591a\u4efb\u52a1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) You Only Look Once (YOLOv8) \u5bf9\u76ee\u6807\u8fdb\u884c\u96f6\u6b21\u548c\u9886\u57df\u7279\u5b9a\u7684\u5355\u6b21\u5b9e\u65f6\u68c0\u6d4b\u3002\u5728\u8f83\u77ed\u8ddd\u79bb\u5185\uff0c\u6211\u4eec\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u5feb\u901f\u5206\u5272 CNN (Fast-SCNN) \u76f8\u7ed3\u5408\uff0c\u4ee5\u4f4e\u5b58\u50a8\u8981\u6c42\u548c\u5feb\u901f\u63a8\u7406\u65f6\u95f4\u5206\u5272\u822a\u5929\u5668\u7ec4\u4ef6\uff0c\u5e76\u5b9e\u73b0\u4ece\u5730\u7403\u66f4\u65b0\u91cd\u91cf\u548c\u53ef\u80fd\u7684\u673a\u8f7d\u8bad\u7ec3\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u6a21\u62df\u592a\u7a7a\u4e2d\u9047\u5230\u7684\u72ec\u7279\u6761\u4ef6\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4ee5\u53ca\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u6211\u4eec\u7684\u65b9\u6cd5\u3002|[2408.12084v1](http://arxiv.org/pdf/2408.12084v1)|null|\n", "2408.12062": "|**2024-08-22**|**Enhancing Sampling Protocol for Robust Point Cloud Classification**|\u589e\u5f3a\u91c7\u6837\u534f\u8bae\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u70b9\u4e91\u5206\u7c7b|Chongshou Li, Pin Tang, Xinke Li, Tianrui Li|Established sampling protocols for 3D point cloud learning, such as Farthest Point Sampling (FPS) and Fixed Sample Size (FSS), have long been recognized and utilized. However, real-world data often suffer from corrputions such as sensor noise, which violates the benignness assumption of point cloud in current protocols. Consequently, they are notably vulnerable to noise, posing significant safety risks in critical applications like autonomous driving. To address these issues, we propose an enhanced point cloud sampling protocol, PointDR, which comprises two components: 1) Downsampling for key point identification and 2) Resampling for flexible sample size. Furthermore, differentiated strategies are implemented for training and inference processes. Particularly, an isolation-rated weight considering local density is designed for the downsampling method, assisting it in performing random key points selection in the training phase and bypassing noise in the inference phase. A local-geometry-preserved upsampling is incorporated into resampling, facilitating it to maintain a stochastic sample size in the training stage and complete insufficient data in the inference. It is crucial to note that the proposed protocol is free of model architecture altering and extra learning, thus minimal efforts are demanded for its replacement of the existing one. Despite the simplicity, it substantially improves the robustness of point cloud learning, showcased by outperforming the state-of-the-art methods on multiple benchmarks of corrupted point cloud classification. The code will be available upon the paper's acceptance.|\u7528\u4e8e 3D \u70b9\u4e91\u5b66\u4e60\u7684\u6210\u719f\u91c7\u6837\u534f\u8bae\uff0c\u4f8b\u5982\u6700\u8fdc\u70b9\u91c7\u6837 (FPS) \u548c\u56fa\u5b9a\u6837\u672c\u5927\u5c0f (FSS)\uff0c\u65e9\u5df2\u5f97\u5230\u8ba4\u53ef\u548c\u5229\u7528\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u901a\u5e38\u4f1a\u53d7\u5230\u4f20\u611f\u5668\u566a\u58f0\u7b49\u635f\u574f\u7684\u5f71\u54cd\uff0c\u8fd9\u8fdd\u53cd\u4e86\u5f53\u524d\u534f\u8bae\u4e2d\u70b9\u4e91\u7684\u826f\u6027\u5047\u8bbe\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u7279\u522b\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7b49\u5173\u952e\u5e94\u7528\u9020\u6210\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u70b9\u4e91\u91c7\u6837\u534f\u8bae PointDR\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a1) \u7528\u4e8e\u5173\u952e\u70b9\u8bc6\u522b\u7684\u4e0b\u91c7\u6837\u548c 2) \u7528\u4e8e\u7075\u6d3b\u6837\u672c\u5927\u5c0f\u7684\u91cd\u91c7\u6837\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u5b9e\u65bd\u4e86\u5dee\u5f02\u5316\u7b56\u7565\u3002\u7279\u522b\u5730\uff0c\u4e3a\u4e0b\u91c7\u6837\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8003\u8651\u5c40\u90e8\u5bc6\u5ea6\u7684\u9694\u79bb\u7ea7\u6743\u91cd\uff0c\u5e2e\u52a9\u5176\u5728\u8bad\u7ec3\u9636\u6bb5\u6267\u884c\u968f\u673a\u5173\u952e\u70b9\u9009\u62e9\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u7ed5\u8fc7\u566a\u58f0\u3002\u5c40\u90e8\u51e0\u4f55\u4fdd\u7559\u7684\u4e0a\u91c7\u6837\u88ab\u7eb3\u5165\u91cd\u91c7\u6837\u4e2d\uff0c\u6709\u52a9\u4e8e\u5728\u8bad\u7ec3\u9636\u6bb5\u4fdd\u6301\u968f\u673a\u6837\u672c\u5927\u5c0f\u5e76\u5728\u63a8\u7406\u4e2d\u8865\u5145\u4e0d\u8db3\u7684\u6570\u636e\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6240\u63d0\u51fa\u7684\u534f\u8bae\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u548c\u989d\u5916\u5b66\u4e60\uff0c\u56e0\u6b64\u53ea\u9700\u4ed8\u51fa\u5f88\u5c11\u7684\u52aa\u529b\u5373\u53ef\u53d6\u4ee3\u73b0\u6709\u7684\u534f\u8bae\u3002\u5c3d\u7ba1\u5f88\u7b80\u5355\uff0c\u4f46\u5b83\u5927\u5927\u63d0\u9ad8\u4e86\u70b9\u4e91\u5b66\u4e60\u7684\u7a33\u5065\u6027\uff0c\u901a\u8fc7\u5728\u635f\u574f\u70b9\u4e91\u5206\u7c7b\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u63d0\u4f9b\u3002|[2408.12062v1](http://arxiv.org/pdf/2408.12062v1)|null|\n", "2408.12048": "|**2024-08-22**|**ISETHDR: A Physics-based Synthetic Radiance Dataset for High Dynamic Range Driving Scenes**|ISETHDR\uff1a\u57fa\u4e8e\u7269\u7406\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u9a7e\u9a76\u573a\u666f\u5408\u6210\u8f90\u5c04\u6570\u636e\u96c6|Zhenyi Liu, Devesh Shah, Brian Wandell|This paper describes a physics-based end-to-end software simulation for image systems. We use the software to explore sensors designed to enhance performance in high dynamic range (HDR) environments, such as driving through daytime tunnels and under nighttime conditions. We synthesize physically realistic HDR spectral radiance images and use them as the input to digital twins that model the optics and sensors of different systems. This paper makes three main contributions: (a) We create a labeled (instance segmentation and depth), synthetic radiance dataset of HDR driving scenes. (b) We describe the development and validation of the end-to-end simulation framework. (c) We present a comparative analysis of two single-shot sensors designed for HDR. We open-source both the dataset and the software.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7aef\u5230\u7aef\u56fe\u50cf\u7cfb\u7edf\u8f6f\u4ef6\u6a21\u62df\u3002\u6211\u4eec\u4f7f\u7528\u8be5\u8f6f\u4ef6\u63a2\u7d22\u65e8\u5728\u63d0\u9ad8\u9ad8\u52a8\u6001\u8303\u56f4 (HDR) \u73af\u5883\u4e2d\u6027\u80fd\u7684\u4f20\u611f\u5668\uff0c\u4f8b\u5982\u5728\u767d\u5929\u96a7\u9053\u548c\u591c\u95f4\u6761\u4ef6\u4e0b\u884c\u9a76\u3002\u6211\u4eec\u5408\u6210\u7269\u7406\u4e0a\u771f\u5b9e\u7684 HDR \u5149\u8c31\u8f90\u5c04\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u6570\u5b57\u5b6a\u751f\u7684\u8f93\u5165\uff0c\u8fd9\u4e9b\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u4e0d\u540c\u7cfb\u7edf\u7684\u5149\u5b66\u548c\u4f20\u611f\u5668\u3002\u672c\u6587\u505a\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a(a) \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5e26\u6807\u7b7e\uff08\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\uff09\u7684 HDR \u9a7e\u9a76\u573a\u666f\u5408\u6210\u8f90\u5c04\u6570\u636e\u96c6\u3002(b) \u6211\u4eec\u63cf\u8ff0\u4e86\u7aef\u5230\u7aef\u6a21\u62df\u6846\u67b6\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002(c) \u6211\u4eec\u5bf9\u4e24\u4e2a\u4e3a HDR \u8bbe\u8ba1\u7684\u5355\u6b21\u4f20\u611f\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u8f6f\u4ef6\u3002|[2408.12048v1](http://arxiv.org/pdf/2408.12048v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2408.12483": "|**2024-08-22**|**Not All Samples Should Be Utilized Equally: Towards Understanding and Improving Dataset Distillation**|\u5e76\u975e\u6240\u6709\u6837\u672c\u90fd\u5e94\u5f97\u5230\u540c\u7b49\u5229\u7528\uff1a\u7406\u89e3\u548c\u6539\u8fdb\u6570\u636e\u96c6\u63d0\u70bc|Shaobo Wang, Yantai Yang, Qilong Wang, Kaixin Li, Linfeng Zhang, Junchi Yan|Dataset Distillation (DD) aims to synthesize a small dataset capable of performing comparably to the original dataset. Despite the success of numerous DD methods, theoretical exploration of this area remains unaddressed. In this paper, we take an initial step towards understanding various matching-based DD methods from the perspective of sample difficulty. We begin by empirically examining sample difficulty, measured by gradient norm, and observe that different matching-based methods roughly correspond to specific difficulty tendencies. We then extend the neural scaling laws of data pruning to DD to theoretically explain these matching-based methods. Our findings suggest that prioritizing the synthesis of easier samples from the original dataset can enhance the quality of distilled datasets, especially in low IPC (image-per-class) settings. Based on our empirical observations and theoretical analysis, we introduce the Sample Difficulty Correction (SDC) approach, designed to predominantly generate easier samples to achieve higher dataset quality. Our SDC can be seamlessly integrated into existing methods as a plugin with minimal code adjustments. Experimental results demonstrate that adding SDC generates higher-quality distilled datasets across 7 distillation methods and 6 datasets.|\u6570\u636e\u96c6\u84b8\u998f (DD) \u65e8\u5728\u5408\u6210\u4e00\u4e2a\u80fd\u591f\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u6027\u80fd\u76f8\u5f53\u7684\u5c0f\u578b\u6570\u636e\u96c6\u3002\u5c3d\u7ba1\u8bb8\u591a DD \u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8be5\u9886\u57df\u7684\u7406\u8bba\u63a2\u7d22\u4ecd\u672a\u5f97\u5230\u89e3\u51b3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u6837\u672c\u96be\u5ea6\u7684\u89d2\u5ea6\u8fc8\u51fa\u4e86\u7406\u89e3\u5404\u79cd\u57fa\u4e8e\u5339\u914d\u7684 DD \u65b9\u6cd5\u7684\u7b2c\u4e00\u6b65\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7\u7ecf\u9a8c\u68c0\u9a8c\u6837\u672c\u96be\u5ea6\uff08\u4ee5\u68af\u5ea6\u8303\u6570\u8861\u91cf\uff09\uff0c\u5e76\u89c2\u5bdf\u5230\u4e0d\u540c\u7684\u57fa\u4e8e\u5339\u914d\u7684\u65b9\u6cd5\u5927\u81f4\u5bf9\u5e94\u4e8e\u7279\u5b9a\u7684\u96be\u5ea6\u8d8b\u52bf\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u6570\u636e\u4fee\u526a\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u6269\u5c55\u5230 DD\uff0c\u4ee5\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u8fd9\u4e9b\u57fa\u4e8e\u5339\u914d\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5148\u4ece\u539f\u59cb\u6570\u636e\u96c6\u5408\u6210\u8f83\u5bb9\u6613\u7684\u6837\u672c\u53ef\u4ee5\u63d0\u9ad8\u84b8\u998f\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u4f4e IPC\uff08\u6bcf\u7c7b\u56fe\u50cf\uff09\u8bbe\u7f6e\u4e0b\u3002\u57fa\u4e8e\u6211\u4eec\u7684\u7ecf\u9a8c\u89c2\u5bdf\u548c\u7406\u8bba\u5206\u6790\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6837\u672c\u96be\u5ea6\u6821\u6b63 (SDC) \u65b9\u6cd5\uff0c\u65e8\u5728\u4e3b\u8981\u751f\u6210\u8f83\u5bb9\u6613\u7684\u6837\u672c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6570\u636e\u96c6\u8d28\u91cf\u3002\u6211\u4eec\u7684 SDC \u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u53ea\u9700\u8fdb\u884c\u6700\u5c11\u7684\u4ee3\u7801\u8c03\u6574\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6dfb\u52a0 SDC \u53ef\u4ee5\u5728 7 \u79cd\u84b8\u998f\u65b9\u6cd5\u548c 6 \u4e2a\u6570\u636e\u96c6\u4e2d\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u84b8\u998f\u6570\u636e\u96c6\u3002|[2408.12483v1](http://arxiv.org/pdf/2408.12483v1)|null|\n", "2408.12463": "|**2024-08-22**|**Smartphone-based Eye Tracking System using Edge Intelligence and Model Optimisation**|\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u91c7\u7528\u8fb9\u7f18\u667a\u80fd\u548c\u6a21\u578b\u4f18\u5316|Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi|A significant limitation of current smartphone-based eye-tracking algorithms is their low accuracy when applied to video-type visual stimuli, as they are typically trained on static images. Also, the increasing demand for real-time interactive applications like games, VR, and AR on smartphones requires overcoming the limitations posed by resource constraints such as limited computational power, battery life, and network bandwidth. Therefore, we developed two new smartphone eye-tracking techniques for video-type visuals by combining Convolutional Neural Networks (CNN) with two different Recurrent Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean Square Error of 0.955cm and 1.091cm, respectively. To address the computational constraints of smartphones, we developed an edge intelligence architecture to enhance the performance of smartphone-based eye tracking. We applied various optimisation methods like quantisation and pruning to deep learning models for better energy, CPU, and memory usage on edge devices, focusing on real-time processing. Using model quantisation, the model inference time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%, respectively, on edge devices.|\u76ee\u524d\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u773c\u52a8\u8ffd\u8e2a\u7b97\u6cd5\u7684\u4e00\u4e2a\u663e\u8457\u9650\u5236\u662f\uff0c\u5f53\u5e94\u7528\u4e8e\u89c6\u9891\u7c7b\u578b\u7684\u89c6\u89c9\u523a\u6fc0\u65f6\uff0c\u5b83\u4eec\u7684\u51c6\u786e\u5ea6\u8f83\u4f4e\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u662f\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u3002\u6b64\u5916\uff0c\u667a\u80fd\u624b\u673a\u4e0a\u5bf9\u6e38\u620f\u3001VR \u548c AR \u7b49\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u5e94\u7528\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u8fd9\u9700\u8981\u514b\u670d\u8d44\u6e90\u9650\u5236\u5e26\u6765\u7684\u9650\u5236\uff0c\u4f8b\u5982\u6709\u9650\u7684\u8ba1\u7b97\u80fd\u529b\u3001\u7535\u6c60\u5bff\u547d\u548c\u7f51\u7edc\u5e26\u5bbd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u4e0e\u4e24\u79cd\u4e0d\u540c\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc (RNN)\uff08\u5373\u957f\u77ed\u671f\u8bb0\u5fc6 (LSTM) \u548c\u95e8\u63a7\u5faa\u73af\u5355\u5143 (GRU)\uff09\u76f8\u7ed3\u5408\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u7528\u4e8e\u89c6\u9891\u7c7b\u578b\u89c6\u89c9\u6548\u679c\u7684\u65b0\u578b\u667a\u80fd\u624b\u673a\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u3002\u6211\u4eec\u7684 CNN+LSTM \u548c CNN+GRU \u6a21\u578b\u5206\u522b\u5b9e\u73b0\u4e86 0.955 \u5398\u7c73\u548c 1.091 \u5398\u7c73\u7684\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u624b\u673a\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u8fb9\u7f18\u667a\u80fd\u67b6\u6784\u6765\u589e\u5f3a\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u773c\u52a8\u8ffd\u8e2a\u7684\u6027\u80fd\u3002\u6211\u4eec\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e94\u7528\u4e86\u91cf\u5316\u3001\u526a\u679d\u7b49\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u7684\u80fd\u8017\u3001CPU \u548c\u5185\u5b58\u4f7f\u7528\u7387\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u65f6\u5904\u7406\u3002\u4f7f\u7528\u6a21\u578b\u91cf\u5316\uff0cCNN+LSTM \u548c CNN+GRU \u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6a21\u578b\u63a8\u7406\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u4e86 21.72% \u548c 19.50%\u3002|[2408.12463v1](http://arxiv.org/pdf/2408.12463v1)|null|\n", "2408.12437": "|**2024-08-22**|**Robotic Eye-in-hand Visual Servo Axially Aligning Nasopharyngeal Swabs with the Nasal Cavity**|\u673a\u5668\u4eba\u201c\u624b\u773c\u201d\u89c6\u89c9\u4f3a\u670d\u8f74\u5411\u5bf9\u51c6\u9f3b\u54bd\u62ed\u5b50\u4e0e\u9f3b\u8154|Peter Q. Lee, John S. Zelek, Katja Mombaur|The nasopharyngeal (NP) swab test is a method for collecting cultures to diagnose for different types of respiratory illnesses, including COVID-19. Delegating this task to robots would be beneficial in terms of reducing infection risks and bolstering the healthcare system, but a critical component of the NP swab test is having the swab aligned properly with the nasal cavity so that it does not cause excessive discomfort or injury by traveling down the wrong passage. Existing research towards robotic NP swabbing typically assumes the patient's head is held within a fixture. This simplifies the alignment problem, but is also dissimilar to clinical scenarios where patients are typically free-standing. Consequently, our work creates a vision-guided pipeline to allow an instrumented robot arm to properly position and orient NP swabs with respect to the nostrils of free-standing patients. The first component of the pipeline is a precomputed joint lookup table to allow the arm to meet the patient's arbitrary position in the designated workspace, while avoiding joint limits. Our pipeline leverages semantic face models from computer vision to estimate the Euclidean pose of the face with respect to a monocular RGB-D camera placed on the end-effector. These estimates are passed into an unscented Kalman filter on manifolds state estimator and a pose based visual servo control loop to move the swab to the designated pose in front of the nostril. Our pipeline was validated with human trials, featuring a cohort of 25 participants. The system is effective, reaching the nostril for 84% of participants, and our statistical analysis did not find significant demographic biases within the cohort.|\u9f3b\u54bd\u62ed\u5b50\u6d4b\u8bd5\u662f\u4e00\u79cd\u6536\u96c6\u57f9\u517b\u7269\u4ee5\u8bca\u65ad\u4e0d\u540c\u7c7b\u578b\u547c\u5438\u9053\u75be\u75c5\uff08\u5305\u62ec COVID-19\uff09\u7684\u65b9\u6cd5\u3002\u5c06\u8fd9\u9879\u4efb\u52a1\u59d4\u6258\u7ed9\u673a\u5668\u4eba\u5c06\u6709\u52a9\u4e8e\u964d\u4f4e\u611f\u67d3\u98ce\u9669\u5e76\u52a0\u5f3a\u533b\u7597\u4fdd\u5065\u7cfb\u7edf\uff0c\u4f46\u9f3b\u54bd\u62ed\u5b50\u6d4b\u8bd5\u7684\u4e00\u4e2a\u5173\u952e\u90e8\u5206\u662f\u8ba9\u62ed\u5b50\u4e0e\u9f3b\u8154\u6b63\u786e\u5bf9\u9f50\uff0c\u4ee5\u514d\u56e0\u8fdb\u5165\u9519\u8bef\u7684\u901a\u9053\u800c\u9020\u6210\u8fc7\u5ea6\u4e0d\u9002\u6216\u4f24\u5bb3\u3002\u73b0\u6709\u7684\u673a\u5668\u4eba\u9f3b\u54bd\u62ed\u5b50\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u60a3\u8005\u7684\u5934\u90e8\u56fa\u5b9a\u5728\u56fa\u5b9a\u88c5\u7f6e\u5185\u3002\u8fd9\u7b80\u5316\u4e86\u5bf9\u9f50\u95ee\u9898\uff0c\u4f46\u4e5f\u4e0d\u540c\u4e8e\u60a3\u8005\u901a\u5e38\u72ec\u7acb\u7ad9\u7acb\u7684\u4e34\u5e8a\u573a\u666f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u521b\u5efa\u4e86\u4e00\u4e2a\u89c6\u89c9\u5f15\u5bfc\u7ba1\u9053\uff0c\u4f7f\u4eea\u5668\u5316\u7684\u673a\u5668\u4eba\u624b\u81c2\u80fd\u591f\u6839\u636e\u72ec\u7acb\u7ad9\u7acb\u60a3\u8005\u7684\u9f3b\u5b54\u6b63\u786e\u5b9a\u4f4d\u548c\u5b9a\u5411\u9f3b\u54bd\u62ed\u5b50\u3002\u7ba1\u9053\u7684\u7b2c\u4e00\u4e2a\u7ec4\u4ef6\u662f\u9884\u5148\u8ba1\u7b97\u7684\u5173\u8282\u67e5\u627e\u8868\uff0c\u4f7f\u624b\u81c2\u80fd\u591f\u6ee1\u8db3\u60a3\u8005\u5728\u6307\u5b9a\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u4efb\u610f\u4f4d\u7f6e\uff0c\u540c\u65f6\u907f\u514d\u5173\u8282\u9650\u5236\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u8bed\u4e49\u4eba\u8138\u6a21\u578b\u6765\u4f30\u8ba1\u4eba\u8138\u76f8\u5bf9\u4e8e\u653e\u7f6e\u5728\u672b\u7aef\u6267\u884c\u5668\u4e0a\u7684\u5355\u76ee RGB-D \u76f8\u673a\u7684\u6b27\u51e0\u91cc\u5f97\u59ff\u52bf\u3002\u8fd9\u4e9b\u4f30\u8ba1\u503c\u88ab\u4f20\u9012\u5230\u6d41\u5f62\u72b6\u6001\u4f30\u8ba1\u5668\u4e0a\u7684\u65e0\u5473\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u57fa\u4e8e\u59ff\u52bf\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u73af\u8def\uff0c\u4ee5\u5c06\u62ed\u5b50\u79fb\u52a8\u5230\u9f3b\u5b54\u524d\u7684\u6307\u5b9a\u59ff\u52bf\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u901a\u8fc7\u4eba\u4f53\u8bd5\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bd5\u9a8c\u5bf9\u8c61\u4e3a 25 \u540d\u53c2\u4e0e\u8005\u3002\u8be5\u7cfb\u7edf\u662f\u6709\u6548\u7684\uff0c84% \u7684\u53c2\u4e0e\u8005\u80fd\u591f\u63a5\u89e6\u9f3b\u5b54\uff0c\u6211\u4eec\u7684\u7edf\u8ba1\u5206\u6790\u672a\u53d1\u73b0\u8be5\u7fa4\u4f53\u4e2d\u5b58\u5728\u660e\u663e\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\u3002|[2408.12437v1](http://arxiv.org/pdf/2408.12437v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2408.12528": "|**2024-08-22**|**Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**|Show-o\uff1a\u4e00\u4e2a Transformer \u5373\u53ef\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210|Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou|We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8f6c\u6362\u5668\uff0c\u5373 Show-o\uff0c\u5b83\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002\u4e0e\u5b8c\u5168\u81ea\u56de\u5f52\u6a21\u578b\u4e0d\u540c\uff0cShow-o \u7edf\u4e00\u4e86\u81ea\u56de\u5f52\u548c\uff08\u79bb\u6563\uff09\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u5904\u7406\u5404\u79cd\u6df7\u5408\u6a21\u6001\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002\u7edf\u4e00\u6a21\u578b\u7075\u6d3b\u5730\u652f\u6301\u5e7f\u6cdb\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\uff0c\u5305\u62ec\u89c6\u89c9\u95ee\u7b54\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c\u5f15\u5bfc\u7684\u4fee\u590d/\u5916\u63a8\u548c\u6df7\u5408\u6a21\u6001\u751f\u6210\u3002\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u5355\u4e2a\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u540c\u7b49\u6216\u66f4\u591a\u7684\u53c2\u6570\u91cf\u8eab\u5b9a\u5236\u7684\u7406\u89e3\u6216\u751f\u6210\u529f\u80fd\u3002\u8fd9\u5927\u5927\u51f8\u663e\u4e86\u5b83\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53d1\u5e03\u4e8e https://github.com/showlab/Show-o\u3002|[2408.12528v1](http://arxiv.org/pdf/2408.12528v1)|null|\n", "2408.12429": "|**2024-08-22**|**FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing**|FlexEdit\uff1a\u5c06\u81ea\u7531\u5f62\u72b6\u8499\u7248\u4e0e VLLM \u7ed3\u5408\u8d77\u6765\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u56fe\u50cf\u7f16\u8f91|Jue Wang, Yuxiang Lin, Tianshuo Yuan, Zhi-Qi Cheng, Xiaolong Wang, Jiao GH, Wei Chen, Xiaojiang Peng|Combining Vision Large Language Models (VLLMs) with diffusion models offers a powerful method for executing image editing tasks based on human language instructions. However, language instructions alone often fall short in accurately conveying user requirements, particularly when users want to add, replace elements in specific areas of an image. Luckily, masks can effectively indicate the exact locations or elements to be edited, while they require users to precisely draw the shapes at the desired locations, which is highly user-unfriendly. To address this, we propose FlexEdit, an end-to-end image editing method that leverages both free-shape masks and language instructions for Flexible Editing. Our approach employs a VLLM in comprehending the image content, mask, and user instructions. Additionally, we introduce the Mask Enhance Adapter (MEA) that fuses the embeddings of the VLLM with the image data, ensuring a seamless integration of mask information and model output embeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically tailored for free-shape mask, including 8 types of free-shape mask. Extensive experiments show that our method achieves state-of-the-art (SOTA) performance in LLM-based image editing, and our simple prompting technique stands out in its effectiveness. The code and data can be found at https://github.com/A-new-b/flex_edit.|\u5c06\u89c6\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b (VLLM) \u4e0e\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u7684\u5f3a\u5927\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u4ec5\u9760\u8bed\u8a00\u6307\u4ee4\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u4f20\u8fbe\u7528\u6237\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5f53\u7528\u6237\u60f3\u8981\u5728\u56fe\u50cf\u7684\u7279\u5b9a\u533a\u57df\u6dfb\u52a0\u3001\u66ff\u6362\u5143\u7d20\u65f6\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u8499\u7248\u53ef\u4ee5\u6709\u6548\u5730\u6307\u793a\u8981\u7f16\u8f91\u7684\u786e\u5207\u4f4d\u7f6e\u6216\u5143\u7d20\uff0c\u4f46\u5b83\u4eec\u8981\u6c42\u7528\u6237\u5728\u6240\u9700\u4f4d\u7f6e\u7cbe\u786e\u7ed8\u5236\u5f62\u72b6\uff0c\u8fd9\u5bf9\u7528\u6237\u975e\u5e38\u4e0d\u53cb\u597d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 FlexEdit\uff0c\u8fd9\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u81ea\u7531\u5f62\u72b6\u8499\u7248\u548c\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u7075\u6d3b\u7f16\u8f91\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528 VLLM \u6765\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\u3001\u8499\u7248\u548c\u7528\u6237\u6307\u4ee4\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8499\u7248\u589e\u5f3a\u9002\u914d\u5668 (MEA)\uff0c\u5b83\u5c06 VLLM \u7684\u5d4c\u5165\u4e0e\u56fe\u50cf\u6570\u636e\u878d\u5408\uff0c\u786e\u4fdd\u8499\u7248\u4fe1\u606f\u548c\u6a21\u578b\u8f93\u51fa\u5d4c\u5165\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86 FSMI-Edit\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u81ea\u7531\u5f62\u72b6\u8499\u7248\u7684\u57fa\u51c6\uff0c\u5305\u62ec 8 \u79cd\u7c7b\u578b\u7684\u81ea\u7531\u5f62\u72b6\u8499\u7248\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u57fa\u4e8e LLM \u7684\u56fe\u50cf\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73 (SOTA) \u6027\u80fd\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u7b80\u5355\u63d0\u793a\u6280\u672f\u5728\u6709\u6548\u6027\u65b9\u9762\u8131\u9896\u800c\u51fa\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/A-new-b/flex_edit \u627e\u5230\u3002|[2408.12429v1](http://arxiv.org/pdf/2408.12429v1)|null|\n", "2408.12418": "|**2024-08-22**|**CODE: Confident Ordinary Differential Editing**|CODE\uff1a\u53ef\u4fe1\u666e\u901a\u5dee\u5f02\u7f16\u8f91|Bastien van Delft, Tommaso Martorella, Alexandre Alahi|Conditioning image generation facilitates seamless editing and the creation of photorealistic images. However, conditioning on noisy or Out-of-Distribution (OoD) images poses significant challenges, particularly in balancing fidelity to the input and realism of the output. We introduce Confident Ordinary Differential Editing (CODE), a novel approach for image synthesis that effectively handles OoD guidance images. Utilizing a diffusion model as a generative prior, CODE enhances images through score-based updates along the probability-flow Ordinary Differential Equation (ODE) trajectory. This method requires no task-specific training, no handcrafted modules, and no assumptions regarding the corruptions affecting the conditioning image. Our method is compatible with any diffusion model. Positioned at the intersection of conditional image generation and blind image restoration, CODE operates in a fully blind manner, relying solely on a pre-trained generative model. Our method introduces an alternative approach to blind restoration: instead of targeting a specific ground truth image based on assumptions about the underlying corruption, CODE aims to increase the likelihood of the input image while maintaining fidelity. This results in the most probable in-distribution image around the input. Our contributions are twofold. First, CODE introduces a novel editing method based on ODE, providing enhanced control, realism, and fidelity compared to its SDE-based counterpart. Second, we introduce a confidence interval-based clipping method, which improves CODE's effectiveness by allowing it to disregard certain pixels or information, thus enhancing the restoration process in a blind manner. Experimental results demonstrate CODE's effectiveness over existing methods, particularly in scenarios involving severe degradation or OoD inputs.|\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6709\u52a9\u4e8e\u65e0\u7f1d\u7f16\u8f91\u548c\u521b\u5efa\u903c\u771f\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5bf9\u566a\u58f0\u6216\u5206\u5e03\u5916 (OoD) \u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u5904\u7406\u4f1a\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5e73\u8861\u8f93\u5165\u4fdd\u771f\u5ea6\u548c\u8f93\u51fa\u771f\u5b9e\u611f\u65b9\u9762\u3002\u6211\u4eec\u5f15\u5165\u4e86\u7f6e\u4fe1\u5e38\u5fae\u5206\u7f16\u8f91 (CODE)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u5904\u7406 OoD \u5f15\u5bfc\u56fe\u50cf\u3002CODE \u5229\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\uff0c\u901a\u8fc7\u6cbf\u6982\u7387\u6d41\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u8f68\u8ff9\u8fdb\u884c\u57fa\u4e8e\u5206\u6570\u7684\u66f4\u65b0\u6765\u589e\u5f3a\u56fe\u50cf\u3002\u6b64\u65b9\u6cd5\u4e0d\u9700\u8981\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8bad\u7ec3\u3001\u624b\u5de5\u5236\u4f5c\u7684\u6a21\u5757\uff0c\u4e5f\u4e0d\u9700\u8981\u5bf9\u5f71\u54cd\u6761\u4ef6\u56fe\u50cf\u7684\u635f\u574f\u505a\u51fa\u4efb\u4f55\u5047\u8bbe\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u4efb\u4f55\u6269\u6563\u6a21\u578b\u517c\u5bb9\u3002CODE \u4f4d\u4e8e\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u76f2\u56fe\u50cf\u6062\u590d\u7684\u4ea4\u6c47\u5904\uff0c\u4ee5\u5b8c\u5168\u76f2\u7684\u65b9\u5f0f\u8fd0\u884c\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u9884\u5148\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u66ff\u4ee3\u76f2\u76ee\u6062\u590d\u7684\u65b9\u6cd5\uff1aCODE \u4e0d\u662f\u57fa\u4e8e\u5bf9\u6f5c\u5728\u635f\u574f\u7684\u5047\u8bbe\u6765\u5b9a\u4f4d\u7279\u5b9a\u7684\u5730\u9762\u5b9e\u51b5\u56fe\u50cf\uff0c\u800c\u662f\u65e8\u5728\u5728\u4fdd\u6301\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u589e\u52a0\u8f93\u5165\u56fe\u50cf\u7684\u53ef\u80fd\u6027\u3002\u8fd9\u6837\u53ef\u4ee5\u5f97\u5230\u8f93\u5165\u5468\u56f4\u6700\u6709\u53ef\u80fd\u7684\u5206\u5e03\u56fe\u50cf\u3002\u6211\u4eec\u7684\u8d21\u732e\u6709\u4e24\u4e2a\u65b9\u9762\u3002\u9996\u5148\uff0cCODE \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e ODE \u7684\u65b0\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e0e\u57fa\u4e8e SDE \u7684\u7f16\u8f91\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u63a7\u5236\u3001\u771f\u5b9e\u611f\u548c\u4fdd\u771f\u5ea6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u533a\u95f4\u7684\u526a\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5141\u8bb8 CODE \u5ffd\u7565\u67d0\u4e9b\u50cf\u7d20\u6216\u4fe1\u606f\u6765\u63d0\u9ad8\u5176\u6709\u6548\u6027\uff0c\u4ece\u800c\u4ee5\u76f2\u76ee\u65b9\u5f0f\u589e\u5f3a\u6062\u590d\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 CODE \u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e25\u91cd\u9000\u5316\u6216 OoD \u8f93\u5165\u7684\u573a\u666f\u4e2d\u3002|[2408.12418v1](http://arxiv.org/pdf/2408.12418v1)|null|\n", "2408.12352": "|**2024-08-22**|**GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections**|GarmentAligner\uff1a\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u7ea7\u6821\u6b63\u5b9e\u73b0\u6587\u672c\u5230\u670d\u88c5\u7684\u751f\u6210|Shiyue Zhang, Zheng Chong, Xujie Zhang, Hanhui Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang|General text-to-image models bring revolutionary innovation to the fields of arts, design, and media. However, when applied to garment generation, even the state-of-the-art text-to-image models suffer from fine-grained semantic misalignment, particularly concerning the quantity, position, and interrelations of garment components. Addressing this, we propose GarmentAligner, a text-to-garment diffusion model trained with retrieval-augmented multi-level corrections. To achieve semantic alignment at the component level, we introduce an automatic component extraction pipeline to obtain spatial and quantitative information of garment components from corresponding images and captions. Subsequently, to exploit component relationships within the garment images, we construct retrieval subsets for each garment by retrieval augmentation based on component-level similarity ranking and conduct contrastive learning to enhance the model perception of components from positive and negative samples. To further enhance the alignment of components across semantic, spatial, and quantitative granularities, we propose the utilization of multi-level correction losses that leverage detailed component information. The experimental findings demonstrate that GarmentAligner achieves superior fidelity and fine-grained semantic alignment when compared to existing competitors.|\u901a\u7528\u7684\u6587\u672c\u8f6c\u56fe\u50cf\u6a21\u578b\u4e3a\u827a\u672f\u3001\u8bbe\u8ba1\u548c\u5a92\u4f53\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u521b\u65b0\u3002\u7136\u800c\uff0c\u5f53\u5e94\u7528\u4e8e\u670d\u88c5\u751f\u6210\u65f6\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6587\u672c\u8f6c\u56fe\u50cf\u6a21\u578b\u4e5f\u4f1a\u53d7\u5230\u7ec6\u7c92\u5ea6\u8bed\u4e49\u9519\u4f4d\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u670d\u88c5\u7ec4\u4ef6\u7684\u6570\u91cf\u3001\u4f4d\u7f6e\u548c\u76f8\u4e92\u5173\u7cfb\u65b9\u9762\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GarmentAligner\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u591a\u7ea7\u6821\u6b63\u8bad\u7ec3\u7684\u6587\u672c\u8f6c\u670d\u88c5\u6269\u6563\u6a21\u578b\u3002\u4e3a\u4e86\u5728\u7ec4\u4ef6\u7ea7\u522b\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u52a8\u7ec4\u4ef6\u63d0\u53d6\u7ba1\u9053\uff0c\u4ee5\u4ece\u76f8\u5e94\u7684\u56fe\u50cf\u548c\u6807\u9898\u4e2d\u83b7\u53d6\u670d\u88c5\u7ec4\u4ef6\u7684\u7a7a\u95f4\u548c\u5b9a\u91cf\u4fe1\u606f\u3002\u968f\u540e\uff0c\u4e3a\u4e86\u5229\u7528\u670d\u88c5\u56fe\u50cf\u4e2d\u7684\u7ec4\u4ef6\u5173\u7cfb\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u7ec4\u4ef6\u7ea7\u76f8\u4f3c\u6027\u6392\u540d\u7684\u68c0\u7d22\u589e\u5f3a\u4e3a\u6bcf\u4ef6\u670d\u88c5\u6784\u5efa\u68c0\u7d22\u5b50\u96c6\uff0c\u5e76\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u7ec4\u4ef6\u7684\u611f\u77e5\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u8de8\u8bed\u4e49\u3001\u7a7a\u95f4\u548c\u5b9a\u91cf\u7c92\u5ea6\u7684\u7ec4\u4ef6\u5bf9\u9f50\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u5229\u7528\u8be6\u7ec6\u7ec4\u4ef6\u4fe1\u606f\u7684\u591a\u7ea7\u6821\u6b63\u635f\u5931\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7ade\u4e89\u5bf9\u624b\u76f8\u6bd4\uff0cGarmentAligner \u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5bf9\u9f50\u3002|[2408.12352v1](http://arxiv.org/pdf/2408.12352v1)|null|\n", "2408.12245": "|**2024-08-22**|**Scalable Autoregressive Image Generation with Mamba**|\u4f7f\u7528 Mamba \u8fdb\u884c\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210|Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li|We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mamba's core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at https://github.com/hp-l33/AiM|\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e Mamba \u67b6\u6784\u7684\u81ea\u56de\u5f52 (AR) \u56fe\u50cf\u751f\u6210\u6a21\u578b AiM\u3002AiM \u91c7\u7528 Mamba\uff08\u4e00\u79cd\u65b0\u578b\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5176\u7279\u70b9\u662f\u5177\u6709\u51fa\u8272\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u6027\u80fd\u548c\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\uff09\u6765\u53d6\u4ee3 AR \u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5e38\u7528\u7684 Transformers\uff0c\u65e8\u5728\u5b9e\u73b0\u5353\u8d8a\u7684\u751f\u6210\u8d28\u91cf\u548c\u589e\u5f3a\u7684\u63a8\u7406\u901f\u5ea6\u3002\u4e0e\u73b0\u6709\u7684\u901a\u8fc7\u591a\u5411\u626b\u63cf\u8c03\u6574 Mamba \u6765\u5904\u7406\u4e8c\u7ef4\u4fe1\u53f7\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cAiM \u76f4\u63a5\u5229\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u8303\u5f0f\u8fdb\u884c\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u8fdb\u884c\u5927\u91cf\u4fee\u6539\u4ee5\u4f7f Mamba \u80fd\u591f\u5b66\u4e60\u4e8c\u7ef4\u7a7a\u95f4\u8868\u793a\u7684\u9700\u8981\u3002\u901a\u8fc7\u5bf9\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u5b9e\u65bd\u76f4\u63a5\u4f46\u6709\u7b56\u7565\u6027\u7684\u4fee\u6539\uff0c\u6211\u4eec\u4fdd\u7559\u4e86 Mamba \u7684\u6838\u5fc3\u7ed3\u6784\uff0c\u5145\u5206\u5229\u7528\u4e86\u5176\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002\u6211\u4eec\u63d0\u4f9b\u5404\u79cd\u89c4\u6a21\u7684 AiM \u6a21\u578b\uff0c\u53c2\u6570\u6570\u91cf\u4ece 1.48 \u4ebf\u5230 13 \u4ebf\u4e0d\u7b49\u3002\u5728 ImageNet1K 256*256 \u57fa\u51c6\u4e0a\uff0c\u6211\u4eec\u6700\u597d\u7684 AiM \u6a21\u578b\u5b9e\u73b0\u4e86 2.21 \u7684 FID\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5177\u6709\u53ef\u6bd4\u53c2\u6570\u6570\u91cf\u7684\u73b0\u6709 AR \u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u663e\u8457\u7684\u7ade\u4e89\u529b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86 2 \u5230 10 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/hp-l33/AiM \u4e0a\u627e\u5230|[2408.12245v1](http://arxiv.org/pdf/2408.12245v1)|null|\n"}, "\u591a\u6a21\u6001": {"2408.12574": "|**2024-08-22**|**MuMA-ToM: Multi-modal Multi-Agent Theory of Mind**|MuMA-ToM\uff1a\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u5fc3\u667a\u7406\u8bba|Haojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin, Layla Isik, Yen-Ling Kuo, Tianmin Shu|Understanding people's social interactions in complex real-world scenarios often relies on intricate mental reasoning. To truly understand how and why people interact with one another, we must infer the underlying mental states that give rise to the social interactions, i.e., Theory of Mind reasoning in multi-agent interactions. Additionally, social interactions are often multi-modal -- we can watch people's actions, hear their conversations, and/or read about their past behaviors. For AI systems to successfully and safely interact with people in real-world environments, they also need to understand people's mental states as well as their inferences about each other's mental states based on multi-modal information about their interactions. For this, we introduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark. MuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates mental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide video and text descriptions of people's multi-modal behavior in realistic household environments. Based on the context, we then ask questions about people's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM in a human experiment and provided a human baseline. We also proposed a novel multi-modal, multi-agent ToM model, LIMP (Language model-based Inverse Multi-agent Planning). Our experimental results show that LIMP significantly outperforms state-of-the-art methods, including large multi-modal models (e.g., GPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.|\u7406\u89e3\u4eba\u4eec\u5728\u590d\u6742\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\u901a\u5e38\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u5fc3\u7406\u63a8\u7406\u3002\u8981\u771f\u6b63\u7406\u89e3\u4eba\u4eec\u5982\u4f55\u4ee5\u53ca\u4e3a\u4f55\u4e92\u52a8\uff0c\u6211\u4eec\u5fc5\u987b\u63a8\u65ad\u51fa\u5f15\u8d77\u793e\u4ea4\u4e92\u52a8\u7684\u6839\u672c\u5fc3\u7406\u72b6\u6001\uff0c\u5373\u591a\u667a\u80fd\u4f53\u4e92\u52a8\u4e2d\u7684\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u3002\u6b64\u5916\uff0c\u793e\u4ea4\u4e92\u52a8\u901a\u5e38\u662f\u591a\u6a21\u6001\u7684\u2014\u2014\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u4eba\u4eec\u7684\u884c\u4e3a\u3001\u542c\u4ed6\u4eec\u7684\u5bf9\u8bdd\u548c/\u6216\u9605\u8bfb\u4ed6\u4eec\u8fc7\u53bb\u7684\u884c\u4e3a\u3002\u4e3a\u4e86\u8ba9\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u5b89\u5168\u5730\u4e0e\u4eba\u4e92\u52a8\uff0c\u5b83\u4eec\u8fd8\u9700\u8981\u4e86\u89e3\u4eba\u4eec\u7684\u5fc3\u7406\u72b6\u6001\u4ee5\u53ca\u4ed6\u4eec\u6839\u636e\u6709\u5173\u4ed6\u4eec\u4e92\u52a8\u7684\u591a\u6a21\u6001\u4fe1\u606f\u5bf9\u5f7c\u6b64\u5fc3\u7406\u72b6\u6001\u7684\u63a8\u65ad\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 MuMA-ToM\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u5fc3\u667a\u7406\u8bba\u57fa\u51c6\u3002MuMA-ToM \u662f\u7b2c\u4e00\u4e2a\u8bc4\u4f30\u5177\u4f53\u591a\u667a\u80fd\u4f53\u4e92\u52a8\u4e2d\u7684\u5fc3\u7406\u63a8\u7406\u7684\u591a\u6a21\u6001\u5fc3\u667a\u7406\u8bba\u57fa\u51c6\u3002\u5728 MuMA-ToM \u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4eba\u4eec\u5728\u73b0\u5b9e\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u591a\u6a21\u6001\u884c\u4e3a\u7684\u89c6\u9891\u548c\u6587\u672c\u63cf\u8ff0\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u4e0a\u4e0b\u6587\u8be2\u95ee\u4eba\u4eec\u7684\u76ee\u6807\u3001\u4fe1\u5ff5\u4ee5\u53ca\u5bf9\u4ed6\u4eba\u76ee\u6807\u7684\u4fe1\u5ff5\u3002\u6211\u4eec\u5728\u4eba\u7c7b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86 MuMA-ToM\uff0c\u5e76\u63d0\u4f9b\u4e86\u4eba\u7c7b\u57fa\u7ebf\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53 ToM \u6a21\u578b LIMP\uff08\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u9006\u591a\u667a\u80fd\u4f53\u89c4\u5212\uff09\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLIMP \u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08\u4f8b\u5982 GPT-4o\u3001Gemini-1.5 Pro\uff09\u548c\u6700\u8fd1\u7684\u591a\u6a21\u6001 ToM \u6a21\u578b BIP-ALM\u3002|[2408.12574v1](http://arxiv.org/pdf/2408.12574v1)|null|\n", "2408.12321": "|**2024-08-22**|**MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model**|MaVEn\uff1a\u4e00\u79cd\u6709\u6548\u7684\u591a\u7c92\u5ea6\u6df7\u5408\u89c6\u89c9\u7f16\u7801\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b|Chaoya Jiang, Jia Hongrui, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang|This paper presents MaVEn, an innovative Multi-granularity Visual Encoding framework designed to enhance the capabilities of Multimodal Large Language Models (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on single-image visual understanding, limiting their ability to interpret and integrate information across multiple images. MaVEn addresses this limitation by combining discrete visual symbol sequences, which abstract coarse-grained semantic concepts, with traditional continuous representation sequences that model fine-grained features. This dual approach bridges the semantic gap between visual and textual data, thereby improving the model's ability to process and interpret information from multiple images effectively. Additionally, we design a dynamic reduction mechanism by for long-sequence continuous features to enhance multi-image processing efficiency. Experimental results demonstrate that MaVEn significantly enhances MLLMs' understanding in complex multi-image scenarios, while also improving performance in single-image contexts.|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u591a\u7c92\u5ea6\u89c6\u89c9\u7f16\u7801\u6846\u67b6 MaVEn\uff0c\u65e8\u5728\u589e\u5f3a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u591a\u56fe\u50cf\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002\u5f53\u524d\u7684 MLLM \u4e3b\u8981\u4e13\u6ce8\u4e8e\u5355\u56fe\u50cf\u89c6\u89c9\u7406\u89e3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u89e3\u91ca\u548c\u6574\u5408\u591a\u56fe\u50cf\u4fe1\u606f\u7684\u80fd\u529b\u3002MaVEn \u901a\u8fc7\u5c06\u62bd\u8c61\u7c97\u7c92\u5ea6\u8bed\u4e49\u6982\u5ff5\u7684\u79bb\u6563\u89c6\u89c9\u7b26\u53f7\u5e8f\u5217\u4e0e\u6a21\u62df\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u4f20\u7edf\u8fde\u7eed\u8868\u793a\u5e8f\u5217\u76f8\u7ed3\u5408\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002\u8fd9\u79cd\u53cc\u91cd\u65b9\u6cd5\u5f25\u5408\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u6709\u6548\u5904\u7406\u548c\u89e3\u91ca\u591a\u56fe\u50cf\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9488\u5bf9\u957f\u5e8f\u5217\u8fde\u7eed\u7279\u5f81\u7684\u52a8\u6001\u7f29\u51cf\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u591a\u56fe\u50cf\u5904\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMaVEn \u663e\u8457\u589e\u5f3a\u4e86 MLLM \u5728\u590d\u6742\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u7684\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u4e5f\u63d0\u9ad8\u4e86\u5355\u56fe\u50cf\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002|[2408.12321v1](http://arxiv.org/pdf/2408.12321v1)|null|\n", "2408.12141": "|**2024-08-22**|**TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Model**|TRRG\uff1a\u5229\u7528\u8de8\u6a21\u6001\u75be\u75c5\u7ebf\u7d22\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u771f\u5b9e\u7684\u653e\u5c04\u5b66\u62a5\u544a|Yuhao Wang, Chao Hao, Yawen Cui, Xinqi Su, Weicheng Xie, Tao Tan, Zitong Yu|The vision-language modeling capability of multi-modal large language models has attracted wide attention from the community. However, in medical domain, radiology report generation using vision-language models still faces significant challenges due to the imbalanced data distribution caused by numerous negated descriptions in radiology reports and issues such as rough alignment between radiology reports and radiography. In this paper, we propose a truthful radiology report generation framework, namely TRRG, based on stage-wise training for cross-modal disease clue injection into large language models. In pre-training stage, During the pre-training phase, contrastive learning is employed to enhance the ability of visual encoder to perceive fine-grained disease details. In fine-tuning stage, the clue injection module we proposed significantly enhances the disease-oriented perception capability of the large language model by effectively incorporating the robust zero-shot disease perception. Finally, through the cross-modal clue interaction module, our model effectively achieves the multi-granular interaction of visual embeddings and an arbitrary number of disease clue embeddings. This significantly enhances the report generation capability and clinical effectiveness of multi-modal large language models in the field of radiology reportgeneration. Experimental results demonstrate that our proposed pre-training and fine-tuning framework achieves state-of-the-art performance in radiology report generation on datasets such as IU-Xray and MIMIC-CXR. Further analysis indicates that our proposed method can effectively enhance the model to perceive diseases and improve its clinical effectiveness.|\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u5f15\u8d77\u4e86\u4e1a\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\u5728\u533b\u5b66\u9886\u57df\uff0c\u7531\u4e8e\u653e\u5c04\u62a5\u544a\u4e2d\u5927\u91cf\u5426\u5b9a\u63cf\u8ff0\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u4ee5\u53ca\u653e\u5c04\u62a5\u544a\u4e0e\u653e\u5c04\u56fe\u7c97\u7cd9\u5bf9\u9f50\u7b49\u95ee\u9898\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u653e\u5c04\u62a5\u544a\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u771f\u5b9e\u7684\u653e\u5c04\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u5373TRRG\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5206\u9636\u6bb5\u8bad\u7ec3\u5c06\u8de8\u6a21\u6001\u75be\u75c5\u7ebf\u7d22\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u5668\u611f\u77e5\u7ec6\u7c92\u5ea6\u75be\u75c5\u7ec6\u8282\u7684\u80fd\u529b\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u7ebf\u7d22\u6ce8\u5165\u6a21\u5757\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u9c81\u68d2\u7684\u96f6\u6837\u672c\u75be\u75c5\u611f\u77e5\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u75be\u75c5\u5bfc\u5411\u611f\u77e5\u80fd\u529b\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7ebf\u7d22\u4ea4\u4e92\u6a21\u5757\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u89c6\u89c9\u5d4c\u5165\u548c\u4efb\u610f\u6570\u91cf\u75be\u75c5\u7ebf\u7d22\u5d4c\u5165\u7684\u591a\u7c92\u5ea6\u4ea4\u4e92\u3002\u8fd9\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u9886\u57df\u7684\u62a5\u544a\u751f\u6210\u80fd\u529b\u548c\u4e34\u5e8a\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6846\u67b6\u5728 IU-Xray \u548c MIMIC-CXR \u7b49\u6570\u636e\u96c6\u7684\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u75be\u75c5\u7684\u611f\u77e5\u80fd\u529b\u5e76\u63d0\u9ad8\u5176\u4e34\u5e8a\u6548\u679c\u3002|[2408.12141v1](http://arxiv.org/pdf/2408.12141v1)|null|\n"}, "LLM": {}, "Transformer": {"2408.12588": "|**2024-08-22**|**Real-Time Video Generation with Pyramid Attention Broadcast**|\u5229\u7528\u91d1\u5b57\u5854\u6ce8\u610f\u529b\u5e7f\u64ad\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u751f\u6210|Xuanlei Zhao, Xiaolong Jin, Kai Wang, Yang You|We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. We further introduce broadcast sequence parallel for more efficient distributed inference. PAB demonstrates superior results across three models compared to baselines, achieving real-time generation for up to 720p videos. We anticipate that our simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation.|\u6211\u4eec\u63d0\u51fa\u4e86\u91d1\u5b57\u5854\u6ce8\u610f\u529b\u5e7f\u64ad (PAB)\uff0c\u8fd9\u662f\u4e00\u79cd\u5b9e\u65f6\u3001\u9ad8\u8d28\u91cf\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e DiT \u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5efa\u7acb\u5728\u4ee5\u4e0b\u89c2\u5bdf\u7684\u57fa\u7840\u4e0a\uff1a\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u5dee\u5f02\u5448\u73b0 U \u5f62\u6a21\u5f0f\uff0c\u8868\u660e\u5b58\u5728\u663e\u8457\u7684\u5197\u4f59\u3002\u6211\u4eec\u901a\u8fc7\u4ee5\u91d1\u5b57\u5854\u5f62\u5f0f\u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u5e7f\u64ad\u5230\u540e\u7eed\u6b65\u9aa4\u6765\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\u3002\u5b83\u6839\u636e\u6bcf\u4e2a\u6ce8\u610f\u529b\u7684\u65b9\u5dee\u5bf9\u5b83\u4eec\u5e94\u7528\u4e0d\u540c\u7684\u5e7f\u64ad\u7b56\u7565\u4ee5\u83b7\u5f97\u6700\u4f73\u6548\u7387\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u5e7f\u64ad\u5e8f\u5217\u5e76\u884c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u3002\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cPAB \u5728\u4e09\u4e2a\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8fbe 720p \u89c6\u9891\u7684\u5b9e\u65f6\u751f\u6210\u3002\u6211\u4eec\u9884\u8ba1\uff0c\u6211\u4eec\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u5c06\u4f5c\u4e3a\u7a33\u5065\u7684\u57fa\u7ebf\uff0c\u5e76\u4fc3\u8fdb\u672a\u6765\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002|[2408.12588v1](http://arxiv.org/pdf/2408.12588v1)|null|\n", "2408.12439": "|**2024-08-22**|**Adapting MIMO video restoration networks to low latency constraints**|\u4f7f MIMO \u89c6\u9891\u6062\u590d\u7f51\u7edc\u9002\u5e94\u4f4e\u5ef6\u8fdf\u7ea6\u675f|Val\u00e9ry Dewil, Zhe Zheng, Arnaud Barral, Lara Raad, Nao Nicolas, Ioannis Cassagne, Jean-michel Morel, Gabriele Facciolo, Bruno Galerne, Pablo Arias|MIMO (multiple input, multiple output) approaches are a recent trend in neural network architectures for video restoration problems, where each network evaluation produces multiple output frames. The video is split into non-overlapping stacks of frames that are processed independently, resulting in a very appealing trade-off between output quality and computational cost. In this work we focus on the low-latency setting by limiting the number of available future frames. We find that MIMO architectures suffer from problems that have received little attention so far, namely (1) the performance drops significantly due to the reduced temporal receptive field, particularly for frames at the borders of the stack, (2) there are strong temporal discontinuities at stack transitions which induce a step-wise motion artifact. We propose two simple solutions to alleviate these problems: recurrence across MIMO stacks to boost the output quality by implicitly increasing the temporal receptive field, and overlapping of the output stacks to smooth the temporal discontinuity at stack transitions. These modifications can be applied to any MIMO architecture. We test them on three state-of-the-art video denoising networks with different computational cost. The proposed contributions result in a new state-of-the-art for low-latency networks, both in terms of reconstruction error and temporal consistency. As an additional contribution, we introduce a new benchmark consisting of drone footage that highlights temporal consistency issues that are not apparent in the standard benchmarks.|MIMO\uff08\u591a\u8f93\u5165\u3001\u591a\u8f93\u51fa\uff09\u65b9\u6cd5\u662f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u7528\u4e8e\u89c6\u9891\u6062\u590d\u95ee\u9898\u7684\u6700\u65b0\u8d8b\u52bf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u7f51\u7edc\u8bc4\u4f30\u90fd\u4f1a\u4ea7\u751f\u591a\u4e2a\u8f93\u51fa\u5e27\u3002\u89c6\u9891\u88ab\u5206\u6210\u4e0d\u91cd\u53e0\u7684\u5e27\u5806\u6808\uff0c\u8fd9\u4e9b\u5e27\u5806\u6808\u88ab\u72ec\u7acb\u5904\u7406\uff0c\u4ece\u800c\u5728\u8f93\u51fa\u8d28\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u975e\u5e38\u6709\u5438\u5f15\u529b\u7684\u6743\u8861\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u9650\u5236\u53ef\u7528\u7684\u672a\u6765\u5e27\u6570\u6765\u5173\u6ce8\u4f4e\u5ef6\u8fdf\u8bbe\u7f6e\u3002\u6211\u4eec\u53d1\u73b0 MIMO \u67b6\u6784\u5b58\u5728\u8fc4\u4eca\u4e3a\u6b62\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u5373 (1) \u7531\u4e8e\u65f6\u95f4\u611f\u53d7\u91ce\u51cf\u5c0f\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5806\u6808\u8fb9\u754c\u5904\u7684\u5e27\uff0c(2) \u5806\u6808\u8f6c\u6362\u65f6\u5b58\u5728\u5f3a\u70c8\u7684\u65f6\u95f4\u4e0d\u8fde\u7eed\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u9010\u6b65\u8fd0\u52a8\u4f2a\u5f71\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff1a\u901a\u8fc7\u9690\u5f0f\u589e\u52a0\u65f6\u95f4\u611f\u53d7\u91ce\uff0c\u5728 MIMO \u5806\u6808\u4e4b\u95f4\u8fdb\u884c\u9012\u5f52\u4ee5\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\uff0c\u4ee5\u53ca\u91cd\u53e0\u8f93\u51fa\u5806\u6808\u4ee5\u5e73\u6ed1\u5806\u6808\u8f6c\u6362\u65f6\u7684\u65f6\u95f4\u4e0d\u8fde\u7eed\u6027\u3002\u8fd9\u4e9b\u4fee\u6539\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55 MIMO \u67b6\u6784\u3002\u6211\u4eec\u5728\u4e09\u79cd\u8ba1\u7b97\u6210\u672c\u4e0d\u540c\u7684\u5148\u8fdb\u89c6\u9891\u53bb\u566a\u7f51\u7edc\u4e0a\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u6240\u63d0\u51fa\u7684\u8d21\u732e\u4e3a\u4f4e\u5ef6\u8fdf\u7f51\u7edc\u5e26\u6765\u4e86\u65b0\u7684\u5148\u8fdb\u6c34\u5e73\uff0c\u65e0\u8bba\u662f\u5728\u91cd\u5efa\u8bef\u5dee\u65b9\u9762\u8fd8\u662f\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u3002\u4f5c\u4e3a\u989d\u5916\u7684\u8d21\u732e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7531\u65e0\u4eba\u673a\u955c\u5934\u7ec4\u6210\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u7a81\u51fa\u4e86\u6807\u51c6\u57fa\u51c6\u4e2d\u672a\u51fa\u73b0\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\u3002|[2408.12439v1](http://arxiv.org/pdf/2408.12439v1)|null|\n", "2408.12340": "|**2024-08-22**|**VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand Priors Embedding**|VTON-HandFit\uff1a\u901a\u8fc7\u624b\u90e8\u5148\u9a8c\u5d4c\u5165\u5f15\u5bfc\u7684\u4efb\u610f\u624b\u52bf\u865a\u62df\u8bd5\u7a7f|Yujie Liang, Xiaobin Hu, Boyuan Jiang, Donghao Luo, Kai WU, Wenhui Han, Taisong Jin, Chengjie Wang|Although diffusion-based image virtual try-on has made considerable progress, emerging approaches still struggle to effectively address the issue of hand occlusion (i.e., clothing regions occluded by the hand part), leading to a notable degradation of the try-on performance. To tackle this issue widely existing in real-world scenarios, we propose VTON-HandFit, leveraging the power of hand priors to reconstruct the appearance and structure for hand occlusion cases. Firstly, we tailor a Handpose Aggregation Net using the ControlNet-based structure explicitly and adaptively encoding the global hand and pose priors. Besides, to fully exploit the hand-related structure and appearance information, we propose Hand-feature Disentanglement Embedding module to disentangle the hand priors into the hand structure-parametric and visual-appearance features, and customize a masked cross attention for further decoupled feature embedding. Lastly, we customize a hand-canny constraint loss to better learn the structure edge knowledge from the hand template of model image. VTON-HandFit outperforms the baselines in qualitative and quantitative evaluations on the public dataset and our self-collected hand-occlusion Handfit-3K dataset particularly for the arbitrary hand pose occlusion cases in real-world scenarios. Code and dataset will be made publicly available.|\u5c3d\u7ba1\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u865a\u62df\u8bd5\u7a7f\u5df2\u7ecf\u53d6\u5f97\u4e86\u957f\u8db3\u7684\u8fdb\u6b65\uff0c\u4f46\u65b0\u5174\u65b9\u6cd5\u4ecd\u7136\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u624b\u90e8\u906e\u6321\u95ee\u9898\uff08\u5373\u8863\u670d\u533a\u57df\u88ab\u624b\u90e8\u906e\u6321\uff09\uff0c\u5bfc\u81f4\u8bd5\u7a7f\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VTON-HandFit\uff0c\u5229\u7528\u624b\u90e8\u5148\u9a8c\u7684\u80fd\u529b\u91cd\u5efa\u624b\u90e8\u906e\u6321\u60c5\u51b5\u7684\u5916\u89c2\u548c\u7ed3\u6784\u3002\u9996\u5148\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e ControlNet \u7684\u7ed3\u6784\u5b9a\u5236\u4e00\u4e2a Handpose Aggregation Net\uff0c\u660e\u786e\u4e14\u81ea\u9002\u5e94\u5730\u7f16\u7801\u5168\u5c40\u624b\u90e8\u548c\u59ff\u52bf\u5148\u9a8c\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528\u4e0e\u624b\u90e8\u76f8\u5173\u7684\u7ed3\u6784\u548c\u5916\u89c2\u4fe1\u606f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u624b\u90e8\u7279\u5f81\u89e3\u7f20\u5d4c\u5165\u6a21\u5757\uff0c\u5c06\u624b\u90e8\u5148\u9a8c\u89e3\u7f20\u4e3a\u624b\u90e8\u7ed3\u6784\u53c2\u6570\u548c\u89c6\u89c9\u5916\u89c2\u7279\u5f81\uff0c\u5e76\u5b9a\u5236\u4e00\u4e2a\u5e26\u63a9\u7801\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u4ee5\u8fdb\u4e00\u6b65\u89e3\u8026\u7279\u5f81\u5d4c\u5165\u3002\u6700\u540e\uff0c\u6211\u4eec\u5b9a\u5236\u4e86\u4e00\u4e2a hand-canny \u7ea6\u675f\u635f\u5931\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u4ece\u6a21\u578b\u56fe\u50cf\u7684\u624b\u90e8\u6a21\u677f\u4e2d\u5b66\u4e60\u7ed3\u6784\u8fb9\u7f18\u77e5\u8bc6\u3002 VTON-HandFit \u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u6211\u4eec\u81ea\u5df1\u6536\u96c6\u7684\u624b\u90e8\u906e\u6321 Handfit-3K \u6570\u636e\u96c6\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4efb\u610f\u624b\u52bf\u906e\u6321\u60c5\u51b5\u4e2d\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u3002|[2408.12340v1](http://arxiv.org/pdf/2408.12340v1)|null|\n", "2408.12317": "|**2024-08-22**|**Adapt CLIP as Aggregation Instructor for Image Dehazing**|\u9002\u5e94 CLIP \u4f5c\u4e3a\u56fe\u50cf\u53bb\u96fe\u7684\u805a\u5408\u6307\u5bfc|Xiaozhe Zhang, Fengying Xie, Haidong Ding, Linpeng Pan, Zhenwei Shi|Most dehazing methods suffer from limited receptive field and do not explore the rich semantic prior encapsulated in vision-language models, which have proven effective in downstream tasks. In this paper, we introduce CLIPHaze, a pioneering hybrid framework that synergizes the efficient global modeling of Mamba with the prior knowledge and zero-shot capabilities of CLIP to address both issues simultaneously. Specifically, our method employs parallel state space model and window-based self-attention to obtain global contextual dependency and local fine-grained perception, respectively. To seamlessly aggregate information from both paths, we introduce CLIP-instructed Aggregation Module (CAM). For non-homogeneous and homogeneous haze, CAM leverages zero-shot estimated haze density map and high-quality image embedding without degradation information to explicitly and implicitly determine the optimal neural operation range for each pixel, thereby adaptively fusing two paths with different receptive fields. Extensive experiments on various benchmarks demonstrate that CLIPHaze achieves state-of-the-art (SOTA) performance, particularly in non-homogeneous haze. Code will be publicly after acceptance.|\u5927\u591a\u6570\u53bb\u96fe\u65b9\u6cd5\u90fd\u53d7\u5230\u611f\u53d7\u91ce\u6709\u9650\u7684\u56f0\u6270\uff0c\u5e76\u4e14\u6ca1\u6709\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5c01\u88c5\u7684\u4e30\u5bcc\u8bed\u4e49\u5148\u9a8c\uff0c\u800c\u8fd9\u4e9b\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u662f\u6709\u6548\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 CLIPHaze\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5b83\u5c06 Mamba \u7684\u9ad8\u6548\u5168\u5c40\u5efa\u6a21\u4e0e CLIP \u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u96f6\u6837\u672c\u80fd\u529b\u7ed3\u5408\u8d77\u6765\uff0c\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5206\u522b\u91c7\u7528\u5e76\u884c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u57fa\u4e8e\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b\u6765\u83b7\u5f97\u5168\u5c40\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u5c40\u90e8\u7ec6\u7c92\u5ea6\u611f\u77e5\u3002\u4e3a\u4e86\u65e0\u7f1d\u5730\u805a\u5408\u6765\u81ea\u4e24\u6761\u8def\u5f84\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CLIP \u6307\u793a\u7684\u805a\u5408\u6a21\u5757 (CAM)\u3002\u5bf9\u4e8e\u975e\u5747\u8d28\u548c\u5747\u8d28\u96fe\u973e\uff0cCAM \u5229\u7528\u96f6\u6837\u672c\u4f30\u8ba1\u7684\u96fe\u973e\u5bc6\u5ea6\u56fe\u548c\u6ca1\u6709\u9000\u5316\u4fe1\u606f\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5d4c\u5165\u6765\u663e\u5f0f\u548c\u9690\u5f0f\u5730\u786e\u5b9a\u6bcf\u4e2a\u50cf\u7d20\u7684\u6700\u4f73\u795e\u7ecf\u64cd\u4f5c\u8303\u56f4\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u878d\u5408\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u4e24\u6761\u8def\u5f84\u3002\u5728\u5404\u79cd\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCLIPHaze \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) \u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u975e\u5747\u5300\u96fe\u973e\u4e2d\u3002\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u516c\u5f00\u3002|[2408.12317v1](http://arxiv.org/pdf/2408.12317v1)|null|\n", "2408.12253": "|**2024-08-22**|**Epsilon: Exploring Comprehensive Visual-Semantic Projection for Multi-Label Zero-Shot Learning**|Epsilon\uff1a\u63a2\u7d22\u591a\u6807\u7b7e\u96f6\u6837\u672c\u5b66\u4e60\u7684\u7efc\u5408\u89c6\u89c9\u8bed\u4e49\u6295\u5f71|Ziming Liu, Jingcai Guo, Song Guo, Xiaocheng Lu|This paper investigates a challenging problem of zero-shot learning in the multi-label scenario (MLZSL), wherein the model is trained to recognize multiple unseen classes within a sample (e.g., an image) based on seen classes and auxiliary knowledge, e.g., semantic information. Existing methods usually resort to analyzing the relationship of various seen classes residing in a sample from the dimension of spatial or semantic characteristics and transferring the learned model to unseen ones. However, they neglect the integrity of local and global features. Although the use of the attention structure will accurately locate local features, especially objects, it will significantly lose its integrity, and the relationship between classes will also be affected. Rough processing of global features will also directly affect comprehensiveness. This neglect will make the model lose its grasp of the main components of the image. Relying only on the local existence of seen classes during the inference stage introduces unavoidable bias. In this paper, we propose a novel and comprehensive visual-semantic framework for MLZSL, dubbed Epsilon, to fully make use of such properties and enable a more accurate and robust visual-semantic projection. In terms of spatial information, we achieve effective refinement by group aggregating image features into several semantic prompts. It can aggregate semantic information rather than class information, preserving the correlation between semantics. In terms of global semantics, we use global forward propagation to collect as much information as possible to ensure that semantics are not omitted. Experiments on large-scale MLZSL benchmark datasets NUS-Wide and Open-Images-v4 demonstrate that the proposed Epsilon outperforms other state-of-the-art methods with large margins.|\u672c\u6587\u7814\u7a76\u4e86\u591a\u6807\u7b7e\u573a\u666f\u4e0b\u96f6\u6837\u672c\u5b66\u4e60\uff08MLZSL\uff09\u7684\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u5176\u4e2d\u6a21\u578b\u8bad\u7ec3\u57fa\u4e8e\u53ef\u89c1\u7c7b\u548c\u8f85\u52a9\u77e5\u8bc6\uff08\u4f8b\u5982\u8bed\u4e49\u4fe1\u606f\uff09\u8bc6\u522b\u6837\u672c\uff08\u4f8b\u5982\uff0c\u56fe\u50cf\uff09\u4e2d\u7684\u591a\u4e2a\u672a\u53ef\u89c1\u7c7b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ece\u7a7a\u95f4\u6216\u8bed\u4e49\u7279\u5f81\u7ef4\u5ea6\u5206\u6790\u6837\u672c\u4e2d\u5404\u4e2a\u53ef\u89c1\u7c7b\u7684\u5173\u7cfb\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u6a21\u578b\u8fc1\u79fb\u5230\u672a\u53ef\u89c1\u7c7b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5ffd\u7565\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u5b8c\u6574\u6027\u3002\u867d\u7136\u4f7f\u7528\u6ce8\u610f\u529b\u7ed3\u6784\u4f1a\u51c6\u786e\u5b9a\u4f4d\u5c40\u90e8\u7279\u5f81\uff0c\u7279\u522b\u662f\u7269\u4f53\uff0c\u4f46\u4f1a\u5927\u5927\u4e27\u5931\u5176\u5b8c\u6574\u6027\uff0c\u7c7b\u4e4b\u95f4\u7684\u5173\u7cfb\u4e5f\u4f1a\u53d7\u5230\u5f71\u54cd\u3002\u5bf9\u5168\u5c40\u7279\u5f81\u7684\u7c97\u7565\u5904\u7406\u4e5f\u4f1a\u76f4\u63a5\u5f71\u54cd\u5168\u9762\u6027\u3002\u8fd9\u79cd\u5ffd\u89c6\u5c06\u4f7f\u6a21\u578b\u5931\u53bb\u5bf9\u56fe\u50cf\u4e3b\u8981\u6210\u5206\u7684\u638c\u63e1\u3002\u5728\u63a8\u7406\u9636\u6bb5\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u53ef\u89c1\u7c7b\u7684\u5c40\u90e8\u5b58\u5728\u4f1a\u5f15\u5165\u4e0d\u53ef\u907f\u514d\u7684\u504f\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u800c\u5168\u9762\u7684 MLZSL \u89c6\u89c9\u8bed\u4e49\u6846\u67b6\uff0c\u79f0\u4e3a Epsilon\uff0c\u4ee5\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u7279\u6027\u5e76\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u4e49\u6295\u5f71\u3002\u5728\u7a7a\u95f4\u4fe1\u606f\u65b9\u9762\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u56fe\u50cf\u7279\u5f81\u5206\u7ec4\u805a\u5408\u4e3a\u51e0\u4e2a\u8bed\u4e49\u63d0\u793a\u6765\u5b9e\u73b0\u6709\u6548\u7684\u7ec6\u5316\u3002\u5b83\u53ef\u4ee5\u805a\u5408\u8bed\u4e49\u4fe1\u606f\u800c\u4e0d\u662f\u7c7b\u4fe1\u606f\uff0c\u4ece\u800c\u4fdd\u7559\u8bed\u4e49\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u5728\u5168\u5c40\u8bed\u4e49\u65b9\u9762\uff0c\u6211\u4eec\u4f7f\u7528\u5168\u5c40\u524d\u5411\u4f20\u64ad\u6765\u6536\u96c6\u5c3d\u53ef\u80fd\u591a\u7684\u4fe1\u606f\uff0c\u4ee5\u786e\u4fdd\u4e0d\u9057\u6f0f\u8bed\u4e49\u3002\u5728\u5927\u89c4\u6a21 MLZSL \u57fa\u51c6\u6570\u636e\u96c6 NUS-Wide \u548c Open-Images-v4 \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684 Epsilon \u4ee5\u8f83\u5927\u7684\u4f18\u52bf\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2408.12253v1](http://arxiv.org/pdf/2408.12253v1)|null|\n"}, "Nerf": {}, "3DGS": {"2408.12282": "|**2024-08-22**|**Subsurface Scattering for 3D Gaussian Splatting**|3D \u9ad8\u65af\u6e85\u5c04\u7684\u6b21\u8868\u9762\u6563\u5c04|Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch|3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting and novel view synthesis at interactive rates. We show successful application on synthetic data and introduce a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes. Project page https://sss.jdihlmann.com/|\u7531\u4e8e\u8868\u9762\u4e0b\u590d\u6742\u7684\u5149\u4f20\u8f93\uff0c\u5bf9\u6563\u5c04\u6750\u6599\u5236\u6210\u7684\u7269\u4f53\u8fdb\u884c 3D \u91cd\u5efa\u548c\u91cd\u65b0\u7167\u660e\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u30023D \u9ad8\u65af Splatting \u5f15\u5165\u4e86\u5b9e\u65f6\u901f\u5ea6\u7684\u9ad8\u8d28\u91cf\u65b0\u89c6\u56fe\u5408\u6210\u3002\u867d\u7136 3D \u9ad8\u65af\u53ef\u4ee5\u6709\u6548\u5730\u8fd1\u4f3c\u7269\u4f53\u7684\u8868\u9762\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u6355\u6349\u6b21\u8868\u9762\u6563\u5c04\u7684\u4f53\u79ef\u7279\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ed9\u5b9a\u591a\u89c6\u56fe OLAT\uff08\u4e00\u6b21\u4e00\u4e2a\u5149\uff09\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u7269\u4f53\u7684\u5f62\u72b6\u4ee5\u53ca\u8f90\u5c04\u4f20\u8f93\u573a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u8868\u793a\u4e3a 3D \u9ad8\u65af\u7684\u663e\u5f0f\u8868\u9762\uff0c\u5177\u6709\u7a7a\u95f4\u53d8\u5316\u7684 BRDF \u548c\u6563\u5c04\u6210\u5206\u7684\u9690\u5f0f\u4f53\u79ef\u8868\u793a\u3002\u5b66\u4e60\u5230\u7684\u5165\u5c04\u5149\u573a\u8003\u8651\u4e86\u9634\u5f71\u3002\u6211\u4eec\u901a\u8fc7\u5149\u7ebf\u8ffd\u8e2a\u53ef\u5fae\u5206\u6e32\u67d3\u8054\u5408\u4f18\u5316\u6240\u6709\u53c2\u6570\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u4ee5\u4ea4\u4e92\u901f\u7387\u8fdb\u884c\u6750\u6599\u7f16\u8f91\u3001\u91cd\u65b0\u7167\u660e\u548c\u65b0\u89c6\u56fe\u5408\u6210\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u6210\u529f\u5e94\u7528\uff0c\u5e76\u5728\u5149\u821e\u53f0\u8bbe\u7f6e\u4e2d\u5f15\u5165\u4e86\u65b0\u83b7\u53d6\u7684\u7269\u4f53\u591a\u89c6\u56fe\u591a\u5149\u6570\u636e\u96c6\u3002\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u4eec\u4ee5\u6781\u5c11\u7684\u4f18\u5316\u548c\u6e32\u67d3\u65f6\u95f4\u5b9e\u73b0\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5bf9\u6750\u8d28\u5c5e\u6027\u7684\u8be6\u7ec6\u63a7\u5236\u3002\u9879\u76ee\u9875\u9762 https://sss.jdihlmann.com/|[2408.12282v1](http://arxiv.org/pdf/2408.12282v1)|null|\n"}, "3D/CG": {"2408.12601": "|**2024-08-22**|**DreamCinema: Cinematic Transfer with Free Camera and 3D Character**|DreamCinema\uff1a\u5177\u6709\u514d\u8d39\u6444\u50cf\u5934\u548c 3D \u89d2\u8272\u7684\u7535\u5f71\u4f20\u8f93|Weiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Haixu Song, Yueqi Duan|We are living in a flourishing era of digital media, where everyone has the potential to become a personal filmmaker. Current research on cinematic transfer empowers filmmakers to reproduce and manipulate the visual elements (e.g., cinematography and character behaviors) from classic shots. However, characters in the reimagined films still rely on manual crafting, which involves significant technical complexity and high costs, making it unattainable for ordinary users. Furthermore, their estimated cinematography lacks smoothness due to inadequate capturing of inter-frame motion and modeling of physical trajectories. Fortunately, the remarkable success of 2D and 3D AIGC has opened up the possibility of efficiently generating characters tailored to users' needs, diversifying cinematography. In this paper, we propose DreamCinema, a novel cinematic transfer framework that pioneers generative AI into the film production paradigm, aiming at facilitating user-friendly film creation. Specifically, we first extract cinematic elements (i.e., human and camera pose) and optimize the camera trajectory. Then, we apply a character generator to efficiently create 3D high-quality characters with a human structure prior. Finally, we develop a structure-guided motion transfer strategy to incorporate generated characters into film creation and transfer it via 3D graphics engines smoothly. Extensive experiments demonstrate the effectiveness of our method for creating high-quality films with free camera and 3D characters.|\u6211\u4eec\u751f\u6d3b\u5728\u4e00\u4e2a\u6570\u5b57\u5a92\u4f53\u84ec\u52c3\u53d1\u5c55\u7684\u65f6\u4ee3\uff0c\u6bcf\u4e2a\u4eba\u90fd\u6709\u6210\u4e3a\u4e2a\u4eba\u7535\u5f71\u5236\u4f5c\u4eba\u7684\u6f5c\u529b\u3002\u5f53\u524d\u5bf9\u7535\u5f71\u4f20\u8f93\u7684\u7814\u7a76\u4f7f\u7535\u5f71\u5236\u4f5c\u4eba\u80fd\u591f\u91cd\u73b0\u548c\u64cd\u7eb5\u7ecf\u5178\u955c\u5934\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\uff08\u4f8b\u5982\u7535\u5f71\u6444\u5f71\u548c\u89d2\u8272\u884c\u4e3a\uff09\u3002\u7136\u800c\uff0c\u91cd\u65b0\u6784\u60f3\u7684\u7535\u5f71\u4e2d\u7684\u89d2\u8272\u4ecd\u7136\u4f9d\u8d56\u4e8e\u624b\u5de5\u5236\u4f5c\uff0c\u8fd9\u6d89\u53ca\u5927\u91cf\u7684\u6280\u672f\u590d\u6742\u6027\u548c\u9ad8\u6210\u672c\uff0c\u666e\u901a\u7528\u6237\u65e0\u6cd5\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u5e27\u95f4\u8fd0\u52a8\u6355\u6349\u4e0d\u8db3\u548c\u7269\u7406\u8f68\u8ff9\u5efa\u6a21\u4e0d\u8db3\uff0c\u4ed6\u4eec\u4f30\u8ba1\u7684\u7535\u5f71\u6444\u5f71\u7f3a\u4e4f\u6d41\u7545\u6027\u3002\u5e78\u8fd0\u7684\u662f\uff0c2D \u548c 3D AIGC \u7684\u663e\u8457\u6210\u529f\u5f00\u8f9f\u4e86\u9ad8\u6548\u751f\u6210\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u89d2\u8272\u7684\u53ef\u80fd\u6027\uff0c\u4f7f\u7535\u5f71\u6444\u5f71\u591a\u6837\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DreamCinema\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7535\u5f71\u4f20\u8f93\u6846\u67b6\uff0c\u5c06\u751f\u6210\u5f0f AI \u5f15\u5165\u7535\u5f71\u5236\u4f5c\u8303\u5f0f\uff0c\u65e8\u5728\u4fc3\u8fdb\u7528\u6237\u53cb\u597d\u7684\u7535\u5f71\u521b\u4f5c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u63d0\u53d6\u7535\u5f71\u5143\u7d20\uff08\u5373\u4eba\u4f53\u548c\u76f8\u673a\u59ff\u52bf\uff09\u5e76\u4f18\u5316\u76f8\u673a\u8f68\u8ff9\u3002\u7136\u540e\uff0c\u6211\u4eec\u5e94\u7528\u89d2\u8272\u751f\u6210\u5668\u9ad8\u6548\u5730\u521b\u5efa\u5177\u6709\u4eba\u4f53\u7ed3\u6784\u4f18\u5148\u7684 3D \u9ad8\u8d28\u91cf\u89d2\u8272\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u6784\u5f15\u5bfc\u7684\u8fd0\u52a8\u4f20\u8f93\u7b56\u7565\uff0c\u5c06\u751f\u6210\u7684\u89d2\u8272\u878d\u5165\u7535\u5f71\u521b\u4f5c\u4e2d\uff0c\u5e76\u901a\u8fc7 3D \u56fe\u5f62\u5f15\u64ce\u6d41\u7545\u5730\u4f20\u8f93\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u4f7f\u7528\u81ea\u7531\u6444\u50cf\u673a\u548c 3D \u89d2\u8272\u521b\u5efa\u9ad8\u8d28\u91cf\u7535\u5f71\u7684\u6709\u6548\u6027\u3002|[2408.12601v1](http://arxiv.org/pdf/2408.12601v1)|null|\n", "2408.12598": "|**2024-08-22**|**ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction**|ND-SDF\uff1a\u5b66\u4e60\u6cd5\u5411\u504f\u8f6c\u573a\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ba4\u5185\u91cd\u5efa|Ziyu Tang, Weicai Ye, Yifan Wang, Di Huang, Hujun Bao, Tong He, Guofeng Zhang|Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Ddeflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method.|\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u8fdb\u884c\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u5df2\u8bc1\u660e\u5176\u5728\u6062\u590d\u5bc6\u96c6\u7684 3D \u8868\u9762\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002\u7136\u800c\uff0c\u540c\u65f6\u6062\u590d\u7cbe\u7ec6\u7684\u51e0\u4f55\u5f62\u72b6\u5e76\u4fdd\u6301\u5177\u6709\u4e0d\u540c\u7279\u5f81\u7684\u533a\u57df\u4e4b\u95f4\u7684\u5e73\u6ed1\u5ea6\u5e76\u975e\u6613\u4e8b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u51e0\u4f55\u5148\u9a8c\uff0c\u800c\u51e0\u4f55\u5148\u9a8c\u901a\u5e38\u53d7\u5230\u5148\u9a8c\u6a21\u578b\u6027\u80fd\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ND-SDF\uff0c\u5b83\u5b66\u4e60\u4e86\u4e00\u4e2a\u6cd5\u7ebf D \u504f\u8f6c\u573a\u6765\u8868\u793a\u573a\u666f\u6cd5\u7ebf\u548c\u5148\u9a8c\u6cd5\u7ebf\u4e4b\u95f4\u7684\u89d2\u5ea6\u504f\u5dee\u3002\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u5728\u6240\u6709\u6837\u672c\u4e0a\u7edf\u4e00\u5e94\u7528\u51e0\u4f55\u5148\u9a8c\u4e0d\u540c\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u51c6\u786e\u6027\u51fa\u73b0\u663e\u8457\u504f\u5dee\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6cd5\u7ebf\u504f\u8f6c\u573a\u4f1a\u6839\u636e\u6837\u672c\u7684\u5177\u4f53\u7279\u5f81\u52a8\u6001\u5b66\u4e60\u548c\u8c03\u6574\u6837\u672c\u7684\u5229\u7528\u7387\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u83b7\u5f97\u5149\u6ed1\u7684\u5f31\u7eb9\u7406\u533a\u57df\uff08\u4f8b\u5982\u5899\u58c1\u548c\u5730\u677f\uff09\uff0c\u8fd8\u53ef\u4ee5\u4fdd\u7559\u590d\u6742\u7ed3\u6784\u7684\u51e0\u4f55\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u8f6c\u89d2\u7684\u65b0\u578b\u5c04\u7ebf\u91c7\u6837\u7b56\u7565\u6765\u4fc3\u8fdb\u65e0\u504f\u6e32\u67d3\u8fc7\u7a0b\uff0c\u8fd9\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u8868\u9762\uff08\u5c24\u5176\u662f\u8584\u7ed3\u6784\uff09\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u7684\u6301\u7eed\u6539\u8fdb\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2408.12598v1](http://arxiv.org/pdf/2408.12598v1)|null|\n", "2408.12443": "|**2024-08-22**|**A Riemannian Approach for Spatiotemporal Analysis and Generation of 4D Tree-shaped Structures**|\u7528\u4e8e\u65f6\u7a7a\u5206\u6790\u548c 4D \u6811\u5f62\u7ed3\u6784\u751f\u6210\u7684\u9ece\u66fc\u65b9\u6cd5|Tahmina Khanam, Hamid Laga, Mohammed Bennamoun, Guanjin Wang, Ferdous Sohel, Farid Boussaid, Guan Wang, Anuj Srivastava|We propose the first comprehensive approach for modeling and analyzing the spatiotemporal shape variability in tree-like 4D objects, i.e., 3D objects whose shapes bend, stretch, and change in their branching structure over time as they deform, grow, and interact with their environment. Our key contribution is the representation of tree-like 3D shapes using Square Root Velocity Function Trees (SRVFT). By solving the spatial registration in the SRVFT space, which is equipped with an L2 metric, 4D tree-shaped structures become time-parameterized trajectories in this space. This reduces the problem of modeling and analyzing 4D tree-like shapes to that of modeling and analyzing elastic trajectories in the SRVFT space, where elasticity refers to time warping. In this paper, we propose a novel mathematical representation of the shape space of such trajectories, a Riemannian metric on that space, and computational tools for fast and accurate spatiotemporal registration and geodesics computation between 4D tree-shaped structures. Leveraging these building blocks, we develop a full framework for modelling the spatiotemporal variability using statistical models and generating novel 4D tree-like structures from a set of exemplars. We demonstrate and validate the proposed framework using real 4D plant data.|\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u79cd\u5168\u9762\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u5206\u6790\u6811\u72b6 4D \u7269\u4f53\u7684\u65f6\u7a7a\u5f62\u72b6\u53d8\u5316\uff0c\u5373\u968f\u7740\u5b83\u4eec\u53d8\u5f62\u3001\u751f\u957f\u548c\u4e0e\u73af\u5883\u76f8\u4e92\u4f5c\u7528\uff0c\u5176\u5f62\u72b6\u4f1a\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u5f2f\u66f2\u3001\u62c9\u4f38\u548c\u6539\u53d8\u5176\u5206\u652f\u7ed3\u6784\u7684 3D \u7269\u4f53\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u4f7f\u7528\u5e73\u65b9\u6839\u901f\u5ea6\u51fd\u6570\u6811 (SRVFT) \u8868\u793a\u6811\u72b6 3D \u5f62\u72b6\u3002\u901a\u8fc7\u89e3\u51b3\u914d\u5907 L2 \u5ea6\u91cf\u7684 SRVFT \u7a7a\u95f4\u4e2d\u7684\u7a7a\u95f4\u914d\u51c6\uff0c4D \u6811\u5f62\u7ed3\u6784\u6210\u4e3a\u8be5\u7a7a\u95f4\u4e2d\u7684\u65f6\u95f4\u53c2\u6570\u5316\u8f68\u8ff9\u3002\u8fd9\u5c06 4D \u6811\u5f62\u5f62\u72b6\u7684\u5efa\u6a21\u548c\u5206\u6790\u95ee\u9898\u7b80\u5316\u4e3a SRVFT \u7a7a\u95f4\u4e2d\u7684\u5f39\u6027\u8f68\u8ff9\u7684\u5efa\u6a21\u548c\u5206\u6790\u95ee\u9898\uff0c\u5176\u4e2d\u5f39\u6027\u662f\u6307\u65f6\u95f4\u626d\u66f2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6b64\u7c7b\u8f68\u8ff9\u5f62\u72b6\u7a7a\u95f4\u7684\u65b0\u578b\u6570\u5b66\u8868\u793a\u3001\u8be5\u7a7a\u95f4\u4e0a\u7684\u9ece\u66fc\u5ea6\u91cf\u4ee5\u53ca\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u5730\u5728 4D \u6811\u5f62\u7ed3\u6784\u4e4b\u95f4\u8fdb\u884c\u65f6\u7a7a\u914d\u51c6\u548c\u6d4b\u5730\u7ebf\u8ba1\u7b97\u7684\u8ba1\u7b97\u5de5\u5177\u3002\u5229\u7528\u8fd9\u4e9b\u6784\u5efa\u6a21\u5757\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u5bf9\u65f6\u7a7a\u53d8\u5f02\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4ece\u4e00\u7ec4\u6837\u672c\u4e2d\u751f\u6210\u65b0\u9896\u7684 4D \u6811\u72b6\u7ed3\u6784\u3002\u6211\u4eec\u4f7f\u7528\u771f\u5b9e\u7684 4D \u690d\u7269\u6570\u636e\u6f14\u793a\u548c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u3002|[2408.12443v1](http://arxiv.org/pdf/2408.12443v1)|null|\n", "2408.12191": "|**2024-08-22**|**Transientangelo: Few-Viewpoint Surface Reconstruction Using Single-Photon Lidar**|Transientangelo\uff1a\u5229\u7528\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5c11\u89c6\u70b9\u8868\u9762\u91cd\u5efa|Weihan Luo, Anagh Malik, David B. Lindell|We consider the problem of few-viewpoint 3D surface reconstruction using raw measurements from a lidar system. Lidar captures 3D scene geometry by emitting pulses of light to a target and recording the speed-of-light time delay of the reflected light. However, conventional lidar systems do not output the raw, captured waveforms of backscattered light; instead, they pre-process these data into a 3D point cloud. Since this procedure typically does not accurately model the noise statistics of the system, exploit spatial priors, or incorporate information about downstream tasks, it ultimately discards useful information that is encoded in raw measurements of backscattered light. Here, we propose to leverage raw measurements captured with a single-photon lidar system from multiple viewpoints to optimize a neural surface representation of a scene. The measurements consist of time-resolved photon count histograms, or transients, which capture information about backscattered light at picosecond time scales. Additionally, we develop new regularization strategies that improve robustness to photon noise, enabling accurate surface reconstruction with as few as 10 photons per pixel. Our method outperforms other techniques for few-viewpoint 3D reconstruction based on depth maps, point clouds, or conventional lidar as demonstrated in simulation and with captured data.|\u6211\u4eec\u8003\u8651\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u7684\u539f\u59cb\u6d4b\u91cf\u503c\u8fdb\u884c\u5c11\u89c6\u70b9 3D \u8868\u9762\u91cd\u5efa\u7684\u95ee\u9898\u3002\u6fc0\u5149\u96f7\u8fbe\u901a\u8fc7\u5411\u76ee\u6807\u53d1\u5c04\u5149\u8109\u51b2\u5e76\u8bb0\u5f55\u53cd\u5c04\u5149\u7684\u5149\u901f\u65f6\u95f4\u5ef6\u8fdf\u6765\u6355\u83b7 3D \u573a\u666f\u51e0\u4f55\u5f62\u72b6\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u4e0d\u4f1a\u8f93\u51fa\u6355\u83b7\u7684\u53cd\u5411\u6563\u5c04\u5149\u7684\u539f\u59cb\u6ce2\u5f62\uff1b\u76f8\u53cd\uff0c\u5b83\u4eec\u5c06\u8fd9\u4e9b\u6570\u636e\u9884\u5904\u7406\u6210 3D \u70b9\u4e91\u3002\u7531\u4e8e\u6b64\u8fc7\u7a0b\u901a\u5e38\u4e0d\u80fd\u51c6\u786e\u5730\u6a21\u62df\u7cfb\u7edf\u7684\u566a\u58f0\u7edf\u8ba1\u6570\u636e\u3001\u5229\u7528\u7a7a\u95f4\u5148\u9a8c\u6216\u6574\u5408\u6709\u5173\u4e0b\u6e38\u4efb\u52a1\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u5b83\u6700\u7ec8\u4f1a\u4e22\u5f03\u7f16\u7801\u5728\u53cd\u5411\u6563\u5c04\u5149\u539f\u59cb\u6d4b\u91cf\u503c\u4e2d\u7684\u6709\u7528\u4fe1\u606f\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u4ece\u591a\u4e2a\u89c6\u70b9\u6355\u83b7\u7684\u539f\u59cb\u6d4b\u91cf\u503c\u6765\u4f18\u5316\u573a\u666f\u7684\u795e\u7ecf\u8868\u9762\u8868\u793a\u3002\u6d4b\u91cf\u503c\u5305\u62ec\u65f6\u95f4\u5206\u8fa8\u7684\u5149\u5b50\u8ba1\u6570\u76f4\u65b9\u56fe\u6216\u77ac\u6001\uff0c\u5b83\u4eec\u4ee5\u76ae\u79d2\u65f6\u95f4\u5c3a\u5ea6\u6355\u83b7\u6709\u5173\u53cd\u5411\u6563\u5c04\u5149\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u5149\u5b50\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u6bcf\u50cf\u7d20\u5c11\u81f3 10 \u4e2a\u5149\u5b50\u8fdb\u884c\u7cbe\u786e\u7684\u8868\u9762\u91cd\u5efa\u3002\u6b63\u5982\u6a21\u62df\u548c\u6355\u83b7\u7684\u6570\u636e\u6240\u8bc1\u660e\u7684\u90a3\u6837\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u56fe\u3001\u70b9\u4e91\u6216\u4f20\u7edf\u6fc0\u5149\u96f7\u8fbe\u7684\u5176\u4ed6\u5c11\u89c6\u70b9 3D \u91cd\u5efa\u6280\u672f\u3002|[2408.12191v1](http://arxiv.org/pdf/2408.12191v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2408.12593": "|**2024-08-22**|**Automating Deformable Gasket Assembly**|\u81ea\u52a8\u5316\u53ef\u53d8\u5f62\u57ab\u7247\u7ec4\u88c5|Simeon Adebola, Tara Sadjadpour, Karim El-Refai, Will Panitch, Zehan Ma, Roy Lin, Tianshuang Qiu, Shreya Ganti, Charlotte Le, Jaimyn Drake, et.al.|In Gasket Assembly, a deformable gasket must be aligned and pressed into a narrow channel. This task is common for sealing surfaces in the manufacturing of automobiles, appliances, electronics, and other products. Gasket Assembly is a long-horizon, high-precision task and the gasket must align with the channel and be fully pressed in to achieve a secure fit. To compare approaches, we present 4 methods for Gasket Assembly: one policy from deep imitation learning and three procedural algorithms. We evaluate these methods with 100 physical trials. Results suggest that the Binary+ algorithm succeeds in 10/10 on the straight channel whereas the learned policy based on 250 human teleoperated demonstrations succeeds in 8/10 trials and is significantly slower. Code, CAD models, videos, and data can be found at https://berkeleyautomation.github.io/robot-gasket/|\u5728\u57ab\u5708\u7ec4\u88c5\u4e2d\uff0c\u5fc5\u987b\u5c06\u53ef\u53d8\u5f62\u57ab\u5708\u5bf9\u9f50\u5e76\u538b\u5165\u72ed\u7a84\u901a\u9053\u3002\u8fd9\u9879\u4efb\u52a1\u901a\u5e38\u7528\u4e8e\u6c7d\u8f66\u3001\u5bb6\u7535\u3001\u7535\u5b50\u4ea7\u54c1\u548c\u5176\u4ed6\u4ea7\u54c1\u5236\u9020\u4e2d\u7684\u5bc6\u5c01\u8868\u9762\u3002\u57ab\u5708\u7ec4\u88c5\u662f\u4e00\u9879\u957f\u671f\u3001\u9ad8\u7cbe\u5ea6\u7684\u4efb\u52a1\uff0c\u57ab\u5708\u5fc5\u987b\u4e0e\u901a\u9053\u5bf9\u9f50\u5e76\u5b8c\u5168\u538b\u5165\u4ee5\u5b9e\u73b0\u7262\u56fa\u914d\u5408\u3002\u4e3a\u4e86\u6bd4\u8f83\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 4 \u79cd\u57ab\u5708\u7ec4\u88c5\u65b9\u6cd5\uff1a\u4e00\u79cd\u6765\u81ea\u6df1\u5ea6\u6a21\u4eff\u5b66\u4e60\u7684\u7b56\u7565\u548c\u4e09\u79cd\u7a0b\u5e8f\u7b97\u6cd5\u3002\u6211\u4eec\u901a\u8fc7 100 \u6b21\u7269\u7406\u8bd5\u9a8c\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0cBinary+ \u7b97\u6cd5\u5728\u76f4\u901a\u9053\u4e0a 10/10 \u6b21\u6210\u529f\uff0c\u800c\u57fa\u4e8e 250 \u6b21\u4eba\u7c7b\u9065\u63a7\u6f14\u793a\u7684\u5b66\u4e60\u7b56\u7565\u5728 8/10 \u6b21\u8bd5\u9a8c\u4e2d\u6210\u529f\uff0c\u5e76\u4e14\u901f\u5ea6\u660e\u663e\u8f83\u6162\u3002\u4ee3\u7801\u3001CAD \u6a21\u578b\u3001\u89c6\u9891\u548c\u6570\u636e\u53ef\u5728 https://berkeleyautomation.github.io/robot-gasket/ \u627e\u5230|[2408.12593v1](http://arxiv.org/pdf/2408.12593v1)|null|\n", "2408.12531": "|**2024-08-22**|**Deep Learning Improvements for Sparse Spatial Field Reconstruction**|\u7a00\u758f\u7a7a\u95f4\u573a\u91cd\u5efa\u7684\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb|Robert Sunderhaft, Logan Frank, Jim Davis|Accurately reconstructing a global spatial field from sparse data has been a longstanding problem in several domains, such as Earth Sciences and Fluid Dynamics. Historically, scientists have approached this problem by employing complex physics models to reconstruct the spatial fields. However, these methods are often computationally intensive. With the increase in popularity of machine learning (ML), several researchers have applied ML to the spatial field reconstruction task and observed improvements in computational efficiency. One such method in arXiv:2101.00554 utilizes a sparse mask of sensor locations and a Voronoi tessellation with sensor measurements as inputs to a convolutional neural network for reconstructing the global spatial field. In this work, we propose multiple adjustments to the aforementioned approach and show improvements on geoscience and fluid dynamics simulation datasets. We identify and discuss scenarios that benefit the most using the proposed ML-based spatial field reconstruction approach.|\u4ece\u7a00\u758f\u6570\u636e\u4e2d\u51c6\u786e\u91cd\u5efa\u5168\u5c40\u7a7a\u95f4\u573a\u4e00\u76f4\u662f\u5730\u7403\u79d1\u5b66\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u7684\u957f\u671f\u95ee\u9898\u3002\u4ece\u5386\u53f2\u4e0a\u770b\uff0c\u79d1\u5b66\u5bb6\u4eec\u901a\u8fc7\u4f7f\u7528\u590d\u6742\u7684\u7269\u7406\u6a21\u578b\u6765\u91cd\u5efa\u7a7a\u95f4\u573a\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u3002\u968f\u7740\u673a\u5668\u5b66\u4e60 (ML) \u7684\u666e\u53ca\uff0c\u4e00\u4e9b\u7814\u7a76\u4eba\u5458\u5df2\u5c06 ML \u5e94\u7528\u4e8e\u7a7a\u95f4\u573a\u91cd\u5efa\u4efb\u52a1\uff0c\u5e76\u89c2\u5bdf\u5230\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u9ad8\u3002arXiv:2101.00554 \u4e2d\u7684\u4e00\u79cd\u65b9\u6cd5\u5229\u7528\u4f20\u611f\u5668\u4f4d\u7f6e\u7684\u7a00\u758f\u63a9\u7801\u548c\u5e26\u6709\u4f20\u611f\u5668\u6d4b\u91cf\u503c\u7684 Voronoi \u9576\u5d4c\u4f5c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u6765\u91cd\u5efa\u5168\u5c40\u7a7a\u95f4\u573a\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u4e0a\u8ff0\u65b9\u6cd5\u7684\u591a\u9879\u8c03\u6574\uff0c\u5e76\u5c55\u793a\u4e86\u5730\u7403\u79d1\u5b66\u548c\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u6570\u636e\u96c6\u7684\u6539\u8fdb\u3002\u6211\u4eec\u786e\u5b9a\u5e76\u8ba8\u8bba\u4e86\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u57fa\u4e8e ML \u7684\u7a7a\u95f4\u573a\u91cd\u5efa\u65b9\u6cd5\u6700\u53d7\u76ca\u7684\u573a\u666f\u3002|[2408.12531v1](http://arxiv.org/pdf/2408.12531v1)|null|\n", "2408.12396": "|**2024-08-22**|**Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision Models for Geophysical Data Analysis**|\u8de8\u9886\u57df\u57fa\u7840\u6a21\u578b\u9002\u914d\uff1a\u7528\u4e8e\u5730\u7403\u7269\u7406\u6570\u636e\u5206\u6790\u7684\u5f00\u521b\u6027\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b|Zhixiang Guo, Xinming Wu, Luming Liang, Hanlin Sheng, Nuo Chen, Zhengfa Bi|We explore adapting foundation models (FMs) from the computer vision domain to geoscience. FMs, large neural networks trained on massive datasets, excel in diverse tasks with remarkable adaptability and generality. However, geoscience faces challenges like lacking curated training datasets and high computational costs for developing specialized FMs. This study considers adapting FMs from computer vision to geoscience, analyzing their scale, adaptability, and generality for geoscientific data analysis. We introduce a workflow that leverages existing computer vision FMs, fine-tuning them for geoscientific tasks, reducing development costs while enhancing accuracy. Through experiments, we demonstrate this workflow's effectiveness in broad applications to process and interpret geoscientific data of lunar images, seismic data, DAS arrays and so on. Our findings introduce advanced ML techniques to geoscience, proving the feasibility and advantages of cross-domain FMs adaptation, driving further advancements in geoscientific data analysis and offering valuable insights for FMs applications in other scientific domains.|\u6211\u4eec\u63a2\u7d22\u5c06\u57fa\u7840\u6a21\u578b (FM) \u4ece\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5e94\u7528\u5230\u5730\u7403\u79d1\u5b66\u3002FM \u662f\u5728\u6d77\u91cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5927\u578b\u795e\u7ecf\u7f51\u7edc\uff0c\u5177\u6709\u51fa\u8272\u7684\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5730\u7403\u79d1\u5b66\u9762\u4e34\u7740\u7f3a\u4e4f\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5f00\u53d1\u4e13\u95e8 FM \u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u8003\u8651\u5c06 FM \u4ece\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u5230\u5730\u7403\u79d1\u5b66\uff0c\u5206\u6790\u5176\u89c4\u6a21\u3001\u9002\u5e94\u6027\u548c\u5730\u7403\u79d1\u5b66\u6570\u636e\u5206\u6790\u7684\u901a\u7528\u6027\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9 FM \u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u4ee5\u9002\u5e94\u5730\u7403\u79d1\u5b66\u4efb\u52a1\uff0c\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5728\u5e7f\u6cdb\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53ef\u4ee5\u5904\u7406\u548c\u89e3\u91ca\u6708\u7403\u56fe\u50cf\u3001\u5730\u9707\u6570\u636e\u3001DAS \u9635\u5217\u7b49\u5730\u7403\u79d1\u5b66\u6570\u636e\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5c06\u5148\u8fdb\u7684 ML \u6280\u672f\u5f15\u5165\u5730\u7403\u79d1\u5b66\uff0c\u8bc1\u660e\u4e86\u8de8\u9886\u57df FM \u9002\u5e94\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u63a8\u52a8\u4e86\u5730\u7403\u79d1\u5b66\u6570\u636e\u5206\u6790\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u5e76\u4e3a FM \u5728\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002|[2408.12396v1](http://arxiv.org/pdf/2408.12396v1)|null|\n", "2408.12380": "|**2024-08-22**|**UMERegRobust -- Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration**|UMERegRobust\u2014\u2014\u7528\u4e8e\u7a33\u5065\u70b9\u4e91\u914d\u51c6\u7684\u901a\u7528\u6d41\u5f62\u5d4c\u5165\u517c\u5bb9\u7279\u5f81|Yuval Haitman, Amit Efraim, Joseph M. Francos|In this paper, we adopt the Universal Manifold Embedding (UME) framework for the estimation of rigid transformations and extend it, so that it can accommodate scenarios involving partial overlap and differently sampled point clouds. UME is a methodology designed for mapping observations of the same object, related by rigid transformations, into a single low-dimensional linear subspace. This process yields a transformation-invariant representation of the observations, with its matrix form representation being covariant (i.e. equivariant) with the transformation. We extend the UME framework by introducing a UME-compatible feature extraction method augmented with a unique UME contrastive loss and a sampling equalizer. These components are integrated into a comprehensive and robust registration pipeline, named UMERegRobust. We propose the RotKITTI registration benchmark, specifically tailored to evaluate registration methods for scenarios involving large rotations. UMERegRobust achieves better than state-of-the-art performance on the KITTI benchmark, especially when strict precision of (1{\\deg}, 10cm) is considered (with an average gain of +9%), and notably outperform SOTA methods on the RotKITTI benchmark (with +45% gain compared the most recent SOTA method). Our code is available at https://github.com/yuvalH9/UMERegRobust.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u901a\u7528\u6d41\u5f62\u5d4c\u5165 (UME) \u6846\u67b6\u6765\u4f30\u8ba1\u521a\u6027\u53d8\u6362\u5e76\u5bf9\u5176\u8fdb\u884c\u6269\u5c55\uff0c\u4ee5\u4fbf\u5b83\u53ef\u4ee5\u9002\u5e94\u6d89\u53ca\u90e8\u5206\u91cd\u53e0\u548c\u4e0d\u540c\u91c7\u6837\u70b9\u4e91\u7684\u573a\u666f\u3002UME \u662f\u4e00\u79cd\u65e8\u5728\u5c06\u901a\u8fc7\u521a\u6027\u53d8\u6362\u5173\u8054\u7684\u540c\u4e00\u5bf9\u8c61\u7684\u89c2\u6d4b\u503c\u6620\u5c04\u5230\u5355\u4e2a\u4f4e\u7ef4\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\u3002\u8be5\u8fc7\u7a0b\u4ea7\u751f\u4e86\u89c2\u6d4b\u503c\u7684\u53d8\u6362\u4e0d\u53d8\u8868\u793a\uff0c\u5176\u77e9\u9635\u5f62\u5f0f\u8868\u793a\u4e0e\u53d8\u6362\u534f\u53d8\uff08\u5373\u7b49\u53d8\uff09\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e0e UME \u517c\u5bb9\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u6269\u5c55 UME \u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u72ec\u7279\u7684 UME \u5bf9\u6bd4\u635f\u5931\u548c\u91c7\u6837\u5747\u8861\u5668\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u96c6\u6210\u5230\u4e00\u4e2a\u5168\u9762\u800c\u5f3a\u5927\u7684\u914d\u51c6\u7ba1\u9053\u4e2d\uff0c\u540d\u4e3a UMERegRobust\u3002\u6211\u4eec\u63d0\u51fa\u4e86 RotKITTI \u914d\u51c6\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u6d89\u53ca\u5927\u65cb\u8f6c\u573a\u666f\u7684\u914d\u51c6\u65b9\u6cd5\u3002 UMERegRobust \u5728 KITTI \u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651 (1{\\deg}, 10cm) \u7684\u4e25\u683c\u7cbe\u5ea6\u65f6\uff08\u5e73\u5747\u589e\u76ca\u4e3a +9%\uff09\uff0c\u5e76\u4e14\u5728 RotKITTI \u57fa\u51c6\u4e0a\u660e\u663e\u4f18\u4e8e SOTA \u65b9\u6cd5\uff08\u4e0e\u6700\u65b0\u7684 SOTA \u65b9\u6cd5\u76f8\u6bd4\u589e\u76ca\u4e3a +45%\uff09\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/yuvalH9/UMERegRobust \u4e0a\u627e\u5230\u3002|[2408.12380v1](http://arxiv.org/pdf/2408.12380v1)|**[link](https://github.com/yuvalh9/umeregrobust)**|\n", "2408.12366": "|**2024-08-22**|**Robust Principal Component Analysis via Discriminant Sample Weight Learning**|\u901a\u8fc7\u5224\u522b\u6837\u672c\u6743\u91cd\u5b66\u4e60\u8fdb\u884c\u7a33\u5065\u4e3b\u6210\u5206\u5206\u6790|Yingzhuo Deng, Ke Hu, Bo Li, Yao Zhang|Principal component analysis (PCA) is a classical feature extraction method, but it may be adversely affected by outliers, resulting in inaccurate learning of the projection matrix. This paper proposes a robust method to estimate both the data mean and the PCA projection matrix by learning discriminant sample weights from data containing outliers. Each sample in the dataset is assigned a weight, and the proposed algorithm iteratively learns the weights, the mean, and the projection matrix, respectively. Specifically, when the mean and the projection matrix are available, via fine-grained analysis of outliers, a weight for each sample is learned hierarchically so that outliers have small weights while normal samples have large weights. With the learned weights available, a weighted optimization problem is solved to estimate both the data mean and the projection matrix. Because the learned weights discriminate outliers from normal samples, the adverse influence of outliers is mitigated due to the corresponding small weights. Experiments on toy data, UCI dataset, and face dataset demonstrate the effectiveness of the proposed method in estimating the mean and the projection matrix from the data containing outliers.|\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u662f\u4e00\u79cd\u7ecf\u5178\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f46\u5b83\u53ef\u80fd\u4f1a\u53d7\u5230\u5f02\u5e38\u503c\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u5bfc\u81f4\u6295\u5f71\u77e9\u9635\u5b66\u4e60\u4e0d\u51c6\u786e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5305\u542b\u5f02\u5e38\u503c\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u5224\u522b\u6837\u672c\u6743\u91cd\u6765\u4f30\u8ba1\u6570\u636e\u5747\u503c\u548cPCA\u6295\u5f71\u77e9\u9635\u3002\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u90fd\u88ab\u5206\u914d\u4e00\u4e2a\u6743\u91cd\uff0c\u8be5\u7b97\u6cd5\u5206\u522b\u8fed\u4ee3\u5b66\u4e60\u6743\u91cd\u3001\u5747\u503c\u548c\u6295\u5f71\u77e9\u9635\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u5df2\u77e5\u5747\u503c\u548c\u6295\u5f71\u77e9\u9635\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5bf9\u5f02\u5e38\u503c\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5206\u5c42\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u7684\u6743\u91cd\uff0c\u4f7f\u5f97\u5f02\u5e38\u503c\u5177\u6709\u8f83\u5c0f\u7684\u6743\u91cd\uff0c\u800c\u6b63\u5e38\u6837\u672c\u5177\u6709\u8f83\u5927\u7684\u6743\u91cd\u3002\u5229\u7528\u5b66\u4e60\u5230\u7684\u6743\u91cd\uff0c\u89e3\u51b3\u52a0\u6743\u4f18\u5316\u95ee\u9898\u4ee5\u4f30\u8ba1\u6570\u636e\u5747\u503c\u548c\u6295\u5f71\u77e9\u9635\u3002\u7531\u4e8e\u5b66\u4e60\u5230\u7684\u6743\u91cd\u5c06\u5f02\u5e38\u503c\u4e0e\u6b63\u5e38\u6837\u672c\u533a\u5206\u5f00\u6765\uff0c\u56e0\u6b64\u5f02\u5e38\u503c\u7684\u4e0d\u5229\u5f71\u54cd\u4f1a\u7531\u4e8e\u76f8\u5e94\u7684\u6743\u91cd\u8f83\u5c0f\u800c\u51cf\u8f7b\u3002\u5728\u73a9\u5177\u6570\u636e\u3001UCI \u6570\u636e\u96c6\u548c\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u4ece\u5305\u542b\u5f02\u5e38\u503c\u7684\u6570\u636e\u4e2d\u4f30\u8ba1\u5747\u503c\u548c\u6295\u5f71\u77e9\u9635\u7684\u6709\u6548\u6027\u3002|[2408.12366v1](http://arxiv.org/pdf/2408.12366v1)|null|\n", "2408.12316": "|**2024-08-22**|**Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement**|\u5c55\u5f00\u5206\u89e3\u975e\u914d\u5bf9\u5b66\u4e60\u4ee5\u5b9e\u73b0\u53ef\u63a7\u4f4e\u5149\u89c6\u9891\u589e\u5f3a|Lingyu Zhu, Wenhan Yang, Baoliang Chen, Hanwei Zhu, Zhangkai Ni, Qi Mao, Shiqi Wang|Obtaining pairs of low/normal-light videos, with motions, is more challenging than still images, which raises technical issues and poses the technical route of unpaired learning as a critical role. This paper makes endeavors in the direction of learning for low-light video enhancement without using paired ground truth. Compared to low-light image enhancement, enhancing low-light videos is more difficult due to the intertwined effects of noise, exposure, and contrast in the spatial domain, jointly with the need for temporal coherence. To address the above challenge, we propose the Unrolled Decomposed Unpaired Network (UDU-Net) for enhancing low-light videos by unrolling the optimization functions into a deep network to decompose the signal into spatial and temporal-related factors, which are updated iteratively. Firstly, we formulate low-light video enhancement as a Maximum A Posteriori estimation (MAP) problem with carefully designed spatial and temporal visual regularization. Then, via unrolling the problem, the optimization of the spatial and temporal constraints can be decomposed into different steps and updated in a stage-wise manner. From the spatial perspective, the designed Intra subnet leverages unpair prior information from expert photography retouched skills to adjust the statistical distribution. Additionally, we introduce a novel mechanism that integrates human perception feedback to guide network optimization, suppressing over/under-exposure conditions. Meanwhile, to address the issue from the temporal perspective, the designed Inter subnet fully exploits temporal cues in progressive optimization, which helps achieve improved temporal consistency in enhancement results. Consequently, the proposed method achieves superior performance to state-of-the-art methods in video illumination, noise suppression, and temporal consistency across outdoor and indoor scenes.|\u83b7\u53d6\u5e26\u6709\u8fd0\u52a8\u7684\u4f4e\u5149 / \u6b63\u5e38\u5149\u89c6\u9891\u5bf9\u6bd4\u83b7\u53d6\u9759\u6b62\u56fe\u50cf\u66f4\u5177\u6311\u6218\u6027\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e9b\u6280\u672f\u95ee\u9898\uff0c\u4e5f\u4f7f\u5f97\u975e\u914d\u5bf9\u5b66\u4e60\u7684\u6280\u672f\u8def\u7ebf\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002\u672c\u6587\u81f4\u529b\u4e8e\u5728\u4e0d\u4f7f\u7528\u914d\u5bf9\u57fa\u672c\u4e8b\u5b9e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u7684\u5b66\u4e60\u3002\u4e0e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u76f8\u6bd4\uff0c\u589e\u5f3a\u4f4e\u5149\u89c6\u9891\u66f4\u52a0\u56f0\u96be\uff0c\u56e0\u4e3a\u7a7a\u57df\u4e2d\u566a\u58f0\u3001\u66dd\u5149\u548c\u5bf9\u6bd4\u5ea6\u76f8\u4e92\u4ea4\u7ec7\uff0c\u540c\u65f6\u9700\u8981\u65f6\u95f4\u8fde\u8d2f\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c55\u5f00\u5206\u89e3\u975e\u914d\u5bf9\u7f51\u7edc (UDU-Net) \u6765\u589e\u5f3a\u4f4e\u5149\u89c6\u9891\uff0c\u5b83\u5c06\u4f18\u5316\u51fd\u6570\u5c55\u5f00\u5230\u6df1\u5ea6\u7f51\u7edc\u4e2d\uff0c\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u56e0\u5b50\uff0c\u5e76\u8fdb\u884c\u8fed\u4ee3\u66f4\u65b0\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c06\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u8868\u8ff0\u4e3a\u6700\u5927\u540e\u9a8c\u4f30\u8ba1 (MAP) \u95ee\u9898\uff0c\u5e76\u7cbe\u5fc3\u8bbe\u8ba1\u7a7a\u95f4\u548c\u65f6\u95f4\u89c6\u89c9\u6b63\u5219\u5316\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5c55\u5f00\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06\u7a7a\u95f4\u548c\u65f6\u95f4\u7ea6\u675f\u7684\u4f18\u5316\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u6b65\u9aa4\u5e76\u5206\u9636\u6bb5\u66f4\u65b0\u3002\u4ece\u7a7a\u95f4\u89d2\u5ea6\u6765\u770b\uff0c\u8bbe\u8ba1\u7684 Intra \u5b50\u7f51\u7edc\u5229\u7528\u6765\u81ea\u4e13\u4e1a\u6444\u5f71\u4fee\u9970\u6280\u80fd\u7684\u672a\u914d\u5bf9\u5148\u9a8c\u4fe1\u606f\u6765\u8c03\u6574\u7edf\u8ba1\u5206\u5e03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5236\uff0c\u8be5\u673a\u5236\u7ed3\u5408\u4e86\u4eba\u7c7b\u611f\u77e5\u53cd\u9988\u6765\u6307\u5bfc\u7f51\u7edc\u4f18\u5316\uff0c\u6291\u5236\u8fc7\u5ea6/\u66dd\u5149\u4e0d\u8db3\u7684\u60c5\u51b5\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u4ece\u65f6\u95f4\u89d2\u5ea6\u89e3\u51b3\u95ee\u9898\uff0c\u8bbe\u8ba1\u7684 Inter \u5b50\u7f51\u7edc\u5145\u5206\u5229\u7528\u4e86\u6e10\u8fdb\u5f0f\u4f18\u5316\u4e2d\u7684\u65f6\u95f4\u7ebf\u7d22\uff0c\u8fd9\u6709\u52a9\u4e8e\u5728\u589e\u5f3a\u7ed3\u679c\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u56e0\u6b64\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u7167\u660e\u3001\u566a\u58f0\u6291\u5236\u548c\u5ba4\u5185\u5916\u573a\u666f\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002|[2408.12316v1](http://arxiv.org/pdf/2408.12316v1)|**[link](https://github.com/lingyzhu0101/udu)**|\n", "2408.12292": "|**2024-08-22**|**Towards Deconfounded Image-Text Matching with Causal Inference**|\u5229\u7528\u56e0\u679c\u63a8\u7406\u5b9e\u73b0\u53bb\u6df7\u6dc6\u56fe\u50cf\u6587\u672c\u5339\u914d|Wenhui Li, Xinqi Su, Dan Song, Lanjun Wang, Kun Zhang, An-An Liu|Prior image-text matching methods have shown remarkable performance on many benchmark datasets, but most of them overlook the bias in the dataset, which exists in intra-modal and inter-modal, and tend to learn the spurious correlations that extremely degrade the generalization ability of the model. Furthermore, these methods often incorporate biased external knowledge from large-scale datasets as prior knowledge into image-text matching model, which is inevitable to force model further learn biased associations. To address above limitations, this paper firstly utilizes Structural Causal Models (SCMs) to illustrate how intra- and inter-modal confounders damage the image-text matching. Then, we employ backdoor adjustment to propose an innovative Deconfounded Causal Inference Network (DCIN) for image-text matching task. DCIN (1) decomposes the intra- and inter-modal confounders and incorporates them into the encoding stage of visual and textual features, effectively eliminating the spurious correlations during image-text matching, and (2) uses causal inference to mitigate biases of external knowledge. Consequently, the model can learn causality instead of spurious correlations caused by dataset bias. Extensive experiments on two well-known benchmark datasets, i.e., Flickr30K and MSCOCO, demonstrate the superiority of our proposed method.|\u5148\u524d\u7684\u56fe\u50cf-\u6587\u672c\u5339\u914d\u65b9\u6cd5\u5728\u8bb8\u591a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4e2d\u7684\u5927\u591a\u6570\u90fd\u5ffd\u7565\u4e86\u5b58\u5728\u4e8e\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u6570\u636e\u96c6\u504f\u5dee\uff0c\u5e76\u4e14\u503e\u5411\u4e8e\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u8fd9\u6781\u5927\u5730\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5c06\u6765\u81ea\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u6709\u504f\u89c1\u7684\u5916\u90e8\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u7eb3\u5165\u56fe\u50cf-\u6587\u672c\u5339\u914d\u6a21\u578b\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u8feb\u4f7f\u6a21\u578b\u8fdb\u4e00\u6b65\u5b66\u4e60\u6709\u504f\u89c1\u7684\u5173\u8054\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\uff0c\u672c\u6587\u9996\u5148\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6765\u8bf4\u660e\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u6df7\u6742\u56e0\u7d20\u5982\u4f55\u635f\u5bb3\u56fe\u50cf-\u6587\u672c\u5339\u914d\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528\u540e\u95e8\u8c03\u6574\uff0c\u4e3a\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4efb\u52a1\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u53bb\u6df7\u6dc6\u56e0\u679c\u63a8\u7406\u7f51\u7edc\uff08DCIN\uff09\u3002DCIN\uff081\uff09\u5206\u89e3\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u6df7\u6742\u56e0\u7d20\u5e76\u5c06\u5176\u7eb3\u5165\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u7f16\u7801\u9636\u6bb5\uff0c\u6709\u6548\u6d88\u9664\u56fe\u50cf-\u6587\u672c\u5339\u914d\u8fc7\u7a0b\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff1b\uff082\uff09\u4f7f\u7528\u56e0\u679c\u63a8\u7406\u6765\u51cf\u8f7b\u5916\u90e8\u77e5\u8bc6\u7684\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u56e0\u679c\u5173\u7cfb\uff0c\u800c\u4e0d\u662f\u6570\u636e\u96c6\u504f\u5dee\u5bfc\u81f4\u7684\u865a\u5047\u76f8\u5173\u6027\u3002\u5728\u4e24\u4e2a\u8457\u540d\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5373 Flickr30K \u548c MSCOCO\uff09\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002|[2408.12292v1](http://arxiv.org/pdf/2408.12292v1)|null|\n", "2408.12128": "|**2024-08-22**|**Diffusion-Based Visual Art Creation: A Survey and New Perspectives**|\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u827a\u672f\u521b\u4f5c\uff1a\u7efc\u8ff0\u4e0e\u65b0\u89c6\u89d2|Bingyuan Wang, Qifeng Chen, Zeyu Wang|The integration of generative AI in visual art has revolutionized not only how visual content is created but also how AI interacts with and reflects the underlying domain knowledge. This survey explores the emerging realm of diffusion-based visual art creation, examining its development from both artistic and technical perspectives. We structure the survey into three phases, data feature and framework identification, detailed analyses using a structured coding process, and open-ended prospective outlooks. Our findings reveal how artistic requirements are transformed into technical challenges and highlight the design and application of diffusion-based methods within visual art creation. We also provide insights into future directions from technical and synergistic perspectives, suggesting that the confluence of generative AI and art has shifted the creative paradigm and opened up new possibilities. By summarizing the development and trends of this emerging interdisciplinary area, we aim to shed light on the mechanisms through which AI systems emulate and possibly, enhance human capacities in artistic perception and creativity.|\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u89c6\u89c9\u827a\u672f\u7684\u878d\u5408\u4e0d\u4ec5\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u5185\u5bb9\u7684\u521b\u4f5c\u65b9\u5f0f\uff0c\u800c\u4e14\u4e5f\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u5de5\u667a\u80fd\u4e0e\u5e95\u5c42\u9886\u57df\u77e5\u8bc6\u7684\u4ea4\u4e92\u548c\u53cd\u6620\u65b9\u5f0f\u3002\u672c\u8c03\u67e5\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u827a\u672f\u521b\u4f5c\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u4ece\u827a\u672f\u548c\u6280\u672f\u89d2\u5ea6\u5ba1\u89c6\u5176\u53d1\u5c55\u3002\u6211\u4eec\u5c06\u8c03\u67e5\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u7279\u5f81\u548c\u6846\u67b6\u8bc6\u522b\u3001\u4f7f\u7528\u7ed3\u6784\u5316\u7f16\u7801\u8fc7\u7a0b\u7684\u8be6\u7ec6\u5206\u6790\u4ee5\u53ca\u5f00\u653e\u5f0f\u7684\u524d\u77bb\u6027\u5c55\u671b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u827a\u672f\u8981\u6c42\u5982\u4f55\u8f6c\u5316\u4e3a\u6280\u672f\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u8bbe\u8ba1\u548c\u5e94\u7528\u3002\u6211\u4eec\u8fd8\u4ece\u6280\u672f\u548c\u534f\u540c\u7684\u89d2\u5ea6\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765\u65b9\u5411\u7684\u89c1\u89e3\uff0c\u8868\u660e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u827a\u672f\u7684\u878d\u5408\u5df2\u7ecf\u6539\u53d8\u4e86\u521b\u4f5c\u8303\u5f0f\u5e76\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u901a\u8fc7\u603b\u7ed3\u8fd9\u4e00\u65b0\u5174\u8de8\u5b66\u79d1\u9886\u57df\u7684\u53d1\u5c55\u548c\u8d8b\u52bf\uff0c\u6211\u4eec\u65e8\u5728\u9610\u660e\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6a21\u4eff\u5e76\u53ef\u80fd\u589e\u5f3a\u4eba\u7c7b\u827a\u672f\u611f\u77e5\u548c\u521b\u9020\u529b\u80fd\u529b\u7684\u673a\u5236\u3002|[2408.12128v1](http://arxiv.org/pdf/2408.12128v1)|null|\n", "2408.12114": "|**2024-08-22**|**SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models**|SPARK\uff1a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u89c6\u89c9\u4f20\u611f\u5668\u611f\u77e5\u548c\u63a8\u7406\u57fa\u51c6|Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, Yong Man Ro|Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors. They fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properly. Consequently, alignment between the information from the actual physical environment and the text is not achieved correctly, making it difficult to answer complex sensor-related questions that consider the physical environment. In this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensors. We generated 6,248 vision-language test samples automatically to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questions. We utilized these samples to assess ten leading LVLMs. The results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extents. Codes and data are available at https://github.com/top-yun/SPARK|\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u6587\u672c\u5bf9\u9f50\u89c6\u89c9\u8f93\u5165\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u901a\u8fc7\u5c06\u6587\u672c\u6a21\u6001\u4e0e\u89c6\u89c9\u8f93\u5165\u5bf9\u9f50\uff0c\u5b83\u4eec\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u4e5f\u6709\u4eba\u8bd5\u56fe\u5c06\u591a\u89c6\u89c9\u4f20\u611f\u5668\u7eb3\u5165 RGB \u4e4b\u5916\uff0c\u5305\u62ec\u70ed\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u50cf\u548c\u533b\u5b66 X \u5c04\u7ebf\u56fe\u50cf\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5f53\u524d\u7684 LVLM \u5c06\u4ece\u591a\u89c6\u89c9\u4f20\u611f\u5668\u62cd\u6444\u7684\u56fe\u50cf\u89c6\u4e3a\u4f4d\u4e8e\u540c\u4e00 RGB \u57df\u4e2d\uff0c\u800c\u6ca1\u6709\u8003\u8651\u591a\u89c6\u89c9\u4f20\u611f\u5668\u7684\u7269\u7406\u7279\u6027\u3002\u5b83\u4eec\u65e0\u6cd5\u6b63\u786e\u4f20\u8fbe\u6765\u81ea\u6570\u636e\u96c6\u7684\u57fa\u672c\u591a\u89c6\u89c9\u4f20\u611f\u5668\u4fe1\u606f\u548c\u76f8\u5e94\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002\u56e0\u6b64\uff0c\u65e0\u6cd5\u6b63\u786e\u5b9e\u73b0\u6765\u81ea\u5b9e\u9645\u7269\u7406\u73af\u5883\u548c\u6587\u672c\u7684\u4fe1\u606f\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u96be\u4ee5\u56de\u7b54\u8003\u8651\u7269\u7406\u73af\u5883\u7684\u590d\u6742\u4f20\u611f\u5668\u76f8\u5173\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u540d\u4e3a SPARK \u7684\u591a\u89c6\u89c9\u4f20\u611f\u5668\u611f\u77e5\u548c\u63a8\u7406\u57fa\u51c6\uff0c\u4ee5\u51cf\u5c11\u56fe\u50cf\u548c\u591a\u89c6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u57fa\u672c\u591a\u89c6\u89c9\u4f20\u611f\u5668\u4fe1\u606f\u5dee\u8ddd\u3002\u6211\u4eec\u81ea\u52a8\u751f\u6210\u4e86 6248 \u4e2a\u89c6\u89c9\u8bed\u8a00\u6d4b\u8bd5\u6837\u672c\uff0c\u4ee5\u7814\u7a76\u591a\u89c6\u89c9\u611f\u77e5\u548c\u591a\u89c6\u89c9\u611f\u77e5\u63a8\u7406\u5bf9\u4e0d\u540c\u683c\u5f0f\u7684\u7269\u7406\u4f20\u611f\u5668\u77e5\u8bc6\u719f\u7ec3\u7a0b\u5ea6\u7684\u5f71\u54cd\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u4f20\u611f\u5668\u76f8\u5173\u95ee\u9898\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e9b\u6837\u672c\u8bc4\u4f30\u4e86\u5341\u4e2a\u9886\u5148\u7684 LVLM\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u591a\u89c6\u89c9\u611f\u77e5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u7f3a\u9677\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/top-yun/SPARK \u4e0a\u627e\u5230|[2408.12114v1](http://arxiv.org/pdf/2408.12114v1)|**[link](https://github.com/top-yun/spark)**|\n", "2408.12109": "|**2024-08-22**|**RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data**|RoVRM\uff1a\u901a\u8fc7\u8f85\u52a9\u6587\u672c\u504f\u597d\u6570\u636e\u4f18\u5316\u7684\u7a33\u5065\u89c6\u89c9\u5956\u52b1\u6a21\u578b|Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Murun Yang, Qiaozhi He, Tong Xiao, Chunliang Zhang, Tongran Liu, Quan Du, et.al.|Large vision-language models (LVLMs) often fail to align with human preferences, leading to issues like generating misleading content without proper visual context (also known as hallucination). A promising solution to this problem is using human-preference alignment techniques, such as best-of-n sampling and reinforcement learning. However, these techniques face the difficulty arising from the scarcity of visual preference data, which is required to train a visual reward model (VRM). In this work, we continue the line of research. We present a Robust Visual Reward Model (RoVRM) which improves human-preference alignment for LVLMs. RoVRM leverages auxiliary textual preference data through a three-phase progressive training and optimal transport-based preference data selection to effectively mitigate the scarcity of visual preference data. We experiment with RoVRM on the commonly used vision-language tasks based on the LLaVA-1.5-7B and -13B models. Experimental results demonstrate that RoVRM consistently outperforms traditional VRMs. Furthermore, our three-phase progressive training and preference data selection approaches can yield consistent performance gains over ranking-based alignment techniques, such as direct preference optimization.|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u901a\u5e38\u65e0\u6cd5\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4ece\u800c\u5bfc\u81f4\u8bf8\u5982\u5728\u6ca1\u6709\u9002\u5f53\u89c6\u89c9\u80cc\u666f\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u8bef\u5bfc\u6027\u5185\u5bb9\uff08\u4e5f\u79f0\u4e3a\u5e7b\u89c9\uff09\u7b49\u95ee\u9898\u3002\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u6280\u672f\uff0c\u4f8b\u5982\u6700\u4f73 n \u91c7\u6837\u548c\u5f3a\u5316\u5b66\u4e60\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6280\u672f\u9762\u4e34\u7740\u89c6\u89c9\u504f\u597d\u6570\u636e\u7a00\u7f3a\u6240\u5e26\u6765\u7684\u56f0\u96be\uff0c\u800c\u89c6\u89c9\u504f\u597d\u6570\u636e\u662f\u8bad\u7ec3\u89c6\u89c9\u5956\u52b1\u6a21\u578b (VRM) \u6240\u5fc5\u9700\u7684\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7ee7\u7eed\u7814\u7a76\u65b9\u5411\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u89c6\u89c9\u5956\u52b1\u6a21\u578b (RoVRM)\uff0c\u53ef\u6539\u5584 LVLM \u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002RoVRM \u901a\u8fc7\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u57fa\u4e8e\u6700\u4f73\u4f20\u8f93\u7684\u504f\u597d\u6570\u636e\u9009\u62e9\u5229\u7528\u8f85\u52a9\u6587\u672c\u504f\u597d\u6570\u636e\uff0c\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u504f\u597d\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002\u6211\u4eec\u57fa\u4e8e LLaVA-1.5-7B \u548c -13B \u6a21\u578b\u5728\u5e38\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u4f7f\u7528 RoVRM \u8fdb\u884c\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoVRM \u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf VRM\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u504f\u597d\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u53ef\u4ee5\u6bd4\u57fa\u4e8e\u6392\u540d\u7684\u5bf9\u9f50\u6280\u672f\uff08\u4f8b\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u4ea7\u751f\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002|[2408.12109v1](http://arxiv.org/pdf/2408.12109v1)|null|\n", "2408.12100": "|**2024-08-22**|**A Unified Plug-and-Play Algorithm with Projected Landweber Operator for Split Convex Feasibility Problems**|\u5206\u88c2\u51f8\u53ef\u884c\u6027\u95ee\u9898\u7684\u6295\u5f71 Landweber \u7b97\u5b50\u7684\u7edf\u4e00\u5373\u63d2\u5373\u7528\u7b97\u6cd5|Shuchang Zhang, Hongxia Wang|In recent years Plug-and-Play (PnP) methods have achieved state-of-the-art performance in inverse imaging problems by replacing proximal operators with denoisers. Based on the proximal gradient method, some theoretical results of PnP have appeared, where appropriate step size is crucial for convergence analysis. However, in practical applications, applying PnP methods with theoretically guaranteed step sizes is difficult, and these algorithms are limited to Gaussian noise. In this paper,from a perspective of split convex feasibility problems (SCFP), an adaptive PnP algorithm with Projected Landweber Operator (PnP-PLO) is proposed to address these issues. Numerical experiments on image deblurring, super-resolution, and compressed sensing MRI experiments illustrate that PnP-PLO with theoretical guarantees outperforms state-of-the-art methods such as RED and RED-PRO.|\u8fd1\u5e74\u6765\uff0c\u5373\u63d2\u5373\u7528 (PnP) \u65b9\u6cd5\u901a\u8fc7\u7528\u53bb\u566a\u5668\u66ff\u6362\u8fd1\u7aef\u7b97\u5b50\u5728\u9006\u6210\u50cf\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002\u57fa\u4e8e\u8fd1\u7aef\u68af\u5ea6\u6cd5\uff0cPnP \u7684\u4e00\u4e9b\u7406\u8bba\u7ed3\u679c\u5df2\u7ecf\u51fa\u73b0\uff0c\u5176\u4e2d\u5408\u9002\u7684\u6b65\u957f\u5bf9\u4e8e\u6536\u655b\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5e94\u7528\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6b65\u957f\u7684 PnP \u65b9\u6cd5\u5f88\u56f0\u96be\uff0c\u5e76\u4e14\u8fd9\u4e9b\u7b97\u6cd5\u4ec5\u9650\u4e8e\u9ad8\u65af\u566a\u58f0\u3002\u672c\u6587\u4ece\u5206\u88c2\u51f8\u53ef\u884c\u6027\u95ee\u9898 (SCFP) \u7684\u89d2\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u6295\u5f71 Landweber \u7b97\u5b50\u7684\u81ea\u9002\u5e94 PnP \u7b97\u6cd5 (PnP-PLO) \u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u538b\u7f29\u611f\u77e5 MRI \u5b9e\u9a8c\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684 PnP-PLO \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982 RED \u548c RED-PRO\u3002|[2408.12100v1](http://arxiv.org/pdf/2408.12100v1)|null|\n", "2408.12077": "|**2024-08-22**|**Through-the-Wall Radar Human Activity Micro-Doppler Signature Representation Method Based on Joint Boulic-Sinusoidal Pendulum Model**|\u57fa\u4e8e\u8054\u5408\u6b63\u5f26\u6446\u6a21\u578b\u7684\u7a7f\u5899\u96f7\u8fbe\u4eba\u4f53\u6d3b\u52a8\u5fae\u591a\u666e\u52d2\u7279\u5f81\u8868\u793a\u65b9\u6cd5|Xiaopeng Yang, Weicheng Gao, Xiaodong Qu, Zeyu Ma, Hao Zhang|With the help of micro-Doppler signature, ultra-wideband (UWB) through-the-wall radar (TWR) enables the reconstruction of range and velocity information of limb nodes to accurately identify indoor human activities. However, existing methods are usually trained and validated directly using range-time maps (RTM) and Doppler-time maps (DTM), which have high feature redundancy and poor generalization ability. In order to solve this problem, this paper proposes a human activity micro-Doppler signature representation method based on joint Boulic-sinusoidal pendulum motion model. In detail, this paper presents a simplified joint Boulic-sinusoidal pendulum human motion model by taking head, torso, both hands and feet into consideration improved from Boulic-Thalmann kinematic model. The paper also calculates the minimum number of key points needed to describe the Doppler and micro-Doppler information sufficiently. Both numerical simulations and experiments are conducted to verify the effectiveness. The results demonstrate that the proposed number of key points of micro-Doppler signature can precisely represent the indoor human limb node motion characteristics, and substantially improve the generalization capability of the existing methods for different testers.|\u8d85\u5bbd\u5e26\uff08UWB\uff09\u7a7f\u5899\u96f7\u8fbe\uff08TWR\uff09\u5229\u7528\u5fae\u591a\u666e\u52d2\u7279\u5f81\u91cd\u5efa\u80a2\u4f53\u8282\u70b9\u7684\u8ddd\u79bb\u548c\u901f\u5ea6\u4fe1\u606f\uff0c\u4ece\u800c\u51c6\u786e\u8bc6\u522b\u5ba4\u5185\u4eba\u4f53\u6d3b\u52a8\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4f7f\u7528\u8ddd\u79bb-\u65f6\u95f4\u56fe\uff08RTM\uff09\u548c\u591a\u666e\u52d2-\u65f6\u95f4\u56fe\uff08DTM\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u7279\u5f81\u5197\u4f59\u5ea6\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8054\u5408Boulic-\u6b63\u5f26\u6446\u8fd0\u52a8\u6a21\u578b\u7684\u4eba\u4f53\u6d3b\u52a8\u5fae\u591a\u666e\u52d2\u7279\u5f81\u8868\u793a\u65b9\u6cd5\u3002\u5728Boulic-Thalmann\u8fd0\u52a8\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e00\u79cd\u7b80\u5316\u7684\u8054\u5408Boulic-\u6b63\u5f26\u6446\u4eba\u4f53\u8fd0\u52a8\u6a21\u578b\uff0c\u540c\u65f6\u8003\u8651\u5934\u90e8\u3001\u8eaf\u5e72\u548c\u53cc\u624b\u3001\u53cc\u811a\uff0c\u5e76\u8ba1\u7b97\u4e86\u5145\u5206\u63cf\u8ff0\u591a\u666e\u52d2\u548c\u5fae\u591a\u666e\u52d2\u4fe1\u606f\u6240\u9700\u7684\u6700\u5c11\u5173\u952e\u70b9\u4e2a\u6570\u3002\u6570\u503c\u4eff\u771f\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5fae\u591a\u666e\u52d2\u7279\u5f81\u5173\u952e\u70b9\u4e2a\u6570\u80fd\u591f\u51c6\u786e\u8868\u5f81\u5ba4\u5185\u4eba\u4f53\u80a2\u4f53\u8282\u70b9\u8fd0\u52a8\u7279\u5f81\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u540c\u6d4b\u8bd5\u8005\u7684\u6cdb\u5316\u80fd\u529b\u3002|[2408.12077v1](http://arxiv.org/pdf/2408.12077v1)|null|\n"}}