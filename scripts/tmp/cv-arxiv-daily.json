{"\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.05334": "|**2024-01-10**|**URHand: Universal Relightable Hands**|URHand\uff1a\u901a\u7528\u53ef\u91cd\u590d\u7167\u660e\u624b|Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, et.al.|Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations. To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities. The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations. To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature. By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport. This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities. In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization. Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability. We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity.|\u73b0\u6709\u7684\u903c\u771f\u53ef\u91cd\u590d\u7167\u660e\u624b\u6a21\u578b\u9700\u8981\u5728\u4e0d\u540c\u89c6\u56fe\u3001\u59ff\u52bf\u548c\u7167\u660e\u4e0b\u8fdb\u884c\u5e7f\u6cdb\u7684\u7279\u5b9a\u4e8e\u8eab\u4efd\u7684\u89c2\u5bdf\uff0c\u5e76\u4e14\u5728\u63a8\u5e7f\u5230\u81ea\u7136\u7167\u660e\u548c\u65b0\u8eab\u4efd\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 URHand\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u901a\u7528\u7684\u53ef\u91cd\u65b0\u7167\u660e\u7684\u624b\u6a21\u578b\uff0c\u5b83\u6982\u62ec\u4e86\u89c6\u89d2\u3001\u59ff\u52bf\u3001\u7167\u660e\u548c\u8eab\u4efd\u3002\u6211\u4eec\u7684\u6a21\u578b\u5141\u8bb8\u4f7f\u7528\u624b\u673a\u6355\u83b7\u7684\u56fe\u50cf\u8fdb\u884c\u5c11\u91cf\u62cd\u6444\u7684\u4e2a\u6027\u5316\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u65b0\u9896\u7684\u7167\u660e\u4e0b\u8fdb\u884c\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\u3002\u4e3a\u4e86\u7b80\u5316\u4e2a\u6027\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u7559\u7167\u7247\u771f\u5b9e\u611f\uff0c\u6211\u4eec\u57fa\u4e8e\u5728\u5177\u6709\u6570\u767e\u4e2a\u8eab\u4efd\u7684\u5149\u9636\u6bb5\u4e2d\u6355\u83b7\u7684\u624b\u90e8\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u795e\u7ecf\u91cd\u65b0\u7167\u660e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u901a\u7528\u53ef\u91cd\u65b0\u7167\u660e\u5148\u9a8c\u3002\u5173\u952e\u7684\u6311\u6218\u662f\u6269\u5c55\u8de8\u8eab\u4efd\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u6027\u5316\u7684\u4fdd\u771f\u5ea6\u548c\u6e05\u6670\u7684\u7ec6\u8282\uff0c\u800c\u4e0d\u5f71\u54cd\u81ea\u7136\u7167\u660e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u53d8\u5316\u7684\u7ebf\u6027\u5149\u7167\u6a21\u578b\u4f5c\u4e3a\u795e\u7ecf\u6e32\u67d3\u5668\uff0c\u5b83\u5c06\u7269\u7406\u542f\u53d1\u7684\u7740\u8272\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\u3002\u901a\u8fc7\u6d88\u9664\u975e\u7ebf\u6027\u6fc0\u6d3b\u548c\u504f\u5dee\uff0c\u6211\u4eec\u4e13\u95e8\u8bbe\u8ba1\u7684\u7167\u660e\u6a21\u578b\u660e\u786e\u5730\u4fdd\u6301\u4e86\u5149\u4f20\u8f93\u7684\u7ebf\u6027\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u4ece\u5149\u9636\u6bb5\u6570\u636e\u8fdb\u884c\u5355\u9636\u6bb5\u8bad\u7ec3\uff0c\u540c\u65f6\u63a8\u5e7f\u5230\u8de8\u4e0d\u540c\u8eab\u4efd\u7684\u4efb\u610f\u8fde\u7eed\u7167\u660e\u4e0b\u7684\u5b9e\u65f6\u6e32\u67d3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u548c\u795e\u7ecf\u91cd\u65b0\u7167\u660e\u6a21\u578b\u7684\u8054\u5408\u5b66\u4e60\uff0c\u8fd9\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901a\u7528\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u6f14\u793a\u4e86\u901a\u8fc7\u5bf9\u770b\u4e0d\u89c1\u7684\u8eab\u4efd\u8fdb\u884c\u7b80\u77ed\u7684\u624b\u673a\u626b\u63cf\u6765\u5feb\u901f\u4e2a\u6027\u5316 URHand\u3002|[2401.05334v1](http://arxiv.org/pdf/2401.05334v1)|null|\n", "2401.05293": "|**2024-01-10**|**Score Distillation Sampling with Learned Manifold Corrective**|\u4f7f\u7528\u5b66\u4e60\u6d41\u5f62\u6821\u6b63\u5bf9\u84b8\u998f\u91c7\u6837\u8fdb\u884c\u8bc4\u5206|Thiemo Alldieck, Nikos Kolotouros, Cristian Sminchisescu|Score Distillation Sampling (SDS) is a recent but already widely popular method that relies on an image diffusion model to control optimization problems using text prompts. In this paper, we conduct an in-depth analysis of the SDS loss function, identify an inherent problem with its formulation, and propose a surprisingly easy but effective fix. Specifically, we decompose the loss into different factors and isolate the component responsible for noisy gradients. In the original formulation, high text guidance is used to account for the noise, leading to unwanted side effects. Instead, we train a shallow network mimicking the timestep-dependent denoising deficiency of the image diffusion model in order to effectively factor it out. We demonstrate the versatility and the effectiveness of our novel loss formulation through several qualitative and quantitative experiments, including optimization-based image synthesis and editing, zero-shot image translation network training, and text-to-3D synthesis.|\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u662f\u4e00\u79cd\u6700\u8fd1\u4f46\u5df2\u7ecf\u5e7f\u6cdb\u6d41\u884c\u7684\u65b9\u6cd5\uff0c\u5b83\u4f9d\u9760\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f7f\u7528\u6587\u672c\u63d0\u793a\u6765\u63a7\u5236\u4f18\u5316\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9 SDS \u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u786e\u5b9a\u4e86\u5176\u516c\u5f0f\u7684\u56fa\u6709\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u4f46\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u635f\u5931\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u56e0\u7d20\uff0c\u5e76\u9694\u79bb\u5bfc\u81f4\u566a\u58f0\u68af\u5ea6\u7684\u6210\u5206\u3002\u5728\u6700\u521d\u7684\u914d\u65b9\u4e2d\uff0c\u4f7f\u7528\u9ad8\u6587\u672c\u6307\u5bfc\u6765\u89e3\u51b3\u566a\u97f3\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u526f\u4f5c\u7528\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u6d45\u5c42\u7f51\u7edc\u6765\u6a21\u4eff\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u53bb\u566a\u7f3a\u9677\uff0c\u4ee5\u4fbf\u6709\u6548\u5730\u5c06\u5176\u5206\u89e3\u51fa\u6765\u3002\u6211\u4eec\u901a\u8fc7\u51e0\u4e2a\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b0\u9896\u7684\u635f\u5931\u516c\u5f0f\u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u57fa\u4e8e\u4f18\u5316\u7684\u56fe\u50cf\u5408\u6210\u548c\u7f16\u8f91\u3001\u96f6\u6837\u672c\u56fe\u50cf\u7ffb\u8bd1\u7f51\u7edc\u8bad\u7ec3\u4ee5\u53ca\u6587\u672c\u5230 3D \u5408\u6210\u3002|[2401.05293v1](http://arxiv.org/pdf/2401.05293v1)|null|\n", "2401.05168": "|**2024-01-10**|**CLIP-guided Source-free Object Detection in Aerial Images**|CLIP \u5f15\u5bfc\u7684\u822a\u7a7a\u56fe\u50cf\u4e2d\u7684\u65e0\u6e90\u7269\u4f53\u68c0\u6d4b|Nanqing Liu, Xun Xu, Yongyi Su, Chengxin Liu, Peiliang Gong, Heng-Chao Li|Domain adaptation is crucial in aerial imagery, as the visual representation of these images can significantly vary based on factors such as geographic location, time, and weather conditions. Additionally, high-resolution aerial images often require substantial storage space and may not be readily accessible to the public. To address these challenges, we propose a novel Source-Free Object Detection (SFOD) method. Specifically, our approach is built upon a self-training framework; however, self-training can lead to inaccurate learning in the absence of labeled training data. To address this issue, we further integrate Contrastive Language-Image Pre-training (CLIP) to guide the generation of pseudo-labels, termed CLIP-guided Aggregation. By leveraging CLIP's zero-shot classification capability, we use it to aggregate scores with the original predicted bounding boxes, enabling us to obtain refined scores for the pseudo-labels. To validate the effectiveness of our method, we constructed two new datasets from different domains based on the DIOR dataset, named DIOR-C and DIOR-Cloudy. Experiments demonstrate that our method outperforms other comparative algorithms.|\u57df\u9002\u5e94\u5728\u822a\u7a7a\u56fe\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u56fe\u50cf\u7684\u89c6\u89c9\u8868\u793a\u53ef\u80fd\u4f1a\u6839\u636e\u5730\u7406\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u5929\u6c14\u6761\u4ef6\u7b49\u56e0\u7d20\u800c\u53d1\u751f\u663e\u7740\u53d8\u5316\u3002\u6b64\u5916\uff0c\u9ad8\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u901a\u5e38\u9700\u8981\u5927\u91cf\u5b58\u50a8\u7a7a\u95f4\uff0c\u5e76\u4e14\u516c\u4f17\u53ef\u80fd\u65e0\u6cd5\u8f7b\u677e\u8bbf\u95ee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u6e90\u5bf9\u8c61\u68c0\u6d4b\uff08SFOD\uff09\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u5efa\u7acb\u5728\u81ea\u6211\u8bad\u7ec3\u6846\u67b6\u4e4b\u4e0a\u7684\uff1b\u7136\u800c\uff0c\u5728\u6ca1\u6709\u6807\u8bb0\u7684\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u6211\u8bad\u7ec3\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u51c6\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u96c6\u6210\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u6765\u6307\u5bfc\u4f2a\u6807\u7b7e\u7684\u751f\u6210\uff0c\u79f0\u4e3a CLIP \u5f15\u5bfc\u805a\u5408\u3002\u901a\u8fc7\u5229\u7528 CLIP \u7684\u96f6\u6837\u672c\u5206\u7c7b\u529f\u80fd\uff0c\u6211\u4eec\u7528\u5b83\u6765\u805a\u5408\u539f\u59cb\u9884\u6d4b\u8fb9\u754c\u6846\u7684\u5206\u6570\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u83b7\u5f97\u4f2a\u6807\u7b7e\u7684\u7cbe\u786e\u5206\u6570\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u57fa\u4e8e DIOR \u6570\u636e\u96c6\u6784\u5efa\u4e86\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u5206\u522b\u547d\u540d\u4e3a DIOR-C \u548c DIOR-Cloudy\u3002\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6bd4\u8f83\u7b97\u6cd5\u3002|[2401.05168v1](http://arxiv.org/pdf/2401.05168v1)|null|\n", "2401.05159": "|**2024-01-10**|**Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN**|Derm-T2IM\uff1a\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u5229\u7528\u5408\u6210\u76ae\u80a4\u75c5\u53d8\u6570\u636e\uff0c\u4f7f\u7528 ViT \u548c CNN \u589e\u5f3a\u76ae\u80a4\u75be\u75c5\u5206\u7c7b|Muhammad Ali Farooq, Wang Yao, Michael Schukat, Mark A Little, Peter Corcoran|This study explores the utilization of Dermatoscopic synthetic data generated through stable diffusion models as a strategy for enhancing the robustness of machine learning model training. Synthetic data generation plays a pivotal role in mitigating challenges associated with limited labeled datasets, thereby facilitating more effective model training. In this context, we aim to incorporate enhanced data transformation techniques by extending the recent success of few-shot learning and a small amount of data representation in text-to-image latent diffusion models. The optimally tuned model is further used for rendering high-quality skin lesion synthetic data with diverse and realistic characteristics, providing a valuable supplement and diversity to the existing training data. We investigate the impact of incorporating newly generated synthetic data into the training pipeline of state-of-art machine learning models, assessing its effectiveness in enhancing model performance and generalization to unseen real-world data. Our experimental results demonstrate the efficacy of the synthetic data generated through stable diffusion models helps in improving the robustness and adaptability of end-to-end CNN and vision transformer models on two different real-world skin lesion datasets.|\u672c\u7814\u7a76\u63a2\u7d22\u5229\u7528\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u76ae\u80a4\u955c\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7a33\u5065\u6027\u7684\u7b56\u7565\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u5728\u7f13\u89e3\u4e0e\u6709\u9650\u6807\u8bb0\u6570\u636e\u96c6\u76f8\u5173\u7684\u6311\u6218\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u5728\u8fd9\u79cd\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u6269\u5c55\u6700\u8fd1\u6210\u529f\u7684\u5c11\u6837\u672c\u5b66\u4e60\u548c\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5c11\u91cf\u6570\u636e\u8868\u793a\u6765\u6574\u5408\u589e\u5f3a\u7684\u6570\u636e\u8f6c\u6362\u6280\u672f\u3002\u7ecf\u8fc7\u4f18\u5316\u8c03\u6574\u7684\u6a21\u578b\u8fdb\u4e00\u6b65\u7528\u4e8e\u6e32\u67d3\u5177\u6709\u591a\u6837\u5316\u548c\u771f\u5b9e\u7279\u5f81\u7684\u9ad8\u8d28\u91cf\u76ae\u80a4\u75c5\u53d8\u5408\u6210\u6570\u636e\uff0c\u4e3a\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u548c\u591a\u6837\u6027\u3002\u6211\u4eec\u7814\u7a76\u4e86\u5c06\u65b0\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u7eb3\u5165\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u5176\u5728\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u548c\u5bf9\u672a\u89c1\u8fc7\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6cdb\u5316\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u7684\u6709\u6548\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u7aef\u5230\u7aef CNN \u548c\u89c6\u89c9\u53d8\u6362\u5668\u6a21\u578b\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002|[2401.05159v1](http://arxiv.org/pdf/2401.05159v1)|null|\n", "2401.05153": "|**2024-01-10**|**CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model**|CrossDiff\uff1a\u901a\u8fc7\u4ea4\u53c9\u9884\u6d4b\u6269\u6563\u6a21\u578b\u63a2\u7d22\u5168\u8272\u9510\u5316\u7684\u81ea\u76d1\u7763\u8868\u793a|Yinghui Xing, Litao Qu, ShiZhou Zhang, Xiuwei Zhang, Yanning Zhang|Fusion of a panchromatic (PAN) image and corresponding multispectral (MS) image is also known as pansharpening, which aims to combine abundant spatial details of PAN and spectral information of MS. Due to the absence of high-resolution MS images, available deep-learning-based methods usually follow the paradigm of training at reduced resolution and testing at both reduced and full resolution. When taking original MS and PAN images as inputs, they always obtain sub-optimal results due to the scale variation. In this paper, we propose to explore the self-supervised representation of pansharpening by designing a cross-predictive diffusion model, named CrossDiff. It has two-stage training. In the first stage, we introduce a cross-predictive pretext task to pre-train the UNet structure based on conditional DDPM, while in the second stage, the encoders of the UNets are frozen to directly extract spatial and spectral features from PAN and MS, and only the fusion head is trained to adapt for pansharpening task. Extensive experiments show the effectiveness and superiority of the proposed model compared with state-of-the-art supervised and unsupervised methods. Besides, the cross-sensor experiments also verify the generalization ability of proposed self-supervised representation learners for other satellite's datasets. We will release our code for reproducibility.|\u5168\u8272\uff08PAN\uff09\u56fe\u50cf\u548c\u76f8\u5e94\u7684\u591a\u5149\u8c31\uff08MS\uff09\u56fe\u50cf\u7684\u878d\u5408\u4e5f\u79f0\u4e3a\u5168\u8272\u9510\u5316\uff0c\u5176\u76ee\u7684\u662f\u5c06PAN\u7684\u4e30\u5bcc\u7a7a\u95f4\u7ec6\u8282\u548cMS\u7684\u5149\u8c31\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\u3002\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387 MS \u56fe\u50cf\uff0c\u53ef\u7528\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u5e38\u9075\u5faa\u964d\u4f4e\u5206\u8fa8\u7387\u8bad\u7ec3\u4ee5\u53ca\u964d\u4f4e\u5206\u8fa8\u7387\u548c\u5168\u5206\u8fa8\u7387\u6d4b\u8bd5\u7684\u8303\u5f0f\u3002\u5f53\u4ee5\u539f\u59cb MS \u548c PAN \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u65f6\uff0c\u7531\u4e8e\u5c3a\u5ea6\u53d8\u5316\uff0c\u5b83\u4eec\u603b\u662f\u83b7\u5f97\u6b21\u4f18\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u540d\u4e3a CrossDiff \u7684\u4ea4\u53c9\u9884\u6d4b\u6269\u6563\u6a21\u578b\u6765\u63a2\u7d22\u5168\u8272\u9510\u5316\u7684\u81ea\u76d1\u7763\u8868\u793a\u3002\u5b83\u6709\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u5f15\u5165\u4ea4\u53c9\u9884\u6d4b\u501f\u53e3\u4efb\u52a1\u6765\u57fa\u4e8e\u6761\u4ef6DDPM\u9884\u8bad\u7ec3UNet\u7ed3\u6784\uff0c\u800c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0cUNet\u7684\u7f16\u7801\u5668\u88ab\u51bb\u7ed3\u4ee5\u76f4\u63a5\u4ecePAN\u548cMS\u4e2d\u63d0\u53d6\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u4e14\u53ea\u6709\u878d\u5408\u5934\u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u9002\u5e94\u5168\u8272\u9510\u5316\u4efb\u52a1\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u8de8\u4f20\u611f\u5668\u5b9e\u9a8c\u8fd8\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5668\u5bf9\u5176\u4ed6\u536b\u661f\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\u4ee5\u5b9e\u73b0\u53ef\u91cd\u590d\u6027\u3002|[2401.05153v1](http://arxiv.org/pdf/2401.05153v1)|null|\n", "2401.05093": "|**2024-01-10**|**SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image**|SwiMDiff\uff1a\u9065\u611f\u56fe\u50cf\u5177\u6709\u6269\u6563\u7ea6\u675f\u7684\u5168\u573a\u666f\u5339\u914d\u5bf9\u6bd4\u5b66\u4e60|Jiayuan Tian, Jie Lei, Jiaqing Zhang, Weiying Xie, Yunsong Li|With recent advancements in aerospace technology, the volume of unlabeled remote sensing image (RSI) data has increased dramatically. Effectively leveraging this data through self-supervised learning (SSL) is vital in the field of remote sensing. However, current methodologies, particularly contrastive learning (CL), a leading SSL method, encounter specific challenges in this domain. Firstly, CL often mistakenly identifies geographically adjacent samples with similar semantic content as negative pairs, leading to confusion during model training. Secondly, as an instance-level discriminative task, it tends to neglect the essential fine-grained features and complex details inherent in unstructured RSIs. To overcome these obstacles, we introduce SwiMDiff, a novel self-supervised pre-training framework designed for RSIs. SwiMDiff employs a scene-wide matching approach that effectively recalibrates labels to recognize data from the same scene as false negatives. This adjustment makes CL more applicable to the nuances of remote sensing. Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through the implementation of pixel-level diffusion constraints, we enhance the encoder's ability to capture both the global semantic information and the fine-grained features of the images more comprehensively. Our proposed framework significantly enriches the information available for downstream tasks in remote sensing. Demonstrating exceptional performance in change detection and land-cover classification tasks, SwiMDiff proves its substantial utility and value in the field of remote sensing.|\u968f\u7740\u822a\u7a7a\u822a\u5929\u6280\u672f\u7684\u6700\u65b0\u8fdb\u6b65\uff0c\u672a\u6807\u8bb0\u7684\u9065\u611f\u56fe\u50cf\uff08RSI\uff09\u6570\u636e\u91cf\u6025\u5267\u589e\u52a0\u3002\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u5728\u9065\u611f\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\uff08\u4e00\u79cd\u9886\u5148\u7684 SSL \u65b9\u6cd5\uff09\uff0c\u5728\u8be5\u9886\u57df\u9047\u5230\u4e86\u7279\u5b9a\u7684\u6311\u6218\u3002\u9996\u5148\uff0cCL\u7ecf\u5e38\u9519\u8bef\u5730\u5c06\u5177\u6709\u76f8\u4f3c\u8bed\u4e49\u5185\u5bb9\u7684\u5730\u7406\u4e0a\u76f8\u90bb\u7684\u6837\u672c\u8bc6\u522b\u4e3a\u8d1f\u5bf9\uff0c\u5bfc\u81f4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6df7\u4e71\u3002\u5176\u6b21\uff0c\u4f5c\u4e3a\u5b9e\u4f8b\u7ea7\u5224\u522b\u4efb\u52a1\uff0c\u5b83\u5f80\u5f80\u5ffd\u7565\u975e\u7ed3\u6784\u5316 RSI \u56fa\u6709\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u548c\u590d\u6742\u7ec6\u8282\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SwiMDiff\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a RSI \u8bbe\u8ba1\u7684\u65b0\u578b\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\u3002 SwiMDiff \u91c7\u7528\u573a\u666f\u8303\u56f4\u5339\u914d\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u91cd\u65b0\u6821\u51c6\u6807\u7b7e\uff0c\u4ee5\u5c06\u6765\u81ea\u540c\u4e00\u573a\u666f\u7684\u6570\u636e\u8bc6\u522b\u4e3a\u6f0f\u62a5\u3002\u8fd9\u4e00\u8c03\u6574\u4f7f CL \u66f4\u9002\u7528\u4e8e\u9065\u611f\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u6b64\u5916\uff0cSwiMDiff \u5c06 CL \u4e0e\u6269\u6563\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002\u901a\u8fc7\u5b9e\u65bd\u50cf\u7d20\u7ea7\u6269\u6563\u7ea6\u675f\uff0c\u6211\u4eec\u589e\u5f3a\u4e86\u7f16\u7801\u5668\u66f4\u5168\u9762\u5730\u6355\u83b7\u56fe\u50cf\u7684\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u663e\u7740\u4e30\u5bcc\u4e86\u9065\u611f\u4e0b\u6e38\u4efb\u52a1\u53ef\u7528\u7684\u4fe1\u606f\u3002 SwiMDiff \u5728\u53d8\u5316\u68c0\u6d4b\u548c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9065\u611f\u9886\u57df\u7684\u5de8\u5927\u5b9e\u7528\u6027\u548c\u4ef7\u503c\u3002|[2401.05093v1](http://arxiv.org/pdf/2401.05093v1)|null|\n", "2401.05010": "|**2024-01-10**|**Less is More : A Closer Look at Multi-Modal Few-Shot Learning**|\u5c11\u5373\u662f\u591a\uff1a\u4ed4\u7ec6\u89c2\u5bdf\u591a\u6a21\u6001\u5c11\u6837\u672c\u5b66\u4e60|Chunpeng Zhou, Haishuai Wang, Xilu Yuan, Zhi Yu, Jiajun Bu|Few-shot Learning aims to learn and distinguish new categories with a very limited number of available images, presenting a significant challenge in the realm of deep learning. Recent researchers have sought to leverage the additional textual or linguistic information of these rare categories with a pre-trained language model to facilitate learning, thus partially alleviating the problem of insufficient supervision signals. However, the full potential of the textual information and pre-trained language model have been underestimated in the few-shot learning till now, resulting in limited performance enhancements. To address this, we propose a simple but effective framework for few-shot learning tasks, specifically designed to exploit the textual information and language model. In more detail, we explicitly exploit the zero-shot capability of the pre-trained language model with the learnable prompt. And we just add the visual feature with the textual feature for inference directly without the intricate designed fusion modules in previous works. Additionally, we apply the self-ensemble and distillation to further enhance these components. Our extensive experiments conducted across four widely used few-shot datasets demonstrate that our simple framework achieves impressive results. Particularly noteworthy is its outstanding performance in the 1-shot learning task, surpassing state-of-the-art methods by an average of 3.0\\% in classification accuracy. \\footnote{We will make the source codes of the proposed framework publicly available upon acceptance. }.|Few-shot Learning \u65e8\u5728\u5229\u7528\u6570\u91cf\u975e\u5e38\u6709\u9650\u7684\u53ef\u7528\u56fe\u50cf\u6765\u5b66\u4e60\u548c\u533a\u5206\u65b0\u7c7b\u522b\uff0c\u8fd9\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4eba\u5458\u8bd5\u56fe\u5229\u7528\u8fd9\u4e9b\u7f55\u89c1\u7c7b\u522b\u7684\u989d\u5916\u6587\u672c\u6216\u8bed\u8a00\u4fe1\u606f\u4e0e\u9884\u5148\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u4fc3\u8fdb\u5b66\u4e60\uff0c\u4ece\u800c\u90e8\u5206\u7f13\u89e3\u76d1\u7763\u4fe1\u53f7\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fc4\u4eca\u4e3a\u6b62\uff0c\u5728\u5c11\u6570\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u6587\u672c\u4fe1\u606f\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5168\u90e8\u6f5c\u529b\u88ab\u4f4e\u4f30\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u6846\u67b6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5229\u7528\u6587\u672c\u4fe1\u606f\u548c\u8bed\u8a00\u6a21\u578b\u3002\u66f4\u8be6\u7ec6\u5730\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u660e\u786e\u5730\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u529f\u80fd\u3002\u6211\u4eec\u53ea\u662f\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u6587\u672c\u7279\u5f81\u76f4\u63a5\u76f8\u52a0\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u9700\u8981\u50cf\u4e4b\u524d\u7684\u4f5c\u54c1\u90a3\u6837\u590d\u6742\u5730\u8bbe\u8ba1\u878d\u5408\u6a21\u5757\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5e94\u7528\u81ea\u96c6\u6210\u548c\u84b8\u998f\u6765\u8fdb\u4e00\u6b65\u589e\u5f3a\u8fd9\u4e9b\u7ec4\u4ef6\u3002\u6211\u4eec\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b80\u5355\u6846\u67b6\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u7279\u522b\u503c\u5f97\u5173\u6ce8\u7684\u662f\u5b83\u5728 1-shot \u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u51fa\u8272\u8868\u73b0\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u5e73\u5747\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 3.0%\u3002 \\footnote{\u6211\u4eec\u5c06\u5728\u63a5\u53d7\u540e\u516c\u5f00\u62df\u8bae\u6846\u67b6\u7684\u6e90\u4ee3\u7801\u3002 }\u3002|[2401.05010v1](http://arxiv.org/pdf/2401.05010v1)|null|\n", "2401.04961": "|**2024-01-10**|**ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic Polyp Detection**|ECC-PolypDet\uff1a\u5177\u6709\u5bf9\u6bd4\u5b66\u4e60\u7684\u589e\u5f3a\u578b CenterNet\uff0c\u7528\u4e8e\u81ea\u52a8\u606f\u8089\u68c0\u6d4b|Yuncheng Jiang, Zixun Zhang, Yiwen Hu, Guanbin Li, Xiang Wan, Song Wu, Shuguang Cui, Silin Huang, Zhen Li|Accurate polyp detection is critical for early colorectal cancer diagnosis. Although remarkable progress has been achieved in recent years, the complex colon environment and concealed polyps with unclear boundaries still pose severe challenges in this area. Existing methods either involve computationally expensive context aggregation or lack prior modeling of polyps, resulting in poor performance in challenging cases. In this paper, we propose the Enhanced CenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \\& end-to-end inference framework that leverages images and bounding box annotations to train a general model and fine-tune it based on the inference score to obtain a final robust model. Specifically, we conduct Box-assisted Contrastive Learning (BCL) during training to minimize the intra-class difference and maximize the inter-class difference between foreground polyps and backgrounds, enabling our model to capture concealed polyps. Moreover, to enhance the recognition of small polyps, we design the Semantic Flow-guided Feature Pyramid Network (SFFPN) to aggregate multi-scale features and the Heatmap Propagation (HP) module to boost the model's attention on polyp targets. In the fine-tuning stage, we introduce the IoU-guided Sample Re-weighting (ISR) mechanism to prioritize hard samples by adaptively adjusting the loss weight for each sample during fine-tuning. Extensive experiments on six large-scale colonoscopy datasets demonstrate the superiority of our model compared with previous state-of-the-art detectors.|\u51c6\u786e\u7684\u606f\u8089\u68c0\u6d4b\u5bf9\u4e8e\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u5c55\uff0c\u4f46\u590d\u6742\u7684\u7ed3\u80a0\u73af\u5883\u548c\u8fb9\u754c\u4e0d\u6e05\u6670\u7684\u9690\u533f\u6027\u606f\u8089\u4ecd\u7136\u7ed9\u8be5\u9886\u57df\u5e26\u6765\u4e25\u5cfb\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6d89\u53ca\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u8981\u4e48\u7f3a\u4e4f\u606f\u8089\u7684\u9884\u5148\u5efa\u6a21\uff0c\u5bfc\u81f4\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5177\u6709\u5bf9\u6bd4\u5b66\u4e60\u7684\u589e\u5f3a\u578b CenterNet\uff08ECC-PolypDet\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u7aef\u5230\u7aef\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u548c\u8fb9\u754c\u6846\u6ce8\u91ca\u6765\u8bad\u7ec3\u901a\u7528\u6a21\u578b\u5e76\u57fa\u4e8e\u7684\u63a8\u7406\u5206\u6570\u4ee5\u83b7\u5f97\u6700\u7ec8\u7684\u7a33\u5065\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u8fdb\u884c\u6846\u8f85\u52a9\u5bf9\u6bd4\u5b66\u4e60\uff08BCL\uff09\uff0c\u4ee5\u6700\u5c0f\u5316\u7c7b\u5185\u5dee\u5f02\u5e76\u6700\u5927\u5316\u524d\u666f\u606f\u8089\u548c\u80cc\u666f\u4e4b\u95f4\u7684\u7c7b\u95f4\u5dee\u5f02\uff0c\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u6355\u83b7\u9690\u85cf\u7684\u606f\u8089\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u589e\u5f3a\u5bf9\u5c0f\u606f\u8089\u7684\u8bc6\u522b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8bed\u4e49\u6d41\u5f15\u5bfc\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08SFFPN\uff09\u6765\u805a\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u4e86\u70ed\u56fe\u4f20\u64ad\uff08HP\uff09\u6a21\u5757\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u606f\u8089\u76ee\u6807\u7684\u5173\u6ce8\u3002\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6211\u4eec\u5f15\u5165\u4e86 IoU \u5f15\u5bfc\u7684\u6837\u672c\u91cd\u65b0\u52a0\u6743\uff08ISR\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u6743\u91cd\u6765\u4f18\u5148\u8003\u8651\u786c\u6837\u672c\u3002\u5bf9\u516d\u4e2a\u5927\u89c4\u6a21\u7ed3\u80a0\u955c\u68c0\u67e5\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u63a2\u6d4b\u5668\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u3002|[2401.04961v1](http://arxiv.org/pdf/2401.04961v1)|null|\n", "2401.04942": "|**2024-01-10**|**Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics**|\u89c6\u9891\u4e2d\u7684\u5ef6\u8fdf\u611f\u77e5\u9053\u8def\u5f02\u5e38\u5206\u5272\uff1a\u771f\u5b9e\u611f\u6570\u636e\u96c6\u548c\u65b0\u6307\u6807|Beiwen Tian, Huan-ang Gao, Leiyao Cui, Yupeng Zheng, Lan Luo, Baofeng Wang, Rong Zhi, Guyue Zhou, Hao Zhao|In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.|\u5728\u8fc7\u53bb\u7684\u51e0\u5e74\u91cc\uff0c\u9053\u8def\u5f02\u5e38\u5206\u5272\u5728\u5b66\u672f\u754c\u5f97\u5230\u4e86\u79ef\u6781\u7684\u63a2\u7d22\uff0c\u5e76\u8d8a\u6765\u8d8a\u53d7\u5230\u4e1a\u754c\u7684\u5173\u6ce8\u3002\u80cc\u540e\u7684\u539f\u7406\u5f88\u7b80\u5355\uff1a\u5982\u679c\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u80fd\u591f\u5728\u649e\u5230\u5f02\u5e38\u7269\u4f53\u4e4b\u524d\u5239\u8f66\uff0c\u90a3\u4e48\u5b89\u5168\u6027\u5c31\u4f1a\u5f97\u5230\u63d0\u5347\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u57fa\u672c\u539f\u7406\u81ea\u7136\u9700\u8981\u4e00\u4e2a\u4e34\u65f6\u901a\u77e5\u7684\u8bbe\u7f6e\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u662f\u4ee5\u4e0d\u5207\u5b9e\u9645\u7684\u6846\u67b6\u65b9\u5f0f\u8bbe\u8ba1\u7684\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u8d21\u732e\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u89c6\u9891\u5f02\u5e38\u5206\u5272\u6570\u636e\u96c6\u3002\u7531\u4e8e\u5c06\u5404\u79cd\u5f02\u5e38\u7269\u4f53\u653e\u7f6e\u5728\u7e41\u5fd9\u7684\u9053\u8def\u4e0a\u5e76\u5728\u6bcf\u4e00\u5e27\u4e2d\u5bf9\u5176\u8fdb\u884c\u6ce8\u91ca\u65e2\u5371\u9669\u53c8\u6602\u8d35\uff0c\u56e0\u6b64\u6211\u4eec\u6c42\u52a9\u4e8e\u5408\u6210\u6570\u636e\u3002\u4e3a\u4e86\u63d0\u9ad8\u8be5\u5408\u6210\u6570\u636e\u96c6\u4e0e\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u76f8\u5173\u6027\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4ee5\u6e32\u67d3 G \u7f13\u51b2\u533a\u4e3a\u6761\u4ef6\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u4ee5\u589e\u5f3a\u7167\u7247\u771f\u5b9e\u611f\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u7531 7 \u4e2a\u4e0d\u540c\u57ce\u9547\u8bb0\u5f55\u7684 120,000 \u4e2a 60 FPS \u5e27\u901f\u7387\u7684\u9ad8\u5206\u8fa8\u7387\u5e27\u7ec4\u6210\u3002\u4f5c\u4e3a\u521d\u59cb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u4f7f\u7528\u6700\u65b0\u7684\u76d1\u7763\u548c\u65e0\u76d1\u7763\u9053\u8def\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u63d0\u4f9b\u57fa\u7ebf\u3002\u9664\u4e86\u4f20\u7edf\u6307\u6807\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5173\u6ce8\u4e24\u4e2a\u65b0\u6307\u6807\uff1a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5ef6\u8fdf\u611f\u77e5\u6d41\u51c6\u786e\u6027\u3002\u6211\u4eec\u8ba4\u4e3a\u540e\u8005\u5f88\u6709\u4ef7\u503c\uff0c\u56e0\u4e3a\u5b83\u8861\u91cf\u5f02\u5e38\u5206\u5272\u7b97\u6cd5\u662f\u5426\u80fd\u591f\u771f\u6b63\u9632\u6b62\u6c7d\u8f66\u5728\u4e34\u65f6\u4fe1\u606f\u73af\u5883\u4e2d\u53d1\u751f\u78b0\u649e\u3002|[2401.04942v1](http://arxiv.org/pdf/2401.04942v1)|null|\n", "2401.04860": "|**2024-01-10**|**Modality-Aware Representation Learning for Zero-shot Sketch-based Image Retrieval**|\u7528\u4e8e\u57fa\u4e8e\u96f6\u6837\u672c\u8349\u56fe\u7684\u56fe\u50cf\u68c0\u7d22\u7684\u6a21\u6001\u611f\u77e5\u8868\u793a\u5b66\u4e60|Eunyi Lyou, Doyeon Lee, Jooeun Kim, Joonseok Lee|Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings.|\u96f6\u6837\u672c\u5b66\u4e60\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u770b\u4e0d\u89c1\u7684\u7c7b\u522b\uff0c\u907f\u514d\u8be6\u5c3d\u7684\u6570\u636e\u6536\u96c6\u3002\u57fa\u4e8e\u96f6\u6837\u672c\u8349\u56fe\u7684\u56fe\u50cf\u68c0\u7d22 (ZS-SBIR) \u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\uff0c\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u6536\u96c6\u914d\u5bf9\u7684\u8349\u56fe\u7167\u7247\u6837\u672c\u65e2\u56f0\u96be\u53c8\u6602\u8d35\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5bf9\u6bd4\u8349\u56fe\u548c\u7167\u7247\u6765\u95f4\u63a5\u5bf9\u9f50\u8349\u56fe\u548c\u7167\u7247\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u8bbf\u95ee\u8349\u56fe-\u7167\u7247\u5bf9\u7684\u5fc5\u8981\u6027\u3002\u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u663e\u5f0f\u6a21\u6001\u7f16\u7801\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6a21\u6001\u4e0d\u53ef\u77e5\u7684\u8bed\u4e49\u4e0e\u6a21\u6001\u7279\u5b9a\u7684\u4fe1\u606f\u5206\u5f00\uff0c\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\u5e76\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u5185\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u6a21\u6001\u5185\u5bb9\u68c0\u7d22\u3002\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u5728 ZS-SBIR \u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5b83\u4e5f\u53ef\u4ee5\u5e94\u7528\u4e8e\u5e7f\u4e49\u548c\u7ec6\u7c92\u5ea6\u8bbe\u7f6e\u3002|[2401.04860v1](http://arxiv.org/pdf/2401.04860v1)|null|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.05336": "|**2024-01-10**|**Towards Online Sign Language Recognition and Translation**|\u8d70\u5411\u5728\u7ebf\u624b\u8bed\u8bc6\u522b\u548c\u7ffb\u8bd1|Ronglai Zuo, Fangyun Wei, Brian Mak|The objective of sign language recognition is to bridge the communication gap between the deaf and the hearing. Numerous previous works train their models using the well-established connectionist temporal classification (CTC) loss. During the inference stage, the CTC-based models typically take the entire sign video as input to make predictions. This type of inference scheme is referred to as offline recognition. In contrast, while mature speech recognition systems can efficiently recognize spoken words on the fly, sign language recognition still falls short due to the lack of practical online solutions. In this work, we take the first step towards filling this gap. Our approach comprises three phases: 1) developing a sign language dictionary encompassing all glosses present in a target sign language dataset; 2) training an isolated sign language recognition model on augmented signs using both conventional classification loss and our novel saliency loss; 3) employing a sliding window approach on the input sign sequence and feeding each sign clip to the well-optimized model for online recognition. Furthermore, our online recognition model can be extended to boost the performance of any offline model, and to support online translation by appending a gloss-to-text network onto the recognition model. By integrating our online framework with the previously best-performing offline model, TwoStream-SLR, we achieve new state-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T, and CSL-Daily. Code and models will be available at https://github.com/FangyunWei/SLRT|\u624b\u8bed\u8bc6\u522b\u7684\u76ee\u6807\u662f\u5f25\u5408\u804b\u54d1\u4eba\u548c\u542c\u529b\u6b63\u5e38\u8005\u4e4b\u95f4\u7684\u6c9f\u901a\u5dee\u8ddd\u3002\u4e4b\u524d\u7684\u8bb8\u591a\u5de5\u4f5c\u90fd\u4f7f\u7528\u5b8c\u5584\u7684\u8054\u7ed3\u4e3b\u4e49\u65f6\u95f4\u5206\u7c7b\uff08CTC\uff09\u635f\u5931\u6765\u8bad\u7ec3\u4ed6\u4eec\u7684\u6a21\u578b\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u57fa\u4e8e CTC \u7684\u6a21\u578b\u901a\u5e38\u5c06\u6574\u4e2a\u6807\u5fd7\u89c6\u9891\u4f5c\u4e3a\u8f93\u5165\u6765\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u63a8\u7406\u65b9\u6848\u79f0\u4e3a\u79bb\u7ebf\u8bc6\u522b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u867d\u7136\u6210\u719f\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u53ef\u4ee5\u6709\u6548\u5730\u5373\u65f6\u8bc6\u522b\u53e3\u8bed\u5355\u8bcd\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5b9e\u7528\u7684\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u624b\u8bed\u8bc6\u522b\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8fc8\u51fa\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u7684\u7b2c\u4e00\u6b65\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u5f00\u53d1\u5305\u542b\u76ee\u6807\u624b\u8bed\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u6240\u6709\u6ce8\u91ca\u7684\u624b\u8bed\u8bcd\u5178\uff1b 2\uff09\u4f7f\u7528\u4f20\u7edf\u7684\u5206\u7c7b\u635f\u5931\u548c\u6211\u4eec\u65b0\u9896\u7684\u663e\u7740\u6027\u635f\u5931\u6765\u8bad\u7ec3\u589e\u5f3a\u7b26\u53f7\u7684\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6a21\u578b\uff1b 3\uff09\u5bf9\u8f93\u5165\u7b26\u53f7\u5e8f\u5217\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\uff0c\u5e76\u5c06\u6bcf\u4e2a\u7b26\u53f7\u7247\u6bb5\u8f93\u5165\u5230\u4f18\u5316\u826f\u597d\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u5728\u7ebf\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5728\u7ebf\u8bc6\u522b\u6a21\u578b\u53ef\u4ee5\u6269\u5c55\u4ee5\u63d0\u9ad8\u4efb\u4f55\u79bb\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5728\u8bc6\u522b\u6a21\u578b\u4e0a\u9644\u52a0\u6ce8\u91ca\u5230\u6587\u672c\u7f51\u7edc\u6765\u652f\u6301\u5728\u7ebf\u7ffb\u8bd1\u3002\u901a\u8fc7\u5c06\u6211\u4eec\u7684\u5728\u7ebf\u6846\u67b6\u4e0e\u4e4b\u524d\u6027\u80fd\u6700\u4f73\u7684\u79bb\u7ebf\u6a21\u578b TwoStream-SLR \u96c6\u6210\uff0c\u6211\u4eec\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1aPhoenix-2014\u3001Phoenix-2014T \u548c CSL-Daily\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/FangyunWei/SLRT \u83b7\u53d6|[2401.05336v1](http://arxiv.org/pdf/2401.05336v1)|null|\n", "2401.05314": "|**2024-01-10**|**ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video**|ANIM-400K\uff1a\u7528\u4e8e\u89c6\u9891\u81ea\u52a8\u7aef\u5230\u7aef\u914d\u97f3\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6|Kevin Cai, Chonghua Liu, David M. Chan|The Internet's wealth of content, with up to 60% published in English, starkly contrasts the global population, where only 18.8% are English speakers, and just 5.1% consider it their native language, leading to disparities in online information access. Unfortunately, automated processes for dubbing of video - replacing the audio track of a video with a translated alternative - remains a complex and challenging task due to pipelines, necessitating precise timing, facial movement synchronization, and prosody matching. While end-to-end dubbing offers a solution, data scarcity continues to impede the progress of both end-to-end and pipeline-based methods. In this work, we introduce Anim-400K, a comprehensive dataset of over 425K aligned animated video segments in Japanese and English supporting various video-related tasks, including automated dubbing, simultaneous translation, guided video summarization, and genre/theme/style classification. Our dataset is made publicly available for research purposes at https://github.com/davidmchan/Anim400K.|\u4e92\u8054\u7f51\u5185\u5bb9\u4e30\u5bcc\uff0c\u5176\u4e2d\u9ad8\u8fbe 60% \u7684\u5185\u5bb9\u4ee5\u82f1\u8bed\u53d1\u5e03\uff0c\u8fd9\u4e0e\u5168\u7403\u4eba\u53e3\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u5168\u7403\u4eba\u53e3\u4e2d\u53ea\u6709 18.8% \u7684\u4eba\u8bb2\u82f1\u8bed\uff0c\u53ea\u6709 5.1% \u7684\u4eba\u8ba4\u4e3a\u82f1\u8bed\u662f\u81ea\u5df1\u7684\u6bcd\u8bed\uff0c\u5bfc\u81f4\u5728\u7ebf\u4fe1\u606f\u83b7\u53d6\u7684\u5dee\u5f02\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u7531\u4e8e\u7ba1\u9053\u7684\u539f\u56e0\uff0c\u89c6\u9891\u914d\u97f3\u7684\u81ea\u52a8\u5316\u8fc7\u7a0b\uff08\u7528\u7ffb\u8bd1\u540e\u7684\u66ff\u4ee3\u65b9\u6848\u66ff\u6362\u89c6\u9891\u7684\u97f3\u8f68\uff09\u4ecd\u7136\u662f\u4e00\u9879\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u7cbe\u786e\u7684\u8ba1\u65f6\u3001\u9762\u90e8\u8fd0\u52a8\u540c\u6b65\u548c\u97f5\u5f8b\u5339\u914d\u3002\u867d\u7136\u7aef\u5230\u7aef\u914d\u97f3\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u4ecd\u7136\u963b\u788d\u7740\u7aef\u5230\u7aef\u548c\u57fa\u4e8e\u7ba1\u9053\u7684\u65b9\u6cd5\u7684\u8fdb\u5c55\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Anim-400K\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7 425K \u5bf9\u9f50\u7684\u65e5\u8bed\u548c\u82f1\u8bed\u52a8\u753b\u89c6\u9891\u7247\u6bb5\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u652f\u6301\u5404\u79cd\u89c6\u9891\u76f8\u5173\u4efb\u52a1\uff0c\u5305\u62ec\u81ea\u52a8\u914d\u97f3\u3001\u540c\u58f0\u7ffb\u8bd1\u3001\u5f15\u5bfc\u89c6\u9891\u6458\u8981\u548c\u6d41\u6d3e/\u4e3b\u9898/\u98ce\u683c\u5206\u7c7b\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5df2\u516c\u5f00\u7528\u4e8e\u7814\u7a76\u76ee\u7684\uff0c\u7f51\u5740\u4e3a https://github.com/davidmchan/Anim400K\u3002|[2401.05314v1](http://arxiv.org/pdf/2401.05314v1)|null|\n", "2401.05308": "|**2024-01-10**|**Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks**|\u6218\u7565\u5ba2\u6237\u9009\u62e9\u4ee5\u89e3\u51b3\u652f\u6301 HAPS \u7684 FL \u7f51\u7edc\u4e2d\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u95ee\u9898|Amin Farajzadeh, Animesh Yadav, Halim Yanikomeroglu|The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation in FL training, our approach fosters a more uniform and representative data distribution across the network. Our simulations demonstrate that this targeted client selection methodology significantly reduces the training loss of FL models in HAPS networks, thereby effectively tackling a crucial challenge in implementing large-scale FL systems.|\u5728\u5782\u76f4\u5f02\u6784\u7f51\u7edc\u4e2d\u90e8\u7f72\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\uff0c\u4f8b\u5982\u7531\u9ad8\u7a7a\u5e73\u53f0\u7ad9\uff08HAPS\uff09\u652f\u6301\u7684\u7f51\u7edc\uff0c\u63d0\u4f9b\u4e86\u5438\u5f15\u5e7f\u6cdb\u5ba2\u6237\u7684\u673a\u4f1a\uff0c\u6bcf\u4e2a\u5ba2\u6237\u90fd\u5177\u6709\u72ec\u7279\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u80fd\u529b\u3002\u8fd9\u79cd\u591a\u6837\u6027\u4e0d\u4ec5\u63d0\u9ad8\u4e86 FL \u6a21\u578b\u7684\u8bad\u7ec3\u7cbe\u5ea6\uff0c\u800c\u4e14\u52a0\u901f\u4e86\u5b83\u4eec\u7684\u6536\u655b\u3002\u7136\u800c\uff0c\u5728\u8fd9\u4e9b\u5e7f\u9614\u7684\u7f51\u7edc\u4e2d\u5e94\u7528 FL \u4f1a\u5e26\u6765\u663e\u7740\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e2d\u663e\u7740\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6027\u3002\u8fd9\u79cd\u6570\u636e\u5f02\u6784\u6027\u901a\u5e38\u4f1a\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u53d8\u6162\u5e76\u964d\u4f4e\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u9488\u5bf9\u89e3\u51b3\u6b64\u95ee\u9898\u800c\u5b9a\u5236\u7684\u5ba2\u6237\u9009\u62e9\u7b56\u7565\uff0c\u5229\u7528\u7528\u6237\u7f51\u7edc\u6d41\u91cf\u884c\u4e3a\u3002\u8be5\u7b56\u7565\u6d89\u53ca\u6839\u636e\u5ba2\u6237\u7684\u7f51\u7edc\u4f7f\u7528\u6a21\u5f0f\u5bf9\u5ba2\u6237\u8fdb\u884c\u9884\u6d4b\u548c\u5206\u7c7b\uff0c\u540c\u65f6\u4f18\u5148\u8003\u8651\u7528\u6237\u9690\u79c1\u3002\u901a\u8fc7\u6218\u7565\u6027\u5730\u9009\u62e9\u6570\u636e\u8868\u73b0\u51fa\u76f8\u4f3c\u6a21\u5f0f\u7684\u5ba2\u6237\u6765\u53c2\u4e0e FL \u57f9\u8bad\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6574\u4e2a\u7f51\u7edc\u4e2d\u4fc3\u8fdb\u4e86\u66f4\u52a0\u7edf\u4e00\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u5206\u5e03\u3002\u6211\u4eec\u7684\u6a21\u62df\u8868\u660e\uff0c\u8fd9\u79cd\u6709\u9488\u5bf9\u6027\u7684\u5ba2\u6237\u9009\u62e9\u65b9\u6cd5\u663e\u7740\u51cf\u5c11\u4e86 HAPS \u7f51\u7edc\u4e2d FL \u6a21\u578b\u7684\u8bad\u7ec3\u635f\u5931\uff0c\u4ece\u800c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5b9e\u65bd\u5927\u89c4\u6a21 FL \u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u3002|[2401.05308v1](http://arxiv.org/pdf/2401.05308v1)|null|\n", "2401.05294": "|**2024-01-10**|**Enhanced Muscle and Fat Segmentation for CT-Based Body Composition Analysis: A Comparative Study**|\u57fa\u4e8e CT \u7684\u8eab\u4f53\u6210\u5206\u5206\u6790\u7684\u589e\u5f3a\u808c\u8089\u548c\u8102\u80aa\u5206\u5272\uff1a\u6bd4\u8f83\u7814\u7a76|Benjamin Hou, Tejas Sudharshan Mathai, Jianfei Liu, Christopher Parnell, Ronald M. Summers|Purpose: Body composition measurements from routine abdominal CT can yield personalized risk assessments for asymptomatic and diseased patients. In particular, attenuation and volume measures of muscle and fat are associated with important clinical outcomes, such as cardiovascular events, fractures, and death. This study evaluates the reliability of an Internal tool for the segmentation of muscle and fat (subcutaneous and visceral) as compared to the well-established public TotalSegmentator tool.   Methods: We assessed the tools across 900 CT series from the publicly available SAROS dataset, focusing on muscle, subcutaneous fat, and visceral fat. The Dice score was employed to assess accuracy in subcutaneous fat and muscle segmentation. Due to the lack of ground truth segmentations for visceral fat, Cohen's Kappa was utilized to assess segmentation agreement between the tools.   Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for subcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation respectively. A Wilcoxon signed-rank test revealed that our results were statistically different with p<0.01. For visceral fat, the Cohen's kappa score of 0.856 indicated near-perfect agreement between the two tools. Our internal tool also showed very strong correlations for muscle volume (R^2=0.99), muscle attenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate correlation for subcutaneous fat attenuation (R^2=0.45).   Conclusion: Our findings indicated that our Internal tool outperformed TotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen's Kappa score for visceral fat suggests a reliable level of agreement between the two tools. These results demonstrate the potential of our tool in advancing the accuracy of body composition analysis.|\u76ee\u7684\uff1a\u5e38\u89c4\u8179\u90e8 CT \u7684\u8eab\u4f53\u6210\u5206\u6d4b\u91cf\u53ef\u4ee5\u5bf9\u65e0\u75c7\u72b6\u548c\u60a3\u75c5\u60a3\u8005\u8fdb\u884c\u4e2a\u6027\u5316\u98ce\u9669\u8bc4\u4f30\u3002\u7279\u522b\u662f\uff0c\u808c\u8089\u548c\u8102\u80aa\u7684\u8870\u51cf\u548c\u4f53\u79ef\u6d4b\u91cf\u4e0e\u91cd\u8981\u7684\u4e34\u5e8a\u7ed3\u679c\u76f8\u5173\uff0c\u4f8b\u5982\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u3001\u9aa8\u6298\u548c\u6b7b\u4ea1\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u4e8e\u5206\u5272\u808c\u8089\u548c\u8102\u80aa\uff08\u76ae\u4e0b\u548c\u5185\u810f\uff09\u7684\u5185\u90e8\u5de5\u5177\u4e0e\u6210\u719f\u7684\u516c\u5171 TotalSegmentator \u5de5\u5177\u76f8\u6bd4\u7684\u53ef\u9760\u6027\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u8bc4\u4f30\u4e86\u516c\u5f00\u7684 SAROS \u6570\u636e\u96c6\u4e2d\u7684 900 \u4e2a CT \u7cfb\u5217\u7684\u5de5\u5177\uff0c\u91cd\u70b9\u5173\u6ce8\u808c\u8089\u3001\u76ae\u4e0b\u8102\u80aa\u548c\u5185\u810f\u8102\u80aa\u3002 Dice \u8bc4\u5206\u7528\u4e8e\u8bc4\u4f30\u76ae\u4e0b\u8102\u80aa\u548c\u808c\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u3002\u7531\u4e8e\u7f3a\u4e4f\u5185\u810f\u8102\u80aa\u7684\u771f\u5b9e\u5206\u5272\uff0c\u56e0\u6b64\u5229\u7528 Cohen \u7684 Kappa \u6765\u8bc4\u4f30\u5de5\u5177\u4e4b\u95f4\u7684\u5206\u5272\u4e00\u81f4\u6027\u3002\u7ed3\u679c\uff1a\u6211\u4eec\u7684\u5185\u90e8\u5de5\u5177\u5728\u76ae\u4e0b\u8102\u80aa\u65b9\u9762\u7684 Dice \u63d0\u9ad8\u4e86 3%\uff0883.8 \u6bd4 80.8\uff09\uff0c\u5728\u808c\u8089\u5206\u5272\u65b9\u9762\u63d0\u9ad8\u4e86 5%\uff0887.6 \u6bd4 83.2\uff09\u3002 Wilcoxon \u7b26\u53f7\u79e9\u68c0\u9a8c\u663e\u793a\u6211\u4eec\u7684\u7ed3\u679c\u5177\u6709\u7edf\u8ba1\u5b66\u5dee\u5f02\uff0cp<0.01\u3002\u5bf9\u4e8e\u5185\u810f\u8102\u80aa\uff0cCohen \u7684 kappa \u5f97\u5206\u4e3a 0.856\uff0c\u8868\u660e\u4e24\u79cd\u5de5\u5177\u4e4b\u95f4\u51e0\u4e4e\u5b8c\u7f8e\u4e00\u81f4\u3002\u6211\u4eec\u7684\u5185\u90e8\u5de5\u5177\u8fd8\u663e\u793a\uff0c\u808c\u8089\u4f53\u79ef (R^2=0.99)\u3001\u808c\u8089\u8870\u51cf (R^2=0.93) \u548c\u76ae\u4e0b\u8102\u80aa\u4f53\u79ef (R^2=0.99) \u4e4b\u95f4\u5177\u6709\u975e\u5e38\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u800c\u76ae\u4e0b\u8102\u80aa\u8870\u51cf (R^2=0.99) \u5177\u6709\u4e2d\u7b49\u76f8\u5173\u6027\u3002 ^2=0.45\uff09\u3002\u7ed3\u8bba\uff1a\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5185\u90e8\u5de5\u5177\u5728\u6d4b\u91cf\u76ae\u4e0b\u8102\u80aa\u548c\u808c\u8089\u65b9\u9762\u4f18\u4e8e TotalSegmentator\u3002\u5185\u810f\u8102\u80aa\u7684\u9ad8 Cohen Kappa \u5206\u6570\u8868\u660e\u4e24\u79cd\u5de5\u5177\u4e4b\u95f4\u5177\u6709\u53ef\u9760\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u5de5\u5177\u5728\u63d0\u9ad8\u8eab\u4f53\u6210\u5206\u5206\u6790\u51c6\u786e\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002|[2401.05294v1](http://arxiv.org/pdf/2401.05294v1)|null|\n", "2401.05224": "|**2024-01-10**|**Do Vision and Language Encoders Represent the World Similarly?**|\u89c6\u89c9\u548c\u8bed\u8a00\u7f16\u7801\u5668\u662f\u5426\u540c\u6837\u4ee3\u8868\u4e16\u754c\uff1f|Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Mohamed El Amine Seddik, Karttikeya Mangalam, Noel E. O'Connor|Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demonstrate the effectiveness of this on several downstream tasks including cross-lingual, cross-domain caption matching and image classification.|\u5bf9\u9f50\u7684\u6587\u672c\u56fe\u50cf\u7f16\u7801\u5668\uff08\u4f8b\u5982 CLIP\uff09\u5df2\u6210\u4e3a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u4e8b\u5b9e\u4e0a\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7f16\u7801\u5668\u5728\u5404\u81ea\u7684\u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u8fd9\u5c31\u63d0\u51fa\u4e86\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5355\u6a21\u6001\u89c6\u89c9\u548c\u8bed\u8a00\u7f16\u7801\u5668\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u4e00\u81f4\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ece\u6839\u672c\u4e0a\u4ee3\u8868\u4e86\u76f8\u540c\u7684\u7269\u7406\u4e16\u754c\uff1f\u4f7f\u7528\u4e2d\u5fc3\u6838\u5bf9\u9f50\uff08CKA\uff09\u5206\u6790\u56fe\u50cf\u5b57\u5e55\u57fa\u51c6\u4e0a\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\uff0c\u6211\u4eec\u53d1\u73b0\u672a\u5bf9\u9f50\u548c\u5bf9\u9f50\u7f16\u7801\u5668\u7684\u8868\u793a\u7a7a\u95f4\u5728\u8bed\u4e49\u4e0a\u76f8\u4f3c\u3002\u5728\u50cf CLIP \u8fd9\u6837\u7684\u5bf9\u9f50\u7f16\u7801\u5668\u4e2d\u7f3a\u4e4f\u7edf\u8ba1\u76f8\u4f3c\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8868\u660e\u5728\u6ca1\u6709\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b58\u5728\u672a\u5bf9\u9f50\u7f16\u7801\u5668\u7684\u53ef\u80fd\u5339\u914d\u3002\u6211\u4eec\u5c06\u5176\u6784\u5efa\u4e3a\u5229\u7528\u56fe\u4e4b\u95f4\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u79cd\u5b50\u56fe\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5 - \u5feb\u901f\u4e8c\u6b21\u5206\u914d\u95ee\u9898\u4f18\u5316\u548c\u65b0\u9896\u7684\u57fa\u4e8e\u5c40\u90e8 CKA \u5ea6\u91cf\u7684\u5339\u914d/\u68c0\u7d22\u3002\u6211\u4eec\u5728\u51e0\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u8de8\u8bed\u8a00\u3001\u8de8\u57df\u6807\u9898\u5339\u914d\u548c\u56fe\u50cf\u5206\u7c7b\u3002|[2401.05224v1](http://arxiv.org/pdf/2401.05224v1)|null|\n", "2401.05202": "|**2024-01-10**|**Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits**|\u4f7f\u7528\u59ff\u52bf\u4f30\u8ba1\u548c\u591a\u79cd\u8fd0\u52a8\u7279\u5f81\u8fdb\u884c\u57fa\u4e8e\u89c6\u9891\u7684\u5976\u725b\u81ea\u52a8\u8ddb\u884c\u68c0\u6d4b|Helena Russello, Rik van der Tol, Menno Holzhauer, Eldert J. van Henten, Gert Kootstra|This study presents an automated lameness detection system that uses deep-learning image processing techniques to extract multiple locomotion traits associated with lameness. Using the T-LEAP pose estimation model, the motion of nine keypoints was extracted from videos of walking cows. The videos were recorded outdoors, with varying illumination conditions, and T-LEAP extracted 99.6% of correct keypoints. The trajectories of the keypoints were then used to compute six locomotion traits: back posture measurement, head bobbing, tracking distance, stride length, stance duration, and swing duration. The three most important traits were back posture measurement, head bobbing, and tracking distance. For the ground truth, we showed that a thoughtful merging of the scores of the observers could improve intra-observer reliability and agreement. We showed that including multiple locomotion traits improves the classification accuracy from 76.6% with only one trait to 79.9% with the three most important traits and to 80.1% with all six locomotion traits.|\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8ddb\u884c\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u5904\u7406\u6280\u672f\u6765\u63d0\u53d6\u4e0e\u8ddb\u884c\u76f8\u5173\u7684\u591a\u79cd\u8fd0\u52a8\u7279\u5f81\u3002\u4f7f\u7528T-LEAP\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u4ece\u725b\u884c\u8d70\u7684\u89c6\u9891\u4e2d\u63d0\u53d6\u4e86\u4e5d\u4e2a\u5173\u952e\u70b9\u7684\u8fd0\u52a8\u3002\u89c6\u9891\u662f\u5728\u6237\u5916\u3001\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u5f55\u5236\u7684\uff0cT-LEAP \u63d0\u53d6\u4e86 99.6% \u7684\u6b63\u786e\u5173\u952e\u70b9\u3002\u7136\u540e\u4f7f\u7528\u5173\u952e\u70b9\u7684\u8f68\u8ff9\u6765\u8ba1\u7b97\u516d\u79cd\u8fd0\u52a8\u7279\u5f81\uff1a\u80cc\u90e8\u59ff\u52bf\u6d4b\u91cf\u3001\u5934\u90e8\u6446\u52a8\u3001\u8ddf\u8e2a\u8ddd\u79bb\u3001\u6b65\u5e45\u3001\u7ad9\u7acb\u6301\u7eed\u65f6\u95f4\u548c\u6446\u52a8\u6301\u7eed\u65f6\u95f4\u3002\u4e09\u4e2a\u6700\u91cd\u8981\u7684\u7279\u5f81\u662f\u80cc\u90e8\u59ff\u52bf\u6d4b\u91cf\u3001\u5934\u90e8\u6446\u52a8\u548c\u8ddf\u8e2a\u8ddd\u79bb\u3002\u5bf9\u4e8e\u57fa\u672c\u4e8b\u5b9e\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u89c2\u5bdf\u8005\u5206\u6570\u8fdb\u884c\u6df1\u601d\u719f\u8651\u7684\u5408\u5e76\u53ef\u4ee5\u63d0\u9ad8\u89c2\u5bdf\u8005\u5185\u90e8\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5305\u542b\u591a\u4e2a\u8fd0\u52a8\u7279\u5f81\u53ef\u4ee5\u5c06\u5206\u7c7b\u51c6\u786e\u7387\u4ece\u53ea\u6709\u4e00\u79cd\u7279\u5f81\u7684 76.6% \u63d0\u9ad8\u5230\u4e09\u4e2a\u6700\u91cd\u8981\u7279\u5f81\u7684\u5206\u7c7b\u51c6\u786e\u7387 79.9%\uff0c\u4ee5\u53ca\u6240\u6709\u516d\u79cd\u8fd0\u52a8\u7279\u5f81\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u5230 80.1%\u3002|[2401.05202v1](http://arxiv.org/pdf/2401.05202v1)|null|\n", "2401.05167": "|**2024-01-10**|**Watermark Text Pattern Spotting in Document Images**|\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u6c34\u5370\u6587\u672c\u56fe\u6848\u8bc6\u522b|Mateusz Krubinski, Stefan Matcovici, Diana Grigore, Daniel Voinea, Alin-Ionut Popa|Watermark text spotting in document images can offer access to an often unexplored source of information, providing crucial evidence about a record's scope, audience and sometimes even authenticity. Stemming from the problem of text spotting, detecting and understanding watermarks in documents inherits the same hardships - in the wild, writing can come in various fonts, sizes and forms, making generic recognition a very difficult problem. To address the lack of resources in this field and propel further research, we propose a novel benchmark (K-Watermark) containing 65,447 data samples generated using Wrender, a watermark text patterns rendering procedure. A validity study using humans raters yields an authenticity score of 0.51 against pre-generated watermarked documents. To prove the usefulness of the dataset and rendering technique, we developed an end-to-end solution (Wextract) for detecting the bounding box instances of watermark text, while predicting the depicted text. To deal with this specific task, we introduce a variance minimization loss and a hierarchical self-attention mechanism. To the best of our knowledge, we are the first to propose an evaluation benchmark and a complete solution for retrieving watermarks from documents surpassing baselines by 5 AP points in detection and 4 points in character accuracy.|\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u6c34\u5370\u6587\u672c\u8bc6\u522b\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u7ecf\u5e38\u672a\u63a2\u7d22\u7684\u4fe1\u606f\u6e90\u7684\u8bbf\u95ee\uff0c\u63d0\u4f9b\u6709\u5173\u8bb0\u5f55\u8303\u56f4\u3001\u53d7\u4f17\u751a\u81f3\u6709\u65f6\u751a\u81f3\u771f\u5b9e\u6027\u7684\u91cd\u8981\u8bc1\u636e\u3002\u6e90\u4e8e\u6587\u672c\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u68c0\u6d4b\u548c\u7406\u89e3\u6587\u6863\u4e2d\u7684\u6c34\u5370\u4e5f\u9762\u4e34\u7740\u540c\u6837\u7684\u56f0\u96be\u2014\u2014\u5728\u91ce\u5916\uff0c\u4e66\u5199\u53ef\u80fd\u6709\u5404\u79cd\u5b57\u4f53\u3001\u5927\u5c0f\u548c\u5f62\u5f0f\uff0c\u8fd9\u4f7f\u5f97\u901a\u7528\u8bc6\u522b\u6210\u4e3a\u4e00\u4e2a\u975e\u5e38\u56f0\u96be\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8be5\u9886\u57df\u8d44\u6e90\u7f3a\u4e4f\u7684\u95ee\u9898\u5e76\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\uff08K-Watermark\uff09\uff0c\u5176\u4e2d\u5305\u542b\u4f7f\u7528 Wrender\uff08\u4e00\u79cd\u6c34\u5370\u6587\u672c\u6a21\u5f0f\u6e32\u67d3\u7a0b\u5e8f\uff09\u751f\u6210\u7684 65,447 \u4e2a\u6570\u636e\u6837\u672c\u3002\u4f7f\u7528\u4eba\u7c7b\u8bc4\u5206\u8005\u8fdb\u884c\u7684\u6709\u6548\u6027\u7814\u7a76\u76f8\u5bf9\u4e8e\u9884\u5148\u751f\u6210\u7684\u5e26\u6c34\u5370\u7684\u6587\u6863\u5f97\u51fa\u4e86 0.51 \u7684\u771f\u5b9e\u6027\u5f97\u5206\u3002\u4e3a\u4e86\u8bc1\u660e\u6570\u636e\u96c6\u548c\u6e32\u67d3\u6280\u672f\u7684\u6709\u7528\u6027\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff08Wextract\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u6c34\u5370\u6587\u672c\u7684\u8fb9\u754c\u6846\u5b9e\u4f8b\uff0c\u540c\u65f6\u9884\u6d4b\u6240\u63cf\u7ed8\u7684\u6587\u672c\u3002\u4e3a\u4e86\u5904\u7406\u8fd9\u4e2a\u7279\u5b9a\u4efb\u52a1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b9\u5dee\u6700\u5c0f\u5316\u635f\u5931\u548c\u5206\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u63d0\u51fa\u8bc4\u4f30\u57fa\u51c6\u548c\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4ece\u68c0\u6d4b\u8d85\u8fc7\u57fa\u7ebf 5 \u4e2a AP \u70b9\u548c\u5b57\u7b26\u51c6\u786e\u5ea6\u8d85\u8fc7\u57fa\u7ebf 4 \u70b9\u7684\u6587\u6863\u4e2d\u68c0\u7d22\u6c34\u5370\u3002|[2401.05167v1](http://arxiv.org/pdf/2401.05167v1)|null|\n", "2401.05166": "|**2024-01-10**|**REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge**|REACT 2024\uff1a\u7b2c\u4e8c\u5c4a\u591a\u91cd\u9002\u5f53\u9762\u90e8\u53cd\u5e94\u751f\u6210\u6311\u6218\u8d5b|Siyang Song, Micol Spitale, Cheng Luo, Cristina Palmero, German Barquero, Hengde Zhu, Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, et.al.|In dyadic interactions, humans communicate their intentions and state of mind using verbal and non-verbal cues, where multiple different facial reactions might be appropriate in response to a specific speaker behaviour. Then, how to develop a machine learning (ML) model that can automatically generate multiple appropriate, diverse, realistic and synchronised human facial reactions from an previously unseen speaker behaviour is a challenging task. Following the successful organisation of the first REACT challenge (REACT 2023), this edition of the challenge (REACT 2024) employs a subset used by the previous challenge, which contains segmented 30-secs dyadic interaction clips originally recorded as part of the NOXI and RECOLA datasets, encouraging participants to develop and benchmark Machine Learning (ML) models that can generate multiple appropriate facial reactions (including facial image sequences and their attributes) given an input conversational partner's stimulus under various dyadic video conference scenarios. This paper presents: (i) the guidelines of the REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii) the performance of the baseline systems on the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation, respectively. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.|\u5728\u4e8c\u5143\u4e92\u52a8\u4e2d\uff0c\u4eba\u7c7b\u4f7f\u7528\u8a00\u8bed\u548c\u975e\u8a00\u8bed\u6697\u793a\u6765\u4f20\u8fbe\u4ed6\u4eec\u7684\u610f\u56fe\u548c\u7cbe\u795e\u72b6\u6001\uff0c\u5176\u4e2d\u591a\u79cd\u4e0d\u540c\u7684\u9762\u90e8\u53cd\u5e94\u53ef\u80fd\u9002\u5408\u54cd\u5e94\u7279\u5b9a\u7684\u8bf4\u8bdd\u8005\u884c\u4e3a\u3002\u90a3\u4e48\uff0c\u5982\u4f55\u5f00\u53d1\u4e00\u79cd\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u8005\u884c\u4e3a\u81ea\u52a8\u751f\u6210\u591a\u79cd\u9002\u5f53\u7684\u3001\u591a\u6837\u5316\u7684\u3001\u771f\u5b9e\u7684\u548c\u540c\u6b65\u7684\u4eba\u7c7b\u9762\u90e8\u53cd\u5e94\uff0c\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u7ee7\u9996\u5c4a REACT \u6311\u6218\u8d5b (REACT 2023) \u6210\u529f\u7ec4\u7ec7\u4e4b\u540e\uff0c\u672c\u6b21\u6311\u6218\u8d5b (REACT 2024) \u91c7\u7528\u4e86\u4e0a\u4e00\u6311\u6218\u8d5b\u4f7f\u7528\u7684\u5b50\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6700\u521d\u4f5c\u4e3a NOXI \u548c RECOLA \u7684\u4e00\u90e8\u5206\u5f55\u5236\u7684\u5206\u6bb5 30 \u79d2\u4e8c\u5143\u4ea4\u4e92\u526a\u8f91\u6570\u636e\u96c6\uff0c\u9f13\u52b1\u53c2\u4e0e\u8005\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u5728\u5404\u79cd\u4e8c\u5143\u89c6\u9891\u4f1a\u8bae\u573a\u666f\u4e0b\uff0c\u6839\u636e\u8f93\u5165\u5bf9\u8bdd\u4f19\u4f34\u7684\u523a\u6fc0\uff0c\u751f\u6210\u591a\u79cd\u9002\u5f53\u7684\u9762\u90e8\u53cd\u5e94\uff08\u5305\u62ec\u9762\u90e8\u56fe\u50cf\u5e8f\u5217\u53ca\u5176\u5c5e\u6027\uff09\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\uff1a(i) REACT 2024 \u6311\u6218\u8d5b\u7684\u6307\u5357\uff1b (ii) \u6311\u6218\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff1b (iii) \u57fa\u7ebf\u7cfb\u7edf\u5728\u4e24\u4e2a\u62df\u8bae\u5b50\u6311\u6218\u4e0a\u7684\u8868\u73b0\uff1a\u5206\u522b\u662f\u79bb\u7ebf\u591a\u91cd\u9002\u5f53\u9762\u90e8\u53cd\u5e94\u751f\u6210\u548c\u5728\u200b\u200b\u7ebf\u591a\u91cd\u9002\u5f53\u9762\u90e8\u53cd\u5e94\u751f\u6210\u3002\u6311\u6218\u57fa\u7ebf\u4ee3\u7801\u53ef\u5728 https://github.com/reactmultimodalchallenge/baseline_react2024 \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.05166v1](http://arxiv.org/pdf/2401.05166v1)|null|\n", "2401.05163": "|**2024-01-10**|**MISS: A Generative Pretraining and Finetuning Approach for Med-VQA**|MISS\uff1aMed-VQA \u7684\u751f\u6210\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u6cd5|Jiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, Lihua Zhang|Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models. The code and model weights will be released upon the paper's acceptance.|\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u5176\u4e2d\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff08VLP\uff09\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\u3002\u7136\u800c\uff0c\u533b\u5b66\u9886\u57df\u7684\u5927\u591a\u6570\u65b9\u6cd5\u5c06VQA\u89c6\u4e3a\u7b54\u6848\u5206\u7c7b\u4efb\u52a1\uff0c\u5f88\u96be\u8fc1\u79fb\u5230\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u9690\u79c1\u6027\u548c\u6602\u8d35\u7684\u6ce8\u91ca\u8fc7\u7a0b\uff0c\u4e25\u91cd\u7f3a\u4e4f\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u533b\u5b66\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u533b\u5b66 VQA \u4efb\u52a1\u7684\u57fa\u4e8e\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6846\u67b6\uff08MISS\uff09\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5c06\u533b\u5b66 VQA \u89c6\u4e3a\u4e00\u9879\u751f\u6210\u4efb\u52a1\u3002\u6211\u4eec\u7edf\u4e00\u6587\u672c\u7f16\u7801\u5668\u548c\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u5bf9\u9f50\u56fe\u50cf\u6587\u672c\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u8f93\u548c\u6807\u9898\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6269\u5c55\u5355\u6a21\u6001\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u4f7f\u8fd9\u4e9b\u4f20\u7edf\u7684\u533b\u5b66\u89c6\u89c9\u9886\u57df\u4efb\u52a1\u6570\u636e\u80fd\u591f\u5e94\u7528\u4e8e VLP\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7528\u8f83\u5c11\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u751f\u6210\u5f0f VQA \u6a21\u578b\u7684\u4f18\u52bf\u3002\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u53d1\u5e03\u3002|[2401.05163v1](http://arxiv.org/pdf/2401.05163v1)|null|\n", "2401.05157": "|**2024-01-10**|**Toward distortion-aware change detection in realistic scenarios**|\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u5931\u771f\u611f\u77e5\u53d8\u5316\u68c0\u6d4b|Yitao Zhao, Heng-Chao Li, Nanqing Liu, Rui Wang|In the conventional change detection (CD) pipeline, two manually registered and labeled remote sensing datasets serve as the input of the model for training and prediction. However, in realistic scenarios, data from different periods or sensors could fail to be aligned as a result of various coordinate systems. Geometric distortion caused by coordinate shifting remains a thorny issue for CD algorithms. In this paper, we propose a reusable self-supervised framework for bitemporal geometric distortion in CD tasks. The whole framework is composed of Pretext Representation Pre-training, Bitemporal Image Alignment, and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the key components of the framework can be reused for assistance in the bitemporal image alignment, while simultaneously enhancing the performance of the CD decoder. Experimental results in 2 large-scale realistic scenarios demonstrate that our proposed method can alleviate the bitemporal geometric distortion in CD tasks.|\u5728\u4f20\u7edf\u7684\u53d8\u5316\u68c0\u6d4b\uff08CD\uff09\u7ba1\u9053\u4e2d\uff0c\u4e24\u4e2a\u624b\u52a8\u6ce8\u518c\u548c\u6807\u8bb0\u7684\u9065\u611f\u6570\u636e\u96c6\u4f5c\u4e3a\u8bad\u7ec3\u548c\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u5165\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u5750\u6807\u7cfb\u4e0d\u540c\uff0c\u6765\u81ea\u4e0d\u540c\u65f6\u671f\u6216\u4f20\u611f\u5668\u7684\u6570\u636e\u53ef\u80fd\u65e0\u6cd5\u5bf9\u9f50\u3002\u5750\u6807\u504f\u79fb\u5f15\u8d77\u7684\u51e0\u4f55\u5931\u771f\u4ecd\u7136\u662f CD \u7b97\u6cd5\u7684\u68d8\u624b\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u7684\u81ea\u6211\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e CD \u4efb\u52a1\u4e2d\u7684\u53cc\u65f6\u6001\u51e0\u4f55\u5931\u771f\u3002\u6574\u4e2a\u6846\u67b6\u7531\u501f\u53e3\u8868\u793a\u9884\u8bad\u7ec3\u3001\u53cc\u65f6\u56fe\u50cf\u5bf9\u9f50\u548c\u4e0b\u6e38\u89e3\u7801\u5668\u5fae\u8c03\u7ec4\u6210\u3002\u53ea\u9700\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\uff0c\u6846\u67b6\u7684\u5173\u952e\u7ec4\u4ef6\u5c31\u53ef\u4ee5\u91cd\u590d\u4f7f\u7528\uff0c\u4ee5\u5e2e\u52a9\u53cc\u65f6\u56fe\u50cf\u5bf9\u9f50\uff0c\u540c\u65f6\u589e\u5f3a CD \u89e3\u7801\u5668\u7684\u6027\u80fd\u3002 2\u4e2a\u5927\u89c4\u6a21\u73b0\u5b9e\u573a\u666f\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u51cf\u8f7bCD\u4efb\u52a1\u4e2d\u7684\u53cc\u65f6\u6001\u51e0\u4f55\u5931\u771f\u3002|[2401.05157v1](http://arxiv.org/pdf/2401.05157v1)|null|\n", "2401.05137": "|**2024-01-10**|**DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography Angiography for Automatic Diabetic Retinopathy Diagnosis**|\u53d1\u73b0\uff1a\u7528\u4e8e\u81ea\u52a8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u8bca\u65ad\u7684\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u9020\u5f71\u7684\u4e8c\u7ef4\u591a\u89c6\u56fe\u603b\u7ed3|Mostafa El Habib Daho, Yihao Li, Rachid Zeghlache, Hugo Le Boit\u00e9, Pierre Deman, Laurent Borderie, Hugang Ren, Niranchana Mannivanan, Capucine Lepicard, B\u00e9atrice Cochener, et.al.|Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading cause of blindness worldwide. Traditionally, DR is monitored using Color Fundus Photography (CFP), a widespread 2-D imaging modality. However, DR classifications based on CFP have poor predictive power, resulting in suboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a recent 3-D imaging modality offering enhanced structural and functional information (blood flow) with a wider field of view. This paper investigates automatic DR severity assessment using 3-D OCTA. A straightforward solution to this task is a 3-D neural network classifier. However, 3-D architectures have numerous parameters and typically require many training samples. A lighter solution consists in using 2-D neural network classifiers processing 2-D en-face (or frontal) projections and/or 2-D cross-sectional slices. Such an approach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face flow maps are often used to detect avascular zones and neovascularization, and 2) cross-sectional slices are commonly analyzed to detect macular edemas, for instance. However, arbitrary data reduction or selection might result in information loss. Two complementary strategies are thus proposed to optimally summarize OCTA volumes with 2-D images: 1) a parametric en-face projection optimized through deep learning and 2) a cross-sectional slice selection process controlled through gradient-based attribution. The full summarization and DR classification pipeline is trained from end to end. The automatic 2-D summary can be displayed in a viewer or printed in a report to support the decision. We show that the proposed 2-D summarization and classification pipeline outperforms direct 3-D classification with the advantage of improved interpretability.|\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u7cd6\u5c3f\u75c5\u7684\u4e00\u79cd\u773c\u90e8\u5e76\u53d1\u75c7\uff0c\u662f\u5168\u4e16\u754c\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\u3002\u4f20\u7edf\u4e0a\uff0cDR \u4f7f\u7528\u5f69\u8272\u773c\u5e95\u6444\u5f71 (CFP) \u8fdb\u884c\u76d1\u6d4b\uff0c\u8fd9\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e8c\u7ef4\u6210\u50cf\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u57fa\u4e8eCFP\u7684DR\u5206\u7c7b\u9884\u6d4b\u80fd\u529b\u8f83\u5dee\uff0c\u5bfc\u81f4DR\u7ba1\u7406\u4e0d\u7406\u60f3\u3002\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u9020\u5f71 (OCTA) \u662f\u4e00\u79cd\u6700\u65b0\u7684 3D \u6210\u50cf\u65b9\u5f0f\uff0c\u53ef\u63d0\u4f9b\u589e\u5f3a\u7684\u7ed3\u6784\u548c\u529f\u80fd\u4fe1\u606f\uff08\u8840\u6d41\uff09\u4ee5\u53ca\u66f4\u5bbd\u7684\u89c6\u91ce\u3002\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528 3-D OCTA \u8fdb\u884c\u81ea\u52a8 DR \u4e25\u91cd\u6027\u8bc4\u4f30\u3002\u6b64\u4efb\u52a1\u7684\u4e00\u4e2a\u76f4\u63a5\u89e3\u51b3\u65b9\u6848\u662f 3D \u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u3002\u7136\u800c\uff0c3D \u67b6\u6784\u5177\u6709\u5927\u91cf\u53c2\u6570\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\u3002\u66f4\u8f7b\u7684\u89e3\u51b3\u65b9\u6848\u5305\u62ec\u4f7f\u7528 2-D \u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u5904\u7406 2-D \u6b63\u9762\uff08\u6216\u6b63\u9762\uff09\u6295\u5f71\u548c/\u6216 2-D \u6a2a\u622a\u9762\u5207\u7247\u3002\u8fd9\u79cd\u65b9\u6cd5\u6a21\u4eff\u4e86\u773c\u79d1\u533b\u751f\u5206\u6790 OCTA \u91c7\u96c6\u7684\u65b9\u5f0f\uff1a1) \u9762\u90e8\u8840\u6d41\u56fe\u901a\u5e38\u7528\u4e8e\u68c0\u6d4b\u65e0\u8840\u7ba1\u533a\u548c\u65b0\u751f\u8840\u7ba1\u5f62\u6210\uff0c2) \u4f8b\u5982\uff0c\u901a\u5e38\u5206\u6790\u6a2a\u622a\u9762\u5207\u7247\u4ee5\u68c0\u6d4b\u9ec4\u6591\u6c34\u80bf\u3002\u7136\u800c\uff0c\u4efb\u610f\u7684\u6570\u636e\u7f29\u51cf\u6216\u9009\u62e9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u6765\u7528\u4e8c\u7ef4\u56fe\u50cf\u6700\u4f73\u5730\u603b\u7ed3 OCTA \u4f53\u79ef\uff1a1\uff09\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7684\u53c2\u6570\u5316\u6b63\u9762\u6295\u5f71\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u63a7\u5236\u7684\u6a2a\u622a\u9762\u5207\u7247\u9009\u62e9\u8fc7\u7a0b\u3002\u5b8c\u6574\u7684\u6458\u8981\u548c DR \u5206\u7c7b\u7ba1\u9053\u662f\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u3002\u81ea\u52a8\u4e8c\u7ef4\u6458\u8981\u53ef\u4ee5\u663e\u793a\u5728\u67e5\u770b\u5668\u4e2d\u6216\u6253\u5370\u5728\u62a5\u544a\u4e2d\u4ee5\u652f\u6301\u51b3\u7b56\u3002\u6211\u4eec\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 2-D \u6982\u62ec\u548c\u5206\u7c7b\u6d41\u7a0b\u4f18\u4e8e\u76f4\u63a5 3-D \u5206\u7c7b\uff0c\u5177\u6709\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u7684\u4f18\u70b9\u3002|[2401.05137v1](http://arxiv.org/pdf/2401.05137v1)|null|\n", "2401.05126": "|**2024-01-10**|**Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer**|\u901a\u8fc7\u9886\u57df\u9002\u5e94\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u4ee5\u4fdd\u62a4\u9690\u79c1\u7684 Vision Transformer|Teru Nagamori, Sayaka Shiota, Hitoshi Kiya|We propose a novel method for privacy-preserving deep neural networks (DNNs) with the Vision Transformer (ViT). The method allows us not only to train models and test with visually protected images but to also avoid the performance degradation caused from the use of encrypted images, whereas conventional methods cannot avoid the influence of image encryption. A domain adaptation method is used to efficiently fine-tune ViT with encrypted images. In experiments, the method is demonstrated to outperform conventional methods in an image classification task on the CIFAR-10 and ImageNet datasets in terms of classification accuracy.|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528 Vision Transformer (ViT) \u4fdd\u62a4\u9690\u79c1\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4f7f\u6211\u4eec\u80fd\u591f\u4f7f\u7528\u53d7\u89c6\u89c9\u4fdd\u62a4\u7684\u56fe\u50cf\u6765\u8bad\u7ec3\u6a21\u578b\u548c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u800c\u4e14\u8fd8\u53ef\u4ee5\u907f\u514d\u7531\u4e8e\u4f7f\u7528\u52a0\u5bc6\u56fe\u50cf\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u907f\u514d\u56fe\u50cf\u52a0\u5bc6\u7684\u5f71\u54cd\u3002\u57df\u9002\u5e94\u65b9\u6cd5\u7528\u4e8e\u6709\u6548\u5730\u5fae\u8c03\u52a0\u5bc6\u56fe\u50cf\u7684 ViT\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728 CIFAR-10 \u548c ImageNet \u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u88ab\u8bc1\u660e\u5728\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002|[2401.05126v1](http://arxiv.org/pdf/2401.05126v1)|null|\n", "2401.05011": "|**2024-01-10**|**Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection**|\u534a\u76d1\u7763 3D \u7269\u4f53\u68c0\u6d4b\u7684\u53cc\u89c6\u89d2\u77e5\u8bc6\u4e30\u5bcc|Yucheng Han, Na Zhao, Weiling Chen, Keng Teck Ma, Hanwang Zhang|Semi-supervised 3D object detection is a promising yet under-explored direction to reduce data annotation costs, especially for cluttered indoor scenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this task by utilizing a teacher model to generate pseudo-labels for unlabeled samples. However, the availability of unlabeled samples in the 3D domain is relatively limited compared to its 2D counterpart due to the greater effort required to collect 3D data. Moreover, the loose consistency regularization in SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to either low-quality supervision or a limited amount of pseudo labels. To address these issues, we present a novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection. Our DPKE enriches the knowledge of limited training data, particularly unlabeled data, from two perspectives: data-perspective and feature-perspective. Specifically, from the data-perspective, we propose a class-probabilistic data augmentation method that augments the input data with additional instances based on the varying distribution of class probabilities. Our DPKE achieves feature-perspective knowledge enrichment by designing a geometry-aware feature matching method that regularizes feature-level similarity between object proposals from the student and teacher models. Extensive experiments on the two benchmark datasets demonstrate that our DPKE achieves superior performance over existing state-of-the-art approaches under various label ratio conditions. The source code will be made available to the public.|\u534a\u76d1\u7763 3D \u7269\u4f53\u68c0\u6d4b\u662f\u4e00\u4e2a\u6709\u524d\u9014\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u65b9\u5411\uff0c\u53ef\u964d\u4f4e\u6570\u636e\u6ce8\u91ca\u6210\u672c\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u6742\u4e71\u7684\u5ba4\u5185\u573a\u666f\u3002\u4e00\u4e9b\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u4f8b\u5982 SESS \u548c 3DIoUMatch\uff0c\u8bd5\u56fe\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u4e3a\u672a\u6807\u8bb0\u7684\u6837\u672c\u751f\u6210\u4f2a\u6807\u7b7e\u6765\u89e3\u51b3\u6b64\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6536\u96c6 3D \u6570\u636e\u9700\u8981\u4ed8\u51fa\u66f4\u5927\u7684\u52aa\u529b\uff0c\u56e0\u6b64\u4e0e 2D \u9886\u57df\u76f8\u6bd4\uff0c3D \u9886\u57df\u4e2d\u672a\u6807\u8bb0\u6837\u672c\u7684\u53ef\u7528\u6027\u76f8\u5bf9\u6709\u9650\u3002\u6b64\u5916\uff0cSESS \u4e2d\u7684\u677e\u6563\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c 3DIoUMatch \u4e2d\u7684\u53d7\u9650\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\u5bfc\u81f4\u4f4e\u8d28\u91cf\u7684\u76d1\u7763\u6216\u6709\u9650\u6570\u91cf\u7684\u4f2a\u6807\u7b7e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u89c6\u89d2\u77e5\u8bc6\u4e30\u5bcc\u65b9\u6cd5\uff0c\u79f0\u4e3a DPKE\uff0c\u7528\u4e8e\u534a\u76d1\u7763 3D \u5bf9\u8c61\u68c0\u6d4b\u3002\u6211\u4eec\u7684DPKE\u4ece\u6570\u636e\u89d2\u5ea6\u548c\u7279\u5f81\u89d2\u5ea6\u4e24\u4e2a\u89d2\u5ea6\u4e30\u5bcc\u4e86\u6709\u9650\u8bad\u7ec3\u6570\u636e\uff0c\u7279\u522b\u662f\u672a\u6807\u8bb0\u6570\u636e\u7684\u77e5\u8bc6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ece\u6570\u636e\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u6982\u7387\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u7c7b\u6982\u7387\u7684\u53d8\u5316\u5206\u5e03\u4f7f\u7528\u989d\u5916\u7684\u5b9e\u4f8b\u6765\u589e\u5f3a\u8f93\u5165\u6570\u636e\u3002\u6211\u4eec\u7684 DPKE \u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u6765\u5b9e\u73b0\u7279\u5f81\u89c6\u89d2\u7684\u77e5\u8bc6\u4e30\u5bcc\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u89c4\u8303\u5b66\u751f\u6a21\u578b\u548c\u6559\u5e08\u6a21\u578b\u7684\u5bf9\u8c61\u5efa\u8bae\u4e4b\u95f4\u7684\u7279\u5f81\u7ea7\u76f8\u4f3c\u6027\u3002\u5bf9\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 DPKE \u5728\u5404\u79cd\u6807\u7b7e\u6bd4\u7387\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u5c06\u5411\u516c\u4f17\u5f00\u653e\u3002|[2401.05011v1](http://arxiv.org/pdf/2401.05011v1)|null|\n", "2401.04988": "|**2024-01-10**|**Optimising Graph Representation for Hardware Implementation of Graph Convolutional Networks for Event-based Vision**|\u4f18\u5316\u57fa\u4e8e\u4e8b\u4ef6\u89c6\u89c9\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u786c\u4ef6\u5b9e\u73b0\u7684\u56fe\u8868\u793a|Kamil Jeziorek, Piotr Wzorek, Krzysztof Blachut, Andrea Pinna, Tomasz Kryjak|Event-based vision is an emerging research field involving processing data generated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest proposals in this area are Graph Convolutional Networks (GCNs), which allow to process events in its original sparse form while maintaining high detection and classification performance. In this paper, we present the hardware implementation of a~graph generation process from an event camera data stream, taking into account both the advantages and limitations of FPGAs. We propose various ways to simplify the graph representation and use scaling and quantisation of values. We consider both undirected and directed graphs that enable the use of PointNet convolution. The results obtained show that by appropriately modifying the graph representation, it is possible to create a~hardware module for graph generation. Moreover, the proposed modifications have no significant impact on object detection performance, only 0.08% mAP less for the base model and the N-Caltech data set.Finally, we describe the proposed hardware architecture of the graph generation module.|\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u662f\u4e00\u4e2a\u65b0\u5174\u7684\u7814\u7a76\u9886\u57df\uff0c\u6d89\u53ca\u5904\u7406\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff08\u795e\u7ecf\u5f62\u6001\u76f8\u673a\uff09\u751f\u6210\u7684\u6570\u636e\u3002\u8be5\u9886\u57df\u7684\u6700\u65b0\u63d0\u8bae\u4e4b\u4e00\u662f\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\uff0c\u5b83\u5141\u8bb8\u4ee5\u539f\u59cb\u7a00\u758f\u5f62\u5f0f\u5904\u7406\u4e8b\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u5230 FPGA \u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4ecb\u7ecd\u4e86\u4ece\u4e8b\u4ef6\u6444\u50cf\u673a\u6570\u636e\u6d41\u751f\u6210\u56fe\u7684\u786c\u4ef6\u5b9e\u73b0\u8fc7\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5404\u79cd\u65b9\u6cd5\u6765\u7b80\u5316\u56fe\u5f62\u8868\u793a\u5e76\u4f7f\u7528\u503c\u7684\u7f29\u653e\u548c\u91cf\u5316\u3002\u6211\u4eec\u8003\u8651\u4f7f\u7528 PointNet \u5377\u79ef\u7684\u65e0\u5411\u56fe\u548c\u6709\u5411\u56fe\u3002\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u4fee\u6539\u56fe\u8868\u793a\uff0c\u53ef\u4ee5\u521b\u5efa\u7528\u4e8e\u56fe\u751f\u6210\u7684\u786c\u4ef6\u6a21\u5757\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u4fee\u6539\u5bf9\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u6ca1\u6709\u663e\u7740\u5f71\u54cd\uff0c\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u548c N-Caltech \u6570\u636e\u96c6\u4ec5\u51cf\u5c11\u4e86 0.08% mAP\u3002\u6700\u540e\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u6240\u63d0\u51fa\u7684\u56fe\u751f\u6210\u6a21\u5757\u7684\u786c\u4ef6\u67b6\u6784\u3002|[2401.04988v1](http://arxiv.org/pdf/2401.04988v1)|null|\n", "2401.04975": "|**2024-01-10**|**HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition**|HaltingVT\uff1a\u7528\u4e8e\u9ad8\u6548\u89c6\u9891\u8bc6\u522b\u7684\u81ea\u9002\u5e94\u4ee4\u724c\u505c\u6b62\u53d8\u538b\u5668|Qian Wu, Ruoxuan Cui, Yuke Li, Haoqi Zhu|Action recognition in videos poses a challenge due to its high computational cost, especially for Joint Space-Time video transformers (Joint VT). Despite their effectiveness, the excessive number of tokens in such architectures significantly limits their efficiency. In this paper, we propose HaltingVT, an efficient video transformer adaptively removing redundant video patch tokens, which is primarily composed of a Joint VT and a Glimpser module. Specifically, HaltingVT applies data-adaptive token reduction at each layer, resulting in a significant reduction in the overall computational cost. Besides, the Glimpser module quickly removes redundant tokens in shallow transformer layers, which may even be misleading for video recognition tasks based on our observations. To further encourage HaltingVT to focus on the key motion-related information in videos, we design an effective Motion Loss during training. HaltingVT acquires video analysis capabilities and token halting compression strategies simultaneously in a unified training process, without requiring additional training procedures or sub-networks. On the Mini-Kinetics dataset, we achieved 75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely low 9.9 GFLOPs. The code is available at https://github.com/dun-research/HaltingVT.|\u89c6\u9891\u4e2d\u7684\u52a8\u4f5c\u8bc6\u522b\u7531\u4e8e\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8054\u5408\u65f6\u7a7a\u89c6\u9891\u8f6c\u6362\u5668\uff08Joint VT\uff09\u800c\u8a00\u3002\u5c3d\u7ba1\u5b83\u4eec\u5f88\u6709\u6548\uff0c\u4f46\u6b64\u7c7b\u67b6\u6784\u4e2d\u8fc7\u591a\u7684\u4ee3\u5e01\u6781\u5927\u5730\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HaltingVT\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u8f6c\u6362\u5668\uff0c\u53ef\u81ea\u9002\u5e94\u5730\u5220\u9664\u5197\u4f59\u89c6\u9891\u8865\u4e01\u6807\u8bb0\uff0c\u5b83\u4e3b\u8981\u7531 Joint VT \u548c Glimpser \u6a21\u5757\u7ec4\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0cHaltingVT \u5728\u6bcf\u4e00\u5c42\u5e94\u7528\u6570\u636e\u81ea\u9002\u5e94\u4ee4\u724c\u51cf\u5c11\uff0c\u4ece\u800c\u663e\u7740\u964d\u4f4e\u603b\u4f53\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0cGlimpser \u6a21\u5757\u53ef\u4ee5\u5feb\u901f\u5220\u9664\u6d45\u5c42\u8f6c\u6362\u5668\u5c42\u4e2d\u7684\u5197\u4f59\u6807\u8bb0\uff0c\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u8fd9\u751a\u81f3\u53ef\u80fd\u4f1a\u8bef\u5bfc\u89c6\u9891\u8bc6\u522b\u4efb\u52a1\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u9f13\u52b1 HaltingVT \u5173\u6ce8\u89c6\u9891\u4e2d\u4e0e\u8fd0\u52a8\u76f8\u5173\u7684\u5173\u952e\u4fe1\u606f\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u8bbe\u8ba1\u4e86\u6709\u6548\u7684\u8fd0\u52a8\u635f\u5931\u3002 HaltingVT \u5728\u7edf\u4e00\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u540c\u65f6\u83b7\u5f97\u89c6\u9891\u5206\u6790\u80fd\u529b\u548c\u4ee4\u724c\u505c\u6b62\u538b\u7f29\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6216\u5b50\u7f51\u7edc\u3002\u5728 Mini-Kinetics \u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u4ee5 24.2 GFLOP \u5b9e\u73b0\u4e86 75.0% \u7684 top-1 ACC\uff0c\u4ee5\u53ca\u4ee5\u6781\u4f4e\u7684 9.9 GFLOP \u5b9e\u73b0\u4e86 67.2% \u7684 top-1 ACC\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/dun-research/HaltingVT \u83b7\u53d6\u3002|[2401.04975v1](http://arxiv.org/pdf/2401.04975v1)|null|\n", "2401.04956": "|**2024-01-10**|**EmMixformer: Mix transformer for eye movement recognition**|EmMixformer\uff1a\u7528\u4e8e\u773c\u52a8\u8bc6\u522b\u7684\u6df7\u5408\u53d8\u538b\u5668|Huafeng Qin, Hongyu Zhu, Xin Jin, Qun Song, Mounim A. El-Yacoubi, Xinbo Gao|Eye movement (EM) is a new highly secure biometric behavioral modality that has received increasing attention in recent years. Although deep neural networks, such as convolutional neural network (CNN), have recently achieved promising performance, current solutions fail to capture local and global temporal dependencies within eye movement data. To overcome this problem, we propose in this paper a mixed transformer termed EmMixformer to extract time and frequency domain information for eye movement recognition. To this end, we propose a mixed block consisting of three modules, transformer, attention Long short-term memory (attention LSTM), and Fourier transformer. We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement. Second, we incorporate the attention mechanism into LSTM to propose attention LSTM with the aim to learn short temporal dependencies. Third, we perform self attention in the frequency domain to learn global features. As the three modules provide complementary feature representations in terms of local and global dependencies, the proposed EmMixformer is capable of improving recognition accuracy. The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error.|\u773c\u52a8\uff08EM\uff09\u662f\u4e00\u79cd\u65b0\u578b\u7684\u9ad8\u5ea6\u5b89\u5168\u7684\u751f\u7269\u8bc6\u522b\u884c\u4e3a\u65b9\u5f0f\uff0c\u8fd1\u5e74\u6765\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u5c3d\u7ba1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7b49\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6700\u8fd1\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6027\u80fd\uff0c\u4f46\u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6355\u83b7\u773c\u52a8\u6570\u636e\u4e2d\u7684\u5c40\u90e8\u548c\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a EmMixformer \u7684\u6df7\u5408\u53d8\u538b\u5668\u6765\u63d0\u53d6\u65f6\u57df\u548c\u9891\u57df\u4fe1\u606f\u4ee5\u8fdb\u884c\u773c\u52a8\u8bc6\u522b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\u7684\u6df7\u5408\u5757\uff0c\u53d8\u538b\u5668\u3001\u6ce8\u610f\u529b\u957f\u77ed\u671f\u8bb0\u5fc6\uff08\u6ce8\u610f\u529bLSTM\uff09\u548c\u5085\u91cc\u53f6\u53d8\u538b\u5668\u3002\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5c1d\u8bd5\u5229\u7528 Transformer \u6765\u5b66\u4e60\u773c\u7403\u8fd0\u52a8\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u4eba\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5c06\u6ce8\u610f\u529b\u673a\u5236\u878d\u5165\u5230 LSTM \u4e2d\uff0c\u63d0\u51fa\u6ce8\u610f\u529b LSTM\uff0c\u65e8\u5728\u5b66\u4e60\u77ed\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5728\u9891\u57df\u4e2d\u8fdb\u884c\u81ea\u6ce8\u610f\u529b\u4ee5\u5b66\u4e60\u5168\u5c40\u7279\u5f81\u3002\u7531\u4e8e\u8fd9\u4e09\u4e2a\u6a21\u5757\u5728\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u7279\u5f81\u8868\u793a\uff0c\u56e0\u6b64\u6240\u63d0\u51fa\u7684 EmMixformer \u80fd\u591f\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002\u6211\u4eec\u7684\u773c\u52a8\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u773c\u52a8\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 EmMixformer \u901a\u8fc7\u5b9e\u73b0\u6700\u4f4e\u7684\u9a8c\u8bc1\u8bef\u5dee\u800c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002|[2401.04956v1](http://arxiv.org/pdf/2401.04956v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "\u56fe\u50cf\u7406\u89e3": {"2401.05335": "|**2024-01-10**|**InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes**|InseRF\uff1a\u795e\u7ecf 3D \u573a\u666f\u4e2d\u6587\u672c\u9a71\u52a8\u7684\u751f\u6210\u5bf9\u8c61\u63d2\u5165|Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari|We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf.|\u6211\u4eec\u4ecb\u7ecd InseRF\uff0c\u8fd9\u662f\u4e00\u79cd\u5728 3D \u573a\u666f\u7684 NeRF \u91cd\u5efa\u4e2d\u751f\u6210\u5bf9\u8c61\u63d2\u5165\u7684\u65b0\u65b9\u6cd5\u3002\u57fa\u4e8e\u7528\u6237\u63d0\u4f9b\u7684\u6587\u672c\u63cf\u8ff0\u548c\u53c2\u8003\u89c6\u70b9\u4e2d\u7684 2D \u8fb9\u754c\u6846\uff0cInseRF \u5728 3D \u573a\u666f\u4e2d\u751f\u6210\u65b0\u5bf9\u8c61\u3002\u6700\u8fd1\uff0c\u7531\u4e8e\u5728 3D \u751f\u6210\u5efa\u6a21\u4e2d\u4f7f\u7528\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5148\u9a8c\uff0c3D \u573a\u666f\u7f16\u8f91\u65b9\u6cd5\u5df2\u7ecf\u53d1\u751f\u4e86\u6df1\u523b\u7684\u8f6c\u53d8\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u901a\u8fc7\u6837\u5f0f\u548c\u5916\u89c2\u66f4\u6539\u6216\u5220\u9664\u73b0\u6709\u5bf9\u8c61\u6765\u7f16\u8f91 3D \u573a\u666f\u65f6\u6700\u6709\u6548\u3002\u7136\u800c\uff0c\u751f\u6210\u65b0\u5bf9\u8c61\u4ecd\u7136\u662f\u6b64\u7c7b\u65b9\u6cd5\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u5728\u672c\u7814\u7a76\u4e2d\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5efa\u8bae\u5c06 3D \u5bf9\u8c61\u63d2\u5165\u57fa\u7840\u4e3a\u573a\u666f\u53c2\u8003\u89c6\u56fe\u4e2d\u7684 2D \u5bf9\u8c61\u63d2\u5165\u3002\u7136\u540e\u4f7f\u7528\u5355\u89c6\u56fe\u5bf9\u8c61\u91cd\u5efa\u65b9\u6cd5\u5c06 2D \u7f16\u8f91\u63d0\u5347\u4e3a 3D\u3002\u7136\u540e\uff0c\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u5148\u9a8c\u6307\u5bfc\u4e0b\uff0c\u5c06\u91cd\u5efa\u7684\u5bf9\u8c61\u63d2\u5165\u573a\u666f\u4e2d\u3002\u6211\u4eec\u5728\u5404\u79cd 3D \u573a\u666f\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6240\u63d0\u51fa\u7684\u7ec4\u4ef6\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002\u6211\u4eec\u5728\u591a\u4e2a 3D \u573a\u666f\u4e2d\u751f\u6210\u5bf9\u8c61\u63d2\u5165\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002 InseRF \u80fd\u591f\u8fdb\u884c\u53ef\u63a7\u4e14 3D \u4e00\u81f4\u7684\u5bf9\u8c61\u63d2\u5165\uff0c\u800c\u4e0d\u9700\u8981\u660e\u786e\u7684 3D \u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\u3002\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\uff1ahttps://mohamad-shahbazi.github.io/inserf\u3002|[2401.05335v1](http://arxiv.org/pdf/2401.05335v1)|null|\n"}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.05252": "|**2024-01-10**|**PIXART-\u03b4: Fast and Controllable Image Generation with Latent Consistency Models**|PIXART-\u03b4\uff1a\u5177\u6709\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u7684\u5feb\u901f\u4e14\u53ef\u63a7\u7684\u56fe\u50cf\u751f\u6210|Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, Zhenguo Li|This technical report introduces PIXART-{\\delta}, a text-to-image synthesis framework that integrates the Latent Consistency Model (LCM) and ControlNet into the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for its ability to generate high-quality images of 1024px resolution through a remarkably efficient training process. The integration of LCM in PIXART-{\\delta} significantly accelerates the inference speed, enabling the production of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta} achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images, marking a 7x improvement over the PIXART-{\\alpha}. Additionally, PIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUs within a single day. With its 8-bit inference capability (von Platen et al., 2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memory constraints, greatly enhancing its usability and accessibility. Furthermore, incorporating a ControlNet-like module enables fine-grained control over text-to-image diffusion models. We introduce a novel ControlNet-Transformer architecture, specifically tailored for Transformers, achieving explicit controllability alongside high-quality image generation. As a state-of-the-art, open-source image generation model, PIXART-{\\delta} offers a promising alternative to the Stable Diffusion family of models, contributing significantly to text-to-image synthesis.|\u672c\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86 PIXART-{\\delta}\uff0c\u8fd9\u662f\u4e00\u79cd\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5b83\u5c06\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (LCM) \u548c ControlNet \u96c6\u6210\u5230\u5148\u8fdb\u7684 PIXART-{\\alpha} \u6a21\u578b\u4e2d\u3002 PIXART-{\\alpha} \u56e0\u5176\u901a\u8fc7\u975e\u5e38\u9ad8\u6548\u7684\u8bad\u7ec3\u8fc7\u7a0b\u751f\u6210 1024px \u5206\u8fa8\u7387\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u800c\u53d7\u5230\u8ba4\u53ef\u3002 PIXART-{\\delta}\u4e2d LCM \u7684\u96c6\u6210\u663e\u7740\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u53ea\u9700 2-4 \u4e2a\u6b65\u9aa4\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPIXART-{\\delta} \u5728\u751f\u6210 1024x1024 \u50cf\u7d20\u56fe\u50cf\u65b9\u9762\u7a81\u7834\u4e86 0.5 \u79d2\uff0c\u6bd4 PIXART-{\\alpha} \u63d0\u9ad8\u4e86 7 \u500d\u3002\u6b64\u5916\uff0cPIXART-{\\delta} \u8bbe\u8ba1\u4e3a\u53ef\u5728\u4e00\u5929\u5185\u5728 32GB V100 GPU \u4e0a\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002\u51ed\u501f\u5176 8 \u4f4d\u63a8\u7406\u80fd\u529b\uff08von Platen \u7b49\u4eba\uff0c2023\uff09\uff0cPIXART-{\\delta} \u53ef\u4ee5\u5728 8GB GPU \u5185\u5b58\u9650\u5236\u5185\u5408\u6210 1024px \u56fe\u50cf\uff0c\u5927\u5927\u589e\u5f3a\u4e86\u5176\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u7c7b\u4f3c ControlNet \u7684\u6a21\u5757\u53ef\u4ee5\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684 ControlNet-Transformer \u67b6\u6784\uff0c\u4e13\u4e3a Transformer \u91cf\u8eab\u5b9a\u5236\uff0c\u53ef\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u540c\u65f6\u5b9e\u73b0\u660e\u786e\u7684\u53ef\u63a7\u6027\u3002\u4f5c\u4e3a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0cPIXART-{\\delta} \u4e3a\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7cfb\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7684\u5408\u6210\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e\u3002|[2401.05252v1](http://arxiv.org/pdf/2401.05252v1)|null|\n", "2401.05055": "|**2024-01-10**|**Application of Deep Learning in Blind Motion Deblurring: Current Status and Future Prospects**|\u6df1\u5ea6\u5b66\u4e60\u5728\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u4e2d\u7684\u5e94\u7528\uff1a\u73b0\u72b6\u4e0e\u672a\u6765\u5c55\u671b|Yawen Xiang, Heng Zhou, Chengyang Li, Fangwei Sun, Zhongbo Li, Yongqiang Xie|Motion deblurring is one of the fundamental problems of computer vision and has received continuous attention. The variability in blur, both within and across images, imposes limitations on non-blind deblurring techniques that rely on estimating the blur kernel. As a response, blind motion deblurring has emerged, aiming to restore clear and detailed images without prior knowledge of the blur type, fueled by the advancements in deep learning methodologies. Despite strides in this field, a comprehensive synthesis of recent progress in deep learning-based blind motion deblurring is notably absent. This paper fills that gap by providing an exhaustive overview of the role of deep learning in blind motion deblurring, encompassing datasets, evaluation metrics, and methods developed over the last six years. Specifically, we first introduce the types of motion blur and the fundamental principles of deblurring. Next, we outline the shortcomings of traditional non-blind deblurring algorithms, emphasizing the advantages of employing deep learning techniques for deblurring tasks. Following this, we categorize and summarize existing blind motion deblurring methods based on different backbone networks, including convolutional neural networks, generative adversarial networks, recurrent neural networks, and Transformer networks. Subsequently, we elaborate not only on the fundamental principles of these different categories but also provide a comprehensive summary and comparison of their advantages and limitations. Qualitative and quantitative experimental results conducted on four widely used datasets further compare the performance of SOTA methods. Finally, an analysis of present challenges and future pathways. All collected models, benchmark datasets, source code links, and codes for evaluation have been made publicly available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey|\u8fd0\u52a8\u53bb\u6a21\u7cca\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u672c\u95ee\u9898\u4e4b\u4e00\uff0c\u53d7\u5230\u6301\u7eed\u5173\u6ce8\u3002\u56fe\u50cf\u5185\u90e8\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u7cca\u53d8\u5316\u5bf9\u4f9d\u8d56\u4e8e\u4f30\u8ba1\u6a21\u7cca\u5185\u6838\u7684\u975e\u76f2\u53bb\u6a21\u7cca\u6280\u672f\u65bd\u52a0\u4e86\u9650\u5236\u3002\u4f5c\u4e3a\u56de\u5e94\uff0c\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u5e94\u8fd0\u800c\u751f\uff0c\u65e8\u5728\u5728\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u6b65\u7684\u63a8\u52a8\u4e0b\uff0c\u5728\u4e0d\u4e8b\u5148\u4e86\u89e3\u6a21\u7cca\u7c7b\u578b\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u6e05\u6670\u8be6\u7ec6\u7684\u56fe\u50cf\u3002\u5c3d\u7ba1\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u957f\u8db3\u7684\u8fdb\u6b65\uff0c\u4f46\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u5168\u9762\u7efc\u5408\u4ecd\u7136\u660e\u663e\u7f3a\u4e4f\u3002\u672c\u6587\u901a\u8fc7\u8be6\u5c3d\u6982\u8ff0\u6df1\u5ea6\u5b66\u4e60\u5728\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u4e2d\u7684\u4f5c\u7528\uff08\u5305\u62ec\u8fc7\u53bb\u516d\u5e74\u5f00\u53d1\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\uff09\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u8fd0\u52a8\u6a21\u7cca\u7684\u7c7b\u578b\u548c\u53bb\u6a21\u7cca\u7684\u57fa\u672c\u539f\u7406\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u4f20\u7edf\u975e\u76f2\u53bb\u6a21\u7cca\u7b97\u6cd5\u7684\u7f3a\u70b9\uff0c\u5f3a\u8c03\u4e86\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u53bb\u6a21\u7cca\u4efb\u52a1\u7684\u4f18\u52bf\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6839\u636e\u4e0d\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\u5bf9\u73b0\u6709\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u603b\u7ed3\uff0c\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c Transformer \u7f51\u7edc\u3002\u968f\u540e\uff0c\u6211\u4eec\u4e0d\u4ec5\u9610\u8ff0\u4e86\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u522b\u7684\u57fa\u672c\u539f\u7406\uff0c\u8fd8\u5bf9\u5b83\u4eec\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u8fdb\u884c\u4e86\u5168\u9762\u7684\u603b\u7ed3\u548c\u6bd4\u8f83\u3002\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u4e00\u6b65\u6bd4\u8f83\u4e86 SOTA \u65b9\u6cd5\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u5206\u6790\u5f53\u524d\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u8def\u5f84\u3002\u6240\u6709\u6536\u96c6\u7684\u6a21\u578b\u3001\u57fa\u51c6\u6570\u636e\u96c6\u3001\u6e90\u4ee3\u7801\u94fe\u63a5\u548c\u8bc4\u4f30\u4ee3\u7801\u5747\u5df2\u5728 https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey \u4e0a\u516c\u5f00|[2401.05055v1](http://arxiv.org/pdf/2401.05055v1)|null|\n"}, "\u591a\u6a21\u6001": {"2401.04903": "|**2024-01-10**|**SnapCap: Efficient Snapshot Compressive Video Captioning**|SnapCap\uff1a\u9ad8\u6548\u7684\u5feb\u7167\u538b\u7f29\u89c6\u9891\u5b57\u5e55|Jianqiao Sun, Yudi Su, Hao Zhang, Ziheng Cheng, Zequn Zeng, Zhengjue Wang, Bo Chen, Xin Yuan|Video Captioning (VC) is a challenging multi-modal task since it requires describing the scene in language by understanding various and complex videos. For machines, the traditional VC follows the \"imaging-compression-decoding-and-then-captioning\" pipeline, where compression is pivot for storage and transmission. However, in such a pipeline, some potential shortcomings are inevitable, i.e., information redundancy resulting in low efficiency and information loss during the sampling process for captioning. To address these problems, in this paper, we propose a novel VC pipeline to generate captions directly from the compressed measurement, which can be captured by a snapshot compressive sensing camera and we dub our model SnapCap. To be more specific, benefiting from the signal simulation, we have access to obtain abundant measurement-video-annotation data pairs for our model. Besides, to better extract language-related visual representations from the compressed measurement, we propose to distill the knowledge from videos via a pre-trained CLIP with plentiful language-vision associations to guide the learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we conduct experiments on two widely-used VC datasets. Both the qualitative and quantitative results verify the superiority of our pipeline over conventional VC pipelines. In particular, compared to the \"caption-after-reconstruction\" methods, our SnapCap can run at least 3$\\times$ faster, and achieve better caption results.|\u89c6\u9891\u5b57\u5e55\uff08VC\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u901a\u8fc7\u7406\u89e3\u5404\u79cd\u590d\u6742\u7684\u89c6\u9891\u6765\u7528\u8bed\u8a00\u63cf\u8ff0\u573a\u666f\u3002\u5bf9\u4e8e\u673a\u5668\u6765\u8bf4\uff0c\u4f20\u7edf\u7684\u89c6\u9891\u7f16\u7801\u9075\u5faa\u201c\u6210\u50cf-\u538b\u7f29-\u89e3\u7801-\u5b57\u200b\u200b\u5e55\u201d\u7684\u6d41\u7a0b\uff0c\u5176\u4e2d\u538b\u7f29\u662f\u5b58\u50a8\u548c\u4f20\u8f93\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u5728\u8fd9\u6837\u7684\u7ba1\u9053\u4e2d\uff0c\u4e00\u4e9b\u6f5c\u5728\u7684\u7f3a\u70b9\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u5373\u4fe1\u606f\u5197\u4f59\u5bfc\u81f4\u5b57\u5e55\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u4e22\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 VC \u7ba1\u9053\uff0c\u76f4\u63a5\u4ece\u538b\u7f29\u6d4b\u91cf\u751f\u6210\u5b57\u5e55\uff0c\u8be5\u5b57\u5e55\u53ef\u4ee5\u7531\u5feb\u7167\u538b\u7f29\u4f20\u611f\u76f8\u673a\u6355\u83b7\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u79f0\u4e3a SnapCap\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u53d7\u76ca\u4e8e\u4fe1\u53f7\u6a21\u62df\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a\u6211\u4eec\u7684\u6a21\u578b\u83b7\u5f97\u4e30\u5bcc\u7684\u6d4b\u91cf-\u89c6\u9891-\u6ce8\u91ca\u6570\u636e\u5bf9\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u66f4\u597d\u5730\u4ece\u538b\u7f29\u6d4b\u91cf\u4e2d\u63d0\u53d6\u4e0e\u8bed\u8a00\u76f8\u5173\u7684\u89c6\u89c9\u8868\u793a\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u9884\u5148\u8bad\u7ec3\u7684 CLIP \u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u5e76\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u8a00\u89c6\u89c9\u5173\u8054\uff0c\u4ee5\u6307\u5bfc SnapCap \u7684\u5b66\u4e60\u3002\u4e3a\u4e86\u8bc1\u660e SnapCap \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684 VC \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u90fd\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7ba1\u9053\u76f8\u5bf9\u4e8e\u4f20\u7edf VC \u7ba1\u9053\u7684\u4f18\u8d8a\u6027\u3002\u7279\u522b\u662f\uff0c\u4e0e\u201c\u91cd\u5efa\u540e\u7684\u5b57\u5e55\u201d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 SnapCap \u7684\u8fd0\u884c\u901f\u5ea6\u81f3\u5c11\u5feb 3$\\times$\uff0c\u5e76\u83b7\u5f97\u66f4\u597d\u7684\u5b57\u5e55\u6548\u679c\u3002|[2401.04903v1](http://arxiv.org/pdf/2401.04903v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.05018": "|**2024-01-10**|**AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction**|AdvMT\uff1a\u7528\u4e8e\u957f\u671f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u5bf9\u6297\u6027\u8fd0\u52a8\u53d8\u538b\u5668|Sarmad Idrees, Jongeun Choi, Seokman Sohn|To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential. Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses. Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches. Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge. In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator. This combination effectively captures spatial and temporal dependencies simultaneously within frames. With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions. The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions|\u4e3a\u4e86\u5728\u5171\u4eab\u73af\u5883\u4e2d\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u65e0\u7f1d\u534f\u4f5c\uff0c\u51c6\u786e\u9884\u6d4b\u672a\u6765\u7684\u4eba\u7c7b\u8fd0\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u4e0a\uff0c\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u88ab\u89c6\u4e3a\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u5229\u7528\u5386\u53f2\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u6765\u4f30\u8ba1\u672a\u6765\u7684\u59ff\u52bf\u3002\u4ece\u666e\u901a\u7684\u5faa\u73af\u7f51\u7edc\u5f00\u59cb\uff0c\u7814\u7a76\u754c\u7814\u7a76\u4e86\u5404\u79cd\u5b66\u4e60\u4eba\u4f53\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u548c\u751f\u6210\u65b9\u6cd5\u3002\u5c3d\u7ba1\u505a\u51fa\u4e86\u8fd9\u4e9b\u52aa\u529b\uff0c\u5b9e\u73b0\u51c6\u786e\u7684\u957f\u671f\u9884\u6d4b\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u5728\u8fd9\u65b9\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u6297\u8fd0\u52a8\u53d8\u6362\u5668\uff08AdvMT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u96c6\u6210\u4e86\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u8fd0\u52a8\u7f16\u7801\u5668\u548c\u65f6\u95f4\u8fde\u7eed\u6027\u9274\u522b\u5668\u7684\u65b0\u9896\u6a21\u578b\u3002\u8fd9\u79cd\u7ec4\u5408\u6709\u6548\u5730\u5728\u5e27\u5185\u540c\u65f6\u6355\u83b7\u7a7a\u95f4\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u5c11\u4e86\u9884\u6d4b\u4e2d\u4e0d\u9700\u8981\u7684\u4f2a\u5f71\uff0c\u4ece\u800c\u786e\u4fdd\u5b66\u4e60\u66f4\u771f\u5b9e\u3001\u66f4\u6d41\u7545\u7684\u4eba\u4f53\u52a8\u4f5c\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cAdvMT \u6781\u5927\u5730\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e5f\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u77ed\u671f\u9884\u6d4b|[2401.05018v1](http://arxiv.org/pdf/2401.05018v1)|null|\n", "2401.04984": "|**2024-01-10**|**MGNet: Learning Correspondences via Multiple Graphs**|MGNet\uff1a\u901a\u8fc7\u591a\u4e2a\u56fe\u5b66\u4e60\u5bf9\u5e94\u5173\u7cfb|Luanyuan Dai, Xiaoyu Du, Hanwang Zhang, Jinhui Tang|Learning correspondences aims to find correct correspondences (inliers) from the initial correspondence set with an uneven correspondence distribution and a low inlier rate, which can be regarded as graph data. Recent advances usually use graph neural networks (GNNs) to build a single type of graph or simply stack local graphs into the global one to complete the task. But they ignore the complementary relationship between different types of graphs, which can effectively capture potential relationships among sparse correspondences. To address this problem, we propose MGNet to effectively combine multiple complementary graphs. To obtain information integrating implicit and explicit local graphs, we construct local graphs from implicit and explicit aspects and combine them effectively, which is used to build a global graph. Moreover, we propose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse correspondence information at once in the global graph, which can capture and amplify discriminative features. Extensive experiments demonstrate that MGNet outperforms state-of-the-art methods in different visual tasks. The code is provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.|\u5b66\u4e60\u5bf9\u5e94\u5173\u7cfb\u7684\u76ee\u7684\u662f\u4ece\u5bf9\u5e94\u5206\u5e03\u4e0d\u5747\u5300\u4e14\u5185\u70b9\u7387\u8f83\u4f4e\u7684\u521d\u59cb\u5bf9\u5e94\u96c6\u5408\u4e2d\u627e\u5230\u6b63\u786e\u7684\u5bf9\u5e94\u5173\u7cfb\uff08\u5185\u70b9\uff09\uff0c\u53ef\u4ee5\u5c06\u5176\u89c6\u4e3a\u56fe\u6570\u636e\u3002\u6700\u8fd1\u7684\u8fdb\u5c55\u901a\u5e38\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6765\u6784\u5efa\u5355\u4e00\u7c7b\u578b\u7684\u56fe\uff0c\u6216\u8005\u7b80\u5355\u5730\u5c06\u5c40\u90e8\u56fe\u5806\u53e0\u5230\u5168\u5c40\u56fe\u4e2d\u6765\u5b8c\u6210\u4efb\u52a1\u3002\u4f46\u4ed6\u4eec\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u578b\u56fe\u4e4b\u95f4\u7684\u4e92\u8865\u5173\u7cfb\uff0c\u800c\u8fd9\u79cd\u5173\u7cfb\u53ef\u4ee5\u6709\u6548\u6355\u83b7\u7a00\u758f\u5bf9\u5e94\u5173\u7cfb\u4e4b\u95f4\u7684\u6f5c\u5728\u5173\u7cfb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa MGNet \u6765\u6709\u6548\u5730\u7ec4\u5408\u591a\u4e2a\u4e92\u8865\u56fe\u3002\u4e3a\u4e86\u83b7\u5f97\u96c6\u6210\u9690\u5f0f\u548c\u663e\u5f0f\u5c40\u90e8\u56fe\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u4ece\u9690\u5f0f\u548c\u663e\u5f0f\u65b9\u9762\u6784\u5efa\u5c40\u90e8\u56fe\uff0c\u5e76\u5c06\u5b83\u4eec\u6709\u6548\u5730\u7ed3\u5408\u8d77\u6765\uff0c\u7528\u4e8e\u6784\u5efa\u5168\u5c40\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Graph~Soft~Degree~Attention (GSDA)\uff0c\u4ee5\u5145\u5206\u5229\u7528\u5168\u5c40\u56fe\u4e2d\u7684\u6240\u6709\u7a00\u758f\u5bf9\u5e94\u4fe1\u606f\uff0c\u53ef\u4ee5\u6355\u83b7\u548c\u653e\u5927\u5224\u522b\u6027\u7279\u5f81\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e MGNet \u5728\u4e0d\u540c\u7684\u89c6\u89c9\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u5728 https://github.com/DAILUANYUAN/MGNet-2024AAAI \u4e2d\u63d0\u4f9b\u3002|[2401.04984v1](http://arxiv.org/pdf/2401.04984v1)|null|\n", "2401.04921": "|**2024-01-10**|**Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton**|\u57fa\u4e8e\u6269\u6563\u7684\u59ff\u52bf\u7ec6\u5316\u548c\u591a\u5047\u8bbe\u751f\u6210\uff0c\u7528\u4e8e 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1|Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Xinlin Yuan, Wenming Yang|Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to enhance pose accuracy by generating multiple hypotheses. However, most of the hypotheses generated deviate substantially from the true pose. Compared to deterministic models, the excessive uncertainty in probabilistic models leads to weaker performance in single-hypothesis prediction. To address these two challenges, we propose a diffusion-based refinement framework called DRPose, which refines the output of deterministic models by reverse diffusion and achieves more suitable multi-hypothesis prediction for the current pose benchmark by multi-step refinement with multiple noises. To this end, we propose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement Module (PRM) for denoising and refining. Extensive experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art performance on both single and multi-hypothesis 3DHPE. Code is available at https://github.com/KHB1698/DRPose.|\u5148\u524d\u7684 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1 (3DHPE) \u6982\u7387\u6a21\u578b\u65e8\u5728\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5047\u8bbe\u6765\u63d0\u9ad8\u59ff\u52bf\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u751f\u6210\u7684\u5047\u8bbe\u4e0e\u771f\u5b9e\u59ff\u52bf\u6709\u5f88\u5927\u504f\u5dee\u3002\u4e0e\u786e\u5b9a\u6027\u6a21\u578b\u76f8\u6bd4\uff0c\u6982\u7387\u6a21\u578b\u8fc7\u591a\u7684\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u5355\u5047\u8bbe\u9884\u6d4b\u7684\u6027\u80fd\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DRPose \u7684\u57fa\u4e8e\u6269\u6563\u7684\u7ec6\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u53cd\u5411\u6269\u6563\u7ec6\u5316\u786e\u5b9a\u6027\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u591a\u4e2a\u566a\u58f0\u7684\u591a\u6b65\u7ec6\u5316\u5b9e\u73b0\u5bf9\u5f53\u524d\u59ff\u6001\u57fa\u51c6\u66f4\u5408\u9002\u7684\u591a\u5047\u8bbe\u9884\u6d4b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u56fe\u5377\u79ef\u53d8\u6362\u5668\uff08SGCT\uff09\u548c\u7528\u4e8e\u53bb\u566a\u548c\u7ec6\u5316\u7684\u59ff\u52bf\u7ec6\u5316\u6a21\u5757\uff08PRM\uff09\u3002\u5728 Human3.6M \u548c MPI-INF-3DHP \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5355\u5047\u8bbe\u548c\u591a\u5047\u8bbe 3DHPE \u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/KHB1698/DRPost \u83b7\u53d6\u3002|[2401.04921v1](http://arxiv.org/pdf/2401.04921v1)|null|\n", "2401.04872": "|**2024-01-10**|**Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction**|\u7528\u4e8e\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u77e5\u8bc6\u611f\u77e5\u56fe\u8f6c\u6362\u5668|Yu Liu, Yuexin Zhang, Kunming Li, Yongliang Qiao, Stewart Worrall, You-Fu Li, He Kong|Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for training and performance evaluation purposes. The proposed framework is validated and compared against existing methods using popular public datasets, i.e., ETH and UCY. Experimental results demonstrate the improved performance of our proposed scheme.|\u9884\u6d4b\u884c\u4eba\u8fd0\u52a8\u8f68\u8ff9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8def\u5f84\u89c4\u5212\u548c\u8fd0\u52a8\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u4e0d\u540c\u73af\u5883\u4e2d\u4eba\u4f53\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51c6\u786e\u9884\u6d4b\u4eba\u7fa4\u8f68\u8ff9\u5177\u6709\u6311\u6218\u6027\u3002\u5bf9\u4e8e\u8bad\u7ec3\uff0c\u6700\u8fd1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u8f68\u8ff9\u5386\u53f2\u548c\u884c\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u7b49\u4fe1\u606f\u3002\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5dee\u5f02\u5c1a\u672a\u6b63\u786e\u7eb3\u5165\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9650\u5236\u5404\u79cd\u573a\u666f\u7684\u9884\u6d4b\u6027\u80fd\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u8f6c\u6362\u5668\u7ed3\u6784\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u6355\u83b7\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u5404\u4e2a\u7ad9\u70b9\u548c\u573a\u666f\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u7279\u522b\u662f\uff0c\u8bbe\u8ba1\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u9886\u57df\u9002\u5e94\u6a21\u5757\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u51fa\u4e8e\u8bad\u7ec3\u548c\u6027\u80fd\u8bc4\u4f30\u76ee\u7684\uff0c\u5f15\u5165\u4e86\u8003\u8651\u8de8\u6570\u636e\u96c6\u5e8f\u5217\u7684\u9644\u52a0\u5ea6\u91cf\u3002\u4f7f\u7528\u6d41\u884c\u7684\u516c\u5171\u6570\u636e\u96c6\uff08\u5373 ETH \u548c UCY\uff09\u5bf9\u6240\u63d0\u51fa\u7684\u6846\u67b6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6848\u7684\u6027\u80fd\u5f97\u5230\u4e86\u63d0\u9ad8\u3002|[2401.04872v1](http://arxiv.org/pdf/2401.04872v1)|null|\n", "2401.04861": "|**2024-01-10**|**CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from Monocular Video**|CTNeRF\uff1a\u5355\u76ee\u89c6\u9891\u52a8\u6001\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u8de8\u65f6\u95f4\u53d8\u6362\u5668|Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Yang Long, Yefeng Zheng|The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views.|\u6211\u4eec\u5de5\u4f5c\u7684\u76ee\u6807\u662f\u4ece\u590d\u6742\u548c\u52a8\u6001\u573a\u666f\u7684\u5355\u773c\u89c6\u9891\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u3002\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982 DynamicNeRF\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u53d8\u52a8\u6001\u8f90\u5c04\u573a\u663e\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u7cbe\u786e\u5efa\u6a21\u590d\u6742\u7269\u4f53\u7684\u8fd0\u52a8\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7ec6\u8282\u6e32\u67d3\u4e0d\u51c6\u786e\u548c\u6a21\u7cca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u8fd1\u7684\u6cdb\u5316 NeRF \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5c06\u9644\u8fd1\u7684\u89c6\u56fe\u805a\u5408\u5230\u65b0\u7684\u89c2\u70b9\u4e0a\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u65b9\u6cd5\u901a\u5e38\u4ec5\u5bf9\u9759\u6001\u573a\u666f\u6709\u6548\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5728\u65f6\u57df\u548c\u9891\u57df\u4e2d\u8fd0\u884c\u7684\u6a21\u5757\u6765\u805a\u5408\u5bf9\u8c61\u8fd0\u52a8\u7684\u7279\u5f81\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u5b66\u4e60\u5e27\u4e4b\u95f4\u7684\u5173\u7cfb\u5e76\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u52a8\u6001\u573a\u666f\u6570\u636e\u96c6\u4e0a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u663e\u7740\u6539\u8fdb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5408\u6210\u89c6\u56fe\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|[2401.04861v1](http://arxiv.org/pdf/2401.04861v1)|null|\n"}, "Nerf": {}, "3DGS": {}, "3D/CG": {"2401.05236": "|**2024-01-10**|**Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects**|\u91cd\u590d\u7684\u7ed3\u6784\uff1a\u4e00\u5806\u5bf9\u8c61\u7684\u795e\u7ecf\u9006\u5411\u56fe\u5f62|Tianhang Cheng, Wei-Chiu Ma, Kaiyu Guan, Antonio Torralba, Shenlong Wang|Our world is full of identical objects (\\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.|\u6211\u4eec\u7684\u4e16\u754c\u5145\u6ee1\u4e86\u76f8\u540c\u7684\u7269\u4f53\uff08\u4f8b\u5982\uff0c\u53ef\u4e50\u7f50\u3001\u76f8\u540c\u578b\u53f7\u7684\u6c7d\u8f66\uff09\u3002\u5f53\u5c06\u8fd9\u4e9b\u91cd\u590d\u9879\u653e\u5728\u4e00\u8d77\u67e5\u770b\u65f6\uff0c\u5b83\u4eec\u4e3a\u6211\u4eec\u6709\u6548\u5730\u63a8\u7406 3D \u63d0\u4f9b\u4e86\u989d\u5916\u4e14\u5f3a\u6709\u529b\u7684\u7ebf\u7d22\u3002\u53d7\u8fd9\u4e00\u89c2\u5bdf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u91cd\u590d\u7ed3\u6784\uff08SfD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9006\u5411\u56fe\u5f62\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5305\u542b\u591a\u4e2a\u76f8\u540c\u5bf9\u8c61\u7684\u5355\u4e2a\u56fe\u50cf\u4e2d\u91cd\u5efa\u51e0\u4f55\u5f62\u72b6\u3001\u6750\u8d28\u548c\u7167\u660e\u3002 SfD \u9996\u5148\u8bc6\u522b\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u7136\u540e\u8054\u5408\u4f30\u8ba1\u6240\u6709\u5b9e\u4f8b\u7684 6DoF \u59ff\u6001\u3002\u968f\u540e\u91c7\u7528\u9006\u56fe\u5f62\u7ba1\u9053\u6765\u8054\u5408\u63a8\u7406\u5bf9\u8c61\u7684\u5f62\u72b6\u3001\u6750\u8d28\u548c\u73af\u5883\u5149\uff0c\u540c\u65f6\u9075\u5faa\u8de8\u5b9e\u4f8b\u5171\u4eab\u51e0\u4f55\u548c\u6750\u6599\u7ea6\u675f\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u5229\u7528\u5bf9\u8c61\u91cd\u590d\u4f5c\u4e3a\u5355\u56fe\u50cf\u9006\u5411\u56fe\u5f62\u7684\u7a33\u5065\u5148\u9a8c\uff0c\u5e76\u63d0\u51fa\u7528\u4e8e\u8054\u5408 6-DoF \u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u7684\u9762\u5185\u65cb\u8f6c\u7a33\u5065\u8fd0\u52a8\u7ed3\u6784 (SfM) \u516c\u5f0f\u3002\u901a\u8fc7\u5229\u7528\u5355\u4e2a\u56fe\u50cf\u7684\u591a\u89c6\u56fe\u7ebf\u7d22\uff0cSfD \u751f\u6210\u66f4\u771f\u5b9e\u3001\u66f4\u8be6\u7ec6\u7684 3D \u91cd\u5efa\uff0c\u663e\u7740\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u56fe\u50cf\u91cd\u5efa\u6a21\u578b\u548c\u5177\u6709\u76f8\u4f3c\u6216\u66f4\u591a\u6570\u91cf\u89c2\u5bdf\u7684\u591a\u89c6\u56fe\u91cd\u5efa\u65b9\u6cd5\u3002|[2401.05236v1](http://arxiv.org/pdf/2401.05236v1)|null|\n"}, "GNN": {}, "\u5176\u4ed6": {"2401.05232": "|**2024-01-10**|**Measuring Natural Scenes SFR of Automotive Fisheye Cameras**|\u6d4b\u91cf\u6c7d\u8f66\u9c7c\u773c\u76f8\u673a\u7684\u81ea\u7136\u573a\u666f SFR|Daniel Jakab, Eoin Martino Grua, Brian Micheal Deegan, Anthony Scanlan, Pepijn Van De Ven, Ciar\u00e1n Eising|The Modulation Transfer Function (MTF) is an important image quality metric typically used in the automotive domain. However, despite the fact that optical quality has an impact on the performance of computer vision in vehicle automation, for many public datasets, this metric is unknown. Additionally, wide field-of-view (FOV) cameras have become increasingly popular, particularly for low-speed vehicle automation applications. To investigate image quality in datasets, this paper proposes an adaptation of the Natural Scenes Spatial Frequency Response (NS-SFR) algorithm to suit cameras with a wide field-of-view.|\u8c03\u5236\u4f20\u9012\u51fd\u6570 (MTF) \u662f\u6c7d\u8f66\u9886\u57df\u901a\u5e38\u4f7f\u7528\u7684\u91cd\u8981\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5149\u5b66\u8d28\u91cf\u5bf9\u8f66\u8f86\u81ea\u52a8\u5316\u4e2d\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6027\u80fd\u6709\u5f71\u54cd\uff0c\u4f46\u5bf9\u4e8e\u8bb8\u591a\u516c\u5171\u6570\u636e\u96c6\u6765\u8bf4\uff0c\u8fd9\u4e2a\u6307\u6807\u662f\u672a\u77e5\u7684\u3002\u6b64\u5916\uff0c\u5bbd\u89c6\u573a (FOV) \u76f8\u673a\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f4e\u901f\u8f66\u8f86\u81ea\u52a8\u5316\u5e94\u7528\u3002\u4e3a\u4e86\u7814\u7a76\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u7136\u573a\u666f\u7a7a\u95f4\u9891\u7387\u54cd\u5e94\uff08NS-SFR\uff09\u7b97\u6cd5\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u4ee5\u9002\u5e94\u5177\u6709\u5bbd\u89c6\u573a\u7684\u76f8\u673a\u3002|[2401.05232v1](http://arxiv.org/pdf/2401.05232v1)|null|\n", "2401.05217": "|**2024-01-10**|**Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method**|\u63a2\u7d22\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u6f0f\u6d1e\uff1a\u57fa\u4e8e\u67e5\u8be2\u7684\u9ed1\u76d2\u65b9\u6cd5|Chenxi Yang, Yujia Liu, Dingquan Li, Tingting jiang|No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality scores consistent with human perception without relying on pristine reference images, serving as a crucial component in various visual tasks. Ensuring the robustness of NR-IQA methods is vital for reliable comparisons of different image processing techniques and consistent user experiences in recommendations. The attack methods for NR-IQA provide a powerful instrument to test the robustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on the gradient of the NR-IQA model, leading to limitations when the gradient information is unavailable. In this paper, we present a pioneering query-based black box attack against NR-IQA methods. We propose the concept of \\emph{score boundary} and leverage an adaptive iterative approach with multiple score boundaries. Meanwhile, the initial attack directions are also designed to leverage the characteristics of the Human Visual System (HVS). Experiments show our attack method outperforms all compared state-of-the-art methods and is far ahead of previous black-box methods. The effective DBCNN model suffers a Spearman rank-order correlation coefficient (SROCC) decline of $0.6972$ attacked by our method, revealing the vulnerability of NR-IQA to black-box attacks. The proposed attack method also provides a potent tool for further exploration into NR-IQA robustness.|\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u65e8\u5728\u9884\u6d4b\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u7684\u56fe\u50cf\u8d28\u91cf\u5206\u6570\uff0c\u800c\u4e0d\u4f9d\u8d56\u539f\u59cb\u53c2\u8003\u56fe\u50cf\uff0c\u4f5c\u4e3a\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u786e\u4fdd NR-IQA \u65b9\u6cd5\u7684\u7a33\u5065\u6027\u5bf9\u4e8e\u4e0d\u540c\u56fe\u50cf\u5904\u7406\u6280\u672f\u7684\u53ef\u9760\u6bd4\u8f83\u548c\u63a8\u8350\u4e2d\u4e00\u81f4\u7684\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002 NR-IQA\u7684\u653b\u51fb\u65b9\u6cd5\u4e3a\u6d4b\u8bd5NR-IQA\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684NR-IQA\u653b\u51fb\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56NR-IQA\u6a21\u578b\u7684\u68af\u5ea6\uff0c\u5bfc\u81f4\u5728\u68af\u5ea6\u4fe1\u606f\u4e0d\u53ef\u7528\u65f6\u53d7\u5230\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9 NR-IQA \u65b9\u6cd5\u7684\u5f00\u521b\u6027\u7684\u57fa\u4e8e\u67e5\u8be2\u7684\u9ed1\u76d2\u653b\u51fb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\\emph{\u5206\u6570\u8fb9\u754c}\u7684\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u5177\u6709\u591a\u4e2a\u5206\u6570\u8fb9\u754c\u7684\u81ea\u9002\u5e94\u8fed\u4ee3\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u6700\u521d\u7684\u653b\u51fb\u65b9\u5411\u4e5f\u88ab\u8bbe\u8ba1\u4e3a\u5229\u7528\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\uff08HVS\uff09\u7684\u7279\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u653b\u51fb\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u6bd4\u8f83\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u8fdc\u8fdc\u9886\u5148\u4e8e\u4ee5\u524d\u7684\u9ed1\u76d2\u65b9\u6cd5\u3002\u6709\u6548\u7684 DBCNN \u6a21\u578b\u53d7\u5230\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u653b\u51fb\uff0cSpearman \u6392\u5e8f\u76f8\u5173\u7cfb\u6570 (SROCC) \u4e0b\u964d\u4e86 0.6972 \u7f8e\u5143\uff0c\u8fd9\u63ed\u793a\u4e86 NR-IQA \u5bb9\u6613\u53d7\u5230\u9ed1\u76d2\u653b\u51fb\u3002\u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u8fd8\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22 NR-IQA \u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002|[2401.05217v1](http://arxiv.org/pdf/2401.05217v1)|null|\n", "2401.05049": "|**2024-01-10**|**Content-Aware Depth-Adaptive Image Restoration**|\u5185\u5bb9\u611f\u77e5\u6df1\u5ea6\u81ea\u9002\u5e94\u56fe\u50cf\u6062\u590d|Tom Richard Vargis, Siavash Ghiasvand|This work prioritizes building a modular pipeline that utilizes existing models to systematically restore images, rather than creating new restoration models from scratch. Restoration is carried out at an object-specific level, with each object regenerated using its corresponding class label information. The approach stands out by providing complete user control over the entire restoration process. Users can select models for specialized restoration steps, customize the sequence of steps to meet their needs, and refine the resulting regenerated image with depth awareness. The research provides two distinct pathways for implementing image regeneration, allowing for a comparison of their respective strengths and limitations. The most compelling aspect of this versatile system is its adaptability. This adaptability enables users to target particular object categories, including medical images, by providing models that are trained on those object classes.|\u8fd9\u9879\u5de5\u4f5c\u4f18\u5148\u8003\u8651\u6784\u5efa\u4e00\u4e2a\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u5229\u7528\u73b0\u6709\u6a21\u578b\u7cfb\u7edf\u5730\u6062\u590d\u56fe\u50cf\uff0c\u800c\u4e0d\u662f\u4ece\u5934\u5f00\u59cb\u521b\u5efa\u65b0\u7684\u6062\u590d\u6a21\u578b\u3002\u6062\u590d\u662f\u5728\u7279\u5b9a\u4e8e\u5bf9\u8c61\u7684\u7ea7\u522b\u8fdb\u884c\u7684\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u4f7f\u7528\u5176\u76f8\u5e94\u7684\u7c7b\u6807\u7b7e\u4fe1\u606f\u91cd\u65b0\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u7684\u7a81\u51fa\u4e4b\u5904\u5728\u4e8e\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u5bf9\u6574\u4e2a\u6062\u590d\u8fc7\u7a0b\u7684\u5b8c\u5168\u63a7\u5236\u3002\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4e13\u95e8\u7684\u6062\u590d\u6b65\u9aa4\u7684\u6a21\u578b\uff0c\u81ea\u5b9a\u4e49\u6b65\u9aa4\u987a\u5e8f\u4ee5\u6ee1\u8db3\u4ed6\u4eec\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6765\u7ec6\u5316\u751f\u6210\u7684\u518d\u751f\u56fe\u50cf\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u5b9e\u73b0\u56fe\u50cf\u518d\u751f\u7684\u9014\u5f84\uff0c\u53ef\u4ee5\u6bd4\u8f83\u5b83\u4eec\u5404\u81ea\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u3002\u8fd9\u4e2a\u591a\u529f\u80fd\u7cfb\u7edf\u6700\u5f15\u4eba\u6ce8\u76ee\u7684\u65b9\u9762\u662f\u5b83\u7684\u9002\u5e94\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u6027\u4f7f\u7528\u6237\u80fd\u591f\u901a\u8fc7\u63d0\u4f9b\u9488\u5bf9\u7279\u5b9a\u5bf9\u8c61\u7c7b\u522b\uff08\u5305\u62ec\u533b\u5b66\u56fe\u50cf\uff09\u8fdb\u884c\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u5b9a\u4f4d\u8fd9\u4e9b\u5bf9\u8c61\u7c7b\u522b\u3002|[2401.05049v1](http://arxiv.org/pdf/2401.05049v1)|null|\n", "2401.05014": "|**2024-01-10**|**Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data**|\u901a\u8fc7\u91ca\u653e\u4efb\u52a1\u65e0\u5173\u6570\u636e\u7684\u6f5c\u529b\u5b9e\u73b0\u65e0\u6e90\u8de8\u6a21\u5f0f\u77e5\u8bc6\u8f6c\u79fb|Jinjing Zhu, Yucheng Chen, Lin Wang|Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns. A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap. However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality. To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer. Our work is buttressed by two key technical components. Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module. It translates the target modality data (e.g., infrared) into the source-like RGB images based on paired TI data and the guidance of the available source model to alleviate two key gaps: 1) inter-modality gap between the paired TI data; 2) intra-modality gap between TI and TR target data. We then propose a Task-irrelevant data-Guided Knowledge Transfer (TGKT) module that transfers knowledge from the source model to the target model by leveraging the paired TI data. Notably, due to the unavailability of labels for the TR target data and its less reliable prediction from the source model, our TGKT model incorporates a self-supervised pseudo-labeling approach to enable the target model to learn from its predictions. Extensive experiments show that our method achieves state-of-the-art performance on three datasets (RGB-to-depth and RGB-to-infrared).|\u65e0\u6e90\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u662f\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u76ee\u7684\u662f\u5c06\u77e5\u8bc6\u4ece\u4e00\u79cd\u6e90\u6a21\u6001\uff08\u4f8b\u5982 RGB\uff09\u8f6c\u79fb\u5230\u76ee\u6807\u6a21\u6001\uff08\u4f8b\u5982\u6df1\u5ea6\u6216\u7ea2\u5916\uff09\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u4efb\u52a1\u76f8\u5173\u7684\uff08TR \uff09\u51fa\u4e8e\u5185\u5b58\u548c\u9690\u79c1\u95ee\u9898\u800c\u6e90\u6570\u636e\u3002\u6700\u8fd1\u7684\u4e00\u9879\u5c1d\u8bd5\u5229\u7528\u914d\u5bf9\u7684\u4efb\u52a1\u65e0\u5173\uff08TI\uff09\u6570\u636e\u5e76\u76f4\u63a5\u5339\u914d\u5176\u4e2d\u7684\u7279\u5f81\u4ee5\u6d88\u9664\u6a21\u6001\u5dee\u8ddd\u3002\u7136\u800c\uff0c\u5b83\u5ffd\u7565\u4e86\u4e00\u4e2a\u5173\u952e\u7ebf\u7d22\uff0c\u5373\u914d\u5bf9\u7684 TI \u6570\u636e\u53ef\u7528\u4e8e\u6709\u6548\u4f30\u8ba1\u6e90\u6570\u636e\u5206\u5e03\u5e76\u66f4\u597d\u5730\u4fc3\u8fdb\u77e5\u8bc6\u5411\u76ee\u6807\u6a21\u6001\u7684\u8fc1\u79fb\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u800c\u7b80\u6d01\u7684\u6846\u67b6\uff0c\u4ee5\u91ca\u653e\u914d\u5bf9 TI \u6570\u636e\u7684\u6f5c\u529b\uff0c\u4ee5\u589e\u5f3a\u65e0\u6e90\u8de8\u6a21\u5f0f\u77e5\u8bc6\u8f6c\u79fb\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7531\u4e24\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u6210\u90e8\u5206\u652f\u6491\u3002\u9996\u5148\uff0c\u4e3a\u4e86\u66f4\u597d\u5730\u4f30\u8ba1\u6e90\u6570\u636e\u5206\u5e03\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4efb\u52a1\u65e0\u5173\u6570\u636e\u5f15\u5bfc\u6a21\u6001\u6865\u63a5\uff08TGMB\uff09\u6a21\u5757\u3002\u5b83\u57fa\u4e8e\u914d\u5bf9 TI \u6570\u636e\u548c\u53ef\u7528\u6e90\u6a21\u578b\u7684\u6307\u5bfc\uff0c\u5c06\u76ee\u6807\u6a21\u6001\u6570\u636e\uff08\u4f8b\u5982\u7ea2\u5916\uff09\u8f6c\u6362\u4e3a\u7c7b\u6e90 RGB \u56fe\u50cf\uff0c\u4ee5\u7f29\u5c0f\u4e24\u4e2a\u5173\u952e\u5dee\u8ddd\uff1a1\uff09\u914d\u5bf9 TI \u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u95f4\u5dee\u8ddd\uff1b 2\uff09TI\u548cTR\u76ee\u6807\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5185\u5dee\u8ddd\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u5f15\u5bfc\u77e5\u8bc6\u8f6c\u79fb\uff08TGKT\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u901a\u8fc7\u5229\u7528\u914d\u5bf9\u7684 TI \u6570\u636e\u5c06\u77e5\u8bc6\u4ece\u6e90\u6a21\u578b\u8f6c\u79fb\u5230\u76ee\u6807\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e TR \u76ee\u6807\u6570\u636e\u7684\u6807\u7b7e\u4e0d\u53ef\u7528\uff0c\u5e76\u4e14\u6e90\u6a21\u578b\u7684\u9884\u6d4b\u4e0d\u592a\u53ef\u9760\uff0c\u6211\u4eec\u7684 TGKT \u6a21\u578b\u91c7\u7528\u4e86\u81ea\u6211\u76d1\u7763\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u4f7f\u76ee\u6807\u6a21\u578b\u80fd\u591f\u4ece\u5176\u9884\u6d4b\u4e2d\u5b66\u4e60\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08RGB \u5230\u6df1\u5ea6\u548c RGB \u5230\u7ea2\u5916\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|[2401.05014v1](http://arxiv.org/pdf/2401.05014v1)|null|\n", "2401.04962": "|**2024-01-10**|**Large Model based Sequential Keyframe Extraction for Video Summarization**|\u57fa\u4e8e\u5927\u578b\u6a21\u578b\u7684\u89c6\u9891\u6458\u8981\u5e8f\u5217\u5173\u952e\u5e27\u63d0\u53d6|Kailong Tan, Yuxiang Zhou, Qianchen Xia, Rui Liu, Yong Chen|Keyframe extraction aims to sum up a video's semantics with the minimum number of its frames. This paper puts forward a Large Model based Sequential Keyframe Extraction for video summarization, dubbed LMSKE, which contains three stages as below. First, we use the large model \"TransNetV21\" to cut the video into consecutive shots, and employ the large model \"CLIP2\" to generate each frame's visual feature within each shot; Second, we develop an adaptive clustering algorithm to yield candidate keyframes for each shot, with each candidate keyframe locating nearest to a cluster center; Third, we further reduce the above candidate keyframes via redundancy elimination within each shot, and finally concatenate them in accordance with the sequence of shots as the final sequential keyframes. To evaluate LMSKE, we curate a benchmark dataset and conduct rich experiments, whose results exhibit that LMSKE performs much better than quite a few SOTA competitors with average F1 of 0.5311, average fidelity of 0.8141, and average compression ratio of 0.9922.|\u5173\u952e\u5e27\u63d0\u53d6\u7684\u76ee\u7684\u662f\u7528\u6700\u5c11\u7684\u5e27\u6570\u603b\u7ed3\u89c6\u9891\u7684\u8bed\u4e49\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u6a21\u578b\u7684\u89c6\u9891\u6458\u8981\u5e8f\u5217\u5173\u952e\u5e27\u63d0\u53d6\uff0c\u79f0\u4e3aLMSKE\uff0c\u5b83\u5305\u542b\u4ee5\u4e0b\u4e09\u4e2a\u9636\u6bb5\u3002\u9996\u5148\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u6a21\u578b\u201cTransNetV21\u201d\u5c06\u89c6\u9891\u5207\u5272\u6210\u8fde\u7eed\u7684\u955c\u5934\uff0c\u5e76\u4f7f\u7528\u5927\u6a21\u578b\u201cCLIP2\u201d\u751f\u6210\u6bcf\u4e2a\u955c\u5934\u5185\u6bcf\u4e00\u5e27\u7684\u89c6\u89c9\u7279\u5f81\uff1b\u5176\u6b21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u805a\u7c7b\u7b97\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u955c\u5934\u751f\u6210\u5019\u9009\u5173\u952e\u5e27\uff0c\u6bcf\u4e2a\u5019\u9009\u5173\u952e\u5e27\u8ddd\u79bb\u805a\u7c7b\u4e2d\u5fc3\u6700\u8fd1\uff1b\u7b2c\u4e09\uff0c\u6211\u4eec\u901a\u8fc7\u6bcf\u4e2a\u955c\u5934\u5185\u7684\u5197\u4f59\u6d88\u9664\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e0a\u8ff0\u5019\u9009\u5173\u952e\u5e27\uff0c\u6700\u540e\u6309\u7167\u955c\u5934\u987a\u5e8f\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\u4f5c\u4e3a\u6700\u7ec8\u7684\u987a\u5e8f\u5173\u952e\u5e27\u3002\u4e3a\u4e86\u8bc4\u4f30 LMSKE\uff0c\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u4e86\u4e30\u5bcc\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e LMSKE \u7684\u5e73\u5747 F1 \u4e3a 0.5311\uff0c\u5e73\u5747\u4fdd\u771f\u5ea6\u4e3a 0.8141\uff0c\u5e73\u5747\u538b\u7f29\u6bd4\u4e3a 0.9922\uff0c\u6bd4\u76f8\u5f53\u591a\u7684 SOTA \u7ade\u4e89\u5bf9\u624b\u8868\u73b0\u5f97\u66f4\u597d\u3002|[2401.04962v1](http://arxiv.org/pdf/2401.04962v1)|null|\n", "2401.04923": "|**2024-01-10**|**Inconsistency-Based Data-Centric Active Open-Set Annotation**|\u57fa\u4e8e\u4e0d\u4e00\u81f4\u6027\u7684\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4e3b\u52a8\u5f00\u653e\u96c6\u6ce8\u91ca|Ruiyu Mao, Ouyang Xu, Yunhui Guo|Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks. However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes. This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem. The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce. To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data. NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data. It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative samples from those classes based on a consistency criterion that measures inconsistencies between model predictions and local feature distribution. Unlike the recently proposed learning-centric method for the same problem, NEAT is much more computationally efficient and is a data-centric active open-set annotation method. Our experiments demonstrate that NEAT achieves significantly better performance than state-of-the-art active learning methods for active open-set annotation.|\u4e3b\u52a8\u5b66\u4e60\u662f\u4e00\u79cd\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6240\u9700\u7684\u6807\u8bb0\u5de5\u4f5c\u3002\u7136\u800c\uff0c\u5f53\u524d\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53d7\u5230\u5176\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\u7684\u9650\u5236\uff0c\u8be5\u5047\u8bbe\u5047\u8bbe\u672a\u6807\u8bb0\u6c60\u4e2d\u7684\u6240\u6709\u6570\u636e\u90fd\u6765\u81ea\u4e00\u7ec4\u9884\u5b9a\u4e49\u7684\u5df2\u77e5\u7c7b\u3002\u8fd9\u79cd\u5047\u8bbe\u5728\u5b9e\u9645\u60c5\u51b5\u4e2d\u901a\u5e38\u662f\u65e0\u6548\u7684\uff0c\u56e0\u4e3a\u672a\u6807\u8bb0\u7684\u6570\u636e\u4e2d\u53ef\u80fd\u5b58\u5728\u672a\u77e5\u7684\u7c7b\uff0c\u4ece\u800c\u5bfc\u81f4\u4e3b\u52a8\u5f00\u653e\u96c6\u6ce8\u91ca\u95ee\u9898\u3002\u7531\u4e8e\u5f15\u5165\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6570\u636e\u4e2d\u672a\u77e5\u7c7b\u7684\u5b58\u5728\u53ef\u80fd\u4f1a\u663e\u7740\u5f71\u54cd\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a NEAT \u7684\u65b0\u578b\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u4e3b\u52a8\u6ce8\u91ca\u5f00\u653e\u96c6\u6570\u636e\u3002 NEAT \u65e8\u5728\u6807\u8bb0\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u672a\u6807\u8bb0\u6570\u636e\u6c60\u4e2d\u7684\u5df2\u77e5\u7c7b\u6570\u636e\u3002\u5b83\u5229\u7528\u6807\u7b7e\u7684\u53ef\u805a\u7c7b\u6027\u4ece\u672a\u6807\u8bb0\u7684\u6c60\u4e2d\u8bc6\u522b\u5df2\u77e5\u7c7b\u522b\uff0c\u5e76\u6839\u636e\u8861\u91cf\u6a21\u578b\u9884\u6d4b\u548c\u5c40\u90e8\u7279\u5f81\u5206\u5e03\u4e4b\u95f4\u4e0d\u4e00\u81f4\u7684\u4e00\u81f4\u6027\u6807\u51c6\u4ece\u8fd9\u4e9b\u7c7b\u522b\u4e2d\u9009\u62e9\u4fe1\u606f\u6837\u672c\u3002\u4e0e\u6700\u8fd1\u63d0\u51fa\u7684\u9488\u5bf9\u540c\u4e00\u95ee\u9898\u7684\u4ee5\u5b66\u4e60\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cNEAT \u7684\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u4e14\u662f\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4e3b\u52a8\u5f00\u653e\u96c6\u6ce8\u91ca\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u4e3b\u52a8\u5f00\u653e\u96c6\u6ce8\u91ca\uff0cNEAT \u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002|[2401.04923v1](http://arxiv.org/pdf/2401.04923v1)|null|\n"}}