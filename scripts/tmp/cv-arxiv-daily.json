{"\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2401.13666": "|**2024-01-24**|**Algebraic methods for solving recognition problems with non-crossing classes**|\u89e3\u51b3\u975e\u4ea4\u53c9\u7c7b\u8bc6\u522b\u95ee\u9898\u7684\u4ee3\u6570\u65b9\u6cd5|Anvar Kabulov, Alimdzhan Babadzhanov, Islambek Saymanov|In this paper, we propose to consider various models of pattern recognition. At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule. Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created. An upper estimate is constructed for the model, which guarantees the completeness of the extension.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u8003\u8651\u5404\u79cd\u6a21\u5f0f\u8bc6\u522b\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5efa\u8bae\u8003\u8651\u4e24\u4e2a\u7b97\u5b50\u5f62\u5f0f\u7684\u6a21\u578b\uff1a\u8bc6\u522b\u7b97\u5b50\u548c\u51b3\u7b56\u89c4\u5219\u3002\u5f15\u5165\u4ee3\u6570\u8fd0\u7b97\u6765\u8bc6\u522b\u7b97\u5b50\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u7b97\u5b50\u7684\u5e94\u7528\uff0c\u521b\u5efa\u4e86\u4e00\u7cfb\u5217\u8bc6\u522b\u7b97\u6cd5\u3002\u4e3a\u6a21\u578b\u6784\u5efa\u4e86\u4e0a\u9650\u4f30\u8ba1\uff0c\u4fdd\u8bc1\u4e86\u6269\u5c55\u7684\u5b8c\u6574\u6027\u3002|[2401.13666v1](http://arxiv.org/pdf/2401.13666v1)|null|\n", "2401.13650": "|**2024-01-24**|**Tyche: Stochastic In-Context Learning for Medical Image Segmentation**|Tyche\uff1a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u968f\u673a\u4e0a\u4e0b\u6587\u5b66\u4e60|Marianne Rakic, Hallee E. Wong, Jose Javier Gonzalez Ortiz, Beth Cimini, John Guttag, Adrian V. Dalca|Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.|\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u89e3\u51b3\u65b9\u6848\u6709\u4e24\u4e2a\u91cd\u8981\u7684\u7f3a\u70b9\u3002\u9996\u5148\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u65b0\u7684\u5206\u5272\u4efb\u52a1\uff0c\u5fc5\u987b\u8bad\u7ec3\u6216\u5fae\u8c03\u65b0\u6a21\u578b\u3002\u8fd9\u9700\u8981\u5927\u91cf\u7684\u8d44\u6e90\u548c\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u5bf9\u4e8e\u533b\u5b66\u7814\u7a76\u4eba\u5458\u548c\u4e34\u5e8a\u533b\u751f\u6765\u8bf4\u901a\u5e38\u662f\u4e0d\u53ef\u884c\u7684\u3002\u5176\u6b21\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u5206\u5272\u65b9\u6cd5\u4e3a\u7ed9\u5b9a\u56fe\u50cf\u751f\u6210\u5355\u4e2a\u786e\u5b9a\u6027\u5206\u5272\u63a9\u6a21\u3002\u7136\u800c\u5728\u5b9e\u8df5\u4e2d\uff0c\u5173\u4e8e\u4ec0\u4e48\u6784\u6210\u6b63\u786e\u7684\u5206\u5272\u901a\u5e38\u5b58\u5728\u76f8\u5f53\u5927\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e14\u4e0d\u540c\u7684\u4e13\u5bb6\u6ce8\u91ca\u8005\u901a\u5e38\u4f1a\u5bf9\u540c\u4e00\u56fe\u50cf\u8fdb\u884c\u4e0d\u540c\u7684\u5206\u5272\u3002\u6211\u4eec\u7528 Tyche \u89e3\u51b3\u4e86\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u4e0a\u4e0b\u6587\u96c6\u4e3a\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u751f\u6210\u968f\u673a\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002 Tyche \u5728\u4e24\u4e2a\u91cd\u8981\u65b9\u9762\u4e0d\u540c\u4e8e\u5176\u4ed6\u4e0a\u4e0b\u6587\u5206\u5272\u65b9\u6cd5\u3002 (1)\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5377\u79ef\u5757\u67b6\u6784\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9884\u6d4b\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002 (2)\u6211\u4eec\u5f15\u5165\u4e86\u4e0a\u4e0b\u6587\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\uff0c\u8fd9\u662f\u4e00\u79cd\u63d0\u4f9b\u9884\u6d4b\u968f\u673a\u6027\u7684\u65b0\u673a\u5236\u3002\u5f53\u4e0e\u9002\u5f53\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\u76f8\u7ed3\u5408\u65f6\uff0cTyche \u53ef\u4ee5\u4e3a\u65b0\u7684\u6216\u672a\u89c1\u8fc7\u7684\u533b\u5b66\u56fe\u50cf\u548c\u5206\u5272\u4efb\u52a1\u9884\u6d4b\u4e00\u7ec4\u770b\u4f3c\u5408\u7406\u7684\u591a\u6837\u5316\u5206\u5272\u5019\u9009\u8005\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002|[2401.13650v1](http://arxiv.org/pdf/2401.13650v1)|null|\n", "2401.13641": "|**2024-01-24**|**How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability**|ChatGPT \u5728\u4eba\u8138\u751f\u7269\u8bc6\u522b\u65b9\u9762\u6709\u591a\u51fa\u8272\uff1f\u521d\u6b65\u63a2\u8ba8\u8bc6\u522b\u3001\u8f6f\u751f\u7269\u8bc6\u522b\u6280\u672f\u548c\u53ef\u89e3\u91ca\u6027|Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia|Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).   The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub.|OpenAI \u5f00\u53d1\u7684 GPT \u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u7ecf\u663e\u793a\u51fa\u60ca\u4eba\u7684\u7ed3\u679c\uff0c\u7ed9\u6211\u4eec\u7684\u793e\u4f1a\u5e26\u6765\u4e86\u5feb\u901f\u7684\u53d8\u5316\u3002 ChatGPT \u7684\u53d1\u5e03\u8fdb\u4e00\u6b65\u5f3a\u5316\u4e86\u8fd9\u4e00\u70b9\uff0c\u5b83\u5141\u8bb8\u4efb\u4f55\u4eba\u4ee5\u7b80\u5355\u7684\u5bf9\u8bdd\u65b9\u5f0f\u4e0e\u6cd5\u5b66\u7855\u58eb\u8fdb\u884c\u4ea4\u4e92\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u8be5\u9886\u57df\u7684\u7ecf\u9a8c\u3002\u56e0\u6b64\uff0cChatGPT \u5df2\u8fc5\u901f\u5e94\u7528\u4e8e\u8bb8\u591a\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u4ee3\u7801\u548c\u6b4c\u66f2\u7f16\u5199\u3001\u6559\u80b2\u3001\u865a\u62df\u52a9\u7406\u7b49\uff0c\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u4efb\u52a1\uff08\u96f6\u6837\u672c\u5b66\u4e60\uff09\u4e2d\u663e\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u57fa\u4e8e\u6700\u8fd1\u7684 GPT-4 \u591a\u6a21\u6001 LLM \u6765\u63a2\u7d22 ChatGPT \u5728\u4eba\u8138\u751f\u7269\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u6211\u4eec\u7279\u522b\u5206\u6790\u4e86 ChatGPT \u6267\u884c\u9762\u90e8\u9a8c\u8bc1\u3001\u8f6f\u751f\u7269\u8bc6\u522b\u4f30\u8ba1\u548c\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u7b49\u4efb\u52a1\u7684\u80fd\u529b\u3002 ChatGPT \u5bf9\u4e8e\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4eba\u7c7b\u573a\u666f\u4e2d\u81ea\u52a8\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u975e\u5e38\u6709\u4ef7\u503c\u3002\u8fdb\u884c\u5b9e\u9a8c\u662f\u4e3a\u4e86\u8bc4\u4f30 ChatGPT \u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4f7f\u7528\u6d41\u884c\u7684\u516c\u5171\u57fa\u51c6\u5e76\u5c06\u7ed3\u679c\u4e0e\u200b\u200b\u8be5\u9886\u57df\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u672c\u7814\u7a76\u53d6\u5f97\u7684\u7ed3\u679c\u663e\u793a\u4e86 ChatGPT \u7b49\u6cd5\u5b66\u7855\u58eb\u5728\u4eba\u8138\u751f\u7269\u8bc6\u522b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u3002\u51fa\u4e8e\u53ef\u91cd\u590d\u6027\u7684\u539f\u56e0\uff0c\u6211\u4eec\u5728 GitHub \u4e2d\u53d1\u5e03\u4e86\u6240\u6709\u4ee3\u7801\u3002|[2401.13641v1](http://arxiv.org/pdf/2401.13641v1)|null|\n", "2401.13613": "|**2024-01-24**|**Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode**|\u589e\u5f3a\u56fe\u50cf\u68c0\u7d22:\u4f7f\u7528CLIP\u6a21\u5f0f\u8fdb\u884c\u7167\u7247\u641c\u7d22\u7684\u7efc\u5408\u7814\u7a76|Naresh Kumar Lahajal, Harini S|Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications|\u7167\u7247\u641c\u7d22\u662f\u57fa\u4e8e\u6587\u672c\u67e5\u8be2\u68c0\u7d22\u56fe\u50cf\u7684\u4efb\u52a1\uff0c\u968f\u7740 CLIP\uff08\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff09\u6a21\u578b\u7684\u5f15\u5165\uff0c\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002 CLIP \u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b66\u4e60\u56fe\u50cf\u548c\u6587\u672c\u7684\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u6a21\u5f0f\u7406\u89e3\u3002\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u7406\u89e3\u4e0d\u540c\u56fe\u50cf\u548c\u6587\u672c\u5bf9\u4e4b\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u5141\u8bb8\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u7d22\u56fe\u50cf\u3002\u901a\u8fc7\u5bf9\u5305\u542b\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6587\u672c\u63cf\u8ff0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0cCLIP \u5b9e\u73b0\u4e86\u663e\u7740\u7684\u6cdb\u5316\uff0c\u4e3a\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5206\u7c7b\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002\u672c\u6458\u8981\u603b\u7ed3\u4e86 CLIP \u7684\u57fa\u672c\u539f\u7406\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u5bf9\u63a8\u8fdb\u7167\u7247\u641c\u7d22\u9886\u57df\u3001\u4fc3\u8fdb\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u65e0\u7f1d\u96c6\u6210\u4ee5\u6539\u8fdb\u591a\u5a92\u4f53\u5e94\u7528\u4e2d\u7684\u4fe1\u606f\u68c0\u7d22\u7684\u6f5c\u5728\u5f71\u54cd|[2401.13613v1](http://arxiv.org/pdf/2401.13613v1)|null|\n", "2401.13596": "|**2024-01-24**|**PLATE: A perception-latency aware estimator,**|PLATE\uff1a\u611f\u77e5\u5ef6\u8fdf\u611f\u77e5\u4f30\u8ba1\u5668\uff0c|Rodrigo Aldana-L\u00f3pez, Rosario Arag\u00fc\u00e9s, Carlos Sag\u00fc\u00e9s|Target tracking is a popular problem with many potential applications. There has been a lot of effort on improving the quality of the detection of targets using cameras through different techniques. In general, with higher computational effort applied, i.e., a longer perception-latency, a better detection accuracy is obtained. However, it is not always useful to apply the longest perception-latency allowed, particularly when the environment doesn't require to and when the computational resources are shared between other tasks. In this work, we propose a new Perception-LATency aware Estimator (PLATE), which uses different perception configurations in different moments of time in order to optimize a certain performance measure. This measure takes into account a perception-latency and accuracy trade-off aiming for a good compromise between quality and resource usage. Compared to other heuristic frame-skipping techniques, PLATE comes with a formal complexity and optimality analysis. The advantages of PLATE are verified by several experiments including an evaluation over a standard benchmark with real data and using state of the art deep learning object detection methods for the perception stage.|\u76ee\u6807\u8ddf\u8e2a\u662f\u8bb8\u591a\u6f5c\u5728\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u666e\u904d\u95ee\u9898\u3002\u4eba\u4eec\u5728\u901a\u8fc7\u4e0d\u540c\u6280\u672f\u63d0\u9ad8\u76f8\u673a\u76ee\u6807\u68c0\u6d4b\u8d28\u91cf\u65b9\u9762\u505a\u51fa\u4e86\u5f88\u591a\u52aa\u529b\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u5e94\u7528\u66f4\u9ad8\u7684\u8ba1\u7b97\u91cf\uff0c\u5373\u66f4\u957f\u7684\u611f\u77e5\u5ef6\u8fdf\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u5e94\u7528\u5141\u8bb8\u7684\u6700\u957f\u611f\u77e5\u5ef6\u8fdf\u5e76\u4e0d\u603b\u662f\u6709\u7528\u7684\uff0c\u7279\u522b\u662f\u5f53\u73af\u5883\u4e0d\u9700\u8981\u5e76\u4e14\u8ba1\u7b97\u8d44\u6e90\u5728\u5176\u4ed6\u4efb\u52a1\u4e4b\u95f4\u5171\u4eab\u65f6\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u611f\u77e5\u5ef6\u8fdf\u611f\u77e5\u4f30\u8ba1\u5668\uff08PLATE\uff09\uff0c\u5b83\u5728\u4e0d\u540c\u65f6\u523b\u4f7f\u7528\u4e0d\u540c\u7684\u611f\u77e5\u914d\u7f6e\u6765\u4f18\u5316\u7279\u5b9a\u7684\u6027\u80fd\u6307\u6807\u3002\u8be5\u63aa\u65bd\u8003\u8651\u4e86\u611f\u77e5\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u7684\u6743\u8861\uff0c\u65e8\u5728\u5728\u8d28\u91cf\u548c\u8d44\u6e90\u4f7f\u7528\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u6298\u8877\u3002\u4e0e\u5176\u4ed6\u542f\u53d1\u5f0f\u8df3\u5e27\u6280\u672f\u76f8\u6bd4\uff0cPLATE \u5177\u6709\u5f62\u5f0f\u590d\u6742\u6027\u548c\u6700\u4f18\u6027\u5206\u6790\u3002 PLATE \u7684\u4f18\u52bf\u5df2\u901a\u8fc7\u591a\u9879\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\uff0c\u5305\u62ec\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u5bf9\u6807\u51c6\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u53ca\u5728\u611f\u77e5\u9636\u6bb5\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u3002|[2401.13596v1](http://arxiv.org/pdf/2401.13596v1)|null|\n", "2401.13560": "|**2024-01-24**|**SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation**|SegMamba\uff1a\u7528\u4e8e 3D \u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8fdc\u7a0b\u987a\u5e8f\u5efa\u6a21 Mamba|Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu|The Transformer architecture has shown a remarkable ability in modeling global relationships. However, it poses a significant computational challenge when processing high-dimensional medical images. This hinders its development and widespread adoption in this task. Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed. Inspired by its success, we introduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation \\textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale. Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba. The code for SegMamba is available at: https://github.com/ge-xing/SegMamba|Transformer \u67b6\u6784\u5728\u5efa\u6a21\u5168\u5c40\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u9ad8\u7ef4\u533b\u5b66\u56fe\u50cf\u65f6\uff0c\u5b83\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002\u8fd9\u963b\u788d\u4e86\u5b83\u5728\u8fd9\u9879\u4efb\u52a1\u4e2d\u7684\u53d1\u5c55\u548c\u5e7f\u6cdb\u91c7\u7528\u3002 Mamba \u4f5c\u4e3a\u4e00\u79cd\u72b6\u6001\u7a7a\u95f4\u6a21\u578b (SSM)\uff0c\u6700\u8fd1\u6210\u4e3a\u987a\u5e8f\u5efa\u6a21\u4e2d\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u7684\u4e00\u79cd\u8457\u540d\u65b9\u5f0f\uff0c\u4ee5\u5176\u5353\u8d8a\u7684\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u901f\u5ea6\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002\u53d7\u5176\u6210\u529f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 SegMamba\uff0c\u4e00\u79cd\u65b0\u9896\u7684 3D \u533b\u5b66\u56fe\u50cf \\textbf{Seg}mentation \\textbf{Mamba} \u6a21\u578b\uff0c\u65e8\u5728\u6709\u6548\u6355\u83b7\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u6574\u4e2a\u4f53\u79ef\u7279\u5f81\u4e2d\u7684\u8fdc\u7a0b\u4f9d\u8d56\u6027\u3002\u4e0e\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 SegMamba \u4ece\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u6574\u4e2a\u4f53\u79ef\u7279\u5f81\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u4f53\u79ef\u7279\u5f81\u7684\u5206\u8fa8\u7387\u4e3a {$64\\times 64\\times 64$}\uff0c\u4e5f\u80fd\u4fdd\u6301\u5353\u8d8a\u7684\u5904\u7406\u901f\u5ea6\u3002 BraTS2023 \u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 SegMamba \u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002 SegMamba \u7684\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/ge-xing/SegMamba|[2401.13560v1](http://arxiv.org/pdf/2401.13560v1)|**[link](https://github.com/ge-xing/segmamba)**|\n", "2401.13554": "|**2024-01-24**|**PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition**|PanAf20K\uff1a\u7528\u4e8e\u91ce\u751f\u733f\u68c0\u6d4b\u548c\u884c\u4e3a\u8bc6\u522b\u7684\u5927\u578b\u89c6\u9891\u6570\u636e\u96c6|Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel Angedakin, Katherine Corogenes, Dervla Dowd, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, et.al.|We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 18 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts.|\u6211\u4eec\u63d0\u51fa\u4e86 PanAf20K \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u81ea\u7136\u73af\u5883\u4e2d\u7c7b\u4eba\u733f\u6700\u5927\u3001\u6700\u591a\u6837\u5316\u7684\u5f00\u653e\u83b7\u53d6\u5e26\u6ce8\u91ca\u89c6\u9891\u6570\u636e\u96c6\u3002\u5b83\u5305\u542b\u8d85\u8fc7 700 \u4e07\u5e27\u7684\u7ea6 20,000 \u4e2a\u9ed1\u7329\u7329\u548c\u5927\u7329\u7329\u7684\u76f8\u673a\u9677\u9631\u89c6\u9891\uff0c\u8fd9\u4e9b\u89c6\u9891\u662f\u5728\u70ed\u5e26\u975e\u6d32\u7684 18 \u4e2a\u91ce\u5916\u5730\u70b9\u6536\u96c6\u7684\uff0c\u662f\u6cdb\u975e\u8ba1\u5212\uff1a\u517b\u6b96\u9ed1\u7329\u7329\u7684\u4e00\u90e8\u5206\u3002\u8be5\u955c\u5934\u9644\u6709\u4e30\u5bcc\u7684\u6ce8\u91ca\u548c\u57fa\u51c6\uff0c\u4f7f\u5176\u9002\u5408\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u4e14\u5bf9\u751f\u6001\u91cd\u8981\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5305\u62ec\u733f\u68c0\u6d4b\u548c\u884c\u4e3a\u8bc6\u522b\u3002\u9274\u4e8e\u56fd\u9645\u81ea\u7136\u4fdd\u62a4\u8054\u76df\u73b0\u5df2\u5c06\u7c7b\u4eba\u733f\u5bb6\u65cf\u7684\u6240\u6709\u7269\u79cd\u5217\u4e3a\u6fd2\u5371\u6216\u6781\u5ea6\u6fd2\u5371\uff0c\u8fdb\u4e00\u6b65\u5bf9\u76f8\u673a\u9677\u9631\u4fe1\u606f\u8fdb\u884c\u4eba\u5de5\u667a\u80fd\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u5e0c\u671b\u8be5\u6570\u636e\u96c6\u80fd\u4e3a\u4eba\u5de5\u667a\u80fd\u793e\u533a\u7684\u53c2\u4e0e\u5960\u5b9a\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3001\u6548\u7387\u548c\u7ed3\u679c\u89e3\u91ca\uff0c\u4ee5\u652f\u6301\u5bf9\u7c7b\u4eba\u733f\u7684\u5b58\u5728\u3001\u4e30\u5ea6\u3001\u5206\u5e03\u548c\u884c\u4e3a\u7684\u8bc4\u4f30\uff0c\u4ece\u800c\u5e2e\u52a9\u4fdd\u62a4\u5de5\u4f5c\u3002|[2401.13554v1](http://arxiv.org/pdf/2401.13554v1)|null|\n", "2401.13551": "|**2024-01-24**|**Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection**|\u5c06\u4e00\u7c7b\u6a21\u578b\u548c\u5f31\u76d1\u7763\u6a21\u578b\u4e0e\u81ea\u9002\u5e94\u9608\u503c\u4ea4\u9519\u8fdb\u884c\u65e0\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b|Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai|Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.|\u5982\u679c\u6ca1\u6709\u4eba\u5de5\u6ce8\u91ca\uff0c\u5178\u578b\u7684\u65e0\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08UVAD\uff09\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u4e24\u4e2a\u6a21\u578b\uff0c\u4e3a\u5f7c\u6b64\u751f\u6210\u4f2a\u6807\u7b7e\u3002\u5728\u4e4b\u524d\u7684\u5de5\u4f5c\u4e2d\uff0c\u8fd9\u4e24\u4e2a\u6a21\u578b\u5f7c\u6b64\u7d27\u5bc6\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5e76\u4e14\u4e0d\u77e5\u9053\u5982\u4f55\u5728\u4e0d\u663e\u7740\u4fee\u6539\u5176\u8bad\u7ec3\u6846\u67b6\u7684\u60c5\u51b5\u4e0b\u5347\u7ea7\u5176\u65b9\u6cd5\u3002\u5176\u6b21\uff0c\u4ee5\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u9608\u503c\u6765\u83b7\u5f97\u4f2a\u6807\u7b7e\uff0c\u4f46\u7528\u6237\u6307\u5b9a\u7684\u9608\u503c\u5e76\u4e0d\u53ef\u9760\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8bef\u5dee\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u9519\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ea4\u66ff\u8bad\u7ec3 UVAD \u7684\u5355\u7c7b\u5206\u7c7b\uff08OCC\uff09\u6a21\u578b\u548c\u5f31\u76d1\u7763\uff08WS\uff09\u6a21\u578b\u3002\u6211\u4eec\u65b9\u6cd5\u4e2d\u7684OCC\u6216WS\u6a21\u578b\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u66ff\u6362\u4e3a\u5176\u4ed6OCC\u6216WS\u6a21\u578b\uff0c\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u968f\u7740\u8fd9\u4e24\u4e2a\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u800c\u5347\u7ea7\u3002\u4e3a\u4e86\u5904\u7406\u56fa\u5b9a\u9608\u503c\u95ee\u9898\uff0c\u6211\u4eec\u7a81\u7834\u4e86\u4f20\u7edf\u7684\u8ba4\u77e5\u8fb9\u754c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u5728\u6b63\u5e38\u548c\u5f02\u5e38\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u52a0\u6743 OCC \u6a21\u578b\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\uff0c\u7528\u4e8e\u4ee5\u5bbd\u677e\u5230\u4e25\u683c\u7684\u65b9\u5f0f\u81ea\u52a8\u627e\u5230 WS \u6a21\u578b\u7684\u6700\u4f73\u9608\u503c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 UVAD \u65b9\u6cd5\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002|[2401.13551v1](http://arxiv.org/pdf/2401.13551v1)|null|\n", "2401.13531": "|**2024-01-24**|**QAGait: Revisit Gait Recognition from a Quality Perspective**|QGait\uff1a\u4ece\u8d28\u91cf\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6b65\u6001\u8bc6\u522b|Zengbin Wang, Saihui Hou, Man Zhang, Xu Liu, Chunshui Cao, Yongzhen Huang, Peipei Li, Shibiao Xu|Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait.|\u6b65\u6001\u8bc6\u522b\u662f\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u751f\u7269\u8bc6\u522b\u65b9\u6cd5\uff0c\u65e8\u5728\u6839\u636e\u884c\u4eba\u72ec\u7279\u7684\u884c\u8d70\u6a21\u5f0f\u6765\u8bc6\u522b\u884c\u4eba\u3002 Silhouette \u6a21\u6001\u4ee5\u5176\u6613\u4e8e\u83b7\u53d6\u3001\u7ed3\u6784\u7b80\u5355\u3001\u7a00\u758f\u8868\u793a\u548c\u65b9\u4fbf\u5efa\u6a21\u800c\u95fb\u540d\uff0c\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u7814\u7a76\u4e2d\u3002\u7136\u800c\uff0c\u968f\u7740\u6b65\u6001\u8bc6\u522b\u4ece\u5b9e\u9a8c\u5ba4\u573a\u666f\u8fc5\u901f\u53d1\u5c55\u5230\u91ce\u5916\u573a\u666f\uff0c\u5404\u79cd\u6761\u4ef6\u5bf9\u8f6e\u5ed3\u6a21\u6001\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5305\u62ec1\uff09\u65e0\u6cd5\u8bc6\u522b\u7684\u4f4e\u8d28\u91cf\u8f6e\u5ed3\uff08\u5f02\u5e38\u5206\u5272\u3001\u4e25\u91cd\u906e\u6321\uff0c\u751a\u81f3\u975e\u4eba\u7c7b\u8f6e\u5ed3\uff09\u5f62\u72b6\uff09\uff0c2\uff09\u53ef\u8bc6\u522b\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u8f6e\u5ed3\uff08\u80cc\u666f\u566a\u97f3\u3001\u975e\u6807\u51c6\u59ff\u52bf\u3001\u8f7b\u5fae\u906e\u6321\uff09\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u6b65\u6001\u8bc6\u522b\u6d41\u7a0b\uff0c\u5e76\u4ece\u8d28\u91cf\u89d2\u5ea6\u5904\u7406\u6b65\u6001\u8bc6\u522b\uff0c\u5373 QAGait\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\uff0c\u5305\u62ec\u6700\u5927\u8fde\u63a5\u9762\u79ef\u548c\u6a21\u677f\u5339\u914d\u4ee5\u6d88\u9664\u80cc\u666f\u566a\u97f3\u548c\u65e0\u6cd5\u8bc6\u522b\u7684\u8f6e\u5ed3\uff0c\u5bf9\u9f50\u7b56\u7565\u4ee5\u5904\u7406\u975e\u6807\u51c6\u59ff\u52bf\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e24\u4e2a\u8d28\u91cf\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u5c06\u8f6e\u5ed3\u8d28\u91cf\u96c6\u6210\u5230\u5d4c\u5165\u7a7a\u95f4\u5185\u7684\u4f18\u5316\u4e2d\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u6211\u4eec\u7684 QGait \u53ef\u4ee5\u4fdd\u8bc1\u6b65\u6001\u53ef\u9760\u6027\u548c\u6027\u80fd\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u6b65\u6001\u6570\u636e\u96c6\u65e0\u7f1d\u96c6\u6210\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u4f18\u52bf\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/wzb-bupt/QAGait \u83b7\u53d6\u3002|[2401.13531v1](http://arxiv.org/pdf/2401.13531v1)|**[link](https://github.com/wzb-bupt/qagait)**|\n", "2401.13516": "|**2024-01-24**|**Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces**|Delocate\uff1a\u5177\u6709\u968f\u673a\u4f4d\u7f6e\u7684\u7be1\u6539\u75d5\u8ff9\u7684 Deepfake \u89c6\u9891\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d|Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou|Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames. Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages named recoveringand localization. In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces. Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance.|Deepfake \u89c6\u9891\u53d8\u5f97\u8d8a\u6765\u8d8a\u771f\u5b9e\uff0c\u5728\u4e0d\u540c\u5e27\u4e4b\u95f4\u7684\u9762\u90e8\u533a\u57df\u663e\u793a\u51fa\u5fae\u5999\u7684\u7be1\u6539\u75d5\u8ff9\u3002\u56e0\u6b64\uff0c\u8bb8\u591a\u73b0\u6709\u7684 Deepfake \u68c0\u6d4b\u65b9\u6cd5\u5f88\u96be\u68c0\u6d4b\u672a\u77e5\u9886\u57df\u7684 Deepfake \u89c6\u9891\uff0c\u540c\u65f6\u51c6\u786e\u5b9a\u4f4d\u88ab\u7be1\u6539\u7684\u533a\u57df\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Delocate\uff0c\u4e00\u79cd\u65b0\u9896\u7684 Deepfake \u68c0\u6d4b\u6a21\u578b\uff0c\u53ef\u4ee5\u8bc6\u522b\u548c\u5b9a\u4f4d\u672a\u77e5\u9886\u57df\u7684 Deepfake \u89c6\u9891\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u6062\u590d\u548c\u5b9a\u4f4d\u3002\u5728\u6062\u590d\u9636\u6bb5\uff0c\u6a21\u578b\u968f\u673a\u5c4f\u853d\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u5e76\u5728\u4e0d\u7be1\u6539\u75d5\u8ff9\u7684\u60c5\u51b5\u4e0b\u91cd\u5efa\u771f\u5b9e\u4eba\u8138\uff0c\u5bfc\u81f4\u771f\u5b9e\u4eba\u8138\u6062\u590d\u6548\u679c\u76f8\u5bf9\u8f83\u597d\uff0c\u800c\u5bf9\u4e8e\u5047\u4eba\u8138\u6062\u590d\u6548\u679c\u8f83\u5dee\u3002\u5728\u5b9a\u4f4d\u9636\u6bb5\uff0c\u6062\u590d\u9636\u6bb5\u7684\u8f93\u51fa\u548c\u4f2a\u9020\u5730\u9762\u771f\u503c\u63a9\u6a21\u4f5c\u4e3a\u76d1\u7763\u6765\u6307\u5bfc\u4f2a\u9020\u5b9a\u4f4d\u8fc7\u7a0b\u200b\u200b\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u7b56\u7565\u6027\u5730\u5f3a\u8c03\u4e86\u6062\u590d\u8f83\u5dee\u7684\u5047\u8138\u7684\u6062\u590d\u9636\u6bb5\uff0c\u6709\u5229\u4e8e\u88ab\u7be1\u6539\u533a\u57df\u7684\u672c\u5730\u5316\u3002\u6211\u4eec\u5bf9\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDelocate \u4e0d\u4ec5\u5728\u5b9a\u4f4d\u7be1\u6539\u533a\u57df\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u4e14\u8fd8\u589e\u5f3a\u4e86\u8de8\u57df\u68c0\u6d4b\u6027\u80fd\u3002|[2401.13516v1](http://arxiv.org/pdf/2401.13516v1)|null|\n", "2401.13511": "|**2024-01-24**|**Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images**|\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u4e2d\u7684\u7ec4\u7ec7\u6a2a\u622a\u9762\u548c\u7b14\u6807\u8bb0\u5206\u5272|Ruben T. Lucassen, Willeke A. M. Blokx, Mitko Veta|Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram. On an independent test set, the model achieved a mean Dice score of 0.981$\\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\\pm$0.090 for pen marking segmentation. The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\\pm$0.350. Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts. The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter.|\u7ec4\u7ec7\u5206\u5272\u662f\u4e00\u79cd\u5e38\u89c4\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u53ef\u901a\u8fc7\u6392\u9664\u80cc\u666f\u533a\u57df\u6765\u964d\u4f4e\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf (WSI) \u5206\u6790\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4f20\u7edf\u7684\u56fe\u50cf\u5904\u7406\u6280\u672f\u901a\u5e38\u7528\u4e8e\u7ec4\u7ec7\u5206\u5272\uff0c\u4f46\u901a\u5e38\u9700\u8981\u624b\u52a8\u8c03\u6574\u975e\u5178\u578b\u60c5\u51b5\u7684\u53c2\u6570\u503c\uff0c\u65e0\u6cd5\u4ece\u80cc\u666f\u4e2d\u6392\u9664\u6240\u6709\u5e7b\u706f\u7247\u548c\u626b\u63cf\u4f2a\u5f71\uff0c\u5e76\u4e14\u65e0\u6cd5\u5206\u5272\u8102\u80aa\u7ec4\u7ec7\u3002\u5982\u679c\u4e0d\u6d88\u9664\u7b14\u6807\u8bb0\u4f2a\u5f71\uff0c\u5c24\u5176\u53ef\u80fd\u6210\u4e3a\u540e\u7eed\u5206\u6790\u7684\u6f5c\u5728\u504f\u5dee\u6765\u6e90\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u5e94\u7528\u9700\u8981\u5206\u79bb\u5404\u4e2a\u6a2a\u622a\u9762\uff0c\u7531\u4e8e\u7ec4\u7ec7\u788e\u7247\u548c\u76f8\u90bb\u5b9a\u4f4d\uff0c\u8fd9\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u4f7f\u7528 200 \u4e2a H&E \u67d3\u8272 WSI \u7684\u6570\u636e\u96c6\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u7ec4\u7ec7\u548c\u7b14\u6807\u8bb0\u5206\u5272\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u4e3a\u4e86\u5206\u79bb\u7ec4\u7ec7\u6a2a\u622a\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u4e8c\u7ef4\u76f4\u65b9\u56fe\u4e2d\u6a2a\u622a\u9762\u7684\u9884\u6d4b\u8d28\u5fc3\u4f4d\u7f6e\u8fdb\u884c\u805a\u7c7b\u7684\u65b0\u9896\u540e\u5904\u7406\u65b9\u6cd5\u3002\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5728\u7ec4\u7ec7\u5206\u5272\u65b9\u9762\u7684\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.981$\\pm$0.033\uff0c\u5728\u7b14\u6807\u8bb0\u5206\u5272\u65b9\u9762\u7684\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.912$\\pm$0.090\u3002\u5e26\u6ce8\u91ca\u548c\u5206\u79bb\u7684\u6a2a\u622a\u9762\u6570\u91cf\u4e4b\u95f4\u7684\u5e73\u5747\u7edd\u5bf9\u5dee\u4e3a 0.075$\\pm$0.350\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u51c6\u786e\u5730\u5206\u5272 WSI \u4e2d\u7684 H&E \u67d3\u8272\u7ec4\u7ec7\u6a2a\u622a\u9762\u548c\u7b14\u6807\u8bb0\uff0c\u540c\u65f6\u5bf9\u8bb8\u591a\u5e38\u89c1\u7684\u8f7d\u73bb\u7247\u548c\u626b\u63cf\u4f2a\u5f71\u5177\u6709\u9c81\u68d2\u6027\u3002\u5177\u6709\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u53c2\u6570\u548c\u540e\u5904\u7406\u65b9\u6cd5\u7684\u6a21\u578b\u4f5c\u4e3a\u540d\u4e3a SlideSegmenter \u7684 Python \u5305\u516c\u5f00\u63d0\u4f9b\u3002|[2401.13511v1](http://arxiv.org/pdf/2401.13511v1)|null|\n", "2401.13504": "|**2024-01-24**|**Research about the Ability of LLM in the Tamper-Detection Area**|\u6cd5\u5b66\u7855\u58eb\u5728\u7be1\u6539\u68c0\u6d4b\u9886\u57df\u7684\u80fd\u529b\u7814\u7a76|Xinyu Yang, Jizhe Zhou|In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains. In the field of tamper detection, LLMs are capable of identifying basic tampering activities.To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection. AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated. Manipulation detection, on the other hand, focuses on identifying tampered images. According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye. All of the LLMs can't identify carefully forged images and very realistic images generated by AI. In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality.|\u8fd1\u5e74\u6765\uff0c\u7279\u522b\u662f\u81ea 2020 \u5e74\u4ee3\u521d\u4ee5\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u4e3a\u89e3\u51b3\u5404\u79cd\u6311\u6218\uff08\u4ece\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5230\u89e3\u51b3\u5404\u4e2a\u9886\u57df\u7684\u590d\u6742\u95ee\u9898\uff09\u7684\u6700\u5f3a\u5927\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u3002\u5728\u7be1\u6539\u68c0\u6d4b\u9886\u57df\uff0cLLM\u80fd\u591f\u8bc6\u522b\u57fa\u672c\u7684\u7be1\u6539\u6d3b\u52a8\u3002\u4e3a\u4e86\u8bc4\u4f30LLM\u5728\u66f4\u4e13\u4e1a\u9886\u57df\u7684\u80fd\u529b\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e0d\u540c\u516c\u53f8\u5f00\u53d1\u7684\u4e94\u79cd\u4e0d\u540c\u7684LLM\uff1aGPT-4\u3001LLaMA\u3001Bard\u3001ERNIE Bot 4.0\u3001\u548c\u540c\u4e49\u94b1\u6587\u3002\u8fd9\u79cd\u591a\u6837\u5316\u7684\u6a21\u578b\u53ef\u4ee5\u5168\u9762\u8bc4\u4f30\u5176\u5728\u68c0\u6d4b\u590d\u6742\u7be1\u6539\u5b9e\u4f8b\u65b9\u9762\u7684\u6027\u80fd\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u68c0\u6d4b\u9886\u57df\uff1a\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9 (AIGC) \u68c0\u6d4b\u548c\u64cd\u7eb5\u68c0\u6d4b\u3002 AIGC\u68c0\u6d4b\u65e8\u5728\u6d4b\u8bd5\u533a\u5206\u56fe\u50cf\u662f\u771f\u5b9e\u56fe\u50cf\u8fd8\u662f\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u80fd\u529b\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u64cd\u7eb5\u68c0\u6d4b\u4fa7\u91cd\u4e8e\u8bc6\u522b\u88ab\u7be1\u6539\u7684\u56fe\u50cf\u3002\u6839\u636e\u6211\u4eec\u7684\u5b9e\u9a8c\uff0c\u5927\u591a\u6570LLM\u53ef\u4ee5\u8bc6\u522b\u4e0e\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u5408\u6210\u56fe\u7247\uff0c\u53ea\u6709\u66f4\u5f3a\u5927\u7684LLM\u624d\u80fd\u533a\u5206\u903b\u8f91\u4e0a\u7684\u3001\u4f46\u4eba\u773c\u53ef\u89c1\u7684\u7be1\u6539\u8ff9\u8c61\u3002\u6240\u6709\u6cd5\u5b66\u7855\u58eb\u90fd\u65e0\u6cd5\u8bc6\u522b\u7cbe\u5fc3\u4f2a\u9020\u7684\u56fe\u50cf\u548c\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u975e\u5e38\u903c\u771f\u7684\u56fe\u50cf\u3002\u5728\u7be1\u6539\u68c0\u6d4b\u9886\u57df\uff0c\u6cd5\u5b66\u7855\u58eb\u8fd8\u6709\u5f88\u957f\u7684\u8def\u8981\u8d70\uff0c\u7279\u522b\u662f\u5728\u53ef\u9760\u5730\u8bc6\u522b\u9ad8\u5ea6\u590d\u6742\u7684\u8d5d\u54c1\u548c\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u975e\u5e38\u6a21\u4eff\u73b0\u5b9e\u7684\u56fe\u50cf\u65b9\u9762\u3002|[2401.13504v1](http://arxiv.org/pdf/2401.13504v1)|null|\n", "2401.13499": "|**2024-01-24**|**LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning**|LDCA\uff1a\u5177\u6709\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u5b66\u4e60|Maofa Wang, Bingchen Yan|Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data. Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors. In this work, we introduce a novel approach termed \"Local Descriptor with Contextual Augmentation (LDCA)\". Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module. This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances. By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative. Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks.|\u5c11\u955c\u5934\u56fe\u50cf\u5206\u7c7b\u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u51f8\u663e\u4e86\u4ee5\u6700\u5c11\u7684\u6807\u8bb0\u6570\u636e\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u56fe\u50cf\u7ea7\u7279\u5f81\u6216\u5c40\u90e8\u63cf\u8ff0\u7b26\uff0c\u901a\u5e38\u5ffd\u7565\u4e86\u8fd9\u4e9b\u63cf\u8ff0\u7b26\u5468\u56f4\u7684\u6574\u4f53\u4e0a\u4e0b\u6587\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5177\u6709\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u5c40\u90e8\u63cf\u8ff0\u7b26\uff08LDCA\uff09\u201d\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u81ea\u9002\u5e94\u5168\u5c40\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5757\uff0c\u72ec\u7279\u5730\u5f25\u5408\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u6a21\u5757\u5305\u542b\u4e00\u4e2a\u89c6\u89c9\u8f6c\u6362\u5668\uff0c\u8d4b\u4e88\u5c40\u90e8\u63cf\u8ff0\u7b26\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u8303\u56f4\u4ece\u5e7f\u6cdb\u7684\u5168\u7403\u89c6\u89d2\u5230\u590d\u6742\u7684\u5468\u56f4\u7ec6\u5fae\u5dee\u522b\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0cLDCA \u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u6bcf\u4e2a\u5c40\u90e8\u7279\u5f81\u5728\u5176\u66f4\u5927\u7684\u89c6\u89c9\u53d9\u4e8b\u4e2d\u5f97\u5230\u89e3\u91ca\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u6bd4\u6b21\u4f18\u65b9\u6cd5\u7684\u6700\u5927\u7edd\u5bf9\u6539\u8fdb\u4e3a 20%\uff0c\u4ece\u800c\u8bc1\u660e\u4e86\u5728\u5c11\u6570\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u663e\u7740\u8fdb\u6b65\u3002|[2401.13499v1](http://arxiv.org/pdf/2401.13499v1)|null|\n", "2401.13472": "|**2024-01-24**|**Segmenting Cardiac Muscle Z-disks with Deep Neural Networks**|\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u5272\u5fc3\u808c Z \u76d8|Mihaela Croitor Ibrahim, Nishant Ravikumar, Alistair Curd, Joanna Leng, Oliver Umney, Michelle Peckham|Z-disks are complex structures that delineate repeating sarcomeres in striated muscle. They play significant roles in cardiomyocytes such as providing mechanical stability for the contracting sarcomere, cell signalling and autophagy. Changes in Z-disk architecture have been associated with impaired cardiac function. Hence, there is a strong need to create tools to segment Z-disks from microscopy images, that overcome traditional limitations such as variability in image brightness and staining technique. In this study, we apply deep learning based segmentation models to extract Z-disks in images of striated muscle tissue. We leverage a novel Airyscan confocal dataset, which comprises high resolution images of Z-disks of healthy heart tissue, stained with Affimers for specific Z-disk proteins. We employed an interactive labelling tool, Ilastik to obtain ground truth segmentation masks and use the resulting data set to train and evaluate the performance of several state-of-the-art segmentation networks. On the test set, UNet++ achieves best segmentation performance for Z-disks in cardiomyocytes, with an average Dice score of 0.91 and outperforms other established segmentation methods including UNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improved generalisation, when tested on an additional dataset of cardiomyocytes with a titin mutation. This is the first study to demonstrate that automated machine learning-based segmentation approaches may be used effectively to segment Z-disks in confocal microscopy images. Automated segmentation approaches and predicted segmentation masks could be used to derive morphological features of Z-disks (e.g. width and orientation), and subsequently, to quantify disease-related changes to cardiac microstructure.|Z \u76d8\u662f\u63cf\u7ed8\u6a2a\u7eb9\u808c\u4e2d\u91cd\u590d\u808c\u8282\u7684\u590d\u6742\u7ed3\u6784\u3002\u5b83\u4eec\u5728\u5fc3\u808c\u7ec6\u80de\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f8b\u5982\u4e3a\u6536\u7f29\u808c\u8282\u63d0\u4f9b\u673a\u68b0\u7a33\u5b9a\u6027\u3001\u7ec6\u80de\u4fe1\u53f7\u4f20\u5bfc\u548c\u81ea\u566c\u3002 Z \u76d8\u7ed3\u6784\u7684\u53d8\u5316\u4e0e\u5fc3\u810f\u529f\u80fd\u53d7\u635f\u6709\u5173\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u521b\u5efa\u4ece\u663e\u5fae\u56fe\u50cf\u4e2d\u5206\u5272 Z \u76d8\u7684\u5de5\u5177\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u7684\u9650\u5236\uff0c\u4f8b\u5982\u56fe\u50cf\u4eae\u5ea6\u548c\u67d3\u8272\u6280\u672f\u7684\u53d8\u5316\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u5272\u6a21\u578b\u6765\u63d0\u53d6\u6a2a\u7eb9\u808c\u7ec4\u7ec7\u56fe\u50cf\u4e2d\u7684 Z \u76d8\u3002\u6211\u4eec\u5229\u7528\u65b0\u9896\u7684 Airyscan \u5171\u805a\u7126\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5065\u5eb7\u5fc3\u810f\u7ec4\u7ec7 Z \u76d8\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u7528 Affimer \u67d3\u8272\u7279\u5b9a\u7684 Z \u76d8\u86cb\u767d\u8d28\u3002\u6211\u4eec\u91c7\u7528\u4ea4\u4e92\u5f0f\u6807\u8bb0\u5de5\u5177 Ilastik \u6765\u83b7\u53d6\u5730\u9762\u771f\u5b9e\u5206\u5272\u63a9\u6a21\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u51e0\u4e2a\u6700\u5148\u8fdb\u7684\u5206\u5272\u7f51\u7edc\u7684\u6027\u80fd\u3002\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0cUNet++ \u5bf9\u5fc3\u808c\u7ec6\u80de\u4e2d\u7684 Z \u76d8\u5b9e\u73b0\u4e86\u6700\u4f73\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.91\uff0c\u4f18\u4e8e\u5176\u4ed6\u5df2\u5efa\u7acb\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5305\u62ec UNet\u3001FPN\u3001DeepLabv3+ \u548c pix2pix\u3002\u7136\u800c\uff0c\u5f53\u5728\u5177\u6709\u808c\u8054\u86cb\u767d\u7a81\u53d8\u7684\u5fc3\u808c\u7ec6\u80de\u7684\u9644\u52a0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0cpix2pix \u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u662f\u7b2c\u4e00\u9879\u8bc1\u660e\u57fa\u4e8e\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7684\u5206\u5272\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5206\u5272\u5171\u7126\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684 Z \u76d8\u7684\u7814\u7a76\u3002\u81ea\u52a8\u5206\u5272\u65b9\u6cd5\u548c\u9884\u6d4b\u5206\u5272\u63a9\u6a21\u53ef\u7528\u4e8e\u5bfc\u51fa Z \u76d8\u7684\u5f62\u6001\u7279\u5f81\uff08\u4f8b\u5982\u5bbd\u5ea6\u548c\u65b9\u5411\uff09\uff0c\u5e76\u968f\u540e\u91cf\u5316\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u5fc3\u810f\u5fae\u89c2\u7ed3\u6784\u53d8\u5316\u3002|[2401.13472v1](http://arxiv.org/pdf/2401.13472v1)|null|\n", "2401.13414": "|**2024-01-24**|**GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition**|GTAutoAct\uff1a\u57fa\u4e8e\u6e38\u620f\u5f15\u64ce\u91cd\u65b0\u5f00\u53d1\u7684\u52a8\u4f5c\u8bc6\u522b\u81ea\u52a8\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6|Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi|Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.|\u5f53\u524d\u7684\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u6570\u636e\u96c6\u9762\u4e34\u7740\u4f20\u7edf\u6536\u96c6\u548c\u751f\u6210\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5305\u62ec\u52a8\u4f5c\u7c7b\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u591a\u89c6\u70b9\u8bb0\u5f55\u3001\u591a\u6837\u6027\u6709\u9650\u3001\u89c6\u9891\u8d28\u91cf\u5dee\u4ee5\u53ca\u52b3\u52a8\u5bc6\u96c6\u578b\u624b\u52a8\u6536\u96c6\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 GTAutoAct\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u6e38\u620f\u5f15\u64ce\u6280\u672f\u6765\u4fc3\u8fdb\u52a8\u4f5c\u8bc6\u522b\u7684\u8fdb\u6b65\u3002 GTAutoAct \u64c5\u957f\u81ea\u52a8\u521b\u5efa\u5927\u89c4\u6a21\u3001\u6ce8\u91ca\u826f\u597d\u7684\u6570\u636e\u96c6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u52a8\u4f5c\u7c7b\u548c\u5353\u8d8a\u7684\u89c6\u9891\u8d28\u91cf\u3002\u6211\u4eec\u7684\u6846\u67b6\u7684\u72ec\u7279\u8d21\u732e\u5305\u62ec\uff1a\uff081\uff09\u5b83\u521b\u65b0\u5730\u5c06\u73b0\u6709\u7684\u57fa\u4e8e\u5750\u6807\u7684 3D \u4eba\u4f53\u8fd0\u52a8\u8f6c\u6362\u4e3a\u9762\u5411\u65cb\u8f6c\u7684\u8868\u793a\uff0c\u5e76\u589e\u5f3a\u4e86\u591a\u89c6\u89d2\u7684\u9002\u7528\u6027\uff1b (2)\u5b83\u91c7\u7528\u52a8\u6001\u5206\u5272\u548c\u65cb\u8f6c\u5e8f\u5217\u63d2\u503c\u6765\u521b\u5efa\u6d41\u7545\u4e14\u903c\u771f\u7684\u52a8\u4f5c\u52a8\u753b\uff1b (3)\u5b83\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u53ef\u5b9a\u5236\u7684\u52a8\u753b\u573a\u666f\uff1b (4) \u5b83\u5b9e\u73b0\u4e86\u81ea\u4e3b\u89c6\u9891\u6355\u83b7\u548c\u5904\u7406\u7ba1\u9053\uff0c\u5177\u6709\u968f\u673a\u5bfc\u822a\u76f8\u673a\uff0c\u5177\u6709\u81ea\u52a8\u4fee\u526a\u548c\u6807\u8bb0\u529f\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u8be5\u6846\u67b6\u7684\u7a33\u5065\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u663e\u7740\u6539\u5584\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u8bad\u7ec3\u7684\u6f5c\u529b\u3002|[2401.13414v1](http://arxiv.org/pdf/2401.13414v1)|null|\n", "2401.13405": "|**2024-01-24**|**Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter**|\u5408\u6210\u6570\u636e\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u6ce8\u91ca\u548c\u5f3a\u5927\u7684\u5206\u5272\uff0c\u4ee5\u5b9e\u73b0\u6742\u4e71\u4e2d\u7684\u591a\u5bf9\u8c61\u6293\u53d6|Dongmyoung Lee, Wei Chen, Nicolas Rojas|Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation. In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset). Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically. RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively. Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation.|\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u7684\u5bf9\u8c61\u8bc6\u522b\u548c\u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u6784\u5efa\u6807\u8bb0\u6570\u636e\u96c6\u5728\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u65b9\u9762\u53ef\u80fd\u975e\u5e38\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0e\u8f83\u5c0f\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff08\u6df7\u5408\u6570\u636e\u96c6\uff09\u76f8\u7ed3\u5408\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4eba\u4e3a\u5e72\u9884\u5e76\u4f7f\u4e0b\u6e38\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u66f4\u52a0\u7a33\u5065\u3002\u6ce8\u91ca\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5408\u6210\u573a\u666f\u751f\u6210\u53ef\u4ee5\u663e\u7740\u51cf\u5c11\u6807\u8bb0\u65f6\u95f4\u3002 RGB \u56fe\u50cf\u5206\u5272\u4f7f\u7528\u6df7\u5408\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e0e\u6df1\u5ea6\u4fe1\u606f\u76f8\u7ed3\u5408\uff0c\u4ee5\u751f\u6210\u5404\u4e2a\u5206\u5272\u5bf9\u8c61\u7684\u50cf\u7d20\u5230\u70b9\u5bf9\u5e94\u5173\u7cfb\u3002\u7136\u540e\u7531\u5206\u5272\u7b97\u6cd5\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u786e\u5b9a\u8981\u6293\u53d6\u7684\u5bf9\u8c61\u3002\u62fe\u653e\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6211\u4eec\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5206\u5272 (98.9%, 70%) \u5728\u6807\u7b7e\u65b9\u9762\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u548c\u516c\u5f00\u6570\u636e\u96c6 (6.7%, 18.8%) \u548c (2.8%, 10%)\u548c\u6293\u53d6\u6210\u529f\u7387\u3002\u8865\u5145\u6750\u6599\u53ef\u5728 https://sites.google.com/view/synthetic-dataset- Generation \u4e0a\u83b7\u53d6\u3002|[2401.13405v1](http://arxiv.org/pdf/2401.13405v1)|null|\n", "2401.13403": "|**2024-01-24**|**SEDNet: Shallow Encoder-Decoder Network for Brain Tumor Segmentation**|SEDNet\uff1a\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u7684\u6d45\u5c42\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc|Chollette C. Olisah|Despite the advancement in computational modeling towards brain tumor segmentation, of which several models have been developed, it is evident from the computational complexity of existing models which are still at an all-time high, that performance and efficiency under clinical application scenarios are limited. Therefore, this paper proposes a shallow encoder and decoder network named SEDNet for brain tumor segmentation. The proposed network is adapted from the U-Net structure. Though brain tumors do not assume complex structures like the task the traditional U-Net was designed for, their variance in appearance, shape, and ambiguity of boundaries makes it a compelling complex task to solve. SEDNet architecture design is inspired by the localized nature of brain tumors in brain images, thus consists of sufficient hierarchical convolutional blocks in the encoding pathway capable of learning the intrinsic features of brain tumors in brain slices, and a decoding pathway with selective skip path sufficient for capturing miniature local-level spatial features alongside the global-level features of brain tumor. SEDNet with the integration of the proposed preprocessing algorithm and optimization function on the BraTS2020 set reserved for testing achieves impressive dice and Hausdorff scores of 0.9308, 0.9451, 0.9026, and 0.7040, 1.2866, 0.7762 for non-enhancing tumor core (NTC), peritumoral edema (ED), and enhancing tumor (ET), respectively. Furthermore, through transfer learning with initialized SEDNet pre-trained weights, termed SEDNetX, a performance increase is observed. The dice and Hausdorff scores recorded are 0.9336, 0.9478, 0.9061, 0.6983, 1.2691, and 0.7711 for NTC, ED, and ET, respectively. With about 1.3 million parameters and impressive performance in comparison to the state-of-the-art, SEDNet(X) is shown to be computationally efficient for real-time clinical diagnosis.|\u5c3d\u7ba1\u8111\u80bf\u7624\u5206\u5272\u7684\u8ba1\u7b97\u6a21\u578b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u4f46\u4ece\u73b0\u6709\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4ecd\u7136\u5904\u4e8e\u5386\u53f2\u6700\u9ad8\u6c34\u5e73\u53ef\u4ee5\u770b\u51fa\uff0c\u4e34\u5e8a\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u6548\u7387\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEDNet\u7684\u6d45\u5c42\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u3002\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u6539\u7f16\u81ea U-Net \u7ed3\u6784\u3002\u5c3d\u7ba1\u8111\u80bf\u7624\u4e0d\u50cf\u4f20\u7edf U-Net \u6240\u8bbe\u8ba1\u7684\u4efb\u52a1\u90a3\u6837\u5177\u6709\u590d\u6742\u7684\u7ed3\u6784\uff0c\u4f46\u5b83\u4eec\u5728\u5916\u89c2\u3001\u5f62\u72b6\u548c\u8fb9\u754c\u6a21\u7cca\u6027\u65b9\u9762\u7684\u5dee\u5f02\u4f7f\u5176\u6210\u4e3a\u4e00\u9879\u5f15\u4eba\u6ce8\u76ee\u7684\u590d\u6742\u4efb\u52a1\u3002 SEDNet\u67b6\u6784\u8bbe\u8ba1\u7684\u7075\u611f\u6765\u81ea\u4e8e\u8111\u56fe\u50cf\u4e2d\u8111\u80bf\u7624\u7684\u5c40\u90e8\u6027\u8d28\uff0c\u56e0\u6b64\u7531\u7f16\u7801\u8def\u5f84\u4e2d\u8db3\u591f\u7684\u5206\u5c42\u5377\u79ef\u5757\u7ec4\u6210\uff0c\u80fd\u591f\u5b66\u4e60\u8111\u5207\u7247\u4e2d\u8111\u80bf\u7624\u7684\u5185\u5728\u7279\u5f81\uff0c\u4ee5\u53ca\u5177\u6709\u8db3\u4ee5\u7528\u4e8e\u5b66\u4e60\u8111\u5207\u7247\u4e2d\u8111\u80bf\u7624\u7684\u5185\u5728\u7279\u5f81\u7684\u89e3\u7801\u8def\u5f84\u3002\u6355\u83b7\u5fae\u578b\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u4ee5\u53ca\u8111\u80bf\u7624\u7684\u5168\u5c40\u7279\u5f81\u3002 SEDNet \u5c06\u6240\u63d0\u51fa\u7684\u9884\u5904\u7406\u7b97\u6cd5\u548c\u4f18\u5316\u529f\u80fd\u96c6\u6210\u5728\u4e3a\u6d4b\u8bd5\u4fdd\u7559\u7684 BraTS2020 \u96c6\u4e0a\uff0c\u5bf9\u4e8e\u975e\u589e\u5f3a\u80bf\u7624\u6838\u5fc3 (NTC)\u3001\u7624\u5468\u6c34\u80bf\uff0c\u83b7\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 dice \u548c Hausdorff \u5206\u6570 0.9308\u30010.9451\u30010.9026 \u548c 0.7040\u30011.2866\u30010.7762\u5206\u522b\u662f\uff08ED\uff09\u548c\u589e\u5f3a\u80bf\u7624\uff08ET\uff09\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4f7f\u7528\u521d\u59cb\u5316\u7684 SEDNet \u9884\u8bad\u7ec3\u6743\u91cd\uff08\u79f0\u4e3a SEDNetX\uff09\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u6027\u80fd\u7684\u63d0\u9ad8\u3002 NTC\u3001ED \u548c ET \u8bb0\u5f55\u7684\u9ab0\u5b50\u5206\u6570\u548c Hausdorff \u5206\u6570\u5206\u522b\u4e3a 0.9336\u30010.9478\u30010.9061\u30010.6983\u30011.2691 \u548c 0.7711\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0cSEDNet(X) \u62e5\u6709\u7ea6 130 \u4e07\u4e2a\u53c2\u6570\u548c\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u5728\u5b9e\u65f6\u4e34\u5e8a\u8bca\u65ad\u65b9\u9762\u5177\u6709\u8ba1\u7b97\u6548\u7387\u3002|[2401.13403v1](http://arxiv.org/pdf/2401.13403v1)|null|\n", "2401.13388": "|**2024-01-24**|**UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion**|UNIMO-G\uff1a\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u6269\u6563\u751f\u6210\u7edf\u4e00\u56fe\u50cf|Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao|Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents \\textbf{UNIMO-G}, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.|\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u4e3b\u8981\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u50cf\u3002\u7136\u800c\uff0c\u6587\u672c\u63cf\u8ff0\u56fa\u6709\u7684\u7b80\u6d01\u6027\u7ed9\u5fe0\u5b9e\u5408\u6210\u5177\u6709\u590d\u6742\u7ec6\u8282\uff08\u4f8b\u5982\u7279\u5b9a\u5b9e\u4f53\u6216\u573a\u666f\uff09\u7684\u56fe\u50cf\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86 \\textbf{UNIMO-G}\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u6a21\u6001\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u5b83\u5728\u5177\u6709\u4ea4\u9519\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u7684\u591a\u6a21\u6001\u63d0\u793a\u4e0a\u8fd0\u884c\uff0c\u5c55\u793a\u4e86\u6587\u672c\u9a71\u52a8\u548c\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u7684\u7edf\u4e00\u80fd\u529b\u3002 UNIMO-G \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7528\u4e8e\u7f16\u7801\u591a\u6a21\u6001\u63d0\u793a\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM)\uff0c\u4ee5\u53ca\u7528\u4e8e\u57fa\u4e8e\u7f16\u7801\u7684\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u56fe\u50cf\u7684\u6761\u4ef6\u53bb\u566a\u6269\u6563\u7f51\u7edc\u3002\u6211\u4eec\u5229\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6765\u6709\u6548\u8bad\u7ec3\u6846\u67b6\uff1a\u9996\u5148\u5bf9\u5927\u89c4\u6a21\u6587\u672c\u56fe\u50cf\u5bf9\u8fdb\u884c\u9884\u8bad\u7ec3\u4ee5\u5f00\u53d1\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u7136\u540e\u4f7f\u7528\u591a\u6a21\u6001\u63d0\u793a\u8fdb\u884c\u6307\u4ee4\u8c03\u6574\u4ee5\u5b9e\u73b0\u7edf\u4e00\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002\u91c7\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d89\u53ca\u8bed\u8a00\u57fa\u7840\u548c\u56fe\u50cf\u5206\u5272\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u6765\u6784\u5efa\u591a\u6a21\u5f0f\u63d0\u793a\u3002 UNIMO-G \u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u96f6\u6837\u672c\u4e3b\u9898\u9a71\u52a8\u5408\u6210\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u4ece\u6d89\u53ca\u591a\u4e2a\u56fe\u50cf\u5b9e\u4f53\u7684\u590d\u6742\u591a\u6a21\u6001\u63d0\u793a\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u65b9\u9762\u5c24\u5176\u6709\u6548\u3002|[2401.13388v1](http://arxiv.org/pdf/2401.13388v1)|null|\n", "2401.13386": "|**2024-01-24**|**Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain**|\u6df7\u5408\u9891\u8272\u57df\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b|Dong Han, Yong Li, Joachim Denzler|Face recognition technology has been deployed in various real-life applications. The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy. It is quite common for clients to upload face images to the service provider in order to access the model inference. However, the face image is a type of sensitive biometric attribute tied to the identity information of each user. Directly exposing the raw face image to the service provider poses a threat to the user's privacy. Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding. The noticeable drop in recognition accuracy is a pitfall for most methods. This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain. Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise. Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities. Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference. The proposed method performs well on multiple widely used verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario.|\u4eba\u8138\u8bc6\u522b\u6280\u672f\u5df2\u88ab\u90e8\u7f72\u5728\u5404\u79cd\u73b0\u5b9e\u751f\u6d3b\u4e2d\u3002\u6700\u590d\u6742\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4f9d\u9760\u901a\u8fc7\u590d\u6742\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6570\u767e\u4e07\u5f20\u4eba\u8138\u56fe\u50cf\u6765\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u5ba2\u6237\u5c06\u4eba\u8138\u56fe\u50cf\u4e0a\u4f20\u5230\u670d\u52a1\u63d0\u4f9b\u5546\u4ee5\u8bbf\u95ee\u6a21\u578b\u63a8\u7406\u662f\u5f88\u5e38\u89c1\u7684\u3002\u7136\u800c\uff0c\u9762\u90e8\u56fe\u50cf\u662f\u4e00\u79cd\u4e0e\u6bcf\u4e2a\u7528\u6237\u7684\u8eab\u4efd\u4fe1\u606f\u76f8\u5173\u7684\u654f\u611f\u751f\u7269\u7279\u5f81\u5c5e\u6027\u3002\u5c06\u539f\u59cb\u4eba\u8138\u56fe\u50cf\u76f4\u63a5\u66b4\u9732\u7ed9\u670d\u52a1\u63d0\u4f9b\u5546\u4f1a\u5bf9\u7528\u6237\u7684\u9690\u79c1\u9020\u6210\u5a01\u80c1\u3002\u5f53\u524d\u4eba\u8138\u8bc6\u522b\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u9690\u85cf\u6a21\u578b\u8f93\u5165\u4e0a\u7684\u89c6\u89c9\u4fe1\u606f\u6216\u4fdd\u62a4\u6a21\u578b\u8f93\u51fa\u4eba\u8138\u5d4c\u5165\u3002\u8bc6\u522b\u51c6\u786e\u5ea6\u7684\u663e\u7740\u4e0b\u964d\u662f\u5927\u591a\u6570\u65b9\u6cd5\u7684\u7f3a\u9677\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u9891\u7387-\u989c\u8272\u878d\u5408\u65b9\u6cd5\u6765\u964d\u4f4e\u9891\u57df\u4e2d\u4eba\u8138\u8bc6\u522b\u7684\u8f93\u5165\u7ef4\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7a00\u758f\u989c\u8272\u4fe1\u606f\uff0c\u4ee5\u51cf\u8f7b\u6dfb\u52a0\u5dee\u5206\u9690\u79c1\u566a\u58f0\u540e\u7cbe\u5ea6\u7684\u663e\u7740\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u4e86\u7279\u5b9a\u4e8e\u8eab\u4efd\u7684\u5d4c\u5165\u6620\u5c04\u65b9\u6848\uff0c\u901a\u8fc7\u6269\u5927\u8eab\u4efd\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u4fdd\u62a4\u539f\u59cb\u4eba\u8138\u5d4c\u5165\u3002\u6700\u540e\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff0c\u4ee5\u5728\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b89\u5168\u5730\u8ba1\u7b97\u5d4c\u5165\u8ddd\u79bb\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002\u6b64\u5916\uff0c\u5728 1:N \u9a8c\u8bc1\u573a\u666f\u4e2d\uff0c\u5b83\u7684\u51c6\u786e\u5ea6\u6bd4\u6700\u5148\u8fdb\u7684\u6280\u672f\u9ad8\u51fa\u7ea6 2.6% \u81f3 4.2%\u3002|[2401.13386v1](http://arxiv.org/pdf/2401.13386v1)|null|\n", "2401.13330": "|**2024-01-24**|**NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks**|NACHOS\uff1a\u786c\u4ef6\u7ea6\u675f\u63d0\u524d\u9000\u51fa\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22|Matteo Gambella, Jary Pomponi, Simone Scardapane, Manuel Roveri|Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN|\u65e9\u671f\u9000\u51fa\u795e\u7ecf\u7f51\u7edc (EENN) \u8d4b\u4e88\u6807\u51c6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u65e9\u671f\u9000\u51fa\u5206\u7c7b\u5668 (EEC)\uff0c\u4ee5\u4fbf\u5728\u5206\u7c7b\u83b7\u5f97\u8db3\u591f\u7684\u7f6e\u4fe1\u5ea6\u65f6\u5728\u5904\u7406\u7684\u4e2d\u95f4\u70b9\u63d0\u4f9b\u9884\u6d4b\u3002\u8fd9\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5e26\u6765\u4e86\u8bb8\u591a\u597d\u5904\u3002\u76ee\u524d\uff0cEENN \u7684\u8bbe\u8ba1\u662f\u7531\u4e13\u5bb6\u624b\u52a8\u8fdb\u884c\u7684\uff0c\u8fd9\u662f\u4e00\u9879\u590d\u6742\u4e14\u8017\u65f6\u7684\u4efb\u52a1\uff0c\u9700\u8981\u8003\u8651\u8bb8\u591a\u65b9\u9762\uff0c\u5305\u62ec EEC \u7684\u6b63\u786e\u653e\u7f6e\u3001\u9608\u503c\u5904\u7406\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u6b64\uff0c\u8be5\u7814\u7a76\u6b63\u5728\u63a2\u7d22\u4f7f\u7528\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u6765\u81ea\u52a8\u5316 EENN \u7684\u8bbe\u8ba1\u3002\u76ee\u524d\uff0c\u6587\u732e\u4e2d\u5f88\u5c11\u63d0\u51fa\u9488\u5bf9 EENN \u7684\u5168\u9762 NAS \u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8003\u8651\u9aa8\u5e72\u7f51\u548c EEC \u7684\u5168\u81ea\u52a8\u8054\u5408\u8bbe\u8ba1\u7b56\u7565\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u786c\u4ef6\u7ea6\u675f\u63d0\u524d\u9000\u51fa\u795e\u7ecf\u7f51\u7edc (NACHOS) \u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bbe\u8ba1\u6700\u4f73 EENN \u7684 NAS \u6846\u67b6\uff0c\u6ee1\u8db3\u7cbe\u5ea6\u4ee5\u53ca\u4e58\u6cd5\u548c\u7d2f\u52a0 (MAC) \u64cd\u4f5c\u6267\u884c\u6b21\u6570\u7684\u7ea6\u675f\u3002\u63a8\u7406\u65f6\u7684 EENN\u3002\u7279\u522b\u662f\uff0c\u8fd9\u63d0\u4f9b\u4e86\u9aa8\u5e72\u7f51\u548c EEC \u7684\u8054\u5408\u8bbe\u8ba1\uff0c\u4ee5\u5728\u51c6\u786e\u6027\u548c MAC \u6570\u91cf\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u65b9\u9762\u9009\u62e9\u4e00\u7ec4\u53ef\u63a5\u53d7\u7684\uff08\u5373\u5c0a\u91cd\u7ea6\u675f\uff09\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002\u7ed3\u679c\u8868\u660e\uff0cNACHOS \u8bbe\u8ba1\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684 EENN \u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u7814\u7a76\u4e86\u4e24\u4e2a\u65b0\u9896\u7684\u6b63\u5219\u5316\u9879\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e9b\u6b63\u5219\u5316\u9879\u662f\u4e3a\u4f18\u5316 EENN \u8f85\u52a9\u5206\u7c7b\u5668\u800c\u8bbe\u8ba1\u7684\u3002|[2401.13330v1](http://arxiv.org/pdf/2401.13330v1)|null|\n", "2401.13325": "|**2024-01-24**|**Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery**|\u7528\u4e8e\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u7684\u5185\u5b58\u4e00\u81f4\u6027\u5f15\u5bfc\u5206\u800c\u6cbb\u4e4b\u5b66\u4e60|Yuanpeng Tu, Zhun Zhong, Yuxi Li, Hengshuang Zhao|Generalized category discovery (GCD) aims at addressing a more realistic and challenging setting of semi-supervised learning, where only part of the category labels are assigned to certain training samples. Previous methods generally employ naive contrastive learning or unsupervised clustering scheme for all the samples. Nevertheless, they usually ignore the inherent critical information within the historical predictions of the model being trained. Specifically, we empirically reveal that a significant number of salient unlabeled samples yield consistent historical predictions corresponding to their ground truth category. From this observation, we propose a Memory Consistency guided Divide-and-conquer Learning framework (MCDL). In this framework, we introduce two memory banks to record historical prediction of unlabeled data, which are exploited to measure the credibility of each sample in terms of its prediction consistency. With the guidance of credibility, we can design a divide-and-conquer learning strategy to fully utilize the discriminative information of unlabeled data while alleviating the negative influence of noisy labels. Extensive experimental results on multiple benchmarks demonstrate the generality and superiority of our method, where our method outperforms state-of-the-art models by a large margin on both seen and unseen classes of the generic image recognition and challenging semantic shift settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).|\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u65e8\u5728\u89e3\u51b3\u66f4\u73b0\u5b9e\u4e14\u66f4\u5177\u6311\u6218\u6027\u7684\u534a\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5176\u4e2d\u4ec5\u5c06\u90e8\u5206\u7c7b\u522b\u6807\u7b7e\u5206\u914d\u7ed9\u67d0\u4e9b\u8bad\u7ec3\u6837\u672c\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u6837\u672c\u91c7\u7528\u6734\u7d20\u5bf9\u6bd4\u5b66\u4e60\u6216\u65e0\u76d1\u7763\u805a\u7c7b\u65b9\u6848\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u901a\u5e38\u5ffd\u7565\u6b63\u5728\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u5386\u53f2\u9884\u6d4b\u4e2d\u56fa\u6709\u7684\u5173\u952e\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u51ed\u7ecf\u9a8c\u63ed\u793a\uff0c\u5927\u91cf\u663e\u7740\u7684\u672a\u6807\u8bb0\u6837\u672c\u4ea7\u751f\u4e0e\u5176\u771f\u5b9e\u7c7b\u522b\u76f8\u5bf9\u5e94\u7684\u4e00\u81f4\u5386\u53f2\u9884\u6d4b\u3002\u6839\u636e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bb0\u5fc6\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u5206\u800c\u6cbb\u4e4b\u7684\u5b66\u4e60\u6846\u67b6\uff08MCDL\uff09\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u5185\u5b58\u5e93\u6765\u8bb0\u5f55\u672a\u6807\u8bb0\u6570\u636e\u7684\u5386\u53f2\u9884\u6d4b\uff0c\u5229\u7528\u5b83\u4eec\u6765\u8861\u91cf\u6bcf\u4e2a\u6837\u672c\u5728\u9884\u6d4b\u4e00\u81f4\u6027\u65b9\u9762\u7684\u53ef\u4fe1\u5ea6\u3002\u5728\u53ef\u4fe1\u5ea6\u7684\u6307\u5bfc\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u5206\u800c\u6cbb\u4e4b\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u5145\u5206\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u7684\u5224\u522b\u4fe1\u606f\uff0c\u540c\u65f6\u51cf\u8f7b\u566a\u58f0\u6807\u7b7e\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u591a\u4e2a\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u901a\u7528\u56fe\u50cf\u8bc6\u522b\u7684\u53ef\u89c1\u548c\u4e0d\u53ef\u89c1\u7c7b\u522b\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u8bed\u4e49\u8f6c\u79fb\u8bbe\u7f6e\uff08\u5373\uff0c CUB \u6536\u76ca\u589e\u52a0 8.4%\uff0cStandford Cars \u6536\u76ca\u589e\u52a0 8.1%\uff09\u3002|[2401.13325v1](http://arxiv.org/pdf/2401.13325v1)|null|\n", "2401.13315": "|**2024-01-24**|**Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging**|\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u5408\u6210\u7a84\u5e26\u6210\u50cf\u606f\u8089\u68c0\u6d4b|Mathias Ramm Haugland, Hemin Ali Qadir, Ilangko Balasingham|To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the original WLI. This is because our WLI-to-SNBI translation model can enhance the observation of polyp surface patterns in the generated SNBI images.|\u4e3a\u4e86\u5e94\u5bf9\u7ed3\u76f4\u80a0\u764c (CRC) \u65e5\u76ca\u589e\u957f\u7684\u60a3\u75c5\u7387\uff0c\u606f\u8089\u68c0\u6d4b\u548c\u5207\u9664\u7684\u7b5b\u67e5\u8ba1\u5212\u5df2\u8bc1\u660e\u5176\u6709\u7528\u6027\u3002\u7ed3\u80a0\u955c\u68c0\u67e5\u88ab\u8ba4\u4e3a\u662f CRC \u7b5b\u67e5\u6548\u679c\u6700\u597d\u7684\u7a0b\u5e8f\u3002\u4e3a\u4e86\u7b80\u5316\u68c0\u67e5\uff0c\u9488\u5bf9\u4f20\u7edf\u767d\u5149\u6210\u50cf (WLI) \u5f00\u53d1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u606f\u8089\u68c0\u6d4b\u65b9\u6cd5\u3002\u4e0e WLI \u76f8\u6bd4\uff0c\u7a84\u5e26\u6210\u50cf (NBI) \u53ef\u4ee5\u6539\u5584\u7ed3\u80a0\u955c\u68c0\u67e5\u671f\u95f4\u606f\u8089\u7684\u5206\u7c7b\uff0c\u4f46\u9700\u8981\u7279\u6b8a\u8bbe\u5907\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e CycleGAN \u7684\u6846\u67b6\uff0c\u53ef\u5c06\u5e38\u89c4 WLI \u6355\u83b7\u7684\u56fe\u50cf\u8f6c\u6362\u4e3a\u5408\u6210 NBI (SNBI)\uff0c\u4f5c\u4e3a\u4e00\u79cd\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728 NBI \u4e0d\u53ef\u7528\u65f6\u6539\u8fdb WLI \u4e0a\u7684\u5bf9\u8c61\u68c0\u6d4b\u3002\u672c\u6587\u9996\u5148\u8868\u660e\uff0c\u4e0e\u76f8\u5bf9\u76f8\u4f3c\u7684 WLI \u6570\u636e\u96c6\u76f8\u6bd4\uff0cNBI \u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u606f\u8089\u68c0\u6d4b\u7ed3\u679c\u3002\u5176\u6b21\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cb WLI \u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u6001\u8f6c\u6362\u53ef\u4ee5\u5728 WLI \u751f\u6210\u7684 SNBI \u56fe\u50cf\u4e0a\u5b9e\u73b0\u6539\u8fdb\u7684\u606f\u8089\u68c0\u6d4b\u3002\u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u7684 WLI \u5230 SNBI \u8f6c\u6362\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u5bf9\u751f\u6210\u7684 SNBI \u56fe\u50cf\u4e2d\u606f\u8089\u8868\u9762\u56fe\u6848\u7684\u89c2\u5bdf\u3002|[2401.13315v1](http://arxiv.org/pdf/2401.13315v1)|null|\n", "2401.13285": "|**2024-01-24**|**Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region**|LiDAR \u70b9\u4e91\u4e2d\u7684\u5c0f\u7269\u4f53\u8ddf\u8e2a\uff1a\u5b66\u4e60\u76ee\u6807\u611f\u77e5\u539f\u578b\u548c\u7ec6\u7c92\u5ea6\u641c\u7d22\u533a\u57df|Shengjing Tian, Yinan Han, Xiuping Liu, Xiantong Zhao|Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects.|\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4e2d\u7684\u5355\u76ee\u6807\u8ddf\u8e2a\u662f\u73af\u5883\u611f\u77e5\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206\u4e4b\u4e00\uff0c\u5176\u4e2d\u5c0f\u76ee\u6807\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u8fd9\u4f1a\u7ed9\u51c6\u786e\u5b9a\u4f4d\u5e26\u6765\u91cd\u5927\u969c\u788d\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u66f4\u591a\u5730\u96c6\u4e2d\u4e8e\u63a2\u7d22\u901a\u7528\u7c7b\u522b\u7684\u901a\u7528\u67b6\u6784\uff0c\u800c\u5ffd\u89c6\u4e86\u5c0f\u7269\u4f53\u7531\u4e8e\u524d\u666f\u70b9\u76f8\u5bf9\u7f3a\u4e4f\u548c\u5bf9\u5e72\u6270\u7684\u5bb9\u5fcd\u5ea6\u8f83\u4f4e\u800c\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u662f\u68d8\u624b\u7684\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u4f53\u7f51\u7edc\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u5c0f\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7531\u76ee\u6807\u611f\u77e5\u539f\u578b\u6316\u6398\uff08TAPM\uff09\u6a21\u5757\u548c\u533a\u57df\u7f51\u683c\u7ec6\u5206\uff08RGS\uff09\u6a21\u5757\u7ec4\u6210\u3002 TAPM\u6a21\u5757\u91c7\u7528\u63a9\u6a21\u89e3\u7801\u5668\u7684\u91cd\u5efa\u673a\u5236\u6765\u5b66\u4e60\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u539f\u578b\uff0c\u65e8\u5728\u7a81\u51fa\u524d\u666f\u70b9\u7684\u5b58\u5728\uff0c\u4ee5\u5229\u4e8e\u540e\u7eed\u5c0f\u7269\u4f53\u7684\u5b9a\u4f4d\u3002\u901a\u8fc7\u4e0a\u8ff0\u539f\u578b\u80fd\u591f\u5f3a\u8c03\u611f\u5174\u8da3\u7684\u5c0f\u7269\u4f53\uff0c\u7279\u5f81\u56fe\u4e2d\u7684\u5b9a\u4f4d\u504f\u5dee\u4ecd\u7136\u5bfc\u81f4\u8f83\u9ad8\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86 RGS \u6a21\u5757\u6765\u57fa\u4e8e ViT \u548c\u50cf\u7d20\u6d17\u724c\u5c42\u6062\u590d\u641c\u7d22\u533a\u57df\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002\u6b64\u5916\uff0c\u9664\u4e86\u6b63\u5e38\u8bbe\u7f6e\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u7f29\u653e\u5b9e\u9a8c\u6765\u8bc4\u4f30\u4e0d\u540c\u8ddf\u8e2a\u5668\u5728\u5c0f\u7269\u4f53\u4e0a\u7684\u9c81\u68d2\u6027\u3002\u5728 KITTI \u548c nuScenes \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5c0f\u76ee\u6807\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u800c\u4e0d\u5f71\u54cd\u6b63\u5e38\u5927\u5c0f\u7684\u76ee\u6807\u3002|[2401.13285v1](http://arxiv.org/pdf/2401.13285v1)|null|\n", "2401.13280": "|**2024-01-24**|**DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In Machine-Assisted Skin Disease Detection**|DDI-CoCo\uff1a\u7528\u4e8e\u4e86\u89e3\u673a\u5668\u8f85\u52a9\u76ae\u80a4\u75c5\u68c0\u6d4b\u4e2d\u989c\u8272\u5bf9\u6bd4\u5ea6\u6548\u679c\u7684\u6570\u636e\u96c6|Ming-Chang Chiu, Yingfei Wang, Yen-Ju Kuo, Pin-Yu Chen|Skin tone as a demographic bias and inconsistent human labeling poses challenges in dermatology AI. We take another angle to investigate color contrast's impact, beyond skin tones, on malignancy detection in skin disease datasets: We hypothesize that in addition to skin tones, the color difference between the lesion area and skin also plays a role in malignancy detection performance of dermatology AI models. To study this, we first propose a robust labeling method to quantify color contrast scores of each image and validate our method by showing small labeling variations. More importantly, applying our method to \\textit{the only} diverse-skin tone and pathologically-confirmed skin disease dataset DDI, yields \\textbf{DDI-CoCo Dataset}, and we observe a performance gap between the high and low color difference groups. This disparity remains consistent across various state-of-the-art (SoTA) image classification models, which supports our hypothesis. Furthermore, we study the interaction between skin tone and color difference effects and suggest that color difference can be an additional reason behind model performance bias between skin tones. Our work provides a complementary angle to dermatology AI for improving skin disease detection.|\u80a4\u8272\u4f5c\u4e3a\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u548c\u4e0d\u4e00\u81f4\u7684\u4eba\u7c7b\u6807\u7b7e\u7ed9\u76ae\u80a4\u75c5\u5b66\u4eba\u5de5\u667a\u80fd\u5e26\u6765\u4e86\u6311\u6218\u3002\u6211\u4eec\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u6765\u7814\u7a76\u80a4\u8272\u4e4b\u5916\u7684\u989c\u8272\u5bf9\u6bd4\u5bf9\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u4e2d\u6076\u6027\u80bf\u7624\u68c0\u6d4b\u7684\u5f71\u54cd\uff1a\u6211\u4eec\u5047\u8bbe\u9664\u4e86\u80a4\u8272\u4e4b\u5916\uff0c\u75c5\u53d8\u533a\u57df\u548c\u76ae\u80a4\u4e4b\u95f4\u7684\u8272\u5dee\u4e5f\u5728\u76ae\u80a4\u79d1\u6076\u6027\u80bf\u7624\u68c0\u6d4b\u6027\u80fd\u4e2d\u53d1\u6325\u7740\u4f5c\u7528\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u4e3a\u4e86\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6807\u8bb0\u65b9\u6cd5\u6765\u91cf\u5316\u6bcf\u4e2a\u56fe\u50cf\u7684\u989c\u8272\u5bf9\u6bd4\u5ea6\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u663e\u793a\u5c0f\u7684\u6807\u8bb0\u53d8\u5316\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\\textit{\u552f\u4e00}\u591a\u6837\u5316\u80a4\u8272\u548c\u75c5\u7406\u8bc1\u5b9e\u7684\u76ae\u80a4\u75be\u75c5\u6570\u636e\u96c6DDI\uff0c\u4ea7\u751f\\textbf{DDI-CoCo\u6570\u636e\u96c6}\uff0c\u5e76\u4e14\u6211\u4eec\u89c2\u5bdf\u5230\u9ad8\u8272\u5dee\u7ec4\u548c\u4f4e\u8272\u5dee\u7ec4\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u8fd9\u79cd\u5dee\u5f02\u5728\u5404\u79cd\u6700\u5148\u8fdb\u7684 (SoTA) \u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u652f\u6301\u4e86\u6211\u4eec\u7684\u5047\u8bbe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u80a4\u8272\u548c\u8272\u5dee\u6548\u5e94\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u8868\u660e\u8272\u5dee\u53ef\u80fd\u662f\u80a4\u8272\u4e4b\u95f4\u6a21\u578b\u6027\u80fd\u504f\u5dee\u80cc\u540e\u7684\u53e6\u4e00\u4e2a\u539f\u56e0\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u76ae\u80a4\u75c5\u5b66\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8865\u5145\u89d2\u5ea6\uff0c\u4ee5\u6539\u5584\u76ae\u80a4\u75be\u75c5\u7684\u68c0\u6d4b\u3002|[2401.13280v1](http://arxiv.org/pdf/2401.13280v1)|**[link](https://github.com/charismaticchiu/ddi-coco)**|\n", "2401.13264": "|**2024-01-24**|**Enhancing cross-domain detection: adaptive class-aware contrastive transformer**|\u589e\u5f3a\u8de8\u57df\u68c0\u6d4b\uff1a\u81ea\u9002\u5e94\u7c7b\u611f\u77e5\u5bf9\u6bd4\u53d8\u538b\u5668|Ziru Zeng, Yue Ding, Hongtao Lu|Recently,the detection transformer has gained substantial attention for its inherent minimal post-processing requirement.However,this paradigm relies on abundant training data,yet in the context of the cross-domain adaptation,insufficient labels in the target domain exacerbate issues of class imbalance and model performance degradation.To address these challenges, we propose a novel class-aware cross domain detection transformer based on the adversarial learning and mean-teacher framework.First,considering the inconsistencies between the classification and regression tasks,we introduce an IoU-aware prediction branch and exploit the consistency of classification and location scores to filter and reweight pseudo labels.Second, we devise a dynamic category threshold refinement to adaptively manage model confidence.Third,to alleviate the class imbalance,an instance-level class-aware contrastive learning module is presented to encourage the generation of discriminative features for each class,particularly benefiting minority classes.Experimental results across diverse domain-adaptive scenarios validate our method's effectiveness in improving performance and alleviating class imbalance issues,which outperforms the state-of-the-art transformer based methods.|\u8fd1\u5e74\u6765\uff0c\u68c0\u6d4b\u53d8\u538b\u5668\u56e0\u5176\u56fa\u6709\u7684\u6700\u5c0f\u540e\u5904\u7406\u8981\u6c42\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8303\u5f0f\u4f9d\u8d56\u4e8e\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u8de8\u57df\u9002\u5e94\u7684\u80cc\u666f\u4e0b\uff0c\u76ee\u6807\u57df\u4e2d\u7684\u6807\u7b7e\u4e0d\u8db3\u52a0\u5267\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6027\u5b66\u4e60\u548c\u5e73\u5747\u6559\u5e08\u6846\u67b6\u7684\u65b0\u578b\u7c7b\u611f\u77e5\u8de8\u57df\u68c0\u6d4b\u53d8\u538b\u5668\u3002\u9996\u5148\uff0c\u8003\u8651\u5230\u5206\u7c7b\u4efb\u52a1\u548c\u56de\u5f52\u4efb\u52a1\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd IoU \u611f\u77e5\u6a21\u578b\u9884\u6d4b\u5206\u652f\u5e76\u5229\u7528\u5206\u7c7b\u548c\u4f4d\u7f6e\u5206\u6570\u7684\u4e00\u81f4\u6027\u6765\u8fc7\u6ee4\u548c\u91cd\u65b0\u52a0\u6743\u4f2a\u6807\u7b7e\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u52a8\u6001\u7c7b\u522b\u9608\u503c\u7ec6\u5316\u6765\u81ea\u9002\u5e94\u7ba1\u7406\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3002\u7b2c\u4e09\uff0c\u4e3a\u4e86\u7f13\u89e3\u7c7b\u4e0d\u5e73\u8861\uff0c\u5b9e\u4f8b\u7ea7\u7c7b\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u7684\u63d0\u51fa\u662f\u4e3a\u4e86\u9f13\u52b1\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u751f\u6210\u5224\u522b\u6027\u7279\u5f81\uff0c\u7279\u522b\u662f\u6709\u5229\u4e8e\u5c11\u6570\u7c7b\u522b\u3002\u8de8\u4e0d\u540c\u9886\u57df\u81ea\u9002\u5e94\u573a\u666f\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u6027\u80fd\u548c\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65b9\u6cd5\u3002|[2401.13264v1](http://arxiv.org/pdf/2401.13264v1)|null|\n", "2401.13220": "|**2024-01-24**|**Segment Any Cell: A SAM-based Auto-prompting Fine-tuning Framework for Nuclei Segmentation**|\u5206\u5272\u4efb\u610f\u7ec6\u80de\uff1a\u57fa\u4e8e SAM \u7684\u7ec6\u80de\u6838\u5206\u5272\u81ea\u52a8\u63d0\u793a\u5fae\u8c03\u6846\u67b6|Saiyang Na, Yuzhi Guo, Feng Jiang, Hehuan Ma, Junzhou Huang|In the rapidly evolving field of AI research, foundational models like BERT and GPT have significantly advanced language and vision tasks. The advent of pretrain-prompting models such as ChatGPT and Segmentation Anything Model (SAM) has further revolutionized image segmentation. However, their applications in specialized areas, particularly in nuclei segmentation within medical imaging, reveal a key challenge: the generation of high-quality, informative prompts is as crucial as applying state-of-the-art (SOTA) fine-tuning techniques on foundation models. To address this, we introduce Segment Any Cell (SAC), an innovative framework that enhances SAM specifically for nuclei segmentation. SAC integrates a Low-Rank Adaptation (LoRA) within the attention layer of the Transformer to improve the fine-tuning process, outperforming existing SOTA methods. It also introduces an innovative auto-prompt generator that produces effective prompts to guide segmentation, a critical factor in handling the complexities of nuclei segmentation in biomedical imaging. Our extensive experiments demonstrate the superiority of SAC in nuclei segmentation tasks, proving its effectiveness as a tool for pathologists and researchers. Our contributions include a novel prompt generation strategy, automated adaptability for diverse segmentation tasks, the innovative application of Low-Rank Attention Adaptation in SAM, and a versatile framework for semantic segmentation challenges.|\u5728\u5feb\u901f\u53d1\u5c55\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9886\u57df\uff0cBERT \u548c GPT \u7b49\u57fa\u7840\u6a21\u578b\u5177\u6709\u663e\u7740\u5148\u8fdb\u7684\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u3002 ChatGPT \u548c Segmentation Anything Model (SAM) \u7b49\u8bad\u7ec3\u524d\u63d0\u793a\u6a21\u578b\u7684\u51fa\u73b0\u8fdb\u4e00\u6b65\u5f7b\u5e95\u6539\u53d8\u4e86\u56fe\u50cf\u5206\u5272\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u7ec6\u80de\u6838\u5206\u5272\u4e2d\uff0c\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\uff1a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u63d0\u793a\u4e0e\u5c06\u6700\u5148\u8fdb\u7684 (SOTA) \u5fae\u8c03\u6280\u672f\u5e94\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Segment Any Cell (SAC)\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u7ec6\u80de\u6838\u5206\u5272\u589e\u5f3a\u4e86 SAM\u3002 SAC \u5728 Transformer \u7684\u6ce8\u610f\u529b\u5c42\u4e2d\u96c6\u6210\u4e86\u4f4e\u79e9\u9002\u5e94 (LoRA)\uff0c\u4ee5\u6539\u8fdb\u5fae\u8c03\u8fc7\u7a0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684 SOTA \u65b9\u6cd5\u3002\u5b83\u8fd8\u5f15\u5165\u4e86\u521b\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u751f\u6210\u5668\uff0c\u53ef\u4ea7\u751f\u6709\u6548\u7684\u63d0\u793a\u6765\u6307\u5bfc\u5206\u5272\uff0c\u8fd9\u662f\u5904\u7406\u751f\u7269\u533b\u5b66\u6210\u50cf\u4e2d\u7ec6\u80de\u6838\u5206\u5272\u590d\u6742\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86 SAC \u5728\u7ec6\u80de\u6838\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u4f5c\u4e3a\u75c5\u7406\u5b66\u5bb6\u548c\u7814\u7a76\u4eba\u5458\u5de5\u5177\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\u65b0\u9896\u7684\u63d0\u793a\u751f\u6210\u7b56\u7565\u3001\u5bf9\u4e0d\u540c\u5206\u5272\u4efb\u52a1\u7684\u81ea\u52a8\u9002\u5e94\u6027\u3001SAM \u4e2d\u4f4e\u79e9\u6ce8\u610f\u529b\u9002\u5e94\u7684\u521b\u65b0\u5e94\u7528\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u6311\u6218\u7684\u591a\u529f\u80fd\u6846\u67b6\u3002|[2401.13220v1](http://arxiv.org/pdf/2401.13220v1)|null|\n", "2401.13214": "|**2024-01-24**|**AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network**|AMANet\uff1a\u5229\u7528\u81ea\u9002\u5e94\u591a\u5c42\u6b21\u6ce8\u610f\u529b\u7f51\u7edc\u63a8\u8fdb SAR \u8239\u8236\u68c0\u6d4b|Xiaolin Ma, Junkai Cheng, Aihua Li, Yuhua Zhang, Zhilong Lin|Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-hierarchical attention network (AMANet) by embedding the AMAM between the backbone network and the feature pyramid network (FPN). Besides, the AMAM can be readily inserted between different frameworks to improve object detection. Lastly, extensive experiments on two large-scale SAR ship detection datasets demonstrate that our AMANet method is superior to state-of-the-art methods.|\u6700\u8fd1\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u56fe\u50cf\u7684\u8239\u8236\u68c0\u6d4b\u3002\u5c3d\u7ba1\u5f00\u53d1\u4e86\u591a\u79cd\u8239\u8236\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u6cbf\u6d77\u73af\u5883\u7684\u7279\u5f81\u6709\u9650\u548c\u6742\u4e71\uff0c\u68c0\u6d4b\u5c0f\u578b\u6cbf\u6d77\u8239\u8236\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u591a\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\uff08AMAM\uff09\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u805a\u5408\u6765\u81ea\u5404\u4e2a\u7279\u5f81\u5c42\u7684\u663e\u7740\u7279\u5f81\uff0c\u5373\u4f7f\u5728\u590d\u6742\u7684\u73af\u5883\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u878d\u5408\u76f8\u90bb\u7279\u5f81\u5c42\u7684\u4fe1\u606f\u6765\u589e\u5f3a\u5bf9\u8f83\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u8fc7\u6ee4\u6389\u590d\u6742\u80cc\u666f\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u6211\u4eec\u5256\u6790\u4e86\u901a\u9053\u4e0a\u5148\u524d\u878d\u5408\u7684\u591a\u7ea7\u7279\u5f81\uff0c\u5355\u72ec\u6316\u6398\u663e\u7740\u533a\u57df\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u5408\u5e76\u6765\u81ea\u4e0d\u540c\u901a\u9053\u7684\u7279\u5f81\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u4e3b\u5e72\u7f51\u7edc\u548c\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08FPN\uff09\u4e4b\u95f4\u5d4c\u5165AMAM\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u591a\u5c42\u6b21\u6ce8\u610f\u529b\u7f51\u7edc\uff08AMANet\uff09\u3002\u6b64\u5916\uff0cAMAM \u53ef\u4ee5\u8f7b\u677e\u63d2\u5165\u4e0d\u540c\u6846\u67b6\u4e4b\u95f4\uff0c\u4ee5\u6539\u8fdb\u5bf9\u8c61\u68c0\u6d4b\u3002\u6700\u540e\uff0c\u5bf9\u4e24\u4e2a\u5927\u578b SAR \u8239\u8236\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 AMANet \u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|[2401.13214v1](http://arxiv.org/pdf/2401.13214v1)|null|\n", "2401.13213": "|**2024-01-24**|**Common-Sense Bias Discovery and Mitigation for Classification Tasks**|\u5206\u7c7b\u4efb\u52a1\u7684\u5e38\u8bc6\u6027\u504f\u5dee\u53d1\u73b0\u548c\u7f13\u89e3|Miao Zhang, Zee fryer, Ben Colman, Ali Shahriyari, Gaurav Bharaj|Machine learning model bias can arise from dataset composition: sensitive features correlated to the learning target disturb the model decision rule and lead to performance differences along the features. Existing de-biasing work captures prominent and delicate image features which are traceable in model latent space, like colors of digits or background of animals. However, using the latent space is not sufficient to understand all dataset feature correlations. In this work, we propose a framework to extract feature clusters in a dataset based on image descriptions, allowing us to capture both subtle and coarse features of the images. The feature co-occurrence pattern is formulated and correlation is measured, utilizing a human-in-the-loop for examination. The analyzed features and correlations are human-interpretable, so we name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive correlations in a dataset, we demonstrate that downstream model bias can be mitigated by adjusting image sampling weights, without requiring a sensitive group label supervision. Experiments show that our method discovers novel biases on multiple classification tasks for two benchmark image datasets, and the intervention outperforms state-of-the-art unsupervised bias mitigation methods.|\u673a\u5668\u5b66\u4e60\u6a21\u578b\u504f\u5dee\u53ef\u80fd\u6e90\u4e8e\u6570\u636e\u96c6\u7ec4\u6210\uff1a\u4e0e\u5b66\u4e60\u76ee\u6807\u76f8\u5173\u7684\u654f\u611f\u7279\u5f81\u4f1a\u5e72\u6270\u6a21\u578b\u51b3\u7b56\u89c4\u5219\uff0c\u5e76\u5bfc\u81f4\u7279\u5f81\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002\u73b0\u6709\u7684\u53bb\u504f\u5de5\u4f5c\u6355\u83b7\u4e86\u53ef\u5728\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8ffd\u8e2a\u7684\u7a81\u51fa\u4e14\u7cbe\u81f4\u7684\u56fe\u50cf\u7279\u5f81\uff0c\u4f8b\u5982\u6570\u5b57\u7684\u989c\u8272\u6216\u52a8\u7269\u7684\u80cc\u666f\u3002\u7136\u800c\uff0c\u4f7f\u7528\u6f5c\u5728\u7a7a\u95f4\u4e0d\u8db3\u4ee5\u7406\u89e3\u6240\u6709\u6570\u636e\u96c6\u7279\u5f81\u76f8\u5173\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u50cf\u63cf\u8ff0\u63d0\u53d6\u6570\u636e\u96c6\u4e2d\u7684\u7279\u5f81\u7c07\u7684\u6846\u67b6\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6355\u83b7\u56fe\u50cf\u7684\u5fae\u5999\u548c\u7c97\u7565\u7279\u5f81\u3002\u5236\u5b9a\u7279\u5f81\u5171\u73b0\u6a21\u5f0f\u5e76\u6d4b\u91cf\u76f8\u5173\u6027\uff0c\u5229\u7528\u4eba\u5728\u73af\u8fdb\u884c\u68c0\u67e5\u3002\u5206\u6790\u7684\u7279\u5f81\u548c\u76f8\u5173\u6027\u662f\u4eba\u7c7b\u53ef\u4ee5\u89e3\u91ca\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8be5\u65b9\u6cd5\u547d\u540d\u4e3a\u5e38\u8bc6\u504f\u5dee\u53d1\u73b0\uff08CSBD\uff09\u3002\u5728\u66b4\u9732\u6570\u636e\u96c6\u4e2d\u7684\u654f\u611f\u76f8\u5173\u6027\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u56fe\u50cf\u91c7\u6837\u6743\u91cd\u6765\u51cf\u8f7b\u4e0b\u6e38\u6a21\u578b\u504f\u5dee\uff0c\u800c\u4e0d\u9700\u8981\u654f\u611f\u7ec4\u6807\u7b7e\u76d1\u7763\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d1\u73b0\u4e86\u4e24\u4e2a\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u7684\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u7684\u65b0\u504f\u5dee\uff0c\u5e76\u4e14\u5e72\u9884\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u3002|[2401.13213v1](http://arxiv.org/pdf/2401.13213v1)|null|\n", "2401.13212": "|**2024-01-24**|**AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation**|AdCorDA\uff1a\u901a\u8fc7\u5bf9\u6297\u6027\u6821\u6b63\u548c\u57df\u9002\u5e94\u8fdb\u884c\u5206\u7c7b\u5668\u7ec6\u5316|Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark|This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. The adversarial correction technique also results in enhanced robustness to adversarial attacks.|\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\uff0c\u7528\u4e8e\u6539\u8fdb\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u7f51\u7edc\u3002\u6240\u63d0\u51fa\u7684 AdCorDA \u65b9\u6cd5\u57fa\u4e8e\u8bad\u7ec3\u96c6\u7684\u4fee\u6539\u5e76\u5229\u7528\u7f51\u7edc\u6743\u91cd\u548c\u5c42\u8f93\u5165\u4e4b\u95f4\u7684\u5bf9\u5076\u6027\u3002\u6211\u4eec\u79f0\u4e4b\u4e3a\u8f93\u5165\u7a7a\u95f4\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u7531\u4e24\u4e2a\u9636\u6bb5\u7ec4\u6210\uff1a\u5bf9\u6297\u6027\u6821\u6b63\u548c\u57df\u9002\u5e94\u3002\u5bf9\u6297\u6027\u7ea0\u6b63\u4f7f\u7528\u5bf9\u6297\u6027\u653b\u51fb\u6765\u7ea0\u6b63\u4e0d\u6b63\u786e\u7684\u8bad\u7ec3\u96c6\u5206\u7c7b\u3002\u8bad\u7ec3\u96c6\u4e2d\u9519\u8bef\u5206\u7c7b\u7684\u6837\u672c\u88ab\u53bb\u9664\u5e76\u7528\u5bf9\u6297\u6027\u6821\u6b63\u7684\u6837\u672c\u66ff\u6362\u4ee5\u5f62\u6210\u65b0\u7684\u8bad\u7ec3\u96c6\uff0c\u7136\u540e\u5728\u7b2c\u4e8c\u9636\u6bb5\u6267\u884c\u57df\u9002\u5e94\u56de\u5230\u539f\u59cb\u8bad\u7ec3\u96c6\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cCIFAR-100 \u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u663e\u7740\u63d0\u9ad8\u4e86 5% \u4ee5\u4e0a\u3002\u8be5\u6280\u672f\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6743\u91cd\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u7ec6\u5316\uff0c\u5176\u4e2d\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u6bd4\u57fa\u7ebf\u6709\u663e\u7740\u63d0\u9ad8\u3002\u5bf9\u6297\u6027\u6821\u6b63\u6280\u672f\u8fd8\u53ef\u4ee5\u589e\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002|[2401.13212v1](http://arxiv.org/pdf/2401.13212v1)|null|\n", "2401.13205": "|**2024-01-24**|**Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size**|\u901a\u8fc7\u5c40\u90e8\u6df7\u5408\u548c\u81ea\u9002\u5e94\u6b65\u957f\u63d0\u9ad8\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u53ef\u8fc1\u79fb\u6027|Junlin Liu, Xinchen Lyu|Adversarial examples are one critical security threat to various visual applications, where injected human-imperceptible perturbations can confuse the output.Generating transferable adversarial examples in the black-box setting is crucial but challenging in practice. Existing input-diversity-based methods adopt different image transformations, but may be inefficient due to insufficient input diversity and an identical perturbation step size. Motivated by the fact that different image regions have distinctive weights in classification, this paper proposes a black-box adversarial generative framework by jointly designing enhanced input diversity and adaptive step sizes. We design local mixup to randomly mix a group of transformed adversarial images, strengthening the input diversity. For precise adversarial generation, we project the perturbation into the $tanh$ space to relax the boundary constraint. Moreover, the step sizes of different regions can be dynamically adjusted by integrating a second-order momentum.Extensive experiments on ImageNet validate that our framework can achieve superior transferability compared to state-of-the-art baselines.|\u5bf9\u6297\u6027\u793a\u4f8b\u662f\u5bf9\u5404\u79cd\u89c6\u89c9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e00\u79cd\u5173\u952e\u5b89\u5168\u5a01\u80c1\uff0c\u5176\u4e2d\u6ce8\u5165\u7684\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u6270\u52a8\u53ef\u80fd\u4f1a\u6df7\u6dc6\u8f93\u51fa\u3002\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u751f\u6210\u53ef\u8f6c\u79fb\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u8f93\u5165\u591a\u6837\u6027\u7684\u65b9\u6cd5\u91c7\u7528\u4e0d\u540c\u7684\u56fe\u50cf\u53d8\u6362\uff0c\u4f46\u7531\u4e8e\u8f93\u5165\u591a\u6837\u6027\u4e0d\u8db3\u548c\u76f8\u540c\u7684\u6270\u52a8\u6b65\u957f\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u3002\u7531\u4e8e\u4e0d\u540c\u56fe\u50cf\u533a\u57df\u5728\u5206\u7c7b\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u6743\u91cd\uff0c\u672c\u6587\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u589e\u5f3a\u7684\u8f93\u5165\u591a\u6837\u6027\u548c\u81ea\u9002\u5e94\u6b65\u957f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ed1\u76d2\u5bf9\u6297\u751f\u6210\u6846\u67b6\u3002\u6211\u4eec\u8bbe\u8ba1\u5c40\u90e8\u6df7\u5408\u6765\u968f\u673a\u6df7\u5408\u4e00\u7ec4\u53d8\u6362\u540e\u7684\u5bf9\u6297\u56fe\u50cf\uff0c\u589e\u5f3a\u8f93\u5165\u591a\u6837\u6027\u3002\u4e3a\u4e86\u7cbe\u786e\u751f\u6210\u5bf9\u6297\u6027\uff0c\u6211\u4eec\u5c06\u6270\u52a8\u6295\u5f71\u5230 $tanh$ \u7a7a\u95f4\u4e2d\u4ee5\u653e\u677e\u8fb9\u754c\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u53ef\u4ee5\u901a\u8fc7\u79ef\u5206\u4e8c\u9636\u52a8\u91cf\u6765\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u533a\u57df\u7684\u6b65\u957f\u3002\u5728 ImageNet \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6846\u67b6\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u53ef\u8fc1\u79fb\u6027\u3002|[2401.13205v1](http://arxiv.org/pdf/2401.13205v1)|null|\n", "2401.13193": "|**2024-01-24**|**Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN**|Catch-Up Mix\uff1aCNN \u4e2d\u9677\u5165\u56f0\u5883\u7684\u8fc7\u6ee4\u5668\u7684 Catch-Up \u7c7b|Minsoo Kang, Minkoo Kang, Suhyun Kim|Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, our idea is to mitigate the problem of over-reliance on strong filters by substituting highly activated features. To this end, we present a novel method called Catch-up Mix, which provides learning opportunities to a wide range of filters during training, focusing on filters that may lag behind. By mixing activation maps with relatively lower norms, Catch-up Mix promotes the development of more diverse representations and reduces reliance on a small subset of filters. Experimental results demonstrate the superiority of our method in various vision classification datasets, providing enhanced robustness.|\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u5177\u6709\u5f88\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u7ecf\u5e38\u9762\u4e34\u4e0e\u590d\u6742\u6027\u548c\u8fc7\u5ea6\u62df\u5408\u76f8\u5173\u7684\u6311\u6218\u3002\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u95ee\u9898\u662f\uff0c\u8be5\u6a21\u578b\u901a\u5e38\u4e25\u91cd\u4f9d\u8d56\u6709\u9650\u7684\u8fc7\u6ee4\u5668\u5b50\u96c6\u6765\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u79cd\u4f9d\u8d56\u6027\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u635f\uff0c\u5e76\u4e14\u66f4\u5bb9\u6613\u53d7\u5230\u5fae\u5c0f\u53d8\u5316\u7684\u5f71\u54cd\u3002\u867d\u7136\u6743\u91cd\u8870\u51cf\u3001\u4e22\u5931\u548c\u6570\u636e\u589e\u5f3a\u7b49\u6b63\u5219\u5316\u6280\u672f\u901a\u5e38\u7528\u4e8e\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u65e0\u6cd5\u76f4\u63a5\u89e3\u51b3\u5bf9\u7279\u5b9a\u8fc7\u6ee4\u5668\u7684\u4f9d\u8d56\u3002\u6211\u4eec\u7684\u89c2\u5bdf\u8868\u660e\uff0c\u5f53\u6162\u901f\u5b66\u4e60\u8fc7\u6ee4\u5668\u56e0\u5feb\u901f\u5b66\u4e60\u8fc7\u6ee4\u5668\u800c\u88ab\u5265\u593a\u5b66\u4e60\u673a\u4f1a\u65f6\uff0c\u4e25\u91cd\u4f9d\u8d56\u95ee\u9898\u5c31\u4f1a\u53d8\u5f97\u4e25\u91cd\u3002\u4ece\u56fe\u50cf\u589e\u5f3a\u7814\u7a76\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u901a\u8fc7\u5220\u9664\u548c\u66ff\u6362\u90e8\u5206\u56fe\u50cf\u6765\u5bf9\u6297\u5bf9\u7279\u5b9a\u56fe\u50cf\u533a\u57df\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u6211\u4eec\u7684\u60f3\u6cd5\u662f\u901a\u8fc7\u66ff\u6362\u9ad8\u5ea6\u6fc0\u6d3b\u7684\u7279\u5f81\u6765\u51cf\u8f7b\u5bf9\u5f3a\u8fc7\u6ee4\u5668\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a Catch-up Mix \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5b83\u5728\u8bad\u7ec3\u671f\u95f4\u4e3a\u5404\u79cd\u8fc7\u6ee4\u5668\u63d0\u4f9b\u5b66\u4e60\u673a\u4f1a\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u80fd\u843d\u540e\u7684\u8fc7\u6ee4\u5668\u3002\u901a\u8fc7\u5c06\u6fc0\u6d3b\u56fe\u4e0e\u76f8\u5bf9\u8f83\u4f4e\u7684\u89c4\u8303\u6df7\u5408\uff0cCatch-up Mix \u4fc3\u8fdb\u4e86\u66f4\u591a\u6837\u5316\u8868\u793a\u7684\u5f00\u53d1\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u4e00\u5c0f\u90e8\u5206\u8fc7\u6ee4\u5668\u7684\u4f9d\u8d56\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u89c6\u89c9\u5206\u7c7b\u6570\u636e\u96c6\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002|[2401.13193v1](http://arxiv.org/pdf/2401.13193v1)|null|\n", "2401.13191": "|**2024-01-24**|**Towards Multi-domain Face Landmark Detection with Synthetic Data from Diffusion model**|\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u591a\u57df\u4eba\u8138\u7279\u5f81\u70b9\u68c0\u6d4b|Yuanming Li, Gwantae Kim, Jeong-gi Kwak, Bon-hwa Ku, Hanseok Ko|Recently, deep learning-based facial landmark detection for in-the-wild faces has achieved significant improvement. However, there are still challenges in face landmark detection in other domains (e.g. cartoon, caricature, etc). This is due to the scarcity of extensively annotated training data. To tackle this concern, we design a two-stage training approach that effectively leverages limited datasets and the pre-trained diffusion model to obtain aligned pairs of landmarks and face in multiple domains. In the first stage, we train a landmark-conditioned face generation model on a large dataset of real faces. In the second stage, we fine-tune the above model on a small dataset of image-landmark pairs with text prompts for controlling the domain. Our new designs enable our method to generate high-quality synthetic paired datasets from multiple domains while preserving the alignment between landmarks and facial features. Finally, we fine-tuned a pre-trained face landmark detection model on the synthetic dataset to achieve multi-domain face landmark detection. Our qualitative and quantitative results demonstrate that our method outperforms existing methods on multi-domain face landmark detection.|\u6700\u8fd1\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u91ce\u5916\u4eba\u8138\u7279\u5f81\u70b9\u68c0\u6d4b\u53d6\u5f97\u4e86\u663e\u7740\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u5176\u4ed6\u9886\u57df\uff08\u4f8b\u5982\u5361\u901a\u3001\u6f2b\u753b\u7b49\uff09\u7684\u4eba\u8138\u7279\u5f81\u70b9\u68c0\u6d4b\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u8fd9\u662f\u7531\u4e8e\u7f3a\u4e4f\u5e7f\u6cdb\u6ce8\u91ca\u7684\u8bad\u7ec3\u6570\u636e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u6709\u9650\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u6765\u83b7\u5f97\u591a\u4e2a\u9886\u57df\u4e2d\u5bf9\u9f50\u7684\u5730\u6807\u548c\u4eba\u8138\u5bf9\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u5728\u5927\u578b\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5730\u6807\u6761\u4ef6\u4eba\u8138\u751f\u6210\u6a21\u578b\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u5728\u56fe\u50cf\u5730\u6807\u5bf9\u7684\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0a\u8ff0\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5e26\u6709\u7528\u4e8e\u63a7\u5236\u57df\u7684\u6587\u672c\u63d0\u793a\u3002\u6211\u4eec\u7684\u65b0\u8bbe\u8ba1\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u4ece\u591a\u4e2a\u9886\u57df\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u540c\u65f6\u4fdd\u6301\u5730\u6807\u548c\u9762\u90e8\u7279\u5f81\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u4eba\u8138\u6807\u5fd7\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u591a\u57df\u4eba\u8138\u6807\u5fd7\u68c0\u6d4b\u3002\u6211\u4eec\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u57df\u4eba\u8138\u7279\u5f81\u70b9\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|[2401.13191v1](http://arxiv.org/pdf/2401.13191v1)|null|\n", "2401.13174": "|**2024-01-24**|**Boundary and Relation Distillation for Semantic Segmentation**|\u8bed\u4e49\u5206\u5272\u7684\u8fb9\u754c\u548c\u5173\u7cfb\u84b8\u998f|Dong Zhang, Pingcheng Dong, Xinting Hu, Long Chen, Kwang-Ting Cheng|Recently, it has been revealed that small semantic segmentation (SS) models exhibit a tendency to make errors in maintaining boundary region completeness and preserving target region connectivity, despite their effective segmentation of the main object regions. To address these errors, we propose a targeted boundary and relation distillation (BRD) strategy using knowledge distillation from large teacher models to small student models. Specifically, the boundary distillation extracts explicit object boundaries from the hierarchical feature maps of the backbone network, subsequently enhancing the student model's mask quality in boundary regions. Concurrently, the relation distillation transfers implicit relations from the teacher model to the student model using pixel-level self-relation as a bridge, ensuring that the student's mask has strong target region connectivity. The proposed BRD is designed concretely for SS and is characterized by simplicity and efficiency. Through experimental evaluations on multiple SS datasets, including Pascal VOC 2012, Cityscapes, ADE20K, and COCO-Stuff 10K, we demonstrated that BRD significantly surpasses the current methods without increasing the inference costs, generating crisp region boundaries and smooth connecting regions that are challenging for small models.|\u6700\u8fd1\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5c0f\u578b\u8bed\u4e49\u5206\u5272\uff08SS\uff09\u6a21\u578b\u5c3d\u7ba1\u80fd\u591f\u6709\u6548\u5206\u5272\u4e3b\u8981\u5bf9\u8c61\u533a\u57df\uff0c\u4f46\u5728\u4fdd\u6301\u8fb9\u754c\u533a\u57df\u5b8c\u6574\u6027\u548c\u4fdd\u6301\u76ee\u6807\u533a\u57df\u8fde\u901a\u6027\u65b9\u9762\u5bb9\u6613\u51fa\u9519\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9519\u8bef\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u9488\u5bf9\u6027\u7684\u8fb9\u754c\u548c\u5173\u7cfb\u84b8\u998f\uff08BRD\uff09\u7b56\u7565\uff0c\u4f7f\u7528\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u5230\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fb9\u754c\u84b8\u998f\u4ece\u4e3b\u5e72\u7f51\u7edc\u7684\u5206\u5c42\u7279\u5f81\u56fe\u4e2d\u63d0\u53d6\u660e\u786e\u7684\u5bf9\u8c61\u8fb9\u754c\uff0c\u968f\u540e\u589e\u5f3a\u5b66\u751f\u6a21\u578b\u5728\u8fb9\u754c\u533a\u57df\u7684\u63a9\u6a21\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u5173\u7cfb\u84b8\u998f\u4ee5\u50cf\u7d20\u7ea7\u81ea\u5173\u7cfb\u4e3a\u6865\u6881\uff0c\u5c06\u9690\u5f0f\u5173\u7cfb\u4ece\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u5b66\u751f\u6a21\u578b\uff0c\u786e\u4fdd\u5b66\u751f\u63a9\u6a21\u5177\u6709\u8f83\u5f3a\u7684\u76ee\u6807\u533a\u57df\u8fde\u63a5\u6027\u3002\u6240\u63d0\u51fa\u7684BRD\u662f\u9488\u5bf9SS\u5177\u4f53\u8bbe\u8ba1\u7684\uff0c\u5177\u6709\u7b80\u5355\u3001\u9ad8\u6548\u7684\u7279\u70b9\u3002\u901a\u8fc7\u5bf9\u591a\u4e2a SS \u6570\u636e\u96c6\uff08\u5305\u62ec Pascal VOC 2012\u3001Cityscapes\u3001ADE20K \u548c COCO-Stuff 10K\uff09\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e BRD \u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u7740\u8d85\u8d8a\u4e86\u5f53\u524d\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u6e05\u6670\u7684\u533a\u57df\u8fb9\u754c\u548c\u5e73\u6ed1\u7684\u8fde\u63a5\u533a\u57df\uff0c\u8fd9\u5bf9\u4e8e\u5c0f\u578b\u53f7\u3002|[2401.13174v1](http://arxiv.org/pdf/2401.13174v1)|null|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "OCR": {}, "\u751f\u6210\u6a21\u578b": {"2401.13581": "|**2024-01-24**|**Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation**|\u901a\u8fc7\u52a8\u6001\u5206\u7ec4\u548c\u539f\u578b\u805a\u5408\u5b9e\u73b0\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6df1\u5ea6\u805a\u7c7b|Haixin Zhang, Dong Huang|Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability. Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner. To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro. Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups. Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance. Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training. Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art. Code is available at https://github.com/Regan-Zhang/DigPro.|\u4ee5\u524d\u7684\u5bf9\u6bd4\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b9e\u4f8b\u7ea7\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u7ec4/\u7c07\u5185\u7684\u6210\u5458\u5173\u7cfb\uff0c\u8fd9\u53ef\u80fd\u4f1a\u4e25\u91cd\u524a\u5f31\u5176\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u4e00\u4e9b\u5206\u7ec4\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6837\u672c\u6765\u83b7\u53d6\u4f2a\u6807\u7b7e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4ee5\u6279\u91cf\u65b9\u5f0f\u6709\u6548\u66f4\u65b0\u5206\u7ec4\u5206\u914d\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u5177\u6709\u52a8\u6001\u5206\u7ec4\u548c\u539f\u578b\u805a\u5408\u529f\u80fd\uff0c\u79f0\u4e3a DigPro\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001\u5206\u7ec4\u5c06\u5bf9\u6bd4\u5b66\u4e60\u4ece\u5b9e\u4f8b\u7ea7\u6269\u5c55\u5230\u7ec4\u7ea7\uff0c\u8fd9\u5bf9\u4e8e\u53ca\u65f6\u66f4\u65b0\u7ec4\u662f\u6709\u6548\u4e14\u9ad8\u6548\u7684\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5728\u7403\u5f62\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9\u539f\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u79f0\u4e3a\u539f\u578b\u805a\u5408\uff0c\u5176\u76ee\u7684\u662f\u6700\u5927\u5316\u7c07\u95f4\u8ddd\u79bb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u901a\u8fc7\u671f\u671b\u6700\u5927\u5316\u6846\u67b6\uff0cDigPro \u5728\u81ea\u76d1\u7763\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u540c\u65f6\u5229\u7528\u4e86\u7d27\u51d1\u7684\u96c6\u7fa4\u5185\u8fde\u63a5\u3001\u826f\u597d\u5206\u79bb\u7684\u96c6\u7fa4\u548c\u9ad8\u6548\u7684\u7ec4\u66f4\u65b0\u3002\u5bf9\u516d\u4e2a\u56fe\u50cf\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Regan-Zhang/DigPro \u83b7\u53d6\u3002|[2401.13581v1](http://arxiv.org/pdf/2401.13581v1)|null|\n", "2401.13555": "|**2024-01-24**|**Benchmarking the Fairness of Image Upsampling Methods**|\u56fe\u50cf\u4e0a\u91c7\u6837\u65b9\u6cd5\u7684\u516c\u5e73\u6027\u57fa\u51c6\u6d4b\u8bd5|Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer, Johannes Lederer|Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.|\u8fd1\u5e74\u6765\uff0c\u7528\u4e8e\u521b\u5efa\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u5408\u6210\u5a92\u4f53\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8fc5\u901f\u53d1\u5c55\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5f88\u8bf1\u4eba\uff0c\u4f46\u8bc4\u4f30\u5176\u516c\u5e73\u6027\u7684\u56fa\u6709\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7528\u4e8e\u5bf9\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u548c\u516c\u5e73\u6027\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7efc\u5408\u6846\u67b6\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u7ec4\u6307\u6807$\\unicode{x2013}$\uff0c\u5176\u7075\u611f\u6765\u81ea\u4e8e\u5176\u76d1\u7763\u516c\u5e73\u6027\u5bf9\u5e94\u7269$\\unicode{x2013}$\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u3002\u4e13\u6ce8\u4e8e\u56fe\u50cf\u4e0a\u91c7\u6837\u7684\u5177\u4f53\u5e94\u7528\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u6db5\u76d6\u5404\u79cd\u73b0\u4ee3\u4e0a\u91c7\u6837\u65b9\u6cd5\u7684\u57fa\u51c6\u3002\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u5f15\u5165\u4e86 UnfairFace\uff0c\u5b83\u662f FairFace \u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u5b83\u590d\u5236\u4e86\u5e38\u89c1\u7684\u5927\u89c4\u6a21\u4eba\u8138\u6570\u636e\u96c6\u7684\u79cd\u65cf\u5206\u5e03\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u5f3a\u8c03\u4e86\u4f7f\u7528\u65e0\u504f\u8bad\u7ec3\u96c6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u7b97\u6cd5\u5982\u4f55\u54cd\u5e94\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7684\u53d8\u5316\u3002\u4ee4\u4eba\u9707\u60ca\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u6240\u8003\u8651\u7684\u65b9\u6cd5\u90fd\u6ca1\u6709\u4ea7\u751f\u7edf\u8ba1\u4e0a\u516c\u5e73\u4e14\u591a\u6837\u5316\u7684\u7ed3\u679c\u3002|[2401.13555v1](http://arxiv.org/pdf/2401.13555v1)|null|\n", "2401.13505": "|**2024-01-24**|**Generative Human Motion Stylization in Latent Space**|\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u751f\u6210\u4eba\u4f53\u8fd0\u52a8\u98ce\u683c\u5316|Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng|Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reeanactment, content preservation, and generalization across various applications and settings. Project Page: https://yxmu.foo/GenMoStyle|\u4eba\u4f53\u52a8\u4f5c\u98ce\u683c\u5316\u65e8\u5728\u4fee\u6539\u8f93\u5165\u52a8\u4f5c\u7684\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5185\u5bb9\u4e0d\u53d8\u3002\u4e0e\u76f4\u63a5\u5728\u59ff\u52bf\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u7684\u73b0\u6709\u4f5c\u54c1\u4e0d\u540c\uff0c\u6211\u4eec\u5229\u7528\u9884\u8bad\u7ec3\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4f5c\u4e3a\u8fd0\u52a8\u63d0\u53d6\u548c\u6ce8\u5165\u66f4\u5177\u8868\u73b0\u529b\u548c\u9c81\u68d2\u6027\u7684\u8868\u793a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u4ea7\u751f\u5355\u8fd0\u52a8\uff08\u6f5c\u5728\uff09\u4ee3\u7801\u7684\u591a\u79cd\u98ce\u683c\u5316\u7ed3\u679c\u3002\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u8fd0\u52a8\u4ee3\u7801\u88ab\u5206\u89e3\u4e3a\u4e24\u4e2a\u7f16\u7801\u7ec4\u4ef6\uff1a\u786e\u5b9a\u6027\u5185\u5bb9\u4ee3\u7801\u548c\u9075\u5faa\u5148\u9a8c\u5206\u5e03\u7684\u6982\u7387\u98ce\u683c\u4ee3\u7801\uff1b\u7136\u540e\uff0c\u751f\u6210\u5668\u5bf9\u5185\u5bb9\u548c\u98ce\u683c\u4ee3\u7801\u7684\u968f\u673a\u7ec4\u5408\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u91cd\u5efa\u76f8\u5e94\u7684\u52a8\u4f5c\u4ee3\u7801\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u901a\u7528\u7684\uff0c\u5141\u8bb8\u4ece\u6807\u8bb0\u6216\u672a\u6807\u8bb0\u7684\u8fd0\u52a8\u98ce\u683c\u4e2d\u5b66\u4e60\u6982\u7387\u98ce\u683c\u7a7a\u95f4\uff0c\u5728\u98ce\u683c\u5316\u65b9\u9762\u4e5f\u63d0\u4f9b\u4e86\u663e\u7740\u7684\u7075\u6d3b\u6027\u3002\u5728\u63a8\u7406\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u53c2\u8003\u52a8\u4f5c\u6216\u6807\u7b7e\u4e2d\u7684\u98ce\u683c\u63d0\u793a\u6765\u5bf9\u52a8\u4f5c\u8fdb\u884c\u98ce\u683c\u5316\u3002\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u98ce\u683c\u8f93\u5165\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ece\u65e0\u6761\u4ef6\u98ce\u683c\u5148\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\u6765\u4fc3\u8fdb\u65b0\u9896\u7684\u91cd\u65b0\u98ce\u683c\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u98ce\u683c\u5316\u6a21\u578b\u5c3d\u7ba1\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u4f46\u5728\u98ce\u683c\u91cd\u6f14\u3001\u5185\u5bb9\u4fdd\u5b58\u4ee5\u53ca\u8de8\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u548c\u8bbe\u7f6e\u7684\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://yxmu.foo/GenMoStyle|[2401.13505v1](http://arxiv.org/pdf/2401.13505v1)|null|\n", "2401.13503": "|**2024-01-24**|**Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction**|\u901a\u8fc7\u90e8\u5206\u4fe1\u606f\u8fa8\u522b\u548c\u8de8\u7ea7\u4ea4\u4e92\u5b66\u4e60\u805a\u7c7b\u8868\u793a|Hai-Xin Zhang, Dong Huang, Hua-Bao Ling, Guang-Yu Zhang, Wei-jun Sun, Zi-hao Wen|In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework. In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated. After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces. Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches. The source code is available at https://github.com/Regan-Zhang/PICI.|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a PICI \u7684\u65b0\u578b\u6df1\u5ea6\u56fe\u50cf\u805a\u7c7b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8054\u5408\u5b66\u4e60\u6846\u67b6\u4e2d\u5f3a\u5236\u6267\u884c\u90e8\u5206\u4fe1\u606f\u533a\u5206\u548c\u8de8\u7ea7\u522b\u4ea4\u4e92\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5229\u7528 Transformer \u7f16\u7801\u5668\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u901a\u8fc7\u5b83\u5236\u5b9a\u5177\u6709\u4e24\u4e2a\u5e76\u884c\u589e\u5f3a\u89c6\u56fe\u7684\u8499\u7248\u56fe\u50cf\u5efa\u6a21\u3002\u901a\u8fc7 Transformer \u7f16\u7801\u5668\u4ece\u8499\u7248\u56fe\u50cf\u4e2d\u5bfc\u51fa\u7c7b\u522b\u6807\u8bb0\u540e\uff0c\u8fdb\u4e00\u6b65\u5408\u5e76\u4e86\u4e09\u4e2a\u90e8\u5206\u4fe1\u606f\u5b66\u4e60\u6a21\u5757\uff0c\u5305\u62ec\u7528\u4e8e\u901a\u8fc7\u8499\u7248\u56fe\u50cf\u91cd\u5efa\u6765\u8bad\u7ec3\u81ea\u52a8\u7f16\u7801\u5668\u7684 PISD \u6a21\u5757\u3001\u7528\u4e8e\u91c7\u7528\u4e24\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u7684 PICD \u6a21\u5757\u4ee5\u53ca\u7528\u4e8e\u5b9e\u4f8b\u7ea7\u548c\u96c6\u7fa4\u7ea7\u5b50\u7a7a\u95f4\u4e4b\u95f4\u4ea4\u4e92\u7684CLI\u6a21\u5757\u3002\u5728\u516d\u4e2a\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 PICI \u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u805a\u7c7b\u6027\u80fd\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/Regan-Zhang/PICI \u83b7\u53d6\u3002|[2401.13503v1](http://arxiv.org/pdf/2401.13503v1)|**[link](https://github.com/regan-zhang/pici)**|\n"}, "\u591a\u6a21\u6001": {"2401.13649": "|**2024-01-24**|**VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks**|VisualWebArena\uff1a\u5728\u5b9e\u9645\u89c6\u89c9 Web \u4efb\u52a1\u4e0a\u8bc4\u4f30\u591a\u6a21\u5f0f\u4ee3\u7406|Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried|Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.|\u80fd\u591f\u5728\u7f51\u7edc\u4e0a\u89c4\u5212\u3001\u63a8\u7406\u548c\u6267\u884c\u64cd\u4f5c\u7684\u81ea\u4e3b\u4ee3\u7406\u4e3a\u8ba1\u7b97\u673a\u4efb\u52a1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u9014\u7684\u9014\u5f84\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6587\u672c\u7684\u4ee3\u7406\uff0c\u5ffd\u7565\u4e86\u8bb8\u591a\u9700\u8981\u89c6\u89c9\u4fe1\u606f\u624d\u80fd\u6709\u6548\u89e3\u51b3\u7684\u81ea\u7136\u4efb\u52a1\u3002\u9274\u4e8e\u5927\u591a\u6570\u8ba1\u7b97\u673a\u754c\u9762\u8fce\u5408\u4eba\u7c7b\u611f\u77e5\uff0c\u89c6\u89c9\u4fe1\u606f\u901a\u5e38\u4f1a\u4ee5\u7eaf\u6587\u672c\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5229\u7528\u7684\u65b9\u5f0f\u589e\u5f3a\u6587\u672c\u6570\u636e\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 VisualWebArena\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u5f0f Web \u4ee3\u7406\u5728\u73b0\u5b9e \\textit{\u89c6\u89c9\u57fa\u7840\u4efb\u52a1} \u4e0a\u7684\u6027\u80fd\u3002 VisualWebArena \u5305\u542b\u4e00\u7ec4\u591a\u6837\u5316\u4e14\u590d\u6742\u7684\u57fa\u4e8e Web \u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3b\u591a\u6a21\u5f0f\u4ee3\u7406\u7684\u5404\u79cd\u529f\u80fd\u3002\u4e3a\u4e86\u8fbe\u5230\u8fd9\u4e2a\u57fa\u51c6\uff0c\u4ee3\u7406\u9700\u8981\u51c6\u786e\u5730\u5904\u7406\u56fe\u50cf\u6587\u672c\u8f93\u5165\uff0c\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5e76\u5728\u7f51\u7ad9\u4e0a\u6267\u884c\u64cd\u4f5c\u4ee5\u5b9e\u73b0\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\u3002\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684\u57fa\u4e8e LLM \u7684\u81ea\u4e3b\u4ee3\u7406\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u51e0\u4e2a\u591a\u6a21\u5f0f\u6a21\u578b\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u7eaf\u6587\u672c LLM \u4ee3\u7406\u7684\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6700\u5148\u8fdb\u7684\u591a\u6a21\u5f0f\u8bed\u8a00\u4ee3\u7406\u7684\u80fd\u529b\u5dee\u8ddd\u3002 VisualWebArena \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u5f0f\u81ea\u4e3b\u8bed\u8a00\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u7f51\u7edc\u81ea\u4e3b\u4ee3\u7406\u7684\u89c1\u89e3\u3002\u6211\u4eec\u7684\u4ee3\u7801\u3001\u57fa\u7ebf\u6a21\u578b\u548c\u6570\u636e\u53ef\u5728 https://jykoh.com/vwa \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.13649v1](http://arxiv.org/pdf/2401.13649v1)|null|\n", "2401.13627": "|**2024-01-24**|**Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild**|\u6269\u5c55\u81f3\u5353\u8d8a\uff1a\u5b9e\u8df5\u6a21\u578b\u6269\u5c55\u4ee5\u5728\u91ce\u5916\u6062\u590d\u7167\u7247\u822c\u771f\u5b9e\u7684\u56fe\u50cf|Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, Chao Dong|We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.|\u6211\u4eec\u4ecb\u7ecd SUPIR\uff08\u7f29\u653e\u56fe\u50cf\u6062\u590d\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7a81\u7834\u6027\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5148\u9a8c\u548c\u6a21\u578b\u7f29\u653e\u7684\u529b\u91cf\u3002\u5229\u7528\u591a\u6a21\u6001\u6280\u672f\u548c\u5148\u8fdb\u7684\u751f\u6210\u5148\u9a8c\uff0cSUPIR \u6807\u5fd7\u7740\u667a\u80fd\u548c\u771f\u5b9e\u56fe\u50cf\u6062\u590d\u9886\u57df\u7684\u91cd\u5927\u8fdb\u6b65\u3002\u4f5c\u4e3a SUPIR \u4e2d\u7684\u5173\u952e\u50ac\u5316\u5242\uff0c\u6a21\u578b\u7f29\u653e\u6781\u5927\u5730\u589e\u5f3a\u4e86\u5176\u529f\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u56fe\u50cf\u6062\u590d\u7684\u65b0\u6f5c\u529b\u3002\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b 2000 \u4e07\u5f20\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\uff0c\u6bcf\u5f20\u56fe\u50cf\u90fd\u5bcc\u542b\u63cf\u8ff0\u6027\u6587\u672c\u6ce8\u91ca\u3002 SUPIR \u63d0\u4f9b\u4e86\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6062\u590d\u56fe\u50cf\u7684\u529f\u80fd\uff0c\u62d3\u5bbd\u4e86\u5176\u5e94\u7528\u8303\u56f4\u548c\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u8d1f\u9762\u8d28\u91cf\u63d0\u793a\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6062\u590d\u5f15\u5bfc\u91c7\u6837\u65b9\u6cd5\u6765\u6291\u5236\u57fa\u4e8e\u751f\u6210\u7684\u6062\u590d\u4e2d\u9047\u5230\u7684\u4fdd\u771f\u5ea6\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86 SUPIR \u5353\u8d8a\u7684\u6062\u590d\u6548\u679c\u53ca\u5176\u901a\u8fc7\u6587\u672c\u63d0\u793a\u64cd\u4f5c\u6062\u590d\u7684\u65b0\u9896\u80fd\u529b\u3002|[2401.13627v1](http://arxiv.org/pdf/2401.13627v1)|null|\n", "2401.13478": "|**2024-01-24**|**SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval**|SciMMIR\uff1a\u79d1\u5b66\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5|Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, et.al.|Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR.|\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\uff08MMIR\uff09\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u901a\u8fc7\u9ad8\u7ea7\u8868\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u7814\u7a76\uff0c\u5df2\u7ecf\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u6587\u672c\u914d\u5bf9\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u8bc4\u4f30\u79d1\u5b66\u9886\u57df\u5185\u56fe\u6587\u914d\u5bf9 MMIR \u6027\u80fd\u7684\u57fa\u51c6\u663e\u793a\u51fa\u663e\u7740\u7684\u5dee\u8ddd\uff0c\u5176\u4e2d\u7528\u5b66\u672f\u8bed\u8a00\u63cf\u8ff0\u7684\u56fe\u8868\u548c\u8868\u683c\u56fe\u50cf\u901a\u5e38\u4e0d\u8d77\u91cd\u8981\u4f5c\u7528\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u5f00\u653e\u83b7\u53d6\u8bba\u6587\u96c6\u6765\u63d0\u53d6\u4e0e\u79d1\u5b66\u9886\u57df\u76f8\u5173\u7684\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u79d1\u5b66 MMIR (SciMMIR) \u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7531 53 \u4e07\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u7ec4\u6210\uff0c\u8fd9\u4e9b\u56fe\u50cf\u6587\u672c\u5bf9\u662f\u4ece\u79d1\u5b66\u6587\u6863\u4e2d\u5e26\u6709\u8be6\u7ec6\u8bf4\u660e\u7684\u56fe\u8868\u4e2d\u63d0\u53d6\u7684\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7528\u4e24\u7ea7\u5b50\u96c6-\u5b50\u7c7b\u522b\u5c42\u6b21\u7ed3\u6784\u6ce8\u91ca\u6765\u6ce8\u91ca\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u57fa\u7ebf\u8fdb\u884c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u5bf9\u8457\u540d\u7684\u591a\u6a21\u6001\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 CLIP \u548c BLIP\uff09\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u5206\u6790\u4e3a MMIR \u5728\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u8bbe\u7f6e\u7684\u5f71\u54cd\u4ee5\u53ca\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u7684\u5f71\u54cd\u3002\u6211\u4eec\u6240\u6709\u7684\u6570\u636e\u548c\u68c0\u67e5\u70b9\u5747\u53ef\u5728 https://github.com/Wusiwei0410/SciMMIR \u4e0a\u516c\u5f00\u83b7\u53d6\u3002|[2401.13478v1](http://arxiv.org/pdf/2401.13478v1)|null|\n", "2401.13418": "|**2024-01-24**|**Serial fusion of multi-modal biometric systems**|\u591a\u6a21\u6001\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u4e32\u884c\u878d\u5408|Gian Luca Marcialis, Paolo Mastinu, Fabio Roli|Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far. However, this approach exhibits some advantages with respect to the widely adopted parallel approaches. In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors. Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation. Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1.|\u8fc4\u4eca\u4e3a\u6b62\uff0c\u591a\u4e2a\u751f\u7269\u8bc6\u522b\u5339\u914d\u5668\u7684\u4e32\u884c\u6216\u8fde\u7eed\u878d\u5408\u5c1a\u672a\u5f97\u5230\u5f7b\u5e95\u7814\u7a76\u3002\u7136\u800c\uff0c\u76f8\u5bf9\u4e8e\u5e7f\u6cdb\u91c7\u7528\u7684\u5e76\u884c\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8868\u73b0\u51fa\u4e00\u4e9b\u4f18\u52bf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e\u4f5c\u8005\u4e4b\u524d\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u6b64\u7c7b\u7cfb\u7edf\u6027\u80fd\u7684\u65b0\u9896\u7406\u8bba\u6846\u67b6\u3002\u4ece\u7406\u8bba\u4e0a\u8bc4\u4f30\u4e86\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u4ee5\u53ca\u6a21\u578b\u53c2\u6570\u8ba1\u7b97\u4e2d\u7684\u4f30\u8ba1\u8bef\u5dee\u3002\u901a\u8fc7\u5728 NIST \u751f\u7269\u8bc6\u522b\u8bc4\u5206\u96c6 1 \u4e0a\u8fdb\u884c\u7684\u521d\u6b65\u5b9e\u9a8c\uff0c\u4ece\u5176\u4f18\u7f3a\u70b9\u7684\u89d2\u5ea6\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u5206\u6790\u3002|[2401.13418v1](http://arxiv.org/pdf/2401.13418v1)|null|\n", "2401.13329": "|**2024-01-24**|**Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval**|\u7528\u4e8e\u770b\u4e0d\u89c1\u7684\u8de8\u57df\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u751f\u6210\u89c6\u9891\u6269\u6563|Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu|Video Moment Retrieval (VMR) requires precise modelling of fine-grained moment-text associations to capture intricate visual-language relationships. Due to the lack of a diverse and generalisable VMR dataset to facilitate learning scalable moment-text associations, existing methods resort to joint training on both source and target domain videos for cross-domain applications. Meanwhile, recent developments in vision-language multimodal models pre-trained on large-scale image-text and/or video-text pairs are only based on coarse associations (weakly labelled). They are inadequate to provide fine-grained moment-text correlations required for cross-domain VMR. In this work, we solve the problem of unseen cross-domain VMR, where certain visual and textual concepts do not overlap across domains, by only utilising target domain sentences (text prompts) without accessing their videos. To that end, we explore generative video diffusion for fine-grained editing of source videos controlled by the target sentences, enabling us to simulate target domain videos. We address two problems in video editing for optimising unseen domain VMR: (1) generation of high-quality simulation videos of different moments with subtle distinctions, (2) selection of simulation videos that complement existing source training videos without introducing harmful noise or unnecessary repetitions. On the first problem, we formulate a two-stage video diffusion generation controlled simultaneously by (1) the original video structure of a source video, (2) subject specifics, and (3) a target sentence prompt. This ensures fine-grained variations between video moments. On the second problem, we introduce a hybrid selection mechanism that combines two quantitative metrics for noise filtering and one qualitative metric for leveraging VMR prediction on simulation video selection.|\u89c6\u9891\u65f6\u523b\u68c0\u7d22 (VMR) \u9700\u8981\u5bf9\u7ec6\u7c92\u5ea6\u65f6\u523b-\u6587\u672c\u5173\u8054\u8fdb\u884c\u7cbe\u786e\u5efa\u6a21\uff0c\u4ee5\u6355\u83b7\u590d\u6742\u7684\u89c6\u89c9-\u8bed\u8a00\u5173\u7cfb\u3002\u7531\u4e8e\u7f3a\u4e4f\u591a\u6837\u5316\u4e14\u901a\u7528\u7684VMR\u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u5b66\u4e60\u53ef\u6269\u5c55\u7684\u65f6\u523b-\u6587\u672c\u5173\u8054\uff0c\u73b0\u6709\u65b9\u6cd5\u8bc9\u8bf8\u4e8e\u5bf9\u6e90\u57df\u89c6\u9891\u548c\u76ee\u6807\u57df\u89c6\u9891\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u8de8\u57df\u5e94\u7528\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6587\u672c\u548c/\u6216\u89c6\u9891\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u7684\u6700\u65b0\u53d1\u5c55\u4ec5\u57fa\u4e8e\u7c97\u7565\u5173\u8054\uff08\u5f31\u6807\u8bb0\uff09\u3002\u5b83\u4eec\u4e0d\u8db3\u4ee5\u63d0\u4f9b\u8de8\u57df VMR \u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u65f6\u523b\u6587\u672c\u5173\u8054\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4ec5\u5229\u7528\u76ee\u6807\u57df\u53e5\u5b50\uff08\u6587\u672c\u63d0\u793a\uff09\u800c\u4e0d\u8bbf\u95ee\u5176\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u770b\u4e0d\u89c1\u7684\u8de8\u57dfVMR\u95ee\u9898\uff0c\u5176\u4e2d\u67d0\u4e9b\u89c6\u89c9\u548c\u6587\u672c\u6982\u5ff5\u4e0d\u8de8\u57df\u91cd\u53e0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63a2\u7d22\u751f\u6210\u89c6\u9891\u6269\u6563\uff0c\u4ee5\u5bf9\u7531\u76ee\u6807\u53e5\u5b50\u63a7\u5236\u7684\u6e90\u89c6\u9891\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7f16\u8f91\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6a21\u62df\u76ee\u6807\u57df\u89c6\u9891\u3002\u6211\u4eec\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u4e24\u4e2a\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u770b\u4e0d\u89c1\u7684\u57dfVMR\uff1a\uff081\uff09\u751f\u6210\u5177\u6709\u7ec6\u5fae\u5dee\u522b\u7684\u4e0d\u540c\u65f6\u523b\u7684\u9ad8\u8d28\u91cf\u6a21\u62df\u89c6\u9891\uff0c\uff082\uff09\u9009\u62e9\u8865\u5145\u73b0\u6709\u6e90\u8bad\u7ec3\u89c6\u9891\u7684\u6a21\u62df\u89c6\u9891\uff0c\u800c\u4e0d\u4f1a\u5f15\u5165\u6709\u5bb3\u566a\u58f0\u6216\u4e0d\u5fc5\u8981\u7684\u91cd\u590d\u3002\u5173\u4e8e\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5236\u5b9a\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u89c6\u9891\u6269\u6563\u751f\u6210\uff0c\u7531\uff081\uff09\u6e90\u89c6\u9891\u7684\u539f\u59cb\u89c6\u9891\u7ed3\u6784\uff0c\uff082\uff09\u4e3b\u9898\u7ec6\u8282\u548c\uff083\uff09\u76ee\u6807\u53e5\u5b50\u63d0\u793a\u540c\u65f6\u63a7\u5236\u3002\u8fd9\u786e\u4fdd\u4e86\u89c6\u9891\u65f6\u523b\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u53d8\u5316\u3002\u5173\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u9009\u62e9\u673a\u5236\uff0c\u8be5\u673a\u5236\u7ed3\u5408\u4e86\u4e24\u4e2a\u7528\u4e8e\u566a\u58f0\u8fc7\u6ee4\u7684\u5b9a\u91cf\u6307\u6807\u548c\u4e00\u4e2a\u7528\u4e8e\u5728\u6a21\u62df\u89c6\u9891\u9009\u62e9\u4e2d\u5229\u7528 VMR \u9884\u6d4b\u7684\u5b9a\u6027\u6307\u6807\u3002|[2401.13329v1](http://arxiv.org/pdf/2401.13329v1)|null|\n", "2401.13313": "|**2024-01-24**|**InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions**|InstructDoc\uff1a\u5e26\u6709\u6307\u4ee4\u7684\u89c6\u89c9\u6587\u6863\u7406\u89e3\u96f6\u6837\u672c\u6cdb\u5316\u6570\u636e\u96c6|Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko Saito, Jun Suzuki|We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.|\u6211\u4eec\u7814\u7a76\u901a\u8fc7\u4eba\u5de5\u7f16\u5199\u7684\u6307\u4ee4\u5728\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u4e0a\u5b8c\u6210\u5404\u79cd\u89c6\u89c9\u6587\u6863\u7406\u89e3\uff08VDU\uff09\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u95ee\u7b54\u548c\u4fe1\u606f\u63d0\u53d6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 InstructDoc\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b 30 \u4e2a\u516c\u5f00\u53ef\u7528\u7684 VDU \u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u96c6\u5408\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u90fd\u6709\u7edf\u4e00\u683c\u5f0f\u7684\u4e0d\u540c\u6307\u4ee4\uff0c\u6db5\u76d6\u4e86 12 \u79cd\u4efb\u52a1\uff0c\u5e76\u5305\u62ec\u5f00\u653e\u6587\u6863\u7c7b\u578b/\u683c\u5f0f\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u589e\u5f3a VDU \u4efb\u52a1\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u6587\u6863\u9605\u8bfb\u548c\u7406\u89e3\u6a21\u578b InstructDr\uff0c\u5b83\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6865\u63a5\u6a21\u5757\u8fde\u63a5\u6587\u6863\u56fe\u50cf\u3001\u56fe\u50cf\u7f16\u7801\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b (LLM)\u3002\u5b9e\u9a8c\u8868\u660e\uff0cInstructDr \u53ef\u4ee5\u901a\u8fc7\u7ed9\u5b9a\u7684\u6307\u4ee4\u6709\u6548\u5730\u9002\u5e94\u65b0\u7684 VDU \u6570\u636e\u96c6\u3001\u4efb\u52a1\u548c\u9886\u57df\uff0c\u5e76\u4e14\u5728\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001 LLM \u548c ChatGPT\u3002|[2401.13313v1](http://arxiv.org/pdf/2401.13313v1)|**[link](https://github.com/nttmdlab-nlp/instructdoc)**|\n", "2401.13311": "|**2024-01-24**|**ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models**|ConTextual\uff1a\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u63a8\u7406|Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng|Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/|\u4eba\u5de5\u667a\u80fd\u7684\u6700\u65b0\u8fdb\u5c55\u5bfc\u81f4\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u53d1\u5c55\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u6d89\u53ca\u5bf9\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u8fdb\u884c\u8054\u5408\u63a8\u7406\u7684\u590d\u6742\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u5728\u516c\u5171\u573a\u6240\u5bfc\u822a\u5730\u56fe\uff09\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 ConTextual\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7531\u660e\u786e\u8bbe\u8ba1\u7684\u6307\u4ee4\u7ec4\u6210\uff0c\u7528\u4e8e\u8bc4\u4f30 LMM \u6267\u884c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\u3002 ConTextual \u5f3a\u8c03\u591a\u6837\u5316\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\uff08\u4f8b\u5982\uff0c\u65f6\u95f4\u9605\u8bfb\u3001\u5bfc\u822a\u3001\u8d2d\u7269\u7b49\uff09\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6587\u672c\u548c\u89c6\u89c9\u5143\u7d20\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u4eba\u7c7b\u8bc4\u4f30\uff0c\u8868\u73b0\u6700\u597d\u7684 LMM\u3001GPT-4V(ision) \u548c\u4eba\u7c7b\u80fd\u529b\u4e4b\u95f4\u5b58\u5728 30.8% \u7684\u663e\u7740\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd9\u8868\u660e\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u63a8\u7406\u8fd8\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136 GPT-4V \u5728\u6a21\u56e0\u548c\u5f15\u6587\u89e3\u91ca\u7b49\u62bd\u8c61\u7c7b\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6574\u4f53\u8868\u73b0\u4ecd\u7136\u843d\u540e\u4e8e\u4eba\u7c7b\u3002\u9664\u4e86\u4eba\u5de5\u8bc4\u4f30\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u4f7f\u7528 GPT-4 \u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u4e86\u6027\u80fd\u5dee\u5f02\u7684\u7c7b\u4f3c\u8d8b\u52bf\u3002\u6211\u4eec\u8fd8\u5bf9\u4e0d\u540c\u7684\u89c6\u89c9\u73af\u5883\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u5b9a\u6027\u5206\u6790\uff0c\u4e3a LMM \u8bbe\u8ba1\u7684\u672a\u6765\u8fdb\u6b65\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u3002 https://con-textual.github.io/|[2401.13311v1](http://arxiv.org/pdf/2401.13311v1)|null|\n", "2401.13307": "|**2024-01-24**|**ChatterBox: Multi-round Multimodal Referring and Grounding**|ChatterBox\uff1a\u591a\u8f6e\u591a\u6a21\u6001\u53c2\u8003\u548c\u63a5\u5730|Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, Qixiang Ye|In this study, we establish a baseline for a new task named multimodal multi-round referring and grounding (MRG), opening up a promising direction for instance-level multimodal dialogues. We present a new benchmark and an efficient vision-language model for this purpose. The new benchmark, named CB-300K, spans challenges including multi-round dialogue, complex spatial relationships among multiple instances, and consistent reasoning, which are beyond those shown in existing benchmarks. The proposed model, named ChatterBox, utilizes a two-branch architecture to collaboratively handle vision and language tasks. By tokenizing instance regions, the language branch acquires the ability to perceive referential information. Meanwhile, ChatterBox feeds a query embedding in the vision branch to a token receiver for visual grounding. A two-stage optimization strategy is devised, making use of both CB-300K and auxiliary external data to improve the model's stability and capacity for instance-level understanding. Experiments show that ChatterBox outperforms existing models in MRG both quantitatively and qualitatively, paving a new path towards multimodal dialogue scenarios with complicated and precise interactions. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox.|\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4e3a\u540d\u4e3a\u591a\u6a21\u6001\u591a\u8f6e\u53c2\u8003\u548c\u63a5\u5730\uff08MRG\uff09\u7684\u65b0\u4efb\u52a1\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u4e3a\u5b9e\u4f8b\u7ea7\u591a\u6a21\u6001\u5bf9\u8bdd\u5f00\u8f9f\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u548c\u6709\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u540d\u4e3a CB-300K\uff0c\u6db5\u76d6\u4e86\u591a\u8f6e\u5bf9\u8bdd\u3001\u591a\u4e2a\u5b9e\u4f8b\u4e4b\u95f4\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\u4ee5\u53ca\u4e00\u81f4\u63a8\u7406\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u8d85\u51fa\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u8303\u56f4\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u540d\u4e3a ChatterBox\uff0c\u5229\u7528\u4e24\u5206\u652f\u67b6\u6784\u6765\u534f\u4f5c\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u3002\u901a\u8fc7\u5bf9\u5b9e\u4f8b\u533a\u57df\u8fdb\u884c\u6807\u8bb0\uff0c\u8bed\u8a00\u5206\u652f\u83b7\u5f97\u4e86\u611f\u77e5\u53c2\u8003\u4fe1\u606f\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0cChatterBox \u5c06\u5d4c\u5165\u89c6\u89c9\u5206\u652f\u4e2d\u7684\u67e5\u8be2\u63d0\u4f9b\u7ed9\u4ee4\u724c\u63a5\u6536\u5668\u4ee5\u8fdb\u884c\u89c6\u89c9\u57fa\u7840\u3002\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528 CB-300K \u548c\u8f85\u52a9\u5916\u90e8\u6570\u636e\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u5b9e\u4f8b\u7ea7\u7406\u89e3\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cChatterBox \u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e MRG \u4e2d\u7684\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u800c\u7cbe\u786e\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u573a\u666f\u5f00\u8f9f\u4e86\u65b0\u7684\u9053\u8def\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u53d6\uff1ahttps://github.com/sunsmarterjie/ChatterBox\u3002|[2401.13307v1](http://arxiv.org/pdf/2401.13307v1)|**[link](https://github.com/sunsmarterjie/chatterbox)**|\n", "2401.13201": "|**2024-01-24**|**MLLMReID: Multimodal Large Language Model-based Person Re-identification**|MLLMReID\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4eba\u91cd\u65b0\u8bc6\u522b|Shan Yang, Yongfei Zhang|Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instruction, a simple approach that leverages the essence ability of LLMs to continue writing, avoiding complex and diverse instruction design. Secondly, we proposed DirectReID, which effectively employs the latent image feature vectors of images outputted by LLMs in ReID tasks. The experimental results demonstrate the superiority of our method. We will open-source the code on GitHub.|\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fc4\u4eca\u4e3a\u6b62\uff0c\u5b83\u4eec\u5728\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\uff08ReID\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u672c\u6587\u5c06\u7814\u7a76\u5982\u4f55\u4f7f\u5b83\u4eec\u9002\u5e94 ReID \u4efb\u52a1\u3002\u4e00\u4e2a\u76f4\u89c2\u7684\u60f3\u6cd5\u662f\u4f7f\u7528 ReID \u56fe\u50cf\u6587\u672c\u6570\u636e\u96c6\u5fae\u8c03 MLLM\uff0c\u7136\u540e\u4f7f\u7528\u5176\u89c6\u89c9\u7f16\u7801\u5668\u4f5c\u4e3a ReID \u7684\u9aa8\u5e72\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u4e24\u4e2a\u660e\u663e\u7684\u95ee\u9898\uff1a\uff081\uff09\u4e3aReID\u8bbe\u8ba1\u6307\u4ee4\uff0cMLLM\u53ef\u80fd\u4f1a\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u6307\u4ee4\uff0c\u5e76\u4e14\u8bbe\u8ba1\u591a\u79cd\u6307\u4ee4\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u6210\u672c\u3002 (2) \u6765\u81ea LLM \u7684\u6f5c\u5728\u56fe\u50cf\u7279\u5f81\u5411\u91cf\u4e0d\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u3002\u6559\u5b66\u5b66\u4e60\uff0c\u5bf9\u9f50\u56fe\u50cf\u6587\u672c\u7279\u5f81\uff0c\u5bfc\u81f4\u95f4\u63a5\u4f18\u5316\u548c\u672a\u5145\u5206\u5229\u7528\u7279\u5f81\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u9650\u5236\u4e86\u4eba\u7269\u7279\u5f81\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86MLLMReID\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684ReID\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Commonstruction\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7ee7\u7eed\u5199\u4f5c\u7684\u672c\u8d28\u80fd\u529b\uff0c\u907f\u514d\u590d\u6742\u591a\u6837\u7684\u6307\u4ee4\u8bbe\u8ba1\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DirectReID\uff0c\u5b83\u5728 ReID \u4efb\u52a1\u4e2d\u6709\u6548\u5730\u5229\u7528\u4e86 LLM \u8f93\u51fa\u56fe\u50cf\u7684\u6f5c\u5728\u56fe\u50cf\u7279\u5f81\u5411\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u5c06\u5728 GitHub \u4e0a\u5f00\u6e90\u4ee3\u7801\u3002|[2401.13201v1](http://arxiv.org/pdf/2401.13201v1)|null|\n"}, "LLM": {}, "Transformer": {"2401.13221": "|**2024-01-24**|**Unified-Width Adaptive Dynamic Network for All-In-One Image Restoration**|\u7528\u4e8e\u4e00\u4f53\u5316\u56fe\u50cf\u6062\u590d\u7684\u7edf\u4e00\u5bbd\u5ea6\u81ea\u9002\u5e94\u52a8\u6001\u7f51\u7edc|Yimin Xu, Nanxi Gao, Zhongyun Shan, Fei Chao, Rongrong Ji|In contrast to traditional image restoration methods, all-in-one image restoration techniques are gaining increased attention for their ability to restore images affected by diverse and unknown corruption types and levels. However, contemporary all-in-one image restoration methods omit task-wise difficulties and employ the same networks to reconstruct images afflicted by diverse degradations. This practice leads to an underestimation of the task correlations and suboptimal allocation of computational resources. To elucidate task-wise complexities, we introduce a novel concept positing that intricate image degradation can be represented in terms of elementary degradation. Building upon this foundation, we propose an innovative approach, termed the Unified-Width Adaptive Dynamic Network (U-WADN), consisting of two pivotal components: a Width Adaptive Backbone (WAB) and a Width Selector (WS). The WAB incorporates several nested sub-networks with varying widths, which facilitates the selection of the most apt computations tailored to each task, thereby striking a balance between accuracy and computational efficiency during runtime. For different inputs, the WS automatically selects the most appropriate sub-network width, taking into account both task-specific and sample-specific complexities. Extensive experiments across a variety of image restoration tasks demonstrate that the proposed U-WADN achieves better performance while simultaneously reducing up to 32.3\\% of FLOPs and providing approximately 15.7\\% real-time acceleration. The code has been made available at \\url{https://github.com/xuyimin0926/U-WADN}.|\u4e0e\u4f20\u7edf\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e00\u4f53\u5316\u56fe\u50cf\u6062\u590d\u6280\u672f\u56e0\u5176\u80fd\u591f\u6062\u590d\u53d7\u591a\u79cd\u548c\u672a\u77e5\u635f\u574f\u7c7b\u578b\u548c\u7ea7\u522b\u5f71\u54cd\u7684\u56fe\u50cf\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u4ee3\u7684\u4e00\u4f53\u5316\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5ffd\u7565\u4e86\u4efb\u52a1\u65b9\u9762\u7684\u56f0\u96be\uff0c\u5e76\u91c7\u7528\u76f8\u540c\u7684\u7f51\u7edc\u6765\u91cd\u5efa\u53d7\u5230\u4e0d\u540c\u9000\u5316\u5f71\u54cd\u7684\u56fe\u50cf\u3002\u8fd9\u79cd\u505a\u6cd5\u4f1a\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u6027\u7684\u4f4e\u4f30\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u6b21\u4f18\u5206\u914d\u3002\u4e3a\u4e86\u9610\u660e\u4efb\u52a1\u65b9\u9762\u7684\u590d\u6742\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6982\u5ff5\uff0c\u8ba4\u4e3a\u590d\u6742\u7684\u56fe\u50cf\u9000\u5316\u53ef\u4ee5\u7528\u57fa\u672c\u9000\u5316\u6765\u8868\u793a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7edf\u4e00\u5bbd\u5ea6\u81ea\u9002\u5e94\u52a8\u6001\u7f51\u7edc\uff08U-WADN\uff09\uff0c\u7531\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a\u5bbd\u5ea6\u81ea\u9002\u5e94\u9aa8\u5e72\u7f51\uff08WAB\uff09\u548c\u5bbd\u5ea6\u9009\u62e9\u5668\uff08WS\uff09\u3002 WAB \u5305\u542b\u591a\u4e2a\u5bbd\u5ea6\u4e0d\u540c\u7684\u5d4c\u5957\u5b50\u7f51\u7edc\uff0c\u8fd9\u6709\u52a9\u4e8e\u9009\u62e9\u9002\u5408\u6bcf\u4e2a\u4efb\u52a1\u7684\u6700\u5408\u9002\u7684\u8ba1\u7b97\uff0c\u4ece\u800c\u5728\u8fd0\u884c\u65f6\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u5bf9\u4e8e\u4e0d\u540c\u7684\u8f93\u5165\uff0cWS \u4f1a\u81ea\u52a8\u9009\u62e9\u6700\u5408\u9002\u7684\u5b50\u7f51\u7edc\u5bbd\u5ea6\uff0c\u540c\u65f6\u8003\u8651\u7279\u5b9a\u4e8e\u4efb\u52a1\u548c\u7279\u5b9a\u4e8e\u6837\u672c\u7684\u590d\u6742\u6027\u3002\u8de8\u5404\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 U-WADN \u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u9ad8\u8fbe 32.3% \u7684 FLOP\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u7ea6 15.7% \u7684\u5b9e\u65f6\u52a0\u901f\u3002\u8be5\u4ee3\u7801\u5df2\u5728 \\url{https://github.com/xuyimin0926/U-WADN} \u4e2d\u63d0\u4f9b\u3002|[2401.13221v1](http://arxiv.org/pdf/2401.13221v1)|**[link](https://github.com/xuyimin0926/u-wadn)**|\n", "2401.13172": "|**2024-01-24**|**ADMap: Anti-disturbance framework for reconstructing online vectorized HD map**|ADMap\uff1a\u91cd\u5efa\u5728\u7ebf\u77e2\u91cf\u5316\u9ad8\u7cbe\u5730\u56fe\u7684\u6297\u5e72\u6270\u6846\u67b6|Haotian Hu, Fanyi Wang, Yaonong Wang, Laifeng Hu, Jingwei Xu, Zhiwang Zhang|In the field of autonomous driving, online high-definition (HD) map reconstruction is crucial for planning tasks. Recent research has developed several high-performance HD map reconstruction models to meet this necessity. However, the point sequences within the instance vectors may be jittery or jagged due to prediction bias, which can impact subsequent tasks. Therefore, this paper proposes the Anti-disturbance Map reconstruction framework (ADMap). To mitigate point-order jitter, the framework consists of three modules: Multi-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point-order relationships between and within instances in a cascading manner, the model can monitor the point-order prediction process more effectively. ADMap achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios. Code and more demos are available at https://github.com/hht1996ok/ADMap.|\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u5728\u7ebf\u9ad8\u6e05\u5730\u56fe\u91cd\u5efa\u5bf9\u4e8e\u89c4\u5212\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5f00\u53d1\u4e86\u51e0\u79cd\u9ad8\u6027\u80fd\u7684\u9ad8\u6e05\u5730\u56fe\u91cd\u5efa\u6a21\u578b\u6765\u6ee1\u8db3\u8fd9\u79cd\u9700\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9884\u6d4b\u504f\u5dee\uff0c\u5b9e\u4f8b\u5411\u91cf\u5185\u7684\u70b9\u5e8f\u5217\u53ef\u80fd\u4f1a\u51fa\u73b0\u6296\u52a8\u6216\u952f\u9f7f\u72b6\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u540e\u7eed\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u6297\u5e72\u6270\u5730\u56fe\u91cd\u5efa\u6846\u67b6\uff08ADMap\uff09\u3002\u4e3a\u4e86\u51cf\u8f7b\u70b9\u9636\u6296\u52a8\uff0c\u8be5\u6846\u67b6\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u591a\u5c3a\u5ea6\u611f\u77e5\u9888\u3001\u5b9e\u4f8b\u4ea4\u4e92\u6ce8\u610f\uff08IIA\uff09\u548c\u77e2\u91cf\u65b9\u5411\u5dee\u5f02\u635f\u5931\uff08VDDL\uff09\u3002\u901a\u8fc7\u4ee5\u7ea7\u8054\u65b9\u5f0f\u63a2\u7d22\u5b9e\u4f8b\u4e4b\u95f4\u548c\u5b9e\u4f8b\u5185\u7684\u70b9\u5e8f\u5173\u7cfb\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u76d1\u63a7\u70b9\u5e8f\u9884\u6d4b\u8fc7\u7a0b\u3002 ADMap \u5728 nuScenes \u548c Argoverse2 \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5927\u91cf\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u591a\u53d8\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\u751f\u6210\u7a33\u5b9a\u53ef\u9760\u7684\u5730\u56fe\u5143\u7d20\u7684\u80fd\u529b\u3002\u4ee3\u7801\u548c\u66f4\u591a\u6f14\u793a\u53ef\u5728 https://github.com/hht1996ok/ADMap \u83b7\u53d6\u3002|[2401.13172v1](http://arxiv.org/pdf/2401.13172v1)|**[link](https://github.com/hht1996ok/admap)**|\n"}, "Nerf": {"2401.13352": "|**2024-01-24**|**EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction**|EndoGaussians\uff1a\u7528\u4e8e\u53d8\u5f62\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u7684\u5355\u89c6\u56fe\u52a8\u6001\u9ad8\u65af\u6e85\u5c04|Yangsen Chen, Hao Wang|The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.|\u6839\u636e\u5185\u7aa5\u955c\u89c6\u9891\u5bf9\u53ef\u53d8\u5f62\u8f6f\u4f53\u7ec4\u7ec7\u8fdb\u884c\u7cbe\u786e\u7684 3D \u91cd\u5efa\u662f VR \u624b\u672f\u548c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7b49\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5e38\u5e38\u96be\u4ee5\u51c6\u786e\u5730\u7406\u89e3\u5e7b\u89c9\u7ec4\u7ec7\u90e8\u5206\u7684\u6a21\u7cca\u6027\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u7528\u9014\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 EndoGaussians\uff0c\u8fd9\u662f\u4e00\u79cd\u91c7\u7528\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u52a8\u6001\u5185\u7aa5\u955c 3D \u91cd\u5efa\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6807\u5fd7\u7740\u9ad8\u65af\u5206\u5e03\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u9996\u6b21\u4f7f\u7528\uff0c\u514b\u670d\u4e86\u4ee5\u524d\u57fa\u4e8e NeRF \u7684\u6280\u672f\u7684\u5c40\u9650\u6027\u3002\u6b63\u5982\u5bf9\u5404\u79cd\u5185\u7aa5\u955c\u6570\u636e\u96c6\u7684\u5b9a\u91cf\u8bc4\u4f30\u6240\u8bc1\u660e\u7684\u90a3\u6837\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6807\u51c6\u3002\u8fd9\u4e9b\u8fdb\u6b65\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u4e3a\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u6709\u524d\u9014\u7684\u5de5\u5177\uff0c\u4e3a\u533b\u7597\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u9760\u3001\u66f4\u9ad8\u6548\u7684 3D \u91cd\u5efa\u3002|[2401.13352v1](http://arxiv.org/pdf/2401.13352v1)|null|\n"}, "3DGS": {}, "3D/CG": {"2401.13203": "|**2024-01-24**|**Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects**|\u5177\u6709\u89e3\u8026\u5bf9\u8c61\u7684\u98ce\u683c\u4e00\u81f4\u7684 3D \u5ba4\u5185\u573a\u666f\u5408\u6210|Yunfan Zhang, Hong Huang, Zhiwei Xiong, Zhiqi Shen, Guosheng Lin, Hao Wang, Nicholas Vun|Controllable 3D indoor scene synthesis stands at the forefront of technological progress, offering various applications like gaming, film, and augmented/virtual reality. The capability to stylize and de-couple objects within these scenarios is a crucial factor, providing an advanced level of control throughout the editing process. This control extends not just to manipulating geometric attributes like translation and scaling but also includes managing appearances, such as stylization. Current methods for scene stylization are limited to applying styles to the entire scene, without the ability to separate and customize individual objects. Addressing the intricacies of this challenge, we introduce a unique pipeline designed for synthesis 3D indoor scenes. Our approach involves strategically placing objects within the scene, utilizing information from professionally designed bounding boxes. Significantly, our pipeline prioritizes maintaining style consistency across multiple objects within the scene, ensuring a cohesive and visually appealing result aligned with the desired aesthetic. The core strength of our pipeline lies in its ability to generate 3D scenes that are not only visually impressive but also exhibit features like photorealism, multi-view consistency, and diversity. These scenes are crafted in response to various natural language prompts, demonstrating the versatility and adaptability of our model.|\u53ef\u63a7 3D \u5ba4\u5185\u573a\u666f\u5408\u6210\u5904\u4e8e\u6280\u672f\u8fdb\u6b65\u7684\u524d\u6cbf\uff0c\u63d0\u4f9b\u6e38\u620f\u3001\u7535\u5f71\u548c\u589e\u5f3a/\u865a\u62df\u73b0\u5b9e\u7b49\u5404\u79cd\u5e94\u7528\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u5bf9\u5bf9\u8c61\u8fdb\u884c\u98ce\u683c\u5316\u548c\u89e3\u8026\u7684\u80fd\u529b\u662f\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u5b83\u53ef\u4ee5\u5728\u6574\u4e2a\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u9ad8\u7ea7\u7684\u63a7\u5236\u3002\u6b64\u63a7\u4ef6\u4e0d\u4ec5\u6269\u5c55\u5230\u64cd\u7eb5\u51e0\u4f55\u5c5e\u6027\uff08\u4f8b\u5982\u5e73\u79fb\u548c\u7f29\u653e\uff09\uff0c\u8fd8\u5305\u62ec\u7ba1\u7406\u5916\u89c2\uff08\u4f8b\u5982\u6837\u5f0f\u5316\uff09\u3002\u5f53\u524d\u7684\u573a\u666f\u98ce\u683c\u5316\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5c06\u6837\u5f0f\u5e94\u7528\u4e8e\u6574\u4e2a\u573a\u666f\uff0c\u65e0\u6cd5\u5206\u79bb\u548c\u81ea\u5b9a\u4e49\u5355\u4e2a\u5bf9\u8c61\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u590d\u6742\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e13\u4e3a\u5408\u6210 3D \u5ba4\u5185\u573a\u666f\u800c\u8bbe\u8ba1\u7684\u72ec\u7279\u7ba1\u9053\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u4e13\u4e1a\u8bbe\u8ba1\u7684\u8fb9\u754c\u6846\u7684\u4fe1\u606f\uff0c\u7b56\u7565\u6027\u5730\u5c06\u5bf9\u8c61\u653e\u7f6e\u5728\u573a\u666f\u4e2d\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6d41\u7a0b\u4f18\u5148\u8003\u8651\u4fdd\u6301\u573a\u666f\u4e2d\u591a\u4e2a\u5bf9\u8c61\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u5177\u6709\u51dd\u805a\u529b\u548c\u89c6\u89c9\u5438\u5f15\u529b\u7684\u7ed3\u679c\u4e0e\u6240\u9700\u7684\u7f8e\u611f\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u4eec\u7ba1\u9053\u7684\u6838\u5fc3\u4f18\u52bf\u5728\u4e8e\u5b83\u80fd\u591f\u751f\u6210 3D \u573a\u666f\uff0c\u8fd9\u4e9b\u573a\u666f\u4e0d\u4ec5\u5728\u89c6\u89c9\u4e0a\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u800c\u4e14\u8fd8\u8868\u73b0\u51fa\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u3001\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u7b49\u7279\u5f81\u3002\u8fd9\u4e9b\u573a\u666f\u662f\u6839\u636e\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027\u548c\u9002\u5e94\u6027\u3002|[2401.13203v1](http://arxiv.org/pdf/2401.13203v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2401.13432": "|**2024-01-24**|**Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond**|\u7528\u4e8e\u65cb\u8f6c\u6821\u6b63\u53ca\u5176\u4ed6\u7684\u534a\u76d1\u7763\u8026\u5408\u8584\u677f\u6837\u6761\u6a21\u578b|Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao|Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data. It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint. Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction. Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond. The code and data will be available at https://github.com/nie-lang/CoupledTPS.|\u8584\u677f\u6837\u6761 (TPS) \u662f\u4e00\u79cd\u4e3b\u8981\u626d\u66f2\uff0c\u5141\u8bb8\u901a\u8fc7\u63a7\u5236\u70b9\u8fd0\u52a8\u6765\u8868\u793a\u5f39\u6027\u975e\u7ebf\u6027\u53d8\u6362\u3002\u968f\u7740\u63a7\u5236\u70b9\u7684\u589e\u52a0\uff0c\u626d\u66f2\u53d8\u5f97\u8d8a\u6765\u8d8a\u7075\u6d3b\uff0c\u4f46\u901a\u5e38\u4f1a\u9047\u5230\u7531\u4e0d\u826f\u95ee\u9898\uff08\u4f8b\u5982\u5185\u5bb9\u626d\u66f2\uff09\u5f15\u8d77\u7684\u74f6\u9888\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86 TPS \u5728\u57fa\u4e8e\u5355\u56fe\u50cf\u7684\u53d8\u5f62\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u5e94\u7528\uff0c\u4f8b\u5982\u65cb\u8f6c\u6821\u6b63\u3001\u77e9\u5f62\u6821\u6b63\u548c\u8096\u50cf\u6821\u6b63\u3002\u4e3a\u4e86\u6253\u7834\u8fd9\u4e00\u74f6\u9888\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8026\u5408\u8584\u677f\u6837\u6761\u6a21\u578b\uff08CoupledTPS\uff09\uff0c\u8be5\u6a21\u578b\u5c06\u5177\u6709\u6709\u9650\u63a7\u5236\u70b9\u7684\u591a\u4e2aTPS\u8fed\u4ee3\u8026\u5408\u6210\u66f4\u7075\u6d3b\u3001\u66f4\u5f3a\u5927\u7684\u53d8\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e00\u4e2a\u8fed\u4ee3\u641c\u7d22\u6765\u6839\u636e\u5f53\u524d\u7684\u6f5c\u5728\u6761\u4ef6\u9884\u6d4b\u65b0\u7684\u63a7\u5236\u70b9\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u626d\u66f2\u6d41\u4f5c\u4e3a\u4e0d\u540cTPS\u53d8\u6362\u8026\u5408\u7684\u6865\u6881\uff0c\u6709\u6548\u6d88\u9664\u4e86\u591a\u4e2a\u626d\u66f2\u5f15\u8d77\u7684\u63d2\u503c\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u9274\u4e8e\u7e41\u7410\u7684\u6807\u6ce8\u6210\u672c\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e\u6765\u63d0\u9ad8\u53d8\u5f62\u8d28\u91cf\u3002\u5b83\u662f\u901a\u8fc7\u672a\u6807\u8bb0\u6570\u636e\u7684\u641c\u7d22\u63a7\u5236\u70b9\u4e0e\u5176\u56fe\u5f62\u589e\u5f3a\u4e4b\u95f4\u7684\u53cc\u91cd\u53d8\u6362\u6765\u5236\u5b9a\u7684\uff0c\u4ea7\u751f\u9690\u5f0f\u6821\u6b63\u4e00\u81f4\u6027\u7ea6\u675f\u3002\u6700\u540e\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u5927\u91cf\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u4ee5\u5c55\u793a\u6211\u4eec\u7684\u534a\u76d1\u7763\u65b9\u6848\u5728\u65cb\u8f6c\u6821\u6b63\u65b9\u9762\u7684\u4f18\u52bf\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 CoupledTPS \u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u5148\u8fdb (SoTA) \u65cb\u8f6c\u6821\u6b63\u53ca\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u7684\u4f18\u8d8a\u6027\u548c\u901a\u7528\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 https://github.com/nie-lang/CoupledTPS \u4e0a\u83b7\u53d6\u3002|[2401.13432v1](http://arxiv.org/pdf/2401.13432v1)|**[link](https://github.com/nie-lang/coupledtps)**|\n", "2401.13363": "|**2024-01-24**|**Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons**|\u4f60\u4eec\u60f3\u8df3\u821e\u5417\uff1a\u591a\u4eba\u96f6\u955c\u5934\u7ec4\u5408\u4eba\u7c7b\u821e\u8e48\u751f\u6210|Zhe Xu, Kun Wei, Xu Yang, Cheng Deng|Human dance generation (HDG) aims to synthesize realistic videos from images and sequences of driving poses. Despite great success, existing methods are limited to generating videos of a single person with specific backgrounds, while the generalizability for real-world scenarios with multiple persons and complex backgrounds remains unclear. To systematically measure the generalizability of HDG models, we introduce a new task, dataset, and evaluation protocol of compositional human dance generation (cHDG). Evaluating the state-of-the-art methods on cHDG, we empirically find that they fail to generalize to real-world scenarios. To tackle the issue, we propose a novel zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos consistent with arbitrary multiple persons and background while precisely following the driving poses. Specifically, in contrast to straightforward DDIM or null-text inversion, we first present a pose-aware inversion method to obtain the noisy latent code and initialization text embeddings, which can accurately reconstruct the composed reference image. Since directly generating videos from them will lead to severe appearance inconsistency, we propose a compositional augmentation strategy to generate augmented images and utilize them to optimize a set of generalizable text embeddings. In addition, consistency-guided sampling is elaborated to encourage the background and keypoints of the estimated clean image at each reverse step to be close to those of the reference image, further improving the temporal consistency of generated videos. Extensive qualitative and quantitative results demonstrate the effectiveness and superiority of our approach.|\u4eba\u7c7b\u821e\u8e48\u751f\u6210\uff08HDG\uff09\u65e8\u5728\u6839\u636e\u56fe\u50cf\u548c\u9a7e\u9a76\u59ff\u52bf\u5e8f\u5217\u5408\u6210\u903c\u771f\u7684\u89c6\u9891\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u751f\u6210\u5177\u6709\u7279\u5b9a\u80cc\u666f\u7684\u5355\u4e2a\u4eba\u7684\u89c6\u9891\uff0c\u800c\u5bf9\u4e8e\u5177\u6709\u591a\u4eba\u548c\u590d\u6742\u80cc\u666f\u7684\u73b0\u5b9e\u573a\u666f\u7684\u901a\u7528\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u4e3a\u4e86\u7cfb\u7edf\u5730\u8861\u91cf HDG \u6a21\u578b\u7684\u901a\u7528\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4eba\u7c7b\u821e\u8e48\u751f\u6210\uff08cHDG\uff09\u7684\u65b0\u4efb\u52a1\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002\u5728\u8bc4\u4f30 cHDG \u4e0a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u65f6\uff0c\u6211\u4eec\u51ed\u7ecf\u9a8c\u53d1\u73b0\u5b83\u4eec\u65e0\u6cd5\u63a8\u5e7f\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u955c\u5934\u6846\u67b6\uff0c\u79f0\u4e3a MultiDance-Zero\uff0c\u5b83\u53ef\u4ee5\u5408\u6210\u4e0e\u4efb\u610f\u591a\u4eba\u548c\u80cc\u666f\u4e00\u81f4\u7684\u89c6\u9891\uff0c\u540c\u65f6\u7cbe\u786e\u9075\u5faa\u9a7e\u9a76\u59ff\u52bf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e0e\u76f4\u63a5\u7684 DDIM \u6216\u7a7a\u6587\u672c\u53cd\u6f14\u76f8\u6bd4\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e00\u79cd\u59ff\u52bf\u611f\u77e5\u53cd\u6f14\u65b9\u6cd5\u6765\u83b7\u53d6\u566a\u58f0\u6f5c\u5728\u4ee3\u7801\u548c\u521d\u59cb\u5316\u6587\u672c\u5d4c\u5165\uff0c\u5b83\u53ef\u4ee5\u51c6\u786e\u5730\u91cd\u5efa\u5408\u6210\u7684\u53c2\u8003\u56fe\u50cf\u3002\u7531\u4e8e\u76f4\u63a5\u4ece\u5b83\u4eec\u751f\u6210\u89c6\u9891\u5c06\u5bfc\u81f4\u4e25\u91cd\u7684\u5916\u89c2\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u589e\u5f3a\u7b56\u7565\u6765\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u5e76\u5229\u7528\u5b83\u4eec\u6765\u4f18\u5316\u4e00\u7ec4\u53ef\u6982\u62ec\u7684\u6587\u672c\u5d4c\u5165\u3002\u6b64\u5916\uff0c\u8fd8\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e00\u81f4\u6027\u5f15\u5bfc\u91c7\u6837\uff0c\u4ee5\u9f13\u52b1\u6bcf\u4e2a\u53cd\u5411\u6b65\u9aa4\u4e2d\u4f30\u8ba1\u7684\u5e72\u51c0\u56fe\u50cf\u7684\u80cc\u666f\u548c\u5173\u952e\u70b9\u63a5\u8fd1\u53c2\u8003\u56fe\u50cf\u7684\u80cc\u666f\u548c\u5173\u952e\u70b9\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u751f\u6210\u89c6\u9891\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002|[2401.13363v1](http://arxiv.org/pdf/2401.13363v1)|null|\n"}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {}, "\u5176\u4ed6": {"2401.13616": "|**2024-01-24**|**FLLIC: Functionally Lossless Image Compression**|FLLIC\uff1a\u529f\u80fd\u65e0\u635f\u56fe\u50cf\u538b\u7f29|Xi Zhang, Xiaolin Wu|Recently, DNN models for lossless image coding have surpassed their traditional counterparts in compression performance, reducing the bit rate by about ten percent for natural color images. But even with these advances, mathematically lossless image compression (MLLIC) ratios for natural images still fall short of the bandwidth and cost-effectiveness requirements of most practical imaging and vision systems at present and beyond. To break the bottleneck of MLLIC in compression performance, we question the necessity of MLLIC, as almost all digital sensors inherently introduce acquisition noises, making mathematically lossless compression counterproductive. Therefore, in contrast to MLLIC, we propose a new paradigm of joint denoising and compression called functionally lossless image compression (FLLIC), which performs lossless compression of optimally denoised images (the optimality may be task-specific). Although not literally lossless with respect to the noisy input, FLLIC aims to achieve the best possible reconstruction of the latent noise-free original image. Extensive experiments show that FLLIC achieves state-of-the-art performance in joint denoising and compression of noisy images and does so at a lower computational cost.|\u6700\u8fd1\uff0c\u7528\u4e8e\u65e0\u635f\u56fe\u50cf\u7f16\u7801\u7684 DNN \u6a21\u578b\u5728\u538b\u7f29\u6027\u80fd\u65b9\u9762\u8d85\u8d8a\u4e86\u4f20\u7edf\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u5f69\u8272\u56fe\u50cf\u7684\u6bd4\u7279\u7387\u964d\u4f4e\u4e86\u7ea6 10%\u3002\u4f46\u5373\u4f7f\u6709\u4e86\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u81ea\u7136\u56fe\u50cf\u7684\u6570\u5b66\u65e0\u635f\u56fe\u50cf\u538b\u7f29 (MLLIC) \u6bd4\u7387\u4ecd\u7136\u8fbe\u4e0d\u5230\u5f53\u524d\u53ca\u672a\u6765\u5927\u591a\u6570\u5b9e\u7528\u6210\u50cf\u548c\u89c6\u89c9\u7cfb\u7edf\u7684\u5e26\u5bbd\u548c\u6210\u672c\u6548\u76ca\u8981\u6c42\u3002\u4e3a\u4e86\u6253\u7834 MLLIC \u5728\u538b\u7f29\u6027\u80fd\u65b9\u9762\u7684\u74f6\u9888\uff0c\u6211\u4eec\u8d28\u7591 MLLIC \u7684\u5fc5\u8981\u6027\uff0c\u56e0\u4e3a\u51e0\u4e4e\u6240\u6709\u6570\u5b57\u4f20\u611f\u5668\u90fd\u4f1a\u56fa\u6709\u5730\u5f15\u5165\u91c7\u96c6\u566a\u58f0\uff0c\u4f7f\u5f97\u6570\u5b66\u4e0a\u7684\u65e0\u635f\u538b\u7f29\u9002\u5f97\u5176\u53cd\u3002\u56e0\u6b64\uff0c\u4e0e MLLIC \u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u529f\u80fd\u65e0\u635f\u56fe\u50cf\u538b\u7f29\uff08FLLIC\uff09\u7684\u8054\u5408\u53bb\u566a\u548c\u538b\u7f29\u7684\u65b0\u8303\u4f8b\uff0c\u5b83\u5bf9\u6700\u4f73\u53bb\u566a\u56fe\u50cf\u6267\u884c\u65e0\u635f\u538b\u7f29\uff08\u6700\u4f73\u6027\u53ef\u80fd\u662f\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\uff09\u3002\u5c3d\u7ba1\u76f8\u5bf9\u4e8e\u566a\u58f0\u8f93\u5165\u6765\u8bf4\u5e76\u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u65e0\u635f\uff0c\u4f46 FLLIC \u7684\u76ee\u6807\u662f\u5b9e\u73b0\u6f5c\u5728\u65e0\u566a\u58f0\u539f\u59cb\u56fe\u50cf\u7684\u6700\u4f73\u91cd\u5efa\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFLLIC \u5728\u566a\u58f0\u56fe\u50cf\u7684\u8054\u5408\u53bb\u566a\u548c\u538b\u7f29\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002|[2401.13616v1](http://arxiv.org/pdf/2401.13616v1)|null|\n", "2401.13357": "|**2024-01-24**|**Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry**|\u57fa\u4e8e\u4ec5\u4f4d\u59ff\u6210\u50cf\u51e0\u4f55\u7684\u7ebf\u6027\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1|Qi Cai, Xinrui Li, Yuanxin Wu|How to efficiently and accurately handle image matching outliers is a critical issue in two-view relative estimation. The prevailing RANSAC method necessitates that the minimal point pairs be inliers. This paper introduces a linear relative pose estimation algorithm for n $( n \\geq 6$) point pairs, which is founded on the recent pose-only imaging geometry to filter out outliers by proper reweighting. The proposed algorithm is able to handle planar degenerate scenes, and enhance robustness and accuracy in the presence of a substantial ratio of outliers. Specifically, we embed the linear global translation (LiGT) constraint into the strategies of iteratively reweighted least-squares (IRLS) and RANSAC so as to realize robust outlier removal. Simulations and real tests of the Strecha dataset show that the proposed algorithm achieves relative rotation accuracy improvement of 2 $\\sim$ 10 times in face of as large as 80% outliers.|\u5982\u4f55\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5904\u7406\u56fe\u50cf\u5339\u914d\u5f02\u5e38\u503c\u662f\u4e8c\u89c6\u56fe\u76f8\u5bf9\u4f30\u8ba1\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002\u6d41\u884c\u7684 RANSAC \u65b9\u6cd5\u8981\u6c42\u6700\u5c0f\u70b9\u5bf9\u5fc5\u987b\u662f\u5185\u70b9\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9 n $( n \\geq 6$) \u70b9\u5bf9\u7684\u7ebf\u6027\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u6700\u8fd1\u7684\u4ec5\u4f4d\u59ff\u6210\u50cf\u51e0\u4f55\u7ed3\u6784\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u91cd\u65b0\u52a0\u6743\u6765\u8fc7\u6ee4\u5f02\u5e38\u503c\u3002\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u5e73\u9762\u9000\u5316\u573a\u666f\uff0c\u5e76\u5728\u5b58\u5728\u5927\u91cf\u5f02\u5e38\u503c\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u7ebf\u6027\u5168\u5c40\u5e73\u79fb\uff08LiGT\uff09\u7ea6\u675f\u5d4c\u5165\u5230\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\uff08IRLS\uff09\u548cRANSAC\u7b56\u7565\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u5f02\u5e38\u503c\u53bb\u9664\u3002 Strecha\u6570\u636e\u96c6\u7684\u4eff\u771f\u548c\u771f\u5b9e\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u9762\u5bf9\u9ad8\u8fbe80%\u7684\u5f02\u5e38\u503c\u65f6\uff0c\u76f8\u5bf9\u65cb\u8f6c\u7cbe\u5ea6\u63d0\u9ad8\u4e862 $\\sim$ 10\u500d\u3002|[2401.13357v1](http://arxiv.org/pdf/2401.13357v1)|null|\n", "2401.13296": "|**2024-01-24**|**Visual Objectification in Films: Towards a New AI Task for Video Interpretation**|\u7535\u5f71\u4e2d\u7684\u89c6\u89c9\u5bf9\u8c61\u5316\uff1a\u8fc8\u5411\u89c6\u9891\u89e3\u8bfb\u7684\u65b0\u4eba\u5de5\u667a\u80fd\u4efb\u52a1|Julie Tores, Lucile Sassatelli, Hui-Yin Wu, Clement Bergman, Lea Andolfi, Victor Ecrement, Frederic Precioso, Thierry Devars, Magali Guaresi, Virginie Julliard, et.al.|In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.|\u5728\u7535\u5f71\u6027\u522b\u7814\u7a76\u4e2d\uff0c\u201c\u7537\u6027\u51dd\u89c6\u201d\u7684\u6982\u5ff5\u662f\u6307\u89d2\u8272\u5728\u94f6\u5e55\u4e0a\u88ab\u63cf\u7ed8\u6210\u6b32\u671b\u7684\u5bf9\u8c61\u800c\u4e0d\u662f\u4e3b\u4f53\u7684\u65b9\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u89e3\u91ca\u4efb\u52a1\uff0c\u7528\u4e8e\u68c0\u6d4b\u7535\u5f71\u4e2d\u7684\u89d2\u8272\u5ba2\u89c2\u5316\u3002\u76ee\u7684\u662f\u63ed\u793a\u548c\u91cf\u5316\u7535\u5f71\u4e2d\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u7684\u4f7f\u7528\uff0c\u4ee5\u4ea7\u751f\u5ba2\u89c2\u5316\u7684\u8ba4\u77e5\u611f\u77e5\u3002\u6211\u4eec\u4ecb\u7ecd ObyGaze12 \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7531 1914 \u5e74\u7684\u7535\u5f71\u526a\u8f91\u7ec4\u6210\uff0c\u7531\u4e13\u5bb6\u9488\u5bf9\u7535\u5f71\u7814\u7a76\u548c\u5fc3\u7406\u5b66\u4e2d\u786e\u5b9a\u7684\u5ba2\u89c2\u5316\u6982\u5ff5\u8fdb\u884c\u4e86\u5bc6\u96c6\u6ce8\u91ca\u3002\u6211\u4eec\u8bc4\u4f30\u6700\u8fd1\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u5c55\u793a\u4efb\u52a1\u7684\u53ef\u884c\u6027\u4ee5\u53ca\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u65b0\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5411\u793e\u533a\u5f00\u653e\u3002|[2401.13296v1](http://arxiv.org/pdf/2401.13296v1)|null|\n", "2401.13270": "|**2024-01-24**|**Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics**|\u5229\u7528\u97f3\u9891\u573a\u666f\u8bed\u4e49\u6ce8\u5165\u97f3\u9891\u7684\u81ea\u52a8\u56fe\u50cf\u7740\u8272|Pengcheng Zhao, Yanxiang Chen, Yang Zhao, Wei Jia, Zhao Zhang, Ronggang Wang, Richang Hong|Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representation is fed into the pretrained network to finally realize the audio-guided colorization. The whole process is trained in a self-supervised manner without human annotation. In addition, an audiovisual colorization dataset is established for training and testing. Experiments demonstrate that audio guidance can effectively improve the performance of automatic colorization, especially for some scenes that are difficult to understand only from visual modality.|\u81ea\u52a8\u56fe\u50cf\u7740\u8272\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u75c5\u6001\u95ee\u9898\uff0c\u9700\u8981\u5bf9\u573a\u666f\u8fdb\u884c\u51c6\u786e\u7684\u8bed\u4e49\u7406\u89e3\u6765\u4f30\u8ba1\u7070\u5ea6\u56fe\u50cf\u7684\u5408\u7406\u989c\u8272\u3002\u5c3d\u7ba1\u6700\u8fd1\u57fa\u4e8e\u4ea4\u4e92\u7684\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4f46\u4e3a\u81ea\u52a8\u7740\u8272\u63a8\u65ad\u771f\u5b9e\u4e14\u51c6\u786e\u7684\u989c\u8272\u4ecd\u7136\u662f\u4e00\u9879\u975e\u5e38\u56f0\u96be\u7684\u4efb\u52a1\u3002\u4e3a\u4e86\u964d\u4f4e\u7070\u5ea6\u573a\u666f\u8bed\u4e49\u7406\u89e3\u7684\u96be\u5ea6\uff0c\u672c\u6587\u5c1d\u8bd5\u5229\u7528\u76f8\u5e94\u7684\u97f3\u9891\uff0c\u5b83\u81ea\u7136\u5305\u542b\u540c\u4e00\u573a\u666f\u7684\u989d\u5916\u8bed\u4e49\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u97f3\u9891\u6ce8\u5165\u81ea\u52a8\u56fe\u50cf\u7740\u8272\uff08AIAIC\uff09\u7f51\u7edc\uff0c\u5b83\u7531\u4e09\u4e2a\u9636\u6bb5\u7ec4\u6210\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ee5\u5f69\u8272\u56fe\u50cf\u8bed\u4e49\u4e3a\u6865\u6881\uff0c\u9884\u8bad\u7ec3\u7531\u5f69\u8272\u56fe\u50cf\u8bed\u4e49\u5f15\u5bfc\u7684\u5f69\u8272\u5316\u7f51\u7edc\u3002\u5176\u6b21\uff0c\u5229\u7528\u97f3\u9891\u548c\u89c6\u9891\u7684\u81ea\u7136\u5171\u73b0\u6765\u5b66\u4e60\u97f3\u9891\u548c\u89c6\u89c9\u573a\u666f\u4e4b\u95f4\u7684\u989c\u8272\u8bed\u4e49\u76f8\u5173\u6027\u3002\u7b2c\u4e09\uff0c\u5c06\u9690\u5f0f\u97f3\u9891\u8bed\u4e49\u8868\u793a\u8f93\u5165\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u6700\u7ec8\u5b9e\u73b0\u97f3\u9891\u5f15\u5bfc\u7740\u8272\u3002\u6574\u4e2a\u8fc7\u7a0b\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u89c6\u542c\u7740\u8272\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u97f3\u9891\u5f15\u5bfc\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u81ea\u52a8\u7740\u8272\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e00\u4e9b\u4ec5\u4ece\u89c6\u89c9\u6a21\u6001\u96be\u4ee5\u7406\u89e3\u7684\u573a\u666f\u3002|[2401.13270v1](http://arxiv.org/pdf/2401.13270v1)|null|\n", "2401.13267": "|**2024-01-24**|**Dual-modal Dynamic Traceback Learning for Medical Report Generation**|\u7528\u4e8e\u751f\u6210\u533b\u7597\u62a5\u544a\u7684\u53cc\u6a21\u6001\u52a8\u6001\u56de\u6eaf\u5b66\u4e60|Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Jinman Kim|With increasing reliance on medical imaging in clinical practices, automated report generation from medical images is in great demand. Existing report generation methods typically adopt an encoder-decoder deep learning framework to build a uni-directional image-to-report mapping. However, such a framework ignores the bi-directional mutual associations between images and reports, thus incurring difficulties in associating the intrinsic medical meanings between them. Recent generative representation learning methods have demonstrated the benefits of dual-modal learning from both image and text modalities. However, these methods exhibit two major drawbacks for medical report generation: 1) they tend to capture morphological information and have difficulties in capturing subtle pathological semantic information, and 2) they predict masked text rely on both unmasked images and text, inevitably degrading performance when inference is based solely on images. In this study, we propose a new report generation framework with dual-modal dynamic traceback learning (DTrace) to overcome the two identified drawbacks and enable dual-modal learning for medical report generation. To achieve this, our DTrace introduces a traceback mechanism to control the semantic validity of generated content via self-assessment. Further, our DTrace introduces a dynamic learning strategy to adapt to various proportions of image and text input, enabling report generation without reliance on textual input during inference. Extensive experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that our DTrace outperforms state-of-the-art medical report generation methods.|\u968f\u7740\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5bf9\u533b\u5b66\u6210\u50cf\u7684\u4f9d\u8d56\u65e5\u76ca\u589e\u52a0\uff0c\u5bf9\u4ece\u533b\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u62a5\u544a\u7684\u9700\u6c42\u5f88\u5927\u3002\u73b0\u6709\u7684\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6765\u6784\u5efa\u5355\u5411\u56fe\u50cf\u5230\u62a5\u544a\u7684\u6620\u5c04\u3002\u7136\u800c\uff0c\u8fd9\u6837\u7684\u6846\u67b6\u5ffd\u7565\u4e86\u56fe\u50cf\u548c\u62a5\u544a\u4e4b\u95f4\u7684\u53cc\u5411\u76f8\u4e92\u5173\u8054\uff0c\u56e0\u6b64\u5728\u5173\u8054\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u533b\u5b66\u542b\u4e49\u65f6\u9047\u5230\u4e86\u56f0\u96be\u3002\u6700\u8fd1\u7684\u751f\u6210\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5df2\u7ecf\u8bc1\u660e\u4e86\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u53cc\u6a21\u6001\u5b66\u4e60\u7684\u597d\u5904\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u7f3a\u70b9\uff1a1\uff09\u5b83\u4eec\u503e\u5411\u4e8e\u6355\u83b7\u5f62\u6001\u4fe1\u606f\uff0c\u4f46\u96be\u4ee5\u6355\u83b7\u5fae\u5999\u7684\u75c5\u7406\u8bed\u4e49\u4fe1\u606f\uff1b2\uff09\u5b83\u4eec\u9884\u6d4b\u5c4f\u853d\u6587\u672c\u4f9d\u8d56\u4e8e\u672a\u5c4f\u853d\u7684\u56fe\u50cf\u548c\u6587\u672c\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u63a8\u7406\u4ec5\u57fa\u4e8e\u56fe\u50cf\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u6a21\u6001\u52a8\u6001\u56de\u6eaf\u5b66\u4e60\uff08DTrace\uff09\u7684\u65b0\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u4e24\u4e2a\u5df2\u53d1\u73b0\u7684\u7f3a\u70b9\uff0c\u5e76\u5b9e\u73b0\u7528\u4e8e\u533b\u7597\u62a5\u544a\u751f\u6210\u7684\u53cc\u6a21\u6001\u5b66\u4e60\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u7684 DTrace \u5f15\u5165\u4e86\u8ffd\u6eaf\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u4f30\u6765\u63a7\u5236\u751f\u6210\u5185\u5bb9\u7684\u8bed\u4e49\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684 DTrace \u5f15\u5165\u4e86\u52a8\u6001\u5b66\u4e60\u7b56\u7565\u6765\u9002\u5e94\u5404\u79cd\u6bd4\u4f8b\u7684\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\uff0c\u4ece\u800c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700\u4f9d\u8d56\u6587\u672c\u8f93\u5165\u5373\u53ef\u751f\u6210\u62a5\u544a\u3002\u5bf9\u4e24\u4e2a\u57fa\u51c6\u826f\u597d\u7684\u6570\u636e\u96c6\uff08IU-Xray \u548c MIMIC-CXR\uff09\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 DTrace \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u533b\u7597\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u3002|[2401.13267v1](http://arxiv.org/pdf/2401.13267v1)|null|\n", "2401.13197": "|**2024-01-24**|**Predicting Mitral Valve mTEER Surgery Outcomes Using Machine Learning and Deep Learning Techniques**|\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u9884\u6d4b\u4e8c\u5c16\u74e3 mTEER \u624b\u672f\u7ed3\u679c|Tejas Vyas, Mohsena Chowdhury, Xiaojiao Xiao, Mathias Claeys, G\u00e9raldine Ong, Guanghui Wang|Mitral Transcatheter Edge-to-Edge Repair (mTEER) is a medical procedure utilized for the treatment of mitral valve disorders. However, predicting the outcome of the procedure poses a significant challenge. This paper makes the first attempt to harness classical machine learning (ML) and deep learning (DL) techniques for predicting mitral valve mTEER surgery outcomes. To achieve this, we compiled a dataset from 467 patients, encompassing labeled echocardiogram videos and patient reports containing Transesophageal Echocardiography (TEE) measurements detailing Mitral Valve Repair (MVR) treatment outcomes. Leveraging this dataset, we conducted a benchmark evaluation of six ML algorithms and two DL models. The results underscore the potential of ML and DL in predicting mTEER surgery outcomes, providing insight for future investigation and advancements in this domain.|\u4e8c\u5c16\u74e3\u7ecf\u5bfc\u7ba1\u8fb9\u7f18\u5bf9\u8fb9\u7f18\u4fee\u590d\u672f (mTEER) \u662f\u4e00\u79cd\u7528\u4e8e\u6cbb\u7597\u4e8c\u5c16\u74e3\u75be\u75c5\u7684\u533b\u7597\u624b\u672f\u3002\u7136\u800c\uff0c\u9884\u6d4b\u624b\u672f\u7ed3\u679c\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u9996\u6b21\u5c1d\u8bd5\u5229\u7528\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6280\u672f\u6765\u9884\u6d4b\u4e8c\u5c16\u74e3 mTEER \u624b\u672f\u7ed3\u679c\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u7f16\u5236\u4e86 467 \u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u6807\u8bb0\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u548c\u60a3\u8005\u62a5\u544a\uff0c\u5176\u4e2d\u5305\u542b\u8be6\u7ec6\u8bf4\u660e\u4e8c\u5c16\u74e3\u4fee\u590d (MVR) \u6cbb\u7597\u7ed3\u679c\u7684\u7ecf\u98df\u7ba1\u8d85\u58f0\u5fc3\u52a8\u56fe (TEE) \u6d4b\u91cf\u7ed3\u679c\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5bf9\u516d\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u8bc4\u4f30\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86 ML \u548c DL \u5728\u9884\u6d4b mTEER \u624b\u672f\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u548c\u8fdb\u5c55\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002|[2401.13197v1](http://arxiv.org/pdf/2401.13197v1)|null|\n", "2401.13161": "|**2024-01-24**|**A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing Algorithm**|\u4e00\u79cd\u5e7f\u4e49\u7684\u57fa\u4e8e\u591a\u5c3a\u5ea6\u675f\u7684\u9ad8\u5149\u8c31\u7a00\u758f\u89e3\u6df7\u7b97\u6cd5|Luciano Carvalho Ayres, Ricardo Augusto Borsoi, Jos\u00e9 Carlos Moreira Bermudez, S\u00e9rgio Jos\u00e9 Melo de Almeida|In hyperspectral sparse unmixing, a successful approach employs spectral bundles to address the variability of the endmembers in the spatial domain. However, the regularization penalties usually employed aggregate substantial computational complexity, and the solutions are very noise-sensitive. We generalize a multiscale spatial regularization approach to solve the unmixing problem by incorporating group sparsity-inducing mixed norms. Then, we propose a noise-robust method that can take advantage of the bundle structure to deal with endmember variability while ensuring inter- and intra-class sparsity in abundance estimation with reasonable computational cost. We also present a general heuristic to select the \\emph{most representative} abundance estimation over multiple runs of the unmixing process, yielding a solution that is robust and highly reproducible. Experiments illustrate the robustness and consistency of the results when compared to related methods.|\u5728\u9ad8\u5149\u8c31\u7a00\u758f\u5206\u89e3\u4e2d\uff0c\u4e00\u79cd\u6210\u529f\u7684\u65b9\u6cd5\u91c7\u7528\u5149\u8c31\u675f\u6765\u89e3\u51b3\u7a7a\u95f4\u57df\u4e2d\u7aef\u5143\u7684\u53ef\u53d8\u6027\u3002\u7136\u800c\uff0c\u901a\u5e38\u91c7\u7528\u7684\u6b63\u5219\u5316\u60e9\u7f5a\u4f1a\u805a\u96c6\u5927\u91cf\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u4e14\u89e3\u51b3\u65b9\u6848\u5bf9\u566a\u58f0\u975e\u5e38\u654f\u611f\u3002\u6211\u4eec\u63a8\u5e7f\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7fa4\u4f53\u7a00\u758f\u6027\u6df7\u5408\u8303\u6570\u6765\u89e3\u51b3\u6df7\u5408\u95ee\u9898\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6297\u566a\u58f0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u675f\u7ed3\u6784\u6765\u5904\u7406\u7aef\u5143\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4ee5\u5408\u7406\u7684\u8ba1\u7b97\u6210\u672c\u786e\u4fdd\u4e30\u5ea6\u4f30\u8ba1\u4e2d\u7684\u7c7b\u95f4\u548c\u7c7b\u5185\u7a00\u758f\u6027\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u6b21\u8fd0\u884c\u7684\u5206\u79bb\u8fc7\u7a0b\u4e2d\u9009\u62e9 \\emph{\u6700\u5177\u4ee3\u8868\u6027} \u4e30\u5ea6\u4f30\u8ba1\uff0c\u4ece\u800c\u4ea7\u751f\u7a33\u5065\u4e14\u9ad8\u5ea6\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4e0e\u76f8\u5173\u65b9\u6cd5\u76f8\u6bd4\u7ed3\u679c\u7684\u7a33\u5065\u6027\u548c\u4e00\u81f4\u6027\u3002|[2401.13161v1](http://arxiv.org/pdf/2401.13161v1)|**[link](https://github.com/lucayress/gmbua)**|\n"}}