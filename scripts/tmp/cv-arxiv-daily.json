{"\u751f\u6210\u6a21\u578b": {}, "\u591a\u6a21\u6001": {"2402.10896": "|**2024-02-16**|**PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter**|PaLM2-VAdapter\uff1a\u9010\u6b65\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u6253\u9020\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u9002\u914d\u5668|Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang|This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance, and stronger scalability. Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement.|\u672c\u6587\u8bc1\u660e\u4e86\u9010\u6b65\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u8fde\u63a5\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u867d\u7136\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6cd5\u5b66\u7855\u58eb\u7684\u57fa\u672c\u67b6\u6784\u548c\u9884\u8bad\u7ec3\u65b9\u6cd5\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u89c6\u89c9\u8bed\u8a00\u9002\u914d\u5668\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u5728\u6700\u8fd1\u7684\u5de5\u4f5c\u4e2d\u5b58\u5728\u5f88\u5927\u5dee\u5f02\u3002\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684\u611f\u77e5\u5668\u91cd\u91c7\u6837\u5668\u67b6\u6784\u8fdb\u884c\u4e86\u5f7b\u5e95\u7684\u63a2\u7d22\uff0c\u5e76\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u89c6\u89c9\u8bed\u8a00\u4e0e\u611f\u77e5\u5668\u91cd\u91c7\u6837\u5668\u7684\u5bf9\u9f50\u8868\u73b0\u51fa\u7f13\u6162\u7684\u6536\u655b\u6027\u548c\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u7f3a\u4e4f\u76f4\u63a5\u76d1\u7763\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51faPaLM2-VAdapter\uff0c\u91c7\u7528\u6e10\u8fdb\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u9002\u914d\u5668\u3002\u4e0e\u5177\u6709\u611f\u77e5\u5668\u91cd\u91c7\u6837\u5668\u7684\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6839\u636e\u7ecf\u9a8c\u663e\u793a\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u3002\u5bf9\u56fe\u50cf\u548c\u89c6\u9891\u7684\u5404\u79cd\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u5b57\u5e55\u4efb\u52a1\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u6bd4\u6700\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c11 30%~70% \u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u6807\u5fd7\u7740\u6548\u7387\u7684\u663e\u7740\u63d0\u9ad8\u3002|[2402.10896v1](http://arxiv.org/pdf/2402.10896v1)|null|\n", "2402.10894": "|**2024-02-16**|**Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning**|\u878d\u5408\u5f25\u6563\u52a0\u6743 MRI \u548c\u4e34\u5e8a\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u9884\u6d4b\u6025\u6027\u7f3a\u8840\u6027\u4e2d\u98ce\u540e\u7684\u529f\u80fd\u7ed3\u679c|Chia-Ling Tsai, Hui-Yun Su, Shen-Feng Sung, Wei-Yang Lin, Ying-Ying Su, Tzu-Hsien Yang, Man-Lin Mai|Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the prediction of the patient needing long-term care at 3 months after the onset of stroke. Trained and evaluated with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80 and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing models that consolidate both imaging and structured data in the medical domain. If trained with comprehensive clinical variables, including NIHSS and comorbidities, the gain from images on making accurate prediction is not considered substantial, but significant. However, diffusion-weighted MRI can replace NIHSS to achieve comparable level of accuracy combining with other readily available clinical variables for better generalization.|\u4e2d\u98ce\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u81f4\u6b8b\u6027\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u5f71\u54cd\u7ea6\u56db\u5206\u4e4b\u4e00\u7684 25 \u5c81\u4ee5\u4e0a\u6210\u5e74\u4eba\u53e3\uff1b\u8d85\u8fc7\u4e00\u534a\u7684\u60a3\u8005\u5728\u6025\u6027\u4e2d\u98ce\u53d1\u4f5c\u540e\u4ecd\u7136\u9884\u540e\u4e0d\u826f\uff0c\u4f8b\u5982\u6c38\u4e45\u6027\u529f\u80fd\u4f9d\u8d56\u751a\u81f3\u6b7b\u4ea1\u3002\u672c\u7814\u7a76\u7684\u76ee\u7684\u662f\u8c03\u67e5\u5f25\u6563\u52a0\u6743 MRI \u6a21\u5f0f\u4e0e\u7ed3\u6784\u5316\u5065\u5eb7\u72b6\u51b5\u76f8\u7ed3\u5408\u5728\u9884\u6d4b\u529f\u80fd\u7ed3\u679c\u4ee5\u4fc3\u8fdb\u65e9\u671f\u5e72\u9884\u65b9\u9762\u7684\u529f\u6548\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u6df1\u5ea6\u878d\u5408\u5b66\u4e60\u7f51\u7edc\uff1a\u7b2c\u4e00\u9636\u6bb5\u4fa7\u91cd\u4e8e\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4fa7\u91cd\u4e8e\u5206\u7c7b\u3002\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u533a\u5206\u7279\u5f81\uff0c\u5c06\u4e24\u7c7b\u60a3\u8005\u4e0e\u4e2a\u4f53\u6a21\u6001\u5d4c\u5165\u548c\u878d\u5408\u591a\u6a21\u6001\u5d4c\u5165\u533a\u5206\u5f00\u6765\u3002\u8be5\u7f51\u7edc\u5c06 DWI \u548c ADC \u56fe\u50cf\u4ee5\u53ca\u7ed3\u6784\u5316\u5065\u5eb7\u6863\u6848\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002\u7ed3\u679c\u662f\u9884\u6d4b\u60a3\u8005\u5728\u4e2d\u98ce\u53d1\u4f5c\u540e 3 \u4e2a\u6708\u9700\u8981\u957f\u671f\u62a4\u7406\u3002\u4f7f\u7528 3297 \u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u878d\u5408\u6a21\u578b\u7684 AUC\u3001F1 \u5206\u6570\u548c\u51c6\u786e\u6027\u5206\u522b\u8fbe\u5230 0.87\u30010.80 \u548c 80.45%\uff0c\u4f18\u4e8e\u5728\u533b\u5b66\u9886\u57df\u6574\u5408\u6210\u50cf\u548c\u7ed3\u6784\u5316\u6570\u636e\u7684\u73b0\u6709\u6a21\u578b\u3002\u5982\u679c\u4f7f\u7528\u5168\u9762\u7684\u4e34\u5e8a\u53d8\u91cf\uff08\u5305\u62ec NIHSS \u548c\u5408\u5e76\u75c7\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5219\u4ece\u56fe\u50cf\u4e2d\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u7684\u6536\u76ca\u5e76\u4e0d\u88ab\u8ba4\u4e3a\u662f\u5b9e\u8d28\u6027\u7684\uff0c\u4f46\u5374\u662f\u663e\u7740\u7684\u3002\u7136\u800c\uff0c\u5f25\u6563\u52a0\u6743 MRI \u53ef\u4ee5\u53d6\u4ee3 NIHSS\uff0c\u4e0e\u5176\u4ed6\u73b0\u6210\u7684\u4e34\u5e8a\u53d8\u91cf\u76f8\u7ed3\u5408\uff0c\u8fbe\u5230\u53ef\u6bd4\u7684\u51c6\u786e\u5ea6\u6c34\u5e73\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6982\u62ec\u3002|[2402.10894v1](http://arxiv.org/pdf/2402.10894v1)|null|\n", "2402.10884": "|**2024-02-16**|**Multi-modal preference alignment remedies regression of visual instruction tuning on language model**|\u591a\u6a21\u6001\u504f\u597d\u5bf9\u9f50\u8865\u6551\u4e86\u8bed\u8a00\u6a21\u578b\u4e0a\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u7684\u56de\u5f52|Shengzhi Li, Rongyu Lin, Shichao Pei|In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning.|\u5728\u751f\u4ea7\u4e2d\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6709\u671b\u652f\u6301\u4e92\u6362\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u591a\u8f6e\u67e5\u8be2\u3002\u7136\u800c\uff0c\u5f53\u524d\u4f7f\u7528\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\u8bad\u7ec3\u7684 MLLM \u53ef\u80fd\u4f1a\u51fa\u73b0\u9000\u5316\uff0c\u56e0\u4e3a VQA \u6570\u636e\u96c6\u7f3a\u4e4f\u8bad\u7ec3\u5e95\u5c42\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u6587\u672c\u6307\u4ee4\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u9000\u5316\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u6536\u96c6\u4e00\u4e2a\u8f7b\u91cf\u7ea7\uff086k \u6761\u76ee\uff09VQA \u504f\u597d\u6570\u636e\u96c6\uff0c\u5176\u4e2d Gemini \u4ee5\u7ec6\u7c92\u5ea6\u65b9\u5f0f\u5bf9 5 \u4e2a\u8d28\u91cf\u6307\u6807\u7684\u7b54\u6848\u8fdb\u884c\u6ce8\u91ca\uff0c\u5e76\u7814\u7a76\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3001\u62d2\u7edd\u91c7\u6837\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO)\u548c SteerLM\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6570\u636e\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46\u501f\u52a9 DPO\uff0c\u6211\u4eec\u80fd\u591f\u8d85\u8d8a\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u8e2a\u80fd\u529b\uff0c\u5728 MT-Bench \u4e0a\u83b7\u5f97 6.73 \u5206\uff0c\u800c Vicuna \u7684\u5f97\u5206\u4e3a 6.57\uff0cLLaVA \u7684\u5f97\u5206\u4e3a 5.99\u3002\u6587\u672c\u6307\u4ee4\u719f\u7ec3\u7a0b\u5ea6\u7684\u63d0\u9ad8\u4e0e\u89c6\u89c9\u6307\u4ee4\u6027\u80fd\u7684\u63d0\u9ad8\u76f8\u5173\uff08MM-Vet \u4e0a +4.9\\%\uff0cLLaVA-Bench \u4e0a +6\\%\uff09\uff0c\u4e0e\u4e4b\u524d\u7684 RLHF \u65b9\u6cd5\u76f8\u6bd4\uff0c\u89c6\u89c9\u77e5\u8bc6\u57fa\u51c6\u7684\u5bf9\u9f50\u7a0e\u6700\u5c0f\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u6a21\u578b\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u5177\u6709\u7ec6\u7c92\u5ea6\u6ce8\u91ca\uff0c\u53ef\u4ee5\u534f\u8c03 MLLM \u7684\u6587\u672c\u548c\u89c6\u89c9\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u540e\u6062\u590d\u548c\u589e\u5f3a\u8bed\u8a00\u80fd\u529b\u3002|[2402.10884v1](http://arxiv.org/pdf/2402.10884v1)|null|\n", "2402.10855": "|**2024-02-16**|**Control Color: Multimodal Diffusion-based Interactive Image Colorization**|\u63a7\u5236\u989c\u8272\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u7740\u8272|Zhexin Liang, Zhaochen Li, Shangchen Zhou, Chongyi Li, Chen Change Loy|Despite the existence of numerous colorization methods, several limitations still exist, such as lack of user interaction, inflexibility in local colorization, unnatural color rendering, insufficient color variation, and color overflow. To solve these issues, we introduce Control Color (CtrlColor), a multi-modal colorization method that leverages the pre-trained Stable Diffusion (SD) model, offering promising capabilities in highly controllable interactive image colorization. While several diffusion-based methods have been proposed, supporting colorization in multiple modalities remains non-trivial. In this study, we aim to tackle both unconditional and conditional image colorization (text prompts, strokes, exemplars) and address color overflow and incorrect color within a unified framework. Specifically, we present an effective way to encode user strokes to enable precise local color manipulation and employ a practical way to constrain the color distribution similar to exemplars. Apart from accepting text prompts as conditions, these designs add versatility to our approach. We also introduce a novel module based on self-attention and a content-guided deformable autoencoder to address the long-standing issues of color overflow and inaccurate coloring. Extensive comparisons show that our model outperforms state-of-the-art image colorization methods both qualitatively and quantitatively.|\u5c3d\u7ba1\u5b58\u5728\u591a\u79cd\u7740\u8272\u65b9\u6cd5\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u7f3a\u4e4f\u7528\u6237\u4ea4\u4e92\u3001\u5c40\u90e8\u7740\u8272\u4e0d\u7075\u6d3b\u3001\u663e\u8272\u4e0d\u81ea\u7136\u3001\u989c\u8272\u53d8\u5316\u4e0d\u8db3\u548c\u989c\u8272\u6ea2\u51fa\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u63a7\u5236\u989c\u8272\uff08CtrlColor\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\uff08SD\uff09\u6a21\u578b\u7684\u591a\u6a21\u6001\u7740\u8272\u65b9\u6cd5\uff0c\u5728\u9ad8\u5ea6\u53ef\u63a7\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u7740\u8272\u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u524d\u9014\u7684\u529f\u80fd\u3002\u867d\u7136\u5df2\u7ecf\u63d0\u51fa\u4e86\u51e0\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u4f46\u652f\u6301\u591a\u79cd\u6a21\u5f0f\u7684\u7740\u8272\u4ecd\u7136\u5f88\u91cd\u8981\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5728\u7edf\u4e00\u7684\u6846\u67b6\u5185\u89e3\u51b3\u65e0\u6761\u4ef6\u548c\u6761\u4ef6\u56fe\u50cf\u7740\u8272\uff08\u6587\u672c\u63d0\u793a\u3001\u7b14\u5212\u3001\u793a\u4f8b\uff09\u5e76\u89e3\u51b3\u989c\u8272\u6ea2\u51fa\u548c\u4e0d\u6b63\u786e\u7684\u989c\u8272\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f16\u7801\u7528\u6237\u7b14\u753b\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u5c40\u90e8\u989c\u8272\u64cd\u4f5c\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u7ea6\u675f\u7c7b\u4f3c\u4e8e\u793a\u4f8b\u7684\u989c\u8272\u5206\u5e03\u3002\u9664\u4e86\u63a5\u53d7\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u6761\u4ef6\u4e4b\u5916\uff0c\u8fd9\u4e9b\u8bbe\u8ba1\u8fd8\u4e3a\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u52a0\u4e86\u591a\u529f\u80fd\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u65b0\u9896\u6a21\u5757\u548c\u5185\u5bb9\u5f15\u5bfc\u7684\u53ef\u53d8\u5f62\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u4ee5\u89e3\u51b3\u957f\u671f\u5b58\u5728\u7684\u989c\u8272\u6ea2\u51fa\u548c\u7740\u8272\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u5e7f\u6cdb\u7684\u6bd4\u8f83\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u7740\u8272\u65b9\u6cd5\u3002|[2402.10855v1](http://arxiv.org/pdf/2402.10855v1)|null|\n", "2402.10805": "|**2024-02-16**|**Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond**|\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\uff1a\u5728\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e2d\u8bb0\u5fc6\u56fe\u50cf\u4ee5\u4f9b\u68c0\u7d22\u53ca\u5176\u4ed6\u4f7f\u7528|Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, Tat-Seng Chua|The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to \"recall\" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.|\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u8bc1\u660e\u4e86\u5b83\u4eec\u8bb0\u5fc6\u6587\u6863\u77e5\u8bc6\u548c\u56de\u5fc6\u77e5\u8bc6\u4ee5\u6709\u6548\u54cd\u5e94\u7528\u6237\u67e5\u8be2\u7684\u80fd\u529b\u3002\u5728\u6b64\u529f\u80fd\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u80fd\u591f\u5728\u5176\u53c2\u6570\u5185\u8bb0\u5fc6\u548c\u8c03\u7528\u56fe\u50cf\u3002\u7ed9\u5b9a\u7528\u6237\u5bf9\u89c6\u89c9\u5185\u5bb9\u7684\u67e5\u8be2\uff0cMLLM \u9884\u8ba1\u4f1a\u4ece\u5176\u53c2\u6570\u4e2d\u201c\u8c03\u7528\u201d\u76f8\u5173\u56fe\u50cf\u4f5c\u4e3a\u54cd\u5e94\u3002\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u9762\u4e34\u7740\u663e\u7740\u7684\u6311\u6218\uff0c\u5305\u62ec MLLM \u4e2d\u5185\u7f6e\u7684\u89c6\u89c9\u8bb0\u5fc6\u548c\u89c6\u89c9\u56de\u5fc6\u65b9\u6848\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u8de8\u6a21\u5f0f\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5206\u914d\u552f\u4e00\u7684\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u6765\u8868\u793a\u56fe\u50cf\uff0c\u5e76\u6d89\u53ca\u4e24\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff1a\u5b66\u4e60\u8bb0\u5fc6\u548c\u5b66\u4e60\u68c0\u7d22\u3002\u7b2c\u4e00\u6b65\u91cd\u70b9\u662f\u8bad\u7ec3 MLLM \u8bb0\u4f4f\u56fe\u50cf\u4e0e\u5176\u5404\u81ea\u6807\u8bc6\u7b26\u4e4b\u95f4\u7684\u5173\u8054\u3002\u540e\u4e00\u6b65\u6559\u5bfc MLLM \u5728\u7ed9\u5b9a\u6587\u672c\u67e5\u8be2\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u76ee\u6807\u56fe\u50cf\u7684\u76f8\u5e94\u6807\u8bc6\u7b26\u3002\u901a\u8fc7\u5728 MLLM \u4e2d\u8bb0\u5fc6\u56fe\u50cf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u8303\u4f8b\uff0c\u4e0e\u4e4b\u524d\u7684\u5224\u522b\u65b9\u6cd5\u4e0d\u540c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5927\u89c4\u6a21\u56fe\u50cf\u5019\u9009\u96c6\uff0c\u751f\u6210\u8303\u5f0f\u4e5f\u80fd\u6709\u6548\u4e14\u9ad8\u6548\u5730\u6267\u884c\u3002|[2402.10805v1](http://arxiv.org/pdf/2402.10805v1)|null|\n", "2402.10717": "|**2024-02-16**|**BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion**|BioFusionNet\uff1a\u901a\u8fc7\u591a\u7279\u5f81\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u5bf9 ER+ \u4e73\u817a\u764c\u8fdb\u884c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u5b58\u98ce\u9669\u5206\u5c42|Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering|Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive multimodal feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field. The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80--4.68, p<0.005). The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data.|\u4e73\u817a\u764c\u662f\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u5973\u6027\u7684\u91cd\u5927\u5065\u5eb7\u95ee\u9898\u3002\u51c6\u786e\u7684\u751f\u5b58\u98ce\u9669\u5206\u5c42\u5728\u6307\u5bfc\u4e2a\u6027\u5316\u6cbb\u7597\u51b3\u7b56\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 BioFusionNet\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06\u56fe\u50cf\u884d\u751f\u7684\u7279\u5f81\u4e0e\u9057\u4f20\u548c\u4e34\u5e8a\u6570\u636e\u878d\u5408\u8d77\u6765\uff0c\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u60a3\u8005\u6982\u51b5\u5e76\u5bf9 ER+ \u4e73\u817a\u764c\u60a3\u8005\u8fdb\u884c\u751f\u5b58\u98ce\u9669\u5206\u5c42\u3002\u6211\u4eec\u91c7\u7528\u591a\u4e2a\u81ea\u76d1\u7763\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5373 DINO \u548c MoCoV3\uff0c\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u6591\u5757\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u6355\u83b7\u8be6\u7ec6\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7279\u5f81\u3002\u7136\u540e\uff0c\u6211\u4eec\u5229\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08VAE\uff09\u6765\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\uff0c\u5e76\u5229\u7528 VAE \u7684\u6f5c\u5728\u7a7a\u95f4\u5c06\u5176\u8f93\u5165\u5230\u81ea\u6ce8\u610f\u529b\u7f51\u7edc\u4e2d\uff0c\u751f\u6210\u60a3\u8005\u7ea7\u522b\u7684\u7279\u5f81\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u7ec4\u7ec7\u75c5\u7406\u5b66\u7279\u5f81\u4e0e\u9057\u4f20\u6570\u636e\u7ed3\u5408\u8d77\u6765\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u6574\u5408\u4e34\u5e8a\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u9884\u6d4b\u6027\u80fd\u5e76\u5b9e\u73b0\u5168\u9762\u7684\u591a\u6a21\u6001\u7279\u5f81\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a0\u6743 Cox \u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u4e0d\u5e73\u8861\u7684\u751f\u5b58\u6570\u636e\uff0c\u8fd9\u662f\u8be5\u9886\u57df\u7684\u5e38\u89c1\u6311\u6218\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5b9e\u73b0\u4e86 0.77 \u7684\u5e73\u5747\u4e00\u81f4\u6027\u6307\u6570\uff08C \u6307\u6570\uff09\u548c 0.84 \u7684\u65f6\u95f4\u76f8\u5173\u66f2\u7ebf\u4e0b\u9762\u79ef (AUC)\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u5b83\u5728\u5355\u53d8\u91cf\u5206\u6790\u4e2d\u9884\u6d4b\u5bf9\u603b\u751f\u5b58 (OS) \u5177\u6709\u9884\u540e\u610f\u4e49\u7684\u98ce\u9669\uff08\u9ad8\u4e0e\u4f4e\uff09\uff08HR=2.99\uff0c95% CI\uff1a1.88--4.78\uff0cp<0.005\uff09\uff0c\u5e76\u5728\u7eb3\u5165\u6807\u51c6\u4e34\u5e8a\u75c5\u7406\u53d8\u91cf\u7684\u591a\u53d8\u91cf\u5206\u6790\u4e2d\u4fdd\u6301\u72ec\u7acb\u663e\u7740\u6027\uff08HR=2.91\uff0c95% CI\uff1a1.80--4.68\uff0cp<0.005\uff09\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u800c\u4e14\u89e3\u51b3\u4e86\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\u3002|[2402.10717v1](http://arxiv.org/pdf/2402.10717v1)|null|\n", "2402.10698": "|**2024-02-16**|**Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering**|\u96f6\u955c\u5934\u89c6\u9891\u95ee\u7b54\u7684\u95ee\u9898\u6307\u5bfc\u89c6\u89c9\u63cf\u8ff0|David Romero, Thamar Solorio|We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM). The LLM is our reasoning module, and performs the final step of multiple-choice QA. Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA.|\u6211\u4eec\u63d0\u51fa\u4e86 Q-ViD\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\uff08\u89c6\u9891 QA\uff09\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u4e0e\u57fa\u4e8e\u590d\u6742\u67b6\u6784\u3001\u8ba1\u7b97\u6210\u672c\u6602\u8d35\u7684\u7ba1\u9053\u6216\u4f7f\u7528 GPT \u7b49\u5c01\u95ed\u6a21\u578b\u7684\u5148\u524d\u65b9\u6cd5\u4e0d\u540c\uff0cQ-ViD \u4f9d\u8d56\u4e8e\u5355\u4e2a\u6307\u4ee4\u611f\u77e5\u5f00\u653e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08InstructBLIP\uff09\u4f7f\u7528\u5e27\u63cf\u8ff0\u6765\u5904\u7406\u89c6\u9891QA\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u521b\u5efa\u4f9d\u8d56\u4e8e\u89c6\u9891\u76ee\u6807\u95ee\u9898\u7684\u5b57\u5e55\u8bf4\u660e\u63d0\u793a\uff0c\u5e76\u5229\u7528 InstructBLIP \u6765\u83b7\u53d6\u5bf9\u5f53\u524d\u4efb\u52a1\u6709\u7528\u7684\u89c6\u9891\u5e27\u5b57\u5e55\u3002\u968f\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5e27\u6807\u9898\u5f62\u6210\u6574\u4e2a\u89c6\u9891\u7684\u63cf\u8ff0\uff0c\u5e76\u5c06\u8be5\u4fe1\u606f\u4e0e\u95ee\u7b54\u63d0\u793a\u4e00\u8d77\u63d0\u4f9b\u7ed9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3002 LLM \u662f\u6211\u4eec\u7684\u63a8\u7406\u6a21\u5757\uff0c\u6267\u884c\u591a\u9879\u9009\u62e9 QA \u7684\u6700\u540e\u4e00\u6b65\u3002\u6211\u4eec\u7b80\u5355\u7684 Q-ViD \u6846\u67b6\u5728\u5404\u79cd\u89c6\u9891 QA \u57fa\u51c6\uff08\u5305\u62ec NExT-QA\u3001STAR\u3001How2QA\u3001TVQA \u548c IntentQA\uff09\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\u751a\u81f3\u66f4\u9ad8\u7684\u6027\u80fd\u3002|[2402.10698v1](http://arxiv.org/pdf/2402.10698v1)|null|\n", "2402.10580": "|**2024-02-16**|**Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation**|\u8054\u5408\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u9ad8\u6548\u591a\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027|Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich|Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular depth estimation as well as efficient multi-task uncertainty quantification. By implicitly leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient.|\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6210\u4e3a\u89e3\u51b3\u5e38\u89c1\u6311\u6218\u7684\u4e00\u79cd\u53ef\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u4f8b\u5982\u8fc7\u5ea6\u81ea\u4fe1\u6216\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u91cf\u901a\u5e38\u5f88\u5927\u3002\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u5f0f\u7684\uff0c\u56e0\u6b64\u53d7\u76ca\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u8054\u5408\u89e3\u51b3\u65b9\u6848\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u4ef7\u503c\u7684\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5c06\u4e0d\u540c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e0e\u8054\u5408\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u5408\u8d77\u6765\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u4e4b\u95f4\u7684\u6027\u80fd\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63ed\u793a\u4e86\u4e0e\u5355\u72ec\u89e3\u51b3\u4e24\u4e2a\u4efb\u52a1\u76f8\u6bd4\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u65b9\u9762\u7684\u4f18\u52bf\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u5f15\u5165\u4e86 EMUFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u901a\u8fc7\u9690\u5f0f\u5730\u5229\u7528\u6559\u5e08\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0cEMUFormer \u5728 Cityscapes \u548c NYUv2 \u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u8fd8\u4f30\u8ba1\u4e86\u8fd9\u4e24\u9879\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e0e Deep Ensemble \u76f8\u5f53\u6216\u4f18\u4e8e Deep Ensemble\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u987a\u5e8f\u662f\u5e45\u5ea6\u66f4\u6709\u6548\u3002|[2402.10580v1](http://arxiv.org/pdf/2402.10580v1)|null|\n", "2402.10534": "|**2024-02-16**|**Using Left and Right Brains Together: Towards Vision and Language Planning**|\u5171\u540c\u4f7f\u7528\u5de6\u53f3\u8111\uff1a\u8fc8\u5411\u89c6\u89c9\u548c\u8bed\u8a00\u89c4\u5212|Jun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, Jianguo Zhang|Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution.|\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u51b3\u7b56\u5c4f\u853d\u80fd\u529b\u3002\u4f46\u4ed6\u4eec\u672c\u8d28\u4e0a\u662f\u5728\u8bed\u8a00\u7a7a\u95f4\u5185\u8fdb\u884c\u89c4\u5212\uff0c\u7f3a\u4e4f\u89c6\u91ce\u548c\u7a7a\u95f4\u60f3\u8c61\u80fd\u529b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u5728\u601d\u7ef4\u8fc7\u7a0b\u4e2d\u5229\u7528\u5927\u8111\u7684\u5de6\u534a\u7403\u548c\u53f3\u534a\u7403\u8fdb\u884c\u8bed\u8a00\u548c\u89c6\u89c9\u89c4\u5212\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u5bf9\u5177\u6709\u4efb\u4f55\u5f62\u5f0f\u8f93\u5165\u7684\u4efb\u52a1\u6267\u884c\u5e76\u53d1\u89c6\u89c9\u548c\u8bed\u8a00\u89c4\u5212\u3002\u6211\u4eec\u7684\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u89c4\u5212\u6765\u6355\u6349\u590d\u6742\u7684\u73af\u5883\u7ec6\u8282\uff0c\u800c\u8bed\u8a00\u89c4\u5212\u5219\u589e\u5f3a\u4e86\u6574\u4e2a\u7cfb\u7edf\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u3001\u4ec5\u89c6\u89c9\u4efb\u52a1\u548c\u4ec5\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8868\u660e\u89c6\u89c9\u548c\u8bed\u8a00\u89c4\u5212\u7684\u96c6\u6210\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4efb\u52a1\u6267\u884c\u3002|[2402.10534v1](http://arxiv.org/pdf/2402.10534v1)|null|\n", "2402.10454": "|**2024-02-16**|**Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration**|\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u548c\u8f85\u52a9\u4efb\u52a1\u96c6\u6210\u4f18\u5316\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b|Mahapara Khurshid, Mayank Vatsa, Richa Singh|The rising global prevalence of skin conditions, some of which can escalate to life-threatening stages if not timely diagnosed and treated, presents a significant healthcare challenge. This issue is particularly acute in remote areas where limited access to healthcare often results in delayed treatment, allowing skin diseases to advance to more critical stages. One of the primary challenges in diagnosing skin diseases is their low inter-class variations, as many exhibit similar visual characteristics, making accurate classification challenging. This research introduces a novel multimodal method for classifying skin lesions, integrating smartphone-captured images with essential clinical and demographic information. This approach mimics the diagnostic process employed by medical professionals. A distinctive aspect of this method is the integration of an auxiliary task focused on super-resolution image prediction. This component plays a crucial role in refining visual details and enhancing feature extraction, leading to improved differentiation between classes and, consequently, elevating the overall effectiveness of the model. The experimental evaluations have been conducted using the PAD-UFES20 dataset, applying various deep-learning architectures. The results of these experiments not only demonstrate the effectiveness of the proposed method but also its potential applicability under-resourced healthcare environments.|\u5168\u7403\u76ae\u80a4\u75c5\u60a3\u75c5\u7387\u4e0d\u65ad\u4e0a\u5347\uff0c\u5176\u4e2d\u4e00\u4e9b\u5982\u679c\u4e0d\u53ca\u65f6\u8bca\u65ad\u548c\u6cbb\u7597\u53ef\u80fd\u4f1a\u5347\u7ea7\u81f3\u5371\u53ca\u751f\u547d\u7684\u9636\u6bb5\uff0c\u8fd9\u5bf9\u533b\u7597\u4fdd\u5065\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e2a\u95ee\u9898\u5728\u504f\u8fdc\u5730\u533a\u5c24\u4e3a\u4e25\u91cd\uff0c\u8fd9\u4e9b\u5730\u533a\u83b7\u5f97\u533b\u7597\u670d\u52a1\u7684\u673a\u4f1a\u6709\u9650\uff0c\u5e38\u5e38\u5bfc\u81f4\u6cbb\u7597\u5ef6\u8fdf\uff0c\u4ece\u800c\u4f7f\u76ae\u80a4\u75c5\u53d1\u5c55\u5230\u66f4\u4e25\u91cd\u7684\u9636\u6bb5\u3002\u8bca\u65ad\u76ae\u80a4\u75c5\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u5176\u7c7b\u95f4\u5dee\u5f02\u8f83\u5c0f\uff0c\u56e0\u4e3a\u8bb8\u591a\u76ae\u80a4\u75c5\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u8fd9\u4f7f\u5f97\u51c6\u786e\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u9879\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u5f0f\u65b9\u6cd5\u6765\u5bf9\u76ae\u80a4\u75c5\u53d8\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u667a\u80fd\u624b\u673a\u6355\u83b7\u7684\u56fe\u50cf\u4e0e\u57fa\u672c\u7684\u4e34\u5e8a\u548c\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u76f8\u7ed3\u5408\u3002\u8fd9\u79cd\u65b9\u6cd5\u6a21\u4eff\u4e86\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u6240\u91c7\u7528\u7684\u8bca\u65ad\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u7684\u4e00\u4e2a\u72ec\u7279\u4e4b\u5904\u662f\u96c6\u6210\u4e86\u4e13\u6ce8\u4e8e\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u9884\u6d4b\u7684\u8f85\u52a9\u4efb\u52a1\u3002\u8be5\u7ec4\u4ef6\u5728\u7ec6\u5316\u89c6\u89c9\u7ec6\u8282\u548c\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7c7b\u4e4b\u95f4\u7684\u533a\u5206\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8bc4\u4f30\u662f\u4f7f\u7528 PAD-UFES20 \u6570\u636e\u96c6\u8fdb\u884c\u7684\uff0c\u5e94\u7528\u4e86\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u7684\u7ed3\u679c\u4e0d\u4ec5\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u800c\u4e14\u8bc1\u660e\u4e86\u5176\u5728\u8d44\u6e90\u8d2b\u4e4f\u7684\u533b\u7597\u4fdd\u5065\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u9002\u7528\u6027\u3002|[2402.10454v1](http://arxiv.org/pdf/2402.10454v1)|null|\n"}, "Nerf": {}, "3DGS": {}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272/...": {"2402.10887": "|**2024-02-16**|**Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation**|Weak-Mamba-UNet\uff1aVisual Mamba \u4f7f CNN \u548c ViT \u66f4\u597d\u5730\u7528\u4e8e\u57fa\u4e8e Scribble \u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272|Ziyang Wang, Chao Ma|Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.|\u533b\u5b66\u56fe\u50cf\u5206\u5272\u8d8a\u6765\u8d8a\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u4f46\u826f\u597d\u7684\u6027\u80fd\u5f80\u5f80\u4f34\u968f\u7740\u9ad8\u6602\u7684\u6ce8\u91ca\u6210\u672c\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Weak-Mamba-UNet\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5f31\u76d1\u7763\u5b66\u4e60 (WSL) \u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3001\u89c6\u89c9\u53d8\u6362\u5668 (ViT) \u548c\u7528\u4e8e\u533b\u7597\u9886\u57df\u7684\u5c16\u7aef Visual Mamba (VMamba) \u67b6\u6784\u7684\u529f\u80fd\u56fe\u50cf\u5206\u5272\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u57fa\u4e8e\u6d82\u9e26\u7684\u6ce8\u91ca\u65f6\u3002\u6240\u63d0\u51fa\u7684 WSL \u7b56\u7565\u878d\u5408\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\u4f46\u76f8\u540c\u7684\u5bf9\u79f0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff1a\u7528\u4e8e\u8be6\u7ec6\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u7684\u57fa\u4e8e CNN \u7684 UNet\u3001\u7528\u4e8e\u5168\u9762\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u57fa\u4e8e Swin Transformer \u7684 SwinUNet \u4ee5\u53ca\u7528\u4e8e\u9ad8\u6548\u957f\u65f6\u957f\u7684\u57fa\u4e8e VMamba \u7684 Mamba-UNet\u3002 -\u8303\u56f4\u4f9d\u8d56\u5efa\u6a21\u3002\u8be5\u6846\u67b6\u7684\u5173\u952e\u6982\u5ff5\u662f\u4e00\u79cd\u534f\u4f5c\u548c\u4ea4\u53c9\u76d1\u7763\u673a\u5236\uff0c\u5b83\u91c7\u7528\u4f2a\u6807\u7b7e\u6765\u4fc3\u8fdb\u8de8\u7f51\u7edc\u7684\u8fed\u4ee3\u5b66\u4e60\u548c\u7ec6\u5316\u3002 Weak-Mamba-UNet \u7684\u6709\u6548\u6027\u5728\u516c\u5f00\u7684 MRI \u5fc3\u810f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8be5\u6570\u636e\u96c6\u5177\u6709\u7ecf\u8fc7\u5904\u7406\u7684\u6d82\u9e26\u6ce8\u91ca\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u4ec5\u4f7f\u7528 UNet \u6216 SwinUNet \u7684\u7c7b\u4f3c WSL \u6846\u67b6\u7684\u6027\u80fd\u3002\u8fd9\u51f8\u663e\u4e86\u5b83\u5728\u6ce8\u91ca\u7a00\u758f\u6216\u4e0d\u7cbe\u786e\u7684\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002\u6e90\u4ee3\u7801\u53ef\u4f9b\u516c\u5f00\u8bbf\u95ee\u3002|[2402.10887v1](http://arxiv.org/pdf/2402.10887v1)|null|\n", "2402.10851": "|**2024-02-16**|**HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images**|HistoSegCap\uff1a\u7528\u4e8e\u6574\u4e2a\u5e7b\u706f\u7247\u56fe\u50cf\u4e2d\u7ec4\u7ec7\u5b66\u7ec4\u7ec7\u7c7b\u578b\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u80f6\u56ca|Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis|Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.|\u6570\u5b57\u75c5\u7406\u5b66\u6d89\u53ca\u5c06\u7269\u7406\u7ec4\u7ec7\u5207\u7247\u8f6c\u6362\u4e3a\u9ad8\u5206\u8fa8\u7387\u7684\u5168\u5207\u7247\u56fe\u50cf (WSI)\uff0c\u75c5\u7406\u5b66\u5bb6\u53ef\u4ee5\u5206\u6790\u53d7\u75be\u75c5\u5f71\u54cd\u7684\u7ec4\u7ec7\u3002\u7136\u800c\uff0c\u5177\u6709\u5927\u91cf\u5fae\u89c2\u89c6\u91ce\u7684\u5927\u578b\u7ec4\u7ec7\u5b66\u8f7d\u73bb\u7247\u7ed9\u89c6\u89c9\u641c\u7d22\u5e26\u6765\u4e86\u6311\u6218\u3002\u4e3a\u4e86\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\uff0c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad (CAD) \u7cfb\u7edf\u63d0\u4f9b\u89c6\u89c9\u8f85\u52a9\uff0c\u4ee5\u6709\u6548\u68c0\u67e5 WSI \u5e76\u8bc6\u522b\u8bca\u65ad\u76f8\u5173\u533a\u57df\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80f6\u56ca\u7f51\u7edc\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08WSSS\uff09\u7684\u65b0\u578b\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\uff0c\u8fd9\u662f\u6b64\u7c7b\u5e94\u7528\u4e2d\u7684\u7b2c\u4e00\u4e2a\u3002\u4f7f\u7528\u6570\u5b57\u75c5\u7406\u5b66\u56fe\u96c6\uff08ADP\uff09\u6570\u636e\u96c6\u5bf9\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5c06\u5176\u6027\u80fd\u4e0e\u5176\u4ed6\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u80f6\u56ca\u7f51\u7edc\u5728\u63d0\u9ad8\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u5e73\u5747\u4ea4\u5e76\uff08mIoU\uff09\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002|[2402.10851v1](http://arxiv.org/pdf/2402.10851v1)|null|\n", "2402.10847": "|**2024-02-16**|**Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning**|\u589e\u5f3a\u9a71\u52a8\u7684\u9c81\u68d2\u6307\u7eb9\u8868\u793a\u5b66\u4e60\u9884\u8bad\u7ec3|Ekta Gavas, Kaustubh Olpadkar, Anoop Namboodiri|Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.|\u6307\u7eb9\u8bc6\u522b\u662f\u751f\u7269\u8bc6\u522b\u6280\u672f\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5177\u6709\u4ece\u8eab\u4efd\u9a8c\u8bc1\u5230\u9ad8\u7ea7\u641c\u7d22\u5de5\u5177\u7684\u591a\u79cd\u5e94\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u589e\u5f3a\u7684\u9884\u8bad\u7ec3\u6765\u5bfc\u51fa\u9c81\u68d2\u7684\u6307\u7eb9\u8868\u793a\u3002\u57fa\u4e8e\u57fa\u4e8e U-Net \u7684\u6307\u7eb9\u589e\u5f3a\u7684\u6210\u5c31\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e13\u95e8\u7684\u7f16\u7801\u5668\u4ee5\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u4ece\u6307\u7eb9\u56fe\u50cf\u4e2d\u83b7\u53d6\u8868\u793a\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\uff0c\u65e8\u5728\u589e\u5f3a\u9a8c\u8bc1\u80fd\u529b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u5728\u516c\u5f00\u7684\u6307\u7eb9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8868\u660e\u4e0e\u5df2\u5efa\u7acb\u7684\u81ea\u6211\u76d1\u7763\u8bad\u7ec3\u6280\u672f\u76f8\u6bd4\uff0c\u9a8c\u8bc1\u6027\u80fd\u6709\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u5f3a\u8c03\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u800c\u4e14\u4e3a\u6f5c\u5728\u7684\u8fdb\u6b65\u94fa\u5e73\u4e86\u9053\u8def\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e0d\u4f9d\u8d56\u589e\u5f3a\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u4ece\u9000\u5316\u56fe\u50cf\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6307\u7eb9\u8868\u793a\u662f\u53ef\u884c\u7684\u3002|[2402.10847v1](http://arxiv.org/pdf/2402.10847v1)|null|\n", "2402.10821": "|**2024-02-16**|**Training Class-Imbalanced Diffusion Model Via Overlap Optimization**|\u901a\u8fc7\u91cd\u53e0\u4f18\u5316\u8bad\u7ec3\u7c7b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b|Divin Yan, Lu Qi, Vincent Tao Hu, Ming-Hsuan Yang, Meng Tang|Diffusion models have made significant advances recently in high-quality image synthesis and related tasks. However, diffusion models trained on real-world datasets, which often follow long-tailed distributions, yield inferior fidelity for tail classes. Deep generative models, including diffusion models, are biased towards classes with abundant training images. To address the observed appearance overlap between synthesized images of rare classes and tail classes, we propose a method based on contrastive learning to minimize the overlap between distributions of synthetic images for different classes. We show variants of our probabilistic contrastive learning method can be applied to any class conditional diffusion model. We show significant improvement in image synthesis using our loss for multiple datasets with long-tailed distribution. Extensive experimental results demonstrate that the proposed method can effectively handle imbalanced data for diffusion-based generation and classification models. Our code and datasets will be publicly available at https://github.com/yanliang3612/DiffROP.|\u6269\u6563\u6a21\u578b\u6700\u8fd1\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u548c\u76f8\u5173\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u9075\u5faa\u957f\u5c3e\u5206\u5e03\uff0c\u5c3e\u90e8\u7c7b\u522b\u7684\u4fdd\u771f\u5ea6\u8f83\u5dee\u3002\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec\u6269\u6563\u6a21\u578b\uff0c\u504f\u5411\u4e8e\u5177\u6709\u4e30\u5bcc\u8bad\u7ec3\u56fe\u50cf\u7684\u7c7b\u3002\u4e3a\u4e86\u89e3\u51b3\u7a00\u6709\u7c7b\u522b\u548c\u5c3e\u90e8\u7c7b\u522b\u7684\u5408\u6210\u56fe\u50cf\u4e4b\u95f4\u89c2\u5bdf\u5230\u7684\u5916\u89c2\u91cd\u53e0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u4e0d\u540c\u7c7b\u522b\u7684\u5408\u6210\u56fe\u50cf\u5206\u5e03\u4e4b\u95f4\u7684\u91cd\u53e0\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6982\u7387\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u53d8\u4f53\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u7c7b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u4f7f\u7528\u957f\u5c3e\u5206\u5e03\u7684\u591a\u4e2a\u6570\u636e\u96c6\u7684\u635f\u5931\u6765\u663e\u793a\u56fe\u50cf\u5408\u6210\u7684\u663e\u7740\u6539\u8fdb\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u548c\u5206\u7c7b\u6a21\u578b\u7684\u4e0d\u5e73\u8861\u6570\u636e\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 https://github.com/yanliang3612/DiffROP \u516c\u5f00\u63d0\u4f9b\u3002|[2402.10821v1](http://arxiv.org/pdf/2402.10821v1)|null|\n", "2402.10776": "|**2024-02-16**|**In-Vivo Hyperspectral Human Brain Image Database for Brain Cancer Detection**|\u7528\u4e8e\u8111\u764c\u68c0\u6d4b\u7684\u4f53\u5185\u9ad8\u5149\u8c31\u4eba\u8111\u56fe\u50cf\u6570\u636e\u5e93|H. Fabelo, S. Ortega, A. Szolna, D. Bulters, J. F. Pineiro, S. Kabwama, A. Shanahan, H. Bulstrode, S. Bisshopp, B. R. Kiran, et.al.|The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.|\u8fd1\u5e74\u6765\uff0c\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\u3002\u7814\u7a76\u4eba\u5458\u5728\u5f00\u53d1\u7528\u4e8e\u533b\u7597\u5e94\u7528\u7684\u9ad8\u5149\u8c31\u7b97\u6cd5\u65f6\u53d1\u73b0\u7684\u4e3b\u8981\u969c\u788d\u4e4b\u4e00\u662f\u7f3a\u4e4f\u7279\u5b9a\u7684\u3001\u516c\u5f00\u7684\u9ad8\u5149\u8c31\u533b\u5b66\u6570\u636e\u3002\u672c\u6587\u63cf\u8ff0\u7684\u5de5\u4f5c\u662f\u5728\u6b27\u6d32\u9879\u76ee HELICoiD\uff08\u9ad8\u5149\u8c31\u6210\u50cf\u764c\u75c7\u68c0\u6d4b\uff09\u7684\u6846\u67b6\u5185\u5f00\u53d1\u7684\uff0c\u8be5\u9879\u76ee\u7684\u4e3b\u8981\u76ee\u6807\u662f\u5e94\u7528\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u795e\u7ecf\u5916\u79d1\u624b\u672f\u671f\u95f4\u5b9e\u65f6\u63cf\u7ed8\u8111\u80bf\u7624\u3002\u5728\u672c\u6587\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u751f\u6210\u7b2c\u4e00\u4e2a\u4f53\u5185\u4eba\u8111\u7ec4\u7ec7\u9ad8\u5149\u8c31\u6570\u636e\u5e93\u6240\u9075\u5faa\u7684\u65b9\u6cd5\u3002\u4f7f\u7528\u5b9a\u5236\u7684\u9ad8\u5149\u8c31\u91c7\u96c6\u7cfb\u7edf\u91c7\u96c6\u6570\u636e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6355\u83b7 400 \u81f3 1000 nm \u7684\u53ef\u89c1\u5149\u548c\u8fd1\u7ea2\u5916 (VNIR) \u8303\u56f4\u5185\u7684\u4fe1\u606f\u3002\u5bf9\u8fde\u7eed\u6355\u83b7\u540c\u4e00\u573a\u666f\u7684\u4e24\u4e2a\u56fe\u50cf\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u53ef\u91cd\u590d\u6027\u8bc4\u4f30\u3002\u5206\u6790\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728 450 \u81f3 900 nm \u7684\u5149\u8c31\u8303\u56f4\u5185\u5de5\u4f5c\u6548\u7387\u66f4\u9ad8\u3002\u603b\u5171\u83b7\u5f97\u4e86 22 \u540d\u4e0d\u540c\u60a3\u8005\u7684 36 \u5f20\u9ad8\u5149\u8c31\u56fe\u50cf\u3002\u6839\u636e\u8fd9\u4e9b\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u5149\u8c31\u89d2\u5ea6\u6620\u5c04\u5668\u7b97\u6cd5\u7684\u534a\u81ea\u52a8\u65b9\u6cd5\u6807\u8bb0\u4e86\u8d85\u8fc7 300 000 \u4e2a\u5149\u8c31\u7279\u5f81\u3002\u5b9a\u4e49\u4e86\u56db\u4e2a\u4e0d\u540c\u7684\u7c7b\u522b\uff1a\u6b63\u5e38\u7ec4\u7ec7\u3001\u80bf\u7624\u7ec4\u7ec7\u3001\u8840\u7ba1\u548c\u80cc\u666f\u5143\u7d20\u3002\u6240\u6709\u9ad8\u5149\u8c31\u6570\u636e\u5747\u5df2\u5728\u516c\u5171\u5b58\u50a8\u5e93\u4e2d\u63d0\u4f9b\u3002|[2402.10776v1](http://arxiv.org/pdf/2402.10776v1)|null|\n", "2402.10752": "|**2024-02-16**|**STF: Spatio-Temporal Fusion Module for Improving Video Object Detection**|STF\uff1a\u7528\u4e8e\u6539\u8fdb\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\u7684\u65f6\u7a7a\u878d\u5408\u6a21\u5757|Noreen Anwar, Guillaume-Alexandre Bilodeau, Wassim Bouachir|Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task. The objective of our work is to leverage this complementary information to improve detection. Therefore, we propose a spatio-temporal fusion framework (STF). We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust object representations. Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them. Our evaluation is conducted on three different benchmarks including video sequences of moving road users. The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline object detectors. Code is available at https://github.com/noreenanwar/STF-module|\u89c6\u9891\u4e2d\u7684\u8fde\u7eed\u5e27\u5305\u542b\u5197\u4f59\uff0c\u4f46\u5b83\u4eec\u4e5f\u53ef\u80fd\u5305\u542b\u68c0\u6d4b\u4efb\u52a1\u7684\u76f8\u5173\u8865\u5145\u4fe1\u606f\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u76ee\u6807\u662f\u5229\u7528\u8fd9\u4e9b\u8865\u5145\u4fe1\u606f\u6765\u6539\u8fdb\u68c0\u6d4b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u7a7a\u878d\u5408\u6846\u67b6\uff08STF\uff09\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u591a\u5e27\u548c\u5355\u5e27\u6ce8\u610f\u6a21\u5757\uff0c\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5728\u9644\u8fd1\u5e27\u4e4b\u95f4\u5171\u4eab\u7279\u5f81\u56fe\u4ee5\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u5bf9\u8c61\u8868\u793a\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u53cc\u5e27\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4ee5\u53ef\u5b66\u4e60\u7684\u65b9\u5f0f\u5408\u5e76\u7279\u5f81\u56fe\u4ee5\u6539\u8fdb\u5b83\u4eec\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u662f\u6839\u636e\u4e09\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u8fdb\u884c\u7684\uff0c\u5305\u62ec\u79fb\u52a8\u9053\u8def\u4f7f\u7528\u8005\u7684\u89c6\u9891\u5e8f\u5217\u3002\u6240\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76ee\u6807\u68c0\u6d4b\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65f6\u7a7a\u878d\u5408\u6a21\u5757\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/norenanwar/STF-module \u83b7\u53d6|[2402.10752v1](http://arxiv.org/pdf/2402.10752v1)|null|\n", "2402.10728": "|**2024-02-16**|**Semi-weakly-supervised neural network training for medical image registration**|\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u534a\u5f31\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3|Yiwen Li, Yunguan Fu, Iani J. M. B. Gayo, Qianye Yang, Zhe Min, Shaheer U. Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, et.al.|For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.|\u5bf9\u4e8e\u8bad\u7ec3\u914d\u51c6\u7f51\u7edc\uff0c\u6765\u81ea\u5206\u6bb5\u7684\u76f8\u5e94\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u7684\u5f31\u76d1\u7763\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\uff08a\uff09\u8865\u5145\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4ee5\u53ca\uff08b\uff09\u5728\u65e0\u76d1\u7763\u635f\u5931\u4e0d\u53ef\u7528\u6216\u65e0\u6548\u7684\u914d\u51c6\u4efb\u52a1\u4e2d\u72ec\u7acb\u4f7f\u7528\u3002\u8fd9\u79cd\u901a\u4fe1\u901a\u77e5\u76d1\u7763\u9700\u8981\u6ce8\u91ca\u6210\u672c\uff0c\u9700\u8981\u5927\u91cf\u7684\u4e13\u95e8\u5de5\u4f5c\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u79cd\u534a\u5f31\u76d1\u7763\u7684\u914d\u51c6\u7ba1\u9053\uff0c\u5f53\u53ea\u6709\u4e00\u4e2a\u5c0f\u7684\u76f8\u5e94 ROI \u6807\u8bb0\u6570\u636e\u96c6\u53ef\u7528\u65f6\uff0c\u8be5\u7ba1\u9053\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u5bf9\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u7f51\u7edc\u6743\u91cd\u548c\u56fe\u50cf\u91cd\u91c7\u6837\u7684\u6270\u52a8\u6765\u68c0\u67e5\u4e24\u79cd\u7c7b\u578b\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5c06\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u65e0\u76d1\u7763\u635f\u5931\u5e94\u7528\u4e8e\u672a\u6807\u8bb0\u7684\u6570\u636e\u3002\u63d0\u51fa\u4e86\u65b0\u9896\u7684 WarpDDF \u548c RegCut \u65b9\u6cd5\uff0c\u4ee5\u5141\u8bb8\u56fe\u50cf\u5bf9\u548c\u9884\u6d4b\u7684\u7a7a\u95f4\u53d8\u6362\uff08\u5373\u914d\u51c6\u7f51\u7edc\u7684\u76f8\u5e94\u8f93\u5165\u548c\u8f93\u51fa\uff09\u4e4b\u95f4\u8fdb\u884c\u4ea4\u6362\u6270\u52a8\uff0c\u8fd9\u4e0e\u73b0\u6709\u7684\u5206\u7c7b\u6216\u5206\u5272\u6270\u52a8\u65b9\u6cd5\u4e0d\u540c\u3002\u4f7f\u7528 589 \u4e2a\u7537\u6027\u9aa8\u76c6 MR \u56fe\u50cf\uff08\u6807\u6709\u516b\u4e2a\u89e3\u5256 ROI\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\u4e86\u914d\u51c6\u6027\u80fd\u7684\u6539\u8fdb\u4ee5\u53ca\u5404\u4e2a\u7b56\u7565\u7684\u6d88\u878d\u8d21\u732e\u3002\u6b64\u5916\uff0c\u672c\u7814\u7a76\u5c1d\u8bd5\u6784\u5efa\u7b2c\u4e00\u4e2a\u9aa8\u76c6\u7ed3\u6784\u8ba1\u7b97\u56fe\u96c6\uff0c\u901a\u8fc7\u6ce8\u518c\u53d7\u8bd5\u8005\u95f4 MR \u6765\u5b9e\u73b0\uff0c\u5e76\u91cf\u5316\u7531\u4e8e\u6240\u63d0\u51fa\u7684\u534a\u5f31\u76d1\u7763\u800c\u4ea7\u751f\u7684\u663e\u7740\u5dee\u5f02\uff0c\u5e76\u8ba8\u8bba\u793a\u4f8b\u56fe\u96c6\u7684\u6f5c\u5728\u4e34\u5e8a\u7528\u9014\u3002\u5f97\u51fa\u7684\u7edf\u8ba1\u6570\u636e\u3002|[2402.10728v1](http://arxiv.org/pdf/2402.10728v1)|null|\n", "2402.10665": "|**2024-02-16**|**Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift**|\u4f7f\u7528\u4e8b\u540e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u8bed\u4e49\u5206\u5272\u9009\u62e9\u6027\u9884\u6d4b\u53ca\u5176\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd|Bruno Laboissiere Camargos Borges, Bruno Machado Pacheco, Danilo Silva|Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through experiments on three medical imaging tasks. Our findings show that post-hoc confidence estimators offer a cost-effective approach to reducing the impacts of distribution shift.|\u8bed\u4e49\u5206\u5272\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5176\u529f\u6548\u5f80\u5f80\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u800c\u53d7\u5230\u963b\u788d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u4e00\u4e2a\u5e38\u89c1\u7684\u7b56\u7565\u662f\u5229\u7528\u6839\u636e\u4e0d\u540c\u4eba\u7fa4\u7684\u6570\u636e\uff08\u4f8b\u5982\u516c\u5f00\u6570\u636e\u96c6\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u5206\u5e03\u8f6c\u79fb\u95ee\u9898\uff0c\u4ece\u800c\u5bfc\u81f4\u611f\u5174\u8da3\u7fa4\u4f53\u7684\u6027\u80fd\u4e0b\u964d\u3002\u5728\u6a21\u578b\u9519\u8bef\u53ef\u80fd\u4ea7\u751f\u91cd\u5927\u540e\u679c\u7684\u60c5\u51b5\u4e0b\uff0c\u9009\u62e9\u6027\u9884\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u51cf\u8f7b\u98ce\u9669\u5e76\u51cf\u5c11\u5bf9\u4e13\u5bb6\u76d1\u7763\u7684\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002\u672c\u6587\u7814\u7a76\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u8bed\u4e49\u5206\u5272\u7684\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u4ece\u800c\u91cd\u70b9\u5173\u6ce8\u5e94\u7528\u4e8e\u5206\u5e03\u8f6c\u79fb\u4e0b\u8fd0\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e8b\u540e\u7f6e\u4fe1\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u8bed\u4e49\u5206\u5272\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b\u56fe\u50cf\u7ea7\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e8b\u540e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u5206\u5e03\u8f6c\u79fb\u7684\u5f71\u54cd\u3002|[2402.10665v1](http://arxiv.org/pdf/2402.10665v1)|null|\n", "2402.10595": "|**2024-02-16**|**Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification**|\u7528\u4e8e\u5168\u5e7b\u706f\u7247\u56fe\u50cf\u5206\u7c7b\u591a\u5b9e\u4f8b\u5b66\u4e60\u7684\u7d27\u51d1\u4e14\u53bb\u504f\u7684\u8d1f\u5b9e\u4f8b\u5d4c\u5165|Joohyung Lee, Heejeong Nam, Kwanhyung Lee, Sangchul Hahn|Whole-slide image (WSI) classification is a challenging task because 1) patches from WSI lack annotation, and 2) WSI possesses unnecessary variability, e.g., stain protocol. Recently, Multiple-Instance Learning (MIL) has made significant progress, allowing for classification based on slide-level, rather than patch-level, annotations. However, existing MIL methods ignore that all patches from normal slides are normal. Using this free annotation, we introduce a semi-supervision signal to de-bias the inter-slide variability and to capture the common factors of variation within normal patches. Because our method is orthogonal to the MIL algorithm, we evaluate our method on top of the recently proposed MIL algorithms and also compare the performance with other semi-supervised approaches. We evaluate our method on two public WSI datasets including Camelyon-16 and TCGA lung cancer and demonstrate that our approach significantly improves the predictive performance of existing MIL algorithms and outperforms other semi-supervised algorithms. We release our code at https://github.com/AITRICS/pathology_mil.|\u5168\u73bb\u7247\u56fe\u50cf (WSI) \u5206\u7c7b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a 1) WSI \u7684\u8865\u4e01\u7f3a\u4e4f\u6ce8\u91ca\uff0c2) WSI \u5177\u6709\u4e0d\u5fc5\u8981\u7684\u53ef\u53d8\u6027\uff0c\u4f8b\u5982\u67d3\u8272\u534f\u8bae\u3002\u6700\u8fd1\uff0c\u591a\u5b9e\u4f8b\u5b66\u4e60 (MIL) \u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5141\u8bb8\u57fa\u4e8e\u5e7b\u706f\u7247\u7ea7\u522b\u800c\u4e0d\u662f\u8865\u4e01\u7ea7\u522b\u6ce8\u91ca\u8fdb\u884c\u5206\u7c7b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 MIL \u65b9\u6cd5\u5ffd\u7565\u4e86\u6b63\u5e38\u5e7b\u706f\u7247\u7684\u6240\u6709\u8865\u4e01\u90fd\u662f\u6b63\u5e38\u7684\u3002\u4f7f\u7528\u8fd9\u4e2a\u514d\u8d39\u6ce8\u91ca\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u534a\u76d1\u7763\u4fe1\u53f7\u6765\u6d88\u9664\u8f7d\u73bb\u7247\u95f4\u53d8\u5f02\u6027\u7684\u504f\u5dee\u5e76\u6355\u83b7\u6b63\u5e38\u6591\u5757\u5185\u53d8\u5f02\u7684\u5e38\u89c1\u56e0\u7d20\u3002\u7531\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e MIL \u7b97\u6cd5\u6b63\u4ea4\uff0c\u56e0\u6b64\u6211\u4eec\u5728\u6700\u8fd1\u63d0\u51fa\u7684 MIL \u7b97\u6cd5\u7684\u57fa\u7840\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u6027\u80fd\u4e0e\u5176\u4ed6\u534a\u76d1\u7763\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u5728\u5305\u62ec Camelyon-16 \u548c TCGA \u80ba\u764c\u5728\u5185\u7684\u4e24\u4e2a\u516c\u5171 WSI \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u73b0\u6709 MIL \u7b97\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u534a\u76d1\u7763\u7b97\u6cd5\u3002\u6211\u4eec\u5728 https://github.com/AITRICS/pathology_mil \u53d1\u5e03\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u3002|[2402.10595v1](http://arxiv.org/pdf/2402.10595v1)|null|\n", "2402.10520": "|**2024-02-16**|**Real-Time Model-Based Quantitative Ultrasound and Radar**|\u57fa\u4e8e\u5b9e\u65f6\u6a21\u578b\u7684\u5b9a\u91cf\u8d85\u58f0\u548c\u96f7\u8fbe|Tom Sharon, Yonina C. Eldar|Ultrasound and radar signals are highly beneficial for medical imaging as they are non-invasive and non-ionizing. Traditional imaging techniques have limitations in terms of contrast and physical interpretation. Quantitative medical imaging can display various physical properties such as speed of sound, density, conductivity, and relative permittivity. This makes it useful for a wider range of applications, including improving cancer detection, diagnosing fatty liver, and fast stroke imaging. However, current quantitative imaging techniques that estimate physical properties from received signals, such as Full Waveform Inversion, are time-consuming and tend to converge to local minima, making them unsuitable for medical imaging. To address these challenges, we propose a neural network based on the physical model of wave propagation, which defines the relationship between the received signals and physical properties. Our network can reconstruct multiple physical properties in less than one second for complex and realistic scenarios, using data from only eight elements. We demonstrate the effectiveness of our approach for both radar and ultrasound signals.|\u8d85\u58f0\u6ce2\u548c\u96f7\u8fbe\u4fe1\u53f7\u5bf9\u4e8e\u533b\u5b66\u6210\u50cf\u975e\u5e38\u6709\u76ca\uff0c\u56e0\u4e3a\u5b83\u4eec\u662f\u975e\u4fb5\u5165\u6027\u548c\u975e\u7535\u79bb\u7684\u3002\u4f20\u7edf\u6210\u50cf\u6280\u672f\u5728\u5bf9\u6bd4\u5ea6\u548c\u7269\u7406\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u5b9a\u91cf\u533b\u5b66\u6210\u50cf\u53ef\u4ee5\u663e\u793a\u5404\u79cd\u7269\u7406\u7279\u6027\uff0c\u4f8b\u5982\u58f0\u901f\u3001\u5bc6\u5ea6\u3001\u7535\u5bfc\u7387\u548c\u76f8\u5bf9\u4ecb\u7535\u5e38\u6570\u3002\u8fd9\u4f7f\u5f97\u5b83\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6539\u5584\u764c\u75c7\u68c0\u6d4b\u3001\u8bca\u65ad\u8102\u80aa\u809d\u548c\u5feb\u901f\u4e2d\u98ce\u6210\u50cf\u3002\u7136\u800c\uff0c\u5f53\u524d\u4ece\u63a5\u6536\u4fe1\u53f7\u4f30\u8ba1\u7269\u7406\u7279\u6027\u7684\u5b9a\u91cf\u6210\u50cf\u6280\u672f\uff08\u4f8b\u5982\u5168\u6ce2\u5f62\u53cd\u6f14\uff09\u975e\u5e38\u8017\u65f6\uff0c\u5e76\u4e14\u5f80\u5f80\u4f1a\u6536\u655b\u5230\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u5408\u533b\u5b66\u6210\u50cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce2\u4f20\u64ad\u7269\u7406\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5b9a\u4e49\u4e86\u63a5\u6536\u4fe1\u53f7\u548c\u7269\u7406\u7279\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6211\u4eec\u7684\u7f51\u7edc\u53ef\u4ee5\u4ec5\u4f7f\u7528\u516b\u4e2a\u5143\u7d20\u7684\u6570\u636e\uff0c\u5728\u4e0d\u5230\u4e00\u79d2\u7684\u65f6\u95f4\u5185\u9488\u5bf9\u590d\u6742\u548c\u73b0\u5b9e\u7684\u573a\u666f\u91cd\u5efa\u591a\u4e2a\u7269\u7406\u5c5e\u6027\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u96f7\u8fbe\u548c\u8d85\u58f0\u6ce2\u4fe1\u53f7\u7684\u6709\u6548\u6027\u3002|[2402.10520v1](http://arxiv.org/pdf/2402.10520v1)|null|\n", "2402.10478": "|**2024-02-16**|**CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes**|CodaMal\uff1a\u4f4e\u6210\u672c\u663e\u5fae\u955c\u4e2d\u759f\u75be\u68c0\u6d4b\u7684\u5bf9\u6bd4\u57df\u9002\u5e94|Ishan Rajendrakumar Dave, Tristan de Blegiers, Chen Chen, Mubarak Shah|Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adaptive contrastive loss. It reduces the domain shift by promoting similarity between the representations of HCM and its corresponding LCM image, without imposing an additional annotation burden. In addition, the training objective includes object detection objectives with carefully designed augmentations, ensuring the accurate detection of malaria parasites. On the publicly available large-scale M5-dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides 21x speed up during inference, and requires only half learnable parameters than the prior methods. Our code is publicly available.|\u759f\u75be\u662f\u4e16\u754c\u8303\u56f4\u5185\u7684\u4e00\u4e2a\u4e3b\u8981\u5065\u5eb7\u95ee\u9898\uff0c\u5176\u8bca\u65ad\u9700\u8981\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4e0e\u4f4e\u6210\u672c\u663e\u5fae\u955c (LCM) \u6709\u6548\u914d\u5408\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u663e\u5fae\u56fe\u50cf\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5e26\u6ce8\u91ca\u7684\u56fe\u50cf\u6765\u663e\u793a\u53d7\u759f\u75be\u5bc4\u751f\u866b\u5f71\u54cd\u7684\u7ec6\u80de\u53ca\u5176\u751f\u547d\u9636\u6bb5\u3002\u4e0e\u6ce8\u91ca\u6765\u81ea\u9ad8\u6210\u672c\u663e\u5fae\u955c (HCM) \u7684\u56fe\u50cf\u76f8\u6bd4\uff0c\u6ce8\u91ca\u6765\u81ea LCM \u7684\u56fe\u50cf\u663e\u7740\u589e\u52a0\u4e86\u533b\u5b66\u4e13\u5bb6\u7684\u8d1f\u62c5\u3002\u56e0\u6b64\uff0c\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u5c06\u5728 HCM \u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5e94\u8be5\u80fd\u591f\u5f88\u597d\u5730\u63a8\u5e7f\u5230 LCM \u56fe\u50cf\u4e0a\u3002\u867d\u7136\u65e9\u671f\u7684\u65b9\u6cd5\u91c7\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f46\u5b83\u4eec\u6ca1\u6709\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u65b9\u6cd5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\uff0c\u540d\u4e3a CodaMal\uff08\u759f\u75be\u5bf9\u6bd4\u57df\u9002\u5e94\uff09\u3002\u4e3a\u4e86\u5f25\u5408 HCM\uff08\u8bad\u7ec3\uff09\u548c LCM\uff08\u6d4b\u8bd5\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57df\u81ea\u9002\u5e94\u5bf9\u6bd4\u635f\u5931\u3002\u5b83\u901a\u8fc7\u4fc3\u8fdb HCM \u8868\u793a\u53ca\u5176\u76f8\u5e94\u7684 LCM \u56fe\u50cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u51cf\u5c11\u57df\u504f\u79fb\uff0c\u800c\u4e0d\u4f1a\u9020\u6210\u989d\u5916\u7684\u6ce8\u91ca\u8d1f\u62c5\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u76ee\u6807\u5305\u62ec\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u589e\u5f3a\u529f\u80fd\u7684\u7269\u4f53\u68c0\u6d4b\u76ee\u6807\uff0c\u786e\u4fdd\u51c6\u786e\u68c0\u6d4b\u759f\u75be\u5bc4\u751f\u866b\u3002\u5728\u516c\u5f00\u7684\u5927\u89c4\u6a21 M5 \u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u6307\u6807 (mAP) \u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86 16%\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86 21 \u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u4e4b\u524d\u65b9\u6cd5\u4e00\u534a\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u3002\u6211\u4eec\u7684\u4ee3\u7801\u662f\u516c\u5f00\u7684\u3002|[2402.10478v1](http://arxiv.org/pdf/2402.10478v1)|null|\n", "2402.10476": "|**2024-02-16**|**Spike-EVPR: Deep Spiking Residual Network with Cross-Representation Aggregation for Event-Based Visual Place Recognition**|Spike-EVPR\uff1a\u5177\u6709\u4ea4\u53c9\u8868\u793a\u805a\u5408\u7684\u6df1\u5ea6\u5c16\u5cf0\u6b8b\u5dee\u7f51\u7edc\uff0c\u7528\u4e8e\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b|Chenming Hu, Zheng Fang, Kuanxu Hou, Delei Kong, Junjie Jiang, Hao Zhuang, Mingyuan Sun, Xinjie Huang|Event cameras have been successfully applied to visual place recognition (VPR) tasks by using deep artificial neural networks (ANNs) in recent years. However, previously proposed deep ANN architectures are often unable to harness the abundant temporal information presented in event streams. In contrast, deep spiking networks exhibit more intricate spatiotemporal dynamics and are inherently well-suited to process sparse asynchronous event streams. Unfortunately, directly inputting temporal-dense event volumes into the spiking network introduces excessive time steps, resulting in prohibitively high training costs for large-scale VPR tasks. To address the aforementioned issues, we propose a novel deep spiking network architecture called Spike-EVPR for event-based VPR tasks. First, we introduce two novel event representations tailored for SNN to fully exploit the spatio-temporal information from the event streams, and reduce the video memory occupation during training as much as possible. Then, to exploit the full potential of these two representations, we construct a Bifurcated Spike Residual Encoder (BSR-Encoder) with powerful representational capabilities to better extract the high-level features from the two event representations. Next, we introduce a Shared & Specific Descriptor Extractor (SSD-Extractor). This module is designed to extract features shared between the two representations and features specific to each. Finally, we propose a Cross-Descriptor Aggregation Module (CDA-Module) that fuses the above three features to generate a refined, robust global descriptor of the scene. Our experimental results indicate the superior performance of our Spike-EVPR compared to several existing EVPR pipelines on Brisbane-Event-VPR and DDD20 datasets, with the average Recall@1 increased by 7.61% on Brisbane and 13.20% on DDD20.|\u8fd1\u5e74\u6765\uff0c\u4e8b\u4ef6\u6444\u50cf\u673a\u5df2\u901a\u8fc7\u4f7f\u7528\u6df1\u5ea6\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u6210\u529f\u5e94\u7528\u4e8e\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5148\u524d\u63d0\u51fa\u7684\u6df1\u5ea6\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u901a\u5e38\u65e0\u6cd5\u5229\u7528\u4e8b\u4ef6\u6d41\u4e2d\u5448\u73b0\u7684\u4e30\u5bcc\u65f6\u95f4\u4fe1\u606f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6df1\u5ea6\u5c16\u5cf0\u7f51\u7edc\u8868\u73b0\u51fa\u66f4\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u5e76\u4e14\u672c\u8d28\u4e0a\u975e\u5e38\u9002\u5408\u5904\u7406\u7a00\u758f\u5f02\u6b65\u4e8b\u4ef6\u6d41\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u76f4\u63a5\u5c06\u65f6\u95f4\u5bc6\u96c6\u4e8b\u4ef6\u91cf\u8f93\u5165\u5c16\u5cf0\u7f51\u7edc\u4f1a\u5f15\u5165\u8fc7\u591a\u7684\u65f6\u95f4\u6b65\u957f\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21 VPR \u4efb\u52a1\u7684\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Spike-EVPR \u7684\u65b0\u578b\u6df1\u5ea6\u5c16\u5cf0\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u57fa\u4e8e\u4e8b\u4ef6\u7684 VPR \u4efb\u52a1\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u4e13\u4e3a SNN \u5b9a\u5236\u7684\u65b0\u9896\u4e8b\u4ef6\u8868\u793a\u5f62\u5f0f\uff0c\u4ee5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6d41\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u5e76\u5c3d\u53ef\u80fd\u51cf\u5c11\u8bad\u7ec3\u671f\u95f4\u7684\u89c6\u9891\u5185\u5b58\u5360\u7528\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8fd9\u4e24\u79cd\u8868\u793a\u7684\u6f5c\u529b\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u5f3a\u5927\u8868\u793a\u80fd\u529b\u7684\u5206\u53c9\u5c16\u5cf0\u6b8b\u5dee\u7f16\u7801\u5668\uff08BSR-Encoder\uff09\uff0c\u4ee5\u66f4\u597d\u5730\u4ece\u4e24\u4e2a\u4e8b\u4ef6\u8868\u793a\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u7279\u5f81\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4ecb\u7ecd\u5171\u4eab\u548c\u7279\u5b9a\u63cf\u8ff0\u7b26\u63d0\u53d6\u5668\uff08SSD-Extractor\uff09\u3002\u8be5\u6a21\u5757\u65e8\u5728\u63d0\u53d6\u4e24\u79cd\u8868\u793a\u4e4b\u95f4\u5171\u4eab\u7684\u7279\u5f81\u4ee5\u53ca\u6bcf\u79cd\u8868\u793a\u6240\u7279\u6709\u7684\u7279\u5f81\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u63cf\u8ff0\u7b26\u805a\u5408\u6a21\u5757\uff08CDA-Module\uff09\uff0c\u5b83\u878d\u5408\u4e86\u4e0a\u8ff0\u4e09\u4e2a\u7279\u5f81\uff0c\u751f\u6210\u4e00\u4e2a\u7cbe\u81f4\u7684\u3001\u9c81\u68d2\u7684\u573a\u666f\u5168\u5c40\u63cf\u8ff0\u7b26\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e Brisbane-Event-VPR \u548c DDD20 \u6570\u636e\u96c6\u4e0a\u7684\u51e0\u4e2a\u73b0\u6709 EVPR \u7ba1\u9053\u76f8\u6bd4\uff0c\u6211\u4eec\u7684 Spike-EVPR \u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e73\u5747 Recall@1 \u5728 Brisbane \u4e0a\u589e\u52a0\u4e86 7.61%\uff0c\u5728 DDD20 \u4e0a\u589e\u52a0\u4e86 13.20%\u3002|[2402.10476v1](http://arxiv.org/pdf/2402.10476v1)|null|\n", "2402.10435": "|**2024-02-16**|**Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification**|\u7528\u4e8e\u88ab\u906e\u6321\u4eba\u5458\u91cd\u65b0\u8bc6\u522b\u7684\u52a8\u6001\u8865\u4e01\u611f\u77e5\u4e30\u5bcc\u53d8\u538b\u5668|Xin Zhang, Keren Fu, Qijun Zhao|Person re-identification (re-ID) continues to pose a significant challenge, particularly in scenarios involving occlusions. Prior approaches aimed at tackling occlusions have predominantly focused on aligning physical body features through the utilization of external semantic cues. However, these methods tend to be intricate and susceptible to noise. To address the aforementioned challenges, we present an innovative end-to-end solution known as the Dynamic Patch-aware Enrichment Transformer (DPEFormer). This model effectively distinguishes human body information from occlusions automatically and dynamically, eliminating the need for external detectors or precise image alignment. Specifically, we introduce a dynamic patch token selection module (DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify informative occlusion-free tokens. These tokens are then selected for deriving subsequent local part features. To facilitate the seamless integration of global classification features with the finely detailed local features selected by DPSM, we introduce a novel feature blending module (FBM). FBM enhances feature representation through the complementary nature of information and the exploitation of part diversity. Furthermore, to ensure that DPSM and the entire DPEFormer can effectively learn with only identity labels, we also propose a Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the recent advances in the Segment Anything Model (SAM). As a result, it generates occlusion images that closely resemble real-world occlusions, greatly enhancing the subsequent contrastive learning process. Experiments on occluded and holistic re-ID benchmarks signify a substantial advancement of DPEFormer over existing state-of-the-art approaches. The code will be made publicly available.|\u884c\u4eba\u91cd\u65b0\u8bc6\u522b (re-ID) \u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u3002\u5148\u524d\u65e8\u5728\u89e3\u51b3\u906e\u6321\u95ee\u9898\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u901a\u8fc7\u5229\u7528\u5916\u90e8\u8bed\u4e49\u7ebf\u7d22\u6765\u5bf9\u9f50\u8eab\u4f53\u7279\u5f81\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u5f88\u590d\u6742\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u79f0\u4e3a\u52a8\u6001\u8865\u4e01\u611f\u77e5\u4e30\u5bcc\u53d8\u538b\u5668\uff08DPEFormer\uff09\u3002\u8be5\u6a21\u578b\u81ea\u52a8\u52a8\u6001\u5730\u6709\u6548\u533a\u5206\u4eba\u4f53\u4fe1\u606f\u548c\u906e\u6321\uff0c\u65e0\u9700\u5916\u90e8\u63a2\u6d4b\u5668\u6216\u7cbe\u786e\u7684\u56fe\u50cf\u5bf9\u9f50\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u8865\u4e01\u4ee4\u724c\u9009\u62e9\u6a21\u5757\uff08DPSM\uff09\u3002 DPSM \u5229\u7528\u6807\u7b7e\u5f15\u5bfc\u7684\u4ee3\u7406\u4ee4\u724c\u4f5c\u4e3a\u4e2d\u4ecb\u6765\u8bc6\u522b\u4fe1\u606f\u4e30\u5bcc\u7684\u65e0\u906e\u6321\u4ee4\u724c\u3002\u7136\u540e\u9009\u62e9\u8fd9\u4e9b\u6807\u8bb0\u6765\u5bfc\u51fa\u540e\u7eed\u7684\u5c40\u90e8\u7279\u5f81\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5168\u5c40\u5206\u7c7b\u7279\u5f81\u4e0e DPSM \u9009\u62e9\u7684\u7cbe\u7ec6\u5c40\u90e8\u7279\u5f81\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u6df7\u5408\u6a21\u5757\uff08FBM\uff09\u3002 FBM \u901a\u8fc7\u4fe1\u606f\u7684\u4e92\u8865\u6027\u548c\u96f6\u4ef6\u591a\u6837\u6027\u7684\u5229\u7528\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u786e\u4fdd DPSM \u548c\u6574\u4e2a DPEFormer \u80fd\u591f\u4ec5\u4f7f\u7528\u8eab\u4efd\u6807\u7b7e\u8fdb\u884c\u6709\u6548\u5b66\u4e60\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u73b0\u5b9e\u906e\u6321\u589e\u5f3a\uff08ROA\uff09\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5229\u7528\u4e86\u5206\u6bb5\u4efb\u610f\u6a21\u578b (SAM) \u7684\u6700\u65b0\u8fdb\u5c55\u3002\u56e0\u6b64\uff0c\u5b83\u751f\u6210\u7684\u906e\u6321\u56fe\u50cf\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u906e\u6321\u975e\u5e38\u76f8\u4f3c\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u540e\u7eed\u7684\u5bf9\u6bd4\u5b66\u4e60\u8fc7\u7a0b\u3002\u5bf9\u906e\u6321\u548c\u6574\u4f53 re-ID \u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e DPEFormer \u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u6b65\u3002\u8be5\u4ee3\u7801\u5c06\u516c\u5f00\u3002|[2402.10435v1](http://arxiv.org/pdf/2402.10435v1)|null|\n", "2402.10425": "|**2024-02-16**|**DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision**|DABS-LS\uff1a\u4f7f\u7528\u533a\u57df\u6c34\u5e73\u96c6\u81ea\u6211\u76d1\u7763\u7684\u57fa\u4e8e\u6df1\u5ea6\u56fe\u96c6\u7684\u5206\u5272|Hannah G. Mason, Jack H. Noble|Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target volumes and that accurately segment the IAC. We hypothesize that DFs that accurately segment the IAC in target images will also facilitate accurate atlas-based localization of the ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately register the entire volume, our novel contribution is an entirely self-supervised training scheme that aims to produce DFs that accurately segment the target structure. This self-supervision is facilitated using a regional level set (LS) inspired loss function. We call our method Deep Atlas Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS outperforms VoxelMorph for IAC segmentation. Tests with publicly available datasets for trachea and kidney segmentation also show significant improvement in segmentation accuracy, demonstrating the generalizability of the method.|\u4eba\u5de5\u8033\u8717 (CI) \u662f\u7528\u4e8e\u6cbb\u7597\u91cd\u5ea6\u81f3\u6781\u91cd\u5ea6\u542c\u529b\u635f\u5931\u60a3\u8005\u7684\u795e\u7ecf\u4fee\u590d\u4f53\u3002\u9488\u5bf9\u7279\u5b9a\u60a3\u8005\u7684\u542c\u89c9\u795e\u7ecf\u7ea4\u7ef4 (ANF) CI \u523a\u6fc0\u5efa\u6a21\u53ef\u4ee5\u5e2e\u52a9\u542c\u529b\u5b66\u5bb6\u6539\u8fdb CI \u7f16\u7a0b\u3002\u8fd9\u4e9b\u6a21\u578b\u9700\u8981 ANF \u76f8\u5bf9\u4e8e\u5468\u56f4\u89e3\u5256\u7ed3\u6784\u548c CI \u7684\u5b9a\u4f4d\u3002\u5b9a\u4f4d\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a ANF \u975e\u5e38\u5c0f\uff0c\u5728\u4e34\u5e8a\u6210\u50cf\u4e2d\u65e0\u6cd5\u76f4\u63a5\u770b\u5230\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe ANF \u7684\u4f4d\u7f6e\u53ef\u4ee5\u4ece CT \u4e2d\u5177\u6709\u9ad8\u5bf9\u6bd4\u5ea6\u7684\u5185\u8033\u9053 (IAC) \u4f4d\u7f6e\u51c6\u786e\u63a8\u65ad\uff0c\u56e0\u4e3a ANF \u7a7f\u8fc7\u8033\u8717\u548c\u5927\u8111\u4e4b\u95f4\u7684\u8fd9\u6761\u8033\u9053\u3002\u53d7 VoxelMorph \u7684\u542f\u53d1\uff0c\u5728\u672c\u6587\u4e2d\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u56fe\u96c6\u7684 IAC \u5206\u5272\u7f51\u7edc\u3002\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5355\u72ec\u7684\u56fe\u96c6\uff0c\u5176\u4e2d IAC \u548c ANF \u5df2\u9884\u5148\u672c\u5730\u5316\u3002\u6211\u4eec\u7684\u7f51\u7edc\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u53ef\u4ee5\u751f\u6210\u5c06\u56fe\u96c6\u5750\u6807\u6620\u5c04\u5230\u65b0\u76ee\u6807\u4f53\u79ef\u7684\u53d8\u5f62\u573a (DF)\uff0c\u5e76\u51c6\u786e\u5206\u5272 IAC\u3002\u6211\u4eec\u5047\u8bbe\uff0c\u51c6\u786e\u5206\u5272\u76ee\u6807\u56fe\u50cf\u4e2d IAC \u7684 DF \u4e5f\u5c06\u6709\u52a9\u4e8e ANF \u7684\u57fa\u4e8e\u56fe\u96c6\u7684\u51c6\u786e\u5b9a\u4f4d\u3002\u4e0e VoxelMorph \u4e0d\u540c\uff0cVoxelMorph \u65e8\u5728\u751f\u6210\u51c6\u786e\u8bb0\u5f55\u6574\u4e2a\u4f53\u79ef\u7684 DF\uff0c\u6211\u4eec\u7684\u65b0\u9896\u8d21\u732e\u662f\u4e00\u79cd\u5b8c\u5168\u81ea\u6211\u76d1\u7763\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u65e8\u5728\u751f\u6210\u51c6\u786e\u5206\u5272\u76ee\u6807\u7ed3\u6784\u7684 DF\u3002\u4f7f\u7528\u533a\u57df\u6c34\u5e73\u96c6\uff08LS\uff09\u542f\u53d1\u7684\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u4fc3\u8fdb\u8fd9\u79cd\u81ea\u6211\u76d1\u7763\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a\u4f7f\u7528\u6c34\u5e73\u96c6\u7684\u57fa\u4e8e\u6df1\u5ea6\u56fe\u96c6\u7684\u5206\u5272\uff08DABS-LS\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0cDABS-LS \u5728 IAC \u5206\u5272\u65b9\u9762\u4f18\u4e8e VoxelMorph\u3002\u4f7f\u7528\u516c\u5f00\u7684\u6c14\u7ba1\u548c\u80be\u810f\u5206\u5272\u6570\u636e\u96c6\u8fdb\u884c\u7684\u6d4b\u8bd5\u4e5f\u663e\u793a\u51fa\u5206\u5272\u51c6\u786e\u6027\u7684\u663e\u7740\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002|[2402.10425v1](http://arxiv.org/pdf/2402.10425v1)|null|\n"}, "OCR": {}, "GNN": {}, "\u56fe\u50cf\u7406\u89e3": {"2402.10483": "|**2024-02-16**|**GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians**|GaussianHair\uff1a\u4f7f\u7528\u5149\u611f\u77e5\u9ad8\u65af\u6a21\u578b\u8fdb\u884c\u5934\u53d1\u5efa\u6a21\u548c\u6e32\u67d3|Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu|Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the \"GaussianHair Scattering Model\", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.|\u53d1\u578b\u4e4d\u4e00\u770b\u5c31\u53cd\u6620\u4e86\u6587\u5316\u548c\u79cd\u65cf\u3002\u5728\u6570\u5b57\u65f6\u4ee3\uff0c\u5404\u79cd\u903c\u771f\u7684\u4eba\u7c7b\u53d1\u578b\u5bf9\u4e8e\u9ad8\u4fdd\u771f\u6570\u5b57\u4eba\u7c7b\u8d44\u4ea7\u7684\u7f8e\u89c2\u6027\u548c\u5305\u5bb9\u6027\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5934\u53d1\u6570\u91cf\u5e9e\u5927\u3001\u51e0\u4f55\u7ed3\u6784\u590d\u6742\u4ee5\u53ca\u4e0e\u5149\u7ebf\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u903c\u771f\u7684\u5934\u53d1\u5efa\u6a21\u548c\u52a8\u753b\u5b9e\u65f6\u6e32\u67d3\u662f\u4e00\u9879\u8270\u5de8\u7684\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86 GaussianHair\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u663e\u5f0f\u5934\u53d1\u8868\u793a\u3002\u5b83\u53ef\u4ee5\u6839\u636e\u56fe\u50cf\u5bf9\u5934\u53d1\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u8fdb\u884c\u5168\u9762\u5efa\u6a21\uff0c\u4ece\u800c\u4fc3\u8fdb\u521b\u65b0\u7684\u7167\u660e\u6548\u679c\u548c\u52a8\u6001\u52a8\u753b\u529f\u80fd\u3002 GaussianHair \u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6982\u5ff5\uff0c\u5373\u5c06\u6bcf\u6839\u53d1\u4e1d\u8868\u793a\u4e3a\u4e00\u7cfb\u5217\u76f8\u8fde\u7684\u5706\u67f1\u5f62 3D \u9ad8\u65af\u57fa\u5143\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u7559\u4e86\u5934\u53d1\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u5916\u89c2\uff0c\u800c\u4e14\u8fd8\u5141\u8bb8\u5728 2D \u56fe\u50cf\u5e73\u9762\u4e0a\u8fdb\u884c\u6709\u6548\u7684\u5149\u6805\u5316\uff0c\u4ece\u800c\u4fc3\u8fdb\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u3002\u6211\u4eec\u901a\u8fc7\u201cGaussianHair Scattering Model\u201d\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u8be5\u6a21\u578b\uff0c\u64c5\u957f\u91cd\u5efa\u53d1\u4e1d\u7684\u7ec6\u957f\u7ed3\u6784\uff0c\u5e76\u5728\u5747\u5300\u7167\u660e\u4e0b\u51c6\u786e\u6355\u6349\u5176\u5c40\u90e8\u6f2b\u53cd\u5c04\u989c\u8272\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u5b9e GaussianHair \u5728\u51e0\u4f55\u548c\u5916\u89c2\u4fdd\u771f\u5ea6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5934\u53d1\u91cd\u5efa\u65b9\u6cd5\u6240\u9047\u5230\u7684\u9650\u5236\u3002\u9664\u4e86\u8868\u793a\u4e4b\u5916\uff0cGaussianHair \u8fd8\u652f\u6301\u5934\u53d1\u7684\u7f16\u8f91\u3001\u91cd\u65b0\u7167\u660e\u548c\u52a8\u6001\u6e32\u67d3\uff0c\u63d0\u4f9b\u4e0e\u4f20\u7edf CG \u7ba1\u9053\u5de5\u4f5c\u6d41\u7a0b\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u4e3a\u4e86\u8865\u5145\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u6211\u4eec\u7f16\u5236\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u7684\u771f\u5b9e\u4eba\u7c7b\u5934\u53d1\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u90fd\u5177\u6709\u7ec6\u81f4\u7684\u53d1\u4e1d\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|[2402.10483v1](http://arxiv.org/pdf/2402.10483v1)|null|\n", "2402.10404": "|**2024-02-16**|**Explaining generative diffusion models via visual analysis for interpretable decision-making process**|\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u89e3\u91ca\u751f\u6210\u6269\u6563\u6a21\u578b\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fc7\u7a0b|Ji-Hoon Park, Yeong-Joon Ju, Seong-Whan Lee|Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing the diffusion process and answering the aforementioned research questions to render the diffusion process human-understandable. We show how the output is progressively generated in the diffusion process by explaining the level of denoising and highlighting relationships to foundational visual concepts at each time step through the results of experiments with various visual analyses using the tools. Throughout the training of the diffusion model, the model learns diverse visual concepts corresponding to each time-step, enabling the model to predict varying levels of visual concepts at different stages. We substantiate our tools using Area Under Cover (AUC) score, correlation quantification, and cross-attention mapping. Our findings provide insights into the diffusion process and pave the way for further research into explainable diffusion mechanisms.|\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u89e3\u91ca\u6269\u6563\u8fc7\u7a0b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u7cfb\u5217\u53bb\u566a\u566a\u58f0\u56fe\u50cf\uff0c\u4e13\u5bb6\u5f88\u96be\u89e3\u91ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u7814\u7a76\u95ee\u9898\uff0c\u4ece\u6a21\u578b\u751f\u6210\u7684\u89c6\u89c9\u6982\u5ff5\u548c\u6a21\u578b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u53c2\u4e0e\u7684\u533a\u57df\u7684\u89d2\u5ea6\u6765\u89e3\u91ca\u6269\u6563\u8fc7\u7a0b\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u7528\u4e8e\u53ef\u89c6\u5316\u6269\u6563\u8fc7\u7a0b\u5e76\u56de\u7b54\u4e0a\u8ff0\u7814\u7a76\u95ee\u9898\u7684\u5de5\u5177\uff0c\u4ee5\u4f7f\u6269\u6563\u8fc7\u7a0b\u6613\u4e8e\u4eba\u7c7b\u7406\u89e3\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u5de5\u5177\u8fdb\u884c\u5404\u79cd\u89c6\u89c9\u5206\u6790\u7684\u5b9e\u9a8c\u7ed3\u679c\u89e3\u91ca\u53bb\u566a\u6c34\u5e73\u5e76\u5f3a\u8c03\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e0e\u57fa\u672c\u89c6\u89c9\u6982\u5ff5\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u5c55\u793a\u5982\u4f55\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u9010\u6b65\u751f\u6210\u8f93\u51fa\u3002\u5728\u6269\u6563\u6a21\u578b\u7684\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5b66\u4e60\u4e86\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5bf9\u5e94\u7684\u4e0d\u540c\u89c6\u89c9\u6982\u5ff5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u4e0d\u540c\u9636\u6bb5\u4e0d\u540c\u7ea7\u522b\u7684\u89c6\u89c9\u6982\u5ff5\u3002\u6211\u4eec\u4f7f\u7528\u8986\u76d6\u9762\u79ef (AUC) \u8bc4\u5206\u3001\u76f8\u5173\u6027\u91cf\u5316\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6620\u5c04\u6765\u8bc1\u5b9e\u6211\u4eec\u7684\u5de5\u5177\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5bf9\u6269\u6563\u8fc7\u7a0b\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u53ef\u89e3\u91ca\u7684\u6269\u6563\u673a\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002|[2402.10404v1](http://arxiv.org/pdf/2402.10404v1)|null|\n"}, "LLM": {}, "Transformer": {"2402.10885": "|**2024-02-16**|**3D Diffuser Actor: Policy Diffusion with 3D Scene Representations**|3D \u6269\u6563\u5668 Actor\uff1a\u5177\u6709 3D \u573a\u666f\u8868\u793a\u7684\u7b56\u7565\u6269\u6563|Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki|We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.|\u6211\u4eec\u5c06\u6269\u6563\u7b56\u7565\u548c 3D \u573a\u666f\u8868\u793a\u7ed3\u5408\u8d77\u6765\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u6269\u6563\u7b56\u7565\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ee5\u673a\u5668\u4eba\u548c\u73af\u5883\u72b6\u6001\u4e3a\u6761\u4ef6\u7684\u52a8\u4f5c\u5206\u5e03\u3002\u6700\u8fd1\uff0c\u5b83\u4eec\u7684\u8868\u73b0\u4f18\u4e8e\u786e\u5b9a\u6027\u548c\u66ff\u4ee3\u72b6\u6001\u6761\u4ef6\u52a8\u4f5c\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u3002 3D \u673a\u5668\u4eba\u7b56\u7565\u4f7f\u7528\u4f7f\u7528\u611f\u6d4b\u6df1\u5ea6\u4ece\u5355\u4e2a\u6216\u591a\u4e2a\u6444\u50cf\u673a\u89c6\u56fe\u805a\u5408\u7684 3D \u573a\u666f\u7279\u5f81\u8868\u793a\u3002\u5b83\u4eec\u5728\u5404\u4e2a\u6444\u50cf\u673a\u89c6\u70b9\u4e0a\u8868\u73b0\u51fa\u6bd4 2D \u540c\u884c\u66f4\u597d\u7684\u6982\u62ec\u80fd\u529b\u3002\u6211\u4eec\u7edf\u4e00\u4e86\u8fd9\u4e24\u6761\u5de5\u4f5c\u7ebf\uff0c\u5e76\u63d0\u51fa\u4e86 3D Diffuser Actor\uff0c\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u7b56\u7565\u67b6\u6784\uff0c\u5728\u7ed9\u5b9a\u8bed\u8a00\u6307\u4ee4\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u6784\u5efa\u89c6\u89c9\u573a\u666f\u53ca\u5176\u6761\u4ef6\u7684 3D \u8868\u793a\uff0c\u4ee5\u8fed\u4ee3\u5730\u5bf9\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684 3D \u65cb\u8f6c\u548c\u5e73\u79fb\u8fdb\u884c\u53bb\u566a\u3002\u5728\u6bcf\u6b21\u53bb\u566a\u8fed\u4ee3\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c06\u672b\u7aef\u6267\u884c\u5668\u59ff\u52bf\u4f30\u8ba1\u8868\u793a\u4e3a 3D \u573a\u666f\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u5bf9\u5176\u4ed6 3D \u89c6\u89c9\u548c\u8bed\u8a00\u6807\u8bb0\u7684 3D \u76f8\u5bf9\u5173\u6ce8\u6765\u5bf9\u5b83\u4eec\u8fdb\u884c\u7279\u5f81\u5316\uff0c\u4ece\u800c\u9884\u6d4b\u6bcf\u4e2a\u6807\u8bb0\u7684 3D \u5e73\u79fb\u548c\u65cb\u8f6c\u8bef\u5dee\u3002 3D Diffuser Actor \u5728 RLBench \u4e0a\u521b\u4e0b\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u591a\u89c6\u56fe\u8bbe\u7f6e\u4e0a\u6bd4\u5f53\u524d SOTA \u7684\u7edd\u5bf9\u6027\u80fd\u589e\u76ca\u4e3a 16.3%\uff0c\u5728\u5355\u89c6\u56fe\u8bbe\u7f6e\u4e0a\u7684\u7edd\u5bf9\u6027\u80fd\u589e\u76ca\u4e3a 13.1%\u3002\u5728 CALVIN \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u5728\u96f6\u6837\u672c\u672a\u89c1\u573a\u666f\u6cdb\u5316\u7684\u8bbe\u7f6e\u4e0a\u4f18\u4e8e\u5f53\u524d\u7684 SOTA\uff0c\u80fd\u591f\u6210\u529f\u8fd0\u884c 0.2 \u4e2a\u4ee5\u4e0a\u7684\u4efb\u52a1\uff0c\u76f8\u5bf9\u63d0\u9ad8\u4e86 7%\u3002\u901a\u8fc7\u4e00\u4e9b\u6f14\u793a\uff0c\u5b83\u4e5f\u53ef\u4ee5\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53d1\u6325\u4f5c\u7528\u3002\u6211\u4eec\u6d88\u9664\u4e86\u6a21\u578b\u7684\u67b6\u6784\u8bbe\u8ba1\u9009\u62e9\uff0c\u4f8b\u5982 3D \u573a\u666f\u7279\u5f81\u5316\u548c 3D \u76f8\u5bf9\u5173\u6ce8\uff0c\u5e76\u8868\u660e\u5b83\u4eec\u90fd\u6709\u52a9\u4e8e\u6cdb\u5316\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c3D \u573a\u666f\u8868\u793a\u548c\u5f3a\u5927\u7684\u751f\u6210\u5efa\u6a21\u662f\u673a\u5668\u4eba\u901a\u8fc7\u6f14\u793a\u8fdb\u884c\u9ad8\u6548\u5b66\u4e60\u7684\u5173\u952e\u3002|[2402.10885v1](http://arxiv.org/pdf/2402.10885v1)|null|\n", "2402.10798": "|**2024-02-16**|**VATr++: Choose Your Words Wisely for Handwritten Text Generation**|VATr++\uff1a\u660e\u667a\u5730\u9009\u62e9\u5355\u8bcd\u4ee5\u751f\u6210\u624b\u5199\u6587\u672c|Bram Vanherle, Vittorio Pippi, Silvia Cascianelli, Nick Michiels, Frank Van Reeth, Rita Cucchiara|Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluation protocol for HTG and conduct a comprehensive benchmarking of existing approaches. By doing so, we aim to establish a foundation for fair and meaningful comparisons between HTG strategies, fostering progress in the field.|\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u91c7\u7528 GAN\u3001Transformers \u4ee5\u53ca\u6700\u521d\u7684\u6269\u6563\u6a21\u578b\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u7684\u6210\u529f\uff0c\u98ce\u683c\u5316\u624b\u5199\u6587\u672c\u751f\u6210 (HTG) \u53d7\u5230\u4e86\u6781\u5927\u7684\u5173\u6ce8\u3002\u5c3d\u7ba1\u4eba\u4eec\u7684\u5174\u8da3\u6fc0\u589e\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u7684\u65b9\u9762\u2014\u2014\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u5bf9 HTG \u6a21\u578b\u8bad\u7ec3\u7684\u5f71\u54cd\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u540e\u7eed\u5f71\u54cd\u3002\u8fd9\u9879\u7814\u7a76\u6df1\u5165\u7814\u7a76\u4e86\u5c16\u7aef\u7684 Styled-HTG \u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u8f93\u5165\u51c6\u5907\u548c\u8bad\u7ec3\u6b63\u5219\u5316\u7684\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u5e76\u66f4\u597d\u5730\u6cdb\u5316\u3002\u8fd9\u4e9b\u65b9\u9762\u901a\u8fc7\u5bf9\u51e0\u79cd\u4e0d\u540c\u8bbe\u7f6e\u548c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5206\u6790\u5f97\u5230\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8d85\u8d8a\u4e86\u6027\u80fd\u4f18\u5316\u7684\u8303\u56f4\uff0c\u5e76\u89e3\u51b3\u4e86 HTG \u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u969c\u788d\u2014\u2014\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u534f\u8bae\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HTG \u8bc4\u4f30\u534f\u8bae\u7684\u6807\u51c6\u5316\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4e3a HTG \u7b56\u7565\u4e4b\u95f4\u7684\u516c\u5e73\u548c\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u5960\u5b9a\u57fa\u7840\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002|[2402.10798v1](http://arxiv.org/pdf/2402.10798v1)|null|\n", "2402.10739": "|**2024-02-16**|**PointMamba: A Simple State Space Model for Point Cloud Analysis**|PointMamba\uff1a\u7528\u4e8e\u70b9\u4e91\u5206\u6790\u7684\u7b80\u5355\u72b6\u6001\u7a7a\u95f4\u6a21\u578b|Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, Xiaoqing Ye, Xiang Bai|Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity and is difficult to extend to long sequence modeling due to limited computational resources and so on. Recently, state space models (SSM), a new family of deep sequence models, have presented great potential for sequence modeling in NLP tasks. In this paper, taking inspiration from the success of SSM in NLP, we propose PointMamba, a framework with global modeling and linear complexity. Specifically, by taking embedded point patches as input, we proposed a reordering strategy to enhance SSM's global modeling ability by providing a more logical geometric scanning order. The reordered point tokens are then sent to a series of Mamba blocks to causally capture the point cloud structure. Experimental results show our proposed PointMamba outperforms the transformer-based counterparts on different point cloud analysis datasets, while significantly saving about 44.3% parameters and 25% FLOPs, demonstrating the potential option for constructing foundational 3D vision models. We hope our PointMamba can provide a new perspective for point cloud analysis. The code is available at https://github.com/LMD0311/PointMamba.|Transformer \u51ed\u501f\u5176\u51fa\u8272\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u5df2\u6210\u4e3a\u70b9\u4e91\u5206\u6790\u4efb\u52a1\u7684\u57fa\u7840\u67b6\u6784\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7b49\u539f\u56e0\u5f88\u96be\u6269\u5c55\u5230\u957f\u5e8f\u5217\u5efa\u6a21\u3002\u6700\u8fd1\uff0c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\u5bb6\u65cf\uff0c\u5728 NLP \u4efb\u52a1\u4e2d\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u9762\u5c55\u73b0\u51fa\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u53d7\u5230 SSM \u5728 NLP \u4e2d\u6210\u529f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 PointMamba\uff0c\u4e00\u4e2a\u5177\u6709\u5168\u5c40\u5efa\u6a21\u548c\u7ebf\u6027\u590d\u6742\u6027\u7684\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u4ee5\u5d4c\u5165\u70b9\u8865\u4e01\u4f5c\u4e3a\u8f93\u5165\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u6392\u5e8f\u7b56\u7565\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u7b26\u5408\u903b\u8f91\u7684\u51e0\u4f55\u626b\u63cf\u987a\u5e8f\u6765\u589e\u5f3aSSM\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u3002\u7136\u540e\uff0c\u91cd\u65b0\u6392\u5e8f\u7684\u70b9\u4ee4\u724c\u88ab\u53d1\u9001\u5230\u4e00\u7cfb\u5217 Mamba \u5757\uff0c\u4ee5\u56e0\u679c\u5730\u6355\u83b7\u70b9\u4e91\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 PointMamba \u5728\u4e0d\u540c\u7684\u70b9\u4e91\u5206\u6790\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u4e8e Transformer \u7684\u5bf9\u5e94\u65b9\u6848\uff0c\u540c\u65f6\u663e\u7740\u8282\u7701\u4e86\u7ea6 44.3% \u7684\u53c2\u6570\u548c 25% \u7684 FLOP\uff0c\u5c55\u793a\u4e86\u6784\u5efa\u57fa\u7840 3D \u89c6\u89c9\u6a21\u578b\u7684\u6f5c\u5728\u9009\u62e9\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684PointMamba\u80fd\u591f\u4e3a\u70b9\u4e91\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/LMD0311/PointMamba \u83b7\u53d6\u3002|[2402.10739v1](http://arxiv.org/pdf/2402.10739v1)|null|\n"}, "3D/CG": {"2402.10865": "|**2024-02-16**|**Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds**|\u591a\u6a21\u578b 3D \u914d\u51c6\uff1a\u5728\u6742\u4e71\u7684\u70b9\u4e91\u4e2d\u67e5\u627e\u591a\u4e2a\u79fb\u52a8\u7269\u4f53|David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone|We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.|\u6211\u4eec\u7814\u7a76\u4e86 3D \u914d\u51c6\u95ee\u9898\u7684\u4e00\u79cd\u53d8\u4f53\uff0c\u79f0\u4e3a\u591a\u6a21\u578b 3D \u914d\u51c6\u3002\u5728\u591a\u6a21\u578b\u914d\u51c6\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u5f97\u5230\u4e24\u4e2a\u70b9\u4e91\uff0c\u63cf\u7ed8\u4e00\u7ec4\u5904\u4e8e\u4e0d\u540c\u59ff\u52bf\u7684\u5bf9\u8c61\uff08\u53ef\u80fd\u5305\u62ec\u5c5e\u4e8e\u80cc\u666f\u7684\u70b9\uff09\uff0c\u6211\u4eec\u5e0c\u671b\u540c\u65f6\u91cd\u5efa\u6240\u6709\u5bf9\u8c61\u5728\u4e24\u4e2a\u70b9\u4e91\u4e4b\u95f4\u7684\u79fb\u52a8\u65b9\u5f0f\u3002\u6b64\u8bbe\u7f6e\u6982\u62ec\u4e86\u6807\u51c6 3D \u914d\u51c6\uff0c\u5176\u4e2d\u4eba\u4eec\u60f3\u8981\u91cd\u5efa\u5355\u4e2a\u59ff\u52bf\uff0c\u4f8b\u5982\uff0c\u63cf\u7ed8\u9759\u6001\u573a\u666f\u7684\u4f20\u611f\u5668\u7684\u8fd0\u52a8\u3002\u6b64\u5916\uff0c\u5b83\u4e3a\u76f8\u5173\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6570\u5b66\u7684\u516c\u5f0f\uff0c\u4f8b\u5982\uff0c\u673a\u5668\u4eba\u4e0a\u7684\u6df1\u5ea6\u4f20\u611f\u5668\u611f\u77e5\u52a8\u6001\u573a\u666f\uff0c\u5e76\u5177\u6709\u4f30\u8ba1\u5176\u81ea\u8eab\u8fd0\u52a8\uff08\u4ece\u573a\u666f\u7684\u9759\u6001\u90e8\u5206\uff09\u7684\u76ee\u6807\uff0c\u540c\u65f6\u6062\u590d\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u3002\u6240\u6709\u52a8\u6001\u5bf9\u8c61\u3002\u6211\u4eec\u5047\u8bbe\u57fa\u4e8e\u5bf9\u5e94\u7684\u8bbe\u7f6e\uff0c\u5176\u4e2d\u4e24\u4e2a\u70b9\u4e91\u4e4b\u95f4\u6709\u5047\u5b9a\u7684\u5339\u914d\uff0c\u5e76\u8003\u8651\u8fd9\u4e9b\u5bf9\u5e94\u53d7\u5230\u5f02\u5e38\u503c\u56f0\u6270\u7684\u5b9e\u9645\u60c5\u51b5\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86 EM \u65b9\u6cd5\u6536\u655b\u5230\u57fa\u672c\u4e8b\u5b9e\u7684\u7406\u8bba\u6761\u4ef6\u3002\u6211\u4eec\u5728\u4ece\u684c\u9762\u573a\u666f\u5230\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u4e0e\u6700\u5148\u8fdb\u7684\u573a\u666f\u6d41\u65b9\u6cd5\u7ed3\u5408\u5efa\u7acb\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\u65f6\u7684\u6709\u6548\u6027\u3002|[2402.10865v1](http://arxiv.org/pdf/2402.10865v1)|null|\n", "2402.10636": "|**2024-02-16**|**PEGASUS: Personalized Generative 3D Avatars with Composable Attributes**|PEGASUS\uff1a\u5177\u6709\u53ef\u7ec4\u5408\u5c5e\u6027\u7684\u4e2a\u6027\u5316\u751f\u6210 3D \u5316\u8eab|Hyunsoo Cha, Byungjun Kim, Hanbyul Joo|We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources. As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity. We present two key approaches to achieve this goal. First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos. Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism. Subsequently, we introduce a zero-shot approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model.|\u6211\u4eec\u63d0\u51fa PEGASUS\uff0c\u4e00\u79cd\u4ece\u5355\u773c\u89c6\u9891\u6e90\u6784\u5efa\u4e2a\u6027\u5316\u751f\u6210 3D \u9762\u90e8\u5934\u50cf\u7684\u65b9\u6cd5\u3002\u4f5c\u4e3a\u4e00\u4e2a\u7ec4\u5408\u751f\u6210\u6a21\u578b\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5206\u79bb\u63a7\u5236\u6765\u9009\u62e9\u6027\u5730\u6539\u53d8\u76ee\u6807\u4e2a\u4f53\u7684\u9762\u90e8\u5c5e\u6027\uff08\u4f8b\u5982\u5934\u53d1\u6216\u9f3b\u5b50\uff09\uff0c\u540c\u65f6\u4fdd\u7559\u8eab\u4efd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u4e24\u79cd\u5173\u952e\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6784\u5efa\u5177\u6709\u4e0d\u540c\u9762\u90e8\u5c5e\u6027\u7684\u76ee\u6807\u8eab\u4efd\u7684\u5408\u6210\u89c6\u9891\u96c6\u5408\u6765\u6784\u5efa\u7279\u5b9a\u4e8e\u4eba\u7684\u751f\u6210 3D \u5934\u50cf\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u89c6\u9891\u662f\u901a\u8fc7\u501f\u7528\u5176\u4ed6\u5355\u773c\u89c6\u9891\u4e2d\u4e0d\u540c\u4e2a\u4f53\u7684\u90e8\u5206\u6765\u5408\u6210\u7684\u3002\u901a\u8fc7\u591a\u6b21\u5b9e\u9a8c\uff0c\u6211\u4eec\u901a\u8fc7\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u770b\u4e0d\u89c1\u7684\u5c5e\u6027\u6765\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u5353\u8d8a\u6027\u80fd\u3002\u968f\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5148\u524d\u6784\u5efa\u7684\u4e2a\u6027\u5316\u751f\u6210\u6a21\u578b\u6765\u66f4\u6709\u6548\u5730\u5b9e\u73b0\u76f8\u540c\u7684\u751f\u6210\u5efa\u6a21\u3002|[2402.10636v1](http://arxiv.org/pdf/2402.10636v1)|null|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2402.10609": "|**2024-02-16**|**U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model**|U$^2$MRPD\uff1a\u901a\u8fc7\u4fc3\u8fdb\u5927\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u6b20\u91c7\u6837 MRI \u91cd\u5efa|Ziqi Gao, S. Kevin Zhou|Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion methods on in-domain datasets while demonstrating the best generalizability on out-of-domain datasets. To the best of our knowledge, U$^2$MRPD is the {\\bf first} unsupervised method that demonstrates the universal prowess of a LLDM, %trained on magnitude-only natural images in medical imaging, attaining the best adaptability for both MRI database-free and database-available scenarios and generalizability towards out-of-domain data.|\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u5927\u578b\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LLDM\uff09\u4e2d\u7684\u9690\u5f0f\u89c6\u89c9\u77e5\u8bc6\u975e\u5e38\u4e30\u5bcc\uff0c\u5e76\u4e14\u5047\u8bbe\u5bf9\u4e8e\u81ea\u7136\u56fe\u50cf\u548c\u533b\u5b66\u56fe\u50cf\u6765\u8bf4\u662f\u901a\u7528\u7684\u3002\u4e3a\u4e86\u68c0\u9a8c\u8fd9\u4e00\u5047\u8bbe\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u6b20\u91c7\u6837 MRI \u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u9884\u5148\u8bad\u7ec3\u7684\u5927\u578b\u6f5c\u5728\u6269\u6563\u6a21\u578b (U$^2$MRPD)\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u3001\u6709\u76d1\u7763\u7684\u6b20\u91c7\u6837 MRI \u91cd\u5efa\u7f51\u7edc\u901a\u5e38\u5bf9\u4e0d\u540c\u6570\u636e\u91c7\u96c6\u573a\u666f\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u6709\u9650\uff1b\u7136\u800c\uff0cU$^2$MRPD \u901a\u8fc7\u4f7f\u7528\u4e13\u4e3a\u590d\u6742\u503c MRI \u56fe\u50cf\u5b9a\u5236\u7684 MRSampler \u63d0\u793a LLDM\uff0c\u652f\u6301\u56fe\u50cf\u7279\u5b9a\u7684 MRI \u91cd\u5efa\u3002\u5bf9\u4e8e\u4efb\u4f55\u5355\u6e90\u6216\u591a\u6e90 MRI \u6570\u636e\u96c6\uff0cMRAdapter \u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86 U$^2$MRPD \u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u56fe\u50cf\u5148\u9a8c\u5b8c\u6574\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cU$^2$MRPD \u5728\u57df\u5185\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u548c MRI \u6269\u6563\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u4f73\u7684\u901a\u7528\u6027\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0cU$^2$MRPD \u662f{\\bf\u7b2c\u4e00\u4e2a}\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86 LLDM \u7684\u901a\u7528\u80fd\u529b\uff0c%\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u4ec5\u5e45\u5ea6\u81ea\u7136\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u83b7\u5f97\u4e86\u5bf9 MRI \u548c MRI \u7684\u6700\u4f73\u9002\u5e94\u6027\u65e0\u6570\u636e\u5e93\u548c\u6570\u636e\u5e93\u53ef\u7528\u7684\u573a\u666f\u4ee5\u53ca\u5bf9\u57df\u5916\u6570\u636e\u7684\u901a\u7528\u6027\u3002|[2402.10609v1](http://arxiv.org/pdf/2402.10609v1)|null|\n"}, "\u5176\u4ed6": {"2402.10882": "|**2024-02-16**|**Universal Prompt Optimizer for Safe Text-to-Image Generation**|\u7528\u4e8e\u5b89\u5168\u751f\u6210\u6587\u672c\u5230\u56fe\u50cf\u7684\u901a\u7528\u63d0\u793a\u4f18\u5316\u5668|Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang|Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \\textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance.|\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f88\u5bb9\u6613\u53d7\u5230\u4e0d\u5b89\u5168\u8f93\u5165\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u751f\u6210\u6027\u3001\u9a9a\u6270\u548c\u975e\u6cd5\u6d3b\u52a8\u56fe\u50cf\u7b49\u4e0d\u5b89\u5168\u5185\u5bb9\u3002\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u68c0\u67e5\u5668\u3001\u6a21\u578b\u5fae\u8c03\u548c\u5d4c\u5165\u5757\u7684\u7814\u7a76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u56e0\u6b64\uff0c\\textit{\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u9ed1\u76d2\u573a\u666f\u4e2d\u5b89\u5168 T2I \u751f\u6210\u7684\u901a\u7528\u63d0\u793a\u4f18\u5316\u5668}\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7 GPT-3.5 Turbo \u6784\u5efa\u4e00\u4e2a\u7531\u6709\u6bd2-\u6e05\u6d01\u63d0\u793a\u5bf9\u7ec4\u6210\u7684\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u5f15\u5bfc\u4f18\u5316\u5668\u5177\u6709\u5c06\u6709\u6bd2\u63d0\u793a\u8f6c\u6362\u4e3a\u5e72\u51c0\u63d0\u793a\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7528\u4e8e\u6d4b\u91cf\u751f\u6210\u56fe\u50cf\u7684\u6bd2\u6027\u548c\u6587\u672c\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u6765\u8bad\u7ec3\u4f18\u5316\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u5404\u79cd T2I \u6a21\u578b\u751f\u6210\u4e0d\u9002\u5f53\u56fe\u50cf\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4e14\u5bf9\u6587\u672c\u5bf9\u9f50\u6ca1\u6709\u663e\u7740\u5f71\u54cd\u3002\u8fd8\u53ef\u4ee5\u7075\u6d3b\u5730\u4e0e\u65b9\u6cd5\u7ed3\u5408\uff0c\u4ee5\u8fbe\u5230\u66f4\u597d\u7684\u6027\u80fd\u3002|[2402.10882v1](http://arxiv.org/pdf/2402.10882v1)|null|\n", "2402.10747": "|**2024-02-16**|**Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting**|\u7528\u4e8e\u8fde\u7eed\u4e00\u81f4\u7269\u7406\u4fe1\u606f\u7684\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u7684\u5b8c\u5168\u53ef\u5fae\u62c9\u683c\u6717\u65e5\u5377\u79ef\u795e\u7ecf\u7f51\u7edc|Peter Pavl\u00edk, Martin V\u00fdboh, Anna Bou Ezzeddine, Viera Rozinajov\u00e1|This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u4e0e\u7269\u7406\u9886\u57df\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u63d0\u51fa\u4e86 LUPIN\uff0c\u4e00\u79cd\u7528\u4e8e\u7269\u7406\u4fe1\u606f\u4e34\u8fd1\u9884\u62a5\u7684\u62c9\u683c\u6717\u65e5\u53cc U-Net\uff0c\u5b83\u501f\u9274\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u5916\u63a8\u6cd5\u7684\u4e34\u8fd1\u9884\u62a5\u65b9\u6cd5\uff0c\u5e76\u4ee5\u5b8c\u5168\u53ef\u5fae\u5206\u548c GPU \u52a0\u901f\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u6570\u636e\u7684\u62c9\u683c\u6717\u65e5\u5750\u6807\u7cfb\u8f6c\u6362\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u9884\u62a5\u3002\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u6839\u636e\u6211\u4eec\u7684\u8bc4\u4f30\uff0cLUPINE \u5339\u914d\u5e76\u8d85\u8fc7\u4e86\u6240\u9009\u57fa\u51c6\u7684\u6027\u80fd\uff0c\u4e3a\u5176\u4ed6\u62c9\u683c\u6717\u65e5\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6253\u5f00\u4e86\u5927\u95e8\u3002|[2402.10747v1](http://arxiv.org/pdf/2402.10747v1)|null|\n", "2402.10491": "|**2024-02-16**|**Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation**|\u8fdb\u884c\u5ec9\u4ef7\u7684\u7f29\u653e\uff1a\u7528\u4e8e\u66f4\u9ad8\u5206\u8fa8\u7387\u9002\u5e94\u7684\u81ea\u7ea7\u8054\u6269\u6563\u6a21\u578b|Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et.al.|Diffusion models have proven to be highly effective in image and video generation; however, they still face composition challenges when generating images of varying sizes due to single-scale training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational and optimization resources, yet achieving a generation capability comparable to low-resolution models remains elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5X training speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with virtually no additional inference time.|\u6269\u6563\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u975e\u5e38\u6709\u6548\uff1b\u7136\u800c\uff0c\u7531\u4e8e\u5355\u5c3a\u5ea6\u8bad\u7ec3\u6570\u636e\uff0c\u5b83\u4eec\u5728\u751f\u6210\u4e0d\u540c\u5c3a\u5bf8\u7684\u56fe\u50cf\u65f6\u4ecd\u7136\u9762\u4e34\u6784\u56fe\u6311\u6218\u3002\u4f7f\u5927\u578b\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9002\u5e94\u66f4\u9ad8\u5206\u8fa8\u7387\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u548c\u4f18\u5316\u8d44\u6e90\uff0c\u4f46\u5b9e\u73b0\u4e0e\u4f4e\u5206\u8fa8\u7387\u6a21\u578b\u76f8\u5f53\u7684\u751f\u6210\u80fd\u529b\u4ecd\u7136\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u7ea7\u8054\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4ece\u8bad\u7ec3\u6709\u7d20\u7684\u4f4e\u5206\u8fa8\u7387\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u4e30\u5bcc\u77e5\u8bc6\u6765\u5feb\u901f\u9002\u5e94\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u91c7\u7528\u514d\u8c03\u6574\u6216\u5ec9\u4ef7\u7684\u4e0a\u91c7\u6837\u5668\u8c03\u6574\u8303\u4f8b\u3002\u81ea\u7ea7\u8054\u6269\u6563\u6a21\u578b\u96c6\u6210\u4e86\u4e00\u7cfb\u5217\u591a\u5c3a\u5ea6\u4e0a\u91c7\u6837\u5668\u6a21\u5757\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u9002\u5e94\u66f4\u9ad8\u5206\u8fa8\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u7ec4\u6210\u548c\u751f\u6210\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u67a2\u8f74\u5f15\u5bfc\u7684\u566a\u58f0\u91cd\u65b0\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u52a0\u5feb\u63a8\u7406\u8fc7\u7a0b\u5e76\u6539\u5584\u5c40\u90e8\u7ed3\u6784\u7ec6\u8282\u3002\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86 5 \u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u989d\u5916\u7684 0.002M \u8c03\u6574\u53c2\u6570\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u4ec5 10k \u6b65\u9aa4\u7684\u5fae\u8c03\u6765\u5feb\u901f\u9002\u5e94\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\uff0c\u51e0\u4e4e\u6ca1\u6709\u989d\u5916\u7684\u63a8\u7406\u65f6\u95f4\u3002|[2402.10491v1](http://arxiv.org/pdf/2402.10491v1)|null|\n", "2402.10470": "|**2024-02-16**|**Theoretical Understanding of Learning from Adversarial Perturbations**|\u4ece\u5bf9\u6297\u6027\u6270\u52a8\u4e2d\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3|Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki|It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations matches that from standard samples except for specific regions under mild conditions. The code is available at https://github.com/s-kumano/learning-from-adversarial-perturbations.|\u76ee\u524d\u5c1a\u4e0d\u5b8c\u5168\u7406\u89e3\u4e3a\u4ec0\u4e48\u5bf9\u6297\u6027\u4f8b\u5b50\u53ef\u4ee5\u6b3a\u9a97\u795e\u7ecf\u7f51\u7edc\u5e76\u5728\u4e0d\u540c\u7f51\u7edc\u4e4b\u95f4\u8f6c\u79fb\u3002\u4e3a\u4e86\u9610\u660e\u8fd9\u4e00\u70b9\uff0c\u4e00\u4e9b\u7814\u7a76\u5047\u8bbe\u5bf9\u6297\u6027\u6270\u52a8\u867d\u7136\u8868\u73b0\u4e3a\u566a\u97f3\uff0c\u4f46\u5305\u542b\u7c7b\u522b\u7279\u5f81\u3002\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u5728\u9519\u8bef\u6807\u8bb0\u7684\u5bf9\u6297\u6837\u672c\u4e0a\u8bad\u7ec3\u7684\u7f51\u7edc\u4ecd\u7136\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u6b63\u786e\u6807\u8bb0\u7684\u6d4b\u8bd5\u6837\u672c\u3002\u7136\u800c\uff0c\u5bf9\u6270\u52a8\u5982\u4f55\u5305\u542b\u7c7b\u7279\u5f81\u5e76\u6709\u52a9\u4e8e\u6cdb\u5316\u7684\u7406\u8bba\u7406\u89e3\u662f\u6709\u9650\u7684\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u4f7f\u7528\u5728\u76f8\u4e92\u6b63\u4ea4\u6837\u672c\u4e0a\u8bad\u7ec3\u7684\u5355\u9690\u85cf\u5c42\u7f51\u7edc\u4ece\u6270\u52a8\u4e2d\u5b66\u4e60\u3002\u6211\u4eec\u7684\u7ed3\u679c\u5f3a\u8c03\uff0c\u5404\u79cd\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u751a\u81f3\u51e0\u4e2a\u50cf\u7d20\u7684\u6270\u52a8\uff0c\u90fd\u5305\u542b\u8db3\u591f\u7684\u7c7b\u7279\u5f81\u4ee5\u8fdb\u884c\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u9664\u4e86\u6e29\u548c\u6761\u4ef6\u4e0b\u7684\u7279\u5b9a\u533a\u57df\u5916\uff0c\u4ece\u6270\u52a8\u4e2d\u5b66\u4e60\u65f6\u7684\u51b3\u7b56\u8fb9\u754c\u4e0e\u6807\u51c6\u6837\u672c\u4e2d\u7684\u51b3\u7b56\u8fb9\u754c\u76f8\u5339\u914d\u3002\u8be5\u4ee3\u7801\u53ef\u5728 https://github.com/s-kumano/learning-from-adversarial-perturbations \u83b7\u53d6\u3002|[2402.10470v1](http://arxiv.org/pdf/2402.10470v1)|null|\n", "2402.10403": "|**2024-02-16**|**Polyhedral Complex Derivation from Piecewise Trilinear Networks**|\u5206\u6bb5\u4e09\u7ebf\u6027\u7f51\u7edc\u7684\u591a\u9762\u4f53\u590d\u6570\u63a8\u5bfc|Jin-Hwa Kim|Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between the eikonal loss and the planarity of the hypersurfaces.|\u53ef\u89c6\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u65b0\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5bf9\u5176\u7ed3\u6784\u7684\u6df1\u5165\u4e86\u89e3\u4ee5\u53ca\u4ece\u8fde\u7eed\u5206\u6bb5\u4eff\u5c04 (CPWA) \u51fd\u6570\u4e2d\u7684\u7f51\u683c\u63d0\u53d6\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u795e\u7ecf\u8868\u9762\u8868\u793a\u5b66\u4e60\u7684\u53d1\u5c55\u7eb3\u5165\u4e86\u975e\u7ebf\u6027\u4f4d\u7f6e\u7f16\u7801\uff0c\u89e3\u51b3\u4e86\u5149\u8c31\u504f\u5dee\u7b49\u95ee\u9898\uff1b\u7136\u800c\uff0c\u8fd9\u5bf9\u5e94\u7528\u57fa\u4e8e CPWA \u51fd\u6570\u7684\u7f51\u683c\u63d0\u53d6\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u4f5c\u4e3a\u4f4d\u7f6e\u7f16\u7801\u7684\u4e09\u7ebf\u6027\u63d2\u503c\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u5206\u6790\u7f51\u683c\u63d0\u53d6\uff0c\u5c55\u793a\u4e86\u5728 eikonal \u7ea6\u675f\u4e0b\u4e09\u7ebf\u6027\u533a\u57df\u5185\u8d85\u66f2\u9762\u5230\u5e73\u9762\u7684\u53d8\u6362\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8fd1\u4f3c\u4e09\u4e2a\u8d85\u66f2\u9762\u4e4b\u95f4\u7684\u4ea4\u70b9\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u6211\u4eec\u901a\u8fc7\u5012\u89d2\u8ddd\u79bb\u548c\u6548\u7387\u4ee5\u53ca\u89d2\u8ddd\u79bb\u51ed\u7ecf\u9a8c\u9a8c\u8bc1\u6b63\u786e\u6027\u548c\u7b80\u7ea6\u6027\uff0c\u540c\u65f6\u68c0\u67e5 eikonal \u635f\u5931\u548c\u8d85\u66f2\u9762\u7684\u5e73\u9762\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002|[2402.10403v1](http://arxiv.org/pdf/2402.10403v1)|null|\n", "2402.10401": "|**2024-02-16**|**ManiFPT: Defining and Analyzing Fingerprints of Generative Models**|ManiFPT\uff1a\u5b9a\u4e49\u548c\u5206\u6790\u751f\u6210\u6a21\u578b\u7684\u6307\u7eb9|Hae Jin Song, Mahyar Khayatkhoei, Wael AbdAlmageed|Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.|\u6700\u8fd1\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u7684\u6837\u672c\u4e0a\u7559\u4e0b\u4e86\u5176\u6f5c\u5728\u751f\u6210\u8fc7\u7a0b\u7684\u75d5\u8ff9\uff0c\u5e7f\u6cdb\u5730\u79f0\u4e3a\u751f\u6210\u6a21\u578b\u7684\u6307\u7eb9\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u5728\u4ece\u771f\u5b9e\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u6548\u7528\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6307\u7eb9\u53ef\u4ee5\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u533a\u5206\u5404\u79cd\u7c7b\u578b\u7684\u5408\u6210\u56fe\u50cf\u5e76\u5e2e\u52a9\u8bc6\u522b\u6f5c\u5728\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u4f46\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u7279\u522b\u662f\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6307\u7eb9\u7684\u5b9a\u4e49\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u4e3a\u6b64\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f62\u5f0f\u5316\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u5de5\u4ef6\u548c\u6307\u7eb9\u7684\u5b9a\u4e49\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5b9e\u8df5\u4e2d\u8ba1\u7b97\u5b83\u4eec\u7684\u7b97\u6cd5\uff0c\u5e76\u6700\u7ec8\u7814\u7a76\u4e86\u5176\u5728\u533a\u5206\u5927\u91cf\u4e0d\u540c\u751f\u6210\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u5b9a\u4e49\u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u4ece\u6837\u672c\uff08\u6a21\u578b\u5f52\u56e0\uff09\u4e2d\u8bc6\u522b\u6f5c\u5728\u751f\u6210\u8fc7\u7a0b\u7684\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6307\u7eb9\u7684\u7ed3\u6784\uff0c\u5e76\u89c2\u5bdf\u5230\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u9884\u6d4b\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002|[2402.10401v1](http://arxiv.org/pdf/2402.10401v1)|null|\n", "2402.10376": "|**2024-02-16**|**Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)**|\u7528\u7a00\u758f\u7ebf\u6027\u6982\u5ff5\u5d4c\u5165\u89e3\u91ca CLIP (SpLiCE)|Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju|CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.|CLIP \u5d4c\u5165\u5728\u5e7f\u6cdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u9ad8\u7ef4\u3001\u5bc6\u96c6\u7684\u5411\u91cf\u8868\u793a\u4e0d\u5bb9\u6613\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u900f\u660e\u5ea6\u7684\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u7528\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u51ed\u7ecf\u9a8c\u8bc1\u660e CLIP \u7684\u6f5c\u5728\u7a7a\u95f4\u662f\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\uff0c\u56e0\u6b64 CLIP \u8868\u793a\u53ef\u4ee5\u5206\u89e3\u4e3a\u5176\u5e95\u5c42\u8bed\u4e49\u7ec4\u4ef6\u3002\u6211\u4eec\u5229\u7528\u8fd9\u79cd\u7406\u89e3\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u7a00\u758f\u7ebf\u6027\u6982\u5ff5\u5d4c\u5165\uff08SpLiCE\uff09\uff0c\u7528\u4e8e\u5c06 CLIP \u8868\u793a\u8f6c\u6362\u4e3a\u4eba\u7c7b\u53ef\u89e3\u91ca\u6982\u5ff5\u7684\u7a00\u758f\u7ebf\u6027\u7ec4\u5408\u3002\u4e0e\u4ee5\u524d\u7684\u5de5\u4f5c\u4e0d\u540c\uff0cSpLiCE \u4e0d\u9700\u8981\u6982\u5ff5\u6807\u7b7e\uff0c\u53ef\u4ee5\u4e8b\u540e\u5e94\u7528\u3002\u901a\u8fc7\u5bf9\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86 SpLiCE \u8f93\u51fa\u7684\u8868\u793a\u53ef\u4ee5\u89e3\u91ca\u751a\u81f3\u53d6\u4ee3\u4f20\u7edf\u7684\u5bc6\u96c6 CLIP \u8868\u793a\uff0c\u4fdd\u6301\u7b49\u6548\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u540c\u65f6\u663e\u7740\u63d0\u9ad8\u5176\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u8fd8\u6f14\u793a\u4e86 SpLiCE \u8868\u793a\u7684\u51e0\u4e2a\u7528\u4f8b\uff0c\u5305\u62ec\u68c0\u6d4b\u865a\u5047\u76f8\u5173\u6027\u3001\u6a21\u578b\u7f16\u8f91\u548c\u91cf\u5316\u6570\u636e\u96c6\u4e2d\u7684\u8bed\u4e49\u53d8\u5316\u3002|[2402.10376v1](http://arxiv.org/pdf/2402.10376v1)|null|\n"}}